>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphMVP/freesolv/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphMVP/freesolv/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphMVP/freesolv/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphMVP/freesolv/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.05.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.0/freesolv_scaff_6_26-05_11-15-58  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.5394926071167
RMSE train: 4.617024	val: 8.711005	test: 6.813391
MAE train: 3.707607	val: 6.785074	test: 5.955575

Epoch: 2
Loss: 20.215264320373535
RMSE train: 4.449906	val: 8.484043	test: 6.640909
MAE train: 3.569943	val: 6.623210	test: 5.826103

Epoch: 3
Loss: 18.783740997314453
RMSE train: 4.294794	val: 8.264502	test: 6.502775
MAE train: 3.449865	val: 6.459964	test: 5.732410

Epoch: 4
Loss: 17.7384033203125
RMSE train: 4.164869	val: 8.036198	test: 6.384430
MAE train: 3.346480	val: 6.296812	test: 5.683603

Epoch: 5
Loss: 16.236762046813965
RMSE train: 4.066913	val: 7.789096	test: 6.273165
MAE train: 3.267391	val: 6.197261	test: 5.635903

Epoch: 6
Loss: 14.848833084106445
RMSE train: 3.978758	val: 7.463634	test: 6.122579
MAE train: 3.199150	val: 6.106095	test: 5.524095

Epoch: 7
Loss: 13.714852809906006
RMSE train: 3.849908	val: 6.940269	test: 5.878834
MAE train: 3.121096	val: 5.816781	test: 5.303080

Epoch: 8
Loss: 12.300006866455078
RMSE train: 3.614836	val: 6.123859	test: 5.507155
MAE train: 2.996021	val: 5.219129	test: 4.922916

Epoch: 9
Loss: 11.162740707397461
RMSE train: 3.413906	val: 5.333450	test: 5.201770
MAE train: 2.901208	val: 4.646864	test: 4.556528

Epoch: 10
Loss: 10.712295532226562
RMSE train: 3.224711	val: 4.731176	test: 4.975667
MAE train: 2.779371	val: 4.155894	test: 4.284428

Epoch: 11
Loss: 10.533536434173584
RMSE train: 3.045649	val: 4.276784	test: 4.762621
MAE train: 2.620854	val: 3.742023	test: 4.092909

Epoch: 12
Loss: 9.895092964172363
RMSE train: 2.901858	val: 4.002956	test: 4.537247
MAE train: 2.486772	val: 3.512931	test: 3.853464

Epoch: 13
Loss: 9.418049097061157
RMSE train: 2.771030	val: 3.726326	test: 4.177401
MAE train: 2.386935	val: 3.298793	test: 3.508630

Epoch: 14
Loss: 8.895173788070679
RMSE train: 2.743557	val: 3.630899	test: 3.916318
MAE train: 2.390374	val: 3.208933	test: 3.266331

Epoch: 15
Loss: 8.47109603881836
RMSE train: 2.766722	val: 3.602337	test: 3.679118
MAE train: 2.438093	val: 3.127714	test: 3.053184

Epoch: 16
Loss: 7.958093881607056
RMSE train: 2.902572	val: 3.797887	test: 3.722253
MAE train: 2.607242	val: 3.271041	test: 3.091332

Epoch: 17
Loss: 7.82094144821167
RMSE train: 2.965905	val: 3.871123	test: 3.681939
MAE train: 2.678730	val: 3.317106	test: 3.052105

Epoch: 18
Loss: 7.415974378585815
RMSE train: 2.933560	val: 3.806083	test: 3.526048
MAE train: 2.653147	val: 3.217225	test: 2.927795

Epoch: 19
Loss: 7.140003681182861
RMSE train: 2.887775	val: 3.708679	test: 3.424381
MAE train: 2.617065	val: 3.133651	test: 2.846552

Epoch: 20
Loss: 6.761472702026367
RMSE train: 2.839347	val: 3.672675	test: 3.414968
MAE train: 2.566502	val: 3.132324	test: 2.853417

Epoch: 21
Loss: 6.268577337265015
RMSE train: 2.822576	val: 3.665433	test: 3.509929
MAE train: 2.544958	val: 3.174471	test: 2.950499

Epoch: 22
Loss: 6.190694093704224
RMSE train: 2.704237	val: 3.496627	test: 3.497692
MAE train: 2.430761	val: 3.038549	test: 2.942903Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.0/freesolv_scaff_5_26-05_11-15-58  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 22.109490394592285
RMSE train: 4.724022	val: 8.619238	test: 6.778067
MAE train: 3.803451	val: 6.713209	test: 5.936116

Epoch: 2
Loss: 20.858839988708496
RMSE train: 4.618945	val: 8.403418	test: 6.618534
MAE train: 3.727813	val: 6.544380	test: 5.833344

Epoch: 3
Loss: 19.56582260131836
RMSE train: 4.507197	val: 8.163933	test: 6.443485
MAE train: 3.656520	val: 6.348942	test: 5.712280

Epoch: 4
Loss: 18.284968376159668
RMSE train: 4.430767	val: 7.954991	test: 6.277380
MAE train: 3.614813	val: 6.213648	test: 5.610353

Epoch: 5
Loss: 16.77308464050293
RMSE train: 4.385991	val: 7.732470	test: 6.107292
MAE train: 3.599698	val: 6.105837	test: 5.507106

Epoch: 6
Loss: 15.463603973388672
RMSE train: 4.322701	val: 7.400597	test: 5.912313
MAE train: 3.585669	val: 5.994048	test: 5.335789

Epoch: 7
Loss: 14.012449741363525
RMSE train: 4.200444	val: 6.876630	test: 5.670362
MAE train: 3.584257	val: 5.730267	test: 5.040085

Epoch: 8
Loss: 12.538242816925049
RMSE train: 4.031303	val: 6.245225	test: 5.440504
MAE train: 3.554867	val: 5.319255	test: 4.733713

Epoch: 9
Loss: 11.821358680725098
RMSE train: 3.912358	val: 5.687058	test: 5.309602
MAE train: 3.496524	val: 4.982383	test: 4.563753

Epoch: 10
Loss: 11.226573944091797
RMSE train: 3.849849	val: 5.382821	test: 5.271461
MAE train: 3.453443	val: 4.774753	test: 4.522716

Epoch: 11
Loss: 10.461074352264404
RMSE train: 3.710348	val: 5.121538	test: 5.117256
MAE train: 3.333358	val: 4.544463	test: 4.433180

Epoch: 12
Loss: 10.004709720611572
RMSE train: 3.583446	val: 4.987572	test: 4.973386
MAE train: 3.231332	val: 4.456087	test: 4.356938

Epoch: 13
Loss: 9.488273620605469
RMSE train: 3.531718	val: 5.013681	test: 4.870261
MAE train: 3.208710	val: 4.504786	test: 4.277958

Epoch: 14
Loss: 9.089133262634277
RMSE train: 3.539817	val: 5.225298	test: 4.851906
MAE train: 3.220890	val: 4.746740	test: 4.260776

Epoch: 15
Loss: 8.603742122650146
RMSE train: 3.488622	val: 5.251844	test: 4.738006
MAE train: 3.161118	val: 4.790242	test: 4.176115

Epoch: 16
Loss: 8.37578010559082
RMSE train: 3.413603	val: 5.095714	test: 4.542115
MAE train: 3.093468	val: 4.660153	test: 3.995147

Epoch: 17
Loss: 7.844818115234375
RMSE train: 3.330441	val: 4.818022	test: 4.316913
MAE train: 3.030919	val: 4.387998	test: 3.772799

Epoch: 18
Loss: 7.500793695449829
RMSE train: 3.276360	val: 4.564539	test: 4.177795
MAE train: 2.995695	val: 4.142644	test: 3.637835

Epoch: 19
Loss: 7.233494997024536
RMSE train: 3.202349	val: 4.326887	test: 4.059450
MAE train: 2.937262	val: 3.919689	test: 3.533489

Epoch: 20
Loss: 6.914414882659912
RMSE train: 3.118687	val: 4.141436	test: 3.959033
MAE train: 2.865447	val: 3.750705	test: 3.441847

Epoch: 21
Loss: 6.403299808502197
RMSE train: 3.036461	val: 4.069454	test: 3.887138
MAE train: 2.779395	val: 3.682113	test: 3.365341

Epoch: 22
Loss: 6.197184801101685
RMSE train: 2.937429	val: 4.003373	test: 3.807055
MAE train: 2.681499	val: 3.610279	test: 3.276483Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.0/freesolv_scaff_4_26-05_11-15-58  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.91951274871826
RMSE train: 4.669747	val: 8.647097	test: 6.800428
MAE train: 3.800133	val: 6.825343	test: 5.988528

Epoch: 2
Loss: 19.600561141967773
RMSE train: 4.540128	val: 8.426399	test: 6.611259
MAE train: 3.693672	val: 6.666173	test: 5.841218

Epoch: 3
Loss: 18.292670726776123
RMSE train: 4.414591	val: 8.195700	test: 6.412706
MAE train: 3.590599	val: 6.511182	test: 5.686654

Epoch: 4
Loss: 17.001766204833984
RMSE train: 4.271345	val: 7.901250	test: 6.188312
MAE train: 3.485115	val: 6.299253	test: 5.500828

Epoch: 5
Loss: 15.54918622970581
RMSE train: 4.110796	val: 7.526641	test: 5.957187
MAE train: 3.393421	val: 6.013320	test: 5.281373

Epoch: 6
Loss: 14.25002145767212
RMSE train: 3.939843	val: 7.072454	test: 5.739432
MAE train: 3.325301	val: 5.671856	test: 5.048092

Epoch: 7
Loss: 12.73222017288208
RMSE train: 3.818074	val: 6.627311	test: 5.569934
MAE train: 3.281911	val: 5.408249	test: 4.860051

Epoch: 8
Loss: 11.666475772857666
RMSE train: 3.691413	val: 6.166538	test: 5.394525
MAE train: 3.221626	val: 5.156008	test: 4.680063

Epoch: 9
Loss: 10.881729125976562
RMSE train: 3.542899	val: 5.788832	test: 5.219522
MAE train: 3.130914	val: 4.949875	test: 4.491487

Epoch: 10
Loss: 10.252256870269775
RMSE train: 3.400020	val: 5.491952	test: 5.025747
MAE train: 3.027896	val: 4.757773	test: 4.267006

Epoch: 11
Loss: 9.870138168334961
RMSE train: 3.293216	val: 5.362503	test: 4.850219
MAE train: 2.942208	val: 4.706015	test: 4.105111

Epoch: 12
Loss: 9.601674556732178
RMSE train: 3.204173	val: 5.284521	test: 4.751781
MAE train: 2.861025	val: 4.664449	test: 4.014899

Epoch: 13
Loss: 9.103757858276367
RMSE train: 3.104446	val: 5.209088	test: 4.629885
MAE train: 2.757608	val: 4.596475	test: 3.899779

Epoch: 14
Loss: 8.529896020889282
RMSE train: 3.007330	val: 5.165465	test: 4.512730
MAE train: 2.671334	val: 4.532426	test: 3.812594

Epoch: 15
Loss: 8.269965887069702
RMSE train: 2.991527	val: 5.268301	test: 4.481396
MAE train: 2.672857	val: 4.587807	test: 3.812300

Epoch: 16
Loss: 7.829286575317383
RMSE train: 2.982612	val: 5.261096	test: 4.419605
MAE train: 2.666087	val: 4.522683	test: 3.786614

Epoch: 17
Loss: 7.473523139953613
RMSE train: 2.869986	val: 5.039046	test: 4.259125
MAE train: 2.561011	val: 4.287491	test: 3.632247

Epoch: 18
Loss: 6.993819236755371
RMSE train: 2.753433	val: 4.765495	test: 4.095091
MAE train: 2.445093	val: 4.001146	test: 3.474414

Epoch: 19
Loss: 6.649248838424683
RMSE train: 2.657466	val: 4.499718	test: 3.929049
MAE train: 2.340422	val: 3.702282	test: 3.302485

Epoch: 20
Loss: 6.419580936431885
RMSE train: 2.632641	val: 4.415131	test: 3.851323
MAE train: 2.330480	val: 3.593041	test: 3.218185

Epoch: 21
Loss: 6.059041261672974
RMSE train: 2.626561	val: 4.473907	test: 3.819251
MAE train: 2.352243	val: 3.646380	test: 3.170250

Epoch: 22
Loss: 5.5107197761535645
RMSE train: 2.577182	val: 4.484733	test: 3.747195
MAE train: 2.308743	val: 3.646781	test: 3.076022Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.1/freesolv_scaff_4_26-05_11-15-58  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.69050121307373
RMSE train: 4.353528	val: 8.339281	test: 6.366237
MAE train: 3.429296	val: 6.370985	test: 5.486798

Epoch: 2
Loss: 19.26314067840576
RMSE train: 4.197174	val: 8.128659	test: 6.213146
MAE train: 3.304343	val: 6.216514	test: 5.378144

Epoch: 3
Loss: 18.070090293884277
RMSE train: 4.054724	val: 7.914520	test: 6.098463
MAE train: 3.206711	val: 6.052958	test: 5.309117

Epoch: 4
Loss: 16.91497230529785
RMSE train: 3.957342	val: 7.699404	test: 6.025232
MAE train: 3.154113	val: 5.881004	test: 5.288113

Epoch: 5
Loss: 15.742713451385498
RMSE train: 3.885214	val: 7.469818	test: 5.971731
MAE train: 3.134160	val: 5.827850	test: 5.281360

Epoch: 6
Loss: 14.777233123779297
RMSE train: 3.820080	val: 7.194451	test: 5.904508
MAE train: 3.133860	val: 5.765137	test: 5.223041

Epoch: 7
Loss: 13.575889587402344
RMSE train: 3.751441	val: 6.859495	test: 5.809990
MAE train: 3.129050	val: 5.591458	test: 5.113002

Epoch: 8
Loss: 12.826088428497314
RMSE train: 3.635661	val: 6.470406	test: 5.665137
MAE train: 3.048889	val: 5.311602	test: 4.939337

Epoch: 9
Loss: 12.33448839187622
RMSE train: 3.480012	val: 6.125788	test: 5.544273
MAE train: 2.907178	val: 5.043184	test: 4.773550

Epoch: 10
Loss: 11.58542013168335
RMSE train: 3.253486	val: 5.740479	test: 5.319352
MAE train: 2.685291	val: 4.690884	test: 4.526098

Epoch: 11
Loss: 11.345053672790527
RMSE train: 2.951910	val: 5.320892	test: 4.982493
MAE train: 2.381680	val: 4.233824	test: 4.182797

Epoch: 12
Loss: 10.653945922851562
RMSE train: 2.626738	val: 4.830267	test: 4.524548
MAE train: 2.062714	val: 3.680915	test: 3.719565

Epoch: 13
Loss: 9.849126815795898
RMSE train: 2.366960	val: 4.322187	test: 4.055802
MAE train: 1.843465	val: 3.174626	test: 3.271417

Epoch: 14
Loss: 9.535228252410889
RMSE train: 2.168962	val: 3.960096	test: 3.655064
MAE train: 1.701503	val: 2.765186	test: 2.853362

Epoch: 15
Loss: 8.96238374710083
RMSE train: 2.089942	val: 3.825296	test: 3.509392
MAE train: 1.645024	val: 2.683888	test: 2.738533

Epoch: 16
Loss: 8.562563419342041
RMSE train: 2.090477	val: 3.803082	test: 3.497895
MAE train: 1.647709	val: 2.701745	test: 2.769593

Epoch: 17
Loss: 8.232457637786865
RMSE train: 2.138629	val: 3.773932	test: 3.501011
MAE train: 1.711687	val: 2.714421	test: 2.801298

Epoch: 18
Loss: 7.7731733322143555
RMSE train: 2.208420	val: 3.769807	test: 3.551398
MAE train: 1.795294	val: 2.786938	test: 2.904972

Epoch: 19
Loss: 7.8505635261535645
RMSE train: 2.272013	val: 3.869362	test: 3.646810
MAE train: 1.866936	val: 2.930732	test: 3.035533

Epoch: 20
Loss: 6.785022020339966
RMSE train: 2.292008	val: 3.958746	test: 3.671868
MAE train: 1.899830	val: 2.973083	test: 3.070161

Epoch: 21
Loss: 6.526458978652954
RMSE train: 2.283051	val: 4.012263	test: 3.651949
MAE train: 1.904041	val: 2.947564	test: 3.037786

Epoch: 22
Loss: 6.4926042556762695Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.1/freesolv_scaff_6_26-05_11-15-58  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.874985694885254
RMSE train: 4.285877	val: 8.344015	test: 6.428569
MAE train: 3.410886	val: 6.383098	test: 5.511492

Epoch: 2
Loss: 19.909324645996094
RMSE train: 4.170314	val: 8.137522	test: 6.253652
MAE train: 3.315972	val: 6.242900	test: 5.391074

Epoch: 3
Loss: 18.664958953857422
RMSE train: 4.053920	val: 7.902262	test: 6.096029
MAE train: 3.225968	val: 6.076453	test: 5.280254

Epoch: 4
Loss: 17.198952674865723
RMSE train: 3.947459	val: 7.620118	test: 5.965552
MAE train: 3.155217	val: 5.881025	test: 5.210867

Epoch: 5
Loss: 16.091039657592773
RMSE train: 3.869473	val: 7.305554	test: 5.884280
MAE train: 3.123349	val: 5.710976	test: 5.188617

Epoch: 6
Loss: 15.059728145599365
RMSE train: 3.850339	val: 6.984314	test: 5.874514
MAE train: 3.162793	val: 5.611689	test: 5.215620

Epoch: 7
Loss: 13.657267570495605
RMSE train: 3.870395	val: 6.694722	test: 5.946882
MAE train: 3.249502	val: 5.519618	test: 5.305310

Epoch: 8
Loss: 12.900993347167969
RMSE train: 3.876849	val: 6.414575	test: 6.049828
MAE train: 3.321017	val: 5.378206	test: 5.408223

Epoch: 9
Loss: 12.55953598022461
RMSE train: 3.858650	val: 6.198047	test: 6.142877
MAE train: 3.336639	val: 5.249287	test: 5.503691

Epoch: 10
Loss: 11.82995319366455
RMSE train: 3.679876	val: 5.767449	test: 5.957909
MAE train: 3.181757	val: 4.881081	test: 5.322837

Epoch: 11
Loss: 11.204238414764404
RMSE train: 3.476067	val: 5.304884	test: 5.630554
MAE train: 2.994828	val: 4.432064	test: 4.988503

Epoch: 12
Loss: 10.80481481552124
RMSE train: 3.293729	val: 5.079570	test: 5.385784
MAE train: 2.826835	val: 4.192884	test: 4.777872

Epoch: 13
Loss: 10.132643699645996
RMSE train: 3.118892	val: 4.970796	test: 5.146026
MAE train: 2.663952	val: 4.058855	test: 4.590962

Epoch: 14
Loss: 9.92788314819336
RMSE train: 2.992989	val: 4.792490	test: 4.824509
MAE train: 2.552344	val: 3.887903	test: 4.288276

Epoch: 15
Loss: 9.474676609039307
RMSE train: 2.939318	val: 4.597284	test: 4.535277
MAE train: 2.504741	val: 3.687668	test: 3.974652

Epoch: 16
Loss: 8.772998332977295
RMSE train: 2.892954	val: 4.415931	test: 4.289260
MAE train: 2.466094	val: 3.478821	test: 3.657939

Epoch: 17
Loss: 8.61683440208435
RMSE train: 2.855490	val: 4.324901	test: 4.205698
MAE train: 2.433786	val: 3.360333	test: 3.516359

Epoch: 18
Loss: 8.314631223678589
RMSE train: 2.830496	val: 4.278423	test: 4.219932
MAE train: 2.422986	val: 3.369622	test: 3.519541

Epoch: 19
Loss: 7.622704267501831
RMSE train: 2.808882	val: 4.285394	test: 4.290449
MAE train: 2.411871	val: 3.407034	test: 3.597378

Epoch: 20
Loss: 7.2239978313446045
RMSE train: 2.761386	val: 4.242262	test: 4.325163
MAE train: 2.380153	val: 3.393018	test: 3.650576

Epoch: 21
Loss: 6.917783737182617
RMSE train: 2.733177	val: 4.245061	test: 4.390455
MAE train: 2.366173	val: 3.474875	test: 3.786265

Epoch: 22
Loss: 6.78756308555603Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.2/freesolv_scaff_5_26-05_11-15-58  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.40857219696045
RMSE train: 4.488817	val: 8.675259	test: 6.859394
MAE train: 3.582973	val: 6.794407	test: 6.031719

Epoch: 2
Loss: 20.381427764892578
RMSE train: 4.380291	val: 8.509032	test: 6.718007
MAE train: 3.489793	val: 6.682934	test: 5.930944

Epoch: 3
Loss: 19.54701519012451
RMSE train: 4.256186	val: 8.304090	test: 6.587690
MAE train: 3.384713	val: 6.521982	test: 5.841456

Epoch: 4
Loss: 18.579691886901855
RMSE train: 4.105802	val: 7.988587	test: 6.430940
MAE train: 3.256342	val: 6.213869	test: 5.715886

Epoch: 5
Loss: 17.68680477142334
RMSE train: 3.964168	val: 7.624605	test: 6.261558
MAE train: 3.138587	val: 5.846711	test: 5.546383

Epoch: 6
Loss: 16.616326808929443
RMSE train: 3.819063	val: 7.267577	test: 6.110096
MAE train: 3.023958	val: 5.516773	test: 5.351710

Epoch: 7
Loss: 15.7329683303833
RMSE train: 3.702104	val: 6.986637	test: 6.060272
MAE train: 2.938433	val: 5.329357	test: 5.243992

Epoch: 8
Loss: 14.998359680175781
RMSE train: 3.624387	val: 6.793140	test: 6.214417
MAE train: 2.891072	val: 5.379620	test: 5.422641

Epoch: 9
Loss: 13.991017818450928
RMSE train: 3.558551	val: 6.678314	test: 6.444275
MAE train: 2.849767	val: 5.511018	test: 5.672036

Epoch: 10
Loss: 13.703677654266357
RMSE train: 3.492029	val: 6.570715	test: 6.599863
MAE train: 2.802220	val: 5.552628	test: 5.838271

Epoch: 11
Loss: 12.965340614318848
RMSE train: 3.360464	val: 6.330922	test: 6.522431
MAE train: 2.695356	val: 5.394534	test: 5.755802

Epoch: 12
Loss: 12.092469215393066
RMSE train: 3.204821	val: 5.986383	test: 6.272516
MAE train: 2.570212	val: 5.054449	test: 5.475182

Epoch: 13
Loss: 11.463384628295898
RMSE train: 3.082420	val: 5.802759	test: 6.133616
MAE train: 2.468244	val: 4.870429	test: 5.311962

Epoch: 14
Loss: 11.10700511932373
RMSE train: 3.018256	val: 5.675439	test: 6.000757
MAE train: 2.424609	val: 4.715345	test: 5.184885

Epoch: 15
Loss: 10.410830020904541
RMSE train: 2.940115	val: 5.546312	test: 5.876157
MAE train: 2.370393	val: 4.543724	test: 5.060590

Epoch: 16
Loss: 10.132461071014404
RMSE train: 2.872871	val: 5.478631	test: 5.818662
MAE train: 2.325421	val: 4.447070	test: 5.017610

Epoch: 17
Loss: 9.444554328918457
RMSE train: 2.803573	val: 5.417177	test: 5.718162
MAE train: 2.273500	val: 4.331169	test: 4.916915

Epoch: 18
Loss: 8.994892597198486
RMSE train: 2.706357	val: 5.286438	test: 5.548126
MAE train: 2.195458	val: 4.158987	test: 4.728420

Epoch: 19
Loss: 8.509973764419556
RMSE train: 2.617983	val: 5.116731	test: 5.413454
MAE train: 2.130854	val: 4.014414	test: 4.588516

Epoch: 20
Loss: 8.044531345367432
RMSE train: 2.512956	val: 4.930144	test: 5.258733
MAE train: 2.047763	val: 3.849389	test: 4.414974

Epoch: 21
Loss: 7.964614629745483
RMSE train: 2.446364	val: 4.887723	test: 5.229216
MAE train: 2.003428	val: 3.838002	test: 4.381091

Epoch: 22
Loss: 7.312695026397705Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.2/freesolv_scaff_6_26-05_11-15-58  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.061043739318848
RMSE train: 4.333812	val: 8.362820	test: 6.471715
MAE train: 3.450250	val: 6.405607	test: 5.552934

Epoch: 2
Loss: 20.108427047729492
RMSE train: 4.237845	val: 8.163545	test: 6.334886
MAE train: 3.372099	val: 6.266155	test: 5.462253

Epoch: 3
Loss: 18.96302318572998
RMSE train: 4.148516	val: 7.924146	test: 6.208944
MAE train: 3.306429	val: 6.071085	test: 5.366050

Epoch: 4
Loss: 17.69472599029541
RMSE train: 4.065968	val: 7.649419	test: 6.115079
MAE train: 3.259003	val: 5.839425	test: 5.291956

Epoch: 5
Loss: 16.79616641998291
RMSE train: 4.019276	val: 7.342589	test: 6.054163
MAE train: 3.249497	val: 5.582259	test: 5.270084

Epoch: 6
Loss: 15.73516845703125
RMSE train: 3.993327	val: 6.981517	test: 5.995710
MAE train: 3.265785	val: 5.284142	test: 5.226138

Epoch: 7
Loss: 14.567605972290039
RMSE train: 3.982244	val: 6.628166	test: 5.991547
MAE train: 3.296048	val: 5.067888	test: 5.228910

Epoch: 8
Loss: 13.848139762878418
RMSE train: 3.960445	val: 6.289650	test: 6.026758
MAE train: 3.312331	val: 4.867642	test: 5.251246

Epoch: 9
Loss: 13.59310007095337
RMSE train: 3.894987	val: 6.019776	test: 6.091478
MAE train: 3.272249	val: 4.706273	test: 5.306322

Epoch: 10
Loss: 12.470333576202393
RMSE train: 3.713317	val: 5.633891	test: 5.944486
MAE train: 3.109984	val: 4.361712	test: 5.134543

Epoch: 11
Loss: 12.181748867034912
RMSE train: 3.489057	val: 5.246741	test: 5.652284
MAE train: 2.907486	val: 3.886565	test: 4.783534

Epoch: 12
Loss: 11.566147804260254
RMSE train: 3.284381	val: 4.980033	test: 5.371517
MAE train: 2.710010	val: 3.528551	test: 4.432508

Epoch: 13
Loss: 11.048523902893066
RMSE train: 3.095598	val: 4.784501	test: 5.075450
MAE train: 2.529143	val: 3.311516	test: 4.157486

Epoch: 14
Loss: 10.575971126556396
RMSE train: 2.996416	val: 4.622511	test: 4.825096
MAE train: 2.441286	val: 3.154746	test: 3.930929

Epoch: 15
Loss: 10.260694026947021
RMSE train: 2.961328	val: 4.512804	test: 4.644876
MAE train: 2.416966	val: 3.039818	test: 3.772310

Epoch: 16
Loss: 9.737045288085938
RMSE train: 2.897020	val: 4.400001	test: 4.509670
MAE train: 2.362322	val: 2.993328	test: 3.677387

Epoch: 17
Loss: 9.219892978668213
RMSE train: 2.845460	val: 4.348266	test: 4.536197
MAE train: 2.323591	val: 2.979561	test: 3.707479

Epoch: 18
Loss: 8.95621633529663
RMSE train: 2.805513	val: 4.304133	test: 4.591478
MAE train: 2.300208	val: 2.963208	test: 3.759194

Epoch: 19
Loss: 8.231935739517212
RMSE train: 2.783532	val: 4.277367	test: 4.654451
MAE train: 2.295568	val: 2.986495	test: 3.827865

Epoch: 20
Loss: 7.881749629974365
RMSE train: 2.745181	val: 4.249263	test: 4.656728
MAE train: 2.273372	val: 2.973679	test: 3.844567

Epoch: 21
Loss: 7.279876470565796
RMSE train: 2.730129	val: 4.226245	test: 4.634051
MAE train: 2.270991	val: 2.898975	test: 3.817721

Epoch: 22
Loss: 7.109637260437012Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.05/freesolv_scaff_6_26-05_11-15-58  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.886201858520508
RMSE train: 4.265081	val: 8.336775	test: 6.421675
MAE train: 3.391249	val: 6.381269	test: 5.505234

Epoch: 2
Loss: 19.70505142211914
RMSE train: 4.134823	val: 8.109801	test: 6.224181
MAE train: 3.286704	val: 6.227437	test: 5.360514

Epoch: 3
Loss: 18.38718891143799
RMSE train: 3.994987	val: 7.829733	test: 6.027450
MAE train: 3.180355	val: 6.017310	test: 5.200879

Epoch: 4
Loss: 16.897963523864746
RMSE train: 3.851930	val: 7.489665	test: 5.852226
MAE train: 3.085629	val: 5.771624	test: 5.080206

Epoch: 5
Loss: 15.625909805297852
RMSE train: 3.727706	val: 7.087310	test: 5.713251
MAE train: 3.026055	val: 5.516088	test: 4.979496

Epoch: 6
Loss: 14.590850353240967
RMSE train: 3.630095	val: 6.647201	test: 5.632095
MAE train: 3.005877	val: 5.301407	test: 4.909558

Epoch: 7
Loss: 13.057895183563232
RMSE train: 3.572647	val: 6.210125	test: 5.595672
MAE train: 3.025581	val: 5.056471	test: 4.864722

Epoch: 8
Loss: 12.2410569190979
RMSE train: 3.513300	val: 5.752589	test: 5.524153
MAE train: 3.004583	val: 4.745683	test: 4.762681

Epoch: 9
Loss: 12.069406986236572
RMSE train: 3.440291	val: 5.338191	test: 5.403642
MAE train: 2.948538	val: 4.420164	test: 4.631158

Epoch: 10
Loss: 11.394087314605713
RMSE train: 3.253324	val: 4.844675	test: 5.094020
MAE train: 2.770424	val: 4.012189	test: 4.320765

Epoch: 11
Loss: 10.705304622650146
RMSE train: 3.021731	val: 4.347216	test: 4.710332
MAE train: 2.541187	val: 3.602361	test: 3.976062

Epoch: 12
Loss: 10.468053817749023
RMSE train: 2.811785	val: 4.168172	test: 4.470300
MAE train: 2.347497	val: 3.422819	test: 3.804631

Epoch: 13
Loss: 9.79674768447876
RMSE train: 2.622689	val: 4.076356	test: 4.249067
MAE train: 2.180748	val: 3.293792	test: 3.656118

Epoch: 14
Loss: 9.484403133392334
RMSE train: 2.513241	val: 3.975778	test: 4.003807
MAE train: 2.091391	val: 3.162826	test: 3.426549

Epoch: 15
Loss: 8.949849128723145
RMSE train: 2.515705	val: 3.961807	test: 3.906627
MAE train: 2.107251	val: 3.133497	test: 3.331892

Epoch: 16
Loss: 8.54790449142456
RMSE train: 2.540977	val: 3.888440	test: 3.821736
MAE train: 2.154612	val: 3.075017	test: 3.224693

Epoch: 17
Loss: 8.167591571807861
RMSE train: 2.615664	val: 3.848306	test: 3.838566
MAE train: 2.249960	val: 3.094042	test: 3.233034

Epoch: 18
Loss: 7.776357650756836
RMSE train: 2.669072	val: 3.781127	test: 3.842704
MAE train: 2.319328	val: 3.081960	test: 3.221646

Epoch: 19
Loss: 7.142910003662109
RMSE train: 2.697942	val: 3.734837	test: 3.862865
MAE train: 2.358634	val: 3.074817	test: 3.229352

Epoch: 20
Loss: 6.958928108215332
RMSE train: 2.681780	val: 3.681279	test: 3.842187
MAE train: 2.352177	val: 3.046378	test: 3.210385

Epoch: 21
Loss: 6.758907794952393
RMSE train: 2.631634	val: 3.654128	test: 3.806819
MAE train: 2.310892	val: 3.022879	test: 3.207539

Epoch: 22
Loss: 6.488032579421997Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.2/freesolv_scaff_4_26-05_11-15-58  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.747323989868164
RMSE train: 4.400450	val: 8.348803	test: 6.380736
MAE train: 3.461285	val: 6.378321	test: 5.500473

Epoch: 2
Loss: 19.54173183441162
RMSE train: 4.245634	val: 8.155997	test: 6.252699
MAE train: 3.338251	val: 6.243115	test: 5.417058

Epoch: 3
Loss: 18.485794067382812
RMSE train: 4.116427	val: 7.966271	test: 6.192951
MAE train: 3.247746	val: 6.089324	test: 5.398976

Epoch: 4
Loss: 17.567774772644043
RMSE train: 4.026843	val: 7.818000	test: 6.216574
MAE train: 3.192211	val: 5.978942	test: 5.488155

Epoch: 5
Loss: 16.636059761047363
RMSE train: 3.951295	val: 7.675847	test: 6.259154
MAE train: 3.147674	val: 6.041367	test: 5.568046

Epoch: 6
Loss: 15.550435066223145
RMSE train: 3.907343	val: 7.547242	test: 6.318437
MAE train: 3.140512	val: 6.105371	test: 5.631762

Epoch: 7
Loss: 14.513766765594482
RMSE train: 3.868907	val: 7.354936	test: 6.299977
MAE train: 3.141559	val: 6.043843	test: 5.600187

Epoch: 8
Loss: 13.918229579925537
RMSE train: 3.839371	val: 7.123698	test: 6.222160
MAE train: 3.151942	val: 5.895811	test: 5.524810

Epoch: 9
Loss: 13.300292015075684
RMSE train: 3.756789	val: 6.890425	test: 6.124043
MAE train: 3.097095	val: 5.685320	test: 5.403307

Epoch: 10
Loss: 12.638735294342041
RMSE train: 3.622998	val: 6.620847	test: 5.961182
MAE train: 2.982071	val: 5.395599	test: 5.206638

Epoch: 11
Loss: 11.971736907958984
RMSE train: 3.440429	val: 6.279539	test: 5.699667
MAE train: 2.811568	val: 4.972323	test: 4.896053

Epoch: 12
Loss: 11.556772232055664
RMSE train: 3.255978	val: 6.003797	test: 5.432535
MAE train: 2.641366	val: 4.594091	test: 4.603358

Epoch: 13
Loss: 10.777737617492676
RMSE train: 3.084828	val: 5.731713	test: 5.154189
MAE train: 2.484270	val: 4.262374	test: 4.316927

Epoch: 14
Loss: 10.266654968261719
RMSE train: 2.894039	val: 5.398203	test: 4.797206
MAE train: 2.310827	val: 3.927774	test: 3.929021

Epoch: 15
Loss: 9.96003246307373
RMSE train: 2.773908	val: 5.174668	test: 4.604823
MAE train: 2.211994	val: 3.676473	test: 3.715251

Epoch: 16
Loss: 9.416693449020386
RMSE train: 2.729520	val: 5.131872	test: 4.555703
MAE train: 2.184967	val: 3.610175	test: 3.655432

Epoch: 17
Loss: 8.923317909240723
RMSE train: 2.694086	val: 5.042576	test: 4.505561
MAE train: 2.171315	val: 3.515890	test: 3.616056

Epoch: 18
Loss: 8.575491905212402
RMSE train: 2.643139	val: 4.926160	test: 4.420410
MAE train: 2.141021	val: 3.360038	test: 3.523056

Epoch: 19
Loss: 8.377718925476074
RMSE train: 2.563381	val: 4.802993	test: 4.292095
MAE train: 2.078583	val: 3.180180	test: 3.378074

Epoch: 20
Loss: 7.427651405334473
RMSE train: 2.505895	val: 4.722678	test: 4.162135
MAE train: 2.032243	val: 3.019439	test: 3.226791

Epoch: 21
Loss: 7.091907024383545
RMSE train: 2.432560	val: 4.675133	test: 4.047448
MAE train: 1.971090	val: 2.992511	test: 3.144159

Epoch: 22
Loss: 6.969368934631348Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.05/freesolv_scaff_4_26-05_11-15-58  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.55756664276123
RMSE train: 4.309190	val: 8.321048	test: 6.353860
MAE train: 3.379263	val: 6.355423	test: 5.475998

Epoch: 2
Loss: 19.037121772766113
RMSE train: 4.143795	val: 8.091984	test: 6.179103
MAE train: 3.243128	val: 6.191890	test: 5.353844

Epoch: 3
Loss: 17.814993858337402
RMSE train: 3.995014	val: 7.844350	test: 6.033399
MAE train: 3.133948	val: 6.001766	test: 5.257222

Epoch: 4
Loss: 16.54543924331665
RMSE train: 3.887390	val: 7.566877	test: 5.921889
MAE train: 3.071139	val: 5.785346	test: 5.196037

Epoch: 5
Loss: 15.318852424621582
RMSE train: 3.800161	val: 7.240851	test: 5.806797
MAE train: 3.045531	val: 5.665121	test: 5.114271

Epoch: 6
Loss: 14.182087421417236
RMSE train: 3.718359	val: 6.861957	test: 5.683092
MAE train: 3.064387	val: 5.495484	test: 4.970491

Epoch: 7
Loss: 13.015710830688477
RMSE train: 3.649022	val: 6.459000	test: 5.565438
MAE train: 3.084455	val: 5.247726	test: 4.800963

Epoch: 8
Loss: 12.107872009277344
RMSE train: 3.592633	val: 6.131465	test: 5.520644
MAE train: 3.079157	val: 5.064052	test: 4.720449

Epoch: 9
Loss: 11.461742877960205
RMSE train: 3.491290	val: 5.874845	test: 5.482406
MAE train: 2.990503	val: 4.897129	test: 4.653946

Epoch: 10
Loss: 11.185022354125977
RMSE train: 3.296549	val: 5.544819	test: 5.272016
MAE train: 2.790454	val: 4.593340	test: 4.446219

Epoch: 11
Loss: 10.852075576782227
RMSE train: 3.031231	val: 5.091073	test: 4.871609
MAE train: 2.522425	val: 4.147571	test: 4.092775

Epoch: 12
Loss: 9.970191478729248
RMSE train: 2.803810	val: 4.662563	test: 4.460561
MAE train: 2.311777	val: 3.731759	test: 3.728491

Epoch: 13
Loss: 9.494599342346191
RMSE train: 2.678499	val: 4.434201	test: 4.218656
MAE train: 2.205590	val: 3.502147	test: 3.519195

Epoch: 14
Loss: 9.194773197174072
RMSE train: 2.577574	val: 4.255510	test: 4.020013
MAE train: 2.127611	val: 3.307870	test: 3.327832

Epoch: 15
Loss: 8.767336368560791
RMSE train: 2.569827	val: 4.192801	test: 4.000648
MAE train: 2.145724	val: 3.244297	test: 3.319408

Epoch: 16
Loss: 8.389386653900146
RMSE train: 2.598541	val: 4.228984	test: 4.036187
MAE train: 2.196052	val: 3.282089	test: 3.373628

Epoch: 17
Loss: 8.002822637557983
RMSE train: 2.653112	val: 4.257225	test: 4.074859
MAE train: 2.274040	val: 3.346116	test: 3.456711

Epoch: 18
Loss: 7.446550369262695
RMSE train: 2.679637	val: 4.282264	test: 4.067062
MAE train: 2.316453	val: 3.404456	test: 3.491725

Epoch: 19
Loss: 7.442930698394775
RMSE train: 2.704003	val: 4.334638	test: 4.064829
MAE train: 2.353267	val: 3.484871	test: 3.526283

Epoch: 20
Loss: 6.6232521533966064
RMSE train: 2.686073	val: 4.337187	test: 3.998401
MAE train: 2.345238	val: 3.503642	test: 3.467952

Epoch: 21
Loss: 6.341349363327026
RMSE train: 2.616956	val: 4.262671	test: 3.877067
MAE train: 2.281010	val: 3.408867	test: 3.327273

Epoch: 22
Loss: 6.253660202026367Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.05/freesolv_scaff_5_26-05_11-15-58  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.420740127563477
RMSE train: 4.514688	val: 8.653072	test: 6.835287
MAE train: 3.624578	val: 6.772152	test: 6.007451

Epoch: 2
Loss: 20.112448692321777
RMSE train: 4.354455	val: 8.436493	test: 6.644034
MAE train: 3.491840	val: 6.615213	test: 5.858219

Epoch: 3
Loss: 19.08745288848877
RMSE train: 4.179279	val: 8.174665	test: 6.447715
MAE train: 3.350778	val: 6.423178	test: 5.711153

Epoch: 4
Loss: 17.77840805053711
RMSE train: 3.985438	val: 7.858810	test: 6.245121
MAE train: 3.203217	val: 6.188430	test: 5.555592

Epoch: 5
Loss: 16.65533447265625
RMSE train: 3.796294	val: 7.501816	test: 6.053038
MAE train: 3.067059	val: 5.943377	test: 5.396207

Epoch: 6
Loss: 15.243523120880127
RMSE train: 3.621383	val: 7.102883	test: 5.881840
MAE train: 2.950771	val: 5.721549	test: 5.235488

Epoch: 7
Loss: 13.939647197723389
RMSE train: 3.460730	val: 6.641690	test: 5.711988
MAE train: 2.839342	val: 5.491496	test: 5.046124

Epoch: 8
Loss: 12.941756248474121
RMSE train: 3.322028	val: 6.208050	test: 5.543569
MAE train: 2.726720	val: 5.290211	test: 4.885234

Epoch: 9
Loss: 11.9893217086792
RMSE train: 3.206324	val: 5.820392	test: 5.375292
MAE train: 2.636870	val: 5.082134	test: 4.714110

Epoch: 10
Loss: 11.384329795837402
RMSE train: 3.092218	val: 5.447098	test: 5.235687
MAE train: 2.544311	val: 4.782656	test: 4.547418

Epoch: 11
Loss: 11.22221040725708
RMSE train: 2.896340	val: 4.892121	test: 4.946364
MAE train: 2.370943	val: 4.300023	test: 4.267824

Epoch: 12
Loss: 10.244339942932129
RMSE train: 2.719827	val: 4.353619	test: 4.585886
MAE train: 2.221277	val: 3.781314	test: 3.921162

Epoch: 13
Loss: 9.624133110046387
RMSE train: 2.584677	val: 4.098070	test: 4.326937
MAE train: 2.111154	val: 3.473992	test: 3.637014

Epoch: 14
Loss: 9.336910724639893
RMSE train: 2.551253	val: 4.181851	test: 4.348132
MAE train: 2.099541	val: 3.546578	test: 3.686852

Epoch: 15
Loss: 8.922666072845459
RMSE train: 2.529461	val: 4.279051	test: 4.379409
MAE train: 2.101802	val: 3.618848	test: 3.759578

Epoch: 16
Loss: 8.760758876800537
RMSE train: 2.496129	val: 4.279970	test: 4.347207
MAE train: 2.088837	val: 3.577769	test: 3.751200

Epoch: 17
Loss: 8.081734418869019
RMSE train: 2.474148	val: 4.222847	test: 4.212696
MAE train: 2.091042	val: 3.496575	test: 3.635533

Epoch: 18
Loss: 7.897759437561035
RMSE train: 2.463753	val: 4.176623	test: 4.108904
MAE train: 2.104604	val: 3.465484	test: 3.568815

Epoch: 19
Loss: 7.573729038238525
RMSE train: 2.470264	val: 4.157071	test: 4.101672
MAE train: 2.122867	val: 3.481053	test: 3.591930

Epoch: 20
Loss: 7.278454065322876
RMSE train: 2.427513	val: 4.027238	test: 4.023359
MAE train: 2.087206	val: 3.387860	test: 3.497881

Epoch: 21
Loss: 6.659762382507324
RMSE train: 2.376984	val: 3.896674	test: 3.910954
MAE train: 2.046563	val: 3.294182	test: 3.370962

Epoch: 22
Loss: 6.435475826263428Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/freesolv/noise=0.1/freesolv_scaff_5_26-05_11-15-58  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.38850688934326
RMSE train: 4.503341	val: 8.653512	test: 6.837338
MAE train: 3.608992	val: 6.769832	test: 6.006263

Epoch: 2
Loss: 20.2405948638916
RMSE train: 4.351489	val: 8.460508	test: 6.675190
MAE train: 3.480617	val: 6.626998	test: 5.878207

Epoch: 3
Loss: 19.31276798248291
RMSE train: 4.179109	val: 8.200263	test: 6.505883
MAE train: 3.336843	val: 6.402009	test: 5.745157

Epoch: 4
Loss: 18.142648696899414
RMSE train: 3.967833	val: 7.849692	test: 6.269444
MAE train: 3.161810	val: 6.051741	test: 5.527615

Epoch: 5
Loss: 17.103374481201172
RMSE train: 3.782832	val: 7.560028	test: 6.112266
MAE train: 3.016038	val: 5.814648	test: 5.382394

Epoch: 6
Loss: 15.889586448669434
RMSE train: 3.633724	val: 7.293080	test: 6.044081
MAE train: 2.911953	val: 5.691115	test: 5.333263

Epoch: 7
Loss: 14.76995849609375
RMSE train: 3.517681	val: 7.029520	test: 6.045434
MAE train: 2.846084	val: 5.651886	test: 5.357079

Epoch: 8
Loss: 13.753792762756348
RMSE train: 3.422414	val: 6.731231	test: 6.036768
MAE train: 2.792572	val: 5.608430	test: 5.369605

Epoch: 9
Loss: 12.629125595092773
RMSE train: 3.296168	val: 6.328938	test: 5.937484
MAE train: 2.698630	val: 5.384573	test: 5.247874

Epoch: 10
Loss: 12.014244556427002
RMSE train: 3.206569	val: 5.957540	test: 5.834079
MAE train: 2.629678	val: 5.096881	test: 5.118183

Epoch: 11
Loss: 11.675631999969482
RMSE train: 3.075612	val: 5.525028	test: 5.595931
MAE train: 2.516450	val: 4.691682	test: 4.893860

Epoch: 12
Loss: 10.924348831176758
RMSE train: 2.955973	val: 5.165935	test: 5.355434
MAE train: 2.412057	val: 4.252922	test: 4.610523

Epoch: 13
Loss: 10.176328659057617
RMSE train: 2.870190	val: 5.076989	test: 5.280983
MAE train: 2.332135	val: 4.103812	test: 4.473446

Epoch: 14
Loss: 10.030701637268066
RMSE train: 2.800376	val: 5.066469	test: 5.220875
MAE train: 2.261565	val: 4.048181	test: 4.364717

Epoch: 15
Loss: 9.394649028778076
RMSE train: 2.725769	val: 5.042889	test: 5.164794
MAE train: 2.198846	val: 4.014479	test: 4.319654

Epoch: 16
Loss: 9.389558792114258
RMSE train: 2.638337	val: 4.897982	test: 5.002594
MAE train: 2.140915	val: 3.843114	test: 4.151811

Epoch: 17
Loss: 8.646892070770264
RMSE train: 2.562992	val: 4.757735	test: 4.792049
MAE train: 2.092635	val: 3.684917	test: 3.940182

Epoch: 18
Loss: 8.291571617126465
RMSE train: 2.518723	val: 4.650439	test: 4.638136
MAE train: 2.078207	val: 3.574659	test: 3.830916

Epoch: 19
Loss: 8.072799682617188
RMSE train: 2.512502	val: 4.588383	test: 4.625745
MAE train: 2.097649	val: 3.510619	test: 3.882031

Epoch: 20
Loss: 7.511731147766113
RMSE train: 2.473242	val: 4.493028	test: 4.584460
MAE train: 2.075249	val: 3.423685	test: 3.868944

Epoch: 21
Loss: 7.144456148147583
RMSE train: 2.440776	val: 4.423217	test: 4.556760
MAE train: 2.055387	val: 3.381753	test: 3.856050

Epoch: 22
Loss: 6.823047161102295

Epoch: 23
Loss: 5.603111982345581
RMSE train: 2.581927	val: 3.313454	test: 3.455105
MAE train: 2.316343	val: 2.901544	test: 2.883333

Epoch: 24
Loss: 5.469065189361572
RMSE train: 2.492004	val: 3.269078	test: 3.436709
MAE train: 2.239139	val: 2.878028	test: 2.896411

Epoch: 25
Loss: 4.946243047714233
RMSE train: 2.442733	val: 3.356988	test: 3.444288
MAE train: 2.207467	val: 2.923231	test: 2.941096

Epoch: 26
Loss: 4.6379358768463135
RMSE train: 2.438590	val: 3.477646	test: 3.489850
MAE train: 2.207227	val: 3.019960	test: 2.986444

Epoch: 27
Loss: 4.514915943145752
RMSE train: 2.424213	val: 3.484231	test: 3.407767
MAE train: 2.213539	val: 2.995298	test: 2.927358

Epoch: 28
Loss: 4.246291279792786
RMSE train: 2.443083	val: 3.612134	test: 3.434827
MAE train: 2.233609	val: 3.093777	test: 2.960775

Epoch: 29
Loss: 3.8815338611602783
RMSE train: 2.469363	val: 3.813016	test: 3.543362
MAE train: 2.242478	val: 3.281106	test: 3.041014

Epoch: 30
Loss: 3.749634265899658
RMSE train: 2.485421	val: 3.968693	test: 3.585537
MAE train: 2.243157	val: 3.399768	test: 3.065443

Epoch: 31
Loss: 3.5280247926712036
RMSE train: 2.476691	val: 3.952924	test: 3.498336
MAE train: 2.237754	val: 3.339754	test: 2.979546

Epoch: 32
Loss: 3.330458879470825
RMSE train: 2.390764	val: 3.815622	test: 3.355688
MAE train: 2.157770	val: 3.169287	test: 2.844556

Epoch: 33
Loss: 2.959207057952881
RMSE train: 2.234804	val: 3.606731	test: 3.133415
MAE train: 2.025474	val: 2.931509	test: 2.650409

Epoch: 34
Loss: 2.8433315753936768
RMSE train: 2.086772	val: 3.513396	test: 2.979224
MAE train: 1.897096	val: 2.815253	test: 2.489609

Epoch: 35
Loss: 2.628069818019867
RMSE train: 1.989355	val: 3.461995	test: 2.952250
MAE train: 1.789117	val: 2.795982	test: 2.438310

Epoch: 36
Loss: 2.3921703100204468
RMSE train: 1.905800	val: 3.377126	test: 2.895050
MAE train: 1.706306	val: 2.704995	test: 2.376553

Epoch: 37
Loss: 2.36364483833313
RMSE train: 1.814250	val: 3.205571	test: 2.722299
MAE train: 1.631035	val: 2.477949	test: 2.226146

Epoch: 38
Loss: 2.1412278413772583
RMSE train: 1.782867	val: 3.186529	test: 2.631805
MAE train: 1.595634	val: 2.407530	test: 2.134656

Epoch: 39
Loss: 2.108670473098755
RMSE train: 1.771215	val: 3.287755	test: 2.606724
MAE train: 1.568019	val: 2.448682	test: 2.085751

Epoch: 40
Loss: 1.9630680680274963
RMSE train: 1.706271	val: 3.339080	test: 2.620185
MAE train: 1.477437	val: 2.487668	test: 2.059705

Epoch: 41
Loss: 1.697916865348816
RMSE train: 1.588550	val: 3.215736	test: 2.458782
MAE train: 1.394912	val: 2.280450	test: 1.921969

Epoch: 42
Loss: 1.6932379603385925
RMSE train: 1.454045	val: 2.971349	test: 2.196168
MAE train: 1.300773	val: 2.090931	test: 1.668896

Epoch: 43
Loss: 1.60105699300766
RMSE train: 1.375107	val: 2.881741	test: 2.115870
MAE train: 1.238920	val: 2.054056	test: 1.574324

Epoch: 44
Loss: 1.3426567912101746
RMSE train: 1.308543	val: 2.851546	test: 2.161060
MAE train: 1.171388	val: 2.032309	test: 1.631047

Epoch: 45
Loss: 1.2882119417190552
RMSE train: 1.272759	val: 2.848707	test: 2.209090
MAE train: 1.117993	val: 2.009939	test: 1.682527

Epoch: 46
Loss: 1.3615112900733948
RMSE train: 1.245523	val: 2.795825	test: 2.216783
MAE train: 1.087893	val: 2.001438	test: 1.707091

Epoch: 47
Loss: 1.125040352344513
RMSE train: 1.163994	val: 2.732281	test: 2.187816
MAE train: 1.016675	val: 1.963393	test: 1.681737

Epoch: 48
Loss: 1.092407375574112
RMSE train: 1.118462	val: 2.790539	test: 2.215788
MAE train: 0.971224	val: 1.965527	test: 1.698515

Epoch: 49
Loss: 1.112265706062317
RMSE train: 1.052223	val: 2.880418	test: 2.241580
MAE train: 0.902742	val: 1.966998	test: 1.704446

Epoch: 50
Loss: 0.983909398317337
RMSE train: 1.049742	val: 2.950410	test: 2.299362
MAE train: 0.888722	val: 1.960980	test: 1.725135

Epoch: 51
Loss: 0.9074619114398956
RMSE train: 0.988524	val: 2.882055	test: 2.275861
MAE train: 0.832246	val: 1.926845	test: 1.710691

Epoch: 52
Loss: 0.9353108108043671
RMSE train: 0.989668	val: 2.852156	test: 2.235181
MAE train: 0.851337	val: 1.946425	test: 1.670488

Epoch: 53
Loss: 0.8905477821826935
RMSE train: 1.007451	val: 2.917198	test: 2.228508
MAE train: 0.869247	val: 2.009720	test: 1.673092

Epoch: 54
Loss: 0.8456038534641266
RMSE train: 1.041612	val: 3.038738	test: 2.278193
MAE train: 0.893807	val: 2.057384	test: 1.706718

Epoch: 55
Loss: 0.7398380041122437
RMSE train: 1.053036	val: 3.119519	test: 2.321091
MAE train: 0.887255	val: 2.091300	test: 1.755217

Epoch: 56
Loss: 0.9336071610450745
RMSE train: 1.052974	val: 3.120700	test: 2.265225
MAE train: 0.880761	val: 2.071868	test: 1.696128

Epoch: 57
Loss: 0.7984624207019806
RMSE train: 0.953417	val: 3.157769	test: 2.246035
MAE train: 0.784576	val: 2.095019	test: 1.669943

Epoch: 58
Loss: 0.793850302696228
RMSE train: 0.844259	val: 3.099414	test: 2.205081
MAE train: 0.686253	val: 2.058223	test: 1.648935

Epoch: 59
Loss: 0.752719908952713
RMSE train: 0.757692	val: 2.927916	test: 2.097779
MAE train: 0.616221	val: 1.951622	test: 1.553299

Epoch: 60
Loss: 0.7379887998104095
RMSE train: 0.722724	val: 2.839541	test: 1.989233
MAE train: 0.583505	val: 1.896762	test: 1.436375

Epoch: 61
Loss: 0.8577337265014648
RMSE train: 0.736300	val: 2.879745	test: 1.929737
MAE train: 0.582349	val: 1.912771	test: 1.380040

Epoch: 62
Loss: 0.6833631694316864
RMSE train: 0.719472	val: 2.974125	test: 1.936758
MAE train: 0.540204	val: 1.949673	test: 1.387896

Epoch: 63
Loss: 0.8174605965614319
RMSE train: 0.708691	val: 2.987315	test: 1.959026
MAE train: 0.521631	val: 1.945464	test: 1.402058

Epoch: 64
Loss: 0.8900136053562164
RMSE train: 0.676640	val: 2.879214	test: 2.004134
MAE train: 0.501625	val: 1.888733	test: 1.429214

Epoch: 65
Loss: 0.7732913196086884
RMSE train: 0.624031	val: 2.757421	test: 2.020758
MAE train: 0.466065	val: 1.852992	test: 1.443330

Epoch: 66
Loss: 0.790489673614502
RMSE train: 0.538278	val: 2.598025	test: 1.981612
MAE train: 0.404325	val: 1.793736	test: 1.423692

Epoch: 67
Loss: 0.6818146109580994
RMSE train: 0.548364	val: 2.513649	test: 1.978425
MAE train: 0.405700	val: 1.762128	test: 1.447642

Epoch: 68
Loss: 0.8268246650695801
RMSE train: 0.572688	val: 2.483017	test: 1.925617
MAE train: 0.440998	val: 1.723095	test: 1.407031

Epoch: 69
Loss: 0.7783291339874268
RMSE train: 0.644198	val: 2.657941	test: 1.914152
MAE train: 0.504515	val: 1.831901	test: 1.397764

Epoch: 70
Loss: 0.6321559548377991
RMSE train: 0.756813	val: 2.867823	test: 1.929144
MAE train: 0.579562	val: 1.929579	test: 1.404404

Epoch: 71
Loss: 0.7761793434619904
RMSE train: 0.815601	val: 3.012715	test: 1.995121
MAE train: 0.623143	val: 1.984539	test: 1.448226

Epoch: 72
Loss: 0.6558704674243927
RMSE train: 0.780196	val: 2.996879	test: 2.065969
MAE train: 0.600381	val: 1.934886	test: 1.498704

Epoch: 73
Loss: 0.6585731506347656
RMSE train: 0.690584	val: 2.941030	test: 2.093713
MAE train: 0.520121	val: 1.915765	test: 1.520327

Epoch: 74
Loss: 0.6422974169254303
RMSE train: 0.605893	val: 2.873838	test: 2.066503
MAE train: 0.478312	val: 1.953827	test: 1.489860

Epoch: 75
Loss: 0.6585407853126526
RMSE train: 0.589812	val: 2.895681	test: 2.054869
MAE train: 0.463162	val: 1.954538	test: 1.489721

Epoch: 76
Loss: 0.6383827030658722
RMSE train: 0.581633	val: 2.925624	test: 2.025251
MAE train: 0.450673	val: 1.956453	test: 1.456649

Epoch: 77
Loss: 0.766976922750473
RMSE train: 0.590532	val: 2.976598	test: 1.988206
MAE train: 0.449303	val: 1.985880	test: 1.407423

Epoch: 78
Loss: 0.6953814625740051
RMSE train: 0.614216	val: 2.965700	test: 1.966202
MAE train: 0.461959	val: 1.961256	test: 1.412715

Epoch: 79
Loss: 0.6691258549690247
RMSE train: 0.666036	val: 2.932882	test: 1.915258
MAE train: 0.517762	val: 1.914696	test: 1.375734

Epoch: 80
Loss: 0.5496922433376312
RMSE train: 0.706830	val: 2.935783	test: 1.895639
MAE train: 0.560336	val: 1.886529	test: 1.361586

Epoch: 81
Loss: 0.6996974349021912
RMSE train: 0.702375	val: 3.012418	test: 1.956389
MAE train: 0.561211	val: 1.931152	test: 1.416587

Epoch: 82
Loss: 0.5358823537826538
RMSE train: 0.672226	val: 3.080745	test: 2.040129
MAE train: 0.536387	val: 1.959145	test: 1.469301

Epoch: 83
Loss: 0.6394670903682709
RMSE train: 0.643246	val: 3.162960	test: 2.123286
MAE train: 0.501868	val: 1.988911	test: 1.517376

Epoch: 23
Loss: 5.495278835296631
RMSE train: 2.491072	val: 4.466750	test: 3.671006
MAE train: 2.231608	val: 3.618995	test: 2.984730

Epoch: 24
Loss: 5.222957134246826
RMSE train: 2.421504	val: 4.403006	test: 3.587418
MAE train: 2.179207	val: 3.547480	test: 2.914998

Epoch: 25
Loss: 4.921519756317139
RMSE train: 2.344408	val: 4.310167	test: 3.505205
MAE train: 2.110771	val: 3.444036	test: 2.867341

Epoch: 26
Loss: 4.565713405609131
RMSE train: 2.255309	val: 4.253475	test: 3.424752
MAE train: 2.014352	val: 3.355153	test: 2.803103

Epoch: 27
Loss: 4.195981860160828
RMSE train: 2.154612	val: 4.236077	test: 3.343847
MAE train: 1.912592	val: 3.282732	test: 2.706234

Epoch: 28
Loss: 3.9850624799728394
RMSE train: 2.070825	val: 4.263031	test: 3.255751
MAE train: 1.835945	val: 3.226662	test: 2.598379

Epoch: 29
Loss: 3.858951449394226
RMSE train: 2.033986	val: 4.362810	test: 3.140338
MAE train: 1.801287	val: 3.188382	test: 2.471963

Epoch: 30
Loss: 3.5514076948165894
RMSE train: 1.999548	val: 4.417217	test: 3.038623
MAE train: 1.775721	val: 3.128373	test: 2.361642

Epoch: 31
Loss: 3.243582248687744
RMSE train: 1.963582	val: 4.458668	test: 2.992835
MAE train: 1.744419	val: 3.115515	test: 2.306893

Epoch: 32
Loss: 2.920581102371216
RMSE train: 1.946544	val: 4.496792	test: 3.020131
MAE train: 1.731843	val: 3.188981	test: 2.314483

Epoch: 33
Loss: 2.809704065322876
RMSE train: 1.931707	val: 4.486548	test: 3.062581
MAE train: 1.725450	val: 3.247113	test: 2.350862

Epoch: 34
Loss: 2.645452618598938
RMSE train: 1.877175	val: 4.406947	test: 3.028205
MAE train: 1.670100	val: 3.197137	test: 2.325425

Epoch: 35
Loss: 2.4352751970291138
RMSE train: 1.814145	val: 4.307155	test: 2.958210
MAE train: 1.602772	val: 3.078237	test: 2.249853

Epoch: 36
Loss: 2.3398211002349854
RMSE train: 1.756827	val: 4.228316	test: 2.879804
MAE train: 1.539151	val: 2.949530	test: 2.164360

Epoch: 37
Loss: 2.190703511238098
RMSE train: 1.715126	val: 4.225204	test: 2.825863
MAE train: 1.502649	val: 2.894940	test: 2.097684

Epoch: 38
Loss: 2.071526050567627
RMSE train: 1.697101	val: 4.312088	test: 2.851360
MAE train: 1.491027	val: 2.950013	test: 2.121743

Epoch: 39
Loss: 1.8862881064414978
RMSE train: 1.631360	val: 4.297755	test: 2.854789
MAE train: 1.427470	val: 2.966333	test: 2.116693

Epoch: 40
Loss: 1.6647693514823914
RMSE train: 1.559726	val: 4.169376	test: 2.740921
MAE train: 1.367794	val: 2.875527	test: 2.004779

Epoch: 41
Loss: 1.5252768993377686
RMSE train: 1.436974	val: 4.013871	test: 2.577606
MAE train: 1.252473	val: 2.698603	test: 1.847922

Epoch: 42
Loss: 1.5845142602920532
RMSE train: 1.379552	val: 4.021393	test: 2.543816
MAE train: 1.197649	val: 2.692111	test: 1.793171

Epoch: 43
Loss: 1.3413562178611755
RMSE train: 1.404703	val: 4.088256	test: 2.610709
MAE train: 1.179015	val: 2.791806	test: 1.827622

Epoch: 44
Loss: 1.3154249787330627
RMSE train: 1.372076	val: 4.017503	test: 2.570350
MAE train: 1.148236	val: 2.703483	test: 1.799800

Epoch: 45
Loss: 1.2928252220153809
RMSE train: 1.268793	val: 3.821089	test: 2.401647
MAE train: 1.085436	val: 2.467714	test: 1.684495

Epoch: 46
Loss: 1.0308995246887207
RMSE train: 1.167770	val: 3.723215	test: 2.357317
MAE train: 0.969517	val: 2.356227	test: 1.675215

Epoch: 47
Loss: 1.1413769125938416
RMSE train: 1.090208	val: 3.781173	test: 2.380769
MAE train: 0.901869	val: 2.387315	test: 1.674341

Epoch: 48
Loss: 1.074956327676773
RMSE train: 0.997503	val: 3.911110	test: 2.449809
MAE train: 0.800035	val: 2.449659	test: 1.698915

Epoch: 49
Loss: 1.0111672282218933
RMSE train: 0.921362	val: 3.932362	test: 2.432500
MAE train: 0.735485	val: 2.478148	test: 1.679301

Epoch: 50
Loss: 0.9048263728618622
RMSE train: 0.856594	val: 3.857268	test: 2.332832
MAE train: 0.699603	val: 2.447531	test: 1.608893

Epoch: 51
Loss: 0.9894996881484985
RMSE train: 0.822572	val: 3.715982	test: 2.229554
MAE train: 0.683654	val: 2.359487	test: 1.547672

Epoch: 52
Loss: 0.8208946585655212
RMSE train: 0.848413	val: 3.569137	test: 2.150468
MAE train: 0.710260	val: 2.274736	test: 1.501575

Epoch: 53
Loss: 0.9139838516712189
RMSE train: 0.886560	val: 3.601452	test: 2.165693
MAE train: 0.739238	val: 2.286568	test: 1.489591

Epoch: 54
Loss: 0.8710665106773376
RMSE train: 0.842136	val: 3.584375	test: 2.154483
MAE train: 0.697554	val: 2.259178	test: 1.466478

Epoch: 55
Loss: 0.7057217657566071
RMSE train: 0.766509	val: 3.712639	test: 2.219405
MAE train: 0.625923	val: 2.299712	test: 1.502710

Epoch: 56
Loss: 0.7387243211269379
RMSE train: 0.754793	val: 3.857842	test: 2.308932
MAE train: 0.607040	val: 2.395211	test: 1.566583

Epoch: 57
Loss: 0.78282630443573
RMSE train: 0.775425	val: 3.952160	test: 2.397416
MAE train: 0.603342	val: 2.459157	test: 1.648492

Epoch: 58
Loss: 0.9364314377307892
RMSE train: 0.761818	val: 3.863226	test: 2.363548
MAE train: 0.605025	val: 2.422615	test: 1.638033

Epoch: 59
Loss: 0.8262685537338257
RMSE train: 0.765321	val: 3.684925	test: 2.292730
MAE train: 0.618041	val: 2.376356	test: 1.601489

Epoch: 60
Loss: 0.7728051543235779
RMSE train: 0.765526	val: 3.650132	test: 2.287465
MAE train: 0.617404	val: 2.387956	test: 1.619220

Epoch: 61
Loss: 0.8750289678573608
RMSE train: 0.751431	val: 3.666592	test: 2.286326
MAE train: 0.601346	val: 2.390378	test: 1.631962

Epoch: 62
Loss: 0.797824501991272
RMSE train: 0.729391	val: 3.739426	test: 2.285462
MAE train: 0.570972	val: 2.393015	test: 1.601745

Epoch: 63
Loss: 0.7655931413173676
RMSE train: 0.686998	val: 3.725954	test: 2.250084
MAE train: 0.538445	val: 2.344351	test: 1.555005

Epoch: 64
Loss: 0.8288384079933167
RMSE train: 0.632809	val: 3.799954	test: 2.272278
MAE train: 0.489084	val: 2.340877	test: 1.582252

Epoch: 65
Loss: 0.7375524342060089
RMSE train: 0.614643	val: 3.794864	test: 2.275182
MAE train: 0.474310	val: 2.315029	test: 1.592503

Epoch: 66
Loss: 0.8347947895526886
RMSE train: 0.573277	val: 3.733374	test: 2.238150
MAE train: 0.448017	val: 2.321989	test: 1.561152

Epoch: 67
Loss: 0.6992239654064178
RMSE train: 0.580847	val: 3.674647	test: 2.218542
MAE train: 0.444757	val: 2.320984	test: 1.543953

Epoch: 68
Loss: 0.6916148066520691
RMSE train: 0.610877	val: 3.572537	test: 2.201755
MAE train: 0.460145	val: 2.262801	test: 1.551142

Epoch: 69
Loss: 0.7599702179431915
RMSE train: 0.600979	val: 3.403323	test: 2.138400
MAE train: 0.458387	val: 2.165907	test: 1.494535

Epoch: 70
Loss: 0.7041769921779633
RMSE train: 0.638049	val: 3.335895	test: 2.112012
MAE train: 0.489126	val: 2.110409	test: 1.461728

Epoch: 71
Loss: 0.7640853822231293
RMSE train: 0.607033	val: 3.363872	test: 2.088157
MAE train: 0.463156	val: 2.122290	test: 1.437394

Epoch: 72
Loss: 0.8033501207828522
RMSE train: 0.544116	val: 3.464751	test: 2.078750
MAE train: 0.397083	val: 2.186814	test: 1.424708

Epoch: 73
Loss: 0.7330226600170135
RMSE train: 0.523419	val: 3.459904	test: 2.088798
MAE train: 0.386640	val: 2.186762	test: 1.446966

Epoch: 74
Loss: 0.7669664323329926
RMSE train: 0.489911	val: 3.387916	test: 2.108836
MAE train: 0.361716	val: 2.146028	test: 1.475622

Epoch: 75
Loss: 0.7443627417087555
RMSE train: 0.500696	val: 3.351407	test: 2.112546
MAE train: 0.387978	val: 2.117084	test: 1.476563

Epoch: 76
Loss: 0.6991665661334991
RMSE train: 0.604184	val: 3.439226	test: 2.138708
MAE train: 0.481358	val: 2.178268	test: 1.457221

Epoch: 77
Loss: 0.7194731533527374
RMSE train: 0.654949	val: 3.533051	test: 2.196349
MAE train: 0.508633	val: 2.202015	test: 1.464587

Epoch: 78
Loss: 0.8109361231327057
RMSE train: 0.688799	val: 3.652047	test: 2.275726
MAE train: 0.533878	val: 2.245793	test: 1.518877

Epoch: 79
Loss: 0.6343685984611511
RMSE train: 0.675912	val: 3.751054	test: 2.334796
MAE train: 0.517009	val: 2.273223	test: 1.554165

Epoch: 80
Loss: 0.6028904914855957
RMSE train: 0.629281	val: 3.829690	test: 2.348749
MAE train: 0.480128	val: 2.318355	test: 1.568678

Epoch: 81
Loss: 0.6376976072788239
RMSE train: 0.617591	val: 3.881570	test: 2.361049
MAE train: 0.474948	val: 2.354693	test: 1.568763

Epoch: 82
Loss: 0.6057721078395844
RMSE train: 0.656790	val: 3.941681	test: 2.401474
MAE train: 0.511450	val: 2.378746	test: 1.585810

Epoch: 83
Loss: 0.6344873011112213
RMSE train: 0.652250	val: 3.828945	test: 2.345360
MAE train: 0.528592	val: 2.323723	test: 1.554400

Epoch: 23
Loss: 5.873070240020752
RMSE train: 2.782563	val: 3.878371	test: 3.677546
MAE train: 2.536793	val: 3.449462	test: 3.163165

Epoch: 24
Loss: 5.526408910751343
RMSE train: 2.668135	val: 3.829962	test: 3.583853
MAE train: 2.430141	val: 3.343233	test: 3.087172

Epoch: 25
Loss: 5.3944456577301025
RMSE train: 2.625405	val: 3.897264	test: 3.552784
MAE train: 2.393634	val: 3.349768	test: 3.054566

Epoch: 26
Loss: 5.010002851486206
RMSE train: 2.601273	val: 3.974223	test: 3.547091
MAE train: 2.368547	val: 3.383994	test: 3.041694

Epoch: 27
Loss: 4.741928339004517
RMSE train: 2.550171	val: 3.989653	test: 3.539557
MAE train: 2.321233	val: 3.347507	test: 3.038471

Epoch: 28
Loss: 4.448158860206604
RMSE train: 2.474534	val: 4.001874	test: 3.523876
MAE train: 2.250321	val: 3.258916	test: 3.030705

Epoch: 29
Loss: 4.157153844833374
RMSE train: 2.387095	val: 3.940222	test: 3.426538
MAE train: 2.170101	val: 3.144691	test: 2.934828

Epoch: 30
Loss: 4.053418397903442
RMSE train: 2.296558	val: 3.795485	test: 3.306986
MAE train: 2.083916	val: 3.032369	test: 2.813439

Epoch: 31
Loss: 3.7783974409103394
RMSE train: 2.219052	val: 3.709555	test: 3.178587
MAE train: 2.007057	val: 2.935890	test: 2.679067

Epoch: 32
Loss: 3.4683841466903687
RMSE train: 2.165745	val: 3.593388	test: 3.053450
MAE train: 1.948445	val: 2.828358	test: 2.546440

Epoch: 33
Loss: 3.300660252571106
RMSE train: 2.082149	val: 3.375967	test: 2.904492
MAE train: 1.863978	val: 2.640119	test: 2.390699

Epoch: 34
Loss: 2.9806766510009766
RMSE train: 1.994598	val: 3.191305	test: 2.790001
MAE train: 1.779507	val: 2.512660	test: 2.258297

Epoch: 35
Loss: 2.681964159011841
RMSE train: 1.918427	val: 3.056965	test: 2.693943
MAE train: 1.705233	val: 2.427611	test: 2.138018

Epoch: 36
Loss: 2.678053617477417
RMSE train: 1.822079	val: 2.930241	test: 2.551206
MAE train: 1.620148	val: 2.293462	test: 1.999927

Epoch: 37
Loss: 2.3472265005111694
RMSE train: 1.736717	val: 2.855359	test: 2.390121
MAE train: 1.556874	val: 2.199060	test: 1.855410

Epoch: 38
Loss: 2.325586199760437
RMSE train: 1.677817	val: 2.912379	test: 2.311668
MAE train: 1.504086	val: 2.165624	test: 1.757658

Epoch: 39
Loss: 2.1661961674690247
RMSE train: 1.641897	val: 3.065418	test: 2.338492
MAE train: 1.470450	val: 2.223321	test: 1.770100

Epoch: 40
Loss: 1.839856207370758
RMSE train: 1.610970	val: 3.140947	test: 2.390151
MAE train: 1.436491	val: 2.271832	test: 1.829603

Epoch: 41
Loss: 1.8941694498062134
RMSE train: 1.579902	val: 3.187090	test: 2.402130
MAE train: 1.399995	val: 2.284278	test: 1.833681

Epoch: 42
Loss: 1.7249313592910767
RMSE train: 1.501198	val: 3.199841	test: 2.328789
MAE train: 1.340253	val: 2.313039	test: 1.764634

Epoch: 43
Loss: 1.5450118780136108
RMSE train: 1.395230	val: 3.312677	test: 2.262686
MAE train: 1.243122	val: 2.398182	test: 1.698174

Epoch: 44
Loss: 1.4212203621864319
RMSE train: 1.336360	val: 3.416373	test: 2.277955
MAE train: 1.171920	val: 2.443187	test: 1.716494

Epoch: 45
Loss: 1.3833511471748352
RMSE train: 1.297491	val: 3.392110	test: 2.325399
MAE train: 1.117653	val: 2.412765	test: 1.769554

Epoch: 46
Loss: 1.2636706829071045
RMSE train: 1.251601	val: 3.401624	test: 2.335465
MAE train: 1.060862	val: 2.396414	test: 1.774230

Epoch: 47
Loss: 1.218046247959137
RMSE train: 1.202623	val: 3.300461	test: 2.296359
MAE train: 1.017523	val: 2.353413	test: 1.735219

Epoch: 48
Loss: 1.1691377758979797
RMSE train: 1.179693	val: 3.267544	test: 2.255328
MAE train: 1.001378	val: 2.326349	test: 1.678149

Epoch: 49
Loss: 1.1273221969604492
RMSE train: 1.148612	val: 3.304245	test: 2.264918
MAE train: 0.973289	val: 2.334734	test: 1.664565

Epoch: 50
Loss: 1.0359026789665222
RMSE train: 1.117723	val: 3.378943	test: 2.313805
MAE train: 0.939969	val: 2.375755	test: 1.696744

Epoch: 51
Loss: 0.9059924781322479
RMSE train: 1.059787	val: 3.429950	test: 2.300079
MAE train: 0.881582	val: 2.405926	test: 1.672474

Epoch: 52
Loss: 0.925011396408081
RMSE train: 0.987321	val: 3.396791	test: 2.204022
MAE train: 0.813942	val: 2.386309	test: 1.598554

Epoch: 53
Loss: 0.9056044816970825
RMSE train: 0.963941	val: 3.415542	test: 2.147318
MAE train: 0.799285	val: 2.411230	test: 1.548158

Epoch: 54
Loss: 0.8670989274978638
RMSE train: 0.935047	val: 3.381173	test: 2.129283
MAE train: 0.772038	val: 2.354061	test: 1.520695

Epoch: 55
Loss: 0.8895463645458221
RMSE train: 0.888661	val: 3.273378	test: 2.075822
MAE train: 0.726483	val: 2.254732	test: 1.470389

Epoch: 56
Loss: 0.6948780715465546
RMSE train: 0.869217	val: 3.198371	test: 2.077796
MAE train: 0.703562	val: 2.178432	test: 1.450329

Epoch: 57
Loss: 0.8869548141956329
RMSE train: 0.847430	val: 3.092960	test: 2.019176
MAE train: 0.682568	val: 2.080359	test: 1.374206

Epoch: 58
Loss: 0.817408949136734
RMSE train: 0.826449	val: 3.085366	test: 1.995113
MAE train: 0.670598	val: 2.067696	test: 1.342420

Epoch: 59
Loss: 0.768666684627533
RMSE train: 0.821310	val: 3.070996	test: 1.998697
MAE train: 0.669351	val: 2.046322	test: 1.364724

Epoch: 60
Loss: 0.7275091409683228
RMSE train: 0.820109	val: 3.140156	test: 2.055102
MAE train: 0.672633	val: 2.089218	test: 1.416790

Epoch: 61
Loss: 0.7902681231498718
RMSE train: 0.848083	val: 3.234479	test: 2.115317
MAE train: 0.706942	val: 2.168074	test: 1.470295

Epoch: 62
Loss: 0.7596400082111359
RMSE train: 0.868349	val: 3.320291	test: 2.136578
MAE train: 0.738333	val: 2.250524	test: 1.490570

Epoch: 63
Loss: 0.6965042352676392
RMSE train: 0.893948	val: 3.438207	test: 2.157116
MAE train: 0.755780	val: 2.347818	test: 1.495730

Epoch: 64
Loss: 0.8029702305793762
RMSE train: 0.900143	val: 3.564695	test: 2.220410
MAE train: 0.746519	val: 2.410168	test: 1.531525

Epoch: 65
Loss: 0.6615885198116302
RMSE train: 0.899274	val: 3.572008	test: 2.279730
MAE train: 0.734239	val: 2.401057	test: 1.584558

Epoch: 66
Loss: 0.8794404864311218
RMSE train: 0.843002	val: 3.445646	test: 2.273583
MAE train: 0.676768	val: 2.331704	test: 1.583137

Epoch: 67
Loss: 0.7571776807308197
RMSE train: 0.769299	val: 3.205228	test: 2.161322
MAE train: 0.627798	val: 2.239678	test: 1.498921

Epoch: 68
Loss: 0.7127746641635895
RMSE train: 0.725589	val: 2.942959	test: 2.060355
MAE train: 0.589715	val: 2.121712	test: 1.428415

Epoch: 69
Loss: 0.8176076412200928
RMSE train: 0.677398	val: 2.853483	test: 2.042867
MAE train: 0.537057	val: 2.036846	test: 1.414024

Epoch: 70
Loss: 0.7520715594291687
RMSE train: 0.640647	val: 2.951346	test: 2.098218
MAE train: 0.509552	val: 2.070730	test: 1.487996

Epoch: 71
Loss: 0.8166440725326538
RMSE train: 0.652797	val: 3.058581	test: 2.143960
MAE train: 0.513344	val: 2.105188	test: 1.512946

Epoch: 72
Loss: 0.6806528866291046
RMSE train: 0.673233	val: 3.120545	test: 2.166616
MAE train: 0.525660	val: 2.106269	test: 1.502961

Epoch: 73
Loss: 0.7211830019950867
RMSE train: 0.694046	val: 3.037663	test: 2.131709
MAE train: 0.562366	val: 2.037113	test: 1.468464

Epoch: 74
Loss: 0.7678928673267365
RMSE train: 0.722645	val: 3.052712	test: 2.125408
MAE train: 0.604157	val: 2.069268	test: 1.481317

Epoch: 75
Loss: 0.561307430267334
RMSE train: 0.708133	val: 3.142446	test: 2.150161
MAE train: 0.588773	val: 2.101509	test: 1.501769

Epoch: 76
Loss: 0.6609677076339722
RMSE train: 0.670384	val: 3.214520	test: 2.170323
MAE train: 0.542437	val: 2.119959	test: 1.520364

Epoch: 77
Loss: 0.7183017730712891
RMSE train: 0.644308	val: 3.281528	test: 2.180630
MAE train: 0.507019	val: 2.162652	test: 1.523002

Epoch: 78
Loss: 0.6460568308830261
RMSE train: 0.657913	val: 3.311339	test: 2.193272
MAE train: 0.513997	val: 2.187663	test: 1.540038

Epoch: 79
Loss: 0.6999429166316986
RMSE train: 0.689349	val: 3.298668	test: 2.179157
MAE train: 0.525516	val: 2.165178	test: 1.532862

Epoch: 80
Loss: 0.6585913300514221
RMSE train: 0.680906	val: 3.139675	test: 2.110450
MAE train: 0.523679	val: 2.035900	test: 1.458429

Epoch: 81
Loss: 0.6330393552780151
RMSE train: 0.702617	val: 3.086439	test: 2.117702
MAE train: 0.549956	val: 1.964156	test: 1.456976

Epoch: 82
Loss: 0.8250902593135834
RMSE train: 0.701477	val: 3.155276	test: 2.141583
MAE train: 0.555226	val: 1.989810	test: 1.449683

Epoch: 83
Loss: 0.6193355321884155
RMSE train: 0.710936	val: 3.270227	test: 2.192810
MAE train: 0.574646	val: 2.071461	test: 1.495511
RMSE train: 2.246700	val: 4.015704	test: 3.609015
MAE train: 1.874418	val: 2.867582	test: 2.956775

Epoch: 23
Loss: 5.926416397094727
RMSE train: 2.175718	val: 3.937425	test: 3.510602
MAE train: 1.806570	val: 2.767561	test: 2.812202

Epoch: 24
Loss: 5.6611926555633545
RMSE train: 2.099393	val: 3.842585	test: 3.462965
MAE train: 1.736681	val: 2.679105	test: 2.732735

Epoch: 25
Loss: 5.275112271308899
RMSE train: 2.039651	val: 3.739298	test: 3.407362
MAE train: 1.686747	val: 2.590896	test: 2.664834

Epoch: 26
Loss: 5.121880292892456
RMSE train: 2.009755	val: 3.619243	test: 3.359352
MAE train: 1.671946	val: 2.508385	test: 2.600535

Epoch: 27
Loss: 4.626296520233154
RMSE train: 1.992459	val: 3.486718	test: 3.328033
MAE train: 1.669995	val: 2.408912	test: 2.569820

Epoch: 28
Loss: 4.2876877784729
RMSE train: 1.960092	val: 3.383747	test: 3.268544
MAE train: 1.647808	val: 2.393986	test: 2.550118

Epoch: 29
Loss: 4.265994071960449
RMSE train: 1.916212	val: 3.336906	test: 3.230142
MAE train: 1.602351	val: 2.389088	test: 2.531898

Epoch: 30
Loss: 3.822471499443054
RMSE train: 1.878325	val: 3.400221	test: 3.227297
MAE train: 1.563478	val: 2.394226	test: 2.544743

Epoch: 31
Loss: 3.767667531967163
RMSE train: 1.830499	val: 3.431645	test: 3.194004
MAE train: 1.519299	val: 2.354882	test: 2.503374

Epoch: 32
Loss: 3.5408260822296143
RMSE train: 1.789801	val: 3.436403	test: 3.136573
MAE train: 1.481451	val: 2.289169	test: 2.412628

Epoch: 33
Loss: 3.1068555116653442
RMSE train: 1.779129	val: 3.408389	test: 3.158768
MAE train: 1.474290	val: 2.291197	test: 2.432738

Epoch: 34
Loss: 2.734973430633545
RMSE train: 1.725336	val: 3.240584	test: 3.120394
MAE train: 1.437164	val: 2.203950	test: 2.414240

Epoch: 35
Loss: 2.8842148780822754
RMSE train: 1.641265	val: 3.005709	test: 3.018793
MAE train: 1.369913	val: 2.061808	test: 2.303940

Epoch: 36
Loss: 2.62023663520813
RMSE train: 1.543308	val: 2.826098	test: 2.952629
MAE train: 1.279024	val: 1.970581	test: 2.233302

Epoch: 37
Loss: 2.494139313697815
RMSE train: 1.452664	val: 2.802553	test: 2.877418
MAE train: 1.196380	val: 1.964224	test: 2.145799

Epoch: 38
Loss: 2.3025318384170532
RMSE train: 1.375037	val: 2.862952	test: 2.843844
MAE train: 1.119626	val: 1.957850	test: 2.057215

Epoch: 39
Loss: 2.0924167037010193
RMSE train: 1.288452	val: 2.889403	test: 2.769990
MAE train: 1.035454	val: 1.940936	test: 1.961406

Epoch: 40
Loss: 1.9370679259300232
RMSE train: 1.224335	val: 2.850027	test: 2.720408
MAE train: 0.970361	val: 1.943285	test: 1.901173

Epoch: 41
Loss: 1.7655702829360962
RMSE train: 1.191878	val: 2.816196	test: 2.735517
MAE train: 0.940609	val: 1.944476	test: 1.928934

Epoch: 42
Loss: 1.7521947026252747
RMSE train: 1.150240	val: 2.789461	test: 2.776748
MAE train: 0.910270	val: 1.940251	test: 1.989454

Epoch: 43
Loss: 1.6938639879226685
RMSE train: 1.164856	val: 2.798364	test: 2.838208
MAE train: 0.929198	val: 1.914032	test: 2.024631

Epoch: 44
Loss: 1.5959699153900146
RMSE train: 1.206949	val: 2.904407	test: 2.921870
MAE train: 0.966700	val: 1.973046	test: 2.080491

Epoch: 45
Loss: 1.5779521465301514
RMSE train: 1.262889	val: 3.062290	test: 3.001766
MAE train: 1.023180	val: 2.123318	test: 2.157246

Epoch: 46
Loss: 1.4267746806144714
RMSE train: 1.196985	val: 3.112014	test: 2.933420
MAE train: 0.959839	val: 2.158496	test: 2.110631

Epoch: 47
Loss: 1.3916949033737183
RMSE train: 1.136484	val: 3.145166	test: 2.889988
MAE train: 0.893262	val: 2.137890	test: 2.058624

Epoch: 48
Loss: 1.308696687221527
RMSE train: 1.060055	val: 3.039348	test: 2.851532
MAE train: 0.825850	val: 2.076073	test: 2.037433

Epoch: 49
Loss: 1.2465932369232178
RMSE train: 0.986930	val: 2.934741	test: 2.791295
MAE train: 0.771704	val: 2.053228	test: 2.013130

Epoch: 50
Loss: 1.315242052078247
RMSE train: 0.907580	val: 2.834975	test: 2.742215
MAE train: 0.711919	val: 2.037089	test: 2.020581

Epoch: 51
Loss: 1.229026198387146
RMSE train: 0.857349	val: 2.861363	test: 2.763999
MAE train: 0.666703	val: 2.054841	test: 2.047495

Epoch: 52
Loss: 1.1588123440742493
RMSE train: 0.820553	val: 2.888799	test: 2.798308
MAE train: 0.632883	val: 2.088997	test: 2.084473

Epoch: 53
Loss: 1.0788818895816803
RMSE train: 0.803286	val: 2.888187	test: 2.817344
MAE train: 0.618705	val: 2.083679	test: 2.104379

Epoch: 54
Loss: 1.1636874675750732
RMSE train: 0.794828	val: 2.856575	test: 2.817539
MAE train: 0.619735	val: 2.037343	test: 2.095628

Epoch: 55
Loss: 1.308085858821869
RMSE train: 0.779930	val: 2.901084	test: 2.825342
MAE train: 0.610829	val: 2.033571	test: 2.099982

Epoch: 56
Loss: 1.0551010966300964
RMSE train: 0.767585	val: 2.894062	test: 2.834660
MAE train: 0.596623	val: 2.005423	test: 2.092417

Epoch: 57
Loss: 1.0593999028205872
RMSE train: 0.754359	val: 2.867655	test: 2.857760
MAE train: 0.583414	val: 2.023588	test: 2.139581

Epoch: 58
Loss: 0.9349747598171234
RMSE train: 0.744195	val: 2.799111	test: 2.884649
MAE train: 0.578803	val: 2.023905	test: 2.169471

Epoch: 59
Loss: 1.2525891661643982
RMSE train: 0.736260	val: 2.787753	test: 2.895829
MAE train: 0.572345	val: 2.039421	test: 2.193523

Epoch: 60
Loss: 1.0446088910102844
RMSE train: 0.751549	val: 2.789889	test: 2.900059
MAE train: 0.587131	val: 2.059829	test: 2.205626

Epoch: 61
Loss: 0.9134765863418579
RMSE train: 0.756048	val: 2.815135	test: 2.891843
MAE train: 0.594177	val: 2.064662	test: 2.222661

Epoch: 62
Loss: 0.9420126378536224
RMSE train: 0.785356	val: 2.939928	test: 2.883228
MAE train: 0.616116	val: 2.139062	test: 2.225836

Epoch: 63
Loss: 0.9691317081451416
RMSE train: 0.825508	val: 3.060049	test: 2.903163
MAE train: 0.651015	val: 2.140875	test: 2.197567

Epoch: 64
Loss: 0.9931287467479706
RMSE train: 0.863945	val: 3.093268	test: 3.001027
MAE train: 0.683007	val: 2.074535	test: 2.168938

Epoch: 65
Loss: 0.9516773521900177
RMSE train: 0.839575	val: 3.051931	test: 3.035086
MAE train: 0.668764	val: 2.040896	test: 2.183495

Epoch: 66
Loss: 0.958991140127182
RMSE train: 0.761050	val: 2.990586	test: 2.991594
MAE train: 0.600105	val: 2.098982	test: 2.202763

Epoch: 67
Loss: 1.008380502462387
RMSE train: 0.691984	val: 2.930624	test: 2.909885
MAE train: 0.545481	val: 2.151971	test: 2.200907

Epoch: 68
Loss: 0.9932613968849182
RMSE train: 0.660999	val: 2.850310	test: 2.902918
MAE train: 0.526429	val: 2.095219	test: 2.210033

Epoch: 69
Loss: 0.9780367612838745
RMSE train: 0.652018	val: 2.772413	test: 2.944609
MAE train: 0.516011	val: 2.051909	test: 2.219217

Epoch: 70
Loss: 0.8766994476318359
RMSE train: 0.695599	val: 2.749957	test: 2.984738
MAE train: 0.550881	val: 2.018311	test: 2.216881

Epoch: 71
Loss: 0.8757691085338593
RMSE train: 0.759051	val: 2.715756	test: 3.062102
MAE train: 0.606166	val: 1.958896	test: 2.204717

Epoch: 72
Loss: 0.849194347858429
RMSE train: 0.805172	val: 2.711020	test: 3.095785
MAE train: 0.652042	val: 1.952261	test: 2.202597

Epoch: 73
Loss: 0.7709144949913025
RMSE train: 0.816458	val: 2.760039	test: 3.097428
MAE train: 0.658900	val: 1.993817	test: 2.223791

Epoch: 74
Loss: 0.9019132256507874
RMSE train: 0.825435	val: 2.888335	test: 3.061908
MAE train: 0.663334	val: 2.047862	test: 2.209080

Epoch: 75
Loss: 0.9534347355365753
RMSE train: 0.812837	val: 2.998632	test: 2.977550
MAE train: 0.657509	val: 2.153731	test: 2.198527

Epoch: 76
Loss: 0.8942952752113342
RMSE train: 0.818235	val: 3.087221	test: 2.892733
MAE train: 0.661523	val: 2.262994	test: 2.160600

Epoch: 77
Loss: 0.8231723010540009
RMSE train: 0.788489	val: 3.024672	test: 2.868958
MAE train: 0.645358	val: 2.204017	test: 2.134360

Epoch: 78
Loss: 0.7929894328117371
RMSE train: 0.758159	val: 2.980420	test: 2.890302
MAE train: 0.619694	val: 2.130328	test: 2.123856

Epoch: 79
Loss: 0.7665859162807465
RMSE train: 0.724571	val: 2.995681	test: 2.950441
MAE train: 0.583658	val: 2.073678	test: 2.110387

Epoch: 80
Loss: 0.8080470263957977
RMSE train: 0.730662	val: 3.092122	test: 3.052533
MAE train: 0.570312	val: 2.086828	test: 2.149385

Epoch: 81
Loss: 0.8536745011806488
RMSE train: 0.719563	val: 3.085910	test: 3.091925
MAE train: 0.561515	val: 2.064582	test: 2.180950

Epoch: 82
Loss: 0.7017319798469543
RMSE train: 0.663690	val: 3.020917	test: 3.027161
MAE train: 0.517139	val: 2.043228	test: 2.138044

Epoch: 83
Loss: 0.6725250780582428
RMSE train: 2.669044	val: 4.201938	test: 4.318471
MAE train: 2.306682	val: 3.438715	test: 3.746891

Epoch: 23
Loss: 6.280382871627808
RMSE train: 2.580013	val: 4.057208	test: 4.166053
MAE train: 2.224116	val: 3.266892	test: 3.577682

Epoch: 24
Loss: 6.084535837173462
RMSE train: 2.520299	val: 3.945592	test: 4.099926
MAE train: 2.171239	val: 3.152770	test: 3.473290

Epoch: 25
Loss: 5.428200721740723
RMSE train: 2.441373	val: 3.845463	test: 4.023514
MAE train: 2.093542	val: 3.054046	test: 3.367458

Epoch: 26
Loss: 5.265596628189087
RMSE train: 2.347544	val: 3.780177	test: 3.926038
MAE train: 2.001606	val: 2.946542	test: 3.269700

Epoch: 27
Loss: 4.892642378807068
RMSE train: 2.299914	val: 3.826183	test: 3.937997
MAE train: 1.957583	val: 2.998131	test: 3.324538

Epoch: 28
Loss: 4.713736534118652
RMSE train: 2.258132	val: 3.877409	test: 3.965949
MAE train: 1.918788	val: 3.068974	test: 3.403907

Epoch: 29
Loss: 4.30328893661499
RMSE train: 2.246268	val: 3.984794	test: 4.043936
MAE train: 1.909133	val: 3.179647	test: 3.515404

Epoch: 30
Loss: 4.128013491630554
RMSE train: 2.222733	val: 3.976840	test: 4.047206
MAE train: 1.896037	val: 3.182080	test: 3.506392

Epoch: 31
Loss: 3.9312515258789062
RMSE train: 2.211610	val: 3.970584	test: 4.060750
MAE train: 1.895761	val: 3.213071	test: 3.517439

Epoch: 32
Loss: 3.7257351875305176
RMSE train: 2.175721	val: 3.902311	test: 4.017929
MAE train: 1.867746	val: 3.164049	test: 3.463226

Epoch: 33
Loss: 3.207131505012512
RMSE train: 2.101891	val: 3.750815	test: 3.860502
MAE train: 1.802846	val: 3.028186	test: 3.270260

Epoch: 34
Loss: 3.2959959506988525
RMSE train: 2.059894	val: 3.641310	test: 3.705615
MAE train: 1.771224	val: 2.909645	test: 3.124789

Epoch: 35
Loss: 3.001795172691345
RMSE train: 2.004084	val: 3.565621	test: 3.561010
MAE train: 1.726934	val: 2.805952	test: 2.999757

Epoch: 36
Loss: 2.853089690208435
RMSE train: 1.908306	val: 3.510800	test: 3.436536
MAE train: 1.636056	val: 2.723671	test: 2.865527

Epoch: 37
Loss: 2.686156988143921
RMSE train: 1.797931	val: 3.467010	test: 3.295062
MAE train: 1.518670	val: 2.618328	test: 2.693612

Epoch: 38
Loss: 2.448657512664795
RMSE train: 1.718557	val: 3.426417	test: 3.237390
MAE train: 1.442625	val: 2.569297	test: 2.633789

Epoch: 39
Loss: 2.3339844942092896
RMSE train: 1.645471	val: 3.393630	test: 3.217338
MAE train: 1.375145	val: 2.542736	test: 2.618161

Epoch: 40
Loss: 2.197114944458008
RMSE train: 1.557031	val: 3.313386	test: 3.150985
MAE train: 1.296050	val: 2.477819	test: 2.554903

Epoch: 41
Loss: 2.1243215799331665
RMSE train: 1.485498	val: 3.314245	test: 3.097861
MAE train: 1.224650	val: 2.472846	test: 2.479000

Epoch: 42
Loss: 2.1236594915390015
RMSE train: 1.406892	val: 3.273041	test: 2.999086
MAE train: 1.146107	val: 2.461233	test: 2.397601

Epoch: 43
Loss: 1.9360166192054749
RMSE train: 1.301140	val: 3.221451	test: 2.827319
MAE train: 1.057955	val: 2.403101	test: 2.282399

Epoch: 44
Loss: 1.6303581595420837
RMSE train: 1.255215	val: 3.243818	test: 2.677610
MAE train: 1.028490	val: 2.441071	test: 2.180156

Epoch: 45
Loss: 1.5795011520385742
RMSE train: 1.247536	val: 3.243603	test: 2.588281
MAE train: 1.026125	val: 2.400792	test: 2.088521

Epoch: 46
Loss: 1.7083104252815247
RMSE train: 1.219629	val: 3.260779	test: 2.566149
MAE train: 0.995185	val: 2.401312	test: 2.049580

Epoch: 47
Loss: 1.4047396183013916
RMSE train: 1.190221	val: 3.355640	test: 2.658677
MAE train: 0.957431	val: 2.454395	test: 2.089883

Epoch: 48
Loss: 1.4752285480499268
RMSE train: 1.153530	val: 3.473241	test: 2.732236
MAE train: 0.905453	val: 2.611954	test: 2.185406

Epoch: 49
Loss: 1.35526442527771
RMSE train: 1.109114	val: 3.610292	test: 2.814629
MAE train: 0.851889	val: 2.763365	test: 2.272575

Epoch: 50
Loss: 1.2866582870483398
RMSE train: 1.091078	val: 3.572945	test: 2.818696
MAE train: 0.845751	val: 2.641571	test: 2.283962

Epoch: 51
Loss: 1.2328500151634216
RMSE train: 1.095126	val: 3.505317	test: 2.775816
MAE train: 0.862159	val: 2.464296	test: 2.186453

Epoch: 52
Loss: 1.1592063307762146
RMSE train: 1.115070	val: 3.463317	test: 2.750500
MAE train: 0.885077	val: 2.412064	test: 2.149457

Epoch: 53
Loss: 1.2091569304466248
RMSE train: 1.077465	val: 3.378319	test: 2.675745
MAE train: 0.860139	val: 2.351850	test: 2.059478

Epoch: 54
Loss: 1.1777361631393433
RMSE train: 1.039974	val: 3.370776	test: 2.629532
MAE train: 0.819795	val: 2.441490	test: 2.062104

Epoch: 55
Loss: 1.1930561661720276
RMSE train: 1.006370	val: 3.390663	test: 2.634855
MAE train: 0.805286	val: 2.432095	test: 2.032535

Epoch: 56
Loss: 1.0351426601409912
RMSE train: 1.021444	val: 3.498104	test: 2.695963
MAE train: 0.819833	val: 2.457486	test: 2.058972

Epoch: 57
Loss: 1.109758734703064
RMSE train: 1.036743	val: 3.581009	test: 2.753760
MAE train: 0.829129	val: 2.522701	test: 2.119295

Epoch: 58
Loss: 1.1826341152191162
RMSE train: 0.973677	val: 3.519976	test: 2.715551
MAE train: 0.787648	val: 2.521055	test: 2.101565

Epoch: 59
Loss: 1.0619834661483765
RMSE train: 0.930509	val: 3.495327	test: 2.648783
MAE train: 0.753784	val: 2.488486	test: 2.033220

Epoch: 60
Loss: 1.05008864402771
RMSE train: 0.919283	val: 3.552261	test: 2.643781
MAE train: 0.731062	val: 2.462990	test: 2.003272

Epoch: 61
Loss: 1.0525814890861511
RMSE train: 0.893500	val: 3.582594	test: 2.622705
MAE train: 0.698508	val: 2.435128	test: 1.963062

Epoch: 62
Loss: 1.0703713297843933
RMSE train: 0.823611	val: 3.628320	test: 2.629247
MAE train: 0.640519	val: 2.424694	test: 1.969549

Epoch: 63
Loss: 1.0552940964698792
RMSE train: 0.747711	val: 3.659443	test: 2.690300
MAE train: 0.581468	val: 2.410378	test: 2.020943

Epoch: 64
Loss: 1.089173674583435
RMSE train: 0.727231	val: 3.684002	test: 2.788035
MAE train: 0.563712	val: 2.493869	test: 2.150485

Epoch: 65
Loss: 1.0458116233348846
RMSE train: 0.757681	val: 3.702010	test: 2.875746
MAE train: 0.585358	val: 2.585744	test: 2.254584

Epoch: 66
Loss: 1.0885041952133179
RMSE train: 0.798977	val: 3.686977	test: 2.873512
MAE train: 0.618472	val: 2.629153	test: 2.246074

Epoch: 67
Loss: 0.9367922842502594
RMSE train: 0.813017	val: 3.634964	test: 2.826130
MAE train: 0.632999	val: 2.524724	test: 2.178607

Epoch: 68
Loss: 1.0387332439422607
RMSE train: 0.800516	val: 3.621407	test: 2.749786
MAE train: 0.620670	val: 2.506914	test: 2.100280

Epoch: 69
Loss: 0.9046077728271484
RMSE train: 0.717735	val: 3.572548	test: 2.605601
MAE train: 0.556250	val: 2.511061	test: 1.980755

Epoch: 70
Loss: 0.9568199515342712
RMSE train: 0.653230	val: 3.502458	test: 2.516226
MAE train: 0.506321	val: 2.495227	test: 1.892697

Epoch: 71
Loss: 1.0187196135520935
RMSE train: 0.618065	val: 3.470851	test: 2.487607
MAE train: 0.481979	val: 2.517710	test: 1.870228

Epoch: 72
Loss: 0.9381884932518005
RMSE train: 0.621449	val: 3.490955	test: 2.516739
MAE train: 0.485812	val: 2.518962	test: 1.896498

Epoch: 73
Loss: 0.8050097823143005
RMSE train: 0.631652	val: 3.576879	test: 2.596591
MAE train: 0.493515	val: 2.516862	test: 1.949661

Epoch: 74
Loss: 1.0446107387542725
RMSE train: 0.664523	val: 3.633691	test: 2.684892
MAE train: 0.513257	val: 2.520626	test: 2.035555

Epoch: 75
Loss: 0.9049910306930542
RMSE train: 0.719641	val: 3.676661	test: 2.767574
MAE train: 0.550355	val: 2.502026	test: 2.092355

Epoch: 76
Loss: 0.9344145953655243
RMSE train: 0.777541	val: 3.705570	test: 2.857757
MAE train: 0.588510	val: 2.551726	test: 2.167498

Epoch: 77
Loss: 0.8449895977973938
RMSE train: 0.821247	val: 3.728409	test: 2.890847
MAE train: 0.618725	val: 2.612364	test: 2.204003

Epoch: 78
Loss: 1.0156448483467102
RMSE train: 0.859144	val: 3.770247	test: 2.910488
MAE train: 0.644812	val: 2.648609	test: 2.226463

Epoch: 79
Loss: 0.9735841155052185
RMSE train: 0.866969	val: 3.756848	test: 2.897256
MAE train: 0.663573	val: 2.631556	test: 2.209033

Epoch: 80
Loss: 0.8071494698524475
RMSE train: 0.889847	val: 3.716052	test: 2.899445
MAE train: 0.707744	val: 2.588365	test: 2.197054

Epoch: 81
Loss: 0.7195006310939789
RMSE train: 0.915754	val: 3.664933	test: 2.915481
MAE train: 0.740761	val: 2.554227	test: 2.218063

Epoch: 82
Loss: 0.6770990788936615
RMSE train: 0.921022	val: 3.608694	test: 2.927813
MAE train: 0.745655	val: 2.509434	test: 2.239511

Epoch: 83
Loss: 0.854326069355011
RMSE train: 2.678825	val: 4.199567	test: 4.536366
MAE train: 2.225140	val: 2.807256	test: 3.718307

Epoch: 23
Loss: 6.778178691864014
RMSE train: 2.603574	val: 4.148774	test: 4.357694
MAE train: 2.156479	val: 2.731707	test: 3.575856

Epoch: 24
Loss: 6.479969501495361
RMSE train: 2.548371	val: 4.137449	test: 4.237738
MAE train: 2.111866	val: 2.695722	test: 3.468133

Epoch: 25
Loss: 5.910747528076172
RMSE train: 2.450831	val: 4.133197	test: 4.143595
MAE train: 2.022692	val: 2.719739	test: 3.432848

Epoch: 26
Loss: 5.544477224349976
RMSE train: 2.362654	val: 4.113071	test: 4.050801
MAE train: 1.943457	val: 2.764708	test: 3.372879

Epoch: 27
Loss: 5.276795148849487
RMSE train: 2.277006	val: 4.094413	test: 4.037321
MAE train: 1.866843	val: 2.814775	test: 3.392009

Epoch: 28
Loss: 5.025552749633789
RMSE train: 2.211644	val: 4.043073	test: 3.975526
MAE train: 1.814065	val: 2.765545	test: 3.303289

Epoch: 29
Loss: 4.582771301269531
RMSE train: 2.152176	val: 3.931506	test: 3.931145
MAE train: 1.760892	val: 2.642429	test: 3.209642

Epoch: 30
Loss: 4.465455412864685
RMSE train: 2.049419	val: 3.876584	test: 3.883124
MAE train: 1.673580	val: 2.591808	test: 3.144659

Epoch: 31
Loss: 4.0461814403533936
RMSE train: 1.957337	val: 3.863387	test: 3.826990
MAE train: 1.604509	val: 2.620599	test: 3.108604

Epoch: 32
Loss: 3.982677698135376
RMSE train: 1.860367	val: 3.921100	test: 3.804299
MAE train: 1.524401	val: 2.769000	test: 3.164002

Epoch: 33
Loss: 3.6605257987976074
RMSE train: 1.792063	val: 3.944100	test: 3.725997
MAE train: 1.463783	val: 2.799640	test: 3.099518

Epoch: 34
Loss: 3.33981716632843
RMSE train: 1.764933	val: 3.903250	test: 3.586994
MAE train: 1.446306	val: 2.774744	test: 2.974130

Epoch: 35
Loss: 3.235747456550598
RMSE train: 1.752524	val: 3.893058	test: 3.464775
MAE train: 1.437425	val: 2.787195	test: 2.870967

Epoch: 36
Loss: 2.944838523864746
RMSE train: 1.742812	val: 3.867230	test: 3.384797
MAE train: 1.426909	val: 2.764302	test: 2.808484

Epoch: 37
Loss: 2.737921953201294
RMSE train: 1.734476	val: 3.819620	test: 3.283568
MAE train: 1.417986	val: 2.685239	test: 2.688104

Epoch: 38
Loss: 2.545828938484192
RMSE train: 1.668608	val: 3.851510	test: 3.310528
MAE train: 1.364167	val: 2.752947	test: 2.721690

Epoch: 39
Loss: 2.46015465259552
RMSE train: 1.553561	val: 3.960056	test: 3.427173
MAE train: 1.265278	val: 2.955954	test: 2.863712

Epoch: 40
Loss: 2.0988802313804626
RMSE train: 1.422916	val: 4.039964	test: 3.500682
MAE train: 1.150754	val: 3.121520	test: 2.934240

Epoch: 41
Loss: 2.253884792327881
RMSE train: 1.335865	val: 3.975291	test: 3.371800
MAE train: 1.066484	val: 3.089365	test: 2.822550

Epoch: 42
Loss: 2.0279757380485535
RMSE train: 1.307626	val: 3.823586	test: 3.149128
MAE train: 1.039585	val: 2.898925	test: 2.580335

Epoch: 43
Loss: 1.9604265689849854
RMSE train: 1.283186	val: 3.838069	test: 3.075613
MAE train: 1.015669	val: 2.927460	test: 2.493923

Epoch: 44
Loss: 1.8357465267181396
RMSE train: 1.212917	val: 4.150537	test: 3.353507
MAE train: 0.961901	val: 3.351861	test: 2.813106

Epoch: 45
Loss: 1.7128314971923828
RMSE train: 1.173763	val: 4.411815	test: 3.692074
MAE train: 0.926216	val: 3.632028	test: 3.063653

Epoch: 46
Loss: 1.7687325477600098
RMSE train: 1.177253	val: 4.314675	test: 3.504963
MAE train: 0.932929	val: 3.577424	test: 2.938626

Epoch: 47
Loss: 1.7582841515541077
RMSE train: 1.170533	val: 3.943169	test: 3.068868
MAE train: 0.930527	val: 3.176999	test: 2.607207

Epoch: 48
Loss: 1.6589958667755127
RMSE train: 1.133571	val: 3.682126	test: 2.955935
MAE train: 0.897868	val: 2.854490	test: 2.414952

Epoch: 49
Loss: 1.5632854104042053
RMSE train: 1.072325	val: 3.637898	test: 3.013132
MAE train: 0.852222	val: 2.782584	test: 2.445553

Epoch: 50
Loss: 1.3637630343437195
RMSE train: 1.045534	val: 3.626135	test: 2.993832
MAE train: 0.829696	val: 2.760635	test: 2.421815

Epoch: 51
Loss: 1.2977833151817322
RMSE train: 1.000859	val: 3.704886	test: 2.959242
MAE train: 0.789511	val: 2.860401	test: 2.447969

Epoch: 52
Loss: 1.2918683290481567
RMSE train: 1.014237	val: 3.661956	test: 2.877121
MAE train: 0.808661	val: 2.823981	test: 2.341641

Epoch: 53
Loss: 1.3379117846488953
RMSE train: 1.038490	val: 3.656903	test: 2.852490
MAE train: 0.833955	val: 2.801890	test: 2.292652

Epoch: 54
Loss: 1.2288638353347778
RMSE train: 1.055331	val: 3.680735	test: 2.895468
MAE train: 0.844427	val: 2.833770	test: 2.349106

Epoch: 55
Loss: 1.2224488854408264
RMSE train: 1.015612	val: 3.715145	test: 2.953950
MAE train: 0.803241	val: 2.911789	test: 2.413034

Epoch: 56
Loss: 1.309518575668335
RMSE train: 0.955236	val: 3.825121	test: 3.052772
MAE train: 0.739110	val: 3.064466	test: 2.533283

Epoch: 57
Loss: 1.2914837002754211
RMSE train: 0.908174	val: 3.846160	test: 3.054446
MAE train: 0.699009	val: 3.118524	test: 2.534071

Epoch: 58
Loss: 1.3242554068565369
RMSE train: 0.851409	val: 3.798068	test: 2.939525
MAE train: 0.650592	val: 3.083049	test: 2.402156

Epoch: 59
Loss: 1.2387235164642334
RMSE train: 0.831189	val: 3.728793	test: 2.776563
MAE train: 0.627863	val: 3.034178	test: 2.244949

Epoch: 60
Loss: 1.1806439757347107
RMSE train: 0.871020	val: 3.763697	test: 2.680562
MAE train: 0.658847	val: 3.052934	test: 2.125760

Epoch: 61
Loss: 1.1472716927528381
RMSE train: 0.878260	val: 3.922055	test: 2.728334
MAE train: 0.667186	val: 3.221795	test: 2.193028

Epoch: 62
Loss: 1.1297074556350708
RMSE train: 0.851837	val: 4.091791	test: 2.847671
MAE train: 0.650271	val: 3.389420	test: 2.361399

Epoch: 63
Loss: 1.0670185685157776
RMSE train: 0.819635	val: 4.191313	test: 2.951857
MAE train: 0.645086	val: 3.476723	test: 2.459913

Epoch: 64
Loss: 1.1517193913459778
RMSE train: 0.797542	val: 4.342072	test: 3.165853
MAE train: 0.633176	val: 3.619607	test: 2.673433

Epoch: 65
Loss: 1.129600167274475
RMSE train: 0.760686	val: 4.626107	test: 3.494758
MAE train: 0.598331	val: 3.952768	test: 2.938474

Epoch: 66
Loss: 1.2023108005523682
RMSE train: 0.742597	val: 4.691744	test: 3.610636
MAE train: 0.580756	val: 4.007380	test: 3.012146

Epoch: 67
Loss: 1.0668575167655945
RMSE train: 0.742273	val: 4.493158	test: 3.344895
MAE train: 0.580578	val: 3.762504	test: 2.798074

Epoch: 68
Loss: 1.1278918385505676
RMSE train: 0.780271	val: 4.146261	test: 2.951011
MAE train: 0.610603	val: 3.275406	test: 2.410403

Epoch: 69
Loss: 1.1318731904029846
RMSE train: 0.778920	val: 4.050515	test: 2.800923
MAE train: 0.609447	val: 3.191363	test: 2.217639

Epoch: 70
Loss: 1.0399348139762878
RMSE train: 0.777561	val: 4.037184	test: 2.788051
MAE train: 0.598191	val: 3.192149	test: 2.229096

Epoch: 71
Loss: 1.0399648249149323
RMSE train: 0.765785	val: 4.273988	test: 3.040507
MAE train: 0.580778	val: 3.536834	test: 2.525142

Epoch: 72
Loss: 1.0450842380523682
RMSE train: 0.729712	val: 4.374649	test: 3.133175
MAE train: 0.547616	val: 3.661603	test: 2.591512

Epoch: 73
Loss: 0.871062844991684
RMSE train: 0.691000	val: 4.547266	test: 3.300378
MAE train: 0.517464	val: 3.899070	test: 2.719257

Epoch: 74
Loss: 1.008183091878891
RMSE train: 0.678488	val: 4.554796	test: 3.326478
MAE train: 0.515644	val: 3.901740	test: 2.741178

Epoch: 75
Loss: 0.988223522901535
RMSE train: 0.697682	val: 4.437845	test: 3.184959
MAE train: 0.536021	val: 3.738608	test: 2.616342

Epoch: 76
Loss: 1.0341616868972778
RMSE train: 0.706998	val: 4.223333	test: 2.985773
MAE train: 0.536917	val: 3.482097	test: 2.445227

Epoch: 77
Loss: 0.8858985602855682
RMSE train: 0.720204	val: 4.029230	test: 2.808278
MAE train: 0.545813	val: 3.269793	test: 2.270351

Epoch: 78
Loss: 1.0544185042381287
RMSE train: 0.744173	val: 3.899711	test: 2.730987
MAE train: 0.565694	val: 3.111027	test: 2.136282

Epoch: 79
Loss: 1.0339849889278412
RMSE train: 0.741238	val: 3.926020	test: 2.775310
MAE train: 0.573737	val: 3.152633	test: 2.176953

Epoch: 80
Loss: 0.9249028861522675
RMSE train: 0.721608	val: 4.069967	test: 2.909110
MAE train: 0.572138	val: 3.281432	test: 2.242259

Epoch: 81
Loss: 0.864815354347229
RMSE train: 0.687072	val: 4.207767	test: 3.047707
MAE train: 0.546549	val: 3.449102	test: 2.359188

Epoch: 82
Loss: 0.8554671108722687
RMSE train: 0.680050	val: 4.227745	test: 3.097753
MAE train: 0.537435	val: 3.471791	test: 2.424468

Epoch: 83
Loss: 0.7952569127082825
RMSE train: 2.529475	val: 4.173680	test: 3.773219
MAE train: 2.197329	val: 3.282158	test: 3.192724

Epoch: 23
Loss: 5.795539617538452
RMSE train: 2.421469	val: 4.059679	test: 3.710536
MAE train: 2.091149	val: 3.145113	test: 3.090740

Epoch: 24
Loss: 5.491453409194946
RMSE train: 2.301208	val: 3.936729	test: 3.718127
MAE train: 1.967408	val: 3.065885	test: 3.088873

Epoch: 25
Loss: 5.218950986862183
RMSE train: 2.182552	val: 3.739594	test: 3.660472
MAE train: 1.851789	val: 2.895610	test: 3.009842

Epoch: 26
Loss: 4.869893312454224
RMSE train: 2.096026	val: 3.521790	test: 3.541060
MAE train: 1.781779	val: 2.685582	test: 2.855546

Epoch: 27
Loss: 4.480098605155945
RMSE train: 2.043653	val: 3.323258	test: 3.411554
MAE train: 1.750204	val: 2.519792	test: 2.724622

Epoch: 28
Loss: 4.283287286758423
RMSE train: 2.015207	val: 3.201399	test: 3.296872
MAE train: 1.740483	val: 2.451882	test: 2.635496

Epoch: 29
Loss: 4.045283555984497
RMSE train: 2.006425	val: 3.182850	test: 3.268130
MAE train: 1.736646	val: 2.468493	test: 2.608668

Epoch: 30
Loss: 3.769320011138916
RMSE train: 2.040390	val: 3.338322	test: 3.335311
MAE train: 1.768798	val: 2.618086	test: 2.702086

Epoch: 31
Loss: 3.6486849784851074
RMSE train: 2.037776	val: 3.449848	test: 3.367809
MAE train: 1.762794	val: 2.686766	test: 2.728868

Epoch: 32
Loss: 3.3678594827651978
RMSE train: 2.014119	val: 3.504121	test: 3.348629
MAE train: 1.734530	val: 2.672774	test: 2.680234

Epoch: 33
Loss: 2.9831318855285645
RMSE train: 1.990991	val: 3.526710	test: 3.344797
MAE train: 1.705015	val: 2.687083	test: 2.660465

Epoch: 34
Loss: 2.7657182216644287
RMSE train: 1.898682	val: 3.415990	test: 3.264908
MAE train: 1.616950	val: 2.599793	test: 2.578750

Epoch: 35
Loss: 2.715070366859436
RMSE train: 1.742151	val: 3.161460	test: 3.068375
MAE train: 1.476290	val: 2.361360	test: 2.350954

Epoch: 36
Loss: 2.4986612796783447
RMSE train: 1.603242	val: 3.039351	test: 2.994107
MAE train: 1.352815	val: 2.246860	test: 2.261971

Epoch: 37
Loss: 2.2139487266540527
RMSE train: 1.456299	val: 2.969217	test: 2.908753
MAE train: 1.218991	val: 2.178462	test: 2.205025

Epoch: 38
Loss: 2.1093021631240845
RMSE train: 1.362439	val: 2.958966	test: 2.870953
MAE train: 1.137066	val: 2.165832	test: 2.186743

Epoch: 39
Loss: 2.028243064880371
RMSE train: 1.313287	val: 3.003202	test: 2.852501
MAE train: 1.096854	val: 2.150936	test: 2.168452

Epoch: 40
Loss: 1.890821099281311
RMSE train: 1.286217	val: 3.008187	test: 2.844036
MAE train: 1.064503	val: 2.123077	test: 2.161295

Epoch: 41
Loss: 1.653078317642212
RMSE train: 1.270780	val: 2.959348	test: 2.868329
MAE train: 1.041607	val: 2.126395	test: 2.166736

Epoch: 42
Loss: 1.6093018054962158
RMSE train: 1.239386	val: 2.900583	test: 2.906847
MAE train: 1.016245	val: 2.134336	test: 2.170851

Epoch: 43
Loss: 1.539821743965149
RMSE train: 1.237693	val: 2.868177	test: 2.940869
MAE train: 1.020091	val: 2.131443	test: 2.211299

Epoch: 44
Loss: 1.4853485822677612
RMSE train: 1.242805	val: 2.901056	test: 2.968886
MAE train: 1.024503	val: 2.122074	test: 2.229580

Epoch: 45
Loss: 1.4025729894638062
RMSE train: 1.251697	val: 2.951889	test: 2.964151
MAE train: 1.040865	val: 2.093625	test: 2.200565

Epoch: 46
Loss: 1.332754135131836
RMSE train: 1.183019	val: 2.941295	test: 2.871439
MAE train: 0.967733	val: 2.021008	test: 2.099079

Epoch: 47
Loss: 1.3947603106498718
RMSE train: 1.135079	val: 3.038221	test: 2.784216
MAE train: 0.907123	val: 2.023504	test: 2.038584

Epoch: 48
Loss: 1.2214216887950897
RMSE train: 1.086230	val: 3.021801	test: 2.723058
MAE train: 0.869080	val: 2.045650	test: 2.014147

Epoch: 49
Loss: 1.0933777689933777
RMSE train: 1.075426	val: 3.003815	test: 2.720297
MAE train: 0.870981	val: 2.081244	test: 2.006086

Epoch: 50
Loss: 1.2726613283157349
RMSE train: 1.044671	val: 2.997600	test: 2.751105
MAE train: 0.855379	val: 2.112886	test: 2.007977

Epoch: 51
Loss: 1.1882327795028687
RMSE train: 0.974909	val: 3.080584	test: 2.824734
MAE train: 0.788629	val: 2.149682	test: 2.036482

Epoch: 52
Loss: 1.0664790570735931
RMSE train: 0.899877	val: 3.142107	test: 2.828787
MAE train: 0.718411	val: 2.157412	test: 2.050095

Epoch: 53
Loss: 1.0020912289619446
RMSE train: 0.866653	val: 3.232923	test: 2.815833
MAE train: 0.689462	val: 2.200985	test: 2.071224

Epoch: 54
Loss: 1.1526142954826355
RMSE train: 0.873248	val: 3.295685	test: 2.795630
MAE train: 0.698508	val: 2.276693	test: 2.087510

Epoch: 55
Loss: 1.2076141834259033
RMSE train: 0.937877	val: 3.313955	test: 2.770174
MAE train: 0.755479	val: 2.309240	test: 2.040100

Epoch: 56
Loss: 0.931249737739563
RMSE train: 0.975431	val: 3.305352	test: 2.815498
MAE train: 0.780382	val: 2.283213	test: 2.045305

Epoch: 57
Loss: 0.9370585680007935
RMSE train: 0.953340	val: 3.256945	test: 2.834495
MAE train: 0.761068	val: 2.239005	test: 2.056947

Epoch: 58
Loss: 0.9059580564498901
RMSE train: 0.913375	val: 3.181926	test: 2.890786
MAE train: 0.726568	val: 2.177959	test: 2.105291

Epoch: 59
Loss: 1.1310858130455017
RMSE train: 0.844906	val: 3.226087	test: 2.853140
MAE train: 0.674205	val: 2.129956	test: 2.058878

Epoch: 60
Loss: 0.949011355638504
RMSE train: 0.771081	val: 3.259484	test: 2.792235
MAE train: 0.616157	val: 2.122856	test: 2.079689

Epoch: 61
Loss: 0.9788491129875183
RMSE train: 0.711768	val: 3.265342	test: 2.806477
MAE train: 0.565672	val: 2.140693	test: 2.169599

Epoch: 62
Loss: 0.9905967116355896
RMSE train: 0.710737	val: 3.261788	test: 2.846189
MAE train: 0.560272	val: 2.208803	test: 2.249618

Epoch: 63
Loss: 0.919361412525177
RMSE train: 0.729090	val: 3.209623	test: 2.834358
MAE train: 0.571959	val: 2.135241	test: 2.139471

Epoch: 64
Loss: 0.9398508369922638
RMSE train: 0.778841	val: 3.162288	test: 2.910924
MAE train: 0.617939	val: 2.163663	test: 2.080676

Epoch: 65
Loss: 0.9026016294956207
RMSE train: 0.811785	val: 3.159680	test: 2.987596
MAE train: 0.652777	val: 2.210305	test: 2.147319

Epoch: 66
Loss: 0.9499329328536987
RMSE train: 0.777164	val: 3.106757	test: 2.935554
MAE train: 0.621446	val: 2.177297	test: 2.090330

Epoch: 67
Loss: 0.9352318942546844
RMSE train: 0.737310	val: 3.136709	test: 2.861586
MAE train: 0.579214	val: 2.141131	test: 2.070711

Epoch: 68
Loss: 0.9466409981250763
RMSE train: 0.719188	val: 3.246204	test: 2.890752
MAE train: 0.557814	val: 2.195068	test: 2.133622

Epoch: 69
Loss: 0.7913964688777924
RMSE train: 0.751739	val: 3.373981	test: 2.977818
MAE train: 0.582435	val: 2.307714	test: 2.189570

Epoch: 70
Loss: 0.8523033857345581
RMSE train: 0.811519	val: 3.465220	test: 3.067678
MAE train: 0.643693	val: 2.449332	test: 2.252706

Epoch: 71
Loss: 0.7266295552253723
RMSE train: 0.872378	val: 3.507054	test: 3.216925
MAE train: 0.700012	val: 2.587694	test: 2.377811

Epoch: 72
Loss: 0.8273583054542542
RMSE train: 0.907677	val: 3.549390	test: 3.371187
MAE train: 0.733567	val: 2.684302	test: 2.505272

Epoch: 73
Loss: 0.7468712031841278
RMSE train: 0.902155	val: 3.590880	test: 3.369028
MAE train: 0.736215	val: 2.659282	test: 2.459396

Epoch: 74
Loss: 0.7836293578147888
RMSE train: 0.905728	val: 3.696115	test: 3.352467
MAE train: 0.739042	val: 2.583145	test: 2.408663

Epoch: 75
Loss: 0.8485963642597198
RMSE train: 0.919082	val: 3.779791	test: 3.261461
MAE train: 0.737012	val: 2.498650	test: 2.327041

Epoch: 76
Loss: 0.8626042604446411
RMSE train: 0.872782	val: 3.733232	test: 3.185720
MAE train: 0.689599	val: 2.412158	test: 2.294135

Epoch: 77
Loss: 0.75801220536232
RMSE train: 0.761069	val: 3.547487	test: 3.073605
MAE train: 0.603864	val: 2.325923	test: 2.250047

Epoch: 78
Loss: 0.7155075669288635
RMSE train: 0.679266	val: 3.400918	test: 3.000052
MAE train: 0.535612	val: 2.243221	test: 2.192119

Epoch: 79
Loss: 0.7111399173736572
RMSE train: 0.641636	val: 3.326773	test: 2.947230
MAE train: 0.503731	val: 2.178613	test: 2.104138

Epoch: 80
Loss: 0.8136659264564514
RMSE train: 0.641300	val: 3.357884	test: 2.908255
MAE train: 0.500794	val: 2.184170	test: 2.049952

Epoch: 81
Loss: 0.7357815206050873
RMSE train: 0.688450	val: 3.408083	test: 2.924318
MAE train: 0.533619	val: 2.189414	test: 2.051516

Epoch: 82
Loss: 0.6472459435462952
RMSE train: 0.694496	val: 3.462166	test: 2.909291
MAE train: 0.529920	val: 2.219479	test: 2.043031

Epoch: 83
Loss: 0.6455070972442627
RMSE train: 2.545205	val: 3.622875	test: 3.732277
MAE train: 2.230712	val: 2.984854	test: 3.158563

Epoch: 23
Loss: 6.119088172912598
RMSE train: 2.461245	val: 3.598500	test: 3.668698
MAE train: 2.152585	val: 2.964747	test: 3.120841

Epoch: 24
Loss: 5.726127624511719
RMSE train: 2.423961	val: 3.630168	test: 3.672779
MAE train: 2.115867	val: 2.998280	test: 3.144407

Epoch: 25
Loss: 5.196180105209351
RMSE train: 2.368868	val: 3.593200	test: 3.606190
MAE train: 2.056916	val: 2.953504	test: 3.075738

Epoch: 26
Loss: 4.9663004875183105
RMSE train: 2.324313	val: 3.513171	test: 3.497963
MAE train: 2.014136	val: 2.848383	test: 2.950694

Epoch: 27
Loss: 4.691291570663452
RMSE train: 2.262554	val: 3.406633	test: 3.371049
MAE train: 1.958299	val: 2.725534	test: 2.798774

Epoch: 28
Loss: 4.617319345474243
RMSE train: 2.195293	val: 3.372241	test: 3.278502
MAE train: 1.899777	val: 2.662884	test: 2.688661

Epoch: 29
Loss: 4.124510765075684
RMSE train: 2.185808	val: 3.516863	test: 3.343683
MAE train: 1.898572	val: 2.827562	test: 2.788707

Epoch: 30
Loss: 3.8499562740325928
RMSE train: 2.140359	val: 3.564989	test: 3.315852
MAE train: 1.858204	val: 2.899658	test: 2.768906

Epoch: 31
Loss: 3.6836118698120117
RMSE train: 2.105846	val: 3.592402	test: 3.289873
MAE train: 1.829733	val: 2.958419	test: 2.747133

Epoch: 32
Loss: 3.577578067779541
RMSE train: 2.081313	val: 3.613797	test: 3.277860
MAE train: 1.807515	val: 2.986908	test: 2.743990

Epoch: 33
Loss: 3.213854193687439
RMSE train: 2.039633	val: 3.631715	test: 3.215490
MAE train: 1.765000	val: 2.971055	test: 2.688173

Epoch: 34
Loss: 3.0934674739837646
RMSE train: 2.015012	val: 3.641633	test: 3.150215
MAE train: 1.749795	val: 2.950696	test: 2.633119

Epoch: 35
Loss: 2.81035053730011
RMSE train: 1.974484	val: 3.572992	test: 3.026483
MAE train: 1.727038	val: 2.870474	test: 2.525316

Epoch: 36
Loss: 2.505047559738159
RMSE train: 1.916103	val: 3.456861	test: 2.935272
MAE train: 1.676382	val: 2.772137	test: 2.443757

Epoch: 37
Loss: 2.463719964027405
RMSE train: 1.836730	val: 3.314436	test: 2.849478
MAE train: 1.595141	val: 2.648354	test: 2.352560

Epoch: 38
Loss: 2.2438849210739136
RMSE train: 1.771935	val: 3.216326	test: 2.826712
MAE train: 1.524113	val: 2.568827	test: 2.323319

Epoch: 39
Loss: 2.072064995765686
RMSE train: 1.677821	val: 3.031784	test: 2.786351
MAE train: 1.429129	val: 2.452832	test: 2.303464

Epoch: 40
Loss: 2.0739701986312866
RMSE train: 1.576973	val: 2.819550	test: 2.681019
MAE train: 1.337183	val: 2.302961	test: 2.211883

Epoch: 41
Loss: 1.961521327495575
RMSE train: 1.507183	val: 2.879818	test: 2.558020
MAE train: 1.263260	val: 2.325951	test: 2.052292

Epoch: 42
Loss: 1.8915643692016602
RMSE train: 1.450672	val: 3.009250	test: 2.505451
MAE train: 1.202730	val: 2.386973	test: 1.958772

Epoch: 43
Loss: 1.743370771408081
RMSE train: 1.437316	val: 3.113950	test: 2.498875
MAE train: 1.195037	val: 2.443352	test: 1.941538

Epoch: 44
Loss: 1.491250455379486
RMSE train: 1.462862	val: 3.195721	test: 2.471812
MAE train: 1.229621	val: 2.492414	test: 1.919167

Epoch: 45
Loss: 1.4390308856964111
RMSE train: 1.486303	val: 3.211600	test: 2.461766
MAE train: 1.261097	val: 2.538351	test: 1.925003

Epoch: 46
Loss: 1.4091949462890625
RMSE train: 1.453648	val: 3.181396	test: 2.402884
MAE train: 1.231105	val: 2.530992	test: 1.867634

Epoch: 47
Loss: 1.236046016216278
RMSE train: 1.366666	val: 3.169393	test: 2.339113
MAE train: 1.142247	val: 2.514320	test: 1.790465

Epoch: 48
Loss: 1.358752727508545
RMSE train: 1.261113	val: 3.170873	test: 2.267904
MAE train: 1.031109	val: 2.509539	test: 1.706801

Epoch: 49
Loss: 1.153531789779663
RMSE train: 1.136316	val: 3.138854	test: 2.202279
MAE train: 0.910632	val: 2.473232	test: 1.642529

Epoch: 50
Loss: 1.1136615872383118
RMSE train: 1.027077	val: 3.052390	test: 2.165427
MAE train: 0.815398	val: 2.358079	test: 1.592240

Epoch: 51
Loss: 1.1771754026412964
RMSE train: 0.916513	val: 3.002185	test: 2.071593
MAE train: 0.722418	val: 2.260177	test: 1.525668

Epoch: 52
Loss: 1.0525698959827423
RMSE train: 0.882052	val: 2.980324	test: 2.009644
MAE train: 0.693021	val: 2.192917	test: 1.498771

Epoch: 53
Loss: 1.0666562020778656
RMSE train: 0.902616	val: 2.953871	test: 1.994745
MAE train: 0.714705	val: 2.190173	test: 1.515025

Epoch: 54
Loss: 1.055508404970169
RMSE train: 0.959830	val: 2.934341	test: 2.011469
MAE train: 0.759167	val: 2.223243	test: 1.541234

Epoch: 55
Loss: 1.0857428908348083
RMSE train: 1.002979	val: 2.946056	test: 2.106395
MAE train: 0.802637	val: 2.299751	test: 1.612473

Epoch: 56
Loss: 0.9709617495536804
RMSE train: 1.018928	val: 3.013643	test: 2.195324
MAE train: 0.821470	val: 2.362543	test: 1.684414

Epoch: 57
Loss: 0.990671843290329
RMSE train: 0.996967	val: 3.131862	test: 2.202855
MAE train: 0.807312	val: 2.433342	test: 1.677190

Epoch: 58
Loss: 1.007715106010437
RMSE train: 0.946925	val: 3.143960	test: 2.145859
MAE train: 0.770004	val: 2.431213	test: 1.614291

Epoch: 59
Loss: 0.9181389212608337
RMSE train: 0.891163	val: 3.127278	test: 2.074630
MAE train: 0.721202	val: 2.368361	test: 1.527477

Epoch: 60
Loss: 0.9336127042770386
RMSE train: 0.856157	val: 3.122668	test: 2.051185
MAE train: 0.672326	val: 2.260538	test: 1.491574

Epoch: 61
Loss: 0.9054363369941711
RMSE train: 0.836132	val: 3.085494	test: 2.034552
MAE train: 0.644302	val: 2.170969	test: 1.466250

Epoch: 62
Loss: 0.9677082598209381
RMSE train: 0.802927	val: 3.088648	test: 2.020779
MAE train: 0.617078	val: 2.145024	test: 1.455783

Epoch: 63
Loss: 0.8736491501331329
RMSE train: 0.755485	val: 3.064948	test: 2.027011
MAE train: 0.588064	val: 2.130313	test: 1.464636

Epoch: 64
Loss: 0.971724271774292
RMSE train: 0.700723	val: 3.034423	test: 2.036237
MAE train: 0.550477	val: 2.120893	test: 1.494655

Epoch: 65
Loss: 0.9006974399089813
RMSE train: 0.644567	val: 2.956734	test: 2.027767
MAE train: 0.511311	val: 2.160070	test: 1.548180

Epoch: 66
Loss: 0.9717724025249481
RMSE train: 0.655001	val: 2.946717	test: 2.042163
MAE train: 0.523377	val: 2.196194	test: 1.563218

Epoch: 67
Loss: 0.8312038481235504
RMSE train: 0.649010	val: 2.890152	test: 2.045313
MAE train: 0.515027	val: 2.138559	test: 1.528961

Epoch: 68
Loss: 0.9848110973834991
RMSE train: 0.704141	val: 2.945785	test: 2.070465
MAE train: 0.554647	val: 2.159357	test: 1.529409

Epoch: 69
Loss: 0.8098510205745697
RMSE train: 0.726450	val: 2.947887	test: 2.043184
MAE train: 0.569641	val: 2.162919	test: 1.498639

Epoch: 70
Loss: 0.7699112892150879
RMSE train: 0.722573	val: 2.952447	test: 1.994463
MAE train: 0.561580	val: 2.149088	test: 1.467196

Epoch: 71
Loss: 0.799078643321991
RMSE train: 0.673700	val: 2.957808	test: 1.990990
MAE train: 0.526028	val: 2.145945	test: 1.468812

Epoch: 72
Loss: 0.7115440964698792
RMSE train: 0.620547	val: 2.984690	test: 2.016485
MAE train: 0.489583	val: 2.165045	test: 1.512664

Epoch: 73
Loss: 0.6851131021976471
RMSE train: 0.583848	val: 3.015978	test: 2.015754
MAE train: 0.461202	val: 2.148399	test: 1.503994

Epoch: 74
Loss: 0.9079728722572327
RMSE train: 0.577624	val: 3.004578	test: 1.983899
MAE train: 0.454612	val: 2.109456	test: 1.448425

Epoch: 75
Loss: 0.8078800439834595
RMSE train: 0.612916	val: 3.017212	test: 1.965309
MAE train: 0.480244	val: 2.116091	test: 1.427542

Epoch: 76
Loss: 0.8570397198200226
RMSE train: 0.639955	val: 3.005008	test: 1.951013
MAE train: 0.501238	val: 2.103588	test: 1.430039

Epoch: 77
Loss: 0.7840151190757751
RMSE train: 0.684483	val: 3.063171	test: 1.995682
MAE train: 0.532417	val: 2.152119	test: 1.496349

Epoch: 78
Loss: 0.9235370457172394
RMSE train: 0.719603	val: 3.204814	test: 2.087475
MAE train: 0.556892	val: 2.265002	test: 1.585755

Epoch: 79
Loss: 0.8643216788768768
RMSE train: 0.771798	val: 3.304658	test: 2.199187
MAE train: 0.606420	val: 2.360785	test: 1.661965

Epoch: 80
Loss: 0.6568300426006317
RMSE train: 0.836742	val: 3.323595	test: 2.313615
MAE train: 0.658036	val: 2.376314	test: 1.744364

Epoch: 81
Loss: 0.7221938073635101
RMSE train: 0.805564	val: 3.218490	test: 2.247366
MAE train: 0.645972	val: 2.336381	test: 1.666676

Epoch: 82
Loss: 0.6604841947555542
RMSE train: 0.778508	val: 3.095093	test: 2.168115
MAE train: 0.627929	val: 2.286097	test: 1.640004

Epoch: 83
Loss: 0.6984196603298187
RMSE train: 2.415428	val: 4.424183	test: 4.620379
MAE train: 2.037693	val: 3.402372	test: 3.940663

Epoch: 23
Loss: 6.5212562084198
RMSE train: 2.385036	val: 4.456778	test: 4.700307
MAE train: 2.010632	val: 3.475469	test: 4.047017

Epoch: 24
Loss: 5.885998964309692
RMSE train: 2.382629	val: 4.540922	test: 4.773553
MAE train: 2.012097	val: 3.572662	test: 4.128978

Epoch: 25
Loss: 5.753178358078003
RMSE train: 2.373663	val: 4.620748	test: 4.764654
MAE train: 2.009676	val: 3.626645	test: 4.118566

Epoch: 26
Loss: 5.269073724746704
RMSE train: 2.354233	val: 4.683506	test: 4.710967
MAE train: 1.999067	val: 3.644741	test: 4.073735

Epoch: 27
Loss: 5.258209705352783
RMSE train: 2.327270	val: 4.732595	test: 4.665357
MAE train: 1.976290	val: 3.619851	test: 4.035927

Epoch: 28
Loss: 5.01141095161438
RMSE train: 2.277427	val: 4.726110	test: 4.609769
MAE train: 1.928059	val: 3.524527	test: 3.984399

Epoch: 29
Loss: 4.71112322807312
RMSE train: 2.239923	val: 4.689922	test: 4.542861
MAE train: 1.892592	val: 3.422536	test: 3.927027

Epoch: 30
Loss: 4.324043273925781
RMSE train: 2.136729	val: 4.622118	test: 4.493062
MAE train: 1.799238	val: 3.328374	test: 3.885413

Epoch: 31
Loss: 4.19270920753479
RMSE train: 2.029948	val: 4.527540	test: 4.414770
MAE train: 1.708149	val: 3.255582	test: 3.798678

Epoch: 32
Loss: 3.6507487297058105
RMSE train: 1.964801	val: 4.489026	test: 4.352718
MAE train: 1.658219	val: 3.228618	test: 3.733318

Epoch: 33
Loss: 3.590898036956787
RMSE train: 1.916191	val: 4.489031	test: 4.327935
MAE train: 1.620831	val: 3.258162	test: 3.714299

Epoch: 34
Loss: 3.3457577228546143
RMSE train: 1.899177	val: 4.561881	test: 4.392347
MAE train: 1.601903	val: 3.327935	test: 3.784551

Epoch: 35
Loss: 3.2610788345336914
RMSE train: 1.849983	val: 4.600265	test: 4.409243
MAE train: 1.550383	val: 3.349705	test: 3.786681

Epoch: 36
Loss: 2.9295730590820312
RMSE train: 1.777638	val: 4.569850	test: 4.283192
MAE train: 1.482799	val: 3.283388	test: 3.646686

Epoch: 37
Loss: 2.721536874771118
RMSE train: 1.667338	val: 4.483457	test: 4.066525
MAE train: 1.381894	val: 3.127749	test: 3.404250

Epoch: 38
Loss: 2.69343900680542
RMSE train: 1.563245	val: 4.293239	test: 3.799655
MAE train: 1.287038	val: 2.941332	test: 3.165064

Epoch: 39
Loss: 2.3992944955825806
RMSE train: 1.480698	val: 4.096391	test: 3.559567
MAE train: 1.209617	val: 2.776724	test: 2.956126

Epoch: 40
Loss: 2.354637384414673
RMSE train: 1.453028	val: 3.992529	test: 3.474342
MAE train: 1.183633	val: 2.719954	test: 2.891176

Epoch: 41
Loss: 2.121690332889557
RMSE train: 1.414458	val: 3.906238	test: 3.509589
MAE train: 1.149308	val: 2.709685	test: 2.938661

Epoch: 42
Loss: 2.109715521335602
RMSE train: 1.355132	val: 3.823542	test: 3.609880
MAE train: 1.092690	val: 2.687105	test: 3.041858

Epoch: 43
Loss: 1.8248149752616882
RMSE train: 1.307577	val: 3.820938	test: 3.728841
MAE train: 1.038069	val: 2.693169	test: 3.135826

Epoch: 44
Loss: 1.759965717792511
RMSE train: 1.256710	val: 3.844328	test: 3.717908
MAE train: 0.982821	val: 2.659120	test: 3.104991

Epoch: 45
Loss: 1.6700800061225891
RMSE train: 1.218793	val: 3.889438	test: 3.568258
MAE train: 0.953940	val: 2.654982	test: 2.967539

Epoch: 46
Loss: 1.5807647109031677
RMSE train: 1.209815	val: 3.936771	test: 3.459079
MAE train: 0.950963	val: 2.655869	test: 2.874705

Epoch: 47
Loss: 1.4783180356025696
RMSE train: 1.196287	val: 3.958861	test: 3.431487
MAE train: 0.938754	val: 2.688746	test: 2.860987

Epoch: 48
Loss: 1.671687662601471
RMSE train: 1.134928	val: 3.938946	test: 3.424382
MAE train: 0.887565	val: 2.701360	test: 2.862440

Epoch: 49
Loss: 1.3301621079444885
RMSE train: 1.066615	val: 3.921739	test: 3.499484
MAE train: 0.844078	val: 2.790757	test: 2.946450

Epoch: 50
Loss: 1.2719457149505615
RMSE train: 1.006302	val: 3.887230	test: 3.483955
MAE train: 0.793098	val: 2.854678	test: 2.941207

Epoch: 51
Loss: 1.1876808404922485
RMSE train: 0.956567	val: 3.846280	test: 3.387921
MAE train: 0.756071	val: 2.843436	test: 2.849853

Epoch: 52
Loss: 1.2903222739696503
RMSE train: 0.973179	val: 3.869739	test: 3.331123
MAE train: 0.768317	val: 2.848436	test: 2.791832

Epoch: 53
Loss: 1.308955430984497
RMSE train: 0.946529	val: 3.790250	test: 3.192764
MAE train: 0.742117	val: 2.757788	test: 2.641871

Epoch: 54
Loss: 1.290343463420868
RMSE train: 0.925280	val: 3.779785	test: 3.079856
MAE train: 0.714876	val: 2.691312	test: 2.510624

Epoch: 55
Loss: 1.1900615692138672
RMSE train: 0.870168	val: 3.758813	test: 2.991749
MAE train: 0.666173	val: 2.611899	test: 2.416115

Epoch: 56
Loss: 1.2534292936325073
RMSE train: 0.806760	val: 3.686400	test: 2.775920
MAE train: 0.622307	val: 2.570001	test: 2.170228

Epoch: 57
Loss: 1.0744575262069702
RMSE train: 0.773130	val: 3.666251	test: 2.793456
MAE train: 0.596145	val: 2.525061	test: 2.168347

Epoch: 58
Loss: 1.175400733947754
RMSE train: 0.738740	val: 3.652539	test: 2.887370
MAE train: 0.570895	val: 2.508907	test: 2.261319

Epoch: 59
Loss: 1.0933586955070496
RMSE train: 0.731132	val: 3.666201	test: 2.988156
MAE train: 0.560755	val: 2.496701	test: 2.362338

Epoch: 60
Loss: 1.0652003288269043
RMSE train: 0.715202	val: 3.685238	test: 3.042863
MAE train: 0.546675	val: 2.547676	test: 2.413992

Epoch: 61
Loss: 1.1020561456680298
RMSE train: 0.728152	val: 3.747397	test: 3.071654
MAE train: 0.558656	val: 2.601911	test: 2.429630

Epoch: 62
Loss: 1.0023616552352905
RMSE train: 0.734449	val: 3.756136	test: 3.060077
MAE train: 0.571118	val: 2.619959	test: 2.420165

Epoch: 63
Loss: 1.0269717574119568
RMSE train: 0.756065	val: 3.778700	test: 3.077298
MAE train: 0.587756	val: 2.594817	test: 2.433279

Epoch: 64
Loss: 1.0407857298851013
RMSE train: 0.744580	val: 3.786404	test: 3.165884
MAE train: 0.581378	val: 2.582057	test: 2.517129

Epoch: 65
Loss: 0.9431049227714539
RMSE train: 0.701733	val: 3.791591	test: 3.249702
MAE train: 0.550889	val: 2.625433	test: 2.604805

Epoch: 66
Loss: 0.9600967466831207
RMSE train: 0.712646	val: 3.819939	test: 3.197065
MAE train: 0.558238	val: 2.622294	test: 2.548441

Epoch: 67
Loss: 0.951279878616333
RMSE train: 0.735069	val: 3.794168	test: 3.129984
MAE train: 0.572546	val: 2.557878	test: 2.471176

Epoch: 68
Loss: 0.9656082093715668
RMSE train: 0.746615	val: 3.753437	test: 3.001952
MAE train: 0.574741	val: 2.511075	test: 2.342870

Epoch: 69
Loss: 0.911674290895462
RMSE train: 0.734021	val: 3.738936	test: 2.963761
MAE train: 0.560547	val: 2.493249	test: 2.284818

Epoch: 70
Loss: 0.9129866659641266
RMSE train: 0.704731	val: 3.720645	test: 3.002544
MAE train: 0.539303	val: 2.490567	test: 2.315881

Epoch: 71
Loss: 0.8454842269420624
RMSE train: 0.688589	val: 3.737429	test: 3.102746
MAE train: 0.528406	val: 2.480443	test: 2.396957

Epoch: 72
Loss: 0.9058526158332825
RMSE train: 0.691946	val: 3.707453	test: 3.129716
MAE train: 0.535737	val: 2.432863	test: 2.417890

Epoch: 73
Loss: 0.8833502531051636
RMSE train: 0.678885	val: 3.702916	test: 3.070524
MAE train: 0.526967	val: 2.442319	test: 2.363047

Epoch: 74
Loss: 0.8970414400100708
RMSE train: 0.654112	val: 3.706713	test: 3.059505
MAE train: 0.506523	val: 2.500499	test: 2.355714

Epoch: 75
Loss: 0.9042857885360718
RMSE train: 0.670484	val: 3.797356	test: 3.118309
MAE train: 0.516850	val: 2.584502	test: 2.412711

Epoch: 76
Loss: 0.7899423837661743
RMSE train: 0.669449	val: 3.888219	test: 3.232734
MAE train: 0.510080	val: 2.622731	test: 2.521630

Epoch: 77
Loss: 0.7765124142169952
RMSE train: 0.636110	val: 3.918677	test: 3.270972
MAE train: 0.480540	val: 2.636679	test: 2.550851

Epoch: 78
Loss: 0.7640486061573029
RMSE train: 0.595763	val: 3.929381	test: 3.205817
MAE train: 0.450289	val: 2.683682	test: 2.488201

Epoch: 79
Loss: 0.8089580833911896
RMSE train: 0.597422	val: 3.925792	test: 3.210477
MAE train: 0.450346	val: 2.679905	test: 2.490062

Epoch: 80
Loss: 0.8621447086334229
RMSE train: 0.611624	val: 3.877164	test: 3.089145
MAE train: 0.463560	val: 2.643535	test: 2.368553

Epoch: 81
Loss: 0.9144830107688904
RMSE train: 0.622699	val: 3.887230	test: 3.015728
MAE train: 0.477692	val: 2.651511	test: 2.297167

Epoch: 82
Loss: 1.0699931979179382
RMSE train: 0.598892	val: 3.893585	test: 2.979973
MAE train: 0.462889	val: 2.671983	test: 2.282746

Epoch: 83
Loss: 0.9739262759685516
RMSE train: 2.378621	val: 4.840347	test: 5.273038
MAE train: 1.954962	val: 3.822319	test: 4.461804

Epoch: 23
Loss: 7.102534532546997
RMSE train: 2.349281	val: 4.907769	test: 5.384245
MAE train: 1.934830	val: 3.921280	test: 4.607273

Epoch: 24
Loss: 6.482277870178223
RMSE train: 2.327123	val: 4.965459	test: 5.450729
MAE train: 1.917530	val: 4.009954	test: 4.680354

Epoch: 25
Loss: 6.442076206207275
RMSE train: 2.311494	val: 4.985305	test: 5.443998
MAE train: 1.910100	val: 4.048532	test: 4.672429

Epoch: 26
Loss: 5.717625617980957
RMSE train: 2.300191	val: 4.937759	test: 5.354537
MAE train: 1.912262	val: 3.951381	test: 4.573539

Epoch: 27
Loss: 5.612041234970093
RMSE train: 2.320025	val: 4.877563	test: 5.218933
MAE train: 1.945880	val: 3.815693	test: 4.439782

Epoch: 28
Loss: 5.300232887268066
RMSE train: 2.320061	val: 4.749576	test: 5.010353
MAE train: 1.954856	val: 3.672344	test: 4.271967

Epoch: 29
Loss: 5.050559759140015
RMSE train: 2.322894	val: 4.696385	test: 4.907364
MAE train: 1.960282	val: 3.645280	test: 4.216742

Epoch: 30
Loss: 4.642124176025391
RMSE train: 2.246821	val: 4.632992	test: 4.889069
MAE train: 1.899187	val: 3.650899	test: 4.235591

Epoch: 31
Loss: 4.589864134788513
RMSE train: 2.140549	val: 4.588263	test: 4.959712
MAE train: 1.804798	val: 3.659885	test: 4.294916

Epoch: 32
Loss: 4.072129845619202
RMSE train: 2.034038	val: 4.530017	test: 5.017083
MAE train: 1.700043	val: 3.630799	test: 4.316407

Epoch: 33
Loss: 3.8156652450561523
RMSE train: 1.930277	val: 4.499703	test: 5.029480
MAE train: 1.597583	val: 3.545128	test: 4.287435

Epoch: 34
Loss: 3.579763412475586
RMSE train: 1.853260	val: 4.453359	test: 4.944054
MAE train: 1.533027	val: 3.427485	test: 4.182930

Epoch: 35
Loss: 3.396940588951111
RMSE train: 1.825087	val: 4.502282	test: 4.959824
MAE train: 1.513684	val: 3.399298	test: 4.189222

Epoch: 36
Loss: 3.1797810792922974
RMSE train: 1.802623	val: 4.546631	test: 4.908410
MAE train: 1.492579	val: 3.434585	test: 4.172726

Epoch: 37
Loss: 2.826285481452942
RMSE train: 1.733217	val: 4.478910	test: 4.727734
MAE train: 1.429686	val: 3.366500	test: 4.001403

Epoch: 38
Loss: 2.940004348754883
RMSE train: 1.624515	val: 4.325079	test: 4.509681
MAE train: 1.332983	val: 3.237062	test: 3.764584

Epoch: 39
Loss: 2.648584008216858
RMSE train: 1.515644	val: 4.145991	test: 4.209274
MAE train: 1.233615	val: 3.077892	test: 3.453662

Epoch: 40
Loss: 2.469149589538574
RMSE train: 1.476047	val: 4.063812	test: 4.048192
MAE train: 1.200577	val: 3.024545	test: 3.294510

Epoch: 41
Loss: 2.200819969177246
RMSE train: 1.421355	val: 4.009269	test: 4.040672
MAE train: 1.156834	val: 3.001715	test: 3.298170

Epoch: 42
Loss: 2.375195026397705
RMSE train: 1.386446	val: 4.018749	test: 4.090688
MAE train: 1.116937	val: 3.053881	test: 3.399501

Epoch: 43
Loss: 1.8741344809532166
RMSE train: 1.317931	val: 3.985492	test: 4.117969
MAE train: 1.051028	val: 3.049000	test: 3.425112

Epoch: 44
Loss: 2.0331844091415405
RMSE train: 1.200474	val: 3.871706	test: 3.997208
MAE train: 0.946045	val: 2.959328	test: 3.299746

Epoch: 45
Loss: 1.7386212944984436
RMSE train: 1.134027	val: 3.799092	test: 3.738719
MAE train: 0.888783	val: 2.859547	test: 3.043012

Epoch: 46
Loss: 1.8087797164916992
RMSE train: 1.148377	val: 3.807051	test: 3.657314
MAE train: 0.899258	val: 2.874569	test: 3.004408

Epoch: 47
Loss: 1.4735594987869263
RMSE train: 1.153348	val: 3.835685	test: 3.715489
MAE train: 0.903595	val: 2.839604	test: 3.020343

Epoch: 48
Loss: 1.891797125339508
RMSE train: 1.111419	val: 3.889853	test: 3.851906
MAE train: 0.873614	val: 2.839519	test: 3.141180

Epoch: 49
Loss: 1.4684045314788818
RMSE train: 1.037104	val: 3.908456	test: 3.888786
MAE train: 0.819143	val: 2.865995	test: 3.183435

Epoch: 50
Loss: 1.486267328262329
RMSE train: 0.906197	val: 3.832343	test: 3.773065
MAE train: 0.707898	val: 2.855359	test: 3.096867

Epoch: 51
Loss: 1.4202903509140015
RMSE train: 0.839180	val: 3.807670	test: 3.672104
MAE train: 0.641136	val: 2.916206	test: 3.096404

Epoch: 52
Loss: 1.444149672985077
RMSE train: 0.887791	val: 3.846160	test: 3.653901
MAE train: 0.673824	val: 3.011784	test: 3.114863

Epoch: 53
Loss: 1.6118990778923035
RMSE train: 0.930614	val: 3.804898	test: 3.587475
MAE train: 0.708643	val: 2.889978	test: 2.983652

Epoch: 54
Loss: 1.4476262927055359
RMSE train: 0.959216	val: 3.782473	test: 3.533972
MAE train: 0.744513	val: 2.769609	test: 2.829118

Epoch: 55
Loss: 1.2895956635475159
RMSE train: 0.931063	val: 3.757613	test: 3.431178
MAE train: 0.724052	val: 2.738600	test: 2.726021

Epoch: 56
Loss: 1.408810317516327
RMSE train: 0.839792	val: 3.731870	test: 3.295779
MAE train: 0.647828	val: 2.842858	test: 2.692810

Epoch: 57
Loss: 1.355865716934204
RMSE train: 0.771290	val: 3.784032	test: 3.331318
MAE train: 0.595925	val: 2.939153	test: 2.786539

Epoch: 58
Loss: 1.2948063611984253
RMSE train: 0.727745	val: 3.840789	test: 3.383030
MAE train: 0.564471	val: 3.044108	test: 2.866263

Epoch: 59
Loss: 1.2596744894981384
RMSE train: 0.723720	val: 3.801565	test: 3.494544
MAE train: 0.566242	val: 2.909522	test: 2.913932

Epoch: 60
Loss: 1.1022679805755615
RMSE train: 0.765222	val: 3.766096	test: 3.607071
MAE train: 0.601716	val: 2.734206	test: 2.899225

Epoch: 61
Loss: 1.1840015053749084
RMSE train: 0.815517	val: 3.782046	test: 3.591557
MAE train: 0.643506	val: 2.643896	test: 2.837006

Epoch: 62
Loss: 1.0263270437717438
RMSE train: 0.776785	val: 3.703688	test: 3.462158
MAE train: 0.612991	val: 2.606118	test: 2.760363

Epoch: 63
Loss: 1.0684167444705963
RMSE train: 0.724784	val: 3.658658	test: 3.353312
MAE train: 0.569859	val: 2.610122	test: 2.733721

Epoch: 64
Loss: 1.1530197262763977
RMSE train: 0.703717	val: 3.635685	test: 3.369026
MAE train: 0.555703	val: 2.612415	test: 2.768551

Epoch: 65
Loss: 1.1994069814682007
RMSE train: 0.665192	val: 3.687702	test: 3.523773
MAE train: 0.522057	val: 2.713209	test: 2.924634

Epoch: 66
Loss: 1.0289339423179626
RMSE train: 0.700140	val: 3.833974	test: 3.723901
MAE train: 0.548861	val: 2.794246	test: 3.054082

Epoch: 67
Loss: 1.0262081921100616
RMSE train: 0.725838	val: 3.910947	test: 3.778943
MAE train: 0.575412	val: 2.831801	test: 3.070366

Epoch: 68
Loss: 1.16071617603302
RMSE train: 0.724263	val: 3.955654	test: 3.705396
MAE train: 0.566039	val: 2.862421	test: 3.003058

Epoch: 69
Loss: 1.0589081048965454
RMSE train: 0.686628	val: 3.954341	test: 3.519970
MAE train: 0.526357	val: 2.859521	test: 2.873026

Epoch: 70
Loss: 0.9860497117042542
RMSE train: 0.643636	val: 3.945775	test: 3.445838
MAE train: 0.499225	val: 2.867302	test: 2.848062

Epoch: 71
Loss: 0.9899056255817413
RMSE train: 0.635790	val: 3.918711	test: 3.463124
MAE train: 0.502538	val: 2.841794	test: 2.866175

Epoch: 72
Loss: 0.9694518148899078
RMSE train: 0.614816	val: 3.813095	test: 3.509555
MAE train: 0.485135	val: 2.733849	test: 2.887658

Epoch: 73
Loss: 0.9321995079517365
RMSE train: 0.604730	val: 3.773762	test: 3.501385
MAE train: 0.473351	val: 2.685875	test: 2.865186

Epoch: 74
Loss: 0.9473798871040344
RMSE train: 0.608405	val: 3.751711	test: 3.498165
MAE train: 0.470329	val: 2.683369	test: 2.859249

Epoch: 75
Loss: 1.034249722957611
RMSE train: 0.652188	val: 3.799940	test: 3.449685
MAE train: 0.496635	val: 2.736834	test: 2.828073

Epoch: 76
Loss: 0.9832378327846527
RMSE train: 0.668802	val: 3.846583	test: 3.431185
MAE train: 0.505487	val: 2.754012	test: 2.821295

Epoch: 77
Loss: 1.025135338306427
RMSE train: 0.663788	val: 3.938217	test: 3.469733
MAE train: 0.501957	val: 2.846841	test: 2.870019

Epoch: 78
Loss: 0.8788603246212006
RMSE train: 0.660396	val: 3.959147	test: 3.474356
MAE train: 0.505342	val: 2.840509	test: 2.843946

Epoch: 79
Loss: 0.8603426218032837
RMSE train: 0.676147	val: 3.995388	test: 3.635719
MAE train: 0.533181	val: 2.849830	test: 2.891593

Epoch: 80
Loss: 0.9123583436012268
RMSE train: 0.683363	val: 4.012233	test: 3.771144
MAE train: 0.539893	val: 2.861067	test: 2.969081

Epoch: 81
Loss: 0.9482204616069794
RMSE train: 0.649383	val: 3.972721	test: 3.696247
MAE train: 0.513938	val: 2.854975	test: 2.923224

Epoch: 82
Loss: 1.0520058870315552
RMSE train: 0.581602	val: 3.895002	test: 3.494494
MAE train: 0.456980	val: 2.835511	test: 2.787030

Epoch: 83
Loss: 1.0971562266349792
RMSE train: 2.310494	val: 3.741495	test: 3.823509
MAE train: 1.982645	val: 3.161575	test: 3.263831

Epoch: 23
Loss: 6.121273517608643
RMSE train: 2.259920	val: 3.741733	test: 3.811583
MAE train: 1.926433	val: 3.164465	test: 3.259541

Epoch: 24
Loss: 5.655207633972168
RMSE train: 2.239135	val: 3.827988	test: 3.846144
MAE train: 1.902014	val: 3.260240	test: 3.303673

Epoch: 25
Loss: 5.4898107051849365
RMSE train: 2.207088	val: 3.912595	test: 3.844769
MAE train: 1.872962	val: 3.351358	test: 3.313159

Epoch: 26
Loss: 5.020532846450806
RMSE train: 2.190947	val: 4.008219	test: 3.808386
MAE train: 1.870334	val: 3.397705	test: 3.273602

Epoch: 27
Loss: 4.886336803436279
RMSE train: 2.186731	val: 4.146171	test: 3.813975
MAE train: 1.878574	val: 3.416613	test: 3.265065

Epoch: 28
Loss: 4.617389678955078
RMSE train: 2.197896	val: 4.277072	test: 3.851351
MAE train: 1.897767	val: 3.454691	test: 3.297897

Epoch: 29
Loss: 4.3684386014938354
RMSE train: 2.200990	val: 4.311357	test: 3.873053
MAE train: 1.905249	val: 3.427652	test: 3.318307

Epoch: 30
Loss: 3.848999261856079
RMSE train: 2.143583	val: 4.219196	test: 3.839813
MAE train: 1.864802	val: 3.329020	test: 3.291325

Epoch: 31
Loss: 3.9153025150299072
RMSE train: 2.095570	val: 4.142737	test: 3.831288
MAE train: 1.831414	val: 3.251322	test: 3.275001

Epoch: 32
Loss: 3.3414844274520874
RMSE train: 2.056801	val: 4.174929	test: 3.923035
MAE train: 1.801231	val: 3.298811	test: 3.382963

Epoch: 33
Loss: 3.217481851577759
RMSE train: 2.009794	val: 4.187941	test: 3.950340
MAE train: 1.753389	val: 3.358321	test: 3.418320

Epoch: 34
Loss: 3.0302515029907227
RMSE train: 1.959849	val: 4.201254	test: 3.966098
MAE train: 1.696785	val: 3.335696	test: 3.417041

Epoch: 35
Loss: 2.8916702270507812
RMSE train: 1.900089	val: 4.119738	test: 3.913645
MAE train: 1.640424	val: 3.261678	test: 3.356027

Epoch: 36
Loss: 2.5892491340637207
RMSE train: 1.825264	val: 3.996590	test: 3.751092
MAE train: 1.570269	val: 3.141560	test: 3.190730

Epoch: 37
Loss: 2.3790372610092163
RMSE train: 1.736962	val: 3.897045	test: 3.559424
MAE train: 1.469256	val: 3.018653	test: 2.992633

Epoch: 38
Loss: 2.493667721748352
RMSE train: 1.629105	val: 3.736179	test: 3.325160
MAE train: 1.362038	val: 2.830881	test: 2.760473

Epoch: 39
Loss: 2.2734262943267822
RMSE train: 1.507304	val: 3.579712	test: 3.091955
MAE train: 1.252276	val: 2.665271	test: 2.543573

Epoch: 40
Loss: 2.0425806045532227
RMSE train: 1.389342	val: 3.449927	test: 2.917476
MAE train: 1.158963	val: 2.585085	test: 2.393530

Epoch: 41
Loss: 1.8867195844650269
RMSE train: 1.293065	val: 3.388291	test: 2.849970
MAE train: 1.082996	val: 2.565028	test: 2.354539

Epoch: 42
Loss: 1.9071003794670105
RMSE train: 1.238436	val: 3.389969	test: 2.877173
MAE train: 1.032871	val: 2.532410	test: 2.396861

Epoch: 43
Loss: 1.655720055103302
RMSE train: 1.246213	val: 3.452819	test: 2.950917
MAE train: 1.035912	val: 2.550522	test: 2.472121

Epoch: 44
Loss: 1.6421473026275635
RMSE train: 1.201014	val: 3.419528	test: 2.897782
MAE train: 0.981977	val: 2.496758	test: 2.424070

Epoch: 45
Loss: 1.3801074028015137
RMSE train: 1.170500	val: 3.404145	test: 2.801156
MAE train: 0.944889	val: 2.509604	test: 2.318485

Epoch: 46
Loss: 1.4338906407356262
RMSE train: 1.146798	val: 3.425646	test: 2.797504
MAE train: 0.921320	val: 2.549327	test: 2.293772

Epoch: 47
Loss: 1.2617419362068176
RMSE train: 1.103297	val: 3.385944	test: 2.815043
MAE train: 0.883199	val: 2.484116	test: 2.308188

Epoch: 48
Loss: 1.363583505153656
RMSE train: 1.047957	val: 3.325815	test: 2.838397
MAE train: 0.831639	val: 2.366358	test: 2.320635

Epoch: 49
Loss: 1.1460533142089844
RMSE train: 1.008060	val: 3.306076	test: 2.847215
MAE train: 0.806523	val: 2.414152	test: 2.332091

Epoch: 50
Loss: 1.1730387806892395
RMSE train: 0.962134	val: 3.310537	test: 2.806539
MAE train: 0.771199	val: 2.472382	test: 2.268449

Epoch: 51
Loss: 1.100218951702118
RMSE train: 0.950255	val: 3.335326	test: 2.748896
MAE train: 0.757950	val: 2.456947	test: 2.195952

Epoch: 52
Loss: 1.0413366854190826
RMSE train: 0.974154	val: 3.402835	test: 2.723683
MAE train: 0.769578	val: 2.454414	test: 2.147088

Epoch: 53
Loss: 1.198202908039093
RMSE train: 0.944791	val: 3.406598	test: 2.664637
MAE train: 0.738489	val: 2.402496	test: 2.056591

Epoch: 54
Loss: 1.0730484127998352
RMSE train: 0.897904	val: 3.454372	test: 2.597430
MAE train: 0.689681	val: 2.416434	test: 1.956679

Epoch: 55
Loss: 0.9900035262107849
RMSE train: 0.805922	val: 3.456535	test: 2.494039
MAE train: 0.616568	val: 2.427484	test: 1.828709

Epoch: 56
Loss: 1.1388264894485474
RMSE train: 0.736408	val: 3.472461	test: 2.411282
MAE train: 0.566821	val: 2.486402	test: 1.775477

Epoch: 57
Loss: 0.9569447040557861
RMSE train: 0.701697	val: 3.441996	test: 2.440354
MAE train: 0.540272	val: 2.448913	test: 1.789935

Epoch: 58
Loss: 1.0200583636760712
RMSE train: 0.718275	val: 3.389798	test: 2.502851
MAE train: 0.555892	val: 2.417735	test: 1.848423

Epoch: 59
Loss: 0.8527963757514954
RMSE train: 0.775697	val: 3.403117	test: 2.652770
MAE train: 0.590322	val: 2.369050	test: 1.986001

Epoch: 60
Loss: 0.9056712985038757
RMSE train: 0.735880	val: 3.313388	test: 2.699310
MAE train: 0.566827	val: 2.320839	test: 2.042686

Epoch: 61
Loss: 0.8189837634563446
RMSE train: 0.689936	val: 3.272899	test: 2.697405
MAE train: 0.536657	val: 2.317248	test: 2.051472

Epoch: 62
Loss: 0.9382880926132202
RMSE train: 0.668995	val: 3.233185	test: 2.684789
MAE train: 0.522997	val: 2.286650	test: 2.050594

Epoch: 63
Loss: 0.8047660887241364
RMSE train: 0.666840	val: 3.242296	test: 2.643131
MAE train: 0.518132	val: 2.266916	test: 2.017111

Epoch: 64
Loss: 0.8015242516994476
RMSE train: 0.626692	val: 3.256833	test: 2.589561
MAE train: 0.490127	val: 2.292220	test: 1.963826

Epoch: 65
Loss: 0.8768750727176666
RMSE train: 0.612351	val: 3.321110	test: 2.630606
MAE train: 0.485034	val: 2.322333	test: 2.023053

Epoch: 66
Loss: 0.8597223460674286
RMSE train: 0.639806	val: 3.476049	test: 2.712572
MAE train: 0.509346	val: 2.421755	test: 2.092058

Epoch: 67
Loss: 0.9261262714862823
RMSE train: 0.560166	val: 3.393682	test: 2.643747
MAE train: 0.441773	val: 2.301363	test: 1.995090

Epoch: 68
Loss: 0.807837724685669
RMSE train: 0.573153	val: 3.390858	test: 2.598481
MAE train: 0.442719	val: 2.315125	test: 1.939038

Epoch: 69
Loss: 0.8161750137805939
RMSE train: 0.580548	val: 3.351275	test: 2.521749
MAE train: 0.450109	val: 2.343326	test: 1.881446

Epoch: 70
Loss: 0.6965123414993286
RMSE train: 0.597097	val: 3.303398	test: 2.502763
MAE train: 0.470865	val: 2.317889	test: 1.900990

Epoch: 71
Loss: 0.7532558143138885
RMSE train: 0.587170	val: 3.268173	test: 2.545955
MAE train: 0.464247	val: 2.274755	test: 1.935264

Epoch: 72
Loss: 0.7827999293804169
RMSE train: 0.555832	val: 3.273816	test: 2.612806
MAE train: 0.429641	val: 2.187188	test: 1.947104

Epoch: 73
Loss: 0.7671080529689789
RMSE train: 0.563857	val: 3.328748	test: 2.674421
MAE train: 0.427504	val: 2.171734	test: 1.984724

Epoch: 74
Loss: 0.8117318451404572
RMSE train: 0.562376	val: 3.340997	test: 2.674996
MAE train: 0.424558	val: 2.183327	test: 1.978012

Epoch: 75
Loss: 0.7688300907611847
RMSE train: 0.594356	val: 3.396629	test: 2.643822
MAE train: 0.446838	val: 2.251946	test: 1.941595

Epoch: 76
Loss: 0.7063784301280975
RMSE train: 0.597668	val: 3.457339	test: 2.641449
MAE train: 0.447787	val: 2.302845	test: 1.930174

Epoch: 77
Loss: 0.6985194981098175
RMSE train: 0.582203	val: 3.471621	test: 2.608677
MAE train: 0.438352	val: 2.341637	test: 1.899650

Epoch: 78
Loss: 0.6968587934970856
RMSE train: 0.594537	val: 3.533566	test: 2.570199
MAE train: 0.451293	val: 2.432271	test: 1.852480

Epoch: 79
Loss: 0.692604124546051
RMSE train: 0.598060	val: 3.576122	test: 2.580611
MAE train: 0.456469	val: 2.487760	test: 1.859383

Epoch: 80
Loss: 0.756585419178009
RMSE train: 0.610118	val: 3.586764	test: 2.613970
MAE train: 0.467189	val: 2.502070	test: 1.897338

Epoch: 81
Loss: 0.6761308312416077
RMSE train: 0.579660	val: 3.532536	test: 2.589325
MAE train: 0.449316	val: 2.502226	test: 1.899627

Epoch: 82
Loss: 0.9147603213787079
RMSE train: 0.534565	val: 3.477576	test: 2.572775
MAE train: 0.419592	val: 2.502383	test: 1.904721

Epoch: 83
Loss: 0.8847092986106873
RMSE train: 2.366379	val: 4.688655	test: 3.994622
MAE train: 1.914495	val: 3.021123	test: 3.112549

Epoch: 23
Loss: 6.543548107147217
RMSE train: 2.306200	val: 4.685140	test: 4.001530
MAE train: 1.867015	val: 3.004485	test: 3.108732

Epoch: 24
Loss: 6.165433883666992
RMSE train: 2.261408	val: 4.680697	test: 4.057163
MAE train: 1.832498	val: 3.010804	test: 3.164844

Epoch: 25
Loss: 5.903443336486816
RMSE train: 2.210377	val: 4.666823	test: 4.096260
MAE train: 1.788643	val: 3.052281	test: 3.254363

Epoch: 26
Loss: 5.679888725280762
RMSE train: 2.195441	val: 4.661640	test: 4.125643
MAE train: 1.783016	val: 3.099970	test: 3.316634

Epoch: 27
Loss: 5.061870098114014
RMSE train: 2.183679	val: 4.603317	test: 4.128589
MAE train: 1.784333	val: 3.084226	test: 3.345277

Epoch: 28
Loss: 4.887294769287109
RMSE train: 2.140468	val: 4.507449	test: 4.039428
MAE train: 1.751153	val: 2.952404	test: 3.228373

Epoch: 29
Loss: 4.571619749069214
RMSE train: 2.110026	val: 4.462222	test: 3.959140
MAE train: 1.729543	val: 2.826647	test: 3.075339

Epoch: 30
Loss: 4.420055508613586
RMSE train: 2.069700	val: 4.500492	test: 3.964303
MAE train: 1.692574	val: 2.819428	test: 3.074725

Epoch: 31
Loss: 4.354011297225952
RMSE train: 2.042748	val: 4.532313	test: 4.043478
MAE train: 1.673705	val: 2.861272	test: 3.171317

Epoch: 32
Loss: 3.840137481689453
RMSE train: 2.007404	val: 4.559237	test: 4.026120
MAE train: 1.648267	val: 2.900075	test: 3.196940

Epoch: 33
Loss: 3.501936912536621
RMSE train: 1.987578	val: 4.591536	test: 4.052575
MAE train: 1.634377	val: 2.951027	test: 3.244576

Epoch: 34
Loss: 3.2504730224609375
RMSE train: 1.908130	val: 4.492829	test: 3.979247
MAE train: 1.570436	val: 2.873567	test: 3.138955

Epoch: 35
Loss: 3.0817832946777344
RMSE train: 1.823377	val: 4.360380	test: 3.863631
MAE train: 1.508873	val: 2.783502	test: 2.989256

Epoch: 36
Loss: 2.8397923707962036
RMSE train: 1.779499	val: 4.279232	test: 3.818920
MAE train: 1.482402	val: 2.758127	test: 2.939497

Epoch: 37
Loss: 2.7732654809951782
RMSE train: 1.725107	val: 4.261767	test: 3.806781
MAE train: 1.435504	val: 2.776555	test: 2.917548

Epoch: 38
Loss: 2.6936362981796265
RMSE train: 1.621603	val: 4.213922	test: 3.746402
MAE train: 1.335293	val: 2.741014	test: 2.829695

Epoch: 39
Loss: 2.3681036233901978
RMSE train: 1.534707	val: 4.215851	test: 3.688231
MAE train: 1.250107	val: 2.739748	test: 2.769665

Epoch: 40
Loss: 2.150652766227722
RMSE train: 1.463260	val: 4.227776	test: 3.614765
MAE train: 1.178012	val: 2.761480	test: 2.714924

Epoch: 41
Loss: 2.1268826723098755
RMSE train: 1.403078	val: 4.245267	test: 3.554140
MAE train: 1.123122	val: 2.799690	test: 2.662855

Epoch: 42
Loss: 1.9309996962547302
RMSE train: 1.367838	val: 4.275361	test: 3.544068
MAE train: 1.099116	val: 2.863650	test: 2.662984

Epoch: 43
Loss: 1.8488408327102661
RMSE train: 1.327808	val: 4.233555	test: 3.547149
MAE train: 1.065148	val: 2.845184	test: 2.679810

Epoch: 44
Loss: 1.7894969582557678
RMSE train: 1.293898	val: 4.140529	test: 3.568792
MAE train: 1.033197	val: 2.711475	test: 2.687303

Epoch: 45
Loss: 1.6965665221214294
RMSE train: 1.272338	val: 4.066437	test: 3.592398
MAE train: 1.014629	val: 2.662517	test: 2.701126

Epoch: 46
Loss: 1.7865203022956848
RMSE train: 1.169989	val: 4.071188	test: 3.442025
MAE train: 0.918973	val: 2.731463	test: 2.561197

Epoch: 47
Loss: 1.475418210029602
RMSE train: 1.094744	val: 4.049927	test: 3.269080
MAE train: 0.852157	val: 2.750159	test: 2.445509

Epoch: 48
Loss: 1.6055512428283691
RMSE train: 1.034487	val: 3.968868	test: 3.114383
MAE train: 0.802755	val: 2.732120	test: 2.374399

Epoch: 49
Loss: 1.3640309572219849
RMSE train: 0.980064	val: 3.903544	test: 3.050400
MAE train: 0.765374	val: 2.727173	test: 2.374470

Epoch: 50
Loss: 1.4978901743888855
RMSE train: 0.960626	val: 3.957667	test: 3.107162
MAE train: 0.760818	val: 2.853492	test: 2.432626

Epoch: 51
Loss: 1.5635209679603577
RMSE train: 0.968822	val: 4.119390	test: 3.241287
MAE train: 0.765586	val: 3.021827	test: 2.526544

Epoch: 52
Loss: 1.415290892124176
RMSE train: 1.002033	val: 4.249510	test: 3.330362
MAE train: 0.785205	val: 3.147992	test: 2.562683

Epoch: 53
Loss: 1.3422396779060364
RMSE train: 1.026688	val: 4.299555	test: 3.334248
MAE train: 0.798752	val: 3.147793	test: 2.529090

Epoch: 54
Loss: 1.3876036405563354
RMSE train: 1.042106	val: 4.333530	test: 3.261601
MAE train: 0.816034	val: 3.168676	test: 2.470090

Epoch: 55
Loss: 1.4417866468429565
RMSE train: 1.057160	val: 4.396301	test: 3.193162
MAE train: 0.826658	val: 3.251391	test: 2.462013

Epoch: 56
Loss: 1.2081831097602844
RMSE train: 1.014601	val: 4.393970	test: 3.159299
MAE train: 0.786859	val: 3.287882	test: 2.469666

Epoch: 57
Loss: 1.3403180241584778
RMSE train: 0.984173	val: 4.318598	test: 3.177916
MAE train: 0.761763	val: 3.159736	test: 2.479272

Epoch: 58
Loss: 1.2074224948883057
RMSE train: 0.939040	val: 4.211203	test: 3.235950
MAE train: 0.729162	val: 3.035380	test: 2.490171

Epoch: 59
Loss: 1.413015365600586
RMSE train: 0.890802	val: 4.151931	test: 3.260692
MAE train: 0.688235	val: 3.008226	test: 2.506055

Epoch: 60
Loss: 1.1888507008552551
RMSE train: 0.842865	val: 4.105226	test: 3.239440
MAE train: 0.652024	val: 3.022937	test: 2.504835

Epoch: 61
Loss: 1.1962127685546875
RMSE train: 0.830936	val: 4.100581	test: 3.218547
MAE train: 0.645549	val: 3.008201	test: 2.493964

Epoch: 62
Loss: 1.1700215935707092
RMSE train: 0.831299	val: 4.149391	test: 3.215987
MAE train: 0.645770	val: 3.022867	test: 2.503201

Epoch: 63
Loss: 1.159125566482544
RMSE train: 0.887952	val: 4.243330	test: 3.274748
MAE train: 0.693474	val: 3.036874	test: 2.525950

Epoch: 64
Loss: 0.9724750220775604
RMSE train: 0.936410	val: 4.241758	test: 3.321385
MAE train: 0.740957	val: 2.971205	test: 2.520374

Epoch: 65
Loss: 1.0680875182151794
RMSE train: 0.932793	val: 4.214910	test: 3.332348
MAE train: 0.750078	val: 2.941078	test: 2.534354

Epoch: 66
Loss: 0.9937724173069
RMSE train: 0.860799	val: 4.206466	test: 3.349075
MAE train: 0.686516	val: 2.957314	test: 2.563713

Epoch: 67
Loss: 1.0560981035232544
RMSE train: 0.773640	val: 4.184188	test: 3.289942
MAE train: 0.612765	val: 2.975169	test: 2.564847

Epoch: 68
Loss: 1.0468068420886993
RMSE train: 0.706482	val: 4.158791	test: 3.229386
MAE train: 0.558152	val: 2.998776	test: 2.574421

Epoch: 69
Loss: 0.9924877882003784
RMSE train: 0.688750	val: 4.187653	test: 3.218889
MAE train: 0.543057	val: 3.079704	test: 2.614995

Epoch: 70
Loss: 1.0316171944141388
RMSE train: 0.687917	val: 4.263154	test: 3.238404
MAE train: 0.543485	val: 3.236440	test: 2.654734

Epoch: 71
Loss: 0.9928084909915924
RMSE train: 0.696252	val: 4.242275	test: 3.219286
MAE train: 0.548868	val: 3.197624	test: 2.603887

Epoch: 72
Loss: 0.9950629770755768
RMSE train: 0.723004	val: 4.173370	test: 3.191708
MAE train: 0.575667	val: 3.051853	test: 2.521406

Epoch: 73
Loss: 0.9278554916381836
RMSE train: 0.748198	val: 4.139984	test: 3.216913
MAE train: 0.594971	val: 2.991535	test: 2.513885

Epoch: 74
Loss: 0.8976158797740936
RMSE train: 0.771688	val: 4.145968	test: 3.256857
MAE train: 0.613709	val: 2.961372	test: 2.524387

Epoch: 75
Loss: 1.0025556981563568
RMSE train: 0.765441	val: 4.161540	test: 3.254234
MAE train: 0.602908	val: 2.985068	test: 2.540871

Epoch: 76
Loss: 1.0104453563690186
RMSE train: 0.748713	val: 4.146825	test: 3.227396
MAE train: 0.585062	val: 2.959037	test: 2.507536

Epoch: 77
Loss: 1.0311786830425262
RMSE train: 0.731672	val: 4.129350	test: 3.192291
MAE train: 0.575804	val: 2.927123	test: 2.471234

Epoch: 78
Loss: 0.8741999864578247
RMSE train: 0.745132	val: 4.200280	test: 3.222818
MAE train: 0.588445	val: 2.966005	test: 2.469685

Epoch: 79
Loss: 0.9545539617538452
RMSE train: 0.777949	val: 4.290197	test: 3.291851
MAE train: 0.613747	val: 3.027603	test: 2.479890

Epoch: 80
Loss: 0.9213405549526215
RMSE train: 0.770612	val: 4.363696	test: 3.328832
MAE train: 0.602902	val: 3.098269	test: 2.505054

Epoch: 81
Loss: 0.9556025862693787
RMSE train: 0.747819	val: 4.358480	test: 3.332133
MAE train: 0.588844	val: 3.103249	test: 2.517896

Epoch: 82
Loss: 0.8360188007354736
RMSE train: 0.719961	val: 4.356428	test: 3.340881
MAE train: 0.562665	val: 3.128043	test: 2.547957

Epoch: 83
Loss: 0.8650912642478943

Epoch: 84
Loss: 0.6579556465148926
RMSE train: 0.633096	val: 3.234401	test: 2.187647
MAE train: 0.480979	val: 2.022394	test: 1.556985

Epoch: 85
Loss: 0.6437069177627563
RMSE train: 0.614479	val: 3.215699	test: 2.180294
MAE train: 0.470767	val: 2.045332	test: 1.550746

Epoch: 86
Loss: 0.5647064745426178
RMSE train: 0.625213	val: 3.179204	test: 2.145643
MAE train: 0.484385	val: 2.047791	test: 1.530723

Epoch: 87
Loss: 0.5704708397388458
RMSE train: 0.656989	val: 3.114958	test: 2.100674
MAE train: 0.507750	val: 1.998417	test: 1.497565

Epoch: 88
Loss: 0.5545767992734909
RMSE train: 0.673058	val: 3.116532	test: 2.111610
MAE train: 0.515009	val: 1.982137	test: 1.479629

Epoch: 89
Loss: 0.6381385624408722
RMSE train: 0.652881	val: 3.154455	test: 2.143322
MAE train: 0.501590	val: 2.012476	test: 1.483475

Epoch: 90
Loss: 0.6950785219669342
RMSE train: 0.611092	val: 3.185863	test: 2.196812
MAE train: 0.478535	val: 2.064411	test: 1.545644

Epoch: 91
Loss: 0.5707042515277863
RMSE train: 0.610667	val: 3.264671	test: 2.266220
MAE train: 0.489102	val: 2.143063	test: 1.616108

Epoch: 92
Loss: 0.5914973616600037
RMSE train: 0.639981	val: 3.360011	test: 2.353013
MAE train: 0.507179	val: 2.195070	test: 1.686072

Epoch: 93
Loss: 0.5094835460186005
RMSE train: 0.643571	val: 3.349135	test: 2.357325
MAE train: 0.504367	val: 2.161847	test: 1.675864

Epoch: 94
Loss: 0.5275103747844696
RMSE train: 0.620169	val: 3.295090	test: 2.318020
MAE train: 0.475473	val: 2.103332	test: 1.630575

Epoch: 95
Loss: 0.533463716506958
RMSE train: 0.622589	val: 3.281016	test: 2.273993
MAE train: 0.470332	val: 2.112579	test: 1.595086

Epoch: 96
Loss: 0.5859559178352356
RMSE train: 0.605826	val: 3.218292	test: 2.236154
MAE train: 0.448642	val: 2.105114	test: 1.588744

Epoch: 97
Loss: 0.5698902308940887
RMSE train: 0.601812	val: 3.256674	test: 2.221200
MAE train: 0.457983	val: 2.165159	test: 1.610369

Epoch: 98
Loss: 0.5779832899570465
RMSE train: 0.602948	val: 3.237524	test: 2.198212
MAE train: 0.470295	val: 2.149891	test: 1.597544

Epoch: 99
Loss: 0.5388777256011963
RMSE train: 0.614311	val: 3.278598	test: 2.199942
MAE train: 0.473316	val: 2.148493	test: 1.586353

Epoch: 100
Loss: 0.5654288679361343
RMSE train: 0.647620	val: 3.377153	test: 2.276619
MAE train: 0.492674	val: 2.174008	test: 1.635375

Epoch: 101
Loss: 0.5298436880111694
RMSE train: 0.631047	val: 3.361808	test: 2.314514
MAE train: 0.481162	val: 2.173622	test: 1.642010

Epoch: 102
Loss: 0.6059041619300842
RMSE train: 0.617565	val: 3.340360	test: 2.319949
MAE train: 0.471351	val: 2.188588	test: 1.645443

Epoch: 103
Loss: 0.6992416977882385
RMSE train: 0.596260	val: 3.337899	test: 2.275973
MAE train: 0.464434	val: 2.188220	test: 1.629137

Epoch: 104
Loss: 0.5579952895641327
RMSE train: 0.595379	val: 3.220843	test: 2.208814
MAE train: 0.476386	val: 2.083994	test: 1.570259

Epoch: 105
Loss: 0.5478851497173309
RMSE train: 0.584033	val: 3.229777	test: 2.202808
MAE train: 0.457949	val: 2.044973	test: 1.547244

Epoch: 106
Loss: 0.5867792069911957
RMSE train: 0.564798	val: 3.295745	test: 2.190580
MAE train: 0.434511	val: 2.058941	test: 1.531894

Epoch: 107
Loss: 0.47219741344451904
RMSE train: 0.566082	val: 3.409894	test: 2.208012
MAE train: 0.425447	val: 2.114062	test: 1.558367

Epoch: 108
Loss: 0.5813949704170227
RMSE train: 0.576935	val: 3.467608	test: 2.234417
MAE train: 0.426796	val: 2.159090	test: 1.577772

Epoch: 109
Loss: 0.6077263951301575
RMSE train: 0.592990	val: 3.466475	test: 2.261656
MAE train: 0.436423	val: 2.182337	test: 1.583381

Epoch: 110
Loss: 0.5748193562030792
RMSE train: 0.574947	val: 3.367545	test: 2.189674
MAE train: 0.421531	val: 2.133425	test: 1.528844

Epoch: 111
Loss: 0.47111140191555023
RMSE train: 0.539826	val: 3.293006	test: 2.117137
MAE train: 0.400963	val: 2.111469	test: 1.492018

Epoch: 112
Loss: 0.5768416821956635
RMSE train: 0.479478	val: 3.185370	test: 2.102142
MAE train: 0.357987	val: 2.067001	test: 1.491687

Epoch: 113
Loss: 0.5605385899543762
RMSE train: 0.448220	val: 3.230923	test: 2.159724
MAE train: 0.330472	val: 2.091606	test: 1.533487

Epoch: 114
Loss: 0.5063398480415344
RMSE train: 0.454492	val: 3.304021	test: 2.192160
MAE train: 0.336411	val: 2.116890	test: 1.546784

Epoch: 115
Loss: 0.511362761259079
RMSE train: 0.495000	val: 3.276985	test: 2.172659
MAE train: 0.375836	val: 2.076114	test: 1.519666

Epoch: 116
Loss: 0.4679027497768402
RMSE train: 0.530910	val: 3.240685	test: 2.179411
MAE train: 0.411172	val: 2.044417	test: 1.532545

Epoch: 117
Loss: 0.4634348452091217
RMSE train: 0.551956	val: 3.236090	test: 2.176659
MAE train: 0.423347	val: 2.052202	test: 1.558183

Epoch: 118
Loss: 0.46954934298992157
RMSE train: 0.553370	val: 3.179155	test: 2.130279
MAE train: 0.423655	val: 2.033223	test: 1.526165

Epoch: 119
Loss: 0.45979034900665283
RMSE train: 0.548785	val: 3.138040	test: 2.067273
MAE train: 0.421932	val: 2.012006	test: 1.483684

Epoch: 120
Loss: 0.5235554575920105
RMSE train: 0.559556	val: 3.121044	test: 2.069883
MAE train: 0.432894	val: 1.996273	test: 1.483152

Epoch: 121
Loss: 0.5242021232843399
RMSE train: 0.583547	val: 3.206002	test: 2.104473
MAE train: 0.453603	val: 2.011639	test: 1.490934

Early stopping
Best (RMSE):	 train: 0.572688	val: 2.483017	test: 1.925617
Best (MAE):	 train: 0.440998	val: 1.723095	test: 1.407031


Epoch: 84
Loss: 0.6118728816509247
RMSE train: 0.708840	val: 3.367494	test: 2.225835
MAE train: 0.580316	val: 2.189732	test: 1.543120

Epoch: 85
Loss: 0.5913645327091217
RMSE train: 0.743158	val: 3.494590	test: 2.288517
MAE train: 0.611028	val: 2.320439	test: 1.595621

Epoch: 86
Loss: 0.6747312247753143
RMSE train: 0.782111	val: 3.575760	test: 2.355315
MAE train: 0.633401	val: 2.399688	test: 1.646685

Epoch: 87
Loss: 0.6712874174118042
RMSE train: 0.821438	val: 3.559276	test: 2.386525
MAE train: 0.669124	val: 2.379652	test: 1.687120

Epoch: 88
Loss: 0.6158124208450317
RMSE train: 0.844626	val: 3.470536	test: 2.347542
MAE train: 0.695816	val: 2.325294	test: 1.669981

Epoch: 89
Loss: 0.5706845819950104
RMSE train: 0.805971	val: 3.317469	test: 2.244281
MAE train: 0.672147	val: 2.222819	test: 1.585616

Epoch: 90
Loss: 0.7224778831005096
RMSE train: 0.708228	val: 3.236862	test: 2.183541
MAE train: 0.587188	val: 2.152644	test: 1.514384

Epoch: 91
Loss: 0.581671267747879
RMSE train: 0.606331	val: 3.218406	test: 2.175895
MAE train: 0.486147	val: 2.120088	test: 1.486012

Epoch: 92
Loss: 0.5889303684234619
RMSE train: 0.572970	val: 3.210126	test: 2.191546
MAE train: 0.440175	val: 2.095454	test: 1.486257

Epoch: 93
Loss: 0.6283602714538574
RMSE train: 0.584787	val: 3.199814	test: 2.182994
MAE train: 0.445634	val: 2.081016	test: 1.478488

Epoch: 94
Loss: 0.6169883906841278
RMSE train: 0.602858	val: 3.176682	test: 2.151587
MAE train: 0.459906	val: 2.062160	test: 1.466571

Epoch: 95
Loss: 0.5859813094139099
RMSE train: 0.598647	val: 3.163939	test: 2.147328
MAE train: 0.456075	val: 2.046836	test: 1.469082

Epoch: 96
Loss: 0.46968671679496765
RMSE train: 0.558858	val: 3.142121	test: 2.134238
MAE train: 0.438119	val: 2.072088	test: 1.454358

Epoch: 97
Loss: 0.589800238609314
RMSE train: 0.556165	val: 3.139149	test: 2.141907
MAE train: 0.439375	val: 2.082937	test: 1.455578

Epoch: 98
Loss: 0.5726297497749329
RMSE train: 0.569477	val: 3.135476	test: 2.135930
MAE train: 0.441244	val: 2.081614	test: 1.447867

Epoch: 99
Loss: 0.6039930582046509
RMSE train: 0.575068	val: 3.122925	test: 2.119976
MAE train: 0.442273	val: 2.063564	test: 1.429112

Epoch: 100
Loss: 0.5688079595565796
RMSE train: 0.573256	val: 3.171744	test: 2.099287
MAE train: 0.439243	val: 2.090553	test: 1.424566

Epoch: 101
Loss: 0.6473804116249084
RMSE train: 0.618174	val: 3.162611	test: 2.097489
MAE train: 0.473008	val: 2.062115	test: 1.437254

Epoch: 102
Loss: 0.5546946823596954
RMSE train: 0.633655	val: 3.106432	test: 2.129729
MAE train: 0.486757	val: 2.003111	test: 1.448540

Epoch: 103
Loss: 0.5466232299804688
RMSE train: 0.659822	val: 3.130650	test: 2.211894
MAE train: 0.496831	val: 1.989343	test: 1.534079

Epoch: 104
Loss: 0.5879439115524292
RMSE train: 0.606417	val: 3.130575	test: 2.181931
MAE train: 0.457625	val: 1.969954	test: 1.469511

Epoch: 105
Loss: 0.5799486637115479
RMSE train: 0.561117	val: 3.157777	test: 2.146064
MAE train: 0.431454	val: 2.013952	test: 1.421436

Epoch: 106
Loss: 0.564727783203125
RMSE train: 0.603375	val: 3.203477	test: 2.124533
MAE train: 0.466641	val: 2.056121	test: 1.418452

Epoch: 107
Loss: 0.5682957321405411
RMSE train: 0.663372	val: 3.185685	test: 2.124193
MAE train: 0.514003	val: 2.038213	test: 1.400886

Epoch: 108
Loss: 0.4860887676477432
RMSE train: 0.697979	val: 3.201810	test: 2.176492
MAE train: 0.545026	val: 2.050378	test: 1.486559

Epoch: 109
Loss: 0.5065812915563583
RMSE train: 0.702602	val: 3.244493	test: 2.242035
MAE train: 0.553938	val: 2.076011	test: 1.563810

Epoch: 110
Loss: 0.602283775806427
RMSE train: 0.679320	val: 3.296743	test: 2.246630
MAE train: 0.542429	val: 2.118310	test: 1.546702

Epoch: 111
Loss: 0.4932236522436142
RMSE train: 0.625100	val: 3.295158	test: 2.213888
MAE train: 0.488388	val: 2.100472	test: 1.479825

Epoch: 112
Loss: 0.5102569162845612
RMSE train: 0.590807	val: 3.289347	test: 2.201480
MAE train: 0.452651	val: 2.079272	test: 1.455291

Epoch: 113
Loss: 0.5639922618865967
RMSE train: 0.574870	val: 3.331023	test: 2.210319
MAE train: 0.439176	val: 2.098776	test: 1.480391

Epoch: 114
Loss: 0.5699036717414856
RMSE train: 0.581153	val: 3.319178	test: 2.206535
MAE train: 0.449921	val: 2.092453	test: 1.474777

Epoch: 115
Loss: 0.4508236199617386
RMSE train: 0.602785	val: 3.256130	test: 2.156741
MAE train: 0.482019	val: 2.105267	test: 1.445175

Epoch: 116
Loss: 0.48549260199069977
RMSE train: 0.620898	val: 3.153120	test: 2.105892
MAE train: 0.498624	val: 2.058956	test: 1.429035

Epoch: 117
Loss: 0.536587119102478
RMSE train: 0.604724	val: 3.158437	test: 2.062060
MAE train: 0.478038	val: 2.046677	test: 1.394366

Epoch: 118
Loss: 0.4702383875846863
RMSE train: 0.589803	val: 3.137901	test: 2.041727
MAE train: 0.455207	val: 1.984790	test: 1.382716

Epoch: 119
Loss: 0.4909660816192627
RMSE train: 0.585349	val: 3.145262	test: 2.093254
MAE train: 0.452222	val: 1.949426	test: 1.430209

Epoch: 120
Loss: 0.541869044303894
RMSE train: 0.590926	val: 3.220617	test: 2.176833
MAE train: 0.458431	val: 1.986373	test: 1.496535

Epoch: 121
Loss: 0.6065731644630432
RMSE train: 0.603632	val: 3.303215	test: 2.253382
MAE train: 0.474489	val: 2.063778	test: 1.559810

Early stopping
Best (RMSE):	 train: 0.677398	val: 2.853483	test: 2.042867
Best (MAE):	 train: 0.537057	val: 2.036846	test: 1.414024


Epoch: 84
Loss: 0.6880166530609131
RMSE train: 0.670619	val: 3.706808	test: 2.312461
MAE train: 0.545418	val: 2.232743	test: 1.540231

Epoch: 85
Loss: 0.611598014831543
RMSE train: 0.706183	val: 3.619625	test: 2.281859
MAE train: 0.564136	val: 2.165601	test: 1.517485

Epoch: 86
Loss: 0.5729405283927917
RMSE train: 0.724229	val: 3.601215	test: 2.292445
MAE train: 0.552326	val: 2.130692	test: 1.526339

Epoch: 87
Loss: 0.6312802731990814
RMSE train: 0.725327	val: 3.597159	test: 2.302886
MAE train: 0.548723	val: 2.111387	test: 1.540440

Epoch: 88
Loss: 0.5899095237255096
RMSE train: 0.696496	val: 3.606220	test: 2.338972
MAE train: 0.526598	val: 2.140988	test: 1.570682

Epoch: 89
Loss: 0.6100984811782837
RMSE train: 0.657853	val: 3.658645	test: 2.420516
MAE train: 0.491880	val: 2.183899	test: 1.620497

Epoch: 90
Loss: 0.5950997173786163
RMSE train: 0.581430	val: 3.617427	test: 2.402565
MAE train: 0.435789	val: 2.219035	test: 1.621220

Epoch: 91
Loss: 0.606959730386734
RMSE train: 0.533988	val: 3.592556	test: 2.402783
MAE train: 0.405718	val: 2.237271	test: 1.626711

Epoch: 92
Loss: 0.5945759117603302
RMSE train: 0.519279	val: 3.573581	test: 2.386190
MAE train: 0.402095	val: 2.209429	test: 1.624641

Epoch: 93
Loss: 0.6098490953445435
RMSE train: 0.518264	val: 3.497507	test: 2.308862
MAE train: 0.402904	val: 2.124796	test: 1.582584

Epoch: 94
Loss: 0.6993433237075806
RMSE train: 0.545228	val: 3.483713	test: 2.263237
MAE train: 0.413594	val: 2.068576	test: 1.557869

Epoch: 95
Loss: 0.6010289490222931
RMSE train: 0.558034	val: 3.440746	test: 2.212206
MAE train: 0.425195	val: 2.052440	test: 1.510237

Epoch: 96
Loss: 0.5014333426952362
RMSE train: 0.582438	val: 3.460093	test: 2.217029
MAE train: 0.449259	val: 2.087737	test: 1.489972

Epoch: 97
Loss: 0.6508062183856964
RMSE train: 0.611895	val: 3.522705	test: 2.244217
MAE train: 0.473209	val: 2.168823	test: 1.511554

Epoch: 98
Loss: 0.6520202457904816
RMSE train: 0.607087	val: 3.571460	test: 2.268737
MAE train: 0.463786	val: 2.210381	test: 1.551071

Epoch: 99
Loss: 0.54940065741539
RMSE train: 0.613928	val: 3.606484	test: 2.271914
MAE train: 0.465981	val: 2.219428	test: 1.552746

Epoch: 100
Loss: 0.6041897833347321
RMSE train: 0.599252	val: 3.589904	test: 2.272491
MAE train: 0.448396	val: 2.203366	test: 1.558980

Epoch: 101
Loss: 0.5160743445158005
RMSE train: 0.599774	val: 3.602929	test: 2.321701
MAE train: 0.448965	val: 2.211012	test: 1.592239

Epoch: 102
Loss: 0.597682535648346
RMSE train: 0.584808	val: 3.610008	test: 2.372916
MAE train: 0.443724	val: 2.239302	test: 1.640623

Epoch: 103
Loss: 0.6013546586036682
RMSE train: 0.561083	val: 3.642617	test: 2.389674
MAE train: 0.429497	val: 2.291060	test: 1.648394

Epoch: 104
Loss: 0.5787883698940277
RMSE train: 0.527298	val: 3.669270	test: 2.363776
MAE train: 0.404513	val: 2.328918	test: 1.611546

Epoch: 105
Loss: 0.5373974442481995
RMSE train: 0.517281	val: 3.689258	test: 2.288584
MAE train: 0.393407	val: 2.336674	test: 1.544964

Epoch: 106
Loss: 0.5812332630157471
RMSE train: 0.536824	val: 3.634956	test: 2.228803
MAE train: 0.409699	val: 2.279829	test: 1.499611

Epoch: 107
Loss: 0.5601768791675568
RMSE train: 0.588488	val: 3.523514	test: 2.195596
MAE train: 0.450898	val: 2.160188	test: 1.467575

Epoch: 108
Loss: 0.5636957883834839
RMSE train: 0.610965	val: 3.503073	test: 2.205680
MAE train: 0.467449	val: 2.095678	test: 1.462027

Epoch: 109
Loss: 0.4928268641233444
RMSE train: 0.601623	val: 3.523219	test: 2.219974
MAE train: 0.466823	val: 2.101036	test: 1.477443

Epoch: 110
Loss: 0.47972460091114044
RMSE train: 0.589225	val: 3.549295	test: 2.261950
MAE train: 0.460798	val: 2.111312	test: 1.514505

Epoch: 111
Loss: 0.5100833475589752
RMSE train: 0.574027	val: 3.579814	test: 2.306493
MAE train: 0.450164	val: 2.134514	test: 1.555952

Epoch: 112
Loss: 0.4548235982656479
RMSE train: 0.548360	val: 3.575411	test: 2.313578
MAE train: 0.426850	val: 2.137741	test: 1.574065

Epoch: 113
Loss: 0.5314122289419174
RMSE train: 0.558464	val: 3.582128	test: 2.283861
MAE train: 0.430411	val: 2.141621	test: 1.563713

Epoch: 114
Loss: 0.5544508397579193
RMSE train: 0.573597	val: 3.572928	test: 2.228710
MAE train: 0.441485	val: 2.145168	test: 1.507043

Epoch: 115
Loss: 0.49135465919971466
RMSE train: 0.584305	val: 3.548130	test: 2.193137
MAE train: 0.450010	val: 2.120348	test: 1.463616

Epoch: 116
Loss: 0.4870643615722656
RMSE train: 0.603997	val: 3.524974	test: 2.165254
MAE train: 0.459830	val: 2.105894	test: 1.437620

Epoch: 117
Loss: 0.5392929911613464
RMSE train: 0.604897	val: 3.472598	test: 2.128619
MAE train: 0.465046	val: 2.106093	test: 1.414957

Epoch: 118
Loss: 0.535935640335083
RMSE train: 0.565635	val: 3.416207	test: 2.088210
MAE train: 0.428577	val: 2.069932	test: 1.397869

Epoch: 119
Loss: 0.49197371304035187
RMSE train: 0.526166	val: 3.387259	test: 2.048198
MAE train: 0.396378	val: 2.065409	test: 1.381359

Epoch: 120
Loss: 0.4880876988172531
RMSE train: 0.503450	val: 3.436317	test: 2.042557
MAE train: 0.374273	val: 2.110060	test: 1.378228

Epoch: 121
Loss: 0.5276111960411072
RMSE train: 0.461729	val: 3.396687	test: 2.065242
MAE train: 0.343897	val: 2.082768	test: 1.393658

Early stopping
Best (RMSE):	 train: 0.638049	val: 3.335895	test: 2.112012
Best (MAE):	 train: 0.489126	val: 2.110409	test: 1.461728
All runs completed.

RMSE train: 0.609234	val: 2.878704	test: 2.994514
MAE train: 0.475391	val: 1.983584	test: 2.122444

Epoch: 84
Loss: 0.7877489924430847
RMSE train: 0.598615	val: 2.819063	test: 3.000867
MAE train: 0.464338	val: 1.976804	test: 2.143361

Epoch: 85
Loss: 0.7128736078739166
RMSE train: 0.615988	val: 2.824574	test: 3.023715
MAE train: 0.479319	val: 1.992281	test: 2.152893

Epoch: 86
Loss: 0.7322477698326111
RMSE train: 0.647310	val: 2.894342	test: 3.008976
MAE train: 0.510028	val: 2.068326	test: 2.171941

Epoch: 87
Loss: 0.7795857191085815
RMSE train: 0.634393	val: 2.925702	test: 3.010360
MAE train: 0.497952	val: 2.091847	test: 2.185348

Epoch: 88
Loss: 0.791222870349884
RMSE train: 0.611463	val: 2.887749	test: 3.038833
MAE train: 0.478718	val: 2.057988	test: 2.205592

Epoch: 89
Loss: 0.7757123112678528
RMSE train: 0.591567	val: 2.880831	test: 3.061203
MAE train: 0.456429	val: 2.032866	test: 2.229949

Epoch: 90
Loss: 0.7819206118583679
RMSE train: 0.598575	val: 2.959505	test: 3.085804
MAE train: 0.455110	val: 2.120974	test: 2.280085

Epoch: 91
Loss: 0.8050574362277985
RMSE train: 0.600323	val: 3.011926	test: 3.032249
MAE train: 0.463224	val: 2.168154	test: 2.216567

Epoch: 92
Loss: 0.717289537191391
RMSE train: 0.651735	val: 3.144516	test: 2.988538
MAE train: 0.507095	val: 2.265369	test: 2.177898

Epoch: 93
Loss: 0.7240459322929382
RMSE train: 0.719192	val: 3.221837	test: 2.985999
MAE train: 0.570463	val: 2.279816	test: 2.170808

Epoch: 94
Loss: 0.6629678905010223
RMSE train: 0.789627	val: 3.285213	test: 3.007775
MAE train: 0.632053	val: 2.283403	test: 2.181441

Epoch: 95
Loss: 0.6177553534507751
RMSE train: 0.828295	val: 3.293403	test: 2.992070
MAE train: 0.665614	val: 2.280991	test: 2.177752

Epoch: 96
Loss: 0.7297738194465637
RMSE train: 0.782540	val: 3.180955	test: 2.953362
MAE train: 0.627424	val: 2.215732	test: 2.170468

Epoch: 97
Loss: 0.7894785404205322
RMSE train: 0.762178	val: 3.124951	test: 2.915791
MAE train: 0.619213	val: 2.235945	test: 2.159817

Epoch: 98
Loss: 0.678191065788269
RMSE train: 0.750672	val: 3.128392	test: 2.896315
MAE train: 0.607956	val: 2.293431	test: 2.150407

Epoch: 99
Loss: 0.7605559825897217
RMSE train: 0.782811	val: 3.088180	test: 2.933090
MAE train: 0.641330	val: 2.283623	test: 2.170195

Epoch: 100
Loss: 0.6673262417316437
RMSE train: 0.848165	val: 3.159524	test: 3.015066
MAE train: 0.691221	val: 2.273644	test: 2.203524

Epoch: 101
Loss: 0.62726691365242
RMSE train: 0.872830	val: 3.250212	test: 3.056845
MAE train: 0.702393	val: 2.253719	test: 2.217053

Epoch: 102
Loss: 0.5682924091815948
RMSE train: 0.864681	val: 3.277869	test: 3.047957
MAE train: 0.679242	val: 2.221664	test: 2.194623

Epoch: 103
Loss: 0.6975510716438293
RMSE train: 0.849251	val: 3.276468	test: 3.008117
MAE train: 0.661474	val: 2.204077	test: 2.156865

Epoch: 104
Loss: 0.6096364259719849
RMSE train: 0.821567	val: 3.267366	test: 2.942276
MAE train: 0.647660	val: 2.224249	test: 2.122148

Epoch: 105
Loss: 0.5551643669605255
RMSE train: 0.809967	val: 3.285012	test: 2.920573
MAE train: 0.653851	val: 2.298108	test: 2.108797

Epoch: 106
Loss: 0.5396858751773834
RMSE train: 0.800787	val: 3.264158	test: 2.923323
MAE train: 0.649561	val: 2.334135	test: 2.112431

Epoch: 107
Loss: 0.6775206029415131
RMSE train: 0.713701	val: 3.086821	test: 2.953961
MAE train: 0.587617	val: 2.183611	test: 2.130803

Epoch: 108
Loss: 0.5810137987136841
RMSE train: 0.607906	val: 2.934760	test: 2.975140
MAE train: 0.496843	val: 2.054887	test: 2.145498

Epoch: 109
Loss: 0.6636434197425842
RMSE train: 0.550075	val: 2.890873	test: 3.017267
MAE train: 0.443583	val: 2.051009	test: 2.195595

Epoch: 110
Loss: 0.7790842652320862
RMSE train: 0.574085	val: 2.983229	test: 3.038333
MAE train: 0.463415	val: 2.075569	test: 2.181957

Epoch: 111
Loss: 0.5251734554767609
RMSE train: 0.603241	val: 3.080183	test: 3.055026
MAE train: 0.479718	val: 2.137981	test: 2.185416

Epoch: 112
Loss: 0.5421410501003265
RMSE train: 0.607477	val: 3.133254	test: 3.006989
MAE train: 0.472017	val: 2.200011	test: 2.162482

Epoch: 113
Loss: 0.5879296362400055
RMSE train: 0.604777	val: 3.134429	test: 2.973569
MAE train: 0.460769	val: 2.214384	test: 2.143857

Epoch: 114
Loss: 0.6170478165149689
RMSE train: 0.602884	val: 3.118527	test: 2.924115
MAE train: 0.466835	val: 2.239885	test: 2.103563

Epoch: 115
Loss: 0.6168715357780457
RMSE train: 0.571851	val: 3.088665	test: 2.889965
MAE train: 0.454643	val: 2.273828	test: 2.099591

Epoch: 116
Loss: 0.5563744902610779
RMSE train: 0.528642	val: 3.071267	test: 2.919661
MAE train: 0.418580	val: 2.238969	test: 2.115201

Epoch: 117
Loss: 0.5774239599704742
RMSE train: 0.488941	val: 3.013592	test: 2.982136
MAE train: 0.383040	val: 2.170082	test: 2.171566

Epoch: 118
Loss: 0.6174202263355255
RMSE train: 0.494300	val: 2.977973	test: 3.014470
MAE train: 0.387643	val: 2.125818	test: 2.192672

Epoch: 119
Loss: 0.5183364003896713
RMSE train: 0.526393	val: 2.927942	test: 3.030660
MAE train: 0.413700	val: 2.054055	test: 2.193135

Epoch: 120
Loss: 0.5342196226119995
RMSE train: 0.559833	val: 2.927023	test: 3.032080
MAE train: 0.439763	val: 2.039720	test: 2.184868

Epoch: 121
Loss: 0.5988181531429291
RMSE train: 0.595735	val: 3.008421	test: 3.043985
MAE train: 0.469566	val: 2.067409	test: 2.179800

Early stopping
Best (RMSE):	 train: 0.805172	val: 2.711020	test: 3.095785
Best (MAE):	 train: 0.652042	val: 1.952261	test: 2.202597

RMSE train: 0.899012	val: 3.569914	test: 2.930127
MAE train: 0.719797	val: 2.473807	test: 2.255003

Epoch: 84
Loss: 0.8268055617809296
RMSE train: 0.842139	val: 3.574644	test: 2.919767
MAE train: 0.676112	val: 2.452486	test: 2.261321

Epoch: 85
Loss: 0.8294771909713745
RMSE train: 0.785637	val: 3.615677	test: 2.903054
MAE train: 0.627327	val: 2.456517	test: 2.240180

Epoch: 86
Loss: 0.7856252193450928
RMSE train: 0.759174	val: 3.715855	test: 2.878490
MAE train: 0.592991	val: 2.553455	test: 2.216354

Epoch: 87
Loss: 0.738175094127655
RMSE train: 0.754731	val: 3.750480	test: 2.826646
MAE train: 0.579294	val: 2.640415	test: 2.175116

Epoch: 88
Loss: 0.804993748664856
RMSE train: 0.731669	val: 3.765186	test: 2.793474
MAE train: 0.551204	val: 2.643234	test: 2.138419

Epoch: 89
Loss: 0.7111758887767792
RMSE train: 0.714554	val: 3.797643	test: 2.774174
MAE train: 0.525749	val: 2.673166	test: 2.121509

Epoch: 90
Loss: 0.841335266828537
RMSE train: 0.674498	val: 3.776013	test: 2.739400
MAE train: 0.502819	val: 2.628283	test: 2.083785

Epoch: 91
Loss: 0.7314709424972534
RMSE train: 0.677287	val: 3.793632	test: 2.783684
MAE train: 0.512860	val: 2.620298	test: 2.122021

Epoch: 92
Loss: 0.8695299327373505
RMSE train: 0.694467	val: 3.822298	test: 2.860057
MAE train: 0.541126	val: 2.628714	test: 2.188538

Epoch: 93
Loss: 0.7633396983146667
RMSE train: 0.718992	val: 3.858336	test: 2.972333
MAE train: 0.569850	val: 2.649585	test: 2.287284

Epoch: 94
Loss: 0.8089584708213806
RMSE train: 0.703370	val: 3.836768	test: 3.033797
MAE train: 0.569660	val: 2.614495	test: 2.335329

Epoch: 95
Loss: 0.8010465502738953
RMSE train: 0.716785	val: 3.803734	test: 3.049749
MAE train: 0.578333	val: 2.569038	test: 2.357182

Epoch: 96
Loss: 0.7864295244216919
RMSE train: 0.727636	val: 3.756950	test: 3.017148
MAE train: 0.590017	val: 2.541751	test: 2.329041

Epoch: 97
Loss: 0.7430733740329742
RMSE train: 0.759799	val: 3.710785	test: 2.969674
MAE train: 0.614690	val: 2.573715	test: 2.301433

Epoch: 98
Loss: 0.7711376547813416
RMSE train: 0.780091	val: 3.714622	test: 2.916213
MAE train: 0.627436	val: 2.660241	test: 2.277919

Epoch: 99
Loss: 0.710049033164978
RMSE train: 0.818089	val: 3.816698	test: 2.944004
MAE train: 0.649182	val: 2.732528	test: 2.308359

Epoch: 100
Loss: 0.7854525744915009
RMSE train: 0.835016	val: 3.958118	test: 2.995519
MAE train: 0.649546	val: 2.763685	test: 2.318866

Epoch: 101
Loss: 0.6314916610717773
RMSE train: 0.834706	val: 4.073099	test: 3.023853
MAE train: 0.643644	val: 2.756478	test: 2.309712

Epoch: 102
Loss: 0.7082134783267975
RMSE train: 0.820195	val: 4.151483	test: 3.069912
MAE train: 0.628639	val: 2.769554	test: 2.341807

Epoch: 103
Loss: 0.7361790239810944
RMSE train: 0.798819	val: 4.132453	test: 3.117255
MAE train: 0.627764	val: 2.714328	test: 2.347939

Epoch: 104
Loss: 0.7294168770313263
RMSE train: 0.744199	val: 4.027066	test: 3.092655
MAE train: 0.598977	val: 2.615624	test: 2.310083

Epoch: 105
Loss: 0.6398234069347382
RMSE train: 0.686417	val: 3.868314	test: 2.999338
MAE train: 0.547909	val: 2.500275	test: 2.240153

Epoch: 106
Loss: 0.6769275069236755
RMSE train: 0.663826	val: 3.723903	test: 2.931023
MAE train: 0.518713	val: 2.434662	test: 2.208187

Epoch: 107
Loss: 0.5919605195522308
RMSE train: 0.654595	val: 3.623640	test: 2.886658
MAE train: 0.508339	val: 2.382393	test: 2.183807

Epoch: 108
Loss: 0.6279143989086151
RMSE train: 0.641311	val: 3.509600	test: 2.824668
MAE train: 0.504466	val: 2.293322	test: 2.123380

Epoch: 109
Loss: 0.5962820053100586
RMSE train: 0.607041	val: 3.449468	test: 2.790589
MAE train: 0.476496	val: 2.305277	test: 2.106852

Epoch: 110
Loss: 0.6710156798362732
RMSE train: 0.587831	val: 3.497043	test: 2.811197
MAE train: 0.453263	val: 2.391999	test: 2.146408

Epoch: 111
Loss: 0.6458866000175476
RMSE train: 0.578280	val: 3.562623	test: 2.813953
MAE train: 0.443905	val: 2.485366	test: 2.165357

Epoch: 112
Loss: 0.6455320417881012
RMSE train: 0.602674	val: 3.623916	test: 2.814755
MAE train: 0.464385	val: 2.518262	test: 2.169614

Epoch: 113
Loss: 0.5849268436431885
RMSE train: 0.607837	val: 3.670777	test: 2.793394
MAE train: 0.473842	val: 2.483163	test: 2.129469

Epoch: 114
Loss: 0.5924502015113831
RMSE train: 0.655178	val: 3.691249	test: 2.801273
MAE train: 0.517553	val: 2.391752	test: 2.088178

Epoch: 115
Loss: 0.5642888844013214
RMSE train: 0.655980	val: 3.639956	test: 2.793806
MAE train: 0.523000	val: 2.311394	test: 2.071963

Epoch: 116
Loss: 0.659740149974823
RMSE train: 0.613282	val: 3.599809	test: 2.770734
MAE train: 0.493548	val: 2.335094	test: 2.087493

Epoch: 117
Loss: 0.6477295160293579
RMSE train: 0.582623	val: 3.655686	test: 2.815792
MAE train: 0.463589	val: 2.518618	test: 2.163877

Epoch: 118
Loss: 0.618964821100235
RMSE train: 0.583949	val: 3.649281	test: 2.821132
MAE train: 0.461502	val: 2.477399	test: 2.182657

Epoch: 119
Loss: 0.6689130067825317
RMSE train: 0.633657	val: 3.635766	test: 2.802373
MAE train: 0.506450	val: 2.401868	test: 2.133644

Epoch: 120
Loss: 0.6489529311656952
RMSE train: 0.691903	val: 3.664881	test: 2.803530
MAE train: 0.553598	val: 2.416158	test: 2.116296

Epoch: 121
Loss: 0.5168835371732712
RMSE train: 0.714698	val: 3.671453	test: 2.789269
MAE train: 0.569618	val: 2.473373	test: 2.119932

Early stopping
Best (RMSE):	 train: 1.301140	val: 3.221451	test: 2.827319
Best (MAE):	 train: 1.057955	val: 2.403101	test: 2.282399

RMSE train: 0.681969	val: 3.465151	test: 2.943633
MAE train: 0.526059	val: 2.217089	test: 2.056540

Epoch: 84
Loss: 0.7530834972858429
RMSE train: 0.697675	val: 3.508340	test: 2.949736
MAE train: 0.537572	val: 2.265811	test: 2.059735

Epoch: 85
Loss: 0.7085271775722504
RMSE train: 0.695067	val: 3.527359	test: 2.948066
MAE train: 0.532269	val: 2.300132	test: 2.057212

Epoch: 86
Loss: 0.7610520124435425
RMSE train: 0.688413	val: 3.539574	test: 2.942901
MAE train: 0.531559	val: 2.329266	test: 2.054964

Epoch: 87
Loss: 0.6953156590461731
RMSE train: 0.665570	val: 3.516437	test: 2.961439
MAE train: 0.519066	val: 2.338783	test: 2.056229

Epoch: 88
Loss: 0.7627330124378204
RMSE train: 0.633568	val: 3.486205	test: 3.020908
MAE train: 0.500934	val: 2.326411	test: 2.098238

Epoch: 89
Loss: 0.7244084775447845
RMSE train: 0.589784	val: 3.483147	test: 3.057237
MAE train: 0.462078	val: 2.319543	test: 2.124382

Epoch: 90
Loss: 0.7333064675331116
RMSE train: 0.569816	val: 3.549598	test: 3.096163
MAE train: 0.443483	val: 2.379767	test: 2.178327

Epoch: 91
Loss: 0.7176180779933929
RMSE train: 0.557506	val: 3.650042	test: 3.122628
MAE train: 0.437358	val: 2.469570	test: 2.213613

Epoch: 92
Loss: 0.7495838403701782
RMSE train: 0.613065	val: 3.762709	test: 3.159716
MAE train: 0.481988	val: 2.534371	test: 2.246798

Epoch: 93
Loss: 0.6594375371932983
RMSE train: 0.711471	val: 3.828317	test: 3.232204
MAE train: 0.566756	val: 2.544263	test: 2.308610

Epoch: 94
Loss: 0.6346214413642883
RMSE train: 0.851039	val: 3.937899	test: 3.387158
MAE train: 0.680513	val: 2.579484	test: 2.493997

Epoch: 95
Loss: 0.5980088710784912
RMSE train: 0.887821	val: 3.942871	test: 3.413733
MAE train: 0.716186	val: 2.554632	test: 2.510931

Epoch: 96
Loss: 0.6101436913013458
RMSE train: 0.820630	val: 3.816352	test: 3.355441
MAE train: 0.661111	val: 2.473038	test: 2.435018

Epoch: 97
Loss: 0.7768073081970215
RMSE train: 0.758978	val: 3.711813	test: 3.187882
MAE train: 0.612467	val: 2.444714	test: 2.275001

Epoch: 98
Loss: 0.6136778295040131
RMSE train: 0.744592	val: 3.693025	test: 3.110614
MAE train: 0.599108	val: 2.446954	test: 2.200870

Epoch: 99
Loss: 0.6905090808868408
RMSE train: 0.746624	val: 3.671963	test: 3.106341
MAE train: 0.606810	val: 2.425577	test: 2.199329

Epoch: 100
Loss: 0.5734402239322662
RMSE train: 0.782905	val: 3.702673	test: 3.183666
MAE train: 0.636389	val: 2.410170	test: 2.302571

Epoch: 101
Loss: 0.5988221764564514
RMSE train: 0.803910	val: 3.684832	test: 3.163351
MAE train: 0.647209	val: 2.407138	test: 2.286269

Epoch: 102
Loss: 0.5362995862960815
RMSE train: 0.875275	val: 3.697689	test: 3.163593
MAE train: 0.688137	val: 2.425019	test: 2.297502

Epoch: 103
Loss: 0.6817695796489716
RMSE train: 0.949780	val: 3.769004	test: 3.189109
MAE train: 0.751162	val: 2.462800	test: 2.329480

Epoch: 104
Loss: 0.5669519007205963
RMSE train: 0.920088	val: 3.806131	test: 3.145480
MAE train: 0.731465	val: 2.469461	test: 2.279082

Epoch: 105
Loss: 0.60002800822258
RMSE train: 0.863196	val: 3.822545	test: 3.164470
MAE train: 0.693192	val: 2.461664	test: 2.276363

Epoch: 106
Loss: 0.5731377899646759
RMSE train: 0.788238	val: 3.764115	test: 3.219827
MAE train: 0.636970	val: 2.460642	test: 2.314848

Epoch: 107
Loss: 0.5718636512756348
RMSE train: 0.695381	val: 3.643939	test: 3.240131
MAE train: 0.564380	val: 2.446728	test: 2.318467

Epoch: 108
Loss: 0.6321747303009033
RMSE train: 0.624462	val: 3.556802	test: 3.213698
MAE train: 0.495991	val: 2.437221	test: 2.294020

Epoch: 109
Loss: 0.6730008125305176
RMSE train: 0.556896	val: 3.497522	test: 3.180145
MAE train: 0.438441	val: 2.401180	test: 2.255169

Epoch: 110
Loss: 0.6781269013881683
RMSE train: 0.543020	val: 3.535861	test: 3.139204
MAE train: 0.428923	val: 2.401279	test: 2.217380

Epoch: 111
Loss: 0.48928867280483246
RMSE train: 0.495599	val: 3.549414	test: 3.036028
MAE train: 0.389140	val: 2.410536	test: 2.142301

Epoch: 112
Loss: 0.5313433259725571
RMSE train: 0.515047	val: 3.566582	test: 2.996471
MAE train: 0.390416	val: 2.469326	test: 2.163974

Epoch: 113
Loss: 0.6150574088096619
RMSE train: 0.557803	val: 3.559988	test: 3.020787
MAE train: 0.415461	val: 2.460973	test: 2.231163

Epoch: 114
Loss: 0.5991332828998566
RMSE train: 0.579594	val: 3.484641	test: 3.059733
MAE train: 0.449140	val: 2.228689	test: 2.216167

Epoch: 115
Loss: 0.47826385498046875
RMSE train: 0.589322	val: 3.433209	test: 3.069053
MAE train: 0.470963	val: 2.180436	test: 2.195197

Epoch: 116
Loss: 0.5356073677539825
RMSE train: 0.560581	val: 3.319308	test: 3.032943
MAE train: 0.447920	val: 2.143218	test: 2.164935

Epoch: 117
Loss: 0.5471259355545044
RMSE train: 0.507620	val: 3.187753	test: 2.968764
MAE train: 0.400615	val: 2.119992	test: 2.110982

Epoch: 118
Loss: 0.5994247198104858
RMSE train: 0.484228	val: 3.162634	test: 2.934523
MAE train: 0.379045	val: 2.138736	test: 2.079449

Epoch: 119
Loss: 0.49724438786506653
RMSE train: 0.507309	val: 3.203589	test: 2.944184
MAE train: 0.393301	val: 2.169170	test: 2.087581

Epoch: 120
Loss: 0.5164974927902222
RMSE train: 0.546056	val: 3.271914	test: 3.001409
MAE train: 0.421945	val: 2.207307	test: 2.121960

Epoch: 121
Loss: 0.5363617539405823
RMSE train: 0.591023	val: 3.377489	test: 3.114260
MAE train: 0.460909	val: 2.253061	test: 2.221044

Early stopping
Best (RMSE):	 train: 1.237693	val: 2.868177	test: 2.940869
Best (MAE):	 train: 1.020091	val: 2.131443	test: 2.211299

RMSE train: 0.701490	val: 4.217915	test: 3.103554
MAE train: 0.545376	val: 3.463794	test: 2.528251

Epoch: 84
Loss: 1.0235074758529663
RMSE train: 0.714784	val: 4.376656	test: 3.266052
MAE train: 0.549390	val: 3.678672	test: 2.731693

Epoch: 85
Loss: 0.8822569251060486
RMSE train: 0.729727	val: 4.399125	test: 3.213977
MAE train: 0.564116	val: 3.706072	test: 2.673301

Epoch: 86
Loss: 0.9089964628219604
RMSE train: 0.758635	val: 4.285617	test: 3.035511
MAE train: 0.589242	val: 3.543999	test: 2.451758

Epoch: 87
Loss: 0.7477494478225708
RMSE train: 0.721826	val: 4.245786	test: 2.957992
MAE train: 0.562099	val: 3.513163	test: 2.346498

Epoch: 88
Loss: 0.8648342788219452
RMSE train: 0.648398	val: 4.329814	test: 2.997782
MAE train: 0.507685	val: 3.610586	test: 2.351737

Epoch: 89
Loss: 0.7624042332172394
RMSE train: 0.562369	val: 4.485350	test: 3.096166
MAE train: 0.437209	val: 3.803872	test: 2.497312

Epoch: 90
Loss: 0.8673871755599976
RMSE train: 0.531798	val: 4.581855	test: 3.185865
MAE train: 0.414188	val: 3.920359	test: 2.587608

Epoch: 91
Loss: 0.7735070884227753
RMSE train: 0.558345	val: 4.452183	test: 3.126900
MAE train: 0.434871	val: 3.752017	test: 2.512269

Epoch: 92
Loss: 0.8921844959259033
RMSE train: 0.628999	val: 4.406083	test: 3.129193
MAE train: 0.481780	val: 3.667355	test: 2.510100

Epoch: 93
Loss: 0.86722132563591
RMSE train: 0.702984	val: 4.387268	test: 3.159579
MAE train: 0.533101	val: 3.604853	test: 2.547492

Epoch: 94
Loss: 0.8254628777503967
RMSE train: 0.695472	val: 4.445320	test: 3.190158
MAE train: 0.546060	val: 3.676183	test: 2.569244

Epoch: 95
Loss: 0.7495914995670319
RMSE train: 0.654821	val: 4.535221	test: 3.200981
MAE train: 0.523027	val: 3.814981	test: 2.571123

Epoch: 96
Loss: 0.7826641201972961
RMSE train: 0.599572	val: 4.712920	test: 3.264163
MAE train: 0.477371	val: 4.054899	test: 2.632858

Epoch: 97
Loss: 0.7690308392047882
RMSE train: 0.583165	val: 4.619868	test: 3.177910
MAE train: 0.454560	val: 3.973965	test: 2.562201

Epoch: 98
Loss: 0.7154712378978729
RMSE train: 0.601232	val: 4.326503	test: 2.968326
MAE train: 0.465986	val: 3.636685	test: 2.349680

Epoch: 99
Loss: 0.732268899679184
RMSE train: 0.624182	val: 4.217402	test: 2.924295
MAE train: 0.485350	val: 3.495764	test: 2.272646

Epoch: 100
Loss: 0.8245145976543427
RMSE train: 0.650292	val: 4.218451	test: 2.962348
MAE train: 0.499962	val: 3.491379	test: 2.289480

Epoch: 101
Loss: 0.7235434949398041
RMSE train: 0.645952	val: 4.216606	test: 2.963043
MAE train: 0.494199	val: 3.499038	test: 2.308013

Epoch: 102
Loss: 0.700352668762207
RMSE train: 0.596801	val: 4.250231	test: 2.980214
MAE train: 0.460366	val: 3.555104	test: 2.348976

Epoch: 103
Loss: 0.7157301306724548
RMSE train: 0.566591	val: 4.354118	test: 3.047956
MAE train: 0.442525	val: 3.677191	test: 2.423838

Epoch: 104
Loss: 0.7804467976093292
RMSE train: 0.534399	val: 4.555355	test: 3.154713
MAE train: 0.419479	val: 3.898121	test: 2.511824

Epoch: 105
Loss: 0.6644701659679413
RMSE train: 0.543527	val: 4.608280	test: 3.170604
MAE train: 0.418663	val: 3.932873	test: 2.501827

Epoch: 106
Loss: 0.7760909199714661
RMSE train: 0.582107	val: 4.534684	test: 3.115002
MAE train: 0.453594	val: 3.818964	test: 2.418689

Epoch: 107
Loss: 0.6639754772186279
RMSE train: 0.638709	val: 4.446844	test: 3.083226
MAE train: 0.503478	val: 3.662832	test: 2.335878

Epoch: 108
Loss: 0.7024246454238892
RMSE train: 0.666736	val: 4.353906	test: 3.081903
MAE train: 0.519737	val: 3.514732	test: 2.328590

Epoch: 109
Loss: 0.7061907649040222
RMSE train: 0.638748	val: 4.344444	test: 3.081356
MAE train: 0.497562	val: 3.525394	test: 2.335558

Epoch: 110
Loss: 0.7750467956066132
RMSE train: 0.610022	val: 4.348128	test: 3.084022
MAE train: 0.470987	val: 3.562606	test: 2.396953

Epoch: 111
Loss: 0.7826859652996063
RMSE train: 0.594325	val: 4.277124	test: 3.025227
MAE train: 0.459313	val: 3.519601	test: 2.354913

Epoch: 112
Loss: 0.5638694167137146
RMSE train: 0.621064	val: 4.248733	test: 3.017371
MAE train: 0.480599	val: 3.469740	test: 2.284709

Epoch: 113
Loss: 0.6410769820213318
RMSE train: 0.650992	val: 4.282139	test: 3.071963
MAE train: 0.500930	val: 3.482233	test: 2.325025

Epoch: 114
Loss: 0.69460329413414
RMSE train: 0.660822	val: 4.314012	test: 3.104771
MAE train: 0.508618	val: 3.483552	test: 2.343935

Epoch: 115
Loss: 0.6523159444332123
RMSE train: 0.642156	val: 4.355896	test: 3.138108
MAE train: 0.493467	val: 3.571080	test: 2.404175

Epoch: 116
Loss: 0.7000123560428619
RMSE train: 0.597192	val: 4.465474	test: 3.267771
MAE train: 0.456815	val: 3.763071	test: 2.643793

Epoch: 117
Loss: 0.6535986363887787
RMSE train: 0.558000	val: 4.425345	test: 3.247283
MAE train: 0.426396	val: 3.744892	test: 2.641920

Epoch: 118
Loss: 0.5945384502410889
RMSE train: 0.538117	val: 4.404074	test: 3.198830
MAE train: 0.416759	val: 3.714550	test: 2.592887

Epoch: 119
Loss: 0.6099851727485657
RMSE train: 0.537367	val: 4.340125	test: 3.140791
MAE train: 0.417213	val: 3.640890	test: 2.531789

Epoch: 120
Loss: 0.7184584438800812
RMSE train: 0.560760	val: 4.222144	test: 3.018939
MAE train: 0.432957	val: 3.488997	test: 2.345113

Epoch: 121
Loss: 0.6817927658557892
RMSE train: 0.575564	val: 4.225425	test: 2.981269
MAE train: 0.445823	val: 3.486385	test: 2.293294

Early stopping
Best (RMSE):	 train: 1.045534	val: 3.626135	test: 2.993832
Best (MAE):	 train: 0.829696	val: 2.760635	test: 2.421815

RMSE train: 0.788492	val: 3.073249	test: 2.150925
MAE train: 0.623654	val: 2.266655	test: 1.650287

Epoch: 84
Loss: 0.7994529902935028
RMSE train: 0.781307	val: 3.074820	test: 2.139964
MAE train: 0.614975	val: 2.253470	test: 1.629926

Epoch: 85
Loss: 0.7034847438335419
RMSE train: 0.792788	val: 3.128785	test: 2.191577
MAE train: 0.626127	val: 2.262001	test: 1.664122

Epoch: 86
Loss: 0.6490975618362427
RMSE train: 0.768730	val: 3.203023	test: 2.162069
MAE train: 0.602849	val: 2.289723	test: 1.614788

Epoch: 87
Loss: 0.613640546798706
RMSE train: 0.679392	val: 3.191575	test: 2.067914
MAE train: 0.534414	val: 2.272375	test: 1.527519

Epoch: 88
Loss: 0.6781115531921387
RMSE train: 0.632964	val: 3.219914	test: 2.074170
MAE train: 0.496484	val: 2.250766	test: 1.557428

Epoch: 89
Loss: 0.6571945250034332
RMSE train: 0.627171	val: 3.296129	test: 2.109049
MAE train: 0.492897	val: 2.287410	test: 1.581818

Epoch: 90
Loss: 0.6987109482288361
RMSE train: 0.636319	val: 3.289635	test: 2.147794
MAE train: 0.504736	val: 2.305053	test: 1.580620

Epoch: 91
Loss: 0.600478321313858
RMSE train: 0.636639	val: 3.294425	test: 2.161825
MAE train: 0.499885	val: 2.295662	test: 1.579320

Epoch: 92
Loss: 0.7481157183647156
RMSE train: 0.637953	val: 3.282012	test: 2.134588
MAE train: 0.499317	val: 2.277202	test: 1.550112

Epoch: 93
Loss: 0.6158396601676941
RMSE train: 0.637739	val: 3.258439	test: 2.109386
MAE train: 0.500194	val: 2.245752	test: 1.527872

Epoch: 94
Loss: 0.6567286849021912
RMSE train: 0.604609	val: 3.190889	test: 2.087186
MAE train: 0.485946	val: 2.204853	test: 1.511585

Epoch: 95
Loss: 0.6261883080005646
RMSE train: 0.596878	val: 3.131158	test: 2.080505
MAE train: 0.483124	val: 2.174330	test: 1.509323

Epoch: 96
Loss: 0.6790423095226288
RMSE train: 0.586975	val: 3.068396	test: 2.057476
MAE train: 0.474593	val: 2.165964	test: 1.497436

Epoch: 97
Loss: 0.657890647649765
RMSE train: 0.636567	val: 3.089791	test: 2.043170
MAE train: 0.511328	val: 2.213015	test: 1.501118

Epoch: 98
Loss: 0.624747633934021
RMSE train: 0.681372	val: 3.192058	test: 2.057029
MAE train: 0.537049	val: 2.289854	test: 1.502085

Epoch: 99
Loss: 0.629761666059494
RMSE train: 0.719459	val: 3.332182	test: 2.102971
MAE train: 0.559611	val: 2.350162	test: 1.553166

Epoch: 100
Loss: 0.7844360172748566
RMSE train: 0.722161	val: 3.459404	test: 2.134088
MAE train: 0.554068	val: 2.391095	test: 1.570693

Epoch: 101
Loss: 0.606168806552887
RMSE train: 0.711339	val: 3.538122	test: 2.146792
MAE train: 0.546326	val: 2.406318	test: 1.567597

Epoch: 102
Loss: 0.6371466517448425
RMSE train: 0.655956	val: 3.508831	test: 2.139060
MAE train: 0.509700	val: 2.389776	test: 1.559101

Epoch: 103
Loss: 0.5953651070594788
RMSE train: 0.618683	val: 3.421389	test: 2.179864
MAE train: 0.492379	val: 2.353691	test: 1.579013

Epoch: 104
Loss: 0.6129302680492401
RMSE train: 0.571275	val: 3.294564	test: 2.151066
MAE train: 0.456680	val: 2.287988	test: 1.558266

Epoch: 105
Loss: 0.6368539333343506
RMSE train: 0.535761	val: 3.186495	test: 2.065615
MAE train: 0.426768	val: 2.225428	test: 1.524645

Epoch: 106
Loss: 0.6340348422527313
RMSE train: 0.559478	val: 3.092303	test: 2.015309
MAE train: 0.439296	val: 2.189179	test: 1.516021

Epoch: 107
Loss: 0.5741139054298401
RMSE train: 0.614890	val: 3.023996	test: 2.043614
MAE train: 0.476418	val: 2.192721	test: 1.549326

Epoch: 108
Loss: 0.5882973372936249
RMSE train: 0.648823	val: 2.954069	test: 2.126133
MAE train: 0.509411	val: 2.190172	test: 1.602257

Epoch: 109
Loss: 0.5794181227684021
RMSE train: 0.635113	val: 2.930932	test: 2.142618
MAE train: 0.496416	val: 2.200109	test: 1.627199

Epoch: 110
Loss: 0.6117048263549805
RMSE train: 0.590486	val: 2.974398	test: 2.066279
MAE train: 0.458220	val: 2.244116	test: 1.546179

Epoch: 111
Loss: 0.6719533801078796
RMSE train: 0.533945	val: 3.034358	test: 1.998153
MAE train: 0.413064	val: 2.265898	test: 1.505602

Epoch: 112
Loss: 0.5750406384468079
RMSE train: 0.528632	val: 3.052497	test: 2.018188
MAE train: 0.409491	val: 2.222778	test: 1.526055

Epoch: 113
Loss: 0.545721024274826
RMSE train: 0.543415	val: 3.104613	test: 2.090343
MAE train: 0.425626	val: 2.211372	test: 1.577807

Epoch: 114
Loss: 0.521673396229744
RMSE train: 0.600807	val: 3.183805	test: 2.180175
MAE train: 0.472183	val: 2.225859	test: 1.628909

Epoch: 115
Loss: 0.5536755919456482
RMSE train: 0.638773	val: 3.210680	test: 2.218196
MAE train: 0.508008	val: 2.252305	test: 1.653823

Epoch: 116
Loss: 0.6491119265556335
RMSE train: 0.615979	val: 3.174827	test: 2.155183
MAE train: 0.497317	val: 2.261073	test: 1.598045

Epoch: 117
Loss: 0.5727363526821136
RMSE train: 0.578152	val: 3.147488	test: 2.086518
MAE train: 0.467271	val: 2.296206	test: 1.573496

Epoch: 118
Loss: 0.5666502118110657
RMSE train: 0.588197	val: 3.189540	test: 2.094315
MAE train: 0.477806	val: 2.342490	test: 1.591922

Epoch: 119
Loss: 0.5130408704280853
RMSE train: 0.636983	val: 3.233677	test: 2.174621
MAE train: 0.515648	val: 2.382082	test: 1.662575

Epoch: 120
Loss: 0.5538040399551392
RMSE train: 0.693694	val: 3.276907	test: 2.271122
MAE train: 0.554909	val: 2.405529	test: 1.761171

Epoch: 121
Loss: 0.513440728187561
RMSE train: 0.688436	val: 3.273882	test: 2.293129
MAE train: 0.550105	val: 2.420308	test: 1.777163

Early stopping
Best (RMSE):	 train: 1.576973	val: 2.819550	test: 2.681019
Best (MAE):	 train: 1.337183	val: 2.302961	test: 2.211883

RMSE train: 0.635690	val: 3.973824	test: 3.139688
MAE train: 0.492138	val: 2.756014	test: 2.469832

Epoch: 84
Loss: 0.7209195196628571
RMSE train: 0.615301	val: 3.980193	test: 3.156051
MAE train: 0.475818	val: 2.764555	test: 2.495430

Epoch: 85
Loss: 0.8462127447128296
RMSE train: 0.556871	val: 3.905237	test: 3.154537
MAE train: 0.432319	val: 2.682369	test: 2.490388

Epoch: 86
Loss: 0.9523018002510071
RMSE train: 0.529340	val: 3.841132	test: 3.119374
MAE train: 0.413910	val: 2.601736	test: 2.451737

Epoch: 87
Loss: 0.8378039300441742
RMSE train: 0.548063	val: 3.791620	test: 3.073528
MAE train: 0.430541	val: 2.564286	test: 2.398914

Epoch: 88
Loss: 0.798141360282898
RMSE train: 0.611665	val: 3.822935	test: 3.064635
MAE train: 0.485996	val: 2.577702	test: 2.381874

Epoch: 89
Loss: 0.6888297200202942
RMSE train: 0.632003	val: 3.842345	test: 3.059013
MAE train: 0.501315	val: 2.578663	test: 2.369398

Epoch: 90
Loss: 0.7752578258514404
RMSE train: 0.626067	val: 3.827113	test: 3.111496
MAE train: 0.490150	val: 2.523446	test: 2.414486

Epoch: 91
Loss: 0.6286482214927673
RMSE train: 0.603526	val: 3.795252	test: 3.119450
MAE train: 0.465326	val: 2.485545	test: 2.430679

Epoch: 92
Loss: 0.8641282916069031
RMSE train: 0.580649	val: 3.802734	test: 3.240705
MAE train: 0.443963	val: 2.490273	test: 2.558418

Epoch: 93
Loss: 0.7450817227363586
RMSE train: 0.551234	val: 3.814797	test: 3.332770
MAE train: 0.423016	val: 2.494150	test: 2.648814

Epoch: 94
Loss: 0.6893712282180786
RMSE train: 0.525956	val: 3.802716	test: 3.258001
MAE train: 0.410636	val: 2.475227	test: 2.575174

Epoch: 95
Loss: 0.7596944868564606
RMSE train: 0.549550	val: 3.813745	test: 3.104926
MAE train: 0.429299	val: 2.489050	test: 2.392171

Epoch: 96
Loss: 0.7266262173652649
RMSE train: 0.576126	val: 3.789071	test: 3.012097
MAE train: 0.447856	val: 2.477592	test: 2.297823

Epoch: 97
Loss: 0.9038611650466919
RMSE train: 0.636101	val: 3.875408	test: 3.090574
MAE train: 0.492169	val: 2.529576	test: 2.362760

Epoch: 98
Loss: 0.635406106710434
RMSE train: 0.681231	val: 3.986797	test: 3.262014
MAE train: 0.525368	val: 2.634705	test: 2.558295

Epoch: 99
Loss: 0.7181302905082703
RMSE train: 0.641505	val: 3.966025	test: 3.351400
MAE train: 0.495649	val: 2.682334	test: 2.666533

Epoch: 100
Loss: 0.6572518944740295
RMSE train: 0.599040	val: 3.871731	test: 3.330356
MAE train: 0.459561	val: 2.660562	test: 2.647874

Epoch: 101
Loss: 0.7771467566490173
RMSE train: 0.567741	val: 3.803939	test: 3.270462
MAE train: 0.445507	val: 2.636507	test: 2.565783

Epoch: 102
Loss: 0.7020107209682465
RMSE train: 0.579217	val: 3.832596	test: 3.336047
MAE train: 0.459911	val: 2.644145	test: 2.607073

Epoch: 103
Loss: 0.6804823577404022
RMSE train: 0.621557	val: 3.934170	test: 3.416724
MAE train: 0.488873	val: 2.666963	test: 2.666137

Epoch: 104
Loss: 0.6739685535430908
RMSE train: 0.644928	val: 4.012952	test: 3.407856
MAE train: 0.505119	val: 2.678657	test: 2.646760

Epoch: 105
Loss: 0.6152376532554626
RMSE train: 0.602073	val: 4.003055	test: 3.271797
MAE train: 0.474864	val: 2.694207	test: 2.541068

Epoch: 106
Loss: 0.6517247557640076
RMSE train: 0.566691	val: 3.979256	test: 3.174580
MAE train: 0.449218	val: 2.702118	test: 2.470669

Epoch: 107
Loss: 0.5833746790885925
RMSE train: 0.540823	val: 3.950601	test: 3.151758
MAE train: 0.423697	val: 2.698855	test: 2.462299

Epoch: 108
Loss: 0.6600339114665985
RMSE train: 0.556875	val: 3.973343	test: 3.289386
MAE train: 0.430752	val: 2.729170	test: 2.595439

Epoch: 109
Loss: 0.6422120332717896
RMSE train: 0.559801	val: 4.002323	test: 3.361094
MAE train: 0.431269	val: 2.749561	test: 2.664779

Epoch: 110
Loss: 0.6161113083362579
RMSE train: 0.566198	val: 4.013313	test: 3.372372
MAE train: 0.440490	val: 2.718996	test: 2.654078

Epoch: 111
Loss: 0.5899877846240997
RMSE train: 0.561520	val: 4.005976	test: 3.354452
MAE train: 0.439017	val: 2.694738	test: 2.627696

Epoch: 112
Loss: 0.5661167204380035
RMSE train: 0.535245	val: 3.962695	test: 3.325249
MAE train: 0.416946	val: 2.631439	test: 2.598095

Epoch: 113
Loss: 0.489937961101532
RMSE train: 0.511769	val: 3.936985	test: 3.233009
MAE train: 0.395700	val: 2.565403	test: 2.491248

Epoch: 114
Loss: 0.7058334350585938
RMSE train: 0.484854	val: 3.906042	test: 3.208128
MAE train: 0.375040	val: 2.530285	test: 2.456450

Epoch: 115
Loss: 0.6222501993179321
RMSE train: 0.496298	val: 3.955629	test: 3.245336
MAE train: 0.383762	val: 2.563971	test: 2.508446

Epoch: 116
Loss: 0.6072104871273041
RMSE train: 0.514286	val: 3.960412	test: 3.188970
MAE train: 0.394914	val: 2.607121	test: 2.481802

Epoch: 117
Loss: 0.5246964693069458
RMSE train: 0.529647	val: 3.955061	test: 3.201901
MAE train: 0.404283	val: 2.600348	test: 2.492943

Epoch: 118
Loss: 0.5638445019721985
RMSE train: 0.516351	val: 3.921443	test: 3.191137
MAE train: 0.397485	val: 2.637779	test: 2.517614

Epoch: 119
Loss: 0.5356566309928894
RMSE train: 0.487465	val: 3.889788	test: 3.198257
MAE train: 0.375869	val: 2.636048	test: 2.522460

Epoch: 120
Loss: 0.6684278547763824
RMSE train: 0.467114	val: 3.862823	test: 3.163657
MAE train: 0.364759	val: 2.585553	test: 2.460984

Epoch: 121
Loss: 0.6158220767974854
RMSE train: 0.505305	val: 3.918333	test: 3.136697
MAE train: 0.395919	val: 2.595681	test: 2.412873

Early stopping
Best (RMSE):	 train: 0.738740	val: 3.652539	test: 2.887370
Best (MAE):	 train: 0.570895	val: 2.508907	test: 2.261319

RMSE train: 0.589822	val: 3.863723	test: 3.451638
MAE train: 0.461480	val: 2.804058	test: 2.743663

Epoch: 84
Loss: 0.7873470783233643
RMSE train: 0.599527	val: 3.859599	test: 3.409521
MAE train: 0.467677	val: 2.765818	test: 2.692671

Epoch: 85
Loss: 0.9089958667755127
RMSE train: 0.574440	val: 3.858990	test: 3.443409
MAE train: 0.448937	val: 2.720374	test: 2.725833

Epoch: 86
Loss: 0.9368727803230286
RMSE train: 0.537475	val: 3.926177	test: 3.523890
MAE train: 0.420914	val: 2.779515	test: 2.822514

Epoch: 87
Loss: 0.9026023745536804
RMSE train: 0.534310	val: 3.997223	test: 3.604122
MAE train: 0.420114	val: 2.838945	test: 2.893851

Epoch: 88
Loss: 0.8692578971385956
RMSE train: 0.551205	val: 4.032397	test: 3.680466
MAE train: 0.433504	val: 2.861529	test: 2.940348

Epoch: 89
Loss: 0.7876275777816772
RMSE train: 0.543606	val: 4.016525	test: 3.658063
MAE train: 0.423398	val: 2.901363	test: 2.947070

Epoch: 90
Loss: 0.8035922646522522
RMSE train: 0.512619	val: 3.986201	test: 3.620339
MAE train: 0.398019	val: 2.886243	test: 2.929290

Epoch: 91
Loss: 0.7619793117046356
RMSE train: 0.485621	val: 3.940706	test: 3.512809
MAE train: 0.381783	val: 2.823601	test: 2.828437

Epoch: 92
Loss: 1.0152653753757477
RMSE train: 0.499676	val: 3.914350	test: 3.474873
MAE train: 0.398441	val: 2.718855	test: 2.746449

Epoch: 93
Loss: 0.7523490488529205
RMSE train: 0.508193	val: 3.901553	test: 3.507558
MAE train: 0.403820	val: 2.678792	test: 2.770762

Epoch: 94
Loss: 0.7466602921485901
RMSE train: 0.518715	val: 3.903478	test: 3.556362
MAE train: 0.408756	val: 2.737013	test: 2.891100

Epoch: 95
Loss: 0.8561097085475922
RMSE train: 0.558621	val: 3.916603	test: 3.544362
MAE train: 0.444208	val: 2.826886	test: 2.914828

Epoch: 96
Loss: 0.817255437374115
RMSE train: 0.595291	val: 3.942440	test: 3.566334
MAE train: 0.476514	val: 2.841558	test: 2.875007

Epoch: 97
Loss: 0.8560729026794434
RMSE train: 0.673757	val: 4.038656	test: 3.644454
MAE train: 0.534186	val: 2.888681	test: 2.888611

Epoch: 98
Loss: 0.6808758080005646
RMSE train: 0.710896	val: 4.120774	test: 3.717100
MAE train: 0.550503	val: 2.930291	test: 2.953141

Epoch: 99
Loss: 0.8681569695472717
RMSE train: 0.622854	val: 4.051924	test: 3.639819
MAE train: 0.482861	val: 2.886803	test: 2.898370

Epoch: 100
Loss: 0.7254335582256317
RMSE train: 0.528967	val: 3.938063	test: 3.526596
MAE train: 0.404545	val: 2.831629	test: 2.805550

Epoch: 101
Loss: 0.7289929389953613
RMSE train: 0.470976	val: 3.866333	test: 3.461622
MAE train: 0.370419	val: 2.768357	test: 2.788202

Epoch: 102
Loss: 0.7975015044212341
RMSE train: 0.505329	val: 3.901863	test: 3.465069
MAE train: 0.400574	val: 2.773040	test: 2.805779

Epoch: 103
Loss: 0.7610072493553162
RMSE train: 0.593731	val: 3.941750	test: 3.459820
MAE train: 0.469248	val: 2.703182	test: 2.729224

Epoch: 104
Loss: 0.7892302572727203
RMSE train: 0.639191	val: 3.944927	test: 3.403230
MAE train: 0.498244	val: 2.689934	test: 2.655762

Epoch: 105
Loss: 0.7115904092788696
RMSE train: 0.552738	val: 3.862357	test: 3.271545
MAE train: 0.428694	val: 2.706546	test: 2.581008

Epoch: 106
Loss: 0.8521730899810791
RMSE train: 0.481718	val: 3.821925	test: 3.174840
MAE train: 0.375838	val: 2.746796	test: 2.547683

Epoch: 107
Loss: 0.610821545124054
RMSE train: 0.446441	val: 3.813776	test: 3.181036
MAE train: 0.347033	val: 2.759502	test: 2.539823

Epoch: 108
Loss: 0.7135933339595795
RMSE train: 0.455172	val: 3.848118	test: 3.325503
MAE train: 0.350673	val: 2.739984	test: 2.611860

Epoch: 109
Loss: 0.6940875053405762
RMSE train: 0.481848	val: 3.881950	test: 3.420656
MAE train: 0.373243	val: 2.709442	test: 2.658593

Epoch: 110
Loss: 0.6834901571273804
RMSE train: 0.487404	val: 3.910277	test: 3.455362
MAE train: 0.376453	val: 2.703624	test: 2.678949

Epoch: 111
Loss: 0.6821842193603516
RMSE train: 0.439157	val: 3.906804	test: 3.396595
MAE train: 0.335085	val: 2.721842	test: 2.666933

Epoch: 112
Loss: 0.6508613526821136
RMSE train: 0.434299	val: 3.917059	test: 3.312376
MAE train: 0.337442	val: 2.765244	test: 2.601400

Epoch: 113
Loss: 0.652157723903656
RMSE train: 0.460016	val: 3.891156	test: 3.303013
MAE train: 0.362199	val: 2.749561	test: 2.598696

Epoch: 114
Loss: 0.7655175626277924
RMSE train: 0.445715	val: 3.889719	test: 3.461907
MAE train: 0.351509	val: 2.675009	test: 2.695169

Epoch: 115
Loss: 0.6788927614688873
RMSE train: 0.467123	val: 3.938649	test: 3.577972
MAE train: 0.363112	val: 2.690710	test: 2.768824

Epoch: 116
Loss: 0.6238998174667358
RMSE train: 0.487306	val: 4.008520	test: 3.559307
MAE train: 0.366183	val: 2.793050	test: 2.820802

Epoch: 117
Loss: 0.6158404052257538
RMSE train: 0.520235	val: 4.089101	test: 3.518566
MAE train: 0.386698	val: 2.855107	test: 2.797531

Epoch: 118
Loss: 0.638541966676712
RMSE train: 0.529152	val: 4.120903	test: 3.491523
MAE train: 0.393960	val: 2.874983	test: 2.742914

Epoch: 119
Loss: 0.5331041812896729
RMSE train: 0.506846	val: 4.139763	test: 3.563367
MAE train: 0.386143	val: 2.847788	test: 2.761799

Epoch: 120
Loss: 0.7031153738498688
RMSE train: 0.448953	val: 4.077525	test: 3.643016
MAE train: 0.347034	val: 2.835024	test: 2.835670

Epoch: 121
Loss: 0.5621670484542847
RMSE train: 0.422758	val: 4.041398	test: 3.640151
MAE train: 0.322710	val: 2.826378	test: 2.867117

Early stopping
Best (RMSE):	 train: 0.703717	val: 3.635685	test: 3.369026
Best (MAE):	 train: 0.555703	val: 2.612415	test: 2.768551

RMSE train: 0.582373	val: 3.514898	test: 2.723422
MAE train: 0.455518	val: 2.460652	test: 2.039466

Epoch: 84
Loss: 0.6445364654064178
RMSE train: 0.613699	val: 3.572894	test: 2.795999
MAE train: 0.475969	val: 2.463378	test: 2.085303

Epoch: 85
Loss: 0.755966067314148
RMSE train: 0.611547	val: 3.537585	test: 2.798590
MAE train: 0.472480	val: 2.420796	test: 2.093880

Epoch: 86
Loss: 0.8763836622238159
RMSE train: 0.585014	val: 3.497026	test: 2.713098
MAE train: 0.456289	val: 2.421885	test: 2.026512

Epoch: 87
Loss: 0.6527238190174103
RMSE train: 0.591581	val: 3.511575	test: 2.670996
MAE train: 0.469739	val: 2.449715	test: 1.968979

Epoch: 88
Loss: 0.7228241860866547
RMSE train: 0.649448	val: 3.625418	test: 2.730616
MAE train: 0.511632	val: 2.486855	test: 1.996447

Epoch: 89
Loss: 0.7057800590991974
RMSE train: 0.656785	val: 3.661681	test: 2.770215
MAE train: 0.505853	val: 2.484491	test: 2.030367

Epoch: 90
Loss: 0.6339677572250366
RMSE train: 0.609375	val: 3.579771	test: 2.771780
MAE train: 0.461655	val: 2.362284	test: 2.036568

Epoch: 91
Loss: 0.6505405306816101
RMSE train: 0.530736	val: 3.433926	test: 2.610463
MAE train: 0.404104	val: 2.257218	test: 1.902686

Epoch: 92
Loss: 0.8349305093288422
RMSE train: 0.497353	val: 3.269064	test: 2.426236
MAE train: 0.387991	val: 2.137251	test: 1.750108

Epoch: 93
Loss: 0.6009848713874817
RMSE train: 0.483423	val: 3.238510	test: 2.424206
MAE train: 0.380030	val: 2.059020	test: 1.768191

Epoch: 94
Loss: 0.5657579898834229
RMSE train: 0.489090	val: 3.299212	test: 2.491815
MAE train: 0.378306	val: 2.080095	test: 1.810100

Epoch: 95
Loss: 0.6063498854637146
RMSE train: 0.496967	val: 3.368880	test: 2.548147
MAE train: 0.384821	val: 2.145104	test: 1.856520

Epoch: 96
Loss: 0.6279953122138977
RMSE train: 0.507825	val: 3.398387	test: 2.570782
MAE train: 0.394823	val: 2.251402	test: 1.883569

Epoch: 97
Loss: 0.770360916852951
RMSE train: 0.612711	val: 3.528218	test: 2.686055
MAE train: 0.470064	val: 2.391533	test: 2.009069

Epoch: 98
Loss: 0.5482613742351532
RMSE train: 0.688046	val: 3.618325	test: 2.821196
MAE train: 0.522440	val: 2.422066	test: 2.094598

Epoch: 99
Loss: 0.6391558945178986
RMSE train: 0.737875	val: 3.607466	test: 2.831235
MAE train: 0.561916	val: 2.373054	test: 2.107370

Epoch: 100
Loss: 0.5508168339729309
RMSE train: 0.736625	val: 3.539028	test: 2.746301
MAE train: 0.569092	val: 2.320527	test: 2.044944

Epoch: 101
Loss: 0.641867607831955
RMSE train: 0.676238	val: 3.459373	test: 2.639532
MAE train: 0.536365	val: 2.291842	test: 1.955522

Epoch: 102
Loss: 0.6131783127784729
RMSE train: 0.684563	val: 3.518124	test: 2.625792
MAE train: 0.542920	val: 2.361454	test: 1.930572

Epoch: 103
Loss: 0.5924497842788696
RMSE train: 0.719763	val: 3.624225	test: 2.642127
MAE train: 0.557360	val: 2.419035	test: 1.909823

Epoch: 104
Loss: 0.6498270630836487
RMSE train: 0.749020	val: 3.701257	test: 2.673960
MAE train: 0.564906	val: 2.466450	test: 1.911234

Epoch: 105
Loss: 0.6189874410629272
RMSE train: 0.686015	val: 3.632599	test: 2.638152
MAE train: 0.519723	val: 2.458336	test: 1.880420

Epoch: 106
Loss: 0.597627729177475
RMSE train: 0.621820	val: 3.547025	test: 2.564117
MAE train: 0.481413	val: 2.445985	test: 1.825605

Epoch: 107
Loss: 0.48891495168209076
RMSE train: 0.580026	val: 3.455340	test: 2.500757
MAE train: 0.453185	val: 2.400915	test: 1.769847

Epoch: 108
Loss: 0.524103045463562
RMSE train: 0.557794	val: 3.396418	test: 2.510398
MAE train: 0.435574	val: 2.313886	test: 1.764734

Epoch: 109
Loss: 0.5701883435249329
RMSE train: 0.547504	val: 3.360637	test: 2.523973
MAE train: 0.425214	val: 2.255996	test: 1.782423

Epoch: 110
Loss: 0.4877513200044632
RMSE train: 0.567317	val: 3.375440	test: 2.562837
MAE train: 0.438948	val: 2.231350	test: 1.811647

Epoch: 111
Loss: 0.5861731469631195
RMSE train: 0.573634	val: 3.371693	test: 2.534927
MAE train: 0.445432	val: 2.231998	test: 1.801688

Epoch: 112
Loss: 0.5144466608762741
RMSE train: 0.582757	val: 3.399137	test: 2.479198
MAE train: 0.448930	val: 2.304039	test: 1.783447

Epoch: 113
Loss: 0.44688476622104645
RMSE train: 0.572409	val: 3.439829	test: 2.448718
MAE train: 0.438608	val: 2.351072	test: 1.778586

Epoch: 114
Loss: 0.6515308618545532
RMSE train: 0.560590	val: 3.467658	test: 2.545990
MAE train: 0.428974	val: 2.316019	test: 1.854166

Epoch: 115
Loss: 0.48821286857128143
RMSE train: 0.572141	val: 3.530351	test: 2.715926
MAE train: 0.434835	val: 2.300377	test: 1.985173

Epoch: 116
Loss: 0.4706760495901108
RMSE train: 0.536767	val: 3.598898	test: 2.774659
MAE train: 0.404688	val: 2.377390	test: 2.045992

Epoch: 117
Loss: 0.4269667863845825
RMSE train: 0.520886	val: 3.658725	test: 2.789305
MAE train: 0.387923	val: 2.451955	test: 2.071620

Epoch: 118
Loss: 0.5232725590467453
RMSE train: 0.487084	val: 3.665833	test: 2.723172
MAE train: 0.367151	val: 2.529462	test: 2.019786

Epoch: 119
Loss: 0.473150297999382
RMSE train: 0.452628	val: 3.574802	test: 2.666214
MAE train: 0.349966	val: 2.446099	test: 1.955904

Epoch: 120
Loss: 0.5927325189113617
RMSE train: 0.451387	val: 3.491849	test: 2.623125
MAE train: 0.352216	val: 2.309452	test: 1.914034

Epoch: 121
Loss: 0.6064286231994629
RMSE train: 0.463954	val: 3.436536	test: 2.565731
MAE train: 0.361993	val: 2.223894	test: 1.855796

Early stopping
Best (RMSE):	 train: 0.668995	val: 3.233185	test: 2.684789
Best (MAE):	 train: 0.522997	val: 2.286650	test: 2.050594

RMSE train: 0.704118	val: 4.277226	test: 3.310974
MAE train: 0.551394	val: 3.048013	test: 2.528246

Epoch: 84
Loss: 0.8640443086624146
RMSE train: 0.712099	val: 4.227469	test: 3.298732
MAE train: 0.554362	val: 2.971944	test: 2.509370

Epoch: 85
Loss: 0.901109516620636
RMSE train: 0.731732	val: 4.244152	test: 3.276977
MAE train: 0.568134	val: 2.991742	test: 2.481979

Epoch: 86
Loss: 0.8977180123329163
RMSE train: 0.774033	val: 4.296638	test: 3.254719
MAE train: 0.610892	val: 3.081249	test: 2.485035

Epoch: 87
Loss: 0.8113578259944916
RMSE train: 0.802913	val: 4.336595	test: 3.255972
MAE train: 0.638539	val: 3.141356	test: 2.503996

Epoch: 88
Loss: 0.8778137862682343
RMSE train: 0.831275	val: 4.331448	test: 3.268688
MAE train: 0.671587	val: 3.148206	test: 2.528252

Epoch: 89
Loss: 0.8996155858039856
RMSE train: 0.795924	val: 4.334232	test: 3.270737
MAE train: 0.649170	val: 3.163641	test: 2.537210

Epoch: 90
Loss: 0.9302334785461426
RMSE train: 0.736196	val: 4.349326	test: 3.281623
MAE train: 0.602323	val: 3.215806	test: 2.603200

Epoch: 91
Loss: 0.9342567026615143
RMSE train: 0.724657	val: 4.373173	test: 3.320563
MAE train: 0.591451	val: 3.230257	test: 2.635509

Epoch: 92
Loss: 0.8953017294406891
RMSE train: 0.762376	val: 4.357463	test: 3.352924
MAE train: 0.626890	val: 3.114550	test: 2.612955

Epoch: 93
Loss: 0.8510083556175232
RMSE train: 0.803980	val: 4.322831	test: 3.370058
MAE train: 0.659869	val: 3.026386	test: 2.563279

Epoch: 94
Loss: 0.8738479912281036
RMSE train: 0.825279	val: 4.307363	test: 3.380425
MAE train: 0.674376	val: 2.996379	test: 2.595781

Epoch: 95
Loss: 0.7444258332252502
RMSE train: 0.814688	val: 4.326950	test: 3.377691
MAE train: 0.671364	val: 3.043052	test: 2.613348

Epoch: 96
Loss: 0.7903864085674286
RMSE train: 0.787947	val: 4.317494	test: 3.361384
MAE train: 0.651559	val: 3.061424	test: 2.601039

Epoch: 97
Loss: 0.909508228302002
RMSE train: 0.754297	val: 4.378303	test: 3.387388
MAE train: 0.612303	val: 3.138771	test: 2.636895

Epoch: 98
Loss: 0.818626344203949
RMSE train: 0.716815	val: 4.443207	test: 3.424051
MAE train: 0.564953	val: 3.204039	test: 2.703632

Epoch: 99
Loss: 0.8659685254096985
RMSE train: 0.731915	val: 4.431520	test: 3.435277
MAE train: 0.581285	val: 3.160960	test: 2.690591

Epoch: 100
Loss: 0.7456876337528229
RMSE train: 0.774473	val: 4.425025	test: 3.435010
MAE train: 0.620659	val: 3.081221	test: 2.626800

Epoch: 101
Loss: 0.7062214910984039
RMSE train: 0.785195	val: 4.398001	test: 3.388114
MAE train: 0.636877	val: 3.011072	test: 2.566117

Epoch: 102
Loss: 0.6750095784664154
RMSE train: 0.788965	val: 4.392178	test: 3.321430
MAE train: 0.648901	val: 2.975783	test: 2.506335

Epoch: 103
Loss: 0.800553560256958
RMSE train: 0.809002	val: 4.442433	test: 3.275064
MAE train: 0.669470	val: 3.007050	test: 2.519148

Epoch: 104
Loss: 0.7168195247650146
RMSE train: 0.806005	val: 4.484879	test: 3.269577
MAE train: 0.665535	val: 3.073516	test: 2.516671

Epoch: 105
Loss: 0.6464645266532898
RMSE train: 0.830090	val: 4.522153	test: 3.311961
MAE train: 0.687206	val: 3.129794	test: 2.508575

Epoch: 106
Loss: 0.7458649277687073
RMSE train: 0.840012	val: 4.555567	test: 3.385001
MAE train: 0.686392	val: 3.156603	test: 2.534021

Epoch: 107
Loss: 0.7424536049365997
RMSE train: 0.771685	val: 4.535394	test: 3.392051
MAE train: 0.629528	val: 3.157284	test: 2.549341

Epoch: 108
Loss: 0.6379314959049225
RMSE train: 0.695288	val: 4.548241	test: 3.400944
MAE train: 0.565701	val: 3.223420	test: 2.604543

Epoch: 109
Loss: 0.7350421845912933
RMSE train: 0.624179	val: 4.618500	test: 3.411409
MAE train: 0.504081	val: 3.457925	test: 2.699906

Epoch: 110
Loss: 0.7911079227924347
RMSE train: 0.621244	val: 4.651591	test: 3.420615
MAE train: 0.500829	val: 3.506921	test: 2.704868

Epoch: 111
Loss: 0.6916355788707733
RMSE train: 0.615225	val: 4.677251	test: 3.402257
MAE train: 0.497015	val: 3.547270	test: 2.700704

Epoch: 112
Loss: 0.6126308441162109
RMSE train: 0.634013	val: 4.677542	test: 3.359856
MAE train: 0.508663	val: 3.504712	test: 2.648613

Epoch: 113
Loss: 0.6322917640209198
RMSE train: 0.644709	val: 4.692768	test: 3.318529
MAE train: 0.504582	val: 3.492230	test: 2.596583

Epoch: 114
Loss: 0.646475613117218
RMSE train: 0.651214	val: 4.625894	test: 3.266601
MAE train: 0.498395	val: 3.379228	test: 2.509337

Epoch: 115
Loss: 0.6656897962093353
RMSE train: 0.628141	val: 4.550854	test: 3.229567
MAE train: 0.487246	val: 3.301272	test: 2.473370

Epoch: 116
Loss: 0.7508516907691956
RMSE train: 0.621525	val: 4.463868	test: 3.219473
MAE train: 0.488292	val: 3.155502	test: 2.423971

Epoch: 117
Loss: 0.6179409325122833
RMSE train: 0.608287	val: 4.400624	test: 3.238273
MAE train: 0.482025	val: 3.108828	test: 2.403435

Epoch: 118
Loss: 0.611577183008194
RMSE train: 0.577386	val: 4.358421	test: 3.236706
MAE train: 0.457850	val: 3.106799	test: 2.423136

Epoch: 119
Loss: 0.6070468127727509
RMSE train: 0.536695	val: 4.315985	test: 3.218043
MAE train: 0.428870	val: 3.078550	test: 2.449236

Epoch: 120
Loss: 0.6474052667617798
RMSE train: 0.534021	val: 4.298930	test: 3.210562
MAE train: 0.426914	val: 3.060474	test: 2.462988

Epoch: 121
Loss: 0.7353147268295288
RMSE train: 0.579706	val: 4.302956	test: 3.233553
MAE train: 0.464771	val: 3.016821	test: 2.435989

Early stopping
Best (RMSE):	 train: 0.980064	val: 3.903544	test: 3.050400
Best (MAE):	 train: 0.765374	val: 2.727173	test: 2.374470
All runs completed.
All runs completed.
All runs completed.
