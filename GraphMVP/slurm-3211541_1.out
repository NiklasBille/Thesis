>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.6/freesolv_scaff_5_26-05_11-10-19  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 18.7063627243042
RMSE train: 4.276474	val: 5.840344	test: 8.297871
MAE train: 3.425356	val: 4.980988	test: 6.730131

Epoch: 2
Loss: 17.208937644958496
RMSE train: 4.151921	val: 5.806904	test: 8.159534
MAE train: 3.317568	val: 5.007231	test: 6.633364

Epoch: 3
Loss: 15.835517406463623
RMSE train: 4.029308	val: 5.783880	test: 8.026734
MAE train: 3.212906	val: 5.041515	test: 6.526671

Epoch: 4
Loss: 15.428389549255371
RMSE train: 3.919341	val: 5.795323	test: 7.908581
MAE train: 3.117787	val: 5.108217	test: 6.425841

Epoch: 5
Loss: 12.98362112045288
RMSE train: 3.816621	val: 5.821749	test: 7.751184
MAE train: 3.029965	val: 5.197624	test: 6.291290

Epoch: 6
Loss: 11.821089267730713
RMSE train: 3.705117	val: 5.844762	test: 7.542644
MAE train: 2.947857	val: 5.295943	test: 6.115420

Epoch: 7
Loss: 11.476774215698242
RMSE train: 3.612225	val: 5.862334	test: 7.304816
MAE train: 2.891246	val: 5.392117	test: 5.933512

Epoch: 8
Loss: 9.720852375030518
RMSE train: 3.516320	val: 5.851597	test: 6.963178
MAE train: 2.855662	val: 5.457575	test: 5.648316

Epoch: 9
Loss: 9.444869041442871
RMSE train: 3.420826	val: 5.782074	test: 6.531379
MAE train: 2.836037	val: 5.453993	test: 5.246738

Epoch: 10
Loss: 8.798359394073486
RMSE train: 3.342468	val: 5.672221	test: 6.172915
MAE train: 2.800556	val: 5.380836	test: 4.901201

Epoch: 11
Loss: 7.392467021942139
RMSE train: 3.182340	val: 5.267555	test: 5.595425
MAE train: 2.701809	val: 4.968434	test: 4.371743

Epoch: 12
Loss: 7.4178924560546875
RMSE train: 2.972193	val: 4.565387	test: 4.909368
MAE train: 2.564336	val: 4.122996	test: 3.753584

Epoch: 13
Loss: 7.776101350784302
RMSE train: 2.827074	val: 4.217975	test: 4.552676
MAE train: 2.437359	val: 3.686114	test: 3.442498

Epoch: 14
Loss: 6.9016640186309814
RMSE train: 2.785945	val: 4.269708	test: 4.543319
MAE train: 2.394509	val: 3.778933	test: 3.468436

Epoch: 15
Loss: 6.426998138427734
RMSE train: 2.781723	val: 4.360767	test: 4.612132
MAE train: 2.388430	val: 3.933769	test: 3.615076

Epoch: 16
Loss: 5.948992013931274
RMSE train: 2.789535	val: 4.282795	test: 4.555280
MAE train: 2.411718	val: 3.861915	test: 3.652653

Epoch: 17
Loss: 5.72531533241272
RMSE train: 2.769122	val: 4.038346	test: 4.363692
MAE train: 2.416397	val: 3.587130	test: 3.554413

Epoch: 18
Loss: 5.467617750167847
RMSE train: 2.763865	val: 3.887966	test: 4.240222
MAE train: 2.425670	val: 3.435356	test: 3.506440

Epoch: 19
Loss: 4.976911783218384
RMSE train: 2.733660	val: 3.759626	test: 4.055508
MAE train: 2.419538	val: 3.328697	test: 3.420739

Epoch: 20
Loss: 5.055806398391724
RMSE train: 2.639663	val: 3.468654	test: 3.783175
MAE train: 2.347931	val: 3.006086	test: 3.199060

Epoch: 21
Loss: 4.60487961769104
RMSE train: 2.528379	val: 3.338524	test: 3.616550
MAE train: 2.261543	val: 2.913979	test: 3.040125

Epoch: 22
Loss: 4.5061421394348145
RMSE train: 2.454640	val: 3.343823	test: 3.573514
MAE train: 2.205158	val: 2.976272	test: 2.988369Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.6/freesolv_scaff_4_26-05_11-10-19  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 18.053525924682617
RMSE train: 4.184960	val: 5.779446	test: 8.229778
MAE train: 3.383806	val: 4.996792	test: 6.683529

Epoch: 2
Loss: 15.382580280303955
RMSE train: 4.020403	val: 5.694049	test: 8.041154
MAE train: 3.231581	val: 4.952152	test: 6.535433

Epoch: 3
Loss: 14.65119457244873
RMSE train: 3.838473	val: 5.627821	test: 7.830413
MAE train: 3.061807	val: 4.923903	test: 6.367181

Epoch: 4
Loss: 13.323647499084473
RMSE train: 3.656767	val: 5.610235	test: 7.594763
MAE train: 2.893897	val: 4.949378	test: 6.170222

Epoch: 5
Loss: 12.336795806884766
RMSE train: 3.467412	val: 5.531674	test: 7.261546
MAE train: 2.733869	val: 4.917967	test: 5.870420

Epoch: 6
Loss: 11.072441577911377
RMSE train: 3.281903	val: 5.320788	test: 6.797105
MAE train: 2.608588	val: 4.763228	test: 5.460069

Epoch: 7
Loss: 9.508188009262085
RMSE train: 3.095985	val: 4.916643	test: 6.203732
MAE train: 2.499393	val: 4.412621	test: 4.945892

Epoch: 8
Loss: 8.609200954437256
RMSE train: 2.954087	val: 4.538270	test: 5.610617
MAE train: 2.419233	val: 4.067936	test: 4.419761

Epoch: 9
Loss: 8.502050161361694
RMSE train: 2.862622	val: 4.221858	test: 5.143156
MAE train: 2.394883	val: 3.747333	test: 4.010999

Epoch: 10
Loss: 7.123063087463379
RMSE train: 2.735117	val: 4.008013	test: 4.838157
MAE train: 2.324849	val: 3.557638	test: 3.741988

Epoch: 11
Loss: 7.412614107131958
RMSE train: 2.639847	val: 3.830229	test: 4.625799
MAE train: 2.249386	val: 3.409264	test: 3.578702

Epoch: 12
Loss: 7.434042453765869
RMSE train: 2.537920	val: 3.649233	test: 4.447690
MAE train: 2.141621	val: 3.227581	test: 3.442460

Epoch: 13
Loss: 6.215695381164551
RMSE train: 2.427044	val: 3.431446	test: 4.279893
MAE train: 2.023629	val: 2.976040	test: 3.302527

Epoch: 14
Loss: 5.8072829246521
RMSE train: 2.352221	val: 3.164502	test: 4.103275
MAE train: 1.964128	val: 2.641490	test: 3.183173

Epoch: 15
Loss: 6.26624059677124
RMSE train: 2.305213	val: 2.966007	test: 4.032899
MAE train: 1.935268	val: 2.376411	test: 3.134294

Epoch: 16
Loss: 5.638929843902588
RMSE train: 2.323648	val: 2.960625	test: 4.059877
MAE train: 1.977110	val: 2.375967	test: 3.200093

Epoch: 17
Loss: 5.336007833480835
RMSE train: 2.362451	val: 3.115626	test: 4.146988
MAE train: 2.026628	val: 2.562618	test: 3.322551

Epoch: 18
Loss: 5.163865566253662
RMSE train: 2.347030	val: 3.218770	test: 4.206062
MAE train: 2.021391	val: 2.684846	test: 3.418154

Epoch: 19
Loss: 4.544801592826843
RMSE train: 2.296302	val: 3.159182	test: 4.115978
MAE train: 1.980128	val: 2.645002	test: 3.343718

Epoch: 20
Loss: 4.895576238632202
RMSE train: 2.240090	val: 2.932424	test: 3.957902
MAE train: 1.952678	val: 2.424367	test: 3.203394

Epoch: 21
Loss: 4.453504800796509
RMSE train: 2.203147	val: 2.754082	test: 3.810162
MAE train: 1.933263	val: 2.254330	test: 3.045795

Epoch: 22
Loss: 4.1134350299835205
RMSE train: 2.179127	val: 2.567790	test: 3.696456
MAE train: 1.918590	val: 2.066427	test: 2.934045Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.6/freesolv_scaff_6_26-05_11-10-19  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 16.786561012268066
RMSE train: 4.142584	val: 5.737737	test: 8.305348
MAE train: 3.305761	val: 4.903095	test: 6.713753

Epoch: 2
Loss: 15.922243118286133
RMSE train: 3.940018	val: 5.626245	test: 8.077667
MAE train: 3.122653	val: 4.831343	test: 6.529726

Epoch: 3
Loss: 13.977514743804932
RMSE train: 3.757474	val: 5.529105	test: 7.838420
MAE train: 2.962151	val: 4.781290	test: 6.338032

Epoch: 4
Loss: 13.267754077911377
RMSE train: 3.588925	val: 5.463652	test: 7.556810
MAE train: 2.818412	val: 4.771315	test: 6.112492

Epoch: 5
Loss: 12.986713886260986
RMSE train: 3.415142	val: 5.363030	test: 7.172951
MAE train: 2.671369	val: 4.727104	test: 5.798852

Epoch: 6
Loss: 11.729454040527344
RMSE train: 3.212206	val: 5.219274	test: 6.663510
MAE train: 2.510319	val: 4.641721	test: 5.356771

Epoch: 7
Loss: 9.412242650985718
RMSE train: 2.938423	val: 4.822041	test: 5.998152
MAE train: 2.300742	val: 4.293427	test: 4.755917

Epoch: 8
Loss: 9.2074556350708
RMSE train: 2.719513	val: 4.565774	test: 5.416435
MAE train: 2.141302	val: 4.094623	test: 4.236866

Epoch: 9
Loss: 8.677653789520264
RMSE train: 2.641984	val: 4.699211	test: 5.078969
MAE train: 2.081343	val: 4.318582	test: 4.013604

Epoch: 10
Loss: 7.659620046615601
RMSE train: 2.593400	val: 4.616282	test: 4.740395
MAE train: 2.052790	val: 4.292665	test: 3.810833

Epoch: 11
Loss: 7.929797649383545
RMSE train: 2.409060	val: 3.841252	test: 4.180607
MAE train: 1.964319	val: 3.457231	test: 3.428144

Epoch: 12
Loss: 7.6196794509887695
RMSE train: 2.298569	val: 3.259136	test: 3.840659
MAE train: 1.884297	val: 2.709346	test: 3.150463

Epoch: 13
Loss: 6.591845273971558
RMSE train: 2.302733	val: 3.302808	test: 3.861733
MAE train: 1.882914	val: 2.759672	test: 3.154162

Epoch: 14
Loss: 6.088993072509766
RMSE train: 2.338671	val: 3.619218	test: 4.057997
MAE train: 1.887755	val: 3.192192	test: 3.276348

Epoch: 15
Loss: 5.902557849884033
RMSE train: 2.359848	val: 3.835766	test: 4.237367
MAE train: 1.891279	val: 3.479751	test: 3.410344

Epoch: 16
Loss: 5.390896797180176
RMSE train: 2.302682	val: 3.733804	test: 4.150068
MAE train: 1.881099	val: 3.389227	test: 3.328811

Epoch: 17
Loss: 5.581473350524902
RMSE train: 2.251560	val: 3.535281	test: 4.010496
MAE train: 1.914917	val: 3.186267	test: 3.207175

Epoch: 18
Loss: 5.019538640975952
RMSE train: 2.228505	val: 3.176882	test: 3.797246
MAE train: 1.964182	val: 2.766213	test: 3.009482

Epoch: 19
Loss: 4.564701795578003
RMSE train: 2.252873	val: 2.937417	test: 3.581668
MAE train: 2.003692	val: 2.487040	test: 2.822351

Epoch: 20
Loss: 4.766700029373169
RMSE train: 2.326243	val: 3.094097	test: 3.620347
MAE train: 2.065396	val: 2.686353	test: 2.877296

Epoch: 21
Loss: 4.420390367507935
RMSE train: 2.406987	val: 3.358159	test: 3.711891
MAE train: 2.105036	val: 3.002691	test: 2.972529

Epoch: 22
Loss: 4.238685131072998
RMSE train: 2.410152	val: 3.490598	test: 3.790351
MAE train: 2.068299	val: 3.154993	test: 3.052351Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.7/freesolv_scaff_5_26-05_11-10-19  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 27.737768173217773
RMSE train: 5.220764	val: 5.870450	test: 7.057434
MAE train: 3.972401	val: 5.016463	test: 6.047931

Epoch: 2
Loss: 25.875571250915527
RMSE train: 5.067701	val: 5.819486	test: 6.902724
MAE train: 3.860344	val: 5.027380	test: 5.996930

Epoch: 3
Loss: 24.597886085510254
RMSE train: 4.897271	val: 5.766521	test: 6.684951
MAE train: 3.742179	val: 5.022792	test: 5.887639

Epoch: 4
Loss: 22.110679626464844
RMSE train: 4.764114	val: 5.858855	test: 6.557583
MAE train: 3.661172	val: 5.175523	test: 5.857571

Epoch: 5
Loss: 19.97292709350586
RMSE train: 4.615740	val: 6.003171	test: 6.431892
MAE train: 3.583135	val: 5.394172	test: 5.801311

Epoch: 6
Loss: 18.7130069732666
RMSE train: 4.444063	val: 6.148591	test: 6.317768
MAE train: 3.507161	val: 5.625334	test: 5.672249

Epoch: 7
Loss: 16.406639099121094
RMSE train: 4.230347	val: 6.228374	test: 6.207464
MAE train: 3.410950	val: 5.787247	test: 5.417658

Epoch: 8
Loss: 14.391368865966797
RMSE train: 4.019305	val: 6.205440	test: 6.126813
MAE train: 3.335472	val: 5.838760	test: 5.253588

Epoch: 9
Loss: 13.237799167633057
RMSE train: 3.812979	val: 5.887566	test: 5.884370
MAE train: 3.213244	val: 5.582753	test: 5.110910

Epoch: 10
Loss: 12.649960041046143
RMSE train: 3.658444	val: 5.382593	test: 5.531170
MAE train: 3.153060	val: 5.114009	test: 4.816327

Epoch: 11
Loss: 12.2381591796875
RMSE train: 3.544659	val: 4.980493	test: 5.242720
MAE train: 3.076464	val: 4.723958	test: 4.535490

Epoch: 12
Loss: 11.698259353637695
RMSE train: 3.437710	val: 4.816698	test: 5.101997
MAE train: 2.990508	val: 4.561061	test: 4.493162

Epoch: 13
Loss: 11.007647514343262
RMSE train: 3.444061	val: 5.028657	test: 5.203748
MAE train: 2.982806	val: 4.780784	test: 4.676743

Epoch: 14
Loss: 10.374030113220215
RMSE train: 3.536674	val: 5.479100	test: 5.498644
MAE train: 3.020547	val: 5.252681	test: 4.976482

Epoch: 15
Loss: 10.036687850952148
RMSE train: 3.499813	val: 5.203032	test: 5.075956
MAE train: 3.011492	val: 4.920502	test: 4.582205

Epoch: 16
Loss: 9.489863872528076
RMSE train: 3.448148	val: 4.877806	test: 4.719128
MAE train: 2.991508	val: 4.552799	test: 4.234633

Epoch: 17
Loss: 9.153096199035645
RMSE train: 3.429556	val: 4.812701	test: 4.729151
MAE train: 2.977829	val: 4.512610	test: 4.261233

Epoch: 18
Loss: 8.784524917602539
RMSE train: 3.402126	val: 4.813030	test: 4.836215
MAE train: 2.960809	val: 4.551544	test: 4.375616

Epoch: 19
Loss: 8.395114183425903
RMSE train: 3.358864	val: 4.687432	test: 4.810584
MAE train: 2.944029	val: 4.432489	test: 4.346055

Epoch: 20
Loss: 7.957194566726685
RMSE train: 3.292932	val: 4.425502	test: 4.666121
MAE train: 2.910622	val: 4.151256	test: 4.191346

Epoch: 21
Loss: 7.820194721221924
RMSE train: 3.245338	val: 4.229604	test: 4.546772
MAE train: 2.890514	val: 3.934957	test: 4.048245

Epoch: 22
Loss: 7.0367820262908936
RMSE train: 3.205385	val: 4.195604	test: 4.548975
MAE train: 2.860175	val: 3.910753	test: 4.039241Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.7/freesolv_scaff_6_26-05_11-10-19  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.890612602233887
RMSE train: 5.128654	val: 5.744199	test: 7.118681
MAE train: 3.860062	val: 4.907736	test: 6.077800

Epoch: 2
Loss: 25.082226753234863
RMSE train: 4.935387	val: 5.662731	test: 6.963913
MAE train: 3.691759	val: 4.865565	test: 5.979651

Epoch: 3
Loss: 22.854808807373047
RMSE train: 4.743377	val: 5.612887	test: 6.801852
MAE train: 3.546334	val: 4.859372	test: 5.880277

Epoch: 4
Loss: 21.365124702453613
RMSE train: 4.530131	val: 5.581043	test: 6.573991
MAE train: 3.416865	val: 4.874423	test: 5.728662

Epoch: 5
Loss: 19.692907333374023
RMSE train: 4.294663	val: 5.545053	test: 6.251439
MAE train: 3.291682	val: 4.887453	test: 5.483932

Epoch: 6
Loss: 17.88609218597412
RMSE train: 4.021771	val: 5.433973	test: 5.849523
MAE train: 3.171027	val: 4.826422	test: 5.140312

Epoch: 7
Loss: 15.945124626159668
RMSE train: 3.693044	val: 5.281849	test: 5.453231
MAE train: 2.987997	val: 4.737413	test: 4.780582

Epoch: 8
Loss: 13.624356269836426
RMSE train: 3.370183	val: 5.136583	test: 5.189790
MAE train: 2.735913	val: 4.670386	test: 4.515844

Epoch: 9
Loss: 12.439197540283203
RMSE train: 3.181981	val: 4.958575	test: 5.142668
MAE train: 2.568087	val: 4.564986	test: 4.581806

Epoch: 10
Loss: 11.896557331085205
RMSE train: 3.115411	val: 4.615163	test: 5.165689
MAE train: 2.555005	val: 4.279304	test: 4.581667

Epoch: 11
Loss: 11.528275966644287
RMSE train: 3.096308	val: 4.264271	test: 5.140761
MAE train: 2.563659	val: 3.951848	test: 4.514328

Epoch: 12
Loss: 11.152371406555176
RMSE train: 3.013832	val: 3.766633	test: 4.816910
MAE train: 2.521033	val: 3.382131	test: 4.184166

Epoch: 13
Loss: 10.520017623901367
RMSE train: 3.043957	val: 3.682212	test: 4.683571
MAE train: 2.533841	val: 3.197452	test: 4.032714

Epoch: 14
Loss: 10.338855266571045
RMSE train: 3.164396	val: 4.080882	test: 4.848402
MAE train: 2.573669	val: 3.612467	test: 4.257480

Epoch: 15
Loss: 10.03941535949707
RMSE train: 3.245630	val: 4.446482	test: 4.991937
MAE train: 2.610087	val: 4.035790	test: 4.409254

Epoch: 16
Loss: 9.416913032531738
RMSE train: 3.235943	val: 4.405291	test: 4.844464
MAE train: 2.665973	val: 4.020665	test: 4.267576

Epoch: 17
Loss: 8.719028949737549
RMSE train: 3.180682	val: 4.346860	test: 4.735004
MAE train: 2.688790	val: 4.009157	test: 4.176417

Epoch: 18
Loss: 8.281390190124512
RMSE train: 3.121586	val: 4.304235	test: 4.704871
MAE train: 2.688940	val: 4.009677	test: 4.174576

Epoch: 19
Loss: 7.936352968215942
RMSE train: 3.034650	val: 4.169734	test: 4.601547
MAE train: 2.650418	val: 3.886459	test: 4.105228

Epoch: 20
Loss: 7.785093545913696
RMSE train: 2.944553	val: 4.076354	test: 4.534407
MAE train: 2.584131	val: 3.794665	test: 4.053849

Epoch: 21
Loss: 7.627138614654541
RMSE train: 2.841473	val: 3.850576	test: 4.301031
MAE train: 2.506488	val: 3.538545	test: 3.819800

Epoch: 22
Loss: 7.317718029022217
RMSE train: 2.807081	val: 3.700644	test: 4.092680
MAE train: 2.474410	val: 3.328467	test: 3.566508Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.7/freesolv_scaff_4_26-05_11-10-19  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.56770133972168
RMSE train: 5.111428	val: 5.812620	test: 7.161986
MAE train: 3.902463	val: 5.037632	test: 6.212988

Epoch: 2
Loss: 24.233716011047363
RMSE train: 4.936180	val: 5.743712	test: 7.002249
MAE train: 3.757818	val: 5.012505	test: 6.120445

Epoch: 3
Loss: 22.612722396850586
RMSE train: 4.732262	val: 5.683654	test: 6.808658
MAE train: 3.600178	val: 4.993313	test: 6.002844

Epoch: 4
Loss: 20.638035774230957
RMSE train: 4.508397	val: 5.657041	test: 6.581244
MAE train: 3.454909	val: 5.015461	test: 5.865155

Epoch: 5
Loss: 19.088847160339355
RMSE train: 4.277689	val: 5.665895	test: 6.319094
MAE train: 3.336929	val: 5.081345	test: 5.677097

Epoch: 6
Loss: 17.429895877838135
RMSE train: 4.063937	val: 5.632337	test: 6.000498
MAE train: 3.265078	val: 5.117590	test: 5.394353

Epoch: 7
Loss: 15.178912162780762
RMSE train: 3.867339	val: 5.529931	test: 5.674937
MAE train: 3.207465	val: 5.087478	test: 5.008420

Epoch: 8
Loss: 13.492475509643555
RMSE train: 3.744458	val: 5.451980	test: 5.518291
MAE train: 3.166061	val: 5.073983	test: 4.794767

Epoch: 9
Loss: 12.849567890167236
RMSE train: 3.653613	val: 5.263137	test: 5.445898
MAE train: 3.126007	val: 4.943645	test: 4.784169

Epoch: 10
Loss: 12.484757423400879
RMSE train: 3.582982	val: 5.031261	test: 5.425942
MAE train: 3.109816	val: 4.753217	test: 4.809178

Epoch: 11
Loss: 11.542170524597168
RMSE train: 3.512883	val: 4.831678	test: 5.245209
MAE train: 3.078491	val: 4.576520	test: 4.633288

Epoch: 12
Loss: 10.94771957397461
RMSE train: 3.421458	val: 4.558013	test: 4.792745
MAE train: 3.033738	val: 4.290896	test: 4.161359

Epoch: 13
Loss: 10.546620845794678
RMSE train: 3.327414	val: 4.282926	test: 4.396345
MAE train: 2.962671	val: 3.973868	test: 3.760790

Epoch: 14
Loss: 9.926881790161133
RMSE train: 3.384144	val: 4.684472	test: 4.639591
MAE train: 2.993328	val: 4.421361	test: 4.053520

Epoch: 15
Loss: 9.484848022460938
RMSE train: 3.399184	val: 4.723195	test: 4.626689
MAE train: 3.010048	val: 4.435651	test: 4.026732

Epoch: 16
Loss: 8.885824203491211
RMSE train: 3.435735	val: 4.838000	test: 4.800462
MAE train: 3.042683	val: 4.540980	test: 4.201502

Epoch: 17
Loss: 8.658094882965088
RMSE train: 3.363090	val: 4.635366	test: 4.694507
MAE train: 2.980231	val: 4.312482	test: 4.092941

Epoch: 18
Loss: 7.9692909717559814
RMSE train: 3.282007	val: 4.551511	test: 4.770017
MAE train: 2.891383	val: 4.230170	test: 4.175351

Epoch: 19
Loss: 8.348325967788696
RMSE train: 3.207889	val: 4.419026	test: 4.758027
MAE train: 2.820497	val: 4.095427	test: 4.149282

Epoch: 20
Loss: 7.441272497177124
RMSE train: 3.140611	val: 4.275838	test: 4.704115
MAE train: 2.770356	val: 3.956189	test: 4.095639

Epoch: 21
Loss: 7.6115641593933105
RMSE train: 3.086515	val: 4.104662	test: 4.647413
MAE train: 2.734407	val: 3.782610	test: 4.054662

Epoch: 22
Loss: 6.804619312286377
RMSE train: 3.009079	val: 3.843482	test: 4.470556
MAE train: 2.683394	val: 3.507143	test: 3.862554Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.8/freesolv_scaff_5_26-05_11-10-19  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 22.109566688537598
RMSE train: 4.723914	val: 8.618840	test: 6.777844
MAE train: 3.803383	val: 6.712730	test: 5.935927

Epoch: 2
Loss: 20.859050750732422
RMSE train: 4.617492	val: 8.400435	test: 6.617365
MAE train: 3.726774	val: 6.540169	test: 5.832193

Epoch: 3
Loss: 19.56767749786377
RMSE train: 4.501599	val: 8.156489	test: 6.439609
MAE train: 3.652551	val: 6.339238	test: 5.708107

Epoch: 4
Loss: 18.29052734375
RMSE train: 4.422776	val: 7.944817	test: 6.271673
MAE train: 3.609357	val: 6.200945	test: 5.604373

Epoch: 5
Loss: 16.779257774353027
RMSE train: 4.382219	val: 7.728323	test: 6.104893
MAE train: 3.596710	val: 6.100048	test: 5.504800

Epoch: 6
Loss: 15.47153902053833
RMSE train: 4.320578	val: 7.399948	test: 5.911300
MAE train: 3.582511	val: 5.991598	test: 5.335041

Epoch: 7
Loss: 14.025411605834961
RMSE train: 4.196790	val: 6.874184	test: 5.666553
MAE train: 3.579067	val: 5.725205	test: 5.036207

Epoch: 8
Loss: 12.546755313873291
RMSE train: 4.020952	val: 6.236784	test: 5.429867
MAE train: 3.544599	val: 5.307164	test: 4.724285

Epoch: 9
Loss: 11.825750827789307
RMSE train: 3.897107	val: 5.674524	test: 5.294680
MAE train: 3.481974	val: 4.968006	test: 4.549411

Epoch: 10
Loss: 11.231334209442139
RMSE train: 3.834931	val: 5.375418	test: 5.262105
MAE train: 3.438704	val: 4.766445	test: 4.515007

Epoch: 11
Loss: 10.462515830993652
RMSE train: 3.696544	val: 5.119145	test: 5.114905
MAE train: 3.319466	val: 4.543489	test: 4.427104

Epoch: 12
Loss: 10.006024837493896
RMSE train: 3.569054	val: 4.987366	test: 4.974047
MAE train: 3.215735	val: 4.459354	test: 4.354496

Epoch: 13
Loss: 9.48823881149292
RMSE train: 3.515492	val: 5.011760	test: 4.869641
MAE train: 3.190250	val: 4.507885	test: 4.274722

Epoch: 14
Loss: 9.092220306396484
RMSE train: 3.524135	val: 5.222830	test: 4.852878
MAE train: 3.204344	val: 4.749228	test: 4.260924

Epoch: 15
Loss: 8.606754302978516
RMSE train: 3.474747	val: 5.253756	test: 4.739226
MAE train: 3.146755	val: 4.797557	test: 4.174568

Epoch: 16
Loss: 8.379767894744873
RMSE train: 3.404040	val: 5.100441	test: 4.543404
MAE train: 3.083180	val: 4.669697	test: 3.995691

Epoch: 17
Loss: 7.8483476638793945
RMSE train: 3.323894	val: 4.822759	test: 4.315886
MAE train: 3.024098	val: 4.397287	test: 3.772796

Epoch: 18
Loss: 7.501389503479004
RMSE train: 3.271000	val: 4.567509	test: 4.172235
MAE train: 2.991011	val: 4.149139	test: 3.633794

Epoch: 19
Loss: 7.227668762207031
RMSE train: 3.198348	val: 4.327330	test: 4.050708
MAE train: 2.934437	val: 3.923612	test: 3.526135

Epoch: 20
Loss: 6.914966106414795
RMSE train: 3.115788	val: 4.138663	test: 3.950171
MAE train: 2.863566	val: 3.752281	test: 3.432549

Epoch: 21
Loss: 6.39947772026062
RMSE train: 3.035051	val: 4.065555	test: 3.879594
MAE train: 2.778624	val: 3.682967	test: 3.355301

Epoch: 22
Loss: 6.198619604110718
RMSE train: 2.935685	val: 3.997417	test: 3.799415
MAE train: 2.680151	val: 3.609684	test: 3.266044Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.8/freesolv_scaff_6_26-05_11-10-19  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.539511680603027
RMSE train: 4.617043	val: 8.711007	test: 6.813302
MAE train: 3.707650	val: 6.785064	test: 5.955491

Epoch: 2
Loss: 20.21531105041504
RMSE train: 4.450207	val: 8.484157	test: 6.640836
MAE train: 3.570218	val: 6.623384	test: 5.826094

Epoch: 3
Loss: 18.78367805480957
RMSE train: 4.295564	val: 8.265117	test: 6.502994
MAE train: 3.450556	val: 6.460928	test: 5.732777

Epoch: 4
Loss: 17.738056182861328
RMSE train: 4.166345	val: 8.037662	test: 6.385461
MAE train: 3.347726	val: 6.298808	test: 5.684662

Epoch: 5
Loss: 16.23605442047119
RMSE train: 4.068642	val: 7.791039	test: 6.274632
MAE train: 3.268716	val: 6.199477	test: 5.637112

Epoch: 6
Loss: 14.847709655761719
RMSE train: 3.977927	val: 7.463023	test: 6.121548
MAE train: 3.198099	val: 6.104788	test: 5.522449

Epoch: 7
Loss: 13.715663433074951
RMSE train: 3.845052	val: 6.935150	test: 5.873372
MAE train: 3.116617	val: 5.809893	test: 5.296501

Epoch: 8
Loss: 12.300909996032715
RMSE train: 3.603053	val: 6.109493	test: 5.494780
MAE train: 2.985864	val: 5.203142	test: 4.908959

Epoch: 9
Loss: 11.162339210510254
RMSE train: 3.397778	val: 5.313411	test: 5.183767
MAE train: 2.887140	val: 4.625382	test: 4.536740

Epoch: 10
Loss: 10.710955142974854
RMSE train: 3.212777	val: 4.717554	test: 4.962438
MAE train: 2.768480	val: 4.142428	test: 4.270997

Epoch: 11
Loss: 10.530266284942627
RMSE train: 3.035770	val: 4.263615	test: 4.751209
MAE train: 2.611785	val: 3.727523	test: 4.081556

Epoch: 12
Loss: 9.893056869506836
RMSE train: 2.893278	val: 3.984718	test: 4.525068
MAE train: 2.478959	val: 3.493834	test: 3.841128

Epoch: 13
Loss: 9.419381618499756
RMSE train: 2.760411	val: 3.698796	test: 4.157970
MAE train: 2.377725	val: 3.274019	test: 3.489975

Epoch: 14
Loss: 8.893859624862671
RMSE train: 2.735479	val: 3.606589	test: 3.900271
MAE train: 2.383295	val: 3.188932	test: 3.251103

Epoch: 15
Loss: 8.471982955932617
RMSE train: 2.761839	val: 3.588292	test: 3.671809
MAE train: 2.433354	val: 3.116796	test: 3.045875

Epoch: 16
Loss: 7.957977533340454
RMSE train: 2.895247	val: 3.781934	test: 3.711716
MAE train: 2.599719	val: 3.255492	test: 3.082851

Epoch: 17
Loss: 7.817797660827637
RMSE train: 2.961182	val: 3.864776	test: 3.678802
MAE train: 2.673713	val: 3.310485	test: 3.051272

Epoch: 18
Loss: 7.414566278457642
RMSE train: 2.926681	val: 3.798814	test: 3.518036
MAE train: 2.646325	val: 3.209872	test: 2.922081

Epoch: 19
Loss: 7.136986017227173
RMSE train: 2.883578	val: 3.703889	test: 3.416385
MAE train: 2.613213	val: 3.127855	test: 2.840873

Epoch: 20
Loss: 6.760433197021484
RMSE train: 2.839333	val: 3.671341	test: 3.409257
MAE train: 2.567027	val: 3.129506	test: 2.850465

Epoch: 21
Loss: 6.2658140659332275
RMSE train: 2.826532	val: 3.668538	test: 3.508646
MAE train: 2.549327	val: 3.177423	test: 2.952062

Epoch: 22
Loss: 6.190982818603516
RMSE train: 2.713774	val: 3.509403	test: 3.505365
MAE train: 2.440497	val: 3.052580	test: 2.953016Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/scaff/train_prop=0.8/freesolv_scaff_4_26-05_11-10-19  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.9195499420166
RMSE train: 4.669626	val: 8.647020	test: 6.800348
MAE train: 3.800022	val: 6.825265	test: 5.988458

Epoch: 2
Loss: 19.600780487060547
RMSE train: 4.540110	val: 8.426430	test: 6.611444
MAE train: 3.693621	val: 6.666275	test: 5.841420

Epoch: 3
Loss: 18.293015003204346
RMSE train: 4.414775	val: 8.196079	test: 6.413100
MAE train: 3.590640	val: 6.511776	test: 5.687077

Epoch: 4
Loss: 17.00257682800293
RMSE train: 4.271745	val: 7.901878	test: 6.188796
MAE train: 3.485284	val: 6.300221	test: 5.501415

Epoch: 5
Loss: 15.54947566986084
RMSE train: 4.110298	val: 7.525999	test: 5.956626
MAE train: 3.392874	val: 6.013142	test: 5.280836

Epoch: 6
Loss: 14.250410556793213
RMSE train: 3.938325	val: 7.069875	test: 5.737585
MAE train: 3.323994	val: 5.668999	test: 5.046438

Epoch: 7
Loss: 12.732169151306152
RMSE train: 3.815543	val: 6.623361	test: 5.566477
MAE train: 3.279692	val: 5.403304	test: 4.856784

Epoch: 8
Loss: 11.666166305541992
RMSE train: 3.689226	val: 6.164277	test: 5.390139
MAE train: 3.219632	val: 5.152886	test: 4.676383

Epoch: 9
Loss: 10.880319118499756
RMSE train: 3.542782	val: 5.790171	test: 5.216370
MAE train: 3.130931	val: 4.951752	test: 4.489237

Epoch: 10
Loss: 10.251869201660156
RMSE train: 3.402219	val: 5.499482	test: 5.025988
MAE train: 3.030063	val: 4.767973	test: 4.268302

Epoch: 11
Loss: 9.869314193725586
RMSE train: 3.298625	val: 5.376491	test: 4.853273
MAE train: 2.947713	val: 4.723468	test: 4.108096

Epoch: 12
Loss: 9.601483821868896
RMSE train: 3.210266	val: 5.300258	test: 4.756034
MAE train: 2.868175	val: 4.683816	test: 4.019977

Epoch: 13
Loss: 9.103241920471191
RMSE train: 3.110182	val: 5.226154	test: 4.635038
MAE train: 2.764879	val: 4.617125	test: 3.906735

Epoch: 14
Loss: 8.528226613998413
RMSE train: 3.011634	val: 5.181284	test: 4.516225
MAE train: 2.677013	val: 4.550627	test: 3.817883

Epoch: 15
Loss: 8.269048690795898
RMSE train: 2.992177	val: 5.278858	test: 4.482588
MAE train: 2.674351	val: 4.601025	test: 3.815604

Epoch: 16
Loss: 7.827701568603516
RMSE train: 2.980250	val: 5.269516	test: 4.419381
MAE train: 2.664572	val: 4.534689	test: 3.789119

Epoch: 17
Loss: 7.470459699630737
RMSE train: 2.863653	val: 5.042212	test: 4.256140
MAE train: 2.555595	val: 4.293860	test: 3.631917

Epoch: 18
Loss: 6.992230415344238
RMSE train: 2.747746	val: 4.765668	test: 4.090587
MAE train: 2.439892	val: 4.006874	test: 3.473131

Epoch: 19
Loss: 6.6476826667785645
RMSE train: 2.652159	val: 4.492172	test: 3.921833
MAE train: 2.335280	val: 3.700643	test: 3.299321

Epoch: 20
Loss: 6.4182963371276855
RMSE train: 2.623679	val: 4.393039	test: 3.834485
MAE train: 2.320402	val: 3.573809	test: 3.204056

Epoch: 21
Loss: 6.060683965682983
RMSE train: 2.614222	val: 4.442786	test: 3.801529
MAE train: 2.339417	val: 3.620331	test: 3.154175

Epoch: 22
Loss: 5.506244897842407
RMSE train: 2.567516	val: 4.461800	test: 3.736074
MAE train: 2.299270	val: 3.631813	test: 3.065093

Epoch: 23
Loss: 3.8775748014450073
RMSE train: 2.143092	val: 2.490739	test: 3.668522
MAE train: 1.878793	val: 1.991634	test: 2.915364

Epoch: 24
Loss: 3.7138932943344116
RMSE train: 2.100496	val: 2.531669	test: 3.721162
MAE train: 1.830055	val: 2.041971	test: 2.973828

Epoch: 25
Loss: 3.3168152570724487
RMSE train: 2.064503	val: 2.549421	test: 3.765409
MAE train: 1.785359	val: 2.065019	test: 3.016075

Epoch: 26
Loss: 3.1427700519561768
RMSE train: 2.006807	val: 2.455821	test: 3.701180
MAE train: 1.736379	val: 1.969541	test: 2.970749

Epoch: 27
Loss: 3.1054755449295044
RMSE train: 1.930960	val: 2.328769	test: 3.628501
MAE train: 1.685390	val: 1.810633	test: 2.905692

Epoch: 28
Loss: 3.0377511978149414
RMSE train: 1.917783	val: 2.253306	test: 3.644599
MAE train: 1.683059	val: 1.705771	test: 2.930745

Epoch: 29
Loss: 2.598949670791626
RMSE train: 1.911898	val: 2.375196	test: 3.723461
MAE train: 1.663849	val: 1.868229	test: 3.004569

Epoch: 30
Loss: 2.611488938331604
RMSE train: 1.894737	val: 2.489767	test: 3.741507
MAE train: 1.628338	val: 2.023052	test: 3.018845

Epoch: 31
Loss: 2.4454468488693237
RMSE train: 1.852059	val: 2.408518	test: 3.549946
MAE train: 1.603205	val: 1.958334	test: 2.857639

Epoch: 32
Loss: 2.302218437194824
RMSE train: 1.760300	val: 2.094397	test: 3.195061
MAE train: 1.551531	val: 1.628570	test: 2.530944

Epoch: 33
Loss: 2.0989094972610474
RMSE train: 1.703650	val: 1.933033	test: 3.057132
MAE train: 1.516494	val: 1.460929	test: 2.392983

Epoch: 34
Loss: 2.1579554677009583
RMSE train: 1.637010	val: 1.879164	test: 2.986576
MAE train: 1.449578	val: 1.421192	test: 2.330170

Epoch: 35
Loss: 1.8119395971298218
RMSE train: 1.553358	val: 1.883651	test: 2.933487
MAE train: 1.360290	val: 1.441871	test: 2.276252

Epoch: 36
Loss: 1.7510003447532654
RMSE train: 1.490492	val: 1.916624	test: 2.971841
MAE train: 1.279064	val: 1.473252	test: 2.297973

Epoch: 37
Loss: 1.7336028814315796
RMSE train: 1.455062	val: 1.889636	test: 3.010619
MAE train: 1.232899	val: 1.421654	test: 2.331468

Epoch: 38
Loss: 1.5799851417541504
RMSE train: 1.452389	val: 1.819925	test: 3.018756
MAE train: 1.230154	val: 1.321658	test: 2.331568

Epoch: 39
Loss: 1.4487180709838867
RMSE train: 1.410547	val: 1.733495	test: 2.951872
MAE train: 1.195819	val: 1.230491	test: 2.272832

Epoch: 40
Loss: 1.4951998591423035
RMSE train: 1.382976	val: 1.780618	test: 3.043024
MAE train: 1.173026	val: 1.267666	test: 2.341954

Epoch: 41
Loss: 1.2918586134910583
RMSE train: 1.338510	val: 1.855481	test: 3.063584
MAE train: 1.128422	val: 1.347894	test: 2.334839

Epoch: 42
Loss: 1.3276264071464539
RMSE train: 1.273903	val: 1.864526	test: 3.044134
MAE train: 1.058850	val: 1.358276	test: 2.285285

Epoch: 43
Loss: 1.289691984653473
RMSE train: 1.187000	val: 1.730519	test: 2.993584
MAE train: 1.005393	val: 1.214925	test: 2.202841

Epoch: 44
Loss: 1.1989021301269531
RMSE train: 1.142194	val: 1.563144	test: 2.891046
MAE train: 0.985521	val: 1.084895	test: 2.093120

Epoch: 45
Loss: 1.02693510055542
RMSE train: 1.094264	val: 1.526546	test: 2.790328
MAE train: 0.937780	val: 1.079195	test: 2.014939

Epoch: 46
Loss: 0.8659326434135437
RMSE train: 1.033214	val: 1.556978	test: 2.801110
MAE train: 0.851996	val: 1.090349	test: 2.016719

Epoch: 47
Loss: 0.9591272473335266
RMSE train: 1.005595	val: 1.602641	test: 2.870259
MAE train: 0.786517	val: 1.115481	test: 2.048432

Epoch: 48
Loss: 1.206140398979187
RMSE train: 0.998940	val: 1.604549	test: 2.883346
MAE train: 0.768788	val: 1.133512	test: 2.057336

Epoch: 49
Loss: 0.8273878693580627
RMSE train: 0.919868	val: 1.527388	test: 2.753617
MAE train: 0.708438	val: 1.106331	test: 1.968907

Epoch: 50
Loss: 1.0011522769927979
RMSE train: 0.851165	val: 1.473879	test: 2.633404
MAE train: 0.669326	val: 1.077988	test: 1.891471

Epoch: 51
Loss: 0.8625394105911255
RMSE train: 0.769974	val: 1.414639	test: 2.515702
MAE train: 0.623810	val: 1.040416	test: 1.825250

Epoch: 52
Loss: 1.125579833984375
RMSE train: 0.773891	val: 1.394020	test: 2.536490
MAE train: 0.648289	val: 1.034215	test: 1.850515

Epoch: 53
Loss: 0.738474428653717
RMSE train: 0.777077	val: 1.395910	test: 2.572106
MAE train: 0.651924	val: 1.038616	test: 1.868728

Epoch: 54
Loss: 0.6882314383983612
RMSE train: 0.754486	val: 1.394385	test: 2.619215
MAE train: 0.610934	val: 1.033340	test: 1.876693

Epoch: 55
Loss: 0.776263564825058
RMSE train: 0.779718	val: 1.402977	test: 2.710190
MAE train: 0.613991	val: 1.022896	test: 1.930231

Epoch: 56
Loss: 0.7374452650547028
RMSE train: 0.772469	val: 1.396406	test: 2.691595
MAE train: 0.594633	val: 1.021292	test: 1.912381

Epoch: 57
Loss: 0.7582245469093323
RMSE train: 0.725121	val: 1.386318	test: 2.560287
MAE train: 0.558807	val: 1.047190	test: 1.835054

Epoch: 58
Loss: 0.8721983134746552
RMSE train: 0.703086	val: 1.403882	test: 2.484449
MAE train: 0.551740	val: 1.090698	test: 1.817784

Epoch: 59
Loss: 0.7541985809803009
RMSE train: 0.708403	val: 1.367420	test: 2.443207
MAE train: 0.567529	val: 1.045007	test: 1.811677

Epoch: 60
Loss: 0.7385627031326294
RMSE train: 0.722188	val: 1.359501	test: 2.492896
MAE train: 0.573975	val: 1.025711	test: 1.854304

Epoch: 61
Loss: 0.8115555047988892
RMSE train: 0.734022	val: 1.369408	test: 2.558924
MAE train: 0.570827	val: 1.017748	test: 1.893929

Epoch: 62
Loss: 0.7674162983894348
RMSE train: 0.743800	val: 1.376150	test: 2.640353
MAE train: 0.581628	val: 1.003800	test: 1.949888

Epoch: 63
Loss: 0.8201931715011597
RMSE train: 0.702103	val: 1.401380	test: 2.638845
MAE train: 0.551257	val: 1.030433	test: 1.935687

Epoch: 64
Loss: 0.68772953748703
RMSE train: 0.653437	val: 1.454803	test: 2.583395
MAE train: 0.505135	val: 1.102199	test: 1.870607

Epoch: 65
Loss: 0.7112255394458771
RMSE train: 0.626519	val: 1.441429	test: 2.599577
MAE train: 0.479727	val: 1.110579	test: 1.854699

Epoch: 66
Loss: 0.7148543298244476
RMSE train: 0.613843	val: 1.414526	test: 2.630125
MAE train: 0.473618	val: 1.102804	test: 1.858243

Epoch: 67
Loss: 1.0201915800571442
RMSE train: 0.629250	val: 1.408447	test: 2.620107
MAE train: 0.492038	val: 1.107760	test: 1.858530

Epoch: 68
Loss: 0.6838651299476624
RMSE train: 0.663626	val: 1.390371	test: 2.629972
MAE train: 0.530214	val: 1.079616	test: 1.870353

Epoch: 69
Loss: 1.0317109823226929
RMSE train: 0.694084	val: 1.380893	test: 2.650697
MAE train: 0.563107	val: 1.043534	test: 1.897193

Epoch: 70
Loss: 0.6015813648700714
RMSE train: 0.726419	val: 1.382669	test: 2.748778
MAE train: 0.561960	val: 1.026381	test: 1.920083

Epoch: 71
Loss: 0.5448046624660492
RMSE train: 0.791207	val: 1.408870	test: 2.830972
MAE train: 0.564662	val: 1.051137	test: 1.922003

Epoch: 72
Loss: 0.6072144508361816
RMSE train: 0.824150	val: 1.417306	test: 2.853881
MAE train: 0.557666	val: 1.054153	test: 1.908324

Epoch: 73
Loss: 0.7376185059547424
RMSE train: 0.799226	val: 1.409024	test: 2.816421
MAE train: 0.547188	val: 1.047110	test: 1.887536

Epoch: 74
Loss: 0.6018936932086945
RMSE train: 0.770375	val: 1.413057	test: 2.822784
MAE train: 0.561953	val: 1.051992	test: 1.909948

Epoch: 75
Loss: 0.6087005734443665
RMSE train: 0.744593	val: 1.407732	test: 2.784960
MAE train: 0.566837	val: 1.049893	test: 1.906792

Epoch: 76
Loss: 0.6485060751438141
RMSE train: 0.745439	val: 1.403973	test: 2.741393
MAE train: 0.590940	val: 1.031309	test: 1.916412

Epoch: 77
Loss: 0.5991374850273132
RMSE train: 0.747856	val: 1.390384	test: 2.663707
MAE train: 0.595214	val: 1.012237	test: 1.913141

Epoch: 78
Loss: 0.7854831516742706
RMSE train: 0.705659	val: 1.396502	test: 2.577596
MAE train: 0.556593	val: 1.047620	test: 1.873419

Epoch: 79
Loss: 0.6553473770618439
RMSE train: 0.685725	val: 1.391547	test: 2.603354
MAE train: 0.537162	val: 1.041628	test: 1.879113

Epoch: 80
Loss: 0.6145209968090057
RMSE train: 0.649414	val: 1.387450	test: 2.628313
MAE train: 0.501753	val: 1.036689	test: 1.876100

Epoch: 81
Loss: 0.7360330522060394
RMSE train: 0.638063	val: 1.389002	test: 2.631572
MAE train: 0.490708	val: 1.041381	test: 1.865922

Epoch: 82
Loss: 0.5544198304414749
RMSE train: 0.632694	val: 1.388968	test: 2.613655
MAE train: 0.488939	val: 1.048699	test: 1.835426

Epoch: 83
Loss: 0.7966725826263428
RMSE train: 0.639422	val: 1.417106	test: 2.681042
MAE train: 0.491867	val: 1.089872	test: 1.847211

Epoch: 23
Loss: 3.829810619354248
RMSE train: 2.347997	val: 3.168579	test: 3.402658
MAE train: 2.107720	val: 2.815255	test: 2.821554

Epoch: 24
Loss: 4.109559535980225
RMSE train: 2.216527	val: 2.726429	test: 3.108031
MAE train: 1.973561	val: 2.281464	test: 2.497830

Epoch: 25
Loss: 3.6285113096237183
RMSE train: 2.076300	val: 2.280477	test: 2.865115
MAE train: 1.829722	val: 1.704136	test: 2.261147

Epoch: 26
Loss: 3.708011031150818
RMSE train: 2.046707	val: 2.271325	test: 2.861656
MAE train: 1.803397	val: 1.765801	test: 2.265642

Epoch: 27
Loss: 3.291658401489258
RMSE train: 2.029149	val: 2.418186	test: 2.980648
MAE train: 1.785791	val: 2.021697	test: 2.390162

Epoch: 28
Loss: 3.015672445297241
RMSE train: 2.003886	val: 2.569516	test: 3.079867
MAE train: 1.751895	val: 2.232829	test: 2.472930

Epoch: 29
Loss: 2.8491750955581665
RMSE train: 1.963162	val: 2.519090	test: 3.077263
MAE train: 1.708755	val: 2.159573	test: 2.419558

Epoch: 30
Loss: 2.5810035467147827
RMSE train: 1.916693	val: 2.352709	test: 3.097609
MAE train: 1.665681	val: 1.900304	test: 2.342033

Epoch: 31
Loss: 2.7141940593719482
RMSE train: 1.896533	val: 2.283948	test: 3.269047
MAE train: 1.657389	val: 1.755658	test: 2.424109

Epoch: 32
Loss: 2.886288046836853
RMSE train: 1.905402	val: 2.489642	test: 3.454749
MAE train: 1.672203	val: 2.063969	test: 2.618804

Epoch: 33
Loss: 2.2350398302078247
RMSE train: 1.889650	val: 2.695104	test: 3.580560
MAE train: 1.635461	val: 2.353092	test: 2.804620

Epoch: 34
Loss: 2.4778932929039
RMSE train: 1.807352	val: 2.567439	test: 3.472564
MAE train: 1.562691	val: 2.227102	test: 2.740855

Epoch: 35
Loss: 1.8817517161369324
RMSE train: 1.696024	val: 2.166537	test: 3.111426
MAE train: 1.482495	val: 1.748884	test: 2.405869

Epoch: 36
Loss: 1.7492325901985168
RMSE train: 1.613709	val: 1.899866	test: 2.849315
MAE train: 1.403657	val: 1.429627	test: 2.165212

Epoch: 37
Loss: 2.025580048561096
RMSE train: 1.571431	val: 1.890844	test: 2.783243
MAE train: 1.361208	val: 1.443768	test: 2.107522

Epoch: 38
Loss: 1.6962018609046936
RMSE train: 1.529503	val: 2.008443	test: 2.833680
MAE train: 1.308228	val: 1.607943	test: 2.158544

Epoch: 39
Loss: 1.6349313259124756
RMSE train: 1.467994	val: 1.988198	test: 2.845231
MAE train: 1.261096	val: 1.584543	test: 2.150708

Epoch: 40
Loss: 1.4342296123504639
RMSE train: 1.410528	val: 1.778716	test: 2.882956
MAE train: 1.217791	val: 1.303677	test: 2.081887

Epoch: 41
Loss: 1.4647833108901978
RMSE train: 1.410209	val: 1.678268	test: 2.979639
MAE train: 1.206821	val: 1.171618	test: 2.118973

Epoch: 42
Loss: 1.6420261859893799
RMSE train: 1.430486	val: 1.799840	test: 3.128287
MAE train: 1.218043	val: 1.307416	test: 2.230614

Epoch: 43
Loss: 1.3229787945747375
RMSE train: 1.371759	val: 1.800941	test: 3.141955
MAE train: 1.164785	val: 1.314487	test: 2.273733

Epoch: 44
Loss: 1.1588728725910187
RMSE train: 1.302732	val: 1.674450	test: 3.024847
MAE train: 1.118619	val: 1.169244	test: 2.195101

Epoch: 45
Loss: 1.498094618320465
RMSE train: 1.250508	val: 1.599188	test: 2.931770
MAE train: 1.087258	val: 1.101265	test: 2.128194

Epoch: 46
Loss: 1.116797924041748
RMSE train: 1.155929	val: 1.558611	test: 2.833251
MAE train: 1.001652	val: 1.064096	test: 2.017161

Epoch: 47
Loss: 0.9961154460906982
RMSE train: 1.006088	val: 1.474009	test: 2.604768
MAE train: 0.846250	val: 0.989680	test: 1.804718

Epoch: 48
Loss: 0.9805189967155457
RMSE train: 0.896636	val: 1.428374	test: 2.495356
MAE train: 0.738770	val: 0.954149	test: 1.714923

Epoch: 49
Loss: 1.1589657068252563
RMSE train: 0.879547	val: 1.362644	test: 2.438970
MAE train: 0.730251	val: 0.913403	test: 1.668561

Epoch: 50
Loss: 0.8235658705234528
RMSE train: 0.914368	val: 1.309739	test: 2.392975
MAE train: 0.758101	val: 0.896624	test: 1.645653

Epoch: 51
Loss: 0.8338889479637146
RMSE train: 0.931155	val: 1.340102	test: 2.346881
MAE train: 0.774704	val: 0.930156	test: 1.602836

Epoch: 52
Loss: 0.8533304929733276
RMSE train: 0.941220	val: 1.392023	test: 2.334284
MAE train: 0.780161	val: 1.016167	test: 1.603011

Epoch: 53
Loss: 1.1241622567176819
RMSE train: 0.909979	val: 1.415146	test: 2.363494
MAE train: 0.759351	val: 1.029728	test: 1.622199

Epoch: 54
Loss: 0.7256354093551636
RMSE train: 0.893080	val: 1.380527	test: 2.423809
MAE train: 0.764199	val: 0.965832	test: 1.666163

Epoch: 55
Loss: 0.7400956451892853
RMSE train: 0.866284	val: 1.320436	test: 2.558001
MAE train: 0.741952	val: 0.895860	test: 1.798836

Epoch: 56
Loss: 0.7141530513763428
RMSE train: 0.855595	val: 1.356077	test: 2.674170
MAE train: 0.714464	val: 0.902002	test: 1.872715

Epoch: 57
Loss: 0.7665950655937195
RMSE train: 0.867584	val: 1.480149	test: 2.779633
MAE train: 0.689867	val: 1.016669	test: 1.912450

Epoch: 58
Loss: 0.8735023140907288
RMSE train: 0.872030	val: 1.535998	test: 2.775286
MAE train: 0.689605	val: 1.074693	test: 1.905168

Epoch: 59
Loss: 0.7409535646438599
RMSE train: 0.873054	val: 1.525826	test: 2.665484
MAE train: 0.700816	val: 1.072894	test: 1.833887

Epoch: 60
Loss: 0.7784754931926727
RMSE train: 0.856242	val: 1.445180	test: 2.477963
MAE train: 0.693799	val: 1.015883	test: 1.698405

Epoch: 61
Loss: 0.589269608259201
RMSE train: 0.812984	val: 1.379591	test: 2.330705
MAE train: 0.660403	val: 0.977203	test: 1.595387

Epoch: 62
Loss: 0.6745171248912811
RMSE train: 0.771065	val: 1.399219	test: 2.262543
MAE train: 0.621794	val: 1.020831	test: 1.564625

Epoch: 63
Loss: 0.7829816341400146
RMSE train: 0.725680	val: 1.401900	test: 2.209278
MAE train: 0.571209	val: 1.032904	test: 1.544253

Epoch: 64
Loss: 0.592284619808197
RMSE train: 0.716909	val: 1.363328	test: 2.236495
MAE train: 0.560198	val: 0.991115	test: 1.553560

Epoch: 65
Loss: 0.6606847643852234
RMSE train: 0.768619	val: 1.351181	test: 2.320849
MAE train: 0.602846	val: 0.979059	test: 1.592916

Epoch: 66
Loss: 0.7462542951107025
RMSE train: 0.819994	val: 1.347902	test: 2.369063
MAE train: 0.645174	val: 0.974824	test: 1.611009

Epoch: 67
Loss: 0.665551096200943
RMSE train: 0.823071	val: 1.295798	test: 2.397479
MAE train: 0.664570	val: 0.922883	test: 1.625189

Epoch: 68
Loss: 0.7458137273788452
RMSE train: 0.791311	val: 1.244005	test: 2.405424
MAE train: 0.632876	val: 0.863559	test: 1.649292

Epoch: 69
Loss: 0.8011339008808136
RMSE train: 0.752008	val: 1.253625	test: 2.308594
MAE train: 0.601987	val: 0.861516	test: 1.596067

Epoch: 70
Loss: 0.7598807215690613
RMSE train: 0.733943	val: 1.288156	test: 2.226136
MAE train: 0.557842	val: 0.918783	test: 1.557004

Epoch: 71
Loss: 0.6364866495132446
RMSE train: 0.725637	val: 1.282134	test: 2.192495
MAE train: 0.535508	val: 0.929263	test: 1.531029

Epoch: 72
Loss: 0.6819842159748077
RMSE train: 0.696082	val: 1.211109	test: 2.150894
MAE train: 0.537482	val: 0.862402	test: 1.506869

Epoch: 73
Loss: 0.632234513759613
RMSE train: 0.700751	val: 1.201425	test: 2.195192
MAE train: 0.551110	val: 0.876596	test: 1.543205

Epoch: 74
Loss: 0.6220577657222748
RMSE train: 0.740347	val: 1.210934	test: 2.198846
MAE train: 0.587178	val: 0.904122	test: 1.549524

Epoch: 75
Loss: 0.6796566545963287
RMSE train: 0.736891	val: 1.196430	test: 2.193748
MAE train: 0.587093	val: 0.864649	test: 1.535942

Epoch: 76
Loss: 0.6574112176895142
RMSE train: 0.739911	val: 1.229020	test: 2.257290
MAE train: 0.581944	val: 0.863079	test: 1.583939

Epoch: 77
Loss: 0.7433020770549774
RMSE train: 0.699703	val: 1.237329	test: 2.256942
MAE train: 0.541446	val: 0.867601	test: 1.582577

Epoch: 78
Loss: 0.7403222322463989
RMSE train: 0.644851	val: 1.193003	test: 2.166284
MAE train: 0.507759	val: 0.830967	test: 1.513328

Epoch: 79
Loss: 0.7650523781776428
RMSE train: 0.618789	val: 1.149163	test: 2.013203
MAE train: 0.494316	val: 0.807958	test: 1.424051

Epoch: 80
Loss: 0.5462437570095062
RMSE train: 0.600983	val: 1.144317	test: 2.055530
MAE train: 0.463993	val: 0.813983	test: 1.445960

Epoch: 81
Loss: 0.5949921905994415
RMSE train: 0.580308	val: 1.154427	test: 2.162683
MAE train: 0.451240	val: 0.831527	test: 1.524260

Epoch: 82
Loss: 0.516541063785553
RMSE train: 0.585588	val: 1.173303	test: 2.226068
MAE train: 0.459797	val: 0.847440	test: 1.558991

Epoch: 83
Loss: 0.6764305830001831
RMSE train: 0.597843	val: 1.221070	test: 2.260516
MAE train: 0.460563	val: 0.880288	test: 1.569980

Epoch: 23
Loss: 4.003946542739868
RMSE train: 2.345088	val: 3.403473	test: 3.752358
MAE train: 2.007155	val: 3.061630	test: 3.044213

Epoch: 24
Loss: 3.8551712036132812
RMSE train: 2.281582	val: 3.185121	test: 3.679791
MAE train: 1.969187	val: 2.807985	test: 3.006408

Epoch: 25
Loss: 3.7888693809509277
RMSE train: 2.184823	val: 2.883728	test: 3.555521
MAE train: 1.905022	val: 2.460280	test: 2.905068

Epoch: 26
Loss: 3.1958812475204468
RMSE train: 2.075876	val: 2.723450	test: 3.502987
MAE train: 1.818174	val: 2.277298	test: 2.856965

Epoch: 27
Loss: 3.274024724960327
RMSE train: 2.000598	val: 2.742648	test: 3.546644
MAE train: 1.755833	val: 2.299400	test: 2.878690

Epoch: 28
Loss: 2.8025141954421997
RMSE train: 1.934399	val: 2.774431	test: 3.594705
MAE train: 1.692130	val: 2.322179	test: 2.867460

Epoch: 29
Loss: 2.4908227920532227
RMSE train: 1.875718	val: 2.713083	test: 3.549618
MAE train: 1.630148	val: 2.231057	test: 2.764810

Epoch: 30
Loss: 2.6100767254829407
RMSE train: 1.780347	val: 2.509143	test: 3.348794
MAE train: 1.544181	val: 1.983547	test: 2.538483

Epoch: 31
Loss: 2.419572353363037
RMSE train: 1.711895	val: 2.400336	test: 3.201790
MAE train: 1.500444	val: 1.857455	test: 2.385149

Epoch: 32
Loss: 2.7400444746017456
RMSE train: 1.666242	val: 2.375323	test: 3.153718
MAE train: 1.458777	val: 1.837403	test: 2.326651

Epoch: 33
Loss: 2.2821330428123474
RMSE train: 1.632328	val: 2.484913	test: 3.158997
MAE train: 1.402744	val: 1.964864	test: 2.374233

Epoch: 34
Loss: 1.9644485712051392
RMSE train: 1.600340	val: 2.638261	test: 3.195723
MAE train: 1.320333	val: 2.130562	test: 2.427139

Epoch: 35
Loss: 1.8616437911987305
RMSE train: 1.523804	val: 2.461867	test: 2.998277
MAE train: 1.270013	val: 1.937467	test: 2.230019

Epoch: 36
Loss: 1.9832276105880737
RMSE train: 1.466847	val: 2.098098	test: 2.689545
MAE train: 1.273156	val: 1.570414	test: 1.921832

Epoch: 37
Loss: 2.34042751789093
RMSE train: 1.464400	val: 1.974250	test: 2.570785
MAE train: 1.292389	val: 1.470481	test: 1.846753

Epoch: 38
Loss: 1.4939568042755127
RMSE train: 1.441989	val: 2.122843	test: 2.607392
MAE train: 1.246029	val: 1.593911	test: 1.867329

Epoch: 39
Loss: 1.57034432888031
RMSE train: 1.423924	val: 2.274998	test: 2.621299
MAE train: 1.201246	val: 1.781400	test: 1.934080

Epoch: 40
Loss: 1.5799205303192139
RMSE train: 1.380295	val: 2.241217	test: 2.578003
MAE train: 1.151090	val: 1.759843	test: 1.912198

Epoch: 41
Loss: 1.29352205991745
RMSE train: 1.272323	val: 2.028277	test: 2.389642
MAE train: 1.065103	val: 1.544042	test: 1.761354

Epoch: 42
Loss: 1.1713294684886932
RMSE train: 1.173270	val: 1.834523	test: 2.383552
MAE train: 0.994063	val: 1.366393	test: 1.709843

Epoch: 43
Loss: 1.2804163694381714
RMSE train: 1.123671	val: 1.717078	test: 2.346337
MAE train: 0.960929	val: 1.283341	test: 1.667845

Epoch: 44
Loss: 1.1613892912864685
RMSE train: 1.117682	val: 1.723791	test: 2.319509
MAE train: 0.959606	val: 1.290969	test: 1.637395

Epoch: 45
Loss: 1.0916301310062408
RMSE train: 1.125365	val: 1.853554	test: 2.399957
MAE train: 0.934024	val: 1.390416	test: 1.683215

Epoch: 46
Loss: 1.02824205160141
RMSE train: 1.111363	val: 1.910125	test: 2.513790
MAE train: 0.894590	val: 1.433260	test: 1.752914

Epoch: 47
Loss: 0.9168035984039307
RMSE train: 1.074878	val: 1.794920	test: 2.509721
MAE train: 0.889506	val: 1.335335	test: 1.720093

Epoch: 48
Loss: 1.3271688520908356
RMSE train: 1.037376	val: 1.649431	test: 2.457011
MAE train: 0.887317	val: 1.218383	test: 1.680274

Epoch: 49
Loss: 0.8595480024814606
RMSE train: 0.966661	val: 1.584477	test: 2.435220
MAE train: 0.821680	val: 1.165374	test: 1.662205

Epoch: 50
Loss: 0.8480061888694763
RMSE train: 0.929119	val: 1.627397	test: 2.465118
MAE train: 0.768316	val: 1.191859	test: 1.679761

Epoch: 51
Loss: 0.8163692355155945
RMSE train: 0.884534	val: 1.616357	test: 2.399278
MAE train: 0.700389	val: 1.184185	test: 1.649531

Epoch: 52
Loss: 0.9892386496067047
RMSE train: 0.821164	val: 1.456189	test: 2.252393
MAE train: 0.663987	val: 1.054235	test: 1.542953

Epoch: 53
Loss: 0.8178113102912903
RMSE train: 0.782753	val: 1.351097	test: 2.143403
MAE train: 0.637375	val: 0.990165	test: 1.499971

Epoch: 54
Loss: 0.8991875052452087
RMSE train: 0.772112	val: 1.377903	test: 2.122746
MAE train: 0.598246	val: 0.992798	test: 1.481518

Epoch: 55
Loss: 0.8282196223735809
RMSE train: 0.763310	val: 1.412079	test: 2.109206
MAE train: 0.553215	val: 1.010322	test: 1.496703

Epoch: 56
Loss: 0.7346858978271484
RMSE train: 0.707378	val: 1.332708	test: 2.001357
MAE train: 0.527775	val: 0.963368	test: 1.415713

Epoch: 57
Loss: 1.0780677199363708
RMSE train: 0.693690	val: 1.324955	test: 2.049063
MAE train: 0.564664	val: 1.000319	test: 1.436407

Epoch: 58
Loss: 0.6987650394439697
RMSE train: 0.709318	val: 1.341509	test: 2.073692
MAE train: 0.590360	val: 1.025480	test: 1.461416

Epoch: 59
Loss: 0.6819210648536682
RMSE train: 0.698437	val: 1.386202	test: 2.134442
MAE train: 0.582847	val: 1.043279	test: 1.485528

Epoch: 60
Loss: 0.6982614099979401
RMSE train: 0.687700	val: 1.451996	test: 2.204918
MAE train: 0.557815	val: 1.081176	test: 1.528329

Epoch: 61
Loss: 0.6934625208377838
RMSE train: 0.692426	val: 1.467012	test: 2.225319
MAE train: 0.547403	val: 1.091835	test: 1.537727

Epoch: 62
Loss: 0.8195255696773529
RMSE train: 0.683896	val: 1.407053	test: 2.245086
MAE train: 0.549386	val: 1.073203	test: 1.545718

Epoch: 63
Loss: 0.6710245013237
RMSE train: 0.685175	val: 1.356993	test: 2.242437
MAE train: 0.554624	val: 1.065299	test: 1.568288

Epoch: 64
Loss: 0.6788549721240997
RMSE train: 0.662394	val: 1.353407	test: 2.205001
MAE train: 0.529855	val: 1.045873	test: 1.522267

Epoch: 65
Loss: 0.8423139452934265
RMSE train: 0.636235	val: 1.351182	test: 2.200586
MAE train: 0.507356	val: 1.032284	test: 1.515291

Epoch: 66
Loss: 0.6676493585109711
RMSE train: 0.667981	val: 1.383291	test: 2.283436
MAE train: 0.534459	val: 1.057915	test: 1.553508

Epoch: 67
Loss: 0.820912629365921
RMSE train: 0.649656	val: 1.383010	test: 2.290005
MAE train: 0.516067	val: 1.052033	test: 1.555218

Epoch: 68
Loss: 0.751374751329422
RMSE train: 0.640844	val: 1.407297	test: 2.337040
MAE train: 0.493792	val: 1.033139	test: 1.559223

Epoch: 69
Loss: 0.5732756555080414
RMSE train: 0.619071	val: 1.403008	test: 2.342489
MAE train: 0.465759	val: 1.023117	test: 1.563168

Epoch: 70
Loss: 0.6642578840255737
RMSE train: 0.614940	val: 1.382881	test: 2.335272
MAE train: 0.475477	val: 1.025846	test: 1.564219

Epoch: 71
Loss: 0.5976947247982025
RMSE train: 0.642413	val: 1.355540	test: 2.327672
MAE train: 0.507222	val: 1.038258	test: 1.573221

Epoch: 72
Loss: 0.5746040344238281
RMSE train: 0.618923	val: 1.351367	test: 2.299090
MAE train: 0.476966	val: 1.031251	test: 1.552270

Epoch: 73
Loss: 0.6461450755596161
RMSE train: 0.626550	val: 1.378475	test: 2.324136
MAE train: 0.467943	val: 1.032736	test: 1.556771

Epoch: 74
Loss: 0.9550084471702576
RMSE train: 0.621559	val: 1.394333	test: 2.358801
MAE train: 0.465548	val: 1.050091	test: 1.565090

Epoch: 75
Loss: 0.603197455406189
RMSE train: 0.665378	val: 1.424257	test: 2.477597
MAE train: 0.516269	val: 1.101885	test: 1.625857

Epoch: 76
Loss: 0.683828204870224
RMSE train: 0.672175	val: 1.428676	test: 2.519324
MAE train: 0.530252	val: 1.107891	test: 1.658664

Epoch: 77
Loss: 0.834472119808197
RMSE train: 0.649881	val: 1.404323	test: 2.467500
MAE train: 0.514553	val: 1.078671	test: 1.631562

Epoch: 78
Loss: 0.7178056538105011
RMSE train: 0.671543	val: 1.373239	test: 2.417238
MAE train: 0.526784	val: 1.053941	test: 1.607779

Epoch: 79
Loss: 0.7577386200428009
RMSE train: 0.667138	val: 1.336840	test: 2.310991
MAE train: 0.515312	val: 0.996386	test: 1.574162

Epoch: 80
Loss: 0.5483041703701019
RMSE train: 0.655281	val: 1.301404	test: 2.132300
MAE train: 0.504768	val: 0.949961	test: 1.494578

Epoch: 81
Loss: 0.5044723600149155
RMSE train: 0.628119	val: 1.271228	test: 2.058074
MAE train: 0.488204	val: 0.931461	test: 1.443832

Epoch: 82
Loss: 0.5419866293668747
RMSE train: 0.628894	val: 1.296341	test: 2.144947
MAE train: 0.487438	val: 0.956015	test: 1.481893

Epoch: 83
Loss: 0.6018876135349274
RMSE train: 0.645153	val: 1.315240	test: 2.313695
MAE train: 0.500663	val: 1.002936	test: 1.547931

Epoch: 23
Loss: 6.700521469116211
RMSE train: 2.781743	val: 3.602957	test: 3.956426
MAE train: 2.440994	val: 3.191752	test: 3.390064

Epoch: 24
Loss: 6.483081579208374
RMSE train: 2.749106	val: 3.498450	test: 3.817255
MAE train: 2.400641	val: 3.046847	test: 3.205274

Epoch: 25
Loss: 6.075414657592773
RMSE train: 2.707794	val: 3.497611	test: 3.791286
MAE train: 2.359217	val: 3.069064	test: 3.176581

Epoch: 26
Loss: 6.012543201446533
RMSE train: 2.621941	val: 3.396028	test: 3.687660
MAE train: 2.287073	val: 2.982236	test: 3.077676

Epoch: 27
Loss: 5.587209224700928
RMSE train: 2.538487	val: 3.268947	test: 3.579612
MAE train: 2.226182	val: 2.866905	test: 2.976762

Epoch: 28
Loss: 5.204631090164185
RMSE train: 2.458781	val: 3.153795	test: 3.494043
MAE train: 2.170717	val: 2.763301	test: 2.897938

Epoch: 29
Loss: 5.052760601043701
RMSE train: 2.417790	val: 3.170830	test: 3.577382
MAE train: 2.132297	val: 2.824114	test: 3.008639

Epoch: 30
Loss: 4.661925554275513
RMSE train: 2.407514	val: 3.263565	test: 3.752579
MAE train: 2.110398	val: 2.953912	test: 3.194543

Epoch: 31
Loss: 4.678098678588867
RMSE train: 2.382691	val: 3.245537	test: 3.793169
MAE train: 2.082337	val: 2.933112	test: 3.225925

Epoch: 32
Loss: 4.116004824638367
RMSE train: 2.299528	val: 3.070653	test: 3.657538
MAE train: 2.006089	val: 2.730448	test: 3.074395

Epoch: 33
Loss: 4.297788858413696
RMSE train: 2.190140	val: 2.860931	test: 3.465676
MAE train: 1.910465	val: 2.492295	test: 2.857863

Epoch: 34
Loss: 3.7440414428710938
RMSE train: 2.093047	val: 2.668167	test: 3.293101
MAE train: 1.822341	val: 2.264162	test: 2.650377

Epoch: 35
Loss: 3.5369927883148193
RMSE train: 2.043759	val: 2.748041	test: 3.374737
MAE train: 1.747779	val: 2.398051	test: 2.753554

Epoch: 36
Loss: 3.4685146808624268
RMSE train: 2.006750	val: 2.686250	test: 3.319917
MAE train: 1.712955	val: 2.345904	test: 2.673817

Epoch: 37
Loss: 3.1510311365127563
RMSE train: 1.980919	val: 2.534238	test: 3.166666
MAE train: 1.704805	val: 2.149057	test: 2.467502

Epoch: 38
Loss: 2.943711519241333
RMSE train: 1.969454	val: 2.545799	test: 3.145387
MAE train: 1.699678	val: 2.136993	test: 2.427843

Epoch: 39
Loss: 2.984649419784546
RMSE train: 1.951214	val: 2.612395	test: 3.196987
MAE train: 1.684170	val: 2.224601	test: 2.530049

Epoch: 40
Loss: 2.6656181812286377
RMSE train: 1.903393	val: 2.635627	test: 3.239412
MAE train: 1.640252	val: 2.286172	test: 2.610284

Epoch: 41
Loss: 2.401291847229004
RMSE train: 1.813663	val: 2.546440	test: 3.190594
MAE train: 1.564358	val: 2.219444	test: 2.578776

Epoch: 42
Loss: 2.163461208343506
RMSE train: 1.712267	val: 2.311082	test: 3.010355
MAE train: 1.480636	val: 1.985480	test: 2.403685

Epoch: 43
Loss: 2.2621020674705505
RMSE train: 1.614902	val: 2.073642	test: 2.805585
MAE train: 1.389178	val: 1.712727	test: 2.163323

Epoch: 44
Loss: 2.0427790880203247
RMSE train: 1.535866	val: 1.977371	test: 2.694614
MAE train: 1.309737	val: 1.564851	test: 2.020398

Epoch: 45
Loss: 1.9360404014587402
RMSE train: 1.508006	val: 2.040896	test: 2.703116
MAE train: 1.271540	val: 1.621749	test: 2.033890

Epoch: 46
Loss: 1.9126128554344177
RMSE train: 1.470912	val: 2.148769	test: 2.771828
MAE train: 1.203729	val: 1.762189	test: 2.159728

Epoch: 47
Loss: 1.7627415657043457
RMSE train: 1.401096	val: 2.066017	test: 2.702921
MAE train: 1.161949	val: 1.686034	test: 2.095861

Epoch: 48
Loss: 1.5265960097312927
RMSE train: 1.329876	val: 1.808352	test: 2.522652
MAE train: 1.154485	val: 1.410739	test: 1.907549

Epoch: 49
Loss: 1.4955185651779175
RMSE train: 1.281069	val: 1.612427	test: 2.390455
MAE train: 1.128262	val: 1.207952	test: 1.754772

Epoch: 50
Loss: 1.4096373915672302
RMSE train: 1.228979	val: 1.596681	test: 2.417478
MAE train: 1.064115	val: 1.214142	test: 1.796344

Epoch: 51
Loss: 1.2169129848480225
RMSE train: 1.151746	val: 1.583607	test: 2.428602
MAE train: 0.955883	val: 1.207809	test: 1.802458

Epoch: 52
Loss: 1.221392035484314
RMSE train: 1.111263	val: 1.606770	test: 2.460894
MAE train: 0.884837	val: 1.228085	test: 1.848861

Epoch: 53
Loss: 1.1201785504817963
RMSE train: 1.055382	val: 1.558392	test: 2.410355
MAE train: 0.829845	val: 1.154551	test: 1.782614

Epoch: 54
Loss: 1.2989147007465363
RMSE train: 0.966251	val: 1.431939	test: 2.300323
MAE train: 0.779800	val: 1.019575	test: 1.663678

Epoch: 55
Loss: 1.1960458755493164
RMSE train: 0.932941	val: 1.321295	test: 2.221137
MAE train: 0.773559	val: 0.918218	test: 1.626035

Epoch: 56
Loss: 1.0079927444458008
RMSE train: 0.944270	val: 1.403254	test: 2.217429
MAE train: 0.776598	val: 1.001873	test: 1.582288

Epoch: 57
Loss: 1.0073038637638092
RMSE train: 0.989699	val: 1.680712	test: 2.448934
MAE train: 0.765603	val: 1.298637	test: 1.842840

Epoch: 58
Loss: 0.9266891181468964
RMSE train: 1.016576	val: 1.761944	test: 2.505537
MAE train: 0.782122	val: 1.372088	test: 1.898502

Epoch: 59
Loss: 1.0180950164794922
RMSE train: 1.014542	val: 1.629898	test: 2.351513
MAE train: 0.824604	val: 1.186939	test: 1.692284

Epoch: 60
Loss: 1.016359269618988
RMSE train: 1.021836	val: 1.516358	test: 2.262492
MAE train: 0.847733	val: 1.020138	test: 1.596255

Epoch: 61
Loss: 0.8416192829608917
RMSE train: 0.972519	val: 1.519455	test: 2.262346
MAE train: 0.794519	val: 1.053297	test: 1.591198

Epoch: 62
Loss: 1.046988695859909
RMSE train: 0.881705	val: 1.539268	test: 2.295809
MAE train: 0.682041	val: 1.130331	test: 1.673658

Epoch: 63
Loss: 0.7859526872634888
RMSE train: 0.821510	val: 1.469388	test: 2.269584
MAE train: 0.601630	val: 1.085839	test: 1.670853

Epoch: 64
Loss: 0.8435906171798706
RMSE train: 0.780647	val: 1.313526	test: 2.163726
MAE train: 0.585248	val: 0.937837	test: 1.570024

Epoch: 65
Loss: 0.8918065130710602
RMSE train: 0.799082	val: 1.208464	test: 2.128000
MAE train: 0.610948	val: 0.841962	test: 1.570620

Epoch: 66
Loss: 0.8831538259983063
RMSE train: 0.763225	val: 1.219825	test: 2.132876
MAE train: 0.602036	val: 0.843736	test: 1.564466

Epoch: 67
Loss: 0.787406325340271
RMSE train: 0.726643	val: 1.288983	test: 2.161337
MAE train: 0.568566	val: 0.884937	test: 1.556267

Epoch: 68
Loss: 0.8517346680164337
RMSE train: 0.730341	val: 1.348001	test: 2.179833
MAE train: 0.548220	val: 0.931176	test: 1.555335

Epoch: 69
Loss: 0.8441407382488251
RMSE train: 0.744649	val: 1.345773	test: 2.152608
MAE train: 0.552293	val: 0.929057	test: 1.500392

Epoch: 70
Loss: 0.9043805301189423
RMSE train: 0.758216	val: 1.323907	test: 2.120188
MAE train: 0.571122	val: 0.916912	test: 1.469662

Epoch: 71
Loss: 0.7411300539970398
RMSE train: 0.782707	val: 1.327289	test: 2.123292
MAE train: 0.581184	val: 0.907891	test: 1.506638

Epoch: 72
Loss: 0.8913519382476807
RMSE train: 0.774825	val: 1.360873	test: 2.166842
MAE train: 0.570278	val: 0.921147	test: 1.558092

Epoch: 73
Loss: 1.0929178297519684
RMSE train: 0.722181	val: 1.379191	test: 2.152900
MAE train: 0.532569	val: 0.934287	test: 1.527916

Epoch: 74
Loss: 0.8391140401363373
RMSE train: 0.655050	val: 1.401432	test: 2.171670
MAE train: 0.485141	val: 0.966324	test: 1.549346

Epoch: 75
Loss: 0.9138346016407013
RMSE train: 0.665535	val: 1.387967	test: 2.175282
MAE train: 0.494386	val: 0.956248	test: 1.545829

Epoch: 76
Loss: 0.8131844699382782
RMSE train: 0.684987	val: 1.321363	test: 2.150902
MAE train: 0.521901	val: 0.912760	test: 1.529948

Epoch: 77
Loss: 0.7918165028095245
RMSE train: 0.731521	val: 1.289435	test: 2.125701
MAE train: 0.564386	val: 0.886370	test: 1.520036

Epoch: 78
Loss: 0.7562417387962341
RMSE train: 0.785211	val: 1.324956	test: 2.155481
MAE train: 0.609655	val: 0.904354	test: 1.547134

Epoch: 79
Loss: 0.8819853067398071
RMSE train: 0.834945	val: 1.453183	test: 2.281792
MAE train: 0.626810	val: 1.002252	test: 1.674363

Epoch: 80
Loss: 0.7845511436462402
RMSE train: 0.864400	val: 1.512742	test: 2.317671
MAE train: 0.630771	val: 1.043475	test: 1.703949

Epoch: 81
Loss: 0.6732335090637207
RMSE train: 0.811836	val: 1.459432	test: 2.251794
MAE train: 0.602069	val: 0.991675	test: 1.610468

Epoch: 82
Loss: 0.868357926607132
RMSE train: 0.710740	val: 1.417127	test: 2.208756
MAE train: 0.541324	val: 0.980618	test: 1.539277

Epoch: 83
Loss: 0.6302242279052734
RMSE train: 0.673661	val: 1.396116	test: 2.175573
MAE train: 0.520548	val: 0.991488	test: 1.494076

Epoch: 23
Loss: 7.497103691101074
RMSE train: 3.143884	val: 4.115009	test: 4.466033
MAE train: 2.814167	val: 3.835435	test: 3.943477

Epoch: 24
Loss: 6.522872686386108
RMSE train: 3.051379	val: 4.053168	test: 4.436061
MAE train: 2.723675	val: 3.788786	test: 3.900974

Epoch: 25
Loss: 6.387138366699219
RMSE train: 2.947910	val: 3.866751	test: 4.277043
MAE train: 2.634747	val: 3.593326	test: 3.719774

Epoch: 26
Loss: 6.11790919303894
RMSE train: 2.815019	val: 3.561721	test: 4.031734
MAE train: 2.523561	val: 3.252630	test: 3.439006

Epoch: 27
Loss: 5.856929063796997
RMSE train: 2.708750	val: 3.263096	test: 3.794008
MAE train: 2.433149	val: 2.913960	test: 3.162598

Epoch: 28
Loss: 5.492506742477417
RMSE train: 2.642740	val: 3.350974	test: 3.879012
MAE train: 2.355667	val: 3.053640	test: 3.309208

Epoch: 29
Loss: 5.196546316146851
RMSE train: 2.539001	val: 3.377461	test: 3.906047
MAE train: 2.230060	val: 3.105266	test: 3.356043

Epoch: 30
Loss: 5.098997354507446
RMSE train: 2.465249	val: 3.343715	test: 3.907003
MAE train: 2.146320	val: 3.081214	test: 3.361492

Epoch: 31
Loss: 5.244670629501343
RMSE train: 2.434088	val: 3.237084	test: 3.849289
MAE train: 2.124388	val: 2.968912	test: 3.285862

Epoch: 32
Loss: 4.955589532852173
RMSE train: 2.378523	val: 2.999898	test: 3.682121
MAE train: 2.089793	val: 2.709535	test: 3.081721

Epoch: 33
Loss: 4.323457479476929
RMSE train: 2.331227	val: 2.787633	test: 3.502139
MAE train: 2.061133	val: 2.469213	test: 2.851727

Epoch: 34
Loss: 4.011398792266846
RMSE train: 2.311156	val: 2.670475	test: 3.404732
MAE train: 2.045335	val: 2.327950	test: 2.703437

Epoch: 35
Loss: 3.848381757736206
RMSE train: 2.263423	val: 2.667014	test: 3.460834
MAE train: 1.994131	val: 2.352658	test: 2.788278

Epoch: 36
Loss: 3.4558461904525757
RMSE train: 2.170712	val: 2.638713	test: 3.462662
MAE train: 1.894655	val: 2.341170	test: 2.798029

Epoch: 37
Loss: 3.2713375091552734
RMSE train: 2.080180	val: 2.545280	test: 3.360549
MAE train: 1.799638	val: 2.231693	test: 2.689948

Epoch: 38
Loss: 3.2244412899017334
RMSE train: 1.969957	val: 2.417976	test: 3.209398
MAE train: 1.691926	val: 2.082733	test: 2.525619

Epoch: 39
Loss: 2.9547539949417114
RMSE train: 1.872572	val: 2.376245	test: 3.137293
MAE train: 1.583424	val: 2.031311	test: 2.436349

Epoch: 40
Loss: 2.6415733098983765
RMSE train: 1.788194	val: 2.373143	test: 3.133993
MAE train: 1.499378	val: 2.040710	test: 2.461587

Epoch: 41
Loss: 2.6184513568878174
RMSE train: 1.709313	val: 2.325581	test: 3.099202
MAE train: 1.435274	val: 1.991254	test: 2.421230

Epoch: 42
Loss: 3.3807828426361084
RMSE train: 1.646740	val: 2.273339	test: 3.073363
MAE train: 1.375553	val: 1.960871	test: 2.422748

Epoch: 43
Loss: 2.2829891443252563
RMSE train: 1.616131	val: 2.224329	test: 3.084685
MAE train: 1.362357	val: 1.936757	test: 2.469158

Epoch: 44
Loss: 2.1618987321853638
RMSE train: 1.612345	val: 2.096265	test: 2.940893
MAE train: 1.392012	val: 1.800685	test: 2.313156

Epoch: 45
Loss: 2.1228984594345093
RMSE train: 1.611863	val: 1.962802	test: 2.763146
MAE train: 1.416365	val: 1.625483	test: 2.053581

Epoch: 46
Loss: 1.898649275302887
RMSE train: 1.594272	val: 1.921451	test: 2.720479
MAE train: 1.381718	val: 1.552760	test: 1.993617

Epoch: 47
Loss: 1.5850833654403687
RMSE train: 1.552582	val: 1.980483	test: 2.789769
MAE train: 1.306798	val: 1.611135	test: 2.031862

Epoch: 48
Loss: 1.764622688293457
RMSE train: 1.483719	val: 1.905889	test: 2.759379
MAE train: 1.213002	val: 1.507230	test: 1.975157

Epoch: 49
Loss: 1.9508900046348572
RMSE train: 1.396424	val: 1.829038	test: 2.700773
MAE train: 1.122724	val: 1.430942	test: 1.912775

Epoch: 50
Loss: 1.5522041320800781
RMSE train: 1.316652	val: 1.747721	test: 2.592898
MAE train: 1.059626	val: 1.372354	test: 1.825289

Epoch: 51
Loss: 1.427418291568756
RMSE train: 1.239437	val: 1.615995	test: 2.446355
MAE train: 1.034297	val: 1.256691	test: 1.713892

Epoch: 52
Loss: 1.2957785725593567
RMSE train: 1.200450	val: 1.584512	test: 2.384889
MAE train: 1.032483	val: 1.247223	test: 1.720215

Epoch: 53
Loss: 1.2265756130218506
RMSE train: 1.147725	val: 1.517420	test: 2.318715
MAE train: 0.997297	val: 1.192608	test: 1.669348

Epoch: 54
Loss: 1.3937155604362488
RMSE train: 1.112854	val: 1.468574	test: 2.287612
MAE train: 0.965113	val: 1.142724	test: 1.619957

Epoch: 55
Loss: 1.314619541168213
RMSE train: 1.078586	val: 1.443109	test: 2.296341
MAE train: 0.917234	val: 1.114455	test: 1.612291

Epoch: 56
Loss: 0.9863295257091522
RMSE train: 1.083348	val: 1.453329	test: 2.327675
MAE train: 0.897453	val: 1.122116	test: 1.635075

Epoch: 57
Loss: 1.1227407455444336
RMSE train: 1.043944	val: 1.374324	test: 2.247794
MAE train: 0.863358	val: 1.030362	test: 1.541106

Epoch: 58
Loss: 1.2762126326560974
RMSE train: 1.059326	val: 1.404483	test: 2.347537
MAE train: 0.874209	val: 1.078077	test: 1.672803

Epoch: 59
Loss: 1.1714942455291748
RMSE train: 1.060015	val: 1.426665	test: 2.358251
MAE train: 0.873012	val: 1.099387	test: 1.685582

Epoch: 60
Loss: 0.9675784111022949
RMSE train: 1.025685	val: 1.392689	test: 2.261041
MAE train: 0.848909	val: 1.062741	test: 1.549382

Epoch: 61
Loss: 0.9288209080696106
RMSE train: 1.013648	val: 1.393715	test: 2.216253
MAE train: 0.835946	val: 1.064066	test: 1.524451

Epoch: 62
Loss: 1.0433290004730225
RMSE train: 0.959040	val: 1.382645	test: 2.144160
MAE train: 0.781697	val: 1.050014	test: 1.472552

Epoch: 63
Loss: 0.9675010442733765
RMSE train: 0.920768	val: 1.390849	test: 2.107439
MAE train: 0.733003	val: 1.043385	test: 1.436053

Epoch: 64
Loss: 0.8247084021568298
RMSE train: 0.877732	val: 1.424408	test: 2.109142
MAE train: 0.690332	val: 1.082879	test: 1.429697

Epoch: 65
Loss: 0.9141373038291931
RMSE train: 0.857362	val: 1.385660	test: 2.085202
MAE train: 0.687983	val: 1.048912	test: 1.428857

Epoch: 66
Loss: 0.8784371614456177
RMSE train: 0.862112	val: 1.370702	test: 2.072376
MAE train: 0.699019	val: 1.032839	test: 1.417623

Epoch: 67
Loss: 0.8141022026538849
RMSE train: 0.834979	val: 1.344382	test: 2.066036
MAE train: 0.668355	val: 0.999824	test: 1.406643

Epoch: 68
Loss: 1.0625272989273071
RMSE train: 0.781889	val: 1.312587	test: 2.068358
MAE train: 0.608166	val: 0.952585	test: 1.434095

Epoch: 69
Loss: 0.851219892501831
RMSE train: 0.773830	val: 1.303516	test: 2.065436
MAE train: 0.595844	val: 0.935020	test: 1.471772

Epoch: 70
Loss: 0.9888915121555328
RMSE train: 0.812492	val: 1.401432	test: 2.153298
MAE train: 0.619098	val: 1.049866	test: 1.500554

Epoch: 71
Loss: 0.7980177998542786
RMSE train: 0.822628	val: 1.420664	test: 2.262120
MAE train: 0.625547	val: 1.085850	test: 1.614292

Epoch: 72
Loss: 0.814905047416687
RMSE train: 0.804753	val: 1.305319	test: 2.164711
MAE train: 0.648166	val: 0.978012	test: 1.486950

Epoch: 73
Loss: 0.782847136259079
RMSE train: 0.814365	val: 1.240968	test: 2.092019
MAE train: 0.671799	val: 0.911046	test: 1.446965

Epoch: 74
Loss: 0.8614700734615326
RMSE train: 0.802729	val: 1.279485	test: 2.105985
MAE train: 0.655787	val: 0.943009	test: 1.425723

Epoch: 75
Loss: 0.8731703758239746
RMSE train: 0.788917	val: 1.385230	test: 2.235203
MAE train: 0.609260	val: 1.060471	test: 1.581810

Epoch: 76
Loss: 0.7664563059806824
RMSE train: 0.744324	val: 1.356881	test: 2.188448
MAE train: 0.551627	val: 1.019346	test: 1.489087

Epoch: 77
Loss: 0.8197387456893921
RMSE train: 0.710728	val: 1.256422	test: 2.078919
MAE train: 0.524717	val: 0.909157	test: 1.355078

Epoch: 78
Loss: 0.8043408989906311
RMSE train: 0.732860	val: 1.243848	test: 2.097855
MAE train: 0.560667	val: 0.905552	test: 1.477470

Epoch: 79
Loss: 1.0026383101940155
RMSE train: 0.766555	val: 1.277683	test: 2.072103
MAE train: 0.600908	val: 0.920919	test: 1.370060

Epoch: 80
Loss: 0.8326092064380646
RMSE train: 0.792689	val: 1.386148	test: 2.116190
MAE train: 0.621809	val: 1.019722	test: 1.396187

Epoch: 81
Loss: 0.8138436079025269
RMSE train: 0.795262	val: 1.447048	test: 2.149108
MAE train: 0.605483	val: 1.078214	test: 1.463288

Epoch: 82
Loss: 0.6679883301258087
RMSE train: 0.755123	val: 1.403373	test: 2.118604
MAE train: 0.573856	val: 1.030104	test: 1.459699

Epoch: 83
Loss: 0.8213722705841064
RMSE train: 0.732530	val: 1.327837	test: 2.090045
MAE train: 0.560801	val: 0.965875	test: 1.428947

Epoch: 23
Loss: 6.702948331832886
RMSE train: 2.978189	val: 3.658919	test: 4.354482
MAE train: 2.672209	val: 3.306065	test: 3.707262

Epoch: 24
Loss: 6.538606405258179
RMSE train: 2.949242	val: 3.580752	test: 4.302454
MAE train: 2.651090	val: 3.236144	test: 3.645775

Epoch: 25
Loss: 5.921176910400391
RMSE train: 2.904847	val: 3.511520	test: 4.227016
MAE train: 2.612335	val: 3.176708	test: 3.557210

Epoch: 26
Loss: 5.803979396820068
RMSE train: 2.835174	val: 3.469749	test: 4.166563
MAE train: 2.542791	val: 3.157328	test: 3.524923

Epoch: 27
Loss: 5.302864074707031
RMSE train: 2.747658	val: 3.388343	test: 4.044572
MAE train: 2.449067	val: 3.080242	test: 3.411081

Epoch: 28
Loss: 5.268289804458618
RMSE train: 2.653484	val: 3.262997	test: 3.899502
MAE train: 2.365693	val: 2.948636	test: 3.271570

Epoch: 29
Loss: 4.995978593826294
RMSE train: 2.532510	val: 3.069929	test: 3.691288
MAE train: 2.268324	val: 2.732627	test: 3.033146

Epoch: 30
Loss: 4.648863315582275
RMSE train: 2.409985	val: 2.786224	test: 3.429719
MAE train: 2.156949	val: 2.383197	test: 2.669581

Epoch: 31
Loss: 4.497551679611206
RMSE train: 2.361671	val: 2.782213	test: 3.412417
MAE train: 2.108651	val: 2.382411	test: 2.665196

Epoch: 32
Loss: 4.273353099822998
RMSE train: 2.370377	val: 3.010149	test: 3.622254
MAE train: 2.101902	val: 2.661727	test: 2.985155

Epoch: 33
Loss: 3.9694812297821045
RMSE train: 2.362585	val: 3.156926	test: 3.779472
MAE train: 2.078621	val: 2.822273	test: 3.172615

Epoch: 34
Loss: 3.5928510427474976
RMSE train: 2.329808	val: 3.162858	test: 3.793595
MAE train: 2.048803	val: 2.828717	test: 3.185812

Epoch: 35
Loss: 3.5272666215896606
RMSE train: 2.272699	val: 2.997495	test: 3.651333
MAE train: 2.015270	val: 2.653875	test: 3.052074

Epoch: 36
Loss: 3.4749330282211304
RMSE train: 2.182284	val: 2.682061	test: 3.355028
MAE train: 1.958885	val: 2.313139	test: 2.712355

Epoch: 37
Loss: 3.0664472579956055
RMSE train: 2.094916	val: 2.518702	test: 3.212438
MAE train: 1.876624	val: 2.130246	test: 2.524803

Epoch: 38
Loss: 2.7464191913604736
RMSE train: 2.032436	val: 2.538923	test: 3.227839
MAE train: 1.792220	val: 2.155026	test: 2.557200

Epoch: 39
Loss: 2.7538955211639404
RMSE train: 1.976850	val: 2.584688	test: 3.247607
MAE train: 1.701041	val: 2.195379	test: 2.600784

Epoch: 40
Loss: 2.5852251052856445
RMSE train: 1.918331	val: 2.603340	test: 3.232050
MAE train: 1.606887	val: 2.199929	test: 2.594299

Epoch: 41
Loss: 2.3961101770401
RMSE train: 1.847480	val: 2.512070	test: 3.124320
MAE train: 1.537773	val: 2.093008	test: 2.453829

Epoch: 42
Loss: 2.3854787349700928
RMSE train: 1.722163	val: 2.271260	test: 2.898809
MAE train: 1.462717	val: 1.818439	test: 2.144963

Epoch: 43
Loss: 2.022070825099945
RMSE train: 1.607457	val: 2.069487	test: 2.718762
MAE train: 1.380524	val: 1.593170	test: 1.972580

Epoch: 44
Loss: 1.8970150351524353
RMSE train: 1.555523	val: 2.172289	test: 2.818515
MAE train: 1.330966	val: 1.783910	test: 2.169579

Epoch: 45
Loss: 1.7564281821250916
RMSE train: 1.519497	val: 2.174357	test: 2.836451
MAE train: 1.297075	val: 1.812226	test: 2.245189

Epoch: 46
Loss: 1.775067925453186
RMSE train: 1.460124	val: 2.060674	test: 2.710840
MAE train: 1.252654	val: 1.693854	test: 2.123745

Epoch: 47
Loss: 1.893955945968628
RMSE train: 1.451119	val: 1.963328	test: 2.654730
MAE train: 1.264544	val: 1.582240	test: 2.045997

Epoch: 48
Loss: 1.4317522048950195
RMSE train: 1.476542	val: 1.925922	test: 2.654446
MAE train: 1.289988	val: 1.526475	test: 2.014819

Epoch: 49
Loss: 1.490706205368042
RMSE train: 1.482360	val: 1.910575	test: 2.649983
MAE train: 1.288862	val: 1.496363	test: 1.988666

Epoch: 50
Loss: 1.4335792064666748
RMSE train: 1.433327	val: 1.925586	test: 2.631413
MAE train: 1.222774	val: 1.511755	test: 1.983414

Epoch: 51
Loss: 1.2974476218223572
RMSE train: 1.383846	val: 2.006508	test: 2.696547
MAE train: 1.155380	val: 1.609632	test: 2.054018

Epoch: 52
Loss: 1.3051854372024536
RMSE train: 1.353079	val: 1.951313	test: 2.632413
MAE train: 1.127087	val: 1.532305	test: 1.949364

Epoch: 53
Loss: 1.1511261463165283
RMSE train: 1.292858	val: 1.748233	test: 2.485018
MAE train: 1.077213	val: 1.301135	test: 1.753841

Epoch: 54
Loss: 1.3522929549217224
RMSE train: 1.206398	val: 1.614787	test: 2.388953
MAE train: 1.017832	val: 1.159580	test: 1.667083

Epoch: 55
Loss: 1.2080730199813843
RMSE train: 1.128217	val: 1.585950	test: 2.329440
MAE train: 0.946924	val: 1.126512	test: 1.604196

Epoch: 56
Loss: 1.0920970439910889
RMSE train: 1.029035	val: 1.627083	test: 2.315341
MAE train: 0.834140	val: 1.182881	test: 1.545068

Epoch: 57
Loss: 1.2662134766578674
RMSE train: 0.977753	val: 1.593811	test: 2.275357
MAE train: 0.763162	val: 1.141432	test: 1.517464

Epoch: 58
Loss: 0.9705708622932434
RMSE train: 0.955155	val: 1.473344	test: 2.178946
MAE train: 0.761112	val: 1.021843	test: 1.470999

Epoch: 59
Loss: 0.9893182516098022
RMSE train: 0.913585	val: 1.356887	test: 2.092599
MAE train: 0.738665	val: 0.934247	test: 1.452568

Epoch: 60
Loss: 0.899483323097229
RMSE train: 0.904079	val: 1.333589	test: 2.072317
MAE train: 0.736535	val: 0.943176	test: 1.434045

Epoch: 61
Loss: 1.0574798583984375
RMSE train: 0.894121	val: 1.385186	test: 2.138936
MAE train: 0.726008	val: 1.002822	test: 1.493099

Epoch: 62
Loss: 0.9095167517662048
RMSE train: 0.855215	val: 1.370523	test: 2.091511
MAE train: 0.684862	val: 0.999128	test: 1.473397

Epoch: 63
Loss: 1.1605085730552673
RMSE train: 0.808241	val: 1.347218	test: 1.993599
MAE train: 0.630928	val: 0.957809	test: 1.404469

Epoch: 64
Loss: 0.8163294494152069
RMSE train: 0.799020	val: 1.366775	test: 1.992131
MAE train: 0.615469	val: 0.954542	test: 1.408283

Epoch: 65
Loss: 0.9427152574062347
RMSE train: 0.775716	val: 1.338872	test: 2.012063
MAE train: 0.597156	val: 0.930138	test: 1.451585

Epoch: 66
Loss: 0.8391091823577881
RMSE train: 0.782105	val: 1.340077	test: 2.038001
MAE train: 0.615436	val: 0.932578	test: 1.463863

Epoch: 67
Loss: 1.0698485970497131
RMSE train: 0.788954	val: 1.385390	test: 2.069523
MAE train: 0.633062	val: 0.988666	test: 1.449765

Epoch: 68
Loss: 0.7768872082233429
RMSE train: 0.799944	val: 1.492319	test: 2.147597
MAE train: 0.628725	val: 1.111373	test: 1.571603

Epoch: 69
Loss: 0.8648497462272644
RMSE train: 0.836687	val: 1.473410	test: 2.084632
MAE train: 0.651265	val: 1.098727	test: 1.515948

Epoch: 70
Loss: 0.9265214800834656
RMSE train: 0.843913	val: 1.379204	test: 1.990252
MAE train: 0.676849	val: 1.007262	test: 1.388163

Epoch: 71
Loss: 1.0098786354064941
RMSE train: 0.835062	val: 1.366200	test: 2.029985
MAE train: 0.676061	val: 0.976445	test: 1.383487

Epoch: 72
Loss: 0.9051927030086517
RMSE train: 0.822052	val: 1.403578	test: 2.168542
MAE train: 0.663853	val: 0.999813	test: 1.496290

Epoch: 73
Loss: 1.0536222457885742
RMSE train: 0.801145	val: 1.414884	test: 2.217312
MAE train: 0.628819	val: 1.004624	test: 1.535866

Epoch: 74
Loss: 0.9197278022766113
RMSE train: 0.782398	val: 1.366679	test: 2.184195
MAE train: 0.608097	val: 0.968170	test: 1.499591

Epoch: 75
Loss: 0.8809723258018494
RMSE train: 0.764187	val: 1.327978	test: 2.140003
MAE train: 0.592531	val: 0.933729	test: 1.464015

Epoch: 76
Loss: 0.7601270377635956
RMSE train: 0.740041	val: 1.279390	test: 2.054392
MAE train: 0.574168	val: 0.905259	test: 1.420158

Epoch: 77
Loss: 1.062690556049347
RMSE train: 0.754539	val: 1.278204	test: 2.046774
MAE train: 0.586737	val: 0.928426	test: 1.412075

Epoch: 78
Loss: 0.843341052532196
RMSE train: 0.788689	val: 1.307012	test: 2.057222
MAE train: 0.615291	val: 0.954013	test: 1.438286

Epoch: 79
Loss: 0.7643955647945404
RMSE train: 0.773163	val: 1.357592	test: 2.084604
MAE train: 0.599021	val: 0.988226	test: 1.476499

Epoch: 80
Loss: 0.8383439183235168
RMSE train: 0.756767	val: 1.367133	test: 2.040344
MAE train: 0.595291	val: 1.004632	test: 1.449529

Epoch: 81
Loss: 0.7102595865726471
RMSE train: 0.747818	val: 1.368990	test: 1.981521
MAE train: 0.603701	val: 1.007360	test: 1.412008

Epoch: 82
Loss: 0.8020215630531311
RMSE train: 0.759775	val: 1.361145	test: 1.918688
MAE train: 0.617827	val: 0.985656	test: 1.340778

Epoch: 83
Loss: 0.680261492729187
RMSE train: 0.802116	val: 1.325394	test: 1.886883
MAE train: 0.625826	val: 0.943740	test: 1.316465

Epoch: 23
Loss: 5.874642372131348
RMSE train: 2.781133	val: 3.872999	test: 3.670297
MAE train: 2.535670	val: 3.450946	test: 3.154424

Epoch: 24
Loss: 5.530903100967407
RMSE train: 2.666438	val: 3.823843	test: 3.576595
MAE train: 2.428380	val: 3.345033	test: 3.078670

Epoch: 25
Loss: 5.399635076522827
RMSE train: 2.626158	val: 3.895143	test: 3.549962
MAE train: 2.394103	val: 3.355452	test: 3.048965

Epoch: 26
Loss: 5.01212739944458
RMSE train: 2.605251	val: 3.977138	test: 3.548355
MAE train: 2.372152	val: 3.393542	test: 3.039722

Epoch: 27
Loss: 4.7428882122039795
RMSE train: 2.557543	val: 3.995541	test: 3.544689
MAE train: 2.328130	val: 3.359894	test: 3.040590

Epoch: 28
Loss: 4.446951627731323
RMSE train: 2.483043	val: 4.003757	test: 3.528576
MAE train: 2.258521	val: 3.267712	test: 3.033373

Epoch: 29
Loss: 4.1600282192230225
RMSE train: 2.394691	val: 3.936233	test: 3.429691
MAE train: 2.177676	val: 3.146827	test: 2.936441

Epoch: 30
Loss: 4.0540595054626465
RMSE train: 2.302593	val: 3.785289	test: 3.308904
MAE train: 2.090018	val: 3.028898	test: 2.813166

Epoch: 31
Loss: 3.780291199684143
RMSE train: 2.221773	val: 3.697551	test: 3.180047
MAE train: 2.009682	val: 2.929453	test: 2.677204

Epoch: 32
Loss: 3.466812014579773
RMSE train: 2.167786	val: 3.584242	test: 3.056801
MAE train: 1.949776	val: 2.821992	test: 2.545111

Epoch: 33
Loss: 3.3034353256225586
RMSE train: 2.082878	val: 3.367377	test: 2.907080
MAE train: 1.863464	val: 2.633731	test: 2.387681

Epoch: 34
Loss: 2.982128858566284
RMSE train: 1.996696	val: 3.182034	test: 2.794034
MAE train: 1.780420	val: 2.507179	test: 2.255986

Epoch: 35
Loss: 2.6849043369293213
RMSE train: 1.923706	val: 3.053168	test: 2.703568
MAE train: 1.708778	val: 2.426331	test: 2.141435

Epoch: 36
Loss: 2.6783294677734375
RMSE train: 1.829570	val: 2.929740	test: 2.564904
MAE train: 1.625670	val: 2.294898	test: 2.009423

Epoch: 37
Loss: 2.3491233587265015
RMSE train: 1.744245	val: 2.854885	test: 2.404403
MAE train: 1.563260	val: 2.201406	test: 1.866658

Epoch: 38
Loss: 2.3273247480392456
RMSE train: 1.683457	val: 2.905324	test: 2.320300
MAE train: 1.509076	val: 2.161714	test: 1.764937

Epoch: 39
Loss: 2.1685222387313843
RMSE train: 1.646445	val: 3.053447	test: 2.339890
MAE train: 1.474644	val: 2.211558	test: 1.767603

Epoch: 40
Loss: 1.83994060754776
RMSE train: 1.614429	val: 3.121351	test: 2.383255
MAE train: 1.439884	val: 2.251774	test: 1.819720

Epoch: 41
Loss: 1.8990543484687805
RMSE train: 1.585820	val: 3.169564	test: 2.396934
MAE train: 1.405892	val: 2.268218	test: 1.826020

Epoch: 42
Loss: 1.7268050909042358
RMSE train: 1.508563	val: 3.180953	test: 2.327252
MAE train: 1.346987	val: 2.303434	test: 1.761723

Epoch: 43
Loss: 1.5420747995376587
RMSE train: 1.402198	val: 3.292015	test: 2.262114
MAE train: 1.249339	val: 2.391746	test: 1.697290

Epoch: 44
Loss: 1.4223583936691284
RMSE train: 1.341506	val: 3.393224	test: 2.274684
MAE train: 1.175873	val: 2.431570	test: 1.712430

Epoch: 45
Loss: 1.3846911191940308
RMSE train: 1.300801	val: 3.362526	test: 2.317829
MAE train: 1.119157	val: 2.397515	test: 1.758222

Epoch: 46
Loss: 1.2666207551956177
RMSE train: 1.249988	val: 3.366293	test: 2.321382
MAE train: 1.057714	val: 2.372202	test: 1.759314

Epoch: 47
Loss: 1.2173466086387634
RMSE train: 1.197379	val: 3.261975	test: 2.279600
MAE train: 1.010872	val: 2.321612	test: 1.718766

Epoch: 48
Loss: 1.170940101146698
RMSE train: 1.173046	val: 3.231028	test: 2.239993
MAE train: 0.994306	val: 2.294436	test: 1.663229

Epoch: 49
Loss: 1.1232098937034607
RMSE train: 1.142572	val: 3.271714	test: 2.251187
MAE train: 0.967520	val: 2.312961	test: 1.650262

Epoch: 50
Loss: 1.0323160886764526
RMSE train: 1.111540	val: 3.344397	test: 2.296487
MAE train: 0.934570	val: 2.360663	test: 1.678680

Epoch: 51
Loss: 0.9035793542861938
RMSE train: 1.052556	val: 3.393356	test: 2.279165
MAE train: 0.876071	val: 2.394755	test: 1.654341

Epoch: 52
Loss: 0.9261024594306946
RMSE train: 0.979987	val: 3.363930	test: 2.185209
MAE train: 0.807513	val: 2.380457	test: 1.582473

Epoch: 53
Loss: 0.908148318529129
RMSE train: 0.956643	val: 3.387917	test: 2.131594
MAE train: 0.790369	val: 2.406135	test: 1.533862

Epoch: 54
Loss: 0.8689900636672974
RMSE train: 0.929111	val: 3.358032	test: 2.118698
MAE train: 0.764121	val: 2.348212	test: 1.510233

Epoch: 55
Loss: 0.8842392265796661
RMSE train: 0.885154	val: 3.256979	test: 2.065101
MAE train: 0.721796	val: 2.250241	test: 1.462412

Epoch: 56
Loss: 0.6965504288673401
RMSE train: 0.867300	val: 3.182678	test: 2.063216
MAE train: 0.702224	val: 2.172318	test: 1.439197

Epoch: 57
Loss: 0.8882371783256531
RMSE train: 0.847043	val: 3.078909	test: 2.004020
MAE train: 0.683060	val: 2.075522	test: 1.359077

Epoch: 58
Loss: 0.8239652216434479
RMSE train: 0.828621	val: 3.077210	test: 1.985511
MAE train: 0.673098	val: 2.065278	test: 1.334888

Epoch: 59
Loss: 0.7677138149738312
RMSE train: 0.824666	val: 3.064811	test: 1.994227
MAE train: 0.673026	val: 2.044621	test: 1.363409

Epoch: 60
Loss: 0.7257042825222015
RMSE train: 0.821417	val: 3.129786	test: 2.049598
MAE train: 0.674325	val: 2.082541	test: 1.413037

Epoch: 61
Loss: 0.7866001725196838
RMSE train: 0.845450	val: 3.219911	test: 2.106409
MAE train: 0.704906	val: 2.157636	test: 1.458061

Epoch: 62
Loss: 0.7604655623435974
RMSE train: 0.865618	val: 3.302102	test: 2.125155
MAE train: 0.735752	val: 2.237311	test: 1.477545

Epoch: 63
Loss: 0.6935931742191315
RMSE train: 0.895571	val: 3.420665	test: 2.148805
MAE train: 0.757705	val: 2.332071	test: 1.485406

Epoch: 64
Loss: 0.8031801879405975
RMSE train: 0.907414	val: 3.557660	test: 2.219439
MAE train: 0.753423	val: 2.403082	test: 1.527725

Epoch: 65
Loss: 0.6598402857780457
RMSE train: 0.909305	val: 3.579632	test: 2.289893
MAE train: 0.743166	val: 2.405972	test: 1.590591

Epoch: 66
Loss: 0.8759128153324127
RMSE train: 0.851147	val: 3.458792	test: 2.287159
MAE train: 0.685007	val: 2.341507	test: 1.591376

Epoch: 67
Loss: 0.7508641183376312
RMSE train: 0.777372	val: 3.226558	test: 2.180263
MAE train: 0.633394	val: 2.251235	test: 1.511607

Epoch: 68
Loss: 0.7109906673431396
RMSE train: 0.731799	val: 2.970094	test: 2.083773
MAE train: 0.592396	val: 2.131158	test: 1.445912

Epoch: 69
Loss: 0.820161759853363
RMSE train: 0.681702	val: 2.875110	test: 2.064034
MAE train: 0.538969	val: 2.040837	test: 1.422102

Epoch: 70
Loss: 0.7576363682746887
RMSE train: 0.643408	val: 2.966456	test: 2.114090
MAE train: 0.511176	val: 2.070216	test: 1.493250

Epoch: 71
Loss: 0.8170192837715149
RMSE train: 0.655440	val: 3.072980	test: 2.157865
MAE train: 0.515260	val: 2.105314	test: 1.517372

Epoch: 72
Loss: 0.6808075606822968
RMSE train: 0.674161	val: 3.128782	test: 2.174753
MAE train: 0.526955	val: 2.102727	test: 1.502333

Epoch: 73
Loss: 0.7232047617435455
RMSE train: 0.694491	val: 3.040090	test: 2.137773
MAE train: 0.561427	val: 2.033812	test: 1.466191

Epoch: 74
Loss: 0.7654106914997101
RMSE train: 0.720927	val: 3.043994	test: 2.126241
MAE train: 0.600896	val: 2.058723	test: 1.476363

Epoch: 75
Loss: 0.5606561303138733
RMSE train: 0.704100	val: 3.124114	test: 2.147041
MAE train: 0.583807	val: 2.088307	test: 1.497561

Epoch: 76
Loss: 0.6613849401473999
RMSE train: 0.666922	val: 3.188220	test: 2.160608
MAE train: 0.539664	val: 2.106750	test: 1.511879

Epoch: 77
Loss: 0.7192074358463287
RMSE train: 0.641121	val: 3.248579	test: 2.165822
MAE train: 0.505092	val: 2.148434	test: 1.510979

Epoch: 78
Loss: 0.6487718522548676
RMSE train: 0.657850	val: 3.277473	test: 2.179625
MAE train: 0.513742	val: 2.171488	test: 1.533026

Epoch: 79
Loss: 0.6985653936862946
RMSE train: 0.695717	val: 3.270577	test: 2.167277
MAE train: 0.528854	val: 2.155652	test: 1.527579

Epoch: 80
Loss: 0.6587918102741241
RMSE train: 0.690156	val: 3.118308	test: 2.100010
MAE train: 0.530064	val: 2.029744	test: 1.449984

Epoch: 81
Loss: 0.6311626434326172
RMSE train: 0.712909	val: 3.074716	test: 2.111574
MAE train: 0.559216	val: 1.954728	test: 1.453862

Epoch: 82
Loss: 0.8260919749736786
RMSE train: 0.712229	val: 3.150111	test: 2.140611
MAE train: 0.564556	val: 1.986867	test: 1.448106

Epoch: 83
Loss: 0.6203016340732574
RMSE train: 0.720869	val: 3.264570	test: 2.194712
MAE train: 0.582241	val: 2.066855	test: 1.494849

Epoch: 84
Loss: 0.5760093033313751
RMSE train: 0.620674	val: 1.242983	test: 2.267121
MAE train: 0.473585	val: 0.889522	test: 1.571591

Epoch: 85
Loss: 0.5909966230392456
RMSE train: 0.621508	val: 1.203890	test: 2.125061
MAE train: 0.479741	val: 0.847426	test: 1.496459

Epoch: 86
Loss: 0.5657157301902771
RMSE train: 0.633033	val: 1.187530	test: 2.073005
MAE train: 0.495464	val: 0.826455	test: 1.474907

Epoch: 87
Loss: 0.5136425942182541
RMSE train: 0.628673	val: 1.195241	test: 2.114820
MAE train: 0.491455	val: 0.833872	test: 1.493452

Epoch: 88
Loss: 0.499657541513443
RMSE train: 0.630817	val: 1.212510	test: 2.216416
MAE train: 0.492657	val: 0.834074	test: 1.548356

Epoch: 89
Loss: 0.7738634347915649
RMSE train: 0.644175	val: 1.242643	test: 2.329962
MAE train: 0.511653	val: 0.879450	test: 1.614374

Epoch: 90
Loss: 0.5199931263923645
RMSE train: 0.694479	val: 1.250072	test: 2.464393
MAE train: 0.554237	val: 0.892310	test: 1.677068

Epoch: 91
Loss: 0.4928835928440094
RMSE train: 0.751204	val: 1.235190	test: 2.498778
MAE train: 0.605997	val: 0.891525	test: 1.695594

Epoch: 92
Loss: 0.7432907223701477
RMSE train: 0.777031	val: 1.249413	test: 2.513763
MAE train: 0.642687	val: 0.918057	test: 1.718824

Epoch: 93
Loss: 1.085703730583191
RMSE train: 0.781468	val: 1.304289	test: 2.584192
MAE train: 0.654224	val: 0.984447	test: 1.798001

Epoch: 94
Loss: 0.5417927503585815
RMSE train: 0.771682	val: 1.361959	test: 2.480853
MAE train: 0.635282	val: 1.051926	test: 1.796900

Epoch: 95
Loss: 0.7079849243164062
RMSE train: 0.745625	val: 1.386309	test: 2.433510
MAE train: 0.586018	val: 1.070921	test: 1.773987

Epoch: 96
Loss: 0.5982685983181
RMSE train: 0.612745	val: 1.247473	test: 2.249163
MAE train: 0.482089	val: 0.925986	test: 1.588650

Epoch: 97
Loss: 0.6311992406845093
RMSE train: 0.583698	val: 1.202599	test: 2.250800
MAE train: 0.458250	val: 0.852648	test: 1.579919

Epoch: 98
Loss: 0.5037939846515656
RMSE train: 0.614166	val: 1.221380	test: 2.342425
MAE train: 0.482342	val: 0.912598	test: 1.664137

Epoch: 99
Loss: 0.9460766017436981
RMSE train: 0.533023	val: 1.167039	test: 2.367124
MAE train: 0.415275	val: 0.851811	test: 1.645290

Epoch: 100
Loss: 0.5306910127401352
RMSE train: 0.531936	val: 1.187110	test: 2.484779
MAE train: 0.402827	val: 0.844633	test: 1.690043

Epoch: 101
Loss: 0.41076868772506714
RMSE train: 0.552446	val: 1.220379	test: 2.530373
MAE train: 0.408765	val: 0.864490	test: 1.716485

Epoch: 102
Loss: 0.5489031672477722
RMSE train: 0.545462	val: 1.186499	test: 2.450340
MAE train: 0.411086	val: 0.821365	test: 1.663957

Epoch: 103
Loss: 0.520860105752945
RMSE train: 0.554234	val: 1.172251	test: 2.328715
MAE train: 0.426429	val: 0.809366	test: 1.599845

Epoch: 104
Loss: 0.5193278789520264
RMSE train: 0.521417	val: 1.213468	test: 2.230291
MAE train: 0.389701	val: 0.892497	test: 1.555802

Epoch: 105
Loss: 0.5924443900585175
RMSE train: 0.485976	val: 1.227086	test: 2.251669
MAE train: 0.368071	val: 0.930182	test: 1.595879

Epoch: 106
Loss: 0.5644572675228119
RMSE train: 0.461171	val: 1.186674	test: 2.325150
MAE train: 0.349468	val: 0.879036	test: 1.645428

Epoch: 107
Loss: 0.4214240461587906
RMSE train: 0.456983	val: 1.161488	test: 2.315861
MAE train: 0.347221	val: 0.828802	test: 1.625255

Epoch: 108
Loss: 0.8287586271762848
RMSE train: 0.482757	val: 1.174679	test: 2.307966
MAE train: 0.368588	val: 0.813978	test: 1.603999

Epoch: 109
Loss: 0.5732168257236481
RMSE train: 0.537705	val: 1.208422	test: 2.322765
MAE train: 0.412018	val: 0.843673	test: 1.615862

Epoch: 110
Loss: 0.4614860415458679
RMSE train: 0.620963	val: 1.192110	test: 2.249789
MAE train: 0.492507	val: 0.818973	test: 1.563495

Epoch: 111
Loss: 0.4400545805692673
RMSE train: 0.685550	val: 1.189921	test: 2.179618
MAE train: 0.554151	val: 0.805701	test: 1.516034

Epoch: 112
Loss: 0.5002757012844086
RMSE train: 0.665614	val: 1.181152	test: 2.147095
MAE train: 0.534638	val: 0.803598	test: 1.494428

Epoch: 113
Loss: 0.5455597937107086
RMSE train: 0.605809	val: 1.195783	test: 2.265259
MAE train: 0.472001	val: 0.830236	test: 1.573209

Epoch: 114
Loss: 0.4706611633300781
RMSE train: 0.598662	val: 1.235505	test: 2.384092
MAE train: 0.461678	val: 0.868613	test: 1.659838

Epoch: 115
Loss: 0.5503763854503632
RMSE train: 0.612852	val: 1.258186	test: 2.502299
MAE train: 0.475330	val: 0.898116	test: 1.738312

Epoch: 116
Loss: 0.5060095489025116
RMSE train: 0.664294	val: 1.252471	test: 2.645376
MAE train: 0.516788	val: 0.892280	test: 1.830225

Epoch: 117
Loss: 0.39572441577911377
RMSE train: 0.686307	val: 1.212770	test: 2.612535
MAE train: 0.537515	val: 0.861939	test: 1.811056

Epoch: 118
Loss: 0.6724209487438202
RMSE train: 0.652613	val: 1.204657	test: 2.544917
MAE train: 0.521970	val: 0.853535	test: 1.761169

Epoch: 119
Loss: 0.39010368287563324
RMSE train: 0.615875	val: 1.231569	test: 2.489557
MAE train: 0.494127	val: 0.889127	test: 1.742622

Epoch: 120
Loss: 0.4927125722169876
RMSE train: 0.585583	val: 1.209137	test: 2.402651
MAE train: 0.473777	val: 0.888036	test: 1.694307

Epoch: 121
Loss: 0.4647112339735031
RMSE train: 0.562227	val: 1.167805	test: 2.350475
MAE train: 0.456780	val: 0.837546	test: 1.648174

Early stopping
Best (RMSE):	 train: 0.600983	val: 1.144317	test: 2.055530
Best (MAE):	 train: 0.463993	val: 0.813983	test: 1.445960


Epoch: 23
Loss: 5.604001045227051
RMSE train: 2.596640	val: 3.334956	test: 3.470295
MAE train: 2.331262	val: 2.925130	test: 2.900840

Epoch: 24
Loss: 5.465473175048828
RMSE train: 2.508054	val: 3.292276	test: 3.453112
MAE train: 2.255602	val: 2.902580	test: 2.915866

Epoch: 25
Loss: 4.9474804401397705
RMSE train: 2.455583	val: 3.374437	test: 3.456466
MAE train: 2.220321	val: 2.941831	test: 2.956499

Epoch: 26
Loss: 4.637653827667236
RMSE train: 2.449123	val: 3.487374	test: 3.498159
MAE train: 2.218451	val: 3.030150	test: 2.998679

Epoch: 27
Loss: 4.5136038064956665
RMSE train: 2.435424	val: 3.492485	test: 3.416007
MAE train: 2.225547	val: 3.002963	test: 2.938989

Epoch: 28
Loss: 4.247121214866638
RMSE train: 2.452410	val: 3.616855	test: 3.441838
MAE train: 2.243472	val: 3.098799	test: 2.969972

Epoch: 29
Loss: 3.880849599838257
RMSE train: 2.475657	val: 3.813035	test: 3.547406
MAE train: 2.249159	val: 3.282568	test: 3.046791

Epoch: 30
Loss: 3.7499277591705322
RMSE train: 2.487749	val: 3.960199	test: 3.583155
MAE train: 2.246386	val: 3.393444	test: 3.064820

Epoch: 31
Loss: 3.5266289710998535
RMSE train: 2.471577	val: 3.928473	test: 3.483327
MAE train: 2.234550	val: 3.318924	test: 2.967577

Epoch: 32
Loss: 3.331756591796875
RMSE train: 2.376460	val: 3.772829	test: 3.325022
MAE train: 2.146276	val: 3.130688	test: 2.819667

Epoch: 33
Loss: 2.959138035774231
RMSE train: 2.218307	val: 3.561709	test: 3.098066
MAE train: 2.011103	val: 2.885845	test: 2.619748

Epoch: 34
Loss: 2.841790795326233
RMSE train: 2.072349	val: 3.476842	test: 2.951017
MAE train: 1.884428	val: 2.775047	test: 2.464127

Epoch: 35
Loss: 2.629017651081085
RMSE train: 1.978181	val: 3.430550	test: 2.931660
MAE train: 1.778856	val: 2.763546	test: 2.418272

Epoch: 36
Loss: 2.395745277404785
RMSE train: 1.894610	val: 3.346475	test: 2.873034
MAE train: 1.696480	val: 2.673689	test: 2.354707

Epoch: 37
Loss: 2.365725874900818
RMSE train: 1.806802	val: 3.180773	test: 2.699035
MAE train: 1.624546	val: 2.448918	test: 2.201678

Epoch: 38
Loss: 2.1416070461273193
RMSE train: 1.780308	val: 3.171652	test: 2.614598
MAE train: 1.593032	val: 2.385583	test: 2.115428

Epoch: 39
Loss: 2.1092947721481323
RMSE train: 1.769820	val: 3.279938	test: 2.596388
MAE train: 1.566588	val: 2.434357	test: 2.074215

Epoch: 40
Loss: 1.964450478553772
RMSE train: 1.701160	val: 3.330335	test: 2.610158
MAE train: 1.472304	val: 2.473180	test: 2.048877

Epoch: 41
Loss: 1.7000590562820435
RMSE train: 1.577774	val: 3.201878	test: 2.444260
MAE train: 1.385846	val: 2.262466	test: 1.907641

Epoch: 42
Loss: 1.698761761188507
RMSE train: 1.442228	val: 2.959501	test: 2.179566
MAE train: 1.290694	val: 2.086248	test: 1.651645

Epoch: 43
Loss: 1.603258728981018
RMSE train: 1.363964	val: 2.873687	test: 2.103302
MAE train: 1.229199	val: 2.052947	test: 1.560522

Epoch: 44
Loss: 1.3413974046707153
RMSE train: 1.298499	val: 2.846923	test: 2.153922
MAE train: 1.162731	val: 2.032117	test: 1.625674

Epoch: 45
Loss: 1.289952576160431
RMSE train: 1.265882	val: 2.850855	test: 2.206264
MAE train: 1.111006	val: 2.010328	test: 1.680934

Epoch: 46
Loss: 1.3647660613059998
RMSE train: 1.240886	val: 2.806710	test: 2.216213
MAE train: 1.082772	val: 2.005914	test: 1.707993

Epoch: 47
Loss: 1.1276493668556213
RMSE train: 1.160486	val: 2.746200	test: 2.188072
MAE train: 1.012739	val: 1.969573	test: 1.681026

Epoch: 48
Loss: 1.0920487344264984
RMSE train: 1.115359	val: 2.801813	test: 2.214288
MAE train: 0.967331	val: 1.969420	test: 1.696740

Epoch: 49
Loss: 1.1119827032089233
RMSE train: 1.051994	val: 2.894543	test: 2.241508
MAE train: 0.901057	val: 1.973264	test: 1.699592

Epoch: 50
Loss: 0.9849268198013306
RMSE train: 1.048439	val: 2.957532	test: 2.297495
MAE train: 0.886211	val: 1.963585	test: 1.724571

Epoch: 51
Loss: 0.9112671911716461
RMSE train: 0.984927	val: 2.881726	test: 2.271045
MAE train: 0.827592	val: 1.926393	test: 1.711195

Epoch: 52
Loss: 0.9349673986434937
RMSE train: 0.987583	val: 2.856368	test: 2.230264
MAE train: 0.848407	val: 1.947679	test: 1.669708

Epoch: 53
Loss: 0.8902512490749359
RMSE train: 1.008905	val: 2.925306	test: 2.224744
MAE train: 0.870373	val: 2.012813	test: 1.670270

Epoch: 54
Loss: 0.840231716632843
RMSE train: 1.044174	val: 3.044693	test: 2.272555
MAE train: 0.895725	val: 2.060485	test: 1.706504

Epoch: 55
Loss: 0.7365596890449524
RMSE train: 1.051991	val: 3.118949	test: 2.312812
MAE train: 0.886869	val: 2.092094	test: 1.752427

Epoch: 56
Loss: 0.9343666434288025
RMSE train: 1.050296	val: 3.120087	test: 2.256098
MAE train: 0.878905	val: 2.071943	test: 1.693375

Epoch: 57
Loss: 0.7974718511104584
RMSE train: 0.948831	val: 3.160321	test: 2.237091
MAE train: 0.781167	val: 2.093572	test: 1.666455

Epoch: 58
Loss: 0.7920919954776764
RMSE train: 0.839002	val: 3.102329	test: 2.196428
MAE train: 0.682228	val: 2.058461	test: 1.643899

Epoch: 59
Loss: 0.7552421689033508
RMSE train: 0.756888	val: 2.928959	test: 2.088601
MAE train: 0.616418	val: 1.952933	test: 1.547853

Epoch: 60
Loss: 0.7380387187004089
RMSE train: 0.724209	val: 2.837001	test: 1.980000
MAE train: 0.586693	val: 1.896767	test: 1.433083

Epoch: 61
Loss: 0.8592970669269562
RMSE train: 0.739200	val: 2.881593	test: 1.922209
MAE train: 0.587836	val: 1.917026	test: 1.377121

Epoch: 62
Loss: 0.6839965879917145
RMSE train: 0.723548	val: 2.986397	test: 1.935148
MAE train: 0.546114	val: 1.958993	test: 1.388309

Epoch: 63
Loss: 0.8177959322929382
RMSE train: 0.714028	val: 3.004820	test: 1.962648
MAE train: 0.527779	val: 1.957000	test: 1.403906

Epoch: 64
Loss: 0.891579657793045
RMSE train: 0.681781	val: 2.896053	test: 2.009075
MAE train: 0.508049	val: 1.902186	test: 1.430695

Epoch: 65
Loss: 0.7734144926071167
RMSE train: 0.629051	val: 2.771939	test: 2.025482
MAE train: 0.472762	val: 1.864960	test: 1.445799

Epoch: 66
Loss: 0.7914809286594391
RMSE train: 0.544901	val: 2.614902	test: 1.987297
MAE train: 0.411196	val: 1.807637	test: 1.428999

Epoch: 67
Loss: 0.6794063746929169
RMSE train: 0.555407	val: 2.537350	test: 1.983991
MAE train: 0.412781	val: 1.779145	test: 1.450372

Epoch: 68
Loss: 0.8332696557044983
RMSE train: 0.581069	val: 2.512245	test: 1.932221
MAE train: 0.449896	val: 1.743180	test: 1.410162

Epoch: 69
Loss: 0.7795417904853821
RMSE train: 0.656737	val: 2.691155	test: 1.923452
MAE train: 0.516279	val: 1.853549	test: 1.407315

Epoch: 70
Loss: 0.6308386623859406
RMSE train: 0.775637	val: 2.901879	test: 1.942391
MAE train: 0.596908	val: 1.952711	test: 1.416098

Epoch: 71
Loss: 0.7766254544258118
RMSE train: 0.837405	val: 3.042891	test: 2.006338
MAE train: 0.642325	val: 2.004649	test: 1.457232

Epoch: 72
Loss: 0.6579129695892334
RMSE train: 0.799977	val: 3.016837	test: 2.071543
MAE train: 0.617524	val: 1.946437	test: 1.503383

Epoch: 73
Loss: 0.660260945558548
RMSE train: 0.711111	val: 2.954169	test: 2.094046
MAE train: 0.536834	val: 1.921395	test: 1.523485

Epoch: 74
Loss: 0.6492704153060913
RMSE train: 0.626181	val: 2.887364	test: 2.066405
MAE train: 0.495881	val: 1.963678	test: 1.488814

Epoch: 75
Loss: 0.6600652635097504
RMSE train: 0.609063	val: 2.911977	test: 2.055654
MAE train: 0.482067	val: 1.971228	test: 1.489372

Epoch: 76
Loss: 0.6435571610927582
RMSE train: 0.600382	val: 2.941425	test: 2.030664
MAE train: 0.468792	val: 1.971352	test: 1.461451

Epoch: 77
Loss: 0.7729395925998688
RMSE train: 0.602089	val: 2.989068	test: 1.995663
MAE train: 0.460157	val: 1.999000	test: 1.412468

Epoch: 78
Loss: 0.6968409419059753
RMSE train: 0.622949	val: 2.975517	test: 1.974443
MAE train: 0.469491	val: 1.972204	test: 1.419168

Epoch: 79
Loss: 0.6728182733058929
RMSE train: 0.670703	val: 2.932555	test: 1.919462
MAE train: 0.522248	val: 1.923395	test: 1.376191

Epoch: 80
Loss: 0.5498605370521545
RMSE train: 0.705558	val: 2.923839	test: 1.889785
MAE train: 0.559651	val: 1.890817	test: 1.356451

Epoch: 81
Loss: 0.6985834538936615
RMSE train: 0.703331	val: 2.993996	test: 1.941688
MAE train: 0.563263	val: 1.931537	test: 1.409163

Epoch: 82
Loss: 0.5401308536529541
RMSE train: 0.673642	val: 3.061968	test: 2.023459
MAE train: 0.539264	val: 1.958468	test: 1.461287

Epoch: 83
Loss: 0.6361467838287354
RMSE train: 0.640168	val: 3.146161	test: 2.109955
MAE train: 0.501121	val: 1.985844	test: 1.511815

Epoch: 23
Loss: 5.4932544231414795
RMSE train: 2.488462	val: 4.458756	test: 3.667862
MAE train: 2.227717	val: 3.621681	test: 2.980301

Epoch: 24
Loss: 5.222569465637207
RMSE train: 2.423967	val: 4.401791	test: 3.588111
MAE train: 2.180738	val: 3.563305	test: 2.915239

Epoch: 25
Loss: 4.920062065124512
RMSE train: 2.349767	val: 4.304022	test: 3.506619
MAE train: 2.116373	val: 3.462952	test: 2.870259

Epoch: 26
Loss: 4.565958023071289
RMSE train: 2.263345	val: 4.243128	test: 3.425349
MAE train: 2.022492	val: 3.370211	test: 2.809756

Epoch: 27
Loss: 4.196148991584778
RMSE train: 2.166267	val: 4.224182	test: 3.344507
MAE train: 1.924274	val: 3.293750	test: 2.714254

Epoch: 28
Loss: 3.988006114959717
RMSE train: 2.084477	val: 4.255173	test: 3.259005
MAE train: 1.849826	val: 3.237361	test: 2.606212

Epoch: 29
Loss: 3.8583528995513916
RMSE train: 2.047425	val: 4.357248	test: 3.145940
MAE train: 1.815483	val: 3.199873	test: 2.482978

Epoch: 30
Loss: 3.552680015563965
RMSE train: 2.013132	val: 4.416491	test: 3.049501
MAE train: 1.790672	val: 3.142474	test: 2.378536

Epoch: 31
Loss: 3.2413158416748047
RMSE train: 1.975271	val: 4.455139	test: 3.001957
MAE train: 1.757733	val: 3.126288	test: 2.325185

Epoch: 32
Loss: 2.919637680053711
RMSE train: 1.956280	val: 4.487654	test: 3.021314
MAE train: 1.742132	val: 3.194960	test: 2.328549

Epoch: 33
Loss: 2.8102937936782837
RMSE train: 1.940589	val: 4.473956	test: 3.059351
MAE train: 1.733945	val: 3.249949	test: 2.360331

Epoch: 34
Loss: 2.646078109741211
RMSE train: 1.886152	val: 4.394437	test: 3.025362
MAE train: 1.678755	val: 3.201007	test: 2.332416

Epoch: 35
Loss: 2.433088541030884
RMSE train: 1.821419	val: 4.295668	test: 2.957328
MAE train: 1.610072	val: 3.082407	test: 2.256119

Epoch: 36
Loss: 2.343324303627014
RMSE train: 1.757010	val: 4.211010	test: 2.874411
MAE train: 1.539899	val: 2.944178	test: 2.164921

Epoch: 37
Loss: 2.1947787404060364
RMSE train: 1.710481	val: 4.204664	test: 2.816028
MAE train: 1.498702	val: 2.883351	test: 2.093383

Epoch: 38
Loss: 2.071176052093506
RMSE train: 1.691900	val: 4.290052	test: 2.841171
MAE train: 1.486307	val: 2.933755	test: 2.116196

Epoch: 39
Loss: 1.886815071105957
RMSE train: 1.625633	val: 4.273130	test: 2.844003
MAE train: 1.422687	val: 2.945005	test: 2.112543

Epoch: 40
Loss: 1.6631152629852295
RMSE train: 1.552185	val: 4.142685	test: 2.728469
MAE train: 1.361059	val: 2.852670	test: 1.997739

Epoch: 41
Loss: 1.5231825709342957
RMSE train: 1.432258	val: 3.994869	test: 2.573327
MAE train: 1.247111	val: 2.684442	test: 1.847528

Epoch: 42
Loss: 1.5847046971321106
RMSE train: 1.376129	val: 4.006388	test: 2.543442
MAE train: 1.193267	val: 2.684855	test: 1.796669

Epoch: 43
Loss: 1.3380001783370972
RMSE train: 1.403481	val: 4.076895	test: 2.610922
MAE train: 1.176059	val: 2.785963	test: 1.829838

Epoch: 44
Loss: 1.3148861527442932
RMSE train: 1.367324	val: 4.001795	test: 2.566443
MAE train: 1.144471	val: 2.687053	test: 1.802349

Epoch: 45
Loss: 1.2935038805007935
RMSE train: 1.264499	val: 3.811508	test: 2.399497
MAE train: 1.080832	val: 2.457314	test: 1.691076

Epoch: 46
Loss: 1.0349513590335846
RMSE train: 1.165294	val: 3.729632	test: 2.364509
MAE train: 0.965922	val: 2.357988	test: 1.686105

Epoch: 47
Loss: 1.1378301978111267
RMSE train: 1.089620	val: 3.796537	test: 2.394204
MAE train: 0.900518	val: 2.394159	test: 1.687713

Epoch: 48
Loss: 1.0724107921123505
RMSE train: 1.002972	val: 3.933909	test: 2.469065
MAE train: 0.802877	val: 2.462065	test: 1.721446

Epoch: 49
Loss: 1.0146792232990265
RMSE train: 0.928564	val: 3.950738	test: 2.446785
MAE train: 0.740756	val: 2.488477	test: 1.694330

Epoch: 50
Loss: 0.9094310700893402
RMSE train: 0.862373	val: 3.867873	test: 2.338841
MAE train: 0.705024	val: 2.454710	test: 1.610062

Epoch: 51
Loss: 0.9927040934562683
RMSE train: 0.823100	val: 3.714578	test: 2.227827
MAE train: 0.683513	val: 2.359193	test: 1.543309

Epoch: 52
Loss: 0.8226504623889923
RMSE train: 0.841773	val: 3.559508	test: 2.142955
MAE train: 0.702532	val: 2.266499	test: 1.491085

Epoch: 53
Loss: 0.9144906103610992
RMSE train: 0.872789	val: 3.584001	test: 2.153503
MAE train: 0.725343	val: 2.271026	test: 1.476390

Epoch: 54
Loss: 0.867518961429596
RMSE train: 0.826134	val: 3.565053	test: 2.142290
MAE train: 0.681177	val: 2.242863	test: 1.456323

Epoch: 55
Loss: 0.7073569297790527
RMSE train: 0.752862	val: 3.688536	test: 2.208482
MAE train: 0.613319	val: 2.283977	test: 1.500920

Epoch: 56
Loss: 0.740064948797226
RMSE train: 0.745448	val: 3.833185	test: 2.296039
MAE train: 0.599489	val: 2.382060	test: 1.560082

Epoch: 57
Loss: 0.7787172794342041
RMSE train: 0.771261	val: 3.930066	test: 2.384714
MAE train: 0.601000	val: 2.445121	test: 1.643083

Epoch: 58
Loss: 0.9330829977989197
RMSE train: 0.759541	val: 3.843732	test: 2.350501
MAE train: 0.603010	val: 2.410234	test: 1.631140

Epoch: 59
Loss: 0.8280787169933319
RMSE train: 0.762509	val: 3.666671	test: 2.278228
MAE train: 0.615572	val: 2.362421	test: 1.592536

Epoch: 60
Loss: 0.7725702524185181
RMSE train: 0.767439	val: 3.638510	test: 2.277615
MAE train: 0.619167	val: 2.377544	test: 1.614952

Epoch: 61
Loss: 0.8749820291996002
RMSE train: 0.759134	val: 3.659452	test: 2.281477
MAE train: 0.607722	val: 2.383862	test: 1.630531

Epoch: 62
Loss: 0.8006712198257446
RMSE train: 0.736660	val: 3.725259	test: 2.275223
MAE train: 0.577068	val: 2.383052	test: 1.597537

Epoch: 63
Loss: 0.7643143832683563
RMSE train: 0.693098	val: 3.703664	test: 2.237072
MAE train: 0.543834	val: 2.330565	test: 1.549149

Epoch: 64
Loss: 0.8285657167434692
RMSE train: 0.634562	val: 3.775411	test: 2.265593
MAE train: 0.491004	val: 2.326889	test: 1.581893

Epoch: 65
Loss: 0.7413681745529175
RMSE train: 0.613370	val: 3.768392	test: 2.276116
MAE train: 0.473017	val: 2.300291	test: 1.595898

Epoch: 66
Loss: 0.8379885256290436
RMSE train: 0.570079	val: 3.706154	test: 2.237713
MAE train: 0.445335	val: 2.306031	test: 1.564032

Epoch: 67
Loss: 0.7021467089653015
RMSE train: 0.577020	val: 3.647585	test: 2.213041
MAE train: 0.441603	val: 2.302082	test: 1.539540

Epoch: 68
Loss: 0.690175324678421
RMSE train: 0.608786	val: 3.550110	test: 2.197729
MAE train: 0.458778	val: 2.248025	test: 1.549900

Epoch: 69
Loss: 0.7581488192081451
RMSE train: 0.602988	val: 3.385510	test: 2.138210
MAE train: 0.460055	val: 2.152625	test: 1.495363

Epoch: 70
Loss: 0.7068792581558228
RMSE train: 0.637997	val: 3.324864	test: 2.113969
MAE train: 0.489373	val: 2.097473	test: 1.465558

Epoch: 71
Loss: 0.7659576833248138
RMSE train: 0.602728	val: 3.352977	test: 2.088994
MAE train: 0.460283	val: 2.105174	test: 1.434776

Epoch: 72
Loss: 0.801510751247406
RMSE train: 0.545436	val: 3.460588	test: 2.085365
MAE train: 0.397611	val: 2.175681	test: 1.429002

Epoch: 73
Loss: 0.7333562076091766
RMSE train: 0.524487	val: 3.456099	test: 2.097078
MAE train: 0.387022	val: 2.178017	test: 1.450528

Epoch: 74
Loss: 0.7614783048629761
RMSE train: 0.489819	val: 3.384485	test: 2.117194
MAE train: 0.361481	val: 2.137761	test: 1.480934

Epoch: 75
Loss: 0.747050017118454
RMSE train: 0.503366	val: 3.346637	test: 2.120825
MAE train: 0.390913	val: 2.109206	test: 1.482999

Epoch: 76
Loss: 0.6963678002357483
RMSE train: 0.604203	val: 3.430785	test: 2.143239
MAE train: 0.481856	val: 2.169394	test: 1.464714

Epoch: 77
Loss: 0.7228847742080688
RMSE train: 0.651586	val: 3.516650	test: 2.194433
MAE train: 0.505391	val: 2.190759	test: 1.469709

Epoch: 78
Loss: 0.806639552116394
RMSE train: 0.683964	val: 3.631439	test: 2.272441
MAE train: 0.528920	val: 2.230419	test: 1.520773

Epoch: 79
Loss: 0.631291925907135
RMSE train: 0.669738	val: 3.730044	test: 2.331458
MAE train: 0.510220	val: 2.261703	test: 1.558150

Epoch: 80
Loss: 0.5993062555789948
RMSE train: 0.620751	val: 3.805848	test: 2.344855
MAE train: 0.471861	val: 2.309885	test: 1.572891

Epoch: 81
Loss: 0.6356102228164673
RMSE train: 0.606284	val: 3.851644	test: 2.354436
MAE train: 0.464903	val: 2.342902	test: 1.573958

Epoch: 82
Loss: 0.6050480902194977
RMSE train: 0.643577	val: 3.903982	test: 2.388589
MAE train: 0.501810	val: 2.358216	test: 1.581499

Epoch: 83
Loss: 0.6319984495639801
RMSE train: 0.643620	val: 3.793908	test: 2.335129
MAE train: 0.521601	val: 2.305677	test: 1.550260

Epoch: 84
Loss: 0.7164808213710785
RMSE train: 0.665521	val: 1.426337	test: 2.759717
MAE train: 0.506427	val: 1.087072	test: 1.881983

Epoch: 85
Loss: 0.5835693776607513
RMSE train: 0.698608	val: 1.450698	test: 2.847379
MAE train: 0.511387	val: 1.080445	test: 1.929627

Epoch: 86
Loss: 0.7049315571784973
RMSE train: 0.707864	val: 1.480301	test: 2.864938
MAE train: 0.500643	val: 1.090856	test: 1.947954

Epoch: 87
Loss: 0.5657598972320557
RMSE train: 0.702054	val: 1.475715	test: 2.745286
MAE train: 0.499825	val: 1.101814	test: 1.883281

Epoch: 88
Loss: 0.5580814778804779
RMSE train: 0.676827	val: 1.474570	test: 2.631874
MAE train: 0.501089	val: 1.120104	test: 1.812481

Epoch: 89
Loss: 0.693771481513977
RMSE train: 0.651266	val: 1.455559	test: 2.549505
MAE train: 0.517368	val: 1.120196	test: 1.769933

Epoch: 90
Loss: 0.6789095401763916
RMSE train: 0.640226	val: 1.389221	test: 2.465055
MAE train: 0.526231	val: 1.061152	test: 1.751406

Epoch: 91
Loss: 0.5659643113613129
RMSE train: 0.626526	val: 1.332635	test: 2.442990
MAE train: 0.518619	val: 1.001491	test: 1.752063

Epoch: 92
Loss: 0.5370869338512421
RMSE train: 0.624900	val: 1.297793	test: 2.474613
MAE train: 0.499368	val: 0.946396	test: 1.766424

Epoch: 93
Loss: 0.4378948211669922
RMSE train: 0.642250	val: 1.297009	test: 2.485495
MAE train: 0.496366	val: 0.948387	test: 1.773517

Epoch: 94
Loss: 0.5636525452136993
RMSE train: 0.660699	val: 1.306118	test: 2.499818
MAE train: 0.509853	val: 0.973239	test: 1.783920

Epoch: 95
Loss: 0.6238716840744019
RMSE train: 0.678521	val: 1.315629	test: 2.493299
MAE train: 0.523031	val: 0.994474	test: 1.793314

Epoch: 96
Loss: 0.51785808801651
RMSE train: 0.691714	val: 1.343341	test: 2.529866
MAE train: 0.530429	val: 1.033066	test: 1.814012

Epoch: 97
Loss: 0.5367510169744492
RMSE train: 0.694873	val: 1.345107	test: 2.551729
MAE train: 0.531703	val: 1.031576	test: 1.832024

Epoch: 98
Loss: 0.55845807492733
RMSE train: 0.674267	val: 1.321544	test: 2.541371
MAE train: 0.522160	val: 0.983426	test: 1.839139

Epoch: 99
Loss: 0.4486779272556305
RMSE train: 0.663557	val: 1.322611	test: 2.561157
MAE train: 0.511260	val: 0.968413	test: 1.856675

Epoch: 100
Loss: 0.6278077960014343
RMSE train: 0.624120	val: 1.317939	test: 2.461655
MAE train: 0.484412	val: 0.986656	test: 1.797345

Epoch: 101
Loss: 0.5334710031747818
RMSE train: 0.582362	val: 1.357934	test: 2.411326
MAE train: 0.457321	val: 1.073479	test: 1.743732

Epoch: 102
Loss: 0.5288200974464417
RMSE train: 0.541677	val: 1.466966	test: 2.343763
MAE train: 0.425721	val: 1.226404	test: 1.675275

Epoch: 103
Loss: 0.5773493051528931
RMSE train: 0.539379	val: 1.523672	test: 2.388989
MAE train: 0.423210	val: 1.270991	test: 1.700463

Epoch: 104
Loss: 0.5333574414253235
RMSE train: 0.555384	val: 1.501657	test: 2.487721
MAE train: 0.418611	val: 1.226550	test: 1.748998

Epoch: 105
Loss: 0.5321646630764008
RMSE train: 0.551754	val: 1.470860	test: 2.508196
MAE train: 0.423513	val: 1.188827	test: 1.779568

Epoch: 106
Loss: 0.567332535982132
RMSE train: 0.562734	val: 1.488896	test: 2.451219
MAE train: 0.456610	val: 1.213096	test: 1.780931

Epoch: 107
Loss: 0.4156930148601532
RMSE train: 0.607611	val: 1.508298	test: 2.433236
MAE train: 0.486029	val: 1.236342	test: 1.783298

Epoch: 108
Loss: 0.6997539103031158
RMSE train: 0.590772	val: 1.498376	test: 2.370550
MAE train: 0.464668	val: 1.225625	test: 1.739732

Epoch: 109
Loss: 0.4986145943403244
RMSE train: 0.552600	val: 1.431185	test: 2.416543
MAE train: 0.422306	val: 1.147316	test: 1.750991

Epoch: 110
Loss: 0.45191189646720886
RMSE train: 0.556280	val: 1.383365	test: 2.478500
MAE train: 0.404623	val: 1.076297	test: 1.783723

Epoch: 111
Loss: 0.509444385766983
RMSE train: 0.570516	val: 1.399773	test: 2.419519
MAE train: 0.414791	val: 1.087686	test: 1.751829

Epoch: 112
Loss: 0.38560932874679565
RMSE train: 0.589434	val: 1.494698	test: 2.345318
MAE train: 0.441274	val: 1.198087	test: 1.692649

Epoch: 113
Loss: 0.4477193355560303
RMSE train: 0.620699	val: 1.547777	test: 2.328696
MAE train: 0.470588	val: 1.262980	test: 1.681256

Epoch: 114
Loss: 0.43198978900909424
RMSE train: 0.608771	val: 1.486450	test: 2.369885
MAE train: 0.457653	val: 1.208972	test: 1.702871

Epoch: 115
Loss: 0.4641621559858322
RMSE train: 0.623477	val: 1.389934	test: 2.469988
MAE train: 0.475222	val: 1.082078	test: 1.767398

Epoch: 116
Loss: 0.6153535544872284
RMSE train: 0.653704	val: 1.379317	test: 2.558180
MAE train: 0.497476	val: 1.033335	test: 1.832489

Epoch: 117
Loss: 0.4636845737695694
RMSE train: 0.651129	val: 1.393694	test: 2.615206
MAE train: 0.473401	val: 1.017053	test: 1.845839

Epoch: 118
Loss: 0.5446081757545471
RMSE train: 0.607797	val: 1.412569	test: 2.649643
MAE train: 0.441675	val: 1.063497	test: 1.851575

Epoch: 119
Loss: 0.5266828089952469
RMSE train: 0.539289	val: 1.442881	test: 2.612473
MAE train: 0.401567	val: 1.143504	test: 1.825460

Epoch: 120
Loss: 0.39869967103004456
RMSE train: 0.502116	val: 1.459042	test: 2.579768
MAE train: 0.389224	val: 1.179752	test: 1.807222

Epoch: 121
Loss: 0.48734408617019653
RMSE train: 0.474293	val: 1.463286	test: 2.531194
MAE train: 0.370094	val: 1.188208	test: 1.782173

Epoch: 122
Loss: 0.6377175152301788
RMSE train: 0.475116	val: 1.459399	test: 2.518018
MAE train: 0.356248	val: 1.179970	test: 1.799092

Epoch: 123
Loss: 0.5002201795578003
RMSE train: 0.471976	val: 1.448604	test: 2.478701
MAE train: 0.353312	val: 1.166816	test: 1.795278

Epoch: 124
Loss: 0.37401142716407776
RMSE train: 0.470199	val: 1.423397	test: 2.498651
MAE train: 0.351020	val: 1.141854	test: 1.808257

Epoch: 125
Loss: 0.49006882309913635
RMSE train: 0.479273	val: 1.406737	test: 2.495417
MAE train: 0.357242	val: 1.128011	test: 1.798402

Epoch: 126
Loss: 0.5229165256023407
RMSE train: 0.499017	val: 1.430397	test: 2.457058
MAE train: 0.375886	val: 1.160992	test: 1.778336

Epoch: 127
Loss: 0.7640881538391113
RMSE train: 0.534757	val: 1.510881	test: 2.432401
MAE train: 0.403173	val: 1.263004	test: 1.765893

Epoch: 128
Loss: 0.7905958890914917
RMSE train: 0.597887	val: 1.510913	test: 2.467049
MAE train: 0.447521	val: 1.262085	test: 1.797803

Early stopping
Best (RMSE):	 train: 0.642250	val: 1.297009	test: 2.485495
Best (MAE):	 train: 0.496366	val: 0.948387	test: 1.773517


Epoch: 84
Loss: 0.6641174852848053
RMSE train: 0.653499	val: 1.355751	test: 2.462023
MAE train: 0.495009	val: 1.044426	test: 1.640043

Epoch: 85
Loss: 0.6675234138965607
RMSE train: 0.650080	val: 1.398713	test: 2.555637
MAE train: 0.487179	val: 1.054453	test: 1.696405

Epoch: 86
Loss: 0.5681080222129822
RMSE train: 0.636926	val: 1.386996	test: 2.486532
MAE train: 0.480677	val: 1.039201	test: 1.649022

Epoch: 87
Loss: 0.5188648700714111
RMSE train: 0.623200	val: 1.376709	test: 2.442423
MAE train: 0.477859	val: 1.036610	test: 1.622829

Epoch: 88
Loss: 0.6239801347255707
RMSE train: 0.633200	val: 1.374636	test: 2.486139
MAE train: 0.497494	val: 1.058642	test: 1.660564

Epoch: 89
Loss: 0.7249632477760315
RMSE train: 0.640200	val: 1.366637	test: 2.531601
MAE train: 0.504096	val: 1.046441	test: 1.683069

Epoch: 90
Loss: 0.663919597864151
RMSE train: 0.629932	val: 1.369095	test: 2.546315
MAE train: 0.474392	val: 1.026465	test: 1.665053

Epoch: 91
Loss: 0.6217215359210968
RMSE train: 0.610356	val: 1.317735	test: 2.437197
MAE train: 0.460714	val: 0.990119	test: 1.602730

Epoch: 92
Loss: 0.5133075267076492
RMSE train: 0.600777	val: 1.293561	test: 2.345124
MAE train: 0.467133	val: 1.005521	test: 1.561387

Epoch: 93
Loss: 0.5568014979362488
RMSE train: 0.593119	val: 1.283884	test: 2.275627
MAE train: 0.465270	val: 0.982485	test: 1.523307

Epoch: 94
Loss: 0.6832019984722137
RMSE train: 0.581767	val: 1.297357	test: 2.201837
MAE train: 0.435015	val: 0.974327	test: 1.498060

Epoch: 95
Loss: 0.6516334712505341
RMSE train: 0.580317	val: 1.250898	test: 2.016852
MAE train: 0.435369	val: 0.940170	test: 1.407356

Epoch: 96
Loss: 0.6183558702468872
RMSE train: 0.590676	val: 1.218831	test: 1.933574
MAE train: 0.444380	val: 0.920116	test: 1.357747

Epoch: 97
Loss: 0.541744589805603
RMSE train: 0.608455	val: 1.203453	test: 1.900124
MAE train: 0.461171	val: 0.938181	test: 1.331144

Epoch: 98
Loss: 0.506411224603653
RMSE train: 0.604924	val: 1.212060	test: 1.908654
MAE train: 0.461960	val: 0.946656	test: 1.334877

Epoch: 99
Loss: 0.6979988217353821
RMSE train: 0.541605	val: 1.225081	test: 1.928304
MAE train: 0.405551	val: 0.934156	test: 1.366538

Epoch: 100
Loss: 0.6270763278007507
RMSE train: 0.522987	val: 1.290156	test: 2.098978
MAE train: 0.388454	val: 0.981772	test: 1.461566

Epoch: 101
Loss: 0.5545534789562225
RMSE train: 0.554690	val: 1.395615	test: 2.355200
MAE train: 0.409737	val: 1.046203	test: 1.607283

Epoch: 102
Loss: 0.6430616676807404
RMSE train: 0.557933	val: 1.425765	test: 2.429073
MAE train: 0.420491	val: 1.071243	test: 1.647369

Epoch: 103
Loss: 0.5134333521127701
RMSE train: 0.592587	val: 1.439321	test: 2.444929
MAE train: 0.459203	val: 1.094567	test: 1.658618

Epoch: 104
Loss: 0.5285356044769287
RMSE train: 0.602988	val: 1.438087	test: 2.479918
MAE train: 0.455135	val: 1.094045	test: 1.699607

Epoch: 105
Loss: 0.4724041819572449
RMSE train: 0.596744	val: 1.435647	test: 2.541259
MAE train: 0.438586	val: 1.077798	test: 1.750561

Epoch: 106
Loss: 0.5632786154747009
RMSE train: 0.586898	val: 1.418256	test: 2.552431
MAE train: 0.423891	val: 1.084805	test: 1.745756

Epoch: 107
Loss: 0.5441447049379349
RMSE train: 0.603086	val: 1.423536	test: 2.580445
MAE train: 0.429415	val: 1.119422	test: 1.735338

Epoch: 108
Loss: 0.5124553143978119
RMSE train: 0.566567	val: 1.389014	test: 2.434184
MAE train: 0.416747	val: 1.113705	test: 1.628261

Epoch: 109
Loss: 0.5670180320739746
RMSE train: 0.525552	val: 1.337550	test: 2.278791
MAE train: 0.391755	val: 1.053490	test: 1.523992

Epoch: 110
Loss: 0.5807648301124573
RMSE train: 0.518860	val: 1.328736	test: 2.310767
MAE train: 0.372073	val: 0.970622	test: 1.572836

Epoch: 111
Loss: 0.5654040277004242
RMSE train: 0.492028	val: 1.281909	test: 2.168943
MAE train: 0.361842	val: 0.928222	test: 1.492822

Epoch: 112
Loss: 0.5081869065761566
RMSE train: 0.487464	val: 1.261837	test: 2.094259
MAE train: 0.372164	val: 0.959745	test: 1.434536

Epoch: 113
Loss: 0.4618625193834305
RMSE train: 0.515038	val: 1.300862	test: 2.148140
MAE train: 0.399834	val: 1.047296	test: 1.451679

Epoch: 114
Loss: 0.495778426527977
RMSE train: 0.534402	val: 1.328787	test: 2.308464
MAE train: 0.403498	val: 1.059577	test: 1.550956

Epoch: 115
Loss: 0.5416373908519745
RMSE train: 0.509621	val: 1.320338	test: 2.329499
MAE train: 0.382352	val: 1.015165	test: 1.580043

Epoch: 116
Loss: 0.7617191076278687
RMSE train: 0.557876	val: 1.337388	test: 2.359650
MAE train: 0.423550	val: 1.024731	test: 1.606833

Epoch: 117
Loss: 0.44479627907276154
RMSE train: 0.611623	val: 1.360825	test: 2.322123
MAE train: 0.461175	val: 1.065345	test: 1.576804

Epoch: 118
Loss: 0.6393611878156662
RMSE train: 0.635512	val: 1.385768	test: 2.327713
MAE train: 0.475337	val: 1.101661	test: 1.563953

Epoch: 119
Loss: 0.5939266383647919
RMSE train: 0.574042	val: 1.367833	test: 2.285475
MAE train: 0.426688	val: 1.080296	test: 1.541426

Epoch: 120
Loss: 0.4349743276834488
RMSE train: 0.552292	val: 1.373339	test: 2.414456
MAE train: 0.394226	val: 1.042847	test: 1.626536

Epoch: 121
Loss: 0.5293943881988525
RMSE train: 0.548571	val: 1.372452	test: 2.501402
MAE train: 0.387404	val: 1.040788	test: 1.678342

Epoch: 122
Loss: 0.5085786283016205
RMSE train: 0.481383	val: 1.358510	test: 2.430656
MAE train: 0.344570	val: 1.073275	test: 1.628280

Epoch: 123
Loss: 0.4793272614479065
RMSE train: 0.458344	val: 1.396094	test: 2.400969
MAE train: 0.333925	val: 1.137548	test: 1.617693

Epoch: 124
Loss: 0.43361982703208923
RMSE train: 0.444681	val: 1.372451	test: 2.332468
MAE train: 0.334555	val: 1.122033	test: 1.592740

Epoch: 125
Loss: 0.5803102254867554
RMSE train: 0.466362	val: 1.321951	test: 2.315597
MAE train: 0.356699	val: 1.059297	test: 1.575803

Epoch: 126
Loss: 0.46964290738105774
RMSE train: 0.503504	val: 1.286039	test: 2.366092
MAE train: 0.372153	val: 0.991606	test: 1.622322

Epoch: 127
Loss: 0.7624866366386414
RMSE train: 0.526933	val: 1.268305	test: 2.346791
MAE train: 0.394433	val: 0.949028	test: 1.621033

Epoch: 128
Loss: 0.5775522142648697
RMSE train: 0.526636	val: 1.255107	test: 2.283595
MAE train: 0.408991	val: 0.949788	test: 1.576886

Epoch: 129
Loss: 0.4616664946079254
RMSE train: 0.547494	val: 1.249577	test: 2.333244
MAE train: 0.426495	val: 0.951315	test: 1.603665

Epoch: 130
Loss: 0.5124260634183884
RMSE train: 0.585719	val: 1.264855	test: 2.438069
MAE train: 0.453204	val: 0.949063	test: 1.668071

Epoch: 131
Loss: 0.6731298267841339
RMSE train: 0.629588	val: 1.346796	test: 2.589698
MAE train: 0.476783	val: 0.992436	test: 1.750500

Epoch: 132
Loss: 0.4958903640508652
RMSE train: 0.648859	val: 1.373768	test: 2.642635
MAE train: 0.480138	val: 1.008968	test: 1.774019

Early stopping
Best (RMSE):	 train: 0.608455	val: 1.203453	test: 1.900124
Best (MAE):	 train: 0.461171	val: 0.938181	test: 1.331144
All runs completed.


Epoch: 84
Loss: 0.77119579911232
RMSE train: 0.675357	val: 1.357328	test: 2.143050
MAE train: 0.510958	val: 0.981459	test: 1.461175

Epoch: 85
Loss: 0.8740207254886627
RMSE train: 0.680257	val: 1.342876	test: 2.139723
MAE train: 0.507328	val: 0.972299	test: 1.462004

Epoch: 86
Loss: 0.976441890001297
RMSE train: 0.701349	val: 1.307371	test: 2.135443
MAE train: 0.525407	val: 0.935257	test: 1.472801

Epoch: 87
Loss: 0.8052627742290497
RMSE train: 0.691333	val: 1.255948	test: 2.137561
MAE train: 0.531673	val: 0.874775	test: 1.525567

Epoch: 88
Loss: 0.8560787737369537
RMSE train: 0.663967	val: 1.221172	test: 2.133023
MAE train: 0.512731	val: 0.845822	test: 1.573318

Epoch: 89
Loss: 0.7042049765586853
RMSE train: 0.617660	val: 1.205807	test: 2.086667
MAE train: 0.472737	val: 0.853539	test: 1.559733

Epoch: 90
Loss: 0.5521624982357025
RMSE train: 0.576925	val: 1.214753	test: 2.059217
MAE train: 0.431268	val: 0.866372	test: 1.511474

Epoch: 91
Loss: 0.8217694163322449
RMSE train: 0.544075	val: 1.235263	test: 2.056263
MAE train: 0.404242	val: 0.883609	test: 1.488486

Epoch: 92
Loss: 0.9592026472091675
RMSE train: 0.560095	val: 1.278987	test: 2.078140
MAE train: 0.405936	val: 0.910473	test: 1.473765

Epoch: 93
Loss: 0.9560345113277435
RMSE train: 0.608251	val: 1.309280	test: 2.096035
MAE train: 0.433975	val: 0.939940	test: 1.475274

Epoch: 94
Loss: 0.7031526565551758
RMSE train: 0.627286	val: 1.286601	test: 2.089099
MAE train: 0.456295	val: 0.910655	test: 1.490315

Epoch: 95
Loss: 0.7909361720085144
RMSE train: 0.640062	val: 1.281742	test: 2.097777
MAE train: 0.474475	val: 0.902858	test: 1.507099

Epoch: 96
Loss: 0.7192389369010925
RMSE train: 0.641433	val: 1.278315	test: 2.112748
MAE train: 0.472319	val: 0.902736	test: 1.529888

Epoch: 97
Loss: 0.8026566803455353
RMSE train: 0.613032	val: 1.294452	test: 2.110608
MAE train: 0.449351	val: 0.923727	test: 1.522772

Epoch: 98
Loss: 0.6571338772773743
RMSE train: 0.642586	val: 1.331005	test: 2.059878
MAE train: 0.459193	val: 0.966669	test: 1.470209

Epoch: 99
Loss: 0.7408654987812042
RMSE train: 0.649844	val: 1.350557	test: 2.037828
MAE train: 0.460517	val: 0.983600	test: 1.456242

Epoch: 100
Loss: 0.8786014020442963
RMSE train: 0.658994	val: 1.343193	test: 2.026041
MAE train: 0.471055	val: 0.979714	test: 1.463628

Epoch: 101
Loss: 0.7422434091567993
RMSE train: 0.651666	val: 1.322565	test: 2.031054
MAE train: 0.478372	val: 0.973994	test: 1.518951

Epoch: 102
Loss: 0.5650293231010437
RMSE train: 0.631281	val: 1.291636	test: 2.030494
MAE train: 0.472761	val: 0.941479	test: 1.508202

Epoch: 103
Loss: 0.7261173129081726
RMSE train: 0.591388	val: 1.273018	test: 2.032148
MAE train: 0.438262	val: 0.913292	test: 1.503104

Epoch: 104
Loss: 0.7455456256866455
RMSE train: 0.601897	val: 1.291810	test: 2.065201
MAE train: 0.430576	val: 0.927073	test: 1.516105

Epoch: 105
Loss: 0.5712467432022095
RMSE train: 0.598070	val: 1.280365	test: 2.054778
MAE train: 0.430605	val: 0.917880	test: 1.501591

Epoch: 106
Loss: 0.6576041877269745
RMSE train: 0.569059	val: 1.268239	test: 2.044717
MAE train: 0.419529	val: 0.901411	test: 1.485147

Epoch: 107
Loss: 0.7738165259361267
RMSE train: 0.578922	val: 1.238234	test: 2.023461
MAE train: 0.435594	val: 0.881843	test: 1.473677

Epoch: 108
Loss: 0.5349081456661224
RMSE train: 0.640377	val: 1.230658	test: 2.019174
MAE train: 0.474262	val: 0.884990	test: 1.470370

Epoch: 109
Loss: 0.6759830415248871
RMSE train: 0.644687	val: 1.223884	test: 2.031721
MAE train: 0.479131	val: 0.872048	test: 1.462998

Epoch: 110
Loss: 0.5907549858093262
RMSE train: 0.686021	val: 1.258969	test: 2.094279
MAE train: 0.496936	val: 0.883561	test: 1.479355

Epoch: 111
Loss: 0.6129632890224457
RMSE train: 0.692529	val: 1.294381	test: 2.166127
MAE train: 0.503280	val: 0.896648	test: 1.524587

Epoch: 112
Loss: 1.028797686100006
RMSE train: 0.680595	val: 1.290494	test: 2.136144
MAE train: 0.516601	val: 0.887183	test: 1.481142

Epoch: 113
Loss: 0.7663087844848633
RMSE train: 0.627124	val: 1.251210	test: 2.096756
MAE train: 0.484498	val: 0.880695	test: 1.478990

Epoch: 114
Loss: 0.7744764089584351
RMSE train: 0.615722	val: 1.226378	test: 2.085290
MAE train: 0.479218	val: 0.889898	test: 1.495095

Epoch: 115
Loss: 0.6910525560379028
RMSE train: 0.604527	val: 1.215903	test: 2.077437
MAE train: 0.457720	val: 0.882254	test: 1.478063

Epoch: 116
Loss: 0.6935290396213531
RMSE train: 0.626232	val: 1.216511	test: 2.103722
MAE train: 0.463439	val: 0.874349	test: 1.505479

Epoch: 117
Loss: 0.5945795774459839
RMSE train: 0.682579	val: 1.213182	test: 2.065835
MAE train: 0.494100	val: 0.875398	test: 1.489745

Epoch: 118
Loss: 0.6126068830490112
RMSE train: 0.711395	val: 1.209879	test: 2.034137
MAE train: 0.499452	val: 0.876758	test: 1.486519

Epoch: 119
Loss: 0.6358529925346375
RMSE train: 0.688577	val: 1.210961	test: 2.027782
MAE train: 0.480621	val: 0.870491	test: 1.524895

Epoch: 120
Loss: 0.8268634378910065
RMSE train: 0.603432	val: 1.216394	test: 2.045923
MAE train: 0.433999	val: 0.866578	test: 1.566951

Epoch: 121
Loss: 0.6163482666015625
RMSE train: 0.557994	val: 1.236346	test: 2.102160
MAE train: 0.406736	val: 0.887185	test: 1.610315

Epoch: 122
Loss: 0.6999633312225342
RMSE train: 0.549179	val: 1.238875	test: 2.069638
MAE train: 0.401734	val: 0.876264	test: 1.535973

Epoch: 123
Loss: 0.6831433773040771
RMSE train: 0.559289	val: 1.268995	test: 2.048520
MAE train: 0.417092	val: 0.894387	test: 1.466499

Epoch: 124
Loss: 0.5425774157047272
RMSE train: 0.604091	val: 1.308119	test: 2.048468
MAE train: 0.457866	val: 0.950577	test: 1.461999

Early stopping
Best (RMSE):	 train: 0.617660	val: 1.205807	test: 2.086667
Best (MAE):	 train: 0.472737	val: 0.853539	test: 1.559733


Epoch: 84
Loss: 0.7040018737316132
RMSE train: 0.755630	val: 1.306518	test: 2.083442
MAE train: 0.579747	val: 0.958277	test: 1.423236

Epoch: 85
Loss: 0.7442788481712341
RMSE train: 0.782761	val: 1.322979	test: 2.117765
MAE train: 0.611925	val: 0.976130	test: 1.443497

Epoch: 86
Loss: 0.7777728140354156
RMSE train: 0.811735	val: 1.388041	test: 2.178659
MAE train: 0.642102	val: 1.030125	test: 1.486593

Epoch: 87
Loss: 0.5839126706123352
RMSE train: 0.793157	val: 1.363211	test: 2.124257
MAE train: 0.616169	val: 0.985574	test: 1.407213

Epoch: 88
Loss: 0.7288754284381866
RMSE train: 0.772726	val: 1.331279	test: 2.073468
MAE train: 0.593685	val: 0.943618	test: 1.371853

Epoch: 89
Loss: 0.7254820764064789
RMSE train: 0.724803	val: 1.267438	test: 2.020631
MAE train: 0.552771	val: 0.893721	test: 1.388890

Epoch: 90
Loss: 0.6656108796596527
RMSE train: 0.672375	val: 1.226062	test: 1.998942
MAE train: 0.519214	val: 0.874850	test: 1.329301

Epoch: 91
Loss: 0.9059065878391266
RMSE train: 0.648062	val: 1.207572	test: 2.066008
MAE train: 0.499890	val: 0.884045	test: 1.376109

Epoch: 92
Loss: 0.6564830243587494
RMSE train: 0.646879	val: 1.196132	test: 2.060260
MAE train: 0.491512	val: 0.878941	test: 1.375806

Epoch: 93
Loss: 0.7154906988143921
RMSE train: 0.663978	val: 1.191806	test: 2.050591
MAE train: 0.502819	val: 0.870143	test: 1.353123

Epoch: 94
Loss: 0.7785548567771912
RMSE train: 0.684866	val: 1.198214	test: 2.007619
MAE train: 0.526174	val: 0.862571	test: 1.344670

Epoch: 95
Loss: 0.6403314173221588
RMSE train: 0.730427	val: 1.241367	test: 2.005463
MAE train: 0.565215	val: 0.876026	test: 1.360135

Epoch: 96
Loss: 0.7650012373924255
RMSE train: 0.777159	val: 1.355913	test: 2.074704
MAE train: 0.608755	val: 0.981633	test: 1.419793

Epoch: 97
Loss: 1.2398906350135803
RMSE train: 0.818667	val: 1.416037	test: 2.118639
MAE train: 0.648149	val: 1.043906	test: 1.454243

Epoch: 98
Loss: 0.6135125160217285
RMSE train: 0.848707	val: 1.468647	test: 2.191238
MAE train: 0.678830	val: 1.103845	test: 1.508835

Epoch: 99
Loss: 0.7356915473937988
RMSE train: 0.827342	val: 1.418192	test: 2.149184
MAE train: 0.666425	val: 1.060507	test: 1.467995

Epoch: 100
Loss: 0.6272646486759186
RMSE train: 0.761384	val: 1.368609	test: 2.074249
MAE train: 0.595447	val: 1.008582	test: 1.402389

Epoch: 101
Loss: 0.6287555992603302
RMSE train: 0.738412	val: 1.338118	test: 2.026512
MAE train: 0.577915	val: 0.976636	test: 1.382641

Epoch: 102
Loss: 1.0875308215618134
RMSE train: 0.739587	val: 1.365754	test: 2.046815
MAE train: 0.582359	val: 0.998275	test: 1.408909

Epoch: 103
Loss: 0.7948353886604309
RMSE train: 0.737793	val: 1.429011	test: 2.145256
MAE train: 0.569991	val: 1.051062	test: 1.523813

Epoch: 104
Loss: 0.6829175055027008
RMSE train: 0.728837	val: 1.410874	test: 2.139739
MAE train: 0.559263	val: 1.043168	test: 1.503023

Epoch: 105
Loss: 0.6054969131946564
RMSE train: 0.689895	val: 1.298745	test: 2.001654
MAE train: 0.548098	val: 0.949692	test: 1.349434

Epoch: 106
Loss: 1.1797153055667877
RMSE train: 0.719674	val: 1.310112	test: 2.019226
MAE train: 0.559229	val: 0.971311	test: 1.330146

Epoch: 107
Loss: 0.6170265972614288
RMSE train: 0.730797	val: 1.329579	test: 2.055900
MAE train: 0.587749	val: 0.996178	test: 1.365939

Epoch: 108
Loss: 0.5701985657215118
RMSE train: 0.742791	val: 1.322639	test: 2.068420
MAE train: 0.602685	val: 0.991833	test: 1.401190

Epoch: 109
Loss: 0.8697826564311981
RMSE train: 0.716815	val: 1.304250	test: 2.057787
MAE train: 0.563800	val: 0.963232	test: 1.424016

Epoch: 110
Loss: 0.9839972853660583
RMSE train: 0.692317	val: 1.285792	test: 2.039190
MAE train: 0.535867	val: 0.927701	test: 1.385272

Epoch: 111
Loss: 0.7619225680828094
RMSE train: 0.622572	val: 1.257399	test: 1.999602
MAE train: 0.483523	val: 0.892903	test: 1.332483

Epoch: 112
Loss: 0.7958032488822937
RMSE train: 0.567964	val: 1.229348	test: 1.944560
MAE train: 0.450262	val: 0.872271	test: 1.302227

Epoch: 113
Loss: 0.6610449552536011
RMSE train: 0.572288	val: 1.234569	test: 1.979572
MAE train: 0.447444	val: 0.887705	test: 1.322577

Epoch: 114
Loss: 0.6421952247619629
RMSE train: 0.580455	val: 1.249839	test: 1.997875
MAE train: 0.451515	val: 0.906932	test: 1.349905

Epoch: 115
Loss: 0.5983864963054657
RMSE train: 0.580965	val: 1.261388	test: 2.002107
MAE train: 0.456252	val: 0.918501	test: 1.360196

Epoch: 116
Loss: 0.6729231774806976
RMSE train: 0.570964	val: 1.245277	test: 1.963190
MAE train: 0.449415	val: 0.905511	test: 1.329682

Epoch: 117
Loss: 0.6937403976917267
RMSE train: 0.637502	val: 1.246981	test: 1.952898
MAE train: 0.484313	val: 0.902817	test: 1.343837

Epoch: 118
Loss: 0.6003126502037048
RMSE train: 0.732531	val: 1.256829	test: 1.964724
MAE train: 0.521891	val: 0.906268	test: 1.365344

Epoch: 119
Loss: 0.6471467912197113
RMSE train: 0.746537	val: 1.293522	test: 1.993810
MAE train: 0.525602	val: 0.945564	test: 1.373409

Epoch: 120
Loss: 0.5951623618602753
RMSE train: 0.715760	val: 1.350666	test: 2.063205
MAE train: 0.504536	val: 1.011916	test: 1.450642

Epoch: 121
Loss: 0.8741851449012756
RMSE train: 0.639227	val: 1.316673	test: 2.028485
MAE train: 0.476930	val: 0.991145	test: 1.404058

Epoch: 122
Loss: 0.6008402109146118
RMSE train: 0.587327	val: 1.245876	test: 1.906031
MAE train: 0.449918	val: 0.911187	test: 1.278184

Epoch: 123
Loss: 0.6465966999530792
RMSE train: 0.619677	val: 1.278777	test: 1.909190
MAE train: 0.475515	val: 0.939670	test: 1.367808

Epoch: 124
Loss: 0.8215040266513824
RMSE train: 0.623634	val: 1.254518	test: 1.893380
MAE train: 0.479829	val: 0.911236	test: 1.294828

Epoch: 125
Loss: 0.6150260269641876
RMSE train: 0.652856	val: 1.278334	test: 2.004614
MAE train: 0.480169	val: 0.925258	test: 1.371626

Epoch: 126
Loss: 0.7989471852779388
RMSE train: 0.668924	val: 1.344461	test: 2.109605
MAE train: 0.466003	val: 0.989256	test: 1.497605

Epoch: 127
Loss: 0.6313980221748352
RMSE train: 0.611493	val: 1.338228	test: 2.061871
MAE train: 0.434812	val: 0.980834	test: 1.420824

Epoch: 128
Loss: 0.8064988255500793
RMSE train: 0.599246	val: 1.279723	test: 1.987792
MAE train: 0.449509	val: 0.930698	test: 1.332695

Early stopping
Best (RMSE):	 train: 0.663978	val: 1.191806	test: 2.050591
Best (MAE):	 train: 0.502819	val: 0.870143	test: 1.353123


Epoch: 84
Loss: 0.6110325157642365
RMSE train: 0.713490	val: 3.358800	test: 2.225571
MAE train: 0.582209	val: 2.174814	test: 1.538649

Epoch: 85
Loss: 0.5931386053562164
RMSE train: 0.744613	val: 3.485070	test: 2.281414
MAE train: 0.610370	val: 2.307122	test: 1.585848

Epoch: 86
Loss: 0.6734614074230194
RMSE train: 0.787575	val: 3.569170	test: 2.341565
MAE train: 0.637342	val: 2.397456	test: 1.633092

Epoch: 87
Loss: 0.6730192601680756
RMSE train: 0.833112	val: 3.557612	test: 2.378754
MAE train: 0.679561	val: 2.378422	test: 1.680698

Epoch: 88
Loss: 0.6199024021625519
RMSE train: 0.860974	val: 3.475423	test: 2.351622
MAE train: 0.709475	val: 2.322794	test: 1.674049

Epoch: 89
Loss: 0.571627289056778
RMSE train: 0.819532	val: 3.324620	test: 2.253006
MAE train: 0.683048	val: 2.219843	test: 1.592609

Epoch: 90
Loss: 0.7284033596515656
RMSE train: 0.718769	val: 3.244998	test: 2.194637
MAE train: 0.595007	val: 2.145755	test: 1.521860

Epoch: 91
Loss: 0.5822899341583252
RMSE train: 0.616817	val: 3.221975	test: 2.186314
MAE train: 0.493730	val: 2.113048	test: 1.494709

Epoch: 92
Loss: 0.5879794657230377
RMSE train: 0.584895	val: 3.210344	test: 2.202554
MAE train: 0.449506	val: 2.097555	test: 1.500977

Epoch: 93
Loss: 0.6299474835395813
RMSE train: 0.593720	val: 3.189948	test: 2.188489
MAE train: 0.453511	val: 2.081158	test: 1.487874

Epoch: 94
Loss: 0.6160090863704681
RMSE train: 0.609415	val: 3.162652	test: 2.154629
MAE train: 0.466807	val: 2.060253	test: 1.474458

Epoch: 95
Loss: 0.583222508430481
RMSE train: 0.604092	val: 3.149858	test: 2.149220
MAE train: 0.461864	val: 2.038619	test: 1.475249

Epoch: 96
Loss: 0.4669168293476105
RMSE train: 0.560606	val: 3.126498	test: 2.133818
MAE train: 0.438995	val: 2.056165	test: 1.457733

Epoch: 97
Loss: 0.5893549025058746
RMSE train: 0.554535	val: 3.125076	test: 2.140386
MAE train: 0.436253	val: 2.065127	test: 1.453178

Epoch: 98
Loss: 0.5724921226501465
RMSE train: 0.565131	val: 3.123952	test: 2.132275
MAE train: 0.435882	val: 2.066827	test: 1.442368

Epoch: 99
Loss: 0.6052956879138947
RMSE train: 0.567501	val: 3.107223	test: 2.112298
MAE train: 0.434520	val: 2.050139	test: 1.417628

Epoch: 100
Loss: 0.5639886558055878
RMSE train: 0.562422	val: 3.153536	test: 2.090950
MAE train: 0.429614	val: 2.078559	test: 1.418224

Epoch: 101
Loss: 0.6488548815250397
RMSE train: 0.607771	val: 3.138631	test: 2.091185
MAE train: 0.463811	val: 2.048997	test: 1.432054

Epoch: 102
Loss: 0.5503420829772949
RMSE train: 0.624414	val: 3.074540	test: 2.119364
MAE train: 0.478441	val: 1.989271	test: 1.437606

Epoch: 103
Loss: 0.547086626291275
RMSE train: 0.651490	val: 3.090658	test: 2.198581
MAE train: 0.489362	val: 1.967461	test: 1.521764

Epoch: 104
Loss: 0.5887812376022339
RMSE train: 0.600736	val: 3.089049	test: 2.169562
MAE train: 0.451496	val: 1.945598	test: 1.461125

Epoch: 105
Loss: 0.5832728743553162
RMSE train: 0.555989	val: 3.122385	test: 2.130460
MAE train: 0.426919	val: 1.994788	test: 1.410227

Epoch: 106
Loss: 0.5683651566505432
RMSE train: 0.599943	val: 3.173373	test: 2.108803
MAE train: 0.464110	val: 2.042767	test: 1.409319

Epoch: 107
Loss: 0.5736882984638214
RMSE train: 0.662510	val: 3.155243	test: 2.111129
MAE train: 0.514007	val: 2.022145	test: 1.390885

Epoch: 108
Loss: 0.48438452184200287
RMSE train: 0.703039	val: 3.175102	test: 2.175779
MAE train: 0.550319	val: 2.033454	test: 1.496646

Epoch: 109
Loss: 0.5053235590457916
RMSE train: 0.715835	val: 3.225611	test: 2.246954
MAE train: 0.565458	val: 2.070012	test: 1.579992

Epoch: 110
Loss: 0.6053281426429749
RMSE train: 0.692945	val: 3.277423	test: 2.246165
MAE train: 0.555695	val: 2.112078	test: 1.559381

Epoch: 111
Loss: 0.49060942232608795
RMSE train: 0.639437	val: 3.276209	test: 2.208194
MAE train: 0.502205	val: 2.092262	test: 1.484165

Epoch: 112
Loss: 0.5094521641731262
RMSE train: 0.604295	val: 3.271869	test: 2.192835
MAE train: 0.464100	val: 2.067728	test: 1.456135

Epoch: 113
Loss: 0.5613439679145813
RMSE train: 0.586888	val: 3.318944	test: 2.198963
MAE train: 0.449004	val: 2.091950	test: 1.478631

Epoch: 114
Loss: 0.5687795579433441
RMSE train: 0.593583	val: 3.312483	test: 2.195959
MAE train: 0.459423	val: 2.091579	test: 1.473105

Epoch: 115
Loss: 0.44924749433994293
RMSE train: 0.616003	val: 3.251358	test: 2.143366
MAE train: 0.492169	val: 2.105366	test: 1.443975

Epoch: 116
Loss: 0.4842709004878998
RMSE train: 0.631888	val: 3.148133	test: 2.088168
MAE train: 0.506922	val: 2.056396	test: 1.428341

Epoch: 117
Loss: 0.534320741891861
RMSE train: 0.612168	val: 3.151307	test: 2.038521
MAE train: 0.482861	val: 2.044527	test: 1.384561

Epoch: 118
Loss: 0.4755132496356964
RMSE train: 0.592336	val: 3.125893	test: 2.018289
MAE train: 0.455586	val: 1.978316	test: 1.372711

Epoch: 119
Loss: 0.49409399926662445
RMSE train: 0.583427	val: 3.129704	test: 2.071885
MAE train: 0.449130	val: 1.943322	test: 1.419057

Epoch: 120
Loss: 0.5398969203233719
RMSE train: 0.589310	val: 3.202362	test: 2.156550
MAE train: 0.455783	val: 1.977011	test: 1.484234

Epoch: 121
Loss: 0.6066552996635437
RMSE train: 0.600916	val: 3.277958	test: 2.229458
MAE train: 0.470832	val: 2.051535	test: 1.547155

Early stopping
Best (RMSE):	 train: 1.744245	val: 2.854885	test: 2.404403
Best (MAE):	 train: 1.563260	val: 2.201406	test: 1.866658


Epoch: 84
Loss: 0.6531261205673218
RMSE train: 0.625467	val: 3.224321	test: 2.180978
MAE train: 0.475726	val: 2.024984	test: 1.553370

Epoch: 85
Loss: 0.6429420113563538
RMSE train: 0.602685	val: 3.213931	test: 2.176732
MAE train: 0.462917	val: 2.053602	test: 1.548477

Epoch: 86
Loss: 0.5670978128910065
RMSE train: 0.609576	val: 3.182823	test: 2.143785
MAE train: 0.473298	val: 2.059547	test: 1.533186

Epoch: 87
Loss: 0.5708380937576294
RMSE train: 0.639067	val: 3.119526	test: 2.099104
MAE train: 0.494507	val: 2.007738	test: 1.495570

Epoch: 88
Loss: 0.5542853325605392
RMSE train: 0.658714	val: 3.120296	test: 2.109476
MAE train: 0.503224	val: 1.989939	test: 1.478391

Epoch: 89
Loss: 0.6360044479370117
RMSE train: 0.643381	val: 3.157612	test: 2.140198
MAE train: 0.493128	val: 2.021427	test: 1.481959

Epoch: 90
Loss: 0.6991373002529144
RMSE train: 0.606682	val: 3.190231	test: 2.193303
MAE train: 0.473829	val: 2.075309	test: 1.540538

Epoch: 91
Loss: 0.5724155604839325
RMSE train: 0.605291	val: 3.266798	test: 2.263897
MAE train: 0.484072	val: 2.151178	test: 1.612022

Epoch: 92
Loss: 0.5921217501163483
RMSE train: 0.631026	val: 3.359380	test: 2.353427
MAE train: 0.499570	val: 2.202684	test: 1.685979

Epoch: 93
Loss: 0.5140527784824371
RMSE train: 0.632559	val: 3.344351	test: 2.359929
MAE train: 0.495338	val: 2.168715	test: 1.677358

Epoch: 94
Loss: 0.5332944095134735
RMSE train: 0.612171	val: 3.284500	test: 2.321853
MAE train: 0.468815	val: 2.111988	test: 1.631788

Epoch: 95
Loss: 0.5386384576559067
RMSE train: 0.619702	val: 3.270027	test: 2.280175
MAE train: 0.468442	val: 2.118639	test: 1.597200

Epoch: 96
Loss: 0.5878033638000488
RMSE train: 0.608771	val: 3.211906	test: 2.245021
MAE train: 0.451084	val: 2.113229	test: 1.592373

Epoch: 97
Loss: 0.5743729174137115
RMSE train: 0.605412	val: 3.255026	test: 2.227591
MAE train: 0.460367	val: 2.177790	test: 1.613295

Epoch: 98
Loss: 0.5790418088436127
RMSE train: 0.605564	val: 3.233814	test: 2.197960
MAE train: 0.471993	val: 2.162414	test: 1.596087

Epoch: 99
Loss: 0.5418754518032074
RMSE train: 0.613171	val: 3.271111	test: 2.195790
MAE train: 0.472089	val: 2.157451	test: 1.582188

Epoch: 100
Loss: 0.5696514397859573
RMSE train: 0.640483	val: 3.358502	test: 2.268226
MAE train: 0.487455	val: 2.176153	test: 1.629502

Epoch: 101
Loss: 0.5323149263858795
RMSE train: 0.625938	val: 3.331232	test: 2.305823
MAE train: 0.477539	val: 2.163491	test: 1.636604

Epoch: 102
Loss: 0.6071107685565948
RMSE train: 0.617496	val: 3.299188	test: 2.306494
MAE train: 0.471425	val: 2.168274	test: 1.635561

Epoch: 103
Loss: 0.6983159482479095
RMSE train: 0.596970	val: 3.294120	test: 2.261897
MAE train: 0.464044	val: 2.168070	test: 1.618925

Epoch: 104
Loss: 0.5555175244808197
RMSE train: 0.591457	val: 3.179356	test: 2.198825
MAE train: 0.471563	val: 2.071000	test: 1.562830

Epoch: 105
Loss: 0.5510518848896027
RMSE train: 0.571995	val: 3.192983	test: 2.195927
MAE train: 0.448261	val: 2.040896	test: 1.546129

Epoch: 106
Loss: 0.5888019800186157
RMSE train: 0.553028	val: 3.265315	test: 2.191219
MAE train: 0.425316	val: 2.054876	test: 1.539175

Epoch: 107
Loss: 0.471098393201828
RMSE train: 0.557080	val: 3.375915	test: 2.210309
MAE train: 0.417911	val: 2.106902	test: 1.565545

Epoch: 108
Loss: 0.5806622207164764
RMSE train: 0.570413	val: 3.427153	test: 2.227776
MAE train: 0.421451	val: 2.150165	test: 1.577903

Epoch: 109
Loss: 0.6012536883354187
RMSE train: 0.589613	val: 3.422707	test: 2.248719
MAE train: 0.433759	val: 2.172380	test: 1.577883

Epoch: 110
Loss: 0.5735102891921997
RMSE train: 0.575277	val: 3.327134	test: 2.173370
MAE train: 0.421418	val: 2.124491	test: 1.520033

Epoch: 111
Loss: 0.4696030169725418
RMSE train: 0.540538	val: 3.254739	test: 2.098474
MAE train: 0.401299	val: 2.103136	test: 1.481798

Epoch: 112
Loss: 0.5766726434230804
RMSE train: 0.480157	val: 3.149550	test: 2.083577
MAE train: 0.359001	val: 2.059724	test: 1.482109

Epoch: 113
Loss: 0.560968428850174
RMSE train: 0.450142	val: 3.197852	test: 2.143307
MAE train: 0.332182	val: 2.084524	test: 1.522537

Epoch: 114
Loss: 0.5103176832199097
RMSE train: 0.456337	val: 3.267849	test: 2.178672
MAE train: 0.338114	val: 2.107065	test: 1.538301

Epoch: 115
Loss: 0.5121103525161743
RMSE train: 0.494641	val: 3.233640	test: 2.161063
MAE train: 0.375633	val: 2.063186	test: 1.512617

Epoch: 116
Loss: 0.4637404680252075
RMSE train: 0.530871	val: 3.189396	test: 2.163633
MAE train: 0.412080	val: 2.025166	test: 1.523619

Epoch: 117
Loss: 0.4615366607904434
RMSE train: 0.549855	val: 3.183041	test: 2.158028
MAE train: 0.423973	val: 2.029437	test: 1.546386

Epoch: 118
Loss: 0.47016364336013794
RMSE train: 0.549047	val: 3.132765	test: 2.115634
MAE train: 0.421901	val: 2.020449	test: 1.519195

Epoch: 119
Loss: 0.4631499946117401
RMSE train: 0.541008	val: 3.104055	test: 2.059831
MAE train: 0.416358	val: 2.010801	test: 1.482082

Epoch: 120
Loss: 0.5217026174068451
RMSE train: 0.555604	val: 3.092724	test: 2.063978
MAE train: 0.430126	val: 1.997947	test: 1.484490

Epoch: 121
Loss: 0.5267332196235657
RMSE train: 0.579514	val: 3.176096	test: 2.095989
MAE train: 0.451058	val: 2.011895	test: 1.489347

Early stopping
Best (RMSE):	 train: 0.581069	val: 2.512245	test: 1.932221
Best (MAE):	 train: 0.449896	val: 1.743180	test: 1.410162


Epoch: 84
Loss: 0.6855487823486328
RMSE train: 0.664678	val: 3.672378	test: 2.303581
MAE train: 0.540116	val: 2.209367	test: 1.539365

Epoch: 85
Loss: 0.6120288372039795
RMSE train: 0.703166	val: 3.587371	test: 2.274325
MAE train: 0.561772	val: 2.143154	test: 1.519282

Epoch: 86
Loss: 0.569890022277832
RMSE train: 0.722568	val: 3.568365	test: 2.282219
MAE train: 0.550557	val: 2.109491	test: 1.527491

Epoch: 87
Loss: 0.6347566545009613
RMSE train: 0.725124	val: 3.571092	test: 2.294695
MAE train: 0.548298	val: 2.097845	test: 1.541704

Epoch: 88
Loss: 0.5900234580039978
RMSE train: 0.695760	val: 3.588623	test: 2.333433
MAE train: 0.525316	val: 2.132647	test: 1.570926

Epoch: 89
Loss: 0.6140351295471191
RMSE train: 0.657794	val: 3.647014	test: 2.416475
MAE train: 0.492287	val: 2.175064	test: 1.620581

Epoch: 90
Loss: 0.5969293415546417
RMSE train: 0.584552	val: 3.613280	test: 2.401886
MAE train: 0.439331	val: 2.209123	test: 1.620421

Epoch: 91
Loss: 0.6044661402702332
RMSE train: 0.536985	val: 3.589371	test: 2.401635
MAE train: 0.408911	val: 2.226499	test: 1.625201

Epoch: 92
Loss: 0.5948617458343506
RMSE train: 0.519760	val: 3.564797	test: 2.380328
MAE train: 0.403548	val: 2.195102	test: 1.621771

Epoch: 93
Loss: 0.6081329584121704
RMSE train: 0.518902	val: 3.487192	test: 2.304998
MAE train: 0.403652	val: 2.110239	test: 1.580823

Epoch: 94
Loss: 0.7000693380832672
RMSE train: 0.545930	val: 3.469717	test: 2.257610
MAE train: 0.413499	val: 2.055257	test: 1.552297

Epoch: 95
Loss: 0.6023799479007721
RMSE train: 0.556925	val: 3.425167	test: 2.205910
MAE train: 0.423267	val: 2.039521	test: 1.503201

Epoch: 96
Loss: 0.5031135678291321
RMSE train: 0.579360	val: 3.441944	test: 2.211111
MAE train: 0.446383	val: 2.073913	test: 1.485615

Epoch: 97
Loss: 0.6511703133583069
RMSE train: 0.608533	val: 3.507288	test: 2.242103
MAE train: 0.470622	val: 2.156772	test: 1.511812

Epoch: 98
Loss: 0.6474000215530396
RMSE train: 0.605260	val: 3.557457	test: 2.269289
MAE train: 0.462288	val: 2.198351	test: 1.551619

Epoch: 99
Loss: 0.549046516418457
RMSE train: 0.612167	val: 3.585820	test: 2.272034
MAE train: 0.463795	val: 2.202439	test: 1.554293

Epoch: 100
Loss: 0.6018036007881165
RMSE train: 0.598659	val: 3.568781	test: 2.275486
MAE train: 0.446990	val: 2.178348	test: 1.563433

Epoch: 101
Loss: 0.5138173252344131
RMSE train: 0.601114	val: 3.584605	test: 2.321410
MAE train: 0.450619	val: 2.181978	test: 1.595569

Epoch: 102
Loss: 0.5945273339748383
RMSE train: 0.590278	val: 3.597825	test: 2.371325
MAE train: 0.448989	val: 2.207951	test: 1.639243

Epoch: 103
Loss: 0.6028438806533813
RMSE train: 0.569001	val: 3.632703	test: 2.387705
MAE train: 0.437367	val: 2.262460	test: 1.646908

Epoch: 104
Loss: 0.5766467154026031
RMSE train: 0.534978	val: 3.657642	test: 2.358911
MAE train: 0.412384	val: 2.305655	test: 1.610139

Epoch: 105
Loss: 0.5389602929353714
RMSE train: 0.523276	val: 3.676343	test: 2.282763
MAE train: 0.399315	val: 2.317849	test: 1.542802

Epoch: 106
Loss: 0.5748217850923538
RMSE train: 0.542600	val: 3.625080	test: 2.225803
MAE train: 0.415019	val: 2.262273	test: 1.499287

Epoch: 107
Loss: 0.5611103773117065
RMSE train: 0.593343	val: 3.522206	test: 2.193023
MAE train: 0.455439	val: 2.146087	test: 1.466734

Epoch: 108
Loss: 0.5679117739200592
RMSE train: 0.614079	val: 3.508824	test: 2.205509
MAE train: 0.469984	val: 2.085753	test: 1.462818

Epoch: 109
Loss: 0.4934742599725723
RMSE train: 0.603867	val: 3.533733	test: 2.221642
MAE train: 0.468789	val: 2.096544	test: 1.477343

Epoch: 110
Loss: 0.48449964821338654
RMSE train: 0.592037	val: 3.558854	test: 2.264034
MAE train: 0.463663	val: 2.107812	test: 1.511714

Epoch: 111
Loss: 0.5066479444503784
RMSE train: 0.578225	val: 3.585472	test: 2.309507
MAE train: 0.454303	val: 2.129377	test: 1.555381

Epoch: 112
Loss: 0.4534883052110672
RMSE train: 0.554846	val: 3.576563	test: 2.317350
MAE train: 0.432252	val: 2.129916	test: 1.575340

Epoch: 113
Loss: 0.5314134061336517
RMSE train: 0.567722	val: 3.585602	test: 2.291321
MAE train: 0.438013	val: 2.133426	test: 1.563902

Epoch: 114
Loss: 0.5512792766094208
RMSE train: 0.580717	val: 3.574891	test: 2.237252
MAE train: 0.447966	val: 2.135910	test: 1.506365

Epoch: 115
Loss: 0.4891217052936554
RMSE train: 0.588834	val: 3.549634	test: 2.202448
MAE train: 0.454589	val: 2.116953	test: 1.464849

Epoch: 116
Loss: 0.4897712469100952
RMSE train: 0.604768	val: 3.522781	test: 2.173601
MAE train: 0.461612	val: 2.105335	test: 1.438552

Epoch: 117
Loss: 0.538442999124527
RMSE train: 0.609684	val: 3.473946	test: 2.139867
MAE train: 0.469586	val: 2.108590	test: 1.420619

Epoch: 118
Loss: 0.5388936996459961
RMSE train: 0.576358	val: 3.423623	test: 2.102351
MAE train: 0.437201	val: 2.070621	test: 1.402615

Epoch: 119
Loss: 0.49578818678855896
RMSE train: 0.539836	val: 3.395669	test: 2.061839
MAE train: 0.406664	val: 2.064675	test: 1.385519

Epoch: 120
Loss: 0.4867742955684662
RMSE train: 0.517363	val: 3.446514	test: 2.052989
MAE train: 0.384652	val: 2.112696	test: 1.381131

Epoch: 121
Loss: 0.5241479277610779
RMSE train: 0.473848	val: 3.410566	test: 2.072179
MAE train: 0.353432	val: 2.090177	test: 1.396695

Early stopping
Best (RMSE):	 train: 0.637997	val: 3.324864	test: 2.113969
Best (MAE):	 train: 0.489373	val: 2.097473	test: 1.465558
All runs completed.


Epoch: 84
Loss: 0.847166508436203
RMSE train: 0.845010	val: 1.367837	test: 1.935768
MAE train: 0.647853	val: 0.963693	test: 1.338876

Epoch: 85
Loss: 0.8221277892589569
RMSE train: 0.843086	val: 1.452507	test: 2.045293
MAE train: 0.634171	val: 1.031781	test: 1.428679

Epoch: 86
Loss: 0.8528514206409454
RMSE train: 0.767472	val: 1.395762	test: 2.006531
MAE train: 0.574328	val: 1.005261	test: 1.420414

Epoch: 87
Loss: 0.6583897769451141
RMSE train: 0.689288	val: 1.284754	test: 1.895756
MAE train: 0.526894	val: 0.931453	test: 1.374344

Epoch: 88
Loss: 0.6625836789608002
RMSE train: 0.651898	val: 1.238138	test: 1.862812
MAE train: 0.500448	val: 0.897700	test: 1.383204

Epoch: 89
Loss: 0.6686916351318359
RMSE train: 0.670053	val: 1.235523	test: 1.877612
MAE train: 0.527820	val: 0.895339	test: 1.396866

Epoch: 90
Loss: 1.0789396166801453
RMSE train: 0.675745	val: 1.258932	test: 1.903431
MAE train: 0.530468	val: 0.913936	test: 1.378658

Epoch: 91
Loss: 0.6487165987491608
RMSE train: 0.713162	val: 1.316609	test: 1.951533
MAE train: 0.534740	val: 0.951553	test: 1.402096

Epoch: 92
Loss: 0.6111590564250946
RMSE train: 0.739740	val: 1.329704	test: 1.963065
MAE train: 0.550781	val: 0.951413	test: 1.382136

Epoch: 93
Loss: 0.6244371831417084
RMSE train: 0.783488	val: 1.308338	test: 1.937415
MAE train: 0.588531	val: 0.937657	test: 1.339807

Epoch: 94
Loss: 0.7604534327983856
RMSE train: 0.799379	val: 1.279106	test: 1.934807
MAE train: 0.604823	val: 0.918362	test: 1.319829

Epoch: 95
Loss: 0.6368695497512817
RMSE train: 0.799822	val: 1.263684	test: 1.940493
MAE train: 0.617958	val: 0.899959	test: 1.322265

Epoch: 96
Loss: 0.6670221388339996
RMSE train: 0.768452	val: 1.269219	test: 1.952899
MAE train: 0.599711	val: 0.901308	test: 1.338898

Epoch: 97
Loss: 0.6253075301647186
RMSE train: 0.728381	val: 1.280733	test: 1.966799
MAE train: 0.576144	val: 0.914825	test: 1.360724

Epoch: 98
Loss: 0.7372561395168304
RMSE train: 0.670331	val: 1.282536	test: 1.933006
MAE train: 0.516524	val: 0.911272	test: 1.347207

Epoch: 99
Loss: 0.6669094860553741
RMSE train: 0.640357	val: 1.261816	test: 1.885818
MAE train: 0.487062	val: 0.891329	test: 1.334488

Epoch: 100
Loss: 0.6044146716594696
RMSE train: 0.647600	val: 1.257765	test: 1.888710
MAE train: 0.492685	val: 0.901711	test: 1.412383

Epoch: 101
Loss: 0.6703047454357147
RMSE train: 0.650404	val: 1.268184	test: 1.835220
MAE train: 0.501622	val: 0.904769	test: 1.319767

Epoch: 102
Loss: 0.769940584897995
RMSE train: 0.699558	val: 1.309333	test: 1.850612
MAE train: 0.535237	val: 0.941775	test: 1.305402

Epoch: 103
Loss: 0.7629745602607727
RMSE train: 0.722517	val: 1.331466	test: 1.894598
MAE train: 0.564188	val: 0.967848	test: 1.346856

Epoch: 104
Loss: 0.6332298517227173
RMSE train: 0.725039	val: 1.340235	test: 1.928412
MAE train: 0.577580	val: 0.972714	test: 1.371864

Epoch: 105
Loss: 0.5567896366119385
RMSE train: 0.703136	val: 1.339343	test: 1.939587
MAE train: 0.558321	val: 0.969105	test: 1.376181

Epoch: 106
Loss: 0.724960446357727
RMSE train: 0.671791	val: 1.356394	test: 1.990834
MAE train: 0.519277	val: 0.969511	test: 1.413855

Epoch: 107
Loss: 0.5588673502206802
RMSE train: 0.627834	val: 1.346730	test: 1.969224
MAE train: 0.475099	val: 0.958163	test: 1.381404

Epoch: 108
Loss: 0.6400096714496613
RMSE train: 0.611933	val: 1.317874	test: 1.947344
MAE train: 0.459694	val: 0.944374	test: 1.348022

Epoch: 109
Loss: 0.7783094644546509
RMSE train: 0.637459	val: 1.304208	test: 1.924856
MAE train: 0.473311	val: 0.943608	test: 1.320233

Epoch: 110
Loss: 1.0115826427936554
RMSE train: 0.645623	val: 1.300151	test: 1.925511
MAE train: 0.485624	val: 0.938900	test: 1.322128

Epoch: 111
Loss: 0.6471889317035675
RMSE train: 0.694336	val: 1.296601	test: 1.923359
MAE train: 0.528424	val: 0.936771	test: 1.331245

Epoch: 112
Loss: 0.6611860394477844
RMSE train: 0.727361	val: 1.289209	test: 1.929989
MAE train: 0.563311	val: 0.923426	test: 1.335962

Epoch: 113
Loss: 0.5549622774124146
RMSE train: 0.725649	val: 1.279747	test: 1.946894
MAE train: 0.570168	val: 0.910310	test: 1.354283

Epoch: 114
Loss: 0.6519781351089478
RMSE train: 0.728683	val: 1.287317	test: 1.977133
MAE train: 0.565777	val: 0.913175	test: 1.377544

Epoch: 115
Loss: 0.5513996183872223
RMSE train: 0.708406	val: 1.296268	test: 1.977322
MAE train: 0.530503	val: 0.923798	test: 1.389169

Epoch: 116
Loss: 0.6805510520935059
RMSE train: 0.693042	val: 1.271053	test: 1.949596
MAE train: 0.511644	val: 0.913122	test: 1.364783

Epoch: 117
Loss: 0.6475485563278198
RMSE train: 0.718212	val: 1.263715	test: 1.960860
MAE train: 0.526058	val: 0.908100	test: 1.353149

Epoch: 118
Loss: 0.7097931802272797
RMSE train: 0.709456	val: 1.252316	test: 1.972188
MAE train: 0.534854	val: 0.885327	test: 1.354815

Epoch: 119
Loss: 0.6323782801628113
RMSE train: 0.716730	val: 1.246508	test: 1.972177
MAE train: 0.549278	val: 0.874622	test: 1.347295

Epoch: 120
Loss: 0.6233408749103546
RMSE train: 0.697658	val: 1.247006	test: 1.977414
MAE train: 0.536075	val: 0.879310	test: 1.339052

Epoch: 121
Loss: 0.6501817405223846
RMSE train: 0.674643	val: 1.257165	test: 2.032362
MAE train: 0.506719	val: 0.897837	test: 1.453080

Epoch: 122
Loss: 0.6076183915138245
RMSE train: 0.657914	val: 1.226117	test: 2.023507
MAE train: 0.492399	val: 0.876656	test: 1.473274

Epoch: 123
Loss: 0.6377801299095154
RMSE train: 0.625111	val: 1.204217	test: 1.983522
MAE train: 0.478310	val: 0.854008	test: 1.443362

Epoch: 124
Loss: 0.6409850120544434
RMSE train: 0.588555	val: 1.171913	test: 1.908473
MAE train: 0.458202	val: 0.830420	test: 1.386052

Epoch: 125
Loss: 0.5925315320491791
RMSE train: 0.615590	val: 1.169805	test: 1.911182
MAE train: 0.482178	val: 0.824395	test: 1.402568

Epoch: 126
Loss: 0.7307601869106293
RMSE train: 0.676481	val: 1.194552	test: 1.949912
MAE train: 0.535450	val: 0.834071	test: 1.432606

Epoch: 127
Loss: 0.5424655973911285
RMSE train: 0.758876	val: 1.248532	test: 2.009397
MAE train: 0.618706	val: 0.873727	test: 1.465197

Epoch: 128
Loss: 0.5184022933244705
RMSE train: 0.827223	val: 1.326023	test: 2.087424
MAE train: 0.690694	val: 0.942841	test: 1.525040

Epoch: 129
Loss: 0.5994619131088257
RMSE train: 0.775746	val: 1.298273	test: 2.055008
MAE train: 0.646130	val: 0.920922	test: 1.496456

Epoch: 130
Loss: 0.47542649507522583
RMSE train: 0.715021	val: 1.253851	test: 1.996922
MAE train: 0.577994	val: 0.888257	test: 1.427330

Epoch: 131
Loss: 0.497820645570755
RMSE train: 0.686046	val: 1.232612	test: 1.968350
MAE train: 0.529232	val: 0.878846	test: 1.386837

Epoch: 132
Loss: 0.767191469669342
RMSE train: 0.657172	val: 1.246140	test: 1.999879
MAE train: 0.500335	val: 0.880100	test: 1.402167

Epoch: 133
Loss: 0.5606742203235626
RMSE train: 0.638025	val: 1.229655	test: 1.995852
MAE train: 0.498083	val: 0.861083	test: 1.404584

Epoch: 134
Loss: 0.5967393517494202
RMSE train: 0.633349	val: 1.257604	test: 2.023677
MAE train: 0.499202	val: 0.871073	test: 1.411828

Epoch: 135
Loss: 0.818968653678894
RMSE train: 0.650091	val: 1.237347	test: 2.008786
MAE train: 0.516191	val: 0.857618	test: 1.413635

Epoch: 136
Loss: 0.561267226934433
RMSE train: 0.658387	val: 1.205451	test: 1.954735
MAE train: 0.516086	val: 0.845716	test: 1.387613

Epoch: 137
Loss: 0.7267970144748688
RMSE train: 0.680754	val: 1.210134	test: 1.922839
MAE train: 0.523399	val: 0.857032	test: 1.369325

Epoch: 138
Loss: 0.6031304001808167
RMSE train: 0.669789	val: 1.219549	test: 1.913465
MAE train: 0.511732	val: 0.869056	test: 1.366059

Epoch: 139
Loss: 0.6998512148857117
RMSE train: 0.648025	val: 1.200746	test: 1.880703
MAE train: 0.490392	val: 0.858954	test: 1.334127

Epoch: 140
Loss: 0.5750132501125336
RMSE train: 0.637557	val: 1.178463	test: 1.867346
MAE train: 0.485412	val: 0.839076	test: 1.321502

Epoch: 141
Loss: 0.8869287669658661
RMSE train: 0.630182	val: 1.169457	test: 1.873979
MAE train: 0.488203	val: 0.829535	test: 1.304834

Epoch: 142
Loss: 0.7718352377414703
RMSE train: 0.629631	val: 1.181835	test: 1.924282
MAE train: 0.493307	val: 0.826588	test: 1.326289

Epoch: 143
Loss: 0.5890506505966187
RMSE train: 0.633763	val: 1.204851	test: 1.961654
MAE train: 0.501661	val: 0.833235	test: 1.359193

Epoch: 144
Loss: 0.6496019661426544
RMSE train: 0.611181	val: 1.180753	test: 1.923937
MAE train: 0.482752	val: 0.815584	test: 1.330966

Epoch: 145
Loss: 0.537623792886734
RMSE train: 0.582648	val: 1.180776	test: 1.909966
MAE train: 0.452390	val: 0.824211	test: 1.322790

Epoch: 146
Loss: 0.525880753993988
RMSE train: 0.516925	val: 1.154809	test: 1.873262
MAE train: 0.397572	val: 0.823264	test: 1.315227

Epoch: 147
Loss: 0.45762762427330017
RMSE train: 0.505114	val: 1.137208	test: 1.860379
MAE train: 0.385066	val: 0.817049	test: 1.313472

Epoch: 148
Loss: 0.5743609368801117
RMSE train: 0.532659	val: 1.139069	test: 1.850804
MAE train: 0.398661	val: 0.829435	test: 1.306849

Epoch: 149
Loss: 0.790318638086319
RMSE train: 0.579498	val: 1.155433	test: 1.861250
MAE train: 0.438078	val: 0.839770	test: 1.299942

Epoch: 150
Loss: 0.5145174413919449
RMSE train: 0.625044	val: 1.198495	test: 1.961653
MAE train: 0.475560	val: 0.850102	test: 1.352738

Epoch: 151
Loss: 0.4962898790836334
RMSE train: 0.613159	val: 1.244322	test: 2.034288
MAE train: 0.461283	val: 0.872936	test: 1.447499

Epoch: 152
Loss: 0.5061697661876678
RMSE train: 0.542808	val: 1.240126	test: 2.022498
MAE train: 0.408557	val: 0.862411	test: 1.437978

Epoch: 153
Loss: 0.5041530579328537
RMSE train: 0.499543	val: 1.195565	test: 1.976274
MAE train: 0.397921	val: 0.841225	test: 1.396254

Epoch: 154
Loss: 0.7102051973342896
RMSE train: 0.531447	val: 1.204463	test: 1.982794
MAE train: 0.426420	val: 0.863055	test: 1.402331

Epoch: 155
Loss: 0.5131449103355408
RMSE train: 0.603034	val: 1.261107	test: 2.006263
MAE train: 0.466287	val: 0.903820	test: 1.377004

Epoch: 156
Loss: 0.43514324724674225
RMSE train: 0.644910	val: 1.313629	test: 2.045400
MAE train: 0.483417	val: 0.929551	test: 1.387007

Epoch: 157
Loss: 0.556612491607666
RMSE train: 0.606617	val: 1.289671	test: 2.007797
MAE train: 0.452638	val: 0.912577	test: 1.393762

Epoch: 158
Loss: 0.4759901762008667
RMSE train: 0.527459	val: 1.236631	test: 1.940010
MAE train: 0.402768	val: 0.874590	test: 1.373426

Epoch: 159
Loss: 0.8155149519443512
RMSE train: 0.491749	val: 1.212564	test: 1.862843
MAE train: 0.383630	val: 0.860629	test: 1.330754

Epoch: 160
Loss: 0.5286770612001419
RMSE train: 0.539178	val: 1.209865	test: 1.830367
MAE train: 0.430486	val: 0.862806	test: 1.309495

Epoch: 161
Loss: 0.4600602388381958
RMSE train: 0.580544	val: 1.227635	test: 1.851052
MAE train: 0.468442	val: 0.869534	test: 1.333532

Epoch: 162
Loss: 0.622395932674408
RMSE train: 0.611219	val: 1.274562	test: 1.930444
MAE train: 0.475510	val: 0.881666	test: 1.378682

Epoch: 163
Loss: 0.45772698521614075
RMSE train: 0.629519	val: 1.291506	test: 1.978364
MAE train: 0.472949	val: 0.893261	test: 1.406498

Epoch: 164
Loss: 0.47354714572429657
RMSE train: 0.606877	val: 1.253887	test: 1.955202
MAE train: 0.446726	val: 0.879629	test: 1.372881

Epoch: 165
Loss: 0.6207448840141296
RMSE train: 0.546235	val: 1.200992	test: 1.920429
MAE train: 0.411086	val: 0.851903	test: 1.360449

Epoch: 166
Loss: 0.838289737701416
RMSE train: 0.524976	val: 1.182072	test: 1.906246
MAE train: 0.403592	val: 0.848076	test: 1.363686

Epoch: 167
Loss: 0.4779328852891922
RMSE train: 0.550654	val: 1.180168	test: 1.910183
MAE train: 0.431754	val: 0.860294	test: 1.369266

Epoch: 168
Loss: 0.4885682761669159
RMSE train: 0.573463	val: 1.176597	test: 1.901297
MAE train: 0.452301	val: 0.844442	test: 1.366517

Epoch: 169
Loss: 0.413345605134964
RMSE train: 0.612046	val: 1.185321	test: 1.924264
MAE train: 0.480564	val: 0.835257	test: 1.394210

Epoch: 170
Loss: 0.3837132304906845
RMSE train: 0.652965	val: 1.205716	test: 1.983520
MAE train: 0.513125	val: 0.847088	test: 1.483398

Epoch: 171
Loss: 0.3310204893350601
RMSE train: 0.687320	val: 1.236795	test: 2.028820
MAE train: 0.530977	val: 0.869071	test: 1.538338

Epoch: 172
Loss: 0.5197955667972565
RMSE train: 0.700579	val: 1.239239	test: 2.019195
MAE train: 0.536044	val: 0.866251	test: 1.533206

Epoch: 173
Loss: 0.5103389471769333
RMSE train: 0.674939	val: 1.219668	test: 1.968235
MAE train: 0.515354	val: 0.866119	test: 1.481967

Epoch: 174
Loss: 0.5384027361869812
RMSE train: 0.655963	val: 1.205117	test: 1.926368
MAE train: 0.504240	val: 0.873748	test: 1.433139

Epoch: 175
Loss: 0.4672643542289734
RMSE train: 0.599579	val: 1.184681	test: 1.877109
MAE train: 0.458881	val: 0.869497	test: 1.382498

Epoch: 176
Loss: 0.48562274873256683
RMSE train: 0.577222	val: 1.167993	test: 1.856614
MAE train: 0.436624	val: 0.863269	test: 1.353595

Epoch: 177
Loss: 0.41722869873046875
RMSE train: 0.540122	val: 1.157222	test: 1.844382
MAE train: 0.416475	val: 0.848265	test: 1.325878

Epoch: 178
Loss: 0.5045213252305984
RMSE train: 0.555351	val: 1.165219	test: 1.869083
MAE train: 0.432149	val: 0.846565	test: 1.336255

Epoch: 179
Loss: 0.4651889204978943
RMSE train: 0.576140	val: 1.177572	test: 1.885929
MAE train: 0.462477	val: 0.853036	test: 1.347405

Epoch: 180
Loss: 0.3966480493545532
RMSE train: 0.584818	val: 1.181502	test: 1.864966
MAE train: 0.469141	val: 0.858577	test: 1.306313

Epoch: 181
Loss: 0.47379444539546967
RMSE train: 0.568122	val: 1.172241	test: 1.830729
MAE train: 0.453571	val: 0.863364	test: 1.298074

Epoch: 182
Loss: 0.4675258994102478
RMSE train: 0.541118	val: 1.176963	test: 1.820678
MAE train: 0.419064	val: 0.869755	test: 1.306993

Early stopping
Best (RMSE):	 train: 0.505114	val: 1.137208	test: 1.860379
Best (MAE):	 train: 0.385066	val: 0.817049	test: 1.313472
All runs completed.
