>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphMVP/lipo/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphMVP/lipo/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphMVP/lipo/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphMVP/lipo/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.05.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.0/lipophilicity_scaff_4_26-05_11-19-06  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.2134009429386685
RMSE train: 2.124589	val: 2.125183	test: 2.203429
MAE train: 1.873627	val: 1.855488	test: 1.952371

Epoch: 2
Loss: 3.285038879939488
RMSE train: 1.455905	val: 1.485428	test: 1.518626
MAE train: 1.242931	val: 1.255256	test: 1.296759

Epoch: 3
Loss: 2.061549961566925
RMSE train: 1.096160	val: 1.150064	test: 1.138954
MAE train: 0.914051	val: 0.950632	test: 0.943266

Epoch: 4
Loss: 1.3167766715799059
RMSE train: 0.877823	val: 0.995509	test: 0.939395
MAE train: 0.707539	val: 0.796214	test: 0.774988

Epoch: 5
Loss: 0.864589695419584
RMSE train: 0.796329	val: 0.934829	test: 0.867810
MAE train: 0.623589	val: 0.734489	test: 0.698308

Epoch: 6
Loss: 0.7630184165069035
RMSE train: 0.788993	val: 0.936784	test: 0.851679
MAE train: 0.620993	val: 0.736139	test: 0.684766

Epoch: 7
Loss: 0.7558342346123287
RMSE train: 0.750748	val: 0.926659	test: 0.848481
MAE train: 0.581675	val: 0.724675	test: 0.678519

Epoch: 8
Loss: 0.7113167430673327
RMSE train: 0.770782	val: 0.932009	test: 0.856274
MAE train: 0.597892	val: 0.730563	test: 0.669273

Epoch: 9
Loss: 0.7013023325375148
RMSE train: 0.750507	val: 0.913002	test: 0.856979
MAE train: 0.580115	val: 0.706406	test: 0.686702

Epoch: 10
Loss: 0.6734653285571507
RMSE train: 0.738153	val: 0.916824	test: 0.840426
MAE train: 0.568523	val: 0.714541	test: 0.667906

Epoch: 11
Loss: 0.6437284903866904
RMSE train: 0.706629	val: 0.873453	test: 0.809306
MAE train: 0.546708	val: 0.672007	test: 0.634386

Epoch: 12
Loss: 0.6151757666042873
RMSE train: 0.722586	val: 0.907740	test: 0.841421
MAE train: 0.553169	val: 0.705346	test: 0.668421

Epoch: 13
Loss: 0.5944131250892367
RMSE train: 0.669945	val: 0.850680	test: 0.792573
MAE train: 0.516450	val: 0.658586	test: 0.627801

Epoch: 14
Loss: 0.5916770058018821
RMSE train: 0.662270	val: 0.840026	test: 0.780460
MAE train: 0.513855	val: 0.650599	test: 0.614607

Epoch: 15
Loss: 0.5765467073236193
RMSE train: 0.673178	val: 0.867095	test: 0.810268
MAE train: 0.516803	val: 0.661360	test: 0.653248

Epoch: 16
Loss: 0.5543750801256725
RMSE train: 0.671344	val: 0.852673	test: 0.812810
MAE train: 0.517350	val: 0.658537	test: 0.640166

Epoch: 17
Loss: 0.5496573043721062
RMSE train: 0.634255	val: 0.820618	test: 0.778116
MAE train: 0.490033	val: 0.628866	test: 0.613670

Epoch: 18
Loss: 0.5423663173403058
RMSE train: 0.637533	val: 0.837764	test: 0.793420
MAE train: 0.492974	val: 0.644478	test: 0.626389

Epoch: 19
Loss: 0.5346478755984988
RMSE train: 0.657248	val: 0.840247	test: 0.797497
MAE train: 0.516235	val: 0.648907	test: 0.631961

Epoch: 20
Loss: 0.5381970682314464
RMSE train: 0.645956	val: 0.843427	test: 0.789190
MAE train: 0.493905	val: 0.640274	test: 0.627125

Epoch: 21
Loss: 0.5347580100808825
RMSE train: 0.620443	val: 0.813285	test: 0.755778
MAE train: 0.478521	val: 0.629371	test: 0.595815

Epoch: 22
Loss: 0.5355450468403953
RMSE train: 0.661317	val: 0.862409	test: 0.796792
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.0/lipophilicity_scaff_6_26-05_11-19-06  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.961428948811123
RMSE train: 2.057028	val: 2.043841	test: 2.150573
MAE train: 1.805357	val: 1.770681	test: 1.899967

Epoch: 2
Loss: 3.2048513889312744
RMSE train: 1.609161	val: 1.620999	test: 1.681752
MAE train: 1.393406	val: 1.384872	test: 1.446780

Epoch: 3
Loss: 1.9145533868244715
RMSE train: 1.204388	val: 1.239483	test: 1.241911
MAE train: 1.006826	val: 1.031654	test: 1.032261

Epoch: 4
Loss: 1.1409213287489754
RMSE train: 0.865887	val: 0.977987	test: 0.942543
MAE train: 0.695396	val: 0.785966	test: 0.772459

Epoch: 5
Loss: 0.8343080537659782
RMSE train: 0.825106	val: 0.936817	test: 0.878943
MAE train: 0.658801	val: 0.738148	test: 0.713135

Epoch: 6
Loss: 0.724542247397559
RMSE train: 0.769142	val: 0.915009	test: 0.841987
MAE train: 0.600564	val: 0.723062	test: 0.674338

Epoch: 7
Loss: 0.717792672770364
RMSE train: 0.821534	val: 0.911104	test: 0.909398
MAE train: 0.657440	val: 0.709453	test: 0.727089

Epoch: 8
Loss: 0.7238266638347081
RMSE train: 0.779581	val: 0.926439	test: 0.852464
MAE train: 0.597413	val: 0.734868	test: 0.671386

Epoch: 9
Loss: 0.6742394119501114
RMSE train: 0.723814	val: 0.879510	test: 0.834116
MAE train: 0.569775	val: 0.686497	test: 0.664089

Epoch: 10
Loss: 0.6311087778636387
RMSE train: 0.722863	val: 0.883727	test: 0.835026
MAE train: 0.565860	val: 0.677296	test: 0.662066

Epoch: 11
Loss: 0.6014853077275413
RMSE train: 0.722650	val: 0.893063	test: 0.836226
MAE train: 0.565933	val: 0.687364	test: 0.658151

Epoch: 12
Loss: 0.6110885313579014
RMSE train: 0.692269	val: 0.858937	test: 0.807223
MAE train: 0.538505	val: 0.674705	test: 0.639877

Epoch: 13
Loss: 0.5941070360796792
RMSE train: 0.666594	val: 0.826203	test: 0.792919
MAE train: 0.518555	val: 0.632928	test: 0.630833

Epoch: 14
Loss: 0.562841134411948
RMSE train: 0.714177	val: 0.864730	test: 0.828995
MAE train: 0.565742	val: 0.661949	test: 0.663341

Epoch: 15
Loss: 0.5752294191292354
RMSE train: 0.677949	val: 0.841115	test: 0.808583
MAE train: 0.532184	val: 0.647394	test: 0.648925

Epoch: 16
Loss: 0.6032454073429108
RMSE train: 0.649699	val: 0.843344	test: 0.795090
MAE train: 0.499997	val: 0.651080	test: 0.626382

Epoch: 17
Loss: 0.5532861394541604
RMSE train: 0.660078	val: 0.829885	test: 0.775346
MAE train: 0.509147	val: 0.644570	test: 0.614828

Epoch: 18
Loss: 0.5391569520745959
RMSE train: 0.634610	val: 0.820282	test: 0.775742
MAE train: 0.491327	val: 0.629126	test: 0.611730

Epoch: 19
Loss: 0.5270748074565615
RMSE train: 0.643506	val: 0.825454	test: 0.784951
MAE train: 0.503267	val: 0.633885	test: 0.626435

Epoch: 20
Loss: 0.5279609773840223
RMSE train: 0.635162	val: 0.811839	test: 0.761674
MAE train: 0.493914	val: 0.620611	test: 0.606342

Epoch: 21
Loss: 0.524026904787336
RMSE train: 0.610160	val: 0.804431	test: 0.777073
MAE train: 0.469622	val: 0.616519	test: 0.610546

Epoch: 22
Loss: 0.49838486526693615
RMSE train: 0.664219	val: 0.831874	test: 0.804508
MAE train: 0.526573	val: 0.640751	test: 0.648657Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.0/lipophilicity_scaff_5_26-05_11-19-06  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.669889501162937
RMSE train: 1.852353	val: 1.838153	test: 1.938047
MAE train: 1.607168	val: 1.569748	test: 1.682415

Epoch: 2
Loss: 2.9683145795549666
RMSE train: 1.424781	val: 1.444975	test: 1.480272
MAE train: 1.209541	val: 1.218354	test: 1.254785

Epoch: 3
Loss: 1.8247418999671936
RMSE train: 1.105392	val: 1.159632	test: 1.152389
MAE train: 0.914021	val: 0.964232	test: 0.943248

Epoch: 4
Loss: 1.0890610899244035
RMSE train: 0.892459	val: 0.993003	test: 0.955862
MAE train: 0.719789	val: 0.791204	test: 0.774543

Epoch: 5
Loss: 0.8661046368735177
RMSE train: 0.831695	val: 0.948732	test: 0.906529
MAE train: 0.665794	val: 0.745074	test: 0.732906

Epoch: 6
Loss: 0.7941567344324929
RMSE train: 0.798520	val: 0.950789	test: 0.876920
MAE train: 0.624655	val: 0.756273	test: 0.700150

Epoch: 7
Loss: 0.767979200397219
RMSE train: 0.764268	val: 0.892005	test: 0.842408
MAE train: 0.607608	val: 0.695968	test: 0.680679

Epoch: 8
Loss: 0.694153219461441
RMSE train: 0.721712	val: 0.883186	test: 0.817941
MAE train: 0.562358	val: 0.695655	test: 0.648898

Epoch: 9
Loss: 0.6790733592850822
RMSE train: 0.732338	val: 0.874839	test: 0.822666
MAE train: 0.580768	val: 0.681317	test: 0.666059

Epoch: 10
Loss: 0.6497183867863247
RMSE train: 0.690178	val: 0.846421	test: 0.792892
MAE train: 0.536681	val: 0.664021	test: 0.630644

Epoch: 11
Loss: 0.6106895932129451
RMSE train: 0.686984	val: 0.854582	test: 0.781818
MAE train: 0.533305	val: 0.669581	test: 0.627915

Epoch: 12
Loss: 0.6311714947223663
RMSE train: 0.679001	val: 0.827561	test: 0.773241
MAE train: 0.527036	val: 0.643562	test: 0.616370

Epoch: 13
Loss: 0.5991562498467309
RMSE train: 0.685162	val: 0.865395	test: 0.798120
MAE train: 0.526242	val: 0.678110	test: 0.628720

Epoch: 14
Loss: 0.6023043606962476
RMSE train: 0.654695	val: 0.816130	test: 0.787043
MAE train: 0.505128	val: 0.635292	test: 0.617937

Epoch: 15
Loss: 0.5917328808988843
RMSE train: 0.679095	val: 0.849784	test: 0.780939
MAE train: 0.529300	val: 0.664521	test: 0.627311

Epoch: 16
Loss: 0.5848944144589561
RMSE train: 0.663980	val: 0.814782	test: 0.778878
MAE train: 0.521550	val: 0.639904	test: 0.614577

Epoch: 17
Loss: 0.5638371535709926
RMSE train: 0.660366	val: 0.835580	test: 0.784580
MAE train: 0.513804	val: 0.643869	test: 0.625720

Epoch: 18
Loss: 0.5541891157627106
RMSE train: 0.658644	val: 0.826790	test: 0.785265
MAE train: 0.506560	val: 0.644375	test: 0.621138

Epoch: 19
Loss: 0.5578683742455074
RMSE train: 0.625849	val: 0.811016	test: 0.770222
MAE train: 0.484015	val: 0.634783	test: 0.608634

Epoch: 20
Loss: 0.5134984787021365
RMSE train: 0.634673	val: 0.805265	test: 0.760402
MAE train: 0.499755	val: 0.629090	test: 0.602633

Epoch: 21
Loss: 0.5161305112498147
RMSE train: 0.633846	val: 0.813631	test: 0.758320
MAE train: 0.492876	val: 0.617472	test: 0.598881

Epoch: 22
Loss: 0.4754464796611241
RMSE train: 0.627617	val: 0.815994	test: 0.765057
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.05/lipophilicity_scaff_4_26-05_11-19-06  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.374563591820853
RMSE train: 2.208735	val: 2.109767	test: 2.230708
MAE train: 1.946940	val: 1.834716	test: 1.982589

Epoch: 2
Loss: 3.5551470007215227
RMSE train: 1.780155	val: 1.727753	test: 1.786316
MAE train: 1.551232	val: 1.498673	test: 1.551067

Epoch: 3
Loss: 2.156625577381679
RMSE train: 1.312422	val: 1.214859	test: 1.233109
MAE train: 1.102250	val: 1.006035	test: 1.024292

Epoch: 4
Loss: 1.3826382500784737
RMSE train: 0.960394	val: 1.014353	test: 0.964525
MAE train: 0.783152	val: 0.826457	test: 0.791389

Epoch: 5
Loss: 1.0110015060220445
RMSE train: 0.903074	val: 0.966266	test: 0.910873
MAE train: 0.731621	val: 0.771491	test: 0.742384

Epoch: 6
Loss: 0.856859837259565
RMSE train: 0.888025	val: 0.946865	test: 0.906871
MAE train: 0.715270	val: 0.753176	test: 0.734053

Epoch: 7
Loss: 0.8504956407206399
RMSE train: 0.833813	val: 0.947189	test: 0.899409
MAE train: 0.662466	val: 0.753878	test: 0.726843

Epoch: 8
Loss: 0.8115772136620113
RMSE train: 0.806737	val: 0.952672	test: 0.888555
MAE train: 0.632616	val: 0.747900	test: 0.715048

Epoch: 9
Loss: 0.8093273895127433
RMSE train: 0.810314	val: 0.928762	test: 0.875984
MAE train: 0.631383	val: 0.734562	test: 0.698603

Epoch: 10
Loss: 0.7785868176392147
RMSE train: 0.783775	val: 0.930846	test: 0.872138
MAE train: 0.615150	val: 0.736394	test: 0.697659

Epoch: 11
Loss: 0.7349140729222979
RMSE train: 0.779435	val: 0.928816	test: 0.879636
MAE train: 0.608610	val: 0.735254	test: 0.706205

Epoch: 12
Loss: 0.7295315435954502
RMSE train: 0.749802	val: 0.887582	test: 0.844206
MAE train: 0.586917	val: 0.705652	test: 0.670086

Epoch: 13
Loss: 0.7219642741339547
RMSE train: 0.736247	val: 0.886685	test: 0.851529
MAE train: 0.576290	val: 0.704759	test: 0.681575

Epoch: 14
Loss: 0.6767806495938983
RMSE train: 0.735079	val: 0.907085	test: 0.850702
MAE train: 0.578762	val: 0.714778	test: 0.686107

Epoch: 15
Loss: 0.6729070246219635
RMSE train: 0.729390	val: 0.897044	test: 0.859756
MAE train: 0.574548	val: 0.707200	test: 0.684471

Epoch: 16
Loss: 0.6481512529509408
RMSE train: 0.704947	val: 0.867923	test: 0.848359
MAE train: 0.551318	val: 0.683797	test: 0.669924

Epoch: 17
Loss: 0.6393142853464399
RMSE train: 0.700527	val: 0.872531	test: 0.847224
MAE train: 0.550052	val: 0.684135	test: 0.673120

Epoch: 18
Loss: 0.6100110071046012
RMSE train: 0.690591	val: 0.847663	test: 0.825668
MAE train: 0.539685	val: 0.675537	test: 0.648600

Epoch: 19
Loss: 0.6216315499373845
RMSE train: 0.706303	val: 0.866577	test: 0.865981
MAE train: 0.555358	val: 0.683614	test: 0.689373

Epoch: 20
Loss: 0.6235292894499642
RMSE train: 0.689178	val: 0.826157	test: 0.824477
MAE train: 0.546620	val: 0.647810	test: 0.648793

Epoch: 21
Loss: 0.6234196892806462
RMSE train: 0.677591	val: 0.855666	test: 0.840382
MAE train: 0.531459	val: 0.672510	test: 0.669834Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.05/lipophilicity_scaff_6_26-05_11-19-06  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.694787161690848
RMSE train: 2.043125	val: 1.912213	test: 2.011337
MAE train: 1.796094	val: 1.641123	test: 1.762430

Epoch: 2
Loss: 2.9683684962136403
RMSE train: 1.586954	val: 1.482972	test: 1.509063
MAE train: 1.358075	val: 1.239937	test: 1.282700

Epoch: 3
Loss: 1.8512664692742484
RMSE train: 1.327276	val: 1.218933	test: 1.216629
MAE train: 1.117513	val: 1.008789	test: 1.009740

Epoch: 4
Loss: 1.223464982850211
RMSE train: 1.078471	val: 1.069554	test: 1.046135
MAE train: 0.885623	val: 0.870632	test: 0.859624

Epoch: 5
Loss: 1.0084914224488395
RMSE train: 0.927771	val: 0.967459	test: 0.906926
MAE train: 0.751729	val: 0.783376	test: 0.727805

Epoch: 6
Loss: 0.9053406247070858
RMSE train: 0.887074	val: 1.066763	test: 0.969895
MAE train: 0.694831	val: 0.829952	test: 0.758200

Epoch: 7
Loss: 0.8876181074551174
RMSE train: 0.838327	val: 1.019031	test: 0.941924
MAE train: 0.658291	val: 0.793724	test: 0.749206

Epoch: 8
Loss: 0.8175636827945709
RMSE train: 0.826936	val: 0.974395	test: 0.895792
MAE train: 0.653846	val: 0.766296	test: 0.709597

Epoch: 9
Loss: 0.7860542663506099
RMSE train: 0.820899	val: 0.937036	test: 0.876845
MAE train: 0.652713	val: 0.738822	test: 0.697770

Epoch: 10
Loss: 0.7735240076269422
RMSE train: 0.816361	val: 0.930761	test: 0.883574
MAE train: 0.650676	val: 0.742468	test: 0.698929

Epoch: 11
Loss: 0.754745636667524
RMSE train: 0.770761	val: 0.934261	test: 0.861805
MAE train: 0.605690	val: 0.739353	test: 0.672782

Epoch: 12
Loss: 0.7102393422807965
RMSE train: 0.764344	val: 0.974053	test: 0.877284
MAE train: 0.601252	val: 0.763691	test: 0.687241

Epoch: 13
Loss: 0.709658397095544
RMSE train: 0.761061	val: 0.977605	test: 0.928102
MAE train: 0.593273	val: 0.759784	test: 0.733070

Epoch: 14
Loss: 0.7120851831776756
RMSE train: 0.744633	val: 0.954469	test: 0.880213
MAE train: 0.584560	val: 0.749241	test: 0.690514

Epoch: 15
Loss: 0.6648768420730319
RMSE train: 0.727925	val: 0.927641	test: 0.848998
MAE train: 0.569912	val: 0.734183	test: 0.664496

Epoch: 16
Loss: 0.6626522966793605
RMSE train: 0.698113	val: 0.887487	test: 0.815606
MAE train: 0.546240	val: 0.694077	test: 0.641972

Epoch: 17
Loss: 0.6763469917433602
RMSE train: 0.728534	val: 0.890461	test: 0.824719
MAE train: 0.581516	val: 0.707183	test: 0.654913

Epoch: 18
Loss: 0.6514759191444942
RMSE train: 0.702576	val: 0.921020	test: 0.840933
MAE train: 0.550620	val: 0.730711	test: 0.665188

Epoch: 19
Loss: 0.642143828528268
RMSE train: 0.700057	val: 0.867492	test: 0.804756
MAE train: 0.559115	val: 0.680192	test: 0.643860

Epoch: 20
Loss: 0.615051133292062
RMSE train: 0.695181	val: 0.904034	test: 0.825260
MAE train: 0.543425	val: 0.719240	test: 0.650623

Epoch: 21
Loss: 0.6196461256061282
RMSE train: 0.665094	val: 0.870753	test: 0.809311
MAE train: 0.522808	val: 0.685302	test: 0.642897Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.1/lipophilicity_scaff_4_26-05_11-19-06  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.406726769038609
RMSE train: 2.240243	val: 2.110821	test: 2.221739
MAE train: 1.982613	val: 1.839890	test: 1.978858

Epoch: 2
Loss: 3.6125117199761525
RMSE train: 1.738827	val: 1.637785	test: 1.699217
MAE train: 1.504772	val: 1.406098	test: 1.457625

Epoch: 3
Loss: 2.235974771635873
RMSE train: 1.389112	val: 1.319821	test: 1.343511
MAE train: 1.165011	val: 1.104119	test: 1.123386

Epoch: 4
Loss: 1.4607694404465812
RMSE train: 1.044377	val: 1.094804	test: 1.050090
MAE train: 0.858107	val: 0.905652	test: 0.871392

Epoch: 5
Loss: 1.1049548174653734
RMSE train: 0.954897	val: 1.036699	test: 0.975792
MAE train: 0.771364	val: 0.835191	test: 0.794749

Epoch: 6
Loss: 0.9681601652077266
RMSE train: 0.920000	val: 1.007682	test: 0.952139
MAE train: 0.739126	val: 0.803215	test: 0.769681

Epoch: 7
Loss: 0.9363554801259723
RMSE train: 0.886912	val: 0.992548	test: 0.921355
MAE train: 0.711309	val: 0.790027	test: 0.749852

Epoch: 8
Loss: 0.9131114312580654
RMSE train: 0.856334	val: 0.979383	test: 0.900353
MAE train: 0.680744	val: 0.778002	test: 0.724612

Epoch: 9
Loss: 0.9059793778828212
RMSE train: 0.849957	val: 0.960129	test: 0.893100
MAE train: 0.675108	val: 0.770256	test: 0.719444

Epoch: 10
Loss: 0.8635961243084499
RMSE train: 0.827882	val: 0.925153	test: 0.872522
MAE train: 0.660681	val: 0.749798	test: 0.702996

Epoch: 11
Loss: 0.8020530130181994
RMSE train: 0.814857	val: 0.921262	test: 0.873937
MAE train: 0.647579	val: 0.747386	test: 0.696325

Epoch: 12
Loss: 0.8077330887317657
RMSE train: 0.804839	val: 0.917653	test: 0.864464
MAE train: 0.636726	val: 0.731001	test: 0.693850

Epoch: 13
Loss: 0.8000132867268154
RMSE train: 0.782119	val: 0.906501	test: 0.860466
MAE train: 0.617877	val: 0.730940	test: 0.687441

Epoch: 14
Loss: 0.7362255326339177
RMSE train: 0.778841	val: 0.892015	test: 0.862481
MAE train: 0.616578	val: 0.719100	test: 0.686524

Epoch: 15
Loss: 0.7292800630841937
RMSE train: 0.775539	val: 0.913464	test: 0.871268
MAE train: 0.617443	val: 0.733180	test: 0.693145

Epoch: 16
Loss: 0.7144157056297574
RMSE train: 0.765462	val: 0.892991	test: 0.862795
MAE train: 0.609322	val: 0.714912	test: 0.678187

Epoch: 17
Loss: 0.7118340049471173
RMSE train: 0.750090	val: 0.894941	test: 0.873773
MAE train: 0.594143	val: 0.718393	test: 0.698890

Epoch: 18
Loss: 0.6798557681696755
RMSE train: 0.748438	val: 0.894206	test: 0.859787
MAE train: 0.593454	val: 0.726921	test: 0.679941

Epoch: 19
Loss: 0.6787789676870618
RMSE train: 0.737230	val: 0.874984	test: 0.861368
MAE train: 0.585278	val: 0.699031	test: 0.690057

Epoch: 20
Loss: 0.6603859748159137
RMSE train: 0.721260	val: 0.876033	test: 0.856464
MAE train: 0.568609	val: 0.705728	test: 0.677116

Epoch: 21
Loss: 0.6672320919377464
RMSE train: 0.722831	val: 0.882536	test: 0.890875
MAE train: 0.574843	val: 0.706298	test: 0.709751Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.05/lipophilicity_scaff_5_26-05_11-19-06  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.115341663360596
RMSE train: 2.179904	val: 2.161205	test: 2.288195
MAE train: 1.930840	val: 1.884445	test: 2.046605

Epoch: 2
Loss: 3.404009257044111
RMSE train: 1.639035	val: 1.628382	test: 1.694302
MAE train: 1.408315	val: 1.377315	test: 1.461426

Epoch: 3
Loss: 2.126814901828766
RMSE train: 1.333870	val: 1.284276	test: 1.281535
MAE train: 1.132173	val: 1.079657	test: 1.068064

Epoch: 4
Loss: 1.3190857342311315
RMSE train: 0.939515	val: 1.021277	test: 0.939924
MAE train: 0.765089	val: 0.836761	test: 0.777295

Epoch: 5
Loss: 0.9845740582261767
RMSE train: 0.904555	val: 1.054925	test: 0.948020
MAE train: 0.720597	val: 0.837424	test: 0.778389

Epoch: 6
Loss: 0.8988860121795109
RMSE train: 0.844884	val: 0.997294	test: 0.887707
MAE train: 0.666652	val: 0.782977	test: 0.711585

Epoch: 7
Loss: 0.8432934199060712
RMSE train: 0.844569	val: 0.998795	test: 0.876185
MAE train: 0.661483	val: 0.791768	test: 0.702966

Epoch: 8
Loss: 0.7948279082775116
RMSE train: 0.793877	val: 0.941985	test: 0.851706
MAE train: 0.625269	val: 0.749383	test: 0.685234

Epoch: 9
Loss: 0.7918868064880371
RMSE train: 0.785961	val: 0.929423	test: 0.853510
MAE train: 0.621746	val: 0.736835	test: 0.689320

Epoch: 10
Loss: 0.79432270356587
RMSE train: 0.779041	val: 0.922706	test: 0.852431
MAE train: 0.621871	val: 0.733841	test: 0.688010

Epoch: 11
Loss: 0.7260026293141502
RMSE train: 0.763658	val: 0.951714	test: 0.860613
MAE train: 0.591155	val: 0.751716	test: 0.693797

Epoch: 12
Loss: 0.6737791832004275
RMSE train: 0.767811	val: 0.971519	test: 0.859076
MAE train: 0.596547	val: 0.772992	test: 0.698543

Epoch: 13
Loss: 0.6707859188318253
RMSE train: 0.728826	val: 0.905196	test: 0.824995
MAE train: 0.569989	val: 0.724501	test: 0.653005

Epoch: 14
Loss: 0.670743852853775
RMSE train: 0.740012	val: 0.949360	test: 0.864880
MAE train: 0.576561	val: 0.751383	test: 0.692251

Epoch: 15
Loss: 0.651726382119315
RMSE train: 0.727221	val: 0.939967	test: 0.852179
MAE train: 0.567131	val: 0.743577	test: 0.678935

Epoch: 16
Loss: 0.6659302839211055
RMSE train: 0.707803	val: 0.906525	test: 0.832301
MAE train: 0.555252	val: 0.721226	test: 0.663974

Epoch: 17
Loss: 0.6121596310819898
RMSE train: 0.720706	val: 0.927608	test: 0.834695
MAE train: 0.560441	val: 0.729651	test: 0.668968

Epoch: 18
Loss: 0.5973963737487793
RMSE train: 0.677819	val: 0.877806	test: 0.799640
MAE train: 0.532372	val: 0.698770	test: 0.640487

Epoch: 19
Loss: 0.6148180110113961
RMSE train: 0.677037	val: 0.900766	test: 0.822423
MAE train: 0.527531	val: 0.710349	test: 0.659692

Epoch: 20
Loss: 0.6023875943252018
RMSE train: 0.676322	val: 0.902593	test: 0.826441
MAE train: 0.529916	val: 0.708542	test: 0.659057

Epoch: 21
Loss: 0.5869344856057849
RMSE train: 0.685495	val: 0.906808	test: 0.826677
MAE train: 0.536666	val: 0.713716	test: 0.668160Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.2/lipophilicity_scaff_4_26-05_11-19-06  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.418856995446341
RMSE train: 2.151523	val: 2.045735	test: 2.154718
MAE train: 1.898925	val: 1.778308	test: 1.912114

Epoch: 2
Loss: 3.632143889154707
RMSE train: 1.675591	val: 1.583279	test: 1.639025
MAE train: 1.443813	val: 1.350018	test: 1.406103

Epoch: 3
Loss: 2.2958010860851834
RMSE train: 1.429765	val: 1.323351	test: 1.334073
MAE train: 1.208898	val: 1.107765	test: 1.118103

Epoch: 4
Loss: 1.5331017204693385
RMSE train: 1.083215	val: 1.112378	test: 1.040585
MAE train: 0.899620	val: 0.907921	test: 0.856209

Epoch: 5
Loss: 1.1999329456261225
RMSE train: 1.022108	val: 1.072808	test: 0.996071
MAE train: 0.838965	val: 0.857375	test: 0.802959

Epoch: 6
Loss: 1.0856268107891083
RMSE train: 0.979914	val: 1.072241	test: 0.991246
MAE train: 0.799859	val: 0.859033	test: 0.796676

Epoch: 7
Loss: 1.0348465612956457
RMSE train: 0.945627	val: 1.074882	test: 0.984019
MAE train: 0.767725	val: 0.850417	test: 0.794485

Epoch: 8
Loss: 0.9755936818463462
RMSE train: 0.919099	val: 1.013678	test: 0.944510
MAE train: 0.745168	val: 0.819209	test: 0.770773

Epoch: 9
Loss: 0.9657747362341199
RMSE train: 0.888282	val: 1.005574	test: 0.946681
MAE train: 0.711532	val: 0.799922	test: 0.771730

Epoch: 10
Loss: 0.9546393411500114
RMSE train: 0.886390	val: 0.989653	test: 0.932764
MAE train: 0.713301	val: 0.802945	test: 0.760561

Epoch: 11
Loss: 0.8892246101583753
RMSE train: 0.873348	val: 1.000263	test: 0.951158
MAE train: 0.701930	val: 0.804249	test: 0.770860

Epoch: 12
Loss: 0.8618064522743225
RMSE train: 0.845196	val: 0.997365	test: 0.937882
MAE train: 0.672928	val: 0.804794	test: 0.757588

Epoch: 13
Loss: 0.8920659337724958
RMSE train: 0.831285	val: 0.972171	test: 0.917201
MAE train: 0.662505	val: 0.780737	test: 0.742295

Epoch: 14
Loss: 0.8339127898216248
RMSE train: 0.825593	val: 0.997608	test: 0.941797
MAE train: 0.660547	val: 0.803015	test: 0.764112

Epoch: 15
Loss: 0.8044500265802655
RMSE train: 0.796745	val: 0.979922	test: 0.929567
MAE train: 0.632176	val: 0.781679	test: 0.747011

Epoch: 16
Loss: 0.7877511424677712
RMSE train: 0.805428	val: 0.980339	test: 0.924705
MAE train: 0.637089	val: 0.778703	test: 0.745884

Epoch: 17
Loss: 0.7805384056908744
RMSE train: 0.775309	val: 0.974009	test: 0.946524
MAE train: 0.615983	val: 0.774786	test: 0.763409

Epoch: 18
Loss: 0.7330539269106728
RMSE train: 0.762118	val: 0.959682	test: 0.910145
MAE train: 0.604709	val: 0.769260	test: 0.731563

Epoch: 19
Loss: 0.7430127092770168
RMSE train: 0.758343	val: 0.956839	test: 0.923447
MAE train: 0.600501	val: 0.760278	test: 0.738772

Epoch: 20
Loss: 0.7098717050892966
RMSE train: 0.748225	val: 0.956118	test: 0.927723
MAE train: 0.593876	val: 0.767244	test: 0.737974

Epoch: 21
Loss: 0.7377545280115945
RMSE train: 0.734407	val: 0.998665	test: 0.960442
MAE train: 0.578732	val: 0.792575	test: 0.763554Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.2/lipophilicity_scaff_5_26-05_11-19-06  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.1691842419760565
RMSE train: 2.204370	val: 2.188208	test: 2.299690
MAE train: 1.954069	val: 1.917431	test: 2.065935

Epoch: 2
Loss: 3.510330625942775
RMSE train: 1.680340	val: 1.587858	test: 1.657634
MAE train: 1.441360	val: 1.346876	test: 1.426393

Epoch: 3
Loss: 2.3033700244767323
RMSE train: 1.453950	val: 1.388356	test: 1.424558
MAE train: 1.232531	val: 1.168784	test: 1.215284

Epoch: 4
Loss: 1.5499254976000105
RMSE train: 1.124740	val: 1.225132	test: 1.212568
MAE train: 0.933143	val: 1.027858	test: 1.019691

Epoch: 5
Loss: 1.2170332074165344
RMSE train: 1.013267	val: 1.100152	test: 0.997059
MAE train: 0.822905	val: 0.889461	test: 0.809430

Epoch: 6
Loss: 1.1271959628377641
RMSE train: 0.991250	val: 1.062116	test: 0.945875
MAE train: 0.795173	val: 0.854438	test: 0.763467

Epoch: 7
Loss: 1.0816610583237238
RMSE train: 0.984076	val: 1.121501	test: 0.969019
MAE train: 0.780822	val: 0.878436	test: 0.763450

Epoch: 8
Loss: 1.0434782079287939
RMSE train: 0.937770	val: 1.047052	test: 0.948292
MAE train: 0.760566	val: 0.845414	test: 0.764284

Epoch: 9
Loss: 1.0002026472772871
RMSE train: 0.914142	val: 1.040305	test: 0.954461
MAE train: 0.735316	val: 0.829760	test: 0.764685

Epoch: 10
Loss: 1.0095345207623072
RMSE train: 0.896911	val: 1.027177	test: 0.941275
MAE train: 0.720582	val: 0.824123	test: 0.755504

Epoch: 11
Loss: 0.9187630372388023
RMSE train: 0.893300	val: 1.027322	test: 0.924270
MAE train: 0.707362	val: 0.811995	test: 0.729948

Epoch: 12
Loss: 0.9083495054926191
RMSE train: 0.878242	val: 1.051217	test: 0.956267
MAE train: 0.699590	val: 0.840259	test: 0.758105

Epoch: 13
Loss: 0.8984634407929012
RMSE train: 0.852449	val: 1.023877	test: 0.919593
MAE train: 0.677448	val: 0.823591	test: 0.729499

Epoch: 14
Loss: 0.8708602743489402
RMSE train: 0.843487	val: 0.990552	test: 0.884385
MAE train: 0.661870	val: 0.797542	test: 0.703948

Epoch: 15
Loss: 0.861447525875909
RMSE train: 0.839419	val: 1.020328	test: 0.931203
MAE train: 0.658263	val: 0.816375	test: 0.747599

Epoch: 16
Loss: 0.8352447918483189
RMSE train: 0.812778	val: 0.972888	test: 0.893256
MAE train: 0.647267	val: 0.784765	test: 0.709524

Epoch: 17
Loss: 0.7945245206356049
RMSE train: 0.799690	val: 0.979252	test: 0.890176
MAE train: 0.626735	val: 0.792340	test: 0.706777

Epoch: 18
Loss: 0.7996698660509927
RMSE train: 0.790040	val: 1.019232	test: 0.934869
MAE train: 0.618303	val: 0.821294	test: 0.746911

Epoch: 19
Loss: 0.7899326256343296
RMSE train: 0.792357	val: 0.998058	test: 0.906324
MAE train: 0.616242	val: 0.796009	test: 0.717831

Epoch: 20
Loss: 0.7655942142009735
RMSE train: 0.772580	val: 0.953765	test: 0.881406
MAE train: 0.602687	val: 0.761749	test: 0.696789

Epoch: 21
Loss: 0.7404985768454415
RMSE train: 0.774173	val: 0.989863	test: 0.906248
MAE train: 0.601670	val: 0.791661	test: 0.719678Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.2/lipophilicity_scaff_6_26-05_11-19-06  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.747282522065299
RMSE train: 2.061454	val: 1.738515	test: 1.826979
MAE train: 1.809687	val: 1.475575	test: 1.583964

Epoch: 2
Loss: 3.0817680188587735
RMSE train: 1.635788	val: 1.431220	test: 1.450950
MAE train: 1.391391	val: 1.186402	test: 1.216886

Epoch: 3
Loss: 1.9863785334995814
RMSE train: 1.289866	val: 1.196416	test: 1.163117
MAE train: 1.078468	val: 0.969259	test: 0.958911

Epoch: 4
Loss: 1.3890269228390284
RMSE train: 1.089343	val: 1.150541	test: 1.042331
MAE train: 0.900360	val: 0.905145	test: 0.831599

Epoch: 5
Loss: 1.171028537409646
RMSE train: 1.008321	val: 1.201631	test: 1.074249
MAE train: 0.819685	val: 0.944249	test: 0.844310

Epoch: 6
Loss: 1.123968209539141
RMSE train: 0.999194	val: 1.200064	test: 1.078188
MAE train: 0.800879	val: 0.945118	test: 0.848666

Epoch: 7
Loss: 1.091419415814536
RMSE train: 0.952892	val: 1.143214	test: 1.046267
MAE train: 0.775675	val: 0.892880	test: 0.825509

Epoch: 8
Loss: 1.0028529167175293
RMSE train: 0.935047	val: 1.171014	test: 1.061409
MAE train: 0.748036	val: 0.916864	test: 0.836155

Epoch: 9
Loss: 0.9910409961427961
RMSE train: 0.910156	val: 1.104022	test: 1.006528
MAE train: 0.737533	val: 0.859873	test: 0.800869

Epoch: 10
Loss: 0.9911928985800061
RMSE train: 0.914046	val: 1.128258	test: 1.021751
MAE train: 0.736006	val: 0.889381	test: 0.807521

Epoch: 11
Loss: 0.9504968225955963
RMSE train: 0.873359	val: 1.080137	test: 0.972329
MAE train: 0.700553	val: 0.844935	test: 0.774881

Epoch: 12
Loss: 0.8873439005443028
RMSE train: 0.868355	val: 1.125553	test: 1.017323
MAE train: 0.696462	val: 0.876743	test: 0.801230

Epoch: 13
Loss: 0.9060497283935547
RMSE train: 0.865403	val: 1.077070	test: 0.985979
MAE train: 0.699300	val: 0.841532	test: 0.782952

Epoch: 14
Loss: 0.872019716671535
RMSE train: 0.859625	val: 1.062446	test: 0.953522
MAE train: 0.695676	val: 0.828733	test: 0.757155

Epoch: 15
Loss: 0.8401706687041691
RMSE train: 0.823291	val: 1.141994	test: 1.031319
MAE train: 0.656528	val: 0.888098	test: 0.818842

Epoch: 16
Loss: 0.8257887406008584
RMSE train: 0.821570	val: 1.101213	test: 1.004943
MAE train: 0.651792	val: 0.859593	test: 0.803890

Epoch: 17
Loss: 0.8314738273620605
RMSE train: 0.802936	val: 1.061491	test: 0.954047
MAE train: 0.644176	val: 0.829543	test: 0.764928

Epoch: 18
Loss: 0.7936187088489532
RMSE train: 0.793028	val: 1.153984	test: 1.038604
MAE train: 0.627079	val: 0.899654	test: 0.824365

Epoch: 19
Loss: 0.7971784557615008
RMSE train: 0.781866	val: 1.061781	test: 0.957916
MAE train: 0.625029	val: 0.826551	test: 0.765384

Epoch: 20
Loss: 0.7550081993852343
RMSE train: 0.773698	val: 1.053351	test: 0.936938
MAE train: 0.614483	val: 0.826687	test: 0.747206

Epoch: 21
Loss: 0.7483222697462354
RMSE train: 0.752530	val: 1.058130	test: 0.943821
MAE train: 0.601802	val: 0.831782	test: 0.754786Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.1/lipophilicity_scaff_6_26-05_11-19-06  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.734508344105312
RMSE train: 2.073621	val: 1.872029	test: 1.970651
MAE train: 1.820119	val: 1.600324	test: 1.722077

Epoch: 2
Loss: 3.036027227129255
RMSE train: 1.572847	val: 1.446529	test: 1.481034
MAE train: 1.336842	val: 1.206818	test: 1.256935

Epoch: 3
Loss: 1.9241219844136919
RMSE train: 1.296963	val: 1.159326	test: 1.141713
MAE train: 1.086699	val: 0.948894	test: 0.946139

Epoch: 4
Loss: 1.3014503887721471
RMSE train: 1.065633	val: 1.069195	test: 1.022997
MAE train: 0.875450	val: 0.867423	test: 0.835648

Epoch: 5
Loss: 1.101686349936894
RMSE train: 0.962380	val: 1.001758	test: 0.919920
MAE train: 0.779713	val: 0.812906	test: 0.742807

Epoch: 6
Loss: 1.0161556005477905
RMSE train: 0.929116	val: 1.084554	test: 0.964118
MAE train: 0.736458	val: 0.853667	test: 0.766876

Epoch: 7
Loss: 0.9658745782715934
RMSE train: 0.879614	val: 0.985539	test: 0.904470
MAE train: 0.700428	val: 0.777106	test: 0.723636

Epoch: 8
Loss: 0.8962491750717163
RMSE train: 0.882225	val: 0.992165	test: 0.904885
MAE train: 0.695686	val: 0.786569	test: 0.717444

Epoch: 9
Loss: 0.8609363521848407
RMSE train: 0.858556	val: 0.957514	test: 0.895078
MAE train: 0.686424	val: 0.772472	test: 0.718915

Epoch: 10
Loss: 0.8649374502045768
RMSE train: 0.838650	val: 0.961252	test: 0.887742
MAE train: 0.665494	val: 0.776521	test: 0.713299

Epoch: 11
Loss: 0.8309609804834638
RMSE train: 0.822248	val: 0.950552	test: 0.867762
MAE train: 0.646914	val: 0.768354	test: 0.689006

Epoch: 12
Loss: 0.7859241877283368
RMSE train: 0.808582	val: 0.944713	test: 0.864594
MAE train: 0.639744	val: 0.758043	test: 0.688752

Epoch: 13
Loss: 0.7638627546174186
RMSE train: 0.795575	val: 0.945008	test: 0.879856
MAE train: 0.625919	val: 0.750421	test: 0.693061

Epoch: 14
Loss: 0.747539724622454
RMSE train: 0.779507	val: 0.984690	test: 0.898368
MAE train: 0.613608	val: 0.773255	test: 0.711961

Epoch: 15
Loss: 0.7318744318825858
RMSE train: 0.762223	val: 0.956904	test: 0.875369
MAE train: 0.600073	val: 0.761156	test: 0.692321

Epoch: 16
Loss: 0.7237205037048885
RMSE train: 0.737798	val: 0.898280	test: 0.839073
MAE train: 0.579934	val: 0.719533	test: 0.663384

Epoch: 17
Loss: 0.7077541777065822
RMSE train: 0.737575	val: 0.941239	test: 0.864548
MAE train: 0.582683	val: 0.746480	test: 0.694651

Epoch: 18
Loss: 0.681822840656553
RMSE train: 0.749987	val: 0.906149	test: 0.849833
MAE train: 0.593524	val: 0.725372	test: 0.681204

Epoch: 19
Loss: 0.6880643665790558
RMSE train: 0.720250	val: 0.960960	test: 0.880738
MAE train: 0.568599	val: 0.754535	test: 0.706557

Epoch: 20
Loss: 0.6465926596096584
RMSE train: 0.715058	val: 0.930515	test: 0.856791
MAE train: 0.561313	val: 0.740028	test: 0.673240

Epoch: 21
Loss: 0.6485242077282497
RMSE train: 0.693558	val: 0.902376	test: 0.849940
MAE train: 0.548588	val: 0.714450	test: 0.678129Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/lipophilicity/noise=0.1/lipophilicity_scaff_5_26-05_11-19-06  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.142333831105914
RMSE train: 2.191928	val: 2.163530	test: 2.289793
MAE train: 1.941466	val: 1.887553	test: 2.048239

Epoch: 2
Loss: 3.4549531596047536
RMSE train: 1.697043	val: 1.660529	test: 1.737066
MAE train: 1.458536	val: 1.412281	test: 1.502726

Epoch: 3
Loss: 2.209567742688315
RMSE train: 1.369126	val: 1.249282	test: 1.249567
MAE train: 1.159255	val: 1.052497	test: 1.045415

Epoch: 4
Loss: 1.41433550630297
RMSE train: 0.992329	val: 1.038685	test: 0.964427
MAE train: 0.806192	val: 0.848356	test: 0.793595

Epoch: 5
Loss: 1.0974223102842058
RMSE train: 0.943703	val: 1.033220	test: 0.938438
MAE train: 0.761668	val: 0.825604	test: 0.767868

Epoch: 6
Loss: 0.9881454621042524
RMSE train: 0.919309	val: 1.034169	test: 0.937661
MAE train: 0.718162	val: 0.813872	test: 0.746708

Epoch: 7
Loss: 0.9619084426334926
RMSE train: 0.913699	val: 1.028641	test: 0.911247
MAE train: 0.717562	val: 0.813289	test: 0.726643

Epoch: 8
Loss: 0.8981246948242188
RMSE train: 0.860425	val: 0.989201	test: 0.906977
MAE train: 0.678907	val: 0.777035	test: 0.725060

Epoch: 9
Loss: 0.8871209025382996
RMSE train: 0.850263	val: 0.976181	test: 0.902506
MAE train: 0.673945	val: 0.763617	test: 0.730276

Epoch: 10
Loss: 0.8769195760999408
RMSE train: 0.832341	val: 0.951576	test: 0.875339
MAE train: 0.659658	val: 0.752450	test: 0.700951

Epoch: 11
Loss: 0.795188056571143
RMSE train: 0.813278	val: 0.953131	test: 0.862056
MAE train: 0.635156	val: 0.749023	test: 0.684105

Epoch: 12
Loss: 0.7705651351383754
RMSE train: 0.821021	val: 0.987514	test: 0.897934
MAE train: 0.639333	val: 0.780271	test: 0.716318

Epoch: 13
Loss: 0.7575130739382335
RMSE train: 0.777669	val: 0.937838	test: 0.852338
MAE train: 0.609012	val: 0.739622	test: 0.668586

Epoch: 14
Loss: 0.7486702161175864
RMSE train: 0.774842	val: 0.942566	test: 0.867089
MAE train: 0.604758	val: 0.747110	test: 0.691528

Epoch: 15
Loss: 0.7409976593085698
RMSE train: 0.783021	val: 0.951077	test: 0.888815
MAE train: 0.613986	val: 0.751961	test: 0.699396

Epoch: 16
Loss: 0.738177627325058
RMSE train: 0.758234	val: 0.928982	test: 0.866227
MAE train: 0.589408	val: 0.736127	test: 0.688960

Epoch: 17
Loss: 0.7011428390230451
RMSE train: 0.757149	val: 0.919840	test: 0.865744
MAE train: 0.592066	val: 0.728211	test: 0.686665

Epoch: 18
Loss: 0.685303828545979
RMSE train: 0.719978	val: 0.904534	test: 0.845377
MAE train: 0.561488	val: 0.714733	test: 0.670824

Epoch: 19
Loss: 0.6780943487371717
RMSE train: 0.752484	val: 0.947526	test: 0.877796
MAE train: 0.583082	val: 0.744922	test: 0.704688

Epoch: 20
Loss: 0.6722395036901746
RMSE train: 0.708541	val: 0.900835	test: 0.850471
MAE train: 0.555860	val: 0.706139	test: 0.679797

Epoch: 21
Loss: 0.6469650779451642
RMSE train: 0.703754	val: 0.918672	test: 0.871525
MAE train: 0.552528	val: 0.722859	test: 0.695417MAE train: 0.508503	val: 0.661457	test: 0.634394

Epoch: 23
Loss: 0.549098225576537
RMSE train: 0.640965	val: 0.834473	test: 0.771709
MAE train: 0.496056	val: 0.644927	test: 0.620054

Epoch: 24
Loss: 0.49156707738127026
RMSE train: 0.626597	val: 0.840143	test: 0.782839
MAE train: 0.478811	val: 0.643376	test: 0.620011

Epoch: 25
Loss: 0.4873755212341036
RMSE train: 0.627782	val: 0.829379	test: 0.778520
MAE train: 0.486495	val: 0.637011	test: 0.619704

Epoch: 26
Loss: 0.49984571976321085
RMSE train: 0.613581	val: 0.811382	test: 0.764132
MAE train: 0.473233	val: 0.624694	test: 0.599414

Epoch: 27
Loss: 0.4577063683952604
RMSE train: 0.596571	val: 0.797468	test: 0.739476
MAE train: 0.459304	val: 0.615181	test: 0.575435

Epoch: 28
Loss: 0.4751772646393095
RMSE train: 0.598743	val: 0.802540	test: 0.759983
MAE train: 0.458370	val: 0.614688	test: 0.595704

Epoch: 29
Loss: 0.46072760011468616
RMSE train: 0.598319	val: 0.810772	test: 0.765364
MAE train: 0.463451	val: 0.618160	test: 0.607293

Epoch: 30
Loss: 0.465894701225417
RMSE train: 0.598726	val: 0.800129	test: 0.752962
MAE train: 0.460630	val: 0.613544	test: 0.596708

Epoch: 31
Loss: 0.44253542380673544
RMSE train: 0.590392	val: 0.810746	test: 0.774049
MAE train: 0.456453	val: 0.620136	test: 0.614349

Epoch: 32
Loss: 0.41919076229844776
RMSE train: 0.586029	val: 0.790735	test: 0.763440
MAE train: 0.452663	val: 0.615939	test: 0.596115

Epoch: 33
Loss: 0.43423743546009064
RMSE train: 0.586378	val: 0.794001	test: 0.781603
MAE train: 0.452472	val: 0.607583	test: 0.613581

Epoch: 34
Loss: 0.4173975735902786
RMSE train: 0.577561	val: 0.803794	test: 0.769524
MAE train: 0.443389	val: 0.604994	test: 0.599897

Epoch: 35
Loss: 0.42958774736949373
RMSE train: 0.567671	val: 0.789037	test: 0.756066
MAE train: 0.436100	val: 0.602147	test: 0.596973

Epoch: 36
Loss: 0.4269949900252478
RMSE train: 0.558218	val: 0.773548	test: 0.747320
MAE train: 0.429857	val: 0.590711	test: 0.580087

Epoch: 37
Loss: 0.43434976041316986
RMSE train: 0.568345	val: 0.772344	test: 0.725372
MAE train: 0.438423	val: 0.594390	test: 0.565005

Epoch: 38
Loss: 0.4254271409341267
RMSE train: 0.544373	val: 0.771425	test: 0.744438
MAE train: 0.419402	val: 0.587757	test: 0.576197

Epoch: 39
Loss: 0.4037513669048037
RMSE train: 0.605192	val: 0.817631	test: 0.787305
MAE train: 0.465276	val: 0.615799	test: 0.628107

Epoch: 40
Loss: 0.3989451016698565
RMSE train: 0.560613	val: 0.768034	test: 0.745325
MAE train: 0.434536	val: 0.593145	test: 0.580306

Epoch: 41
Loss: 0.3859704221997942
RMSE train: 0.543188	val: 0.762541	test: 0.743975
MAE train: 0.421505	val: 0.588971	test: 0.576487

Epoch: 42
Loss: 0.39013270820890156
RMSE train: 0.549557	val: 0.773867	test: 0.743205
MAE train: 0.424710	val: 0.589879	test: 0.580741

Epoch: 43
Loss: 0.3926059114081519
RMSE train: 0.568062	val: 0.810783	test: 0.767799
MAE train: 0.438824	val: 0.621049	test: 0.601121

Epoch: 44
Loss: 0.41410104504653383
RMSE train: 0.558053	val: 0.788393	test: 0.734948
MAE train: 0.430061	val: 0.598465	test: 0.570003

Epoch: 45
Loss: 0.395713672041893
RMSE train: 0.552220	val: 0.794393	test: 0.756962
MAE train: 0.429634	val: 0.599555	test: 0.595802

Epoch: 46
Loss: 0.3673884655748095
RMSE train: 0.555815	val: 0.791599	test: 0.752925
MAE train: 0.430066	val: 0.594807	test: 0.593405

Epoch: 47
Loss: 0.3919090692486082
RMSE train: 0.567599	val: 0.800878	test: 0.756893
MAE train: 0.439860	val: 0.603625	test: 0.602322

Epoch: 48
Loss: 0.38263208525521414
RMSE train: 0.551472	val: 0.795468	test: 0.768406
MAE train: 0.429103	val: 0.603951	test: 0.606372

Epoch: 49
Loss: 0.39778039285114836
RMSE train: 0.563485	val: 0.792423	test: 0.763258
MAE train: 0.438747	val: 0.601277	test: 0.593408

Epoch: 50
Loss: 0.38445809483528137
RMSE train: 0.536204	val: 0.756363	test: 0.741605
MAE train: 0.414115	val: 0.584831	test: 0.580552

Epoch: 51
Loss: 0.396445774606296
RMSE train: 0.539805	val: 0.763293	test: 0.735099
MAE train: 0.416930	val: 0.582956	test: 0.575038

Epoch: 52
Loss: 0.37156773039272856
RMSE train: 0.582123	val: 0.840226	test: 0.790745
MAE train: 0.453960	val: 0.629468	test: 0.624142

Epoch: 53
Loss: 0.3584674618073872
RMSE train: 0.515966	val: 0.769582	test: 0.756137
MAE train: 0.397360	val: 0.576693	test: 0.588030

Epoch: 54
Loss: 0.38057587827954975
RMSE train: 0.530841	val: 0.779641	test: 0.747918
MAE train: 0.411678	val: 0.587417	test: 0.591097

Epoch: 55
Loss: 0.3719574809074402
RMSE train: 0.531047	val: 0.780480	test: 0.750134
MAE train: 0.408999	val: 0.593501	test: 0.585757

Epoch: 56
Loss: 0.3469128757715225
RMSE train: 0.513439	val: 0.775569	test: 0.728901
MAE train: 0.398256	val: 0.592489	test: 0.565631

Epoch: 57
Loss: 0.3546232943023954
RMSE train: 0.522735	val: 0.789103	test: 0.748249
MAE train: 0.405737	val: 0.602952	test: 0.576859

Epoch: 58
Loss: 0.3817267950092043
RMSE train: 0.518157	val: 0.760485	test: 0.723049
MAE train: 0.400328	val: 0.588746	test: 0.568963

Epoch: 59
Loss: 0.4327696008341653
RMSE train: 0.606214	val: 0.809017	test: 0.766229
MAE train: 0.470985	val: 0.615134	test: 0.606461

Epoch: 60
Loss: 0.3829267216580255
RMSE train: 0.589922	val: 0.817578	test: 0.782737
MAE train: 0.460883	val: 0.625323	test: 0.617432

Epoch: 61
Loss: 0.36035415530204773
RMSE train: 0.523851	val: 0.782720	test: 0.731391
MAE train: 0.405457	val: 0.591112	test: 0.567702

Epoch: 62
Loss: 0.352126345038414
RMSE train: 0.540903	val: 0.796549	test: 0.749501
MAE train: 0.421915	val: 0.602813	test: 0.586372

Epoch: 63
Loss: 0.35630415805748533
RMSE train: 0.554496	val: 0.793791	test: 0.760546
MAE train: 0.432217	val: 0.594992	test: 0.602393

Epoch: 64
Loss: 0.33068753140313284
RMSE train: 0.574144	val: 0.835689	test: 0.800008
MAE train: 0.450664	val: 0.629882	test: 0.630239

Epoch: 65
Loss: 0.3305184713431767
RMSE train: 0.508226	val: 0.760764	test: 0.746297
MAE train: 0.393297	val: 0.572933	test: 0.586650

Epoch: 66
Loss: 0.34038313158920835
RMSE train: 0.509607	val: 0.764624	test: 0.740339
MAE train: 0.394132	val: 0.577446	test: 0.582008

Epoch: 67
Loss: 0.3399656351123537
RMSE train: 0.511875	val: 0.778634	test: 0.747495
MAE train: 0.398126	val: 0.589948	test: 0.585531

Epoch: 68
Loss: 0.32914578914642334
RMSE train: 0.507891	val: 0.777434	test: 0.738041
MAE train: 0.392647	val: 0.585372	test: 0.572625

Epoch: 69
Loss: 0.33309742169720785
RMSE train: 0.515390	val: 0.784977	test: 0.741738
MAE train: 0.402226	val: 0.588925	test: 0.585377

Epoch: 70
Loss: 0.30763985216617584
RMSE train: 0.488762	val: 0.773532	test: 0.738262
MAE train: 0.376585	val: 0.584950	test: 0.574994

Epoch: 71
Loss: 0.311371773481369
RMSE train: 0.577034	val: 0.850575	test: 0.802778
MAE train: 0.455447	val: 0.645781	test: 0.639084

Epoch: 72
Loss: 0.3273752565894808
RMSE train: 0.513158	val: 0.788351	test: 0.739731
MAE train: 0.397969	val: 0.595777	test: 0.576819

Epoch: 73
Loss: 0.32456203017915997
RMSE train: 0.555250	val: 0.827885	test: 0.779436
MAE train: 0.437339	val: 0.625738	test: 0.614535

Epoch: 74
Loss: 0.313183673790523
RMSE train: 0.480082	val: 0.764194	test: 0.748980
MAE train: 0.371569	val: 0.575430	test: 0.580942

Epoch: 75
Loss: 0.3079017869063786
RMSE train: 0.477154	val: 0.764072	test: 0.733074
MAE train: 0.367433	val: 0.574688	test: 0.570367

Epoch: 76
Loss: 0.30232330092362
RMSE train: 0.484658	val: 0.752153	test: 0.719621
MAE train: 0.374868	val: 0.579006	test: 0.561355

Epoch: 77
Loss: 0.29656327409403666
RMSE train: 0.492432	val: 0.762865	test: 0.746679
MAE train: 0.381744	val: 0.589990	test: 0.582912

Epoch: 78
Loss: 0.30644587746688295
RMSE train: 0.484730	val: 0.772465	test: 0.745948
MAE train: 0.373604	val: 0.582209	test: 0.574942

Epoch: 79
Loss: 0.2958020323089191
RMSE train: 0.481884	val: 0.767986	test: 0.750406
MAE train: 0.374080	val: 0.577517	test: 0.585753

Epoch: 80
Loss: 0.2985974975994655
RMSE train: 0.471285	val: 0.748692	test: 0.724683
MAE train: 0.363724	val: 0.565659	test: 0.563327

Epoch: 81
Loss: 0.2920693244252886
RMSE train: 0.513662	val: 0.793850	test: 0.776053
MAE train: 0.400727	val: 0.594630	test: 0.605971

Epoch: 82
Loss: 0.3155907690525055
RMSE train: 0.493409	val: 0.777630	test: 0.758946
MAE train: 0.383436	val: 0.582927	test: 0.595882

Epoch: 83
Loss: 0.2945438964026315MAE train: 0.488022	val: 0.626201	test: 0.612536

Epoch: 23
Loss: 0.5001601278781891
RMSE train: 0.630033	val: 0.826825	test: 0.773321
MAE train: 0.487834	val: 0.639063	test: 0.622450

Epoch: 24
Loss: 0.4791430426495416
RMSE train: 0.610344	val: 0.791459	test: 0.762929
MAE train: 0.470765	val: 0.611144	test: 0.610083

Epoch: 25
Loss: 0.4729430356195995
RMSE train: 0.602061	val: 0.785370	test: 0.743038
MAE train: 0.468841	val: 0.606739	test: 0.582873

Epoch: 26
Loss: 0.460874080657959
RMSE train: 0.598001	val: 0.788925	test: 0.757592
MAE train: 0.462827	val: 0.600526	test: 0.594975

Epoch: 27
Loss: 0.4742692857980728
RMSE train: 0.608264	val: 0.789449	test: 0.756780
MAE train: 0.473329	val: 0.601661	test: 0.600666

Epoch: 28
Loss: 0.4390752741268703
RMSE train: 0.605114	val: 0.794318	test: 0.758266
MAE train: 0.469396	val: 0.599368	test: 0.600266

Epoch: 29
Loss: 0.4391337037086487
RMSE train: 0.588197	val: 0.795103	test: 0.763204
MAE train: 0.457151	val: 0.608144	test: 0.594124

Epoch: 30
Loss: 0.45195260431085316
RMSE train: 0.652223	val: 0.888247	test: 0.819135
MAE train: 0.509062	val: 0.673840	test: 0.653485

Epoch: 31
Loss: 0.4545012967927115
RMSE train: 0.592242	val: 0.797282	test: 0.750154
MAE train: 0.460529	val: 0.600790	test: 0.596334

Epoch: 32
Loss: 0.46314813835280283
RMSE train: 0.602722	val: 0.809462	test: 0.768137
MAE train: 0.466407	val: 0.604571	test: 0.611563

Epoch: 33
Loss: 0.45063098626477377
RMSE train: 0.600168	val: 0.794092	test: 0.746475
MAE train: 0.461591	val: 0.616897	test: 0.577764

Epoch: 34
Loss: 0.43999997419970377
RMSE train: 0.592240	val: 0.782342	test: 0.761703
MAE train: 0.454801	val: 0.597725	test: 0.603930

Epoch: 35
Loss: 0.43282216574464527
RMSE train: 0.612728	val: 0.811393	test: 0.778472
MAE train: 0.473044	val: 0.623982	test: 0.609917

Epoch: 36
Loss: 0.46958946117333006
RMSE train: 0.587705	val: 0.782690	test: 0.772090
MAE train: 0.450894	val: 0.593310	test: 0.607519

Epoch: 37
Loss: 0.4064941831997463
RMSE train: 0.578264	val: 0.780093	test: 0.742971
MAE train: 0.446083	val: 0.591449	test: 0.582516

Epoch: 38
Loss: 0.39941658079624176
RMSE train: 0.552297	val: 0.768283	test: 0.743997
MAE train: 0.424991	val: 0.573553	test: 0.583204

Epoch: 39
Loss: 0.4116436541080475
RMSE train: 0.575862	val: 0.766428	test: 0.756592
MAE train: 0.449369	val: 0.585576	test: 0.591152

Epoch: 40
Loss: 0.41463567529405865
RMSE train: 0.559624	val: 0.775458	test: 0.743371
MAE train: 0.436341	val: 0.591931	test: 0.583709

Epoch: 41
Loss: 0.3985048851796559
RMSE train: 0.558184	val: 0.779456	test: 0.753654
MAE train: 0.435223	val: 0.597389	test: 0.591275

Epoch: 42
Loss: 0.39104530215263367
RMSE train: 0.561805	val: 0.771713	test: 0.736455
MAE train: 0.434707	val: 0.588416	test: 0.573218

Epoch: 43
Loss: 0.37325905902045114
RMSE train: 0.575493	val: 0.799438	test: 0.753736
MAE train: 0.449690	val: 0.605396	test: 0.603729

Epoch: 44
Loss: 0.36578636297157835
RMSE train: 0.546370	val: 0.762662	test: 0.731758
MAE train: 0.422520	val: 0.571243	test: 0.580615

Epoch: 45
Loss: 0.3946880804640906
RMSE train: 0.543123	val: 0.762234	test: 0.742290
MAE train: 0.419880	val: 0.581841	test: 0.577691

Epoch: 46
Loss: 0.42710818563188824
RMSE train: 0.566016	val: 0.803110	test: 0.768164
MAE train: 0.438432	val: 0.612271	test: 0.600347

Epoch: 47
Loss: 0.38827031212193625
RMSE train: 0.533130	val: 0.765782	test: 0.750918
MAE train: 0.409980	val: 0.565978	test: 0.585683

Epoch: 48
Loss: 0.3876108399459294
RMSE train: 0.541574	val: 0.760800	test: 0.740888
MAE train: 0.423379	val: 0.577598	test: 0.576352

Epoch: 49
Loss: 0.3762846120766231
RMSE train: 0.538859	val: 0.755278	test: 0.744991
MAE train: 0.415917	val: 0.575508	test: 0.582046

Epoch: 50
Loss: 0.41763491715703693
RMSE train: 0.542056	val: 0.767950	test: 0.740485
MAE train: 0.420837	val: 0.584116	test: 0.584772

Epoch: 51
Loss: 0.3894178548029491
RMSE train: 0.530322	val: 0.762424	test: 0.749335
MAE train: 0.411095	val: 0.574205	test: 0.583886

Epoch: 52
Loss: 0.3642388539654868
RMSE train: 0.520753	val: 0.739592	test: 0.745459
MAE train: 0.398583	val: 0.560651	test: 0.577440

Epoch: 53
Loss: 0.3777733083282198
RMSE train: 0.562339	val: 0.795471	test: 0.764882
MAE train: 0.434909	val: 0.587198	test: 0.602289

Epoch: 54
Loss: 0.3623864416565214
RMSE train: 0.513888	val: 0.750659	test: 0.730812
MAE train: 0.397751	val: 0.557327	test: 0.567978

Epoch: 55
Loss: 0.36613853914397104
RMSE train: 0.523494	val: 0.775986	test: 0.747563
MAE train: 0.403445	val: 0.576809	test: 0.582351

Epoch: 56
Loss: 0.3520479202270508
RMSE train: 0.542659	val: 0.784262	test: 0.765855
MAE train: 0.419286	val: 0.589468	test: 0.602847

Epoch: 57
Loss: 0.35563245841435026
RMSE train: 0.505097	val: 0.750140	test: 0.735735
MAE train: 0.390349	val: 0.569717	test: 0.556172

Epoch: 58
Loss: 0.35992786288261414
RMSE train: 0.522578	val: 0.761362	test: 0.746191
MAE train: 0.409654	val: 0.575921	test: 0.582534

Epoch: 59
Loss: 0.34553056955337524
RMSE train: 0.522264	val: 0.768528	test: 0.729669
MAE train: 0.406685	val: 0.578219	test: 0.579218

Epoch: 60
Loss: 0.33271717386586325
RMSE train: 0.551412	val: 0.785609	test: 0.771995
MAE train: 0.424490	val: 0.583336	test: 0.598433

Epoch: 61
Loss: 0.3461828338248389
RMSE train: 0.504464	val: 0.755057	test: 0.748746
MAE train: 0.389856	val: 0.561283	test: 0.573681

Epoch: 62
Loss: 0.35786829037325724
RMSE train: 0.532343	val: 0.760092	test: 0.744546
MAE train: 0.416785	val: 0.578980	test: 0.579042

Epoch: 63
Loss: 0.3633015091930117
RMSE train: 0.541216	val: 0.760828	test: 0.764404
MAE train: 0.427774	val: 0.575623	test: 0.596284

Epoch: 64
Loss: 0.3586640251534326
RMSE train: 0.522283	val: 0.759338	test: 0.738321
MAE train: 0.407432	val: 0.573115	test: 0.574182

Epoch: 65
Loss: 0.3469523787498474
RMSE train: 0.502894	val: 0.761139	test: 0.731311
MAE train: 0.387564	val: 0.573551	test: 0.568225

Epoch: 66
Loss: 0.32498717095170704
RMSE train: 0.506302	val: 0.767489	test: 0.738752
MAE train: 0.391132	val: 0.569156	test: 0.572592

Epoch: 67
Loss: 0.3269290562186922
RMSE train: 0.498488	val: 0.757142	test: 0.734890
MAE train: 0.385847	val: 0.567454	test: 0.566932

Epoch: 68
Loss: 0.33582676521369387
RMSE train: 0.483285	val: 0.758100	test: 0.733437
MAE train: 0.374549	val: 0.564647	test: 0.558485

Epoch: 69
Loss: 0.3138280766350882
RMSE train: 0.502245	val: 0.769716	test: 0.750155
MAE train: 0.389133	val: 0.575911	test: 0.581685

Epoch: 70
Loss: 0.30653019675186705
RMSE train: 0.477961	val: 0.755211	test: 0.733964
MAE train: 0.367955	val: 0.557935	test: 0.565471

Epoch: 71
Loss: 0.3123043881995337
RMSE train: 0.501270	val: 0.759525	test: 0.729369
MAE train: 0.388454	val: 0.570765	test: 0.560848

Epoch: 72
Loss: 0.3044686658041818
RMSE train: 0.502822	val: 0.749021	test: 0.726424
MAE train: 0.387866	val: 0.560047	test: 0.563954

Epoch: 73
Loss: 0.3412747063807079
RMSE train: 0.522688	val: 0.752564	test: 0.755844
MAE train: 0.406241	val: 0.577136	test: 0.591321

Epoch: 74
Loss: 0.3216936332838876
RMSE train: 0.512506	val: 0.796225	test: 0.753706
MAE train: 0.398883	val: 0.594701	test: 0.584386

Epoch: 75
Loss: 0.30090196217809406
RMSE train: 0.480886	val: 0.738173	test: 0.733210
MAE train: 0.370866	val: 0.549782	test: 0.562269

Epoch: 76
Loss: 0.29726489207574297
RMSE train: 0.488677	val: 0.762518	test: 0.733278
MAE train: 0.375988	val: 0.571727	test: 0.573223

Epoch: 77
Loss: 0.29792696237564087
RMSE train: 0.472046	val: 0.737615	test: 0.728717
MAE train: 0.365482	val: 0.556055	test: 0.557797

Epoch: 78
Loss: 0.2932026982307434
RMSE train: 0.505145	val: 0.785273	test: 0.752190
MAE train: 0.393510	val: 0.578949	test: 0.581394

Epoch: 79
Loss: 0.3039920649358204
RMSE train: 0.460529	val: 0.747869	test: 0.726397
MAE train: 0.354086	val: 0.563117	test: 0.552634

Epoch: 80
Loss: 0.31321206263133455
RMSE train: 0.468359	val: 0.760132	test: 0.723635
MAE train: 0.360581	val: 0.573857	test: 0.559556

Epoch: 81
Loss: 0.3026515522173473
RMSE train: 0.494800	val: 0.763913	test: 0.742961
MAE train: 0.382140	val: 0.575732	test: 0.585896

Epoch: 82
Loss: 0.3028201109596661
RMSE train: 0.483736	val: 0.751421	test: 0.736765
MAE train: 0.374486	val: 0.559761	test: 0.562150

Epoch: 83
Loss: 0.2958003452845982

Epoch: 23
Loss: 0.500345053417342
RMSE train: 0.627576	val: 0.815759	test: 0.769816
MAE train: 0.486350	val: 0.634334	test: 0.606703

Epoch: 24
Loss: 0.49076015182903837
RMSE train: 0.624676	val: 0.822593	test: 0.771793
MAE train: 0.482261	val: 0.628145	test: 0.609440

Epoch: 25
Loss: 0.4719285113470895
RMSE train: 0.606982	val: 0.796071	test: 0.778905
MAE train: 0.476904	val: 0.605236	test: 0.612903

Epoch: 26
Loss: 0.4776405670813152
RMSE train: 0.605967	val: 0.812221	test: 0.768334
MAE train: 0.468593	val: 0.626763	test: 0.607624

Epoch: 27
Loss: 0.469327592423984
RMSE train: 0.596578	val: 0.792544	test: 0.764697
MAE train: 0.462444	val: 0.600898	test: 0.596574

Epoch: 28
Loss: 0.4606811191354479
RMSE train: 0.587928	val: 0.786328	test: 0.761419
MAE train: 0.456209	val: 0.605546	test: 0.590042

Epoch: 29
Loss: 0.43665726482868195
RMSE train: 0.615110	val: 0.795143	test: 0.778743
MAE train: 0.478684	val: 0.611678	test: 0.612106

Epoch: 30
Loss: 0.44882514008453916
RMSE train: 0.576478	val: 0.793061	test: 0.763085
MAE train: 0.442071	val: 0.606209	test: 0.595932

Epoch: 31
Loss: 0.4517876143966402
RMSE train: 0.599775	val: 0.805309	test: 0.774630
MAE train: 0.465331	val: 0.626246	test: 0.610191

Epoch: 32
Loss: 0.44370665294783457
RMSE train: 0.581511	val: 0.777497	test: 0.760975
MAE train: 0.450889	val: 0.598583	test: 0.589675

Epoch: 33
Loss: 0.4476101355893271
RMSE train: 0.577439	val: 0.790729	test: 0.757393
MAE train: 0.444457	val: 0.597546	test: 0.596024

Epoch: 34
Loss: 0.419915252498218
RMSE train: 0.546098	val: 0.757690	test: 0.751816
MAE train: 0.424549	val: 0.578719	test: 0.585310

Epoch: 35
Loss: 0.42176264950207304
RMSE train: 0.566548	val: 0.779108	test: 0.761117
MAE train: 0.441807	val: 0.593974	test: 0.599146

Epoch: 36
Loss: 0.41380685567855835
RMSE train: 0.581384	val: 0.802647	test: 0.770348
MAE train: 0.448646	val: 0.607102	test: 0.604886

Epoch: 37
Loss: 0.4179025398833411
RMSE train: 0.571858	val: 0.793450	test: 0.767615
MAE train: 0.441225	val: 0.598619	test: 0.596086

Epoch: 38
Loss: 0.3984418681689671
RMSE train: 0.549579	val: 0.770527	test: 0.756044
MAE train: 0.426106	val: 0.577415	test: 0.596082

Epoch: 39
Loss: 0.37602533400058746
RMSE train: 0.546767	val: 0.780511	test: 0.745674
MAE train: 0.422035	val: 0.591407	test: 0.576662

Epoch: 40
Loss: 0.3916482946702412
RMSE train: 0.544022	val: 0.770091	test: 0.744950
MAE train: 0.419399	val: 0.582151	test: 0.576259

Epoch: 41
Loss: 0.40341202701841083
RMSE train: 0.538825	val: 0.777778	test: 0.754698
MAE train: 0.415445	val: 0.581957	test: 0.585963

Epoch: 42
Loss: 0.3827139160462788
RMSE train: 0.552269	val: 0.797878	test: 0.766552
MAE train: 0.425499	val: 0.597433	test: 0.595281

Epoch: 43
Loss: 0.4045965735401426
RMSE train: 0.546353	val: 0.777625	test: 0.760271
MAE train: 0.426610	val: 0.590633	test: 0.594711

Epoch: 44
Loss: 0.38153121939727236
RMSE train: 0.570900	val: 0.773590	test: 0.770081
MAE train: 0.451794	val: 0.583458	test: 0.604247

Epoch: 45
Loss: 0.37454554438591003
RMSE train: 0.525754	val: 0.773709	test: 0.745575
MAE train: 0.406555	val: 0.580612	test: 0.575254

Epoch: 46
Loss: 0.3679624093430383
RMSE train: 0.532117	val: 0.778699	test: 0.775704
MAE train: 0.409577	val: 0.585197	test: 0.599792

Epoch: 47
Loss: 0.37392987310886383
RMSE train: 0.523541	val: 0.753766	test: 0.763156
MAE train: 0.405595	val: 0.575141	test: 0.594836

Epoch: 48
Loss: 0.361901906984193
RMSE train: 0.513847	val: 0.753040	test: 0.742346
MAE train: 0.400047	val: 0.572064	test: 0.580038

Epoch: 49
Loss: 0.3634257082428251
RMSE train: 0.502107	val: 0.748042	test: 0.744956
MAE train: 0.391128	val: 0.563527	test: 0.577360

Epoch: 50
Loss: 0.3719329301800047
RMSE train: 0.520236	val: 0.751730	test: 0.758330
MAE train: 0.406935	val: 0.559516	test: 0.597320

Epoch: 51
Loss: 0.3606378989560263
RMSE train: 0.515087	val: 0.766961	test: 0.749421
MAE train: 0.398888	val: 0.569770	test: 0.584275

Epoch: 52
Loss: 0.3485270398003714
RMSE train: 0.510945	val: 0.776325	test: 0.761206
MAE train: 0.394706	val: 0.583433	test: 0.588799

Epoch: 53
Loss: 0.366050979920796
RMSE train: 0.511529	val: 0.776040	test: 0.756016
MAE train: 0.400112	val: 0.581324	test: 0.584926

Epoch: 54
Loss: 0.38182117896420614
RMSE train: 0.526371	val: 0.790590	test: 0.784014
MAE train: 0.407881	val: 0.594350	test: 0.595614

Epoch: 55
Loss: 0.373981420482908
RMSE train: 0.540959	val: 0.772827	test: 0.775910
MAE train: 0.420386	val: 0.581229	test: 0.596743

Epoch: 56
Loss: 0.36342271523816244
RMSE train: 0.524634	val: 0.786684	test: 0.765269
MAE train: 0.408051	val: 0.585678	test: 0.603774

Epoch: 57
Loss: 0.36117941566876005
RMSE train: 0.583257	val: 0.802060	test: 0.793432
MAE train: 0.465249	val: 0.605731	test: 0.615467

Epoch: 58
Loss: 0.3517652302980423
RMSE train: 0.512743	val: 0.769587	test: 0.771783
MAE train: 0.400251	val: 0.579989	test: 0.584732

Epoch: 59
Loss: 0.3288957689489637
RMSE train: 0.512350	val: 0.759854	test: 0.761856
MAE train: 0.400180	val: 0.567695	test: 0.589138

Epoch: 60
Loss: 0.34151837868349894
RMSE train: 0.526828	val: 0.793786	test: 0.755495
MAE train: 0.407130	val: 0.587737	test: 0.581186

Epoch: 61
Loss: 0.3382252390895571
RMSE train: 0.496982	val: 0.758311	test: 0.746638
MAE train: 0.384530	val: 0.570540	test: 0.574932

Epoch: 62
Loss: 0.32994381870542255
RMSE train: 0.487261	val: 0.751640	test: 0.735847
MAE train: 0.374053	val: 0.562623	test: 0.561881

Epoch: 63
Loss: 0.3328836475099836
RMSE train: 0.496233	val: 0.766316	test: 0.737164
MAE train: 0.380426	val: 0.575585	test: 0.560163

Epoch: 64
Loss: 0.32233037054538727
RMSE train: 0.525105	val: 0.772099	test: 0.750672
MAE train: 0.405795	val: 0.576118	test: 0.575529

Epoch: 65
Loss: 0.3219061642885208
RMSE train: 0.483338	val: 0.764471	test: 0.752759
MAE train: 0.373089	val: 0.569685	test: 0.575276

Epoch: 66
Loss: 0.31939616799354553
RMSE train: 0.488498	val: 0.764783	test: 0.753820
MAE train: 0.378726	val: 0.563608	test: 0.576309

Epoch: 67
Loss: 0.34772583629403797
RMSE train: 0.522476	val: 0.812623	test: 0.769517
MAE train: 0.406523	val: 0.602072	test: 0.594242

Epoch: 68
Loss: 0.322190848844392
RMSE train: 0.487042	val: 0.775724	test: 0.766821
MAE train: 0.373644	val: 0.581255	test: 0.576320

Epoch: 69
Loss: 0.31981720668928965
RMSE train: 0.489158	val: 0.770055	test: 0.743149
MAE train: 0.376824	val: 0.573101	test: 0.578743

Epoch: 70
Loss: 0.3114560672215053
RMSE train: 0.476956	val: 0.740550	test: 0.753786
MAE train: 0.373234	val: 0.564148	test: 0.578692

Epoch: 71
Loss: 0.30955675670078825
RMSE train: 0.481616	val: 0.754764	test: 0.727868
MAE train: 0.371388	val: 0.569468	test: 0.559281

Epoch: 72
Loss: 0.29656227252313067
RMSE train: 0.452425	val: 0.743151	test: 0.734702
MAE train: 0.349305	val: 0.548376	test: 0.549305

Epoch: 73
Loss: 0.3080874958208629
RMSE train: 0.474623	val: 0.772848	test: 0.745811
MAE train: 0.369404	val: 0.574450	test: 0.578493

Epoch: 74
Loss: 0.3220365632857595
RMSE train: 0.462669	val: 0.767738	test: 0.748340
MAE train: 0.357114	val: 0.558016	test: 0.567515

Epoch: 75
Loss: 0.3018841637032373
RMSE train: 0.493334	val: 0.782897	test: 0.763884
MAE train: 0.379153	val: 0.581488	test: 0.580656

Epoch: 76
Loss: 0.31608937680721283
RMSE train: 0.474241	val: 0.766001	test: 0.743430
MAE train: 0.365775	val: 0.577368	test: 0.570854

Epoch: 77
Loss: 0.3050582217318671
RMSE train: 0.459036	val: 0.750613	test: 0.744807
MAE train: 0.357382	val: 0.561321	test: 0.562598

Epoch: 78
Loss: 0.2905870761190142
RMSE train: 0.457829	val: 0.759600	test: 0.739815
MAE train: 0.353299	val: 0.562928	test: 0.565477

Epoch: 79
Loss: 0.29130506834813524
RMSE train: 0.486764	val: 0.760417	test: 0.755528
MAE train: 0.382905	val: 0.573438	test: 0.573767

Epoch: 80
Loss: 0.29787727126053404
RMSE train: 0.461140	val: 0.786946	test: 0.745969
MAE train: 0.356687	val: 0.582134	test: 0.567634

Epoch: 81
Loss: 0.29455995559692383
RMSE train: 0.456849	val: 0.742640	test: 0.739848
MAE train: 0.355159	val: 0.554860	test: 0.561285

Epoch: 82
Loss: 0.3090978967291968
RMSE train: 0.461624	val: 0.781806	test: 0.743089
MAE train: 0.357134	val: 0.581769	test: 0.563127

Epoch: 83
Loss: 0.3127878116709845
RMSE train: 0.498691	val: 0.789188	test: 0.778679

Epoch: 22
Loss: 0.5917901056153434
RMSE train: 0.684550	val: 0.844241	test: 0.840823
MAE train: 0.536320	val: 0.651977	test: 0.666962

Epoch: 23
Loss: 0.5993218379361289
RMSE train: 0.663622	val: 0.854865	test: 0.839258
MAE train: 0.517543	val: 0.677121	test: 0.669663

Epoch: 24
Loss: 0.5861088335514069
RMSE train: 0.648224	val: 0.842582	test: 0.822766
MAE train: 0.506736	val: 0.663613	test: 0.660614

Epoch: 25
Loss: 0.5386510342359543
RMSE train: 0.643527	val: 0.855473	test: 0.811568
MAE train: 0.501719	val: 0.675016	test: 0.644491

Epoch: 26
Loss: 0.5385171260152545
RMSE train: 0.638457	val: 0.829315	test: 0.820607
MAE train: 0.500617	val: 0.641885	test: 0.652582

Epoch: 27
Loss: 0.5317449143954686
RMSE train: 0.616360	val: 0.818832	test: 0.814656
MAE train: 0.479049	val: 0.642879	test: 0.640795

Epoch: 28
Loss: 0.5412353553942272
RMSE train: 0.611273	val: 0.831173	test: 0.802962
MAE train: 0.476722	val: 0.655153	test: 0.638956

Epoch: 29
Loss: 0.5185845302683967
RMSE train: 0.619985	val: 0.833149	test: 0.827789
MAE train: 0.487300	val: 0.655640	test: 0.660125

Epoch: 30
Loss: 0.4969256137098585
RMSE train: 0.611200	val: 0.813635	test: 0.822723
MAE train: 0.478664	val: 0.630229	test: 0.650875

Epoch: 31
Loss: 0.4975311372961317
RMSE train: 0.596620	val: 0.832831	test: 0.815027
MAE train: 0.466747	val: 0.656790	test: 0.646569

Epoch: 32
Loss: 0.4742359582866941
RMSE train: 0.589036	val: 0.810858	test: 0.820499
MAE train: 0.458011	val: 0.632611	test: 0.649050

Epoch: 33
Loss: 0.4662616082600185
RMSE train: 0.605477	val: 0.817630	test: 0.829163
MAE train: 0.477004	val: 0.641320	test: 0.648483

Epoch: 34
Loss: 0.4586103686264583
RMSE train: 0.595037	val: 0.821067	test: 0.817586
MAE train: 0.466084	val: 0.643829	test: 0.646791

Epoch: 35
Loss: 0.44028433093002867
RMSE train: 0.591567	val: 0.845097	test: 0.829329
MAE train: 0.463163	val: 0.654768	test: 0.660325

Epoch: 36
Loss: 0.4664200650794165
RMSE train: 0.558374	val: 0.805901	test: 0.794237
MAE train: 0.435592	val: 0.626843	test: 0.628822

Epoch: 37
Loss: 0.4582459181547165
RMSE train: 0.601085	val: 0.842779	test: 0.819571
MAE train: 0.470273	val: 0.663545	test: 0.642852

Epoch: 38
Loss: 0.4471929392644337
RMSE train: 0.562292	val: 0.822942	test: 0.821558
MAE train: 0.441603	val: 0.641145	test: 0.651816

Epoch: 39
Loss: 0.4381788671016693
RMSE train: 0.567160	val: 0.821145	test: 0.828647
MAE train: 0.447012	val: 0.643354	test: 0.655085

Epoch: 40
Loss: 0.4360305688210896
RMSE train: 0.589374	val: 0.859214	test: 0.829901
MAE train: 0.461717	val: 0.663009	test: 0.654980

Epoch: 41
Loss: 0.43560207528727396
RMSE train: 0.543911	val: 0.793719	test: 0.798202
MAE train: 0.425986	val: 0.620039	test: 0.624510

Epoch: 42
Loss: 0.4276575573853084
RMSE train: 0.541511	val: 0.807643	test: 0.822422
MAE train: 0.423509	val: 0.625074	test: 0.645852

Epoch: 43
Loss: 0.4341917357274464
RMSE train: 0.567041	val: 0.834690	test: 0.833257
MAE train: 0.443523	val: 0.640300	test: 0.665660

Epoch: 44
Loss: 0.4070087437118803
RMSE train: 0.537027	val: 0.838777	test: 0.828075
MAE train: 0.420735	val: 0.658263	test: 0.656011

Epoch: 45
Loss: 0.402007058262825
RMSE train: 0.542897	val: 0.790042	test: 0.801837
MAE train: 0.426107	val: 0.627383	test: 0.625422

Epoch: 46
Loss: 0.3788515329360962
RMSE train: 0.513084	val: 0.803186	test: 0.804120
MAE train: 0.401028	val: 0.621620	test: 0.634744

Epoch: 47
Loss: 0.3760323056152889
RMSE train: 0.513180	val: 0.808416	test: 0.822636
MAE train: 0.400368	val: 0.620975	test: 0.650996

Epoch: 48
Loss: 0.3745735117367336
RMSE train: 0.509532	val: 0.795911	test: 0.800101
MAE train: 0.398655	val: 0.627743	test: 0.627803

Epoch: 49
Loss: 0.3631243578025273
RMSE train: 0.522771	val: 0.827598	test: 0.810910
MAE train: 0.412200	val: 0.640592	test: 0.643409

Epoch: 50
Loss: 0.3582089032445635
RMSE train: 0.492014	val: 0.802931	test: 0.793415
MAE train: 0.384022	val: 0.619718	test: 0.623414

Epoch: 51
Loss: 0.3620415117059435
RMSE train: 0.485337	val: 0.779946	test: 0.804233
MAE train: 0.379914	val: 0.610224	test: 0.633472

Epoch: 52
Loss: 0.35802763274737764
RMSE train: 0.516062	val: 0.836076	test: 0.835989
MAE train: 0.404824	val: 0.638653	test: 0.662921

Epoch: 53
Loss: 0.3510773948260716
RMSE train: 0.498111	val: 0.791109	test: 0.801726
MAE train: 0.388417	val: 0.616871	test: 0.631469

Epoch: 54
Loss: 0.34750220605305265
RMSE train: 0.500617	val: 0.795574	test: 0.797231
MAE train: 0.391383	val: 0.611568	test: 0.626393

Epoch: 55
Loss: 0.35374074961457935
RMSE train: 0.482819	val: 0.821824	test: 0.820842
MAE train: 0.379458	val: 0.632053	test: 0.640694

Epoch: 56
Loss: 0.35814098375184195
RMSE train: 0.499883	val: 0.799247	test: 0.799070
MAE train: 0.395692	val: 0.624874	test: 0.633528

Epoch: 57
Loss: 0.338909496154104
RMSE train: 0.462706	val: 0.779112	test: 0.806116
MAE train: 0.359193	val: 0.601771	test: 0.633589

Epoch: 58
Loss: 0.34570432347910746
RMSE train: 0.486636	val: 0.808923	test: 0.831581
MAE train: 0.380512	val: 0.614770	test: 0.648571

Epoch: 59
Loss: 0.35299239839826313
RMSE train: 0.486683	val: 0.814867	test: 0.854883
MAE train: 0.382804	val: 0.635962	test: 0.668044

Epoch: 60
Loss: 0.3285735824278423
RMSE train: 0.471616	val: 0.810701	test: 0.797677
MAE train: 0.369681	val: 0.621615	test: 0.629195

Epoch: 61
Loss: 0.3204484667096819
RMSE train: 0.476196	val: 0.797630	test: 0.797394
MAE train: 0.373508	val: 0.613000	test: 0.639105

Epoch: 62
Loss: 0.3282799337591444
RMSE train: 0.452942	val: 0.788614	test: 0.802315
MAE train: 0.353847	val: 0.608598	test: 0.629700

Epoch: 63
Loss: 0.3224936532122748
RMSE train: 0.449475	val: 0.783336	test: 0.774844
MAE train: 0.352984	val: 0.609232	test: 0.607229

Epoch: 64
Loss: 0.3040448384625571
RMSE train: 0.446756	val: 0.790424	test: 0.797017
MAE train: 0.352557	val: 0.614792	test: 0.626244

Epoch: 65
Loss: 0.31316440233162474
RMSE train: 0.420552	val: 0.767047	test: 0.779105
MAE train: 0.325978	val: 0.593339	test: 0.614189

Epoch: 66
Loss: 0.2857424095273018
RMSE train: 0.456139	val: 0.792600	test: 0.798249
MAE train: 0.356814	val: 0.613846	test: 0.629454

Epoch: 67
Loss: 0.298979069505419
RMSE train: 0.422827	val: 0.782770	test: 0.810707
MAE train: 0.327738	val: 0.602875	test: 0.635075

Epoch: 68
Loss: 0.27807810051100595
RMSE train: 0.422850	val: 0.780955	test: 0.786264
MAE train: 0.329009	val: 0.606614	test: 0.616031

Epoch: 69
Loss: 0.27239964050906046
RMSE train: 0.423229	val: 0.798524	test: 0.805538
MAE train: 0.332689	val: 0.605446	test: 0.633301

Epoch: 70
Loss: 0.3053313674671309
RMSE train: 0.447081	val: 0.796967	test: 0.816962
MAE train: 0.350674	val: 0.616641	test: 0.634498

Epoch: 71
Loss: 0.2826727202960423
RMSE train: 0.402694	val: 0.773993	test: 0.786851
MAE train: 0.314256	val: 0.594320	test: 0.619088

Epoch: 72
Loss: 0.288063115307263
RMSE train: 0.440207	val: 0.785112	test: 0.800100
MAE train: 0.345860	val: 0.602039	test: 0.628980

Epoch: 73
Loss: 0.2663031082068171
RMSE train: 0.416112	val: 0.788008	test: 0.789996
MAE train: 0.327001	val: 0.605940	test: 0.621648

Epoch: 74
Loss: 0.26590729717697414
RMSE train: 0.406125	val: 0.792377	test: 0.792502
MAE train: 0.318715	val: 0.607869	test: 0.624023

Epoch: 75
Loss: 0.2656135622944151
RMSE train: 0.420668	val: 0.769442	test: 0.783755
MAE train: 0.332284	val: 0.603508	test: 0.608611

Epoch: 76
Loss: 0.2558408496635301
RMSE train: 0.441501	val: 0.813045	test: 0.785413
MAE train: 0.351775	val: 0.621980	test: 0.625159

Epoch: 77
Loss: 0.2675794075642313
RMSE train: 0.427198	val: 0.785045	test: 0.793796
MAE train: 0.338406	val: 0.604739	test: 0.632538

Epoch: 78
Loss: 0.2523918833051409
RMSE train: 0.441765	val: 0.800133	test: 0.793808
MAE train: 0.346392	val: 0.613364	test: 0.631043

Epoch: 79
Loss: 0.24594853392669133
RMSE train: 0.399315	val: 0.782568	test: 0.768351
MAE train: 0.313030	val: 0.609251	test: 0.601490

Epoch: 80
Loss: 0.238238910479205
RMSE train: 0.407303	val: 0.773004	test: 0.789966
MAE train: 0.321266	val: 0.605911	test: 0.616116

Epoch: 81
Loss: 0.2509852168815477
RMSE train: 0.426153	val: 0.801879	test: 0.810001
MAE train: 0.336990	val: 0.636622	test: 0.638838

Epoch: 82
Loss: 0.21422352588602475
RMSE train: 0.420725	val: 0.809485	test: 0.790534

Epoch: 22
Loss: 0.5540890097618103
RMSE train: 0.665747	val: 0.889350	test: 0.825724
MAE train: 0.519198	val: 0.706132	test: 0.647687

Epoch: 23
Loss: 0.5633291218961988
RMSE train: 0.675978	val: 0.933147	test: 0.866943
MAE train: 0.528626	val: 0.743478	test: 0.686516

Epoch: 24
Loss: 0.5646702264036451
RMSE train: 0.640457	val: 0.882304	test: 0.825268
MAE train: 0.505177	val: 0.703162	test: 0.647105

Epoch: 25
Loss: 0.5457626623766763
RMSE train: 0.635760	val: 0.879423	test: 0.818130
MAE train: 0.497004	val: 0.694238	test: 0.635867

Epoch: 26
Loss: 0.5216485538652965
RMSE train: 0.627069	val: 0.893633	test: 0.838720
MAE train: 0.491009	val: 0.707098	test: 0.655641

Epoch: 27
Loss: 0.5088593278612409
RMSE train: 0.618708	val: 0.852354	test: 0.808464
MAE train: 0.488193	val: 0.677831	test: 0.632988

Epoch: 28
Loss: 0.5021914073399135
RMSE train: 0.612918	val: 0.861028	test: 0.812727
MAE train: 0.478757	val: 0.679815	test: 0.632727

Epoch: 29
Loss: 0.48414243119103567
RMSE train: 0.614014	val: 0.860221	test: 0.795002
MAE train: 0.484149	val: 0.677571	test: 0.624899

Epoch: 30
Loss: 0.4814275928906032
RMSE train: 0.629804	val: 0.849897	test: 0.785636
MAE train: 0.501970	val: 0.675321	test: 0.615680

Epoch: 31
Loss: 0.4737529286316463
RMSE train: 0.602917	val: 0.882860	test: 0.825099
MAE train: 0.473064	val: 0.698910	test: 0.649427

Epoch: 32
Loss: 0.46615513946328846
RMSE train: 0.612415	val: 0.904046	test: 0.832778
MAE train: 0.477153	val: 0.713392	test: 0.654700

Epoch: 33
Loss: 0.4708655668156488
RMSE train: 0.595181	val: 0.880111	test: 0.820919
MAE train: 0.467514	val: 0.689858	test: 0.644589

Epoch: 34
Loss: 0.4734026151044028
RMSE train: 0.592333	val: 0.848573	test: 0.793384
MAE train: 0.467848	val: 0.670143	test: 0.613381

Epoch: 35
Loss: 0.47556438616343905
RMSE train: 0.578759	val: 0.860846	test: 0.809969
MAE train: 0.453774	val: 0.675993	test: 0.640807

Epoch: 36
Loss: 0.48385332950523924
RMSE train: 0.590445	val: 0.907170	test: 0.838545
MAE train: 0.461402	val: 0.711676	test: 0.641224

Epoch: 37
Loss: 0.47841200658253263
RMSE train: 0.582751	val: 0.891652	test: 0.822314
MAE train: 0.455562	val: 0.706616	test: 0.641539

Epoch: 38
Loss: 0.4484659050192152
RMSE train: 0.570316	val: 0.868437	test: 0.822772
MAE train: 0.446688	val: 0.676064	test: 0.643578

Epoch: 39
Loss: 0.45197366178035736
RMSE train: 0.583187	val: 0.846237	test: 0.790771
MAE train: 0.463688	val: 0.663110	test: 0.619146

Epoch: 40
Loss: 0.45361916933740887
RMSE train: 0.593964	val: 0.915590	test: 0.860871
MAE train: 0.463328	val: 0.710467	test: 0.671546

Epoch: 41
Loss: 0.4249809341771262
RMSE train: 0.571639	val: 0.869892	test: 0.809499
MAE train: 0.445546	val: 0.678969	test: 0.636445

Epoch: 42
Loss: 0.40306573254721506
RMSE train: 0.529237	val: 0.857219	test: 0.812602
MAE train: 0.414581	val: 0.667692	test: 0.634984

Epoch: 43
Loss: 0.4015840675149645
RMSE train: 0.565699	val: 0.904924	test: 0.841932
MAE train: 0.442535	val: 0.710038	test: 0.649115

Epoch: 44
Loss: 0.39177377309118
RMSE train: 0.535451	val: 0.844032	test: 0.786244
MAE train: 0.420075	val: 0.658332	test: 0.619640

Epoch: 45
Loss: 0.3986128100327083
RMSE train: 0.524703	val: 0.831818	test: 0.790872
MAE train: 0.413621	val: 0.646994	test: 0.621898

Epoch: 46
Loss: 0.4066773440156664
RMSE train: 0.551124	val: 0.836071	test: 0.789441
MAE train: 0.439188	val: 0.653416	test: 0.620357

Epoch: 47
Loss: 0.39005247609955923
RMSE train: 0.512677	val: 0.810341	test: 0.781104
MAE train: 0.409147	val: 0.632750	test: 0.601129

Epoch: 48
Loss: 0.37495301025254385
RMSE train: 0.550708	val: 0.919508	test: 0.856479
MAE train: 0.427774	val: 0.716451	test: 0.666089

Epoch: 49
Loss: 0.38166374606745584
RMSE train: 0.506846	val: 0.878948	test: 0.832881
MAE train: 0.397659	val: 0.691296	test: 0.648119

Epoch: 50
Loss: 0.35925054124423433
RMSE train: 0.511721	val: 0.853180	test: 0.796953
MAE train: 0.403889	val: 0.669177	test: 0.620438

Epoch: 51
Loss: 0.3606006609542029
RMSE train: 0.528492	val: 0.840315	test: 0.798474
MAE train: 0.417502	val: 0.660301	test: 0.607429

Epoch: 52
Loss: 0.3607480547257832
RMSE train: 0.509951	val: 0.871094	test: 0.816580
MAE train: 0.398430	val: 0.684449	test: 0.626221

Epoch: 53
Loss: 0.33388962915965487
RMSE train: 0.487206	val: 0.827645	test: 0.790415
MAE train: 0.382259	val: 0.649211	test: 0.618879

Epoch: 54
Loss: 0.3504778913089207
RMSE train: 0.472438	val: 0.836580	test: 0.788910
MAE train: 0.373144	val: 0.649911	test: 0.614551

Epoch: 55
Loss: 0.3333588434117181
RMSE train: 0.487652	val: 0.848719	test: 0.798087
MAE train: 0.385881	val: 0.670968	test: 0.618694

Epoch: 56
Loss: 0.3251516414540155
RMSE train: 0.512773	val: 0.882599	test: 0.836663
MAE train: 0.401943	val: 0.690105	test: 0.648174

Epoch: 57
Loss: 0.3233060304607664
RMSE train: 0.462390	val: 0.849659	test: 0.795836
MAE train: 0.363741	val: 0.661081	test: 0.614994

Epoch: 58
Loss: 0.30651351383754183
RMSE train: 0.461191	val: 0.855344	test: 0.819812
MAE train: 0.362738	val: 0.665232	test: 0.629454

Epoch: 59
Loss: 0.3300799642290388
RMSE train: 0.479918	val: 0.838783	test: 0.793461
MAE train: 0.376696	val: 0.654236	test: 0.620227

Epoch: 60
Loss: 0.3063010254076549
RMSE train: 0.455172	val: 0.842056	test: 0.795239
MAE train: 0.358815	val: 0.657497	test: 0.619538

Epoch: 61
Loss: 0.2966714714254652
RMSE train: 0.453798	val: 0.821031	test: 0.785625
MAE train: 0.359482	val: 0.642172	test: 0.610297

Epoch: 62
Loss: 0.30264632190976826
RMSE train: 0.475330	val: 0.868128	test: 0.813745
MAE train: 0.373052	val: 0.674207	test: 0.632805

Epoch: 63
Loss: 0.3053897757615362
RMSE train: 0.436951	val: 0.821007	test: 0.777792
MAE train: 0.346275	val: 0.642798	test: 0.606798

Epoch: 64
Loss: 0.3026616136942591
RMSE train: 0.491713	val: 0.889870	test: 0.845555
MAE train: 0.387333	val: 0.690579	test: 0.663853

Epoch: 65
Loss: 0.31655171300683704
RMSE train: 0.440422	val: 0.820219	test: 0.771908
MAE train: 0.346271	val: 0.639937	test: 0.601932

Epoch: 66
Loss: 0.31160156428813934
RMSE train: 0.442348	val: 0.840879	test: 0.802906
MAE train: 0.348858	val: 0.650460	test: 0.625729

Epoch: 67
Loss: 0.29105461069515776
RMSE train: 0.459227	val: 0.883126	test: 0.830768
MAE train: 0.360187	val: 0.685422	test: 0.648134

Epoch: 68
Loss: 0.26471190048115595
RMSE train: 0.421411	val: 0.832408	test: 0.791535
MAE train: 0.329549	val: 0.640407	test: 0.618645

Epoch: 69
Loss: 0.2568237228052957
RMSE train: 0.482049	val: 0.816444	test: 0.779096
MAE train: 0.384939	val: 0.642982	test: 0.602792

Epoch: 70
Loss: 0.28830233429159435
RMSE train: 0.422538	val: 0.836559	test: 0.801451
MAE train: 0.333543	val: 0.645122	test: 0.622223

Epoch: 71
Loss: 0.2841418460011482
RMSE train: 0.428357	val: 0.827835	test: 0.791685
MAE train: 0.337923	val: 0.654889	test: 0.610443

Epoch: 72
Loss: 0.26086517742701937
RMSE train: 0.398577	val: 0.799771	test: 0.778699
MAE train: 0.315187	val: 0.620160	test: 0.606673

Epoch: 73
Loss: 0.2773155633892332
RMSE train: 0.409995	val: 0.835408	test: 0.775834
MAE train: 0.323006	val: 0.649859	test: 0.606213

Epoch: 74
Loss: 0.2635003168668066
RMSE train: 0.422476	val: 0.871653	test: 0.811025
MAE train: 0.332205	val: 0.671976	test: 0.632759

Epoch: 75
Loss: 0.25248273674930843
RMSE train: 0.420493	val: 0.861647	test: 0.815808
MAE train: 0.329679	val: 0.670573	test: 0.635900

Epoch: 76
Loss: 0.25318733709199087
RMSE train: 0.425835	val: 0.857713	test: 0.801319
MAE train: 0.335727	val: 0.661625	test: 0.624575

Epoch: 77
Loss: 0.2543774736779077
RMSE train: 0.401554	val: 0.828064	test: 0.778816
MAE train: 0.317066	val: 0.646856	test: 0.608474

Epoch: 78
Loss: 0.2621288427284786
RMSE train: 0.401784	val: 0.823992	test: 0.788825
MAE train: 0.317340	val: 0.640929	test: 0.613515

Epoch: 79
Loss: 0.2589562726872308
RMSE train: 0.383465	val: 0.846574	test: 0.803489
MAE train: 0.300173	val: 0.653425	test: 0.624810

Epoch: 80
Loss: 0.2575169450470379
RMSE train: 0.401524	val: 0.812932	test: 0.766864
MAE train: 0.316812	val: 0.630488	test: 0.592333

Epoch: 81
Loss: 0.2522725058453424
RMSE train: 0.427874	val: 0.876780	test: 0.836964
MAE train: 0.338926	val: 0.676716	test: 0.656193

Epoch: 82
Loss: 0.23006037303379603
RMSE train: 0.425963	val: 0.873901	test: 0.831541

Epoch: 22
Loss: 0.6652932720524924
RMSE train: 0.696463	val: 0.864810	test: 0.864909
MAE train: 0.547881	val: 0.688969	test: 0.676568

Epoch: 23
Loss: 0.6460333594254085
RMSE train: 0.695701	val: 0.871111	test: 0.863072
MAE train: 0.546289	val: 0.698904	test: 0.682131

Epoch: 24
Loss: 0.6040017604827881
RMSE train: 0.685400	val: 0.880402	test: 0.856317
MAE train: 0.532872	val: 0.710364	test: 0.676335

Epoch: 25
Loss: 0.5997766554355621
RMSE train: 0.679375	val: 0.875611	test: 0.845217
MAE train: 0.535297	val: 0.709504	test: 0.669424

Epoch: 26
Loss: 0.6207952669688633
RMSE train: 0.656026	val: 0.855604	test: 0.848686
MAE train: 0.513525	val: 0.685842	test: 0.673850

Epoch: 27
Loss: 0.5779467821121216
RMSE train: 0.656286	val: 0.854301	test: 0.849750
MAE train: 0.513240	val: 0.684825	test: 0.673694

Epoch: 28
Loss: 0.5675280115434101
RMSE train: 0.656888	val: 0.864237	test: 0.854439
MAE train: 0.516612	val: 0.686377	test: 0.684703

Epoch: 29
Loss: 0.5459524307932172
RMSE train: 0.650705	val: 0.861263	test: 0.852558
MAE train: 0.510218	val: 0.691247	test: 0.678647

Epoch: 30
Loss: 0.5570878450359616
RMSE train: 0.639620	val: 0.861897	test: 0.866191
MAE train: 0.502409	val: 0.689720	test: 0.679863

Epoch: 31
Loss: 0.5479253785950797
RMSE train: 0.633172	val: 0.869747	test: 0.847234
MAE train: 0.494175	val: 0.695499	test: 0.666933

Epoch: 32
Loss: 0.5298048726149968
RMSE train: 0.620166	val: 0.856234	test: 0.858020
MAE train: 0.486879	val: 0.687200	test: 0.681753

Epoch: 33
Loss: 0.5259869779859271
RMSE train: 0.623753	val: 0.854630	test: 0.870509
MAE train: 0.491060	val: 0.680196	test: 0.691295

Epoch: 34
Loss: 0.5062143972941807
RMSE train: 0.626403	val: 0.864229	test: 0.863359
MAE train: 0.492984	val: 0.695356	test: 0.687401

Epoch: 35
Loss: 0.4958995708397457
RMSE train: 0.614971	val: 0.847641	test: 0.841577
MAE train: 0.477390	val: 0.674909	test: 0.669938

Epoch: 36
Loss: 0.4930813269955771
RMSE train: 0.587987	val: 0.830833	test: 0.841590
MAE train: 0.460546	val: 0.663399	test: 0.673734

Epoch: 37
Loss: 0.483297409755843
RMSE train: 0.578958	val: 0.817662	test: 0.831574
MAE train: 0.451501	val: 0.656397	test: 0.657594

Epoch: 38
Loss: 0.48108655852930887
RMSE train: 0.591206	val: 0.854770	test: 0.871129
MAE train: 0.464059	val: 0.684291	test: 0.693828

Epoch: 39
Loss: 0.4634608519928796
RMSE train: 0.593849	val: 0.855666	test: 0.869106
MAE train: 0.468661	val: 0.687284	test: 0.691163

Epoch: 40
Loss: 0.4543327050549643
RMSE train: 0.582319	val: 0.839936	test: 0.841566
MAE train: 0.453550	val: 0.667571	test: 0.668938

Epoch: 41
Loss: 0.43714037963322233
RMSE train: 0.548170	val: 0.830081	test: 0.843769
MAE train: 0.427785	val: 0.661382	test: 0.665052

Epoch: 42
Loss: 0.4525014545236315
RMSE train: 0.583991	val: 0.846610	test: 0.848185
MAE train: 0.458317	val: 0.680746	test: 0.667297

Epoch: 43
Loss: 0.4486825742891857
RMSE train: 0.564657	val: 0.846162	test: 0.837833
MAE train: 0.442172	val: 0.680035	test: 0.669436

Epoch: 44
Loss: 0.4445744731596538
RMSE train: 0.537473	val: 0.839522	test: 0.847259
MAE train: 0.417060	val: 0.669538	test: 0.660509

Epoch: 45
Loss: 0.42572696932724546
RMSE train: 0.593153	val: 0.873998	test: 0.901832
MAE train: 0.469707	val: 0.707495	test: 0.720799

Epoch: 46
Loss: 0.4140834552901132
RMSE train: 0.557928	val: 0.860105	test: 0.873227
MAE train: 0.440062	val: 0.690132	test: 0.682525

Epoch: 47
Loss: 0.4002029129437038
RMSE train: 0.557229	val: 0.835397	test: 0.844374
MAE train: 0.433829	val: 0.664901	test: 0.676655

Epoch: 48
Loss: 0.41455570076193127
RMSE train: 0.558724	val: 0.846726	test: 0.860283
MAE train: 0.438951	val: 0.676462	test: 0.685823

Epoch: 49
Loss: 0.4025288884128843
RMSE train: 0.546069	val: 0.831627	test: 0.842779
MAE train: 0.426010	val: 0.660571	test: 0.665329

Epoch: 50
Loss: 0.3744318432041577
RMSE train: 0.516619	val: 0.843738	test: 0.859212
MAE train: 0.403258	val: 0.673270	test: 0.684485

Epoch: 51
Loss: 0.3702986261674336
RMSE train: 0.509943	val: 0.824981	test: 0.839357
MAE train: 0.401599	val: 0.662268	test: 0.664048

Epoch: 52
Loss: 0.36081293863909586
RMSE train: 0.486701	val: 0.825009	test: 0.857184
MAE train: 0.378932	val: 0.661753	test: 0.675808

Epoch: 53
Loss: 0.3504036090203694
RMSE train: 0.482880	val: 0.828276	test: 0.854308
MAE train: 0.375986	val: 0.658303	test: 0.672249

Epoch: 54
Loss: 0.34990401353154865
RMSE train: 0.477500	val: 0.817830	test: 0.832122
MAE train: 0.373341	val: 0.653990	test: 0.660929

Epoch: 55
Loss: 0.3862267796482359
RMSE train: 0.497888	val: 0.836932	test: 0.861729
MAE train: 0.390489	val: 0.666189	test: 0.680626

Epoch: 56
Loss: 0.3691859415599278
RMSE train: 0.488791	val: 0.832765	test: 0.831490
MAE train: 0.383410	val: 0.666976	test: 0.654814

Epoch: 57
Loss: 0.3476593494415283
RMSE train: 0.479388	val: 0.825525	test: 0.842442
MAE train: 0.376002	val: 0.663096	test: 0.663876

Epoch: 58
Loss: 0.34552796610764097
RMSE train: 0.458636	val: 0.824724	test: 0.845802
MAE train: 0.358645	val: 0.654757	test: 0.660189

Epoch: 59
Loss: 0.33289157705647604
RMSE train: 0.499895	val: 0.854428	test: 0.888666
MAE train: 0.397984	val: 0.685797	test: 0.697711

Epoch: 60
Loss: 0.3255807246480669
RMSE train: 0.463375	val: 0.850419	test: 0.868698
MAE train: 0.363557	val: 0.679387	test: 0.676180

Epoch: 61
Loss: 0.3158521481922695
RMSE train: 0.477020	val: 0.833187	test: 0.852444
MAE train: 0.376416	val: 0.670910	test: 0.669162

Epoch: 62
Loss: 0.31339709141424726
RMSE train: 0.461067	val: 0.836515	test: 0.854987
MAE train: 0.364970	val: 0.666320	test: 0.671801

Epoch: 63
Loss: 0.30567856026547296
RMSE train: 0.418884	val: 0.825444	test: 0.839732
MAE train: 0.328099	val: 0.656806	test: 0.665202

Epoch: 64
Loss: 0.29296805816037313
RMSE train: 0.469248	val: 0.835338	test: 0.828328
MAE train: 0.372802	val: 0.656048	test: 0.650908

Epoch: 65
Loss: 0.2940867788025311
RMSE train: 0.424102	val: 0.838954	test: 0.875416
MAE train: 0.333609	val: 0.673046	test: 0.686948

Epoch: 66
Loss: 0.2758552080818585
RMSE train: 0.448404	val: 0.834349	test: 0.844943
MAE train: 0.355112	val: 0.662994	test: 0.670621

Epoch: 67
Loss: 0.28762475507599966
RMSE train: 0.430645	val: 0.819302	test: 0.845318
MAE train: 0.337758	val: 0.657338	test: 0.657689

Epoch: 68
Loss: 0.27107722631522585
RMSE train: 0.482221	val: 0.868453	test: 0.882885
MAE train: 0.387017	val: 0.696877	test: 0.701312

Epoch: 69
Loss: 0.2687387019395828
RMSE train: 0.391699	val: 0.835658	test: 0.843633
MAE train: 0.304461	val: 0.671851	test: 0.658741

Epoch: 70
Loss: 0.2975191207868712
RMSE train: 0.412731	val: 0.839668	test: 0.843437
MAE train: 0.324828	val: 0.673072	test: 0.664084

Epoch: 71
Loss: 0.2702351063489914
RMSE train: 0.426117	val: 0.825368	test: 0.851067
MAE train: 0.335726	val: 0.661510	test: 0.663859

Epoch: 72
Loss: 0.26206732222012113
RMSE train: 0.443258	val: 0.857907	test: 0.889469
MAE train: 0.349758	val: 0.679991	test: 0.702712

Epoch: 73
Loss: 0.24153749006135122
RMSE train: 0.413652	val: 0.824607	test: 0.836885
MAE train: 0.327192	val: 0.649546	test: 0.650645

Epoch: 74
Loss: 0.2490460223385266
RMSE train: 0.377288	val: 0.821751	test: 0.844009
MAE train: 0.296964	val: 0.656481	test: 0.656321

Epoch: 75
Loss: 0.24649415697370256
RMSE train: 0.375212	val: 0.818740	test: 0.845266
MAE train: 0.295472	val: 0.650722	test: 0.659068

Epoch: 76
Loss: 0.22985263488122396
RMSE train: 0.394433	val: 0.817653	test: 0.822501
MAE train: 0.311799	val: 0.645038	test: 0.635658

Epoch: 77
Loss: 0.25350750769887653
RMSE train: 0.375451	val: 0.834382	test: 0.853515
MAE train: 0.295305	val: 0.671013	test: 0.668395

Epoch: 78
Loss: 0.22661576100758143
RMSE train: 0.400175	val: 0.839119	test: 0.853684
MAE train: 0.315432	val: 0.673434	test: 0.668652

Epoch: 79
Loss: 0.23382851587874548
RMSE train: 0.388816	val: 0.839187	test: 0.842098
MAE train: 0.305700	val: 0.675874	test: 0.652192

Epoch: 80
Loss: 0.23825497605970927
RMSE train: 0.360315	val: 0.842707	test: 0.872920
MAE train: 0.284402	val: 0.680632	test: 0.680433

Epoch: 81
Loss: 0.22355365859610693
RMSE train: 0.389146	val: 0.844249	test: 0.862571
MAE train: 0.310012	val: 0.677187	test: 0.681591

Epoch: 82
Loss: 0.21103291000638688
RMSE train: 0.379374	val: 0.821032	test: 0.839085

Epoch: 22
Loss: 0.583389652626855
RMSE train: 0.676169	val: 0.907266	test: 0.828750
MAE train: 0.527994	val: 0.707188	test: 0.671706

Epoch: 23
Loss: 0.5789425798824855
RMSE train: 0.639995	val: 0.866772	test: 0.803844
MAE train: 0.503209	val: 0.679300	test: 0.647803

Epoch: 24
Loss: 0.5629763518060956
RMSE train: 0.669199	val: 0.915377	test: 0.836791
MAE train: 0.520004	val: 0.718091	test: 0.674145

Epoch: 25
Loss: 0.5241547503641674
RMSE train: 0.628943	val: 0.871010	test: 0.798274
MAE train: 0.491711	val: 0.678334	test: 0.642120

Epoch: 26
Loss: 0.5352147413151604
RMSE train: 0.693821	val: 0.940133	test: 0.866580
MAE train: 0.541009	val: 0.731793	test: 0.689841

Epoch: 27
Loss: 0.5255271643400192
RMSE train: 0.622470	val: 0.858010	test: 0.803062
MAE train: 0.485896	val: 0.666347	test: 0.646886

Epoch: 28
Loss: 0.5215038763625282
RMSE train: 0.618817	val: 0.887448	test: 0.818136
MAE train: 0.482467	val: 0.693753	test: 0.656400

Epoch: 29
Loss: 0.5130352888788495
RMSE train: 0.603129	val: 0.843101	test: 0.801270
MAE train: 0.473810	val: 0.663004	test: 0.643692

Epoch: 30
Loss: 0.5262281468936375
RMSE train: 0.624650	val: 0.891981	test: 0.837570
MAE train: 0.487994	val: 0.686976	test: 0.672308

Epoch: 31
Loss: 0.5229550493615014
RMSE train: 0.584819	val: 0.838457	test: 0.807821
MAE train: 0.458452	val: 0.645663	test: 0.644433

Epoch: 32
Loss: 0.46912163708891186
RMSE train: 0.586056	val: 0.850331	test: 0.796062
MAE train: 0.457166	val: 0.658413	test: 0.630293

Epoch: 33
Loss: 0.5135422455413001
RMSE train: 0.584517	val: 0.845319	test: 0.801892
MAE train: 0.456139	val: 0.652296	test: 0.637824

Epoch: 34
Loss: 0.4746582486799785
RMSE train: 0.602707	val: 0.855315	test: 0.815056
MAE train: 0.469537	val: 0.670205	test: 0.654622

Epoch: 35
Loss: 0.4596359005996159
RMSE train: 0.577791	val: 0.848683	test: 0.795819
MAE train: 0.453407	val: 0.665256	test: 0.623915

Epoch: 36
Loss: 0.45402297590460095
RMSE train: 0.604365	val: 0.881082	test: 0.832126
MAE train: 0.470312	val: 0.671962	test: 0.668046

Epoch: 37
Loss: 0.455559430377824
RMSE train: 0.573497	val: 0.859819	test: 0.816197
MAE train: 0.450037	val: 0.664452	test: 0.653528

Epoch: 38
Loss: 0.4491401357310159
RMSE train: 0.572446	val: 0.869218	test: 0.827777
MAE train: 0.447904	val: 0.668739	test: 0.655777

Epoch: 39
Loss: 0.44569796536649975
RMSE train: 0.612851	val: 0.892968	test: 0.842889
MAE train: 0.481042	val: 0.684313	test: 0.682712

Epoch: 40
Loss: 0.45756214431353975
RMSE train: 0.546583	val: 0.843556	test: 0.803404
MAE train: 0.430314	val: 0.652398	test: 0.638704

Epoch: 41
Loss: 0.45535083753722055
RMSE train: 0.558981	val: 0.854080	test: 0.812998
MAE train: 0.442842	val: 0.656422	test: 0.645385

Epoch: 42
Loss: 0.4348167095865522
RMSE train: 0.571283	val: 0.872327	test: 0.823476
MAE train: 0.448911	val: 0.678543	test: 0.652676

Epoch: 43
Loss: 0.40852920923914227
RMSE train: 0.561070	val: 0.849411	test: 0.809037
MAE train: 0.444462	val: 0.653129	test: 0.637717

Epoch: 44
Loss: 0.40897765117032187
RMSE train: 0.559003	val: 0.858216	test: 0.810455
MAE train: 0.439511	val: 0.667525	test: 0.648100

Epoch: 45
Loss: 0.38838320119040354
RMSE train: 0.536120	val: 0.847265	test: 0.794786
MAE train: 0.420985	val: 0.657971	test: 0.628343

Epoch: 46
Loss: 0.3835479659693582
RMSE train: 0.553213	val: 0.862163	test: 0.824915
MAE train: 0.433683	val: 0.671188	test: 0.651932

Epoch: 47
Loss: 0.3734945761305945
RMSE train: 0.534274	val: 0.862692	test: 0.824355
MAE train: 0.421433	val: 0.668464	test: 0.648167

Epoch: 48
Loss: 0.3737830455814089
RMSE train: 0.522195	val: 0.836544	test: 0.779416
MAE train: 0.410563	val: 0.650504	test: 0.616524

Epoch: 49
Loss: 0.3551495575479099
RMSE train: 0.534699	val: 0.843500	test: 0.791549
MAE train: 0.421624	val: 0.653286	test: 0.625064

Epoch: 50
Loss: 0.3508513399532863
RMSE train: 0.480896	val: 0.800995	test: 0.769432
MAE train: 0.376560	val: 0.618511	test: 0.604783

Epoch: 51
Loss: 0.35358670141015736
RMSE train: 0.496471	val: 0.824967	test: 0.788965
MAE train: 0.387832	val: 0.633075	test: 0.633907

Epoch: 52
Loss: 0.3525787570646831
RMSE train: 0.533161	val: 0.845379	test: 0.795070
MAE train: 0.422202	val: 0.651150	test: 0.634544

Epoch: 53
Loss: 0.33357815018721987
RMSE train: 0.474015	val: 0.808352	test: 0.775738
MAE train: 0.373209	val: 0.620884	test: 0.614392

Epoch: 54
Loss: 0.33728413922446115
RMSE train: 0.467188	val: 0.794333	test: 0.778393
MAE train: 0.367437	val: 0.617930	test: 0.616792

Epoch: 55
Loss: 0.34754996427467894
RMSE train: 0.503380	val: 0.865626	test: 0.826410
MAE train: 0.395196	val: 0.663925	test: 0.663098

Epoch: 56
Loss: 0.3383335641452244
RMSE train: 0.510428	val: 0.825533	test: 0.788273
MAE train: 0.403809	val: 0.646434	test: 0.633057

Epoch: 57
Loss: 0.3348468669823238
RMSE train: 0.495845	val: 0.819677	test: 0.790721
MAE train: 0.391133	val: 0.629398	test: 0.631382

Epoch: 58
Loss: 0.3272058814764023
RMSE train: 0.458986	val: 0.795468	test: 0.770673
MAE train: 0.363196	val: 0.614423	test: 0.611148

Epoch: 59
Loss: 0.3376045823097229
RMSE train: 0.475546	val: 0.820980	test: 0.780009
MAE train: 0.373903	val: 0.627632	test: 0.625665

Epoch: 60
Loss: 0.32485392689704895
RMSE train: 0.454175	val: 0.788157	test: 0.773355
MAE train: 0.359194	val: 0.611906	test: 0.609227

Epoch: 61
Loss: 0.3264176547527313
RMSE train: 0.459143	val: 0.797063	test: 0.784887
MAE train: 0.363151	val: 0.616788	test: 0.623355

Epoch: 62
Loss: 0.31406095198222567
RMSE train: 0.451512	val: 0.814745	test: 0.776592
MAE train: 0.353696	val: 0.617459	test: 0.617799

Epoch: 63
Loss: 0.2878967651299068
RMSE train: 0.451675	val: 0.807639	test: 0.763961
MAE train: 0.352627	val: 0.617767	test: 0.597370

Epoch: 64
Loss: 0.2868809551000595
RMSE train: 0.484178	val: 0.855963	test: 0.815244
MAE train: 0.378898	val: 0.659326	test: 0.646334

Epoch: 65
Loss: 0.28374166573796955
RMSE train: 0.469511	val: 0.822492	test: 0.767651
MAE train: 0.371227	val: 0.636964	test: 0.610579

Epoch: 66
Loss: 0.30124179806028095
RMSE train: 0.441998	val: 0.820993	test: 0.784333
MAE train: 0.348656	val: 0.628747	test: 0.617257

Epoch: 67
Loss: 0.29039404009069714
RMSE train: 0.435740	val: 0.817346	test: 0.780761
MAE train: 0.344371	val: 0.626569	test: 0.618395

Epoch: 68
Loss: 0.2800254928214209
RMSE train: 0.452596	val: 0.833972	test: 0.789333
MAE train: 0.357758	val: 0.633238	test: 0.625527

Epoch: 69
Loss: 0.2772864188466753
RMSE train: 0.446429	val: 0.819658	test: 0.784172
MAE train: 0.352282	val: 0.633107	test: 0.622603

Epoch: 70
Loss: 0.27467529475688934
RMSE train: 0.419126	val: 0.817107	test: 0.793858
MAE train: 0.330101	val: 0.625809	test: 0.630279

Epoch: 71
Loss: 0.26568784564733505
RMSE train: 0.425703	val: 0.798212	test: 0.775860
MAE train: 0.337558	val: 0.605913	test: 0.617905

Epoch: 72
Loss: 0.26057769038847517
RMSE train: 0.400485	val: 0.779352	test: 0.765865
MAE train: 0.314868	val: 0.592790	test: 0.604616

Epoch: 73
Loss: 0.2728049691234316
RMSE train: 0.489562	val: 0.851901	test: 0.805232
MAE train: 0.393028	val: 0.649608	test: 0.640800

Epoch: 74
Loss: 0.2775541458811079
RMSE train: 0.421019	val: 0.803243	test: 0.767994
MAE train: 0.331534	val: 0.625402	test: 0.600573

Epoch: 75
Loss: 0.28489518804209574
RMSE train: 0.428540	val: 0.821722	test: 0.791317
MAE train: 0.339450	val: 0.632639	test: 0.623666

Epoch: 76
Loss: 0.267427391239575
RMSE train: 0.463921	val: 0.833072	test: 0.796841
MAE train: 0.368278	val: 0.643388	test: 0.635666

Epoch: 77
Loss: 0.2628400613154684
RMSE train: 0.420205	val: 0.799538	test: 0.770477
MAE train: 0.330312	val: 0.620894	test: 0.615147

Epoch: 78
Loss: 0.2495425652180399
RMSE train: 0.399692	val: 0.785359	test: 0.768286
MAE train: 0.316587	val: 0.600926	test: 0.609510

Epoch: 79
Loss: 0.2480123064347676
RMSE train: 0.406400	val: 0.785022	test: 0.763986
MAE train: 0.321206	val: 0.596540	test: 0.599496

Epoch: 80
Loss: 0.23482210721288407
RMSE train: 0.439870	val: 0.800111	test: 0.780914
MAE train: 0.349272	val: 0.610545	test: 0.626643

Epoch: 81
Loss: 0.24860080225127085
RMSE train: 0.381866	val: 0.788770	test: 0.779039
MAE train: 0.302752	val: 0.597547	test: 0.612592

Epoch: 82
Loss: 0.24224831057446344
RMSE train: 0.376260	val: 0.793119	test: 0.752207

Epoch: 22
Loss: 0.6980044671467373
RMSE train: 0.729949	val: 0.968241	test: 0.933006
MAE train: 0.574121	val: 0.775023	test: 0.735507

Epoch: 23
Loss: 0.6774242222309113
RMSE train: 0.695251	val: 0.960077	test: 0.927816
MAE train: 0.547092	val: 0.763923	test: 0.743647

Epoch: 24
Loss: 0.6391990738255637
RMSE train: 0.704316	val: 0.946819	test: 0.927359
MAE train: 0.556526	val: 0.759091	test: 0.734395

Epoch: 25
Loss: 0.6279778416667666
RMSE train: 0.687640	val: 0.952514	test: 0.920558
MAE train: 0.544903	val: 0.765804	test: 0.724197

Epoch: 26
Loss: 0.6217049871172223
RMSE train: 0.665734	val: 0.953879	test: 0.911020
MAE train: 0.522341	val: 0.758191	test: 0.716598

Epoch: 27
Loss: 0.6055252381733486
RMSE train: 0.658151	val: 0.980729	test: 0.945135
MAE train: 0.516356	val: 0.770720	test: 0.751353

Epoch: 28
Loss: 0.6014163451535361
RMSE train: 0.658158	val: 0.967447	test: 0.931172
MAE train: 0.520007	val: 0.765973	test: 0.748465

Epoch: 29
Loss: 0.5911115961415427
RMSE train: 0.640542	val: 0.951517	test: 0.935676
MAE train: 0.500829	val: 0.757475	test: 0.739728

Epoch: 30
Loss: 0.5471999581371035
RMSE train: 0.629201	val: 0.953913	test: 0.940664
MAE train: 0.493399	val: 0.755186	test: 0.740611

Epoch: 31
Loss: 0.5405680239200592
RMSE train: 0.626813	val: 0.988960	test: 0.959415
MAE train: 0.492796	val: 0.779010	test: 0.763744

Epoch: 32
Loss: 0.5341750915561404
RMSE train: 0.601167	val: 0.994777	test: 0.960621
MAE train: 0.469464	val: 0.782991	test: 0.766896

Epoch: 33
Loss: 0.5210329081330981
RMSE train: 0.611320	val: 0.962089	test: 0.949002
MAE train: 0.487986	val: 0.769209	test: 0.756609

Epoch: 34
Loss: 0.5125813718353
RMSE train: 0.581915	val: 0.980285	test: 0.967658
MAE train: 0.455075	val: 0.775216	test: 0.769277

Epoch: 35
Loss: 0.48206777232033865
RMSE train: 0.610061	val: 1.048177	test: 1.025609
MAE train: 0.478840	val: 0.819776	test: 0.823328

Epoch: 36
Loss: 0.5095645615032741
RMSE train: 0.587987	val: 1.008127	test: 0.996170
MAE train: 0.457948	val: 0.790405	test: 0.784525

Epoch: 37
Loss: 0.4861715244395392
RMSE train: 0.572770	val: 0.990190	test: 0.980943
MAE train: 0.453701	val: 0.784463	test: 0.781812

Epoch: 38
Loss: 0.4823979522500719
RMSE train: 0.559310	val: 0.991321	test: 0.984797
MAE train: 0.439039	val: 0.780501	test: 0.787234

Epoch: 39
Loss: 0.45559059934956686
RMSE train: 0.588364	val: 1.045070	test: 1.045199
MAE train: 0.466170	val: 0.823566	test: 0.828697

Epoch: 40
Loss: 0.4324389547109604
RMSE train: 0.573870	val: 1.026641	test: 1.018501
MAE train: 0.450412	val: 0.807890	test: 0.812481

Epoch: 41
Loss: 0.45039212916578564
RMSE train: 0.540600	val: 0.977860	test: 0.981128
MAE train: 0.421566	val: 0.762327	test: 0.771248

Epoch: 42
Loss: 0.454142302274704
RMSE train: 0.527672	val: 0.964649	test: 0.957652
MAE train: 0.415108	val: 0.752833	test: 0.761577

Epoch: 43
Loss: 0.4481993189879826
RMSE train: 0.536605	val: 1.009049	test: 1.010720
MAE train: 0.421231	val: 0.797429	test: 0.807647

Epoch: 44
Loss: 0.4093851425818035
RMSE train: 0.505597	val: 1.022947	test: 1.026475
MAE train: 0.398092	val: 0.802278	test: 0.815744

Epoch: 45
Loss: 0.3883077012641089
RMSE train: 0.485959	val: 0.984859	test: 0.980111
MAE train: 0.378493	val: 0.771020	test: 0.770285

Epoch: 46
Loss: 0.40041099914482664
RMSE train: 0.480480	val: 0.969420	test: 0.973422
MAE train: 0.374078	val: 0.763105	test: 0.767319

Epoch: 47
Loss: 0.3698002334151949
RMSE train: 0.494256	val: 0.986761	test: 0.991446
MAE train: 0.390635	val: 0.769422	test: 0.781799

Epoch: 48
Loss: 0.3664107642003468
RMSE train: 0.481638	val: 1.024657	test: 1.009884
MAE train: 0.376086	val: 0.806051	test: 0.805615

Epoch: 49
Loss: 0.3606307698147638
RMSE train: 0.498703	val: 1.001879	test: 1.018168
MAE train: 0.396396	val: 0.785028	test: 0.811657

Epoch: 50
Loss: 0.34717210807970594
RMSE train: 0.464023	val: 1.004180	test: 0.994780
MAE train: 0.361642	val: 0.784852	test: 0.786116

Epoch: 51
Loss: 0.3397927837712424
RMSE train: 0.434821	val: 1.006875	test: 1.005124
MAE train: 0.340636	val: 0.786737	test: 0.797396

Epoch: 52
Loss: 0.3318308059658323
RMSE train: 0.451799	val: 0.984560	test: 1.000851
MAE train: 0.354428	val: 0.765684	test: 0.782281

Epoch: 53
Loss: 0.33864566896642956
RMSE train: 0.433346	val: 1.018212	test: 1.013812
MAE train: 0.336601	val: 0.790684	test: 0.801716

Epoch: 54
Loss: 0.31222756632736753
RMSE train: 0.425514	val: 1.001489	test: 1.004932
MAE train: 0.332759	val: 0.781245	test: 0.782378

Epoch: 55
Loss: 0.32815967074462343
RMSE train: 0.421495	val: 1.012297	test: 1.001968
MAE train: 0.330038	val: 0.792468	test: 0.788744

Epoch: 56
Loss: 0.3479366621800831
RMSE train: 0.433524	val: 0.988086	test: 0.958976
MAE train: 0.341437	val: 0.779414	test: 0.765482

Epoch: 57
Loss: 0.3112236665827887
RMSE train: 0.544534	val: 0.984334	test: 1.004452
MAE train: 0.434463	val: 0.775140	test: 0.789368

Epoch: 58
Loss: 0.3185244117464338
RMSE train: 0.447722	val: 1.010998	test: 1.004181
MAE train: 0.356611	val: 0.791610	test: 0.797532

Epoch: 59
Loss: 0.3291482073920114
RMSE train: 0.406587	val: 1.000243	test: 1.001347
MAE train: 0.319502	val: 0.782280	test: 0.787017

Epoch: 60
Loss: 0.31850503385066986
RMSE train: 0.404621	val: 0.989693	test: 0.977436
MAE train: 0.319885	val: 0.782447	test: 0.778314

Epoch: 61
Loss: 0.3023431258542197
RMSE train: 0.435378	val: 1.040250	test: 1.035353
MAE train: 0.342904	val: 0.814049	test: 0.812545

Epoch: 62
Loss: 0.29590117931365967
RMSE train: 0.385965	val: 0.988589	test: 0.977106
MAE train: 0.304229	val: 0.772050	test: 0.770585

Epoch: 63
Loss: 0.2724768083010401
RMSE train: 0.370665	val: 1.017578	test: 1.007952
MAE train: 0.292874	val: 0.798187	test: 0.801558

Epoch: 64
Loss: 0.27343365656478064
RMSE train: 0.386741	val: 1.002676	test: 0.986822
MAE train: 0.306200	val: 0.785391	test: 0.777534

Epoch: 65
Loss: 0.26155822085482733
RMSE train: 0.420451	val: 0.998620	test: 0.984968
MAE train: 0.335329	val: 0.786120	test: 0.775943

Epoch: 66
Loss: 0.2684267684817314
RMSE train: 0.428133	val: 1.043958	test: 1.032068
MAE train: 0.341743	val: 0.813008	test: 0.813125

Epoch: 67
Loss: 0.26892840223652975
RMSE train: 0.380571	val: 1.033401	test: 1.020527
MAE train: 0.301972	val: 0.810977	test: 0.808606

Epoch: 68
Loss: 0.24306363186665944
RMSE train: 0.339290	val: 0.974946	test: 0.959985
MAE train: 0.267326	val: 0.763473	test: 0.756266

Epoch: 69
Loss: 0.2504977988345282
RMSE train: 0.347995	val: 1.010627	test: 1.005375
MAE train: 0.272512	val: 0.783468	test: 0.786707

Epoch: 70
Loss: 0.24856987382684434
RMSE train: 0.354381	val: 0.988314	test: 0.968717
MAE train: 0.279279	val: 0.776811	test: 0.758550

Epoch: 71
Loss: 0.2498296137366976
RMSE train: 0.342669	val: 0.990425	test: 0.966239
MAE train: 0.268611	val: 0.780401	test: 0.764564

Epoch: 72
Loss: 0.2413859793118068
RMSE train: 0.377623	val: 1.034294	test: 1.018948
MAE train: 0.301320	val: 0.806689	test: 0.811238

Epoch: 73
Loss: 0.23397364786693028
RMSE train: 0.337543	val: 0.986228	test: 0.968853
MAE train: 0.266206	val: 0.773631	test: 0.764453

Epoch: 74
Loss: 0.20977656011070525
RMSE train: 0.320371	val: 1.014704	test: 1.001795
MAE train: 0.253107	val: 0.792306	test: 0.788848

Epoch: 75
Loss: 0.21503558967794692
RMSE train: 0.339775	val: 1.023435	test: 0.997794
MAE train: 0.267135	val: 0.803195	test: 0.786079

Epoch: 76
Loss: 0.20878468879631587
RMSE train: 0.318094	val: 0.983350	test: 0.975005
MAE train: 0.251907	val: 0.771632	test: 0.769180

Epoch: 77
Loss: 0.23506792208978108
RMSE train: 0.313358	val: 0.991226	test: 0.984482
MAE train: 0.247586	val: 0.774660	test: 0.772058

Epoch: 78
Loss: 0.2132327758840152
RMSE train: 0.343915	val: 1.012173	test: 0.994387
MAE train: 0.272351	val: 0.796717	test: 0.790745

Epoch: 79
Loss: 0.20890048039811
RMSE train: 0.367810	val: 1.010765	test: 0.994592
MAE train: 0.291745	val: 0.791118	test: 0.776644

Epoch: 80
Loss: 0.21509919528450286
RMSE train: 0.458445	val: 1.036422	test: 1.002999
MAE train: 0.369289	val: 0.816328	test: 0.800823

Epoch: 81
Loss: 0.2056915664247104
RMSE train: 0.330114	val: 0.983844	test: 0.964459
MAE train: 0.262164	val: 0.770949	test: 0.765812

Epoch: 82
Loss: 0.189140435308218
RMSE train: 0.307756	val: 0.988984	test: 0.958056

Epoch: 22
Loss: 0.7436032167502812
RMSE train: 0.765327	val: 0.983456	test: 0.920387
MAE train: 0.611839	val: 0.784971	test: 0.744344

Epoch: 23
Loss: 0.7153650011335101
RMSE train: 0.761921	val: 0.997916	test: 0.935899
MAE train: 0.596058	val: 0.802091	test: 0.744188

Epoch: 24
Loss: 0.7120668845517295
RMSE train: 0.744187	val: 0.980474	test: 0.898415
MAE train: 0.579988	val: 0.785553	test: 0.718668

Epoch: 25
Loss: 0.6939776156629834
RMSE train: 0.720021	val: 1.013347	test: 0.972980
MAE train: 0.564212	val: 0.807666	test: 0.790256

Epoch: 26
Loss: 0.6712584240095956
RMSE train: 0.707261	val: 0.971633	test: 0.902245
MAE train: 0.552757	val: 0.777427	test: 0.725089

Epoch: 27
Loss: 0.6190220287867955
RMSE train: 0.699529	val: 0.946439	test: 0.882298
MAE train: 0.548583	val: 0.759614	test: 0.703026

Epoch: 28
Loss: 0.6454291343688965
RMSE train: 0.684555	val: 0.988074	test: 0.933560
MAE train: 0.538964	val: 0.797155	test: 0.754930

Epoch: 29
Loss: 0.5892526251929147
RMSE train: 0.680781	val: 0.994090	test: 0.970342
MAE train: 0.534023	val: 0.804327	test: 0.780421

Epoch: 30
Loss: 0.6097193786076137
RMSE train: 0.669270	val: 1.002046	test: 0.948427
MAE train: 0.523176	val: 0.801257	test: 0.766958

Epoch: 31
Loss: 0.6208840480872563
RMSE train: 0.680602	val: 0.958880	test: 0.906308
MAE train: 0.532893	val: 0.775217	test: 0.718689

Epoch: 32
Loss: 0.5734693322862897
RMSE train: 0.647729	val: 0.980665	test: 0.921842
MAE train: 0.503371	val: 0.784846	test: 0.738314

Epoch: 33
Loss: 0.5852884650230408
RMSE train: 0.624189	val: 0.960976	test: 0.918292
MAE train: 0.489076	val: 0.768274	test: 0.728510

Epoch: 34
Loss: 0.5766061842441559
RMSE train: 0.677849	val: 0.995490	test: 0.919027
MAE train: 0.538472	val: 0.799494	test: 0.744349

Epoch: 35
Loss: 0.5504013150930405
RMSE train: 0.630615	val: 0.994949	test: 0.928102
MAE train: 0.494462	val: 0.806207	test: 0.745551

Epoch: 36
Loss: 0.5091837282691684
RMSE train: 0.638342	val: 0.979359	test: 0.909939
MAE train: 0.499326	val: 0.782412	test: 0.728171

Epoch: 37
Loss: 0.515288274203028
RMSE train: 0.617475	val: 0.948023	test: 0.883731
MAE train: 0.482393	val: 0.750023	test: 0.707397

Epoch: 38
Loss: 0.4958471932581493
RMSE train: 0.621141	val: 0.992312	test: 0.970611
MAE train: 0.488418	val: 0.804056	test: 0.775061

Epoch: 39
Loss: 0.506211097751345
RMSE train: 0.592479	val: 0.958348	test: 0.894804
MAE train: 0.466794	val: 0.774170	test: 0.718848

Epoch: 40
Loss: 0.49140751574720654
RMSE train: 0.580116	val: 0.984364	test: 0.927049
MAE train: 0.456987	val: 0.793744	test: 0.735684

Epoch: 41
Loss: 0.5020747780799866
RMSE train: 0.581618	val: 0.982644	test: 0.922002
MAE train: 0.458596	val: 0.795914	test: 0.735567

Epoch: 42
Loss: 0.4741523861885071
RMSE train: 0.580920	val: 0.964967	test: 0.915962
MAE train: 0.455009	val: 0.768312	test: 0.721684

Epoch: 43
Loss: 0.4222274912255151
RMSE train: 0.538935	val: 0.977047	test: 0.906314
MAE train: 0.427648	val: 0.779571	test: 0.723555

Epoch: 44
Loss: 0.45015463658741545
RMSE train: 0.518024	val: 0.994373	test: 0.926734
MAE train: 0.407328	val: 0.789855	test: 0.742593

Epoch: 45
Loss: 0.42903561677251545
RMSE train: 0.567457	val: 0.979099	test: 0.899573
MAE train: 0.451152	val: 0.789209	test: 0.720018

Epoch: 46
Loss: 0.41456780050482067
RMSE train: 0.605591	val: 0.970964	test: 0.897029
MAE train: 0.478140	val: 0.778650	test: 0.723220

Epoch: 47
Loss: 0.41064723261765074
RMSE train: 0.530986	val: 0.949283	test: 0.883667
MAE train: 0.421360	val: 0.763708	test: 0.706432

Epoch: 48
Loss: 0.3805483856371471
RMSE train: 0.503213	val: 0.957801	test: 0.888801
MAE train: 0.398288	val: 0.772247	test: 0.715122

Epoch: 49
Loss: 0.373399202312742
RMSE train: 0.506897	val: 0.976397	test: 0.924558
MAE train: 0.398240	val: 0.784389	test: 0.740782

Epoch: 50
Loss: 0.3670293816498348
RMSE train: 0.472162	val: 0.945850	test: 0.907118
MAE train: 0.372556	val: 0.760853	test: 0.720147

Epoch: 51
Loss: 0.3860155812331608
RMSE train: 0.549521	val: 0.961664	test: 0.889934
MAE train: 0.436686	val: 0.772836	test: 0.718198

Epoch: 52
Loss: 0.3617659956216812
RMSE train: 0.490052	val: 0.967572	test: 0.914846
MAE train: 0.387252	val: 0.787312	test: 0.733127

Epoch: 53
Loss: 0.3469098891530718
RMSE train: 0.448526	val: 0.944072	test: 0.887780
MAE train: 0.353516	val: 0.753275	test: 0.709806

Epoch: 54
Loss: 0.32417114078998566
RMSE train: 0.471784	val: 0.930424	test: 0.874412
MAE train: 0.375869	val: 0.750627	test: 0.701318

Epoch: 55
Loss: 0.3550738160099302
RMSE train: 0.439681	val: 0.942238	test: 0.879907
MAE train: 0.347194	val: 0.754003	test: 0.706051

Epoch: 56
Loss: 0.35081430843898226
RMSE train: 0.475661	val: 0.953523	test: 0.900335
MAE train: 0.377848	val: 0.760002	test: 0.718316

Epoch: 57
Loss: 0.3460340329578945
RMSE train: 0.582973	val: 0.945062	test: 0.888518
MAE train: 0.465471	val: 0.764923	test: 0.714738

Epoch: 58
Loss: 0.3340239099093846
RMSE train: 0.450781	val: 0.891976	test: 0.859164
MAE train: 0.356264	val: 0.708681	test: 0.680500

Epoch: 59
Loss: 0.3234149558203561
RMSE train: 0.421969	val: 0.931206	test: 0.857176
MAE train: 0.330726	val: 0.748445	test: 0.692570

Epoch: 60
Loss: 0.305365726351738
RMSE train: 0.429598	val: 0.931433	test: 0.877555
MAE train: 0.339279	val: 0.744691	test: 0.698521

Epoch: 61
Loss: 0.2975145344223295
RMSE train: 0.470340	val: 0.939856	test: 0.886652
MAE train: 0.366651	val: 0.760758	test: 0.709574

Epoch: 62
Loss: 0.30021912285259794
RMSE train: 0.468644	val: 0.955978	test: 0.910291
MAE train: 0.375219	val: 0.763203	test: 0.735208

Epoch: 63
Loss: 0.29663608968257904
RMSE train: 0.430861	val: 0.939858	test: 0.884865
MAE train: 0.338186	val: 0.759486	test: 0.710410

Epoch: 64
Loss: 0.29968186574322836
RMSE train: 0.403861	val: 0.923859	test: 0.878904
MAE train: 0.317534	val: 0.737649	test: 0.705731

Epoch: 65
Loss: 0.28156901257378714
RMSE train: 0.400924	val: 0.956732	test: 0.896228
MAE train: 0.316154	val: 0.769813	test: 0.732139

Epoch: 66
Loss: 0.2765991591981479
RMSE train: 0.375854	val: 0.922936	test: 0.869554
MAE train: 0.296345	val: 0.724457	test: 0.692178

Epoch: 67
Loss: 0.285501523741654
RMSE train: 0.457665	val: 0.952953	test: 0.880794
MAE train: 0.362341	val: 0.760924	test: 0.701305

Epoch: 68
Loss: 0.2696201450058392
RMSE train: 0.444485	val: 0.934726	test: 0.854251
MAE train: 0.351663	val: 0.746695	test: 0.679490

Epoch: 69
Loss: 0.2548648289271763
RMSE train: 0.411895	val: 0.929764	test: 0.855932
MAE train: 0.328636	val: 0.741325	test: 0.680804

Epoch: 70
Loss: 0.25430083594151903
RMSE train: 0.464522	val: 0.964992	test: 0.869431
MAE train: 0.372024	val: 0.763620	test: 0.687861

Epoch: 71
Loss: 0.2357115394302777
RMSE train: 0.345923	val: 0.922724	test: 0.865872
MAE train: 0.271254	val: 0.725980	test: 0.689563

Epoch: 72
Loss: 0.24359099992683955
RMSE train: 0.384887	val: 0.938011	test: 0.881996
MAE train: 0.305217	val: 0.755407	test: 0.712698

Epoch: 73
Loss: 0.23896506535155432
RMSE train: 0.402164	val: 0.928882	test: 0.878416
MAE train: 0.320280	val: 0.739550	test: 0.702249

Epoch: 74
Loss: 0.24857655486890248
RMSE train: 0.365510	val: 0.953580	test: 0.885118
MAE train: 0.290305	val: 0.749036	test: 0.700019

Epoch: 75
Loss: 0.2469568316425596
RMSE train: 0.354357	val: 0.927462	test: 0.893290
MAE train: 0.281409	val: 0.750554	test: 0.717647

Epoch: 76
Loss: 0.23493281219686782
RMSE train: 0.416008	val: 0.992256	test: 0.908094
MAE train: 0.333143	val: 0.784952	test: 0.718501

Epoch: 77
Loss: 0.2251712754368782
RMSE train: 0.409345	val: 0.928894	test: 0.879316
MAE train: 0.323477	val: 0.754287	test: 0.704391

Epoch: 78
Loss: 0.21434603844370162
RMSE train: 0.328444	val: 0.904563	test: 0.845929
MAE train: 0.259398	val: 0.716047	test: 0.673262

Epoch: 79
Loss: 0.21784724720886775
RMSE train: 0.412421	val: 0.953540	test: 0.901208
MAE train: 0.330821	val: 0.749209	test: 0.709155

Epoch: 80
Loss: 0.21409325833831513
RMSE train: 0.325691	val: 0.914633	test: 0.865907
MAE train: 0.256064	val: 0.731989	test: 0.695600

Epoch: 81
Loss: 0.21311360065426146
RMSE train: 0.347020	val: 0.940296	test: 0.890858
MAE train: 0.275338	val: 0.740327	test: 0.701980

Epoch: 82
Loss: 0.2029812155025346
RMSE train: 0.373982	val: 0.927183	test: 0.874485

Epoch: 22
Loss: 0.7180619069508144
RMSE train: 0.755664	val: 1.064289	test: 0.967053
MAE train: 0.603456	val: 0.830068	test: 0.766594

Epoch: 23
Loss: 0.7100515365600586
RMSE train: 0.729603	val: 1.119853	test: 1.021744
MAE train: 0.576732	val: 0.875735	test: 0.817735

Epoch: 24
Loss: 0.6901312853608813
RMSE train: 0.729728	val: 1.043672	test: 0.939361
MAE train: 0.580414	val: 0.824647	test: 0.748520

Epoch: 25
Loss: 0.6456039249897003
RMSE train: 0.710206	val: 0.983337	test: 0.889338
MAE train: 0.568980	val: 0.780091	test: 0.711484

Epoch: 26
Loss: 0.6395512308393206
RMSE train: 0.708044	val: 1.023287	test: 0.925978
MAE train: 0.562715	val: 0.801762	test: 0.739274

Epoch: 27
Loss: 0.6208463524069104
RMSE train: 0.713946	val: 1.013551	test: 0.935427
MAE train: 0.572466	val: 0.798493	test: 0.744904

Epoch: 28
Loss: 0.6041030415466854
RMSE train: 0.680657	val: 1.095338	test: 0.991610
MAE train: 0.540951	val: 0.852939	test: 0.784020

Epoch: 29
Loss: 0.5913846237318856
RMSE train: 0.689540	val: 1.032442	test: 0.945174
MAE train: 0.552183	val: 0.805291	test: 0.758467

Epoch: 30
Loss: 0.6024789895330157
RMSE train: 0.683834	val: 1.000288	test: 0.908955
MAE train: 0.545044	val: 0.791496	test: 0.731577

Epoch: 31
Loss: 0.5760735252073833
RMSE train: 0.696655	val: 1.057260	test: 0.971735
MAE train: 0.555992	val: 0.820501	test: 0.769893

Epoch: 32
Loss: 0.580812360559191
RMSE train: 0.670458	val: 1.130270	test: 1.029746
MAE train: 0.528949	val: 0.878159	test: 0.817194

Epoch: 33
Loss: 0.5451761718307223
RMSE train: 0.637885	val: 1.053063	test: 0.970075
MAE train: 0.507329	val: 0.818940	test: 0.765951

Epoch: 34
Loss: 0.5402062301124845
RMSE train: 0.628841	val: 0.981515	test: 0.915378
MAE train: 0.501240	val: 0.773633	test: 0.739105

Epoch: 35
Loss: 0.5366637131997517
RMSE train: 0.640956	val: 1.047252	test: 0.971320
MAE train: 0.508755	val: 0.819923	test: 0.769923

Epoch: 36
Loss: 0.5342341661453247
RMSE train: 0.606280	val: 1.048672	test: 0.953813
MAE train: 0.477595	val: 0.820304	test: 0.755748

Epoch: 37
Loss: 0.5021301060914993
RMSE train: 0.599659	val: 1.055975	test: 0.962730
MAE train: 0.471231	val: 0.832421	test: 0.766689

Epoch: 38
Loss: 0.4848372297627585
RMSE train: 0.580534	val: 1.004843	test: 0.927882
MAE train: 0.459807	val: 0.790052	test: 0.743233

Epoch: 39
Loss: 0.4639902732201985
RMSE train: 0.554260	val: 0.960600	test: 0.898195
MAE train: 0.440989	val: 0.753939	test: 0.718769

Epoch: 40
Loss: 0.46811532761369434
RMSE train: 0.550241	val: 0.996661	test: 0.921766
MAE train: 0.433844	val: 0.778123	test: 0.730866

Epoch: 41
Loss: 0.44759190721171244
RMSE train: 0.551713	val: 1.106000	test: 1.010126
MAE train: 0.436500	val: 0.871956	test: 0.816586

Epoch: 42
Loss: 0.4393740402800696
RMSE train: 0.537128	val: 1.010064	test: 0.935733
MAE train: 0.424402	val: 0.781562	test: 0.744159

Epoch: 43
Loss: 0.4014029737029757
RMSE train: 0.538303	val: 1.038864	test: 0.952407
MAE train: 0.428854	val: 0.810622	test: 0.755023

Epoch: 44
Loss: 0.3991000247853143
RMSE train: 0.539797	val: 1.007401	test: 0.923248
MAE train: 0.430059	val: 0.789306	test: 0.735182

Epoch: 45
Loss: 0.4103996604681015
RMSE train: 0.511364	val: 1.004706	test: 0.931733
MAE train: 0.400452	val: 0.783217	test: 0.738225

Epoch: 46
Loss: 0.39839779904910494
RMSE train: 0.532226	val: 1.160242	test: 1.082113
MAE train: 0.421234	val: 0.898596	test: 0.862084

Epoch: 47
Loss: 0.37506250398499624
RMSE train: 0.456883	val: 1.039758	test: 0.959448
MAE train: 0.358719	val: 0.807712	test: 0.761243

Epoch: 48
Loss: 0.36562170939786093
RMSE train: 0.464858	val: 1.100228	test: 1.027065
MAE train: 0.365761	val: 0.851775	test: 0.814383

Epoch: 49
Loss: 0.35845679470470976
RMSE train: 0.481453	val: 0.981269	test: 0.910170
MAE train: 0.382672	val: 0.778371	test: 0.725566

Epoch: 50
Loss: 0.3454056190592902
RMSE train: 0.466982	val: 1.137055	test: 1.091041
MAE train: 0.366956	val: 0.882175	test: 0.879144

Epoch: 51
Loss: 0.3653672145945685
RMSE train: 0.448305	val: 1.068682	test: 0.990585
MAE train: 0.355060	val: 0.827878	test: 0.784070

Epoch: 52
Loss: 0.32103427180222105
RMSE train: 0.446379	val: 1.056623	test: 0.978801
MAE train: 0.353530	val: 0.813106	test: 0.773656

Epoch: 53
Loss: 0.31264381962163107
RMSE train: 0.467238	val: 1.104722	test: 1.034094
MAE train: 0.366903	val: 0.851676	test: 0.823176

Epoch: 54
Loss: 0.30804440804890226
RMSE train: 0.501874	val: 1.217874	test: 1.138846
MAE train: 0.394747	val: 0.947506	test: 0.910558

Epoch: 55
Loss: 0.31669771671295166
RMSE train: 0.467213	val: 1.033557	test: 0.954238
MAE train: 0.369633	val: 0.801530	test: 0.757521

Epoch: 56
Loss: 0.30816095428807394
RMSE train: 0.419732	val: 1.112099	test: 1.039007
MAE train: 0.333139	val: 0.857808	test: 0.824991

Epoch: 57
Loss: 0.29788910491125925
RMSE train: 0.416570	val: 1.009135	test: 0.930169
MAE train: 0.328712	val: 0.790341	test: 0.742740

Epoch: 58
Loss: 0.2952426682625498
RMSE train: 0.457935	val: 1.119796	test: 1.049848
MAE train: 0.360164	val: 0.870899	test: 0.836686

Epoch: 59
Loss: 0.291447642658438
RMSE train: 0.439428	val: 1.074447	test: 1.003826
MAE train: 0.350261	val: 0.835905	test: 0.805068

Epoch: 60
Loss: 0.2781048150999205
RMSE train: 0.383408	val: 1.049552	test: 0.984236
MAE train: 0.300428	val: 0.812478	test: 0.785329

Epoch: 61
Loss: 0.2733461282082966
RMSE train: 0.380955	val: 1.070521	test: 1.003294
MAE train: 0.297952	val: 0.827519	test: 0.796435

Epoch: 62
Loss: 0.2926201596856117
RMSE train: 0.433681	val: 1.082031	test: 1.010616
MAE train: 0.343461	val: 0.836108	test: 0.800468

Epoch: 63
Loss: 0.27741752032722744
RMSE train: 0.394589	val: 1.173569	test: 1.085698
MAE train: 0.311164	val: 0.912061	test: 0.867356

Epoch: 64
Loss: 0.28109737379210337
RMSE train: 0.391917	val: 1.006081	test: 0.930740
MAE train: 0.311887	val: 0.785794	test: 0.728007

Epoch: 65
Loss: 0.25962644602571217
RMSE train: 0.399720	val: 1.064913	test: 0.993527
MAE train: 0.315540	val: 0.824107	test: 0.784727

Epoch: 66
Loss: 0.25621859303542543
RMSE train: 0.393494	val: 1.132551	test: 1.058424
MAE train: 0.309163	val: 0.877854	test: 0.842910

Epoch: 67
Loss: 0.26804024406841825
RMSE train: 0.371556	val: 1.041567	test: 0.962007
MAE train: 0.292281	val: 0.801809	test: 0.763201

Epoch: 68
Loss: 0.24443403099264419
RMSE train: 0.365203	val: 1.093001	test: 1.012267
MAE train: 0.289551	val: 0.849396	test: 0.806289

Epoch: 69
Loss: 0.23100773138659342
RMSE train: 0.435755	val: 1.266104	test: 1.178433
MAE train: 0.343792	val: 0.991949	test: 0.952673

Epoch: 70
Loss: 0.22283832622425898
RMSE train: 0.332797	val: 1.049510	test: 0.988605
MAE train: 0.262068	val: 0.810724	test: 0.788807

Epoch: 71
Loss: 0.2455209270119667
RMSE train: 0.386555	val: 1.087052	test: 1.006005
MAE train: 0.302630	val: 0.839295	test: 0.797732

Epoch: 72
Loss: 0.22564255339758738
RMSE train: 0.357159	val: 1.131770	test: 1.049878
MAE train: 0.282555	val: 0.878737	test: 0.833436

Epoch: 73
Loss: 0.2300823171223913
RMSE train: 0.354725	val: 1.048230	test: 0.968401
MAE train: 0.282377	val: 0.818012	test: 0.771976

Epoch: 74
Loss: 0.212890302496297
RMSE train: 0.362899	val: 1.161249	test: 1.085790
MAE train: 0.286318	val: 0.900405	test: 0.870259

Epoch: 75
Loss: 0.20923494707260812
RMSE train: 0.331945	val: 1.060219	test: 0.973503
MAE train: 0.264545	val: 0.821553	test: 0.774190

Epoch: 76
Loss: 0.2032714592558997
RMSE train: 0.351441	val: 1.178234	test: 1.096338
MAE train: 0.275257	val: 0.915846	test: 0.882150

Epoch: 77
Loss: 0.1953847035765648
RMSE train: 0.333506	val: 1.073669	test: 1.009383
MAE train: 0.261534	val: 0.830557	test: 0.802445

Epoch: 78
Loss: 0.19999969324895314
RMSE train: 0.351931	val: 1.049354	test: 0.999102
MAE train: 0.282935	val: 0.803581	test: 0.792635

Epoch: 79
Loss: 0.19283798656293324
RMSE train: 0.354211	val: 1.075758	test: 1.011377
MAE train: 0.279318	val: 0.833846	test: 0.806516

Epoch: 80
Loss: 0.19579726138285228
RMSE train: 0.353492	val: 1.298290	test: 1.208833
MAE train: 0.282515	val: 1.025480	test: 0.980662

Epoch: 81
Loss: 0.18836091033049993
RMSE train: 0.323478	val: 0.991847	test: 0.932692
MAE train: 0.256238	val: 0.769314	test: 0.745170

Epoch: 82
Loss: 0.17016483311142241
RMSE train: 0.388924	val: 1.112052	test: 1.034791

Epoch: 22
Loss: 0.6214887925556728
RMSE train: 0.690669	val: 0.919378	test: 0.845628
MAE train: 0.545463	val: 0.738071	test: 0.676481

Epoch: 23
Loss: 0.6190337112971714
RMSE train: 0.681444	val: 0.905069	test: 0.854496
MAE train: 0.535400	val: 0.720528	test: 0.673733

Epoch: 24
Loss: 0.6208138593605587
RMSE train: 0.694599	val: 0.952265	test: 0.869770
MAE train: 0.547301	val: 0.756924	test: 0.689654

Epoch: 25
Loss: 0.5841708651610783
RMSE train: 0.655496	val: 0.897865	test: 0.842119
MAE train: 0.514190	val: 0.719905	test: 0.659788

Epoch: 26
Loss: 0.5782811726842608
RMSE train: 0.657525	val: 0.944410	test: 0.878236
MAE train: 0.519267	val: 0.744937	test: 0.695766

Epoch: 27
Loss: 0.570184235061918
RMSE train: 0.644771	val: 0.897774	test: 0.841990
MAE train: 0.509347	val: 0.718412	test: 0.671804

Epoch: 28
Loss: 0.5550614084516253
RMSE train: 0.623650	val: 0.874171	test: 0.832292
MAE train: 0.492458	val: 0.695949	test: 0.651564

Epoch: 29
Loss: 0.517661030803408
RMSE train: 0.627077	val: 0.896018	test: 0.836914
MAE train: 0.495863	val: 0.707047	test: 0.668190

Epoch: 30
Loss: 0.518046817609242
RMSE train: 0.677462	val: 0.895077	test: 0.849860
MAE train: 0.540360	val: 0.711755	test: 0.680187

Epoch: 31
Loss: 0.523815278496061
RMSE train: 0.676187	val: 0.959044	test: 0.898076
MAE train: 0.534854	val: 0.759706	test: 0.705289

Epoch: 32
Loss: 0.5232585817575455
RMSE train: 0.642142	val: 0.905658	test: 0.852907
MAE train: 0.505232	val: 0.721232	test: 0.672181

Epoch: 33
Loss: 0.5146414360829762
RMSE train: 0.613019	val: 0.951408	test: 0.899966
MAE train: 0.483443	val: 0.743471	test: 0.718262

Epoch: 34
Loss: 0.49396558105945587
RMSE train: 0.594843	val: 0.891668	test: 0.839761
MAE train: 0.467903	val: 0.706978	test: 0.664894

Epoch: 35
Loss: 0.49503657860415323
RMSE train: 0.610002	val: 0.907990	test: 0.850188
MAE train: 0.482373	val: 0.716721	test: 0.671449

Epoch: 36
Loss: 0.4930842050484249
RMSE train: 0.626732	val: 0.961795	test: 0.881360
MAE train: 0.495421	val: 0.765516	test: 0.691281

Epoch: 37
Loss: 0.4883014474596296
RMSE train: 0.617700	val: 0.935715	test: 0.860722
MAE train: 0.481313	val: 0.739535	test: 0.672254

Epoch: 38
Loss: 0.46759811469486784
RMSE train: 0.675685	val: 1.023482	test: 0.954623
MAE train: 0.538656	val: 0.806159	test: 0.757387

Epoch: 39
Loss: 0.4633495169026511
RMSE train: 0.605256	val: 0.861627	test: 0.835951
MAE train: 0.482279	val: 0.685346	test: 0.663089

Epoch: 40
Loss: 0.4562995859554836
RMSE train: 0.561934	val: 0.916411	test: 0.858386
MAE train: 0.442231	val: 0.722866	test: 0.684061

Epoch: 41
Loss: 0.44854288016046795
RMSE train: 0.548325	val: 0.882198	test: 0.812272
MAE train: 0.436094	val: 0.700213	test: 0.645629

Epoch: 42
Loss: 0.41160627773829866
RMSE train: 0.542060	val: 0.914053	test: 0.870675
MAE train: 0.428961	val: 0.715684	test: 0.687300

Epoch: 43
Loss: 0.4089710201535906
RMSE train: 0.548464	val: 0.919847	test: 0.861296
MAE train: 0.434080	val: 0.731513	test: 0.678127

Epoch: 44
Loss: 0.39042313396930695
RMSE train: 0.545365	val: 0.951369	test: 0.874422
MAE train: 0.430574	val: 0.747291	test: 0.694484

Epoch: 45
Loss: 0.4063555576971599
RMSE train: 0.554340	val: 0.863745	test: 0.820120
MAE train: 0.438922	val: 0.686625	test: 0.650922

Epoch: 46
Loss: 0.394308260508946
RMSE train: 0.585911	val: 0.886106	test: 0.840143
MAE train: 0.467937	val: 0.710254	test: 0.667695

Epoch: 47
Loss: 0.3904628838811602
RMSE train: 0.499619	val: 0.910254	test: 0.843726
MAE train: 0.394353	val: 0.712719	test: 0.670279

Epoch: 48
Loss: 0.37254683460508076
RMSE train: 0.596018	val: 0.867996	test: 0.827745
MAE train: 0.480771	val: 0.694544	test: 0.653562

Epoch: 49
Loss: 0.37554780713149477
RMSE train: 0.489383	val: 0.866053	test: 0.817304
MAE train: 0.386989	val: 0.691227	test: 0.654293

Epoch: 50
Loss: 0.3424042207854135
RMSE train: 0.509970	val: 0.927428	test: 0.865635
MAE train: 0.399466	val: 0.724758	test: 0.693004

Epoch: 51
Loss: 0.3681645840406418
RMSE train: 0.522608	val: 0.941959	test: 0.865875
MAE train: 0.410963	val: 0.744679	test: 0.680855

Epoch: 52
Loss: 0.343530861394746
RMSE train: 0.501357	val: 0.919058	test: 0.847170
MAE train: 0.396680	val: 0.721549	test: 0.671779

Epoch: 53
Loss: 0.3489744471652167
RMSE train: 0.566706	val: 1.036427	test: 0.950335
MAE train: 0.451417	val: 0.813193	test: 0.758521

Epoch: 54
Loss: 0.35572834951536997
RMSE train: 0.520821	val: 0.986222	test: 0.917221
MAE train: 0.414375	val: 0.766301	test: 0.735345

Epoch: 55
Loss: 0.32797578828675406
RMSE train: 0.507823	val: 0.976240	test: 0.886912
MAE train: 0.399911	val: 0.766396	test: 0.705558

Epoch: 56
Loss: 0.33310085960796904
RMSE train: 0.481239	val: 0.919112	test: 0.837677
MAE train: 0.379937	val: 0.727563	test: 0.661923

Epoch: 57
Loss: 0.32086867306913647
RMSE train: 0.462602	val: 0.940183	test: 0.860597
MAE train: 0.365257	val: 0.737604	test: 0.691879

Epoch: 58
Loss: 0.31018385929720743
RMSE train: 0.454120	val: 0.911230	test: 0.837281
MAE train: 0.357926	val: 0.723548	test: 0.657029

Epoch: 59
Loss: 0.30839100905827116
RMSE train: 0.525327	val: 1.020265	test: 0.937539
MAE train: 0.420984	val: 0.803680	test: 0.746085

Epoch: 60
Loss: 0.2995581179857254
RMSE train: 0.448396	val: 0.916552	test: 0.847825
MAE train: 0.353354	val: 0.718266	test: 0.666612

Epoch: 61
Loss: 0.297840704875333
RMSE train: 0.451380	val: 0.904417	test: 0.855932
MAE train: 0.356774	val: 0.710749	test: 0.682206

Epoch: 62
Loss: 0.31076422333717346
RMSE train: 0.424061	val: 0.927053	test: 0.863791
MAE train: 0.334413	val: 0.733472	test: 0.685297

Epoch: 63
Loss: 0.3063209376164845
RMSE train: 0.454632	val: 0.896500	test: 0.821752
MAE train: 0.359994	val: 0.712020	test: 0.657669

Epoch: 64
Loss: 0.3000322622912271
RMSE train: 0.425646	val: 0.888551	test: 0.826214
MAE train: 0.335290	val: 0.703277	test: 0.655261

Epoch: 65
Loss: 0.2949479584183012
RMSE train: 0.448840	val: 0.862989	test: 0.813787
MAE train: 0.357487	val: 0.689512	test: 0.638771

Epoch: 66
Loss: 0.2952349728771618
RMSE train: 0.472230	val: 0.973372	test: 0.886199
MAE train: 0.376158	val: 0.759741	test: 0.696962

Epoch: 67
Loss: 0.28041232909475056
RMSE train: 0.463776	val: 0.978434	test: 0.891716
MAE train: 0.367792	val: 0.768180	test: 0.708700

Epoch: 68
Loss: 0.27157760305064066
RMSE train: 0.417854	val: 0.948239	test: 0.859908
MAE train: 0.329820	val: 0.741885	test: 0.685778

Epoch: 69
Loss: 0.2616424613765308
RMSE train: 0.394099	val: 0.893685	test: 0.831895
MAE train: 0.310432	val: 0.698985	test: 0.655455

Epoch: 70
Loss: 0.2686959792460714
RMSE train: 0.428846	val: 0.870455	test: 0.821642
MAE train: 0.340215	val: 0.686196	test: 0.657281

Epoch: 71
Loss: 0.2775359185678618
RMSE train: 0.489357	val: 0.861695	test: 0.829798
MAE train: 0.390668	val: 0.686208	test: 0.661452

Epoch: 72
Loss: 0.2736954710313252
RMSE train: 0.405102	val: 0.883587	test: 0.828210
MAE train: 0.319282	val: 0.693823	test: 0.655424

Epoch: 73
Loss: 0.2610126393181937
RMSE train: 0.423755	val: 0.890452	test: 0.825888
MAE train: 0.337953	val: 0.701175	test: 0.660621

Epoch: 74
Loss: 0.2417723195893424
RMSE train: 0.373184	val: 0.903091	test: 0.826329
MAE train: 0.295090	val: 0.715357	test: 0.663541

Epoch: 75
Loss: 0.23314975202083588
RMSE train: 0.451023	val: 0.995634	test: 0.900218
MAE train: 0.362353	val: 0.774493	test: 0.718817

Epoch: 76
Loss: 0.22257049913917268
RMSE train: 0.365464	val: 0.896802	test: 0.824215
MAE train: 0.287821	val: 0.710497	test: 0.653186

Epoch: 77
Loss: 0.22718493321112224
RMSE train: 0.367965	val: 0.912149	test: 0.834172
MAE train: 0.291009	val: 0.713494	test: 0.659760

Epoch: 78
Loss: 0.24172397702932358
RMSE train: 0.367397	val: 0.891052	test: 0.833665
MAE train: 0.292762	val: 0.699293	test: 0.664459

Epoch: 79
Loss: 0.23584587233407156
RMSE train: 0.374015	val: 0.888509	test: 0.827291
MAE train: 0.295760	val: 0.700499	test: 0.668083

Epoch: 80
Loss: 0.23158756217786244
RMSE train: 0.353148	val: 0.878233	test: 0.812357
MAE train: 0.280421	val: 0.693372	test: 0.645368

Epoch: 81
Loss: 0.23920315078326634
RMSE train: 0.432552	val: 0.888632	test: 0.836177
MAE train: 0.345291	val: 0.703156	test: 0.666121

Epoch: 82
Loss: 0.21657088292496546
RMSE train: 0.520355	val: 1.045464	test: 0.959513

Epoch: 22
Loss: 0.6445213982037136
RMSE train: 0.719085	val: 0.897164	test: 0.876581
MAE train: 0.563054	val: 0.712586	test: 0.706070

Epoch: 23
Loss: 0.6168890723160335
RMSE train: 0.684783	val: 0.884205	test: 0.847466
MAE train: 0.533075	val: 0.695385	test: 0.672800

Epoch: 24
Loss: 0.6171510815620422
RMSE train: 0.698380	val: 0.916047	test: 0.874678
MAE train: 0.540874	val: 0.724667	test: 0.699713

Epoch: 25
Loss: 0.5936165452003479
RMSE train: 0.669524	val: 0.905986	test: 0.856073
MAE train: 0.520255	val: 0.713807	test: 0.684954

Epoch: 26
Loss: 0.5874207126242774
RMSE train: 0.646637	val: 0.888431	test: 0.838663
MAE train: 0.502127	val: 0.698600	test: 0.664447

Epoch: 27
Loss: 0.5815424237932477
RMSE train: 0.669549	val: 0.878470	test: 0.838158
MAE train: 0.525150	val: 0.689188	test: 0.675208

Epoch: 28
Loss: 0.5932952272040504
RMSE train: 0.648848	val: 0.883139	test: 0.839845
MAE train: 0.512464	val: 0.692687	test: 0.672480

Epoch: 29
Loss: 0.5514468167509351
RMSE train: 0.647899	val: 0.872454	test: 0.848420
MAE train: 0.509056	val: 0.690915	test: 0.683756

Epoch: 30
Loss: 0.5641037481171745
RMSE train: 0.618374	val: 0.881108	test: 0.857315
MAE train: 0.480348	val: 0.692948	test: 0.680824

Epoch: 31
Loss: 0.547316238284111
RMSE train: 0.622878	val: 0.874102	test: 0.848756
MAE train: 0.487163	val: 0.688821	test: 0.674017

Epoch: 32
Loss: 0.5225630509001868
RMSE train: 0.614643	val: 0.872516	test: 0.840379
MAE train: 0.480091	val: 0.681489	test: 0.661648

Epoch: 33
Loss: 0.55922389456204
RMSE train: 0.639687	val: 0.876471	test: 0.837558
MAE train: 0.497215	val: 0.684215	test: 0.672520

Epoch: 34
Loss: 0.5439310563462121
RMSE train: 0.683755	val: 0.908304	test: 0.855776
MAE train: 0.533386	val: 0.714531	test: 0.686229

Epoch: 35
Loss: 0.5088749208620617
RMSE train: 0.620979	val: 0.887645	test: 0.866635
MAE train: 0.489362	val: 0.705619	test: 0.699069

Epoch: 36
Loss: 0.49054038311753956
RMSE train: 0.605520	val: 0.867241	test: 0.841169
MAE train: 0.469832	val: 0.679911	test: 0.673346

Epoch: 37
Loss: 0.48882132130009787
RMSE train: 0.595650	val: 0.897910	test: 0.864640
MAE train: 0.464716	val: 0.718975	test: 0.692216

Epoch: 38
Loss: 0.4903837399823325
RMSE train: 0.578187	val: 0.871285	test: 0.842582
MAE train: 0.452322	val: 0.679620	test: 0.677733

Epoch: 39
Loss: 0.48354435179914745
RMSE train: 0.578748	val: 0.882518	test: 0.853080
MAE train: 0.451407	val: 0.694453	test: 0.685887

Epoch: 40
Loss: 0.4736796149185726
RMSE train: 0.572506	val: 0.872836	test: 0.858638
MAE train: 0.449731	val: 0.683039	test: 0.694472

Epoch: 41
Loss: 0.4769300009523119
RMSE train: 0.556017	val: 0.862984	test: 0.834180
MAE train: 0.433794	val: 0.673983	test: 0.671855

Epoch: 42
Loss: 0.4635594593627112
RMSE train: 0.627174	val: 0.917862	test: 0.861799
MAE train: 0.491050	val: 0.735232	test: 0.679924

Epoch: 43
Loss: 0.43573037002767834
RMSE train: 0.541635	val: 0.905552	test: 0.858701
MAE train: 0.424589	val: 0.709203	test: 0.686700

Epoch: 44
Loss: 0.4461612786565508
RMSE train: 0.564236	val: 0.877955	test: 0.848888
MAE train: 0.441706	val: 0.694226	test: 0.680453

Epoch: 45
Loss: 0.40732792019844055
RMSE train: 0.588587	val: 0.928891	test: 0.885469
MAE train: 0.460761	val: 0.730482	test: 0.697566

Epoch: 46
Loss: 0.4277626020567758
RMSE train: 0.548726	val: 0.884485	test: 0.849244
MAE train: 0.430131	val: 0.706721	test: 0.671846

Epoch: 47
Loss: 0.40850930341652464
RMSE train: 0.521204	val: 0.887822	test: 0.858748
MAE train: 0.406349	val: 0.697731	test: 0.684745

Epoch: 48
Loss: 0.3997209817171097
RMSE train: 0.516035	val: 0.882266	test: 0.856612
MAE train: 0.403769	val: 0.708089	test: 0.687721

Epoch: 49
Loss: 0.372651212981769
RMSE train: 0.518591	val: 0.867217	test: 0.829681
MAE train: 0.404397	val: 0.695703	test: 0.662089

Epoch: 50
Loss: 0.37800518955503193
RMSE train: 0.496102	val: 0.877360	test: 0.841250
MAE train: 0.387680	val: 0.691885	test: 0.669660

Epoch: 51
Loss: 0.36112541386059355
RMSE train: 0.501361	val: 0.861825	test: 0.845086
MAE train: 0.391810	val: 0.681632	test: 0.673864

Epoch: 52
Loss: 0.36969248737607685
RMSE train: 0.493264	val: 0.880195	test: 0.842569
MAE train: 0.385770	val: 0.696052	test: 0.673307

Epoch: 53
Loss: 0.34317208400794436
RMSE train: 0.472752	val: 0.863998	test: 0.849881
MAE train: 0.368951	val: 0.686854	test: 0.677823

Epoch: 54
Loss: 0.35193480125495363
RMSE train: 0.525793	val: 0.858559	test: 0.844547
MAE train: 0.414432	val: 0.678150	test: 0.683589

Epoch: 55
Loss: 0.34798331345830646
RMSE train: 0.518311	val: 0.924802	test: 0.907420
MAE train: 0.406148	val: 0.727001	test: 0.735245

Epoch: 56
Loss: 0.3461237975529262
RMSE train: 0.465087	val: 0.852699	test: 0.849778
MAE train: 0.365189	val: 0.672311	test: 0.683861

Epoch: 57
Loss: 0.35377988645008634
RMSE train: 0.504065	val: 0.866872	test: 0.843619
MAE train: 0.397507	val: 0.684892	test: 0.683407

Epoch: 58
Loss: 0.3281093899692808
RMSE train: 0.467152	val: 0.858290	test: 0.822551
MAE train: 0.367708	val: 0.675841	test: 0.663222

Epoch: 59
Loss: 0.32799346106392996
RMSE train: 0.448839	val: 0.858128	test: 0.826758
MAE train: 0.351276	val: 0.683693	test: 0.652826

Epoch: 60
Loss: 0.3178374320268631
RMSE train: 0.458586	val: 0.869532	test: 0.844538
MAE train: 0.361122	val: 0.685795	test: 0.678711

Epoch: 61
Loss: 0.2995190428836005
RMSE train: 0.447075	val: 0.869931	test: 0.838026
MAE train: 0.352432	val: 0.690182	test: 0.669021

Epoch: 62
Loss: 0.29027564717190607
RMSE train: 0.493923	val: 0.883849	test: 0.845316
MAE train: 0.391201	val: 0.693861	test: 0.690458

Epoch: 63
Loss: 0.2933667206338474
RMSE train: 0.469967	val: 0.858469	test: 0.837830
MAE train: 0.370960	val: 0.667834	test: 0.677281

Epoch: 64
Loss: 0.3017973005771637
RMSE train: 0.433074	val: 0.871467	test: 0.849821
MAE train: 0.341531	val: 0.698040	test: 0.679018

Epoch: 65
Loss: 0.29201408475637436
RMSE train: 0.456064	val: 0.850097	test: 0.832323
MAE train: 0.361299	val: 0.673103	test: 0.670599

Epoch: 66
Loss: 0.29382315278053284
RMSE train: 0.450444	val: 0.889771	test: 0.870314
MAE train: 0.353465	val: 0.697767	test: 0.699073

Epoch: 67
Loss: 0.2927395775914192
RMSE train: 0.403650	val: 0.884515	test: 0.860836
MAE train: 0.319502	val: 0.694445	test: 0.688093

Epoch: 68
Loss: 0.2775818535259792
RMSE train: 0.424060	val: 0.872094	test: 0.859219
MAE train: 0.334612	val: 0.689442	test: 0.690296

Epoch: 69
Loss: 0.26222981831857134
RMSE train: 0.410711	val: 0.847594	test: 0.817495
MAE train: 0.323086	val: 0.675521	test: 0.653047

Epoch: 70
Loss: 0.25702274058546337
RMSE train: 0.415781	val: 0.869365	test: 0.845279
MAE train: 0.330735	val: 0.681808	test: 0.682600

Epoch: 71
Loss: 0.26314504763909746
RMSE train: 0.404238	val: 0.880403	test: 0.848066
MAE train: 0.320334	val: 0.691905	test: 0.687661

Epoch: 72
Loss: 0.2512578730072294
RMSE train: 0.429432	val: 0.903088	test: 0.873274
MAE train: 0.339938	val: 0.711078	test: 0.707551

Epoch: 73
Loss: 0.2509755364486149
RMSE train: 0.384445	val: 0.855062	test: 0.834421
MAE train: 0.302782	val: 0.674998	test: 0.668604

Epoch: 74
Loss: 0.2682459939803396
RMSE train: 0.389553	val: 0.908178	test: 0.879431
MAE train: 0.308436	val: 0.718873	test: 0.693637

Epoch: 75
Loss: 0.25857044969286236
RMSE train: 0.410929	val: 0.896334	test: 0.862480
MAE train: 0.323076	val: 0.705887	test: 0.691726

Epoch: 76
Loss: 0.25157202567373005
RMSE train: 0.433913	val: 0.911065	test: 0.876848
MAE train: 0.347580	val: 0.715394	test: 0.712821

Epoch: 77
Loss: 0.24716394820383616
RMSE train: 0.414922	val: 0.849050	test: 0.811049
MAE train: 0.333326	val: 0.673876	test: 0.649570

Epoch: 78
Loss: 0.22656558879784175
RMSE train: 0.414130	val: 0.887583	test: 0.863945
MAE train: 0.332614	val: 0.696106	test: 0.691770

Epoch: 79
Loss: 0.2307188766343253
RMSE train: 0.424200	val: 0.879601	test: 0.857386
MAE train: 0.339039	val: 0.689530	test: 0.687950

Epoch: 80
Loss: 0.24014087234224593
RMSE train: 0.403789	val: 0.840857	test: 0.820392
MAE train: 0.320032	val: 0.662625	test: 0.661184

Epoch: 81
Loss: 0.23204327481133596
RMSE train: 0.373879	val: 0.877171	test: 0.844149
MAE train: 0.295097	val: 0.690305	test: 0.686447

Epoch: 82
Loss: 0.21182088660342352
RMSE train: 0.368660	val: 0.861664	test: 0.831490
MAE train: 0.240771	val: 0.783144	test: 0.758509

Epoch: 83
Loss: 0.201049981372697
RMSE train: 0.290539	val: 0.989929	test: 0.990414
MAE train: 0.227377	val: 0.768844	test: 0.778079

Epoch: 84
Loss: 0.211357872400965
RMSE train: 0.393510	val: 1.031895	test: 1.013022
MAE train: 0.316934	val: 0.801211	test: 0.806473

Epoch: 85
Loss: 0.1939774602651596
RMSE train: 0.353122	val: 1.001351	test: 0.962370
MAE train: 0.278166	val: 0.782997	test: 0.763330

Epoch: 86
Loss: 0.19223705040557043
RMSE train: 0.319062	val: 0.976041	test: 0.947353
MAE train: 0.254809	val: 0.768304	test: 0.754817

Epoch: 87
Loss: 0.19146487542561122
RMSE train: 0.286077	val: 1.034407	test: 0.991440
MAE train: 0.224338	val: 0.810669	test: 0.788099

Epoch: 88
Loss: 0.18020092163767135
RMSE train: 0.341221	val: 1.006709	test: 0.971236
MAE train: 0.271610	val: 0.795996	test: 0.773031

Epoch: 89
Loss: 0.18611808440514974
RMSE train: 0.308820	val: 0.976569	test: 0.964160
MAE train: 0.244302	val: 0.760963	test: 0.763648

Epoch: 90
Loss: 0.1945186768259321
RMSE train: 0.326293	val: 0.969395	test: 0.969695
MAE train: 0.260568	val: 0.758816	test: 0.757059

Epoch: 91
Loss: 0.1878782712987491
RMSE train: 0.317738	val: 1.002451	test: 0.981954
MAE train: 0.249435	val: 0.783469	test: 0.781332

Epoch: 92
Loss: 0.17381483422858374
RMSE train: 0.304542	val: 1.017294	test: 0.980849
MAE train: 0.240744	val: 0.799005	test: 0.788804

Epoch: 93
Loss: 0.17006026740585053
RMSE train: 0.329263	val: 1.024154	test: 0.996701
MAE train: 0.263149	val: 0.798893	test: 0.788834

Epoch: 94
Loss: 0.17222395007099425
RMSE train: 0.279055	val: 0.998789	test: 0.970431
MAE train: 0.221538	val: 0.787014	test: 0.772477

Epoch: 95
Loss: 0.16363036100353515
RMSE train: 0.296361	val: 1.037204	test: 1.013721
MAE train: 0.235968	val: 0.810923	test: 0.809527

Epoch: 96
Loss: 0.17362185886928014
RMSE train: 0.288934	val: 1.054406	test: 1.052924
MAE train: 0.232573	val: 0.824089	test: 0.829185

Epoch: 97
Loss: 0.1763558621917452
RMSE train: 0.321837	val: 1.019321	test: 1.004733
MAE train: 0.255979	val: 0.803801	test: 0.802918

Epoch: 98
Loss: 0.16025601540293014
RMSE train: 0.258774	val: 0.986552	test: 0.983327
MAE train: 0.202948	val: 0.775632	test: 0.774535

Epoch: 99
Loss: 0.16630768775939941
RMSE train: 0.287864	val: 1.017361	test: 0.994373
MAE train: 0.227360	val: 0.801540	test: 0.787363

Epoch: 100
Loss: 0.1701865132365908
RMSE train: 0.257524	val: 0.996930	test: 0.985758
MAE train: 0.205272	val: 0.780868	test: 0.776513

Epoch: 101
Loss: 0.15374583963836944
RMSE train: 0.297486	val: 1.028242	test: 1.006002
MAE train: 0.237546	val: 0.809383	test: 0.805671

Epoch: 102
Loss: 0.1489205669079508
RMSE train: 0.327411	val: 1.024640	test: 1.013784
MAE train: 0.264238	val: 0.801146	test: 0.804308

Epoch: 103
Loss: 0.15443121109689986
RMSE train: 0.267037	val: 1.010257	test: 1.001835
MAE train: 0.211018	val: 0.789518	test: 0.791502

Epoch: 104
Loss: 0.15757082189832414
RMSE train: 0.276610	val: 0.990963	test: 0.982699
MAE train: 0.216779	val: 0.776695	test: 0.775650

Epoch: 105
Loss: 0.15517952293157578
RMSE train: 0.280689	val: 0.991504	test: 0.980335
MAE train: 0.224503	val: 0.779200	test: 0.777041

Epoch: 106
Loss: 0.14781334464039123
RMSE train: 0.246963	val: 1.017215	test: 1.006484
MAE train: 0.194896	val: 0.798953	test: 0.796053

Epoch: 107
Loss: 0.15749654067414148
RMSE train: 0.247900	val: 1.028200	test: 1.015630
MAE train: 0.197559	val: 0.808756	test: 0.811977

Epoch: 108
Loss: 0.1437539287975856
RMSE train: 0.249210	val: 1.031779	test: 1.013943
MAE train: 0.198113	val: 0.811316	test: 0.811454

Epoch: 109
Loss: 0.1362773057605539
RMSE train: 0.246039	val: 0.972985	test: 0.959803
MAE train: 0.195217	val: 0.766146	test: 0.748750

Epoch: 110
Loss: 0.1402921820325511
RMSE train: 0.288481	val: 1.008611	test: 0.971862
MAE train: 0.228772	val: 0.797319	test: 0.769078

Epoch: 111
Loss: 0.14625776878425054
RMSE train: 0.237257	val: 0.993382	test: 0.987518
MAE train: 0.184129	val: 0.775680	test: 0.778974

Epoch: 112
Loss: 0.1479011486683573
RMSE train: 0.275156	val: 1.006797	test: 0.985430
MAE train: 0.216660	val: 0.795679	test: 0.786236

Epoch: 113
Loss: 0.14402069843241147
RMSE train: 0.249713	val: 0.988577	test: 0.983103
MAE train: 0.195730	val: 0.772530	test: 0.777740

Epoch: 114
Loss: 0.16019201651215553
RMSE train: 0.223423	val: 0.989147	test: 0.977616
MAE train: 0.175031	val: 0.773350	test: 0.772041

Epoch: 115
Loss: 0.15497548558882304
RMSE train: 0.291270	val: 1.012179	test: 0.994212
MAE train: 0.229042	val: 0.793760	test: 0.785900

Epoch: 116
Loss: 0.14696622214147023
RMSE train: 0.268057	val: 0.983168	test: 0.988226
MAE train: 0.211216	val: 0.768215	test: 0.779273

Epoch: 117
Loss: 0.1405596812920911
RMSE train: 0.268394	val: 0.976824	test: 0.973171
MAE train: 0.211684	val: 0.767756	test: 0.767436

Epoch: 118
Loss: 0.1355286245899541
RMSE train: 0.244620	val: 1.023004	test: 1.018107
MAE train: 0.192798	val: 0.801954	test: 0.801684

Epoch: 119
Loss: 0.12543403410485812
RMSE train: 0.224847	val: 0.986069	test: 0.976249
MAE train: 0.178098	val: 0.771958	test: 0.771036

Epoch: 120
Loss: 0.13445145796452249
RMSE train: 0.260696	val: 1.005165	test: 0.982546
MAE train: 0.209303	val: 0.790318	test: 0.780465

Epoch: 121
Loss: 0.13423347100615501
RMSE train: 0.235443	val: 0.998168	test: 0.990923
MAE train: 0.186477	val: 0.778653	test: 0.782505

Early stopping
Best (RMSE):	 train: 0.704316	val: 0.946819	test: 0.927359
Best (MAE):	 train: 0.556526	val: 0.759091	test: 0.734395

MAE train: 0.301153	val: 0.738707	test: 0.699805

Epoch: 83
Loss: 0.21084135877234594
RMSE train: 0.327636	val: 0.918449	test: 0.873497
MAE train: 0.259025	val: 0.731222	test: 0.693984

Epoch: 84
Loss: 0.22259089244263514
RMSE train: 0.360723	val: 0.933086	test: 0.886246
MAE train: 0.283667	val: 0.728789	test: 0.699195

Epoch: 85
Loss: 0.2190436859216009
RMSE train: 0.399678	val: 0.960426	test: 0.882493
MAE train: 0.320593	val: 0.756589	test: 0.697521

Epoch: 86
Loss: 0.22000363043376378
RMSE train: 0.390884	val: 0.951584	test: 0.879201
MAE train: 0.311907	val: 0.748368	test: 0.693743

Epoch: 87
Loss: 0.19694042844431742
RMSE train: 0.445357	val: 0.989637	test: 0.939427
MAE train: 0.350655	val: 0.779606	test: 0.742857

Epoch: 88
Loss: 0.1909665816596576
RMSE train: 0.301010	val: 0.922803	test: 0.870828
MAE train: 0.238378	val: 0.725083	test: 0.685314

Epoch: 89
Loss: 0.19097482519490377
RMSE train: 0.306328	val: 0.920166	test: 0.859736
MAE train: 0.244094	val: 0.736213	test: 0.686232

Epoch: 90
Loss: 0.18444413478885377
RMSE train: 0.297185	val: 0.904448	test: 0.884496
MAE train: 0.233803	val: 0.725835	test: 0.709564

Epoch: 91
Loss: 0.1920297262924058
RMSE train: 0.355041	val: 0.965714	test: 0.903066
MAE train: 0.279610	val: 0.758530	test: 0.718460

Epoch: 92
Loss: 0.18835482320615224
RMSE train: 0.291278	val: 0.910431	test: 0.892832
MAE train: 0.229116	val: 0.720020	test: 0.707223

Epoch: 93
Loss: 0.18175780666725977
RMSE train: 0.387474	val: 0.946040	test: 0.884732
MAE train: 0.310947	val: 0.748490	test: 0.698960

Epoch: 94
Loss: 0.16788930126598903
RMSE train: 0.284532	val: 0.918356	test: 0.890328
MAE train: 0.223085	val: 0.733038	test: 0.714431

Epoch: 95
Loss: 0.1759564078279904
RMSE train: 0.337110	val: 0.925443	test: 0.873130
MAE train: 0.267203	val: 0.732939	test: 0.694804

Epoch: 96
Loss: 0.19236209243535995
RMSE train: 0.318821	val: 0.961494	test: 0.889119
MAE train: 0.250196	val: 0.753243	test: 0.704409

Epoch: 97
Loss: 0.1905492405806269
RMSE train: 0.421081	val: 0.929264	test: 0.875425
MAE train: 0.350939	val: 0.733717	test: 0.702248

Epoch: 98
Loss: 0.19262200381074632
RMSE train: 0.555359	val: 1.006115	test: 0.929311
MAE train: 0.455355	val: 0.793176	test: 0.737542

Epoch: 99
Loss: 0.17250325317893708
RMSE train: 0.289869	val: 0.920526	test: 0.889907
MAE train: 0.228847	val: 0.735742	test: 0.718195

Epoch: 100
Loss: 0.16293959958212717
RMSE train: 0.287694	val: 0.944055	test: 0.874085
MAE train: 0.226208	val: 0.749250	test: 0.695896

Epoch: 101
Loss: 0.15710851975849696
RMSE train: 0.284427	val: 0.902168	test: 0.870914
MAE train: 0.224868	val: 0.726878	test: 0.700040

Epoch: 102
Loss: 0.15850461380822317
RMSE train: 0.294493	val: 0.905039	test: 0.857599
MAE train: 0.234425	val: 0.718549	test: 0.687279

Epoch: 103
Loss: 0.14682779886892863
RMSE train: 0.332390	val: 0.938309	test: 0.872160
MAE train: 0.265963	val: 0.744811	test: 0.696710

Epoch: 104
Loss: 0.1558207699230739
RMSE train: 0.251360	val: 0.916020	test: 0.855231
MAE train: 0.197276	val: 0.730495	test: 0.683133

Epoch: 105
Loss: 0.16315775577511107
RMSE train: 0.293960	val: 0.919244	test: 0.872372
MAE train: 0.229773	val: 0.733735	test: 0.701227

Epoch: 106
Loss: 0.16203090867825917
RMSE train: 0.351387	val: 0.930645	test: 0.878720
MAE train: 0.286031	val: 0.741594	test: 0.705561

Epoch: 107
Loss: 0.15120167710951396
RMSE train: 0.339931	val: 0.925012	test: 0.873885
MAE train: 0.273070	val: 0.734256	test: 0.699401

Epoch: 108
Loss: 0.14242086293441908
RMSE train: 0.250973	val: 0.923614	test: 0.876248
MAE train: 0.197251	val: 0.727726	test: 0.698493

Epoch: 109
Loss: 0.14687805197068624
RMSE train: 0.271334	val: 0.934059	test: 0.881970
MAE train: 0.215510	val: 0.737419	test: 0.705837

Epoch: 110
Loss: 0.14957264278616225
RMSE train: 0.270095	val: 0.916670	test: 0.875352
MAE train: 0.213088	val: 0.728376	test: 0.703198

Epoch: 111
Loss: 0.1429991919015135
RMSE train: 0.240973	val: 0.895606	test: 0.857460
MAE train: 0.188612	val: 0.719587	test: 0.685409

Epoch: 112
Loss: 0.1531535450901304
RMSE train: 0.251130	val: 0.915677	test: 0.864859
MAE train: 0.198448	val: 0.728633	test: 0.691444

Epoch: 113
Loss: 0.15162268706730433
RMSE train: 0.362567	val: 0.950191	test: 0.888254
MAE train: 0.289061	val: 0.756769	test: 0.714204

Epoch: 114
Loss: 0.14792859075324877
RMSE train: 0.279090	val: 0.948722	test: 0.890116
MAE train: 0.222298	val: 0.747425	test: 0.706925

Epoch: 115
Loss: 0.15139677481991903
RMSE train: 0.307310	val: 0.905996	test: 0.858321
MAE train: 0.251370	val: 0.727482	test: 0.687629

Epoch: 116
Loss: 0.14111044364316122
RMSE train: 0.268673	val: 0.918284	test: 0.872981
MAE train: 0.211376	val: 0.736117	test: 0.694190

Epoch: 117
Loss: 0.14588255328791483
RMSE train: 0.292153	val: 0.929467	test: 0.865044
MAE train: 0.230966	val: 0.738618	test: 0.686683

Epoch: 118
Loss: 0.14456574831690108
RMSE train: 0.283260	val: 0.929406	test: 0.873490
MAE train: 0.226202	val: 0.738312	test: 0.697143

Epoch: 119
Loss: 0.13415629310267313
RMSE train: 0.320302	val: 0.908245	test: 0.869623
MAE train: 0.257321	val: 0.730544	test: 0.697993

Epoch: 120
Loss: 0.12788512451308115
RMSE train: 0.348772	val: 0.945078	test: 0.897451
MAE train: 0.284513	val: 0.742530	test: 0.709981

Epoch: 121
Loss: 0.1325810221689088
RMSE train: 0.278569	val: 0.934005	test: 0.867208
MAE train: 0.218453	val: 0.748561	test: 0.687745

Early stopping
Best (RMSE):	 train: 0.450781	val: 0.891976	test: 0.859164
Best (MAE):	 train: 0.356264	val: 0.708681	test: 0.680500

MAE train: 0.313784	val: 0.857682	test: 0.827431

Epoch: 83
Loss: 0.18764341090406692
RMSE train: 0.318848	val: 1.108849	test: 1.039754
MAE train: 0.251302	val: 0.861722	test: 0.832344

Epoch: 84
Loss: 0.17274762902941024
RMSE train: 0.295536	val: 1.225538	test: 1.130151
MAE train: 0.234333	val: 0.958477	test: 0.905148

Epoch: 85
Loss: 0.18920148483344487
RMSE train: 0.338210	val: 1.134763	test: 1.063898
MAE train: 0.266140	val: 0.879373	test: 0.847210

Epoch: 86
Loss: 0.17423144515071595
RMSE train: 0.353989	val: 1.063427	test: 1.004891
MAE train: 0.277613	val: 0.827652	test: 0.797659

Epoch: 87
Loss: 0.16934308409690857
RMSE train: 0.274183	val: 1.094194	test: 1.020638
MAE train: 0.217983	val: 0.849505	test: 0.812626

Epoch: 88
Loss: 0.1718663935150419
RMSE train: 0.337367	val: 1.138510	test: 1.064064
MAE train: 0.268651	val: 0.888148	test: 0.852488

Epoch: 89
Loss: 0.16652782474245345
RMSE train: 0.292166	val: 1.074763	test: 1.003433
MAE train: 0.231125	val: 0.831013	test: 0.800703

Epoch: 90
Loss: 0.16292475376810348
RMSE train: 0.280150	val: 1.054199	test: 0.985974
MAE train: 0.220763	val: 0.809969	test: 0.782899

Epoch: 91
Loss: 0.1669803825872285
RMSE train: 0.356917	val: 1.274514	test: 1.177098
MAE train: 0.287205	val: 1.007181	test: 0.950908

Epoch: 92
Loss: 0.16488269397190639
RMSE train: 0.294542	val: 1.073025	test: 1.002569
MAE train: 0.233039	val: 0.828472	test: 0.799058

Epoch: 93
Loss: 0.16412330951009477
RMSE train: 0.367280	val: 1.103325	test: 1.026599
MAE train: 0.293141	val: 0.852266	test: 0.811910

Epoch: 94
Loss: 0.16616252809762955
RMSE train: 0.550588	val: 1.384440	test: 1.288678
MAE train: 0.465593	val: 1.112911	test: 1.057846

Epoch: 95
Loss: 0.16018625455243246
RMSE train: 0.308125	val: 1.042867	test: 0.964692
MAE train: 0.243191	val: 0.816813	test: 0.766689

Epoch: 96
Loss: 0.148766071668693
RMSE train: 0.303353	val: 1.101045	test: 1.015337
MAE train: 0.239890	val: 0.848596	test: 0.804307

Epoch: 97
Loss: 0.1538496453847204
RMSE train: 0.469370	val: 1.245263	test: 1.141488
MAE train: 0.394004	val: 0.978070	test: 0.919046

Epoch: 98
Loss: 0.14379385060497693
RMSE train: 0.263813	val: 1.052818	test: 0.967689
MAE train: 0.208830	val: 0.817378	test: 0.767015

Epoch: 99
Loss: 0.15415157164846147
RMSE train: 0.387758	val: 1.280516	test: 1.194276
MAE train: 0.318337	val: 1.003273	test: 0.959904

Epoch: 100
Loss: 0.15518613745059287
RMSE train: 0.315795	val: 1.041052	test: 0.966106
MAE train: 0.250843	val: 0.813504	test: 0.763566

Epoch: 101
Loss: 0.14671524933406285
RMSE train: 0.340412	val: 1.245444	test: 1.159592
MAE train: 0.272568	val: 0.976004	test: 0.937368

Epoch: 102
Loss: 0.14675400512559073
RMSE train: 0.274022	val: 1.115777	test: 1.034810
MAE train: 0.215144	val: 0.863899	test: 0.823727

Epoch: 103
Loss: 0.13801733404397964
RMSE train: 0.301404	val: 1.144741	test: 1.062537
MAE train: 0.238467	val: 0.889356	test: 0.851007

Epoch: 104
Loss: 0.15054152214101382
RMSE train: 0.247372	val: 1.066098	test: 0.995872
MAE train: 0.195597	val: 0.823458	test: 0.788621

Epoch: 105
Loss: 0.14871707558631897
RMSE train: 0.304776	val: 1.143733	test: 1.069651
MAE train: 0.243620	val: 0.886828	test: 0.855244

Epoch: 106
Loss: 0.13309806478875025
RMSE train: 0.310918	val: 1.141259	test: 1.056909
MAE train: 0.245939	val: 0.888908	test: 0.845436

Epoch: 107
Loss: 0.13950313148753984
RMSE train: 0.376320	val: 1.134702	test: 1.053227
MAE train: 0.299395	val: 0.883891	test: 0.843026

Epoch: 108
Loss: 0.1421495026775769
RMSE train: 0.255220	val: 1.168342	test: 1.088857
MAE train: 0.201552	val: 0.909316	test: 0.869585

Epoch: 109
Loss: 0.13646496619497026
RMSE train: 0.272050	val: 1.128418	test: 1.042817
MAE train: 0.215348	val: 0.873145	test: 0.830370

Epoch: 110
Loss: 0.141421413315194
RMSE train: 0.420545	val: 1.211050	test: 1.137447
MAE train: 0.349979	val: 0.950701	test: 0.917213

Epoch: 111
Loss: 0.1362006435436862
RMSE train: 0.223721	val: 1.074162	test: 1.005632
MAE train: 0.175445	val: 0.828610	test: 0.799994

Epoch: 112
Loss: 0.12909914233854838
RMSE train: 0.254323	val: 1.119714	test: 1.045039
MAE train: 0.202272	val: 0.868917	test: 0.835846

Epoch: 113
Loss: 0.12813705472009523
RMSE train: 0.534239	val: 1.273238	test: 1.175028
MAE train: 0.457308	val: 0.999915	test: 0.947055

Epoch: 114
Loss: 0.13068248704075813
RMSE train: 0.259426	val: 1.032287	test: 0.967092
MAE train: 0.206273	val: 0.805060	test: 0.771728

Epoch: 115
Loss: 0.12238342847142901
RMSE train: 0.262863	val: 1.015107	test: 0.956280
MAE train: 0.207329	val: 0.789513	test: 0.765765

Epoch: 116
Loss: 0.12427410909107753
RMSE train: 0.361589	val: 1.231167	test: 1.142664
MAE train: 0.293427	val: 0.962404	test: 0.919063

Epoch: 117
Loss: 0.11627157777547836
RMSE train: 0.273594	val: 1.219876	test: 1.143239
MAE train: 0.218234	val: 0.955376	test: 0.920534

Epoch: 118
Loss: 0.1243675494832652
RMSE train: 0.229289	val: 1.067745	test: 0.999334
MAE train: 0.180750	val: 0.825263	test: 0.792842

Epoch: 119
Loss: 0.1241986480142389
RMSE train: 0.233561	val: 1.096545	test: 1.023178
MAE train: 0.185510	val: 0.849542	test: 0.819017

Epoch: 120
Loss: 0.12553562436785018
RMSE train: 0.218697	val: 1.064900	test: 1.005524
MAE train: 0.172344	val: 0.819523	test: 0.801313

Epoch: 121
Loss: 0.11185815983584949
RMSE train: 0.251362	val: 1.092668	test: 1.021282
MAE train: 0.197989	val: 0.848797	test: 0.817884

Early stopping
Best (RMSE):	 train: 0.554260	val: 0.960600	test: 0.898195
Best (MAE):	 train: 0.440989	val: 0.753939	test: 0.718769

RMSE train: 0.497761	val: 0.775115	test: 0.742460
MAE train: 0.386679	val: 0.575674	test: 0.582362

Epoch: 84
Loss: 0.29706315270492006
RMSE train: 0.475094	val: 0.761766	test: 0.755986
MAE train: 0.365705	val: 0.570928	test: 0.586388

Epoch: 85
Loss: 0.2807845483933176
RMSE train: 0.499891	val: 0.796793	test: 0.781215
MAE train: 0.388518	val: 0.597274	test: 0.598608

Epoch: 86
Loss: 0.30302309138434275
RMSE train: 0.489062	val: 0.757347	test: 0.743567
MAE train: 0.378561	val: 0.572791	test: 0.574363

Epoch: 87
Loss: 0.2757779668484415
RMSE train: 0.618049	val: 0.868811	test: 0.843521
MAE train: 0.498701	val: 0.671043	test: 0.665718

Epoch: 88
Loss: 0.3044821683849607
RMSE train: 0.458401	val: 0.730009	test: 0.733692
MAE train: 0.355842	val: 0.553525	test: 0.566340

Epoch: 89
Loss: 0.2911237563405718
RMSE train: 0.511417	val: 0.794409	test: 0.771772
MAE train: 0.400432	val: 0.594820	test: 0.601207

Epoch: 90
Loss: 0.2904700083392007
RMSE train: 0.492706	val: 0.774738	test: 0.747212
MAE train: 0.382519	val: 0.578299	test: 0.581691

Epoch: 91
Loss: 0.28948138654232025
RMSE train: 0.470314	val: 0.763968	test: 0.740571
MAE train: 0.364694	val: 0.578516	test: 0.579002

Epoch: 92
Loss: 0.27742806183440344
RMSE train: 0.474975	val: 0.775855	test: 0.766855
MAE train: 0.368376	val: 0.581466	test: 0.590990

Epoch: 93
Loss: 0.2949117549828121
RMSE train: 0.499329	val: 0.778554	test: 0.776397
MAE train: 0.384864	val: 0.589275	test: 0.608633

Epoch: 94
Loss: 0.2697866154568536
RMSE train: 0.453186	val: 0.757590	test: 0.732628
MAE train: 0.349930	val: 0.571295	test: 0.569789

Epoch: 95
Loss: 0.26836513940777096
RMSE train: 0.461424	val: 0.757407	test: 0.747541
MAE train: 0.356440	val: 0.567110	test: 0.578287

Epoch: 96
Loss: 0.27328040982995716
RMSE train: 0.464952	val: 0.744846	test: 0.745114
MAE train: 0.359318	val: 0.556021	test: 0.580383

Epoch: 97
Loss: 0.26331423595547676
RMSE train: 0.468794	val: 0.770819	test: 0.778792
MAE train: 0.363513	val: 0.587427	test: 0.608964

Epoch: 98
Loss: 0.2760460876992771
RMSE train: 0.448872	val: 0.747912	test: 0.731297
MAE train: 0.344877	val: 0.567144	test: 0.566323

Epoch: 99
Loss: 0.267879632966859
RMSE train: 0.471531	val: 0.762257	test: 0.744010
MAE train: 0.364034	val: 0.574867	test: 0.581375

Epoch: 100
Loss: 0.2680294151817049
RMSE train: 0.444761	val: 0.742307	test: 0.736671
MAE train: 0.343163	val: 0.557151	test: 0.570567

Epoch: 101
Loss: 0.2721698922770364
RMSE train: 0.491615	val: 0.777597	test: 0.753175
MAE train: 0.383245	val: 0.586051	test: 0.589756

Epoch: 102
Loss: 0.2796593766127314
RMSE train: 0.469698	val: 0.782204	test: 0.758938
MAE train: 0.363604	val: 0.584900	test: 0.596415

Epoch: 103
Loss: 0.2776449610080038
RMSE train: 0.444699	val: 0.757011	test: 0.739563
MAE train: 0.343899	val: 0.564524	test: 0.568099

Epoch: 104
Loss: 0.2747220993041992
RMSE train: 0.461685	val: 0.764769	test: 0.747933
MAE train: 0.355933	val: 0.574869	test: 0.579334

Epoch: 105
Loss: 0.27403326651879717
RMSE train: 0.462221	val: 0.770718	test: 0.752844
MAE train: 0.357483	val: 0.581017	test: 0.584994

Epoch: 106
Loss: 0.27487175379480633
RMSE train: 0.494092	val: 0.795460	test: 0.770906
MAE train: 0.386306	val: 0.598032	test: 0.605878

Epoch: 107
Loss: 0.2576651849917003
RMSE train: 0.454044	val: 0.762594	test: 0.738698
MAE train: 0.349468	val: 0.577308	test: 0.567354

Epoch: 108
Loss: 0.275410669190543
RMSE train: 0.458468	val: 0.765147	test: 0.750670
MAE train: 0.357765	val: 0.579953	test: 0.578988

Epoch: 109
Loss: 0.27962841412850786
RMSE train: 0.464147	val: 0.774082	test: 0.759595
MAE train: 0.362495	val: 0.585827	test: 0.585897

Epoch: 110
Loss: 0.2608343352164541
RMSE train: 0.469493	val: 0.775733	test: 0.752094
MAE train: 0.362012	val: 0.579679	test: 0.588227

Epoch: 111
Loss: 0.27277092209884096
RMSE train: 0.455559	val: 0.747496	test: 0.753188
MAE train: 0.349766	val: 0.564130	test: 0.588362

Epoch: 112
Loss: 0.27138879150152206
RMSE train: 0.472906	val: 0.791298	test: 0.787878
MAE train: 0.364379	val: 0.589116	test: 0.600275

Epoch: 113
Loss: 0.27106822707823347
RMSE train: 0.492441	val: 0.793062	test: 0.782679
MAE train: 0.382552	val: 0.600930	test: 0.611401

Epoch: 114
Loss: 0.27347905188798904
RMSE train: 0.498877	val: 0.803324	test: 0.774753
MAE train: 0.389190	val: 0.616277	test: 0.615333

Epoch: 115
Loss: 0.2616374141403607
RMSE train: 0.492664	val: 0.796706	test: 0.775002
MAE train: 0.384044	val: 0.600557	test: 0.608039

Epoch: 116
Loss: 0.2543045367513384
RMSE train: 0.504103	val: 0.810329	test: 0.779668
MAE train: 0.396484	val: 0.613299	test: 0.608510

Epoch: 117
Loss: 0.2618219128676823
RMSE train: 0.458315	val: 0.759148	test: 0.749115
MAE train: 0.356979	val: 0.573310	test: 0.589779

Epoch: 118
Loss: 0.25565252772399355
RMSE train: 0.435634	val: 0.768186	test: 0.763660
MAE train: 0.336496	val: 0.577520	test: 0.583295

Epoch: 119
Loss: 0.28378651823316303
RMSE train: 0.441682	val: 0.752457	test: 0.741974
MAE train: 0.341460	val: 0.568192	test: 0.577114

Epoch: 120
Loss: 0.2558425324303763
RMSE train: 0.449563	val: 0.772102	test: 0.759079
MAE train: 0.346407	val: 0.575909	test: 0.588355

Epoch: 121
Loss: 0.25385139137506485
RMSE train: 0.455843	val: 0.762981	test: 0.755483
MAE train: 0.357288	val: 0.569833	test: 0.584955

Epoch: 122
Loss: 0.2750935437423842
RMSE train: 0.546669	val: 0.843940	test: 0.804307
MAE train: 0.428150	val: 0.638222	test: 0.634959

Epoch: 123
Loss: 0.24584664510829107
RMSE train: 0.481338	val: 0.789027	test: 0.782178
MAE train: 0.376813	val: 0.590598	test: 0.606606

Early stopping
Best (RMSE):	 train: 0.458401	val: 0.730009	test: 0.733692
Best (MAE):	 train: 0.355842	val: 0.553525	test: 0.566340
All runs completed.

MAE train: 0.299538	val: 0.648653	test: 0.659199

Epoch: 83
Loss: 0.21778035589626857
RMSE train: 0.345906	val: 0.837516	test: 0.854085
MAE train: 0.273453	val: 0.672679	test: 0.666442

Epoch: 84
Loss: 0.21407660309757506
RMSE train: 0.427210	val: 0.817510	test: 0.825741
MAE train: 0.345273	val: 0.642368	test: 0.641160

Epoch: 85
Loss: 0.21705698541232518
RMSE train: 0.350588	val: 0.816591	test: 0.844601
MAE train: 0.278223	val: 0.650402	test: 0.659559

Epoch: 86
Loss: 0.22182882683617727
RMSE train: 0.398045	val: 0.869462	test: 0.878830
MAE train: 0.319189	val: 0.701841	test: 0.689360

Epoch: 87
Loss: 0.21530086547136307
RMSE train: 0.343729	val: 0.838738	test: 0.880708
MAE train: 0.269818	val: 0.664990	test: 0.691846

Epoch: 88
Loss: 0.2001896777323314
RMSE train: 0.371377	val: 0.833046	test: 0.841602
MAE train: 0.293890	val: 0.659043	test: 0.662922

Epoch: 89
Loss: 0.2102648647768157
RMSE train: 0.351069	val: 0.824486	test: 0.846554
MAE train: 0.278410	val: 0.651894	test: 0.658743

Epoch: 90
Loss: 0.21281008103064128
RMSE train: 0.432547	val: 0.841570	test: 0.871771
MAE train: 0.343128	val: 0.666792	test: 0.686574

Epoch: 91
Loss: 0.2059834258896964
RMSE train: 0.325648	val: 0.829603	test: 0.854091
MAE train: 0.256372	val: 0.659553	test: 0.666934

Epoch: 92
Loss: 0.1892061425106866
RMSE train: 0.329954	val: 0.832194	test: 0.844763
MAE train: 0.260892	val: 0.666822	test: 0.659470

Epoch: 93
Loss: 0.19116696183170592
RMSE train: 0.397384	val: 0.810459	test: 0.849537
MAE train: 0.319031	val: 0.639810	test: 0.663640

Epoch: 94
Loss: 0.2020169198513031
RMSE train: 0.324812	val: 0.809945	test: 0.830453
MAE train: 0.256972	val: 0.644484	test: 0.642920

Epoch: 95
Loss: 0.1996970453432628
RMSE train: 0.372171	val: 0.818711	test: 0.824301
MAE train: 0.293160	val: 0.651957	test: 0.650569

Epoch: 96
Loss: 0.19227155297994614
RMSE train: 0.303042	val: 0.830723	test: 0.863098
MAE train: 0.238616	val: 0.666151	test: 0.673964

Epoch: 97
Loss: 0.18462856539658137
RMSE train: 0.331378	val: 0.835285	test: 0.857271
MAE train: 0.260360	val: 0.667148	test: 0.670583

Epoch: 98
Loss: 0.18340835933174407
RMSE train: 0.317910	val: 0.841944	test: 0.875001
MAE train: 0.252795	val: 0.671180	test: 0.688901

Epoch: 99
Loss: 0.18547305358307703
RMSE train: 0.424451	val: 0.831163	test: 0.829114
MAE train: 0.347371	val: 0.650134	test: 0.649015

Epoch: 100
Loss: 0.1859275017465864
RMSE train: 0.292910	val: 0.820975	test: 0.837635
MAE train: 0.229747	val: 0.651310	test: 0.655688

Epoch: 101
Loss: 0.17157572082110814
RMSE train: 0.318565	val: 0.817456	test: 0.823981
MAE train: 0.249447	val: 0.654504	test: 0.642822

Epoch: 102
Loss: 0.17015977948904037
RMSE train: 0.311691	val: 0.829406	test: 0.847015
MAE train: 0.247335	val: 0.660021	test: 0.666634

Epoch: 103
Loss: 0.168626640524183
RMSE train: 0.339208	val: 0.838340	test: 0.853354
MAE train: 0.271564	val: 0.667867	test: 0.674310

Epoch: 104
Loss: 0.1713894084095955
RMSE train: 0.348451	val: 0.820999	test: 0.838635
MAE train: 0.274798	val: 0.652677	test: 0.651416

Epoch: 105
Loss: 0.17334976153714315
RMSE train: 0.326137	val: 0.815383	test: 0.841930
MAE train: 0.257294	val: 0.649207	test: 0.663248

Epoch: 106
Loss: 0.1734431886247226
RMSE train: 0.306838	val: 0.835377	test: 0.858645
MAE train: 0.243046	val: 0.671685	test: 0.670244

Epoch: 107
Loss: 0.1828752394233431
RMSE train: 0.290744	val: 0.817936	test: 0.844039
MAE train: 0.228352	val: 0.658409	test: 0.659194

Epoch: 108
Loss: 0.16311785791601455
RMSE train: 0.310839	val: 0.818652	test: 0.843246
MAE train: 0.245392	val: 0.652897	test: 0.666392

Epoch: 109
Loss: 0.16074105565037047
RMSE train: 0.308797	val: 0.813658	test: 0.814424
MAE train: 0.243257	val: 0.644447	test: 0.634461

Epoch: 110
Loss: 0.17056404586349214
RMSE train: 0.339295	val: 0.837575	test: 0.830992
MAE train: 0.268379	val: 0.670611	test: 0.656466

Epoch: 111
Loss: 0.15512648118393763
RMSE train: 0.318300	val: 0.879722	test: 0.899525
MAE train: 0.254307	val: 0.707701	test: 0.716894

Epoch: 112
Loss: 0.1443656012415886
RMSE train: 0.354096	val: 0.819686	test: 0.819698
MAE train: 0.282997	val: 0.648296	test: 0.641597

Epoch: 113
Loss: 0.14211596229246684
RMSE train: 0.257959	val: 0.822900	test: 0.835782
MAE train: 0.203641	val: 0.658581	test: 0.653746

Epoch: 114
Loss: 0.15382506591933115
RMSE train: 0.293235	val: 0.812011	test: 0.819627
MAE train: 0.230399	val: 0.635867	test: 0.644208

Epoch: 115
Loss: 0.14926710299083165
RMSE train: 0.301190	val: 0.860271	test: 0.874729
MAE train: 0.240861	val: 0.690851	test: 0.687590

Epoch: 116
Loss: 0.15212130333696092
RMSE train: 0.366327	val: 0.828906	test: 0.833343
MAE train: 0.300818	val: 0.644327	test: 0.648313

Epoch: 117
Loss: 0.14570973547441618
RMSE train: 0.282532	val: 0.828734	test: 0.833429
MAE train: 0.222660	val: 0.660499	test: 0.652641

Epoch: 118
Loss: 0.1474334624196802
RMSE train: 0.248849	val: 0.829680	test: 0.846557
MAE train: 0.196291	val: 0.658994	test: 0.660700

Epoch: 119
Loss: 0.12942048641187803
RMSE train: 0.259796	val: 0.821560	test: 0.837812
MAE train: 0.204131	val: 0.658538	test: 0.652033

Epoch: 120
Loss: 0.1530965646462781
RMSE train: 0.287818	val: 0.824577	test: 0.826918
MAE train: 0.228684	val: 0.648542	test: 0.651759

Epoch: 121
Loss: 0.14614632512841905
RMSE train: 0.297349	val: 0.823431	test: 0.838215
MAE train: 0.235385	val: 0.657550	test: 0.653517

Epoch: 122
Loss: 0.14699786901474
RMSE train: 0.333759	val: 0.812655	test: 0.822431
MAE train: 0.265239	val: 0.645022	test: 0.650302

Epoch: 123
Loss: 0.1357377799493926
RMSE train: 0.278737	val: 0.857597	test: 0.876426
MAE train: 0.219175	val: 0.686959	test: 0.691652

Epoch: 124
Loss: 0.14417288505605289
RMSE train: 0.296738	val: 0.826058	test: 0.823992
MAE train: 0.238736	val: 0.648055	test: 0.644791

Epoch: 125
Loss: 0.15520036167332105
RMSE train: 0.256737	val: 0.825813	test: 0.837916
MAE train: 0.204356	val: 0.660123	test: 0.654276

Epoch: 126
Loss: 0.1558568733079093
RMSE train: 0.309157	val: 0.819218	test: 0.817060
MAE train: 0.245203	val: 0.647532	test: 0.634628

Epoch: 127
Loss: 0.12822047088827407
RMSE train: 0.244241	val: 0.826553	test: 0.845517
MAE train: 0.192698	val: 0.656318	test: 0.658300

Epoch: 128
Loss: 0.13315313735178538
RMSE train: 0.266460	val: 0.852717	test: 0.890797
MAE train: 0.212093	val: 0.682469	test: 0.703557

Epoch: 129
Loss: 0.1411255252148424
RMSE train: 0.291620	val: 0.820502	test: 0.847726
MAE train: 0.232153	val: 0.660599	test: 0.665304

Early stopping
Best (RMSE):	 train: 0.324812	val: 0.809945	test: 0.830453
Best (MAE):	 train: 0.256972	val: 0.644484	test: 0.642920

MAE train: 0.386349	val: 0.585375	test: 0.586834

Epoch: 84
Loss: 0.28914005735090803
RMSE train: 0.483459	val: 0.782929	test: 0.746912
MAE train: 0.376405	val: 0.580999	test: 0.572866

Epoch: 85
Loss: 0.30357261853558676
RMSE train: 0.450180	val: 0.771045	test: 0.735902
MAE train: 0.346440	val: 0.564837	test: 0.556486

Epoch: 86
Loss: 0.28186273894139696
RMSE train: 0.457384	val: 0.779525	test: 0.733474
MAE train: 0.351654	val: 0.577976	test: 0.547495

Epoch: 87
Loss: 0.2714779004454613
RMSE train: 0.443720	val: 0.756494	test: 0.730959
MAE train: 0.345196	val: 0.553248	test: 0.557771

Epoch: 88
Loss: 0.2742284813097545
RMSE train: 0.447914	val: 0.764637	test: 0.744266
MAE train: 0.346817	val: 0.559185	test: 0.571205

Epoch: 89
Loss: 0.2799651835645948
RMSE train: 0.482569	val: 0.788564	test: 0.759645
MAE train: 0.374962	val: 0.587199	test: 0.584588

Epoch: 90
Loss: 0.27869012845414026
RMSE train: 0.451595	val: 0.753092	test: 0.730829
MAE train: 0.350789	val: 0.555001	test: 0.562600

Epoch: 91
Loss: 0.288934982248715
RMSE train: 0.454242	val: 0.775391	test: 0.742281
MAE train: 0.349871	val: 0.574848	test: 0.553855

Epoch: 92
Loss: 0.2854309028812817
RMSE train: 0.456707	val: 0.778475	test: 0.736421
MAE train: 0.352588	val: 0.567538	test: 0.564346

Epoch: 93
Loss: 0.2809196710586548
RMSE train: 0.455174	val: 0.748954	test: 0.736916
MAE train: 0.352207	val: 0.554213	test: 0.562613

Epoch: 94
Loss: 0.2761205956339836
RMSE train: 0.438378	val: 0.762013	test: 0.748247
MAE train: 0.336309	val: 0.558384	test: 0.560175

Epoch: 95
Loss: 0.28405161095517023
RMSE train: 0.442028	val: 0.756546	test: 0.716853
MAE train: 0.340825	val: 0.568305	test: 0.543824

Epoch: 96
Loss: 0.28147371645484653
RMSE train: 0.442396	val: 0.771999	test: 0.744253
MAE train: 0.342223	val: 0.571578	test: 0.555806

Epoch: 97
Loss: 0.27617717534303665
RMSE train: 0.456151	val: 0.766721	test: 0.730987
MAE train: 0.354885	val: 0.578055	test: 0.557948

Epoch: 98
Loss: 0.28039165479796274
RMSE train: 0.450874	val: 0.749425	test: 0.734970
MAE train: 0.351015	val: 0.560462	test: 0.557825

Epoch: 99
Loss: 0.2871965318918228
RMSE train: 0.441097	val: 0.766610	test: 0.734839
MAE train: 0.341252	val: 0.565696	test: 0.555151

Epoch: 100
Loss: 0.27259504050016403
RMSE train: 0.454756	val: 0.755721	test: 0.755674
MAE train: 0.350298	val: 0.557058	test: 0.574841

Epoch: 101
Loss: 0.26470309495925903
RMSE train: 0.430278	val: 0.754604	test: 0.732244
MAE train: 0.333905	val: 0.555284	test: 0.555163

Epoch: 102
Loss: 0.27092214460883823
RMSE train: 0.441355	val: 0.747245	test: 0.734522
MAE train: 0.345137	val: 0.556174	test: 0.564272

Epoch: 103
Loss: 0.286641854260649
RMSE train: 0.447647	val: 0.779299	test: 0.736365
MAE train: 0.346678	val: 0.582144	test: 0.551523

Epoch: 104
Loss: 0.27558630811316626
RMSE train: 0.450139	val: 0.773730	test: 0.756015
MAE train: 0.348745	val: 0.576032	test: 0.569549

Epoch: 105
Loss: 0.2685430720448494
RMSE train: 0.449601	val: 0.758474	test: 0.732289
MAE train: 0.351004	val: 0.567100	test: 0.558033

Epoch: 106
Loss: 0.2580226404326303
RMSE train: 0.452281	val: 0.757689	test: 0.747867
MAE train: 0.345280	val: 0.561116	test: 0.560124

Epoch: 107
Loss: 0.2612429069621222
RMSE train: 0.438763	val: 0.754380	test: 0.715876
MAE train: 0.340655	val: 0.563133	test: 0.549166

Epoch: 108
Loss: 0.25045058131217957
RMSE train: 0.443829	val: 0.765048	test: 0.732151
MAE train: 0.342590	val: 0.555447	test: 0.556863

Epoch: 109
Loss: 0.24177805334329605
RMSE train: 0.463889	val: 0.784465	test: 0.749104
MAE train: 0.362617	val: 0.585479	test: 0.565739

Epoch: 110
Loss: 0.2598343278680529
RMSE train: 0.428704	val: 0.746056	test: 0.727299
MAE train: 0.333165	val: 0.557271	test: 0.552202

Epoch: 111
Loss: 0.24635375291109085
RMSE train: 0.445591	val: 0.778120	test: 0.734848
MAE train: 0.346301	val: 0.573582	test: 0.556515

Epoch: 112
Loss: 0.24026689997741155
RMSE train: 0.416694	val: 0.739747	test: 0.735770
MAE train: 0.323039	val: 0.546701	test: 0.557154

Epoch: 113
Loss: 0.2584787628480366
RMSE train: 0.419198	val: 0.736740	test: 0.737476
MAE train: 0.323758	val: 0.545846	test: 0.558609

Epoch: 114
Loss: 0.2566039242914745
RMSE train: 0.429779	val: 0.738354	test: 0.716358
MAE train: 0.336348	val: 0.548665	test: 0.553883

Epoch: 115
Loss: 0.24564444486583983
RMSE train: 0.421085	val: 0.731628	test: 0.723886
MAE train: 0.327319	val: 0.544327	test: 0.544376

Epoch: 116
Loss: 0.26549820814813885
RMSE train: 0.435052	val: 0.751245	test: 0.730710
MAE train: 0.335737	val: 0.567382	test: 0.550362

Epoch: 117
Loss: 0.2539815604686737
RMSE train: 0.465389	val: 0.764542	test: 0.788757
MAE train: 0.359144	val: 0.571679	test: 0.588464

Epoch: 118
Loss: 0.25811553852898733
RMSE train: 0.467869	val: 0.792064	test: 0.764416
MAE train: 0.362458	val: 0.588633	test: 0.585613

Epoch: 119
Loss: 0.23661168877567565
RMSE train: 0.459651	val: 0.779816	test: 0.753688
MAE train: 0.360359	val: 0.569145	test: 0.573115

Epoch: 120
Loss: 0.24448812965835845
RMSE train: 0.411850	val: 0.737703	test: 0.726257
MAE train: 0.319702	val: 0.546729	test: 0.554387

Epoch: 121
Loss: 0.25895857491663526
RMSE train: 0.429181	val: 0.745609	test: 0.734108
MAE train: 0.333556	val: 0.555856	test: 0.549339

Epoch: 122
Loss: 0.24222134479454585
RMSE train: 0.440185	val: 0.767228	test: 0.742173
MAE train: 0.341974	val: 0.565275	test: 0.576938

Epoch: 123
Loss: 0.2512702207480158
RMSE train: 0.416407	val: 0.757713	test: 0.745679
MAE train: 0.321148	val: 0.559266	test: 0.559174

Epoch: 124
Loss: 0.23779787974698202
RMSE train: 0.418979	val: 0.755523	test: 0.736362
MAE train: 0.321459	val: 0.561825	test: 0.553973

Epoch: 125
Loss: 0.2381227378334318
RMSE train: 0.418277	val: 0.750131	test: 0.737249
MAE train: 0.324802	val: 0.556343	test: 0.559743

Epoch: 126
Loss: 0.2353648149541446
RMSE train: 0.412337	val: 0.738161	test: 0.723143
MAE train: 0.319828	val: 0.546297	test: 0.551739

Epoch: 127
Loss: 0.23479874219213212
RMSE train: 0.422932	val: 0.742577	test: 0.735343
MAE train: 0.330960	val: 0.544313	test: 0.562549

Epoch: 128
Loss: 0.23124824464321136
RMSE train: 0.423307	val: 0.766196	test: 0.721386
MAE train: 0.329013	val: 0.567732	test: 0.549614

Epoch: 129
Loss: 0.2326787124787058
RMSE train: 0.420378	val: 0.754999	test: 0.746474
MAE train: 0.325637	val: 0.557898	test: 0.564457

Epoch: 130
Loss: 0.23168776503631047
RMSE train: 0.430210	val: 0.778704	test: 0.757707
MAE train: 0.334317	val: 0.580243	test: 0.575474

Epoch: 131
Loss: 0.23588212260178157
RMSE train: 0.424712	val: 0.753845	test: 0.724533
MAE train: 0.331548	val: 0.559284	test: 0.547837

Epoch: 132
Loss: 0.2383317266191755
RMSE train: 0.412526	val: 0.747360	test: 0.731150
MAE train: 0.321752	val: 0.563763	test: 0.561960

Epoch: 133
Loss: 0.23552006695951735
RMSE train: 0.405768	val: 0.767444	test: 0.728025
MAE train: 0.314279	val: 0.559344	test: 0.549450

Epoch: 134
Loss: 0.23679443235908235
RMSE train: 0.408112	val: 0.766673	test: 0.739670
MAE train: 0.317014	val: 0.563049	test: 0.541676

Epoch: 135
Loss: 0.24148367451769964
RMSE train: 0.456619	val: 0.780075	test: 0.760172
MAE train: 0.357553	val: 0.575787	test: 0.586966

Epoch: 136
Loss: 0.22508185782602855
RMSE train: 0.409007	val: 0.749404	test: 0.722966
MAE train: 0.315480	val: 0.550217	test: 0.538294

Epoch: 137
Loss: 0.24104031601122447
RMSE train: 0.387892	val: 0.735854	test: 0.719637
MAE train: 0.301641	val: 0.544294	test: 0.541869

Epoch: 138
Loss: 0.2336153302873884
RMSE train: 0.414704	val: 0.771421	test: 0.745131
MAE train: 0.322162	val: 0.567317	test: 0.564224

Epoch: 139
Loss: 0.24190631083079747
RMSE train: 0.425532	val: 0.781317	test: 0.742326
MAE train: 0.331046	val: 0.573737	test: 0.559824

Epoch: 140
Loss: 0.21826831357819693
RMSE train: 0.404920	val: 0.735960	test: 0.728214
MAE train: 0.316900	val: 0.549998	test: 0.555399

Epoch: 141
Loss: 0.2235988144363676
RMSE train: 0.392888	val: 0.751127	test: 0.722387
MAE train: 0.303541	val: 0.555070	test: 0.549936

Epoch: 142
Loss: 0.22668085460151946
RMSE train: 0.406839	val: 0.752492	test: 0.734363
MAE train: 0.315567	val: 0.558036	test: 0.559406

Epoch: 143
Loss: 0.21925553040845053
RMSE train: 0.403734	val: 0.759046	test: 0.733512
MAE train: 0.312857	val: 0.552729	test: 0.547670
RMSE train: 0.491223	val: 0.773909	test: 0.750094
MAE train: 0.380295	val: 0.579309	test: 0.583646

Epoch: 84
Loss: 0.2901406873549734
RMSE train: 0.473507	val: 0.755099	test: 0.728997
MAE train: 0.362322	val: 0.566100	test: 0.558320

Epoch: 85
Loss: 0.30927661806344986
RMSE train: 0.486606	val: 0.771311	test: 0.745403
MAE train: 0.374444	val: 0.569895	test: 0.578228

Epoch: 86
Loss: 0.29515096119471956
RMSE train: 0.481251	val: 0.795415	test: 0.754359
MAE train: 0.373276	val: 0.590511	test: 0.579983

Epoch: 87
Loss: 0.2834777129547937
RMSE train: 0.479049	val: 0.778269	test: 0.761092
MAE train: 0.370630	val: 0.581776	test: 0.586096

Epoch: 88
Loss: 0.29368661663361956
RMSE train: 0.482319	val: 0.770261	test: 0.739783
MAE train: 0.373338	val: 0.582012	test: 0.573053

Epoch: 89
Loss: 0.2779825819390161
RMSE train: 0.461489	val: 0.754976	test: 0.732920
MAE train: 0.355376	val: 0.569643	test: 0.565824

Epoch: 90
Loss: 0.29284284370286123
RMSE train: 0.481558	val: 0.794100	test: 0.759951
MAE train: 0.373071	val: 0.594151	test: 0.590742

Epoch: 91
Loss: 0.2692079778228487
RMSE train: 0.451097	val: 0.752673	test: 0.725788
MAE train: 0.347603	val: 0.559615	test: 0.548482

Epoch: 92
Loss: 0.2764848215239389
RMSE train: 0.542978	val: 0.827297	test: 0.794250
MAE train: 0.428565	val: 0.619766	test: 0.624480

Epoch: 93
Loss: 0.28441495661224636
RMSE train: 0.464205	val: 0.770480	test: 0.729993
MAE train: 0.358601	val: 0.575696	test: 0.567937

Epoch: 94
Loss: 0.270494995372636
RMSE train: 0.455026	val: 0.762256	test: 0.729500
MAE train: 0.351518	val: 0.568026	test: 0.560241

Epoch: 95
Loss: 0.2752000774656023
RMSE train: 0.507413	val: 0.789419	test: 0.760521
MAE train: 0.397913	val: 0.592735	test: 0.605422

Epoch: 96
Loss: 0.2901830109102385
RMSE train: 0.447722	val: 0.753111	test: 0.737133
MAE train: 0.345892	val: 0.554443	test: 0.562134

Epoch: 97
Loss: 0.272114885704858
RMSE train: 0.480126	val: 0.786636	test: 0.756158
MAE train: 0.374036	val: 0.575858	test: 0.578564

Epoch: 98
Loss: 0.29740723967552185
RMSE train: 0.502255	val: 0.801521	test: 0.770530
MAE train: 0.387915	val: 0.602784	test: 0.604009

Epoch: 99
Loss: 0.2821269908121654
RMSE train: 0.468992	val: 0.756186	test: 0.754914
MAE train: 0.361953	val: 0.562607	test: 0.581486

Epoch: 100
Loss: 0.2836242935487202
RMSE train: 0.457043	val: 0.753588	test: 0.714490
MAE train: 0.352332	val: 0.565903	test: 0.546297

Epoch: 101
Loss: 0.2729777597955295
RMSE train: 0.472160	val: 0.771868	test: 0.745768
MAE train: 0.366264	val: 0.573784	test: 0.576751

Epoch: 102
Loss: 0.2677798814007214
RMSE train: 0.455216	val: 0.755358	test: 0.730639
MAE train: 0.352397	val: 0.565100	test: 0.562681

Epoch: 103
Loss: 0.2708897090383938
RMSE train: 0.446884	val: 0.746928	test: 0.720717
MAE train: 0.341380	val: 0.560019	test: 0.554417

Epoch: 104
Loss: 0.269499982041972
RMSE train: 0.496365	val: 0.804742	test: 0.775051
MAE train: 0.391130	val: 0.598083	test: 0.602744

Epoch: 105
Loss: 0.25944081587450846
RMSE train: 0.456322	val: 0.772603	test: 0.759708
MAE train: 0.355747	val: 0.571195	test: 0.576814

Epoch: 106
Loss: 0.2731258560504232
RMSE train: 0.446059	val: 0.748036	test: 0.726949
MAE train: 0.344252	val: 0.559810	test: 0.551947

Epoch: 107
Loss: 0.25575379601546694
RMSE train: 0.497213	val: 0.783126	test: 0.759691
MAE train: 0.386970	val: 0.583669	test: 0.592586

Epoch: 108
Loss: 0.25191841061626163
RMSE train: 0.424947	val: 0.716370	test: 0.707475
MAE train: 0.327170	val: 0.541559	test: 0.539672

Epoch: 109
Loss: 0.24957832161869323
RMSE train: 0.439782	val: 0.745492	test: 0.729984
MAE train: 0.341154	val: 0.552360	test: 0.559423

Epoch: 110
Loss: 0.24744702236992971
RMSE train: 0.468118	val: 0.764587	test: 0.748864
MAE train: 0.358857	val: 0.572620	test: 0.581011

Epoch: 111
Loss: 0.24517803639173508
RMSE train: 0.452737	val: 0.784945	test: 0.756579
MAE train: 0.350570	val: 0.582087	test: 0.583682

Epoch: 112
Loss: 0.26542133199317114
RMSE train: 0.437223	val: 0.770860	test: 0.742209
MAE train: 0.339250	val: 0.570602	test: 0.566610

Epoch: 113
Loss: 0.2501489690371922
RMSE train: 0.461484	val: 0.765641	test: 0.741277
MAE train: 0.361048	val: 0.577897	test: 0.578449

Epoch: 114
Loss: 0.2626444867679051
RMSE train: 0.433992	val: 0.760319	test: 0.718435
MAE train: 0.334742	val: 0.572145	test: 0.549667

Epoch: 115
Loss: 0.253114572593144
RMSE train: 0.463746	val: 0.766633	test: 0.734254
MAE train: 0.361481	val: 0.578190	test: 0.570107

Epoch: 116
Loss: 0.2551380374601909
RMSE train: 0.445428	val: 0.767449	test: 0.745586
MAE train: 0.346772	val: 0.569600	test: 0.571737

Epoch: 117
Loss: 0.2559341809579304
RMSE train: 0.528049	val: 0.844892	test: 0.793645
MAE train: 0.421247	val: 0.636877	test: 0.622677

Epoch: 118
Loss: 0.28410854403461727
RMSE train: 0.442712	val: 0.745694	test: 0.727108
MAE train: 0.342688	val: 0.553955	test: 0.559837

Epoch: 119
Loss: 0.2510380297899246
RMSE train: 0.438708	val: 0.747100	test: 0.722353
MAE train: 0.339015	val: 0.551038	test: 0.562502

Epoch: 120
Loss: 0.24543337098189763
RMSE train: 0.433629	val: 0.740429	test: 0.724968
MAE train: 0.336868	val: 0.555216	test: 0.562583

Epoch: 121
Loss: 0.25611366970198496
RMSE train: 0.443601	val: 0.745973	test: 0.728907
MAE train: 0.340891	val: 0.559456	test: 0.565410

Epoch: 122
Loss: 0.24523207119532994
RMSE train: 0.433712	val: 0.750120	test: 0.733624
MAE train: 0.334802	val: 0.560911	test: 0.569010

Epoch: 123
Loss: 0.24540447763034276
RMSE train: 0.419908	val: 0.743398	test: 0.716262
MAE train: 0.323560	val: 0.560499	test: 0.551632

Epoch: 124
Loss: 0.2555557829993112
RMSE train: 0.443084	val: 0.763589	test: 0.740518
MAE train: 0.344615	val: 0.567285	test: 0.567778

Epoch: 125
Loss: 0.2389018333383969
RMSE train: 0.460038	val: 0.783304	test: 0.746125
MAE train: 0.360851	val: 0.584840	test: 0.586813

Epoch: 126
Loss: 0.23758037388324738
RMSE train: 0.451107	val: 0.756928	test: 0.730328
MAE train: 0.349773	val: 0.567150	test: 0.569110

Epoch: 127
Loss: 0.24635158692087447
RMSE train: 0.425581	val: 0.760806	test: 0.728035
MAE train: 0.328921	val: 0.565792	test: 0.559105

Epoch: 128
Loss: 0.24188236253602163
RMSE train: 0.430682	val: 0.751993	test: 0.731480
MAE train: 0.333755	val: 0.560458	test: 0.569912

Epoch: 129
Loss: 0.25887959556920187
RMSE train: 0.440213	val: 0.739238	test: 0.727013
MAE train: 0.338643	val: 0.559655	test: 0.552317

Epoch: 130
Loss: 0.24617988296917506
RMSE train: 0.502419	val: 0.818036	test: 0.784360
MAE train: 0.397828	val: 0.617235	test: 0.618555

Epoch: 131
Loss: 0.23391296075923101
RMSE train: 0.415405	val: 0.742360	test: 0.727517
MAE train: 0.320306	val: 0.554190	test: 0.548795

Epoch: 132
Loss: 0.22560217071856772
RMSE train: 0.407951	val: 0.748120	test: 0.720008
MAE train: 0.314220	val: 0.559128	test: 0.547137

Epoch: 133
Loss: 0.22421901992389134
RMSE train: 0.431947	val: 0.744945	test: 0.723609
MAE train: 0.334146	val: 0.556302	test: 0.554194

Epoch: 134
Loss: 0.23956776303904398
RMSE train: 0.456983	val: 0.762180	test: 0.728180
MAE train: 0.354843	val: 0.568720	test: 0.568037

Epoch: 135
Loss: 0.23447870995317185
RMSE train: 0.428935	val: 0.751100	test: 0.727532
MAE train: 0.332856	val: 0.560039	test: 0.564024

Epoch: 136
Loss: 0.23971482047012874
RMSE train: 0.443538	val: 0.754569	test: 0.727952
MAE train: 0.346078	val: 0.562502	test: 0.565018

Epoch: 137
Loss: 0.23152246751955577
RMSE train: 0.435397	val: 0.766988	test: 0.728270
MAE train: 0.337916	val: 0.579774	test: 0.565746

Epoch: 138
Loss: 0.23303198175770895
RMSE train: 0.417171	val: 0.752427	test: 0.719662
MAE train: 0.324098	val: 0.564402	test: 0.549392

Epoch: 139
Loss: 0.2528678689684187
RMSE train: 0.403752	val: 0.737744	test: 0.706952
MAE train: 0.311615	val: 0.556373	test: 0.539823

Epoch: 140
Loss: 0.23615026154688426
RMSE train: 0.462891	val: 0.770321	test: 0.728247
MAE train: 0.363425	val: 0.578147	test: 0.571138

Epoch: 141
Loss: 0.21717507605041778
RMSE train: 0.435923	val: 0.766088	test: 0.729994
MAE train: 0.341180	val: 0.568915	test: 0.568828

Epoch: 142
Loss: 0.21788768896034785
RMSE train: 0.434497	val: 0.750086	test: 0.741357
MAE train: 0.335416	val: 0.565849	test: 0.576233

Epoch: 143
Loss: 0.23194801594529832
RMSE train: 0.425024	val: 0.745972	test: 0.711722
MAE train: 0.326359	val: 0.552903	test: 0.547177

Early stopping
Best (RMSE):	 train: 0.424947	val: 0.716370	test: 0.707475
Best (MAE):	 train: 0.327170	val: 0.541559	test: 0.539672

MAE train: 0.334855	val: 0.621742	test: 0.620586

Epoch: 83
Loss: 0.23706122594220297
RMSE train: 0.370600	val: 0.778434	test: 0.770692
MAE train: 0.289902	val: 0.602769	test: 0.607652

Epoch: 84
Loss: 0.24852113319294794
RMSE train: 0.419009	val: 0.787675	test: 0.818397
MAE train: 0.329936	val: 0.603449	test: 0.648661

Epoch: 85
Loss: 0.23599259768213546
RMSE train: 0.375070	val: 0.783608	test: 0.764722
MAE train: 0.293286	val: 0.613266	test: 0.600853

Epoch: 86
Loss: 0.22572381900889532
RMSE train: 0.354471	val: 0.780462	test: 0.768907
MAE train: 0.279268	val: 0.599947	test: 0.601043

Epoch: 87
Loss: 0.23422572229589736
RMSE train: 0.368336	val: 0.785822	test: 0.785067
MAE train: 0.285882	val: 0.606920	test: 0.622876

Epoch: 88
Loss: 0.2129372081586293
RMSE train: 0.412111	val: 0.809932	test: 0.821090
MAE train: 0.322846	val: 0.622227	test: 0.647416

Epoch: 89
Loss: 0.2193712911435536
RMSE train: 0.450692	val: 0.797999	test: 0.803887
MAE train: 0.360413	val: 0.617009	test: 0.635059

Epoch: 90
Loss: 0.22016023631606782
RMSE train: 0.381007	val: 0.774214	test: 0.791392
MAE train: 0.299733	val: 0.597626	test: 0.622285

Epoch: 91
Loss: 0.20849186714206422
RMSE train: 0.343280	val: 0.774740	test: 0.774070
MAE train: 0.268354	val: 0.602336	test: 0.609013

Epoch: 92
Loss: 0.2045344582625798
RMSE train: 0.343017	val: 0.774199	test: 0.755792
MAE train: 0.268047	val: 0.594475	test: 0.599279

Epoch: 93
Loss: 0.20995975924389704
RMSE train: 0.349521	val: 0.771517	test: 0.797237
MAE train: 0.274227	val: 0.590309	test: 0.629899

Epoch: 94
Loss: 0.2091662883758545
RMSE train: 0.331482	val: 0.776598	test: 0.782013
MAE train: 0.260727	val: 0.592307	test: 0.615402

Epoch: 95
Loss: 0.20857715074505126
RMSE train: 0.341934	val: 0.772038	test: 0.780037
MAE train: 0.267946	val: 0.597013	test: 0.617165

Epoch: 96
Loss: 0.21289583614894322
RMSE train: 0.353488	val: 0.781767	test: 0.802701
MAE train: 0.276901	val: 0.604663	test: 0.630337

Epoch: 97
Loss: 0.20894680065768106
RMSE train: 0.394196	val: 0.844164	test: 0.819544
MAE train: 0.307497	val: 0.642162	test: 0.662340

Epoch: 98
Loss: 0.21066713758877345
RMSE train: 0.398678	val: 0.792635	test: 0.808478
MAE train: 0.313138	val: 0.617748	test: 0.637499

Epoch: 99
Loss: 0.20447577642542974
RMSE train: 0.409850	val: 0.829249	test: 0.804844
MAE train: 0.331817	val: 0.632662	test: 0.639732

Epoch: 100
Loss: 0.20659357522215163
RMSE train: 0.361344	val: 0.788433	test: 0.796921
MAE train: 0.288509	val: 0.600597	test: 0.630493

Epoch: 101
Loss: 0.18437773840767996
RMSE train: 0.352669	val: 0.797037	test: 0.786150
MAE train: 0.277513	val: 0.613910	test: 0.623127

Epoch: 102
Loss: 0.188847582255091
RMSE train: 0.392874	val: 0.799714	test: 0.796878
MAE train: 0.306753	val: 0.617750	test: 0.621113

Epoch: 103
Loss: 0.19199726837021963
RMSE train: 0.346747	val: 0.777726	test: 0.786718
MAE train: 0.271847	val: 0.614721	test: 0.616422

Epoch: 104
Loss: 0.19441003884587968
RMSE train: 0.378385	val: 0.815559	test: 0.822454
MAE train: 0.303090	val: 0.622049	test: 0.649801

Epoch: 105
Loss: 0.19237204960414342
RMSE train: 0.372658	val: 0.798518	test: 0.815382
MAE train: 0.291963	val: 0.616127	test: 0.635471

Epoch: 106
Loss: 0.18802635265248163
RMSE train: 0.309384	val: 0.788233	test: 0.808687
MAE train: 0.243006	val: 0.603775	test: 0.635254

Epoch: 107
Loss: 0.201101439339774
RMSE train: 0.318310	val: 0.777828	test: 0.784262
MAE train: 0.251084	val: 0.600586	test: 0.619168

Epoch: 108
Loss: 0.17552583877529418
RMSE train: 0.363998	val: 0.800741	test: 0.786516
MAE train: 0.290511	val: 0.617637	test: 0.619765

Epoch: 109
Loss: 0.1716004769716944
RMSE train: 0.305625	val: 0.780654	test: 0.787075
MAE train: 0.239472	val: 0.597605	test: 0.617215

Epoch: 110
Loss: 0.18058336206844874
RMSE train: 0.326379	val: 0.804873	test: 0.776415
MAE train: 0.256798	val: 0.616586	test: 0.615465

Epoch: 111
Loss: 0.1592478049652917
RMSE train: 0.312168	val: 0.779366	test: 0.797830
MAE train: 0.245085	val: 0.602902	test: 0.627858

Epoch: 112
Loss: 0.1648928022810391
RMSE train: 0.321797	val: 0.780359	test: 0.795264
MAE train: 0.252590	val: 0.603279	test: 0.622352

Epoch: 113
Loss: 0.17019898231540406
RMSE train: 0.315378	val: 0.795612	test: 0.800381
MAE train: 0.248794	val: 0.607715	test: 0.628256

Epoch: 114
Loss: 0.17185238216604506
RMSE train: 0.291046	val: 0.765004	test: 0.768865
MAE train: 0.227189	val: 0.596543	test: 0.605514

Epoch: 115
Loss: 0.16397104838064738
RMSE train: 0.338395	val: 0.796685	test: 0.764483
MAE train: 0.264892	val: 0.614062	test: 0.607289

Epoch: 116
Loss: 0.17373264048780715
RMSE train: 0.351728	val: 0.811468	test: 0.826457
MAE train: 0.277044	val: 0.613361	test: 0.648820

Epoch: 117
Loss: 0.16620572124208724
RMSE train: 0.299357	val: 0.767499	test: 0.784747
MAE train: 0.235415	val: 0.590230	test: 0.613922

Epoch: 118
Loss: 0.1590696413602148
RMSE train: 0.370231	val: 0.804241	test: 0.816972
MAE train: 0.294288	val: 0.611991	test: 0.641926

Epoch: 119
Loss: 0.15491921880415507
RMSE train: 0.290853	val: 0.783688	test: 0.785276
MAE train: 0.226021	val: 0.603435	test: 0.620565

Epoch: 120
Loss: 0.17147590539285115
RMSE train: 0.300102	val: 0.797226	test: 0.790371
MAE train: 0.236165	val: 0.607687	test: 0.622889

Epoch: 121
Loss: 0.15440078399011067
RMSE train: 0.328580	val: 0.795449	test: 0.802458
MAE train: 0.259773	val: 0.612247	test: 0.628556

Epoch: 122
Loss: 0.1547859630414418
RMSE train: 0.284800	val: 0.785426	test: 0.800197
MAE train: 0.221925	val: 0.604854	test: 0.627007

Epoch: 123
Loss: 0.15284673390643938
RMSE train: 0.376418	val: 0.823299	test: 0.822921
MAE train: 0.305962	val: 0.627849	test: 0.641144

Epoch: 124
Loss: 0.15498708614281245
RMSE train: 0.325497	val: 0.815188	test: 0.805136
MAE train: 0.258718	val: 0.624358	test: 0.637670

Epoch: 125
Loss: 0.17761520083461488
RMSE train: 0.295089	val: 0.785880	test: 0.803995
MAE train: 0.232586	val: 0.611553	test: 0.627579

Epoch: 126
Loss: 0.17189373821020126
RMSE train: 0.322107	val: 0.802887	test: 0.788079
MAE train: 0.251502	val: 0.618631	test: 0.618781

Epoch: 127
Loss: 0.14916670322418213
RMSE train: 0.444556	val: 0.828246	test: 0.817158
MAE train: 0.373589	val: 0.631454	test: 0.645306

Epoch: 128
Loss: 0.15272494086197444
RMSE train: 0.320803	val: 0.792718	test: 0.794917
MAE train: 0.253302	val: 0.609205	test: 0.629227

Epoch: 129
Loss: 0.15633546827094896
RMSE train: 0.273427	val: 0.789478	test: 0.777066
MAE train: 0.214196	val: 0.605765	test: 0.610977

Epoch: 130
Loss: 0.14344946880425727
RMSE train: 0.315245	val: 0.776433	test: 0.798092
MAE train: 0.250349	val: 0.602577	test: 0.629968

Epoch: 131
Loss: 0.15246661912117684
RMSE train: 0.390620	val: 0.837443	test: 0.819131
MAE train: 0.318419	val: 0.640040	test: 0.644644

Epoch: 132
Loss: 0.14457082322665624
RMSE train: 0.299932	val: 0.796538	test: 0.793295
MAE train: 0.233879	val: 0.610548	test: 0.623222

Epoch: 133
Loss: 0.15089146420359612
RMSE train: 0.308327	val: 0.812257	test: 0.806773
MAE train: 0.242844	val: 0.623652	test: 0.639485

Epoch: 134
Loss: 0.1486596997295107
RMSE train: 0.284650	val: 0.797170	test: 0.795938
MAE train: 0.223700	val: 0.617095	test: 0.627970

Epoch: 135
Loss: 0.14960596710443497
RMSE train: 0.279440	val: 0.796797	test: 0.784347
MAE train: 0.217785	val: 0.612063	test: 0.621120

Epoch: 136
Loss: 0.13745005482009479
RMSE train: 0.288917	val: 0.787076	test: 0.786437
MAE train: 0.226213	val: 0.610447	test: 0.616252

Epoch: 137
Loss: 0.15320672201258795
RMSE train: 0.275162	val: 0.788342	test: 0.782235
MAE train: 0.215317	val: 0.607679	test: 0.620432

Epoch: 138
Loss: 0.13409647824508802
RMSE train: 0.292447	val: 0.801636	test: 0.789107
MAE train: 0.233598	val: 0.621758	test: 0.617975

Epoch: 139
Loss: 0.13255128104771888
RMSE train: 0.242842	val: 0.792168	test: 0.785540
MAE train: 0.189965	val: 0.608458	test: 0.615262

Epoch: 140
Loss: 0.1268482846873147
RMSE train: 0.328547	val: 0.798701	test: 0.808584
MAE train: 0.261831	val: 0.623173	test: 0.633315

Epoch: 141
Loss: 0.15543475481016295
RMSE train: 0.368507	val: 0.838385	test: 0.826904
MAE train: 0.289060	val: 0.644608	test: 0.654916

Epoch: 142
Loss: 0.14054071424262865
RMSE train: 0.358581	val: 0.812176	test: 0.804153
MAE train: 0.288338	val: 0.618027	test: 0.630533
MAE train: 0.338980	val: 0.674023	test: 0.648298

Epoch: 83
Loss: 0.24441089161804744
RMSE train: 0.377791	val: 0.822770	test: 0.787622
MAE train: 0.297377	val: 0.632215	test: 0.603185

Epoch: 84
Loss: 0.2310543464762824
RMSE train: 0.386923	val: 0.839403	test: 0.792378
MAE train: 0.303692	val: 0.661253	test: 0.607117

Epoch: 85
Loss: 0.23005445088659013
RMSE train: 0.363192	val: 0.861461	test: 0.832254
MAE train: 0.286549	val: 0.663473	test: 0.646259

Epoch: 86
Loss: 0.23261281422206334
RMSE train: 0.427292	val: 0.875590	test: 0.818229
MAE train: 0.335887	val: 0.680011	test: 0.632531

Epoch: 87
Loss: 0.2142896471279008
RMSE train: 0.405544	val: 0.873932	test: 0.839776
MAE train: 0.318961	val: 0.671109	test: 0.659281

Epoch: 88
Loss: 0.20896843182189123
RMSE train: 0.360305	val: 0.825634	test: 0.784318
MAE train: 0.283224	val: 0.637094	test: 0.605451

Epoch: 89
Loss: 0.20622642125402177
RMSE train: 0.379458	val: 0.870179	test: 0.819591
MAE train: 0.301966	val: 0.667712	test: 0.632308

Epoch: 90
Loss: 0.21538342854806355
RMSE train: 0.378050	val: 0.835557	test: 0.803659
MAE train: 0.298700	val: 0.644009	test: 0.625264

Epoch: 91
Loss: 0.2018224916287831
RMSE train: 0.367491	val: 0.819097	test: 0.795961
MAE train: 0.290150	val: 0.637926	test: 0.616685

Epoch: 92
Loss: 0.20592074415513448
RMSE train: 0.354803	val: 0.824868	test: 0.780376
MAE train: 0.281087	val: 0.643374	test: 0.600503

Epoch: 93
Loss: 0.2006454116531781
RMSE train: 0.339060	val: 0.839768	test: 0.800036
MAE train: 0.266468	val: 0.652194	test: 0.622883

Epoch: 94
Loss: 0.2078543965305601
RMSE train: 0.389125	val: 0.871797	test: 0.824573
MAE train: 0.309088	val: 0.671867	test: 0.638880

Epoch: 95
Loss: 0.19871804543903895
RMSE train: 0.390246	val: 0.849069	test: 0.801254
MAE train: 0.308340	val: 0.651670	test: 0.624072

Epoch: 96
Loss: 0.20037560697112763
RMSE train: 0.357311	val: 0.808458	test: 0.786249
MAE train: 0.283077	val: 0.633067	test: 0.606122

Epoch: 97
Loss: 0.19253776435341155
RMSE train: 0.386927	val: 0.876261	test: 0.834801
MAE train: 0.307501	val: 0.676592	test: 0.645307

Epoch: 98
Loss: 0.18482282012701035
RMSE train: 0.381058	val: 0.878055	test: 0.832574
MAE train: 0.305005	val: 0.675602	test: 0.652327

Epoch: 99
Loss: 0.2095167264342308
RMSE train: 0.336599	val: 0.824707	test: 0.791405
MAE train: 0.263700	val: 0.640960	test: 0.615178

Epoch: 100
Loss: 0.19680213289601461
RMSE train: 0.330337	val: 0.817376	test: 0.775301
MAE train: 0.261049	val: 0.635073	test: 0.603254

Epoch: 101
Loss: 0.2058007067867688
RMSE train: 0.330989	val: 0.827380	test: 0.784979
MAE train: 0.260411	val: 0.638746	test: 0.608407

Epoch: 102
Loss: 0.19038627190249308
RMSE train: 0.394966	val: 0.874454	test: 0.828232
MAE train: 0.313292	val: 0.685270	test: 0.643779

Epoch: 103
Loss: 0.18375357879059656
RMSE train: 0.372165	val: 0.876469	test: 0.831765
MAE train: 0.294221	val: 0.678125	test: 0.650410

Epoch: 104
Loss: 0.1815374078495162
RMSE train: 0.363129	val: 0.852477	test: 0.812604
MAE train: 0.288831	val: 0.662712	test: 0.625861

Epoch: 105
Loss: 0.1737812544618334
RMSE train: 0.317376	val: 0.832350	test: 0.789450
MAE train: 0.250471	val: 0.638459	test: 0.611825

Epoch: 106
Loss: 0.16344913627420152
RMSE train: 0.355902	val: 0.844237	test: 0.791014
MAE train: 0.283074	val: 0.657130	test: 0.615468

Epoch: 107
Loss: 0.18761242074625833
RMSE train: 0.374170	val: 0.903616	test: 0.848585
MAE train: 0.296722	val: 0.692507	test: 0.655750

Epoch: 108
Loss: 0.19102566369942256
RMSE train: 0.341016	val: 0.866008	test: 0.803960
MAE train: 0.269308	val: 0.679845	test: 0.628789

Epoch: 109
Loss: 0.17487307265400887
RMSE train: 0.458088	val: 0.900944	test: 0.863224
MAE train: 0.374184	val: 0.695875	test: 0.679547

Epoch: 110
Loss: 0.17147793620824814
RMSE train: 0.332886	val: 0.833295	test: 0.793543
MAE train: 0.264537	val: 0.630457	test: 0.612471

Epoch: 111
Loss: 0.17403520963021687
RMSE train: 0.300475	val: 0.796521	test: 0.768892
MAE train: 0.237027	val: 0.612602	test: 0.591950

Epoch: 112
Loss: 0.16039024612733296
RMSE train: 0.408851	val: 0.867186	test: 0.823660
MAE train: 0.322771	val: 0.673133	test: 0.643064

Epoch: 113
Loss: 0.14992940319435938
RMSE train: 0.324999	val: 0.837895	test: 0.788745
MAE train: 0.255010	val: 0.647953	test: 0.616118

Epoch: 114
Loss: 0.16031592339277267
RMSE train: 0.355364	val: 0.835837	test: 0.802579
MAE train: 0.283461	val: 0.651985	test: 0.626915

Epoch: 115
Loss: 0.16501163691282272
RMSE train: 0.389824	val: 0.882345	test: 0.840965
MAE train: 0.315153	val: 0.687882	test: 0.658127

Epoch: 116
Loss: 0.15368136071733066
RMSE train: 0.365048	val: 0.808691	test: 0.770318
MAE train: 0.296611	val: 0.631289	test: 0.600759

Epoch: 117
Loss: 0.15368217868464334
RMSE train: 0.292184	val: 0.846139	test: 0.794604
MAE train: 0.231909	val: 0.648239	test: 0.619808

Epoch: 118
Loss: 0.16106765504394258
RMSE train: 0.323363	val: 0.862194	test: 0.821452
MAE train: 0.256433	val: 0.667320	test: 0.635831

Epoch: 119
Loss: 0.16252314618655614
RMSE train: 0.339713	val: 0.835656	test: 0.790327
MAE train: 0.266794	val: 0.644900	test: 0.612044

Epoch: 120
Loss: 0.16007956649575913
RMSE train: 0.276308	val: 0.818543	test: 0.781230
MAE train: 0.217531	val: 0.629068	test: 0.606532

Epoch: 121
Loss: 0.14181840206895555
RMSE train: 0.354386	val: 0.830420	test: 0.780262
MAE train: 0.280223	val: 0.646622	test: 0.606701

Epoch: 122
Loss: 0.15475220339638845
RMSE train: 0.398253	val: 0.874575	test: 0.832573
MAE train: 0.321371	val: 0.679180	test: 0.648966

Epoch: 123
Loss: 0.15023971881185258
RMSE train: 0.298380	val: 0.830454	test: 0.785597
MAE train: 0.232670	val: 0.646136	test: 0.609899

Epoch: 124
Loss: 0.14344428213579313
RMSE train: 0.322914	val: 0.858629	test: 0.800283
MAE train: 0.252978	val: 0.666936	test: 0.626293

Epoch: 125
Loss: 0.1477342721607004
RMSE train: 0.429434	val: 0.943832	test: 0.883358
MAE train: 0.341210	val: 0.731600	test: 0.697198

Epoch: 126
Loss: 0.1440778034073966
RMSE train: 0.389087	val: 0.920842	test: 0.866887
MAE train: 0.309527	val: 0.707499	test: 0.671890

Epoch: 127
Loss: 0.13907484550561225
RMSE train: 0.298488	val: 0.834718	test: 0.791907
MAE train: 0.234986	val: 0.650297	test: 0.616117

Epoch: 128
Loss: 0.14656036400369235
RMSE train: 0.267743	val: 0.846478	test: 0.802375
MAE train: 0.211050	val: 0.652002	test: 0.625611

Epoch: 129
Loss: 0.14007489755749702
RMSE train: 0.346788	val: 0.867668	test: 0.811895
MAE train: 0.274732	val: 0.671881	test: 0.640400

Epoch: 130
Loss: 0.1404268704354763
RMSE train: 0.295224	val: 0.818205	test: 0.780371
MAE train: 0.233827	val: 0.633605	test: 0.605276

Epoch: 131
Loss: 0.14801698071616037
RMSE train: 0.327518	val: 0.866062	test: 0.815618
MAE train: 0.260232	val: 0.668016	test: 0.645809

Epoch: 132
Loss: 0.14957233837672643
RMSE train: 0.310407	val: 0.850436	test: 0.789608
MAE train: 0.244757	val: 0.649262	test: 0.612275

Epoch: 133
Loss: 0.14423748691167151
RMSE train: 0.337614	val: 0.889996	test: 0.849922
MAE train: 0.272149	val: 0.685940	test: 0.666156

Epoch: 134
Loss: 0.14267885897840774
RMSE train: 0.305786	val: 0.849898	test: 0.789018
MAE train: 0.240850	val: 0.666752	test: 0.610093

Epoch: 135
Loss: 0.1379085113959653
RMSE train: 0.307651	val: 0.882616	test: 0.849710
MAE train: 0.245811	val: 0.680927	test: 0.665067

Epoch: 136
Loss: 0.14959222610507691
RMSE train: 0.372493	val: 0.820685	test: 0.793521
MAE train: 0.296830	val: 0.631113	test: 0.625201

Epoch: 137
Loss: 0.14793471566268376
RMSE train: 0.317573	val: 0.848107	test: 0.797790
MAE train: 0.252662	val: 0.653138	test: 0.624567

Epoch: 138
Loss: 0.13281663347567832
RMSE train: 0.282575	val: 0.801265	test: 0.768583
MAE train: 0.224547	val: 0.619464	test: 0.590842

Epoch: 139
Loss: 0.13190881854721478
RMSE train: 0.400765	val: 0.899247	test: 0.846395
MAE train: 0.320947	val: 0.698455	test: 0.661304

Epoch: 140
Loss: 0.1390366724559239
RMSE train: 0.286872	val: 0.816476	test: 0.783138
MAE train: 0.225457	val: 0.636853	test: 0.614286

Epoch: 141
Loss: 0.1337626991527421
RMSE train: 0.418157	val: 0.907077	test: 0.855177
MAE train: 0.346997	val: 0.702985	test: 0.673171

Epoch: 142
Loss: 0.1249163949063846
RMSE train: 0.310511	val: 0.873475	test: 0.832096
MAE train: 0.249791	val: 0.678144	test: 0.646337
MAE train: 0.295476	val: 0.611147	test: 0.592042

Epoch: 83
Loss: 0.22208692665610993
RMSE train: 0.394683	val: 0.793322	test: 0.773655
MAE train: 0.312463	val: 0.605824	test: 0.616482

Epoch: 84
Loss: 0.25128321775368284
RMSE train: 0.378432	val: 0.783792	test: 0.760972
MAE train: 0.297320	val: 0.600002	test: 0.599851

Epoch: 85
Loss: 0.23408125340938568
RMSE train: 0.412567	val: 0.821803	test: 0.801058
MAE train: 0.326062	val: 0.628876	test: 0.640052

Epoch: 86
Loss: 0.23889047439609254
RMSE train: 0.410278	val: 0.812984	test: 0.782857
MAE train: 0.326660	val: 0.632038	test: 0.616937

Epoch: 87
Loss: 0.22072108622108186
RMSE train: 0.419406	val: 0.803045	test: 0.789225
MAE train: 0.332986	val: 0.611542	test: 0.626781

Epoch: 88
Loss: 0.21388084228549684
RMSE train: 0.412705	val: 0.814575	test: 0.766778
MAE train: 0.330767	val: 0.621648	test: 0.610536

Epoch: 89
Loss: 0.2290669626423291
RMSE train: 0.409057	val: 0.815513	test: 0.780289
MAE train: 0.323821	val: 0.626883	test: 0.617562

Epoch: 90
Loss: 0.22735546635729925
RMSE train: 0.400128	val: 0.797369	test: 0.767505
MAE train: 0.318970	val: 0.610574	test: 0.598363

Epoch: 91
Loss: 0.22390927374362946
RMSE train: 0.437690	val: 0.852273	test: 0.801650
MAE train: 0.348293	val: 0.661759	test: 0.629806

Epoch: 92
Loss: 0.21550750306674413
RMSE train: 0.366446	val: 0.789423	test: 0.762214
MAE train: 0.290116	val: 0.602233	test: 0.595688

Epoch: 93
Loss: 0.21538658440113068
RMSE train: 0.383971	val: 0.772188	test: 0.765487
MAE train: 0.304648	val: 0.597265	test: 0.611372

Epoch: 94
Loss: 0.20902704234634126
RMSE train: 0.391006	val: 0.807972	test: 0.780718
MAE train: 0.311258	val: 0.617707	test: 0.620203

Epoch: 95
Loss: 0.19913512894085475
RMSE train: 0.385428	val: 0.803680	test: 0.767384
MAE train: 0.307181	val: 0.615190	test: 0.607906

Epoch: 96
Loss: 0.2238015862447875
RMSE train: 0.371830	val: 0.790691	test: 0.772170
MAE train: 0.297280	val: 0.609287	test: 0.603038

Epoch: 97
Loss: 0.2203218596322196
RMSE train: 0.371395	val: 0.812957	test: 0.777950
MAE train: 0.294661	val: 0.623423	test: 0.613217

Epoch: 98
Loss: 0.2120179533958435
RMSE train: 0.426951	val: 0.820865	test: 0.799867
MAE train: 0.338543	val: 0.635259	test: 0.635529

Epoch: 99
Loss: 0.207451836339065
RMSE train: 0.398507	val: 0.786182	test: 0.798476
MAE train: 0.316273	val: 0.602111	test: 0.648452

Epoch: 100
Loss: 0.19701539086444037
RMSE train: 0.367821	val: 0.796118	test: 0.769252
MAE train: 0.290051	val: 0.616492	test: 0.607680

Epoch: 101
Loss: 0.1907276404755456
RMSE train: 0.367779	val: 0.819141	test: 0.773128
MAE train: 0.292328	val: 0.632120	test: 0.613888

Epoch: 102
Loss: 0.19670164585113525
RMSE train: 0.363725	val: 0.800500	test: 0.773022
MAE train: 0.289292	val: 0.616848	test: 0.617049

Epoch: 103
Loss: 0.18541690920080459
RMSE train: 0.341578	val: 0.777500	test: 0.764273
MAE train: 0.270091	val: 0.600184	test: 0.599760

Epoch: 104
Loss: 0.1911722400358745
RMSE train: 0.318206	val: 0.780718	test: 0.757623
MAE train: 0.249578	val: 0.595983	test: 0.599232

Epoch: 105
Loss: 0.19152287713118962
RMSE train: 0.405173	val: 0.843793	test: 0.801113
MAE train: 0.326905	val: 0.649231	test: 0.632890

Epoch: 106
Loss: 0.19354750961065292
RMSE train: 0.314282	val: 0.775712	test: 0.755833
MAE train: 0.247219	val: 0.593328	test: 0.595416

Epoch: 107
Loss: 0.18261118339640753
RMSE train: 0.327605	val: 0.783757	test: 0.773250
MAE train: 0.257598	val: 0.599347	test: 0.607140

Epoch: 108
Loss: 0.17074552604130336
RMSE train: 0.333545	val: 0.792282	test: 0.773449
MAE train: 0.262902	val: 0.603960	test: 0.616473

Epoch: 109
Loss: 0.17347133372511184
RMSE train: 0.329863	val: 0.777430	test: 0.771686
MAE train: 0.260283	val: 0.597304	test: 0.614891

Epoch: 110
Loss: 0.17846394330263138
RMSE train: 0.404459	val: 0.823036	test: 0.786991
MAE train: 0.328937	val: 0.624813	test: 0.629546

Epoch: 111
Loss: 0.18032644156898772
RMSE train: 0.320150	val: 0.802630	test: 0.764782
MAE train: 0.251623	val: 0.622906	test: 0.592273

Epoch: 112
Loss: 0.19152127738509858
RMSE train: 0.397923	val: 0.778692	test: 0.762879
MAE train: 0.320472	val: 0.593195	test: 0.609359

Epoch: 113
Loss: 0.18109363636800221
RMSE train: 0.322472	val: 0.776873	test: 0.751342
MAE train: 0.253261	val: 0.589219	test: 0.594883

Epoch: 114
Loss: 0.1750924629824502
RMSE train: 0.324132	val: 0.792035	test: 0.771964
MAE train: 0.258085	val: 0.602401	test: 0.605290

Epoch: 115
Loss: 0.1848565169743129
RMSE train: 0.336358	val: 0.811646	test: 0.771249
MAE train: 0.267807	val: 0.628205	test: 0.599777

Epoch: 116
Loss: 0.19107341021299362
RMSE train: 0.371705	val: 0.831812	test: 0.780171
MAE train: 0.297421	val: 0.635839	test: 0.606926

Epoch: 117
Loss: 0.18845866407666886
RMSE train: 0.436597	val: 0.888370	test: 0.834213
MAE train: 0.354369	val: 0.690842	test: 0.665365

Epoch: 118
Loss: 0.17425148508378438
RMSE train: 0.312155	val: 0.783375	test: 0.755559
MAE train: 0.244290	val: 0.609963	test: 0.597223

Epoch: 119
Loss: 0.16796161021505082
RMSE train: 0.336726	val: 0.792090	test: 0.783871
MAE train: 0.267033	val: 0.605301	test: 0.619524

Epoch: 120
Loss: 0.15718594087021692
RMSE train: 0.337634	val: 0.810838	test: 0.778081
MAE train: 0.265196	val: 0.626089	test: 0.617380

Epoch: 121
Loss: 0.16130237707069942
RMSE train: 0.285630	val: 0.786432	test: 0.759697
MAE train: 0.223928	val: 0.605489	test: 0.595517

Epoch: 122
Loss: 0.1935165332896369
RMSE train: 0.283069	val: 0.768484	test: 0.742664
MAE train: 0.222778	val: 0.584600	test: 0.583198

Epoch: 123
Loss: 0.16249898501804896
RMSE train: 0.408120	val: 0.843060	test: 0.811464
MAE train: 0.324058	val: 0.653328	test: 0.641278

Epoch: 124
Loss: 0.18029348977974483
RMSE train: 0.313459	val: 0.789811	test: 0.767527
MAE train: 0.245886	val: 0.602470	test: 0.604751

Epoch: 125
Loss: 0.15638994425535202
RMSE train: 0.340588	val: 0.788897	test: 0.752150
MAE train: 0.269426	val: 0.605771	test: 0.595479

Epoch: 126
Loss: 0.14734710327216557
RMSE train: 0.282339	val: 0.780608	test: 0.756013
MAE train: 0.221532	val: 0.595268	test: 0.598371

Epoch: 127
Loss: 0.14787451390709197
RMSE train: 0.334717	val: 0.807390	test: 0.785558
MAE train: 0.272819	val: 0.620894	test: 0.618284

Epoch: 128
Loss: 0.1460460445710591
RMSE train: 0.341819	val: 0.817315	test: 0.790269
MAE train: 0.268185	val: 0.623170	test: 0.627092

Epoch: 129
Loss: 0.14535571794424737
RMSE train: 0.305943	val: 0.785791	test: 0.758333
MAE train: 0.244081	val: 0.603098	test: 0.592160

Epoch: 130
Loss: 0.14875153132847377
RMSE train: 0.318569	val: 0.803964	test: 0.766226
MAE train: 0.253642	val: 0.610231	test: 0.602199

Epoch: 131
Loss: 0.1641367514218603
RMSE train: 0.304795	val: 0.785645	test: 0.758151
MAE train: 0.241820	val: 0.606674	test: 0.599668

Epoch: 132
Loss: 0.14482524352414267
RMSE train: 0.298412	val: 0.795004	test: 0.758353
MAE train: 0.238890	val: 0.612574	test: 0.587072

Epoch: 133
Loss: 0.14638414712888853
RMSE train: 0.267590	val: 0.786351	test: 0.760357
MAE train: 0.212660	val: 0.599063	test: 0.597797

Epoch: 134
Loss: 0.14578689634799957
RMSE train: 0.280221	val: 0.782720	test: 0.744416
MAE train: 0.222180	val: 0.597123	test: 0.590418

Epoch: 135
Loss: 0.14328692853450775
RMSE train: 0.352208	val: 0.835799	test: 0.789147
MAE train: 0.281926	val: 0.633116	test: 0.626325

Epoch: 136
Loss: 0.15324831753969193
RMSE train: 0.299546	val: 0.784415	test: 0.770641
MAE train: 0.238271	val: 0.597580	test: 0.609614

Epoch: 137
Loss: 0.13812491723469325
RMSE train: 0.290239	val: 0.807657	test: 0.760678
MAE train: 0.231101	val: 0.615730	test: 0.597368

Epoch: 138
Loss: 0.13264143307294166
RMSE train: 0.283730	val: 0.781900	test: 0.750769
MAE train: 0.224330	val: 0.598910	test: 0.600093

Epoch: 139
Loss: 0.13389566860028676
RMSE train: 0.276549	val: 0.777580	test: 0.761945
MAE train: 0.220038	val: 0.598153	test: 0.603440

Epoch: 140
Loss: 0.1391571526016508
RMSE train: 0.354373	val: 0.835005	test: 0.788551
MAE train: 0.284432	val: 0.643016	test: 0.627547

Epoch: 141
Loss: 0.1384207872407777
RMSE train: 0.266189	val: 0.781554	test: 0.750883
MAE train: 0.212109	val: 0.595878	test: 0.586933

Epoch: 142
Loss: 0.1239480727485248
RMSE train: 0.256920	val: 0.773102	test: 0.745762
MAE train: 0.202924	val: 0.592724	test: 0.594801
MAE train: 0.425117	val: 0.819319	test: 0.765246

Epoch: 83
Loss: 0.22289858971323287
RMSE train: 0.346971	val: 0.911853	test: 0.854069
MAE train: 0.274460	val: 0.718840	test: 0.681410

Epoch: 84
Loss: 0.2195880977170808
RMSE train: 0.365759	val: 0.926444	test: 0.848945
MAE train: 0.288539	val: 0.726306	test: 0.669918

Epoch: 85
Loss: 0.21282663302762167
RMSE train: 0.334134	val: 0.883165	test: 0.820104
MAE train: 0.264102	val: 0.689804	test: 0.660265

Epoch: 86
Loss: 0.2061256542801857
RMSE train: 0.360993	val: 0.868820	test: 0.808142
MAE train: 0.285919	val: 0.694683	test: 0.647085

Epoch: 87
Loss: 0.1991148740053177
RMSE train: 0.360288	val: 0.884089	test: 0.840139
MAE train: 0.286795	val: 0.688321	test: 0.675987

Epoch: 88
Loss: 0.2003032692841121
RMSE train: 0.508796	val: 1.060793	test: 0.972395
MAE train: 0.421807	val: 0.833333	test: 0.778295

Epoch: 89
Loss: 0.18894887822014944
RMSE train: 0.367082	val: 0.872607	test: 0.814446
MAE train: 0.292903	val: 0.693553	test: 0.644228

Epoch: 90
Loss: 0.19520920195749827
RMSE train: 0.314859	val: 0.863033	test: 0.809049
MAE train: 0.248899	val: 0.686912	test: 0.646079

Epoch: 91
Loss: 0.19138969587428228
RMSE train: 0.316926	val: 0.933353	test: 0.880121
MAE train: 0.249318	val: 0.730756	test: 0.702311

Epoch: 92
Loss: 0.18866490785564696
RMSE train: 0.329386	val: 0.901165	test: 0.834632
MAE train: 0.261025	val: 0.710750	test: 0.667071

Epoch: 93
Loss: 0.1830119485301631
RMSE train: 0.340692	val: 0.923485	test: 0.856391
MAE train: 0.268862	val: 0.733170	test: 0.678474

Epoch: 94
Loss: 0.18036457470485143
RMSE train: 0.424233	val: 1.005517	test: 0.936704
MAE train: 0.344975	val: 0.788334	test: 0.747687

Epoch: 95
Loss: 0.1711948961019516
RMSE train: 0.355623	val: 0.966094	test: 0.886366
MAE train: 0.283453	val: 0.762147	test: 0.703454

Epoch: 96
Loss: 0.1890334508248738
RMSE train: 0.308393	val: 0.921208	test: 0.863257
MAE train: 0.245133	val: 0.720542	test: 0.686860

Epoch: 97
Loss: 0.17348571866750717
RMSE train: 0.344387	val: 0.920121	test: 0.850359
MAE train: 0.274214	val: 0.718286	test: 0.677154

Epoch: 98
Loss: 0.16694557986089162
RMSE train: 0.332028	val: 0.950538	test: 0.873826
MAE train: 0.264700	val: 0.745949	test: 0.702218

Epoch: 99
Loss: 0.18093795967953546
RMSE train: 0.314642	val: 0.875591	test: 0.825285
MAE train: 0.251093	val: 0.691983	test: 0.661549

Epoch: 100
Loss: 0.16604759065168245
RMSE train: 0.331910	val: 0.868859	test: 0.820784
MAE train: 0.263871	val: 0.693793	test: 0.656800

Epoch: 101
Loss: 0.16695796698331833
RMSE train: 0.273728	val: 0.896961	test: 0.845286
MAE train: 0.217636	val: 0.696149	test: 0.672566

Epoch: 102
Loss: 0.16719447182757513
RMSE train: 0.301229	val: 0.890595	test: 0.847825
MAE train: 0.240649	val: 0.705234	test: 0.678745

Epoch: 103
Loss: 0.16700612966503417
RMSE train: 0.325059	val: 0.932297	test: 0.859125
MAE train: 0.256910	val: 0.727149	test: 0.687979

Epoch: 104
Loss: 0.1734309419989586
RMSE train: 0.341569	val: 0.975507	test: 0.908989
MAE train: 0.271544	val: 0.767543	test: 0.725843

Epoch: 105
Loss: 0.15895754098892212
RMSE train: 0.302376	val: 0.925163	test: 0.849316
MAE train: 0.239663	val: 0.725411	test: 0.684569

Epoch: 106
Loss: 0.15485181233712605
RMSE train: 0.295892	val: 0.882743	test: 0.820261
MAE train: 0.232436	val: 0.698658	test: 0.652222

Epoch: 107
Loss: 0.1555376233799117
RMSE train: 0.377743	val: 1.024962	test: 0.971905
MAE train: 0.305145	val: 0.805269	test: 0.775566

Epoch: 108
Loss: 0.16367601603269577
RMSE train: 0.508069	val: 1.029102	test: 0.941392
MAE train: 0.428036	val: 0.809533	test: 0.750182

Epoch: 109
Loss: 0.14791553946478025
RMSE train: 0.321502	val: 0.885630	test: 0.829196
MAE train: 0.253706	val: 0.703898	test: 0.671382

Epoch: 110
Loss: 0.1496651502592223
RMSE train: 0.279086	val: 0.894483	test: 0.837942
MAE train: 0.220250	val: 0.706551	test: 0.668615

Epoch: 111
Loss: 0.15295991088662828
RMSE train: 0.283829	val: 0.925172	test: 0.860242
MAE train: 0.229121	val: 0.727824	test: 0.689147

Epoch: 112
Loss: 0.14937611030680792
RMSE train: 0.467958	val: 1.069183	test: 0.981186
MAE train: 0.387457	val: 0.838647	test: 0.790268

Epoch: 113
Loss: 0.13888506112354143
RMSE train: 0.297752	val: 0.943455	test: 0.864206
MAE train: 0.236162	val: 0.742062	test: 0.686517

Epoch: 114
Loss: 0.14610852939741953
RMSE train: 0.341682	val: 0.963841	test: 0.900742
MAE train: 0.278500	val: 0.758469	test: 0.718861

Epoch: 115
Loss: 0.1522364690899849
RMSE train: 0.467345	val: 1.063907	test: 0.987566
MAE train: 0.395407	val: 0.840330	test: 0.790527

Epoch: 116
Loss: 0.14301304519176483
RMSE train: 0.276616	val: 0.880927	test: 0.821504
MAE train: 0.216882	val: 0.696452	test: 0.652105

Epoch: 117
Loss: 0.1451712559376444
RMSE train: 0.286632	val: 0.851249	test: 0.807359
MAE train: 0.228578	val: 0.666135	test: 0.645361

Epoch: 118
Loss: 0.1550058690564973
RMSE train: 0.327614	val: 1.025234	test: 0.946757
MAE train: 0.258587	val: 0.801895	test: 0.750868

Epoch: 119
Loss: 0.14432555330651148
RMSE train: 0.315760	val: 0.969293	test: 0.883903
MAE train: 0.250637	val: 0.761154	test: 0.705856

Epoch: 120
Loss: 0.1438012814947537
RMSE train: 0.274386	val: 0.894908	test: 0.826950
MAE train: 0.216034	val: 0.702582	test: 0.658566

Epoch: 121
Loss: 0.13459853348987444
RMSE train: 0.299758	val: 0.939441	test: 0.875744
MAE train: 0.237858	val: 0.737313	test: 0.693872

Epoch: 122
Loss: 0.12652915769389697
RMSE train: 0.277318	val: 0.910778	test: 0.847407
MAE train: 0.219638	val: 0.719937	test: 0.677915

Epoch: 123
Loss: 0.1399290912917682
RMSE train: 0.282756	val: 0.952958	test: 0.874276
MAE train: 0.222441	val: 0.753824	test: 0.696133

Epoch: 124
Loss: 0.12496570125222206
RMSE train: 0.287000	val: 0.919450	test: 0.852209
MAE train: 0.229720	val: 0.718674	test: 0.677132

Epoch: 125
Loss: 0.1342639167393957
RMSE train: 0.351729	val: 1.037437	test: 0.960640
MAE train: 0.282663	val: 0.818521	test: 0.763168

Epoch: 126
Loss: 0.12542161239045008
RMSE train: 0.250862	val: 0.879004	test: 0.822603
MAE train: 0.198099	val: 0.695767	test: 0.654050

Epoch: 127
Loss: 0.1273697204887867
RMSE train: 0.286114	val: 0.881408	test: 0.834885
MAE train: 0.227717	val: 0.698112	test: 0.655866

Epoch: 128
Loss: 0.13273934115256583
RMSE train: 0.258144	val: 0.925010	test: 0.857970
MAE train: 0.202092	val: 0.728143	test: 0.680428

Epoch: 129
Loss: 0.12332262843847275
RMSE train: 0.315330	val: 0.931923	test: 0.867696
MAE train: 0.251855	val: 0.731486	test: 0.684669

Epoch: 130
Loss: 0.13211190487657273
RMSE train: 0.311561	val: 0.983701	test: 0.915203
MAE train: 0.247529	val: 0.768136	test: 0.729422

Epoch: 131
Loss: 0.12921946974737303
RMSE train: 0.381392	val: 0.991514	test: 0.908608
MAE train: 0.304779	val: 0.777207	test: 0.724039

Epoch: 132
Loss: 0.13377257968698228
RMSE train: 0.280958	val: 0.921126	test: 0.864782
MAE train: 0.220484	val: 0.715891	test: 0.692310

Epoch: 133
Loss: 0.13322567247918674
RMSE train: 0.289998	val: 0.872077	test: 0.829238
MAE train: 0.228793	val: 0.691969	test: 0.661709

Epoch: 134
Loss: 0.123045521655253
RMSE train: 0.273848	val: 0.889952	test: 0.841858
MAE train: 0.215708	val: 0.706197	test: 0.670495

Epoch: 135
Loss: 0.12401812896132469
RMSE train: 0.251364	val: 0.914698	test: 0.867331
MAE train: 0.200287	val: 0.716158	test: 0.693664

Epoch: 136
Loss: 0.12277501981173243
RMSE train: 0.356812	val: 0.854828	test: 0.811304
MAE train: 0.288764	val: 0.674588	test: 0.649148

Epoch: 137
Loss: 0.12132323852607182
RMSE train: 0.316785	val: 0.927599	test: 0.853706
MAE train: 0.252919	val: 0.737179	test: 0.678652

Epoch: 138
Loss: 0.11935139926416534
RMSE train: 0.288701	val: 0.967419	test: 0.893753
MAE train: 0.233457	val: 0.758077	test: 0.712475

Epoch: 139
Loss: 0.11454875128609794
RMSE train: 0.267890	val: 0.968778	test: 0.898754
MAE train: 0.209622	val: 0.760678	test: 0.719315

Epoch: 140
Loss: 0.11335745134523936
RMSE train: 0.288219	val: 0.933728	test: 0.860947
MAE train: 0.230507	val: 0.730708	test: 0.690477

Epoch: 141
Loss: 0.10538711824587413
RMSE train: 0.284182	val: 0.971090	test: 0.904757
MAE train: 0.227421	val: 0.764432	test: 0.721783

Epoch: 142
Loss: 0.10524141415953636
RMSE train: 0.268310	val: 0.932883	test: 0.875799
MAE train: 0.214391	val: 0.728810	test: 0.701944
MAE train: 0.288335	val: 0.682669	test: 0.665259

Epoch: 83
Loss: 0.21875629361186708
RMSE train: 0.363822	val: 0.845472	test: 0.820156
MAE train: 0.286408	val: 0.668636	test: 0.647479

Epoch: 84
Loss: 0.2396972562585558
RMSE train: 0.428187	val: 0.865585	test: 0.838434
MAE train: 0.344235	val: 0.670366	test: 0.676542

Epoch: 85
Loss: 0.23504366619246347
RMSE train: 0.523570	val: 0.958623	test: 0.903835
MAE train: 0.425279	val: 0.754127	test: 0.739279

Epoch: 86
Loss: 0.22723774079765593
RMSE train: 0.358362	val: 0.867020	test: 0.828240
MAE train: 0.281879	val: 0.687108	test: 0.656800

Epoch: 87
Loss: 0.20915104768105916
RMSE train: 0.387149	val: 0.861623	test: 0.819166
MAE train: 0.304978	val: 0.676666	test: 0.653567

Epoch: 88
Loss: 0.2135483186159815
RMSE train: 0.316973	val: 0.838265	test: 0.809161
MAE train: 0.249323	val: 0.659749	test: 0.636132

Epoch: 89
Loss: 0.21782082957880838
RMSE train: 0.534711	val: 0.974631	test: 0.937903
MAE train: 0.439494	val: 0.762650	test: 0.753455

Epoch: 90
Loss: 0.2142292239836284
RMSE train: 0.416255	val: 0.869553	test: 0.827339
MAE train: 0.330943	val: 0.687058	test: 0.654467

Epoch: 91
Loss: 0.2058895496385438
RMSE train: 0.379610	val: 0.915855	test: 0.867037
MAE train: 0.304094	val: 0.720912	test: 0.684898

Epoch: 92
Loss: 0.19724781598363603
RMSE train: 0.330318	val: 0.843145	test: 0.805282
MAE train: 0.262673	val: 0.660669	test: 0.640112

Epoch: 93
Loss: 0.21157641602413996
RMSE train: 0.394279	val: 0.884074	test: 0.841519
MAE train: 0.313474	val: 0.697370	test: 0.676553

Epoch: 94
Loss: 0.2139354550412723
RMSE train: 0.338772	val: 0.904938	test: 0.881895
MAE train: 0.269065	val: 0.706549	test: 0.713843

Epoch: 95
Loss: 0.19755882024765015
RMSE train: 0.371788	val: 0.883898	test: 0.847715
MAE train: 0.297300	val: 0.696159	test: 0.681795

Epoch: 96
Loss: 0.20479229305471694
RMSE train: 0.404450	val: 0.931948	test: 0.896881
MAE train: 0.323562	val: 0.725440	test: 0.723597

Epoch: 97
Loss: 0.2027637426342283
RMSE train: 0.335013	val: 0.861580	test: 0.839826
MAE train: 0.265425	val: 0.680995	test: 0.659733

Epoch: 98
Loss: 0.2040716697062765
RMSE train: 0.419919	val: 0.952977	test: 0.910616
MAE train: 0.344614	val: 0.746606	test: 0.741781

Epoch: 99
Loss: 0.1827957843031202
RMSE train: 0.307360	val: 0.853205	test: 0.831128
MAE train: 0.241800	val: 0.670409	test: 0.658540

Epoch: 100
Loss: 0.18011287919112615
RMSE train: 0.356726	val: 0.928453	test: 0.885526
MAE train: 0.285880	val: 0.734287	test: 0.714285

Epoch: 101
Loss: 0.16865200762237822
RMSE train: 0.290046	val: 0.838661	test: 0.816983
MAE train: 0.228716	val: 0.657817	test: 0.642333

Epoch: 102
Loss: 0.1750304667013032
RMSE train: 0.373807	val: 0.949705	test: 0.895810
MAE train: 0.301957	val: 0.750713	test: 0.719276

Epoch: 103
Loss: 0.1755281686782837
RMSE train: 0.316241	val: 0.881345	test: 0.847448
MAE train: 0.249943	val: 0.696639	test: 0.667062

Epoch: 104
Loss: 0.1665161337171282
RMSE train: 0.339836	val: 0.906286	test: 0.871440
MAE train: 0.271353	val: 0.713594	test: 0.693636

Epoch: 105
Loss: 0.17565860279968806
RMSE train: 0.339976	val: 0.901330	test: 0.852104
MAE train: 0.270212	val: 0.706091	test: 0.683676

Epoch: 106
Loss: 0.1787377851349967
RMSE train: 0.263833	val: 0.860472	test: 0.828718
MAE train: 0.207709	val: 0.676818	test: 0.663408

Epoch: 107
Loss: 0.15768390893936157
RMSE train: 0.304138	val: 0.869516	test: 0.842823
MAE train: 0.244044	val: 0.677453	test: 0.675472

Epoch: 108
Loss: 0.15104962246758596
RMSE train: 0.319208	val: 0.917734	test: 0.869378
MAE train: 0.254594	val: 0.714771	test: 0.707832

Epoch: 109
Loss: 0.17927651639495576
RMSE train: 0.265588	val: 0.852099	test: 0.817307
MAE train: 0.209172	val: 0.671060	test: 0.657253

Epoch: 110
Loss: 0.16951023042201996
RMSE train: 0.286711	val: 0.851487	test: 0.815841
MAE train: 0.224244	val: 0.666296	test: 0.650775

Epoch: 111
Loss: 0.16040333147559846
RMSE train: 0.285931	val: 0.890126	test: 0.849810
MAE train: 0.225508	val: 0.693585	test: 0.671242

Epoch: 112
Loss: 0.17464673199823924
RMSE train: 0.338957	val: 0.935099	test: 0.888020
MAE train: 0.274596	val: 0.734787	test: 0.714758

Epoch: 113
Loss: 0.16145906278065272
RMSE train: 0.409797	val: 0.928893	test: 0.883514
MAE train: 0.335217	val: 0.733233	test: 0.714394

Epoch: 114
Loss: 0.15760722596730506
RMSE train: 0.295701	val: 0.892335	test: 0.849806
MAE train: 0.235204	val: 0.696002	test: 0.674804

Epoch: 115
Loss: 0.17343472902263915
RMSE train: 0.285634	val: 0.864666	test: 0.836751
MAE train: 0.227203	val: 0.687929	test: 0.668772

Epoch: 116
Loss: 0.16058366426399776
RMSE train: 0.295056	val: 0.902807	test: 0.863590
MAE train: 0.233880	val: 0.706608	test: 0.689930

Epoch: 117
Loss: 0.1578192732163838
RMSE train: 0.320500	val: 0.919127	test: 0.875898
MAE train: 0.255318	val: 0.720531	test: 0.699310

Epoch: 118
Loss: 0.1486297654254096
RMSE train: 0.251701	val: 0.835902	test: 0.817099
MAE train: 0.197856	val: 0.660683	test: 0.645075

Epoch: 119
Loss: 0.15056868749005453
RMSE train: 0.347101	val: 0.877838	test: 0.839568
MAE train: 0.276510	val: 0.691911	test: 0.674725

Epoch: 120
Loss: 0.14194759353995323
RMSE train: 0.280364	val: 0.848949	test: 0.828990
MAE train: 0.219152	val: 0.668605	test: 0.661316

Epoch: 121
Loss: 0.1451487418796335
RMSE train: 0.264089	val: 0.890847	test: 0.846419
MAE train: 0.210921	val: 0.694471	test: 0.669529

Epoch: 122
Loss: 0.18230933908905303
RMSE train: 0.267564	val: 0.889541	test: 0.841352
MAE train: 0.210788	val: 0.698234	test: 0.671470

Epoch: 123
Loss: 0.14881796655910356
RMSE train: 0.446104	val: 0.947686	test: 0.908227
MAE train: 0.363132	val: 0.743501	test: 0.732881

Epoch: 124
Loss: 0.1514048927596637
RMSE train: 0.245573	val: 0.834393	test: 0.804223
MAE train: 0.192442	val: 0.651941	test: 0.631614

Epoch: 125
Loss: 0.14621346763202123
RMSE train: 0.281367	val: 0.862648	test: 0.847946
MAE train: 0.222218	val: 0.674748	test: 0.674470

Epoch: 126
Loss: 0.14300748226898058
RMSE train: 0.273124	val: 0.843577	test: 0.831703
MAE train: 0.211486	val: 0.658574	test: 0.658172

Epoch: 127
Loss: 0.1457218016896929
RMSE train: 0.290396	val: 0.888695	test: 0.854442
MAE train: 0.228239	val: 0.698054	test: 0.675562

Epoch: 128
Loss: 0.14055647275277547
RMSE train: 0.384933	val: 0.910977	test: 0.851129
MAE train: 0.313995	val: 0.709193	test: 0.682223

Epoch: 129
Loss: 0.13244524598121643
RMSE train: 0.335627	val: 0.899855	test: 0.848433
MAE train: 0.271695	val: 0.702566	test: 0.677070

Epoch: 130
Loss: 0.13351990016443388
RMSE train: 0.261067	val: 0.842849	test: 0.815416
MAE train: 0.209904	val: 0.664562	test: 0.639342

Epoch: 131
Loss: 0.1496871326650892
RMSE train: 0.265413	val: 0.867899	test: 0.825568
MAE train: 0.210489	val: 0.680804	test: 0.655508

Epoch: 132
Loss: 0.13829513319901057
RMSE train: 0.459904	val: 0.955557	test: 0.928909
MAE train: 0.378651	val: 0.744706	test: 0.748745

Epoch: 133
Loss: 0.13745980337262154
RMSE train: 0.254326	val: 0.865905	test: 0.830214
MAE train: 0.200308	val: 0.679010	test: 0.661703

Epoch: 134
Loss: 0.13200934390936578
RMSE train: 0.245947	val: 0.836482	test: 0.814998
MAE train: 0.195665	val: 0.661509	test: 0.645641

Epoch: 135
Loss: 0.12826073435800417
RMSE train: 0.298056	val: 0.876891	test: 0.837527
MAE train: 0.241394	val: 0.684050	test: 0.665820

Epoch: 136
Loss: 0.1326527281531266
RMSE train: 0.233183	val: 0.890650	test: 0.860593
MAE train: 0.183181	val: 0.696431	test: 0.685971

Epoch: 137
Loss: 0.11646563400115285
RMSE train: 0.235179	val: 0.881607	test: 0.846679
MAE train: 0.185679	val: 0.685796	test: 0.672574

Epoch: 138
Loss: 0.1272922812827996
RMSE train: 0.280108	val: 0.935073	test: 0.884491
MAE train: 0.224612	val: 0.734468	test: 0.705507

Epoch: 139
Loss: 0.1309990451804229
RMSE train: 0.283449	val: 0.877393	test: 0.858694
MAE train: 0.221997	val: 0.690442	test: 0.687922

Epoch: 140
Loss: 0.13395356706210546
RMSE train: 0.296317	val: 0.902382	test: 0.856524
MAE train: 0.235776	val: 0.706071	test: 0.685109

Epoch: 141
Loss: 0.12860597829733575
RMSE train: 0.277694	val: 0.857181	test: 0.831468
MAE train: 0.220187	val: 0.670142	test: 0.664196

Epoch: 142
Loss: 0.12807227937238558
RMSE train: 0.222295	val: 0.870188	test: 0.826710
MAE train: 0.174954	val: 0.681022	test: 0.654174

Epoch: 143
Loss: 0.12882985760058677
RMSE train: 0.294257	val: 0.850440	test: 0.791645
MAE train: 0.234601	val: 0.653412	test: 0.614901

Epoch: 144
Loss: 0.13528839126229286
RMSE train: 0.290947	val: 0.867084	test: 0.817588
MAE train: 0.230182	val: 0.667611	test: 0.640992

Epoch: 145
Loss: 0.12716035491653851
RMSE train: 0.308615	val: 0.857234	test: 0.790825
MAE train: 0.242256	val: 0.665564	test: 0.617003

Epoch: 146
Loss: 0.13248138023274286
RMSE train: 0.326554	val: 0.868427	test: 0.812443
MAE train: 0.258139	val: 0.681777	test: 0.632129

Early stopping
Best (RMSE):	 train: 0.300475	val: 0.796521	test: 0.768892
Best (MAE):	 train: 0.237027	val: 0.612602	test: 0.591950


Epoch: 143
Loss: 0.13881895691156387
RMSE train: 0.471398	val: 0.836954	test: 0.823817
MAE train: 0.398655	val: 0.647586	test: 0.650170

Epoch: 144
Loss: 0.12313374291573252
RMSE train: 0.257418	val: 0.776027	test: 0.766342
MAE train: 0.200675	val: 0.602501	test: 0.600427

Epoch: 145
Loss: 0.12632521401558602
RMSE train: 0.261531	val: 0.793678	test: 0.793335
MAE train: 0.206230	val: 0.612838	test: 0.623270

Epoch: 146
Loss: 0.12867681522454535
RMSE train: 0.252407	val: 0.794976	test: 0.787803
MAE train: 0.197444	val: 0.613426	test: 0.617607

Epoch: 147
Loss: 0.13259443853582656
RMSE train: 0.283275	val: 0.821435	test: 0.790264
MAE train: 0.222088	val: 0.629369	test: 0.626327

Epoch: 148
Loss: 0.14124245675546782
RMSE train: 0.308803	val: 0.831454	test: 0.811782
MAE train: 0.241795	val: 0.638033	test: 0.641907

Epoch: 149
Loss: 0.14233697312218802
RMSE train: 0.349070	val: 0.830444	test: 0.827206
MAE train: 0.285429	val: 0.637500	test: 0.649749

Early stopping
Best (RMSE):	 train: 0.291046	val: 0.765004	test: 0.768865
Best (MAE):	 train: 0.227189	val: 0.596543	test: 0.605514


Epoch: 143
Loss: 0.11722684012992042
RMSE train: 0.295615	val: 0.911095	test: 0.847392
MAE train: 0.236854	val: 0.724719	test: 0.684285

Epoch: 144
Loss: 0.11312879355890411
RMSE train: 0.276608	val: 0.956776	test: 0.887078
MAE train: 0.219969	val: 0.745782	test: 0.712781

Epoch: 145
Loss: 0.10661153282438006
RMSE train: 0.348712	val: 1.043841	test: 0.955881
MAE train: 0.285753	val: 0.826412	test: 0.763095

Epoch: 146
Loss: 0.11106272201452937
RMSE train: 0.270418	val: 0.923283	test: 0.848554
MAE train: 0.211790	val: 0.729942	test: 0.674153

Epoch: 147
Loss: 0.11503778504473823
RMSE train: 0.236157	val: 0.923411	test: 0.862332
MAE train: 0.186030	val: 0.729950	test: 0.690753

Epoch: 148
Loss: 0.11943602242640086
RMSE train: 0.250834	val: 0.908291	test: 0.848323
MAE train: 0.199403	val: 0.716417	test: 0.679746

Epoch: 149
Loss: 0.10017604061535426
RMSE train: 0.276700	val: 0.993610	test: 0.906975
MAE train: 0.221565	val: 0.779379	test: 0.721600

Epoch: 150
Loss: 0.10672628560236522
RMSE train: 0.344981	val: 1.030014	test: 0.938405
MAE train: 0.283093	val: 0.812230	test: 0.748930

Epoch: 151
Loss: 0.1022156828216144
RMSE train: 0.267021	val: 0.915074	test: 0.849280
MAE train: 0.211786	val: 0.716638	test: 0.677106

Epoch: 152
Loss: 0.11173001197831971
RMSE train: 0.226983	val: 0.908293	test: 0.841032
MAE train: 0.179240	val: 0.722366	test: 0.664132

Early stopping
Best (RMSE):	 train: 0.286632	val: 0.851249	test: 0.807359
Best (MAE):	 train: 0.228578	val: 0.666135	test: 0.645361


Epoch: 143
Loss: 0.13440495410135814
RMSE train: 0.323319	val: 0.826488	test: 0.780207
MAE train: 0.261385	val: 0.627405	test: 0.620393

Epoch: 144
Loss: 0.14188497034566744
RMSE train: 0.274395	val: 0.776671	test: 0.760245
MAE train: 0.218392	val: 0.601885	test: 0.600639

Epoch: 145
Loss: 0.13356867379375867
RMSE train: 0.310164	val: 0.815004	test: 0.782783
MAE train: 0.249470	val: 0.626111	test: 0.612896

Epoch: 146
Loss: 0.1338840504842145
RMSE train: 0.255040	val: 0.766352	test: 0.742239
MAE train: 0.202734	val: 0.588504	test: 0.585077

Epoch: 147
Loss: 0.1276612292443003
RMSE train: 0.306486	val: 0.798473	test: 0.767313
MAE train: 0.244603	val: 0.618015	test: 0.604549

Epoch: 148
Loss: 0.13738081923552922
RMSE train: 0.339937	val: 0.852515	test: 0.816562
MAE train: 0.273213	val: 0.657957	test: 0.646322

Epoch: 149
Loss: 0.133977827749082
RMSE train: 0.251587	val: 0.768859	test: 0.746645
MAE train: 0.198949	val: 0.592191	test: 0.586521

Epoch: 150
Loss: 0.12549535930156708
RMSE train: 0.285674	val: 0.801028	test: 0.760369
MAE train: 0.226890	val: 0.616321	test: 0.596580

Epoch: 151
Loss: 0.12919987099511282
RMSE train: 0.299622	val: 0.816321	test: 0.769071
MAE train: 0.239953	val: 0.625494	test: 0.607211

Epoch: 152
Loss: 0.12704491295984813
RMSE train: 0.299733	val: 0.805286	test: 0.763712
MAE train: 0.235627	val: 0.619131	test: 0.601975

Epoch: 153
Loss: 0.1314291858247348
RMSE train: 0.308081	val: 0.815915	test: 0.763141
MAE train: 0.248951	val: 0.628591	test: 0.602235

Epoch: 154
Loss: 0.13383233972958156
RMSE train: 0.266357	val: 0.774265	test: 0.755496
MAE train: 0.210187	val: 0.595067	test: 0.598490

Epoch: 155
Loss: 0.1283432229288987
RMSE train: 0.341885	val: 0.834665	test: 0.791152
MAE train: 0.277995	val: 0.647402	test: 0.632055

Epoch: 156
Loss: 0.12867050298622676
RMSE train: 0.267384	val: 0.756142	test: 0.754692
MAE train: 0.213389	val: 0.579580	test: 0.593009

Epoch: 157
Loss: 0.125993334289108
RMSE train: 0.268426	val: 0.786640	test: 0.762439
MAE train: 0.212855	val: 0.603240	test: 0.600681

Epoch: 158
Loss: 0.12499948644212314
RMSE train: 0.392006	val: 0.856796	test: 0.801816
MAE train: 0.329822	val: 0.654440	test: 0.628106

Epoch: 159
Loss: 0.14882067271641322
RMSE train: 0.230959	val: 0.762655	test: 0.747779
MAE train: 0.182872	val: 0.588879	test: 0.590148

Epoch: 160
Loss: 0.11584300175309181
RMSE train: 0.288522	val: 0.797095	test: 0.757265
MAE train: 0.229877	val: 0.613890	test: 0.604277

Epoch: 161
Loss: 0.11033494717308454
RMSE train: 0.248351	val: 0.775737	test: 0.754147
MAE train: 0.196892	val: 0.585460	test: 0.600124

Epoch: 162
Loss: 0.1082565039396286
RMSE train: 0.304157	val: 0.812799	test: 0.785682
MAE train: 0.242691	val: 0.621738	test: 0.627652

Epoch: 163
Loss: 0.11068700732929367
RMSE train: 0.212777	val: 0.765132	test: 0.752280
MAE train: 0.168175	val: 0.586411	test: 0.594154

Epoch: 164
Loss: 0.11568591796926089
RMSE train: 0.247396	val: 0.797139	test: 0.764695
MAE train: 0.195433	val: 0.615544	test: 0.608257

Epoch: 165
Loss: 0.1062160384442125
RMSE train: 0.396835	val: 0.836999	test: 0.811671
MAE train: 0.339690	val: 0.636236	test: 0.640658

Epoch: 166
Loss: 0.10934294547353472
RMSE train: 0.286130	val: 0.796610	test: 0.763071
MAE train: 0.227598	val: 0.610124	test: 0.602543

Epoch: 167
Loss: 0.10508600462760244
RMSE train: 0.302037	val: 0.799943	test: 0.779107
MAE train: 0.243742	val: 0.606896	test: 0.615637

Epoch: 168
Loss: 0.11649431447897639
RMSE train: 0.225585	val: 0.781584	test: 0.748253
MAE train: 0.177267	val: 0.602652	test: 0.586412

Epoch: 169
Loss: 0.10448368213006429
RMSE train: 0.262513	val: 0.777501	test: 0.748513
MAE train: 0.209372	val: 0.597342	test: 0.591991

Epoch: 170
Loss: 0.1278564195547785
RMSE train: 0.319885	val: 0.817174	test: 0.771797
MAE train: 0.259310	val: 0.630006	test: 0.614715

Epoch: 171
Loss: 0.1173248535820416
RMSE train: 0.363410	val: 0.850751	test: 0.809937
MAE train: 0.298116	val: 0.651836	test: 0.647249

Epoch: 172
Loss: 0.12334049705948148
RMSE train: 0.245887	val: 0.789694	test: 0.758901
MAE train: 0.195966	val: 0.606500	test: 0.603736

Epoch: 173
Loss: 0.113322984959398
RMSE train: 0.296013	val: 0.820427	test: 0.766447
MAE train: 0.239033	val: 0.621738	test: 0.608148

Epoch: 174
Loss: 0.11415455171040126
RMSE train: 0.235786	val: 0.785034	test: 0.740483
MAE train: 0.189506	val: 0.601094	test: 0.585530

Epoch: 175
Loss: 0.12376685121229716
RMSE train: 0.249992	val: 0.787149	test: 0.754624
MAE train: 0.197744	val: 0.606405	test: 0.595269

Epoch: 176
Loss: 0.10592292728168624
RMSE train: 0.320984	val: 0.833614	test: 0.785434
MAE train: 0.263814	val: 0.636982	test: 0.618670

Epoch: 177
Loss: 0.09789202042988368
RMSE train: 0.257319	val: 0.811006	test: 0.767646
MAE train: 0.204575	val: 0.621750	test: 0.608384

Epoch: 178
Loss: 0.10775926230209214
RMSE train: 0.216523	val: 0.778765	test: 0.755136
MAE train: 0.171349	val: 0.596131	test: 0.595405

Epoch: 179
Loss: 0.1106356829404831
RMSE train: 0.242167	val: 0.795974	test: 0.763879
MAE train: 0.192825	val: 0.614867	test: 0.597076

Epoch: 180
Loss: 0.10820692937288966
RMSE train: 0.279232	val: 0.805270	test: 0.762419
MAE train: 0.218492	val: 0.615289	test: 0.601923

Epoch: 181
Loss: 0.10539262156401362
RMSE train: 0.220931	val: 0.779402	test: 0.750001
MAE train: 0.173604	val: 0.594851	test: 0.592909

Epoch: 182
Loss: 0.11261375408087458
RMSE train: 0.247740	val: 0.802852	test: 0.762583
MAE train: 0.196787	val: 0.616046	test: 0.605430

Epoch: 183
Loss: 0.10572516119905881
RMSE train: 0.275188	val: 0.798507	test: 0.772447
MAE train: 0.219946	val: 0.610977	test: 0.606896

Epoch: 184
Loss: 0.10621914586850575
RMSE train: 0.297640	val: 0.830899	test: 0.781846
MAE train: 0.240153	val: 0.633697	test: 0.616064

Epoch: 185
Loss: 0.10925916795219694
RMSE train: 0.235983	val: 0.786727	test: 0.753183
MAE train: 0.186573	val: 0.604920	test: 0.593946

Epoch: 186
Loss: 0.10804445136870656
RMSE train: 0.340438	val: 0.823752	test: 0.777906
MAE train: 0.278658	val: 0.629118	test: 0.614635

Epoch: 187
Loss: 0.10663093679717608
RMSE train: 0.220161	val: 0.776705	test: 0.743988
MAE train: 0.173515	val: 0.588853	test: 0.589438

Epoch: 188
Loss: 0.09866665356925555
RMSE train: 0.255569	val: 0.798457	test: 0.772645
MAE train: 0.203239	val: 0.615261	test: 0.613352

Epoch: 189
Loss: 0.09493829416377204
RMSE train: 0.206726	val: 0.776161	test: 0.751329
MAE train: 0.163759	val: 0.598257	test: 0.591739

Epoch: 190
Loss: 0.10204999414937836
RMSE train: 0.237815	val: 0.783179	test: 0.759074
MAE train: 0.187429	val: 0.605401	test: 0.599004

Epoch: 191
Loss: 0.10108526955757823
RMSE train: 0.404419	val: 0.866608	test: 0.804560
MAE train: 0.338339	val: 0.657800	test: 0.639900

Early stopping
Best (RMSE):	 train: 0.267384	val: 0.756142	test: 0.754692
Best (MAE):	 train: 0.213389	val: 0.579580	test: 0.593009
All runs completed.


Epoch: 144
Loss: 0.226762589599405
RMSE train: 0.402464	val: 0.744186	test: 0.723015
MAE train: 0.315292	val: 0.550846	test: 0.547449

Epoch: 145
Loss: 0.21842879269804275
RMSE train: 0.421756	val: 0.741964	test: 0.724730
MAE train: 0.326525	val: 0.551709	test: 0.554336

Epoch: 146
Loss: 0.23392637712614878
RMSE train: 0.395733	val: 0.754171	test: 0.722374
MAE train: 0.307873	val: 0.552744	test: 0.543290

Epoch: 147
Loss: 0.23735812945025309
RMSE train: 0.456106	val: 0.793792	test: 0.761789
MAE train: 0.358585	val: 0.586799	test: 0.582943

Epoch: 148
Loss: 0.21604898678404943
RMSE train: 0.436881	val: 0.779569	test: 0.744994
MAE train: 0.342010	val: 0.574500	test: 0.562061

Epoch: 149
Loss: 0.21373314091137477
RMSE train: 0.396723	val: 0.739030	test: 0.724096
MAE train: 0.306396	val: 0.538803	test: 0.548405

Epoch: 150
Loss: 0.21211701525109156
RMSE train: 0.386384	val: 0.726240	test: 0.704954
MAE train: 0.299509	val: 0.540277	test: 0.534834

Epoch: 151
Loss: 0.2148076742887497
RMSE train: 0.399781	val: 0.752204	test: 0.721555
MAE train: 0.311296	val: 0.559100	test: 0.550848

Epoch: 152
Loss: 0.20656569302082062
RMSE train: 0.383824	val: 0.747197	test: 0.720049
MAE train: 0.296764	val: 0.544300	test: 0.539424

Epoch: 153
Loss: 0.20451165735721588
RMSE train: 0.458025	val: 0.794503	test: 0.780466
MAE train: 0.361857	val: 0.589762	test: 0.599883

Epoch: 154
Loss: 0.20805115465606963
RMSE train: 0.394076	val: 0.744598	test: 0.726097
MAE train: 0.303108	val: 0.553677	test: 0.546693

Epoch: 155
Loss: 0.20928056431668146
RMSE train: 0.397677	val: 0.760128	test: 0.730767
MAE train: 0.305473	val: 0.561937	test: 0.549385

Epoch: 156
Loss: 0.2027199821812766
RMSE train: 0.438286	val: 0.798408	test: 0.771265
MAE train: 0.345864	val: 0.590723	test: 0.583623

Epoch: 157
Loss: 0.21151178436619894
RMSE train: 0.405361	val: 0.769794	test: 0.745482
MAE train: 0.314909	val: 0.564981	test: 0.561404

Epoch: 158
Loss: 0.20494485007865088
RMSE train: 0.385768	val: 0.757373	test: 0.724563
MAE train: 0.298143	val: 0.553551	test: 0.546073

Epoch: 159
Loss: 0.22255296685865947
RMSE train: 0.406174	val: 0.759787	test: 0.728980
MAE train: 0.314784	val: 0.568531	test: 0.550362

Epoch: 160
Loss: 0.2096975019999913
RMSE train: 0.363439	val: 0.731011	test: 0.729733
MAE train: 0.279471	val: 0.540371	test: 0.534539

Epoch: 161
Loss: 0.2092112355998584
RMSE train: 0.423412	val: 0.754611	test: 0.745992
MAE train: 0.329425	val: 0.557515	test: 0.565769

Epoch: 162
Loss: 0.2063692073736872
RMSE train: 0.394474	val: 0.746875	test: 0.741266
MAE train: 0.307615	val: 0.546149	test: 0.551419

Epoch: 163
Loss: 0.20049700140953064
RMSE train: 0.373679	val: 0.739518	test: 0.728892
MAE train: 0.288782	val: 0.537428	test: 0.547569

Epoch: 164
Loss: 0.19522326439619064
RMSE train: 0.384248	val: 0.752350	test: 0.740663
MAE train: 0.297118	val: 0.548989	test: 0.552739

Epoch: 165
Loss: 0.2379618106143815
RMSE train: 0.384306	val: 0.733161	test: 0.731883
MAE train: 0.300711	val: 0.542997	test: 0.557290

Epoch: 166
Loss: 0.1965602817279952
RMSE train: 0.366365	val: 0.736504	test: 0.733289
MAE train: 0.283299	val: 0.532719	test: 0.550393

Epoch: 167
Loss: 0.19798457196780614
RMSE train: 0.371645	val: 0.739021	test: 0.732300
MAE train: 0.288677	val: 0.545936	test: 0.551676

Epoch: 168
Loss: 0.19352444580623082
RMSE train: 0.375226	val: 0.733787	test: 0.722031
MAE train: 0.290438	val: 0.539456	test: 0.545660

Epoch: 169
Loss: 0.2147507763334683
RMSE train: 0.385015	val: 0.741782	test: 0.726846
MAE train: 0.300883	val: 0.550870	test: 0.559751

Epoch: 170
Loss: 0.19810751506260463
RMSE train: 0.388867	val: 0.743601	test: 0.720171
MAE train: 0.303256	val: 0.554834	test: 0.548929

Epoch: 171
Loss: 0.2049499132803508
RMSE train: 0.407952	val: 0.790122	test: 0.766632
MAE train: 0.316952	val: 0.569174	test: 0.584798

Epoch: 172
Loss: 0.21340392636401312
RMSE train: 0.400788	val: 0.756322	test: 0.708909
MAE train: 0.312296	val: 0.561605	test: 0.537507

Epoch: 173
Loss: 0.20170059374400548
RMSE train: 0.419566	val: 0.799161	test: 0.784145
MAE train: 0.331070	val: 0.573035	test: 0.592909

Epoch: 174
Loss: 0.21947396440165384
RMSE train: 0.408570	val: 0.738318	test: 0.738502
MAE train: 0.319192	val: 0.548361	test: 0.565157

Epoch: 175
Loss: 0.2152797899075917
RMSE train: 0.400607	val: 0.757374	test: 0.727386
MAE train: 0.311207	val: 0.551708	test: 0.550486

Epoch: 176
Loss: 0.20202260251556123
RMSE train: 0.448457	val: 0.816443	test: 0.754657
MAE train: 0.349553	val: 0.606257	test: 0.576321

Epoch: 177
Loss: 0.20444896604333604
RMSE train: 0.375368	val: 0.747881	test: 0.732340
MAE train: 0.289047	val: 0.555093	test: 0.541077

Epoch: 178
Loss: 0.20878923577921732
RMSE train: 0.392231	val: 0.763378	test: 0.729911
MAE train: 0.305430	val: 0.568803	test: 0.556550

Epoch: 179
Loss: 0.20074503549507686
RMSE train: 0.385561	val: 0.734670	test: 0.728912
MAE train: 0.300167	val: 0.544767	test: 0.552516

Epoch: 180
Loss: 0.2041147298046521
RMSE train: 0.380822	val: 0.756907	test: 0.735004
MAE train: 0.296522	val: 0.559029	test: 0.549689

Epoch: 181
Loss: 0.20268939754792623
RMSE train: 0.368559	val: 0.735686	test: 0.715244
MAE train: 0.286871	val: 0.541916	test: 0.537649

Epoch: 182
Loss: 0.19700189786297934
RMSE train: 0.409274	val: 0.726912	test: 0.710608
MAE train: 0.323200	val: 0.544079	test: 0.530952

Epoch: 183
Loss: 0.19328622094222478
RMSE train: 0.405459	val: 0.762594	test: 0.748516
MAE train: 0.314320	val: 0.567718	test: 0.559124

Epoch: 184
Loss: 0.18879476029958045
RMSE train: 0.433221	val: 0.784914	test: 0.754781
MAE train: 0.338363	val: 0.585995	test: 0.578791

Epoch: 185
Loss: 0.19676138992820466
RMSE train: 0.382009	val: 0.725252	test: 0.730701
MAE train: 0.295630	val: 0.534040	test: 0.549679

Epoch: 186
Loss: 0.18374823459557124
RMSE train: 0.371940	val: 0.724118	test: 0.722498
MAE train: 0.288393	val: 0.540998	test: 0.550017

Epoch: 187
Loss: 0.19435487048966543
RMSE train: 0.418475	val: 0.790605	test: 0.765574
MAE train: 0.328154	val: 0.575295	test: 0.579053

Epoch: 188
Loss: 0.20060476447854722
RMSE train: 0.388225	val: 0.742955	test: 0.711733
MAE train: 0.299814	val: 0.547472	test: 0.536477

Epoch: 189
Loss: 0.20911391292299544
RMSE train: 0.399709	val: 0.747052	test: 0.726627
MAE train: 0.309127	val: 0.553460	test: 0.560394

Epoch: 190
Loss: 0.19653172684567316
RMSE train: 0.356879	val: 0.732459	test: 0.727912
MAE train: 0.275735	val: 0.537581	test: 0.543017

Epoch: 191
Loss: 0.19984925431864603
RMSE train: 0.365699	val: 0.738473	test: 0.716638
MAE train: 0.283268	val: 0.542242	test: 0.541268

Epoch: 192
Loss: 0.21428834008319037
RMSE train: 0.378140	val: 0.743902	test: 0.720607
MAE train: 0.295310	val: 0.549080	test: 0.545701

Epoch: 193
Loss: 0.19294871389865875
RMSE train: 0.363445	val: 0.754008	test: 0.731821
MAE train: 0.280885	val: 0.550394	test: 0.547816

Epoch: 194
Loss: 0.19054516511304037
RMSE train: 0.391815	val: 0.757549	test: 0.728099
MAE train: 0.302314	val: 0.561622	test: 0.551262

Epoch: 195
Loss: 0.18953304631369455
RMSE train: 0.374925	val: 0.750922	test: 0.728023
MAE train: 0.291083	val: 0.556169	test: 0.546438

Epoch: 196
Loss: 0.18125251467738832
RMSE train: 0.366621	val: 0.730152	test: 0.721397
MAE train: 0.282433	val: 0.532196	test: 0.537494

Epoch: 197
Loss: 0.19298451713153295
RMSE train: 0.378226	val: 0.749729	test: 0.737078
MAE train: 0.295051	val: 0.548868	test: 0.545795

Epoch: 198
Loss: 0.18758844690663473
RMSE train: 0.383910	val: 0.751609	test: 0.715424
MAE train: 0.297828	val: 0.560606	test: 0.542844

Epoch: 199
Loss: 0.1985567467553275
RMSE train: 0.376764	val: 0.726965	test: 0.711426
MAE train: 0.294242	val: 0.546828	test: 0.533732

Epoch: 200
Loss: 0.18462386408022471
RMSE train: 0.378275	val: 0.759352	test: 0.735607
MAE train: 0.291185	val: 0.560630	test: 0.554778

Epoch: 201
Loss: 0.19148795093808854
RMSE train: 0.376001	val: 0.757532	test: 0.731133
MAE train: 0.291922	val: 0.557322	test: 0.547980

Epoch: 202
Loss: 0.18368020866598403
RMSE train: 0.383230	val: 0.772048	test: 0.753215
MAE train: 0.297689	val: 0.561039	test: 0.566820

Epoch: 203
Loss: 0.19253611671073095
RMSE train: 0.370598	val: 0.766591	test: 0.728710
MAE train: 0.287414	val: 0.566248	test: 0.553476

Epoch: 143
Loss: 0.12157949326293808
RMSE train: 0.298000	val: 0.924886	test: 0.904526
MAE train: 0.241024	val: 0.717688	test: 0.728018

Epoch: 144
Loss: 0.12720806151628494
RMSE train: 0.246819	val: 0.837661	test: 0.820432
MAE train: 0.196300	val: 0.658248	test: 0.643689

Epoch: 145
Loss: 0.12012223739709173
RMSE train: 0.255556	val: 0.874387	test: 0.841420
MAE train: 0.201342	val: 0.685167	test: 0.677346

Epoch: 146
Loss: 0.11360228540641922
RMSE train: 0.300334	val: 0.917581	test: 0.875569
MAE train: 0.242867	val: 0.710254	test: 0.703532

Epoch: 147
Loss: 0.11039318836161069
RMSE train: 0.249358	val: 0.880113	test: 0.853480
MAE train: 0.195629	val: 0.689691	test: 0.686717

Epoch: 148
Loss: 0.11183706138815198
RMSE train: 0.232429	val: 0.887775	test: 0.844499
MAE train: 0.183579	val: 0.701519	test: 0.664136

Epoch: 149
Loss: 0.11337754768984658
RMSE train: 0.225410	val: 0.866440	test: 0.829557
MAE train: 0.179166	val: 0.680184	test: 0.658117

Epoch: 150
Loss: 0.11787101413522448
RMSE train: 0.408526	val: 0.961629	test: 0.912070
MAE train: 0.348385	val: 0.750426	test: 0.731805

Epoch: 151
Loss: 0.11105630440371377
RMSE train: 0.316032	val: 0.906652	test: 0.863400
MAE train: 0.253979	val: 0.707596	test: 0.691614

Epoch: 152
Loss: 0.12071140163711139
RMSE train: 0.215610	val: 0.886811	test: 0.855090
MAE train: 0.170159	val: 0.695912	test: 0.684216

Epoch: 153
Loss: 0.11985723940389496
RMSE train: 0.226822	val: 0.861419	test: 0.838891
MAE train: 0.179408	val: 0.677265	test: 0.655873

Epoch: 154
Loss: 0.11805826106241771
RMSE train: 0.336243	val: 0.966726	test: 0.925210
MAE train: 0.277154	val: 0.755761	test: 0.751122

Epoch: 155
Loss: 0.11432393853153501
RMSE train: 0.253282	val: 0.902692	test: 0.858391
MAE train: 0.201703	val: 0.707583	test: 0.688082

Epoch: 156
Loss: 0.1141938359609672
RMSE train: 0.269416	val: 0.849431	test: 0.839231
MAE train: 0.216933	val: 0.671165	test: 0.663303

Epoch: 157
Loss: 0.1158234817641122
RMSE train: 0.244346	val: 0.871962	test: 0.840841
MAE train: 0.193246	val: 0.678882	test: 0.675511

Epoch: 158
Loss: 0.12379993711199079
RMSE train: 0.313566	val: 0.922060	test: 0.884438
MAE train: 0.249960	val: 0.722843	test: 0.711128

Epoch: 159
Loss: 0.14003719495875494
RMSE train: 0.228882	val: 0.829710	test: 0.815912
MAE train: 0.180207	val: 0.648669	test: 0.653414

Epoch: 160
Loss: 0.10476955718227796
RMSE train: 0.305278	val: 0.913223	test: 0.880610
MAE train: 0.249130	val: 0.715828	test: 0.710162

Epoch: 161
Loss: 0.10528644333992686
RMSE train: 0.226681	val: 0.839758	test: 0.827450
MAE train: 0.178896	val: 0.647395	test: 0.658894

Epoch: 162
Loss: 0.10019456861274582
RMSE train: 0.264253	val: 0.860935	test: 0.834299
MAE train: 0.209088	val: 0.678887	test: 0.665951

Epoch: 163
Loss: 0.09638822983418192
RMSE train: 0.290724	val: 0.897931	test: 0.864205
MAE train: 0.235575	val: 0.703418	test: 0.694329

Epoch: 164
Loss: 0.09640092455915042
RMSE train: 0.236704	val: 0.896576	test: 0.851444
MAE train: 0.188379	val: 0.707623	test: 0.682481

Epoch: 165
Loss: 0.09662138138498579
RMSE train: 0.303499	val: 0.881960	test: 0.849945
MAE train: 0.249247	val: 0.684594	test: 0.681430

Epoch: 166
Loss: 0.09574208834341594
RMSE train: 0.213503	val: 0.847197	test: 0.824522
MAE train: 0.170653	val: 0.661631	test: 0.659076

Epoch: 167
Loss: 0.09838773044092315
RMSE train: 0.245471	val: 0.913256	test: 0.872208
MAE train: 0.195868	val: 0.714813	test: 0.704171

Epoch: 168
Loss: 0.10150834705148425
RMSE train: 0.241640	val: 0.902511	test: 0.880695
MAE train: 0.190645	val: 0.703892	test: 0.705567

Epoch: 169
Loss: 0.09604220677699361
RMSE train: 0.233458	val: 0.867146	test: 0.829035
MAE train: 0.185232	val: 0.683058	test: 0.660542

Epoch: 170
Loss: 0.11004951649478503
RMSE train: 0.327268	val: 0.928528	test: 0.881896
MAE train: 0.267673	val: 0.728747	test: 0.705802

Epoch: 171
Loss: 0.09701409776295934
RMSE train: 0.302212	val: 0.966511	test: 0.915862
MAE train: 0.245035	val: 0.754769	test: 0.739735

Epoch: 172
Loss: 0.11165718414953776
RMSE train: 0.196805	val: 0.866150	test: 0.833234
MAE train: 0.155362	val: 0.681090	test: 0.663766

Epoch: 173
Loss: 0.10142767748662404
RMSE train: 0.228560	val: 0.859181	test: 0.835475
MAE train: 0.180133	val: 0.664915	test: 0.674464

Epoch: 174
Loss: 0.10088436358741351
RMSE train: 0.260861	val: 0.896351	test: 0.863914
MAE train: 0.205703	val: 0.700819	test: 0.697116

Epoch: 175
Loss: 0.1049590999526637
RMSE train: 0.305854	val: 0.954797	test: 0.922523
MAE train: 0.253216	val: 0.743303	test: 0.741213

Epoch: 176
Loss: 0.08722809861813273
RMSE train: 0.268197	val: 0.883325	test: 0.855580
MAE train: 0.214996	val: 0.689696	test: 0.686460

Epoch: 177
Loss: 0.08988182832087789
RMSE train: 0.203621	val: 0.869731	test: 0.837159
MAE train: 0.159377	val: 0.682681	test: 0.665936

Epoch: 178
Loss: 0.09744683120931898
RMSE train: 0.215536	val: 0.850989	test: 0.830047
MAE train: 0.172270	val: 0.660410	test: 0.666887

Epoch: 179
Loss: 0.09601141884922981
RMSE train: 0.271679	val: 0.880011	test: 0.838177
MAE train: 0.217077	val: 0.685660	test: 0.675864

Epoch: 180
Loss: 0.10420328325458936
RMSE train: 0.218757	val: 0.885166	test: 0.847487
MAE train: 0.170346	val: 0.690422	test: 0.681522

Epoch: 181
Loss: 0.10274340105908257
RMSE train: 0.258155	val: 0.883500	test: 0.849754
MAE train: 0.207630	val: 0.689855	test: 0.680818

Epoch: 182
Loss: 0.10494882666638919
RMSE train: 0.211661	val: 0.861973	test: 0.836563
MAE train: 0.167254	val: 0.675889	test: 0.673448

Epoch: 183
Loss: 0.09325275304062026
RMSE train: 0.199533	val: 0.862887	test: 0.838702
MAE train: 0.157558	val: 0.673285	test: 0.676275

Epoch: 184
Loss: 0.09557034234915461
RMSE train: 0.194751	val: 0.875990	test: 0.839661
MAE train: 0.153904	val: 0.687899	test: 0.667726

Epoch: 185
Loss: 0.10067469147699219
RMSE train: 0.216341	val: 0.889390	test: 0.857392
MAE train: 0.172250	val: 0.693137	test: 0.688912

Epoch: 186
Loss: 0.09737485860075269
RMSE train: 0.229935	val: 0.894402	test: 0.858062
MAE train: 0.180452	val: 0.701333	test: 0.678266

Epoch: 187
Loss: 0.09156245791486331
RMSE train: 0.242241	val: 0.873307	test: 0.840437
MAE train: 0.193667	val: 0.681314	test: 0.679314

Epoch: 188
Loss: 0.08647981179612023
RMSE train: 0.272102	val: 0.897418	test: 0.867919
MAE train: 0.219616	val: 0.699391	test: 0.697710

Epoch: 189
Loss: 0.08621867001056671
RMSE train: 0.350182	val: 0.937434	test: 0.911414
MAE train: 0.293649	val: 0.725378	test: 0.732846

Epoch: 190
Loss: 0.09025773725339345
RMSE train: 0.207479	val: 0.829262	test: 0.845700
MAE train: 0.164806	val: 0.654389	test: 0.673508

Epoch: 191
Loss: 0.09006930249077934
RMSE train: 0.291956	val: 0.911506	test: 0.871519
MAE train: 0.233049	val: 0.718956	test: 0.700124

Epoch: 192
Loss: 0.09250123053789139
RMSE train: 0.233726	val: 0.865298	test: 0.835915
MAE train: 0.184404	val: 0.681334	test: 0.670307

Epoch: 193
Loss: 0.08877829409071378
RMSE train: 0.221939	val: 0.895162	test: 0.856412
MAE train: 0.175025	val: 0.701712	test: 0.688990

Epoch: 194
Loss: 0.07899335133177894
RMSE train: 0.212527	val: 0.848643	test: 0.829383
MAE train: 0.168527	val: 0.659391	test: 0.665835

Epoch: 195
Loss: 0.08941522879259926
RMSE train: 0.184591	val: 0.879270	test: 0.834043
MAE train: 0.145504	val: 0.689152	test: 0.663359

Epoch: 196
Loss: 0.07979576662182808
RMSE train: 0.220243	val: 0.875733	test: 0.844085
MAE train: 0.174934	val: 0.683014	test: 0.674470

Epoch: 197
Loss: 0.09613327575581414
RMSE train: 0.322363	val: 0.914869	test: 0.871939
MAE train: 0.264669	val: 0.715066	test: 0.701251

Epoch: 198
Loss: 0.08088463651282447
RMSE train: 0.256549	val: 0.885140	test: 0.852467
MAE train: 0.203844	val: 0.694045	test: 0.690195

Epoch: 199
Loss: 0.09503724904997009
RMSE train: 0.198225	val: 0.838271	test: 0.830051
MAE train: 0.155977	val: 0.659255	test: 0.660377

Epoch: 200
Loss: 0.08650140038558415
RMSE train: 0.319126	val: 0.929751	test: 0.878233
MAE train: 0.262176	val: 0.723356	test: 0.708220

Epoch: 201
Loss: 0.08958229582224574
RMSE train: 0.209865	val: 0.850619	test: 0.818272
MAE train: 0.168280	val: 0.663956	test: 0.656883

Epoch: 202
Loss: 0.08143928380949157
RMSE train: 0.209952	val: 0.882466	test: 0.843530
MAE train: 0.165712	val: 0.690922	test: 0.672437

Epoch: 203
Loss: 0.07446659835321563
RMSE train: 0.217391	val: 0.890233	test: 0.853960
MAE train: 0.173162	val: 0.692625	test: 0.685505

Epoch: 204
Loss: 0.08310164670859065
RMSE train: 0.248912	val: 0.898307	test: 0.861409
MAE train: 0.199180	val: 0.709549	test: 0.689218

Epoch: 205
Loss: 0.0789523119372981
RMSE train: 0.223772	val: 0.863779	test: 0.828039
MAE train: 0.178419	val: 0.675631	test: 0.660064

Epoch: 206
Loss: 0.08371854946017265
RMSE train: 0.254779	val: 0.880750	test: 0.839442
MAE train: 0.201721	val: 0.688147	test: 0.680214

Epoch: 207
Loss: 0.08323804448757853
RMSE train: 0.212695	val: 0.869072	test: 0.839989
MAE train: 0.169076	val: 0.673821	test: 0.674044

Epoch: 208
Loss: 0.09945236252886909
RMSE train: 0.269991	val: 0.891026	test: 0.847848
MAE train: 0.224076	val: 0.693210	test: 0.676452

Epoch: 209
Loss: 0.09209833666682243
RMSE train: 0.322992	val: 0.953790	test: 0.916967
MAE train: 0.258347	val: 0.747390	test: 0.739584

Epoch: 210
Loss: 0.08606470855219024
RMSE train: 0.196007	val: 0.874489	test: 0.834322
MAE train: 0.154324	val: 0.682744	test: 0.657785

Epoch: 211
Loss: 0.08956852608493396
RMSE train: 0.298890	val: 0.905892	test: 0.858352
MAE train: 0.241415	val: 0.712332	test: 0.688019

Epoch: 212
Loss: 0.08520544107471194
RMSE train: 0.192283	val: 0.867010	test: 0.829126
MAE train: 0.150733	val: 0.678924	test: 0.665847

Epoch: 213
Loss: 0.08422055787273816
RMSE train: 0.196607	val: 0.869644	test: 0.840498
MAE train: 0.153798	val: 0.685511	test: 0.670672

Epoch: 214
Loss: 0.0752698720565864
RMSE train: 0.178611	val: 0.855165	test: 0.828183
MAE train: 0.139354	val: 0.669048	test: 0.660591

Epoch: 215
Loss: 0.0753214258168425
RMSE train: 0.282802	val: 0.935888	test: 0.882970
MAE train: 0.229671	val: 0.727760	test: 0.709569

Epoch: 216
Loss: 0.08089629401053701
RMSE train: 0.205283	val: 0.869590	test: 0.838429
MAE train: 0.161362	val: 0.685149	test: 0.669842

Epoch: 217
Loss: 0.08802739690457072
RMSE train: 0.226915	val: 0.877096	test: 0.826152
MAE train: 0.177347	val: 0.692612	test: 0.658727

Epoch: 218
Loss: 0.07358182221651077
RMSE train: 0.197806	val: 0.863704	test: 0.836525
MAE train: 0.156474	val: 0.675124	test: 0.673926

Epoch: 219
Loss: 0.07745010618652616
RMSE train: 0.249787	val: 0.910135	test: 0.873173
MAE train: 0.204209	val: 0.710154	test: 0.700215

Epoch: 220
Loss: 0.0798424423805305
RMSE train: 0.200984	val: 0.882867	test: 0.850385
MAE train: 0.158898	val: 0.689016	test: 0.678517

Epoch: 221
Loss: 0.08259340349052634
RMSE train: 0.225383	val: 0.894197	test: 0.849684
MAE train: 0.176293	val: 0.701168	test: 0.680444

Epoch: 222
Loss: 0.1104221221591745
RMSE train: 0.255130	val: 0.883111	test: 0.830999
MAE train: 0.201076	val: 0.695749	test: 0.667782

Epoch: 223
Loss: 0.08935188874602318
RMSE train: 0.318609	val: 0.930107	test: 0.899183
MAE train: 0.251697	val: 0.719711	test: 0.726187

Epoch: 224
Loss: 0.08853787236980029
RMSE train: 0.208986	val: 0.866291	test: 0.829774
MAE train: 0.163332	val: 0.683114	test: 0.660425

Epoch: 225
Loss: 0.08437435754707881
RMSE train: 0.272617	val: 0.913539	test: 0.870267
MAE train: 0.216178	val: 0.704181	test: 0.700012

Early stopping
Best (RMSE):	 train: 0.207479	val: 0.829262	test: 0.845700
Best (MAE):	 train: 0.164806	val: 0.654389	test: 0.673508
All runs completed.


Epoch: 204
Loss: 0.1870565733739308
RMSE train: 0.383867	val: 0.770903	test: 0.739039
MAE train: 0.298865	val: 0.575965	test: 0.556692

Epoch: 205
Loss: 0.1726085575563567
RMSE train: 0.361459	val: 0.715959	test: 0.724784
MAE train: 0.280335	val: 0.533782	test: 0.537116

Epoch: 206
Loss: 0.18402736527579172
RMSE train: 0.366825	val: 0.753022	test: 0.730776
MAE train: 0.282293	val: 0.553171	test: 0.556122

Epoch: 207
Loss: 0.17262133423771178
RMSE train: 0.376827	val: 0.758385	test: 0.726016
MAE train: 0.289829	val: 0.563392	test: 0.551795

Epoch: 208
Loss: 0.17473159836871283
RMSE train: 0.348849	val: 0.725998	test: 0.712039
MAE train: 0.269857	val: 0.541344	test: 0.534570

Epoch: 209
Loss: 0.18511030503681727
RMSE train: 0.358539	val: 0.751051	test: 0.738472
MAE train: 0.277905	val: 0.544775	test: 0.555864

Epoch: 210
Loss: 0.1947260754449027
RMSE train: 0.362921	val: 0.760530	test: 0.736970
MAE train: 0.279807	val: 0.556229	test: 0.544854

Epoch: 211
Loss: 0.1854129582643509
RMSE train: 0.432399	val: 0.801096	test: 0.782462
MAE train: 0.343068	val: 0.582625	test: 0.602138

Epoch: 212
Loss: 0.1808874990258898
RMSE train: 0.365401	val: 0.759793	test: 0.717202
MAE train: 0.281444	val: 0.556316	test: 0.544478

Epoch: 213
Loss: 0.1823838044490133
RMSE train: 0.365936	val: 0.768368	test: 0.748147
MAE train: 0.282120	val: 0.553201	test: 0.558338

Epoch: 214
Loss: 0.18649978297097342
RMSE train: 0.352343	val: 0.734384	test: 0.723698
MAE train: 0.272428	val: 0.532595	test: 0.540217

Epoch: 215
Loss: 0.17776811442204885
RMSE train: 0.360105	val: 0.763261	test: 0.726427
MAE train: 0.277771	val: 0.555035	test: 0.549484

Epoch: 216
Loss: 0.16916531643697194
RMSE train: 0.350479	val: 0.736646	test: 0.718653
MAE train: 0.273239	val: 0.546511	test: 0.544328

Epoch: 217
Loss: 0.19476708556924546
RMSE train: 0.357427	val: 0.756183	test: 0.722618
MAE train: 0.277793	val: 0.556222	test: 0.546512

Epoch: 218
Loss: 0.18305042279618128
RMSE train: 0.383439	val: 0.776068	test: 0.766669
MAE train: 0.299422	val: 0.568117	test: 0.578371

Epoch: 219
Loss: 0.17794605025223323
RMSE train: 0.360135	val: 0.741556	test: 0.726612
MAE train: 0.279155	val: 0.553180	test: 0.557483

Epoch: 220
Loss: 0.17014196940830775
RMSE train: 0.351061	val: 0.738045	test: 0.705592
MAE train: 0.271666	val: 0.548603	test: 0.535098

Epoch: 221
Loss: 0.16751880943775177
RMSE train: 0.360710	val: 0.731249	test: 0.718865
MAE train: 0.280178	val: 0.540967	test: 0.545784

Epoch: 222
Loss: 0.17433519874300277
RMSE train: 0.340158	val: 0.731367	test: 0.711479
MAE train: 0.262397	val: 0.538345	test: 0.537611

Epoch: 223
Loss: 0.16150575556925365
RMSE train: 0.371495	val: 0.742064	test: 0.717401
MAE train: 0.288473	val: 0.551578	test: 0.551299

Epoch: 224
Loss: 0.1852502663220678
RMSE train: 0.350615	val: 0.736266	test: 0.701216
MAE train: 0.271425	val: 0.538918	test: 0.530089

Epoch: 225
Loss: 0.18560126956020082
RMSE train: 0.384637	val: 0.786427	test: 0.758262
MAE train: 0.301377	val: 0.576511	test: 0.574954

Epoch: 226
Loss: 0.18443886829274042
RMSE train: 0.344699	val: 0.739510	test: 0.703518
MAE train: 0.267601	val: 0.555725	test: 0.532235

Epoch: 227
Loss: 0.17203619650432042
RMSE train: 0.407155	val: 0.792768	test: 0.758124
MAE train: 0.319374	val: 0.578868	test: 0.580564

Epoch: 228
Loss: 0.16877837958080427
RMSE train: 0.351344	val: 0.744906	test: 0.708613
MAE train: 0.272719	val: 0.551077	test: 0.538253

Epoch: 229
Loss: 0.16635468282869884
RMSE train: 0.336881	val: 0.727479	test: 0.699951
MAE train: 0.259017	val: 0.542543	test: 0.522625

Epoch: 230
Loss: 0.16722934586661203
RMSE train: 0.390540	val: 0.783658	test: 0.743478
MAE train: 0.305420	val: 0.575023	test: 0.567478

Epoch: 231
Loss: 0.17746886717421667
RMSE train: 0.394383	val: 0.788574	test: 0.721129
MAE train: 0.307762	val: 0.580746	test: 0.542685

Epoch: 232
Loss: 0.1783706545829773
RMSE train: 0.358813	val: 0.738200	test: 0.694987
MAE train: 0.278974	val: 0.549129	test: 0.528456

Epoch: 233
Loss: 0.1867474913597107
RMSE train: 0.402862	val: 0.776033	test: 0.717048
MAE train: 0.314878	val: 0.577015	test: 0.550645

Epoch: 234
Loss: 0.17494432095970427
RMSE train: 0.352191	val: 0.759078	test: 0.717616
MAE train: 0.273832	val: 0.557652	test: 0.544939

Epoch: 235
Loss: 0.17721739092043468
RMSE train: 0.343079	val: 0.736670	test: 0.696796
MAE train: 0.264903	val: 0.543607	test: 0.522150

Epoch: 236
Loss: 0.16374919137784413
RMSE train: 0.354827	val: 0.731571	test: 0.712403
MAE train: 0.273450	val: 0.548118	test: 0.537534

Epoch: 237
Loss: 0.17259845244032995
RMSE train: 0.377682	val: 0.766001	test: 0.727258
MAE train: 0.297263	val: 0.568951	test: 0.558860

Epoch: 238
Loss: 0.17110146156379155
RMSE train: 0.363399	val: 0.756252	test: 0.717236
MAE train: 0.283304	val: 0.560473	test: 0.541125

Epoch: 239
Loss: 0.1539602673479489
RMSE train: 0.339389	val: 0.756178	test: 0.707293
MAE train: 0.263407	val: 0.560605	test: 0.534705

Epoch: 240
Loss: 0.16961962410381862
RMSE train: 0.335613	val: 0.755104	test: 0.704884
MAE train: 0.259821	val: 0.555616	test: 0.531423

Early stopping
Best (RMSE):	 train: 0.361459	val: 0.715959	test: 0.724784
Best (MAE):	 train: 0.280335	val: 0.533782	test: 0.537116
All runs completed.
