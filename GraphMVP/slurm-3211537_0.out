>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.6/esol_scaff_5_26-05_10-16-28  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.417267163594564
RMSE train: 3.355400	val: 3.861358	test: 4.282321
MAE train: 2.708676	val: 3.325560	test: 3.771968

Epoch: 2
Loss: 10.123800913492838
RMSE train: 3.273099	val: 3.922894	test: 4.389095
MAE train: 2.646458	val: 3.394174	test: 3.912216

Epoch: 3
Loss: 9.575529098510742
RMSE train: 3.120611	val: 3.892359	test: 4.378337
MAE train: 2.526559	val: 3.383517	test: 3.918389

Epoch: 4
Loss: 8.627188682556152
RMSE train: 2.924473	val: 3.781340	test: 4.250039
MAE train: 2.377656	val: 3.301510	test: 3.805068

Epoch: 5
Loss: 8.123907566070557
RMSE train: 2.760466	val: 3.666648	test: 4.106290
MAE train: 2.246762	val: 3.211873	test: 3.683701

Epoch: 6
Loss: 7.463332494099935
RMSE train: 2.693144	val: 3.558812	test: 3.976689
MAE train: 2.231726	val: 3.115398	test: 3.564634

Epoch: 7
Loss: 6.907200654347737
RMSE train: 2.677281	val: 3.539154	test: 3.925463
MAE train: 2.243482	val: 3.098983	test: 3.516759

Epoch: 8
Loss: 6.618219057718913
RMSE train: 2.691825	val: 3.571142	test: 3.940730
MAE train: 2.285521	val: 3.125259	test: 3.540037

Epoch: 9
Loss: 6.0246992111206055
RMSE train: 2.669877	val: 3.549372	test: 3.911323
MAE train: 2.281385	val: 3.094925	test: 3.511022

Epoch: 10
Loss: 5.471712430318196
RMSE train: 2.537220	val: 3.427217	test: 3.783198
MAE train: 2.147538	val: 2.963157	test: 3.358768

Epoch: 11
Loss: 5.098086039225261
RMSE train: 2.428661	val: 3.279331	test: 3.600652
MAE train: 2.052448	val: 2.812339	test: 3.154421

Epoch: 12
Loss: 4.696731408437093
RMSE train: 2.328807	val: 3.125889	test: 3.396049
MAE train: 1.967613	val: 2.663594	test: 2.943233

Epoch: 13
Loss: 4.289555390675862
RMSE train: 2.253666	val: 3.025610	test: 3.236047
MAE train: 1.905886	val: 2.580318	test: 2.777443

Epoch: 14
Loss: 4.120257536570231
RMSE train: 2.196089	val: 2.945672	test: 3.085151
MAE train: 1.876513	val: 2.509994	test: 2.607354

Epoch: 15
Loss: 3.5828290780385337
RMSE train: 2.143125	val: 2.913577	test: 3.011423
MAE train: 1.847864	val: 2.470759	test: 2.517280

Epoch: 16
Loss: 3.21167000134786
RMSE train: 2.052199	val: 2.847755	test: 2.918912
MAE train: 1.763953	val: 2.413324	test: 2.429014

Epoch: 17
Loss: 2.922442595163981
RMSE train: 1.931918	val: 2.751465	test: 2.819880
MAE train: 1.652964	val: 2.343468	test: 2.345707

Epoch: 18
Loss: 2.684333324432373
RMSE train: 1.802259	val: 2.604014	test: 2.668682
MAE train: 1.510247	val: 2.227553	test: 2.218796

Epoch: 19
Loss: 2.3282090425491333
RMSE train: 1.661031	val: 2.444410	test: 2.482185
MAE train: 1.370416	val: 2.076950	test: 2.053462

Epoch: 20
Loss: 2.2059234778086343
RMSE train: 1.539982	val: 2.358530	test: 2.368054
MAE train: 1.261589	val: 1.996302	test: 1.967510

Epoch: 21
Loss: 1.932096242904663
RMSE train: 1.441495	val: 2.347930	test: 2.352190
MAE train: 1.179960	val: 1.990826	test: 1.950247

Epoch: 22
Loss: 1.7233633200327556
RMSE train: 1.336850	val: 2.313796	test: 2.296224
MAE train: 1.085242	val: 1.970236	test: 1.875126

Epoch: 23
Loss: 1.6556298732757568Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.6/esol_scaff_4_26-05_10-16-28  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.620328267415365
RMSE train: 3.196454	val: 3.982259	test: 4.603946
MAE train: 2.536443	val: 3.495860	test: 4.148233

Epoch: 2
Loss: 10.600152969360352
RMSE train: 3.109578	val: 3.961203	test: 4.667936
MAE train: 2.461541	val: 3.472097	test: 4.236031

Epoch: 3
Loss: 9.750592231750488
RMSE train: 3.025908	val: 3.900291	test: 4.576427
MAE train: 2.417485	val: 3.447883	test: 4.187005

Epoch: 4
Loss: 9.041752815246582
RMSE train: 3.022559	val: 3.892358	test: 4.475452
MAE train: 2.490712	val: 3.492431	test: 4.126969

Epoch: 5
Loss: 8.26097043355306
RMSE train: 3.085508	val: 3.954362	test: 4.470216
MAE train: 2.637233	val: 3.570522	test: 4.132988

Epoch: 6
Loss: 7.7428352038065595
RMSE train: 2.983701	val: 3.964642	test: 4.451664
MAE train: 2.559476	val: 3.575538	test: 4.107296

Epoch: 7
Loss: 7.100872198740642
RMSE train: 2.846707	val: 3.944206	test: 4.421624
MAE train: 2.444147	val: 3.538501	test: 4.062833

Epoch: 8
Loss: 6.827315012613933
RMSE train: 2.654914	val: 3.743597	test: 4.182439
MAE train: 2.263152	val: 3.340161	test: 3.799428

Epoch: 9
Loss: 5.945057551066081
RMSE train: 2.501348	val: 3.521257	test: 3.912614
MAE train: 2.112476	val: 3.115679	test: 3.505005

Epoch: 10
Loss: 5.6571346918741865
RMSE train: 2.395753	val: 3.342810	test: 3.690874
MAE train: 2.017935	val: 2.939093	test: 3.265487

Epoch: 11
Loss: 5.221548080444336
RMSE train: 2.323556	val: 3.250161	test: 3.579684
MAE train: 1.972598	val: 2.849438	test: 3.153021

Epoch: 12
Loss: 4.719793637593587
RMSE train: 2.240859	val: 3.183130	test: 3.513619
MAE train: 1.899769	val: 2.785554	test: 3.089506

Epoch: 13
Loss: 4.244280656178792
RMSE train: 2.177406	val: 3.113835	test: 3.435683
MAE train: 1.832789	val: 2.718840	test: 3.011383

Epoch: 14
Loss: 3.877772887547811
RMSE train: 2.046452	val: 2.983492	test: 3.269393
MAE train: 1.702873	val: 2.585140	test: 2.829465

Epoch: 15
Loss: 3.582571109135946
RMSE train: 1.896528	val: 2.819713	test: 3.043048
MAE train: 1.572156	val: 2.418183	test: 2.590735

Epoch: 16
Loss: 3.4009571075439453
RMSE train: 1.788224	val: 2.657806	test: 2.816458
MAE train: 1.482368	val: 2.257153	test: 2.356576

Epoch: 17
Loss: 3.1297949949900308
RMSE train: 1.706156	val: 2.571839	test: 2.706874
MAE train: 1.394555	val: 2.165714	test: 2.249854

Epoch: 18
Loss: 2.7756401697794595
RMSE train: 1.624913	val: 2.515375	test: 2.636438
MAE train: 1.304297	val: 2.107378	test: 2.183663

Epoch: 19
Loss: 2.558000922203064
RMSE train: 1.570313	val: 2.479263	test: 2.565909
MAE train: 1.258014	val: 2.071975	test: 2.130133

Epoch: 20
Loss: 2.2036630312601724
RMSE train: 1.542808	val: 2.457353	test: 2.517517
MAE train: 1.244847	val: 2.043966	test: 2.091160

Epoch: 21
Loss: 2.035300056139628
RMSE train: 1.507826	val: 2.402480	test: 2.431924
MAE train: 1.228782	val: 1.997742	test: 2.004413

Epoch: 22
Loss: 1.7221592267354329
RMSE train: 1.393039	val: 2.340966	test: 2.324097
MAE train: 1.122629	val: 1.935644	test: 1.900183

Epoch: 23
Loss: 1.4328609307607014Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.6/esol_scaff_6_26-05_10-16-28  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 10.914263725280762
RMSE train: 2.941169	val: 3.506454	test: 3.998198
MAE train: 2.349454	val: 2.997966	test: 3.471728

Epoch: 2
Loss: 9.705737749735514
RMSE train: 2.839644	val: 3.485746	test: 3.996607
MAE train: 2.264450	val: 2.983158	test: 3.511453

Epoch: 3
Loss: 8.902268091837565
RMSE train: 2.801193	val: 3.526536	test: 4.029255
MAE train: 2.244558	val: 3.036726	test: 3.567675

Epoch: 4
Loss: 8.327699025472006
RMSE train: 2.745944	val: 3.567090	test: 4.075230
MAE train: 2.219253	val: 3.087564	test: 3.623250

Epoch: 5
Loss: 7.668988545735677
RMSE train: 2.693065	val: 3.588322	test: 4.069744
MAE train: 2.216216	val: 3.116019	test: 3.606908

Epoch: 6
Loss: 7.289010047912598
RMSE train: 2.700496	val: 3.579013	test: 4.042058
MAE train: 2.240166	val: 3.108150	test: 3.571995

Epoch: 7
Loss: 6.657015164693196
RMSE train: 2.673175	val: 3.508634	test: 3.955602
MAE train: 2.224560	val: 3.039373	test: 3.485472

Epoch: 8
Loss: 6.345277309417725
RMSE train: 2.631221	val: 3.361233	test: 3.753570
MAE train: 2.206755	val: 2.901434	test: 3.281417

Epoch: 9
Loss: 5.683019638061523
RMSE train: 2.537083	val: 3.166693	test: 3.501266
MAE train: 2.135231	val: 2.722295	test: 3.023213

Epoch: 10
Loss: 5.098203341166179
RMSE train: 2.487540	val: 3.023924	test: 3.304909
MAE train: 2.105182	val: 2.604021	test: 2.831544

Epoch: 11
Loss: 4.756777922312419
RMSE train: 2.480561	val: 2.983742	test: 3.253330
MAE train: 2.118431	val: 2.576298	test: 2.796768

Epoch: 12
Loss: 4.408625761667888
RMSE train: 2.479820	val: 3.014355	test: 3.286452
MAE train: 2.139319	val: 2.604388	test: 2.841689

Epoch: 13
Loss: 3.929165999094645
RMSE train: 2.397824	val: 3.023062	test: 3.292210
MAE train: 2.077834	val: 2.606788	test: 2.846420

Epoch: 14
Loss: 3.5212814807891846
RMSE train: 2.267495	val: 2.880503	test: 3.098216
MAE train: 1.949378	val: 2.480319	test: 2.644513

Epoch: 15
Loss: 3.1936964988708496
RMSE train: 2.103954	val: 2.682276	test: 2.827766
MAE train: 1.806557	val: 2.311968	test: 2.371563

Epoch: 16
Loss: 2.9650792280832925
RMSE train: 1.986459	val: 2.516518	test: 2.598546
MAE train: 1.686854	val: 2.164486	test: 2.149999

Epoch: 17
Loss: 2.6084144910176597
RMSE train: 1.885384	val: 2.454353	test: 2.483853
MAE train: 1.593163	val: 2.110214	test: 2.034380

Epoch: 18
Loss: 2.350080728530884
RMSE train: 1.775627	val: 2.431517	test: 2.412635
MAE train: 1.497541	val: 2.077092	test: 1.963194

Epoch: 19
Loss: 2.196949005126953
RMSE train: 1.676209	val: 2.406777	test: 2.346395
MAE train: 1.415801	val: 2.046071	test: 1.898172

Epoch: 20
Loss: 1.9110756715138753
RMSE train: 1.543585	val: 2.352817	test: 2.220772
MAE train: 1.285641	val: 1.979604	test: 1.792680

Epoch: 21
Loss: 1.7066637674967449
RMSE train: 1.469597	val: 2.243892	test: 2.064020
MAE train: 1.213263	val: 1.872857	test: 1.665208

Epoch: 22
Loss: 1.4931883811950684
RMSE train: 1.417762	val: 2.166327	test: 1.969172
MAE train: 1.145322	val: 1.797197	test: 1.587464

Epoch: 23
Loss: 1.397018313407898Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.7/esol_scaff_5_26-05_10-16-28  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.366277694702148
RMSE train: 3.301345	val: 3.638675	test: 4.320909
MAE train: 2.678060	val: 3.135462	test: 3.794323

Epoch: 2
Loss: 9.821919679641724
RMSE train: 3.081698	val: 3.439554	test: 4.134839
MAE train: 2.492618	val: 2.931490	test: 3.657093

Epoch: 3
Loss: 11.424787521362305
RMSE train: 2.967015	val: 3.329511	test: 3.975917
MAE train: 2.410771	val: 2.865513	test: 3.546692

Epoch: 4
Loss: 8.86648964881897
RMSE train: 2.894686	val: 3.263724	test: 3.837899
MAE train: 2.367616	val: 2.837908	test: 3.450061

Epoch: 5
Loss: 8.672281265258789
RMSE train: 2.813359	val: 3.191057	test: 3.708123
MAE train: 2.317530	val: 2.792429	test: 3.353926

Epoch: 6
Loss: 6.771296739578247
RMSE train: 2.687862	val: 3.108037	test: 3.580354
MAE train: 2.215738	val: 2.721433	test: 3.245431

Epoch: 7
Loss: 6.839703321456909
RMSE train: 2.553400	val: 3.022861	test: 3.480917
MAE train: 2.097138	val: 2.646356	test: 3.146949

Epoch: 8
Loss: 5.489403605461121
RMSE train: 2.351023	val: 2.838651	test: 3.265818
MAE train: 1.902658	val: 2.473919	test: 2.920779

Epoch: 9
Loss: 5.498610734939575
RMSE train: 2.219115	val: 2.680873	test: 3.085161
MAE train: 1.770427	val: 2.315112	test: 2.728329

Epoch: 10
Loss: 5.31034529209137
RMSE train: 2.163365	val: 2.594727	test: 2.965574
MAE train: 1.722332	val: 2.228709	test: 2.615965

Epoch: 11
Loss: 3.9785261154174805
RMSE train: 2.136035	val: 2.496487	test: 2.832591
MAE train: 1.714776	val: 2.141122	test: 2.477100

Epoch: 12
Loss: 3.5235350131988525
RMSE train: 2.051580	val: 2.459226	test: 2.810151
MAE train: 1.644709	val: 2.130048	test: 2.460896

Epoch: 13
Loss: 3.267889380455017
RMSE train: 1.885217	val: 2.366127	test: 2.712416
MAE train: 1.473767	val: 2.044199	test: 2.370012

Epoch: 14
Loss: 4.333215594291687
RMSE train: 1.848771	val: 2.413895	test: 2.749542
MAE train: 1.430138	val: 2.105068	test: 2.428213

Epoch: 15
Loss: 3.1225520968437195
RMSE train: 1.733430	val: 2.287879	test: 2.609647
MAE train: 1.334432	val: 1.971134	test: 2.286688

Epoch: 16
Loss: 2.4974573850631714
RMSE train: 1.573978	val: 2.137594	test: 2.468061
MAE train: 1.184268	val: 1.817694	test: 2.122820

Epoch: 17
Loss: 2.064405530691147
RMSE train: 1.452551	val: 1.968486	test: 2.272213
MAE train: 1.086735	val: 1.647220	test: 1.914397

Epoch: 18
Loss: 2.042195647954941
RMSE train: 1.299043	val: 1.854565	test: 2.110736
MAE train: 0.968094	val: 1.550981	test: 1.732978

Epoch: 19
Loss: 1.9067415595054626
RMSE train: 1.151935	val: 1.714977	test: 1.955288
MAE train: 0.845722	val: 1.417955	test: 1.573763

Epoch: 20
Loss: 1.7160541415214539
RMSE train: 1.034161	val: 1.682812	test: 1.917155
MAE train: 0.763150	val: 1.377741	test: 1.540516

Epoch: 21
Loss: 1.3322950452566147
RMSE train: 0.964049	val: 1.658259	test: 1.869747
MAE train: 0.724684	val: 1.326150	test: 1.505333

Epoch: 22
Loss: 1.5086331069469452
RMSE train: 0.941746	val: 1.579312	test: 1.743187
MAE train: 0.730861	val: 1.245845	test: 1.408017

Epoch: 23
Loss: 1.2314964830875397Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.7/esol_scaff_4_26-05_10-16-28  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.145726919174194
RMSE train: 3.229237	val: 3.768764	test: 4.521350
MAE train: 2.599181	val: 3.308885	test: 4.053176

Epoch: 2
Loss: 9.712909579277039
RMSE train: 3.122037	val: 3.704750	test: 4.417409
MAE train: 2.526452	val: 3.264610	test: 3.990133

Epoch: 3
Loss: 9.555119276046753
RMSE train: 3.076165	val: 3.734230	test: 4.405096
MAE train: 2.507020	val: 3.313302	test: 4.030709

Epoch: 4
Loss: 8.817891478538513
RMSE train: 3.029270	val: 3.751428	test: 4.392468
MAE train: 2.494746	val: 3.377385	test: 4.064478

Epoch: 5
Loss: 9.91074275970459
RMSE train: 2.996342	val: 3.816108	test: 4.426148
MAE train: 2.502844	val: 3.456296	test: 4.122940

Epoch: 6
Loss: 7.704000234603882
RMSE train: 2.912954	val: 3.741472	test: 4.311271
MAE train: 2.451857	val: 3.366019	test: 4.009743

Epoch: 7
Loss: 7.098000764846802
RMSE train: 2.723310	val: 3.483318	test: 4.027055
MAE train: 2.270687	val: 3.106408	test: 3.716243

Epoch: 8
Loss: 6.477985739707947
RMSE train: 2.488167	val: 3.240242	test: 3.774941
MAE train: 2.036316	val: 2.859648	test: 3.454568

Epoch: 9
Loss: 5.410499572753906
RMSE train: 2.345860	val: 3.133895	test: 3.669398
MAE train: 1.897701	val: 2.766455	test: 3.351820

Epoch: 10
Loss: 5.176562190055847
RMSE train: 2.256806	val: 3.038753	test: 3.569577
MAE train: 1.794119	val: 2.672845	test: 3.246594

Epoch: 11
Loss: 4.008048951625824
RMSE train: 2.198380	val: 2.860501	test: 3.363048
MAE train: 1.768559	val: 2.495422	test: 3.032490

Epoch: 12
Loss: 3.9903109669685364
RMSE train: 2.162098	val: 2.712202	test: 3.186004
MAE train: 1.759421	val: 2.346079	test: 2.840280

Epoch: 13
Loss: 3.9442290663719177
RMSE train: 2.110600	val: 2.657441	test: 3.094135
MAE train: 1.735280	val: 2.297525	test: 2.744539

Epoch: 14
Loss: 2.96367210149765
RMSE train: 1.957416	val: 2.564085	test: 2.967944
MAE train: 1.591657	val: 2.219723	test: 2.601029

Epoch: 15
Loss: 2.5716459155082703
RMSE train: 1.784925	val: 2.416461	test: 2.789639
MAE train: 1.420499	val: 2.088840	test: 2.391374

Epoch: 16
Loss: 2.863683760166168
RMSE train: 1.586737	val: 2.172971	test: 2.528039
MAE train: 1.246967	val: 1.856343	test: 2.136523

Epoch: 17
Loss: 2.512710690498352
RMSE train: 1.463778	val: 2.035909	test: 2.367609
MAE train: 1.138018	val: 1.725666	test: 2.012777

Epoch: 18
Loss: 2.196989983320236
RMSE train: 1.427698	val: 1.922747	test: 2.224952
MAE train: 1.112180	val: 1.617100	test: 1.897266

Epoch: 19
Loss: 1.7212487757205963
RMSE train: 1.346564	val: 1.872266	test: 2.147962
MAE train: 1.041932	val: 1.569998	test: 1.794607

Epoch: 20
Loss: 1.746056705713272
RMSE train: 1.188382	val: 1.937141	test: 2.164624
MAE train: 0.893519	val: 1.593738	test: 1.793909

Epoch: 21
Loss: 1.467312753200531
RMSE train: 1.231126	val: 1.990532	test: 2.222001
MAE train: 0.953938	val: 1.651178	test: 1.840331

Epoch: 22
Loss: 1.718512237071991
RMSE train: 1.164312	val: 1.799412	test: 2.012800
MAE train: 0.905653	val: 1.515686	test: 1.680118

Epoch: 23
Loss: 1.2869773507118225Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.7/esol_scaff_6_26-05_10-16-28  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 10.922791957855225
RMSE train: 3.031344	val: 3.465357	test: 4.148787
MAE train: 2.449967	val: 3.003171	test: 3.619249

Epoch: 2
Loss: 9.38318681716919
RMSE train: 2.878171	val: 3.428297	test: 4.094371
MAE train: 2.313138	val: 2.984455	test: 3.638152

Epoch: 3
Loss: 8.998182773590088
RMSE train: 2.772445	val: 3.377932	test: 4.003340
MAE train: 2.235066	val: 2.960794	test: 3.595309

Epoch: 4
Loss: 8.028041005134583
RMSE train: 2.733144	val: 3.340003	test: 3.896099
MAE train: 2.230056	val: 2.948641	test: 3.525628

Epoch: 5
Loss: 7.669664740562439
RMSE train: 2.623767	val: 3.296733	test: 3.787618
MAE train: 2.136445	val: 2.922954	test: 3.442650

Epoch: 6
Loss: 8.651148557662964
RMSE train: 2.513341	val: 3.289679	test: 3.762248
MAE train: 2.025790	val: 2.934265	test: 3.436731

Epoch: 7
Loss: 6.452061295509338
RMSE train: 2.409636	val: 3.150670	test: 3.603546
MAE train: 1.934792	val: 2.801494	test: 3.271913

Epoch: 8
Loss: 5.405369877815247
RMSE train: 2.394858	val: 3.043785	test: 3.493699
MAE train: 1.949187	val: 2.692637	test: 3.157096

Epoch: 9
Loss: 4.831992328166962
RMSE train: 2.416897	val: 2.966581	test: 3.380510
MAE train: 1.994812	val: 2.604334	test: 3.038357

Epoch: 10
Loss: 5.417425751686096
RMSE train: 2.437489	val: 2.997880	test: 3.402805
MAE train: 2.036777	val: 2.632383	test: 3.056746

Epoch: 11
Loss: 5.251860618591309
RMSE train: 2.381777	val: 3.138047	test: 3.560512
MAE train: 1.987746	val: 2.778848	test: 3.243115

Epoch: 12
Loss: 3.2793416380882263
RMSE train: 2.156659	val: 2.938045	test: 3.343539
MAE train: 1.774773	val: 2.604511	test: 3.037890

Epoch: 13
Loss: 3.3621227741241455
RMSE train: 1.975207	val: 2.639368	test: 3.022298
MAE train: 1.601743	val: 2.320297	test: 2.697289

Epoch: 14
Loss: 3.122732102870941
RMSE train: 1.836479	val: 2.536311	test: 2.906665
MAE train: 1.483408	val: 2.213506	test: 2.548955

Epoch: 15
Loss: 2.5808945298194885
RMSE train: 1.817844	val: 2.671782	test: 3.052028
MAE train: 1.447189	val: 2.312446	test: 2.627071

Epoch: 16
Loss: 2.8823189735412598
RMSE train: 1.780468	val: 2.595798	test: 2.951872
MAE train: 1.451746	val: 2.247461	test: 2.532883

Epoch: 17
Loss: 2.1067270934581757
RMSE train: 1.804886	val: 2.315083	test: 2.616773
MAE train: 1.433450	val: 2.013120	test: 2.297289

Epoch: 18
Loss: 1.638822227716446
RMSE train: 1.551913	val: 1.985129	test: 2.202520
MAE train: 1.182746	val: 1.680946	test: 1.858120

Epoch: 19
Loss: 1.8284925818443298
RMSE train: 1.300177	val: 1.887524	test: 2.046544
MAE train: 0.990774	val: 1.576493	test: 1.683202

Epoch: 20
Loss: 1.7045613527297974
RMSE train: 1.140601	val: 1.791353	test: 1.859149
MAE train: 0.850657	val: 1.459614	test: 1.507389

Epoch: 21
Loss: 1.7435979843139648
RMSE train: 1.021797	val: 1.640476	test: 1.703360
MAE train: 0.767765	val: 1.320450	test: 1.380501

Epoch: 22
Loss: 1.3883602023124695
RMSE train: 1.083355	val: 1.486545	test: 1.601403
MAE train: 0.835637	val: 1.195572	test: 1.286208

Epoch: 23
Loss: 1.1915218532085419Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.8/esol_scaff_4_26-05_10-16-28  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.28052043914795
RMSE train: 3.256388	val: 4.246143	test: 4.380830
MAE train: 2.639857	val: 3.782244	test: 3.875092

Epoch: 2
Loss: 10.751828908920288
RMSE train: 3.173231	val: 4.178565	test: 4.332778
MAE train: 2.592280	val: 3.723008	test: 3.886892

Epoch: 3
Loss: 9.8357675075531
RMSE train: 3.152172	val: 4.258809	test: 4.453132
MAE train: 2.615630	val: 3.905699	test: 4.092224

Epoch: 4
Loss: 9.005284547805786
RMSE train: 3.121614	val: 4.216015	test: 4.447224
MAE train: 2.619966	val: 3.894020	test: 4.120867

Epoch: 5
Loss: 8.208670973777771
RMSE train: 3.083355	val: 4.170752	test: 4.399562
MAE train: 2.623529	val: 3.874520	test: 4.096982

Epoch: 6
Loss: 7.245251297950745
RMSE train: 3.021078	val: 4.121177	test: 4.379519
MAE train: 2.582317	val: 3.821747	test: 4.091946

Epoch: 7
Loss: 6.675509810447693
RMSE train: 2.745746	val: 3.885016	test: 4.072892
MAE train: 2.307085	val: 3.597751	test: 3.779305

Epoch: 8
Loss: 5.857557773590088
RMSE train: 2.422060	val: 3.382575	test: 3.529831
MAE train: 1.979673	val: 3.075445	test: 3.182800

Epoch: 9
Loss: 5.348997950553894
RMSE train: 2.308397	val: 3.115308	test: 3.320118
MAE train: 1.881759	val: 2.788565	test: 2.961035

Epoch: 10
Loss: 5.03838050365448
RMSE train: 2.255277	val: 3.139200	test: 3.362998
MAE train: 1.858225	val: 2.813606	test: 3.021172

Epoch: 11
Loss: 4.422597527503967
RMSE train: 2.201576	val: 3.092445	test: 3.277468
MAE train: 1.833765	val: 2.794956	test: 2.946017

Epoch: 12
Loss: 3.831796407699585
RMSE train: 2.051970	val: 2.909243	test: 3.057627
MAE train: 1.688730	val: 2.622788	test: 2.719435

Epoch: 13
Loss: 3.417251169681549
RMSE train: 1.809721	val: 2.589280	test: 2.726500
MAE train: 1.454770	val: 2.294903	test: 2.367616

Epoch: 14
Loss: 2.941096544265747
RMSE train: 1.703277	val: 2.423579	test: 2.528492
MAE train: 1.352675	val: 2.117263	test: 2.167440

Epoch: 15
Loss: 2.511250674724579
RMSE train: 1.496943	val: 2.187072	test: 2.264799
MAE train: 1.168534	val: 1.910175	test: 1.916683

Epoch: 16
Loss: 2.2294231355190277
RMSE train: 1.313490	val: 2.012085	test: 2.090564
MAE train: 1.004145	val: 1.743789	test: 1.737408

Epoch: 17
Loss: 1.925108164548874
RMSE train: 1.206937	val: 1.858922	test: 1.910454
MAE train: 0.919808	val: 1.557053	test: 1.523028

Epoch: 18
Loss: 1.731421411037445
RMSE train: 1.060554	val: 1.700352	test: 1.739952
MAE train: 0.802203	val: 1.415440	test: 1.382644

Epoch: 19
Loss: 1.5454657673835754
RMSE train: 0.932306	val: 1.602599	test: 1.629991
MAE train: 0.699988	val: 1.312419	test: 1.286487

Epoch: 20
Loss: 1.2521983087062836
RMSE train: 0.911777	val: 1.614725	test: 1.632851
MAE train: 0.681416	val: 1.273673	test: 1.294391

Epoch: 21
Loss: 1.185048222541809
RMSE train: 0.830515	val: 1.349649	test: 1.456491
MAE train: 0.608687	val: 1.076809	test: 1.126391

Epoch: 22
Loss: 1.0791761726140976
RMSE train: 0.838420	val: 1.248282	test: 1.375352
MAE train: 0.619712	val: 1.003531	test: 1.069997

Epoch: 23
Loss: 1.0396639108657837Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.8/esol_scaff_5_26-05_10-16-28  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.874908685684204
RMSE train: 3.303003	val: 4.014784	test: 4.209071
MAE train: 2.682690	val: 3.534570	test: 3.685193

Epoch: 2
Loss: 10.539811849594116
RMSE train: 3.234433	val: 4.194211	test: 4.385622
MAE train: 2.639037	val: 3.822101	test: 3.979346

Epoch: 3
Loss: 9.718185186386108
RMSE train: 3.115253	val: 4.163542	test: 4.393129
MAE train: 2.551109	val: 3.848468	test: 4.039929

Epoch: 4
Loss: 8.407893538475037
RMSE train: 2.961163	val: 4.017880	test: 4.313043
MAE train: 2.426684	val: 3.732936	test: 3.995856

Epoch: 5
Loss: 8.036072015762329
RMSE train: 2.761774	val: 3.689552	test: 4.015694
MAE train: 2.280137	val: 3.397387	test: 3.717616

Epoch: 6
Loss: 6.958311676979065
RMSE train: 2.625885	val: 3.472904	test: 3.801503
MAE train: 2.179965	val: 3.171436	test: 3.498452

Epoch: 7
Loss: 6.331592321395874
RMSE train: 2.442121	val: 3.246241	test: 3.552371
MAE train: 2.013525	val: 2.948417	test: 3.225574

Epoch: 8
Loss: 5.76556134223938
RMSE train: 2.263226	val: 3.081976	test: 3.345406
MAE train: 1.839633	val: 2.786929	test: 3.010809

Epoch: 9
Loss: 5.44429349899292
RMSE train: 2.267689	val: 3.146884	test: 3.366868
MAE train: 1.867451	val: 2.854729	test: 3.051711

Epoch: 10
Loss: 4.777638256549835
RMSE train: 2.320219	val: 3.254081	test: 3.438070
MAE train: 1.955145	val: 2.968069	test: 3.146651

Epoch: 11
Loss: 4.291011273860931
RMSE train: 2.236510	val: 3.173965	test: 3.310235
MAE train: 1.879751	val: 2.884690	test: 3.014048

Epoch: 12
Loss: 3.572931230068207
RMSE train: 1.977881	val: 2.824663	test: 2.920591
MAE train: 1.630294	val: 2.525826	test: 2.585465

Epoch: 13
Loss: 3.122771203517914
RMSE train: 1.764227	val: 2.456052	test: 2.540251
MAE train: 1.434497	val: 2.141563	test: 2.180819

Epoch: 14
Loss: 2.7498968839645386
RMSE train: 1.633250	val: 2.374620	test: 2.493459
MAE train: 1.317289	val: 2.058419	test: 2.145629

Epoch: 15
Loss: 2.462097316980362
RMSE train: 1.426879	val: 2.151881	test: 2.287408
MAE train: 1.103661	val: 1.843686	test: 1.936588

Epoch: 16
Loss: 2.147474557161331
RMSE train: 1.241329	val: 1.838337	test: 1.971707
MAE train: 0.925220	val: 1.540364	test: 1.592367

Epoch: 17
Loss: 1.813225895166397
RMSE train: 1.149533	val: 1.777139	test: 1.894103
MAE train: 0.853453	val: 1.475074	test: 1.503419

Epoch: 18
Loss: 1.6526694893836975
RMSE train: 0.968056	val: 1.585184	test: 1.707189
MAE train: 0.702805	val: 1.284556	test: 1.348646

Epoch: 19
Loss: 1.4515262842178345
RMSE train: 0.877951	val: 1.497402	test: 1.639880
MAE train: 0.630766	val: 1.201375	test: 1.303046

Epoch: 20
Loss: 1.2529216706752777
RMSE train: 0.811887	val: 1.375373	test: 1.548168
MAE train: 0.606149	val: 1.088355	test: 1.229748

Epoch: 21
Loss: 1.102084830403328
RMSE train: 0.810337	val: 1.258225	test: 1.433658
MAE train: 0.619587	val: 0.972694	test: 1.138482

Epoch: 22
Loss: 1.0352337509393692
RMSE train: 0.811113	val: 1.379233	test: 1.504763
MAE train: 0.607080	val: 1.113745	test: 1.175346

Epoch: 23
Loss: 0.9665523171424866Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.8/esol_scaff_6_26-05_10-16-28  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.326371431350708
RMSE train: 3.082901	val: 4.001659	test: 4.123943
MAE train: 2.505940	val: 3.553750	test: 3.596428

Epoch: 2
Loss: 10.512169122695923
RMSE train: 2.971008	val: 4.006635	test: 4.115740
MAE train: 2.410410	val: 3.614977	test: 3.668943

Epoch: 3
Loss: 8.913074731826782
RMSE train: 2.847609	val: 3.938499	test: 4.058278
MAE train: 2.309421	val: 3.575101	test: 3.673589

Epoch: 4
Loss: 8.123925566673279
RMSE train: 2.874134	val: 4.052384	test: 4.144715
MAE train: 2.374409	val: 3.763657	test: 3.823963

Epoch: 5
Loss: 7.407328367233276
RMSE train: 2.988542	val: 4.156855	test: 4.161252
MAE train: 2.513592	val: 3.879879	test: 3.838853

Epoch: 6
Loss: 6.985883831977844
RMSE train: 2.813178	val: 3.747056	test: 3.867340
MAE train: 2.368988	val: 3.449872	test: 3.513384

Epoch: 7
Loss: 5.984776854515076
RMSE train: 2.531027	val: 3.391321	test: 3.652712
MAE train: 2.111276	val: 3.079651	test: 3.297367

Epoch: 8
Loss: 5.401319622993469
RMSE train: 2.399506	val: 3.297788	test: 3.557711
MAE train: 1.985536	val: 2.992101	test: 3.213129

Epoch: 9
Loss: 5.000706076622009
RMSE train: 2.382241	val: 3.185599	test: 3.433227
MAE train: 1.972113	val: 2.894344	test: 3.072540

Epoch: 10
Loss: 4.320676326751709
RMSE train: 2.208823	val: 3.003355	test: 3.261362
MAE train: 1.840511	val: 2.719979	test: 2.933250

Epoch: 11
Loss: 3.9136555790901184
RMSE train: 2.014245	val: 2.800148	test: 3.029752
MAE train: 1.682705	val: 2.511077	test: 2.719936

Epoch: 12
Loss: 3.446953296661377
RMSE train: 1.895751	val: 2.653477	test: 2.877370
MAE train: 1.579788	val: 2.361851	test: 2.571902

Epoch: 13
Loss: 2.9312782287597656
RMSE train: 1.697955	val: 2.382409	test: 2.597438
MAE train: 1.389728	val: 2.108014	test: 2.283354

Epoch: 14
Loss: 2.55562287569046
RMSE train: 1.506391	val: 2.169353	test: 2.356243
MAE train: 1.199820	val: 1.889291	test: 2.028139

Epoch: 15
Loss: 2.1455380022525787
RMSE train: 1.234272	val: 1.835266	test: 1.996867
MAE train: 0.946956	val: 1.541752	test: 1.651664

Epoch: 16
Loss: 1.8614283502101898
RMSE train: 1.110813	val: 1.560248	test: 1.734157
MAE train: 0.836360	val: 1.296920	test: 1.391036

Epoch: 17
Loss: 1.5711849629878998
RMSE train: 0.964860	val: 1.409709	test: 1.622381
MAE train: 0.713584	val: 1.157247	test: 1.292603

Epoch: 18
Loss: 1.4177770912647247
RMSE train: 0.872049	val: 1.297963	test: 1.530665
MAE train: 0.645366	val: 1.063727	test: 1.210721

Epoch: 19
Loss: 1.2618616819381714
RMSE train: 0.840455	val: 1.293454	test: 1.469804
MAE train: 0.632931	val: 1.057350	test: 1.168039

Epoch: 20
Loss: 1.2588029503822327
RMSE train: 0.778418	val: 1.237285	test: 1.375537
MAE train: 0.588508	val: 0.998849	test: 1.109480

Epoch: 21
Loss: 1.0544760078191757
RMSE train: 0.743891	val: 1.160388	test: 1.313085
MAE train: 0.565036	val: 0.958090	test: 1.035400

Epoch: 22
Loss: 0.9287320226430893
RMSE train: 0.692320	val: 1.076453	test: 1.258478
MAE train: 0.531610	val: 0.889492	test: 0.995419

Epoch: 23
Loss: 0.8745548725128174
RMSE train: 1.214244	val: 2.205912	test: 2.122960
MAE train: 0.958806	val: 1.794857	test: 1.729240

Epoch: 24
Loss: 1.302730917930603
RMSE train: 1.079809	val: 2.100640	test: 1.971255
MAE train: 0.844025	val: 1.698983	test: 1.596977

Epoch: 25
Loss: 1.2951008876164753
RMSE train: 0.994828	val: 2.029381	test: 1.897377
MAE train: 0.779087	val: 1.632577	test: 1.527927

Epoch: 26
Loss: 1.101889153321584
RMSE train: 0.902626	val: 1.967221	test: 1.776323
MAE train: 0.679854	val: 1.577799	test: 1.453208

Epoch: 27
Loss: 1.0304707884788513
RMSE train: 0.816427	val: 1.930343	test: 1.716834
MAE train: 0.610952	val: 1.543977	test: 1.385511

Epoch: 28
Loss: 0.8669732411702474
RMSE train: 0.774855	val: 1.880963	test: 1.691290
MAE train: 0.583346	val: 1.476690	test: 1.326051

Epoch: 29
Loss: 0.7989017764727274
RMSE train: 0.834992	val: 1.867694	test: 1.624366
MAE train: 0.615363	val: 1.455356	test: 1.275768

Epoch: 30
Loss: 0.7477239569028219
RMSE train: 0.802257	val: 1.947866	test: 1.653111
MAE train: 0.602936	val: 1.532001	test: 1.315235

Epoch: 31
Loss: 0.8289975523948669
RMSE train: 0.771523	val: 2.001895	test: 1.738257
MAE train: 0.586523	val: 1.574361	test: 1.365904

Epoch: 32
Loss: 0.7340380350748698
RMSE train: 0.754870	val: 2.022091	test: 1.719165
MAE train: 0.567634	val: 1.597051	test: 1.352416

Epoch: 33
Loss: 0.7232473293940226
RMSE train: 0.751747	val: 1.991945	test: 1.659526
MAE train: 0.563450	val: 1.588973	test: 1.315067

Epoch: 34
Loss: 0.7464253107706705
RMSE train: 0.888628	val: 2.050773	test: 1.821992
MAE train: 0.660629	val: 1.639995	test: 1.428322

Epoch: 35
Loss: 0.6363121668497721
RMSE train: 0.813617	val: 1.988167	test: 1.754644
MAE train: 0.620666	val: 1.597948	test: 1.375406

Epoch: 36
Loss: 0.6827481389045715
RMSE train: 0.725756	val: 1.957128	test: 1.662857
MAE train: 0.550786	val: 1.557187	test: 1.306351

Epoch: 37
Loss: 0.6409351627031962
RMSE train: 0.708422	val: 1.924954	test: 1.595705
MAE train: 0.536968	val: 1.521952	test: 1.248973

Epoch: 38
Loss: 0.6373028953870138
RMSE train: 0.686177	val: 1.990052	test: 1.636028
MAE train: 0.526987	val: 1.545013	test: 1.252918

Epoch: 39
Loss: 0.5975207289059957
RMSE train: 0.648956	val: 2.000539	test: 1.626011
MAE train: 0.496090	val: 1.551785	test: 1.252028

Epoch: 40
Loss: 0.6368897954622904
RMSE train: 0.616378	val: 2.024911	test: 1.633734
MAE train: 0.463826	val: 1.591107	test: 1.281429

Epoch: 41
Loss: 0.6787596344947815
RMSE train: 0.603620	val: 2.071834	test: 1.714547
MAE train: 0.455451	val: 1.621167	test: 1.345135

Epoch: 42
Loss: 0.6493695179621378
RMSE train: 0.579315	val: 2.054570	test: 1.778430
MAE train: 0.435155	val: 1.616543	test: 1.408319

Epoch: 43
Loss: 0.6504875620206197
RMSE train: 0.565659	val: 1.949502	test: 1.658070
MAE train: 0.429446	val: 1.544492	test: 1.287889

Epoch: 44
Loss: 0.5643407305081686
RMSE train: 0.565094	val: 1.935127	test: 1.663037
MAE train: 0.427955	val: 1.541787	test: 1.288872

Epoch: 45
Loss: 0.592016359170278
RMSE train: 0.630586	val: 1.969433	test: 1.743913
MAE train: 0.483663	val: 1.562448	test: 1.355883

Epoch: 46
Loss: 0.6683575709660848
RMSE train: 0.599738	val: 1.925465	test: 1.702131
MAE train: 0.456860	val: 1.536712	test: 1.320811

Epoch: 47
Loss: 0.5819681882858276
RMSE train: 0.698655	val: 1.872715	test: 1.633035
MAE train: 0.509624	val: 1.499447	test: 1.277862

Epoch: 48
Loss: 0.5223795473575592
RMSE train: 0.646725	val: 1.873658	test: 1.667584
MAE train: 0.495262	val: 1.497673	test: 1.295055

Epoch: 49
Loss: 0.5412437419096628
RMSE train: 0.693308	val: 1.920383	test: 1.815240
MAE train: 0.525503	val: 1.525153	test: 1.415232

Epoch: 50
Loss: 0.532293458779653
RMSE train: 0.569908	val: 1.861221	test: 1.745471
MAE train: 0.426664	val: 1.483693	test: 1.366200

Epoch: 51
Loss: 0.5467958847681681
RMSE train: 0.578161	val: 1.793585	test: 1.605826
MAE train: 0.433120	val: 1.429507	test: 1.255499

Epoch: 52
Loss: 0.53225177526474
RMSE train: 0.577121	val: 1.773708	test: 1.579483
MAE train: 0.421149	val: 1.396130	test: 1.222380

Epoch: 53
Loss: 0.5027904113133749
RMSE train: 0.602676	val: 1.776783	test: 1.641380
MAE train: 0.450431	val: 1.400147	test: 1.258440

Epoch: 54
Loss: 0.526027242342631
RMSE train: 0.575463	val: 1.781320	test: 1.629852
MAE train: 0.438334	val: 1.409878	test: 1.245958

Epoch: 55
Loss: 0.4689643979072571
RMSE train: 0.604428	val: 1.775285	test: 1.591708
MAE train: 0.462790	val: 1.403212	test: 1.244054

Epoch: 56
Loss: 0.5039585332075754
RMSE train: 0.629349	val: 1.759816	test: 1.597710
MAE train: 0.480673	val: 1.386186	test: 1.211606

Epoch: 57
Loss: 0.49869540333747864
RMSE train: 0.617989	val: 1.751033	test: 1.595332
MAE train: 0.471002	val: 1.375314	test: 1.214816

Epoch: 58
Loss: 0.4560980200767517
RMSE train: 0.570825	val: 1.734744	test: 1.560201
MAE train: 0.429604	val: 1.360868	test: 1.197828

Epoch: 59
Loss: 0.47239871819814044
RMSE train: 0.582746	val: 1.730707	test: 1.555977
MAE train: 0.435567	val: 1.351391	test: 1.197796

Epoch: 60
Loss: 0.509941816329956
RMSE train: 0.658611	val: 1.726629	test: 1.572773
MAE train: 0.479174	val: 1.344595	test: 1.211626

Epoch: 61
Loss: 0.4399912158648173
RMSE train: 0.636165	val: 1.753542	test: 1.557579
MAE train: 0.472277	val: 1.359538	test: 1.211987

Epoch: 62
Loss: 0.3976418773333232
RMSE train: 0.575196	val: 1.762310	test: 1.576967
MAE train: 0.435825	val: 1.366170	test: 1.231044

Epoch: 63
Loss: 0.39674509565035504
RMSE train: 0.604564	val: 1.770625	test: 1.648059
MAE train: 0.465919	val: 1.378617	test: 1.285155

Epoch: 64
Loss: 0.40887414415677387
RMSE train: 0.673657	val: 1.774158	test: 1.636945
MAE train: 0.511407	val: 1.398326	test: 1.280777

Epoch: 65
Loss: 0.44585561752319336
RMSE train: 0.641141	val: 1.742502	test: 1.599836
MAE train: 0.479772	val: 1.370768	test: 1.235794

Epoch: 66
Loss: 0.37220704555511475
RMSE train: 0.588262	val: 1.740015	test: 1.615514
MAE train: 0.439081	val: 1.380007	test: 1.263820

Epoch: 67
Loss: 0.4109370509783427
RMSE train: 0.619505	val: 1.710863	test: 1.593742
MAE train: 0.456619	val: 1.348338	test: 1.216774

Epoch: 68
Loss: 0.40687471628189087
RMSE train: 0.637618	val: 1.730430	test: 1.604249
MAE train: 0.466734	val: 1.348077	test: 1.245142

Epoch: 69
Loss: 0.38441187143325806
RMSE train: 0.552651	val: 1.742016	test: 1.575665
MAE train: 0.416028	val: 1.365798	test: 1.214670

Epoch: 70
Loss: 0.42273751894632977
RMSE train: 0.558230	val: 1.743283	test: 1.563071
MAE train: 0.419574	val: 1.371526	test: 1.204332

Epoch: 71
Loss: 0.39697082837422687
RMSE train: 0.606257	val: 1.747812	test: 1.593778
MAE train: 0.441501	val: 1.369623	test: 1.237127

Epoch: 72
Loss: 0.4195329149564107
RMSE train: 0.585318	val: 1.759945	test: 1.591106
MAE train: 0.438545	val: 1.400332	test: 1.213649

Epoch: 73
Loss: 0.4043726126352946
RMSE train: 0.539449	val: 1.782837	test: 1.581196
MAE train: 0.405182	val: 1.406607	test: 1.210927

Epoch: 74
Loss: 0.36282084385554
RMSE train: 0.590210	val: 1.822037	test: 1.609287
MAE train: 0.425549	val: 1.404861	test: 1.263613

Epoch: 75
Loss: 0.38305339217185974
RMSE train: 0.628259	val: 1.798401	test: 1.619390
MAE train: 0.458569	val: 1.392041	test: 1.250295

Epoch: 76
Loss: 0.3568178912003835
RMSE train: 0.580707	val: 1.787266	test: 1.624574
MAE train: 0.426542	val: 1.381178	test: 1.257542

Epoch: 77
Loss: 0.4197622835636139
RMSE train: 0.624151	val: 1.818307	test: 1.640219
MAE train: 0.437260	val: 1.394743	test: 1.301820

Epoch: 78
Loss: 0.4447687268257141
RMSE train: 0.565072	val: 1.787116	test: 1.635415
MAE train: 0.414636	val: 1.400896	test: 1.265219

Epoch: 79
Loss: 0.34124542276064557
RMSE train: 0.519684	val: 1.784541	test: 1.635224
MAE train: 0.388226	val: 1.399216	test: 1.276548

Epoch: 80
Loss: 0.33754419287045795
RMSE train: 0.520755	val: 1.781286	test: 1.592838
MAE train: 0.371508	val: 1.388730	test: 1.256069

Epoch: 81
Loss: 0.36908111969629925
RMSE train: 0.503339	val: 1.772390	test: 1.573238
MAE train: 0.369194	val: 1.373539	test: 1.245575

Epoch: 82
Loss: 0.3899597426255544
RMSE train: 0.484367	val: 1.802048	test: 1.712300
MAE train: 0.356267	val: 1.398242	test: 1.363527

Epoch: 83
Loss: 0.4572013517220815
RMSE train: 0.533794	val: 1.729568	test: 1.560428
MAE train: 0.382988	val: 1.349124	test: 1.213559
RMSE train: 1.207433	val: 2.289267	test: 2.229895
MAE train: 0.951735	val: 1.953957	test: 1.799463

Epoch: 24
Loss: 1.3864376544952393
RMSE train: 1.095545	val: 2.257401	test: 2.123346
MAE train: 0.834324	val: 1.947105	test: 1.706810

Epoch: 25
Loss: 1.225662092367808
RMSE train: 0.986063	val: 2.255038	test: 2.080473
MAE train: 0.733996	val: 1.946120	test: 1.668871

Epoch: 26
Loss: 1.1766410668690999
RMSE train: 0.932587	val: 2.270946	test: 2.053673
MAE train: 0.693151	val: 1.948160	test: 1.647563

Epoch: 27
Loss: 1.0695236523946126
RMSE train: 0.913574	val: 2.248281	test: 2.029010
MAE train: 0.677158	val: 1.929127	test: 1.632428

Epoch: 28
Loss: 0.9456006288528442
RMSE train: 0.851394	val: 2.208516	test: 1.981871
MAE train: 0.621357	val: 1.895604	test: 1.594233

Epoch: 29
Loss: 0.8468972643216451
RMSE train: 0.823528	val: 2.179497	test: 1.953138
MAE train: 0.604845	val: 1.858626	test: 1.565725

Epoch: 30
Loss: 0.8416761557261149
RMSE train: 0.818358	val: 2.182363	test: 1.972445
MAE train: 0.607730	val: 1.837663	test: 1.562508

Epoch: 31
Loss: 0.8051407933235168
RMSE train: 0.811749	val: 2.132928	test: 1.924829
MAE train: 0.608563	val: 1.820068	test: 1.523500

Epoch: 32
Loss: 0.7050439914067587
RMSE train: 0.814701	val: 2.124411	test: 1.924557
MAE train: 0.622143	val: 1.820807	test: 1.531436

Epoch: 33
Loss: 0.8469937642415365
RMSE train: 0.790204	val: 2.112153	test: 1.941799
MAE train: 0.608136	val: 1.804768	test: 1.531206

Epoch: 34
Loss: 0.6811674038569132
RMSE train: 0.748863	val: 2.079161	test: 1.923721
MAE train: 0.571767	val: 1.760018	test: 1.513303

Epoch: 35
Loss: 0.660489559173584
RMSE train: 0.709266	val: 2.034810	test: 1.898980
MAE train: 0.545503	val: 1.702591	test: 1.501542

Epoch: 36
Loss: 0.6138809124628702
RMSE train: 0.693767	val: 2.020483	test: 1.918948
MAE train: 0.536122	val: 1.687929	test: 1.511097

Epoch: 37
Loss: 0.6992301146189371
RMSE train: 0.661848	val: 1.951975	test: 1.867233
MAE train: 0.503348	val: 1.631175	test: 1.468835

Epoch: 38
Loss: 0.6013621290524801
RMSE train: 0.607265	val: 1.957823	test: 1.861586
MAE train: 0.458982	val: 1.610744	test: 1.466086

Epoch: 39
Loss: 0.5685999790827433
RMSE train: 0.619794	val: 1.992867	test: 1.928039
MAE train: 0.469212	val: 1.612959	test: 1.515082

Epoch: 40
Loss: 0.6181590557098389
RMSE train: 0.592205	val: 1.986279	test: 1.894055
MAE train: 0.450945	val: 1.617301	test: 1.494430

Epoch: 41
Loss: 0.5936656395594279
RMSE train: 0.559095	val: 2.001165	test: 1.830916
MAE train: 0.417026	val: 1.625052	test: 1.457114

Epoch: 42
Loss: 0.5756035049756368
RMSE train: 0.577955	val: 2.031539	test: 1.822825
MAE train: 0.433309	val: 1.626322	test: 1.457008

Epoch: 43
Loss: 0.5944810509681702
RMSE train: 0.620563	val: 1.918557	test: 1.722889
MAE train: 0.474533	val: 1.562722	test: 1.363991

Epoch: 44
Loss: 0.6131747762362162
RMSE train: 0.606694	val: 2.042354	test: 1.853069
MAE train: 0.473659	val: 1.619831	test: 1.469732

Epoch: 45
Loss: 0.61092076698939
RMSE train: 0.574953	val: 2.093428	test: 1.899813
MAE train: 0.432539	val: 1.649614	test: 1.520956

Epoch: 46
Loss: 0.5755888621012369
RMSE train: 0.606364	val: 1.867228	test: 1.745132
MAE train: 0.449651	val: 1.526301	test: 1.399143

Epoch: 47
Loss: 0.5595627625783285
RMSE train: 0.666623	val: 1.743191	test: 1.729965
MAE train: 0.478808	val: 1.456228	test: 1.413329

Epoch: 48
Loss: 0.5958994626998901
RMSE train: 0.569388	val: 1.847594	test: 1.830470
MAE train: 0.426278	val: 1.497931	test: 1.462326

Epoch: 49
Loss: 0.5726716121037801
RMSE train: 0.561461	val: 2.045127	test: 2.067366
MAE train: 0.423910	val: 1.627603	test: 1.644955

Epoch: 50
Loss: 0.5265105764071146
RMSE train: 0.535363	val: 1.902500	test: 1.866794
MAE train: 0.404813	val: 1.531106	test: 1.480074

Epoch: 51
Loss: 0.5062659482161204
RMSE train: 0.545888	val: 1.793917	test: 1.716682
MAE train: 0.409876	val: 1.455832	test: 1.377905

Epoch: 52
Loss: 0.5458732148011526
RMSE train: 0.535057	val: 1.891295	test: 1.850074
MAE train: 0.406639	val: 1.517359	test: 1.475552

Epoch: 53
Loss: 0.47653019428253174
RMSE train: 0.547798	val: 1.990541	test: 1.988070
MAE train: 0.415416	val: 1.585924	test: 1.605331

Epoch: 54
Loss: 0.5258616308371226
RMSE train: 0.553567	val: 1.785556	test: 1.763350
MAE train: 0.417378	val: 1.446535	test: 1.407549

Epoch: 55
Loss: 0.5171428521474203
RMSE train: 0.564706	val: 1.713306	test: 1.679771
MAE train: 0.413330	val: 1.395012	test: 1.352769

Epoch: 56
Loss: 0.4501967628796895
RMSE train: 0.527052	val: 1.754422	test: 1.731663
MAE train: 0.388810	val: 1.412255	test: 1.379441

Epoch: 57
Loss: 0.5069668094317118
RMSE train: 0.524055	val: 1.807075	test: 1.789179
MAE train: 0.391749	val: 1.454848	test: 1.432341

Epoch: 58
Loss: 0.42366930842399597
RMSE train: 0.579275	val: 1.743956	test: 1.709549
MAE train: 0.437882	val: 1.410495	test: 1.360888

Epoch: 59
Loss: 0.4469185173511505
RMSE train: 0.534368	val: 1.755800	test: 1.684538
MAE train: 0.398037	val: 1.414045	test: 1.345959

Epoch: 60
Loss: 0.4815501272678375
RMSE train: 0.521250	val: 1.814881	test: 1.761558
MAE train: 0.392960	val: 1.456597	test: 1.396329

Epoch: 61
Loss: 0.44275816281636554
RMSE train: 0.540372	val: 1.822771	test: 1.753108
MAE train: 0.414684	val: 1.476380	test: 1.382145

Epoch: 62
Loss: 0.4537044366200765
RMSE train: 0.534985	val: 1.860754	test: 1.771867
MAE train: 0.408971	val: 1.510123	test: 1.396479

Epoch: 63
Loss: 0.44124942024548847
RMSE train: 0.536736	val: 1.869104	test: 1.759716
MAE train: 0.404010	val: 1.524849	test: 1.386270

Epoch: 64
Loss: 0.5064915617307028
RMSE train: 0.558859	val: 1.821268	test: 1.738299
MAE train: 0.426269	val: 1.476021	test: 1.364196

Epoch: 65
Loss: 0.41481586297353107
RMSE train: 0.527102	val: 1.838844	test: 1.754123
MAE train: 0.396377	val: 1.485480	test: 1.381386

Epoch: 66
Loss: 0.45770219961802167
RMSE train: 0.526285	val: 1.790726	test: 1.699851
MAE train: 0.391093	val: 1.466354	test: 1.338792

Epoch: 67
Loss: 0.42212246855099994
RMSE train: 0.624736	val: 1.749275	test: 1.680482
MAE train: 0.473896	val: 1.443960	test: 1.319089

Epoch: 68
Loss: 0.4580359061559041
RMSE train: 0.634830	val: 1.762998	test: 1.700378
MAE train: 0.490016	val: 1.461298	test: 1.330052

Epoch: 69
Loss: 0.4931080440680186
RMSE train: 0.511915	val: 1.759455	test: 1.663322
MAE train: 0.386177	val: 1.453311	test: 1.315568

Epoch: 70
Loss: 0.41419784228007
RMSE train: 0.517737	val: 1.682561	test: 1.609662
MAE train: 0.386976	val: 1.395833	test: 1.279215

Epoch: 71
Loss: 0.43492797017097473
RMSE train: 0.536042	val: 1.660029	test: 1.635589
MAE train: 0.412594	val: 1.374073	test: 1.295459

Epoch: 72
Loss: 0.39449577530225116
RMSE train: 0.490242	val: 1.697036	test: 1.716027
MAE train: 0.379066	val: 1.393631	test: 1.359637

Epoch: 73
Loss: 0.4048551122347514
RMSE train: 0.487894	val: 1.611379	test: 1.613196
MAE train: 0.359897	val: 1.333892	test: 1.274228

Epoch: 74
Loss: 0.44179611404736835
RMSE train: 0.508621	val: 1.596277	test: 1.578871
MAE train: 0.370513	val: 1.315394	test: 1.250449

Epoch: 75
Loss: 0.4274885058403015
RMSE train: 0.517799	val: 1.674542	test: 1.720052
MAE train: 0.396410	val: 1.376764	test: 1.353604

Epoch: 76
Loss: 0.41973332564036053
RMSE train: 0.520122	val: 1.698909	test: 1.722648
MAE train: 0.393297	val: 1.400177	test: 1.357525

Epoch: 77
Loss: 0.35480523109436035
RMSE train: 0.514451	val: 1.694983	test: 1.662483
MAE train: 0.384676	val: 1.405603	test: 1.315192

Epoch: 78
Loss: 0.48906968037287396
RMSE train: 0.524918	val: 1.718380	test: 1.655307
MAE train: 0.389952	val: 1.421615	test: 1.309477

Epoch: 79
Loss: 0.37032880385716754
RMSE train: 0.593482	val: 1.866065	test: 1.822012
MAE train: 0.445250	val: 1.509047	test: 1.433215

Epoch: 80
Loss: 0.3764484723409017
RMSE train: 0.530846	val: 1.830006	test: 1.757129
MAE train: 0.398285	val: 1.478212	test: 1.390354

Epoch: 81
Loss: 0.38749411702156067
RMSE train: 0.513608	val: 1.787544	test: 1.727058
MAE train: 0.386670	val: 1.442572	test: 1.371238

Epoch: 82
Loss: 0.4043319821357727
RMSE train: 0.526876	val: 1.769445	test: 1.747740
MAE train: 0.407442	val: 1.433358	test: 1.391098

Epoch: 83
Loss: 0.43428416053454083
RMSE train: 0.582150	val: 1.783426	test: 1.789683
MAE train: 0.471845	val: 1.437050	test: 1.429188
RMSE train: 1.311516	val: 2.211632	test: 2.018888
MAE train: 1.067199	val: 1.818426	test: 1.608640

Epoch: 24
Loss: 1.1694653828938801
RMSE train: 1.224917	val: 2.227819	test: 2.039388
MAE train: 0.980386	val: 1.828783	test: 1.633783

Epoch: 25
Loss: 1.0467671155929565
RMSE train: 1.134855	val: 2.159222	test: 1.969561
MAE train: 0.897130	val: 1.769385	test: 1.600693

Epoch: 26
Loss: 0.9155452648798624
RMSE train: 1.080882	val: 2.097938	test: 1.886500
MAE train: 0.846890	val: 1.701318	test: 1.489233

Epoch: 27
Loss: 0.8525242805480957
RMSE train: 1.069692	val: 2.122770	test: 1.942370
MAE train: 0.838654	val: 1.702368	test: 1.520298

Epoch: 28
Loss: 0.8204468488693237
RMSE train: 0.999067	val: 2.105793	test: 1.909416
MAE train: 0.772676	val: 1.717409	test: 1.500229

Epoch: 29
Loss: 0.8266977071762085
RMSE train: 0.894279	val: 2.052493	test: 1.858929
MAE train: 0.680337	val: 1.673300	test: 1.468591

Epoch: 30
Loss: 0.7407723069190979
RMSE train: 0.768957	val: 2.095023	test: 1.857384
MAE train: 0.582179	val: 1.638678	test: 1.443644

Epoch: 31
Loss: 0.8019301096598307
RMSE train: 0.778011	val: 2.007239	test: 1.733658
MAE train: 0.582287	val: 1.571287	test: 1.374433

Epoch: 32
Loss: 0.6815783580144247
RMSE train: 0.748579	val: 2.001163	test: 1.680309
MAE train: 0.557156	val: 1.559862	test: 1.339974

Epoch: 33
Loss: 0.7259860436121622
RMSE train: 0.669008	val: 2.119196	test: 1.768390
MAE train: 0.490723	val: 1.606846	test: 1.397414

Epoch: 34
Loss: 0.6394517322381338
RMSE train: 0.647076	val: 2.251939	test: 2.006656
MAE train: 0.480634	val: 1.671769	test: 1.615546

Epoch: 35
Loss: 0.6448652148246765
RMSE train: 0.690538	val: 2.004312	test: 1.856598
MAE train: 0.513486	val: 1.550266	test: 1.504208

Epoch: 36
Loss: 0.6880929668744405
RMSE train: 0.690266	val: 1.827636	test: 1.706238
MAE train: 0.510283	val: 1.452434	test: 1.369495

Epoch: 37
Loss: 0.6271462440490723
RMSE train: 0.669144	val: 1.852010	test: 1.725763
MAE train: 0.506266	val: 1.471558	test: 1.380810

Epoch: 38
Loss: 0.5859288573265076
RMSE train: 0.629318	val: 1.995485	test: 1.888769
MAE train: 0.484928	val: 1.545354	test: 1.524628

Epoch: 39
Loss: 0.5448947151501974
RMSE train: 0.663762	val: 1.893946	test: 1.767351
MAE train: 0.505018	val: 1.486662	test: 1.420401

Epoch: 40
Loss: 0.5751831531524658
RMSE train: 0.699994	val: 1.846599	test: 1.718780
MAE train: 0.524393	val: 1.452001	test: 1.381199

Epoch: 41
Loss: 0.6090838313102722
RMSE train: 0.724394	val: 1.840588	test: 1.652487
MAE train: 0.518826	val: 1.439904	test: 1.340612

Epoch: 42
Loss: 0.5324461658795675
RMSE train: 0.681174	val: 1.863458	test: 1.649981
MAE train: 0.489936	val: 1.454747	test: 1.344413

Epoch: 43
Loss: 0.5287809868653616
RMSE train: 0.650263	val: 1.814271	test: 1.599122
MAE train: 0.477731	val: 1.428220	test: 1.307727

Epoch: 44
Loss: 0.5708757638931274
RMSE train: 0.667706	val: 1.827049	test: 1.594862
MAE train: 0.491243	val: 1.431645	test: 1.301339

Epoch: 45
Loss: 0.5333875815073649
RMSE train: 0.720848	val: 1.784598	test: 1.595421
MAE train: 0.520918	val: 1.408574	test: 1.299279

Epoch: 46
Loss: 0.5952122410138448
RMSE train: 0.738451	val: 1.759636	test: 1.649214
MAE train: 0.539532	val: 1.398373	test: 1.325140

Epoch: 47
Loss: 0.5889928738276163
RMSE train: 0.675787	val: 1.780613	test: 1.649706
MAE train: 0.501041	val: 1.416704	test: 1.328180

Epoch: 48
Loss: 0.49965458114941913
RMSE train: 0.661855	val: 1.781568	test: 1.624264
MAE train: 0.494367	val: 1.424739	test: 1.288560

Epoch: 49
Loss: 0.528907577196757
RMSE train: 0.646996	val: 1.825340	test: 1.626812
MAE train: 0.481284	val: 1.442082	test: 1.284910

Epoch: 50
Loss: 0.47571009397506714
RMSE train: 0.596009	val: 2.010975	test: 1.815796
MAE train: 0.447916	val: 1.550121	test: 1.480088

Epoch: 51
Loss: 0.5226253072420756
RMSE train: 0.605088	val: 1.902756	test: 1.662439
MAE train: 0.444851	val: 1.483150	test: 1.357629

Epoch: 52
Loss: 0.477390597263972
RMSE train: 0.626310	val: 1.896908	test: 1.567168
MAE train: 0.455886	val: 1.474441	test: 1.278081

Epoch: 53
Loss: 0.4800315201282501
RMSE train: 0.607948	val: 2.085428	test: 1.572698
MAE train: 0.446562	val: 1.582529	test: 1.267859

Epoch: 54
Loss: 0.4555387496948242
RMSE train: 0.554017	val: 2.229086	test: 1.664336
MAE train: 0.409992	val: 1.663168	test: 1.339371

Epoch: 55
Loss: 0.47890864809354144
RMSE train: 0.601175	val: 2.082568	test: 1.602615
MAE train: 0.443373	val: 1.607802	test: 1.279965

Epoch: 56
Loss: 0.4747205277283986
RMSE train: 0.644446	val: 1.987762	test: 1.606236
MAE train: 0.472719	val: 1.548102	test: 1.278884

Epoch: 57
Loss: 0.4997760554154714
RMSE train: 0.576473	val: 2.078991	test: 1.711250
MAE train: 0.421003	val: 1.584789	test: 1.384248

Epoch: 58
Loss: 0.4037529726823171
RMSE train: 0.571708	val: 2.033473	test: 1.706891
MAE train: 0.420144	val: 1.555214	test: 1.378419

Epoch: 59
Loss: 0.47682321071624756
RMSE train: 0.591413	val: 1.927663	test: 1.655496
MAE train: 0.427894	val: 1.492010	test: 1.338373

Epoch: 60
Loss: 0.4266105790932973
RMSE train: 0.625226	val: 1.871367	test: 1.623968
MAE train: 0.457378	val: 1.464465	test: 1.292577

Epoch: 61
Loss: 0.3730831543604533
RMSE train: 0.589408	val: 1.939427	test: 1.676791
MAE train: 0.436986	val: 1.515375	test: 1.337914

Epoch: 62
Loss: 0.4574983815352122
RMSE train: 0.645969	val: 1.964772	test: 1.732891
MAE train: 0.486623	val: 1.538202	test: 1.366674

Epoch: 63
Loss: 0.3933190703392029
RMSE train: 0.670018	val: 1.876751	test: 1.679341
MAE train: 0.499982	val: 1.481320	test: 1.328995

Epoch: 64
Loss: 0.3844503064950307
RMSE train: 0.658117	val: 1.898387	test: 1.667098
MAE train: 0.487258	val: 1.482955	test: 1.318234

Epoch: 65
Loss: 0.48589207728703815
RMSE train: 0.658202	val: 1.910149	test: 1.800275
MAE train: 0.485938	val: 1.476904	test: 1.433768

Epoch: 66
Loss: 0.458364874124527
RMSE train: 0.650258	val: 1.837571	test: 1.729809
MAE train: 0.478416	val: 1.427348	test: 1.374307

Epoch: 67
Loss: 0.39454110463460285
RMSE train: 0.643909	val: 1.774252	test: 1.668114
MAE train: 0.482971	val: 1.391972	test: 1.331568

Epoch: 68
Loss: 0.44595398505528766
RMSE train: 0.604392	val: 1.787463	test: 1.660796
MAE train: 0.441159	val: 1.410122	test: 1.323219

Epoch: 69
Loss: 0.39367517828941345
RMSE train: 0.554391	val: 1.926389	test: 1.695119
MAE train: 0.398651	val: 1.490896	test: 1.352714

Epoch: 70
Loss: 0.3902263840039571
RMSE train: 0.561254	val: 1.860033	test: 1.629974
MAE train: 0.403669	val: 1.443335	test: 1.291301

Epoch: 71
Loss: 0.391322781642278
RMSE train: 0.566766	val: 1.817275	test: 1.599889
MAE train: 0.408382	val: 1.423238	test: 1.263158

Epoch: 72
Loss: 0.39573564132054645
RMSE train: 0.512338	val: 1.872107	test: 1.670212
MAE train: 0.372875	val: 1.454088	test: 1.317063

Epoch: 73
Loss: 0.3846907416979472
RMSE train: 0.525616	val: 1.934652	test: 1.783099
MAE train: 0.386374	val: 1.487538	test: 1.399606

Epoch: 74
Loss: 0.4292234381039937
RMSE train: 0.499547	val: 1.930585	test: 1.751849
MAE train: 0.364852	val: 1.482904	test: 1.388282

Epoch: 75
Loss: 0.36917593081792194
RMSE train: 0.519729	val: 1.882486	test: 1.645901
MAE train: 0.379553	val: 1.466401	test: 1.306127

Epoch: 76
Loss: 0.43349852164586383
RMSE train: 0.544144	val: 1.861044	test: 1.605272
MAE train: 0.398466	val: 1.463629	test: 1.267552

Epoch: 77
Loss: 0.432975172996521
RMSE train: 0.527006	val: 1.915855	test: 1.675894
MAE train: 0.394921	val: 1.496264	test: 1.323575

Epoch: 78
Loss: 0.4041002094745636
RMSE train: 0.510120	val: 1.977590	test: 1.801861
MAE train: 0.387707	val: 1.531176	test: 1.438648

Epoch: 79
Loss: 0.3376044233640035
RMSE train: 0.505159	val: 1.897232	test: 1.679953
MAE train: 0.379397	val: 1.477139	test: 1.351690

Epoch: 80
Loss: 0.39187055826187134
RMSE train: 0.508333	val: 1.892101	test: 1.660580
MAE train: 0.380129	val: 1.470791	test: 1.347187

Epoch: 81
Loss: 0.36262238025665283
RMSE train: 0.480759	val: 1.956569	test: 1.773582
MAE train: 0.361265	val: 1.508318	test: 1.419113

Epoch: 82
Loss: 0.3469260334968567
RMSE train: 0.500052	val: 2.001327	test: 1.873337
MAE train: 0.375252	val: 1.533241	test: 1.495447

Epoch: 83
Loss: 0.38914541403452557
RMSE train: 0.548571	val: 1.900293	test: 1.743994
MAE train: 0.409676	val: 1.472323	test: 1.392330
RMSE train: 1.071521	val: 1.664692	test: 1.854289
MAE train: 0.814034	val: 1.393975	test: 1.566056

Epoch: 24
Loss: 1.3238429427146912
RMSE train: 0.901465	val: 1.483571	test: 1.617017
MAE train: 0.682452	val: 1.198506	test: 1.336970

Epoch: 25
Loss: 0.9991567581892014
RMSE train: 0.843632	val: 1.428504	test: 1.507874
MAE train: 0.641723	val: 1.135093	test: 1.208650

Epoch: 26
Loss: 1.0161716789007187
RMSE train: 0.808057	val: 1.435688	test: 1.507088
MAE train: 0.608330	val: 1.133725	test: 1.215267

Epoch: 27
Loss: 1.149578258395195
RMSE train: 0.864175	val: 1.435100	test: 1.523464
MAE train: 0.668439	val: 1.161042	test: 1.241631

Epoch: 28
Loss: 1.0063665211200714
RMSE train: 0.843804	val: 1.469413	test: 1.561451
MAE train: 0.639472	val: 1.190048	test: 1.258200

Epoch: 29
Loss: 1.0991570055484772
RMSE train: 0.796303	val: 1.489833	test: 1.564290
MAE train: 0.592617	val: 1.205179	test: 1.257157

Epoch: 30
Loss: 1.1102342009544373
RMSE train: 0.760130	val: 1.454511	test: 1.508507
MAE train: 0.582226	val: 1.163766	test: 1.227060

Epoch: 31
Loss: 1.0661967992782593
RMSE train: 0.793078	val: 1.438383	test: 1.479008
MAE train: 0.613786	val: 1.127182	test: 1.211029

Epoch: 32
Loss: 0.909996971487999
RMSE train: 0.781936	val: 1.452239	test: 1.509307
MAE train: 0.606802	val: 1.138162	test: 1.224937

Epoch: 33
Loss: 0.9297211617231369
RMSE train: 0.731697	val: 1.385859	test: 1.474254
MAE train: 0.563321	val: 1.093987	test: 1.174487

Epoch: 34
Loss: 0.766337588429451
RMSE train: 0.739892	val: 1.379659	test: 1.508496
MAE train: 0.561559	val: 1.109392	test: 1.196664

Epoch: 35
Loss: 0.987300843000412
RMSE train: 0.676853	val: 1.395433	test: 1.527773
MAE train: 0.500073	val: 1.119637	test: 1.213051

Epoch: 36
Loss: 0.7687286138534546
RMSE train: 0.729559	val: 1.404566	test: 1.587360
MAE train: 0.535561	val: 1.094076	test: 1.259096

Epoch: 37
Loss: 0.8595136106014252
RMSE train: 0.687918	val: 1.370788	test: 1.554928
MAE train: 0.515854	val: 1.061772	test: 1.233272

Epoch: 38
Loss: 0.7721713185310364
RMSE train: 0.711302	val: 1.334717	test: 1.501116
MAE train: 0.528878	val: 1.062785	test: 1.176248

Epoch: 39
Loss: 0.7503909915685654
RMSE train: 0.779907	val: 1.368852	test: 1.526772
MAE train: 0.583157	val: 1.112896	test: 1.212548

Epoch: 40
Loss: 1.0947979539632797
RMSE train: 0.979427	val: 1.362575	test: 1.495972
MAE train: 0.681808	val: 1.109329	test: 1.187646

Epoch: 41
Loss: 0.9560187458992004
RMSE train: 0.880017	val: 1.347474	test: 1.491598
MAE train: 0.624492	val: 1.100752	test: 1.174587

Epoch: 42
Loss: 0.7883089333772659
RMSE train: 0.764633	val: 1.485440	test: 1.664299
MAE train: 0.579950	val: 1.186026	test: 1.338799

Epoch: 43
Loss: 0.9280006140470505
RMSE train: 0.805238	val: 1.557477	test: 1.712826
MAE train: 0.621996	val: 1.250180	test: 1.384489

Epoch: 44
Loss: 0.997055783867836
RMSE train: 0.682631	val: 1.551654	test: 1.673629
MAE train: 0.513635	val: 1.220964	test: 1.329450

Epoch: 45
Loss: 0.7985152900218964
RMSE train: 0.671016	val: 1.478213	test: 1.591355
MAE train: 0.505222	val: 1.149507	test: 1.247427

Epoch: 46
Loss: 0.933016300201416
RMSE train: 0.658733	val: 1.344416	test: 1.463697
MAE train: 0.507943	val: 1.041729	test: 1.170266

Epoch: 47
Loss: 1.0996648967266083
RMSE train: 0.742946	val: 1.363022	test: 1.513524
MAE train: 0.570753	val: 1.058634	test: 1.195172

Epoch: 48
Loss: 0.6839235723018646
RMSE train: 0.694810	val: 1.354676	test: 1.502159
MAE train: 0.522975	val: 1.080804	test: 1.183794

Epoch: 49
Loss: 0.7336918264627457
RMSE train: 0.743852	val: 1.372111	test: 1.506881
MAE train: 0.554704	val: 1.125123	test: 1.201189

Epoch: 50
Loss: 0.7327333390712738
RMSE train: 0.674800	val: 1.391568	test: 1.512076
MAE train: 0.512120	val: 1.121721	test: 1.215342

Epoch: 51
Loss: 0.8561174124479294
RMSE train: 0.695443	val: 1.333653	test: 1.456919
MAE train: 0.536570	val: 1.074631	test: 1.170468

Epoch: 52
Loss: 0.6973164677619934
RMSE train: 0.742446	val: 1.309964	test: 1.443295
MAE train: 0.573998	val: 1.046821	test: 1.151480

Epoch: 53
Loss: 0.8159029334783554
RMSE train: 0.707058	val: 1.349456	test: 1.453427
MAE train: 0.538302	val: 1.069110	test: 1.158020

Epoch: 54
Loss: 0.6824834793806076
RMSE train: 0.693299	val: 1.350938	test: 1.420126
MAE train: 0.522337	val: 1.056924	test: 1.143204

Epoch: 55
Loss: 0.7284424602985382
RMSE train: 0.718848	val: 1.428584	test: 1.495449
MAE train: 0.552075	val: 1.129496	test: 1.197895

Epoch: 56
Loss: 0.665667250752449
RMSE train: 0.709808	val: 1.449758	test: 1.517579
MAE train: 0.553834	val: 1.145297	test: 1.215164

Epoch: 57
Loss: 0.7610788494348526
RMSE train: 0.683997	val: 1.387525	test: 1.464713
MAE train: 0.526114	val: 1.113803	test: 1.182633

Epoch: 58
Loss: 0.6397355049848557
RMSE train: 0.723413	val: 1.333810	test: 1.431163
MAE train: 0.559530	val: 1.079418	test: 1.138418

Epoch: 59
Loss: 1.3167448788881302
RMSE train: 0.752529	val: 1.361747	test: 1.485815
MAE train: 0.567777	val: 1.086035	test: 1.176361

Epoch: 60
Loss: 0.6887983232736588
RMSE train: 0.734993	val: 1.305922	test: 1.385310
MAE train: 0.540030	val: 1.036686	test: 1.121533

Epoch: 61
Loss: 0.7857270240783691
RMSE train: 0.757167	val: 1.353232	test: 1.415011
MAE train: 0.563777	val: 1.069718	test: 1.121158

Epoch: 62
Loss: 0.6677717119455338
RMSE train: 0.712312	val: 1.390228	test: 1.464650
MAE train: 0.539541	val: 1.097310	test: 1.148541

Epoch: 63
Loss: 0.7481969594955444
RMSE train: 0.703472	val: 1.413891	test: 1.525982
MAE train: 0.534148	val: 1.135588	test: 1.211240

Epoch: 64
Loss: 0.6850490570068359
RMSE train: 0.703306	val: 1.427103	test: 1.536840
MAE train: 0.546750	val: 1.129967	test: 1.213744

Epoch: 65
Loss: 0.6373874992132187
RMSE train: 0.609870	val: 1.371391	test: 1.462714
MAE train: 0.469997	val: 1.076179	test: 1.160791

Epoch: 66
Loss: 0.6343227326869965
RMSE train: 0.601783	val: 1.301148	test: 1.398042
MAE train: 0.452428	val: 1.036595	test: 1.115093

Epoch: 67
Loss: 0.7753886431455612
RMSE train: 0.607219	val: 1.278182	test: 1.348359
MAE train: 0.449842	val: 1.011479	test: 1.083690

Epoch: 68
Loss: 0.969808503985405
RMSE train: 0.617307	val: 1.295314	test: 1.378698
MAE train: 0.465059	val: 1.025174	test: 1.114256

Epoch: 69
Loss: 0.4959862157702446
RMSE train: 0.674297	val: 1.303922	test: 1.422505
MAE train: 0.515178	val: 1.041248	test: 1.137219

Epoch: 70
Loss: 0.909382164478302
RMSE train: 0.729353	val: 1.386492	test: 1.505108
MAE train: 0.560048	val: 1.088355	test: 1.201475

Epoch: 71
Loss: 0.5717653930187225
RMSE train: 0.727200	val: 1.411124	test: 1.500505
MAE train: 0.551597	val: 1.117310	test: 1.202007

Epoch: 72
Loss: 0.8117898553609848
RMSE train: 0.663802	val: 1.349730	test: 1.402097
MAE train: 0.502872	val: 1.069361	test: 1.126933

Epoch: 73
Loss: 0.5206464305520058
RMSE train: 0.635730	val: 1.339620	test: 1.383524
MAE train: 0.478961	val: 1.055101	test: 1.110527

Epoch: 74
Loss: 0.7462474554777145
RMSE train: 0.622640	val: 1.309701	test: 1.358923
MAE train: 0.467089	val: 1.025023	test: 1.086717

Epoch: 75
Loss: 0.699378713965416
RMSE train: 0.622141	val: 1.359118	test: 1.436564
MAE train: 0.457130	val: 1.061638	test: 1.139336

Epoch: 76
Loss: 0.5659817308187485
RMSE train: 0.662411	val: 1.327280	test: 1.394712
MAE train: 0.488483	val: 1.047510	test: 1.107367

Epoch: 77
Loss: 0.7536188513040543
RMSE train: 0.637022	val: 1.315004	test: 1.359895
MAE train: 0.474595	val: 1.036617	test: 1.090794

Epoch: 78
Loss: 0.6330402418971062
RMSE train: 0.617096	val: 1.330018	test: 1.362207
MAE train: 0.480490	val: 1.032552	test: 1.100323

Epoch: 79
Loss: 0.6279034316539764
RMSE train: 0.578246	val: 1.365787	test: 1.412475
MAE train: 0.452159	val: 1.047261	test: 1.153682

Epoch: 80
Loss: 0.6758296638727188
RMSE train: 0.625348	val: 1.445981	test: 1.581777
MAE train: 0.486247	val: 1.092275	test: 1.258342

Epoch: 81
Loss: 0.6255266964435577
RMSE train: 0.676766	val: 1.393527	test: 1.563280
MAE train: 0.525221	val: 1.069292	test: 1.244702

Epoch: 82
Loss: 0.569595530629158
RMSE train: 0.703150	val: 1.340242	test: 1.492837
MAE train: 0.539041	val: 1.067157	test: 1.193048

Epoch: 83
Loss: 0.9464659094810486
RMSE train: 0.679814	val: 1.343896	test: 1.460520
MAE train: 0.520855	val: 1.063073	test: 1.180330

Epoch: 84
Loss: 0.717260479927063
RMSE train: 0.963272	val: 1.609479	test: 1.757537
MAE train: 0.738449	val: 1.288097	test: 1.430304

Epoch: 24
Loss: 1.193067878484726
RMSE train: 1.049277	val: 1.650175	test: 1.808405
MAE train: 0.801943	val: 1.345079	test: 1.494492

Epoch: 25
Loss: 1.853631317615509
RMSE train: 1.025591	val: 1.718066	test: 1.927059
MAE train: 0.792725	val: 1.413579	test: 1.583708

Epoch: 26
Loss: 1.2732116878032684
RMSE train: 0.986247	val: 1.713327	test: 1.940559
MAE train: 0.761178	val: 1.421688	test: 1.575245

Epoch: 27
Loss: 1.436228483915329
RMSE train: 0.878490	val: 1.615240	test: 1.825820
MAE train: 0.666236	val: 1.313005	test: 1.466606

Epoch: 28
Loss: 1.0968880951404572
RMSE train: 0.841469	val: 1.570152	test: 1.669141
MAE train: 0.634972	val: 1.208075	test: 1.340691

Epoch: 29
Loss: 0.9940681755542755
RMSE train: 0.855167	val: 1.610350	test: 1.659679
MAE train: 0.647763	val: 1.212939	test: 1.337684

Epoch: 30
Loss: 1.0102273523807526
RMSE train: 0.883915	val: 1.648964	test: 1.761484
MAE train: 0.687569	val: 1.279479	test: 1.430215

Epoch: 31
Loss: 0.9859022349119186
RMSE train: 0.874217	val: 1.633735	test: 1.809871
MAE train: 0.678503	val: 1.313900	test: 1.471929

Epoch: 32
Loss: 0.9673408567905426
RMSE train: 0.748905	val: 1.560044	test: 1.689167
MAE train: 0.570074	val: 1.229353	test: 1.351893

Epoch: 33
Loss: 1.0388406068086624
RMSE train: 0.751233	val: 1.749917	test: 1.774603
MAE train: 0.569204	val: 1.383041	test: 1.413231

Epoch: 34
Loss: 0.8659693002700806
RMSE train: 0.778272	val: 1.673212	test: 1.642204
MAE train: 0.600191	val: 1.347019	test: 1.333358

Epoch: 35
Loss: 0.89627306163311
RMSE train: 0.795435	val: 1.541365	test: 1.611239
MAE train: 0.612434	val: 1.244453	test: 1.282702

Epoch: 36
Loss: 0.8454773277044296
RMSE train: 0.762025	val: 1.377857	test: 1.542850
MAE train: 0.578346	val: 1.117409	test: 1.254876

Epoch: 37
Loss: 1.1141590029001236
RMSE train: 0.678847	val: 1.318077	test: 1.464741
MAE train: 0.524147	val: 1.060475	test: 1.183402

Epoch: 38
Loss: 0.8371296674013138
RMSE train: 0.712316	val: 1.465761	test: 1.634885
MAE train: 0.545597	val: 1.143183	test: 1.286516

Epoch: 39
Loss: 0.9082297086715698
RMSE train: 0.677280	val: 1.485139	test: 1.658122
MAE train: 0.521353	val: 1.161162	test: 1.312858

Epoch: 40
Loss: 0.7440217733383179
RMSE train: 0.673042	val: 1.361140	test: 1.535120
MAE train: 0.518763	val: 1.111639	test: 1.232596

Epoch: 41
Loss: 0.7627718448638916
RMSE train: 0.728028	val: 1.384059	test: 1.574719
MAE train: 0.549839	val: 1.131033	test: 1.250712

Epoch: 42
Loss: 0.9094465970993042
RMSE train: 0.777426	val: 1.475711	test: 1.654235
MAE train: 0.595616	val: 1.176511	test: 1.304392

Epoch: 43
Loss: 0.9054686427116394
RMSE train: 0.742886	val: 1.523078	test: 1.635386
MAE train: 0.575526	val: 1.209766	test: 1.297060

Epoch: 44
Loss: 0.7753258496522903
RMSE train: 0.679300	val: 1.406031	test: 1.532830
MAE train: 0.528326	val: 1.111791	test: 1.228080

Epoch: 45
Loss: 0.7711582332849503
RMSE train: 0.652539	val: 1.334602	test: 1.481708
MAE train: 0.510155	val: 1.075827	test: 1.199341

Epoch: 46
Loss: 0.8291267603635788
RMSE train: 0.669721	val: 1.402354	test: 1.563079
MAE train: 0.520544	val: 1.133331	test: 1.245785

Epoch: 47
Loss: 0.8890598267316818
RMSE train: 0.777717	val: 1.483865	test: 1.671658
MAE train: 0.608047	val: 1.202283	test: 1.339410

Epoch: 48
Loss: 1.1497035771608353
RMSE train: 0.720328	val: 1.351139	test: 1.522359
MAE train: 0.543337	val: 1.096730	test: 1.225627

Epoch: 49
Loss: 0.8694747090339661
RMSE train: 0.836897	val: 1.293062	test: 1.405513
MAE train: 0.624843	val: 1.028308	test: 1.121121

Epoch: 50
Loss: 0.9340900480747223
RMSE train: 0.729762	val: 1.416672	test: 1.597880
MAE train: 0.567720	val: 1.151582	test: 1.269252

Epoch: 51
Loss: 1.0551990270614624
RMSE train: 0.897791	val: 1.582178	test: 1.715143
MAE train: 0.695381	val: 1.267497	test: 1.382356

Epoch: 52
Loss: 0.929406464099884
RMSE train: 0.804265	val: 1.435576	test: 1.528700
MAE train: 0.613594	val: 1.167898	test: 1.251748

Epoch: 53
Loss: 0.7455016523599625
RMSE train: 0.894711	val: 1.458279	test: 1.566945
MAE train: 0.698357	val: 1.106104	test: 1.237568

Epoch: 54
Loss: 0.7681471705436707
RMSE train: 0.675299	val: 1.441661	test: 1.528233
MAE train: 0.530054	val: 1.125955	test: 1.222444

Epoch: 55
Loss: 0.92149218916893
RMSE train: 0.724892	val: 1.555736	test: 1.666112
MAE train: 0.561484	val: 1.222104	test: 1.351267

Epoch: 56
Loss: 0.9482613205909729
RMSE train: 0.686447	val: 1.364963	test: 1.473661
MAE train: 0.537702	val: 1.106927	test: 1.195130

Epoch: 57
Loss: 1.0390545278787613
RMSE train: 0.670465	val: 1.335887	test: 1.453385
MAE train: 0.518170	val: 1.097984	test: 1.171037

Epoch: 58
Loss: 0.8321379125118256
RMSE train: 0.635347	val: 1.340587	test: 1.447252
MAE train: 0.493179	val: 1.091715	test: 1.159395

Epoch: 59
Loss: 0.8298606127500534
RMSE train: 0.649096	val: 1.334237	test: 1.429035
MAE train: 0.506180	val: 1.054285	test: 1.141950

Epoch: 60
Loss: 0.7656794637441635
RMSE train: 0.724150	val: 1.311506	test: 1.411530
MAE train: 0.554577	val: 1.021577	test: 1.132359

Epoch: 61
Loss: 0.674249917268753
RMSE train: 0.696538	val: 1.259866	test: 1.409794
MAE train: 0.534167	val: 0.996171	test: 1.138485

Epoch: 62
Loss: 0.709006205201149
RMSE train: 0.614606	val: 1.279074	test: 1.432256
MAE train: 0.473761	val: 1.013467	test: 1.146234

Epoch: 63
Loss: 0.6061911135911942
RMSE train: 0.586860	val: 1.276246	test: 1.437992
MAE train: 0.452608	val: 1.024169	test: 1.153259

Epoch: 64
Loss: 0.6475688219070435
RMSE train: 0.582904	val: 1.292647	test: 1.434709
MAE train: 0.450279	val: 1.029026	test: 1.156885

Epoch: 65
Loss: 0.5865124017000198
RMSE train: 0.588320	val: 1.321924	test: 1.451739
MAE train: 0.459665	val: 1.033332	test: 1.166069

Epoch: 66
Loss: 0.7633149325847626
RMSE train: 0.594762	val: 1.340179	test: 1.452229
MAE train: 0.468705	val: 1.039843	test: 1.176596

Epoch: 67
Loss: 0.6700755506753922
RMSE train: 0.606592	val: 1.409597	test: 1.504650
MAE train: 0.466536	val: 1.115783	test: 1.220903

Epoch: 68
Loss: 0.5476081073284149
RMSE train: 0.630632	val: 1.421969	test: 1.513237
MAE train: 0.478090	val: 1.128850	test: 1.214838

Epoch: 69
Loss: 0.9481956735253334
RMSE train: 0.626668	val: 1.323831	test: 1.441858
MAE train: 0.465210	val: 1.074335	test: 1.161831

Epoch: 70
Loss: 0.7174205332994461
RMSE train: 0.656576	val: 1.312641	test: 1.452791
MAE train: 0.492220	val: 1.077329	test: 1.166724

Epoch: 71
Loss: 0.7389675080776215
RMSE train: 0.670409	val: 1.354827	test: 1.504396
MAE train: 0.511953	val: 1.107417	test: 1.202159

Epoch: 72
Loss: 0.6335672438144684
RMSE train: 0.652875	val: 1.424998	test: 1.519305
MAE train: 0.500572	val: 1.146790	test: 1.223686

Epoch: 73
Loss: 0.6664571464061737
RMSE train: 0.673763	val: 1.320453	test: 1.423984
MAE train: 0.508039	val: 1.073928	test: 1.151686

Epoch: 74
Loss: 0.7067743986845016
RMSE train: 0.644489	val: 1.302679	test: 1.423054
MAE train: 0.479135	val: 1.066335	test: 1.157015

Epoch: 75
Loss: 0.5771259814500809
RMSE train: 0.611853	val: 1.323636	test: 1.398008
MAE train: 0.465983	val: 1.071546	test: 1.144560

Epoch: 76
Loss: 0.5313141793012619
RMSE train: 0.639318	val: 1.392375	test: 1.468828
MAE train: 0.491405	val: 1.106157	test: 1.193072

Epoch: 77
Loss: 0.5599378347396851
RMSE train: 0.569232	val: 1.419111	test: 1.486341
MAE train: 0.440069	val: 1.119041	test: 1.204703

Epoch: 78
Loss: 0.5869800597429276
RMSE train: 0.576322	val: 1.423239	test: 1.486750
MAE train: 0.437485	val: 1.126139	test: 1.204148

Epoch: 79
Loss: 0.5472536981105804
RMSE train: 0.595089	val: 1.327673	test: 1.397624
MAE train: 0.458421	val: 1.056059	test: 1.125421

Epoch: 80
Loss: 0.5484430640935898
RMSE train: 0.649298	val: 1.344740	test: 1.416355
MAE train: 0.498222	val: 1.058656	test: 1.141672

Epoch: 81
Loss: 0.5200526341795921
RMSE train: 0.593002	val: 1.380254	test: 1.443134
MAE train: 0.458644	val: 1.079831	test: 1.163438

Epoch: 82
Loss: 0.6792808920145035
RMSE train: 0.558728	val: 1.351400	test: 1.428144
MAE train: 0.433635	val: 1.060866	test: 1.150629

Epoch: 83
Loss: 0.5103921890258789
RMSE train: 0.579010	val: 1.292368	test: 1.373839
MAE train: 0.445577	val: 1.023061	test: 1.095469

Epoch: 84
Loss: 0.8953349143266678
RMSE train: 0.973879	val: 1.605255	test: 1.786501
MAE train: 0.710632	val: 1.276993	test: 1.427223

Epoch: 24
Loss: 1.2451402842998505
RMSE train: 0.957401	val: 1.609906	test: 1.804505
MAE train: 0.708008	val: 1.306168	test: 1.464815

Epoch: 25
Loss: 0.9937499463558197
RMSE train: 1.021345	val: 1.661560	test: 1.850839
MAE train: 0.756480	val: 1.389077	test: 1.563114

Epoch: 26
Loss: 1.3168833404779434
RMSE train: 0.910936	val: 1.591311	test: 1.749391
MAE train: 0.672309	val: 1.305064	test: 1.459996

Epoch: 27
Loss: 1.1395654827356339
RMSE train: 0.825154	val: 1.543877	test: 1.653275
MAE train: 0.622245	val: 1.233666	test: 1.360515

Epoch: 28
Loss: 0.985348179936409
RMSE train: 0.784686	val: 1.441565	test: 1.591838
MAE train: 0.593380	val: 1.152749	test: 1.291121

Epoch: 29
Loss: 1.0218685567378998
RMSE train: 0.794457	val: 1.595827	test: 1.736424
MAE train: 0.595632	val: 1.239815	test: 1.385961

Epoch: 30
Loss: 0.9867758005857468
RMSE train: 0.848094	val: 1.576015	test: 1.694603
MAE train: 0.606986	val: 1.219592	test: 1.340801

Epoch: 31
Loss: 1.0735307782888412
RMSE train: 0.850812	val: 1.538449	test: 1.719734
MAE train: 0.643245	val: 1.241488	test: 1.382277

Epoch: 32
Loss: 0.794930562376976
RMSE train: 0.979270	val: 1.722658	test: 1.971542
MAE train: 0.764024	val: 1.433716	test: 1.642611

Epoch: 33
Loss: 0.9793245643377304
RMSE train: 0.748043	val: 1.547376	test: 1.718490
MAE train: 0.556328	val: 1.235764	test: 1.385910

Epoch: 34
Loss: 0.8005013763904572
RMSE train: 0.659242	val: 1.606560	test: 1.685175
MAE train: 0.499230	val: 1.220264	test: 1.345537

Epoch: 35
Loss: 0.6802749186754227
RMSE train: 0.698151	val: 1.414268	test: 1.455794
MAE train: 0.542905	val: 1.070070	test: 1.171401

Epoch: 36
Loss: 1.0433686971664429
RMSE train: 0.741806	val: 1.360950	test: 1.458926
MAE train: 0.570486	val: 1.061160	test: 1.192402

Epoch: 37
Loss: 0.8307520747184753
RMSE train: 0.759910	val: 1.441355	test: 1.620919
MAE train: 0.576865	val: 1.138096	test: 1.299493

Epoch: 38
Loss: 0.8075686544179916
RMSE train: 0.701448	val: 1.371674	test: 1.604960
MAE train: 0.529649	val: 1.098188	test: 1.274518

Epoch: 39
Loss: 0.8199625313282013
RMSE train: 0.731109	val: 1.362143	test: 1.605812
MAE train: 0.553894	val: 1.087549	test: 1.277367

Epoch: 40
Loss: 0.9580926150083542
RMSE train: 0.700202	val: 1.405573	test: 1.640095
MAE train: 0.529394	val: 1.104099	test: 1.300670

Epoch: 41
Loss: 0.7878429293632507
RMSE train: 0.657113	val: 1.484733	test: 1.662620
MAE train: 0.497074	val: 1.142487	test: 1.316428

Epoch: 42
Loss: 0.7002077847719193
RMSE train: 0.679566	val: 1.522834	test: 1.663202
MAE train: 0.511393	val: 1.183226	test: 1.333074

Epoch: 43
Loss: 0.7251448780298233
RMSE train: 0.677689	val: 1.431551	test: 1.571969
MAE train: 0.507889	val: 1.132503	test: 1.262369

Epoch: 44
Loss: 0.8661388456821442
RMSE train: 0.689246	val: 1.435000	test: 1.556650
MAE train: 0.513165	val: 1.123795	test: 1.248540

Epoch: 45
Loss: 0.9647553563117981
RMSE train: 0.652502	val: 1.527039	test: 1.555914
MAE train: 0.484629	val: 1.155296	test: 1.241790

Epoch: 46
Loss: 0.7560497969388962
RMSE train: 0.697589	val: 1.654445	test: 1.588449
MAE train: 0.504418	val: 1.236560	test: 1.274187

Epoch: 47
Loss: 0.8825413584709167
RMSE train: 0.760375	val: 1.657226	test: 1.591930
MAE train: 0.541934	val: 1.235135	test: 1.279309

Epoch: 48
Loss: 0.7765506654977798
RMSE train: 0.839387	val: 1.689499	test: 1.718303
MAE train: 0.603873	val: 1.274682	test: 1.364735

Epoch: 49
Loss: 0.9311176240444183
RMSE train: 0.774855	val: 1.615697	test: 1.698199
MAE train: 0.560976	val: 1.234692	test: 1.350162

Epoch: 50
Loss: 0.9336422234773636
RMSE train: 0.679407	val: 1.545849	test: 1.688018
MAE train: 0.495910	val: 1.195070	test: 1.371452

Epoch: 51
Loss: 0.976441890001297
RMSE train: 0.780434	val: 1.352874	test: 1.490548
MAE train: 0.590977	val: 1.088781	test: 1.212954

Epoch: 52
Loss: 0.7346885651350021
RMSE train: 0.805292	val: 1.387670	test: 1.563670
MAE train: 0.584510	val: 1.124682	test: 1.285267

Epoch: 53
Loss: 0.7773327529430389
RMSE train: 0.673421	val: 1.274855	test: 1.498942
MAE train: 0.502254	val: 1.009545	test: 1.208758

Epoch: 54
Loss: 0.7498007565736771
RMSE train: 0.717365	val: 1.397904	test: 1.562375
MAE train: 0.520253	val: 1.084237	test: 1.264598

Epoch: 55
Loss: 0.6427533626556396
RMSE train: 0.672539	val: 1.347908	test: 1.454789
MAE train: 0.489794	val: 1.049775	test: 1.173978

Epoch: 56
Loss: 0.7576696574687958
RMSE train: 0.648159	val: 1.338300	test: 1.443010
MAE train: 0.473926	val: 1.060417	test: 1.174203

Epoch: 57
Loss: 0.7681688070297241
RMSE train: 0.674825	val: 1.461642	test: 1.529515
MAE train: 0.502703	val: 1.155752	test: 1.254554

Epoch: 58
Loss: 0.6457216590642929
RMSE train: 0.668207	val: 1.500688	test: 1.542022
MAE train: 0.500303	val: 1.157882	test: 1.254887

Epoch: 59
Loss: 0.8865868300199509
RMSE train: 0.654594	val: 1.344766	test: 1.371839
MAE train: 0.488394	val: 1.015387	test: 1.102459

Epoch: 60
Loss: 0.7797371447086334
RMSE train: 0.673350	val: 1.333965	test: 1.375032
MAE train: 0.501213	val: 1.045823	test: 1.125092

Epoch: 61
Loss: 0.6014758348464966
RMSE train: 0.629769	val: 1.336422	test: 1.359989
MAE train: 0.468620	val: 1.044506	test: 1.110703

Epoch: 62
Loss: 0.7163970470428467
RMSE train: 0.618265	val: 1.383496	test: 1.366017
MAE train: 0.452879	val: 1.057878	test: 1.112313

Epoch: 63
Loss: 0.792461097240448
RMSE train: 0.648381	val: 1.411779	test: 1.367485
MAE train: 0.472235	val: 1.082768	test: 1.113308

Epoch: 64
Loss: 1.018848955631256
RMSE train: 0.711124	val: 1.341593	test: 1.358843
MAE train: 0.516637	val: 1.062899	test: 1.088335

Epoch: 65
Loss: 0.6079362332820892
RMSE train: 0.658684	val: 1.191521	test: 1.272885
MAE train: 0.484784	val: 0.937570	test: 1.028620

Epoch: 66
Loss: 0.5729429721832275
RMSE train: 0.601236	val: 1.167497	test: 1.272758
MAE train: 0.443284	val: 0.907122	test: 1.026294

Epoch: 67
Loss: 0.6182254403829575
RMSE train: 0.632543	val: 1.242661	test: 1.354073
MAE train: 0.464646	val: 0.996496	test: 1.085477

Epoch: 68
Loss: 0.8619557172060013
RMSE train: 0.726600	val: 1.367401	test: 1.524255
MAE train: 0.529818	val: 1.102825	test: 1.227647

Epoch: 69
Loss: 0.7068365514278412
RMSE train: 0.811137	val: 1.477942	test: 1.664377
MAE train: 0.598703	val: 1.170961	test: 1.334347

Epoch: 70
Loss: 0.6569315493106842
RMSE train: 0.782247	val: 1.398310	test: 1.551120
MAE train: 0.561600	val: 1.107139	test: 1.229447

Epoch: 71
Loss: 0.7092106714844704
RMSE train: 0.686038	val: 1.224897	test: 1.284470
MAE train: 0.500820	val: 0.965138	test: 1.021949

Epoch: 72
Loss: 0.5707657784223557
RMSE train: 0.624595	val: 1.282539	test: 1.301468
MAE train: 0.458267	val: 0.994858	test: 1.036831

Epoch: 73
Loss: 0.6267729699611664
RMSE train: 0.667249	val: 1.434566	test: 1.427030
MAE train: 0.503512	val: 1.138125	test: 1.154426

Epoch: 74
Loss: 0.58400559425354
RMSE train: 0.698529	val: 1.415961	test: 1.489026
MAE train: 0.516018	val: 1.135640	test: 1.214767

Epoch: 75
Loss: 0.59610366076231
RMSE train: 0.733843	val: 1.262739	test: 1.400499
MAE train: 0.527955	val: 1.013495	test: 1.124899

Epoch: 76
Loss: 0.5066060870885849
RMSE train: 0.702214	val: 1.262818	test: 1.406138
MAE train: 0.514179	val: 1.029524	test: 1.129618

Epoch: 77
Loss: 1.253187581896782
RMSE train: 0.648710	val: 1.269380	test: 1.388988
MAE train: 0.481862	val: 1.038535	test: 1.115546

Epoch: 78
Loss: 0.5821170583367348
RMSE train: 0.599764	val: 1.232809	test: 1.304914
MAE train: 0.448023	val: 0.980638	test: 1.034732

Epoch: 79
Loss: 0.5892189294099808
RMSE train: 0.584559	val: 1.209833	test: 1.264880
MAE train: 0.434415	val: 0.940138	test: 1.007907

Epoch: 80
Loss: 0.9327998608350754
RMSE train: 0.710945	val: 1.281907	test: 1.339645
MAE train: 0.518923	val: 1.012276	test: 1.068570

Epoch: 81
Loss: 0.5574134513735771
RMSE train: 0.708802	val: 1.306353	test: 1.393128
MAE train: 0.530170	val: 1.027078	test: 1.108523

Epoch: 82
Loss: 0.811993420124054
RMSE train: 0.592174	val: 1.201606	test: 1.271791
MAE train: 0.445253	val: 0.933330	test: 1.008282

Epoch: 83
Loss: 0.6714015752077103
RMSE train: 0.543240	val: 1.231579	test: 1.306644
MAE train: 0.410020	val: 0.966001	test: 1.032374

Epoch: 84
Loss: 0.5224740281701088
RMSE train: 0.762925	val: 1.330927	test: 1.463686
MAE train: 0.584226	val: 1.057514	test: 1.160651

Epoch: 24
Loss: 0.8637210875749588
RMSE train: 0.652539	val: 1.217109	test: 1.309772
MAE train: 0.512973	val: 0.954052	test: 1.058461

Epoch: 25
Loss: 0.8285830020904541
RMSE train: 0.643811	val: 1.260621	test: 1.312868
MAE train: 0.500157	val: 0.997513	test: 1.044460

Epoch: 26
Loss: 0.8302510231733322
RMSE train: 0.681587	val: 1.262984	test: 1.360803
MAE train: 0.515240	val: 1.015614	test: 1.056013

Epoch: 27
Loss: 0.7815231829881668
RMSE train: 0.668653	val: 1.186424	test: 1.296656
MAE train: 0.504506	val: 0.957973	test: 1.002466

Epoch: 28
Loss: 0.7291228324174881
RMSE train: 0.625315	val: 1.073535	test: 1.259287
MAE train: 0.463284	val: 0.863436	test: 0.998661

Epoch: 29
Loss: 0.7069573253393173
RMSE train: 0.634408	val: 1.117513	test: 1.311602
MAE train: 0.478076	val: 0.894012	test: 1.035224

Epoch: 30
Loss: 0.7194871753454208
RMSE train: 0.669473	val: 1.249603	test: 1.349567
MAE train: 0.506237	val: 1.009857	test: 1.023148

Epoch: 31
Loss: 0.68059441447258
RMSE train: 0.697700	val: 1.408498	test: 1.370894
MAE train: 0.539889	val: 1.105703	test: 1.081293

Epoch: 32
Loss: 0.7061015218496323
RMSE train: 0.628227	val: 1.206026	test: 1.259001
MAE train: 0.484139	val: 0.961073	test: 0.990585

Epoch: 33
Loss: 0.6567252725362778
RMSE train: 0.597744	val: 1.164601	test: 1.231699
MAE train: 0.458328	val: 0.935839	test: 0.950350

Epoch: 34
Loss: 0.6807234883308411
RMSE train: 0.568324	val: 1.159260	test: 1.204240
MAE train: 0.438591	val: 0.907977	test: 0.946582

Epoch: 35
Loss: 0.6692682504653931
RMSE train: 0.601542	val: 1.150141	test: 1.239415
MAE train: 0.469334	val: 0.881656	test: 0.997597

Epoch: 36
Loss: 0.7124040573835373
RMSE train: 0.601732	val: 1.185852	test: 1.205015
MAE train: 0.469950	val: 0.942912	test: 0.936151

Epoch: 37
Loss: 0.6444552540779114
RMSE train: 0.630172	val: 1.112117	test: 1.186103
MAE train: 0.498736	val: 0.887702	test: 0.923523

Epoch: 38
Loss: 0.6568121314048767
RMSE train: 0.629416	val: 1.178290	test: 1.228196
MAE train: 0.490742	val: 0.928935	test: 0.974462

Epoch: 39
Loss: 0.6369462013244629
RMSE train: 0.666310	val: 1.415558	test: 1.338847
MAE train: 0.516157	val: 1.082455	test: 1.056613

Epoch: 40
Loss: 0.6138703227043152
RMSE train: 0.610590	val: 1.231899	test: 1.279268
MAE train: 0.454380	val: 1.001153	test: 0.957196

Epoch: 41
Loss: 0.5603348314762115
RMSE train: 0.588543	val: 1.224250	test: 1.242371
MAE train: 0.445655	val: 0.978954	test: 0.935422

Epoch: 42
Loss: 0.6054189056158066
RMSE train: 0.632715	val: 1.305253	test: 1.313316
MAE train: 0.478112	val: 1.041165	test: 0.986165

Epoch: 43
Loss: 0.5635611116886139
RMSE train: 0.616314	val: 1.159090	test: 1.281888
MAE train: 0.460841	val: 0.938169	test: 0.993479

Epoch: 44
Loss: 0.5690961480140686
RMSE train: 0.598519	val: 1.233451	test: 1.243738
MAE train: 0.450454	val: 0.963525	test: 0.956239

Epoch: 45
Loss: 0.5356489270925522
RMSE train: 0.662487	val: 1.405336	test: 1.334352
MAE train: 0.502924	val: 1.079610	test: 1.023178

Epoch: 46
Loss: 0.5531632900238037
RMSE train: 0.535711	val: 1.228590	test: 1.192295
MAE train: 0.405188	val: 0.972841	test: 0.934740

Epoch: 47
Loss: 0.5120806470513344
RMSE train: 0.557962	val: 1.121173	test: 1.178349
MAE train: 0.422505	val: 0.932741	test: 0.920293

Epoch: 48
Loss: 0.5177237167954445
RMSE train: 0.551755	val: 1.114169	test: 1.155813
MAE train: 0.418830	val: 0.907255	test: 0.890460

Epoch: 49
Loss: 0.5404517576098442
RMSE train: 0.536522	val: 1.064283	test: 1.133890
MAE train: 0.407687	val: 0.848655	test: 0.888944

Epoch: 50
Loss: 0.49382687360048294
RMSE train: 0.542514	val: 1.119059	test: 1.158555
MAE train: 0.404336	val: 0.910856	test: 0.896167

Epoch: 51
Loss: 0.5236399620771408
RMSE train: 0.598379	val: 1.232350	test: 1.260741
MAE train: 0.455104	val: 0.996957	test: 0.948579

Epoch: 52
Loss: 0.6042681783437729
RMSE train: 0.518266	val: 1.159135	test: 1.178452
MAE train: 0.390458	val: 0.916592	test: 0.926441

Epoch: 53
Loss: 0.5369193032383919
RMSE train: 0.564633	val: 1.296385	test: 1.260790
MAE train: 0.424110	val: 1.005964	test: 0.993314

Epoch: 54
Loss: 0.5285887271165848
RMSE train: 0.672388	val: 1.406372	test: 1.364708
MAE train: 0.514775	val: 1.100880	test: 1.061054

Epoch: 55
Loss: 0.52458655834198
RMSE train: 0.539753	val: 1.186304	test: 1.212039
MAE train: 0.409914	val: 0.958184	test: 0.964873

Epoch: 56
Loss: 0.48063892126083374
RMSE train: 0.531746	val: 1.250453	test: 1.235453
MAE train: 0.402838	val: 1.002306	test: 0.974513

Epoch: 57
Loss: 0.47008833289146423
RMSE train: 0.593557	val: 1.320566	test: 1.288069
MAE train: 0.442602	val: 1.052341	test: 0.986315

Epoch: 58
Loss: 0.4575752317905426
RMSE train: 0.664673	val: 1.357838	test: 1.365445
MAE train: 0.489435	val: 1.079535	test: 1.016032

Epoch: 59
Loss: 0.4768872931599617
RMSE train: 0.646813	val: 1.245997	test: 1.281009
MAE train: 0.479871	val: 1.007211	test: 0.963332

Epoch: 60
Loss: 0.4745234325528145
RMSE train: 0.695446	val: 1.428512	test: 1.385040
MAE train: 0.511926	val: 1.123014	test: 1.035296

Epoch: 61
Loss: 0.45422976464033127
RMSE train: 0.619293	val: 1.234698	test: 1.274764
MAE train: 0.459285	val: 1.010147	test: 0.958200

Epoch: 62
Loss: 0.4614245742559433
RMSE train: 0.557712	val: 1.238009	test: 1.191745
MAE train: 0.415702	val: 0.975594	test: 0.897144

Epoch: 63
Loss: 0.49409856647253036
RMSE train: 0.521658	val: 1.237807	test: 1.156996
MAE train: 0.394393	val: 0.979387	test: 0.873972

Epoch: 64
Loss: 0.43860866129398346
RMSE train: 0.556860	val: 1.111165	test: 1.120968
MAE train: 0.432584	val: 0.878814	test: 0.886762

Epoch: 65
Loss: 0.43402961641550064
RMSE train: 0.565290	val: 1.388122	test: 1.227090
MAE train: 0.445940	val: 1.097453	test: 0.967692

Epoch: 66
Loss: 0.39909809082746506
RMSE train: 0.495919	val: 1.184035	test: 1.147305
MAE train: 0.380657	val: 0.936033	test: 0.921072

Epoch: 67
Loss: 0.5327972024679184
RMSE train: 0.507171	val: 1.143507	test: 1.129803
MAE train: 0.385596	val: 0.911822	test: 0.899417

Epoch: 68
Loss: 0.4098798334598541
RMSE train: 0.569057	val: 1.345107	test: 1.262652
MAE train: 0.430955	val: 1.075300	test: 0.975751

Epoch: 69
Loss: 0.4091069921851158
RMSE train: 0.550701	val: 1.151524	test: 1.176050
MAE train: 0.409998	val: 0.950244	test: 0.922778

Epoch: 70
Loss: 0.41526149958372116
RMSE train: 0.543342	val: 1.220809	test: 1.211773
MAE train: 0.406994	val: 0.989012	test: 0.941034

Epoch: 71
Loss: 0.42199648171663284
RMSE train: 0.560293	val: 1.273159	test: 1.232430
MAE train: 0.423123	val: 1.012490	test: 0.957554

Epoch: 72
Loss: 0.43400561064481735
RMSE train: 0.523188	val: 1.071603	test: 1.145937
MAE train: 0.392057	val: 0.867445	test: 0.917813

Epoch: 73
Loss: 0.4175332933664322
RMSE train: 0.504391	val: 1.131876	test: 1.167339
MAE train: 0.378076	val: 0.908662	test: 0.923551

Epoch: 74
Loss: 0.39115430414676666
RMSE train: 0.519562	val: 1.233227	test: 1.208559
MAE train: 0.393976	val: 0.969868	test: 0.951356

Epoch: 75
Loss: 0.4073745757341385
RMSE train: 0.487813	val: 1.160608	test: 1.155411
MAE train: 0.375757	val: 0.914962	test: 0.936307

Epoch: 76
Loss: 0.3820459470152855
RMSE train: 0.499894	val: 1.119907	test: 1.126412
MAE train: 0.385247	val: 0.888199	test: 0.908956

Epoch: 77
Loss: 0.4328114539384842
RMSE train: 0.511263	val: 1.203100	test: 1.135752
MAE train: 0.393412	val: 0.957043	test: 0.888433

Epoch: 78
Loss: 0.3610841631889343
RMSE train: 0.520932	val: 1.154053	test: 1.131226
MAE train: 0.399794	val: 0.915601	test: 0.902202

Epoch: 79
Loss: 0.357436403632164
RMSE train: 0.510597	val: 1.116340	test: 1.129138
MAE train: 0.394536	val: 0.886808	test: 0.905698

Epoch: 80
Loss: 0.4169583097100258
RMSE train: 0.488871	val: 1.145045	test: 1.149476
MAE train: 0.372938	val: 0.932315	test: 0.908752

Epoch: 81
Loss: 0.3776978552341461
RMSE train: 0.544710	val: 1.230699	test: 1.193852
MAE train: 0.413953	val: 0.980182	test: 0.933038

Epoch: 82
Loss: 0.38382235914468765
RMSE train: 0.540891	val: 1.183165	test: 1.172321
MAE train: 0.408780	val: 0.948579	test: 0.904942

Epoch: 83
Loss: 0.4265347570180893
RMSE train: 0.538090	val: 1.212098	test: 1.212813
MAE train: 0.396597	val: 0.979050	test: 0.918581
RMSE train: 0.805322	val: 1.427392	test: 1.525366
MAE train: 0.610366	val: 1.112631	test: 1.221945

Epoch: 24
Loss: 0.8989389836788177
RMSE train: 0.751010	val: 1.297266	test: 1.432209
MAE train: 0.562024	val: 1.030874	test: 1.134294

Epoch: 25
Loss: 0.9480735510587692
RMSE train: 0.745252	val: 1.253081	test: 1.382329
MAE train: 0.554581	val: 0.988037	test: 1.083933

Epoch: 26
Loss: 0.8694351315498352
RMSE train: 0.724666	val: 1.301259	test: 1.339473
MAE train: 0.556634	val: 0.995874	test: 1.052655

Epoch: 27
Loss: 0.747241199016571
RMSE train: 0.621475	val: 1.236982	test: 1.244057
MAE train: 0.476498	val: 0.959040	test: 0.982251

Epoch: 28
Loss: 0.7189215123653412
RMSE train: 0.689302	val: 1.269695	test: 1.223455
MAE train: 0.533600	val: 0.989588	test: 0.961185

Epoch: 29
Loss: 0.7119603008031845
RMSE train: 0.662915	val: 1.222408	test: 1.182729
MAE train: 0.509174	val: 0.958297	test: 0.923242

Epoch: 30
Loss: 0.7801840603351593
RMSE train: 0.597697	val: 1.226688	test: 1.233831
MAE train: 0.452790	val: 0.968543	test: 0.993942

Epoch: 31
Loss: 0.7764984220266342
RMSE train: 0.768373	val: 1.443883	test: 1.464435
MAE train: 0.591166	val: 1.118322	test: 1.158355

Epoch: 32
Loss: 0.7487560361623764
RMSE train: 0.671553	val: 1.275621	test: 1.316393
MAE train: 0.514993	val: 1.036661	test: 1.021763

Epoch: 33
Loss: 0.6930549740791321
RMSE train: 0.636702	val: 1.353991	test: 1.394314
MAE train: 0.484732	val: 1.073357	test: 1.121601

Epoch: 34
Loss: 0.6904908865690231
RMSE train: 0.678328	val: 1.388840	test: 1.433111
MAE train: 0.510217	val: 1.115422	test: 1.132456

Epoch: 35
Loss: 0.6280646622180939
RMSE train: 0.694533	val: 1.336669	test: 1.361231
MAE train: 0.522940	val: 1.104185	test: 1.060019

Epoch: 36
Loss: 0.6604996025562286
RMSE train: 0.667240	val: 1.291412	test: 1.314534
MAE train: 0.511628	val: 1.037875	test: 1.034489

Epoch: 37
Loss: 0.620661661028862
RMSE train: 0.626873	val: 1.219276	test: 1.289642
MAE train: 0.486625	val: 0.987911	test: 1.025264

Epoch: 38
Loss: 0.6559316217899323
RMSE train: 0.660928	val: 1.231678	test: 1.243805
MAE train: 0.502512	val: 1.008477	test: 0.956298

Epoch: 39
Loss: 0.6067700982093811
RMSE train: 0.648619	val: 1.205765	test: 1.216019
MAE train: 0.490276	val: 0.972729	test: 0.926615

Epoch: 40
Loss: 0.6658915877342224
RMSE train: 0.661474	val: 1.143401	test: 1.209748
MAE train: 0.503532	val: 0.916461	test: 0.959930

Epoch: 41
Loss: 0.6259525120258331
RMSE train: 0.689410	val: 1.371539	test: 1.370453
MAE train: 0.523511	val: 1.091864	test: 1.063038

Epoch: 42
Loss: 0.589808464050293
RMSE train: 0.668913	val: 1.331847	test: 1.336847
MAE train: 0.506270	val: 1.086105	test: 1.041960

Epoch: 43
Loss: 0.6001755893230438
RMSE train: 0.652407	val: 1.298068	test: 1.339286
MAE train: 0.495600	val: 1.057082	test: 1.045391

Epoch: 44
Loss: 0.5669799894094467
RMSE train: 0.625360	val: 1.278543	test: 1.305989
MAE train: 0.471668	val: 1.030498	test: 1.027177

Epoch: 45
Loss: 0.6264712139964104
RMSE train: 0.659813	val: 1.278724	test: 1.261622
MAE train: 0.502365	val: 1.024701	test: 0.968996

Epoch: 46
Loss: 0.5946935266256332
RMSE train: 0.647591	val: 1.146229	test: 1.170107
MAE train: 0.490245	val: 0.929744	test: 0.904251

Epoch: 47
Loss: 0.5216649025678635
RMSE train: 0.672811	val: 1.272749	test: 1.237734
MAE train: 0.508484	val: 1.019022	test: 0.940766

Epoch: 48
Loss: 0.6227197349071503
RMSE train: 0.697262	val: 1.333579	test: 1.280483
MAE train: 0.533399	val: 1.064813	test: 0.971106

Epoch: 49
Loss: 0.5872887372970581
RMSE train: 0.566892	val: 1.080709	test: 1.098693
MAE train: 0.422183	val: 0.857737	test: 0.856674

Epoch: 50
Loss: 0.5273882821202278
RMSE train: 0.559721	val: 1.112470	test: 1.103865
MAE train: 0.420921	val: 0.891359	test: 0.867275

Epoch: 51
Loss: 0.5002386495471001
RMSE train: 0.679689	val: 1.320542	test: 1.264885
MAE train: 0.516465	val: 1.050791	test: 0.995376

Epoch: 52
Loss: 0.5518090724945068
RMSE train: 0.597794	val: 1.260727	test: 1.300179
MAE train: 0.447239	val: 1.017731	test: 1.027714

Epoch: 53
Loss: 0.5086263343691826
RMSE train: 0.607165	val: 1.251309	test: 1.280338
MAE train: 0.454218	val: 1.022416	test: 1.015337

Epoch: 54
Loss: 0.5555344820022583
RMSE train: 0.592755	val: 1.188774	test: 1.206961
MAE train: 0.445320	val: 0.970629	test: 0.952798

Epoch: 55
Loss: 0.47575943171977997
RMSE train: 0.613531	val: 1.211139	test: 1.188659
MAE train: 0.463843	val: 0.975439	test: 0.923355

Epoch: 56
Loss: 0.47325970232486725
RMSE train: 0.647058	val: 1.222707	test: 1.224335
MAE train: 0.485728	val: 0.998449	test: 0.938662

Epoch: 57
Loss: 0.4285581186413765
RMSE train: 0.661704	val: 1.300053	test: 1.261532
MAE train: 0.500194	val: 1.046065	test: 0.975701

Epoch: 58
Loss: 0.48178335279226303
RMSE train: 0.735029	val: 1.444438	test: 1.386254
MAE train: 0.555549	val: 1.164682	test: 1.089303

Epoch: 59
Loss: 0.48293738067150116
RMSE train: 0.692258	val: 1.325810	test: 1.323529
MAE train: 0.523056	val: 1.093802	test: 1.026899

Epoch: 60
Loss: 0.49333325773477554
RMSE train: 0.697479	val: 1.308353	test: 1.312312
MAE train: 0.515695	val: 1.063760	test: 1.005807

Epoch: 61
Loss: 0.46148934960365295
RMSE train: 0.678315	val: 1.291951	test: 1.287659
MAE train: 0.510162	val: 1.057758	test: 0.988593

Epoch: 62
Loss: 0.4547072499990463
RMSE train: 0.592384	val: 1.279805	test: 1.270270
MAE train: 0.441658	val: 1.024202	test: 0.989212

Epoch: 63
Loss: 0.48365797847509384
RMSE train: 0.639514	val: 1.279836	test: 1.260169
MAE train: 0.469416	val: 1.030275	test: 0.980119

Epoch: 64
Loss: 0.45535580068826675
RMSE train: 0.706941	val: 1.357658	test: 1.283143
MAE train: 0.531800	val: 1.088695	test: 0.993649

Epoch: 65
Loss: 0.5001742318272591
RMSE train: 0.576319	val: 1.281678	test: 1.154622
MAE train: 0.430879	val: 1.025903	test: 0.893477

Epoch: 66
Loss: 0.454584077000618
RMSE train: 0.635672	val: 1.172038	test: 1.142516
MAE train: 0.478141	val: 0.953795	test: 0.870324

Epoch: 67
Loss: 0.5570609122514725
RMSE train: 0.695244	val: 1.340855	test: 1.232020
MAE train: 0.528974	val: 1.068188	test: 0.955001

Epoch: 68
Loss: 0.46606508642435074
RMSE train: 0.567406	val: 1.220403	test: 1.140730
MAE train: 0.427186	val: 0.971241	test: 0.892320

Epoch: 69
Loss: 0.4919993206858635
RMSE train: 0.612540	val: 1.234733	test: 1.184258
MAE train: 0.453947	val: 0.988834	test: 0.919616

Epoch: 70
Loss: 0.4268074557185173
RMSE train: 0.729086	val: 1.489785	test: 1.334322
MAE train: 0.533830	val: 1.178892	test: 1.044636

Epoch: 71
Loss: 0.4410684257745743
RMSE train: 0.572306	val: 1.187676	test: 1.148593
MAE train: 0.426635	val: 0.960698	test: 0.882889

Epoch: 72
Loss: 0.4031374230980873
RMSE train: 0.626854	val: 1.160864	test: 1.150318
MAE train: 0.466525	val: 0.947319	test: 0.899476

Epoch: 73
Loss: 0.4481629952788353
RMSE train: 0.646135	val: 1.313670	test: 1.268306
MAE train: 0.477341	val: 1.058750	test: 0.975043

Epoch: 74
Loss: 0.4593557193875313
RMSE train: 0.586029	val: 1.228319	test: 1.206931
MAE train: 0.432247	val: 0.992996	test: 0.932636

Epoch: 75
Loss: 0.45690347999334335
RMSE train: 0.614775	val: 1.219522	test: 1.214682
MAE train: 0.452187	val: 0.975800	test: 0.931247

Epoch: 76
Loss: 0.41890861839056015
RMSE train: 0.658611	val: 1.276754	test: 1.246284
MAE train: 0.486159	val: 1.017380	test: 0.953794

Epoch: 77
Loss: 0.40476229041814804
RMSE train: 0.582028	val: 1.168161	test: 1.169589
MAE train: 0.428950	val: 0.955980	test: 0.887598

Epoch: 78
Loss: 0.39783991128206253
RMSE train: 0.588166	val: 1.242758	test: 1.172663
MAE train: 0.434040	val: 0.983421	test: 0.890495

Epoch: 79
Loss: 0.4219171777367592
RMSE train: 0.599734	val: 1.266905	test: 1.200827
MAE train: 0.449251	val: 1.001205	test: 0.913897

Epoch: 80
Loss: 0.35246453434228897
RMSE train: 0.538766	val: 1.148535	test: 1.134042
MAE train: 0.400355	val: 0.931020	test: 0.882815

Epoch: 81
Loss: 0.3989380821585655
RMSE train: 0.568617	val: 1.204466	test: 1.180825
MAE train: 0.425402	val: 0.976256	test: 0.912237

Epoch: 82
Loss: 0.40315205603837967
RMSE train: 0.633394	val: 1.334546	test: 1.277989
MAE train: 0.472741	val: 1.052811	test: 0.996894

Epoch: 83
Loss: 0.3935741260647774
RMSE train: 0.564950	val: 1.148725	test: 1.167503
MAE train: 0.418392	val: 0.937370	test: 0.908301
RMSE train: 0.706397	val: 1.117274	test: 1.273087
MAE train: 0.542919	val: 0.904730	test: 1.010264

Epoch: 24
Loss: 0.9233057498931885
RMSE train: 0.669966	val: 1.144982	test: 1.280644
MAE train: 0.506770	val: 0.904041	test: 1.022830

Epoch: 25
Loss: 0.8242586404085159
RMSE train: 0.674536	val: 1.163691	test: 1.280402
MAE train: 0.502794	val: 0.941698	test: 1.002703

Epoch: 26
Loss: 0.7331718057394028
RMSE train: 0.635572	val: 1.135174	test: 1.215698
MAE train: 0.484072	val: 0.910970	test: 0.954245

Epoch: 27
Loss: 0.7892587780952454
RMSE train: 0.635121	val: 1.112268	test: 1.195571
MAE train: 0.491212	val: 0.898848	test: 0.940756

Epoch: 28
Loss: 0.7793630510568619
RMSE train: 0.657151	val: 1.141272	test: 1.210603
MAE train: 0.495633	val: 0.936445	test: 0.940371

Epoch: 29
Loss: 0.7184469252824783
RMSE train: 0.680837	val: 1.293125	test: 1.310576
MAE train: 0.500661	val: 1.046073	test: 1.032820

Epoch: 30
Loss: 0.7409098446369171
RMSE train: 0.658941	val: 1.230345	test: 1.249500
MAE train: 0.481293	val: 0.999102	test: 0.983513

Epoch: 31
Loss: 0.6973013579845428
RMSE train: 0.695640	val: 1.181404	test: 1.228726
MAE train: 0.512495	val: 0.977734	test: 0.949241

Epoch: 32
Loss: 0.6650757342576981
RMSE train: 0.689038	val: 1.236958	test: 1.221647
MAE train: 0.514484	val: 1.003784	test: 0.944641

Epoch: 33
Loss: 0.6736897379159927
RMSE train: 0.632762	val: 1.113786	test: 1.119699
MAE train: 0.474262	val: 0.924579	test: 0.863787

Epoch: 34
Loss: 0.6949905753135681
RMSE train: 0.624797	val: 1.185534	test: 1.152942
MAE train: 0.461654	val: 0.968741	test: 0.887004

Epoch: 35
Loss: 0.6786088198423386
RMSE train: 0.639006	val: 1.204895	test: 1.182863
MAE train: 0.474262	val: 0.984848	test: 0.913824

Epoch: 36
Loss: 0.5787013322114944
RMSE train: 0.631679	val: 1.134352	test: 1.140222
MAE train: 0.469501	val: 0.935195	test: 0.878263

Epoch: 37
Loss: 0.6056516766548157
RMSE train: 0.668310	val: 1.208270	test: 1.192101
MAE train: 0.489378	val: 0.980955	test: 0.905987

Epoch: 38
Loss: 0.5817536860704422
RMSE train: 0.681911	val: 1.162303	test: 1.137860
MAE train: 0.492658	val: 0.958126	test: 0.872225

Epoch: 39
Loss: 0.6179441213607788
RMSE train: 0.609794	val: 1.180177	test: 1.157564
MAE train: 0.455741	val: 0.958990	test: 0.902250

Epoch: 40
Loss: 0.5801585465669632
RMSE train: 0.669838	val: 1.266520	test: 1.242509
MAE train: 0.499238	val: 1.004245	test: 0.985987

Epoch: 41
Loss: 0.6110293716192245
RMSE train: 0.684825	val: 1.235828	test: 1.228254
MAE train: 0.505507	val: 0.992690	test: 0.965601

Epoch: 42
Loss: 0.5819987878203392
RMSE train: 0.638888	val: 1.155347	test: 1.153597
MAE train: 0.475107	val: 0.953509	test: 0.890188

Epoch: 43
Loss: 0.5862629860639572
RMSE train: 0.629243	val: 1.277518	test: 1.237070
MAE train: 0.468347	val: 1.058674	test: 0.988632

Epoch: 44
Loss: 0.5556038916110992
RMSE train: 0.599844	val: 1.170711	test: 1.161106
MAE train: 0.449942	val: 0.972347	test: 0.889074

Epoch: 45
Loss: 0.6349428296089172
RMSE train: 0.546775	val: 0.995571	test: 1.077731
MAE train: 0.425024	val: 0.831334	test: 0.866779

Epoch: 46
Loss: 0.5268933773040771
RMSE train: 0.559180	val: 1.253556	test: 1.172282
MAE train: 0.424870	val: 1.029993	test: 0.939202

Epoch: 47
Loss: 0.5248223319649696
RMSE train: 0.600877	val: 1.214630	test: 1.148673
MAE train: 0.443166	val: 0.998418	test: 0.859287

Epoch: 48
Loss: 0.5443930774927139
RMSE train: 0.610440	val: 1.270017	test: 1.142791
MAE train: 0.455334	val: 1.030321	test: 0.867486

Epoch: 49
Loss: 0.5630426481366158
RMSE train: 0.524444	val: 1.067848	test: 1.038374
MAE train: 0.405469	val: 0.879793	test: 0.842132

Epoch: 50
Loss: 0.5210003033280373
RMSE train: 0.624432	val: 1.251205	test: 1.190620
MAE train: 0.471909	val: 0.998495	test: 0.917862

Epoch: 51
Loss: 0.5231296271085739
RMSE train: 0.630055	val: 1.245741	test: 1.236657
MAE train: 0.473043	val: 0.995845	test: 0.960886

Epoch: 52
Loss: 0.46114643663167953
RMSE train: 0.569987	val: 1.090080	test: 1.118711
MAE train: 0.439072	val: 0.889559	test: 0.907230

Epoch: 53
Loss: 0.4757898673415184
RMSE train: 0.550641	val: 1.220019	test: 1.162754
MAE train: 0.414550	val: 0.975988	test: 0.932266

Epoch: 54
Loss: 0.5265534073114395
RMSE train: 0.617728	val: 1.305943	test: 1.258054
MAE train: 0.454576	val: 1.029009	test: 0.981662

Epoch: 55
Loss: 0.44240573048591614
RMSE train: 0.592952	val: 1.170678	test: 1.149260
MAE train: 0.440015	val: 0.976245	test: 0.868953

Epoch: 56
Loss: 0.45885003358125687
RMSE train: 0.561099	val: 1.149874	test: 1.135868
MAE train: 0.415235	val: 0.953791	test: 0.892694

Epoch: 57
Loss: 0.5071664080023766
RMSE train: 0.572569	val: 1.238646	test: 1.209998
MAE train: 0.420723	val: 1.009475	test: 0.954825

Epoch: 58
Loss: 0.5094589665532112
RMSE train: 0.576043	val: 1.136730	test: 1.142806
MAE train: 0.428810	val: 0.956435	test: 0.885701

Epoch: 59
Loss: 0.4750085324048996
RMSE train: 0.591873	val: 1.227958	test: 1.177519
MAE train: 0.445892	val: 0.999884	test: 0.932977

Epoch: 60
Loss: 0.4723798632621765
RMSE train: 0.588026	val: 1.066221	test: 1.081854
MAE train: 0.433498	val: 0.904298	test: 0.827963

Epoch: 61
Loss: 0.49239473789930344
RMSE train: 0.620735	val: 1.267281	test: 1.163862
MAE train: 0.463706	val: 1.019189	test: 0.911183

Epoch: 62
Loss: 0.48761487007141113
RMSE train: 0.524967	val: 1.137099	test: 1.075523
MAE train: 0.397118	val: 0.941440	test: 0.853066

Epoch: 63
Loss: 0.5163471102714539
RMSE train: 0.607434	val: 1.262693	test: 1.219000
MAE train: 0.442226	val: 1.007142	test: 0.955385

Epoch: 64
Loss: 0.43288882821798325
RMSE train: 0.566577	val: 1.190738	test: 1.180454
MAE train: 0.412402	val: 0.969009	test: 0.918609

Epoch: 65
Loss: 0.4439038038253784
RMSE train: 0.547251	val: 1.105682	test: 1.126589
MAE train: 0.403699	val: 0.926875	test: 0.877139

Epoch: 66
Loss: 0.41757141798734665
RMSE train: 0.567585	val: 1.173302	test: 1.143169
MAE train: 0.416193	val: 0.967148	test: 0.874169

Epoch: 67
Loss: 0.4058360606431961
RMSE train: 0.548252	val: 1.156760	test: 1.122338
MAE train: 0.400074	val: 0.960883	test: 0.863816

Epoch: 68
Loss: 0.39586129784584045
RMSE train: 0.561059	val: 1.167672	test: 1.154313
MAE train: 0.415156	val: 0.968487	test: 0.891285

Epoch: 69
Loss: 0.4116404056549072
RMSE train: 0.549188	val: 1.185811	test: 1.188285
MAE train: 0.414951	val: 0.977799	test: 0.931646

Epoch: 70
Loss: 0.43563660234212875
RMSE train: 0.538388	val: 1.196047	test: 1.182308
MAE train: 0.409100	val: 0.992709	test: 0.924170

Epoch: 71
Loss: 0.4368840605020523
RMSE train: 0.582848	val: 1.329473	test: 1.245555
MAE train: 0.435146	val: 1.077073	test: 0.967017

Epoch: 72
Loss: 0.38673947751522064
RMSE train: 0.541741	val: 1.149587	test: 1.134368
MAE train: 0.405505	val: 0.957191	test: 0.889613

Epoch: 73
Loss: 0.3583496958017349
RMSE train: 0.540872	val: 1.169171	test: 1.118203
MAE train: 0.407091	val: 0.983393	test: 0.855250

Epoch: 74
Loss: 0.39299383759498596
RMSE train: 0.512855	val: 1.165082	test: 1.106930
MAE train: 0.389958	val: 0.974665	test: 0.863208

Epoch: 75
Loss: 0.4182882085442543
RMSE train: 0.537150	val: 1.088919	test: 1.100306
MAE train: 0.402236	val: 0.931935	test: 0.859931

Epoch: 76
Loss: 0.38823020458221436
RMSE train: 0.632891	val: 1.263775	test: 1.228833
MAE train: 0.473449	val: 1.032562	test: 0.938495

Epoch: 77
Loss: 0.4090185686945915
RMSE train: 0.508156	val: 1.123198	test: 1.116915
MAE train: 0.379578	val: 0.934909	test: 0.884516

Epoch: 78
Loss: 0.4842984527349472
RMSE train: 0.513151	val: 1.079909	test: 1.104468
MAE train: 0.389430	val: 0.909853	test: 0.889442

Epoch: 79
Loss: 0.4222482666373253
RMSE train: 0.568746	val: 1.329219	test: 1.287831
MAE train: 0.445116	val: 1.060385	test: 1.018923

Epoch: 80
Loss: 0.3696635887026787
RMSE train: 0.525141	val: 1.123125	test: 1.100019
MAE train: 0.404783	val: 0.955592	test: 0.881648

Epoch: 81
Loss: 0.417557455599308
RMSE train: 0.532111	val: 1.242963	test: 1.146808
MAE train: 0.412086	val: 1.018414	test: 0.891727

Epoch: 82
Loss: 0.37892159074544907
RMSE train: 0.546707	val: 1.267321	test: 1.174906
MAE train: 0.410603	val: 1.034881	test: 0.913184

Epoch: 83
Loss: 0.37930409610271454
RMSE train: 0.570484	val: 1.082632	test: 1.101129
MAE train: 0.425289	val: 0.925660	test: 0.859300

Epoch: 84
Loss: 0.36075981458028156
RMSE train: 0.581912	val: 1.872544	test: 1.712419
MAE train: 0.435277	val: 1.468985	test: 1.357467

Epoch: 85
Loss: 0.39606520533561707
RMSE train: 0.518265	val: 1.927764	test: 1.708832
MAE train: 0.382909	val: 1.508554	test: 1.354894

Epoch: 86
Loss: 0.3802177608013153
RMSE train: 0.509016	val: 1.919811	test: 1.619265
MAE train: 0.369023	val: 1.510556	test: 1.290983

Epoch: 87
Loss: 0.48559868335723877
RMSE train: 0.558977	val: 1.887324	test: 1.635050
MAE train: 0.415696	val: 1.481439	test: 1.293050

Epoch: 88
Loss: 0.35864971081415814
RMSE train: 0.655137	val: 1.889306	test: 1.680911
MAE train: 0.481663	val: 1.473411	test: 1.320247

Epoch: 89
Loss: 0.46846161286036175
RMSE train: 0.556698	val: 1.897046	test: 1.645975
MAE train: 0.410230	val: 1.473795	test: 1.315058

Epoch: 90
Loss: 0.34922390182813007
RMSE train: 0.518913	val: 1.925249	test: 1.648586
MAE train: 0.382774	val: 1.483686	test: 1.330833

Epoch: 91
Loss: 0.32428916295369464
RMSE train: 0.519797	val: 1.938942	test: 1.675643
MAE train: 0.394055	val: 1.486626	test: 1.336051

Epoch: 92
Loss: 0.34957947333653766
RMSE train: 0.547025	val: 1.931503	test: 1.659296
MAE train: 0.413833	val: 1.481816	test: 1.315124

Epoch: 93
Loss: 0.330150842666626
RMSE train: 0.532879	val: 1.866990	test: 1.556050
MAE train: 0.391644	val: 1.441906	test: 1.236262

Epoch: 94
Loss: 0.3269327978293101
RMSE train: 0.503023	val: 1.860929	test: 1.524745
MAE train: 0.368067	val: 1.443269	test: 1.216917

Epoch: 95
Loss: 0.3465169866879781
RMSE train: 0.444677	val: 1.984513	test: 1.653490
MAE train: 0.335698	val: 1.526300	test: 1.336100

Epoch: 96
Loss: 0.33755308389663696
RMSE train: 0.481660	val: 2.052774	test: 1.746335
MAE train: 0.364126	val: 1.568423	test: 1.417246

Epoch: 97
Loss: 0.3313402533531189
RMSE train: 0.553571	val: 1.899835	test: 1.594133
MAE train: 0.425302	val: 1.472404	test: 1.245356

Epoch: 98
Loss: 0.41120778520901996
RMSE train: 0.482537	val: 2.008049	test: 1.652114
MAE train: 0.371293	val: 1.537360	test: 1.320241

Epoch: 99
Loss: 0.31019025047620136
RMSE train: 0.450559	val: 2.060853	test: 1.715947
MAE train: 0.343726	val: 1.562981	test: 1.374454

Epoch: 100
Loss: 0.3331228196620941
RMSE train: 0.482792	val: 1.978699	test: 1.689373
MAE train: 0.375481	val: 1.517603	test: 1.326030

Epoch: 101
Loss: 0.29159146547317505
RMSE train: 0.486441	val: 1.918362	test: 1.632935
MAE train: 0.371933	val: 1.487956	test: 1.277613

Epoch: 102
Loss: 0.34897900621096295
RMSE train: 0.493346	val: 1.913943	test: 1.589118
MAE train: 0.363618	val: 1.485125	test: 1.255377

Epoch: 103
Loss: 0.3905420204003652
RMSE train: 0.472784	val: 1.926435	test: 1.636297
MAE train: 0.363216	val: 1.483927	test: 1.281113

Epoch: 104
Loss: 0.2879294107357661
RMSE train: 0.523306	val: 1.948312	test: 1.713721
MAE train: 0.413035	val: 1.495862	test: 1.350592

Epoch: 105
Loss: 0.31876254081726074
RMSE train: 0.490227	val: 1.850639	test: 1.574784
MAE train: 0.368805	val: 1.433850	test: 1.240605

Epoch: 106
Loss: 0.3335873981316884
RMSE train: 0.472538	val: 1.892889	test: 1.564676
MAE train: 0.352007	val: 1.453365	test: 1.251890

Epoch: 107
Loss: 0.3482045332590739
RMSE train: 0.435945	val: 1.967318	test: 1.672001
MAE train: 0.324772	val: 1.491865	test: 1.334741

Epoch: 108
Loss: 0.29328619440396625
RMSE train: 0.469601	val: 1.893497	test: 1.612552
MAE train: 0.351778	val: 1.465409	test: 1.280931

Epoch: 109
Loss: 0.27571917573610943
RMSE train: 0.485804	val: 1.882745	test: 1.620550
MAE train: 0.362959	val: 1.469925	test: 1.280087

Epoch: 110
Loss: 0.3327218492825826
RMSE train: 0.487357	val: 1.921473	test: 1.684036
MAE train: 0.370714	val: 1.489200	test: 1.319435

Epoch: 111
Loss: 0.34700841705004376
RMSE train: 0.491981	val: 1.876851	test: 1.614033
MAE train: 0.370090	val: 1.462107	test: 1.274442

Epoch: 112
Loss: 0.3470858534177144
RMSE train: 0.508099	val: 1.821012	test: 1.577870
MAE train: 0.372778	val: 1.424404	test: 1.246728

Epoch: 113
Loss: 0.329708069562912
RMSE train: 0.486507	val: 1.894344	test: 1.669579
MAE train: 0.359867	val: 1.454601	test: 1.309981

Epoch: 114
Loss: 0.2893661558628082
RMSE train: 0.484552	val: 1.923068	test: 1.725808
MAE train: 0.360399	val: 1.472513	test: 1.352363

Epoch: 115
Loss: 0.29822736978530884
RMSE train: 0.531921	val: 1.827428	test: 1.639673
MAE train: 0.393828	val: 1.413900	test: 1.271823

Epoch: 116
Loss: 0.30724693338076275
RMSE train: 0.533624	val: 1.799070	test: 1.591372
MAE train: 0.399471	val: 1.411764	test: 1.253941

Epoch: 117
Loss: 0.290931095679601
RMSE train: 0.468614	val: 1.919859	test: 1.724127
MAE train: 0.361764	val: 1.470800	test: 1.351318

Epoch: 118
Loss: 0.3587467471758525
RMSE train: 0.497269	val: 1.927158	test: 1.753954
MAE train: 0.380250	val: 1.471576	test: 1.367928

Epoch: 119
Loss: 0.3530973692735036
RMSE train: 0.530663	val: 1.901253	test: 1.680607
MAE train: 0.394989	val: 1.453042	test: 1.310692

Epoch: 120
Loss: 0.3090895315011342
RMSE train: 0.480374	val: 1.975781	test: 1.760325
MAE train: 0.354697	val: 1.489380	test: 1.376472

Epoch: 121
Loss: 0.29657475153605145
RMSE train: 0.485431	val: 1.978955	test: 1.805384
MAE train: 0.366109	val: 1.504381	test: 1.403882

Early stopping
Best (RMSE):	 train: 0.738451	val: 1.759636	test: 1.649214
Best (MAE):	 train: 0.539532	val: 1.398373	test: 1.325140


Epoch: 84
Loss: 0.38122979799906415
RMSE train: 0.544396	val: 1.761873	test: 1.554116
MAE train: 0.379108	val: 1.363331	test: 1.217221

Epoch: 85
Loss: 0.3655657072861989
RMSE train: 0.478151	val: 1.757727	test: 1.533198
MAE train: 0.352019	val: 1.376695	test: 1.195819

Epoch: 86
Loss: 0.3117123742898305
RMSE train: 0.467184	val: 1.756678	test: 1.566371
MAE train: 0.344734	val: 1.385793	test: 1.223126

Epoch: 87
Loss: 0.40241095423698425
RMSE train: 0.543218	val: 1.714013	test: 1.544378
MAE train: 0.390465	val: 1.355049	test: 1.193913

Epoch: 88
Loss: 0.4085335930188497
RMSE train: 0.567579	val: 1.735338	test: 1.563307
MAE train: 0.411333	val: 1.385718	test: 1.200475

Epoch: 89
Loss: 0.32584279775619507
RMSE train: 0.482848	val: 1.794011	test: 1.592906
MAE train: 0.352809	val: 1.418041	test: 1.233526

Epoch: 90
Loss: 0.4093744456768036
RMSE train: 0.513622	val: 1.774369	test: 1.583432
MAE train: 0.365079	val: 1.415828	test: 1.226047

Epoch: 91
Loss: 0.32635231812795
RMSE train: 0.606830	val: 1.798526	test: 1.642425
MAE train: 0.444595	val: 1.447783	test: 1.261604

Epoch: 92
Loss: 0.3486799895763397
RMSE train: 0.589545	val: 1.829308	test: 1.634594
MAE train: 0.432528	val: 1.453009	test: 1.247139

Epoch: 93
Loss: 0.36681782205899555
RMSE train: 0.591443	val: 1.787774	test: 1.568277
MAE train: 0.433235	val: 1.412388	test: 1.212514

Epoch: 94
Loss: 0.34822461009025574
RMSE train: 0.571794	val: 1.771655	test: 1.566170
MAE train: 0.415254	val: 1.395478	test: 1.210056

Epoch: 95
Loss: 0.38037895162900287
RMSE train: 0.576792	val: 1.768034	test: 1.585388
MAE train: 0.421499	val: 1.398670	test: 1.218556

Epoch: 96
Loss: 0.3236759801705678
RMSE train: 0.522520	val: 1.809929	test: 1.581670
MAE train: 0.387574	val: 1.434491	test: 1.219700

Epoch: 97
Loss: 0.34421148896217346
RMSE train: 0.557141	val: 1.821901	test: 1.567872
MAE train: 0.401856	val: 1.441598	test: 1.225171

Epoch: 98
Loss: 0.28177132705847424
RMSE train: 0.567739	val: 1.819531	test: 1.575758
MAE train: 0.411056	val: 1.446279	test: 1.229749

Epoch: 99
Loss: 0.3276597162087758
RMSE train: 0.562459	val: 1.838889	test: 1.618099
MAE train: 0.417835	val: 1.477591	test: 1.258099

Epoch: 100
Loss: 0.3312144875526428
RMSE train: 0.558696	val: 1.799811	test: 1.605338
MAE train: 0.413368	val: 1.448458	test: 1.253014

Epoch: 101
Loss: 0.31434162457784015
RMSE train: 0.596511	val: 1.774835	test: 1.575342
MAE train: 0.436451	val: 1.413787	test: 1.222343

Epoch: 102
Loss: 0.3581749002138774
RMSE train: 0.547645	val: 1.780153	test: 1.597007
MAE train: 0.396053	val: 1.413713	test: 1.243812

Epoch: 103
Loss: 0.2781501313050588
RMSE train: 0.514177	val: 1.735267	test: 1.572552
MAE train: 0.379400	val: 1.384887	test: 1.215513

Epoch: 104
Loss: 0.3080425560474396
RMSE train: 0.472449	val: 1.704898	test: 1.547775
MAE train: 0.349391	val: 1.351569	test: 1.209403

Epoch: 105
Loss: 0.3281681537628174
RMSE train: 0.441276	val: 1.726774	test: 1.611359
MAE train: 0.329696	val: 1.374893	test: 1.267793

Epoch: 106
Loss: 0.31765925884246826
RMSE train: 0.443650	val: 1.733746	test: 1.670081
MAE train: 0.332424	val: 1.381113	test: 1.317490

Epoch: 107
Loss: 0.2712999681631724
RMSE train: 0.470061	val: 1.708305	test: 1.629725
MAE train: 0.349402	val: 1.356569	test: 1.277311

Epoch: 108
Loss: 0.36159441868464154
RMSE train: 0.535433	val: 1.684220	test: 1.595357
MAE train: 0.391169	val: 1.322633	test: 1.243893

Epoch: 109
Loss: 0.31370916962623596
RMSE train: 0.511497	val: 1.713482	test: 1.591419
MAE train: 0.372672	val: 1.339992	test: 1.254873

Epoch: 110
Loss: 0.3040417532126109
RMSE train: 0.493759	val: 1.743019	test: 1.636545
MAE train: 0.366568	val: 1.376372	test: 1.285475

Epoch: 111
Loss: 0.31803982456525165
RMSE train: 0.568925	val: 1.738475	test: 1.602587
MAE train: 0.391784	val: 1.366003	test: 1.260619

Epoch: 112
Loss: 0.32025424639383954
RMSE train: 0.512146	val: 1.724809	test: 1.573130
MAE train: 0.372204	val: 1.377620	test: 1.230395

Epoch: 113
Loss: 0.336411048968633
RMSE train: 0.504280	val: 1.769269	test: 1.619628
MAE train: 0.376023	val: 1.415100	test: 1.269107

Epoch: 114
Loss: 0.2858765621980031
RMSE train: 0.524460	val: 1.771393	test: 1.579614
MAE train: 0.391881	val: 1.418827	test: 1.223683

Epoch: 115
Loss: 0.31723495324452716
RMSE train: 0.573516	val: 1.766422	test: 1.589757
MAE train: 0.419576	val: 1.413207	test: 1.218734

Epoch: 116
Loss: 0.2713306248188019
RMSE train: 0.557626	val: 1.775703	test: 1.599958
MAE train: 0.403415	val: 1.422882	test: 1.226541

Epoch: 117
Loss: 0.3170100549856822
RMSE train: 0.485239	val: 1.784771	test: 1.594403
MAE train: 0.361661	val: 1.421428	test: 1.244488

Epoch: 118
Loss: 0.2797574996948242
RMSE train: 0.524396	val: 1.728666	test: 1.555898
MAE train: 0.387315	val: 1.374339	test: 1.209710

Epoch: 119
Loss: 0.28159111738204956
RMSE train: 0.516446	val: 1.722444	test: 1.585240
MAE train: 0.379760	val: 1.374159	test: 1.239825

Epoch: 120
Loss: 0.29412811001141864
RMSE train: 0.543665	val: 1.832327	test: 1.754839
MAE train: 0.400632	val: 1.457023	test: 1.375640

Epoch: 121
Loss: 0.32917116085688275
RMSE train: 0.502138	val: 1.732469	test: 1.597992
MAE train: 0.364996	val: 1.368865	test: 1.263455

Epoch: 122
Loss: 0.29566089312235516
RMSE train: 0.514643	val: 1.734390	test: 1.577547
MAE train: 0.368302	val: 1.362251	test: 1.253557

Epoch: 123
Loss: 0.2998406986395518
RMSE train: 0.460130	val: 1.772153	test: 1.626358
MAE train: 0.336111	val: 1.407160	test: 1.278097

Epoch: 124
Loss: 0.29584741592407227
RMSE train: 0.445826	val: 1.851563	test: 1.731536
MAE train: 0.332829	val: 1.480683	test: 1.365933

Epoch: 125
Loss: 0.32643140355745953
RMSE train: 0.524117	val: 1.711315	test: 1.555160
MAE train: 0.376907	val: 1.351976	test: 1.223622

Epoch: 126
Loss: 0.3207036356131236
RMSE train: 0.481878	val: 1.721365	test: 1.541264
MAE train: 0.356447	val: 1.358881	test: 1.216540

Epoch: 127
Loss: 0.3075914879639943
RMSE train: 0.410287	val: 1.778449	test: 1.660153
MAE train: 0.312677	val: 1.435683	test: 1.300031

Epoch: 128
Loss: 0.2835397521654765
RMSE train: 0.486631	val: 1.762910	test: 1.677774
MAE train: 0.368985	val: 1.420609	test: 1.299139

Epoch: 129
Loss: 0.2996145685513814
RMSE train: 0.478761	val: 1.680318	test: 1.539701
MAE train: 0.356753	val: 1.328419	test: 1.182833

Epoch: 130
Loss: 0.2740819255510966
RMSE train: 0.451206	val: 1.699142	test: 1.538843
MAE train: 0.341061	val: 1.330010	test: 1.198065

Epoch: 131
Loss: 0.31222183505694073
RMSE train: 0.409126	val: 1.691080	test: 1.566031
MAE train: 0.308562	val: 1.338841	test: 1.211092

Epoch: 132
Loss: 0.2843969364960988
RMSE train: 0.529533	val: 1.664268	test: 1.586509
MAE train: 0.407285	val: 1.327329	test: 1.208003

Epoch: 133
Loss: 0.31377578775087994
RMSE train: 0.516719	val: 1.655190	test: 1.546300
MAE train: 0.380394	val: 1.297622	test: 1.180058

Epoch: 134
Loss: 0.28570197025934857
RMSE train: 0.479819	val: 1.680798	test: 1.550062
MAE train: 0.349491	val: 1.310280	test: 1.198577

Epoch: 135
Loss: 0.27938222885131836
RMSE train: 0.448107	val: 1.689754	test: 1.556249
MAE train: 0.330206	val: 1.317505	test: 1.209378

Epoch: 136
Loss: 0.2691786785920461
RMSE train: 0.530252	val: 1.703635	test: 1.588276
MAE train: 0.372169	val: 1.331470	test: 1.234979

Epoch: 137
Loss: 0.27791827917099
RMSE train: 0.502585	val: 1.673142	test: 1.594937
MAE train: 0.378541	val: 1.328428	test: 1.225800

Epoch: 138
Loss: 0.3444667359193166
RMSE train: 0.469583	val: 1.713438	test: 1.659955
MAE train: 0.348008	val: 1.364527	test: 1.298918

Epoch: 139
Loss: 0.26948192218939465
RMSE train: 0.510767	val: 1.723812	test: 1.569634
MAE train: 0.360259	val: 1.348441	test: 1.230150

Epoch: 140
Loss: 0.28274474541346234
RMSE train: 0.455876	val: 1.694336	test: 1.590141
MAE train: 0.334965	val: 1.344690	test: 1.242135

Epoch: 141
Loss: 0.22553368906180063
RMSE train: 0.455905	val: 1.693204	test: 1.646120
MAE train: 0.346475	val: 1.358626	test: 1.288901

Epoch: 142
Loss: 0.3049170672893524
RMSE train: 0.463982	val: 1.690473	test: 1.578776
MAE train: 0.342387	val: 1.340984	test: 1.240752

Epoch: 143
Loss: 0.2589336136976878
RMSE train: 0.485914	val: 1.692720	test: 1.577148
MAE train: 0.353408	val: 1.340550	test: 1.237401

Epoch: 144
Loss: 0.2650328278541565

Epoch: 84
Loss: 0.4206306536992391
RMSE train: 0.516317	val: 1.862600	test: 1.795004
MAE train: 0.403333	val: 1.489766	test: 1.434634

Epoch: 85
Loss: 0.4239768087863922
RMSE train: 0.445513	val: 1.851568	test: 1.743684
MAE train: 0.328355	val: 1.487430	test: 1.399895

Epoch: 86
Loss: 0.37736615538597107
RMSE train: 0.535790	val: 1.773463	test: 1.673609
MAE train: 0.386558	val: 1.456648	test: 1.347795

Epoch: 87
Loss: 0.3637413481871287
RMSE train: 0.579686	val: 1.834553	test: 1.732950
MAE train: 0.431079	val: 1.477456	test: 1.389996

Epoch: 88
Loss: 0.37232569853464764
RMSE train: 0.499749	val: 1.923723	test: 1.789525
MAE train: 0.373603	val: 1.532446	test: 1.429915

Epoch: 89
Loss: 0.3635534942150116
RMSE train: 0.480776	val: 1.906192	test: 1.769984
MAE train: 0.355226	val: 1.532512	test: 1.412176

Epoch: 90
Loss: 0.3477813998858134
RMSE train: 0.509533	val: 1.841635	test: 1.737911
MAE train: 0.386060	val: 1.499278	test: 1.377343

Epoch: 91
Loss: 0.3510031799475352
RMSE train: 0.526377	val: 1.780143	test: 1.729399
MAE train: 0.404507	val: 1.450925	test: 1.360051

Epoch: 92
Loss: 0.34017810225486755
RMSE train: 0.473664	val: 1.791459	test: 1.777378
MAE train: 0.357609	val: 1.437573	test: 1.398829

Epoch: 93
Loss: 0.3869357605775197
RMSE train: 0.457303	val: 1.733833	test: 1.728177
MAE train: 0.338205	val: 1.400183	test: 1.369079

Epoch: 94
Loss: 0.37762322028477985
RMSE train: 0.549242	val: 1.654638	test: 1.660275
MAE train: 0.408272	val: 1.354428	test: 1.343601

Epoch: 95
Loss: 0.33008211851119995
RMSE train: 0.488361	val: 1.714021	test: 1.719544
MAE train: 0.364825	val: 1.393983	test: 1.371787

Epoch: 96
Loss: 0.35274872183799744
RMSE train: 0.493105	val: 1.789042	test: 1.837093
MAE train: 0.369360	val: 1.438072	test: 1.464766

Epoch: 97
Loss: 0.4531249503294627
RMSE train: 0.512534	val: 1.658338	test: 1.668153
MAE train: 0.385552	val: 1.346388	test: 1.318294

Epoch: 98
Loss: 0.34587301810582477
RMSE train: 0.614635	val: 1.635641	test: 1.648328
MAE train: 0.463518	val: 1.343316	test: 1.317694

Epoch: 99
Loss: 0.403405765692393
RMSE train: 0.460321	val: 1.774220	test: 1.715038
MAE train: 0.355000	val: 1.412770	test: 1.370304

Epoch: 100
Loss: 0.4198392530282338
RMSE train: 0.444014	val: 1.818216	test: 1.762598
MAE train: 0.343359	val: 1.426289	test: 1.400694

Epoch: 101
Loss: 0.3519676725069682
RMSE train: 0.505723	val: 1.653843	test: 1.563788
MAE train: 0.377375	val: 1.330952	test: 1.257209

Epoch: 102
Loss: 0.3865307966868083
RMSE train: 0.517634	val: 1.646080	test: 1.601098
MAE train: 0.392885	val: 1.334227	test: 1.276021

Epoch: 103
Loss: 0.3869590957959493
RMSE train: 0.481925	val: 1.704348	test: 1.667561
MAE train: 0.374651	val: 1.384311	test: 1.317585

Epoch: 104
Loss: 0.3524877925713857
RMSE train: 0.454817	val: 1.727524	test: 1.711512
MAE train: 0.344614	val: 1.404384	test: 1.336150

Epoch: 105
Loss: 0.3346192439397176
RMSE train: 0.496082	val: 1.697566	test: 1.747341
MAE train: 0.371172	val: 1.381954	test: 1.352671

Epoch: 106
Loss: 0.30619102716445923
RMSE train: 0.497933	val: 1.689130	test: 1.728994
MAE train: 0.375883	val: 1.376909	test: 1.344812

Epoch: 107
Loss: 0.3429643213748932
RMSE train: 0.478757	val: 1.658827	test: 1.608937
MAE train: 0.358137	val: 1.351930	test: 1.266299

Epoch: 108
Loss: 0.4019443690776825
RMSE train: 0.463195	val: 1.682296	test: 1.619457
MAE train: 0.346317	val: 1.365602	test: 1.280204

Epoch: 109
Loss: 0.32234198848406476
RMSE train: 0.480662	val: 1.715666	test: 1.652704
MAE train: 0.362583	val: 1.396729	test: 1.316859

Epoch: 110
Loss: 0.3860505521297455
RMSE train: 0.541868	val: 1.687917	test: 1.612766
MAE train: 0.414179	val: 1.386441	test: 1.282536

Epoch: 111
Loss: 0.3497173289457957
RMSE train: 0.503774	val: 1.693940	test: 1.583237
MAE train: 0.376707	val: 1.383442	test: 1.269918

Epoch: 112
Loss: 0.35358985265096027
RMSE train: 0.465809	val: 1.785360	test: 1.704711
MAE train: 0.349613	val: 1.442865	test: 1.371454

Epoch: 113
Loss: 0.3194975157578786
RMSE train: 0.487909	val: 1.824987	test: 1.814949
MAE train: 0.364566	val: 1.469653	test: 1.450564

Epoch: 114
Loss: 0.3170616726080577
RMSE train: 0.545927	val: 1.762448	test: 1.782326
MAE train: 0.410392	val: 1.441282	test: 1.419429

Epoch: 115
Loss: 0.28827986121177673
RMSE train: 0.482559	val: 1.723640	test: 1.736418
MAE train: 0.361802	val: 1.420831	test: 1.387861

Epoch: 116
Loss: 0.2707010954618454
RMSE train: 0.449727	val: 1.747828	test: 1.776329
MAE train: 0.339327	val: 1.428901	test: 1.417205

Epoch: 117
Loss: 0.41660277048746747
RMSE train: 0.503227	val: 1.660500	test: 1.690684
MAE train: 0.371555	val: 1.357375	test: 1.363059

Epoch: 118
Loss: 0.3151688774426778
RMSE train: 0.472739	val: 1.711894	test: 1.758073
MAE train: 0.360187	val: 1.369225	test: 1.395632

Epoch: 119
Loss: 0.33373727401097614
RMSE train: 0.433924	val: 1.788094	test: 1.842451
MAE train: 0.334147	val: 1.427886	test: 1.469404

Epoch: 120
Loss: 0.3367019792397817
RMSE train: 0.430507	val: 1.685863	test: 1.720629
MAE train: 0.334387	val: 1.374049	test: 1.379102

Epoch: 121
Loss: 0.3539307415485382
RMSE train: 0.458607	val: 1.571178	test: 1.638829
MAE train: 0.345125	val: 1.296087	test: 1.313248

Epoch: 122
Loss: 0.2922154466311137
RMSE train: 0.486421	val: 1.561245	test: 1.680143
MAE train: 0.371746	val: 1.280324	test: 1.326833

Epoch: 123
Loss: 0.2746427059173584
RMSE train: 0.463488	val: 1.578147	test: 1.747230
MAE train: 0.353122	val: 1.302451	test: 1.381160

Epoch: 124
Loss: 0.33681313196818036
RMSE train: 0.499713	val: 1.486249	test: 1.532736
MAE train: 0.368312	val: 1.210189	test: 1.215245

Epoch: 125
Loss: 0.31182915965716046
RMSE train: 0.484488	val: 1.470647	test: 1.512690
MAE train: 0.361448	val: 1.190401	test: 1.199050

Epoch: 126
Loss: 0.2987998326619466
RMSE train: 0.445891	val: 1.469177	test: 1.577921
MAE train: 0.344515	val: 1.207697	test: 1.254845

Epoch: 127
Loss: 0.327290415763855
RMSE train: 0.463239	val: 1.461838	test: 1.589522
MAE train: 0.347125	val: 1.201539	test: 1.276202

Epoch: 128
Loss: 0.3182634115219116
RMSE train: 0.483424	val: 1.460349	test: 1.509236
MAE train: 0.367960	val: 1.176845	test: 1.202825

Epoch: 129
Loss: 0.2961956361929576
RMSE train: 0.438273	val: 1.505831	test: 1.566636
MAE train: 0.336723	val: 1.229575	test: 1.252478

Epoch: 130
Loss: 0.29020654161771137
RMSE train: 0.434772	val: 1.557088	test: 1.658223
MAE train: 0.343894	val: 1.286914	test: 1.320032

Epoch: 131
Loss: 0.30245255430539447
RMSE train: 0.416739	val: 1.529376	test: 1.605514
MAE train: 0.318237	val: 1.259067	test: 1.278781

Epoch: 132
Loss: 0.31841011842091876
RMSE train: 0.422046	val: 1.528737	test: 1.568030
MAE train: 0.318839	val: 1.247456	test: 1.249081

Epoch: 133
Loss: 0.2662130892276764
RMSE train: 0.461396	val: 1.567018	test: 1.652128
MAE train: 0.358878	val: 1.280709	test: 1.296721

Epoch: 134
Loss: 0.2979704986015956
RMSE train: 0.448994	val: 1.584050	test: 1.652633
MAE train: 0.346028	val: 1.302051	test: 1.307909

Epoch: 135
Loss: 0.3110892375310262
RMSE train: 0.453886	val: 1.565213	test: 1.628544
MAE train: 0.336385	val: 1.292461	test: 1.304642

Epoch: 136
Loss: 0.27214117844899494
RMSE train: 0.417922	val: 1.564037	test: 1.690993
MAE train: 0.315855	val: 1.295980	test: 1.339051

Epoch: 137
Loss: 0.23984905083974203
RMSE train: 0.441732	val: 1.595697	test: 1.781637
MAE train: 0.345034	val: 1.327996	test: 1.399577

Epoch: 138
Loss: 0.258370836575826
RMSE train: 0.467536	val: 1.551340	test: 1.725402
MAE train: 0.364670	val: 1.296678	test: 1.358317

Epoch: 139
Loss: 0.2783145954211553
RMSE train: 0.462154	val: 1.537348	test: 1.664403
MAE train: 0.351152	val: 1.276405	test: 1.321740

Epoch: 140
Loss: 0.28717098633448285
RMSE train: 0.497209	val: 1.552384	test: 1.660021
MAE train: 0.373177	val: 1.286743	test: 1.324628

Epoch: 141
Loss: 0.31767625610033673
RMSE train: 0.464727	val: 1.577294	test: 1.768628
MAE train: 0.350914	val: 1.302851	test: 1.398454

Epoch: 142
Loss: 0.2981819013754527
RMSE train: 0.481007	val: 1.576806	test: 1.821942
MAE train: 0.360531	val: 1.302147	test: 1.437133

Epoch: 143
Loss: 0.2810109655062358
RMSE train: 0.483492	val: 1.495274	test: 1.671994
MAE train: 0.363248	val: 1.227502	test: 1.313929

Epoch: 144
Loss: 0.2338578999042511
RMSE train: 0.486033	val: 1.695088	test: 1.580484
MAE train: 0.358905	val: 1.330286	test: 1.244536

Epoch: 145
Loss: 0.2966036796569824
RMSE train: 0.481171	val: 1.715834	test: 1.599772
MAE train: 0.358232	val: 1.351382	test: 1.259730

Epoch: 146
Loss: 0.24680598576863608
RMSE train: 0.470285	val: 1.752078	test: 1.617717
MAE train: 0.354264	val: 1.395411	test: 1.277295

Epoch: 147
Loss: 0.25647776822249096
RMSE train: 0.480317	val: 1.767250	test: 1.645666
MAE train: 0.365764	val: 1.403476	test: 1.291283

Epoch: 148
Loss: 0.26153045396010083
RMSE train: 0.502504	val: 1.764175	test: 1.642991
MAE train: 0.378220	val: 1.392798	test: 1.284281

Epoch: 149
Loss: 0.29720354080200195
RMSE train: 0.472522	val: 1.761599	test: 1.580369
MAE train: 0.350753	val: 1.386855	test: 1.243728

Epoch: 150
Loss: 0.2795535624027252
RMSE train: 0.415148	val: 1.783069	test: 1.589329
MAE train: 0.310462	val: 1.401256	test: 1.252501

Epoch: 151
Loss: 0.26562561094760895
RMSE train: 0.482805	val: 1.774360	test: 1.579272
MAE train: 0.349150	val: 1.408108	test: 1.244508

Epoch: 152
Loss: 0.31888405978679657
RMSE train: 0.464600	val: 1.795489	test: 1.619706
MAE train: 0.351552	val: 1.418935	test: 1.270716

Epoch: 153
Loss: 0.26548577348391217
RMSE train: 0.430250	val: 1.858863	test: 1.753087
MAE train: 0.333254	val: 1.449235	test: 1.380092

Epoch: 154
Loss: 0.22073272367318472
RMSE train: 0.452230	val: 1.818884	test: 1.688349
MAE train: 0.342904	val: 1.418957	test: 1.347799

Epoch: 155
Loss: 0.2911035070816676
RMSE train: 0.522897	val: 1.847639	test: 1.656880
MAE train: 0.360822	val: 1.476046	test: 1.333783

Epoch: 156
Loss: 0.26367494463920593
RMSE train: 0.461677	val: 1.858336	test: 1.759235
MAE train: 0.340688	val: 1.431683	test: 1.393854

Epoch: 157
Loss: 0.2607818841934204
RMSE train: 0.514030	val: 1.815036	test: 1.671388
MAE train: 0.378556	val: 1.434642	test: 1.337704

Epoch: 158
Loss: 0.28429656724135083
RMSE train: 0.564206	val: 1.836313	test: 1.648584
MAE train: 0.370179	val: 1.474696	test: 1.303632

Epoch: 159
Loss: 0.2844572861989339
RMSE train: 0.401613	val: 1.819957	test: 1.619815
MAE train: 0.301962	val: 1.448924	test: 1.268220

Epoch: 160
Loss: 0.2848239094018936
RMSE train: 0.448629	val: 1.821355	test: 1.636767
MAE train: 0.344304	val: 1.463053	test: 1.273320

Epoch: 161
Loss: 0.23652325570583344
RMSE train: 0.514325	val: 1.767980	test: 1.563829
MAE train: 0.374414	val: 1.424536	test: 1.215378

Epoch: 162
Loss: 0.23859560986359915
RMSE train: 0.468235	val: 1.769042	test: 1.539270
MAE train: 0.339329	val: 1.418097	test: 1.198507

Epoch: 163
Loss: 0.3229684034983317
RMSE train: 0.384174	val: 1.734748	test: 1.585482
MAE train: 0.290267	val: 1.391741	test: 1.245221

Epoch: 164
Loss: 0.3039514521757762
RMSE train: 0.487073	val: 1.756610	test: 1.555915
MAE train: 0.339812	val: 1.404522	test: 1.204196

Epoch: 165
Loss: 0.262500062584877
RMSE train: 0.497335	val: 1.741604	test: 1.555570
MAE train: 0.358536	val: 1.385215	test: 1.211425

Epoch: 166
Loss: 0.24160859485467276
RMSE train: 0.472090	val: 1.713948	test: 1.604063
MAE train: 0.353803	val: 1.356207	test: 1.252284

Epoch: 167
Loss: 0.2557169397672017
RMSE train: 0.487034	val: 1.712160	test: 1.562628
MAE train: 0.358132	val: 1.342987	test: 1.223701

Epoch: 168
Loss: 0.2431789686282476
RMSE train: 0.480326	val: 1.727314	test: 1.562241
MAE train: 0.342064	val: 1.377203	test: 1.216138

Early stopping
Best (RMSE):	 train: 0.516719	val: 1.655190	test: 1.546300
Best (MAE):	 train: 0.380394	val: 1.297622	test: 1.180058


Epoch: 84
Loss: 0.35084789246320724
RMSE train: 0.545684	val: 1.160596	test: 1.140572
MAE train: 0.405979	val: 0.942185	test: 0.895724

Epoch: 85
Loss: 0.4113180860877037
RMSE train: 0.585788	val: 1.192700	test: 1.188649
MAE train: 0.436066	val: 0.966143	test: 0.922080

Epoch: 86
Loss: 0.3651202470064163
RMSE train: 0.571816	val: 1.170559	test: 1.168197
MAE train: 0.423820	val: 0.947732	test: 0.908902

Epoch: 87
Loss: 0.37105199694633484
RMSE train: 0.619864	val: 1.225306	test: 1.229006
MAE train: 0.453496	val: 1.001346	test: 0.947170

Epoch: 88
Loss: 0.40449710190296173
RMSE train: 0.582518	val: 1.190868	test: 1.196529
MAE train: 0.430806	val: 0.985213	test: 0.926392

Epoch: 89
Loss: 0.41053253412246704
RMSE train: 0.513767	val: 1.127453	test: 1.129587
MAE train: 0.376837	val: 0.939516	test: 0.884007

Epoch: 90
Loss: 0.4503687471151352
RMSE train: 0.609106	val: 1.251826	test: 1.203697
MAE train: 0.459877	val: 1.035576	test: 0.926939

Epoch: 91
Loss: 0.3580283522605896
RMSE train: 0.572071	val: 1.231468	test: 1.173862
MAE train: 0.434521	val: 1.006837	test: 0.913759

Epoch: 92
Loss: 0.3945823982357979
RMSE train: 0.534641	val: 1.200990	test: 1.169096
MAE train: 0.397713	val: 0.972826	test: 0.920800

Epoch: 93
Loss: 0.36511168628931046
RMSE train: 0.568613	val: 1.241841	test: 1.201019
MAE train: 0.426171	val: 0.994709	test: 0.945517

Epoch: 94
Loss: 0.351305291056633
RMSE train: 0.570891	val: 1.194647	test: 1.175644
MAE train: 0.428973	val: 0.958802	test: 0.929036

Epoch: 95
Loss: 0.3531925603747368
RMSE train: 0.533143	val: 1.151275	test: 1.148710
MAE train: 0.404020	val: 0.930103	test: 0.913779

Epoch: 96
Loss: 0.3692512661218643
RMSE train: 0.517331	val: 1.181336	test: 1.153290
MAE train: 0.389590	val: 0.959991	test: 0.905591

Epoch: 97
Loss: 0.38966166228055954
RMSE train: 0.593077	val: 1.182047	test: 1.168229
MAE train: 0.446073	val: 0.983462	test: 0.908174

Epoch: 98
Loss: 0.32859407365322113
RMSE train: 0.570188	val: 1.173890	test: 1.157393
MAE train: 0.427630	val: 0.959589	test: 0.891887

Epoch: 99
Loss: 0.34981103241443634
RMSE train: 0.593842	val: 1.225183	test: 1.222473
MAE train: 0.448486	val: 1.000867	test: 0.921401

Epoch: 100
Loss: 0.3512814790010452
RMSE train: 0.548759	val: 1.224961	test: 1.208233
MAE train: 0.412972	val: 0.993603	test: 0.932681

Epoch: 101
Loss: 0.3540768027305603
RMSE train: 0.523117	val: 1.172658	test: 1.147387
MAE train: 0.392954	val: 0.959859	test: 0.896094

Epoch: 102
Loss: 0.39266955852508545
RMSE train: 0.551106	val: 1.197682	test: 1.163109
MAE train: 0.424223	val: 0.985485	test: 0.898993

Epoch: 103
Loss: 0.42804793268442154
RMSE train: 0.519995	val: 1.212454	test: 1.187963
MAE train: 0.392769	val: 0.981867	test: 0.926434

Epoch: 104
Loss: 0.35275948792696
RMSE train: 0.528272	val: 1.164530	test: 1.168188
MAE train: 0.406736	val: 0.961137	test: 0.917253

Epoch: 105
Loss: 0.38181985914707184
RMSE train: 0.536745	val: 1.176273	test: 1.158796
MAE train: 0.407669	val: 0.962786	test: 0.899519

Epoch: 106
Loss: 0.3200373500585556
RMSE train: 0.525367	val: 1.135969	test: 1.133468
MAE train: 0.398878	val: 0.938493	test: 0.884006

Epoch: 107
Loss: 0.3361184522509575
RMSE train: 0.546580	val: 1.188360	test: 1.177371
MAE train: 0.412217	val: 0.973404	test: 0.917660

Epoch: 108
Loss: 0.337653785943985
RMSE train: 0.587174	val: 1.215390	test: 1.200579
MAE train: 0.450458	val: 0.993039	test: 0.936651

Epoch: 109
Loss: 0.3113214150071144
RMSE train: 0.591458	val: 1.252814	test: 1.233344
MAE train: 0.455190	val: 1.016902	test: 0.963307

Epoch: 110
Loss: 0.35392939299345016
RMSE train: 0.540396	val: 1.164052	test: 1.176425
MAE train: 0.409805	val: 0.950165	test: 0.920197

Epoch: 111
Loss: 0.3494608774781227
RMSE train: 0.517438	val: 1.128704	test: 1.137869
MAE train: 0.384810	val: 0.935196	test: 0.883300

Epoch: 112
Loss: 0.3066035509109497
RMSE train: 0.557540	val: 1.196228	test: 1.178138
MAE train: 0.418727	val: 0.976789	test: 0.900408

Epoch: 113
Loss: 0.304096058011055
RMSE train: 0.582901	val: 1.311058	test: 1.280839
MAE train: 0.443414	val: 1.049531	test: 0.981559

Epoch: 114
Loss: 0.31497590988874435
RMSE train: 0.585378	val: 1.234577	test: 1.232112
MAE train: 0.441086	val: 1.006401	test: 0.942518

Epoch: 115
Loss: 0.3311520665884018
RMSE train: 0.617327	val: 1.223428	test: 1.230609
MAE train: 0.464415	val: 0.997247	test: 0.926350

Epoch: 116
Loss: 0.3217807859182358
RMSE train: 0.591215	val: 1.290869	test: 1.231756
MAE train: 0.437058	val: 1.039695	test: 0.937335

Epoch: 117
Loss: 0.2910536304116249
RMSE train: 0.547206	val: 1.184934	test: 1.151763
MAE train: 0.413272	val: 0.973594	test: 0.864210

Epoch: 118
Loss: 0.31295720487833023
RMSE train: 0.519809	val: 1.241462	test: 1.153661
MAE train: 0.398065	val: 1.002828	test: 0.891774

Epoch: 119
Loss: 0.30758753418922424
RMSE train: 0.502183	val: 1.185231	test: 1.142273
MAE train: 0.385839	val: 0.969181	test: 0.893680

Epoch: 120
Loss: 0.32251591980457306
RMSE train: 0.529450	val: 1.302815	test: 1.294298
MAE train: 0.404764	val: 1.040947	test: 0.990005

Epoch: 121
Loss: 0.32024677842855453
RMSE train: 0.491312	val: 1.195348	test: 1.223006
MAE train: 0.373214	val: 0.972037	test: 0.964304

Early stopping
Best (RMSE):	 train: 0.566892	val: 1.080709	test: 1.098693
Best (MAE):	 train: 0.422183	val: 0.857737	test: 0.856674

RMSE train: 0.620387	val: 1.298575	test: 1.377554
MAE train: 0.480205	val: 1.022276	test: 1.118087

Epoch: 85
Loss: 0.8757892996072769
RMSE train: 0.690306	val: 1.323350	test: 1.421692
MAE train: 0.528582	val: 1.037224	test: 1.132551

Epoch: 86
Loss: 0.6244636625051498
RMSE train: 0.742377	val: 1.331487	test: 1.439690
MAE train: 0.565349	val: 1.051064	test: 1.149110

Epoch: 87
Loss: 0.7081765234470367
RMSE train: 0.713482	val: 1.322181	test: 1.432759
MAE train: 0.538821	val: 1.043069	test: 1.140326

Epoch: 88
Loss: 0.8034444153308868
RMSE train: 0.645355	val: 1.258578	test: 1.366740
MAE train: 0.479149	val: 0.999328	test: 1.090306

Epoch: 89
Loss: 0.5600800663232803
RMSE train: 0.617450	val: 1.265145	test: 1.382883
MAE train: 0.459425	val: 1.009560	test: 1.104658

Epoch: 90
Loss: 0.5869534462690353
RMSE train: 0.641710	val: 1.269686	test: 1.373515
MAE train: 0.475879	val: 1.024975	test: 1.105197

Epoch: 91
Loss: 0.8296357467770576
RMSE train: 0.750277	val: 1.387612	test: 1.523233
MAE train: 0.550058	val: 1.128524	test: 1.251057

Epoch: 92
Loss: 0.5203791409730911
RMSE train: 0.693024	val: 1.374130	test: 1.514825
MAE train: 0.530901	val: 1.102934	test: 1.212655

Epoch: 93
Loss: 0.5231491401791573
RMSE train: 0.617080	val: 1.369519	test: 1.503519
MAE train: 0.474820	val: 1.093277	test: 1.198825

Epoch: 94
Loss: 0.509434163570404
RMSE train: 0.552949	val: 1.316158	test: 1.419390
MAE train: 0.413798	val: 1.037393	test: 1.128607

Epoch: 95
Loss: 0.5807928889989853
RMSE train: 0.625502	val: 1.224060	test: 1.336711
MAE train: 0.462540	val: 0.991348	test: 1.079012

Epoch: 96
Loss: 0.5232587456703186
RMSE train: 0.707909	val: 1.276192	test: 1.404403
MAE train: 0.528176	val: 1.012431	test: 1.136062

Epoch: 97
Loss: 0.561883382499218
RMSE train: 0.656491	val: 1.248553	test: 1.392297
MAE train: 0.507537	val: 0.979836	test: 1.103331

Epoch: 98
Loss: 0.4994604140520096
RMSE train: 0.626906	val: 1.340008	test: 1.500718
MAE train: 0.484748	val: 1.039949	test: 1.193240

Epoch: 99
Loss: 0.5112172216176987
RMSE train: 0.556389	val: 1.257929	test: 1.389844
MAE train: 0.426095	val: 0.975763	test: 1.120649

Epoch: 100
Loss: 0.49244750291109085
RMSE train: 0.571767	val: 1.191718	test: 1.282552
MAE train: 0.427493	val: 0.921787	test: 1.033718

Epoch: 101
Loss: 0.7044595628976822
RMSE train: 0.578592	val: 1.172332	test: 1.255342
MAE train: 0.432668	val: 0.915972	test: 1.021905

Epoch: 102
Loss: 0.48736266791820526
RMSE train: 0.611746	val: 1.248768	test: 1.374443
MAE train: 0.456027	val: 0.999899	test: 1.096491

Epoch: 103
Loss: 0.43462494388222694
RMSE train: 0.655962	val: 1.324343	test: 1.480009
MAE train: 0.493418	val: 1.043419	test: 1.147965

Epoch: 104
Loss: 0.5431719794869423
RMSE train: 0.589732	val: 1.284106	test: 1.415247
MAE train: 0.431283	val: 1.024951	test: 1.116570

Epoch: 105
Loss: 0.5285638347268105
RMSE train: 0.591429	val: 1.259242	test: 1.369981
MAE train: 0.435220	val: 1.013387	test: 1.103714

Epoch: 106
Loss: 0.4482547231018543
RMSE train: 0.583288	val: 1.259588	test: 1.360581
MAE train: 0.431915	val: 1.014631	test: 1.107827

Epoch: 107
Loss: 0.6122036576271057
RMSE train: 0.608806	val: 1.306574	test: 1.428411
MAE train: 0.463370	val: 1.033602	test: 1.155052

Epoch: 108
Loss: 0.5315508246421814
RMSE train: 0.638085	val: 1.255618	test: 1.375269
MAE train: 0.496309	val: 0.995514	test: 1.115764

Epoch: 109
Loss: 0.6377051919698715
RMSE train: 0.573104	val: 1.207093	test: 1.309171
MAE train: 0.436116	val: 0.976818	test: 1.065493

Epoch: 110
Loss: 0.6436322778463364
RMSE train: 0.555769	val: 1.223927	test: 1.325124
MAE train: 0.426503	val: 0.986426	test: 1.073736

Epoch: 111
Loss: 0.3479876480996609
RMSE train: 0.656828	val: 1.243612	test: 1.373114
MAE train: 0.493761	val: 0.986769	test: 1.074216

Epoch: 112
Loss: 0.5587415024638176
RMSE train: 0.629155	val: 1.222977	test: 1.322423
MAE train: 0.464134	val: 0.971765	test: 1.039111

Epoch: 113
Loss: 0.47575242072343826
RMSE train: 0.597302	val: 1.200924	test: 1.283676
MAE train: 0.442100	val: 0.964539	test: 1.025619

Epoch: 114
Loss: 0.5280321389436722
RMSE train: 0.615341	val: 1.233935	test: 1.333168
MAE train: 0.450650	val: 0.982634	test: 1.051321

Epoch: 115
Loss: 0.43804602324962616
RMSE train: 0.679210	val: 1.247514	test: 1.368430
MAE train: 0.491837	val: 0.996284	test: 1.089150

Epoch: 116
Loss: 0.5509738028049469
RMSE train: 0.683024	val: 1.246746	test: 1.381580
MAE train: 0.493845	val: 1.000504	test: 1.108618

Epoch: 117
Loss: 0.4374060034751892
RMSE train: 0.642578	val: 1.224225	test: 1.356979
MAE train: 0.467711	val: 0.984141	test: 1.093881

Epoch: 118
Loss: 0.5508767068386078
RMSE train: 0.606052	val: 1.245192	test: 1.360484
MAE train: 0.438164	val: 0.981959	test: 1.107422

Epoch: 119
Loss: 0.42833173274993896
RMSE train: 0.606194	val: 1.249070	test: 1.348427
MAE train: 0.445876	val: 0.986420	test: 1.088178

Epoch: 120
Loss: 0.5354438349604607
RMSE train: 0.599697	val: 1.259041	test: 1.356138
MAE train: 0.449852	val: 0.993710	test: 1.088853

Epoch: 121
Loss: 0.5955830663442612
RMSE train: 0.570411	val: 1.272123	test: 1.361527
MAE train: 0.422723	val: 1.011719	test: 1.099536

Epoch: 122
Loss: 0.46665316075086594
RMSE train: 0.567745	val: 1.259059	test: 1.355657
MAE train: 0.413776	val: 0.994058	test: 1.090428

Epoch: 123
Loss: 0.6369537711143494
RMSE train: 0.633522	val: 1.232701	test: 1.351638
MAE train: 0.467505	val: 0.984131	test: 1.083067

Epoch: 124
Loss: 0.44866248965263367
RMSE train: 0.667817	val: 1.254110	test: 1.363507
MAE train: 0.507063	val: 0.995345	test: 1.078671

Epoch: 125
Loss: 0.42923638969659805
RMSE train: 0.601535	val: 1.228078	test: 1.298405
MAE train: 0.466517	val: 0.977766	test: 1.046016

Epoch: 126
Loss: 0.49955224245786667
RMSE train: 0.526907	val: 1.218594	test: 1.281951
MAE train: 0.402606	val: 0.973922	test: 1.032210

Epoch: 127
Loss: 0.5339616015553474
RMSE train: 0.539675	val: 1.244369	test: 1.315807
MAE train: 0.404110	val: 0.989331	test: 1.053705

Epoch: 128
Loss: 0.49309825152158737
RMSE train: 0.676063	val: 1.377535	test: 1.448281
MAE train: 0.514361	val: 1.099473	test: 1.153831

Epoch: 129
Loss: 0.5731918588280678
RMSE train: 0.593251	val: 1.286252	test: 1.344678
MAE train: 0.445297	val: 1.005395	test: 1.080953

Epoch: 130
Loss: 0.42784032970666885
RMSE train: 0.603643	val: 1.245096	test: 1.284694
MAE train: 0.467361	val: 0.980881	test: 1.036651

Epoch: 131
Loss: 0.7993670776486397
RMSE train: 0.541150	val: 1.271097	test: 1.298060
MAE train: 0.418194	val: 0.999551	test: 1.047961

Epoch: 132
Loss: 0.3796573728322983
RMSE train: 0.592177	val: 1.326609	test: 1.368231
MAE train: 0.462475	val: 1.042428	test: 1.099151

Epoch: 133
Loss: 0.5115791782736778
RMSE train: 0.577307	val: 1.355811	test: 1.416745
MAE train: 0.449117	val: 1.056424	test: 1.124940

Epoch: 134
Loss: 0.3847915455698967
RMSE train: 0.547907	val: 1.301765	test: 1.342040
MAE train: 0.407501	val: 1.009080	test: 1.052298

Epoch: 135
Loss: 0.38454560190439224
RMSE train: 0.558306	val: 1.273324	test: 1.323894
MAE train: 0.407314	val: 0.989565	test: 1.045101

Epoch: 136
Loss: 0.4711266756057739
RMSE train: 0.582905	val: 1.277847	test: 1.335942
MAE train: 0.422177	val: 0.984185	test: 1.058463

Early stopping
Best (RMSE):	 train: 0.578592	val: 1.172332	test: 1.255342
Best (MAE):	 train: 0.432668	val: 0.915972	test: 1.021905


Epoch: 84
Loss: 0.38818537443876266
RMSE train: 0.584058	val: 1.192994	test: 1.162032
MAE train: 0.437749	val: 0.981723	test: 0.889171

Epoch: 85
Loss: 0.34266484528779984
RMSE train: 0.548757	val: 1.218139	test: 1.173279
MAE train: 0.419381	val: 0.999378	test: 0.913420

Epoch: 86
Loss: 0.4221693351864815
RMSE train: 0.537401	val: 1.118239	test: 1.109314
MAE train: 0.416663	val: 0.927520	test: 0.863887

Epoch: 87
Loss: 0.386773481965065
RMSE train: 0.548166	val: 1.092268	test: 1.110594
MAE train: 0.425181	val: 0.914866	test: 0.860223

Epoch: 88
Loss: 0.4135754704475403
RMSE train: 0.558282	val: 1.149517	test: 1.125527
MAE train: 0.426023	val: 0.954930	test: 0.865882

Epoch: 89
Loss: 0.3871249035000801
RMSE train: 0.542509	val: 1.162001	test: 1.141589
MAE train: 0.412230	val: 0.973874	test: 0.869128

Epoch: 90
Loss: 0.3768865391612053
RMSE train: 0.549599	val: 1.215400	test: 1.187690
MAE train: 0.419864	val: 0.994796	test: 0.923565

Epoch: 91
Loss: 0.35910864174366
RMSE train: 0.565967	val: 1.173463	test: 1.181774
MAE train: 0.431178	val: 0.974242	test: 0.923156

Epoch: 92
Loss: 0.34489942342042923
RMSE train: 0.531591	val: 1.111264	test: 1.144370
MAE train: 0.405696	val: 0.938537	test: 0.902329

Epoch: 93
Loss: 0.3432835713028908
RMSE train: 0.504224	val: 1.124144	test: 1.145282
MAE train: 0.380463	val: 0.950822	test: 0.901838

Epoch: 94
Loss: 0.3487885370850563
RMSE train: 0.548704	val: 1.250967	test: 1.215755
MAE train: 0.414097	val: 1.021463	test: 0.946509

Epoch: 95
Loss: 0.3386281952261925
RMSE train: 0.552553	val: 1.188960	test: 1.184250
MAE train: 0.427366	val: 0.987040	test: 0.925720

Epoch: 96
Loss: 0.3391152396798134
RMSE train: 0.519202	val: 1.048641	test: 1.114337
MAE train: 0.399537	val: 0.881704	test: 0.893900

Epoch: 97
Loss: 0.3956972062587738
RMSE train: 0.559020	val: 1.097894	test: 1.133673
MAE train: 0.426783	val: 0.945610	test: 0.867173

Epoch: 98
Loss: 0.4114752784371376
RMSE train: 0.554738	val: 1.212443	test: 1.213788
MAE train: 0.431484	val: 1.002487	test: 0.949483

Epoch: 99
Loss: 0.3710651099681854
RMSE train: 0.512402	val: 1.076563	test: 1.147280
MAE train: 0.394500	val: 0.896968	test: 0.931710

Epoch: 100
Loss: 0.34069719910621643
RMSE train: 0.512576	val: 1.109675	test: 1.135307
MAE train: 0.393507	val: 0.934870	test: 0.893097

Epoch: 101
Loss: 0.36418522894382477
RMSE train: 0.518912	val: 1.166676	test: 1.168432
MAE train: 0.398912	val: 0.975210	test: 0.903208

Epoch: 102
Loss: 0.32752083241939545
RMSE train: 0.499329	val: 1.111455	test: 1.164413
MAE train: 0.387458	val: 0.925073	test: 0.927574

Epoch: 103
Loss: 0.32038789242506027
RMSE train: 0.481771	val: 1.175215	test: 1.194755
MAE train: 0.376835	val: 0.967074	test: 0.951307

Epoch: 104
Loss: 0.2986273989081383
RMSE train: 0.476517	val: 1.137512	test: 1.158545
MAE train: 0.366418	val: 0.937908	test: 0.925224

Epoch: 105
Loss: 0.3414576128125191
RMSE train: 0.478043	val: 1.124967	test: 1.152903
MAE train: 0.366782	val: 0.939293	test: 0.915298

Epoch: 106
Loss: 0.3155553266406059
RMSE train: 0.486421	val: 1.074817	test: 1.117672
MAE train: 0.374690	val: 0.914948	test: 0.889124

Epoch: 107
Loss: 0.34329595416784286
RMSE train: 0.497660	val: 1.103076	test: 1.106582
MAE train: 0.386975	val: 0.938173	test: 0.863213

Epoch: 108
Loss: 0.3560797870159149
RMSE train: 0.520146	val: 1.255532	test: 1.212741
MAE train: 0.409356	val: 1.040786	test: 0.960912

Epoch: 109
Loss: 0.366274893283844
RMSE train: 0.551961	val: 1.123163	test: 1.177709
MAE train: 0.426151	val: 0.943170	test: 0.922585

Epoch: 110
Loss: 0.357624351978302
RMSE train: 0.502534	val: 1.046163	test: 1.118527
MAE train: 0.391893	val: 0.885473	test: 0.891020

Epoch: 111
Loss: 0.33552250266075134
RMSE train: 0.539263	val: 1.104743	test: 1.133328
MAE train: 0.429565	val: 0.926061	test: 0.891126

Epoch: 112
Loss: 0.3293968588113785
RMSE train: 0.495902	val: 1.098526	test: 1.155338
MAE train: 0.389507	val: 0.929118	test: 0.911369

Epoch: 113
Loss: 0.46738241612911224
RMSE train: 0.555025	val: 1.279150	test: 1.302422
MAE train: 0.425431	val: 1.029987	test: 1.014577

Epoch: 114
Loss: 0.36631616950035095
RMSE train: 0.553307	val: 1.219100	test: 1.241765
MAE train: 0.412964	val: 1.033829	test: 0.966406

Epoch: 115
Loss: 0.3660435155034065
RMSE train: 0.498284	val: 1.205954	test: 1.214971
MAE train: 0.387241	val: 1.008727	test: 0.944641

Epoch: 116
Loss: 0.30320093780755997
RMSE train: 0.488957	val: 1.173378	test: 1.182614
MAE train: 0.380982	val: 0.993226	test: 0.918787

Epoch: 117
Loss: 0.3305908516049385
RMSE train: 0.524316	val: 1.139635	test: 1.147549
MAE train: 0.408658	val: 0.988594	test: 0.878532

Epoch: 118
Loss: 0.31505849957466125
RMSE train: 0.484399	val: 1.170368	test: 1.185364
MAE train: 0.371526	val: 0.981157	test: 0.920895

Epoch: 119
Loss: 0.31619375944137573
RMSE train: 0.484373	val: 1.110958	test: 1.182614
MAE train: 0.364639	val: 0.959972	test: 0.923698

Epoch: 120
Loss: 0.3036235123872757
RMSE train: 0.495315	val: 1.145041	test: 1.193149
MAE train: 0.382406	val: 0.988811	test: 0.936177

Epoch: 121
Loss: 0.3150900527834892
RMSE train: 0.490370	val: 1.231890	test: 1.302739
MAE train: 0.386792	val: 1.017274	test: 1.007386

Early stopping
Best (RMSE):	 train: 0.546775	val: 0.995571	test: 1.077731
Best (MAE):	 train: 0.425024	val: 0.831334	test: 0.866779

RMSE train: 0.640906	val: 1.393740	test: 1.531314
MAE train: 0.499332	val: 1.116897	test: 1.211253

Epoch: 85
Loss: 1.1311578303575516
RMSE train: 0.601718	val: 1.290493	test: 1.366921
MAE train: 0.472104	val: 1.024519	test: 1.077726

Epoch: 86
Loss: 0.5361619517207146
RMSE train: 0.600761	val: 1.260042	test: 1.331735
MAE train: 0.461594	val: 1.005213	test: 1.062844

Epoch: 87
Loss: 0.7128765732049942
RMSE train: 0.608403	val: 1.273333	test: 1.323561
MAE train: 0.469807	val: 1.002917	test: 1.067243

Epoch: 88
Loss: 0.5713316947221756
RMSE train: 0.598868	val: 1.284861	test: 1.340579
MAE train: 0.453395	val: 1.013632	test: 1.089502

Epoch: 89
Loss: 0.5753737092018127
RMSE train: 0.615417	val: 1.218737	test: 1.312302
MAE train: 0.455198	val: 0.972557	test: 1.045961

Epoch: 90
Loss: 0.5652739256620407
RMSE train: 0.636127	val: 1.220054	test: 1.358419
MAE train: 0.464008	val: 0.969205	test: 1.068953

Epoch: 91
Loss: 0.6489611342549324
RMSE train: 0.635329	val: 1.256252	test: 1.398608
MAE train: 0.466054	val: 0.996620	test: 1.099300

Epoch: 92
Loss: 0.550449974834919
RMSE train: 0.674084	val: 1.287046	test: 1.410495
MAE train: 0.498873	val: 1.020010	test: 1.104062

Epoch: 93
Loss: 0.49020761996507645
RMSE train: 0.630265	val: 1.204777	test: 1.279195
MAE train: 0.468778	val: 0.950468	test: 1.005397

Epoch: 94
Loss: 0.7197357639670372
RMSE train: 0.611905	val: 1.208368	test: 1.219911
MAE train: 0.470099	val: 0.952038	test: 0.977280

Epoch: 95
Loss: 0.5596615895628929
RMSE train: 0.607002	val: 1.195383	test: 1.218334
MAE train: 0.463372	val: 0.941873	test: 0.973378

Epoch: 96
Loss: 0.7208315506577492
RMSE train: 0.706586	val: 1.329990	test: 1.435031
MAE train: 0.543571	val: 1.056406	test: 1.119581

Epoch: 97
Loss: 0.7110592052340508
RMSE train: 0.827410	val: 1.451897	test: 1.640050
MAE train: 0.603617	val: 1.136466	test: 1.275218

Epoch: 98
Loss: 0.779541090130806
RMSE train: 0.774194	val: 1.465850	test: 1.668883
MAE train: 0.545468	val: 1.153219	test: 1.296589

Epoch: 99
Loss: 0.61168272793293
RMSE train: 0.633185	val: 1.354027	test: 1.522976
MAE train: 0.462554	val: 1.072917	test: 1.165104

Epoch: 100
Loss: 0.5133915841579437
RMSE train: 0.553875	val: 1.269472	test: 1.372066
MAE train: 0.417089	val: 1.024103	test: 1.081128

Epoch: 101
Loss: 0.4923769012093544
RMSE train: 0.561058	val: 1.222596	test: 1.291258
MAE train: 0.432757	val: 0.980935	test: 1.036346

Epoch: 102
Loss: 0.7968411892652512
RMSE train: 0.558969	val: 1.270872	test: 1.333885
MAE train: 0.433126	val: 1.004175	test: 1.069203

Epoch: 103
Loss: 0.7311459258198738
RMSE train: 0.614864	val: 1.200836	test: 1.263602
MAE train: 0.482992	val: 0.911305	test: 1.009498

Epoch: 104
Loss: 0.4890923649072647
RMSE train: 0.664107	val: 1.145973	test: 1.225305
MAE train: 0.525599	val: 0.879087	test: 0.990149

Epoch: 105
Loss: 0.7413807809352875
RMSE train: 0.566676	val: 1.164202	test: 1.269983
MAE train: 0.435781	val: 0.923642	test: 1.029556

Epoch: 106
Loss: 0.5921483486890793
RMSE train: 0.639926	val: 1.379986	test: 1.474467
MAE train: 0.501018	val: 1.080855	test: 1.200159

Epoch: 107
Loss: 0.594117745757103
RMSE train: 0.679122	val: 1.566523	test: 1.628913
MAE train: 0.525595	val: 1.247047	test: 1.346398

Epoch: 108
Loss: 0.5604413598775864
RMSE train: 0.542210	val: 1.276037	test: 1.333581
MAE train: 0.418083	val: 0.992272	test: 1.071432

Epoch: 109
Loss: 0.5537166073918343
RMSE train: 0.605144	val: 1.226140	test: 1.304990
MAE train: 0.467061	val: 0.972349	test: 1.041644

Epoch: 110
Loss: 0.44962726160883904
RMSE train: 0.597497	val: 1.396696	test: 1.494448
MAE train: 0.451022	val: 1.105514	test: 1.193161

Epoch: 111
Loss: 0.5129653289914131
RMSE train: 0.670107	val: 1.453930	test: 1.572269
MAE train: 0.502749	val: 1.160216	test: 1.255229

Epoch: 112
Loss: 0.7037956714630127
RMSE train: 0.587135	val: 1.264626	test: 1.350797
MAE train: 0.433029	val: 1.029987	test: 1.079459

Epoch: 113
Loss: 0.6015252247452736
RMSE train: 0.657039	val: 1.178711	test: 1.232674
MAE train: 0.491933	val: 0.947617	test: 0.971627

Epoch: 114
Loss: 0.49090637266635895
RMSE train: 0.618759	val: 1.276120	test: 1.364363
MAE train: 0.470568	val: 1.063424	test: 1.087192

Epoch: 115
Loss: 0.5098639503121376
RMSE train: 0.621468	val: 1.320818	test: 1.429103
MAE train: 0.470658	val: 1.082874	test: 1.143233

Epoch: 116
Loss: 0.6467694342136383
RMSE train: 0.644112	val: 1.367618	test: 1.464404
MAE train: 0.476045	val: 1.120406	test: 1.171716

Epoch: 117
Loss: 0.6011566817760468
RMSE train: 0.597974	val: 1.244312	test: 1.280897
MAE train: 0.440030	val: 1.009854	test: 0.991203

Epoch: 118
Loss: 0.4894895702600479
RMSE train: 0.651843	val: 1.222765	test: 1.239925
MAE train: 0.469458	val: 0.985911	test: 0.969903

Epoch: 119
Loss: 0.5644735842943192
RMSE train: 0.674985	val: 1.251347	test: 1.344613
MAE train: 0.499321	val: 1.005479	test: 1.053034

Epoch: 120
Loss: 0.48993465304374695
RMSE train: 0.698696	val: 1.330213	test: 1.480234
MAE train: 0.531748	val: 1.070798	test: 1.149858

Epoch: 121
Loss: 0.4334021210670471
RMSE train: 0.615667	val: 1.304722	test: 1.449374
MAE train: 0.469372	val: 1.059358	test: 1.143568

Epoch: 122
Loss: 0.5208022892475128
RMSE train: 0.533991	val: 1.247403	test: 1.336694
MAE train: 0.396805	val: 1.020021	test: 1.056596

Epoch: 123
Loss: 0.5128438621759415
RMSE train: 0.551594	val: 1.257590	test: 1.322587
MAE train: 0.404076	val: 1.022752	test: 1.043903

Epoch: 124
Loss: 0.5752912685275078
RMSE train: 0.540596	val: 1.302298	test: 1.404160
MAE train: 0.398799	val: 1.050898	test: 1.121589

Epoch: 125
Loss: 0.3768499791622162
RMSE train: 0.669890	val: 1.327772	test: 1.463321
MAE train: 0.500006	val: 1.047041	test: 1.151897

Epoch: 126
Loss: 0.5192951261997223
RMSE train: 0.623079	val: 1.263668	test: 1.349678
MAE train: 0.471360	val: 0.987225	test: 1.074678

Epoch: 127
Loss: 0.5127603858709335
RMSE train: 0.552627	val: 1.241481	test: 1.273642
MAE train: 0.424471	val: 0.959743	test: 1.023398

Epoch: 128
Loss: 0.5228101685643196
RMSE train: 0.509248	val: 1.301056	test: 1.351506
MAE train: 0.382435	val: 1.014802	test: 1.078823

Epoch: 129
Loss: 0.5048932209610939
RMSE train: 0.543028	val: 1.281722	test: 1.369555
MAE train: 0.413993	val: 1.006707	test: 1.090638

Epoch: 130
Loss: 0.46864093095064163
RMSE train: 0.565332	val: 1.190217	test: 1.258979
MAE train: 0.438151	val: 0.932933	test: 0.997289

Epoch: 131
Loss: 0.3977948799729347
RMSE train: 0.601666	val: 1.283853	test: 1.411222
MAE train: 0.459269	val: 0.991306	test: 1.129227

Epoch: 132
Loss: 1.2163350656628609
RMSE train: 0.605845	val: 1.341119	test: 1.495748
MAE train: 0.469746	val: 1.033861	test: 1.196929

Epoch: 133
Loss: 0.37123772501945496
RMSE train: 0.572289	val: 1.233793	test: 1.368945
MAE train: 0.437209	val: 0.987882	test: 1.070207

Epoch: 134
Loss: 0.4631384089589119
RMSE train: 0.584063	val: 1.258173	test: 1.392882
MAE train: 0.445828	val: 1.007975	test: 1.087797

Epoch: 135
Loss: 0.3862236440181732
RMSE train: 0.561003	val: 1.253416	test: 1.387161
MAE train: 0.419455	val: 1.007754	test: 1.081264

Epoch: 136
Loss: 0.5893400758504868
RMSE train: 0.567785	val: 1.263821	test: 1.388932
MAE train: 0.419941	val: 1.019117	test: 1.091847

Epoch: 137
Loss: 0.38706309348344803
RMSE train: 0.637095	val: 1.283511	test: 1.429520
MAE train: 0.459799	val: 1.035604	test: 1.110187

Epoch: 138
Loss: 0.577398031949997
RMSE train: 0.670985	val: 1.306100	test: 1.448707
MAE train: 0.484083	val: 1.042729	test: 1.119428

Epoch: 139
Loss: 0.6566492989659309
RMSE train: 0.591421	val: 1.230741	test: 1.334879
MAE train: 0.441965	val: 0.986260	test: 1.042629

Early stopping
Best (RMSE):	 train: 0.664107	val: 1.145973	test: 1.225305
Best (MAE):	 train: 0.525599	val: 0.879087	test: 0.990149

RMSE train: 0.605297	val: 1.303968	test: 1.419655
MAE train: 0.465564	val: 1.030927	test: 1.141691

Epoch: 85
Loss: 0.7308630496263504
RMSE train: 0.591860	val: 1.299765	test: 1.442310
MAE train: 0.455485	val: 1.023200	test: 1.156222

Epoch: 86
Loss: 0.5459538698196411
RMSE train: 0.565358	val: 1.307823	test: 1.430627
MAE train: 0.449442	val: 1.017475	test: 1.143351

Epoch: 87
Loss: 0.6683290004730225
RMSE train: 0.574290	val: 1.324441	test: 1.446308
MAE train: 0.459642	val: 1.029150	test: 1.159330

Epoch: 88
Loss: 0.42886149883270264
RMSE train: 0.552363	val: 1.276803	test: 1.401280
MAE train: 0.427036	val: 1.006692	test: 1.122680

Epoch: 89
Loss: 0.6328827291727066
RMSE train: 0.586576	val: 1.242235	test: 1.367782
MAE train: 0.443720	val: 0.998722	test: 1.092726

Epoch: 90
Loss: 0.46451059728860855
RMSE train: 0.621639	val: 1.246486	test: 1.364955
MAE train: 0.468833	val: 1.003491	test: 1.090298

Epoch: 91
Loss: 0.4982346147298813
RMSE train: 0.610361	val: 1.285116	test: 1.401993
MAE train: 0.462122	val: 1.018023	test: 1.119805

Epoch: 92
Loss: 0.5844562351703644
RMSE train: 0.554840	val: 1.315175	test: 1.406526
MAE train: 0.418776	val: 1.030872	test: 1.130194

Epoch: 93
Loss: 0.5842873230576515
RMSE train: 0.538044	val: 1.293471	test: 1.363327
MAE train: 0.408218	val: 1.014896	test: 1.088696

Epoch: 94
Loss: 0.6101943999528885
RMSE train: 0.577505	val: 1.224403	test: 1.292237
MAE train: 0.447089	val: 0.968061	test: 1.030558

Epoch: 95
Loss: 0.5322192162275314
RMSE train: 0.627156	val: 1.260892	test: 1.342657
MAE train: 0.496139	val: 0.990528	test: 1.072439

Epoch: 96
Loss: 0.48304497450590134
RMSE train: 0.650346	val: 1.402718	test: 1.485013
MAE train: 0.517012	val: 1.092909	test: 1.207010

Epoch: 97
Loss: 0.8477956876158714
RMSE train: 0.551073	val: 1.293375	test: 1.364037
MAE train: 0.421436	val: 1.012624	test: 1.096564

Epoch: 98
Loss: 0.5352923199534416
RMSE train: 0.532345	val: 1.225258	test: 1.263569
MAE train: 0.396757	val: 0.961381	test: 1.009303

Epoch: 99
Loss: 0.5030499324202538
RMSE train: 0.590616	val: 1.345144	test: 1.387716
MAE train: 0.451117	val: 1.060324	test: 1.110491

Epoch: 100
Loss: 0.6770578846335411
RMSE train: 0.613703	val: 1.412488	test: 1.461735
MAE train: 0.477419	val: 1.106734	test: 1.184291

Epoch: 101
Loss: 0.4805746003985405
RMSE train: 0.621474	val: 1.286278	test: 1.292324
MAE train: 0.496534	val: 1.002408	test: 1.035676

Epoch: 102
Loss: 0.6518889516592026
RMSE train: 0.667567	val: 1.265595	test: 1.325454
MAE train: 0.540472	val: 0.979227	test: 1.053425

Epoch: 103
Loss: 0.5669183656573296
RMSE train: 0.620788	val: 1.314688	test: 1.446226
MAE train: 0.500362	val: 1.000774	test: 1.140695

Epoch: 104
Loss: 0.5912053436040878
RMSE train: 0.558088	val: 1.212398	test: 1.353900
MAE train: 0.433854	val: 0.961716	test: 1.087319

Epoch: 105
Loss: 0.5697217360138893
RMSE train: 0.575562	val: 1.201556	test: 1.353253
MAE train: 0.449184	val: 0.954312	test: 1.093519

Epoch: 106
Loss: 0.5265980362892151
RMSE train: 0.576368	val: 1.211627	test: 1.360289
MAE train: 0.451490	val: 0.955369	test: 1.100880

Epoch: 107
Loss: 0.568391963839531
RMSE train: 0.549449	val: 1.212643	test: 1.337481
MAE train: 0.430964	val: 0.945541	test: 1.080575

Epoch: 108
Loss: 0.5731396600604057
RMSE train: 0.508646	val: 1.229010	test: 1.340154
MAE train: 0.393777	val: 0.938592	test: 1.068561

Epoch: 109
Loss: 0.7094484865665436
RMSE train: 0.481345	val: 1.213959	test: 1.320847
MAE train: 0.367882	val: 0.951248	test: 1.054113

Epoch: 110
Loss: 0.5052874311804771
RMSE train: 0.584806	val: 1.251349	test: 1.381679
MAE train: 0.455883	val: 1.018044	test: 1.111515

Epoch: 111
Loss: 1.0211207121610641
RMSE train: 0.550339	val: 1.201563	test: 1.305535
MAE train: 0.422179	val: 0.939775	test: 1.041897

Epoch: 112
Loss: 0.6295376047492027
RMSE train: 0.684325	val: 1.246180	test: 1.376316
MAE train: 0.531068	val: 0.971104	test: 1.068300

Epoch: 113
Loss: 0.7554106563329697
RMSE train: 0.583040	val: 1.366792	test: 1.471895
MAE train: 0.435246	val: 1.049211	test: 1.192669

Epoch: 114
Loss: 0.7182628959417343
RMSE train: 0.585869	val: 1.388101	test: 1.463688
MAE train: 0.459442	val: 1.097977	test: 1.184153

Epoch: 115
Loss: 0.5761087834835052
RMSE train: 0.687424	val: 1.315206	test: 1.398842
MAE train: 0.545395	val: 1.054574	test: 1.127522

Epoch: 116
Loss: 0.5450623780488968
RMSE train: 0.667236	val: 1.287617	test: 1.409015
MAE train: 0.510753	val: 1.028314	test: 1.129580

Epoch: 117
Loss: 0.655808612704277
RMSE train: 0.582509	val: 1.258744	test: 1.376752
MAE train: 0.453561	val: 0.982735	test: 1.094243

Epoch: 118
Loss: 0.4840552359819412
RMSE train: 0.571426	val: 1.279554	test: 1.388992
MAE train: 0.453038	val: 0.966073	test: 1.079485

Epoch: 119
Loss: 0.4309590756893158
RMSE train: 0.534483	val: 1.244615	test: 1.358332
MAE train: 0.409819	val: 0.975764	test: 1.081403

Epoch: 120
Loss: 0.4158677011728287
RMSE train: 0.614239	val: 1.346970	test: 1.458197
MAE train: 0.480570	val: 1.062202	test: 1.164598

Epoch: 121
Loss: 0.42816104739904404
RMSE train: 0.596969	val: 1.336548	test: 1.458168
MAE train: 0.462092	val: 1.064248	test: 1.160726

Epoch: 122
Loss: 0.3510403521358967
RMSE train: 0.565161	val: 1.226420	test: 1.367337
MAE train: 0.428083	val: 0.987650	test: 1.108310

Epoch: 123
Loss: 0.5195379853248596
RMSE train: 0.523832	val: 1.208907	test: 1.323501
MAE train: 0.397821	val: 0.968821	test: 1.067165

Epoch: 124
Loss: 0.557307705283165
RMSE train: 0.604541	val: 1.269666	test: 1.391869
MAE train: 0.460576	val: 1.011158	test: 1.116178

Epoch: 125
Loss: 0.5553168505430222
RMSE train: 0.597363	val: 1.259199	test: 1.350659
MAE train: 0.455783	val: 0.998035	test: 1.074167

Epoch: 126
Loss: 0.4124024286866188
RMSE train: 0.548558	val: 1.249338	test: 1.317893
MAE train: 0.421781	val: 0.971893	test: 1.038273

Epoch: 127
Loss: 0.42465321719646454
RMSE train: 0.527934	val: 1.342723	test: 1.410720
MAE train: 0.400617	val: 1.034363	test: 1.114818

Epoch: 128
Loss: 0.555507592856884
RMSE train: 0.535705	val: 1.344651	test: 1.405593
MAE train: 0.411326	val: 1.055717	test: 1.121313

Epoch: 129
Loss: 0.5110708624124527
RMSE train: 0.626017	val: 1.305307	test: 1.396672
MAE train: 0.495257	val: 1.037856	test: 1.114643

Epoch: 130
Loss: 0.5019995793700218
RMSE train: 0.649940	val: 1.325215	test: 1.432298
MAE train: 0.521626	val: 1.040518	test: 1.155718

Epoch: 131
Loss: 0.5278668776154518
RMSE train: 0.527241	val: 1.216119	test: 1.303012
MAE train: 0.398245	val: 0.975329	test: 1.049107

Epoch: 132
Loss: 0.4468330889940262
RMSE train: 0.583683	val: 1.215076	test: 1.314567
MAE train: 0.430485	val: 0.963571	test: 1.057122

Epoch: 133
Loss: 0.5853520706295967
RMSE train: 0.551273	val: 1.210233	test: 1.286622
MAE train: 0.412248	val: 0.961355	test: 1.044995

Epoch: 134
Loss: 0.42426788806915283
RMSE train: 0.613921	val: 1.249490	test: 1.336521
MAE train: 0.451781	val: 0.978126	test: 1.070800

Epoch: 135
Loss: 0.42412666976451874
RMSE train: 0.628207	val: 1.193197	test: 1.313923
MAE train: 0.452101	val: 0.950468	test: 1.063501

Epoch: 136
Loss: 0.4680154547095299
RMSE train: 0.545274	val: 1.201938	test: 1.281535
MAE train: 0.402710	val: 0.953161	test: 1.027314

Epoch: 137
Loss: 0.4975631460547447
RMSE train: 0.553793	val: 1.261754	test: 1.326953
MAE train: 0.432824	val: 1.008186	test: 1.071968

Epoch: 138
Loss: 0.549973875284195
RMSE train: 0.591365	val: 1.234185	test: 1.287269
MAE train: 0.464567	val: 0.976714	test: 1.042018

Epoch: 139
Loss: 0.41568902879953384
RMSE train: 0.534872	val: 1.190146	test: 1.251951
MAE train: 0.419814	val: 0.928011	test: 0.991699

Epoch: 140
Loss: 0.4089188203215599
RMSE train: 0.498528	val: 1.248119	test: 1.355358
MAE train: 0.377367	val: 0.961580	test: 1.075573

Epoch: 141
Loss: 0.4761636182665825
RMSE train: 0.499493	val: 1.256161	test: 1.378649
MAE train: 0.372090	val: 0.979926	test: 1.111473

Epoch: 142
Loss: 0.44447507709264755
RMSE train: 0.607495	val: 1.240497	test: 1.359605
MAE train: 0.470437	val: 0.979008	test: 1.088957

Epoch: 143
Loss: 0.5266163498163223
RMSE train: 0.630334	val: 1.272213	test: 1.385755
MAE train: 0.492861	val: 0.993723	test: 1.113123

Epoch: 144
Loss: 0.4421485513448715
RMSE train: 0.580187	val: 1.248166	test: 1.382412

Epoch: 84
Loss: 0.3614654913544655
RMSE train: 0.517996	val: 1.183334	test: 1.175857
MAE train: 0.389930	val: 0.959963	test: 0.890956

Epoch: 85
Loss: 0.38925954699516296
RMSE train: 0.530548	val: 1.239367	test: 1.192566
MAE train: 0.406874	val: 1.000797	test: 0.919263

Epoch: 86
Loss: 0.35384000837802887
RMSE train: 0.499792	val: 1.123451	test: 1.124916
MAE train: 0.384882	val: 0.913900	test: 0.868185

Epoch: 87
Loss: 0.34538570791482925
RMSE train: 0.499641	val: 1.098077	test: 1.098419
MAE train: 0.388093	val: 0.900964	test: 0.861716

Epoch: 88
Loss: 0.347508504986763
RMSE train: 0.482364	val: 1.179260	test: 1.123488
MAE train: 0.371661	val: 0.962204	test: 0.894084

Epoch: 89
Loss: 0.3852468729019165
RMSE train: 0.513033	val: 1.136015	test: 1.148710
MAE train: 0.388940	val: 0.931834	test: 0.892502

Epoch: 90
Loss: 0.4106801375746727
RMSE train: 0.467169	val: 1.138318	test: 1.088777
MAE train: 0.361374	val: 0.916689	test: 0.870341

Epoch: 91
Loss: 0.3651024028658867
RMSE train: 0.498021	val: 1.215342	test: 1.111402
MAE train: 0.381651	val: 0.964822	test: 0.882620

Epoch: 92
Loss: 0.3725082203745842
RMSE train: 0.495101	val: 1.106352	test: 1.105715
MAE train: 0.376718	val: 0.901407	test: 0.877312

Epoch: 93
Loss: 0.41481148451566696
RMSE train: 0.535639	val: 1.310530	test: 1.171573
MAE train: 0.408506	val: 1.038052	test: 0.923833

Epoch: 94
Loss: 0.3636396676301956
RMSE train: 0.520419	val: 1.127190	test: 1.159271
MAE train: 0.387181	val: 0.925068	test: 0.910026

Epoch: 95
Loss: 0.3869553953409195
RMSE train: 0.512378	val: 1.232816	test: 1.222364
MAE train: 0.389967	val: 1.001010	test: 0.948358

Epoch: 96
Loss: 0.38942018151283264
RMSE train: 0.505399	val: 1.310311	test: 1.255726
MAE train: 0.395651	val: 1.038518	test: 0.984401

Epoch: 97
Loss: 0.3741665855050087
RMSE train: 0.526674	val: 1.042356	test: 1.138255
MAE train: 0.419374	val: 0.840033	test: 0.915032

Epoch: 98
Loss: 0.4077649340033531
RMSE train: 0.503215	val: 1.183011	test: 1.098338
MAE train: 0.393845	val: 0.938324	test: 0.877417

Epoch: 99
Loss: 0.3680931404232979
RMSE train: 0.473895	val: 1.174698	test: 1.102656
MAE train: 0.375863	val: 0.924666	test: 0.898461

Epoch: 100
Loss: 0.38581884652376175
RMSE train: 0.514693	val: 1.153376	test: 1.169407
MAE train: 0.414111	val: 0.915708	test: 0.934666

Epoch: 101
Loss: 0.37615178525447845
RMSE train: 0.468498	val: 1.278266	test: 1.191052
MAE train: 0.367437	val: 1.016965	test: 0.947605

Epoch: 102
Loss: 0.3724247068166733
RMSE train: 0.439706	val: 1.155563	test: 1.137999
MAE train: 0.341149	val: 0.939101	test: 0.907113

Epoch: 103
Loss: 0.3438781872391701
RMSE train: 0.470960	val: 1.095312	test: 1.128831
MAE train: 0.356016	val: 0.910060	test: 0.896030

Epoch: 104
Loss: 0.3213404342532158
RMSE train: 0.528987	val: 1.258747	test: 1.211269
MAE train: 0.400800	val: 1.002566	test: 0.928716

Epoch: 105
Loss: 0.3179992064833641
RMSE train: 0.499409	val: 1.237041	test: 1.197276
MAE train: 0.374283	val: 0.984945	test: 0.921041

Epoch: 106
Loss: 0.37549588084220886
RMSE train: 0.499812	val: 1.179059	test: 1.160024
MAE train: 0.374719	val: 0.938686	test: 0.909072

Epoch: 107
Loss: 0.32901160418987274
RMSE train: 0.503317	val: 1.321879	test: 1.154527
MAE train: 0.389599	val: 1.027478	test: 0.903267

Epoch: 108
Loss: 0.3747204467654228
RMSE train: 0.483507	val: 1.181884	test: 1.143110
MAE train: 0.361819	val: 0.920279	test: 0.901450

Epoch: 109
Loss: 0.3626590073108673
RMSE train: 0.459273	val: 1.178636	test: 1.128360
MAE train: 0.347930	val: 0.928963	test: 0.901627

Epoch: 110
Loss: 0.3370608687400818
RMSE train: 0.549340	val: 1.337395	test: 1.250019
MAE train: 0.416383	val: 1.046476	test: 0.969227

Epoch: 111
Loss: 0.3482664078474045
RMSE train: 0.481455	val: 1.160174	test: 1.174957
MAE train: 0.366907	val: 0.931842	test: 0.933357

Epoch: 112
Loss: 0.387559175491333
RMSE train: 0.461839	val: 1.089333	test: 1.165183
MAE train: 0.353682	val: 0.884451	test: 0.929825

Epoch: 113
Loss: 0.33975251764059067
RMSE train: 0.539777	val: 1.203484	test: 1.196465
MAE train: 0.414479	val: 0.976832	test: 0.912037

Epoch: 114
Loss: 0.3111172541975975
RMSE train: 0.503910	val: 1.096019	test: 1.156870
MAE train: 0.386486	val: 0.917051	test: 0.890601

Epoch: 115
Loss: 0.32081388682127
RMSE train: 0.542951	val: 1.196972	test: 1.218603
MAE train: 0.410254	val: 0.982763	test: 0.914178

Epoch: 116
Loss: 0.28467319905757904
RMSE train: 0.526924	val: 1.188858	test: 1.193980
MAE train: 0.402716	val: 0.975359	test: 0.890761

Epoch: 117
Loss: 0.3903384953737259
RMSE train: 0.481786	val: 1.062590	test: 1.130909
MAE train: 0.361847	val: 0.884655	test: 0.864069

Epoch: 118
Loss: 0.30274084210395813
RMSE train: 0.475932	val: 1.076572	test: 1.120609
MAE train: 0.357301	val: 0.886313	test: 0.855690

Epoch: 119
Loss: 0.28932350128889084
RMSE train: 0.486017	val: 1.262607	test: 1.156138
MAE train: 0.376608	val: 1.005925	test: 0.899293

Epoch: 120
Loss: 0.3192110061645508
RMSE train: 0.438813	val: 1.105436	test: 1.103661
MAE train: 0.334566	val: 0.888964	test: 0.879793

Epoch: 121
Loss: 0.3501845449209213
RMSE train: 0.438770	val: 1.068826	test: 1.101165
MAE train: 0.333394	val: 0.872461	test: 0.865128

Epoch: 122
Loss: 0.3038457781076431
RMSE train: 0.440472	val: 1.227512	test: 1.144219
MAE train: 0.340607	val: 0.979252	test: 0.917719

Epoch: 123
Loss: 0.3029263988137245
RMSE train: 0.427306	val: 1.150142	test: 1.140895
MAE train: 0.334045	val: 0.917383	test: 0.919020

Epoch: 124
Loss: 0.2768377214670181
RMSE train: 0.442434	val: 1.152102	test: 1.110925
MAE train: 0.350020	val: 0.913705	test: 0.892894

Epoch: 125
Loss: 0.25865451991558075
RMSE train: 0.460359	val: 1.184440	test: 1.091860
MAE train: 0.367011	val: 0.921641	test: 0.871650

Epoch: 126
Loss: 0.31507638841867447
RMSE train: 0.431530	val: 1.069348	test: 1.092525
MAE train: 0.330457	val: 0.848574	test: 0.848437

Epoch: 127
Loss: 0.296132355928421
RMSE train: 0.420451	val: 1.134969	test: 1.109139
MAE train: 0.323518	val: 0.906564	test: 0.858479

Epoch: 128
Loss: 0.31222161650657654
RMSE train: 0.463392	val: 1.173744	test: 1.134989
MAE train: 0.360448	val: 0.931341	test: 0.884978

Epoch: 129
Loss: 0.30751144886016846
RMSE train: 0.433029	val: 1.077074	test: 1.107357
MAE train: 0.341534	val: 0.849221	test: 0.875065

Epoch: 130
Loss: 0.27303582429885864
RMSE train: 0.442946	val: 1.104363	test: 1.123737
MAE train: 0.341907	val: 0.886538	test: 0.897425

Epoch: 131
Loss: 0.2967202924191952
RMSE train: 0.453169	val: 1.158996	test: 1.178172
MAE train: 0.347805	val: 0.931482	test: 0.921990

Epoch: 132
Loss: 0.29420075565576553
RMSE train: 0.503247	val: 1.181378	test: 1.219538
MAE train: 0.376962	val: 0.948291	test: 0.938419

Early stopping
Best (RMSE):	 train: 0.526674	val: 1.042356	test: 1.138255
Best (MAE):	 train: 0.419374	val: 0.840033	test: 0.915032
All runs completed.

RMSE train: 0.457793	val: 1.468667	test: 1.585009
MAE train: 0.350994	val: 1.200171	test: 1.254597

Epoch: 145
Loss: 0.25494300822416943
RMSE train: 0.442455	val: 1.458691	test: 1.575537
MAE train: 0.334512	val: 1.182588	test: 1.250912

Epoch: 146
Loss: 0.27785201867421466
RMSE train: 0.464672	val: 1.431755	test: 1.581583
MAE train: 0.359618	val: 1.156100	test: 1.248667

Epoch: 147
Loss: 0.2609407951434453
RMSE train: 0.451529	val: 1.423222	test: 1.597338
MAE train: 0.352408	val: 1.148618	test: 1.257984

Epoch: 148
Loss: 0.265636404355367
RMSE train: 0.463085	val: 1.427396	test: 1.533400
MAE train: 0.348583	val: 1.163516	test: 1.208639

Epoch: 149
Loss: 0.28270086149374646
RMSE train: 0.463123	val: 1.468306	test: 1.501771
MAE train: 0.348466	val: 1.212866	test: 1.176845

Epoch: 150
Loss: 0.24082188308238983
RMSE train: 0.444060	val: 1.446581	test: 1.650034
MAE train: 0.337751	val: 1.166905	test: 1.282435

Epoch: 151
Loss: 0.3029120961825053
RMSE train: 0.436178	val: 1.432661	test: 1.606562
MAE train: 0.332443	val: 1.142952	test: 1.256591

Epoch: 152
Loss: 0.30264122287432355
RMSE train: 0.430309	val: 1.407851	test: 1.532454
MAE train: 0.325295	val: 1.115531	test: 1.211347

Epoch: 153
Loss: 0.3237479825814565
RMSE train: 0.416781	val: 1.395510	test: 1.557711
MAE train: 0.319060	val: 1.111295	test: 1.229483

Epoch: 154
Loss: 0.27362718681494397
RMSE train: 0.428042	val: 1.407520	test: 1.519878
MAE train: 0.327723	val: 1.133720	test: 1.192635

Epoch: 155
Loss: 0.26847266654173535
RMSE train: 0.442961	val: 1.423649	test: 1.513785
MAE train: 0.334953	val: 1.162960	test: 1.191640

Epoch: 156
Loss: 0.2896401385466258
RMSE train: 0.432433	val: 1.468644	test: 1.694506
MAE train: 0.337700	val: 1.211466	test: 1.327546

Epoch: 157
Loss: 0.2842698097229004
RMSE train: 0.429869	val: 1.475005	test: 1.621898
MAE train: 0.328146	val: 1.203629	test: 1.273749

Epoch: 158
Loss: 0.2724663068850835
RMSE train: 0.436180	val: 1.478207	test: 1.500895
MAE train: 0.326411	val: 1.204641	test: 1.187430

Epoch: 159
Loss: 0.2545555780331294
RMSE train: 0.436426	val: 1.510139	test: 1.582637
MAE train: 0.345422	val: 1.227585	test: 1.237189

Epoch: 160
Loss: 0.26526857912540436
RMSE train: 0.472549	val: 1.603627	test: 1.738462
MAE train: 0.366964	val: 1.301406	test: 1.376463

Epoch: 161
Loss: 0.2681708832581838
RMSE train: 0.406242	val: 1.557232	test: 1.610817
MAE train: 0.308148	val: 1.249935	test: 1.279855

Epoch: 162
Loss: 0.30156266689300537
RMSE train: 0.424073	val: 1.554131	test: 1.558186
MAE train: 0.321120	val: 1.254179	test: 1.212817

Epoch: 163
Loss: 0.25245794653892517
RMSE train: 0.483978	val: 1.650815	test: 1.639058
MAE train: 0.377843	val: 1.329477	test: 1.275479

Epoch: 164
Loss: 0.2842484414577484
RMSE train: 0.441754	val: 1.704024	test: 1.694225
MAE train: 0.342132	val: 1.357078	test: 1.315155

Epoch: 165
Loss: 0.24868988494078317
RMSE train: 0.404974	val: 1.682769	test: 1.691491
MAE train: 0.308272	val: 1.328443	test: 1.321787

Epoch: 166
Loss: 0.2617465903361638
RMSE train: 0.424009	val: 1.628872	test: 1.558617
MAE train: 0.319111	val: 1.293981	test: 1.222706

Epoch: 167
Loss: 0.2620592614014943
RMSE train: 0.460037	val: 1.612064	test: 1.578379
MAE train: 0.355550	val: 1.300340	test: 1.233448

Epoch: 168
Loss: 0.247991810242335
RMSE train: 0.434996	val: 1.596278	test: 1.572916
MAE train: 0.333967	val: 1.286309	test: 1.224492

Epoch: 169
Loss: 0.24120446542898813
RMSE train: 0.377324	val: 1.639140	test: 1.620301
MAE train: 0.291445	val: 1.306380	test: 1.265530

Epoch: 170
Loss: 0.27858130633831024
RMSE train: 0.376336	val: 1.619897	test: 1.601386
MAE train: 0.282782	val: 1.292748	test: 1.247497

Epoch: 171
Loss: 0.2506479223569234
RMSE train: 0.478414	val: 1.601155	test: 1.554015
MAE train: 0.366064	val: 1.285477	test: 1.210287

Epoch: 172
Loss: 0.30910319089889526
RMSE train: 0.426763	val: 1.630758	test: 1.566486
MAE train: 0.335478	val: 1.288973	test: 1.227046

Epoch: 173
Loss: 0.2659284273783366
RMSE train: 0.378784	val: 1.668616	test: 1.639993
MAE train: 0.290912	val: 1.315473	test: 1.280467

Epoch: 174
Loss: 0.291710764169693
RMSE train: 0.454354	val: 1.688367	test: 1.707322
MAE train: 0.345096	val: 1.355859	test: 1.327507

Epoch: 175
Loss: 0.2525997459888458
RMSE train: 0.439071	val: 1.663550	test: 1.707880
MAE train: 0.330175	val: 1.340129	test: 1.329145

Epoch: 176
Loss: 0.23134896159172058
RMSE train: 0.402492	val: 1.576517	test: 1.629016
MAE train: 0.304079	val: 1.275967	test: 1.269634

Epoch: 177
Loss: 0.2612110773722331
RMSE train: 0.402050	val: 1.571269	test: 1.679891
MAE train: 0.305360	val: 1.285899	test: 1.298040

Epoch: 178
Loss: 0.24457481503486633
RMSE train: 0.395835	val: 1.539533	test: 1.659617
MAE train: 0.305286	val: 1.262118	test: 1.289661

Epoch: 179
Loss: 0.21776392559210458
RMSE train: 0.431102	val: 1.541780	test: 1.689223
MAE train: 0.332706	val: 1.265780	test: 1.313633

Epoch: 180
Loss: 0.2754499862591426
RMSE train: 0.435578	val: 1.473943	test: 1.525819
MAE train: 0.325971	val: 1.198729	test: 1.181350

Epoch: 181
Loss: 0.24291057387987772
RMSE train: 0.450460	val: 1.478831	test: 1.498054
MAE train: 0.326655	val: 1.211269	test: 1.154365

Epoch: 182
Loss: 0.3416481912136078
RMSE train: 0.397253	val: 1.529516	test: 1.614515
MAE train: 0.292134	val: 1.241976	test: 1.253960

Epoch: 183
Loss: 0.24289261798063913
RMSE train: 0.506448	val: 1.599983	test: 1.735997
MAE train: 0.377882	val: 1.303718	test: 1.346708

Epoch: 184
Loss: 0.28779401381810504
RMSE train: 0.437238	val: 1.483861	test: 1.589787
MAE train: 0.327565	val: 1.205905	test: 1.218675

Epoch: 185
Loss: 0.24368926386038461
RMSE train: 0.375013	val: 1.458005	test: 1.597576
MAE train: 0.281975	val: 1.188901	test: 1.215159

Epoch: 186
Loss: 0.27441583077112836
RMSE train: 0.385358	val: 1.478216	test: 1.665431
MAE train: 0.296050	val: 1.207080	test: 1.275248

Epoch: 187
Loss: 0.24444916347662607
RMSE train: 0.431646	val: 1.483633	test: 1.687706
MAE train: 0.328093	val: 1.211752	test: 1.300878

Epoch: 188
Loss: 0.23755494256814322
RMSE train: 0.453755	val: 1.443891	test: 1.605157
MAE train: 0.338218	val: 1.174033	test: 1.238643

Early stopping
Best (RMSE):	 train: 0.416781	val: 1.395510	test: 1.557711
Best (MAE):	 train: 0.319060	val: 1.111295	test: 1.229483
All runs completed.

MAE train: 0.444684	val: 0.980370	test: 1.115942

Epoch: 145
Loss: 0.46237632632255554
RMSE train: 0.509595	val: 1.168037	test: 1.307329
MAE train: 0.375697	val: 0.918912	test: 1.049772

Epoch: 146
Loss: 0.5127425640821457
RMSE train: 0.514890	val: 1.178922	test: 1.304212
MAE train: 0.379760	val: 0.924973	test: 1.047688

Epoch: 147
Loss: 0.42875124514102936
RMSE train: 0.524605	val: 1.306811	test: 1.422496
MAE train: 0.398788	val: 1.014798	test: 1.150386

Epoch: 148
Loss: 0.3766074851155281
RMSE train: 0.590051	val: 1.357552	test: 1.462052
MAE train: 0.464444	val: 1.059484	test: 1.180481

Epoch: 149
Loss: 0.5446688681840897
RMSE train: 0.530498	val: 1.262645	test: 1.338905
MAE train: 0.416099	val: 1.000015	test: 1.079528

Epoch: 150
Loss: 0.5964450612664223
RMSE train: 0.497329	val: 1.221753	test: 1.297632
MAE train: 0.385282	val: 0.951454	test: 1.046537

Epoch: 151
Loss: 0.5503784865140915
RMSE train: 0.486939	val: 1.233098	test: 1.374445
MAE train: 0.368118	val: 0.982629	test: 1.104325

Epoch: 152
Loss: 0.6639040857553482
RMSE train: 0.586893	val: 1.217189	test: 1.408024
MAE train: 0.448002	val: 0.988156	test: 1.132574

Epoch: 153
Loss: 0.5141912549734116
RMSE train: 0.586645	val: 1.240462	test: 1.413785
MAE train: 0.458671	val: 0.975774	test: 1.130494

Epoch: 154
Loss: 0.4513091966509819
RMSE train: 0.511944	val: 1.228967	test: 1.352122
MAE train: 0.407641	val: 0.955512	test: 1.087199

Epoch: 155
Loss: 0.4992997273802757
RMSE train: 0.531908	val: 1.244075	test: 1.321993
MAE train: 0.436656	val: 0.952607	test: 1.067326

Epoch: 156
Loss: 0.6487690508365631
RMSE train: 0.549065	val: 1.374327	test: 1.425120
MAE train: 0.442018	val: 1.044038	test: 1.154565

Epoch: 157
Loss: 0.4419567435979843
RMSE train: 0.608696	val: 1.410384	test: 1.470154
MAE train: 0.483617	val: 1.083322	test: 1.194296

Epoch: 158
Loss: 0.6444186344742775
RMSE train: 0.591805	val: 1.349705	test: 1.434162
MAE train: 0.473767	val: 1.042805	test: 1.156854

Epoch: 159
Loss: 0.4286099374294281
RMSE train: 0.523352	val: 1.361654	test: 1.473894
MAE train: 0.406667	val: 1.069064	test: 1.197170

Epoch: 160
Loss: 0.4270423650741577
RMSE train: 0.483991	val: 1.292351	test: 1.388429
MAE train: 0.368780	val: 1.028361	test: 1.124686

Epoch: 161
Loss: 0.45907895267009735
RMSE train: 0.512536	val: 1.277386	test: 1.382778
MAE train: 0.384925	val: 1.028119	test: 1.122845

Epoch: 162
Loss: 0.5599715113639832
RMSE train: 0.521737	val: 1.292892	test: 1.391665
MAE train: 0.394595	val: 1.027744	test: 1.125683

Epoch: 163
Loss: 0.4158986508846283
RMSE train: 0.646318	val: 1.366690	test: 1.511210
MAE train: 0.479543	val: 1.082038	test: 1.215205

Epoch: 164
Loss: 0.37355970591306686
RMSE train: 0.564909	val: 1.289238	test: 1.406677
MAE train: 0.424874	val: 1.018008	test: 1.127710

Epoch: 165
Loss: 0.36497192084789276
RMSE train: 0.514030	val: 1.226764	test: 1.321448
MAE train: 0.388886	val: 0.970013	test: 1.064007

Epoch: 166
Loss: 0.5322680026292801
RMSE train: 0.495396	val: 1.296959	test: 1.386555
MAE train: 0.377429	val: 1.004198	test: 1.112221

Epoch: 167
Loss: 0.33271337300539017
RMSE train: 0.464277	val: 1.332806	test: 1.420167
MAE train: 0.353767	val: 1.020139	test: 1.145461

Epoch: 168
Loss: 0.3447142653167248
RMSE train: 0.495449	val: 1.273727	test: 1.356145
MAE train: 0.381581	val: 0.988591	test: 1.083258

Epoch: 169
Loss: 0.3449826054275036
RMSE train: 0.558872	val: 1.290003	test: 1.379096
MAE train: 0.432793	val: 1.003370	test: 1.099275

Epoch: 170
Loss: 0.39636313170194626
RMSE train: 0.553761	val: 1.279674	test: 1.368741
MAE train: 0.427089	val: 1.003384	test: 1.089572

Epoch: 171
Loss: 0.3539794310927391
RMSE train: 0.501339	val: 1.303437	test: 1.407438
MAE train: 0.383564	val: 1.012097	test: 1.127013

Epoch: 172
Loss: 0.5314128324389458
RMSE train: 0.520037	val: 1.312526	test: 1.420705
MAE train: 0.398030	val: 1.018797	test: 1.133481

Epoch: 173
Loss: 0.3477771393954754
RMSE train: 0.534183	val: 1.227650	test: 1.339680
MAE train: 0.407521	val: 0.983457	test: 1.073172

Epoch: 174
Loss: 0.4253023862838745
RMSE train: 0.544312	val: 1.288400	test: 1.420529
MAE train: 0.419659	val: 1.019528	test: 1.146681

Epoch: 175
Loss: 0.3681338205933571
RMSE train: 0.526333	val: 1.318468	test: 1.444740
MAE train: 0.397198	val: 1.049473	test: 1.167835

Epoch: 176
Loss: 0.3362446054816246
RMSE train: 0.537724	val: 1.253317	test: 1.345229
MAE train: 0.388154	val: 1.002458	test: 1.078518

Epoch: 177
Loss: 0.3949557840824127
RMSE train: 0.526737	val: 1.219463	test: 1.293243
MAE train: 0.394280	val: 0.967024	test: 1.034328

Epoch: 178
Loss: 0.6935116499662399
RMSE train: 0.493236	val: 1.264649	test: 1.340817
MAE train: 0.378863	val: 0.980511	test: 1.071733

Epoch: 179
Loss: 0.490495927631855
RMSE train: 0.490696	val: 1.309425	test: 1.386184
MAE train: 0.380469	val: 1.006445	test: 1.109776

Epoch: 180
Loss: 0.3598719760775566
RMSE train: 0.506002	val: 1.321773	test: 1.375617
MAE train: 0.400198	val: 1.013206	test: 1.104322

Early stopping
Best (RMSE):	 train: 0.509595	val: 1.168037	test: 1.307329
Best (MAE):	 train: 0.375697	val: 0.918912	test: 1.049772
All runs completed.
