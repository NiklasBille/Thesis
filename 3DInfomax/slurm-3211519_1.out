>>> Starting run for dataset: bbbp
Running configs_static_noise_experiments/GraphCL/bbbp/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/GraphCL/bbbp/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/GraphCL/bbbp/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/GraphCL/bbbp/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.05/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.05_5_26-05_10-25-47
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.05
logdir: runs/static_noise/GraphCL/bbbp/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6933680
[Epoch 1] ogbg-molbbbp: 0.620034 val loss: 0.691401
[Epoch 1] ogbg-molbbbp: 0.547261 test loss: 0.692270
[Epoch 2; Iter     5/   55] train: loss: 0.6895829
[Epoch 2; Iter    35/   55] train: loss: 0.6908755
[Epoch 2] ogbg-molbbbp: 0.553221 val loss: 0.690096
[Epoch 2] ogbg-molbbbp: 0.572049 test loss: 0.690793
[Epoch 3; Iter    10/   55] train: loss: 0.6920294
[Epoch 3; Iter    40/   55] train: loss: 0.6891128
[Epoch 3] ogbg-molbbbp: 0.563377 val loss: 0.690095
[Epoch 3] ogbg-molbbbp: 0.570505 test loss: 0.690807
[Epoch 4; Iter    15/   55] train: loss: 0.6915734
[Epoch 4; Iter    45/   55] train: loss: 0.6909980
[Epoch 4] ogbg-molbbbp: 0.587773 val loss: 0.689667
[Epoch 4] ogbg-molbbbp: 0.573110 test loss: 0.690465
[Epoch 5; Iter    20/   55] train: loss: 0.6891560
[Epoch 5; Iter    50/   55] train: loss: 0.6878657
[Epoch 5] ogbg-molbbbp: 0.589565 val loss: 0.689860
[Epoch 5] ogbg-molbbbp: 0.582948 test loss: 0.690038
[Epoch 6; Iter    25/   55] train: loss: 0.6920459
[Epoch 6; Iter    55/   55] train: loss: 0.6868108
[Epoch 6] ogbg-molbbbp: 0.581400 val loss: 0.689532
[Epoch 6] ogbg-molbbbp: 0.572820 test loss: 0.690332
[Epoch 7; Iter    30/   55] train: loss: 0.6831301
[Epoch 7] ogbg-molbbbp: 0.592851 val loss: 0.689729
[Epoch 7] ogbg-molbbbp: 0.584684 test loss: 0.689591
[Epoch 8; Iter     5/   55] train: loss: 0.6860273
[Epoch 8; Iter    35/   55] train: loss: 0.6837773
[Epoch 8] ogbg-molbbbp: 0.603704 val loss: 0.689583
[Epoch 8] ogbg-molbbbp: 0.584298 test loss: 0.689609
[Epoch 9; Iter    10/   55] train: loss: 0.6809671
[Epoch 9; Iter    40/   55] train: loss: 0.6832955
[Epoch 9] ogbg-molbbbp: 0.621826 val loss: 0.689474
[Epoch 9] ogbg-molbbbp: 0.583816 test loss: 0.689327
[Epoch 10; Iter    15/   55] train: loss: 0.6789730
[Epoch 10; Iter    45/   55] train: loss: 0.6766293
[Epoch 10] ogbg-molbbbp: 0.644529 val loss: 0.689397
[Epoch 10] ogbg-molbbbp: 0.593557 test loss: 0.689020
[Epoch 11; Iter    20/   55] train: loss: 0.6762549
[Epoch 11; Iter    50/   55] train: loss: 0.6757666
[Epoch 11] ogbg-molbbbp: 0.661854 val loss: 0.688529
[Epoch 11] ogbg-molbbbp: 0.593654 test loss: 0.688523
[Epoch 12; Iter    25/   55] train: loss: 0.6771121
[Epoch 12; Iter    55/   55] train: loss: 0.6757162
[Epoch 12] ogbg-molbbbp: 0.675296 val loss: 0.688892
[Epoch 12] ogbg-molbbbp: 0.600019 test loss: 0.688210
[Epoch 13; Iter    30/   55] train: loss: 0.6758933
[Epoch 13] ogbg-molbbbp: 0.887086 val loss: 0.650529
[Epoch 13] ogbg-molbbbp: 0.586709 test loss: 0.684799
[Epoch 14; Iter     5/   55] train: loss: 0.6654325
[Epoch 14; Iter    35/   55] train: loss: 0.6215883
[Epoch 14] ogbg-molbbbp: 0.824754 val loss: 0.624757
[Epoch 14] ogbg-molbbbp: 0.636478 test loss: 0.668645
[Epoch 15; Iter    10/   55] train: loss: 0.5981723
[Epoch 15; Iter    40/   55] train: loss: 0.5498682
[Epoch 15] ogbg-molbbbp: 0.879618 val loss: 0.588365
[Epoch 15] ogbg-molbbbp: 0.638021 test loss: 0.725715
[Epoch 16; Iter    15/   55] train: loss: 0.4587839
[Epoch 16; Iter    45/   55] train: loss: 0.5394275
[Epoch 16] ogbg-molbbbp: 0.916260 val loss: 0.531722
[Epoch 16] ogbg-molbbbp: 0.644290 test loss: 0.768640
[Epoch 17; Iter    20/   55] train: loss: 0.3583268
[Epoch 17; Iter    50/   55] train: loss: 0.5113896
[Epoch 17] ogbg-molbbbp: 0.823758 val loss: 0.651814
[Epoch 17] ogbg-molbbbp: 0.629051 test loss: 0.793741
[Epoch 18; Iter    25/   55] train: loss: 0.3705954
[Epoch 18; Iter    55/   55] train: loss: 0.4513265
[Epoch 18] ogbg-molbbbp: 0.910385 val loss: 0.654015
[Epoch 18] ogbg-molbbbp: 0.633005 test loss: 1.074800
[Epoch 19; Iter    30/   55] train: loss: 0.4035589
[Epoch 19] ogbg-molbbbp: 0.936174 val loss: 0.467089
[Epoch 19] ogbg-molbbbp: 0.625965 test loss: 1.040576
[Epoch 20; Iter     5/   55] train: loss: 0.4044296
[Epoch 20; Iter    35/   55] train: loss: 0.2419277
[Epoch 20] ogbg-molbbbp: 0.878821 val loss: 0.799124
[Epoch 20] ogbg-molbbbp: 0.623746 test loss: 1.112806
[Epoch 21; Iter    10/   55] train: loss: 0.3543537
[Epoch 21; Iter    40/   55] train: loss: 0.5480832
[Epoch 21] ogbg-molbbbp: 0.790999 val loss: 0.990311
[Epoch 21] ogbg-molbbbp: 0.594136 test loss: 1.115046
[Epoch 22; Iter    15/   55] train: loss: 0.3670719
[Epoch 22; Iter    45/   55] train: loss: 0.3750719
[Epoch 22] ogbg-molbbbp: 0.819277 val loss: 1.012502
[Epoch 22] ogbg-molbbbp: 0.583140 test loss: 1.170090
[Epoch 23; Iter    20/   55] train: loss: 0.3519604
[Epoch 23; Iter    50/   55] train: loss: 0.2935335
[Epoch 23] ogbg-molbbbp: 0.841282 val loss: 0.924423
[Epoch 23] ogbg-molbbbp: 0.600212 test loss: 1.265360
[Epoch 24; Iter    25/   55] train: loss: 0.5618780
[Epoch 24; Iter    55/   55] train: loss: 0.1717171
[Epoch 24] ogbg-molbbbp: 0.860799 val loss: 0.737578
[Epoch 24] ogbg-molbbbp: 0.639564 test loss: 1.023592
[Epoch 25; Iter    30/   55] train: loss: 0.2751124
[Epoch 25] ogbg-molbbbp: 0.864981 val loss: 0.780239
[Epoch 25] ogbg-molbbbp: 0.625000 test loss: 1.218389
[Epoch 26; Iter     5/   55] train: loss: 0.3228967
[Epoch 26; Iter    35/   55] train: loss: 0.2719693
[Epoch 26] ogbg-molbbbp: 0.873345 val loss: 0.610888
[Epoch 26] ogbg-molbbbp: 0.632137 test loss: 1.018412
[Epoch 27; Iter    10/   55] train: loss: 0.2661747
[Epoch 27; Iter    40/   55] train: loss: 0.2779050
[Epoch 27] ogbg-molbbbp: 0.927412 val loss: 0.418013
[Epoch 27] ogbg-molbbbp: 0.601466 test loss: 1.186253
[Epoch 28; Iter    15/   55] train: loss: 0.2523583
[Epoch 28; Iter    45/   55] train: loss: 0.1940001
[Epoch 28] ogbg-molbbbp: 0.873345 val loss: 0.712228
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.1/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.1_6_26-05_10-25-52
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.1
logdir: runs/static_noise/GraphCL/bbbp/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6904545
[Epoch 1] ogbg-molbbbp: 0.828039 val loss: 0.689998
[Epoch 1] ogbg-molbbbp: 0.554977 test loss: 0.692204
[Epoch 2; Iter     5/   55] train: loss: 0.6938904
[Epoch 2; Iter    35/   55] train: loss: 0.6919437
[Epoch 2] ogbg-molbbbp: 0.831823 val loss: 0.676801
[Epoch 2] ogbg-molbbbp: 0.583140 test loss: 0.689858
[Epoch 3; Iter    10/   55] train: loss: 0.6925102
[Epoch 3; Iter    40/   55] train: loss: 0.6873702
[Epoch 3] ogbg-molbbbp: 0.810017 val loss: 0.678459
[Epoch 3] ogbg-molbbbp: 0.586227 test loss: 0.689936
[Epoch 4; Iter    15/   55] train: loss: 0.6870177
[Epoch 4; Iter    45/   55] train: loss: 0.6906654
[Epoch 4] ogbg-molbbbp: 0.826944 val loss: 0.676514
[Epoch 4] ogbg-molbbbp: 0.587095 test loss: 0.689563
[Epoch 5; Iter    20/   55] train: loss: 0.6889075
[Epoch 5; Iter    50/   55] train: loss: 0.6861067
[Epoch 5] ogbg-molbbbp: 0.823061 val loss: 0.677005
[Epoch 5] ogbg-molbbbp: 0.592785 test loss: 0.689413
[Epoch 6; Iter    25/   55] train: loss: 0.6933991
[Epoch 6; Iter    55/   55] train: loss: 0.6872718
[Epoch 6] ogbg-molbbbp: 0.827044 val loss: 0.676288
[Epoch 6] ogbg-molbbbp: 0.590278 test loss: 0.689100
[Epoch 7; Iter    30/   55] train: loss: 0.6837851
[Epoch 7] ogbg-molbbbp: 0.825550 val loss: 0.676259
[Epoch 7] ogbg-molbbbp: 0.595679 test loss: 0.688817
[Epoch 8; Iter     5/   55] train: loss: 0.6873506
[Epoch 8; Iter    35/   55] train: loss: 0.6840113
[Epoch 8] ogbg-molbbbp: 0.826347 val loss: 0.676977
[Epoch 8] ogbg-molbbbp: 0.588542 test loss: 0.688826
[Epoch 9; Iter    10/   55] train: loss: 0.6800671
[Epoch 9; Iter    40/   55] train: loss: 0.6789539
[Epoch 9] ogbg-molbbbp: 0.844270 val loss: 0.676034
[Epoch 9] ogbg-molbbbp: 0.590278 test loss: 0.688495
[Epoch 10; Iter    15/   55] train: loss: 0.6795338
[Epoch 10; Iter    45/   55] train: loss: 0.6776021
[Epoch 10] ogbg-molbbbp: 0.839092 val loss: 0.677271
[Epoch 10] ogbg-molbbbp: 0.587770 test loss: 0.688416
[Epoch 11; Iter    20/   55] train: loss: 0.6796719
[Epoch 11; Iter    50/   55] train: loss: 0.6715820
[Epoch 11] ogbg-molbbbp: 0.847157 val loss: 0.676996
[Epoch 11] ogbg-molbbbp: 0.589410 test loss: 0.688066
[Epoch 12; Iter    25/   55] train: loss: 0.6712548
[Epoch 12; Iter    55/   55] train: loss: 0.6868832
[Epoch 12] ogbg-molbbbp: 0.849049 val loss: 0.676234
[Epoch 12] ogbg-molbbbp: 0.591821 test loss: 0.687752
[Epoch 13; Iter    30/   55] train: loss: 0.6697013
[Epoch 13] ogbg-molbbbp: 0.887086 val loss: 0.623499
[Epoch 13] ogbg-molbbbp: 0.594425 test loss: 0.680021
[Epoch 14; Iter     5/   55] train: loss: 0.6369070
[Epoch 14; Iter    35/   55] train: loss: 0.6041762
[Epoch 14] ogbg-molbbbp: 0.897142 val loss: 0.562213
[Epoch 14] ogbg-molbbbp: 0.614873 test loss: 0.685341
[Epoch 15; Iter    10/   55] train: loss: 0.5985966
[Epoch 15; Iter    40/   55] train: loss: 0.5469236
[Epoch 15] ogbg-molbbbp: 0.860301 val loss: 0.639171
[Epoch 15] ogbg-molbbbp: 0.615355 test loss: 0.764564
[Epoch 16; Iter    15/   55] train: loss: 0.5276416
[Epoch 16; Iter    45/   55] train: loss: 0.4800028
[Epoch 16] ogbg-molbbbp: 0.821368 val loss: 0.797601
[Epoch 16] ogbg-molbbbp: 0.586613 test loss: 0.855382
[Epoch 17; Iter    20/   55] train: loss: 0.5102033
[Epoch 17; Iter    50/   55] train: loss: 0.4230980
[Epoch 17] ogbg-molbbbp: 0.905506 val loss: 0.636525
[Epoch 17] ogbg-molbbbp: 0.612365 test loss: 0.869560
[Epoch 18; Iter    25/   55] train: loss: 0.3386674
[Epoch 18; Iter    55/   55] train: loss: 0.2555767
[Epoch 18] ogbg-molbbbp: 0.714826 val loss: 1.172691
[Epoch 18] ogbg-molbbbp: 0.547164 test loss: 1.103928
[Epoch 19; Iter    30/   55] train: loss: 0.2859081
[Epoch 19] ogbg-molbbbp: 0.794085 val loss: 0.979600
[Epoch 19] ogbg-molbbbp: 0.609857 test loss: 1.042725
[Epoch 20; Iter     5/   55] train: loss: 0.3766839
[Epoch 20; Iter    35/   55] train: loss: 0.5658771
[Epoch 20] ogbg-molbbbp: 0.862093 val loss: 0.773416
[Epoch 20] ogbg-molbbbp: 0.637346 test loss: 0.924535
[Epoch 21; Iter    10/   55] train: loss: 0.4682484
[Epoch 21; Iter    40/   55] train: loss: 0.3674583
[Epoch 21] ogbg-molbbbp: 0.709947 val loss: 1.639380
[Epoch 21] ogbg-molbbbp: 0.594232 test loss: 1.430145
[Epoch 22; Iter    15/   55] train: loss: 0.2774633
[Epoch 22; Iter    45/   55] train: loss: 0.3431123
[Epoch 22] ogbg-molbbbp: 0.768894 val loss: 1.234725
[Epoch 22] ogbg-molbbbp: 0.628376 test loss: 1.137691
[Epoch 23; Iter    20/   55] train: loss: 0.4971072
[Epoch 23; Iter    50/   55] train: loss: 0.3486992
[Epoch 23] ogbg-molbbbp: 0.713333 val loss: 1.268594
[Epoch 23] ogbg-molbbbp: 0.609664 test loss: 1.126815
[Epoch 24; Iter    25/   55] train: loss: 0.3233275
[Epoch 24; Iter    55/   55] train: loss: 0.1370991
[Epoch 24] ogbg-molbbbp: 0.824754 val loss: 0.785879
[Epoch 24] ogbg-molbbbp: 0.619792 test loss: 0.949574
[Epoch 25; Iter    30/   55] train: loss: 0.2742756
[Epoch 25] ogbg-molbbbp: 0.782436 val loss: 1.145263
[Epoch 25] ogbg-molbbbp: 0.574749 test loss: 1.290232
[Epoch 26; Iter     5/   55] train: loss: 0.3970367
[Epoch 26; Iter    35/   55] train: loss: 0.2167292
[Epoch 26] ogbg-molbbbp: 0.860002 val loss: 0.652459
[Epoch 26] ogbg-molbbbp: 0.626447 test loss: 0.957505
[Epoch 27; Iter    10/   55] train: loss: 0.2578063
[Epoch 27; Iter    40/   55] train: loss: 0.2447802
[Epoch 27] ogbg-molbbbp: 0.868167 val loss: 1.242799
[Epoch 27] ogbg-molbbbp: 0.639082 test loss: 0.940200
[Epoch 28; Iter    15/   55] train: loss: 0.3042915
[Epoch 28; Iter    45/   55] train: loss: 0.2297132
[Epoch 28] ogbg-molbbbp: 0.851738 val loss: 3.532327
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.1/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.1_5_26-05_10-25-51
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.1
logdir: runs/static_noise/GraphCL/bbbp/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6939878
[Epoch 1] ogbg-molbbbp: 0.610674 val loss: 0.691447
[Epoch 1] ogbg-molbbbp: 0.580247 test loss: 0.692015
[Epoch 2; Iter     5/   55] train: loss: 0.6881640
[Epoch 2; Iter    35/   55] train: loss: 0.6898606
[Epoch 2] ogbg-molbbbp: 0.534800 val loss: 0.689111
[Epoch 2] ogbg-molbbbp: 0.612751 test loss: 0.689400
[Epoch 3; Iter    10/   55] train: loss: 0.6909226
[Epoch 3; Iter    40/   55] train: loss: 0.6900982
[Epoch 3] ogbg-molbbbp: 0.547247 val loss: 0.689542
[Epoch 3] ogbg-molbbbp: 0.611593 test loss: 0.689476
[Epoch 4; Iter    15/   55] train: loss: 0.6919654
[Epoch 4; Iter    45/   55] train: loss: 0.6914781
[Epoch 4] ogbg-molbbbp: 0.565867 val loss: 0.689242
[Epoch 4] ogbg-molbbbp: 0.619985 test loss: 0.689043
[Epoch 5; Iter    20/   55] train: loss: 0.6895594
[Epoch 5; Iter    50/   55] train: loss: 0.6874671
[Epoch 5] ogbg-molbbbp: 0.563178 val loss: 0.688729
[Epoch 5] ogbg-molbbbp: 0.615162 test loss: 0.688541
[Epoch 6; Iter    25/   55] train: loss: 0.6921746
[Epoch 6; Iter    55/   55] train: loss: 0.6868089
[Epoch 6] ogbg-molbbbp: 0.570248 val loss: 0.688186
[Epoch 6] ogbg-molbbbp: 0.609086 test loss: 0.688710
[Epoch 7; Iter    30/   55] train: loss: 0.6836410
[Epoch 7] ogbg-molbbbp: 0.573833 val loss: 0.688533
[Epoch 7] ogbg-molbbbp: 0.614969 test loss: 0.688077
[Epoch 8; Iter     5/   55] train: loss: 0.6849605
[Epoch 8; Iter    35/   55] train: loss: 0.6833698
[Epoch 8] ogbg-molbbbp: 0.583989 val loss: 0.688367
[Epoch 8] ogbg-molbbbp: 0.610147 test loss: 0.688109
[Epoch 9; Iter    10/   55] train: loss: 0.6804360
[Epoch 9; Iter    40/   55] train: loss: 0.6822786
[Epoch 9] ogbg-molbbbp: 0.604899 val loss: 0.688162
[Epoch 9] ogbg-molbbbp: 0.608893 test loss: 0.687845
[Epoch 10; Iter    15/   55] train: loss: 0.6793002
[Epoch 10; Iter    45/   55] train: loss: 0.6762650
[Epoch 10] ogbg-molbbbp: 0.632580 val loss: 0.687796
[Epoch 10] ogbg-molbbbp: 0.618634 test loss: 0.687476
[Epoch 11; Iter    20/   55] train: loss: 0.6765441
[Epoch 11; Iter    50/   55] train: loss: 0.6755481
[Epoch 11] ogbg-molbbbp: 0.622224 val loss: 0.687245
[Epoch 11] ogbg-molbbbp: 0.609954 test loss: 0.686816
[Epoch 12; Iter    25/   55] train: loss: 0.6766444
[Epoch 12; Iter    55/   55] train: loss: 0.6740215
[Epoch 12] ogbg-molbbbp: 0.643632 val loss: 0.688133
[Epoch 12] ogbg-molbbbp: 0.618538 test loss: 0.686554
[Epoch 13; Iter    30/   55] train: loss: 0.6771414
[Epoch 13] ogbg-molbbbp: 0.843971 val loss: 0.647846
[Epoch 13] ogbg-molbbbp: 0.580536 test loss: 0.683358
[Epoch 14; Iter     5/   55] train: loss: 0.6660397
[Epoch 14; Iter    35/   55] train: loss: 0.6296608
[Epoch 14] ogbg-molbbbp: 0.736931 val loss: 0.704886
[Epoch 14] ogbg-molbbbp: 0.597704 test loss: 0.694076
[Epoch 15; Iter    10/   55] train: loss: 0.6094772
[Epoch 15; Iter    40/   55] train: loss: 0.5492446
[Epoch 15] ogbg-molbbbp: 0.873046 val loss: 0.584610
[Epoch 15] ogbg-molbbbp: 0.639950 test loss: 0.710704
[Epoch 16; Iter    15/   55] train: loss: 0.4848525
[Epoch 16; Iter    45/   55] train: loss: 0.5307896
[Epoch 16] ogbg-molbbbp: 0.895748 val loss: 0.667396
[Epoch 16] ogbg-molbbbp: 0.633584 test loss: 0.837552
[Epoch 17; Iter    20/   55] train: loss: 0.3779451
[Epoch 17; Iter    50/   55] train: loss: 0.4874420
[Epoch 17] ogbg-molbbbp: 0.842378 val loss: 0.685772
[Epoch 17] ogbg-molbbbp: 0.625579 test loss: 0.863895
[Epoch 18; Iter    25/   55] train: loss: 0.3752663
[Epoch 18; Iter    55/   55] train: loss: 0.3979324
[Epoch 18] ogbg-molbbbp: 0.907099 val loss: 0.659666
[Epoch 18] ogbg-molbbbp: 0.623746 test loss: 1.065016
[Epoch 19; Iter    30/   55] train: loss: 0.3422022
[Epoch 19] ogbg-molbbbp: 0.904610 val loss: 0.589195
[Epoch 19] ogbg-molbbbp: 0.619309 test loss: 1.071497
[Epoch 20; Iter     5/   55] train: loss: 0.3969164
[Epoch 20; Iter    35/   55] train: loss: 0.2531857
[Epoch 20] ogbg-molbbbp: 0.795579 val loss: 1.100315
[Epoch 20] ogbg-molbbbp: 0.573592 test loss: 1.238274
[Epoch 21; Iter    10/   55] train: loss: 0.3487087
[Epoch 21; Iter    40/   55] train: loss: 0.5713246
[Epoch 21] ogbg-molbbbp: 0.748581 val loss: 1.340249
[Epoch 21] ogbg-molbbbp: 0.559221 test loss: 1.337397
[Epoch 22; Iter    15/   55] train: loss: 0.4478844
[Epoch 22; Iter    45/   55] train: loss: 0.4395962
[Epoch 22] ogbg-molbbbp: 0.778552 val loss: 1.334613
[Epoch 22] ogbg-molbbbp: 0.566840 test loss: 1.356634
[Epoch 23; Iter    20/   55] train: loss: 0.3934673
[Epoch 23; Iter    50/   55] train: loss: 0.3100637
[Epoch 23] ogbg-molbbbp: 0.858708 val loss: 1.032626
[Epoch 23] ogbg-molbbbp: 0.622685 test loss: 1.407609
[Epoch 24; Iter    25/   55] train: loss: 0.5570210
[Epoch 24; Iter    55/   55] train: loss: 0.1376198
[Epoch 24] ogbg-molbbbp: 0.798666 val loss: 1.060711
[Epoch 24] ogbg-molbbbp: 0.600212 test loss: 1.223068
[Epoch 25; Iter    30/   55] train: loss: 0.2742410
[Epoch 25] ogbg-molbbbp: 0.885791 val loss: 0.826982
[Epoch 25] ogbg-molbbbp: 0.633873 test loss: 1.287791
[Epoch 26; Iter     5/   55] train: loss: 0.3613843
[Epoch 26; Iter    35/   55] train: loss: 0.3222059
[Epoch 26] ogbg-molbbbp: 0.808025 val loss: 1.130750
[Epoch 26] ogbg-molbbbp: 0.604360 test loss: 1.456573
[Epoch 27; Iter    10/   55] train: loss: 0.2433828
[Epoch 27; Iter    40/   55] train: loss: 0.2668371
[Epoch 27] ogbg-molbbbp: 0.893159 val loss: 0.689762
[Epoch 27] ogbg-molbbbp: 0.626157 test loss: 1.457840
[Epoch 28; Iter    15/   55] train: loss: 0.2443752
[Epoch 28; Iter    45/   55] train: loss: 0.1758597
[Epoch 28] ogbg-molbbbp: 0.910286 val loss: 0.434535
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.2/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.2_6_26-05_10-25-56
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.2
logdir: runs/static_noise/GraphCL/bbbp/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6917892
[Epoch 1] ogbg-molbbbp: 0.842975 val loss: 0.689000
[Epoch 1] ogbg-molbbbp: 0.568576 test loss: 0.692029
[Epoch 2; Iter     5/   55] train: loss: 0.6928307
[Epoch 2; Iter    35/   55] train: loss: 0.6912535
[Epoch 2] ogbg-molbbbp: 0.838793 val loss: 0.671558
[Epoch 2] ogbg-molbbbp: 0.597319 test loss: 0.689580
[Epoch 3; Iter    10/   55] train: loss: 0.6942486
[Epoch 3; Iter    40/   55] train: loss: 0.6879482
[Epoch 3] ogbg-molbbbp: 0.833615 val loss: 0.671612
[Epoch 3] ogbg-molbbbp: 0.599537 test loss: 0.689406
[Epoch 4; Iter    15/   55] train: loss: 0.6886060
[Epoch 4; Iter    45/   55] train: loss: 0.6898268
[Epoch 4] ogbg-molbbbp: 0.841083 val loss: 0.670911
[Epoch 4] ogbg-molbbbp: 0.600116 test loss: 0.689274
[Epoch 5; Iter    20/   55] train: loss: 0.6911281
[Epoch 5; Iter    50/   55] train: loss: 0.6867517
[Epoch 5] ogbg-molbbbp: 0.847157 val loss: 0.670410
[Epoch 5] ogbg-molbbbp: 0.601755 test loss: 0.689012
[Epoch 6; Iter    25/   55] train: loss: 0.6932381
[Epoch 6; Iter    55/   55] train: loss: 0.6872560
[Epoch 6] ogbg-molbbbp: 0.834113 val loss: 0.671619
[Epoch 6] ogbg-molbbbp: 0.599730 test loss: 0.688913
[Epoch 7; Iter    30/   55] train: loss: 0.6852895
[Epoch 7] ogbg-molbbbp: 0.843573 val loss: 0.671454
[Epoch 7] ogbg-molbbbp: 0.606289 test loss: 0.688416
[Epoch 8; Iter     5/   55] train: loss: 0.6883017
[Epoch 8; Iter    35/   55] train: loss: 0.6856458
[Epoch 8] ogbg-molbbbp: 0.849945 val loss: 0.670238
[Epoch 8] ogbg-molbbbp: 0.606289 test loss: 0.688237
[Epoch 9; Iter    10/   55] train: loss: 0.6801158
[Epoch 9; Iter    40/   55] train: loss: 0.6797052
[Epoch 9] ogbg-molbbbp: 0.846958 val loss: 0.673465
[Epoch 9] ogbg-molbbbp: 0.598476 test loss: 0.688369
[Epoch 10; Iter    15/   55] train: loss: 0.6798358
[Epoch 10; Iter    45/   55] train: loss: 0.6785706
[Epoch 10] ogbg-molbbbp: 0.851339 val loss: 0.672502
[Epoch 10] ogbg-molbbbp: 0.607157 test loss: 0.687836
[Epoch 11; Iter    20/   55] train: loss: 0.6807302
[Epoch 11; Iter    50/   55] train: loss: 0.6707928
[Epoch 11] ogbg-molbbbp: 0.855720 val loss: 0.671354
[Epoch 11] ogbg-molbbbp: 0.606964 test loss: 0.687405
[Epoch 12; Iter    25/   55] train: loss: 0.6709094
[Epoch 12; Iter    55/   55] train: loss: 0.6833340
[Epoch 12] ogbg-molbbbp: 0.869163 val loss: 0.670347
[Epoch 12] ogbg-molbbbp: 0.608314 test loss: 0.687016
[Epoch 13; Iter    30/   55] train: loss: 0.6729137
[Epoch 13] ogbg-molbbbp: 0.872448 val loss: 0.613870
[Epoch 13] ogbg-molbbbp: 0.571470 test loss: 0.680663
[Epoch 14; Iter     5/   55] train: loss: 0.6373535
[Epoch 14; Iter    35/   55] train: loss: 0.6289967
[Epoch 14] ogbg-molbbbp: 0.892861 val loss: 0.574266
[Epoch 14] ogbg-molbbbp: 0.627411 test loss: 0.678161
[Epoch 15; Iter    10/   55] train: loss: 0.6027058
[Epoch 15; Iter    40/   55] train: loss: 0.5551969
[Epoch 15] ogbg-molbbbp: 0.785124 val loss: 0.625718
[Epoch 15] ogbg-molbbbp: 0.541860 test loss: 0.778033
[Epoch 16; Iter    15/   55] train: loss: 0.5172943
[Epoch 16; Iter    45/   55] train: loss: 0.4940014
[Epoch 16] ogbg-molbbbp: 0.839689 val loss: 0.792785
[Epoch 16] ogbg-molbbbp: 0.633970 test loss: 0.829744
[Epoch 17; Iter    20/   55] train: loss: 0.4877585
[Epoch 17; Iter    50/   55] train: loss: 0.4824556
[Epoch 17] ogbg-molbbbp: 0.906502 val loss: 0.685475
[Epoch 17] ogbg-molbbbp: 0.641204 test loss: 0.884800
[Epoch 18; Iter    25/   55] train: loss: 0.3530910
[Epoch 18; Iter    55/   55] train: loss: 0.2645801
[Epoch 18] ogbg-molbbbp: 0.878024 val loss: 1.066931
[Epoch 18] ogbg-molbbbp: 0.612944 test loss: 1.109642
[Epoch 19; Iter    30/   55] train: loss: 0.2949454
[Epoch 19] ogbg-molbbbp: 0.852136 val loss: 0.928034
[Epoch 19] ogbg-molbbbp: 0.642168 test loss: 0.973945
[Epoch 20; Iter     5/   55] train: loss: 0.3897239
[Epoch 20; Iter    35/   55] train: loss: 0.6153138
[Epoch 20] ogbg-molbbbp: 0.880215 val loss: 0.827713
[Epoch 20] ogbg-molbbbp: 0.655768 test loss: 0.913785
[Epoch 21; Iter    10/   55] train: loss: 0.4861350
[Epoch 21; Iter    40/   55] train: loss: 0.3677172
[Epoch 21] ogbg-molbbbp: 0.812407 val loss: 1.174039
[Epoch 21] ogbg-molbbbp: 0.644387 test loss: 1.129582
[Epoch 22; Iter    15/   55] train: loss: 0.3035010
[Epoch 22; Iter    45/   55] train: loss: 0.3775254
[Epoch 22] ogbg-molbbbp: 0.664642 val loss: 1.578143
[Epoch 22] ogbg-molbbbp: 0.612172 test loss: 1.198133
[Epoch 23; Iter    20/   55] train: loss: 0.4417505
[Epoch 23; Iter    50/   55] train: loss: 0.4396556
[Epoch 23] ogbg-molbbbp: 0.688041 val loss: 1.826200
[Epoch 23] ogbg-molbbbp: 0.597704 test loss: 1.504482
[Epoch 24; Iter    25/   55] train: loss: 0.3518079
[Epoch 24; Iter    55/   55] train: loss: 0.2040696
[Epoch 24] ogbg-molbbbp: 0.674301 val loss: 1.617664
[Epoch 24] ogbg-molbbbp: 0.599826 test loss: 1.186733
[Epoch 25; Iter    30/   55] train: loss: 0.2909158
[Epoch 25] ogbg-molbbbp: 0.782734 val loss: 1.455320
[Epoch 25] ogbg-molbbbp: 0.614969 test loss: 1.347307
[Epoch 26; Iter     5/   55] train: loss: 0.3760141
[Epoch 26; Iter    35/   55] train: loss: 0.2238705
[Epoch 26] ogbg-molbbbp: 0.755750 val loss: 1.117237
[Epoch 26] ogbg-molbbbp: 0.605131 test loss: 1.289897
[Epoch 27; Iter    10/   55] train: loss: 0.2282013
[Epoch 27; Iter    40/   55] train: loss: 0.2752072
[Epoch 27] ogbg-molbbbp: 0.820870 val loss: 1.800936
[Epoch 27] ogbg-molbbbp: 0.623650 test loss: 2.002700
[Epoch 28; Iter    15/   55] train: loss: 0.1827568
[Epoch 28; Iter    45/   55] train: loss: 0.2310576
[Epoch 28] ogbg-molbbbp: 0.922035 val loss: 0.348664
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.05/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.05_4_26-05_10-25-47
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.05
logdir: runs/static_noise/GraphCL/bbbp/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6891577
[Epoch 1] ogbg-molbbbp: 0.870059 val loss: 0.686345
[Epoch 1] ogbg-molbbbp: 0.471451 test loss: 0.693219
[Epoch 2; Iter     5/   55] train: loss: 0.6917908
[Epoch 2; Iter    35/   55] train: loss: 0.6885233
[Epoch 2] ogbg-molbbbp: 0.882704 val loss: 0.676852
[Epoch 2] ogbg-molbbbp: 0.499421 test loss: 0.692447
[Epoch 3; Iter    10/   55] train: loss: 0.6927040
[Epoch 3; Iter    40/   55] train: loss: 0.6900584
[Epoch 3] ogbg-molbbbp: 0.881510 val loss: 0.676568
[Epoch 3] ogbg-molbbbp: 0.500772 test loss: 0.692311
[Epoch 4; Iter    15/   55] train: loss: 0.6904465
[Epoch 4; Iter    45/   55] train: loss: 0.6886389
[Epoch 4] ogbg-molbbbp: 0.882406 val loss: 0.676217
[Epoch 4] ogbg-molbbbp: 0.501447 test loss: 0.692269
[Epoch 5; Iter    20/   55] train: loss: 0.6892403
[Epoch 5; Iter    50/   55] train: loss: 0.6897925
[Epoch 5] ogbg-molbbbp: 0.886090 val loss: 0.676040
[Epoch 5] ogbg-molbbbp: 0.510802 test loss: 0.691866
[Epoch 6; Iter    25/   55] train: loss: 0.6866143
[Epoch 6; Iter    55/   55] train: loss: 0.6846884
[Epoch 6] ogbg-molbbbp: 0.887185 val loss: 0.676001
[Epoch 6] ogbg-molbbbp: 0.509549 test loss: 0.691714
[Epoch 7; Iter    30/   55] train: loss: 0.6826349
[Epoch 7] ogbg-molbbbp: 0.888878 val loss: 0.674848
[Epoch 7] ogbg-molbbbp: 0.513214 test loss: 0.691475
[Epoch 8; Iter     5/   55] train: loss: 0.6845463
[Epoch 8; Iter    35/   55] train: loss: 0.6819319
[Epoch 8] ogbg-molbbbp: 0.885791 val loss: 0.675314
[Epoch 8] ogbg-molbbbp: 0.508488 test loss: 0.691451
[Epoch 9; Iter    10/   55] train: loss: 0.6825230
[Epoch 9; Iter    40/   55] train: loss: 0.6819125
[Epoch 9] ogbg-molbbbp: 0.888778 val loss: 0.674887
[Epoch 9] ogbg-molbbbp: 0.516975 test loss: 0.690972
[Epoch 10; Iter    15/   55] train: loss: 0.6789041
[Epoch 10; Iter    45/   55] train: loss: 0.6756003
[Epoch 10] ogbg-molbbbp: 0.888181 val loss: 0.674915
[Epoch 10] ogbg-molbbbp: 0.515239 test loss: 0.690899
[Epoch 11; Iter    20/   55] train: loss: 0.6749474
[Epoch 11; Iter    50/   55] train: loss: 0.6692601
[Epoch 11] ogbg-molbbbp: 0.891666 val loss: 0.673975
[Epoch 11] ogbg-molbbbp: 0.522087 test loss: 0.690448
[Epoch 12; Iter    25/   55] train: loss: 0.6694480
[Epoch 12; Iter    55/   55] train: loss: 0.6743631
[Epoch 12] ogbg-molbbbp: 0.894354 val loss: 0.673415
[Epoch 12] ogbg-molbbbp: 0.525945 test loss: 0.690105
[Epoch 13; Iter    30/   55] train: loss: 0.6704543
[Epoch 13] ogbg-molbbbp: 0.906701 val loss: 0.624330
[Epoch 13] ogbg-molbbbp: 0.599441 test loss: 0.683198
[Epoch 14; Iter     5/   55] train: loss: 0.6639425
[Epoch 14; Iter    35/   55] train: loss: 0.6039447
[Epoch 14] ogbg-molbbbp: 0.914070 val loss: 0.540302
[Epoch 14] ogbg-molbbbp: 0.647377 test loss: 0.668093
[Epoch 15; Iter    10/   55] train: loss: 0.5623665
[Epoch 15; Iter    40/   55] train: loss: 0.5594541
[Epoch 15] ogbg-molbbbp: 0.881908 val loss: 0.575188
[Epoch 15] ogbg-molbbbp: 0.631076 test loss: 0.722448
[Epoch 16; Iter    15/   55] train: loss: 0.5374964
[Epoch 16; Iter    45/   55] train: loss: 0.4022866
[Epoch 16] ogbg-molbbbp: 0.822065 val loss: 0.666397
[Epoch 16] ogbg-molbbbp: 0.600019 test loss: 0.802546
[Epoch 17; Iter    20/   55] train: loss: 0.3966658
[Epoch 17; Iter    50/   55] train: loss: 0.5110155
[Epoch 17] ogbg-molbbbp: 0.864781 val loss: 0.514692
[Epoch 17] ogbg-molbbbp: 0.626736 test loss: 0.790573
[Epoch 18; Iter    25/   55] train: loss: 0.4196359
[Epoch 18; Iter    55/   55] train: loss: 0.3960750
[Epoch 18] ogbg-molbbbp: 0.834412 val loss: 0.905007
[Epoch 18] ogbg-molbbbp: 0.626447 test loss: 1.075250
[Epoch 19; Iter    30/   55] train: loss: 0.3417813
[Epoch 19] ogbg-molbbbp: 0.856218 val loss: 0.641247
[Epoch 19] ogbg-molbbbp: 0.602238 test loss: 0.914475
[Epoch 20; Iter     5/   55] train: loss: 0.3445456
[Epoch 20; Iter    35/   55] train: loss: 0.2018083
[Epoch 20] ogbg-molbbbp: 0.793787 val loss: 0.867386
[Epoch 20] ogbg-molbbbp: 0.598765 test loss: 0.961822
[Epoch 21; Iter    10/   55] train: loss: 0.4200139
[Epoch 21; Iter    40/   55] train: loss: 0.4188984
[Epoch 21] ogbg-molbbbp: 0.891965 val loss: 0.670697
[Epoch 21] ogbg-molbbbp: 0.640818 test loss: 1.006849
[Epoch 22; Iter    15/   55] train: loss: 0.3188506
[Epoch 22; Iter    45/   55] train: loss: 0.3846705
[Epoch 22] ogbg-molbbbp: 0.858807 val loss: 0.816335
[Epoch 22] ogbg-molbbbp: 0.609761 test loss: 1.085660
[Epoch 23; Iter    20/   55] train: loss: 0.4187481
[Epoch 23; Iter    50/   55] train: loss: 0.2625265
[Epoch 23] ogbg-molbbbp: 0.807926 val loss: 1.006087
[Epoch 23] ogbg-molbbbp: 0.615548 test loss: 1.108940
[Epoch 24; Iter    25/   55] train: loss: 0.2993425
[Epoch 24; Iter    55/   55] train: loss: 0.7570268
[Epoch 24] ogbg-molbbbp: 0.817983 val loss: 0.903114
[Epoch 24] ogbg-molbbbp: 0.612172 test loss: 1.144017
[Epoch 25; Iter    30/   55] train: loss: 0.1649123
[Epoch 25] ogbg-molbbbp: 0.893259 val loss: 0.618744
[Epoch 25] ogbg-molbbbp: 0.650656 test loss: 0.994129
[Epoch 26; Iter     5/   55] train: loss: 0.3812620
[Epoch 26; Iter    35/   55] train: loss: 0.2194408
[Epoch 26] ogbg-molbbbp: 0.943244 val loss: 0.313164
[Epoch 26] ogbg-molbbbp: 0.648245 test loss: 0.902127
[Epoch 27; Iter    10/   55] train: loss: 0.1737912
[Epoch 27; Iter    40/   55] train: loss: 0.1761445
[Epoch 27] ogbg-molbbbp: 0.926416 val loss: 0.370460
[Epoch 27] ogbg-molbbbp: 0.655961 test loss: 1.237394
[Epoch 28; Iter    15/   55] train: loss: 0.1653295
[Epoch 28; Iter    45/   55] train: loss: 0.2008269
[Epoch 28] ogbg-molbbbp: 0.948621 val loss: 0.296031
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.05/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.05_6_26-05_10-25-47
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.05
logdir: runs/static_noise/GraphCL/bbbp/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6902094
[Epoch 1] ogbg-molbbbp: 0.813900 val loss: 0.690551
[Epoch 1] ogbg-molbbbp: 0.547840 test loss: 0.692375
[Epoch 2; Iter     5/   55] train: loss: 0.6926991
[Epoch 2; Iter    35/   55] train: loss: 0.6939240
[Epoch 2] ogbg-molbbbp: 0.815792 val loss: 0.681472
[Epoch 2] ogbg-molbbbp: 0.559124 test loss: 0.690898
[Epoch 3; Iter    10/   55] train: loss: 0.6913353
[Epoch 3; Iter    40/   55] train: loss: 0.6894991
[Epoch 3] ogbg-molbbbp: 0.795579 val loss: 0.682174
[Epoch 3] ogbg-molbbbp: 0.563079 test loss: 0.690727
[Epoch 4; Iter    15/   55] train: loss: 0.6904680
[Epoch 4; Iter    45/   55] train: loss: 0.6905611
[Epoch 4] ogbg-molbbbp: 0.817286 val loss: 0.681222
[Epoch 4] ogbg-molbbbp: 0.559703 test loss: 0.690591
[Epoch 5; Iter    20/   55] train: loss: 0.6890682
[Epoch 5; Iter    50/   55] train: loss: 0.6855667
[Epoch 5] ogbg-molbbbp: 0.814498 val loss: 0.681808
[Epoch 5] ogbg-molbbbp: 0.561439 test loss: 0.690539
[Epoch 6; Iter    25/   55] train: loss: 0.6923037
[Epoch 6; Iter    55/   55] train: loss: 0.6856519
[Epoch 6] ogbg-molbbbp: 0.812905 val loss: 0.681016
[Epoch 6] ogbg-molbbbp: 0.565104 test loss: 0.690183
[Epoch 7; Iter    30/   55] train: loss: 0.6841463
[Epoch 7] ogbg-molbbbp: 0.819775 val loss: 0.681382
[Epoch 7] ogbg-molbbbp: 0.568866 test loss: 0.689850
[Epoch 8; Iter     5/   55] train: loss: 0.6865823
[Epoch 8; Iter    35/   55] train: loss: 0.6857748
[Epoch 8] ogbg-molbbbp: 0.822563 val loss: 0.681852
[Epoch 8] ogbg-molbbbp: 0.564525 test loss: 0.689896
[Epoch 9; Iter    10/   55] train: loss: 0.6796823
[Epoch 9; Iter    40/   55] train: loss: 0.6801189
[Epoch 9] ogbg-molbbbp: 0.832022 val loss: 0.681080
[Epoch 9] ogbg-molbbbp: 0.563947 test loss: 0.689737
[Epoch 10; Iter    15/   55] train: loss: 0.6803261
[Epoch 10; Iter    45/   55] train: loss: 0.6769068
[Epoch 10] ogbg-molbbbp: 0.831126 val loss: 0.681864
[Epoch 10] ogbg-molbbbp: 0.566454 test loss: 0.689517
[Epoch 11; Iter    20/   55] train: loss: 0.6784440
[Epoch 11; Iter    50/   55] train: loss: 0.6712483
[Epoch 11] ogbg-molbbbp: 0.833615 val loss: 0.681347
[Epoch 11] ogbg-molbbbp: 0.571373 test loss: 0.689031
[Epoch 12; Iter    25/   55] train: loss: 0.6707648
[Epoch 12; Iter    55/   55] train: loss: 0.6845390
[Epoch 12] ogbg-molbbbp: 0.846560 val loss: 0.681110
[Epoch 12] ogbg-molbbbp: 0.568673 test loss: 0.688961
[Epoch 13; Iter    30/   55] train: loss: 0.6701369
[Epoch 13] ogbg-molbbbp: 0.900627 val loss: 0.621955
[Epoch 13] ogbg-molbbbp: 0.632427 test loss: 0.677097
[Epoch 14; Iter     5/   55] train: loss: 0.6399703
[Epoch 14; Iter    35/   55] train: loss: 0.6048725
[Epoch 14] ogbg-molbbbp: 0.930897 val loss: 0.522076
[Epoch 14] ogbg-molbbbp: 0.635417 test loss: 0.675053
[Epoch 15; Iter    10/   55] train: loss: 0.5936452
[Epoch 15; Iter    40/   55] train: loss: 0.5202824
[Epoch 15] ogbg-molbbbp: 0.890670 val loss: 0.557107
[Epoch 15] ogbg-molbbbp: 0.606481 test loss: 0.742828
[Epoch 16; Iter    15/   55] train: loss: 0.4708398
[Epoch 16; Iter    45/   55] train: loss: 0.4754073
[Epoch 16] ogbg-molbbbp: 0.872747 val loss: 0.599028
[Epoch 16] ogbg-molbbbp: 0.636960 test loss: 0.796887
[Epoch 17; Iter    20/   55] train: loss: 0.4465964
[Epoch 17; Iter    50/   55] train: loss: 0.4026303
[Epoch 17] ogbg-molbbbp: 0.915563 val loss: 0.444827
[Epoch 17] ogbg-molbbbp: 0.669271 test loss: 0.727320
[Epoch 18; Iter    25/   55] train: loss: 0.2976316
[Epoch 18; Iter    55/   55] train: loss: 0.2716275
[Epoch 18] ogbg-molbbbp: 0.867868 val loss: 0.800644
[Epoch 18] ogbg-molbbbp: 0.603684 test loss: 0.975474
[Epoch 19; Iter    30/   55] train: loss: 0.2888277
[Epoch 19] ogbg-molbbbp: 0.876730 val loss: 0.701811
[Epoch 19] ogbg-molbbbp: 0.642361 test loss: 1.001392
[Epoch 20; Iter     5/   55] train: loss: 0.4157630
[Epoch 20; Iter    35/   55] train: loss: 0.5044191
[Epoch 20] ogbg-molbbbp: 0.849049 val loss: 0.738156
[Epoch 20] ogbg-molbbbp: 0.591628 test loss: 1.096305
[Epoch 21; Iter    10/   55] train: loss: 0.4004534
[Epoch 21; Iter    40/   55] train: loss: 0.2786825
[Epoch 21] ogbg-molbbbp: 0.830927 val loss: 0.968160
[Epoch 21] ogbg-molbbbp: 0.612944 test loss: 1.168668
[Epoch 22; Iter    15/   55] train: loss: 0.2834234
[Epoch 22; Iter    45/   55] train: loss: 0.2989458
[Epoch 22] ogbg-molbbbp: 0.768296 val loss: 1.220015
[Epoch 22] ogbg-molbbbp: 0.603395 test loss: 1.186951
[Epoch 23; Iter    20/   55] train: loss: 0.3657982
[Epoch 23; Iter    50/   55] train: loss: 0.3216262
[Epoch 23] ogbg-molbbbp: 0.818381 val loss: 1.021203
[Epoch 23] ogbg-molbbbp: 0.616705 test loss: 1.104403
[Epoch 24; Iter    25/   55] train: loss: 0.2403771
[Epoch 24; Iter    55/   55] train: loss: 0.1318443
[Epoch 24] ogbg-molbbbp: 0.878821 val loss: 0.785185
[Epoch 24] ogbg-molbbbp: 0.648534 test loss: 1.021878
[Epoch 25; Iter    30/   55] train: loss: 0.2448939
[Epoch 25] ogbg-molbbbp: 0.863885 val loss: 0.916256
[Epoch 25] ogbg-molbbbp: 0.626350 test loss: 1.272223
[Epoch 26; Iter     5/   55] train: loss: 0.3313729
[Epoch 26; Iter    35/   55] train: loss: 0.1920809
[Epoch 26] ogbg-molbbbp: 0.834113 val loss: 0.829785
[Epoch 26] ogbg-molbbbp: 0.601080 test loss: 1.168705
[Epoch 27; Iter    10/   55] train: loss: 0.1698043
[Epoch 27; Iter    40/   55] train: loss: 0.1536167
[Epoch 27] ogbg-molbbbp: 0.863587 val loss: 1.126871
[Epoch 27] ogbg-molbbbp: 0.605517 test loss: 1.747807
[Epoch 28; Iter    15/   55] train: loss: 0.2390449
[Epoch 28; Iter    45/   55] train: loss: 0.2996890
[Epoch 28] ogbg-molbbbp: 0.929603 val loss: 0.405757
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.2/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.2_5_26-05_10-25-56
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.2
logdir: runs/static_noise/GraphCL/bbbp/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6938158
[Epoch 1] ogbg-molbbbp: 0.621727 val loss: 0.691206
[Epoch 1] ogbg-molbbbp: 0.588059 test loss: 0.691799
[Epoch 2; Iter     5/   55] train: loss: 0.6878823
[Epoch 2; Iter    35/   55] train: loss: 0.6918073
[Epoch 2] ogbg-molbbbp: 0.579707 val loss: 0.686727
[Epoch 2] ogbg-molbbbp: 0.616319 test loss: 0.687996
[Epoch 3; Iter    10/   55] train: loss: 0.6928382
[Epoch 3; Iter    40/   55] train: loss: 0.6904259
[Epoch 3] ogbg-molbbbp: 0.591755 val loss: 0.686800
[Epoch 3] ogbg-molbbbp: 0.619888 test loss: 0.688225
[Epoch 4; Iter    15/   55] train: loss: 0.6922019
[Epoch 4; Iter    45/   55] train: loss: 0.6915663
[Epoch 4] ogbg-molbbbp: 0.599920 val loss: 0.686106
[Epoch 4] ogbg-molbbbp: 0.612944 test loss: 0.687296
[Epoch 5; Iter    20/   55] train: loss: 0.6906507
[Epoch 5; Iter    50/   55] train: loss: 0.6911719
[Epoch 5] ogbg-molbbbp: 0.600717 val loss: 0.686919
[Epoch 5] ogbg-molbbbp: 0.613812 test loss: 0.687357
[Epoch 6; Iter    25/   55] train: loss: 0.6903888
[Epoch 6; Iter    55/   55] train: loss: 0.6868245
[Epoch 6] ogbg-molbbbp: 0.598725 val loss: 0.684655
[Epoch 6] ogbg-molbbbp: 0.610436 test loss: 0.687107
[Epoch 7; Iter    30/   55] train: loss: 0.6858775
[Epoch 7] ogbg-molbbbp: 0.611172 val loss: 0.683841
[Epoch 7] ogbg-molbbbp: 0.604456 test loss: 0.686770
[Epoch 8; Iter     5/   55] train: loss: 0.6827667
[Epoch 8; Iter    35/   55] train: loss: 0.6831116
[Epoch 8] ogbg-molbbbp: 0.627502 val loss: 0.684920
[Epoch 8] ogbg-molbbbp: 0.602431 test loss: 0.687025
[Epoch 9; Iter    10/   55] train: loss: 0.6845068
[Epoch 9; Iter    40/   55] train: loss: 0.6844888
[Epoch 9] ogbg-molbbbp: 0.627900 val loss: 0.683678
[Epoch 9] ogbg-molbbbp: 0.606385 test loss: 0.686379
[Epoch 10; Iter    15/   55] train: loss: 0.6791140
[Epoch 10; Iter    45/   55] train: loss: 0.6804159
[Epoch 10] ogbg-molbbbp: 0.635866 val loss: 0.684848
[Epoch 10] ogbg-molbbbp: 0.605999 test loss: 0.686408
[Epoch 11; Iter    20/   55] train: loss: 0.6775075
[Epoch 11; Iter    50/   55] train: loss: 0.6772868
[Epoch 11] ogbg-molbbbp: 0.642736 val loss: 0.682436
[Epoch 11] ogbg-molbbbp: 0.599730 test loss: 0.685419
[Epoch 12; Iter    25/   55] train: loss: 0.6764379
[Epoch 12; Iter    55/   55] train: loss: 0.6736681
[Epoch 12] ogbg-molbbbp: 0.652693 val loss: 0.683283
[Epoch 12] ogbg-molbbbp: 0.610822 test loss: 0.685193
[Epoch 13; Iter    30/   55] train: loss: 0.6780626
[Epoch 13] ogbg-molbbbp: 0.820970 val loss: 0.650741
[Epoch 13] ogbg-molbbbp: 0.575231 test loss: 0.682098
[Epoch 14; Iter     5/   55] train: loss: 0.6644443
[Epoch 14; Iter    35/   55] train: loss: 0.6193255
[Epoch 14] ogbg-molbbbp: 0.744598 val loss: 0.762373
[Epoch 14] ogbg-molbbbp: 0.580054 test loss: 0.734210
[Epoch 15; Iter    10/   55] train: loss: 0.6183104
[Epoch 15; Iter    40/   55] train: loss: 0.5502476
[Epoch 15] ogbg-molbbbp: 0.707956 val loss: 0.954452
[Epoch 15] ogbg-molbbbp: 0.580247 test loss: 0.864246
[Epoch 16; Iter    15/   55] train: loss: 0.4781447
[Epoch 16; Iter    45/   55] train: loss: 0.5379550
[Epoch 16] ogbg-molbbbp: 0.780046 val loss: 0.970358
[Epoch 16] ogbg-molbbbp: 0.574942 test loss: 0.984766
[Epoch 17; Iter    20/   55] train: loss: 0.4168448
[Epoch 17; Iter    50/   55] train: loss: 0.4893255
[Epoch 17] ogbg-molbbbp: 0.794982 val loss: 0.882056
[Epoch 17] ogbg-molbbbp: 0.589217 test loss: 0.994598
[Epoch 18; Iter    25/   55] train: loss: 0.4338681
[Epoch 18; Iter    55/   55] train: loss: 0.3818581
[Epoch 18] ogbg-molbbbp: 0.840187 val loss: 1.074959
[Epoch 18] ogbg-molbbbp: 0.582369 test loss: 1.298196
[Epoch 19; Iter    30/   55] train: loss: 0.3796948
[Epoch 19] ogbg-molbbbp: 0.801653 val loss: 1.304964
[Epoch 19] ogbg-molbbbp: 0.567612 test loss: 1.403297
[Epoch 20; Iter     5/   55] train: loss: 0.3789654
[Epoch 20; Iter    35/   55] train: loss: 0.2702535
[Epoch 20] ogbg-molbbbp: 0.621627 val loss: 2.600169
[Epoch 20] ogbg-molbbbp: 0.544560 test loss: 2.017052
[Epoch 21; Iter    10/   55] train: loss: 0.3416077
[Epoch 21; Iter    40/   55] train: loss: 0.5661503
[Epoch 21] ogbg-molbbbp: 0.651399 val loss: 1.991686
[Epoch 21] ogbg-molbbbp: 0.556327 test loss: 1.623704
[Epoch 22; Iter    15/   55] train: loss: 0.4459860
[Epoch 22; Iter    45/   55] train: loss: 0.4723334
[Epoch 22] ogbg-molbbbp: 0.757742 val loss: 1.546416
[Epoch 22] ogbg-molbbbp: 0.587288 test loss: 1.448091
[Epoch 23; Iter    20/   55] train: loss: 0.4202686
[Epoch 23; Iter    50/   55] train: loss: 0.2854284
[Epoch 23] ogbg-molbbbp: 0.790999 val loss: 1.428872
[Epoch 23] ogbg-molbbbp: 0.589988 test loss: 1.538107
[Epoch 24; Iter    25/   55] train: loss: 0.6506358
[Epoch 24; Iter    55/   55] train: loss: 0.1565719
[Epoch 24] ogbg-molbbbp: 0.787713 val loss: 1.204193
[Epoch 24] ogbg-molbbbp: 0.622396 test loss: 1.223411
[Epoch 25; Iter    30/   55] train: loss: 0.3011485
[Epoch 25] ogbg-molbbbp: 0.800060 val loss: 1.247201
[Epoch 25] ogbg-molbbbp: 0.619213 test loss: 1.356882
[Epoch 26; Iter     5/   55] train: loss: 0.3971503
[Epoch 26; Iter    35/   55] train: loss: 0.3798014
[Epoch 26] ogbg-molbbbp: 0.799363 val loss: 1.275457
[Epoch 26] ogbg-molbbbp: 0.595100 test loss: 1.323011
[Epoch 27; Iter    10/   55] train: loss: 0.2514922
[Epoch 27; Iter    40/   55] train: loss: 0.3207510
[Epoch 27] ogbg-molbbbp: 0.930399 val loss: 0.604425
[Epoch 27] ogbg-molbbbp: 0.659144 test loss: 0.885961
[Epoch 28; Iter    15/   55] train: loss: 0.2223443
[Epoch 28; Iter    45/   55] train: loss: 0.1889529
[Epoch 28] ogbg-molbbbp: 0.806333 val loss: 0.774799
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.1/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.1_4_26-05_10-25-52
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.1
logdir: runs/static_noise/GraphCL/bbbp/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6901681
[Epoch 1] ogbg-molbbbp: 0.874042 val loss: 0.685810
[Epoch 1] ogbg-molbbbp: 0.480613 test loss: 0.693233
[Epoch 2; Iter     5/   55] train: loss: 0.6920296
[Epoch 2; Iter    35/   55] train: loss: 0.6895506
[Epoch 2] ogbg-molbbbp: 0.875037 val loss: 0.676088
[Epoch 2] ogbg-molbbbp: 0.505401 test loss: 0.692605
[Epoch 3; Iter    10/   55] train: loss: 0.6925187
[Epoch 3; Iter    40/   55] train: loss: 0.6902815
[Epoch 3] ogbg-molbbbp: 0.863188 val loss: 0.676116
[Epoch 3] ogbg-molbbbp: 0.508873 test loss: 0.692535
[Epoch 4; Iter    15/   55] train: loss: 0.6913759
[Epoch 4; Iter    45/   55] train: loss: 0.6887745
[Epoch 4] ogbg-molbbbp: 0.867769 val loss: 0.675762
[Epoch 4] ogbg-molbbbp: 0.508295 test loss: 0.692399
[Epoch 5; Iter    20/   55] train: loss: 0.6910183
[Epoch 5; Iter    50/   55] train: loss: 0.6892877
[Epoch 5] ogbg-molbbbp: 0.873842 val loss: 0.674957
[Epoch 5] ogbg-molbbbp: 0.515625 test loss: 0.691991
[Epoch 6; Iter    25/   55] train: loss: 0.6863996
[Epoch 6; Iter    55/   55] train: loss: 0.6891694
[Epoch 6] ogbg-molbbbp: 0.873444 val loss: 0.675088
[Epoch 6] ogbg-molbbbp: 0.515336 test loss: 0.691895
[Epoch 7; Iter    30/   55] train: loss: 0.6813322
[Epoch 7] ogbg-molbbbp: 0.877327 val loss: 0.674292
[Epoch 7] ogbg-molbbbp: 0.514853 test loss: 0.691652
[Epoch 8; Iter     5/   55] train: loss: 0.6852140
[Epoch 8; Iter    35/   55] train: loss: 0.6846849
[Epoch 8] ogbg-molbbbp: 0.880414 val loss: 0.673956
[Epoch 8] ogbg-molbbbp: 0.513985 test loss: 0.691487
[Epoch 9; Iter    10/   55] train: loss: 0.6808511
[Epoch 9; Iter    40/   55] train: loss: 0.6827353
[Epoch 9] ogbg-molbbbp: 0.879717 val loss: 0.674315
[Epoch 9] ogbg-molbbbp: 0.518326 test loss: 0.691212
[Epoch 10; Iter    15/   55] train: loss: 0.6791834
[Epoch 10; Iter    45/   55] train: loss: 0.6755890
[Epoch 10] ogbg-molbbbp: 0.877626 val loss: 0.673906
[Epoch 10] ogbg-molbbbp: 0.517650 test loss: 0.691089
[Epoch 11; Iter    20/   55] train: loss: 0.6756552
[Epoch 11; Iter    50/   55] train: loss: 0.6691246
[Epoch 11] ogbg-molbbbp: 0.884397 val loss: 0.673408
[Epoch 11] ogbg-molbbbp: 0.523148 test loss: 0.690607
[Epoch 12; Iter    25/   55] train: loss: 0.6703986
[Epoch 12; Iter    55/   55] train: loss: 0.6773819
[Epoch 12] ogbg-molbbbp: 0.887982 val loss: 0.672445
[Epoch 12] ogbg-molbbbp: 0.527103 test loss: 0.690249
[Epoch 13; Iter    30/   55] train: loss: 0.6706350
[Epoch 13] ogbg-molbbbp: 0.904809 val loss: 0.622680
[Epoch 13] ogbg-molbbbp: 0.594039 test loss: 0.682683
[Epoch 14; Iter     5/   55] train: loss: 0.6628773
[Epoch 14; Iter    35/   55] train: loss: 0.6081064
[Epoch 14] ogbg-molbbbp: 0.876730 val loss: 0.574115
[Epoch 14] ogbg-molbbbp: 0.617573 test loss: 0.675692
[Epoch 15; Iter    10/   55] train: loss: 0.5892197
[Epoch 15; Iter    40/   55] train: loss: 0.6050494
[Epoch 15] ogbg-molbbbp: 0.902917 val loss: 0.558516
[Epoch 15] ogbg-molbbbp: 0.654032 test loss: 0.675090
[Epoch 16; Iter    15/   55] train: loss: 0.5427656
[Epoch 16; Iter    45/   55] train: loss: 0.4096480
[Epoch 16] ogbg-molbbbp: 0.838196 val loss: 0.649645
[Epoch 16] ogbg-molbbbp: 0.613233 test loss: 0.792852
[Epoch 17; Iter    20/   55] train: loss: 0.3959449
[Epoch 17; Iter    50/   55] train: loss: 0.5041751
[Epoch 17] ogbg-molbbbp: 0.778751 val loss: 0.669508
[Epoch 17] ogbg-molbbbp: 0.601948 test loss: 0.762996
[Epoch 18; Iter    25/   55] train: loss: 0.4125219
[Epoch 18; Iter    55/   55] train: loss: 0.3830693
[Epoch 18] ogbg-molbbbp: 0.700388 val loss: 1.144525
[Epoch 18] ogbg-molbbbp: 0.585069 test loss: 1.017047
[Epoch 19; Iter    30/   55] train: loss: 0.3596254
[Epoch 19] ogbg-molbbbp: 0.805835 val loss: 0.690155
[Epoch 19] ogbg-molbbbp: 0.596451 test loss: 0.838284
[Epoch 20; Iter     5/   55] train: loss: 0.3576958
[Epoch 20; Iter    35/   55] train: loss: 0.2102850
[Epoch 20] ogbg-molbbbp: 0.763218 val loss: 0.877278
[Epoch 20] ogbg-molbbbp: 0.629630 test loss: 0.868380
[Epoch 21; Iter    10/   55] train: loss: 0.4579087
[Epoch 21; Iter    40/   55] train: loss: 0.3799295
[Epoch 21] ogbg-molbbbp: 0.845962 val loss: 0.601214
[Epoch 21] ogbg-molbbbp: 0.644869 test loss: 0.830242
[Epoch 22; Iter    15/   55] train: loss: 0.3748591
[Epoch 22; Iter    45/   55] train: loss: 0.4336425
[Epoch 22] ogbg-molbbbp: 0.800259 val loss: 1.009785
[Epoch 22] ogbg-molbbbp: 0.639660 test loss: 1.084289
[Epoch 23; Iter    20/   55] train: loss: 0.3827222
[Epoch 23; Iter    50/   55] train: loss: 0.2500631
[Epoch 23] ogbg-molbbbp: 0.780842 val loss: 0.972657
[Epoch 23] ogbg-molbbbp: 0.637924 test loss: 1.003689
[Epoch 24; Iter    25/   55] train: loss: 0.2914230
[Epoch 24; Iter    55/   55] train: loss: 0.8875944
[Epoch 24] ogbg-molbbbp: 0.812705 val loss: 0.801044
[Epoch 24] ogbg-molbbbp: 0.627508 test loss: 0.951385
[Epoch 25; Iter    30/   55] train: loss: 0.1778300
[Epoch 25] ogbg-molbbbp: 0.860998 val loss: 0.595507
[Epoch 25] ogbg-molbbbp: 0.662133 test loss: 0.818019
[Epoch 26; Iter     5/   55] train: loss: 0.3473245
[Epoch 26; Iter    35/   55] train: loss: 0.2435914
[Epoch 26] ogbg-molbbbp: 0.902818 val loss: 0.394579
[Epoch 26] ogbg-molbbbp: 0.651910 test loss: 1.327405
[Epoch 27; Iter    10/   55] train: loss: 0.1402191
[Epoch 27; Iter    40/   55] train: loss: 0.1452046
[Epoch 27] ogbg-molbbbp: 0.883003 val loss: 0.484932
[Epoch 27] ogbg-molbbbp: 0.680941 test loss: 1.183725
[Epoch 28; Iter    15/   55] train: loss: 0.1590713
[Epoch 28; Iter    45/   55] train: loss: 0.3828180
[Epoch 28] ogbg-molbbbp: 0.938763 val loss: 0.324150
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.0/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.0_4_26-05_10-25-12
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.0
logdir: runs/static_noise/GraphCL/bbbp/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6884762
[Epoch 1] ogbg-molbbbp: 0.786618 val loss: 0.688927
[Epoch 1] ogbg-molbbbp: 0.451003 test loss: 0.693395
[Epoch 2; Iter     5/   55] train: loss: 0.6923946
[Epoch 2; Iter    35/   55] train: loss: 0.6876841
[Epoch 2] ogbg-molbbbp: 0.837997 val loss: 0.682949
[Epoch 2] ogbg-molbbbp: 0.473187 test loss: 0.692958
[Epoch 3; Iter    10/   55] train: loss: 0.6927744
[Epoch 3; Iter    40/   55] train: loss: 0.6897194
[Epoch 3] ogbg-molbbbp: 0.844867 val loss: 0.682070
[Epoch 3] ogbg-molbbbp: 0.473283 test loss: 0.692994
[Epoch 4; Iter    15/   55] train: loss: 0.6907960
[Epoch 4; Iter    45/   55] train: loss: 0.6890080
[Epoch 4] ogbg-molbbbp: 0.845962 val loss: 0.682295
[Epoch 4] ogbg-molbbbp: 0.474151 test loss: 0.692845
[Epoch 5; Iter    20/   55] train: loss: 0.6906404
[Epoch 5; Iter    50/   55] train: loss: 0.6897183
[Epoch 5] ogbg-molbbbp: 0.859703 val loss: 0.681333
[Epoch 5] ogbg-molbbbp: 0.485822 test loss: 0.692551
[Epoch 6; Iter    25/   55] train: loss: 0.6867520
[Epoch 6; Iter    55/   55] train: loss: 0.6828100
[Epoch 6] ogbg-molbbbp: 0.858409 val loss: 0.681337
[Epoch 6] ogbg-molbbbp: 0.485050 test loss: 0.692353
[Epoch 7; Iter    30/   55] train: loss: 0.6822934
[Epoch 7] ogbg-molbbbp: 0.859803 val loss: 0.681038
[Epoch 7] ogbg-molbbbp: 0.483989 test loss: 0.692293
[Epoch 8; Iter     5/   55] train: loss: 0.6849414
[Epoch 8; Iter    35/   55] train: loss: 0.6830447
[Epoch 8] ogbg-molbbbp: 0.859703 val loss: 0.681340
[Epoch 8] ogbg-molbbbp: 0.485243 test loss: 0.692142
[Epoch 9; Iter    10/   55] train: loss: 0.6818204
[Epoch 9; Iter    40/   55] train: loss: 0.6791931
[Epoch 9] ogbg-molbbbp: 0.873743 val loss: 0.680515
[Epoch 9] ogbg-molbbbp: 0.495949 test loss: 0.691665
[Epoch 10; Iter    15/   55] train: loss: 0.6797972
[Epoch 10; Iter    45/   55] train: loss: 0.6759865
[Epoch 10] ogbg-molbbbp: 0.867171 val loss: 0.680906
[Epoch 10] ogbg-molbbbp: 0.488619 test loss: 0.691683
[Epoch 11; Iter    20/   55] train: loss: 0.6755283
[Epoch 11; Iter    50/   55] train: loss: 0.6691516
[Epoch 11] ogbg-molbbbp: 0.875734 val loss: 0.679810
[Epoch 11] ogbg-molbbbp: 0.500579 test loss: 0.691127
[Epoch 12; Iter    25/   55] train: loss: 0.6697974
[Epoch 12; Iter    55/   55] train: loss: 0.6743339
[Epoch 12] ogbg-molbbbp: 0.875834 val loss: 0.679601
[Epoch 12] ogbg-molbbbp: 0.503569 test loss: 0.690819
[Epoch 13; Iter    30/   55] train: loss: 0.6696041
[Epoch 13] ogbg-molbbbp: 0.921040 val loss: 0.619850
[Epoch 13] ogbg-molbbbp: 0.634742 test loss: 0.679863
[Epoch 14; Iter     5/   55] train: loss: 0.6582856
[Epoch 14; Iter    35/   55] train: loss: 0.6013725
[Epoch 14] ogbg-molbbbp: 0.945335 val loss: 0.447476
[Epoch 14] ogbg-molbbbp: 0.661555 test loss: 0.671061
[Epoch 15; Iter    10/   55] train: loss: 0.5356821
[Epoch 15; Iter    40/   55] train: loss: 0.5460823
[Epoch 15] ogbg-molbbbp: 0.932490 val loss: 0.448589
[Epoch 15] ogbg-molbbbp: 0.644001 test loss: 0.711707
[Epoch 16; Iter    15/   55] train: loss: 0.5255836
[Epoch 16; Iter    45/   55] train: loss: 0.3604421
[Epoch 16] ogbg-molbbbp: 0.928010 val loss: 0.423684
[Epoch 16] ogbg-molbbbp: 0.667149 test loss: 0.740127
[Epoch 17; Iter    20/   55] train: loss: 0.3566412
[Epoch 17; Iter    50/   55] train: loss: 0.5332732
[Epoch 17] ogbg-molbbbp: 0.940854 val loss: 0.351445
[Epoch 17] ogbg-molbbbp: 0.665799 test loss: 0.837979
[Epoch 18; Iter    25/   55] train: loss: 0.3472274
[Epoch 18; Iter    55/   55] train: loss: 0.3417291
[Epoch 18] ogbg-molbbbp: 0.944339 val loss: 0.379286
[Epoch 18] ogbg-molbbbp: 0.683931 test loss: 0.876071
[Epoch 19; Iter    30/   55] train: loss: 0.3280640
[Epoch 19] ogbg-molbbbp: 0.937270 val loss: 0.351573
[Epoch 19] ogbg-molbbbp: 0.654417 test loss: 0.838074
[Epoch 20; Iter     5/   55] train: loss: 0.2684816
[Epoch 20; Iter    35/   55] train: loss: 0.1711666
[Epoch 20] ogbg-molbbbp: 0.928507 val loss: 0.421668
[Epoch 20] ogbg-molbbbp: 0.682099 test loss: 0.783989
[Epoch 21; Iter    10/   55] train: loss: 0.3836500
[Epoch 21; Iter    40/   55] train: loss: 0.4311933
[Epoch 21] ogbg-molbbbp: 0.950513 val loss: 0.328312
[Epoch 21] ogbg-molbbbp: 0.663098 test loss: 0.889632
[Epoch 22; Iter    15/   55] train: loss: 0.2950646
[Epoch 22; Iter    45/   55] train: loss: 0.3558119
[Epoch 22] ogbg-molbbbp: 0.946231 val loss: 0.370667
[Epoch 22] ogbg-molbbbp: 0.685378 test loss: 0.947215
[Epoch 23; Iter    20/   55] train: loss: 0.4546368
[Epoch 23; Iter    50/   55] train: loss: 0.2135662
[Epoch 23] ogbg-molbbbp: 0.946530 val loss: 0.411343
[Epoch 23] ogbg-molbbbp: 0.679977 test loss: 1.003616
[Epoch 24; Iter    25/   55] train: loss: 0.2922603
[Epoch 24; Iter    55/   55] train: loss: 0.8133190
[Epoch 24] ogbg-molbbbp: 0.946629 val loss: 0.395135
[Epoch 24] ogbg-molbbbp: 0.677855 test loss: 0.911598
[Epoch 25; Iter    30/   55] train: loss: 0.1332224
[Epoch 25] ogbg-molbbbp: 0.956188 val loss: 0.308469
[Epoch 25] ogbg-molbbbp: 0.704090 test loss: 0.804687
[Epoch 26; Iter     5/   55] train: loss: 0.3537517
[Epoch 26; Iter    35/   55] train: loss: 0.1261112
[Epoch 26] ogbg-molbbbp: 0.929105 val loss: 0.380102
[Epoch 26] ogbg-molbbbp: 0.666667 test loss: 0.860764
[Epoch 27; Iter    10/   55] train: loss: 0.1517573
[Epoch 27; Iter    40/   55] train: loss: 0.1663667
[Epoch 27] ogbg-molbbbp: 0.967141 val loss: 0.247041
[Epoch 27] ogbg-molbbbp: 0.729745 test loss: 0.773834
[Epoch 28; Iter    15/   55] train: loss: 0.1882667
[Epoch 28; Iter    45/   55] train: loss: 0.2517363
[Epoch 28] ogbg-molbbbp: 0.918052 val loss: 0.887085
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.2/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.2_4_26-05_10-25-54
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.2
logdir: runs/static_noise/GraphCL/bbbp/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6885057
[Epoch 1] ogbg-molbbbp: 0.858907 val loss: 0.686140
[Epoch 1] ogbg-molbbbp: 0.486304 test loss: 0.693349
[Epoch 2; Iter     5/   55] train: loss: 0.6932846
[Epoch 2; Iter    35/   55] train: loss: 0.6873041
[Epoch 2] ogbg-molbbbp: 0.831425 val loss: 0.677790
[Epoch 2] ogbg-molbbbp: 0.498071 test loss: 0.693190
[Epoch 3; Iter    10/   55] train: loss: 0.6924118
[Epoch 3; Iter    40/   55] train: loss: 0.6896097
[Epoch 3] ogbg-molbbbp: 0.831724 val loss: 0.676284
[Epoch 3] ogbg-molbbbp: 0.510610 test loss: 0.693143
[Epoch 4; Iter    15/   55] train: loss: 0.6939109
[Epoch 4; Iter    45/   55] train: loss: 0.6901758
[Epoch 4] ogbg-molbbbp: 0.834312 val loss: 0.676601
[Epoch 4] ogbg-molbbbp: 0.508681 test loss: 0.692924
[Epoch 5; Iter    20/   55] train: loss: 0.6911784
[Epoch 5; Iter    50/   55] train: loss: 0.6898376
[Epoch 5] ogbg-molbbbp: 0.836403 val loss: 0.676412
[Epoch 5] ogbg-molbbbp: 0.515625 test loss: 0.692603
[Epoch 6; Iter    25/   55] train: loss: 0.6868811
[Epoch 6; Iter    55/   55] train: loss: 0.6870057
[Epoch 6] ogbg-molbbbp: 0.837897 val loss: 0.676035
[Epoch 6] ogbg-molbbbp: 0.516204 test loss: 0.692484
[Epoch 7; Iter    30/   55] train: loss: 0.6843130
[Epoch 7] ogbg-molbbbp: 0.836105 val loss: 0.675514
[Epoch 7] ogbg-molbbbp: 0.512249 test loss: 0.692330
[Epoch 8; Iter     5/   55] train: loss: 0.6835297
[Epoch 8; Iter    35/   55] train: loss: 0.6838455
[Epoch 8] ogbg-molbbbp: 0.844170 val loss: 0.674840
[Epoch 8] ogbg-molbbbp: 0.519290 test loss: 0.691978
[Epoch 9; Iter    10/   55] train: loss: 0.6823271
[Epoch 9; Iter    40/   55] train: loss: 0.6828788
[Epoch 9] ogbg-molbbbp: 0.840287 val loss: 0.675416
[Epoch 9] ogbg-molbbbp: 0.520448 test loss: 0.691750
[Epoch 10; Iter    15/   55] train: loss: 0.6780155
[Epoch 10; Iter    45/   55] train: loss: 0.6762869
[Epoch 10] ogbg-molbbbp: 0.846460 val loss: 0.675287
[Epoch 10] ogbg-molbbbp: 0.519772 test loss: 0.691534
[Epoch 11; Iter    20/   55] train: loss: 0.6751566
[Epoch 11; Iter    50/   55] train: loss: 0.6714705
[Epoch 11] ogbg-molbbbp: 0.848352 val loss: 0.675002
[Epoch 11] ogbg-molbbbp: 0.522280 test loss: 0.691185
[Epoch 12; Iter    25/   55] train: loss: 0.6720241
[Epoch 12; Iter    55/   55] train: loss: 0.6749502
[Epoch 12] ogbg-molbbbp: 0.855920 val loss: 0.673897
[Epoch 12] ogbg-molbbbp: 0.527296 test loss: 0.690609
[Epoch 13; Iter    30/   55] train: loss: 0.6682013
[Epoch 13] ogbg-molbbbp: 0.897939 val loss: 0.623092
[Epoch 13] ogbg-molbbbp: 0.589217 test loss: 0.683473
[Epoch 14; Iter     5/   55] train: loss: 0.6648685
[Epoch 14; Iter    35/   55] train: loss: 0.6131932
[Epoch 14] ogbg-molbbbp: 0.818879 val loss: 0.617347
[Epoch 14] ogbg-molbbbp: 0.609086 test loss: 0.686760
[Epoch 15; Iter    10/   55] train: loss: 0.5952711
[Epoch 15; Iter    40/   55] train: loss: 0.6193618
[Epoch 15] ogbg-molbbbp: 0.903913 val loss: 0.518817
[Epoch 15] ogbg-molbbbp: 0.638985 test loss: 0.710552
[Epoch 16; Iter    15/   55] train: loss: 0.5899194
[Epoch 16; Iter    45/   55] train: loss: 0.4377989
[Epoch 16] ogbg-molbbbp: 0.901822 val loss: 0.564553
[Epoch 16] ogbg-molbbbp: 0.635224 test loss: 0.814565
[Epoch 17; Iter    20/   55] train: loss: 0.3977526
[Epoch 17; Iter    50/   55] train: loss: 0.5236312
[Epoch 17] ogbg-molbbbp: 0.842079 val loss: 0.729696
[Epoch 17] ogbg-molbbbp: 0.626061 test loss: 0.922475
[Epoch 18; Iter    25/   55] train: loss: 0.4413509
[Epoch 18; Iter    55/   55] train: loss: 0.3994760
[Epoch 18] ogbg-molbbbp: 0.833018 val loss: 0.842334
[Epoch 18] ogbg-molbbbp: 0.642168 test loss: 1.017198
[Epoch 19; Iter    30/   55] train: loss: 0.3706592
[Epoch 19] ogbg-molbbbp: 0.878024 val loss: 0.688131
[Epoch 19] ogbg-molbbbp: 0.629051 test loss: 1.073762
[Epoch 20; Iter     5/   55] train: loss: 0.4018584
[Epoch 20; Iter    35/   55] train: loss: 0.2380736
[Epoch 20] ogbg-molbbbp: 0.688241 val loss: 1.701569
[Epoch 20] ogbg-molbbbp: 0.607639 test loss: 1.458626
[Epoch 21; Iter    10/   55] train: loss: 0.4541527
[Epoch 21; Iter    40/   55] train: loss: 0.3790706
[Epoch 21] ogbg-molbbbp: 0.904610 val loss: 0.731450
[Epoch 21] ogbg-molbbbp: 0.629630 test loss: 1.189204
[Epoch 22; Iter    15/   55] train: loss: 0.3885176
[Epoch 22; Iter    45/   55] train: loss: 0.4107610
[Epoch 22] ogbg-molbbbp: 0.801952 val loss: 1.537798
[Epoch 22] ogbg-molbbbp: 0.625096 test loss: 1.632362
[Epoch 23; Iter    20/   55] train: loss: 0.3759153
[Epoch 23; Iter    50/   55] train: loss: 0.3066780
[Epoch 23] ogbg-molbbbp: 0.742706 val loss: 1.541007
[Epoch 23] ogbg-molbbbp: 0.608700 test loss: 1.456773
[Epoch 24; Iter    25/   55] train: loss: 0.2366860
[Epoch 24; Iter    55/   55] train: loss: 0.9374267
[Epoch 24] ogbg-molbbbp: 0.692323 val loss: 3.580203
[Epoch 24] ogbg-molbbbp: 0.568191 test loss: 2.075684
[Epoch 25; Iter    30/   55] train: loss: 0.1782305
[Epoch 25] ogbg-molbbbp: 0.688041 val loss: 6.628296
[Epoch 25] ogbg-molbbbp: 0.542921 test loss: 3.500613
[Epoch 26; Iter     5/   55] train: loss: 0.4546405
[Epoch 26; Iter    35/   55] train: loss: 0.2464386
[Epoch 26] ogbg-molbbbp: 0.860400 val loss: 0.655140
[Epoch 26] ogbg-molbbbp: 0.656443 test loss: 1.274023
[Epoch 27; Iter    10/   55] train: loss: 0.2709891
[Epoch 27; Iter    40/   55] train: loss: 0.1694193
[Epoch 27] ogbg-molbbbp: 0.890172 val loss: 1.634266
[Epoch 27] ogbg-molbbbp: 0.712770 test loss: 4.819005
[Epoch 28; Iter    15/   55] train: loss: 0.2069329
[Epoch 28; Iter    45/   55] train: loss: 0.4053018
[Epoch 28] ogbg-molbbbp: 0.917256 val loss: 0.755235
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.0/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.0_5_26-05_10-25-12
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.0
logdir: runs/static_noise/GraphCL/bbbp/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6953800
[Epoch 1] ogbg-molbbbp: 0.681968 val loss: 0.690484
[Epoch 1] ogbg-molbbbp: 0.504919 test loss: 0.692650
[Epoch 2; Iter     5/   55] train: loss: 0.6891797
[Epoch 2; Iter    35/   55] train: loss: 0.6913388
[Epoch 2] ogbg-molbbbp: 0.659863 val loss: 0.689653
[Epoch 2] ogbg-molbbbp: 0.511285 test loss: 0.692963
[Epoch 3; Iter    10/   55] train: loss: 0.6917677
[Epoch 3; Iter    40/   55] train: loss: 0.6895813
[Epoch 3] ogbg-molbbbp: 0.662750 val loss: 0.689699
[Epoch 3] ogbg-molbbbp: 0.513792 test loss: 0.692928
[Epoch 4; Iter    15/   55] train: loss: 0.6932127
[Epoch 4; Iter    45/   55] train: loss: 0.6917215
[Epoch 4] ogbg-molbbbp: 0.668724 val loss: 0.689766
[Epoch 4] ogbg-molbbbp: 0.507620 test loss: 0.692964
[Epoch 5; Iter    20/   55] train: loss: 0.6900438
[Epoch 5; Iter    50/   55] train: loss: 0.6886819
[Epoch 5] ogbg-molbbbp: 0.676392 val loss: 0.689756
[Epoch 5] ogbg-molbbbp: 0.524306 test loss: 0.692562
[Epoch 6; Iter    25/   55] train: loss: 0.6907805
[Epoch 6; Iter    55/   55] train: loss: 0.6868229
[Epoch 6] ogbg-molbbbp: 0.688440 val loss: 0.689352
[Epoch 6] ogbg-molbbbp: 0.515625 test loss: 0.692586
[Epoch 7; Iter    30/   55] train: loss: 0.6843200
[Epoch 7] ogbg-molbbbp: 0.687743 val loss: 0.689706
[Epoch 7] ogbg-molbbbp: 0.520158 test loss: 0.692286
[Epoch 8; Iter     5/   55] train: loss: 0.6848198
[Epoch 8; Iter    35/   55] train: loss: 0.6829073
[Epoch 8] ogbg-molbbbp: 0.693020 val loss: 0.689998
[Epoch 8] ogbg-molbbbp: 0.520544 test loss: 0.692166
[Epoch 9; Iter    10/   55] train: loss: 0.6824759
[Epoch 9; Iter    40/   55] train: loss: 0.6832919
[Epoch 9] ogbg-molbbbp: 0.712237 val loss: 0.689589
[Epoch 9] ogbg-molbbbp: 0.525367 test loss: 0.691818
[Epoch 10; Iter    15/   55] train: loss: 0.6793813
[Epoch 10; Iter    45/   55] train: loss: 0.6761037
[Epoch 10] ogbg-molbbbp: 0.716619 val loss: 0.690044
[Epoch 10] ogbg-molbbbp: 0.526331 test loss: 0.691658
[Epoch 11; Iter    20/   55] train: loss: 0.6775451
[Epoch 11; Iter    50/   55] train: loss: 0.6749845
[Epoch 11] ogbg-molbbbp: 0.739022 val loss: 0.689276
[Epoch 11] ogbg-molbbbp: 0.530285 test loss: 0.691331
[Epoch 12; Iter    25/   55] train: loss: 0.6756749
[Epoch 12; Iter    55/   55] train: loss: 0.6737819
[Epoch 12] ogbg-molbbbp: 0.745196 val loss: 0.689362
[Epoch 12] ogbg-molbbbp: 0.533565 test loss: 0.691045
[Epoch 13; Iter    30/   55] train: loss: 0.6755737
[Epoch 13] ogbg-molbbbp: 0.912775 val loss: 0.638866
[Epoch 13] ogbg-molbbbp: 0.614294 test loss: 0.680838
[Epoch 14; Iter     5/   55] train: loss: 0.6600859
[Epoch 14; Iter    35/   55] train: loss: 0.6125320
[Epoch 14] ogbg-molbbbp: 0.912576 val loss: 0.527694
[Epoch 14] ogbg-molbbbp: 0.676215 test loss: 0.653872
[Epoch 15; Iter    10/   55] train: loss: 0.6011708
[Epoch 15; Iter    40/   55] train: loss: 0.5246513
[Epoch 15] ogbg-molbbbp: 0.932391 val loss: 0.429690
[Epoch 15] ogbg-molbbbp: 0.668499 test loss: 0.661629
[Epoch 16; Iter    15/   55] train: loss: 0.4103695
[Epoch 16; Iter    45/   55] train: loss: 0.4990442
[Epoch 16] ogbg-molbbbp: 0.936971 val loss: 0.396988
[Epoch 16] ogbg-molbbbp: 0.644194 test loss: 0.720797
[Epoch 17; Iter    20/   55] train: loss: 0.3242468
[Epoch 17; Iter    50/   55] train: loss: 0.4411244
[Epoch 17] ogbg-molbbbp: 0.931295 val loss: 0.358905
[Epoch 17] ogbg-molbbbp: 0.680170 test loss: 0.706807
[Epoch 18; Iter    25/   55] train: loss: 0.3736207
[Epoch 18; Iter    55/   55] train: loss: 0.5158485
[Epoch 18] ogbg-molbbbp: 0.941153 val loss: 0.375839
[Epoch 18] ogbg-molbbbp: 0.665895 test loss: 0.877649
[Epoch 19; Iter    30/   55] train: loss: 0.3345467
[Epoch 19] ogbg-molbbbp: 0.943443 val loss: 0.314649
[Epoch 19] ogbg-molbbbp: 0.646894 test loss: 0.925791
[Epoch 20; Iter     5/   55] train: loss: 0.3231257
[Epoch 20; Iter    35/   55] train: loss: 0.2377951
[Epoch 20] ogbg-molbbbp: 0.944738 val loss: 0.454619
[Epoch 20] ogbg-molbbbp: 0.685185 test loss: 0.901042
[Epoch 21; Iter    10/   55] train: loss: 0.2851139
[Epoch 21; Iter    40/   55] train: loss: 0.5542256
[Epoch 21] ogbg-molbbbp: 0.957483 val loss: 0.315918
[Epoch 21] ogbg-molbbbp: 0.680941 test loss: 0.836815
[Epoch 22; Iter    15/   55] train: loss: 0.3117546
[Epoch 22; Iter    45/   55] train: loss: 0.3554611
[Epoch 22] ogbg-molbbbp: 0.933586 val loss: 0.557752
[Epoch 22] ogbg-molbbbp: 0.684606 test loss: 0.970966
[Epoch 23; Iter    20/   55] train: loss: 0.2494872
[Epoch 23; Iter    50/   55] train: loss: 0.2501745
[Epoch 23] ogbg-molbbbp: 0.957682 val loss: 0.419206
[Epoch 23] ogbg-molbbbp: 0.687596 test loss: 1.119974
[Epoch 24; Iter    25/   55] train: loss: 0.4575572
[Epoch 24; Iter    55/   55] train: loss: 0.1451092
[Epoch 24] ogbg-molbbbp: 0.939759 val loss: 0.469392
[Epoch 24] ogbg-molbbbp: 0.691358 test loss: 0.998601
[Epoch 25; Iter    30/   55] train: loss: 0.2689428
[Epoch 25] ogbg-molbbbp: 0.950812 val loss: 0.333008
[Epoch 25] ogbg-molbbbp: 0.709973 test loss: 0.962775
[Epoch 26; Iter     5/   55] train: loss: 0.2391358
[Epoch 26; Iter    35/   55] train: loss: 0.2025697
[Epoch 26] ogbg-molbbbp: 0.932889 val loss: 0.433971
[Epoch 26] ogbg-molbbbp: 0.674383 test loss: 1.052091
[Epoch 27; Iter    10/   55] train: loss: 0.2705702
[Epoch 27; Iter    40/   55] train: loss: 0.2238509
[Epoch 27] ogbg-molbbbp: 0.960769 val loss: 0.271955
[Epoch 27] ogbg-molbbbp: 0.698978 test loss: 0.823703
[Epoch 28; Iter    15/   55] train: loss: 0.3020680
[Epoch 28; Iter    45/   55] train: loss: 0.1625158
[Epoch 28] ogbg-molbbbp: 0.953699 val loss: 0.273873
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/bbbp/noise=0.0/PNA_ogbg-molbbbp_GraphCL_bbbp_static_noise=0.0_6_26-05_10-25-12
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bbbp/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_static_noise=0.0
logdir: runs/static_noise/GraphCL/bbbp/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6932197
[Epoch 1] ogbg-molbbbp: 0.762720 val loss: 0.690579
[Epoch 1] ogbg-molbbbp: 0.533083 test loss: 0.692731
[Epoch 2; Iter     5/   55] train: loss: 0.6927039
[Epoch 2; Iter    35/   55] train: loss: 0.6903805
[Epoch 2] ogbg-molbbbp: 0.797272 val loss: 0.685792
[Epoch 2] ogbg-molbbbp: 0.534722 test loss: 0.692064
[Epoch 3; Iter    10/   55] train: loss: 0.6901688
[Epoch 3; Iter    40/   55] train: loss: 0.6889755
[Epoch 3] ogbg-molbbbp: 0.794882 val loss: 0.686051
[Epoch 3] ogbg-molbbbp: 0.536073 test loss: 0.692082
[Epoch 4; Iter    15/   55] train: loss: 0.6905053
[Epoch 4; Iter    45/   55] train: loss: 0.6917712
[Epoch 4] ogbg-molbbbp: 0.820671 val loss: 0.685113
[Epoch 4] ogbg-molbbbp: 0.534144 test loss: 0.691986
[Epoch 5; Iter    20/   55] train: loss: 0.6911498
[Epoch 5; Iter    50/   55] train: loss: 0.6849086
[Epoch 5] ogbg-molbbbp: 0.804441 val loss: 0.685635
[Epoch 5] ogbg-molbbbp: 0.544078 test loss: 0.691602
[Epoch 6; Iter    25/   55] train: loss: 0.6898429
[Epoch 6; Iter    55/   55] train: loss: 0.6864198
[Epoch 6] ogbg-molbbbp: 0.818182 val loss: 0.685093
[Epoch 6] ogbg-molbbbp: 0.542728 test loss: 0.691411
[Epoch 7; Iter    30/   55] train: loss: 0.6864702
[Epoch 7] ogbg-molbbbp: 0.820671 val loss: 0.685014
[Epoch 7] ogbg-molbbbp: 0.550637 test loss: 0.691104
[Epoch 8; Iter     5/   55] train: loss: 0.6864006
[Epoch 8; Iter    35/   55] train: loss: 0.6850717
[Epoch 8] ogbg-molbbbp: 0.823758 val loss: 0.685118
[Epoch 8] ogbg-molbbbp: 0.547261 test loss: 0.691080
[Epoch 9; Iter    10/   55] train: loss: 0.6809853
[Epoch 9; Iter    40/   55] train: loss: 0.6821335
[Epoch 9] ogbg-molbbbp: 0.840984 val loss: 0.684847
[Epoch 9] ogbg-molbbbp: 0.547164 test loss: 0.690830
[Epoch 10; Iter    15/   55] train: loss: 0.6796424
[Epoch 10; Iter    45/   55] train: loss: 0.6761301
[Epoch 10] ogbg-molbbbp: 0.844668 val loss: 0.685269
[Epoch 10] ogbg-molbbbp: 0.547068 test loss: 0.690670
[Epoch 11; Iter    20/   55] train: loss: 0.6766856
[Epoch 11; Iter    50/   55] train: loss: 0.6718633
[Epoch 11] ogbg-molbbbp: 0.851339 val loss: 0.685120
[Epoch 11] ogbg-molbbbp: 0.547647 test loss: 0.690257
[Epoch 12; Iter    25/   55] train: loss: 0.6706553
[Epoch 12; Iter    55/   55] train: loss: 0.6854111
[Epoch 12] ogbg-molbbbp: 0.861097 val loss: 0.684722
[Epoch 12] ogbg-molbbbp: 0.551698 test loss: 0.690112
[Epoch 13; Iter    30/   55] train: loss: 0.6700866
[Epoch 13] ogbg-molbbbp: 0.912277 val loss: 0.609498
[Epoch 13] ogbg-molbbbp: 0.658951 test loss: 0.675043
[Epoch 14; Iter     5/   55] train: loss: 0.6314537
[Epoch 14; Iter    35/   55] train: loss: 0.5839542
[Epoch 14] ogbg-molbbbp: 0.942447 val loss: 0.476285
[Epoch 14] ogbg-molbbbp: 0.656346 test loss: 0.676354
[Epoch 15; Iter    10/   55] train: loss: 0.5738180
[Epoch 15; Iter    40/   55] train: loss: 0.4984138
[Epoch 15] ogbg-molbbbp: 0.915464 val loss: 0.430677
[Epoch 15] ogbg-molbbbp: 0.616802 test loss: 0.716303
[Epoch 16; Iter    15/   55] train: loss: 0.4568293
[Epoch 16; Iter    45/   55] train: loss: 0.4049587
[Epoch 16] ogbg-molbbbp: 0.923131 val loss: 0.420508
[Epoch 16] ogbg-molbbbp: 0.685378 test loss: 0.705190
[Epoch 17; Iter    20/   55] train: loss: 0.3968272
[Epoch 17; Iter    50/   55] train: loss: 0.3701334
[Epoch 17] ogbg-molbbbp: 0.945733 val loss: 0.354612
[Epoch 17] ogbg-molbbbp: 0.683353 test loss: 0.721750
[Epoch 18; Iter    25/   55] train: loss: 0.2525442
[Epoch 18; Iter    55/   55] train: loss: 0.2801490
[Epoch 18] ogbg-molbbbp: 0.942547 val loss: 0.411497
[Epoch 18] ogbg-molbbbp: 0.663002 test loss: 0.857327
[Epoch 19; Iter    30/   55] train: loss: 0.2537983
[Epoch 19] ogbg-molbbbp: 0.948820 val loss: 0.316019
[Epoch 19] ogbg-molbbbp: 0.677566 test loss: 0.813521
[Epoch 20; Iter     5/   55] train: loss: 0.3644337
[Epoch 20; Iter    35/   55] train: loss: 0.4354169
[Epoch 20] ogbg-molbbbp: 0.955491 val loss: 0.286527
[Epoch 20] ogbg-molbbbp: 0.655671 test loss: 0.819072
[Epoch 21; Iter    10/   55] train: loss: 0.3622795
[Epoch 21; Iter    40/   55] train: loss: 0.2449801
[Epoch 21] ogbg-molbbbp: 0.935776 val loss: 0.493187
[Epoch 21] ogbg-molbbbp: 0.674093 test loss: 1.120458
[Epoch 22; Iter    15/   55] train: loss: 0.3581019
[Epoch 22; Iter    45/   55] train: loss: 0.2594362
[Epoch 22] ogbg-molbbbp: 0.929204 val loss: 0.470772
[Epoch 22] ogbg-molbbbp: 0.692226 test loss: 0.952291
[Epoch 23; Iter    20/   55] train: loss: 0.3943421
[Epoch 23; Iter    50/   55] train: loss: 0.3807874
[Epoch 23] ogbg-molbbbp: 0.932391 val loss: 0.453190
[Epoch 23] ogbg-molbbbp: 0.682485 test loss: 0.927744
[Epoch 24; Iter    25/   55] train: loss: 0.2283857
[Epoch 24; Iter    55/   55] train: loss: 0.0774461
[Epoch 24] ogbg-molbbbp: 0.951210 val loss: 0.305556
[Epoch 24] ogbg-molbbbp: 0.684606 test loss: 0.994863
[Epoch 25; Iter    30/   55] train: loss: 0.2697252
[Epoch 25] ogbg-molbbbp: 0.949816 val loss: 0.510730
[Epoch 25] ogbg-molbbbp: 0.679688 test loss: 1.205852
[Epoch 26; Iter     5/   55] train: loss: 0.2486286
[Epoch 26; Iter    35/   55] train: loss: 0.1863474
[Epoch 26] ogbg-molbbbp: 0.920442 val loss: 0.364678
[Epoch 26] ogbg-molbbbp: 0.621528 test loss: 0.926933
[Epoch 27; Iter    10/   55] train: loss: 0.1926475
[Epoch 27; Iter    40/   55] train: loss: 0.1212564
[Epoch 27] ogbg-molbbbp: 0.961764 val loss: 0.274758
[Epoch 27] ogbg-molbbbp: 0.682099 test loss: 1.022249
[Epoch 28; Iter    15/   55] train: loss: 0.1506365
[Epoch 28; Iter    45/   55] train: loss: 0.2235475
[Epoch 28] ogbg-molbbbp: 0.947426 val loss: 0.328096
[Epoch 28] ogbg-molbbbp: 0.631655 test loss: 5.082736
[Epoch 29; Iter    20/   55] train: loss: 0.2375589
[Epoch 29; Iter    50/   55] train: loss: 0.2369232
[Epoch 29] ogbg-molbbbp: 0.851638 val loss: 0.779860
[Epoch 29] ogbg-molbbbp: 0.629244 test loss: 1.087402
[Epoch 30; Iter    25/   55] train: loss: 0.1935012
[Epoch 30; Iter    55/   55] train: loss: 0.1086846
[Epoch 30] ogbg-molbbbp: 0.933685 val loss: 0.462284
[Epoch 30] ogbg-molbbbp: 0.627894 test loss: 1.199745
[Epoch 31; Iter    30/   55] train: loss: 0.3435404
[Epoch 31] ogbg-molbbbp: 0.916459 val loss: 0.660072
[Epoch 31] ogbg-molbbbp: 0.577546 test loss: 1.575135
[Epoch 32; Iter     5/   55] train: loss: 0.1051708
[Epoch 32; Iter    35/   55] train: loss: 0.1784688
[Epoch 32] ogbg-molbbbp: 0.927711 val loss: 0.546679
[Epoch 32] ogbg-molbbbp: 0.609182 test loss: 1.851047
[Epoch 33; Iter    10/   55] train: loss: 0.0948229
[Epoch 33; Iter    40/   55] train: loss: 0.1196732
[Epoch 33] ogbg-molbbbp: 0.931395 val loss: 0.428614
[Epoch 33] ogbg-molbbbp: 0.672550 test loss: 1.671491
[Epoch 34; Iter    15/   55] train: loss: 0.1094139
[Epoch 34; Iter    45/   55] train: loss: 0.0502630
[Epoch 34] ogbg-molbbbp: 0.906303 val loss: 0.671981
[Epoch 34] ogbg-molbbbp: 0.665606 test loss: 1.675747
[Epoch 35; Iter    20/   55] train: loss: 0.1174035
[Epoch 35; Iter    50/   55] train: loss: 0.1062597
[Epoch 35] ogbg-molbbbp: 0.939062 val loss: 0.327621
[Epoch 35] ogbg-molbbbp: 0.617863 test loss: 1.165878
[Epoch 36; Iter    25/   55] train: loss: 0.1040137
[Epoch 36; Iter    55/   55] train: loss: 0.0287199
[Epoch 36] ogbg-molbbbp: 0.940556 val loss: 0.442907
[Epoch 36] ogbg-molbbbp: 0.624325 test loss: 1.917190
[Epoch 37; Iter    30/   55] train: loss: 0.1492660
[Epoch 37] ogbg-molbbbp: 0.940655 val loss: 0.408714
[Epoch 37] ogbg-molbbbp: 0.608989 test loss: 1.428542
[Epoch 38; Iter     5/   55] train: loss: 0.0518911
[Epoch 38; Iter    35/   55] train: loss: 0.0281853
[Epoch 38] ogbg-molbbbp: 0.947725 val loss: 0.363036
[Epoch 38] ogbg-molbbbp: 0.641107 test loss: 1.626594
[Epoch 39; Iter    10/   55] train: loss: 0.0470883
[Epoch 39; Iter    40/   55] train: loss: 0.0903595
[Epoch 39] ogbg-molbbbp: 0.914169 val loss: 0.495236
[Epoch 39] ogbg-molbbbp: 0.623360 test loss: 1.413845
[Epoch 40; Iter    15/   55] train: loss: 0.1095009
[Epoch 40; Iter    45/   55] train: loss: 0.1126662
[Epoch 40] ogbg-molbbbp: 0.952106 val loss: 0.509484
[Epoch 40] ogbg-molbbbp: 0.596258 test loss: 2.468627
[Epoch 41; Iter    20/   55] train: loss: 0.0500347
[Epoch 41; Iter    50/   55] train: loss: 0.0211586
[Epoch 41] ogbg-molbbbp: 0.916658 val loss: 0.816872
[Epoch 41] ogbg-molbbbp: 0.608025 test loss: 2.461649
[Epoch 42; Iter    25/   55] train: loss: 0.0384282
[Epoch 42; Iter    55/   55] train: loss: 0.3635569
[Epoch 42] ogbg-molbbbp: 0.948023 val loss: 0.408463
[Epoch 42] ogbg-molbbbp: 0.644869 test loss: 2.083523
[Epoch 43; Iter    30/   55] train: loss: 0.0287451
[Epoch 43] ogbg-molbbbp: 0.919247 val loss: 0.541047
[Epoch 43] ogbg-molbbbp: 0.628472 test loss: 1.751574
[Epoch 44; Iter     5/   55] train: loss: 0.0448518
[Epoch 44; Iter    35/   55] train: loss: 0.0189121
[Epoch 44] ogbg-molbbbp: 0.932789 val loss: 0.523557
[Epoch 44] ogbg-molbbbp: 0.597512 test loss: 2.139316
[Epoch 45; Iter    10/   55] train: loss: 0.0099655
[Epoch 45; Iter    40/   55] train: loss: 0.0352306
[Epoch 45] ogbg-molbbbp: 0.942746 val loss: 0.486049
[Epoch 45] ogbg-molbbbp: 0.622106 test loss: 2.049588
[Epoch 46; Iter    15/   55] train: loss: 0.0113594
[Epoch 46; Iter    45/   55] train: loss: 0.0329105
[Epoch 46] ogbg-molbbbp: 0.940157 val loss: 0.474297
[Epoch 46] ogbg-molbbbp: 0.574074 test loss: 2.152587
[Epoch 47; Iter    20/   55] train: loss: 0.0061612
[Epoch 47; Iter    50/   55] train: loss: 0.0252632
[Epoch 47] ogbg-molbbbp: 0.911879 val loss: 0.652054
[Epoch 47] ogbg-molbbbp: 0.611304 test loss: 2.006513
[Epoch 48; Iter    25/   55] train: loss: 0.0040719
[Epoch 48; Iter    55/   55] train: loss: 0.3194805
[Epoch 48] ogbg-molbbbp: 0.935677 val loss: 0.604533
[Epoch 48] ogbg-molbbbp: 0.577836 test loss: 2.706921
[Epoch 49; Iter    30/   55] train: loss: 0.0222211
[Epoch 49] ogbg-molbbbp: 0.916260 val loss: 0.561833
[Epoch 49] ogbg-molbbbp: 0.627218 test loss: 1.822614
[Epoch 50; Iter     5/   55] train: loss: 0.0157126
[Epoch 50; Iter    35/   55] train: loss: 0.1412579
[Epoch 50] ogbg-molbbbp: 0.880414 val loss: 1.099663
[Epoch 50] ogbg-molbbbp: 0.581694 test loss: 2.749873
[Epoch 51; Iter    10/   55] train: loss: 0.0450541
[Epoch 51; Iter    40/   55] train: loss: 0.0174468
[Epoch 51] ogbg-molbbbp: 0.913870 val loss: 0.594660
[Epoch 51] ogbg-molbbbp: 0.606385 test loss: 1.955271
[Epoch 52; Iter    15/   55] train: loss: 0.0078401
[Epoch 52; Iter    45/   55] train: loss: 0.0064637
[Epoch 52] ogbg-molbbbp: 0.915464 val loss: 0.631925
[Epoch 52] ogbg-molbbbp: 0.610822 test loss: 2.103746
[Epoch 53; Iter    20/   55] train: loss: 0.0242286
[Epoch 53; Iter    50/   55] train: loss: 0.0083896
[Epoch 53] ogbg-molbbbp: 0.934681 val loss: 0.501088
[Epoch 53] ogbg-molbbbp: 0.627797 test loss: 2.046748
[Epoch 54; Iter    25/   55] train: loss: 0.0271185
[Epoch 54; Iter    55/   55] train: loss: 0.0154432
[Epoch 54] ogbg-molbbbp: 0.869063 val loss: 1.147588
[Epoch 54] ogbg-molbbbp: 0.568480 test loss: 2.682126
[Epoch 55; Iter    30/   55] train: loss: 0.0078747
[Epoch 55] ogbg-molbbbp: 0.933088 val loss: 0.547065
[Epoch 55] ogbg-molbbbp: 0.626350 test loss: 2.144160
[Epoch 56; Iter     5/   55] train: loss: 0.0051986
[Epoch 56; Iter    35/   55] train: loss: 0.0126998
[Epoch 56] ogbg-molbbbp: 0.935876 val loss: 0.500178
[Epoch 56] ogbg-molbbbp: 0.603877 test loss: 2.210960
[Epoch 57; Iter    10/   55] train: loss: 0.0594358
[Epoch 57; Iter    40/   55] train: loss: 0.0333323
[Epoch 57] ogbg-molbbbp: 0.925321 val loss: 0.675902
[Epoch 57] ogbg-molbbbp: 0.650367 test loss: 2.294479
[Epoch 58; Iter    15/   55] train: loss: 0.0010447
[Epoch 58; Iter    45/   55] train: loss: 0.0114934
[Epoch 58] ogbg-molbbbp: 0.938664 val loss: 0.601008
[Epoch 58] ogbg-molbbbp: 0.613522 test loss: 2.136254
[Epoch 59; Iter    20/   55] train: loss: 0.0596815
[Epoch 59; Iter    50/   55] train: loss: 0.0154441
[Epoch 59] ogbg-molbbbp: 0.930399 val loss: 0.578278
[Epoch 59] ogbg-molbbbp: 0.659433 test loss: 1.815113
[Epoch 60; Iter    25/   55] train: loss: 0.0086855
[Epoch 60; Iter    55/   55] train: loss: 0.0116210
[Epoch 60] ogbg-molbbbp: 0.926317 val loss: 0.696145
[Epoch 60] ogbg-molbbbp: 0.641975 test loss: 2.530649
[Epoch 61; Iter    30/   55] train: loss: 0.0026368
[Epoch 61] ogbg-molbbbp: 0.931395 val loss: 0.599050
[Epoch 61] ogbg-molbbbp: 0.601755 test loss: 2.468889
[Epoch 62; Iter     5/   55] train: loss: 0.0035723
[Epoch 62; Iter    35/   55] train: loss: 0.0047024
[Epoch 62] ogbg-molbbbp: 0.920243 val loss: 0.758566
[Epoch 62] ogbg-molbbbp: 0.605999 test loss: 2.545420
[Epoch 63; Iter    10/   55] train: loss: 0.0014532
[Epoch 63; Iter    40/   55] train: loss: 0.0085164
[Epoch 63] ogbg-molbbbp: 0.938066 val loss: 0.605469
[Epoch 63] ogbg-molbbbp: 0.620853 test loss: 2.564426
[Epoch 64; Iter    15/   55] train: loss: 0.0071228
[Epoch 64; Iter    45/   55] train: loss: 0.0337606
[Epoch 64] ogbg-molbbbp: 0.891467 val loss: 0.918829
[Epoch 64] ogbg-molbbbp: 0.600984 test loss: 2.259897
[Epoch 65; Iter    20/   55] train: loss: 0.0020464
[Epoch 65; Iter    50/   55] train: loss: 0.0784408
[Epoch 65] ogbg-molbbbp: 0.920840 val loss: 0.805621
[Epoch 65] ogbg-molbbbp: 0.610822 test loss: 2.285519
[Epoch 66; Iter    25/   55] train: loss: 0.0034076
[Epoch 66; Iter    55/   55] train: loss: 0.0042632
[Epoch 66] ogbg-molbbbp: 0.943742 val loss: 0.583448
[Epoch 66] ogbg-molbbbp: 0.621046 test loss: 2.501984
[Epoch 67; Iter    30/   55] train: loss: 0.0077313
[Epoch 67] ogbg-molbbbp: 0.941651 val loss: 0.599085
[Epoch 67] ogbg-molbbbp: 0.645544 test loss: 2.366930
[Epoch 68; Iter     5/   55] train: loss: 0.0028640
[Epoch 68; Iter    35/   55] train: loss: 0.0070300
[Epoch 68] ogbg-molbbbp: 0.947824 val loss: 0.572984
[Epoch 68] ogbg-molbbbp: 0.604649 test loss: 3.006441
[Epoch 69; Iter    10/   55] train: loss: 0.0020648
[Epoch 28] ogbg-molbbbp: 0.654707 test loss: 1.009621
[Epoch 29; Iter    20/   55] train: loss: 0.2690377
[Epoch 29; Iter    50/   55] train: loss: 0.3094870
[Epoch 29] ogbg-molbbbp: 0.724385 val loss: 0.979433
[Epoch 29] ogbg-molbbbp: 0.557099 test loss: 1.163466
[Epoch 30; Iter    25/   55] train: loss: 0.2334035
[Epoch 30; Iter    55/   55] train: loss: 0.2017884
[Epoch 30] ogbg-molbbbp: 0.900826 val loss: 10.535836
[Epoch 30] ogbg-molbbbp: 0.611786 test loss: 3.468174
[Epoch 31; Iter    30/   55] train: loss: 0.4495301
[Epoch 31] ogbg-molbbbp: 0.903017 val loss: 0.946360
[Epoch 31] ogbg-molbbbp: 0.617188 test loss: 3.102339
[Epoch 32; Iter     5/   55] train: loss: 0.1246105
[Epoch 32; Iter    35/   55] train: loss: 0.1953052
[Epoch 32] ogbg-molbbbp: 0.899930 val loss: 0.432701
[Epoch 32] ogbg-molbbbp: 0.610532 test loss: 1.235612
[Epoch 33; Iter    10/   55] train: loss: 0.1560584
[Epoch 33; Iter    40/   55] train: loss: 0.1773443
[Epoch 33] ogbg-molbbbp: 0.868963 val loss: 0.949232
[Epoch 33] ogbg-molbbbp: 0.607446 test loss: 1.905759
[Epoch 34; Iter    15/   55] train: loss: 0.1558064
[Epoch 34; Iter    45/   55] train: loss: 0.0632777
[Epoch 34] ogbg-molbbbp: 0.919546 val loss: 0.631907
[Epoch 34] ogbg-molbbbp: 0.637924 test loss: 2.683747
[Epoch 35; Iter    20/   55] train: loss: 0.0692408
[Epoch 35; Iter    50/   55] train: loss: 0.1080254
[Epoch 35] ogbg-molbbbp: 0.838793 val loss: 0.944657
[Epoch 35] ogbg-molbbbp: 0.538677 test loss: 1.812134
[Epoch 36; Iter    25/   55] train: loss: 0.2511778
[Epoch 36; Iter    55/   55] train: loss: 0.0565511
[Epoch 36] ogbg-molbbbp: 0.887583 val loss: 0.649785
[Epoch 36] ogbg-molbbbp: 0.553337 test loss: 1.977537
[Epoch 37; Iter    30/   55] train: loss: 0.0827704
[Epoch 37] ogbg-molbbbp: 0.832520 val loss: 0.871979
[Epoch 37] ogbg-molbbbp: 0.581983 test loss: 2.102698
[Epoch 38; Iter     5/   55] train: loss: 0.0294771
[Epoch 38; Iter    35/   55] train: loss: 0.0206143
[Epoch 38] ogbg-molbbbp: 0.896644 val loss: 0.689145
[Epoch 38] ogbg-molbbbp: 0.578414 test loss: 2.295678
[Epoch 39; Iter    10/   55] train: loss: 0.0359488
[Epoch 39; Iter    40/   55] train: loss: 0.0338655
[Epoch 39] ogbg-molbbbp: 0.886488 val loss: 0.826084
[Epoch 39] ogbg-molbbbp: 0.582272 test loss: 2.472392
[Epoch 40; Iter    15/   55] train: loss: 0.0812737
[Epoch 40; Iter    45/   55] train: loss: 0.0210015
[Epoch 40] ogbg-molbbbp: 0.925022 val loss: 0.547261
[Epoch 40] ogbg-molbbbp: 0.591435 test loss: 6.280308
[Epoch 41; Iter    20/   55] train: loss: 0.0510734
[Epoch 41; Iter    50/   55] train: loss: 0.0274541
[Epoch 41] ogbg-molbbbp: 0.915464 val loss: 0.470903
[Epoch 41] ogbg-molbbbp: 0.539062 test loss: 2.572958
[Epoch 42; Iter    25/   55] train: loss: 0.0568446
[Epoch 42; Iter    55/   55] train: loss: 0.0450022
[Epoch 42] ogbg-molbbbp: 0.833018 val loss: 0.874535
[Epoch 42] ogbg-molbbbp: 0.562982 test loss: 1.777515
[Epoch 43; Iter    30/   55] train: loss: 0.0451223
[Epoch 43] ogbg-molbbbp: 0.883401 val loss: 0.812045
[Epoch 43] ogbg-molbbbp: 0.588927 test loss: 2.159199
[Epoch 44; Iter     5/   55] train: loss: 0.0113585
[Epoch 44; Iter    35/   55] train: loss: 0.0164855
[Epoch 44] ogbg-molbbbp: 0.844568 val loss: 2.000299
[Epoch 44] ogbg-molbbbp: 0.594522 test loss: 2.027070
[Epoch 45; Iter    10/   55] train: loss: 0.0066130
[Epoch 45; Iter    40/   55] train: loss: 0.0168773
[Epoch 45] ogbg-molbbbp: 0.930300 val loss: 0.739184
[Epoch 45] ogbg-molbbbp: 0.565683 test loss: 2.387047
[Epoch 46; Iter    15/   55] train: loss: 0.0115434
[Epoch 46; Iter    45/   55] train: loss: 0.0975092
[Epoch 46] ogbg-molbbbp: 0.903216 val loss: 1.878577
[Epoch 46] ogbg-molbbbp: 0.593846 test loss: 6.769953
[Epoch 47; Iter    20/   55] train: loss: 0.0159625
[Epoch 47; Iter    50/   55] train: loss: 0.0276959
[Epoch 47] ogbg-molbbbp: 0.903515 val loss: 0.812635
[Epoch 47] ogbg-molbbbp: 0.589120 test loss: 3.195199
[Epoch 48; Iter    25/   55] train: loss: 0.0035466
[Epoch 48; Iter    55/   55] train: loss: 0.0740374
[Epoch 48] ogbg-molbbbp: 0.871851 val loss: 1.531345
[Epoch 48] ogbg-molbbbp: 0.524595 test loss: 3.605234
[Epoch 49; Iter    30/   55] train: loss: 0.0155053
[Epoch 49] ogbg-molbbbp: 0.903614 val loss: 1.072807
[Epoch 49] ogbg-molbbbp: 0.599730 test loss: 3.550558
[Epoch 50; Iter     5/   55] train: loss: 0.0026174
[Epoch 50; Iter    35/   55] train: loss: 0.0047477
[Epoch 50] ogbg-molbbbp: 0.891865 val loss: 1.872594
[Epoch 50] ogbg-molbbbp: 0.540123 test loss: 3.816495
[Epoch 51; Iter    10/   55] train: loss: 0.0068454
[Epoch 51; Iter    40/   55] train: loss: 0.0905817
[Epoch 51] ogbg-molbbbp: 0.903614 val loss: 5.022513
[Epoch 51] ogbg-molbbbp: 0.559124 test loss: 5.333896
[Epoch 52; Iter    15/   55] train: loss: 0.0834609
[Epoch 52; Iter    45/   55] train: loss: 0.0198254
[Epoch 52] ogbg-molbbbp: 0.901922 val loss: 1.393165
[Epoch 52] ogbg-molbbbp: 0.567033 test loss: 3.462823
[Epoch 53; Iter    20/   55] train: loss: 0.0347515
[Epoch 53; Iter    50/   55] train: loss: 0.0787259
[Epoch 53] ogbg-molbbbp: 0.899831 val loss: 1.335853
[Epoch 53] ogbg-molbbbp: 0.583430 test loss: 3.246885
[Epoch 54; Iter    25/   55] train: loss: 0.0155093
[Epoch 54; Iter    55/   55] train: loss: 0.0542527
[Epoch 54] ogbg-molbbbp: 0.914966 val loss: 0.572126
[Epoch 54] ogbg-molbbbp: 0.573399 test loss: 3.455897
[Epoch 55; Iter    30/   55] train: loss: 0.0619118
[Epoch 55] ogbg-molbbbp: 0.910087 val loss: 2.201215
[Epoch 55] ogbg-molbbbp: 0.597512 test loss: 4.871523
[Epoch 56; Iter     5/   55] train: loss: 0.0037593
[Epoch 56; Iter    35/   55] train: loss: 0.0060059
[Epoch 56] ogbg-molbbbp: 0.924126 val loss: 1.927968
[Epoch 56] ogbg-molbbbp: 0.601273 test loss: 5.162808
[Epoch 57; Iter    10/   55] train: loss: 0.0016290
[Epoch 57; Iter    40/   55] train: loss: 0.0081472
[Epoch 57] ogbg-molbbbp: 0.905407 val loss: 1.549069
[Epoch 57] ogbg-molbbbp: 0.599151 test loss: 4.776660
[Epoch 58; Iter    15/   55] train: loss: 0.0008571
[Epoch 58; Iter    45/   55] train: loss: 0.0096235
[Epoch 58] ogbg-molbbbp: 0.908195 val loss: 0.713077
[Epoch 58] ogbg-molbbbp: 0.573592 test loss: 3.687001
[Epoch 59; Iter    20/   55] train: loss: 0.0317738
[Epoch 59; Iter    50/   55] train: loss: 0.0068320
[Epoch 59] ogbg-molbbbp: 0.917555 val loss: 1.387537
[Epoch 59] ogbg-molbbbp: 0.578414 test loss: 4.941725
[Epoch 60; Iter    25/   55] train: loss: 0.0011719
[Epoch 60; Iter    55/   55] train: loss: 0.0271410
[Epoch 60] ogbg-molbbbp: 0.905705 val loss: 1.471500
[Epoch 60] ogbg-molbbbp: 0.585455 test loss: 4.977178
[Epoch 61; Iter    30/   55] train: loss: 0.0010451
[Epoch 61] ogbg-molbbbp: 0.923828 val loss: 1.098678
[Epoch 61] ogbg-molbbbp: 0.534047 test loss: 5.067980
[Epoch 62; Iter     5/   55] train: loss: 0.0156084
[Epoch 62; Iter    35/   55] train: loss: 0.0037709
[Epoch 62] ogbg-molbbbp: 0.916957 val loss: 1.097729
[Epoch 62] ogbg-molbbbp: 0.587770 test loss: 5.192505
[Epoch 63; Iter    10/   55] train: loss: 0.0026813
[Epoch 63; Iter    40/   55] train: loss: 0.0075506
[Epoch 63] ogbg-molbbbp: 0.888081 val loss: 1.015065
[Epoch 63] ogbg-molbbbp: 0.556038 test loss: 3.179609
[Epoch 64; Iter    15/   55] train: loss: 0.0019710
[Epoch 64; Iter    45/   55] train: loss: 0.0876719
[Epoch 64] ogbg-molbbbp: 0.906801 val loss: 0.703902
[Epoch 64] ogbg-molbbbp: 0.604456 test loss: 3.657284
[Epoch 65; Iter    20/   55] train: loss: 0.0014979
[Epoch 65; Iter    50/   55] train: loss: 0.0030983
[Epoch 65] ogbg-molbbbp: 0.918152 val loss: 0.623231
[Epoch 65] ogbg-molbbbp: 0.564140 test loss: 4.196682
[Epoch 66; Iter    25/   55] train: loss: 0.0008729
[Epoch 66; Iter    55/   55] train: loss: 0.0023339
[Epoch 66] ogbg-molbbbp: 0.914866 val loss: 0.659002
[Epoch 66] ogbg-molbbbp: 0.575907 test loss: 4.199815
[Epoch 67; Iter    30/   55] train: loss: 0.0027336
[Epoch 67] ogbg-molbbbp: 0.907996 val loss: 1.490853
[Epoch 67] ogbg-molbbbp: 0.581115 test loss: 5.731777
[Epoch 68; Iter     5/   55] train: loss: 0.0007515
[Epoch 68; Iter    35/   55] train: loss: 0.0004473
[Epoch 68] ogbg-molbbbp: 0.909290 val loss: 0.772903
[Epoch 68] ogbg-molbbbp: 0.575907 test loss: 3.907254
[Epoch 69; Iter    10/   55] train: loss: 0.0007449
[Epoch 28] ogbg-molbbbp: 0.643808 test loss: 1.114591
[Epoch 29; Iter    20/   55] train: loss: 0.2544726
[Epoch 29; Iter    50/   55] train: loss: 0.3210345
[Epoch 29] ogbg-molbbbp: 0.958180 val loss: 0.267770
[Epoch 29] ogbg-molbbbp: 0.613812 test loss: 1.318155
[Epoch 30; Iter    25/   55] train: loss: 0.1406206
[Epoch 30; Iter    55/   55] train: loss: 0.3181675
[Epoch 30] ogbg-molbbbp: 0.806333 val loss: 1.149312
[Epoch 30] ogbg-molbbbp: 0.655575 test loss: 1.173053
[Epoch 31; Iter    30/   55] train: loss: 0.2654238
[Epoch 31] ogbg-molbbbp: 0.936971 val loss: 0.401652
[Epoch 31] ogbg-molbbbp: 0.668113 test loss: 1.045943
[Epoch 32; Iter     5/   55] train: loss: 0.1327242
[Epoch 32; Iter    35/   55] train: loss: 0.1064177
[Epoch 32] ogbg-molbbbp: 0.950712 val loss: 0.304305
[Epoch 32] ogbg-molbbbp: 0.696277 test loss: 1.046671
[Epoch 33; Iter    10/   55] train: loss: 0.1270034
[Epoch 33; Iter    40/   55] train: loss: 0.1697310
[Epoch 33] ogbg-molbbbp: 0.849447 val loss: 0.965594
[Epoch 33] ogbg-molbbbp: 0.656443 test loss: 1.220427
[Epoch 34; Iter    15/   55] train: loss: 0.3184045
[Epoch 34; Iter    45/   55] train: loss: 0.3271693
[Epoch 34] ogbg-molbbbp: 0.899632 val loss: 0.821630
[Epoch 34] ogbg-molbbbp: 0.692130 test loss: 1.527996
[Epoch 35; Iter    20/   55] train: loss: 0.1913837
[Epoch 35; Iter    50/   55] train: loss: 0.1470337
[Epoch 35] ogbg-molbbbp: 0.894056 val loss: 0.764747
[Epoch 35] ogbg-molbbbp: 0.666763 test loss: 1.365297
[Epoch 36; Iter    25/   55] train: loss: 0.2462382
[Epoch 36; Iter    55/   55] train: loss: 0.0902107
[Epoch 36] ogbg-molbbbp: 0.943344 val loss: 0.508731
[Epoch 36] ogbg-molbbbp: 0.644097 test loss: 1.881054
[Epoch 37; Iter    30/   55] train: loss: 0.2034330
[Epoch 37] ogbg-molbbbp: 0.949617 val loss: 0.338485
[Epoch 37] ogbg-molbbbp: 0.678627 test loss: 1.170961
[Epoch 38; Iter     5/   55] train: loss: 0.0669246
[Epoch 38; Iter    35/   55] train: loss: 0.0634060
[Epoch 38] ogbg-molbbbp: 0.954297 val loss: 0.385465
[Epoch 38] ogbg-molbbbp: 0.613619 test loss: 1.843540
[Epoch 39; Iter    10/   55] train: loss: 0.1473588
[Epoch 39; Iter    40/   55] train: loss: 0.0522084
[Epoch 39] ogbg-molbbbp: 0.915364 val loss: 0.521992
[Epoch 39] ogbg-molbbbp: 0.683353 test loss: 1.275853
[Epoch 40; Iter    15/   55] train: loss: 0.0155476
[Epoch 40; Iter    45/   55] train: loss: 0.2375699
[Epoch 40] ogbg-molbbbp: 0.953002 val loss: 0.386301
[Epoch 40] ogbg-molbbbp: 0.651235 test loss: 1.853594
[Epoch 41; Iter    20/   55] train: loss: 0.0285138
[Epoch 41; Iter    50/   55] train: loss: 0.0409797
[Epoch 41] ogbg-molbbbp: 0.953002 val loss: 0.352027
[Epoch 41] ogbg-molbbbp: 0.679591 test loss: 1.434458
[Epoch 42; Iter    25/   55] train: loss: 0.0403350
[Epoch 42; Iter    55/   55] train: loss: 0.0212478
[Epoch 42] ogbg-molbbbp: 0.959176 val loss: 0.305556
[Epoch 42] ogbg-molbbbp: 0.662326 test loss: 1.508178
[Epoch 43; Iter    30/   55] train: loss: 0.0641228
[Epoch 43] ogbg-molbbbp: 0.946530 val loss: 0.552572
[Epoch 43] ogbg-molbbbp: 0.659144 test loss: 1.729863
[Epoch 44; Iter     5/   55] train: loss: 0.0550947
[Epoch 44; Iter    35/   55] train: loss: 0.0210148
[Epoch 44] ogbg-molbbbp: 0.947824 val loss: 0.539622
[Epoch 44] ogbg-molbbbp: 0.607446 test loss: 2.303905
[Epoch 45; Iter    10/   55] train: loss: 0.0416815
[Epoch 45; Iter    40/   55] train: loss: 0.1265490
[Epoch 45] ogbg-molbbbp: 0.943941 val loss: 0.594208
[Epoch 45] ogbg-molbbbp: 0.663098 test loss: 1.948327
[Epoch 46; Iter    15/   55] train: loss: 0.0493652
[Epoch 46; Iter    45/   55] train: loss: 0.0284539
[Epoch 46] ogbg-molbbbp: 0.939560 val loss: 0.540478
[Epoch 46] ogbg-molbbbp: 0.658758 test loss: 2.012040
[Epoch 47; Iter    20/   55] train: loss: 0.0277205
[Epoch 47; Iter    50/   55] train: loss: 0.2338107
[Epoch 47] ogbg-molbbbp: 0.939659 val loss: 0.469423
[Epoch 47] ogbg-molbbbp: 0.620177 test loss: 1.844693
[Epoch 48; Iter    25/   55] train: loss: 0.0061558
[Epoch 48; Iter    55/   55] train: loss: 0.0283755
[Epoch 48] ogbg-molbbbp: 0.963656 val loss: 0.386996
[Epoch 48] ogbg-molbbbp: 0.682485 test loss: 1.763875
[Epoch 49; Iter    30/   55] train: loss: 0.0067247
[Epoch 49] ogbg-molbbbp: 0.962063 val loss: 0.381593
[Epoch 49] ogbg-molbbbp: 0.694830 test loss: 2.099645
[Epoch 50; Iter     5/   55] train: loss: 0.0098046
[Epoch 50; Iter    35/   55] train: loss: 0.0099721
[Epoch 50] ogbg-molbbbp: 0.949119 val loss: 0.508102
[Epoch 50] ogbg-molbbbp: 0.645640 test loss: 2.321909
[Epoch 51; Iter    10/   55] train: loss: 0.0272452
[Epoch 51; Iter    40/   55] train: loss: 0.0074024
[Epoch 51] ogbg-molbbbp: 0.949019 val loss: 0.426061
[Epoch 51] ogbg-molbbbp: 0.649884 test loss: 1.925646
[Epoch 52; Iter    15/   55] train: loss: 0.0064977
[Epoch 52; Iter    45/   55] train: loss: 0.0071078
[Epoch 52] ogbg-molbbbp: 0.953799 val loss: 0.467830
[Epoch 52] ogbg-molbbbp: 0.628376 test loss: 2.394173
[Epoch 53; Iter    20/   55] train: loss: 0.0029173
[Epoch 53; Iter    50/   55] train: loss: 0.0020799
[Epoch 53] ogbg-molbbbp: 0.952006 val loss: 0.417100
[Epoch 53] ogbg-molbbbp: 0.653260 test loss: 2.137903
[Epoch 54; Iter    25/   55] train: loss: 0.0095294
[Epoch 54; Iter    55/   55] train: loss: 0.3818682
[Epoch 54] ogbg-molbbbp: 0.806333 val loss: 1.737448
[Epoch 54] ogbg-molbbbp: 0.550154 test loss: 2.813991
[Epoch 55; Iter    30/   55] train: loss: 0.0310982
[Epoch 55] ogbg-molbbbp: 0.939659 val loss: 0.951457
[Epoch 55] ogbg-molbbbp: 0.662809 test loss: 3.001980
[Epoch 56; Iter     5/   55] train: loss: 0.0160021
[Epoch 56; Iter    35/   55] train: loss: 0.0186039
[Epoch 56] ogbg-molbbbp: 0.954595 val loss: 0.536352
[Epoch 56] ogbg-molbbbp: 0.664352 test loss: 2.605853
[Epoch 57; Iter    10/   55] train: loss: 0.0257763
[Epoch 57; Iter    40/   55] train: loss: 0.0231284
[Epoch 57] ogbg-molbbbp: 0.952604 val loss: 0.557323
[Epoch 57] ogbg-molbbbp: 0.662712 test loss: 2.707310
[Epoch 58; Iter    15/   55] train: loss: 0.0779727
[Epoch 58; Iter    45/   55] train: loss: 0.0705373
[Epoch 58] ogbg-molbbbp: 0.963855 val loss: 0.407364
[Epoch 58] ogbg-molbbbp: 0.671393 test loss: 2.174704
[Epoch 59; Iter    20/   55] train: loss: 0.0100321
[Epoch 59; Iter    50/   55] train: loss: 0.0281583
[Epoch 59] ogbg-molbbbp: 0.954097 val loss: 0.617806
[Epoch 59] ogbg-molbbbp: 0.633005 test loss: 2.798277
[Epoch 60; Iter    25/   55] train: loss: 0.0086055
[Epoch 60; Iter    55/   55] train: loss: 0.0015556
[Epoch 60] ogbg-molbbbp: 0.955491 val loss: 0.476362
[Epoch 60] ogbg-molbbbp: 0.626157 test loss: 2.843083
[Epoch 61; Iter    30/   55] train: loss: 0.0191226
[Epoch 61] ogbg-molbbbp: 0.944041 val loss: 0.714598
[Epoch 61] ogbg-molbbbp: 0.618056 test loss: 2.616940
[Epoch 62; Iter     5/   55] train: loss: 0.0024478
[Epoch 62; Iter    35/   55] train: loss: 0.0117748
[Epoch 62] ogbg-molbbbp: 0.957582 val loss: 0.492946
[Epoch 62] ogbg-molbbbp: 0.621817 test loss: 2.811484
[Epoch 63; Iter    10/   55] train: loss: 0.0079065
[Epoch 63; Iter    40/   55] train: loss: 0.0086769
[Epoch 63] ogbg-molbbbp: 0.958976 val loss: 0.527455
[Epoch 63] ogbg-molbbbp: 0.646412 test loss: 2.830581
[Epoch 64; Iter    15/   55] train: loss: 0.0019176
[Epoch 64; Iter    45/   55] train: loss: 0.0012558
[Epoch 64] ogbg-molbbbp: 0.949119 val loss: 0.588833
[Epoch 64] ogbg-molbbbp: 0.657504 test loss: 2.531549
[Epoch 65; Iter    20/   55] train: loss: 0.0014227
[Epoch 65; Iter    50/   55] train: loss: 0.0008637
[Epoch 65] ogbg-molbbbp: 0.946530 val loss: 0.636701
[Epoch 65] ogbg-molbbbp: 0.617863 test loss: 3.020117
[Epoch 66; Iter    25/   55] train: loss: 0.0045012
[Epoch 66; Iter    55/   55] train: loss: 0.0068757
[Epoch 66] ogbg-molbbbp: 0.959873 val loss: 0.422093
[Epoch 66] ogbg-molbbbp: 0.632523 test loss: 2.623960
[Epoch 67; Iter    30/   55] train: loss: 0.0065279
[Epoch 67] ogbg-molbbbp: 0.937768 val loss: 0.917403
[Epoch 67] ogbg-molbbbp: 0.594907 test loss: 3.150436
[Epoch 68; Iter     5/   55] train: loss: 0.1681623
[Epoch 68; Iter    35/   55] train: loss: 0.0871496
[Epoch 68] ogbg-molbbbp: 0.944638 val loss: 1.238728
[Epoch 68] ogbg-molbbbp: 0.631173 test loss: 3.714248
[Epoch 69; Iter    10/   55] train: loss: 0.0344720
[Epoch 28] ogbg-molbbbp: 0.660590 test loss: 1.000218
[Epoch 29; Iter    20/   55] train: loss: 0.2109113
[Epoch 29; Iter    50/   55] train: loss: 0.3472537
[Epoch 29] ogbg-molbbbp: 0.924724 val loss: 0.566298
[Epoch 29] ogbg-molbbbp: 0.623843 test loss: 1.879665
[Epoch 30; Iter    25/   55] train: loss: 0.1893317
[Epoch 30; Iter    55/   55] train: loss: 0.3657896
[Epoch 30] ogbg-molbbbp: 0.761525 val loss: 0.877774
[Epoch 30] ogbg-molbbbp: 0.619599 test loss: 1.054350
[Epoch 31; Iter    30/   55] train: loss: 0.2939382
[Epoch 31] ogbg-molbbbp: 0.870955 val loss: 0.749381
[Epoch 31] ogbg-molbbbp: 0.623167 test loss: 1.044776
[Epoch 32; Iter     5/   55] train: loss: 0.1757647
[Epoch 32; Iter    35/   55] train: loss: 0.1269852
[Epoch 32] ogbg-molbbbp: 0.852932 val loss: 2.290986
[Epoch 32] ogbg-molbbbp: 0.665799 test loss: 1.072271
[Epoch 33; Iter    10/   55] train: loss: 0.1674559
[Epoch 33; Iter    40/   55] train: loss: 0.2189357
[Epoch 33] ogbg-molbbbp: 0.715225 val loss: 8.398879
[Epoch 33] ogbg-molbbbp: 0.637731 test loss: 1.510843
[Epoch 34; Iter    15/   55] train: loss: 0.4047431
[Epoch 34; Iter    45/   55] train: loss: 0.2915282
[Epoch 34] ogbg-molbbbp: 0.877029 val loss: 1.594337
[Epoch 34] ogbg-molbbbp: 0.664545 test loss: 1.748164
[Epoch 35; Iter    20/   55] train: loss: 0.1617595
[Epoch 35; Iter    50/   55] train: loss: 0.2689815
[Epoch 35] ogbg-molbbbp: 0.910983 val loss: 0.918886
[Epoch 35] ogbg-molbbbp: 0.667245 test loss: 1.002234
[Epoch 36; Iter    25/   55] train: loss: 0.2521032
[Epoch 36; Iter    55/   55] train: loss: 0.1024440
[Epoch 36] ogbg-molbbbp: 0.861296 val loss: 0.831215
[Epoch 36] ogbg-molbbbp: 0.620467 test loss: 1.293379
[Epoch 37; Iter    30/   55] train: loss: 0.0950807
[Epoch 37] ogbg-molbbbp: 0.942447 val loss: 0.348533
[Epoch 37] ogbg-molbbbp: 0.693191 test loss: 1.147344
[Epoch 38; Iter     5/   55] train: loss: 0.0689226
[Epoch 38; Iter    35/   55] train: loss: 0.1135501
[Epoch 38] ogbg-molbbbp: 0.932192 val loss: 0.458273
[Epoch 38] ogbg-molbbbp: 0.635127 test loss: 1.457189
[Epoch 39; Iter    10/   55] train: loss: 0.0468035
[Epoch 39; Iter    40/   55] train: loss: 0.0682238
[Epoch 39] ogbg-molbbbp: 0.870258 val loss: 1.632525
[Epoch 39] ogbg-molbbbp: 0.707176 test loss: 1.035570
[Epoch 40; Iter    15/   55] train: loss: 0.0580358
[Epoch 40; Iter    45/   55] train: loss: 0.2127582
[Epoch 40] ogbg-molbbbp: 0.928408 val loss: 0.467525
[Epoch 40] ogbg-molbbbp: 0.589699 test loss: 1.940531
[Epoch 41; Iter    20/   55] train: loss: 0.0494017
[Epoch 41; Iter    50/   55] train: loss: 0.0331700
[Epoch 41] ogbg-molbbbp: 0.912974 val loss: 0.459785
[Epoch 41] ogbg-molbbbp: 0.612558 test loss: 1.343099
[Epoch 42; Iter    25/   55] train: loss: 0.0346386
[Epoch 42; Iter    55/   55] train: loss: 0.0322067
[Epoch 42] ogbg-molbbbp: 0.830628 val loss: 1.541228
[Epoch 42] ogbg-molbbbp: 0.628954 test loss: 1.818573
[Epoch 43; Iter    30/   55] train: loss: 0.0288547
[Epoch 43] ogbg-molbbbp: 0.861794 val loss: 1.817315
[Epoch 43] ogbg-molbbbp: 0.602816 test loss: 2.054080
[Epoch 44; Iter     5/   55] train: loss: 0.0170036
[Epoch 44; Iter    35/   55] train: loss: 0.1291426
[Epoch 44] ogbg-molbbbp: 0.843573 val loss: 1.268523
[Epoch 44] ogbg-molbbbp: 0.624132 test loss: 1.553827
[Epoch 45; Iter    10/   55] train: loss: 0.0390632
[Epoch 45; Iter    40/   55] train: loss: 0.1029577
[Epoch 45] ogbg-molbbbp: 0.908294 val loss: 0.540441
[Epoch 45] ogbg-molbbbp: 0.709684 test loss: 1.316751
[Epoch 46; Iter    15/   55] train: loss: 0.0190120
[Epoch 46; Iter    45/   55] train: loss: 0.0659958
[Epoch 46] ogbg-molbbbp: 0.950513 val loss: 0.364309
[Epoch 46] ogbg-molbbbp: 0.660494 test loss: 1.407107
[Epoch 47; Iter    20/   55] train: loss: 0.0163543
[Epoch 47; Iter    50/   55] train: loss: 0.1000233
[Epoch 47] ogbg-molbbbp: 0.942547 val loss: 0.404131
[Epoch 47] ogbg-molbbbp: 0.637924 test loss: 1.697784
[Epoch 48; Iter    25/   55] train: loss: 0.0084967
[Epoch 48; Iter    55/   55] train: loss: 0.0219477
[Epoch 48] ogbg-molbbbp: 0.927213 val loss: 0.502208
[Epoch 48] ogbg-molbbbp: 0.639757 test loss: 1.742959
[Epoch 49; Iter    30/   55] train: loss: 0.0250108
[Epoch 49] ogbg-molbbbp: 0.929901 val loss: 1.050918
[Epoch 49] ogbg-molbbbp: 0.677758 test loss: 1.641675
[Epoch 50; Iter     5/   55] train: loss: 0.2198306
[Epoch 50; Iter    35/   55] train: loss: 0.0543363
[Epoch 50] ogbg-molbbbp: 0.928109 val loss: 0.828385
[Epoch 50] ogbg-molbbbp: 0.616030 test loss: 2.215122
[Epoch 51; Iter    10/   55] train: loss: 0.0319178
[Epoch 51; Iter    40/   55] train: loss: 0.0153676
[Epoch 51] ogbg-molbbbp: 0.928707 val loss: 0.673337
[Epoch 51] ogbg-molbbbp: 0.658565 test loss: 1.702436
[Epoch 52; Iter    15/   55] train: loss: 0.0231749
[Epoch 52; Iter    45/   55] train: loss: 0.0057907
[Epoch 52] ogbg-molbbbp: 0.921936 val loss: 0.632434
[Epoch 52] ogbg-molbbbp: 0.625965 test loss: 1.778998
[Epoch 53; Iter    20/   55] train: loss: 0.0015456
[Epoch 53; Iter    50/   55] train: loss: 0.0015312
[Epoch 53] ogbg-molbbbp: 0.938664 val loss: 0.916137
[Epoch 53] ogbg-molbbbp: 0.656443 test loss: 1.725849
[Epoch 54; Iter    25/   55] train: loss: 0.0041694
[Epoch 54; Iter    55/   55] train: loss: 0.0794023
[Epoch 54] ogbg-molbbbp: 0.944041 val loss: 0.675668
[Epoch 54] ogbg-molbbbp: 0.646894 test loss: 2.028695
[Epoch 55; Iter    30/   55] train: loss: 0.0045061
[Epoch 55] ogbg-molbbbp: 0.943244 val loss: 0.421436
[Epoch 55] ogbg-molbbbp: 0.667728 test loss: 1.643277
[Epoch 56; Iter     5/   55] train: loss: 0.0275379
[Epoch 56; Iter    35/   55] train: loss: 0.0706045
[Epoch 56] ogbg-molbbbp: 0.947725 val loss: 0.477725
[Epoch 56] ogbg-molbbbp: 0.624228 test loss: 2.695392
[Epoch 57; Iter    10/   55] train: loss: 0.0152700
[Epoch 57; Iter    40/   55] train: loss: 0.0113596
[Epoch 57] ogbg-molbbbp: 0.890869 val loss: 1.040587
[Epoch 57] ogbg-molbbbp: 0.644387 test loss: 2.043333
[Epoch 58; Iter    15/   55] train: loss: 0.1019722
[Epoch 58; Iter    45/   55] train: loss: 0.0255431
[Epoch 58] ogbg-molbbbp: 0.940356 val loss: 0.552919
[Epoch 58] ogbg-molbbbp: 0.651524 test loss: 1.794826
[Epoch 59; Iter    20/   55] train: loss: 0.0015341
[Epoch 59; Iter    50/   55] train: loss: 0.0258907
[Epoch 59] ogbg-molbbbp: 0.957682 val loss: 0.377641
[Epoch 59] ogbg-molbbbp: 0.669271 test loss: 1.700347
[Epoch 60; Iter    25/   55] train: loss: 0.0256352
[Epoch 60; Iter    55/   55] train: loss: 0.0073750
[Epoch 60] ogbg-molbbbp: 0.950314 val loss: 0.533157
[Epoch 60] ogbg-molbbbp: 0.656443 test loss: 2.061362
[Epoch 61; Iter    30/   55] train: loss: 0.0300135
[Epoch 61] ogbg-molbbbp: 0.941551 val loss: 0.546653
[Epoch 61] ogbg-molbbbp: 0.655768 test loss: 1.918670
[Epoch 62; Iter     5/   55] train: loss: 0.0018465
[Epoch 62; Iter    35/   55] train: loss: 0.0018160
[Epoch 62] ogbg-molbbbp: 0.915065 val loss: 0.650820
[Epoch 62] ogbg-molbbbp: 0.658179 test loss: 1.881489
[Epoch 63; Iter    10/   55] train: loss: 0.0108549
[Epoch 63; Iter    40/   55] train: loss: 0.0024464
[Epoch 63] ogbg-molbbbp: 0.935677 val loss: 0.532290
[Epoch 63] ogbg-molbbbp: 0.661844 test loss: 1.913709
[Epoch 64; Iter    15/   55] train: loss: 0.0013371
[Epoch 64; Iter    45/   55] train: loss: 0.0015653
[Epoch 64] ogbg-molbbbp: 0.940058 val loss: 0.526754
[Epoch 64] ogbg-molbbbp: 0.660687 test loss: 1.962871
[Epoch 65; Iter    20/   55] train: loss: 0.0006510
[Epoch 65; Iter    50/   55] train: loss: 0.0007290
[Epoch 65] ogbg-molbbbp: 0.932192 val loss: 0.585453
[Epoch 65] ogbg-molbbbp: 0.657215 test loss: 2.042638
[Epoch 66; Iter    25/   55] train: loss: 0.0010626
[Epoch 66; Iter    55/   55] train: loss: 0.0015259
[Epoch 66] ogbg-molbbbp: 0.923728 val loss: 1.005065
[Epoch 66] ogbg-molbbbp: 0.674383 test loss: 2.057284
[Epoch 67; Iter    30/   55] train: loss: 0.0007063
[Epoch 67] ogbg-molbbbp: 0.927014 val loss: 0.873204
[Epoch 67] ogbg-molbbbp: 0.672261 test loss: 2.056208
[Epoch 68; Iter     5/   55] train: loss: 0.0021702
[Epoch 68; Iter    35/   55] train: loss: 0.0004707
[Epoch 68] ogbg-molbbbp: 0.931594 val loss: 0.814852
[Epoch 68] ogbg-molbbbp: 0.670428 test loss: 2.073938
[Epoch 69; Iter    10/   55] train: loss: 0.0004609
[Epoch 28] ogbg-molbbbp: 0.657600 test loss: 1.192557
[Epoch 29; Iter    20/   55] train: loss: 0.2197811
[Epoch 29; Iter    50/   55] train: loss: 0.2870199
[Epoch 29] ogbg-molbbbp: 0.920243 val loss: 0.386402
[Epoch 29] ogbg-molbbbp: 0.647280 test loss: 0.863372
[Epoch 30; Iter    25/   55] train: loss: 0.1981687
[Epoch 30; Iter    55/   55] train: loss: 0.1335747
[Epoch 30] ogbg-molbbbp: 0.942348 val loss: 0.413004
[Epoch 30] ogbg-molbbbp: 0.638407 test loss: 1.442622
[Epoch 31; Iter    30/   55] train: loss: 0.4715517
[Epoch 31] ogbg-molbbbp: 0.924923 val loss: 0.466710
[Epoch 31] ogbg-molbbbp: 0.531346 test loss: 1.839162
[Epoch 32; Iter     5/   55] train: loss: 0.0697600
[Epoch 32; Iter    35/   55] train: loss: 0.1806792
[Epoch 32] ogbg-molbbbp: 0.918650 val loss: 0.397423
[Epoch 32] ogbg-molbbbp: 0.685378 test loss: 0.860382
[Epoch 33; Iter    10/   55] train: loss: 0.1093026
[Epoch 33; Iter    40/   55] train: loss: 0.0844956
[Epoch 33] ogbg-molbbbp: 0.932789 val loss: 0.627679
[Epoch 33] ogbg-molbbbp: 0.645062 test loss: 1.809879
[Epoch 34; Iter    15/   55] train: loss: 0.2503839
[Epoch 34; Iter    45/   55] train: loss: 0.0982010
[Epoch 34] ogbg-molbbbp: 0.814996 val loss: 1.170622
[Epoch 34] ogbg-molbbbp: 0.641590 test loss: 1.407263
[Epoch 35; Iter    20/   55] train: loss: 0.0919105
[Epoch 35; Iter    50/   55] train: loss: 0.1017935
[Epoch 35] ogbg-molbbbp: 0.931594 val loss: 0.346494
[Epoch 35] ogbg-molbbbp: 0.668692 test loss: 1.097387
[Epoch 36; Iter    25/   55] train: loss: 0.2934892
[Epoch 36; Iter    55/   55] train: loss: 0.0417307
[Epoch 36] ogbg-molbbbp: 0.947924 val loss: 0.441612
[Epoch 36] ogbg-molbbbp: 0.629340 test loss: 1.478466
[Epoch 37; Iter    30/   55] train: loss: 0.1995603
[Epoch 37] ogbg-molbbbp: 0.921737 val loss: 0.478139
[Epoch 37] ogbg-molbbbp: 0.671103 test loss: 1.254027
[Epoch 38; Iter     5/   55] train: loss: 0.1001007
[Epoch 38; Iter    35/   55] train: loss: 0.0403656
[Epoch 38] ogbg-molbbbp: 0.945235 val loss: 0.367983
[Epoch 38] ogbg-molbbbp: 0.600887 test loss: 1.626422
[Epoch 39; Iter    10/   55] train: loss: 0.0758775
[Epoch 39; Iter    40/   55] train: loss: 0.1301700
[Epoch 39] ogbg-molbbbp: 0.948920 val loss: 0.365685
[Epoch 39] ogbg-molbbbp: 0.642747 test loss: 1.618373
[Epoch 40; Iter    15/   55] train: loss: 0.0651070
[Epoch 40; Iter    45/   55] train: loss: 0.0327951
[Epoch 40] ogbg-molbbbp: 0.928109 val loss: 0.584309
[Epoch 40] ogbg-molbbbp: 0.615741 test loss: 1.893020
[Epoch 41; Iter    20/   55] train: loss: 0.2169915
[Epoch 41; Iter    50/   55] train: loss: 0.0656985
[Epoch 41] ogbg-molbbbp: 0.897341 val loss: 0.722677
[Epoch 41] ogbg-molbbbp: 0.661265 test loss: 1.468171
[Epoch 42; Iter    25/   55] train: loss: 0.2002123
[Epoch 42; Iter    55/   55] train: loss: 0.1633236
[Epoch 42] ogbg-molbbbp: 0.955691 val loss: 0.409023
[Epoch 42] ogbg-molbbbp: 0.654900 test loss: 1.354380
[Epoch 43; Iter    30/   55] train: loss: 0.0451558
[Epoch 43] ogbg-molbbbp: 0.937071 val loss: 0.565660
[Epoch 43] ogbg-molbbbp: 0.604938 test loss: 2.119897
[Epoch 44; Iter     5/   55] train: loss: 0.0841497
[Epoch 44; Iter    35/   55] train: loss: 0.0535675
[Epoch 44] ogbg-molbbbp: 0.895848 val loss: 4.310265
[Epoch 44] ogbg-molbbbp: 0.660108 test loss: 5.162422
[Epoch 45; Iter    10/   55] train: loss: 0.0277956
[Epoch 45; Iter    40/   55] train: loss: 0.0334074
[Epoch 45] ogbg-molbbbp: 0.941352 val loss: 0.439639
[Epoch 45] ogbg-molbbbp: 0.657118 test loss: 1.841294
[Epoch 46; Iter    15/   55] train: loss: 0.0751173
[Epoch 46; Iter    45/   55] train: loss: 0.1855841
[Epoch 46] ogbg-molbbbp: 0.935477 val loss: 0.494167
[Epoch 46] ogbg-molbbbp: 0.644965 test loss: 1.813081
[Epoch 47; Iter    20/   55] train: loss: 0.0387920
[Epoch 47; Iter    50/   55] train: loss: 0.0387994
[Epoch 47] ogbg-molbbbp: 0.904610 val loss: 0.956844
[Epoch 47] ogbg-molbbbp: 0.626254 test loss: 2.464636
[Epoch 48; Iter    25/   55] train: loss: 0.0077002
[Epoch 48; Iter    55/   55] train: loss: 0.1647407
[Epoch 48] ogbg-molbbbp: 0.947924 val loss: 0.523317
[Epoch 48] ogbg-molbbbp: 0.613329 test loss: 2.433117
[Epoch 49; Iter    30/   55] train: loss: 0.0459169
[Epoch 49] ogbg-molbbbp: 0.934283 val loss: 0.624318
[Epoch 49] ogbg-molbbbp: 0.648341 test loss: 2.575146
[Epoch 50; Iter     5/   55] train: loss: 0.0084253
[Epoch 50; Iter    35/   55] train: loss: 0.0126588
[Epoch 50] ogbg-molbbbp: 0.944240 val loss: 0.609781
[Epoch 50] ogbg-molbbbp: 0.574942 test loss: 2.570428
[Epoch 51; Iter    10/   55] train: loss: 0.0164236
[Epoch 51; Iter    40/   55] train: loss: 0.0265024
[Epoch 51] ogbg-molbbbp: 0.938763 val loss: 0.622279
[Epoch 51] ogbg-molbbbp: 0.671971 test loss: 2.117674
[Epoch 52; Iter    15/   55] train: loss: 0.0478680
[Epoch 52; Iter    45/   55] train: loss: 0.0312839
[Epoch 52] ogbg-molbbbp: 0.946928 val loss: 0.580837
[Epoch 52] ogbg-molbbbp: 0.629437 test loss: 2.437839
[Epoch 53; Iter    20/   55] train: loss: 0.1532227
[Epoch 53; Iter    50/   55] train: loss: 0.0307307
[Epoch 53] ogbg-molbbbp: 0.930698 val loss: 0.871766
[Epoch 53] ogbg-molbbbp: 0.606096 test loss: 2.205464
[Epoch 54; Iter    25/   55] train: loss: 0.0259027
[Epoch 54; Iter    55/   55] train: loss: 0.0293090
[Epoch 54] ogbg-molbbbp: 0.940556 val loss: 0.551473
[Epoch 54] ogbg-molbbbp: 0.641782 test loss: 2.243580
[Epoch 55; Iter    30/   55] train: loss: 0.0051776
[Epoch 55] ogbg-molbbbp: 0.963855 val loss: 0.383872
[Epoch 55] ogbg-molbbbp: 0.657890 test loss: 2.037912
[Epoch 56; Iter     5/   55] train: loss: 0.0279052
[Epoch 56; Iter    35/   55] train: loss: 0.0539494
[Epoch 56] ogbg-molbbbp: 0.944140 val loss: 0.681000
[Epoch 56] ogbg-molbbbp: 0.572724 test loss: 2.947354
[Epoch 57; Iter    10/   55] train: loss: 0.0582917
[Epoch 57; Iter    40/   55] train: loss: 0.0671051
[Epoch 57] ogbg-molbbbp: 0.953002 val loss: 0.561439
[Epoch 57] ogbg-molbbbp: 0.668885 test loss: 2.591908
[Epoch 58; Iter    15/   55] train: loss: 0.0082078
[Epoch 58; Iter    45/   55] train: loss: 0.0330080
[Epoch 58] ogbg-molbbbp: 0.938863 val loss: 0.587101
[Epoch 58] ogbg-molbbbp: 0.674865 test loss: 1.628519
[Epoch 59; Iter    20/   55] train: loss: 0.1792559
[Epoch 59; Iter    50/   55] train: loss: 0.1106739
[Epoch 59] ogbg-molbbbp: 0.898337 val loss: 1.031973
[Epoch 59] ogbg-molbbbp: 0.590953 test loss: 2.549216
[Epoch 60; Iter    25/   55] train: loss: 0.0104707
[Epoch 60; Iter    55/   55] train: loss: 0.0119810
[Epoch 60] ogbg-molbbbp: 0.954197 val loss: 0.504049
[Epoch 60] ogbg-molbbbp: 0.649595 test loss: 2.310559
[Epoch 61; Iter    30/   55] train: loss: 0.0070339
[Epoch 61] ogbg-molbbbp: 0.953102 val loss: 0.575940
[Epoch 61] ogbg-molbbbp: 0.650752 test loss: 2.586024
[Epoch 62; Iter     5/   55] train: loss: 0.0081451
[Epoch 62; Iter    35/   55] train: loss: 0.0063031
[Epoch 62] ogbg-molbbbp: 0.952703 val loss: 0.549580
[Epoch 62] ogbg-molbbbp: 0.638696 test loss: 2.583923
[Epoch 63; Iter    10/   55] train: loss: 0.0111560
[Epoch 63; Iter    40/   55] train: loss: 0.0152132
[Epoch 63] ogbg-molbbbp: 0.953600 val loss: 0.560617
[Epoch 63] ogbg-molbbbp: 0.634742 test loss: 2.704040
[Epoch 64; Iter    15/   55] train: loss: 0.0015954
[Epoch 64; Iter    45/   55] train: loss: 0.0038290
[Epoch 64] ogbg-molbbbp: 0.953799 val loss: 0.549351
[Epoch 64] ogbg-molbbbp: 0.638503 test loss: 2.674490
[Epoch 65; Iter    20/   55] train: loss: 0.0009123
[Epoch 65; Iter    50/   55] train: loss: 0.0019973
[Epoch 65] ogbg-molbbbp: 0.950712 val loss: 0.591180
[Epoch 65] ogbg-molbbbp: 0.626061 test loss: 2.891566
[Epoch 66; Iter    25/   55] train: loss: 0.0014965
[Epoch 66; Iter    55/   55] train: loss: 0.0137953
[Epoch 66] ogbg-molbbbp: 0.951807 val loss: 0.589111
[Epoch 66] ogbg-molbbbp: 0.637442 test loss: 2.839292
[Epoch 67; Iter    30/   55] train: loss: 0.0039900
[Epoch 67] ogbg-molbbbp: 0.953400 val loss: 0.574310
[Epoch 67] ogbg-molbbbp: 0.635417 test loss: 2.788692
[Epoch 68; Iter     5/   55] train: loss: 0.0008169
[Epoch 68; Iter    35/   55] train: loss: 0.0006347
[Epoch 68] ogbg-molbbbp: 0.952703 val loss: 0.588470
[Epoch 68] ogbg-molbbbp: 0.635706 test loss: 2.844423
[Epoch 69; Iter    10/   55] train: loss: 0.0028533
[Epoch 28] ogbg-molbbbp: 0.685089 test loss: 1.014865
[Epoch 29; Iter    20/   55] train: loss: 0.1915030
[Epoch 29; Iter    50/   55] train: loss: 0.2845407
[Epoch 29] ogbg-molbbbp: 0.897640 val loss: 0.613528
[Epoch 29] ogbg-molbbbp: 0.613137 test loss: 1.639695
[Epoch 30; Iter    25/   55] train: loss: 0.2238653
[Epoch 30; Iter    55/   55] train: loss: 0.3052304
[Epoch 30] ogbg-molbbbp: 0.943144 val loss: 0.389066
[Epoch 30] ogbg-molbbbp: 0.652874 test loss: 1.312892
[Epoch 31; Iter    30/   55] train: loss: 0.2983275
[Epoch 31] ogbg-molbbbp: 0.949318 val loss: 0.287860
[Epoch 31] ogbg-molbbbp: 0.662809 test loss: 0.950011
[Epoch 32; Iter     5/   55] train: loss: 0.2305434
[Epoch 32; Iter    35/   55] train: loss: 0.2431732
[Epoch 32] ogbg-molbbbp: 0.920143 val loss: 0.483103
[Epoch 32] ogbg-molbbbp: 0.657697 test loss: 1.459876
[Epoch 33; Iter    10/   55] train: loss: 0.0932757
[Epoch 33; Iter    40/   55] train: loss: 0.3567525
[Epoch 33] ogbg-molbbbp: 0.868067 val loss: 0.859154
[Epoch 33] ogbg-molbbbp: 0.631269 test loss: 1.444764
[Epoch 34; Iter    15/   55] train: loss: 0.1919861
[Epoch 34; Iter    45/   55] train: loss: 0.1794964
[Epoch 34] ogbg-molbbbp: 0.931793 val loss: 0.436334
[Epoch 34] ogbg-molbbbp: 0.645544 test loss: 1.288262
[Epoch 35; Iter    20/   55] train: loss: 0.1007646
[Epoch 35; Iter    50/   55] train: loss: 0.0937619
[Epoch 35] ogbg-molbbbp: 0.925620 val loss: 0.507083
[Epoch 35] ogbg-molbbbp: 0.657890 test loss: 1.414738
[Epoch 36; Iter    25/   55] train: loss: 0.1745379
[Epoch 36; Iter    55/   55] train: loss: 0.2058932
[Epoch 36] ogbg-molbbbp: 0.918451 val loss: 0.525278
[Epoch 36] ogbg-molbbbp: 0.620081 test loss: 1.592085
[Epoch 37; Iter    30/   55] train: loss: 0.0728399
[Epoch 37] ogbg-molbbbp: 0.935876 val loss: 0.451984
[Epoch 37] ogbg-molbbbp: 0.649402 test loss: 1.261382
[Epoch 38; Iter     5/   55] train: loss: 0.0397577
[Epoch 38; Iter    35/   55] train: loss: 0.1487978
[Epoch 38] ogbg-molbbbp: 0.935577 val loss: 0.417249
[Epoch 38] ogbg-molbbbp: 0.663098 test loss: 1.532394
[Epoch 39; Iter    10/   55] train: loss: 0.0474520
[Epoch 39; Iter    40/   55] train: loss: 0.1772528
[Epoch 39] ogbg-molbbbp: 0.932391 val loss: 0.521711
[Epoch 39] ogbg-molbbbp: 0.674093 test loss: 1.470931
[Epoch 40; Iter    15/   55] train: loss: 0.0172882
[Epoch 40; Iter    45/   55] train: loss: 0.1242227
[Epoch 40] ogbg-molbbbp: 0.949119 val loss: 0.405982
[Epoch 40] ogbg-molbbbp: 0.642458 test loss: 1.720275
[Epoch 41; Iter    20/   55] train: loss: 0.0821981
[Epoch 41; Iter    50/   55] train: loss: 0.0484751
[Epoch 41] ogbg-molbbbp: 0.943642 val loss: 0.725444
[Epoch 41] ogbg-molbbbp: 0.693769 test loss: 1.531580
[Epoch 42; Iter    25/   55] train: loss: 0.0685895
[Epoch 42; Iter    55/   55] train: loss: 0.0153384
[Epoch 42] ogbg-molbbbp: 0.926018 val loss: 0.564364
[Epoch 42] ogbg-molbbbp: 0.649788 test loss: 1.889354
[Epoch 43; Iter    30/   55] train: loss: 0.0917590
[Epoch 43] ogbg-molbbbp: 0.938863 val loss: 0.407329
[Epoch 43] ogbg-molbbbp: 0.691358 test loss: 1.265703
[Epoch 44; Iter     5/   55] train: loss: 0.3033935
[Epoch 44; Iter    35/   55] train: loss: 0.0419089
[Epoch 44] ogbg-molbbbp: 0.938863 val loss: 0.493246
[Epoch 44] ogbg-molbbbp: 0.670525 test loss: 1.546604
[Epoch 45; Iter    10/   55] train: loss: 0.0199817
[Epoch 45; Iter    40/   55] train: loss: 0.0317317
[Epoch 45] ogbg-molbbbp: 0.910485 val loss: 0.616127
[Epoch 45] ogbg-molbbbp: 0.656636 test loss: 1.587826
[Epoch 46; Iter    15/   55] train: loss: 0.0210561
[Epoch 46; Iter    45/   55] train: loss: 0.0233419
[Epoch 46] ogbg-molbbbp: 0.929304 val loss: 0.544221
[Epoch 46] ogbg-molbbbp: 0.701100 test loss: 1.395867
[Epoch 47; Iter    20/   55] train: loss: 0.0119950
[Epoch 47; Iter    50/   55] train: loss: 0.0648773
[Epoch 47] ogbg-molbbbp: 0.929005 val loss: 0.547877
[Epoch 47] ogbg-molbbbp: 0.674865 test loss: 1.467490
[Epoch 48; Iter    25/   55] train: loss: 0.0192284
[Epoch 48; Iter    55/   55] train: loss: 0.0262517
[Epoch 48] ogbg-molbbbp: 0.936971 val loss: 0.483353
[Epoch 48] ogbg-molbbbp: 0.690297 test loss: 1.471261
[Epoch 49; Iter    30/   55] train: loss: 0.0101959
[Epoch 49] ogbg-molbbbp: 0.938564 val loss: 0.491878
[Epoch 49] ogbg-molbbbp: 0.695698 test loss: 1.563232
[Epoch 50; Iter     5/   55] train: loss: 0.0056379
[Epoch 50; Iter    35/   55] train: loss: 0.0123984
[Epoch 50] ogbg-molbbbp: 0.914169 val loss: 0.656254
[Epoch 50] ogbg-molbbbp: 0.694059 test loss: 1.556919
[Epoch 51; Iter    10/   55] train: loss: 0.0084443
[Epoch 51; Iter    40/   55] train: loss: 0.0234346
[Epoch 51] ogbg-molbbbp: 0.953301 val loss: 0.413977
[Epoch 51] ogbg-molbbbp: 0.703993 test loss: 1.515915
[Epoch 52; Iter    15/   55] train: loss: 0.0778282
[Epoch 52; Iter    45/   55] train: loss: 0.0275845
[Epoch 52] ogbg-molbbbp: 0.917256 val loss: 0.553794
[Epoch 52] ogbg-molbbbp: 0.684606 test loss: 1.401882
[Epoch 53; Iter    20/   55] train: loss: 0.0066536
[Epoch 53; Iter    50/   55] train: loss: 0.0384774
[Epoch 53] ogbg-molbbbp: 0.920641 val loss: 0.565674
[Epoch 53] ogbg-molbbbp: 0.686921 test loss: 1.438688
[Epoch 54; Iter    25/   55] train: loss: 0.0773862
[Epoch 54; Iter    55/   55] train: loss: 0.0735448
[Epoch 54] ogbg-molbbbp: 0.943344 val loss: 0.424514
[Epoch 54] ogbg-molbbbp: 0.686632 test loss: 1.457310
[Epoch 55; Iter    30/   55] train: loss: 0.0143296
[Epoch 55] ogbg-molbbbp: 0.944538 val loss: 0.512107
[Epoch 55] ogbg-molbbbp: 0.697820 test loss: 1.536324
[Epoch 56; Iter     5/   55] train: loss: 0.0628242
[Epoch 56; Iter    35/   55] train: loss: 0.0081987
[Epoch 56] ogbg-molbbbp: 0.941750 val loss: 0.536639
[Epoch 56] ogbg-molbbbp: 0.659144 test loss: 1.878679
[Epoch 57; Iter    10/   55] train: loss: 0.0082055
[Epoch 57; Iter    40/   55] train: loss: 0.0556488
[Epoch 57] ogbg-molbbbp: 0.942049 val loss: 0.516790
[Epoch 57] ogbg-molbbbp: 0.702836 test loss: 1.655957
[Epoch 58; Iter    15/   55] train: loss: 0.0101053
[Epoch 58; Iter    45/   55] train: loss: 0.0044655
[Epoch 58] ogbg-molbbbp: 0.932391 val loss: 0.553606
[Epoch 58] ogbg-molbbbp: 0.678723 test loss: 1.740315
[Epoch 59; Iter    20/   55] train: loss: 0.0068042
[Epoch 59; Iter    50/   55] train: loss: 0.0048366
[Epoch 59] ogbg-molbbbp: 0.947725 val loss: 0.512233
[Epoch 59] ogbg-molbbbp: 0.670525 test loss: 1.942484
[Epoch 60; Iter    25/   55] train: loss: 0.0106830
[Epoch 60; Iter    55/   55] train: loss: 0.0511409
[Epoch 60] ogbg-molbbbp: 0.922633 val loss: 0.755002
[Epoch 60] ogbg-molbbbp: 0.662616 test loss: 2.097928
[Epoch 61; Iter    30/   55] train: loss: 0.0030234
[Epoch 61] ogbg-molbbbp: 0.936772 val loss: 0.618741
[Epoch 61] ogbg-molbbbp: 0.673225 test loss: 2.063209
[Epoch 62; Iter     5/   55] train: loss: 0.0407451
[Epoch 62; Iter    35/   55] train: loss: 0.0749747
[Epoch 62] ogbg-molbbbp: 0.935677 val loss: 0.613520
[Epoch 62] ogbg-molbbbp: 0.670332 test loss: 2.031234
[Epoch 63; Iter    10/   55] train: loss: 0.0178786
[Epoch 63; Iter    40/   55] train: loss: 0.0008658
[Epoch 63] ogbg-molbbbp: 0.933187 val loss: 0.609160
[Epoch 63] ogbg-molbbbp: 0.663580 test loss: 1.931037
[Epoch 64; Iter    15/   55] train: loss: 0.0016059
[Epoch 64; Iter    45/   55] train: loss: 0.0046289
[Epoch 64] ogbg-molbbbp: 0.940556 val loss: 0.559198
[Epoch 64] ogbg-molbbbp: 0.675637 test loss: 1.866094
[Epoch 65; Iter    20/   55] train: loss: 0.0038574
[Epoch 65; Iter    50/   55] train: loss: 0.0009394
[Epoch 65] ogbg-molbbbp: 0.944240 val loss: 0.556168
[Epoch 65] ogbg-molbbbp: 0.679398 test loss: 1.878971
[Epoch 66; Iter    25/   55] train: loss: 0.0161924
[Epoch 66; Iter    55/   55] train: loss: 0.0002933
[Epoch 66] ogbg-molbbbp: 0.950812 val loss: 0.542046
[Epoch 66] ogbg-molbbbp: 0.675154 test loss: 1.946426
[Epoch 67; Iter    30/   55] train: loss: 0.0006093
[Epoch 67] ogbg-molbbbp: 0.949517 val loss: 0.475619
[Epoch 67] ogbg-molbbbp: 0.658758 test loss: 1.865627
[Epoch 68; Iter     5/   55] train: loss: 0.0049571
[Epoch 68; Iter    35/   55] train: loss: 0.0047665
[Epoch 68] ogbg-molbbbp: 0.947326 val loss: 0.499132
[Epoch 68] ogbg-molbbbp: 0.680556 test loss: 1.773059
[Epoch 69; Iter    10/   55] train: loss: 0.0052633
[Epoch 28] ogbg-molbbbp: 0.628858 test loss: 3.207407
[Epoch 29; Iter    20/   55] train: loss: 0.1980190
[Epoch 29; Iter    50/   55] train: loss: 0.2995250
[Epoch 29] ogbg-molbbbp: 0.874340 val loss: 1.660957
[Epoch 29] ogbg-molbbbp: 0.639660 test loss: 1.270640
[Epoch 30; Iter    25/   55] train: loss: 0.2200360
[Epoch 30; Iter    55/   55] train: loss: 0.3490955
[Epoch 30] ogbg-molbbbp: 0.881111 val loss: 1.486270
[Epoch 30] ogbg-molbbbp: 0.627604 test loss: 1.371731
[Epoch 31; Iter    30/   55] train: loss: 0.2656592
[Epoch 31] ogbg-molbbbp: 0.893956 val loss: 0.667898
[Epoch 31] ogbg-molbbbp: 0.600887 test loss: 1.511603
[Epoch 32; Iter     5/   55] train: loss: 0.2089949
[Epoch 32; Iter    35/   55] train: loss: 0.2294214
[Epoch 32] ogbg-molbbbp: 0.929404 val loss: 0.345817
[Epoch 32] ogbg-molbbbp: 0.629437 test loss: 1.149524
[Epoch 33; Iter    10/   55] train: loss: 0.1648496
[Epoch 33; Iter    40/   55] train: loss: 0.3129322
[Epoch 33] ogbg-molbbbp: 0.922334 val loss: 0.977561
[Epoch 33] ogbg-molbbbp: 0.639178 test loss: 1.772124
[Epoch 34; Iter    15/   55] train: loss: 0.1175158
[Epoch 34; Iter    45/   55] train: loss: 0.1453611
[Epoch 34] ogbg-molbbbp: 0.924325 val loss: 0.488284
[Epoch 34] ogbg-molbbbp: 0.570120 test loss: 2.502258
[Epoch 35; Iter    20/   55] train: loss: 0.0782575
[Epoch 35; Iter    50/   55] train: loss: 0.0741099
[Epoch 35] ogbg-molbbbp: 0.899532 val loss: 1.123117
[Epoch 35] ogbg-molbbbp: 0.613812 test loss: 4.020096
[Epoch 36; Iter    25/   55] train: loss: 0.0994314
[Epoch 36; Iter    55/   55] train: loss: 0.3508219
[Epoch 36] ogbg-molbbbp: 0.922434 val loss: 0.444284
[Epoch 36] ogbg-molbbbp: 0.652488 test loss: 2.705253
[Epoch 37; Iter    30/   55] train: loss: 0.0623150
[Epoch 37] ogbg-molbbbp: 0.909290 val loss: 2.401946
[Epoch 37] ogbg-molbbbp: 0.655382 test loss: 5.141404
[Epoch 38; Iter     5/   55] train: loss: 0.0743352
[Epoch 38; Iter    35/   55] train: loss: 0.0897501
[Epoch 38] ogbg-molbbbp: 0.927512 val loss: 0.468485
[Epoch 38] ogbg-molbbbp: 0.622492 test loss: 2.586476
[Epoch 39; Iter    10/   55] train: loss: 0.0267798
[Epoch 39; Iter    40/   55] train: loss: 0.0789828
[Epoch 39] ogbg-molbbbp: 0.937469 val loss: 0.457464
[Epoch 39] ogbg-molbbbp: 0.662423 test loss: 3.232795
[Epoch 40; Iter    15/   55] train: loss: 0.0180248
[Epoch 40; Iter    45/   55] train: loss: 0.1658607
[Epoch 40] ogbg-molbbbp: 0.897939 val loss: 0.513188
[Epoch 40] ogbg-molbbbp: 0.627701 test loss: 1.799310
[Epoch 41; Iter    20/   55] train: loss: 0.0311542
[Epoch 41; Iter    50/   55] train: loss: 0.0314310
[Epoch 41] ogbg-molbbbp: 0.930200 val loss: 0.740869
[Epoch 41] ogbg-molbbbp: 0.655671 test loss: 2.288812
[Epoch 42; Iter    25/   55] train: loss: 0.0373460
[Epoch 42; Iter    55/   55] train: loss: 0.0141574
[Epoch 42] ogbg-molbbbp: 0.950015 val loss: 0.379522
[Epoch 42] ogbg-molbbbp: 0.658275 test loss: 1.755953
[Epoch 43; Iter    30/   55] train: loss: 0.0128641
[Epoch 43] ogbg-molbbbp: 0.919048 val loss: 0.623787
[Epoch 43] ogbg-molbbbp: 0.625482 test loss: 2.639295
[Epoch 44; Iter     5/   55] train: loss: 0.2736083
[Epoch 44; Iter    35/   55] train: loss: 0.0070720
[Epoch 44] ogbg-molbbbp: 0.935975 val loss: 0.450127
[Epoch 44] ogbg-molbbbp: 0.673708 test loss: 1.731635
[Epoch 45; Iter    10/   55] train: loss: 0.0094953
[Epoch 45; Iter    40/   55] train: loss: 0.0140574
[Epoch 45] ogbg-molbbbp: 0.944937 val loss: 0.425735
[Epoch 45] ogbg-molbbbp: 0.640914 test loss: 2.356335
[Epoch 46; Iter    15/   55] train: loss: 0.0253575
[Epoch 46; Iter    45/   55] train: loss: 0.0084649
[Epoch 46] ogbg-molbbbp: 0.946132 val loss: 0.424391
[Epoch 46] ogbg-molbbbp: 0.639853 test loss: 2.474417
[Epoch 47; Iter    20/   55] train: loss: 0.0231333
[Epoch 47; Iter    50/   55] train: loss: 0.0173203
[Epoch 47] ogbg-molbbbp: 0.931395 val loss: 0.617217
[Epoch 47] ogbg-molbbbp: 0.650752 test loss: 2.385375
[Epoch 48; Iter    25/   55] train: loss: 0.0068248
[Epoch 48; Iter    55/   55] train: loss: 0.0309405
[Epoch 48] ogbg-molbbbp: 0.950314 val loss: 0.738657
[Epoch 48] ogbg-molbbbp: 0.617284 test loss: 2.560558
[Epoch 49; Iter    30/   55] train: loss: 0.0063649
[Epoch 49] ogbg-molbbbp: 0.933884 val loss: 0.818268
[Epoch 49] ogbg-molbbbp: 0.618827 test loss: 2.783402
[Epoch 50; Iter     5/   55] train: loss: 0.0188576
[Epoch 50; Iter    35/   55] train: loss: 0.1350757
[Epoch 50] ogbg-molbbbp: 0.915464 val loss: 0.540092
[Epoch 50] ogbg-molbbbp: 0.656925 test loss: 1.676061
[Epoch 51; Iter    10/   55] train: loss: 0.0112984
[Epoch 51; Iter    40/   55] train: loss: 0.0494675
[Epoch 51] ogbg-molbbbp: 0.919247 val loss: 0.465979
[Epoch 51] ogbg-molbbbp: 0.628762 test loss: 1.598202
[Epoch 52; Iter    15/   55] train: loss: 0.0039331
[Epoch 52; Iter    45/   55] train: loss: 0.0209885
[Epoch 52] ogbg-molbbbp: 0.937170 val loss: 0.535854
[Epoch 52] ogbg-molbbbp: 0.662519 test loss: 2.390446
[Epoch 53; Iter    20/   55] train: loss: 0.0076162
[Epoch 53; Iter    50/   55] train: loss: 0.0144575
[Epoch 53] ogbg-molbbbp: 0.919247 val loss: 0.693344
[Epoch 53] ogbg-molbbbp: 0.610243 test loss: 2.045054
[Epoch 54; Iter    25/   55] train: loss: 0.0098110
[Epoch 54; Iter    55/   55] train: loss: 0.0077360
[Epoch 54] ogbg-molbbbp: 0.920442 val loss: 0.490087
[Epoch 54] ogbg-molbbbp: 0.642650 test loss: 1.674583
[Epoch 55; Iter    30/   55] train: loss: 0.0025217
[Epoch 55] ogbg-molbbbp: 0.940157 val loss: 0.473782
[Epoch 55] ogbg-molbbbp: 0.659240 test loss: 1.807705
[Epoch 56; Iter     5/   55] train: loss: 0.0550871
[Epoch 56; Iter    35/   55] train: loss: 0.0057391
[Epoch 56] ogbg-molbbbp: 0.939659 val loss: 0.504409
[Epoch 56] ogbg-molbbbp: 0.651524 test loss: 1.642577
[Epoch 57; Iter    10/   55] train: loss: 0.0017888
[Epoch 57; Iter    40/   55] train: loss: 0.0523307
[Epoch 57] ogbg-molbbbp: 0.949816 val loss: 0.364391
[Epoch 57] ogbg-molbbbp: 0.662519 test loss: 1.633496
[Epoch 58; Iter    15/   55] train: loss: 0.0696097
[Epoch 58; Iter    45/   55] train: loss: 0.0036466
[Epoch 58] ogbg-molbbbp: 0.955691 val loss: 0.420665
[Epoch 58] ogbg-molbbbp: 0.663677 test loss: 1.835300
[Epoch 59; Iter    20/   55] train: loss: 0.0183454
[Epoch 59; Iter    50/   55] train: loss: 0.0696056
[Epoch 59] ogbg-molbbbp: 0.950513 val loss: 0.400773
[Epoch 59] ogbg-molbbbp: 0.667535 test loss: 2.238439
[Epoch 60; Iter    25/   55] train: loss: 0.0353326
[Epoch 60; Iter    55/   55] train: loss: 0.0236262
[Epoch 60] ogbg-molbbbp: 0.919745 val loss: 0.663292
[Epoch 60] ogbg-molbbbp: 0.637731 test loss: 2.359320
[Epoch 61; Iter    30/   55] train: loss: 0.0094066
[Epoch 61] ogbg-molbbbp: 0.943344 val loss: 0.626348
[Epoch 61] ogbg-molbbbp: 0.654707 test loss: 1.996246
[Epoch 62; Iter     5/   55] train: loss: 0.0434799
[Epoch 62; Iter    35/   55] train: loss: 0.2547273
[Epoch 62] ogbg-molbbbp: 0.936374 val loss: 0.556009
[Epoch 62] ogbg-molbbbp: 0.634259 test loss: 2.137302
[Epoch 63; Iter    10/   55] train: loss: 0.0208815
[Epoch 63; Iter    40/   55] train: loss: 0.0400543
[Epoch 63] ogbg-molbbbp: 0.940655 val loss: 0.656326
[Epoch 63] ogbg-molbbbp: 0.665123 test loss: 1.810020
[Epoch 64; Iter    15/   55] train: loss: 0.0042749
[Epoch 64; Iter    45/   55] train: loss: 0.0084853
[Epoch 64] ogbg-molbbbp: 0.924325 val loss: 0.543068
[Epoch 64] ogbg-molbbbp: 0.627894 test loss: 1.924403
[Epoch 65; Iter    20/   55] train: loss: 0.0314124
[Epoch 65; Iter    50/   55] train: loss: 0.0015140
[Epoch 65] ogbg-molbbbp: 0.955989 val loss: 0.613975
[Epoch 65] ogbg-molbbbp: 0.667149 test loss: 1.984427
[Epoch 66; Iter    25/   55] train: loss: 0.0542414
[Epoch 66; Iter    55/   55] train: loss: 0.0009549
[Epoch 66] ogbg-molbbbp: 0.941352 val loss: 0.542023
[Epoch 66] ogbg-molbbbp: 0.633681 test loss: 2.225466
[Epoch 67; Iter    30/   55] train: loss: 0.0008835
[Epoch 67] ogbg-molbbbp: 0.948920 val loss: 0.550252
[Epoch 67] ogbg-molbbbp: 0.644579 test loss: 2.248225
[Epoch 68; Iter     5/   55] train: loss: 0.0045538
[Epoch 68; Iter    35/   55] train: loss: 0.0027239
[Epoch 68] ogbg-molbbbp: 0.947924 val loss: 0.486468
[Epoch 68] ogbg-molbbbp: 0.637731 test loss: 2.230789
[Epoch 69; Iter    10/   55] train: loss: 0.0017449
[Epoch 28] ogbg-molbbbp: 0.589603 test loss: 1.029241
[Epoch 29; Iter    20/   55] train: loss: 0.2222884
[Epoch 29; Iter    50/   55] train: loss: 0.2592580
[Epoch 29] ogbg-molbbbp: 0.879419 val loss: 1.541387
[Epoch 29] ogbg-molbbbp: 0.603106 test loss: 6.780721
[Epoch 30; Iter    25/   55] train: loss: 0.1557217
[Epoch 30; Iter    55/   55] train: loss: 0.2868144
[Epoch 30] ogbg-molbbbp: 0.919148 val loss: 0.679082
[Epoch 30] ogbg-molbbbp: 0.599441 test loss: 1.549621
[Epoch 31; Iter    30/   55] train: loss: 0.2767843
[Epoch 31] ogbg-molbbbp: 0.932689 val loss: 0.396791
[Epoch 31] ogbg-molbbbp: 0.619502 test loss: 1.206702
[Epoch 32; Iter     5/   55] train: loss: 0.1665644
[Epoch 32; Iter    35/   55] train: loss: 0.1035825
[Epoch 32] ogbg-molbbbp: 0.934780 val loss: 0.338686
[Epoch 32] ogbg-molbbbp: 0.625482 test loss: 1.193323
[Epoch 33; Iter    10/   55] train: loss: 0.1553262
[Epoch 33; Iter    40/   55] train: loss: 0.2862173
[Epoch 33] ogbg-molbbbp: 0.867669 val loss: 0.541882
[Epoch 33] ogbg-molbbbp: 0.625579 test loss: 1.093046
[Epoch 34; Iter    15/   55] train: loss: 0.1649761
[Epoch 34; Iter    45/   55] train: loss: 0.1853336
[Epoch 34] ogbg-molbbbp: 0.923828 val loss: 0.384851
[Epoch 34] ogbg-molbbbp: 0.630401 test loss: 1.375535
[Epoch 35; Iter    20/   55] train: loss: 0.1683314
[Epoch 35; Iter    50/   55] train: loss: 0.1107083
[Epoch 35] ogbg-molbbbp: 0.901026 val loss: 0.438873
[Epoch 35] ogbg-molbbbp: 0.617766 test loss: 1.238773
[Epoch 36; Iter    25/   55] train: loss: 0.1506535
[Epoch 36; Iter    55/   55] train: loss: 0.0858324
[Epoch 36] ogbg-molbbbp: 0.867271 val loss: 0.680962
[Epoch 36] ogbg-molbbbp: 0.594907 test loss: 1.499310
[Epoch 37; Iter    30/   55] train: loss: 0.1480511
[Epoch 37] ogbg-molbbbp: 0.945733 val loss: 0.395988
[Epoch 37] ogbg-molbbbp: 0.627025 test loss: 1.448479
[Epoch 38; Iter     5/   55] train: loss: 0.0969987
[Epoch 38; Iter    35/   55] train: loss: 0.0394990
[Epoch 38] ogbg-molbbbp: 0.947924 val loss: 0.380547
[Epoch 38] ogbg-molbbbp: 0.625965 test loss: 1.554638
[Epoch 39; Iter    10/   55] train: loss: 0.0339204
[Epoch 39; Iter    40/   55] train: loss: 0.0389933
[Epoch 39] ogbg-molbbbp: 0.947526 val loss: 0.389414
[Epoch 39] ogbg-molbbbp: 0.611304 test loss: 1.359671
[Epoch 40; Iter    15/   55] train: loss: 0.0241053
[Epoch 40; Iter    45/   55] train: loss: 0.0354545
[Epoch 40] ogbg-molbbbp: 0.950712 val loss: 0.515938
[Epoch 40] ogbg-molbbbp: 0.587963 test loss: 2.329907
[Epoch 41; Iter    20/   55] train: loss: 0.0438090
[Epoch 41; Iter    50/   55] train: loss: 0.0187092
[Epoch 41] ogbg-molbbbp: 0.926914 val loss: 0.545791
[Epoch 41] ogbg-molbbbp: 0.613812 test loss: 1.628852
[Epoch 42; Iter    25/   55] train: loss: 0.0322195
[Epoch 42; Iter    55/   55] train: loss: 0.0081738
[Epoch 42] ogbg-molbbbp: 0.933984 val loss: 0.421706
[Epoch 42] ogbg-molbbbp: 0.611497 test loss: 1.730850
[Epoch 43; Iter    30/   55] train: loss: 0.0261666
[Epoch 43] ogbg-molbbbp: 0.917853 val loss: 0.481193
[Epoch 43] ogbg-molbbbp: 0.605806 test loss: 1.642720
[Epoch 44; Iter     5/   55] train: loss: 0.0138568
[Epoch 44; Iter    35/   55] train: loss: 0.0349146
[Epoch 44] ogbg-molbbbp: 0.945833 val loss: 0.468523
[Epoch 44] ogbg-molbbbp: 0.566937 test loss: 1.988029
[Epoch 45; Iter    10/   55] train: loss: 0.0179047
[Epoch 45; Iter    40/   55] train: loss: 0.0189904
[Epoch 45] ogbg-molbbbp: 0.920243 val loss: 0.567452
[Epoch 45] ogbg-molbbbp: 0.592400 test loss: 2.145173
[Epoch 46; Iter    15/   55] train: loss: 0.0237577
[Epoch 46; Iter    45/   55] train: loss: 0.0434642
[Epoch 46] ogbg-molbbbp: 0.925620 val loss: 1.631727
[Epoch 46] ogbg-molbbbp: 0.611208 test loss: 3.935749
[Epoch 47; Iter    20/   55] train: loss: 0.0274495
[Epoch 47; Iter    50/   55] train: loss: 0.1284410
[Epoch 47] ogbg-molbbbp: 0.941750 val loss: 1.112203
[Epoch 47] ogbg-molbbbp: 0.608989 test loss: 2.652241
[Epoch 48; Iter    25/   55] train: loss: 0.0094905
[Epoch 48; Iter    55/   55] train: loss: 0.0156436
[Epoch 48] ogbg-molbbbp: 0.938365 val loss: 1.271954
[Epoch 48] ogbg-molbbbp: 0.627797 test loss: 5.198178
[Epoch 49; Iter    30/   55] train: loss: 0.0509911
[Epoch 49] ogbg-molbbbp: 0.936772 val loss: 0.501820
[Epoch 49] ogbg-molbbbp: 0.595486 test loss: 2.175186
[Epoch 50; Iter     5/   55] train: loss: 0.0526598
[Epoch 50; Iter    35/   55] train: loss: 0.0296145
[Epoch 50] ogbg-molbbbp: 0.936573 val loss: 0.551293
[Epoch 50] ogbg-molbbbp: 0.622782 test loss: 1.506506
[Epoch 51; Iter    10/   55] train: loss: 0.0350149
[Epoch 51; Iter    40/   55] train: loss: 0.0078329
[Epoch 51] ogbg-molbbbp: 0.943742 val loss: 0.342045
[Epoch 51] ogbg-molbbbp: 0.592014 test loss: 1.576100
[Epoch 52; Iter    15/   55] train: loss: 0.0086906
[Epoch 52; Iter    45/   55] train: loss: 0.0074131
[Epoch 52] ogbg-molbbbp: 0.941950 val loss: 0.574943
[Epoch 52] ogbg-molbbbp: 0.618538 test loss: 1.605213
[Epoch 53; Iter    20/   55] train: loss: 0.0024194
[Epoch 53; Iter    50/   55] train: loss: 0.0029867
[Epoch 53] ogbg-molbbbp: 0.957981 val loss: 0.451544
[Epoch 53] ogbg-molbbbp: 0.608700 test loss: 1.882409
[Epoch 54; Iter    25/   55] train: loss: 0.0032000
[Epoch 54; Iter    55/   55] train: loss: 0.1439054
[Epoch 54] ogbg-molbbbp: 0.936971 val loss: 0.646398
[Epoch 54] ogbg-molbbbp: 0.616030 test loss: 1.832724
[Epoch 55; Iter    30/   55] train: loss: 0.0881420
[Epoch 55] ogbg-molbbbp: 0.951409 val loss: 0.451164
[Epoch 55] ogbg-molbbbp: 0.622589 test loss: 1.870029
[Epoch 56; Iter     5/   55] train: loss: 0.0040280
[Epoch 56; Iter    35/   55] train: loss: 0.1100299
[Epoch 56] ogbg-molbbbp: 0.940755 val loss: 5.898245
[Epoch 56] ogbg-molbbbp: 0.664931 test loss: 7.331474
[Epoch 57; Iter    10/   55] train: loss: 0.0258552
[Epoch 57; Iter    40/   55] train: loss: 0.0078448
[Epoch 57] ogbg-molbbbp: 0.953201 val loss: 0.382612
[Epoch 57] ogbg-molbbbp: 0.614005 test loss: 4.443289
[Epoch 58; Iter    15/   55] train: loss: 0.1386876
[Epoch 58; Iter    45/   55] train: loss: 0.0392194
[Epoch 58] ogbg-molbbbp: 0.934681 val loss: 2.079905
[Epoch 58] ogbg-molbbbp: 0.623457 test loss: 7.181698
[Epoch 59; Iter    20/   55] train: loss: 0.0061274
[Epoch 59; Iter    50/   55] train: loss: 0.0254030
[Epoch 59] ogbg-molbbbp: 0.941352 val loss: 0.488963
[Epoch 59] ogbg-molbbbp: 0.601659 test loss: 2.276672
[Epoch 60; Iter    25/   55] train: loss: 0.0201390
[Epoch 60; Iter    55/   55] train: loss: 0.0076689
[Epoch 60] ogbg-molbbbp: 0.947924 val loss: 1.077809
[Epoch 60] ogbg-molbbbp: 0.633198 test loss: 3.265942
[Epoch 61; Iter    30/   55] train: loss: 0.0334707
[Epoch 61] ogbg-molbbbp: 0.954297 val loss: 0.409097
[Epoch 61] ogbg-molbbbp: 0.618056 test loss: 2.242896
[Epoch 62; Iter     5/   55] train: loss: 0.0029201
[Epoch 62; Iter    35/   55] train: loss: 0.0573201
[Epoch 62] ogbg-molbbbp: 0.939062 val loss: 0.587521
[Epoch 62] ogbg-molbbbp: 0.612076 test loss: 2.033047
[Epoch 63; Iter    10/   55] train: loss: 0.0132121
[Epoch 63; Iter    40/   55] train: loss: 0.0068727
[Epoch 63] ogbg-molbbbp: 0.947824 val loss: 0.447849
[Epoch 63] ogbg-molbbbp: 0.607157 test loss: 2.041007
[Epoch 64; Iter    15/   55] train: loss: 0.0005278
[Epoch 64; Iter    45/   55] train: loss: 0.0007589
[Epoch 64] ogbg-molbbbp: 0.949517 val loss: 0.464143
[Epoch 64] ogbg-molbbbp: 0.609471 test loss: 2.171909
[Epoch 65; Iter    20/   55] train: loss: 0.0010988
[Epoch 65; Iter    50/   55] train: loss: 0.0011190
[Epoch 65] ogbg-molbbbp: 0.947725 val loss: 0.464268
[Epoch 65] ogbg-molbbbp: 0.609568 test loss: 2.242968
[Epoch 66; Iter    25/   55] train: loss: 0.0014299
[Epoch 66; Iter    55/   55] train: loss: 0.0015694
[Epoch 66] ogbg-molbbbp: 0.948621 val loss: 0.425313
[Epoch 66] ogbg-molbbbp: 0.608121 test loss: 2.335140
[Epoch 67; Iter    30/   55] train: loss: 0.0003985
[Epoch 67] ogbg-molbbbp: 0.949318 val loss: 0.426966
[Epoch 67] ogbg-molbbbp: 0.610340 test loss: 2.367901
[Epoch 68; Iter     5/   55] train: loss: 0.0026386
[Epoch 68; Iter    35/   55] train: loss: 0.0007054
[Epoch 68] ogbg-molbbbp: 0.950413 val loss: 0.474905
[Epoch 68] ogbg-molbbbp: 0.624807 test loss: 2.203191
[Epoch 69; Iter    10/   55] train: loss: 0.0004675
[Epoch 28] ogbg-molbbbp: 0.661748 test loss: 0.897094
[Epoch 29; Iter    20/   55] train: loss: 0.2406542
[Epoch 29; Iter    50/   55] train: loss: 0.2429215
[Epoch 29] ogbg-molbbbp: 0.913273 val loss: 0.496870
[Epoch 29] ogbg-molbbbp: 0.619020 test loss: 1.268151
[Epoch 30; Iter    25/   55] train: loss: 0.1786081
[Epoch 30; Iter    55/   55] train: loss: 0.2293763
[Epoch 30] ogbg-molbbbp: 0.891268 val loss: 1.029030
[Epoch 30] ogbg-molbbbp: 0.684124 test loss: 0.985247
[Epoch 31; Iter    30/   55] train: loss: 0.3070003
[Epoch 31] ogbg-molbbbp: 0.925620 val loss: 0.417293
[Epoch 31] ogbg-molbbbp: 0.625675 test loss: 1.068183
[Epoch 32; Iter     5/   55] train: loss: 0.2357216
[Epoch 32; Iter    35/   55] train: loss: 0.1909348
[Epoch 32] ogbg-molbbbp: 0.915862 val loss: 0.810801
[Epoch 32] ogbg-molbbbp: 0.640818 test loss: 1.322224
[Epoch 33; Iter    10/   55] train: loss: 0.1014618
[Epoch 33; Iter    40/   55] train: loss: 0.3332610
[Epoch 33] ogbg-molbbbp: 0.872050 val loss: 0.904532
[Epoch 33] ogbg-molbbbp: 0.677566 test loss: 1.550842
[Epoch 34; Iter    15/   55] train: loss: 0.1194047
[Epoch 34; Iter    45/   55] train: loss: 0.2623730
[Epoch 34] ogbg-molbbbp: 0.941850 val loss: 0.335076
[Epoch 34] ogbg-molbbbp: 0.694444 test loss: 1.034716
[Epoch 35; Iter    20/   55] train: loss: 0.1123139
[Epoch 35; Iter    50/   55] train: loss: 0.1396262
[Epoch 35] ogbg-molbbbp: 0.940356 val loss: 0.639294
[Epoch 35] ogbg-molbbbp: 0.708044 test loss: 0.974239
[Epoch 36; Iter    25/   55] train: loss: 0.2394068
[Epoch 36; Iter    55/   55] train: loss: 0.4727387
[Epoch 36] ogbg-molbbbp: 0.884596 val loss: 0.735566
[Epoch 36] ogbg-molbbbp: 0.682292 test loss: 1.089846
[Epoch 37; Iter    30/   55] train: loss: 0.0647192
[Epoch 37] ogbg-molbbbp: 0.905407 val loss: 1.305111
[Epoch 37] ogbg-molbbbp: 0.689333 test loss: 1.630162
[Epoch 38; Iter     5/   55] train: loss: 0.0846755
[Epoch 38; Iter    35/   55] train: loss: 0.0554983
[Epoch 38] ogbg-molbbbp: 0.929404 val loss: 0.921602
[Epoch 38] ogbg-molbbbp: 0.660880 test loss: 2.164653
[Epoch 39; Iter    10/   55] train: loss: 0.0324439
[Epoch 39; Iter    40/   55] train: loss: 0.1026696
[Epoch 39] ogbg-molbbbp: 0.919745 val loss: 2.282509
[Epoch 39] ogbg-molbbbp: 0.679591 test loss: 2.063580
[Epoch 40; Iter    15/   55] train: loss: 0.0151671
[Epoch 40; Iter    45/   55] train: loss: 0.0600859
[Epoch 40] ogbg-molbbbp: 0.863686 val loss: 1.002305
[Epoch 40] ogbg-molbbbp: 0.671875 test loss: 1.574038
[Epoch 41; Iter    20/   55] train: loss: 0.0706724
[Epoch 41; Iter    50/   55] train: loss: 0.0354923
[Epoch 41] ogbg-molbbbp: 0.931495 val loss: 0.520858
[Epoch 41] ogbg-molbbbp: 0.673900 test loss: 1.587226
[Epoch 42; Iter    25/   55] train: loss: 0.0606628
[Epoch 42; Iter    55/   55] train: loss: 0.0127168
[Epoch 42] ogbg-molbbbp: 0.873842 val loss: 3.067859
[Epoch 42] ogbg-molbbbp: 0.685571 test loss: 1.796396
[Epoch 43; Iter    30/   55] train: loss: 0.0819019
[Epoch 43] ogbg-molbbbp: 0.911182 val loss: 0.867443
[Epoch 43] ogbg-molbbbp: 0.698206 test loss: 1.260064
[Epoch 44; Iter     5/   55] train: loss: 0.1766871
[Epoch 44; Iter    35/   55] train: loss: 0.0227973
[Epoch 44] ogbg-molbbbp: 0.919048 val loss: 0.773370
[Epoch 44] ogbg-molbbbp: 0.694541 test loss: 1.411564
[Epoch 45; Iter    10/   55] train: loss: 0.0299636
[Epoch 45; Iter    40/   55] train: loss: 0.0093127
[Epoch 45] ogbg-molbbbp: 0.903216 val loss: 0.774666
[Epoch 45] ogbg-molbbbp: 0.680941 test loss: 1.492679
[Epoch 46; Iter    15/   55] train: loss: 0.0077671
[Epoch 46; Iter    45/   55] train: loss: 0.0062226
[Epoch 46] ogbg-molbbbp: 0.928408 val loss: 0.489922
[Epoch 46] ogbg-molbbbp: 0.686728 test loss: 1.436418
[Epoch 47; Iter    20/   55] train: loss: 0.0051753
[Epoch 47; Iter    50/   55] train: loss: 0.0034843
[Epoch 47] ogbg-molbbbp: 0.948023 val loss: 0.477373
[Epoch 47] ogbg-molbbbp: 0.698206 test loss: 1.675676
[Epoch 48; Iter    25/   55] train: loss: 0.0028733
[Epoch 48; Iter    55/   55] train: loss: 0.0037318
[Epoch 48] ogbg-molbbbp: 0.940556 val loss: 0.528418
[Epoch 48] ogbg-molbbbp: 0.693769 test loss: 1.752247
[Epoch 49; Iter    30/   55] train: loss: 0.0024692
[Epoch 49] ogbg-molbbbp: 0.954695 val loss: 0.401075
[Epoch 49] ogbg-molbbbp: 0.693191 test loss: 1.637694
[Epoch 50; Iter     5/   55] train: loss: 0.0166595
[Epoch 50; Iter    35/   55] train: loss: 0.1176620
[Epoch 50] ogbg-molbbbp: 0.894454 val loss: 0.829473
[Epoch 50] ogbg-molbbbp: 0.654610 test loss: 1.847205
[Epoch 51; Iter    10/   55] train: loss: 0.7963136
[Epoch 51; Iter    40/   55] train: loss: 0.0689196
[Epoch 51] ogbg-molbbbp: 0.925122 val loss: 0.919912
[Epoch 51] ogbg-molbbbp: 0.619985 test loss: 1.591844
[Epoch 52; Iter    15/   55] train: loss: 0.1687541
[Epoch 52; Iter    45/   55] train: loss: 0.3406593
[Epoch 52] ogbg-molbbbp: 0.907498 val loss: 0.508566
[Epoch 52] ogbg-molbbbp: 0.643036 test loss: 1.246460
[Epoch 53; Iter    20/   55] train: loss: 0.1149468
[Epoch 53; Iter    50/   55] train: loss: 0.0737887
[Epoch 53] ogbg-molbbbp: 0.938564 val loss: 0.384256
[Epoch 53] ogbg-molbbbp: 0.654225 test loss: 1.335147
[Epoch 54; Iter    25/   55] train: loss: 0.0528753
[Epoch 54; Iter    55/   55] train: loss: 0.0153529
[Epoch 54] ogbg-molbbbp: 0.908394 val loss: 0.451513
[Epoch 54] ogbg-molbbbp: 0.655478 test loss: 1.232491
[Epoch 55; Iter    30/   55] train: loss: 0.0555218
[Epoch 55] ogbg-molbbbp: 0.932988 val loss: 0.547796
[Epoch 55] ogbg-molbbbp: 0.685282 test loss: 1.580938
[Epoch 56; Iter     5/   55] train: loss: 0.0489969
[Epoch 56; Iter    35/   55] train: loss: 0.0485123
[Epoch 56] ogbg-molbbbp: 0.936473 val loss: 0.546627
[Epoch 56] ogbg-molbbbp: 0.662133 test loss: 1.631091
[Epoch 57; Iter    10/   55] train: loss: 0.0563613
[Epoch 57; Iter    40/   55] train: loss: 0.0876192
[Epoch 57] ogbg-molbbbp: 0.902021 val loss: 0.663267
[Epoch 57] ogbg-molbbbp: 0.670621 test loss: 1.660857
[Epoch 58; Iter    15/   55] train: loss: 0.1311494
[Epoch 58; Iter    45/   55] train: loss: 0.0161968
[Epoch 58] ogbg-molbbbp: 0.917555 val loss: 0.594906
[Epoch 58] ogbg-molbbbp: 0.668113 test loss: 1.574168
[Epoch 59; Iter    20/   55] train: loss: 0.0317749
[Epoch 59; Iter    50/   55] train: loss: 0.0610024
[Epoch 59] ogbg-molbbbp: 0.895450 val loss: 0.924821
[Epoch 59] ogbg-molbbbp: 0.681520 test loss: 1.756818
[Epoch 60; Iter    25/   55] train: loss: 0.0821916
[Epoch 60; Iter    55/   55] train: loss: 0.2534414
[Epoch 60] ogbg-molbbbp: 0.942248 val loss: 0.530723
[Epoch 60] ogbg-molbbbp: 0.678916 test loss: 1.829674
[Epoch 61; Iter    30/   55] train: loss: 0.0375133
[Epoch 61] ogbg-molbbbp: 0.934780 val loss: 0.564059
[Epoch 61] ogbg-molbbbp: 0.698881 test loss: 1.595116
[Epoch 62; Iter     5/   55] train: loss: 0.0684061
[Epoch 62; Iter    35/   55] train: loss: 0.0815305
[Epoch 62] ogbg-molbbbp: 0.907498 val loss: 0.986745
[Epoch 62] ogbg-molbbbp: 0.673515 test loss: 2.366700
[Epoch 63; Iter    10/   55] train: loss: 0.0460248
[Epoch 63; Iter    40/   55] train: loss: 0.0050851
[Epoch 63] ogbg-molbbbp: 0.913173 val loss: 0.704485
[Epoch 63] ogbg-molbbbp: 0.671200 test loss: 1.805401
[Epoch 64; Iter    15/   55] train: loss: 0.0030081
[Epoch 64; Iter    45/   55] train: loss: 0.0829741
[Epoch 64] ogbg-molbbbp: 0.927113 val loss: 5.130779
[Epoch 64] ogbg-molbbbp: 0.687596 test loss: 1.938055
[Epoch 65; Iter    20/   55] train: loss: 0.0088628
[Epoch 65; Iter    50/   55] train: loss: 0.0038341
[Epoch 65] ogbg-molbbbp: 0.929603 val loss: 1.775054
[Epoch 65] ogbg-molbbbp: 0.684317 test loss: 1.936192
[Epoch 66; Iter    25/   55] train: loss: 0.0223736
[Epoch 66; Iter    55/   55] train: loss: 0.0009328
[Epoch 66] ogbg-molbbbp: 0.873544 val loss: 6.606950
[Epoch 66] ogbg-molbbbp: 0.658179 test loss: 2.375162
[Epoch 67; Iter    30/   55] train: loss: 0.0015477
[Epoch 67] ogbg-molbbbp: 0.924823 val loss: 7.532561
[Epoch 67] ogbg-molbbbp: 0.679109 test loss: 1.988448
[Epoch 68; Iter     5/   55] train: loss: 0.0031684
[Epoch 68; Iter    35/   55] train: loss: 0.0026246
[Epoch 68] ogbg-molbbbp: 0.921040 val loss: 8.180501
[Epoch 68] ogbg-molbbbp: 0.669850 test loss: 1.923469
[Epoch 69; Iter    10/   55] train: loss: 0.0051787
[Epoch 28] ogbg-molbbbp: 0.695795 test loss: 0.834424
[Epoch 29; Iter    20/   55] train: loss: 0.2572624
[Epoch 29; Iter    50/   55] train: loss: 0.3511262
[Epoch 29] ogbg-molbbbp: 0.956985 val loss: 0.447576
[Epoch 29] ogbg-molbbbp: 0.699942 test loss: 1.446907
[Epoch 30; Iter    25/   55] train: loss: 0.1621436
[Epoch 30; Iter    55/   55] train: loss: 0.4067048
[Epoch 30] ogbg-molbbbp: 0.944837 val loss: 0.457208
[Epoch 30] ogbg-molbbbp: 0.707369 test loss: 1.020358
[Epoch 31; Iter    30/   55] train: loss: 0.2758201
[Epoch 31] ogbg-molbbbp: 0.939859 val loss: 0.463213
[Epoch 31] ogbg-molbbbp: 0.654996 test loss: 1.271590
[Epoch 32; Iter     5/   55] train: loss: 0.1420801
[Epoch 32; Iter    35/   55] train: loss: 0.1069882
[Epoch 32] ogbg-molbbbp: 0.968834 val loss: 0.245653
[Epoch 32] ogbg-molbbbp: 0.686535 test loss: 1.034900
[Epoch 33; Iter    10/   55] train: loss: 0.1383043
[Epoch 33; Iter    40/   55] train: loss: 0.1963476
[Epoch 33] ogbg-molbbbp: 0.951309 val loss: 0.283978
[Epoch 33] ogbg-molbbbp: 0.699171 test loss: 1.073332
[Epoch 34; Iter    15/   55] train: loss: 0.3253691
[Epoch 34; Iter    45/   55] train: loss: 0.3529561
[Epoch 34] ogbg-molbbbp: 0.962163 val loss: 0.266756
[Epoch 34] ogbg-molbbbp: 0.688850 test loss: 1.098710
[Epoch 35; Iter    20/   55] train: loss: 0.2030814
[Epoch 35; Iter    50/   55] train: loss: 0.3046133
[Epoch 35] ogbg-molbbbp: 0.963855 val loss: 0.302603
[Epoch 35] ogbg-molbbbp: 0.680556 test loss: 1.048169
[Epoch 36; Iter    25/   55] train: loss: 0.3347963
[Epoch 36; Iter    55/   55] train: loss: 0.2056999
[Epoch 36] ogbg-molbbbp: 0.942149 val loss: 0.363768
[Epoch 36] ogbg-molbbbp: 0.670718 test loss: 1.105514
[Epoch 37; Iter    30/   55] train: loss: 0.2018259
[Epoch 37] ogbg-molbbbp: 0.928109 val loss: 0.485658
[Epoch 37] ogbg-molbbbp: 0.694348 test loss: 1.058980
[Epoch 38; Iter     5/   55] train: loss: 0.1643714
[Epoch 38; Iter    35/   55] train: loss: 0.1252498
[Epoch 38] ogbg-molbbbp: 0.955093 val loss: 0.291118
[Epoch 38] ogbg-molbbbp: 0.656250 test loss: 1.200496
[Epoch 39; Iter    10/   55] train: loss: 0.0806185
[Epoch 39; Iter    40/   55] train: loss: 0.0790977
[Epoch 39] ogbg-molbbbp: 0.888081 val loss: 0.709347
[Epoch 39] ogbg-molbbbp: 0.627701 test loss: 1.217126
[Epoch 40; Iter    15/   55] train: loss: 0.1007071
[Epoch 40; Iter    45/   55] train: loss: 0.3435299
[Epoch 40] ogbg-molbbbp: 0.962461 val loss: 0.276329
[Epoch 40] ogbg-molbbbp: 0.702643 test loss: 1.096844
[Epoch 41; Iter    20/   55] train: loss: 0.2115120
[Epoch 41; Iter    50/   55] train: loss: 0.0840942
[Epoch 41] ogbg-molbbbp: 0.951409 val loss: 0.330381
[Epoch 41] ogbg-molbbbp: 0.680556 test loss: 1.161217
[Epoch 42; Iter    25/   55] train: loss: 0.1520083
[Epoch 42; Iter    55/   55] train: loss: 0.0405774
[Epoch 42] ogbg-molbbbp: 0.948422 val loss: 0.409877
[Epoch 42] ogbg-molbbbp: 0.688175 test loss: 1.327154
[Epoch 43; Iter    30/   55] train: loss: 0.1313603
[Epoch 43] ogbg-molbbbp: 0.944439 val loss: 0.348215
[Epoch 43] ogbg-molbbbp: 0.672550 test loss: 1.080322
[Epoch 44; Iter     5/   55] train: loss: 0.0986983
[Epoch 44; Iter    35/   55] train: loss: 0.0326762
[Epoch 44] ogbg-molbbbp: 0.956587 val loss: 0.341649
[Epoch 44] ogbg-molbbbp: 0.688079 test loss: 1.357454
[Epoch 45; Iter    10/   55] train: loss: 0.0773007
[Epoch 45; Iter    40/   55] train: loss: 0.1650857
[Epoch 45] ogbg-molbbbp: 0.934382 val loss: 0.428431
[Epoch 45] ogbg-molbbbp: 0.684510 test loss: 1.087497
[Epoch 46; Iter    15/   55] train: loss: 0.2775276
[Epoch 46; Iter    45/   55] train: loss: 0.1999334
[Epoch 46] ogbg-molbbbp: 0.928607 val loss: 0.635860
[Epoch 46] ogbg-molbbbp: 0.634356 test loss: 1.464355
[Epoch 47; Iter    20/   55] train: loss: 0.1268014
[Epoch 47; Iter    50/   55] train: loss: 0.1315223
[Epoch 47] ogbg-molbbbp: 0.952504 val loss: 0.349989
[Epoch 47] ogbg-molbbbp: 0.676022 test loss: 1.393708
[Epoch 48; Iter    25/   55] train: loss: 0.0753964
[Epoch 48; Iter    55/   55] train: loss: 0.0836827
[Epoch 48] ogbg-molbbbp: 0.932092 val loss: 0.470680
[Epoch 48] ogbg-molbbbp: 0.630305 test loss: 1.501559
[Epoch 49; Iter    30/   55] train: loss: 0.1392997
[Epoch 49] ogbg-molbbbp: 0.933386 val loss: 0.479708
[Epoch 49] ogbg-molbbbp: 0.661458 test loss: 1.265371
[Epoch 50; Iter     5/   55] train: loss: 0.1838287
[Epoch 50; Iter    35/   55] train: loss: 0.0273761
[Epoch 50] ogbg-molbbbp: 0.949119 val loss: 0.435812
[Epoch 50] ogbg-molbbbp: 0.666956 test loss: 1.537112
[Epoch 51; Iter    10/   55] train: loss: 0.0609581
[Epoch 51; Iter    40/   55] train: loss: 0.1277563
[Epoch 51] ogbg-molbbbp: 0.934083 val loss: 0.615827
[Epoch 51] ogbg-molbbbp: 0.631559 test loss: 2.056944
[Epoch 52; Iter    15/   55] train: loss: 0.0603076
[Epoch 52; Iter    45/   55] train: loss: 0.0691083
[Epoch 52] ogbg-molbbbp: 0.957184 val loss: 0.412794
[Epoch 52] ogbg-molbbbp: 0.650945 test loss: 1.574086
[Epoch 53; Iter    20/   55] train: loss: 0.1502081
[Epoch 53; Iter    50/   55] train: loss: 0.1030487
[Epoch 53] ogbg-molbbbp: 0.931096 val loss: 0.599022
[Epoch 53] ogbg-molbbbp: 0.640721 test loss: 1.528351
[Epoch 54; Iter    25/   55] train: loss: 0.0466247
[Epoch 54; Iter    55/   55] train: loss: 0.2046984
[Epoch 54] ogbg-molbbbp: 0.903515 val loss: 0.859978
[Epoch 54] ogbg-molbbbp: 0.633681 test loss: 1.551117
[Epoch 55; Iter    30/   55] train: loss: 0.0447564
[Epoch 55] ogbg-molbbbp: 0.958678 val loss: 0.354425
[Epoch 55] ogbg-molbbbp: 0.687404 test loss: 1.457832
[Epoch 56; Iter     5/   55] train: loss: 0.0154964
[Epoch 56; Iter    35/   55] train: loss: 0.1473507
[Epoch 56] ogbg-molbbbp: 0.961665 val loss: 0.353358
[Epoch 56] ogbg-molbbbp: 0.660590 test loss: 1.592314
[Epoch 57; Iter    10/   55] train: loss: 0.0713173
[Epoch 57; Iter    40/   55] train: loss: 0.0517145
[Epoch 57] ogbg-molbbbp: 0.953998 val loss: 0.430242
[Epoch 57] ogbg-molbbbp: 0.658854 test loss: 1.445042
[Epoch 58; Iter    15/   55] train: loss: 0.1115530
[Epoch 58; Iter    45/   55] train: loss: 0.0788312
[Epoch 58] ogbg-molbbbp: 0.937469 val loss: 0.529185
[Epoch 58] ogbg-molbbbp: 0.671296 test loss: 1.539282
[Epoch 59; Iter    20/   55] train: loss: 0.0549490
[Epoch 59; Iter    50/   55] train: loss: 0.1057086
[Epoch 59] ogbg-molbbbp: 0.965648 val loss: 0.306624
[Epoch 59] ogbg-molbbbp: 0.675733 test loss: 1.585930
[Epoch 60; Iter    25/   55] train: loss: 0.0799826
[Epoch 60; Iter    55/   55] train: loss: 0.1453222
[Epoch 60] ogbg-molbbbp: 0.948721 val loss: 0.624203
[Epoch 60] ogbg-molbbbp: 0.633584 test loss: 2.166641
[Epoch 61; Iter    30/   55] train: loss: 0.0605494
[Epoch 61] ogbg-molbbbp: 0.956388 val loss: 0.393805
[Epoch 61] ogbg-molbbbp: 0.682967 test loss: 1.506445
[Epoch 62; Iter     5/   55] train: loss: 0.0349184
[Epoch 62; Iter    35/   55] train: loss: 0.1296166
[Epoch 62] ogbg-molbbbp: 0.952305 val loss: 0.424906
[Epoch 62] ogbg-molbbbp: 0.649981 test loss: 1.519916
[Epoch 63; Iter    10/   55] train: loss: 0.0766734
[Epoch 63; Iter    40/   55] train: loss: 0.0768832
[Epoch 63] ogbg-molbbbp: 0.948920 val loss: 0.439560
[Epoch 63] ogbg-molbbbp: 0.647087 test loss: 1.545591
[Epoch 64; Iter    15/   55] train: loss: 0.0876850
[Epoch 64; Iter    45/   55] train: loss: 0.0970817
[Epoch 64] ogbg-molbbbp: 0.940058 val loss: 0.726170
[Epoch 64] ogbg-molbbbp: 0.668210 test loss: 2.010318
[Epoch 65; Iter    20/   55] train: loss: 0.1442471
[Epoch 65; Iter    50/   55] train: loss: 0.1122759
[Epoch 65] ogbg-molbbbp: 0.953898 val loss: 0.429718
[Epoch 65] ogbg-molbbbp: 0.675154 test loss: 1.420679
[Epoch 66; Iter    25/   55] train: loss: 0.0255534
[Epoch 66; Iter    55/   55] train: loss: 0.0501235
[Epoch 66] ogbg-molbbbp: 0.957483 val loss: 0.387189
[Epoch 66] ogbg-molbbbp: 0.665702 test loss: 1.673984
[Epoch 67; Iter    30/   55] train: loss: 0.0203808
[Epoch 67] ogbg-molbbbp: 0.961366 val loss: 0.360410
[Epoch 67] ogbg-molbbbp: 0.687596 test loss: 1.568069
[Epoch 68; Iter     5/   55] train: loss: 0.0212479
[Epoch 68; Iter    35/   55] train: loss: 0.2186521
[Epoch 68] ogbg-molbbbp: 0.945634 val loss: 0.477884
[Epoch 68] ogbg-molbbbp: 0.623457 test loss: 1.700280
[Epoch 69; Iter    10/   55] train: loss: 0.1154042
[Epoch 28] ogbg-molbbbp: 0.706597 test loss: 5.356084
[Epoch 29; Iter    20/   55] train: loss: 0.1391781
[Epoch 29; Iter    50/   55] train: loss: 0.2384129
[Epoch 29] ogbg-molbbbp: 0.665936 val loss: 40.700312
[Epoch 29] ogbg-molbbbp: 0.591242 test loss: 23.540148
[Epoch 30; Iter    25/   55] train: loss: 0.2685767
[Epoch 30; Iter    55/   55] train: loss: 0.3560107
[Epoch 30] ogbg-molbbbp: 0.933287 val loss: 0.410964
[Epoch 30] ogbg-molbbbp: 0.707948 test loss: 0.815034
[Epoch 31; Iter    30/   55] train: loss: 0.2625220
[Epoch 31] ogbg-molbbbp: 0.942149 val loss: 0.344841
[Epoch 31] ogbg-molbbbp: 0.703607 test loss: 0.918413
[Epoch 32; Iter     5/   55] train: loss: 0.2587498
[Epoch 32; Iter    35/   55] train: loss: 0.3643477
[Epoch 32] ogbg-molbbbp: 0.955790 val loss: 0.285029
[Epoch 32] ogbg-molbbbp: 0.678048 test loss: 0.987085
[Epoch 33; Iter    10/   55] train: loss: 0.2192195
[Epoch 33; Iter    40/   55] train: loss: 0.2873611
[Epoch 33] ogbg-molbbbp: 0.941053 val loss: 0.489459
[Epoch 33] ogbg-molbbbp: 0.699846 test loss: 1.191136
[Epoch 34; Iter    15/   55] train: loss: 0.0950423
[Epoch 34; Iter    45/   55] train: loss: 0.2651731
[Epoch 34] ogbg-molbbbp: 0.959076 val loss: 0.275057
[Epoch 34] ogbg-molbbbp: 0.715085 test loss: 1.033680
[Epoch 35; Iter    20/   55] train: loss: 0.1461640
[Epoch 35; Iter    50/   55] train: loss: 0.1584471
[Epoch 35] ogbg-molbbbp: 0.935278 val loss: 0.485012
[Epoch 35] ogbg-molbbbp: 0.662905 test loss: 1.341902
[Epoch 36; Iter    25/   55] train: loss: 0.3208841
[Epoch 36; Iter    55/   55] train: loss: 0.7168037
[Epoch 36] ogbg-molbbbp: 0.924126 val loss: 0.563353
[Epoch 36] ogbg-molbbbp: 0.666088 test loss: 1.264344
[Epoch 37; Iter    30/   55] train: loss: 0.1310593
[Epoch 37] ogbg-molbbbp: 0.917156 val loss: 0.469411
[Epoch 37] ogbg-molbbbp: 0.660301 test loss: 1.069549
[Epoch 38; Iter     5/   55] train: loss: 0.1820215
[Epoch 38; Iter    35/   55] train: loss: 0.1267001
[Epoch 38] ogbg-molbbbp: 0.954297 val loss: 0.327341
[Epoch 38] ogbg-molbbbp: 0.694155 test loss: 1.334323
[Epoch 39; Iter    10/   55] train: loss: 0.0897391
[Epoch 39; Iter    40/   55] train: loss: 0.1390057
[Epoch 39] ogbg-molbbbp: 0.953500 val loss: 0.318584
[Epoch 39] ogbg-molbbbp: 0.650849 test loss: 1.284735
[Epoch 40; Iter    15/   55] train: loss: 0.0614541
[Epoch 40; Iter    45/   55] train: loss: 0.2649996
[Epoch 40] ogbg-molbbbp: 0.964353 val loss: 0.302158
[Epoch 40] ogbg-molbbbp: 0.708719 test loss: 1.236288
[Epoch 41; Iter    20/   55] train: loss: 0.1436417
[Epoch 41; Iter    50/   55] train: loss: 0.1790774
[Epoch 41] ogbg-molbbbp: 0.966942 val loss: 0.251464
[Epoch 41] ogbg-molbbbp: 0.698688 test loss: 1.169841
[Epoch 42; Iter    25/   55] train: loss: 0.1466627
[Epoch 42; Iter    55/   55] train: loss: 0.3142493
[Epoch 42] ogbg-molbbbp: 0.962959 val loss: 0.300568
[Epoch 42] ogbg-molbbbp: 0.699267 test loss: 1.271651
[Epoch 43; Iter    30/   55] train: loss: 0.1952517
[Epoch 43] ogbg-molbbbp: 0.939659 val loss: 0.535466
[Epoch 43] ogbg-molbbbp: 0.649402 test loss: 1.363938
[Epoch 44; Iter     5/   55] train: loss: 0.2165106
[Epoch 44; Iter    35/   55] train: loss: 0.0842070
[Epoch 44] ogbg-molbbbp: 0.963557 val loss: 0.251844
[Epoch 44] ogbg-molbbbp: 0.702546 test loss: 1.110997
[Epoch 45; Iter    10/   55] train: loss: 0.1741407
[Epoch 45; Iter    40/   55] train: loss: 0.2071454
[Epoch 45] ogbg-molbbbp: 0.952903 val loss: 0.349096
[Epoch 45] ogbg-molbbbp: 0.689140 test loss: 1.243377
[Epoch 46; Iter    15/   55] train: loss: 0.0773660
[Epoch 46; Iter    45/   55] train: loss: 0.1945463
[Epoch 46] ogbg-molbbbp: 0.946928 val loss: 0.429047
[Epoch 46] ogbg-molbbbp: 0.690490 test loss: 1.232244
[Epoch 47; Iter    20/   55] train: loss: 0.1824735
[Epoch 47; Iter    50/   55] train: loss: 0.1447271
[Epoch 47] ogbg-molbbbp: 0.969631 val loss: 0.255613
[Epoch 47] ogbg-molbbbp: 0.701389 test loss: 1.135471
[Epoch 48; Iter    25/   55] train: loss: 0.2425041
[Epoch 48; Iter    55/   55] train: loss: 0.1118491
[Epoch 48] ogbg-molbbbp: 0.960669 val loss: 0.290037
[Epoch 48] ogbg-molbbbp: 0.682195 test loss: 1.110738
[Epoch 49; Iter    30/   55] train: loss: 0.0738671
[Epoch 49] ogbg-molbbbp: 0.968734 val loss: 0.275217
[Epoch 49] ogbg-molbbbp: 0.687596 test loss: 1.383441
[Epoch 50; Iter     5/   55] train: loss: 0.0328903
[Epoch 50; Iter    35/   55] train: loss: 0.0660565
[Epoch 50] ogbg-molbbbp: 0.954197 val loss: 0.372291
[Epoch 50] ogbg-molbbbp: 0.677276 test loss: 1.273064
[Epoch 51; Iter    10/   55] train: loss: 0.1451700
[Epoch 51; Iter    40/   55] train: loss: 0.0711897
[Epoch 51] ogbg-molbbbp: 0.948023 val loss: 0.629660
[Epoch 51] ogbg-molbbbp: 0.688079 test loss: 1.910464
[Epoch 52; Iter    15/   55] train: loss: 0.0522832
[Epoch 52; Iter    45/   55] train: loss: 0.1321314
[Epoch 52] ogbg-molbbbp: 0.972219 val loss: 0.223336
[Epoch 52] ogbg-molbbbp: 0.669464 test loss: 1.188095
[Epoch 53; Iter    20/   55] train: loss: 0.2139199
[Epoch 53; Iter    50/   55] train: loss: 0.1787579
[Epoch 53] ogbg-molbbbp: 0.966444 val loss: 0.299866
[Epoch 53] ogbg-molbbbp: 0.708430 test loss: 1.242184
[Epoch 54; Iter    25/   55] train: loss: 0.0622222
[Epoch 54; Iter    55/   55] train: loss: 0.0474324
[Epoch 54] ogbg-molbbbp: 0.970825 val loss: 0.288635
[Epoch 54] ogbg-molbbbp: 0.710745 test loss: 1.324186
[Epoch 55; Iter    30/   55] train: loss: 0.0638814
[Epoch 55] ogbg-molbbbp: 0.963059 val loss: 0.305986
[Epoch 55] ogbg-molbbbp: 0.694830 test loss: 1.378956
[Epoch 56; Iter     5/   55] train: loss: 0.1439508
[Epoch 56; Iter    35/   55] train: loss: 0.0498423
[Epoch 56] ogbg-molbbbp: 0.965747 val loss: 0.335431
[Epoch 56] ogbg-molbbbp: 0.705150 test loss: 1.430857
[Epoch 57; Iter    10/   55] train: loss: 0.1020616
[Epoch 57; Iter    40/   55] train: loss: 0.2433648
[Epoch 57] ogbg-molbbbp: 0.946331 val loss: 0.566647
[Epoch 57] ogbg-molbbbp: 0.709394 test loss: 1.634215
[Epoch 58; Iter    15/   55] train: loss: 0.0625066
[Epoch 58; Iter    45/   55] train: loss: 0.0278533
[Epoch 58] ogbg-molbbbp: 0.958080 val loss: 0.401358
[Epoch 58] ogbg-molbbbp: 0.695698 test loss: 1.547846
[Epoch 59; Iter    20/   55] train: loss: 0.2554801
[Epoch 59; Iter    50/   55] train: loss: 0.1521993
[Epoch 59] ogbg-molbbbp: 0.941253 val loss: 0.464504
[Epoch 59] ogbg-molbbbp: 0.670814 test loss: 1.682005
[Epoch 60; Iter    25/   55] train: loss: 0.0776599
[Epoch 60; Iter    55/   55] train: loss: 0.5255183
[Epoch 60] ogbg-molbbbp: 0.943642 val loss: 0.515669
[Epoch 60] ogbg-molbbbp: 0.707851 test loss: 1.700519
[Epoch 61; Iter    30/   55] train: loss: 0.1060924
[Epoch 61] ogbg-molbbbp: 0.967639 val loss: 0.304963
[Epoch 61] ogbg-molbbbp: 0.685185 test loss: 1.542782
[Epoch 62; Iter     5/   55] train: loss: 0.0394186
[Epoch 62; Iter    35/   55] train: loss: 0.1073775
[Epoch 62] ogbg-molbbbp: 0.966544 val loss: 0.355979
[Epoch 62] ogbg-molbbbp: 0.685089 test loss: 1.677297
[Epoch 63; Iter    10/   55] train: loss: 0.0665461
[Epoch 63; Iter    40/   55] train: loss: 0.0504424
[Epoch 63] ogbg-molbbbp: 0.970427 val loss: 0.269676
[Epoch 63] ogbg-molbbbp: 0.697145 test loss: 1.474224
[Epoch 64; Iter    15/   55] train: loss: 0.0346270
[Epoch 64; Iter    45/   55] train: loss: 0.1574691
[Epoch 64] ogbg-molbbbp: 0.964254 val loss: 0.350014
[Epoch 64] ogbg-molbbbp: 0.688657 test loss: 1.597573
[Epoch 65; Iter    20/   55] train: loss: 0.0349778
[Epoch 65; Iter    50/   55] train: loss: 0.0070580
[Epoch 65] ogbg-molbbbp: 0.962959 val loss: 0.382134
[Epoch 65] ogbg-molbbbp: 0.689043 test loss: 1.746097
[Epoch 66; Iter    25/   55] train: loss: 0.0647716
[Epoch 66; Iter    55/   55] train: loss: 0.0070498
[Epoch 66] ogbg-molbbbp: 0.968734 val loss: 0.368431
[Epoch 66] ogbg-molbbbp: 0.708140 test loss: 1.636652
[Epoch 67; Iter    30/   55] train: loss: 0.0148622
[Epoch 67] ogbg-molbbbp: 0.970626 val loss: 0.346775
[Epoch 67] ogbg-molbbbp: 0.708623 test loss: 1.703232
[Epoch 68; Iter     5/   55] train: loss: 0.1447750
[Epoch 68; Iter    35/   55] train: loss: 0.0152149
[Epoch 68] ogbg-molbbbp: 0.962561 val loss: 0.404299
[Epoch 68] ogbg-molbbbp: 0.690683 test loss: 1.638688
[Epoch 69; Iter    10/   55] train: loss: 0.0087729
[Epoch 28] ogbg-molbbbp: 0.706983 test loss: 0.925503
[Epoch 29; Iter    20/   55] train: loss: 0.1297160
[Epoch 29; Iter    50/   55] train: loss: 0.3029278
[Epoch 29] ogbg-molbbbp: 0.931893 val loss: 0.433750
[Epoch 29] ogbg-molbbbp: 0.689815 test loss: 0.972341
[Epoch 30; Iter    25/   55] train: loss: 0.1840866
[Epoch 30; Iter    55/   55] train: loss: 0.1644728
[Epoch 30] ogbg-molbbbp: 0.944439 val loss: 0.319015
[Epoch 30] ogbg-molbbbp: 0.635899 test loss: 1.155672
[Epoch 31; Iter    30/   55] train: loss: 0.3910246
[Epoch 31] ogbg-molbbbp: 0.927213 val loss: 0.561637
[Epoch 31] ogbg-molbbbp: 0.625482 test loss: 1.340804
[Epoch 32; Iter     5/   55] train: loss: 0.1224753
[Epoch 32; Iter    35/   55] train: loss: 0.1822701
[Epoch 32] ogbg-molbbbp: 0.921338 val loss: 0.461787
[Epoch 32] ogbg-molbbbp: 0.673804 test loss: 0.914640
[Epoch 33; Iter    10/   55] train: loss: 0.1012750
[Epoch 33; Iter    40/   55] train: loss: 0.1100212
[Epoch 33] ogbg-molbbbp: 0.946331 val loss: 0.400627
[Epoch 33] ogbg-molbbbp: 0.726852 test loss: 1.128783
[Epoch 34; Iter    15/   55] train: loss: 0.5140907
[Epoch 34; Iter    45/   55] train: loss: 0.1057590
[Epoch 34] ogbg-molbbbp: 0.963855 val loss: 0.273490
[Epoch 34] ogbg-molbbbp: 0.702643 test loss: 1.090434
[Epoch 35; Iter    20/   55] train: loss: 0.1417210
[Epoch 35; Iter    50/   55] train: loss: 0.1859246
[Epoch 35] ogbg-molbbbp: 0.952405 val loss: 0.319161
[Epoch 35] ogbg-molbbbp: 0.698206 test loss: 1.219809
[Epoch 36; Iter    25/   55] train: loss: 0.2108904
[Epoch 36; Iter    55/   55] train: loss: 0.0938122
[Epoch 36] ogbg-molbbbp: 0.962262 val loss: 0.305504
[Epoch 36] ogbg-molbbbp: 0.717593 test loss: 1.092444
[Epoch 37; Iter    30/   55] train: loss: 0.4786937
[Epoch 37] ogbg-molbbbp: 0.932789 val loss: 0.415089
[Epoch 37] ogbg-molbbbp: 0.699074 test loss: 0.978603
[Epoch 38; Iter     5/   55] train: loss: 0.1098131
[Epoch 38; Iter    35/   55] train: loss: 0.1328393
[Epoch 38] ogbg-molbbbp: 0.971921 val loss: 0.243860
[Epoch 38] ogbg-molbbbp: 0.726755 test loss: 0.966725
[Epoch 39; Iter    10/   55] train: loss: 0.0976179
[Epoch 39; Iter    40/   55] train: loss: 0.1688185
[Epoch 39] ogbg-molbbbp: 0.958877 val loss: 0.325001
[Epoch 39] ogbg-molbbbp: 0.704090 test loss: 1.176862
[Epoch 40; Iter    15/   55] train: loss: 0.1660212
[Epoch 40; Iter    45/   55] train: loss: 0.0872102
[Epoch 40] ogbg-molbbbp: 0.968436 val loss: 0.256990
[Epoch 40] ogbg-molbbbp: 0.684317 test loss: 1.207931
[Epoch 41; Iter    20/   55] train: loss: 0.3117264
[Epoch 41; Iter    50/   55] train: loss: 0.0361628
[Epoch 41] ogbg-molbbbp: 0.960370 val loss: 0.330006
[Epoch 41] ogbg-molbbbp: 0.690104 test loss: 1.241251
[Epoch 42; Iter    25/   55] train: loss: 0.0669958
[Epoch 42; Iter    55/   55] train: loss: 0.2693722
[Epoch 42] ogbg-molbbbp: 0.956686 val loss: 0.297495
[Epoch 42] ogbg-molbbbp: 0.692612 test loss: 1.064860
[Epoch 43; Iter    30/   55] train: loss: 0.0803158
[Epoch 43] ogbg-molbbbp: 0.952903 val loss: 0.415743
[Epoch 43] ogbg-molbbbp: 0.709008 test loss: 1.113630
[Epoch 44; Iter     5/   55] train: loss: 0.1297622
[Epoch 44; Iter    35/   55] train: loss: 0.2188177
[Epoch 44] ogbg-molbbbp: 0.962461 val loss: 0.321451
[Epoch 44] ogbg-molbbbp: 0.703511 test loss: 1.464100
[Epoch 45; Iter    10/   55] train: loss: 0.1116227
[Epoch 45; Iter    40/   55] train: loss: 0.0580058
[Epoch 45] ogbg-molbbbp: 0.960669 val loss: 0.309379
[Epoch 45] ogbg-molbbbp: 0.674383 test loss: 1.185209
[Epoch 46; Iter    15/   55] train: loss: 0.2476228
[Epoch 46; Iter    45/   55] train: loss: 0.1618935
[Epoch 46] ogbg-molbbbp: 0.947824 val loss: 0.491959
[Epoch 46] ogbg-molbbbp: 0.704186 test loss: 1.551737
[Epoch 47; Iter    20/   55] train: loss: 0.2164210
[Epoch 47; Iter    50/   55] train: loss: 0.1407318
[Epoch 47] ogbg-molbbbp: 0.950513 val loss: 0.415920
[Epoch 47] ogbg-molbbbp: 0.668885 test loss: 1.395967
[Epoch 48; Iter    25/   55] train: loss: 0.0317342
[Epoch 48; Iter    55/   55] train: loss: 0.2673764
[Epoch 48] ogbg-molbbbp: 0.963258 val loss: 0.294513
[Epoch 48] ogbg-molbbbp: 0.658854 test loss: 1.389332
[Epoch 49; Iter    30/   55] train: loss: 0.1238773
[Epoch 49] ogbg-molbbbp: 0.960171 val loss: 0.293144
[Epoch 49] ogbg-molbbbp: 0.687693 test loss: 1.109277
[Epoch 50; Iter     5/   55] train: loss: 0.1353153
[Epoch 50; Iter    35/   55] train: loss: 0.0860732
[Epoch 50] ogbg-molbbbp: 0.946829 val loss: 0.429626
[Epoch 50] ogbg-molbbbp: 0.619020 test loss: 1.771556
[Epoch 51; Iter    10/   55] train: loss: 0.0652721
[Epoch 51; Iter    40/   55] train: loss: 0.1191994
[Epoch 51] ogbg-molbbbp: 0.945833 val loss: 0.629931
[Epoch 51] ogbg-molbbbp: 0.599344 test loss: 2.157666
[Epoch 52; Iter    15/   55] train: loss: 0.1286474
[Epoch 52; Iter    45/   55] train: loss: 0.0331154
[Epoch 52] ogbg-molbbbp: 0.957483 val loss: 0.346562
[Epoch 52] ogbg-molbbbp: 0.647569 test loss: 1.541018
[Epoch 53; Iter    20/   55] train: loss: 0.1343932
[Epoch 53; Iter    50/   55] train: loss: 0.0881102
[Epoch 53] ogbg-molbbbp: 0.943742 val loss: 0.400133
[Epoch 53] ogbg-molbbbp: 0.632137 test loss: 1.331212
[Epoch 54; Iter    25/   55] train: loss: 0.0255518
[Epoch 54; Iter    55/   55] train: loss: 0.0625148
[Epoch 54] ogbg-molbbbp: 0.909887 val loss: 0.795578
[Epoch 54] ogbg-molbbbp: 0.619695 test loss: 1.775690
[Epoch 55; Iter    30/   55] train: loss: 0.0572973
[Epoch 55] ogbg-molbbbp: 0.964154 val loss: 0.318687
[Epoch 55] ogbg-molbbbp: 0.647377 test loss: 1.526264
[Epoch 56; Iter     5/   55] train: loss: 0.0280947
[Epoch 56; Iter    35/   55] train: loss: 0.0483272
[Epoch 56] ogbg-molbbbp: 0.943344 val loss: 0.406894
[Epoch 56] ogbg-molbbbp: 0.642458 test loss: 1.578027
[Epoch 57; Iter    10/   55] train: loss: 0.0966681
[Epoch 57; Iter    40/   55] train: loss: 0.0996561
[Epoch 57] ogbg-molbbbp: 0.951011 val loss: 0.344831
[Epoch 57] ogbg-molbbbp: 0.653935 test loss: 1.219868
[Epoch 58; Iter    15/   55] train: loss: 0.1600789
[Epoch 58; Iter    45/   55] train: loss: 0.0557140
[Epoch 58] ogbg-molbbbp: 0.962561 val loss: 0.312102
[Epoch 58] ogbg-molbbbp: 0.660204 test loss: 1.450805
[Epoch 59; Iter    20/   55] train: loss: 0.2284932
[Epoch 59; Iter    50/   55] train: loss: 0.2446961
[Epoch 59] ogbg-molbbbp: 0.948422 val loss: 0.353973
[Epoch 59] ogbg-molbbbp: 0.665509 test loss: 1.050781
[Epoch 60; Iter    25/   55] train: loss: 0.0227791
[Epoch 60; Iter    55/   55] train: loss: 0.0616158
[Epoch 60] ogbg-molbbbp: 0.958279 val loss: 0.302592
[Epoch 60] ogbg-molbbbp: 0.656636 test loss: 1.324436
[Epoch 61; Iter    30/   55] train: loss: 0.0707805
[Epoch 61] ogbg-molbbbp: 0.957981 val loss: 0.404352
[Epoch 61] ogbg-molbbbp: 0.636285 test loss: 1.949182
[Epoch 62; Iter     5/   55] train: loss: 0.1535522
[Epoch 62; Iter    35/   55] train: loss: 0.0661519
[Epoch 62] ogbg-molbbbp: 0.949617 val loss: 0.428232
[Epoch 62] ogbg-molbbbp: 0.606674 test loss: 1.992187
[Epoch 63; Iter    10/   55] train: loss: 0.0140405
[Epoch 63; Iter    40/   55] train: loss: 0.1114818
[Epoch 63] ogbg-molbbbp: 0.967340 val loss: 0.306250
[Epoch 63] ogbg-molbbbp: 0.654032 test loss: 1.638757
[Epoch 64; Iter    15/   55] train: loss: 0.1074535
[Epoch 64; Iter    45/   55] train: loss: 0.0722127
[Epoch 64] ogbg-molbbbp: 0.944638 val loss: 0.470772
[Epoch 64] ogbg-molbbbp: 0.634066 test loss: 1.726568
[Epoch 65; Iter    20/   55] train: loss: 0.0324065
[Epoch 65; Iter    50/   55] train: loss: 0.0389975
[Epoch 65] ogbg-molbbbp: 0.966146 val loss: 0.362099
[Epoch 65] ogbg-molbbbp: 0.653453 test loss: 1.785397
[Epoch 66; Iter    25/   55] train: loss: 0.0110549
[Epoch 66; Iter    55/   55] train: loss: 0.0193336
[Epoch 66] ogbg-molbbbp: 0.949119 val loss: 0.481120
[Epoch 66] ogbg-molbbbp: 0.649788 test loss: 1.645578
[Epoch 67; Iter    30/   55] train: loss: 0.1192943
[Epoch 67] ogbg-molbbbp: 0.952703 val loss: 0.367051
[Epoch 67] ogbg-molbbbp: 0.642940 test loss: 1.568468
[Epoch 68; Iter     5/   55] train: loss: 0.0133253
[Epoch 68; Iter    35/   55] train: loss: 0.0206876
[Epoch 68] ogbg-molbbbp: 0.916658 val loss: 0.684924
[Epoch 68] ogbg-molbbbp: 0.598090 test loss: 2.162116
[Epoch 69; Iter    10/   55] train: loss: 0.1218441
[Epoch 69; Iter    40/   55] train: loss: 0.0013674
[Epoch 69] ogbg-molbbbp: 0.954496 val loss: 0.564806
[Epoch 69] ogbg-molbbbp: 0.640432 test loss: 2.814604
[Epoch 70; Iter    15/   55] train: loss: 0.0154755
[Epoch 70; Iter    45/   55] train: loss: 0.0010067
[Epoch 70] ogbg-molbbbp: 0.951309 val loss: 0.601229
[Epoch 70] ogbg-molbbbp: 0.640914 test loss: 2.938400
[Epoch 71; Iter    20/   55] train: loss: 0.0004567
[Epoch 71; Iter    50/   55] train: loss: 0.0006788
[Epoch 71] ogbg-molbbbp: 0.950712 val loss: 0.587270
[Epoch 71] ogbg-molbbbp: 0.638792 test loss: 2.816114
[Epoch 72; Iter    25/   55] train: loss: 0.0007661
[Epoch 72; Iter    55/   55] train: loss: 0.0346529
[Epoch 72] ogbg-molbbbp: 0.952903 val loss: 0.593589
[Epoch 72] ogbg-molbbbp: 0.638407 test loss: 2.840795
[Epoch 73; Iter    30/   55] train: loss: 0.0018176
[Epoch 73] ogbg-molbbbp: 0.950712 val loss: 0.604046
[Epoch 73] ogbg-molbbbp: 0.631559 test loss: 2.899355
[Epoch 74; Iter     5/   55] train: loss: 0.0006078
[Epoch 74; Iter    35/   55] train: loss: 0.0006747
[Epoch 74] ogbg-molbbbp: 0.951011 val loss: 0.622510
[Epoch 74] ogbg-molbbbp: 0.628569 test loss: 2.994475
[Epoch 75; Iter    10/   55] train: loss: 0.0007785
[Epoch 75; Iter    40/   55] train: loss: 0.0010067
[Epoch 75] ogbg-molbbbp: 0.948223 val loss: 0.622746
[Epoch 75] ogbg-molbbbp: 0.620081 test loss: 2.957841
[Epoch 76; Iter    15/   55] train: loss: 0.0018757
[Epoch 76; Iter    45/   55] train: loss: 0.0004486
[Epoch 76] ogbg-molbbbp: 0.949716 val loss: 0.673864
[Epoch 76] ogbg-molbbbp: 0.649113 test loss: 3.023041
[Epoch 77; Iter    20/   55] train: loss: 0.0022794
[Epoch 77; Iter    50/   55] train: loss: 0.0035147
[Epoch 77] ogbg-molbbbp: 0.947526 val loss: 0.666106
[Epoch 77] ogbg-molbbbp: 0.644772 test loss: 2.902276
[Epoch 78; Iter    25/   55] train: loss: 0.0034082
[Epoch 78; Iter    55/   55] train: loss: 0.0018606
[Epoch 78] ogbg-molbbbp: 0.944339 val loss: 0.698095
[Epoch 78] ogbg-molbbbp: 0.638407 test loss: 2.963183
[Epoch 79; Iter    30/   55] train: loss: 0.0004043
[Epoch 79] ogbg-molbbbp: 0.949218 val loss: 0.660739
[Epoch 79] ogbg-molbbbp: 0.639564 test loss: 3.030203
[Epoch 80; Iter     5/   55] train: loss: 0.0042541
[Epoch 80; Iter    35/   55] train: loss: 0.0024590
[Epoch 80] ogbg-molbbbp: 0.944738 val loss: 0.718254
[Epoch 80] ogbg-molbbbp: 0.651042 test loss: 2.984279
[Epoch 81; Iter    10/   55] train: loss: 0.0007591
[Epoch 81; Iter    40/   55] train: loss: 0.0012332
[Epoch 81] ogbg-molbbbp: 0.950612 val loss: 0.658305
[Epoch 81] ogbg-molbbbp: 0.644483 test loss: 2.931533
[Epoch 82; Iter    15/   55] train: loss: 0.0002249
[Epoch 82; Iter    45/   55] train: loss: 0.0003252
[Epoch 82] ogbg-molbbbp: 0.944041 val loss: 0.726081
[Epoch 82] ogbg-molbbbp: 0.624518 test loss: 3.141930
[Epoch 83; Iter    20/   55] train: loss: 0.0008585
[Epoch 83; Iter    50/   55] train: loss: 0.0003368
[Epoch 83] ogbg-molbbbp: 0.947526 val loss: 0.705850
[Epoch 83] ogbg-molbbbp: 0.626640 test loss: 3.251328
[Epoch 84; Iter    25/   55] train: loss: 0.0017024
[Epoch 84; Iter    55/   55] train: loss: 0.0009729
[Epoch 84] ogbg-molbbbp: 0.951210 val loss: 0.704477
[Epoch 84] ogbg-molbbbp: 0.620081 test loss: 3.359731
[Epoch 85; Iter    30/   55] train: loss: 0.0012623
[Epoch 85] ogbg-molbbbp: 0.951011 val loss: 0.683132
[Epoch 85] ogbg-molbbbp: 0.623553 test loss: 3.219984
[Epoch 86; Iter     5/   55] train: loss: 0.0015578
[Epoch 86; Iter    35/   55] train: loss: 0.0015613
[Epoch 86] ogbg-molbbbp: 0.956288 val loss: 0.708892
[Epoch 86] ogbg-molbbbp: 0.628858 test loss: 3.566924
[Epoch 87; Iter    10/   55] train: loss: 0.0017646
[Epoch 87; Iter    40/   55] train: loss: 0.0006118
[Epoch 87] ogbg-molbbbp: 0.954396 val loss: 0.655853
[Epoch 87] ogbg-molbbbp: 0.633777 test loss: 3.186142
[Epoch 88; Iter    15/   55] train: loss: 0.0007739
[Epoch 88; Iter    45/   55] train: loss: 0.0029708
[Epoch 88] ogbg-molbbbp: 0.951210 val loss: 0.684934
[Epoch 88] ogbg-molbbbp: 0.640239 test loss: 3.071974
[Epoch 89; Iter    20/   55] train: loss: 0.0006192
[Epoch 89; Iter    50/   55] train: loss: 0.0012914
[Epoch 89] ogbg-molbbbp: 0.960868 val loss: 0.591500
[Epoch 89] ogbg-molbbbp: 0.648148 test loss: 2.974913
[Epoch 90; Iter    25/   55] train: loss: 0.0014220
[Epoch 90; Iter    55/   55] train: loss: 0.0020208
[Epoch 90] ogbg-molbbbp: 0.958877 val loss: 0.607752
[Epoch 90] ogbg-molbbbp: 0.644965 test loss: 2.974828
[Epoch 91; Iter    30/   55] train: loss: 0.0003444
[Epoch 91] ogbg-molbbbp: 0.959076 val loss: 0.604280
[Epoch 91] ogbg-molbbbp: 0.645351 test loss: 2.830560
[Epoch 92; Iter     5/   55] train: loss: 0.0003585
[Epoch 92; Iter    35/   55] train: loss: 0.0003100
[Epoch 92] ogbg-molbbbp: 0.954396 val loss: 0.660378
[Epoch 92] ogbg-molbbbp: 0.627122 test loss: 3.139049
[Epoch 93; Iter    10/   55] train: loss: 0.0003614
[Epoch 93; Iter    40/   55] train: loss: 0.0003201
[Epoch 93] ogbg-molbbbp: 0.953898 val loss: 0.677706
[Epoch 93] ogbg-molbbbp: 0.621817 test loss: 3.227242
[Epoch 94; Iter    15/   55] train: loss: 0.0001886
[Epoch 94; Iter    45/   55] train: loss: 0.0003589
[Epoch 94] ogbg-molbbbp: 0.955292 val loss: 0.664879
[Epoch 94] ogbg-molbbbp: 0.624518 test loss: 3.244515
[Epoch 95; Iter    20/   55] train: loss: 0.0321337
[Epoch 95; Iter    50/   55] train: loss: 0.0006256
[Epoch 95] ogbg-molbbbp: 0.955193 val loss: 0.655489
[Epoch 95] ogbg-molbbbp: 0.618924 test loss: 3.055779
[Epoch 96; Iter    25/   55] train: loss: 0.0004798
[Epoch 96; Iter    55/   55] train: loss: 0.0014904
[Epoch 96] ogbg-molbbbp: 0.955292 val loss: 0.665599
[Epoch 96] ogbg-molbbbp: 0.620177 test loss: 3.115015
[Epoch 97; Iter    30/   55] train: loss: 0.0050937
[Epoch 97] ogbg-molbbbp: 0.953301 val loss: 0.669661
[Epoch 97] ogbg-molbbbp: 0.625193 test loss: 3.160567
[Epoch 98; Iter     5/   55] train: loss: 0.0001588
[Epoch 98; Iter    35/   55] train: loss: 0.0015558
[Epoch 98] ogbg-molbbbp: 0.952803 val loss: 0.675183
[Epoch 98] ogbg-molbbbp: 0.626254 test loss: 3.138320
[Epoch 99; Iter    10/   55] train: loss: 0.0002238
[Epoch 99; Iter    40/   55] train: loss: 0.0000866
[Epoch 99] ogbg-molbbbp: 0.954794 val loss: 0.672597
[Epoch 99] ogbg-molbbbp: 0.625675 test loss: 3.127657
[Epoch 100; Iter    15/   55] train: loss: 0.0008847
[Epoch 100; Iter    45/   55] train: loss: 0.0004439
[Epoch 100] ogbg-molbbbp: 0.951907 val loss: 0.674059
[Epoch 100] ogbg-molbbbp: 0.618827 test loss: 3.148303
[Epoch 101; Iter    20/   55] train: loss: 0.0007893
[Epoch 101; Iter    50/   55] train: loss: 0.0002549
[Epoch 101] ogbg-molbbbp: 0.953600 val loss: 0.682608
[Epoch 101] ogbg-molbbbp: 0.622975 test loss: 3.286714
[Epoch 102; Iter    25/   55] train: loss: 0.0003725
[Epoch 102; Iter    55/   55] train: loss: 0.0001005
[Epoch 102] ogbg-molbbbp: 0.955292 val loss: 0.629826
[Epoch 102] ogbg-molbbbp: 0.637153 test loss: 3.022011
[Epoch 103; Iter    30/   55] train: loss: 0.0025144
[Epoch 103] ogbg-molbbbp: 0.949218 val loss: 0.672276
[Epoch 103] ogbg-molbbbp: 0.611883 test loss: 3.115070
[Epoch 104; Iter     5/   55] train: loss: 0.0001910
[Epoch 104; Iter    35/   55] train: loss: 0.0006358
[Epoch 104] ogbg-molbbbp: 0.952206 val loss: 0.673256
[Epoch 104] ogbg-molbbbp: 0.648727 test loss: 3.131296
[Epoch 105; Iter    10/   55] train: loss: 0.0005536
[Epoch 105; Iter    40/   55] train: loss: 0.0002414
[Epoch 105] ogbg-molbbbp: 0.950812 val loss: 0.714236
[Epoch 105] ogbg-molbbbp: 0.642554 test loss: 3.356760
[Epoch 106; Iter    15/   55] train: loss: 0.0003139
[Epoch 106; Iter    45/   55] train: loss: 0.0002237
[Epoch 106] ogbg-molbbbp: 0.954396 val loss: 0.622728
[Epoch 106] ogbg-molbbbp: 0.644965 test loss: 3.078174
[Epoch 107; Iter    20/   55] train: loss: 0.0003413
[Epoch 107; Iter    50/   55] train: loss: 0.0003772
[Epoch 107] ogbg-molbbbp: 0.952106 val loss: 0.653082
[Epoch 107] ogbg-molbbbp: 0.638696 test loss: 3.188456
[Epoch 108; Iter    25/   55] train: loss: 0.0006367
[Epoch 108; Iter    55/   55] train: loss: 0.0006138
[Epoch 108] ogbg-molbbbp: 0.950911 val loss: 0.704488
[Epoch 108] ogbg-molbbbp: 0.645930 test loss: 3.334877
[Epoch 109; Iter    30/   55] train: loss: 0.0001518
[Epoch 69; Iter    40/   55] train: loss: 0.0024413
[Epoch 69] ogbg-molbbbp: 0.939958 val loss: 0.671736
[Epoch 69] ogbg-molbbbp: 0.616898 test loss: 2.746354
[Epoch 70; Iter    15/   55] train: loss: 0.1397705
[Epoch 70; Iter    45/   55] train: loss: 0.0018570
[Epoch 70] ogbg-molbbbp: 0.895748 val loss: 0.905878
[Epoch 70] ogbg-molbbbp: 0.609471 test loss: 2.563891
[Epoch 71; Iter    20/   55] train: loss: 0.0037336
[Epoch 71; Iter    50/   55] train: loss: 0.0075671
[Epoch 71] ogbg-molbbbp: 0.946132 val loss: 0.530829
[Epoch 71] ogbg-molbbbp: 0.616127 test loss: 2.620207
[Epoch 72; Iter    25/   55] train: loss: 0.0009356
[Epoch 72; Iter    55/   55] train: loss: 0.0683367
[Epoch 72] ogbg-molbbbp: 0.931694 val loss: 0.694581
[Epoch 72] ogbg-molbbbp: 0.638310 test loss: 2.724122
[Epoch 73; Iter    30/   55] train: loss: 0.0008011
[Epoch 73] ogbg-molbbbp: 0.939062 val loss: 0.668523
[Epoch 73] ogbg-molbbbp: 0.640818 test loss: 2.742464
[Epoch 74; Iter     5/   55] train: loss: 0.0050699
[Epoch 74; Iter    35/   55] train: loss: 0.0636196
[Epoch 74] ogbg-molbbbp: 0.954695 val loss: 0.478484
[Epoch 74] ogbg-molbbbp: 0.622203 test loss: 2.643254
[Epoch 75; Iter    10/   55] train: loss: 0.0212051
[Epoch 75; Iter    40/   55] train: loss: 0.0148556
[Epoch 75] ogbg-molbbbp: 0.944439 val loss: 0.497964
[Epoch 75] ogbg-molbbbp: 0.638503 test loss: 2.141907
[Epoch 76; Iter    15/   55] train: loss: 0.0088404
[Epoch 76; Iter    45/   55] train: loss: 0.0058697
[Epoch 76] ogbg-molbbbp: 0.923927 val loss: 0.655238
[Epoch 76] ogbg-molbbbp: 0.650849 test loss: 2.528150
[Epoch 77; Iter    20/   55] train: loss: 0.0160261
[Epoch 77; Iter    50/   55] train: loss: 0.0046452
[Epoch 77] ogbg-molbbbp: 0.940556 val loss: 0.529743
[Epoch 77] ogbg-molbbbp: 0.638021 test loss: 2.509716
[Epoch 78; Iter    25/   55] train: loss: 0.0111197
[Epoch 78; Iter    55/   55] train: loss: 0.0832026
[Epoch 78] ogbg-molbbbp: 0.916061 val loss: 1.117236
[Epoch 78] ogbg-molbbbp: 0.589506 test loss: 3.335781
[Epoch 79; Iter    30/   55] train: loss: 0.0959769
[Epoch 79] ogbg-molbbbp: 0.921338 val loss: 0.723992
[Epoch 79] ogbg-molbbbp: 0.632909 test loss: 2.467824
[Epoch 80; Iter     5/   55] train: loss: 0.0845687
[Epoch 80; Iter    35/   55] train: loss: 0.0032103
[Epoch 80] ogbg-molbbbp: 0.920940 val loss: 0.817296
[Epoch 80] ogbg-molbbbp: 0.610050 test loss: 3.058032
[Epoch 81; Iter    10/   55] train: loss: 0.0021419
[Epoch 81; Iter    40/   55] train: loss: 0.0064391
[Epoch 81] ogbg-molbbbp: 0.930897 val loss: 0.741758
[Epoch 81] ogbg-molbbbp: 0.622975 test loss: 2.870553
[Epoch 82; Iter    15/   55] train: loss: 0.0013418
[Epoch 82; Iter    45/   55] train: loss: 0.0006444
[Epoch 82] ogbg-molbbbp: 0.935477 val loss: 0.657483
[Epoch 82] ogbg-molbbbp: 0.623264 test loss: 2.658527
[Epoch 83; Iter    20/   55] train: loss: 0.0019538
[Epoch 83; Iter    50/   55] train: loss: 0.0009626
[Epoch 83] ogbg-molbbbp: 0.935477 val loss: 0.645302
[Epoch 83] ogbg-molbbbp: 0.631366 test loss: 2.646336
[Epoch 84; Iter    25/   55] train: loss: 0.0056758
[Epoch 84; Iter    55/   55] train: loss: 0.0009953
[Epoch 84] ogbg-molbbbp: 0.937170 val loss: 0.671566
[Epoch 84] ogbg-molbbbp: 0.624904 test loss: 2.809305
[Epoch 85; Iter    30/   55] train: loss: 0.0017389
[Epoch 85] ogbg-molbbbp: 0.928806 val loss: 0.691259
[Epoch 85] ogbg-molbbbp: 0.625096 test loss: 2.633379
[Epoch 86; Iter     5/   55] train: loss: 0.0038536
[Epoch 86; Iter    35/   55] train: loss: 0.0013532
[Epoch 86] ogbg-molbbbp: 0.939361 val loss: 0.666604
[Epoch 86] ogbg-molbbbp: 0.610629 test loss: 2.946054
[Epoch 87; Iter    10/   55] train: loss: 0.0017794
[Epoch 87; Iter    40/   55] train: loss: 0.0009070
[Epoch 87] ogbg-molbbbp: 0.931096 val loss: 0.758515
[Epoch 87] ogbg-molbbbp: 0.608603 test loss: 3.026374
[Epoch 88; Iter    15/   55] train: loss: 0.0009025
[Epoch 88; Iter    45/   55] train: loss: 0.0011870
[Epoch 88] ogbg-molbbbp: 0.930499 val loss: 0.706039
[Epoch 88] ogbg-molbbbp: 0.611690 test loss: 2.860200
[Epoch 89; Iter    20/   55] train: loss: 0.0003534
[Epoch 89; Iter    50/   55] train: loss: 0.0004356
[Epoch 89] ogbg-molbbbp: 0.933586 val loss: 0.714340
[Epoch 89] ogbg-molbbbp: 0.613137 test loss: 2.883342
[Epoch 90; Iter    25/   55] train: loss: 0.0008964
[Epoch 90; Iter    55/   55] train: loss: 0.0037071
[Epoch 90] ogbg-molbbbp: 0.941750 val loss: 0.662024
[Epoch 90] ogbg-molbbbp: 0.617091 test loss: 2.880138
[Epoch 91; Iter    30/   55] train: loss: 0.0004331
[Epoch 91] ogbg-molbbbp: 0.941750 val loss: 0.671844
[Epoch 91] ogbg-molbbbp: 0.622106 test loss: 2.877268
[Epoch 92; Iter     5/   55] train: loss: 0.0012049
[Epoch 92; Iter    35/   55] train: loss: 0.0004932
[Epoch 92] ogbg-molbbbp: 0.937170 val loss: 0.725498
[Epoch 92] ogbg-molbbbp: 0.605228 test loss: 3.106713
[Epoch 93; Iter    10/   55] train: loss: 0.0004406
[Epoch 93; Iter    40/   55] train: loss: 0.0079013
[Epoch 93] ogbg-molbbbp: 0.933287 val loss: 0.719261
[Epoch 93] ogbg-molbbbp: 0.615162 test loss: 2.874908
[Epoch 94; Iter    15/   55] train: loss: 0.0005296
[Epoch 94; Iter    45/   55] train: loss: 0.0003038
[Epoch 94] ogbg-molbbbp: 0.927512 val loss: 0.789452
[Epoch 94] ogbg-molbbbp: 0.600694 test loss: 3.028282
[Epoch 95; Iter    20/   55] train: loss: 0.0173151
[Epoch 95; Iter    50/   55] train: loss: 0.0004880
[Epoch 95] ogbg-molbbbp: 0.926914 val loss: 0.755956
[Epoch 95] ogbg-molbbbp: 0.608507 test loss: 2.848349
[Epoch 96; Iter    25/   55] train: loss: 0.0003525
[Epoch 96; Iter    55/   55] train: loss: 0.0008114
[Epoch 96] ogbg-molbbbp: 0.934382 val loss: 0.731290
[Epoch 96] ogbg-molbbbp: 0.613715 test loss: 2.928729
[Epoch 97; Iter    30/   55] train: loss: 0.0295002
[Epoch 97] ogbg-molbbbp: 0.929503 val loss: 0.796739
[Epoch 97] ogbg-molbbbp: 0.618345 test loss: 3.051347
[Epoch 98; Iter     5/   55] train: loss: 0.0002571
[Epoch 98; Iter    35/   55] train: loss: 0.0007344
[Epoch 98] ogbg-molbbbp: 0.934382 val loss: 0.790143
[Epoch 98] ogbg-molbbbp: 0.617670 test loss: 3.134236
[Epoch 99; Iter    10/   55] train: loss: 0.0003402
[Epoch 99; Iter    40/   55] train: loss: 0.0001473
[Epoch 99] ogbg-molbbbp: 0.935776 val loss: 0.754285
[Epoch 99] ogbg-molbbbp: 0.615355 test loss: 3.087122
[Epoch 100; Iter    15/   55] train: loss: 0.0006129
[Epoch 100; Iter    45/   55] train: loss: 0.0006823
[Epoch 100] ogbg-molbbbp: 0.898636 val loss: 0.961968
[Epoch 100] ogbg-molbbbp: 0.603202 test loss: 2.837090
[Epoch 101; Iter    20/   55] train: loss: 0.0006417
[Epoch 101; Iter    50/   55] train: loss: 0.0005131
[Epoch 101] ogbg-molbbbp: 0.925520 val loss: 0.828275
[Epoch 101] ogbg-molbbbp: 0.607832 test loss: 3.060802
[Epoch 102; Iter    25/   55] train: loss: 0.0004113
[Epoch 102; Iter    55/   55] train: loss: 0.0005648
[Epoch 102] ogbg-molbbbp: 0.930101 val loss: 0.759745
[Epoch 102] ogbg-molbbbp: 0.642843 test loss: 2.840430
[Epoch 103; Iter    30/   55] train: loss: 0.0033163
[Epoch 103] ogbg-molbbbp: 0.929404 val loss: 0.754083
[Epoch 103] ogbg-molbbbp: 0.638310 test loss: 2.831136
[Epoch 104; Iter     5/   55] train: loss: 0.0002552
[Epoch 104; Iter    35/   55] train: loss: 0.0006901
[Epoch 104] ogbg-molbbbp: 0.930698 val loss: 0.847114
[Epoch 104] ogbg-molbbbp: 0.624325 test loss: 3.319099
[Epoch 105; Iter    10/   55] train: loss: 0.0020582
[Epoch 105; Iter    40/   55] train: loss: 0.0001560
[Epoch 105] ogbg-molbbbp: 0.927313 val loss: 0.971571
[Epoch 105] ogbg-molbbbp: 0.621431 test loss: 3.594128
[Epoch 106; Iter    15/   55] train: loss: 0.0001775
[Epoch 106; Iter    45/   55] train: loss: 0.0001509
[Epoch 106] ogbg-molbbbp: 0.928707 val loss: 0.846181
[Epoch 106] ogbg-molbbbp: 0.627122 test loss: 3.188862
[Epoch 107; Iter    20/   55] train: loss: 0.0000763
[Epoch 107; Iter    50/   55] train: loss: 0.0000663
[Epoch 107] ogbg-molbbbp: 0.935179 val loss: 0.805078
[Epoch 107] ogbg-molbbbp: 0.630498 test loss: 3.208122
[Epoch 108; Iter    25/   55] train: loss: 0.0001390
[Epoch 108; Iter    55/   55] train: loss: 0.0001122
[Epoch 108] ogbg-molbbbp: 0.929901 val loss: 0.898365
[Epoch 108] ogbg-molbbbp: 0.626061 test loss: 3.355043
[Epoch 109; Iter    30/   55] train: loss: 0.0003165
[Epoch 69; Iter    40/   55] train: loss: 0.0011522
[Epoch 69] ogbg-molbbbp: 0.904411 val loss: 0.867557
[Epoch 69] ogbg-molbbbp: 0.558642 test loss: 3.530771
[Epoch 70; Iter    15/   55] train: loss: 0.0083270
[Epoch 70; Iter    45/   55] train: loss: 0.0008997
[Epoch 70] ogbg-molbbbp: 0.884895 val loss: 1.233077
[Epoch 70] ogbg-molbbbp: 0.559606 test loss: 3.000419
[Epoch 71; Iter    20/   55] train: loss: 0.0025312
[Epoch 71; Iter    50/   55] train: loss: 0.0025839
[Epoch 71] ogbg-molbbbp: 0.884795 val loss: 1.323687
[Epoch 71] ogbg-molbbbp: 0.572242 test loss: 3.401817
[Epoch 72; Iter    25/   55] train: loss: 0.0004489
[Epoch 72; Iter    55/   55] train: loss: 0.0373724
[Epoch 72] ogbg-molbbbp: 0.909589 val loss: 0.874507
[Epoch 72] ogbg-molbbbp: 0.570120 test loss: 3.853837
[Epoch 73; Iter    30/   55] train: loss: 0.0027846
[Epoch 73] ogbg-molbbbp: 0.892562 val loss: 1.003363
[Epoch 73] ogbg-molbbbp: 0.562211 test loss: 3.719489
[Epoch 74; Iter     5/   55] train: loss: 0.0002872
[Epoch 74; Iter    35/   55] train: loss: 0.0006754
[Epoch 74] ogbg-molbbbp: 0.902320 val loss: 0.821881
[Epoch 74] ogbg-molbbbp: 0.573013 test loss: 3.128634
[Epoch 75; Iter    10/   55] train: loss: 0.0006065
[Epoch 75; Iter    40/   55] train: loss: 0.0009754
[Epoch 75] ogbg-molbbbp: 0.895549 val loss: 0.890771
[Epoch 75] ogbg-molbbbp: 0.571759 test loss: 2.513635
[Epoch 76; Iter    15/   55] train: loss: 0.0048238
[Epoch 76; Iter    45/   55] train: loss: 0.0014039
[Epoch 76] ogbg-molbbbp: 0.902619 val loss: 2.899517
[Epoch 76] ogbg-molbbbp: 0.600598 test loss: 5.614815
[Epoch 77; Iter    20/   55] train: loss: 0.0147344
[Epoch 77; Iter    50/   55] train: loss: 0.0239249
[Epoch 77] ogbg-molbbbp: 0.902519 val loss: 2.594695
[Epoch 77] ogbg-molbbbp: 0.607253 test loss: 4.224690
[Epoch 78; Iter    25/   55] train: loss: 0.0099013
[Epoch 78; Iter    55/   55] train: loss: 0.0031754
[Epoch 78] ogbg-molbbbp: 0.920243 val loss: 1.374777
[Epoch 78] ogbg-molbbbp: 0.600887 test loss: 5.618512
[Epoch 79; Iter    30/   55] train: loss: 0.0009085
[Epoch 79] ogbg-molbbbp: 0.904311 val loss: 2.554766
[Epoch 79] ogbg-molbbbp: 0.607350 test loss: 6.850832
[Epoch 80; Iter     5/   55] train: loss: 0.0307493
[Epoch 80; Iter    35/   55] train: loss: 0.0005207
[Epoch 80] ogbg-molbbbp: 0.901523 val loss: 1.206902
[Epoch 80] ogbg-molbbbp: 0.603684 test loss: 4.751163
[Epoch 81; Iter    10/   55] train: loss: 0.0005938
[Epoch 81; Iter    40/   55] train: loss: 0.0048024
[Epoch 81] ogbg-molbbbp: 0.908095 val loss: 0.954798
[Epoch 81] ogbg-molbbbp: 0.592303 test loss: 4.782572
[Epoch 82; Iter    15/   55] train: loss: 0.0007516
[Epoch 82; Iter    45/   55] train: loss: 0.0004286
[Epoch 82] ogbg-molbbbp: 0.897640 val loss: 1.140431
[Epoch 82] ogbg-molbbbp: 0.607157 test loss: 3.332311
[Epoch 83; Iter    20/   55] train: loss: 0.0108151
[Epoch 83; Iter    50/   55] train: loss: 0.0007001
[Epoch 83] ogbg-molbbbp: 0.896545 val loss: 1.053440
[Epoch 83] ogbg-molbbbp: 0.595293 test loss: 3.373135
[Epoch 84; Iter    25/   55] train: loss: 0.0181609
[Epoch 84; Iter    55/   55] train: loss: 0.0152175
[Epoch 84] ogbg-molbbbp: 0.891367 val loss: 1.181288
[Epoch 84] ogbg-molbbbp: 0.596644 test loss: 2.878249
[Epoch 85; Iter    30/   55] train: loss: 0.0020949
[Epoch 85] ogbg-molbbbp: 0.912775 val loss: 0.753012
[Epoch 85] ogbg-molbbbp: 0.595968 test loss: 2.584928
[Epoch 86; Iter     5/   55] train: loss: 0.0023502
[Epoch 86; Iter    35/   55] train: loss: 0.0054384
[Epoch 86] ogbg-molbbbp: 0.921537 val loss: 0.815546
[Epoch 86] ogbg-molbbbp: 0.584780 test loss: 3.447662
[Epoch 87; Iter    10/   55] train: loss: 0.0120996
[Epoch 87; Iter    40/   55] train: loss: 0.0002421
[Epoch 87] ogbg-molbbbp: 0.912078 val loss: 0.937563
[Epoch 87] ogbg-molbbbp: 0.597608 test loss: 3.033681
[Epoch 88; Iter    15/   55] train: loss: 0.0003603
[Epoch 88; Iter    45/   55] train: loss: 0.0050634
[Epoch 88] ogbg-molbbbp: 0.922334 val loss: 0.675172
[Epoch 88] ogbg-molbbbp: 0.607060 test loss: 3.542132
[Epoch 89; Iter    20/   55] train: loss: 0.0005832
[Epoch 89; Iter    50/   55] train: loss: 0.0002620
[Epoch 89] ogbg-molbbbp: 0.923230 val loss: 0.728903
[Epoch 89] ogbg-molbbbp: 0.597994 test loss: 4.004125
[Epoch 90; Iter    25/   55] train: loss: 0.0015601
[Epoch 90; Iter    55/   55] train: loss: 0.0011743
[Epoch 90] ogbg-molbbbp: 0.911879 val loss: 0.766867
[Epoch 90] ogbg-molbbbp: 0.601755 test loss: 3.659300
[Epoch 91; Iter    30/   55] train: loss: 0.0007163
[Epoch 91] ogbg-molbbbp: 0.918351 val loss: 0.748004
[Epoch 91] ogbg-molbbbp: 0.603009 test loss: 4.141259
[Epoch 92; Iter     5/   55] train: loss: 0.0008853
[Epoch 92; Iter    35/   55] train: loss: 0.0005510
[Epoch 92] ogbg-molbbbp: 0.920243 val loss: 0.733347
[Epoch 92] ogbg-molbbbp: 0.602623 test loss: 3.697296
[Epoch 93; Iter    10/   55] train: loss: 0.0007674
[Epoch 93; Iter    40/   55] train: loss: 0.0003689
[Epoch 93] ogbg-molbbbp: 0.915065 val loss: 0.804764
[Epoch 93] ogbg-molbbbp: 0.598958 test loss: 3.393270
[Epoch 94; Iter    15/   55] train: loss: 0.0023897
[Epoch 94; Iter    45/   55] train: loss: 0.0007871
[Epoch 94] ogbg-molbbbp: 0.925819 val loss: 0.585570
[Epoch 94] ogbg-molbbbp: 0.595293 test loss: 3.530355
[Epoch 95; Iter    20/   55] train: loss: 0.0699867
[Epoch 95; Iter    50/   55] train: loss: 0.0032298
[Epoch 95] ogbg-molbbbp: 0.925620 val loss: 0.667073
[Epoch 95] ogbg-molbbbp: 0.612269 test loss: 3.358708
[Epoch 96; Iter    25/   55] train: loss: 0.0010536
[Epoch 96; Iter    55/   55] train: loss: 0.0006880
[Epoch 96] ogbg-molbbbp: 0.923230 val loss: 0.754574
[Epoch 96] ogbg-molbbbp: 0.597512 test loss: 3.186927
[Epoch 97; Iter    30/   55] train: loss: 0.0261927
[Epoch 97] ogbg-molbbbp: 0.915065 val loss: 0.796054
[Epoch 97] ogbg-molbbbp: 0.586227 test loss: 3.253868
[Epoch 98; Iter     5/   55] train: loss: 0.0005141
[Epoch 98; Iter    35/   55] train: loss: 0.0026009
[Epoch 98] ogbg-molbbbp: 0.922234 val loss: 0.729742
[Epoch 98] ogbg-molbbbp: 0.593171 test loss: 3.345320
[Epoch 99; Iter    10/   55] train: loss: 0.0003099
[Epoch 99; Iter    40/   55] train: loss: 0.0002108
[Epoch 99] ogbg-molbbbp: 0.923230 val loss: 0.709271
[Epoch 99] ogbg-molbbbp: 0.592978 test loss: 3.568515
[Epoch 100; Iter    15/   55] train: loss: 0.0005142
[Epoch 100; Iter    45/   55] train: loss: 0.0009279
[Epoch 100] ogbg-molbbbp: 0.919446 val loss: 0.757933
[Epoch 100] ogbg-molbbbp: 0.592110 test loss: 3.400150
[Epoch 101; Iter    20/   55] train: loss: 0.0007054
[Epoch 101; Iter    50/   55] train: loss: 0.0002238
[Epoch 101] ogbg-molbbbp: 0.919546 val loss: 0.781150
[Epoch 101] ogbg-molbbbp: 0.586034 test loss: 3.153051
[Epoch 102; Iter    25/   55] train: loss: 0.0003256
[Epoch 102; Iter    55/   55] train: loss: 0.0001782
[Epoch 102] ogbg-molbbbp: 0.918451 val loss: 0.909878
[Epoch 102] ogbg-molbbbp: 0.563079 test loss: 3.873197
[Epoch 103; Iter    30/   55] train: loss: 0.0047769
[Epoch 103] ogbg-molbbbp: 0.922434 val loss: 0.791820
[Epoch 103] ogbg-molbbbp: 0.564911 test loss: 4.172456
[Epoch 104; Iter     5/   55] train: loss: 0.0005787
[Epoch 104; Iter    35/   55] train: loss: 0.0009458
[Epoch 104] ogbg-molbbbp: 0.920940 val loss: 0.735831
[Epoch 104] ogbg-molbbbp: 0.583623 test loss: 3.594427
[Epoch 105; Iter    10/   55] train: loss: 0.0016056
[Epoch 105; Iter    40/   55] train: loss: 0.0001761
[Epoch 105] ogbg-molbbbp: 0.919446 val loss: 0.792868
[Epoch 105] ogbg-molbbbp: 0.578993 test loss: 3.385935
[Epoch 106; Iter    15/   55] train: loss: 0.0003382
[Epoch 106; Iter    45/   55] train: loss: 0.0002132
[Epoch 106] ogbg-molbbbp: 0.918351 val loss: 0.776808
[Epoch 106] ogbg-molbbbp: 0.597608 test loss: 3.167152
[Epoch 107; Iter    20/   55] train: loss: 0.0002412
[Epoch 107; Iter    50/   55] train: loss: 0.0001441
[Epoch 107] ogbg-molbbbp: 0.922832 val loss: 0.754759
[Epoch 107] ogbg-molbbbp: 0.599344 test loss: 3.297679
[Epoch 108; Iter    25/   55] train: loss: 0.0002389
[Epoch 108; Iter    55/   55] train: loss: 0.0005596
[Epoch 108] ogbg-molbbbp: 0.922533 val loss: 0.819483
[Epoch 108] ogbg-molbbbp: 0.594425 test loss: 3.270050
[Epoch 109; Iter    30/   55] train: loss: 0.0001204
[Epoch 69; Iter    40/   55] train: loss: 0.0031301
[Epoch 69] ogbg-molbbbp: 0.929901 val loss: 0.709376
[Epoch 69] ogbg-molbbbp: 0.664641 test loss: 2.084709
[Epoch 70; Iter    15/   55] train: loss: 0.0013133
[Epoch 70; Iter    45/   55] train: loss: 0.0046608
[Epoch 70] ogbg-molbbbp: 0.931893 val loss: 0.661322
[Epoch 70] ogbg-molbbbp: 0.668692 test loss: 1.998040
[Epoch 71; Iter    20/   55] train: loss: 0.0071480
[Epoch 71; Iter    50/   55] train: loss: 0.1149177
[Epoch 71] ogbg-molbbbp: 0.942447 val loss: 0.552072
[Epoch 71] ogbg-molbbbp: 0.664255 test loss: 2.012706
[Epoch 72; Iter    25/   55] train: loss: 0.0051537
[Epoch 72; Iter    55/   55] train: loss: 0.0014940
[Epoch 72] ogbg-molbbbp: 0.932988 val loss: 0.687813
[Epoch 72] ogbg-molbbbp: 0.673900 test loss: 2.067634
[Epoch 73; Iter    30/   55] train: loss: 0.0013738
[Epoch 73] ogbg-molbbbp: 0.930101 val loss: 0.715381
[Epoch 73] ogbg-molbbbp: 0.666667 test loss: 2.087719
[Epoch 74; Iter     5/   55] train: loss: 0.0003602
[Epoch 74; Iter    35/   55] train: loss: 0.0336707
[Epoch 74] ogbg-molbbbp: 0.949119 val loss: 0.532159
[Epoch 74] ogbg-molbbbp: 0.671103 test loss: 2.200847
[Epoch 75; Iter    10/   55] train: loss: 0.0074225
[Epoch 75; Iter    40/   55] train: loss: 0.0005847
[Epoch 75] ogbg-molbbbp: 0.949318 val loss: 0.522313
[Epoch 75] ogbg-molbbbp: 0.642072 test loss: 2.402587
[Epoch 76; Iter    15/   55] train: loss: 0.0185072
[Epoch 76; Iter    45/   55] train: loss: 0.0179727
[Epoch 76] ogbg-molbbbp: 0.945932 val loss: 0.579913
[Epoch 76] ogbg-molbbbp: 0.659433 test loss: 2.311100
[Epoch 77; Iter    20/   55] train: loss: 0.0009045
[Epoch 77; Iter    50/   55] train: loss: 0.0005502
[Epoch 77] ogbg-molbbbp: 0.951110 val loss: 0.531975
[Epoch 77] ogbg-molbbbp: 0.645737 test loss: 2.374099
[Epoch 78; Iter    25/   55] train: loss: 0.0001483
[Epoch 78; Iter    55/   55] train: loss: 0.0028556
[Epoch 78] ogbg-molbbbp: 0.956089 val loss: 0.479796
[Epoch 78] ogbg-molbbbp: 0.636574 test loss: 2.326219
[Epoch 79; Iter    30/   55] train: loss: 0.0040467
[Epoch 79] ogbg-molbbbp: 0.953102 val loss: 0.483043
[Epoch 79] ogbg-molbbbp: 0.645737 test loss: 2.469114
[Epoch 80; Iter     5/   55] train: loss: 0.0016386
[Epoch 80; Iter    35/   55] train: loss: 0.0030483
[Epoch 80] ogbg-molbbbp: 0.943742 val loss: 0.547299
[Epoch 80] ogbg-molbbbp: 0.650367 test loss: 2.455504
[Epoch 81; Iter    10/   55] train: loss: 0.0010558
[Epoch 81; Iter    40/   55] train: loss: 0.0020469
[Epoch 81] ogbg-molbbbp: 0.942447 val loss: 0.594194
[Epoch 81] ogbg-molbbbp: 0.632620 test loss: 2.586030
[Epoch 82; Iter    15/   55] train: loss: 0.0007950
[Epoch 82; Iter    45/   55] train: loss: 0.0002506
[Epoch 82] ogbg-molbbbp: 0.942746 val loss: 0.603333
[Epoch 82] ogbg-molbbbp: 0.629437 test loss: 2.661666
[Epoch 83; Iter    20/   55] train: loss: 0.0074603
[Epoch 83; Iter    50/   55] train: loss: 0.0002843
[Epoch 83] ogbg-molbbbp: 0.942348 val loss: 0.606921
[Epoch 83] ogbg-molbbbp: 0.632427 test loss: 2.613910
[Epoch 84; Iter    25/   55] train: loss: 0.0021790
[Epoch 84; Iter    55/   55] train: loss: 0.0030922
[Epoch 84] ogbg-molbbbp: 0.948721 val loss: 0.548938
[Epoch 84] ogbg-molbbbp: 0.650463 test loss: 2.353114
[Epoch 85; Iter    30/   55] train: loss: 0.0003504
[Epoch 85] ogbg-molbbbp: 0.949517 val loss: 0.577655
[Epoch 85] ogbg-molbbbp: 0.658951 test loss: 2.636746
[Epoch 86; Iter     5/   55] train: loss: 0.0025636
[Epoch 86; Iter    35/   55] train: loss: 0.0091517
[Epoch 86] ogbg-molbbbp: 0.943841 val loss: 0.664123
[Epoch 86] ogbg-molbbbp: 0.638889 test loss: 3.124397
[Epoch 87; Iter    10/   55] train: loss: 0.0017139
[Epoch 87; Iter    40/   55] train: loss: 0.0026714
[Epoch 87] ogbg-molbbbp: 0.945634 val loss: 0.598633
[Epoch 87] ogbg-molbbbp: 0.644676 test loss: 2.815837
[Epoch 88; Iter    15/   55] train: loss: 0.0051566
[Epoch 88; Iter    45/   55] train: loss: 0.0004237
[Epoch 88] ogbg-molbbbp: 0.938066 val loss: 0.656139
[Epoch 88] ogbg-molbbbp: 0.640721 test loss: 2.820150
[Epoch 89; Iter    20/   55] train: loss: 0.0011640
[Epoch 89; Iter    50/   55] train: loss: 0.0003238
[Epoch 89] ogbg-molbbbp: 0.945136 val loss: 0.594293
[Epoch 89] ogbg-molbbbp: 0.643711 test loss: 2.657386
[Epoch 90; Iter    25/   55] train: loss: 0.0004791
[Epoch 90; Iter    55/   55] train: loss: 0.0335799
[Epoch 90] ogbg-molbbbp: 0.886787 val loss: 1.147314
[Epoch 90] ogbg-molbbbp: 0.642265 test loss: 2.279064
[Epoch 91; Iter    30/   55] train: loss: 0.0020441
[Epoch 91] ogbg-molbbbp: 0.919247 val loss: 1.005105
[Epoch 91] ogbg-molbbbp: 0.648148 test loss: 2.720468
[Epoch 92; Iter     5/   55] train: loss: 0.0003355
[Epoch 92; Iter    35/   55] train: loss: 0.0008880
[Epoch 92] ogbg-molbbbp: 0.922533 val loss: 0.932666
[Epoch 92] ogbg-molbbbp: 0.647087 test loss: 2.808724
[Epoch 93; Iter    10/   55] train: loss: 0.0005246
[Epoch 93; Iter    40/   55] train: loss: 0.0001848
[Epoch 93] ogbg-molbbbp: 0.926217 val loss: 0.808047
[Epoch 93] ogbg-molbbbp: 0.644290 test loss: 2.695156
[Epoch 94; Iter    15/   55] train: loss: 0.0003603
[Epoch 94; Iter    45/   55] train: loss: 0.0016776
[Epoch 94] ogbg-molbbbp: 0.920840 val loss: 0.910271
[Epoch 94] ogbg-molbbbp: 0.652199 test loss: 2.679278
[Epoch 95; Iter    20/   55] train: loss: 0.0005906
[Epoch 95; Iter    50/   55] train: loss: 0.0003373
[Epoch 95] ogbg-molbbbp: 0.927412 val loss: 0.803248
[Epoch 95] ogbg-molbbbp: 0.652585 test loss: 2.657534
[Epoch 96; Iter    25/   55] train: loss: 0.0028628
[Epoch 96; Iter    55/   55] train: loss: 0.0014188
[Epoch 96] ogbg-molbbbp: 0.932391 val loss: 0.739079
[Epoch 96] ogbg-molbbbp: 0.652874 test loss: 2.650613
[Epoch 97; Iter    30/   55] train: loss: 0.0016874
[Epoch 97] ogbg-molbbbp: 0.935677 val loss: 0.671177
[Epoch 97] ogbg-molbbbp: 0.661748 test loss: 2.529062
[Epoch 98; Iter     5/   55] train: loss: 0.0003192
[Epoch 98; Iter    35/   55] train: loss: 0.0045503
[Epoch 98] ogbg-molbbbp: 0.937270 val loss: 0.664235
[Epoch 98] ogbg-molbbbp: 0.646701 test loss: 2.595975
[Epoch 99; Iter    10/   55] train: loss: 0.0043494
[Epoch 99; Iter    40/   55] train: loss: 0.0003881
[Epoch 99] ogbg-molbbbp: 0.929901 val loss: 0.851583
[Epoch 99] ogbg-molbbbp: 0.641107 test loss: 3.048246
[Epoch 100; Iter    15/   55] train: loss: 0.0004127
[Epoch 100; Iter    45/   55] train: loss: 0.0018787
[Epoch 100] ogbg-molbbbp: 0.933586 val loss: 0.801638
[Epoch 100] ogbg-molbbbp: 0.644869 test loss: 3.020903
[Epoch 101; Iter    20/   55] train: loss: 0.0002852
[Epoch 101; Iter    50/   55] train: loss: 0.0018002
[Epoch 101] ogbg-molbbbp: 0.933486 val loss: 0.751122
[Epoch 101] ogbg-molbbbp: 0.641782 test loss: 3.010980
[Epoch 102; Iter    25/   55] train: loss: 0.0038351
[Epoch 102; Iter    55/   55] train: loss: 0.0001713
[Epoch 102] ogbg-molbbbp: 0.935577 val loss: 0.914968
[Epoch 102] ogbg-molbbbp: 0.653067 test loss: 3.150344
[Epoch 103; Iter    30/   55] train: loss: 0.0002577
[Epoch 103] ogbg-molbbbp: 0.939859 val loss: 0.766840
[Epoch 103] ogbg-molbbbp: 0.637828 test loss: 2.868205
[Epoch 104; Iter     5/   55] train: loss: 0.0002794
[Epoch 104; Iter    35/   55] train: loss: 0.0030772
[Epoch 104] ogbg-molbbbp: 0.939659 val loss: 0.622296
[Epoch 104] ogbg-molbbbp: 0.652585 test loss: 2.565221
[Epoch 105; Iter    10/   55] train: loss: 0.0019168
[Epoch 105; Iter    40/   55] train: loss: 0.0012718
[Epoch 105] ogbg-molbbbp: 0.933287 val loss: 0.633734
[Epoch 105] ogbg-molbbbp: 0.658179 test loss: 2.371849
[Epoch 106; Iter    15/   55] train: loss: 0.0001579
[Epoch 106; Iter    45/   55] train: loss: 0.0004213
[Epoch 106] ogbg-molbbbp: 0.952903 val loss: 0.527881
[Epoch 106] ogbg-molbbbp: 0.661748 test loss: 2.493628
[Epoch 107; Iter    20/   55] train: loss: 0.0012360
[Epoch 107; Iter    50/   55] train: loss: 0.0013875
[Epoch 107] ogbg-molbbbp: 0.950812 val loss: 0.525491
[Epoch 107] ogbg-molbbbp: 0.663194 test loss: 2.500790
[Epoch 108; Iter    25/   55] train: loss: 0.0001761
[Epoch 108; Iter    55/   55] train: loss: 0.0069344
[Epoch 108] ogbg-molbbbp: 0.956587 val loss: 0.504609
[Epoch 108] ogbg-molbbbp: 0.638985 test loss: 2.724432
[Epoch 109; Iter    30/   55] train: loss: 0.0002477
[Epoch 69; Iter    40/   55] train: loss: 0.0074070
[Epoch 69] ogbg-molbbbp: 0.948322 val loss: 0.591529
[Epoch 69] ogbg-molbbbp: 0.643711 test loss: 2.337705
[Epoch 70; Iter    15/   55] train: loss: 0.2180062
[Epoch 70; Iter    45/   55] train: loss: 0.0328834
[Epoch 70] ogbg-molbbbp: 0.933984 val loss: 0.733478
[Epoch 70] ogbg-molbbbp: 0.646412 test loss: 2.628889
[Epoch 71; Iter    20/   55] train: loss: 0.1322563
[Epoch 71; Iter    50/   55] train: loss: 0.2227691
[Epoch 71] ogbg-molbbbp: 0.947824 val loss: 0.697356
[Epoch 71] ogbg-molbbbp: 0.643326 test loss: 2.890127
[Epoch 72; Iter    25/   55] train: loss: 0.0067993
[Epoch 72; Iter    55/   55] train: loss: 0.0016672
[Epoch 72] ogbg-molbbbp: 0.929503 val loss: 0.899460
[Epoch 72] ogbg-molbbbp: 0.644965 test loss: 2.721759
[Epoch 73; Iter    30/   55] train: loss: 0.0037879
[Epoch 73] ogbg-molbbbp: 0.946629 val loss: 0.627775
[Epoch 73] ogbg-molbbbp: 0.650559 test loss: 2.662646
[Epoch 74; Iter     5/   55] train: loss: 0.0007903
[Epoch 74; Iter    35/   55] train: loss: 0.0066804
[Epoch 74] ogbg-molbbbp: 0.956388 val loss: 0.406584
[Epoch 74] ogbg-molbbbp: 0.636671 test loss: 2.180478
[Epoch 75; Iter    10/   55] train: loss: 0.1159271
[Epoch 75; Iter    40/   55] train: loss: 0.0122008
[Epoch 75] ogbg-molbbbp: 0.945036 val loss: 0.757883
[Epoch 75] ogbg-molbbbp: 0.646412 test loss: 2.516913
[Epoch 76; Iter    15/   55] train: loss: 0.0306610
[Epoch 76; Iter    45/   55] train: loss: 0.0296136
[Epoch 76] ogbg-molbbbp: 0.942149 val loss: 0.678649
[Epoch 76] ogbg-molbbbp: 0.660108 test loss: 2.097076
[Epoch 77; Iter    20/   55] train: loss: 0.0013166
[Epoch 77; Iter    50/   55] train: loss: 0.0009744
[Epoch 77] ogbg-molbbbp: 0.946430 val loss: 0.643147
[Epoch 77] ogbg-molbbbp: 0.641975 test loss: 2.604107
[Epoch 78; Iter    25/   55] train: loss: 0.0009390
[Epoch 78; Iter    55/   55] train: loss: 0.0019368
[Epoch 78] ogbg-molbbbp: 0.954595 val loss: 0.589779
[Epoch 78] ogbg-molbbbp: 0.618056 test loss: 2.902260
[Epoch 79; Iter    30/   55] train: loss: 0.0175249
[Epoch 79] ogbg-molbbbp: 0.959176 val loss: 0.601359
[Epoch 79] ogbg-molbbbp: 0.628183 test loss: 3.736722
[Epoch 80; Iter     5/   55] train: loss: 0.0012170
[Epoch 80; Iter    35/   55] train: loss: 0.0139684
[Epoch 80] ogbg-molbbbp: 0.942547 val loss: 0.841070
[Epoch 80] ogbg-molbbbp: 0.667535 test loss: 2.847780
[Epoch 81; Iter    10/   55] train: loss: 0.0007940
[Epoch 81; Iter    40/   55] train: loss: 0.0114393
[Epoch 81] ogbg-molbbbp: 0.928906 val loss: 0.908851
[Epoch 81] ogbg-molbbbp: 0.676119 test loss: 2.327861
[Epoch 82; Iter    15/   55] train: loss: 0.0057604
[Epoch 82; Iter    45/   55] train: loss: 0.0027159
[Epoch 82] ogbg-molbbbp: 0.943742 val loss: 0.690318
[Epoch 82] ogbg-molbbbp: 0.644676 test loss: 2.705241
[Epoch 83; Iter    20/   55] train: loss: 0.0024635
[Epoch 83; Iter    50/   55] train: loss: 0.0041115
[Epoch 83] ogbg-molbbbp: 0.947227 val loss: 0.685627
[Epoch 83] ogbg-molbbbp: 0.639564 test loss: 2.808890
[Epoch 84; Iter    25/   55] train: loss: 0.0022037
[Epoch 84; Iter    55/   55] train: loss: 0.0026785
[Epoch 84] ogbg-molbbbp: 0.952006 val loss: 0.736678
[Epoch 84] ogbg-molbbbp: 0.625000 test loss: 2.935443
[Epoch 85; Iter    30/   55] train: loss: 0.0012991
[Epoch 85] ogbg-molbbbp: 0.943742 val loss: 0.710854
[Epoch 85] ogbg-molbbbp: 0.611111 test loss: 3.041038
[Epoch 86; Iter     5/   55] train: loss: 0.0006522
[Epoch 86; Iter    35/   55] train: loss: 0.0006973
[Epoch 86] ogbg-molbbbp: 0.950612 val loss: 0.675045
[Epoch 86] ogbg-molbbbp: 0.638407 test loss: 2.854545
[Epoch 87; Iter    10/   55] train: loss: 0.0004152
[Epoch 87; Iter    40/   55] train: loss: 0.0009512
[Epoch 87] ogbg-molbbbp: 0.951509 val loss: 0.645573
[Epoch 87] ogbg-molbbbp: 0.640239 test loss: 2.883293
[Epoch 88; Iter    15/   55] train: loss: 0.0002203
[Epoch 88; Iter    45/   55] train: loss: 0.0006670
[Epoch 88] ogbg-molbbbp: 0.947526 val loss: 0.744600
[Epoch 88] ogbg-molbbbp: 0.637731 test loss: 3.034256
[Epoch 89; Iter    20/   55] train: loss: 0.0021957
[Epoch 89; Iter    50/   55] train: loss: 0.0009507
[Epoch 89] ogbg-molbbbp: 0.949318 val loss: 0.707331
[Epoch 89] ogbg-molbbbp: 0.639564 test loss: 2.950185
[Epoch 90; Iter    25/   55] train: loss: 0.0002818
[Epoch 90; Iter    55/   55] train: loss: 0.0122910
[Epoch 90] ogbg-molbbbp: 0.944638 val loss: 0.724424
[Epoch 90] ogbg-molbbbp: 0.640818 test loss: 2.746142
[Epoch 91; Iter    30/   55] train: loss: 0.0009298
[Epoch 91] ogbg-molbbbp: 0.947625 val loss: 0.681769
[Epoch 91] ogbg-molbbbp: 0.646316 test loss: 2.659266
[Epoch 92; Iter     5/   55] train: loss: 0.0002898
[Epoch 92; Iter    35/   55] train: loss: 0.0001865
[Epoch 92] ogbg-molbbbp: 0.950115 val loss: 0.638525
[Epoch 92] ogbg-molbbbp: 0.647569 test loss: 2.723414
[Epoch 93; Iter    10/   55] train: loss: 0.0002272
[Epoch 93; Iter    40/   55] train: loss: 0.0003804
[Epoch 93] ogbg-molbbbp: 0.950314 val loss: 0.660935
[Epoch 93] ogbg-molbbbp: 0.649113 test loss: 2.856060
[Epoch 94; Iter    15/   55] train: loss: 0.0001440
[Epoch 94; Iter    45/   55] train: loss: 0.0007822
[Epoch 94] ogbg-molbbbp: 0.949418 val loss: 0.760288
[Epoch 94] ogbg-molbbbp: 0.639275 test loss: 3.035777
[Epoch 95; Iter    20/   55] train: loss: 0.0013534
[Epoch 95; Iter    50/   55] train: loss: 0.0003277
[Epoch 95] ogbg-molbbbp: 0.952106 val loss: 0.635762
[Epoch 95] ogbg-molbbbp: 0.634549 test loss: 3.012216
[Epoch 96; Iter    25/   55] train: loss: 0.0005003
[Epoch 96; Iter    55/   55] train: loss: 0.0006855
[Epoch 96] ogbg-molbbbp: 0.953002 val loss: 0.664935
[Epoch 96] ogbg-molbbbp: 0.652199 test loss: 2.904072
[Epoch 97; Iter    30/   55] train: loss: 0.0023423
[Epoch 97] ogbg-molbbbp: 0.954396 val loss: 0.633960
[Epoch 97] ogbg-molbbbp: 0.634259 test loss: 3.046802
[Epoch 98; Iter     5/   55] train: loss: 0.0001814
[Epoch 98; Iter    35/   55] train: loss: 0.0039750
[Epoch 98] ogbg-molbbbp: 0.954595 val loss: 0.689545
[Epoch 98] ogbg-molbbbp: 0.651138 test loss: 2.982624
[Epoch 99; Iter    10/   55] train: loss: 0.0000994
[Epoch 99; Iter    40/   55] train: loss: 0.0001950
[Epoch 99] ogbg-molbbbp: 0.950413 val loss: 0.805122
[Epoch 99] ogbg-molbbbp: 0.642747 test loss: 3.187627
[Epoch 100; Iter    15/   55] train: loss: 0.0001127
[Epoch 100; Iter    45/   55] train: loss: 0.0025444
[Epoch 100] ogbg-molbbbp: 0.953400 val loss: 0.663317
[Epoch 100] ogbg-molbbbp: 0.643615 test loss: 3.040017
[Epoch 101; Iter    20/   55] train: loss: 0.0002072
[Epoch 101; Iter    50/   55] train: loss: 0.0009375
[Epoch 101] ogbg-molbbbp: 0.953201 val loss: 0.698143
[Epoch 101] ogbg-molbbbp: 0.641397 test loss: 3.149851
[Epoch 102; Iter    25/   55] train: loss: 0.0030960
[Epoch 102; Iter    55/   55] train: loss: 0.0001570
[Epoch 102] ogbg-molbbbp: 0.953500 val loss: 0.724084
[Epoch 102] ogbg-molbbbp: 0.648245 test loss: 3.127059
[Epoch 103; Iter    30/   55] train: loss: 0.0000527
[Epoch 103] ogbg-molbbbp: 0.954794 val loss: 0.717521
[Epoch 103] ogbg-molbbbp: 0.650077 test loss: 3.194002
[Epoch 104; Iter     5/   55] train: loss: 0.0001728
[Epoch 104; Iter    35/   55] train: loss: 0.0004914
[Epoch 104] ogbg-molbbbp: 0.950413 val loss: 0.776968
[Epoch 104] ogbg-molbbbp: 0.648920 test loss: 3.202181
[Epoch 105; Iter    10/   55] train: loss: 0.0001841
[Epoch 105; Iter    40/   55] train: loss: 0.0001319
[Epoch 105] ogbg-molbbbp: 0.949318 val loss: 0.804034
[Epoch 105] ogbg-molbbbp: 0.644194 test loss: 3.358191
[Epoch 106; Iter    15/   55] train: loss: 0.0000558
[Epoch 106; Iter    45/   55] train: loss: 0.0000741
[Epoch 106] ogbg-molbbbp: 0.950712 val loss: 0.794860
[Epoch 106] ogbg-molbbbp: 0.643422 test loss: 3.349936
[Epoch 107; Iter    20/   55] train: loss: 0.0004735
[Epoch 107; Iter    50/   55] train: loss: 0.0008908
[Epoch 107] ogbg-molbbbp: 0.952604 val loss: 0.739678
[Epoch 107] ogbg-molbbbp: 0.641300 test loss: 3.334259
[Epoch 108; Iter    25/   55] train: loss: 0.0001212
[Epoch 108; Iter    55/   55] train: loss: 0.0405455
[Epoch 108] ogbg-molbbbp: 0.932490 val loss: 0.896000
[Epoch 108] ogbg-molbbbp: 0.647473 test loss: 3.150869
[Epoch 109; Iter    30/   55] train: loss: 0.0002261
[Epoch 69; Iter    40/   55] train: loss: 0.0030518
[Epoch 69] ogbg-molbbbp: 0.950911 val loss: 0.553440
[Epoch 69] ogbg-molbbbp: 0.633681 test loss: 2.251238
[Epoch 70; Iter    15/   55] train: loss: 0.0010337
[Epoch 70; Iter    45/   55] train: loss: 0.0007091
[Epoch 70] ogbg-molbbbp: 0.951210 val loss: 0.544765
[Epoch 70] ogbg-molbbbp: 0.636092 test loss: 2.295381
[Epoch 71; Iter    20/   55] train: loss: 0.0004379
[Epoch 71; Iter    50/   55] train: loss: 0.0020066
[Epoch 71] ogbg-molbbbp: 0.950712 val loss: 0.497909
[Epoch 71] ogbg-molbbbp: 0.630015 test loss: 2.418280
[Epoch 72; Iter    25/   55] train: loss: 0.0003957
[Epoch 72; Iter    55/   55] train: loss: 0.0004761
[Epoch 72] ogbg-molbbbp: 0.949716 val loss: 0.469511
[Epoch 72] ogbg-molbbbp: 0.631752 test loss: 2.307608
[Epoch 73; Iter    30/   55] train: loss: 0.0009649
[Epoch 73] ogbg-molbbbp: 0.953500 val loss: 0.511652
[Epoch 73] ogbg-molbbbp: 0.634163 test loss: 2.363277
[Epoch 74; Iter     5/   55] train: loss: 0.0002574
[Epoch 74; Iter    35/   55] train: loss: 0.0009746
[Epoch 74] ogbg-molbbbp: 0.953201 val loss: 0.494991
[Epoch 74] ogbg-molbbbp: 0.638310 test loss: 2.365222
[Epoch 75; Iter    10/   55] train: loss: 0.0007913
[Epoch 75; Iter    40/   55] train: loss: 0.0005136
[Epoch 75] ogbg-molbbbp: 0.952206 val loss: 0.499142
[Epoch 75] ogbg-molbbbp: 0.636671 test loss: 2.368805
[Epoch 76; Iter    15/   55] train: loss: 0.0021853
[Epoch 76; Iter    45/   55] train: loss: 0.0011722
[Epoch 76] ogbg-molbbbp: 0.951409 val loss: 0.511060
[Epoch 76] ogbg-molbbbp: 0.637635 test loss: 2.337571
[Epoch 77; Iter    20/   55] train: loss: 0.0027437
[Epoch 77; Iter    50/   55] train: loss: 0.0001773
[Epoch 77] ogbg-molbbbp: 0.947725 val loss: 0.483393
[Epoch 77] ogbg-molbbbp: 0.630498 test loss: 2.547905
[Epoch 78; Iter    25/   55] train: loss: 0.0003653
[Epoch 78; Iter    55/   55] train: loss: 0.0007534
[Epoch 78] ogbg-molbbbp: 0.948322 val loss: 0.473457
[Epoch 78] ogbg-molbbbp: 0.638792 test loss: 2.421389
[Epoch 79; Iter    30/   55] train: loss: 0.0002498
[Epoch 79] ogbg-molbbbp: 0.948721 val loss: 0.484891
[Epoch 79] ogbg-molbbbp: 0.635610 test loss: 2.455209
[Epoch 80; Iter     5/   55] train: loss: 0.0001909
[Epoch 80; Iter    35/   55] train: loss: 0.0002641
[Epoch 80] ogbg-molbbbp: 0.945435 val loss: 0.584973
[Epoch 80] ogbg-molbbbp: 0.630594 test loss: 2.508913
[Epoch 81; Iter    10/   55] train: loss: 0.0012196
[Epoch 81; Iter    40/   55] train: loss: 0.0011875
[Epoch 81] ogbg-molbbbp: 0.950712 val loss: 0.538680
[Epoch 81] ogbg-molbbbp: 0.639371 test loss: 2.311654
[Epoch 82; Iter    15/   55] train: loss: 0.0006122
[Epoch 82; Iter    45/   55] train: loss: 0.0006541
[Epoch 82] ogbg-molbbbp: 0.953002 val loss: 0.565228
[Epoch 82] ogbg-molbbbp: 0.635224 test loss: 2.487037
[Epoch 83; Iter    20/   55] train: loss: 0.0003806
[Epoch 83; Iter    50/   55] train: loss: 0.0003659
[Epoch 83] ogbg-molbbbp: 0.951807 val loss: 0.526449
[Epoch 83] ogbg-molbbbp: 0.625193 test loss: 2.519843
[Epoch 84; Iter    25/   55] train: loss: 0.0091644
[Epoch 84; Iter    55/   55] train: loss: 0.0002406
[Epoch 84] ogbg-molbbbp: 0.954097 val loss: 0.461736
[Epoch 84] ogbg-molbbbp: 0.624807 test loss: 2.437514
[Epoch 85; Iter    30/   55] train: loss: 0.0013697
[Epoch 85] ogbg-molbbbp: 0.955193 val loss: 0.457396
[Epoch 85] ogbg-molbbbp: 0.625386 test loss: 2.505810
[Epoch 86; Iter     5/   55] train: loss: 0.0003570
[Epoch 86; Iter    35/   55] train: loss: 0.0056992
[Epoch 86] ogbg-molbbbp: 0.945136 val loss: 0.558587
[Epoch 86] ogbg-molbbbp: 0.618345 test loss: 2.668600
[Epoch 87; Iter    10/   55] train: loss: 0.0008050
[Epoch 87; Iter    40/   55] train: loss: 0.0008516
[Epoch 87] ogbg-molbbbp: 0.950413 val loss: 0.510804
[Epoch 87] ogbg-molbbbp: 0.622010 test loss: 2.672530
[Epoch 88; Iter    15/   55] train: loss: 0.0001902
[Epoch 88; Iter    45/   55] train: loss: 0.0011354
[Epoch 88] ogbg-molbbbp: 0.953102 val loss: 0.495432
[Epoch 88] ogbg-molbbbp: 0.625579 test loss: 2.651385
[Epoch 89; Iter    20/   55] train: loss: 0.0001321
[Epoch 89; Iter    50/   55] train: loss: 0.0001900
[Epoch 89] ogbg-molbbbp: 0.954197 val loss: 0.475133
[Epoch 89] ogbg-molbbbp: 0.630787 test loss: 2.569886
[Epoch 90; Iter    25/   55] train: loss: 0.0005409
[Epoch 90; Iter    55/   55] train: loss: 0.0003507
[Epoch 90] ogbg-molbbbp: 0.954794 val loss: 0.502141
[Epoch 90] ogbg-molbbbp: 0.632137 test loss: 2.632916
[Epoch 91; Iter    30/   55] train: loss: 0.0002768
[Epoch 91] ogbg-molbbbp: 0.953201 val loss: 0.487419
[Epoch 91] ogbg-molbbbp: 0.630787 test loss: 2.621680
[Epoch 92; Iter     5/   55] train: loss: 0.0000883
[Epoch 92; Iter    35/   55] train: loss: 0.0001387
[Epoch 92] ogbg-molbbbp: 0.956786 val loss: 0.463656
[Epoch 92] ogbg-molbbbp: 0.638503 test loss: 2.691346
[Epoch 93; Iter    10/   55] train: loss: 0.0005514
[Epoch 93; Iter    40/   55] train: loss: 0.0001727
[Epoch 93] ogbg-molbbbp: 0.953400 val loss: 0.526606
[Epoch 93] ogbg-molbbbp: 0.626736 test loss: 2.801306
[Epoch 94; Iter    15/   55] train: loss: 0.0005045
[Epoch 94; Iter    45/   55] train: loss: 0.0041663
[Epoch 94] ogbg-molbbbp: 0.953898 val loss: 0.511006
[Epoch 94] ogbg-molbbbp: 0.626350 test loss: 2.767841
[Epoch 95; Iter    20/   55] train: loss: 0.0006936
[Epoch 95; Iter    50/   55] train: loss: 0.0008073
[Epoch 95] ogbg-molbbbp: 0.953600 val loss: 0.498086
[Epoch 95] ogbg-molbbbp: 0.625772 test loss: 2.724202
[Epoch 96; Iter    25/   55] train: loss: 0.0012274
[Epoch 96; Iter    55/   55] train: loss: 0.0003788
[Epoch 96] ogbg-molbbbp: 0.952604 val loss: 0.502716
[Epoch 96] ogbg-molbbbp: 0.621528 test loss: 2.717163
[Epoch 97; Iter    30/   55] train: loss: 0.0005240
[Epoch 97] ogbg-molbbbp: 0.951708 val loss: 0.526071
[Epoch 97] ogbg-molbbbp: 0.623650 test loss: 2.843604
[Epoch 98; Iter     5/   55] train: loss: 0.0003352
[Epoch 98; Iter    35/   55] train: loss: 0.0003139
[Epoch 98] ogbg-molbbbp: 0.953998 val loss: 0.509857
[Epoch 98] ogbg-molbbbp: 0.626833 test loss: 2.696076
[Epoch 99; Iter    10/   55] train: loss: 0.0016873
[Epoch 99; Iter    40/   55] train: loss: 0.0001201
[Epoch 99] ogbg-molbbbp: 0.953301 val loss: 0.507244
[Epoch 99] ogbg-molbbbp: 0.629340 test loss: 2.698635
[Epoch 100; Iter    15/   55] train: loss: 0.0000704
[Epoch 100; Iter    45/   55] train: loss: 0.0001370
[Epoch 100] ogbg-molbbbp: 0.953998 val loss: 0.518874
[Epoch 100] ogbg-molbbbp: 0.627122 test loss: 2.766230
[Epoch 101; Iter    20/   55] train: loss: 0.0001561
[Epoch 101; Iter    50/   55] train: loss: 0.0001320
[Epoch 101] ogbg-molbbbp: 0.954197 val loss: 0.499730
[Epoch 101] ogbg-molbbbp: 0.626254 test loss: 2.697548
[Epoch 102; Iter    25/   55] train: loss: 0.0008595
[Epoch 102; Iter    55/   55] train: loss: 0.0002159
[Epoch 102] ogbg-molbbbp: 0.954595 val loss: 0.503226
[Epoch 102] ogbg-molbbbp: 0.626929 test loss: 2.686680
[Epoch 103; Iter    30/   55] train: loss: 0.0000830
[Epoch 103] ogbg-molbbbp: 0.952504 val loss: 0.519818
[Epoch 103] ogbg-molbbbp: 0.624807 test loss: 2.759604
[Epoch 104; Iter     5/   55] train: loss: 0.0001352
[Epoch 104; Iter    35/   55] train: loss: 0.0003126
[Epoch 104] ogbg-molbbbp: 0.951309 val loss: 0.524882
[Epoch 104] ogbg-molbbbp: 0.621142 test loss: 2.777682
[Epoch 105; Iter    10/   55] train: loss: 0.0012266
[Epoch 105; Iter    40/   55] train: loss: 0.0002229
[Epoch 105] ogbg-molbbbp: 0.953301 val loss: 0.507654
[Epoch 105] ogbg-molbbbp: 0.624035 test loss: 2.737007
[Epoch 106; Iter    15/   55] train: loss: 0.0002004
[Epoch 106; Iter    45/   55] train: loss: 0.0019493
[Epoch 106] ogbg-molbbbp: 0.952903 val loss: 0.508130
[Epoch 106] ogbg-molbbbp: 0.623650 test loss: 2.693621
[Epoch 107; Iter    20/   55] train: loss: 0.0000537
[Epoch 107; Iter    50/   55] train: loss: 0.0000963
[Epoch 107] ogbg-molbbbp: 0.951708 val loss: 0.573654
[Epoch 107] ogbg-molbbbp: 0.626736 test loss: 2.688024
[Epoch 108; Iter    25/   55] train: loss: 0.0001525
[Epoch 108; Iter    55/   55] train: loss: 0.0011495
[Epoch 108] ogbg-molbbbp: 0.950314 val loss: 0.501885
[Epoch 108] ogbg-molbbbp: 0.622685 test loss: 2.631928
[Epoch 109; Iter    30/   55] train: loss: 0.0001935
[Epoch 69; Iter    40/   55] train: loss: 0.0012225
[Epoch 69] ogbg-molbbbp: 0.949218 val loss: 0.418607
[Epoch 69] ogbg-molbbbp: 0.619599 test loss: 2.131270
[Epoch 70; Iter    15/   55] train: loss: 0.0024562
[Epoch 70; Iter    45/   55] train: loss: 0.0073577
[Epoch 70] ogbg-molbbbp: 0.949617 val loss: 0.436022
[Epoch 70] ogbg-molbbbp: 0.623553 test loss: 2.163604
[Epoch 71; Iter    20/   55] train: loss: 0.0184075
[Epoch 71; Iter    50/   55] train: loss: 0.0791031
[Epoch 71] ogbg-molbbbp: 0.947127 val loss: 0.443591
[Epoch 71] ogbg-molbbbp: 0.621431 test loss: 2.277964
[Epoch 72; Iter    25/   55] train: loss: 0.0008458
[Epoch 72; Iter    55/   55] train: loss: 0.0018010
[Epoch 72] ogbg-molbbbp: 0.946231 val loss: 0.479474
[Epoch 72] ogbg-molbbbp: 0.627411 test loss: 2.264264
[Epoch 73; Iter    30/   55] train: loss: 0.0014059
[Epoch 73] ogbg-molbbbp: 0.949119 val loss: 0.529763
[Epoch 73] ogbg-molbbbp: 0.621238 test loss: 2.365049
[Epoch 74; Iter     5/   55] train: loss: 0.0003501
[Epoch 74; Iter    35/   55] train: loss: 0.0124637
[Epoch 74] ogbg-molbbbp: 0.952504 val loss: 0.638183
[Epoch 74] ogbg-molbbbp: 0.636767 test loss: 2.571991
[Epoch 75; Iter    10/   55] train: loss: 0.0010241
[Epoch 75; Iter    40/   55] train: loss: 0.0025325
[Epoch 75] ogbg-molbbbp: 0.950911 val loss: 0.412734
[Epoch 75] ogbg-molbbbp: 0.614005 test loss: 2.132545
[Epoch 76; Iter    15/   55] train: loss: 0.0146727
[Epoch 76; Iter    45/   55] train: loss: 0.0134797
[Epoch 76] ogbg-molbbbp: 0.954396 val loss: 0.402631
[Epoch 76] ogbg-molbbbp: 0.623264 test loss: 2.312152
[Epoch 77; Iter    20/   55] train: loss: 0.0003262
[Epoch 77; Iter    50/   55] train: loss: 0.0004841
[Epoch 77] ogbg-molbbbp: 0.954097 val loss: 0.494632
[Epoch 77] ogbg-molbbbp: 0.628665 test loss: 2.375387
[Epoch 78; Iter    25/   55] train: loss: 0.0008080
[Epoch 78; Iter    55/   55] train: loss: 0.0012920
[Epoch 78] ogbg-molbbbp: 0.953998 val loss: 0.548046
[Epoch 78] ogbg-molbbbp: 0.620563 test loss: 1.932301
[Epoch 79; Iter    30/   55] train: loss: 0.0046001
[Epoch 79] ogbg-molbbbp: 0.945036 val loss: 0.452090
[Epoch 79] ogbg-molbbbp: 0.630691 test loss: 2.114062
[Epoch 80; Iter     5/   55] train: loss: 0.0065710
[Epoch 80; Iter    35/   55] train: loss: 0.0027423
[Epoch 80] ogbg-molbbbp: 0.945534 val loss: 0.507409
[Epoch 80] ogbg-molbbbp: 0.629726 test loss: 2.188850
[Epoch 81; Iter    10/   55] train: loss: 0.0013718
[Epoch 81; Iter    40/   55] train: loss: 0.0012294
[Epoch 81] ogbg-molbbbp: 0.947924 val loss: 0.583834
[Epoch 81] ogbg-molbbbp: 0.622010 test loss: 2.394527
[Epoch 82; Iter    15/   55] train: loss: 0.0013856
[Epoch 82; Iter    45/   55] train: loss: 0.0009561
[Epoch 82] ogbg-molbbbp: 0.952106 val loss: 0.486987
[Epoch 82] ogbg-molbbbp: 0.617091 test loss: 2.450678
[Epoch 83; Iter    20/   55] train: loss: 0.0023094
[Epoch 83; Iter    50/   55] train: loss: 0.0004389
[Epoch 83] ogbg-molbbbp: 0.953799 val loss: 0.502299
[Epoch 83] ogbg-molbbbp: 0.615548 test loss: 2.407665
[Epoch 84; Iter    25/   55] train: loss: 0.0025120
[Epoch 84; Iter    55/   55] train: loss: 0.0199031
[Epoch 84] ogbg-molbbbp: 0.946132 val loss: 0.454213
[Epoch 84] ogbg-molbbbp: 0.604167 test loss: 2.284574
[Epoch 85; Iter    30/   55] train: loss: 0.0010440
[Epoch 85] ogbg-molbbbp: 0.929603 val loss: 0.525181
[Epoch 85] ogbg-molbbbp: 0.632330 test loss: 2.074010
[Epoch 86; Iter     5/   55] train: loss: 0.0019507
[Epoch 86; Iter    35/   55] train: loss: 0.0021442
[Epoch 86] ogbg-molbbbp: 0.947725 val loss: 0.813756
[Epoch 86] ogbg-molbbbp: 0.636478 test loss: 3.835669
[Epoch 87; Iter    10/   55] train: loss: 0.0151666
[Epoch 87; Iter    40/   55] train: loss: 0.0044489
[Epoch 87] ogbg-molbbbp: 0.928707 val loss: 0.676239
[Epoch 87] ogbg-molbbbp: 0.622203 test loss: 3.079413
[Epoch 88; Iter    15/   55] train: loss: 0.0009199
[Epoch 88; Iter    45/   55] train: loss: 0.0017959
[Epoch 88] ogbg-molbbbp: 0.941651 val loss: 0.648076
[Epoch 88] ogbg-molbbbp: 0.626833 test loss: 3.334635
[Epoch 89; Iter    20/   55] train: loss: 0.0054631
[Epoch 89; Iter    50/   55] train: loss: 0.0015354
[Epoch 89] ogbg-molbbbp: 0.942846 val loss: 0.609118
[Epoch 89] ogbg-molbbbp: 0.633584 test loss: 3.062391
[Epoch 90; Iter    25/   55] train: loss: 0.0005796
[Epoch 90; Iter    55/   55] train: loss: 0.0267032
[Epoch 90] ogbg-molbbbp: 0.934581 val loss: 0.584845
[Epoch 90] ogbg-molbbbp: 0.627797 test loss: 2.971341
[Epoch 91; Iter    30/   55] train: loss: 0.0019306
[Epoch 91] ogbg-molbbbp: 0.941452 val loss: 0.561021
[Epoch 91] ogbg-molbbbp: 0.627701 test loss: 3.094219
[Epoch 92; Iter     5/   55] train: loss: 0.0005130
[Epoch 92; Iter    35/   55] train: loss: 0.0010588
[Epoch 92] ogbg-molbbbp: 0.942547 val loss: 0.592114
[Epoch 92] ogbg-molbbbp: 0.626640 test loss: 3.053523
[Epoch 93; Iter    10/   55] train: loss: 0.0004883
[Epoch 93; Iter    40/   55] train: loss: 0.0003589
[Epoch 93] ogbg-molbbbp: 0.945235 val loss: 0.684535
[Epoch 93] ogbg-molbbbp: 0.639275 test loss: 3.047876
[Epoch 94; Iter    15/   55] train: loss: 0.0009112
[Epoch 94; Iter    45/   55] train: loss: 0.0011243
[Epoch 94] ogbg-molbbbp: 0.935079 val loss: 0.565685
[Epoch 94] ogbg-molbbbp: 0.631462 test loss: 2.709345
[Epoch 95; Iter    20/   55] train: loss: 0.0011610
[Epoch 95; Iter    50/   55] train: loss: 0.0004018
[Epoch 95] ogbg-molbbbp: 0.942447 val loss: 0.598217
[Epoch 95] ogbg-molbbbp: 0.632812 test loss: 2.810757
[Epoch 96; Iter    25/   55] train: loss: 0.0004923
[Epoch 96; Iter    55/   55] train: loss: 0.0067321
[Epoch 96] ogbg-molbbbp: 0.946928 val loss: 0.523935
[Epoch 96] ogbg-molbbbp: 0.632909 test loss: 2.835569
[Epoch 97; Iter    30/   55] train: loss: 0.0035441
[Epoch 97] ogbg-molbbbp: 0.948223 val loss: 0.648091
[Epoch 97] ogbg-molbbbp: 0.634452 test loss: 3.068462
[Epoch 98; Iter     5/   55] train: loss: 0.0001677
[Epoch 98; Iter    35/   55] train: loss: 0.0045395
[Epoch 98] ogbg-molbbbp: 0.947326 val loss: 0.612042
[Epoch 98] ogbg-molbbbp: 0.634645 test loss: 3.205890
[Epoch 99; Iter    10/   55] train: loss: 0.0003071
[Epoch 99; Iter    40/   55] train: loss: 0.0003925
[Epoch 99] ogbg-molbbbp: 0.939361 val loss: 0.658126
[Epoch 99] ogbg-molbbbp: 0.639275 test loss: 2.941198
[Epoch 100; Iter    15/   55] train: loss: 0.0006630
[Epoch 100; Iter    45/   55] train: loss: 0.0043928
[Epoch 100] ogbg-molbbbp: 0.942447 val loss: 0.658215
[Epoch 100] ogbg-molbbbp: 0.631173 test loss: 2.846284
[Epoch 101; Iter    20/   55] train: loss: 0.0003507
[Epoch 101; Iter    50/   55] train: loss: 0.0014908
[Epoch 101] ogbg-molbbbp: 0.941153 val loss: 0.701784
[Epoch 101] ogbg-molbbbp: 0.630208 test loss: 2.993642
[Epoch 102; Iter    25/   55] train: loss: 0.0069415
[Epoch 102; Iter    55/   55] train: loss: 0.0001904
[Epoch 102] ogbg-molbbbp: 0.940655 val loss: 0.672137
[Epoch 102] ogbg-molbbbp: 0.633970 test loss: 2.902276
[Epoch 103; Iter    30/   55] train: loss: 0.0002065
[Epoch 103] ogbg-molbbbp: 0.941950 val loss: 0.691506
[Epoch 103] ogbg-molbbbp: 0.635706 test loss: 3.175335
[Epoch 104; Iter     5/   55] train: loss: 0.0002274
[Epoch 104; Iter    35/   55] train: loss: 0.0017635
[Epoch 104] ogbg-molbbbp: 0.939759 val loss: 0.771193
[Epoch 104] ogbg-molbbbp: 0.620949 test loss: 3.140281
[Epoch 105; Iter    10/   55] train: loss: 0.0008231
[Epoch 105; Iter    40/   55] train: loss: 0.0019321
[Epoch 105] ogbg-molbbbp: 0.939859 val loss: 0.753598
[Epoch 105] ogbg-molbbbp: 0.608121 test loss: 3.685849
[Epoch 106; Iter    15/   55] train: loss: 0.0004457
[Epoch 106; Iter    45/   55] train: loss: 0.0004526
[Epoch 106] ogbg-molbbbp: 0.943144 val loss: 0.604365
[Epoch 106] ogbg-molbbbp: 0.631462 test loss: 2.842762
[Epoch 107; Iter    20/   55] train: loss: 0.0076661
[Epoch 107; Iter    50/   55] train: loss: 0.0178540
[Epoch 107] ogbg-molbbbp: 0.939759 val loss: 0.625627
[Epoch 107] ogbg-molbbbp: 0.622685 test loss: 3.117735
[Epoch 108; Iter    25/   55] train: loss: 0.0006607
[Epoch 108; Iter    55/   55] train: loss: 0.0471701
[Epoch 108] ogbg-molbbbp: 0.930798 val loss: 1.236489
[Epoch 108] ogbg-molbbbp: 0.636671 test loss: 4.034601
[Epoch 109; Iter    30/   55] train: loss: 0.0004716
[Epoch 69; Iter    40/   55] train: loss: 0.0005625
[Epoch 69] ogbg-molbbbp: 0.936772 val loss: 0.868828
[Epoch 69] ogbg-molbbbp: 0.684703 test loss: 1.697548
[Epoch 70; Iter    15/   55] train: loss: 0.0013847
[Epoch 70; Iter    45/   55] train: loss: 0.0020927
[Epoch 70] ogbg-molbbbp: 0.939958 val loss: 0.778992
[Epoch 70] ogbg-molbbbp: 0.687693 test loss: 1.693954
[Epoch 71; Iter    20/   55] train: loss: 0.0008233
[Epoch 71; Iter    50/   55] train: loss: 0.0034531
[Epoch 71] ogbg-molbbbp: 0.941452 val loss: 0.682603
[Epoch 71] ogbg-molbbbp: 0.685957 test loss: 1.744212
[Epoch 72; Iter    25/   55] train: loss: 0.0019073
[Epoch 72; Iter    55/   55] train: loss: 0.0003357
[Epoch 72] ogbg-molbbbp: 0.941850 val loss: 0.678188
[Epoch 72] ogbg-molbbbp: 0.682581 test loss: 1.826532
[Epoch 73; Iter    30/   55] train: loss: 0.0016516
[Epoch 73] ogbg-molbbbp: 0.941651 val loss: 0.725860
[Epoch 73] ogbg-molbbbp: 0.684028 test loss: 1.833842
[Epoch 74; Iter     5/   55] train: loss: 0.0008011
[Epoch 74; Iter    35/   55] train: loss: 0.0006493
[Epoch 74] ogbg-molbbbp: 0.938465 val loss: 0.715159
[Epoch 74] ogbg-molbbbp: 0.687789 test loss: 1.861257
[Epoch 75; Iter    10/   55] train: loss: 0.0003938
[Epoch 75; Iter    40/   55] train: loss: 0.0009015
[Epoch 75] ogbg-molbbbp: 0.940356 val loss: 0.678275
[Epoch 75] ogbg-molbbbp: 0.682292 test loss: 1.906020
[Epoch 76; Iter    15/   55] train: loss: 0.0040006
[Epoch 76; Iter    45/   55] train: loss: 0.0008694
[Epoch 76] ogbg-molbbbp: 0.939460 val loss: 0.729406
[Epoch 76] ogbg-molbbbp: 0.681713 test loss: 1.990350
[Epoch 77; Iter    20/   55] train: loss: 0.0021379
[Epoch 77; Iter    50/   55] train: loss: 0.0002847
[Epoch 77] ogbg-molbbbp: 0.932590 val loss: 0.773691
[Epoch 77] ogbg-molbbbp: 0.676890 test loss: 1.909915
[Epoch 78; Iter    25/   55] train: loss: 0.0003373
[Epoch 78; Iter    55/   55] train: loss: 0.0004810
[Epoch 78] ogbg-molbbbp: 0.949318 val loss: 0.535497
[Epoch 78] ogbg-molbbbp: 0.670332 test loss: 2.110184
[Epoch 79; Iter    30/   55] train: loss: 0.0007106
[Epoch 79] ogbg-molbbbp: 0.948621 val loss: 0.514499
[Epoch 79] ogbg-molbbbp: 0.667052 test loss: 2.115982
[Epoch 80; Iter     5/   55] train: loss: 0.0011701
[Epoch 80; Iter    35/   55] train: loss: 0.0110561
[Epoch 80] ogbg-molbbbp: 0.925520 val loss: 1.176134
[Epoch 80] ogbg-molbbbp: 0.694734 test loss: 2.265912
[Epoch 81; Iter    10/   55] train: loss: 0.0059028
[Epoch 81; Iter    40/   55] train: loss: 0.0074585
[Epoch 81] ogbg-molbbbp: 0.928607 val loss: 0.713311
[Epoch 81] ogbg-molbbbp: 0.670910 test loss: 2.135914
[Epoch 82; Iter    15/   55] train: loss: 0.0020047
[Epoch 82; Iter    45/   55] train: loss: 0.0005516
[Epoch 82] ogbg-molbbbp: 0.915364 val loss: 0.966772
[Epoch 82] ogbg-molbbbp: 0.674865 test loss: 2.178182
[Epoch 83; Iter    20/   55] train: loss: 0.0008516
[Epoch 83; Iter    50/   55] train: loss: 0.0022500
[Epoch 83] ogbg-molbbbp: 0.935079 val loss: 0.889471
[Epoch 83] ogbg-molbbbp: 0.686343 test loss: 2.415487
[Epoch 84; Iter    25/   55] train: loss: 0.0633465
[Epoch 84; Iter    55/   55] train: loss: 0.0016517
[Epoch 84] ogbg-molbbbp: 0.936871 val loss: 0.623250
[Epoch 84] ogbg-molbbbp: 0.658661 test loss: 2.360877
[Epoch 85; Iter    30/   55] train: loss: 0.0043524
[Epoch 85] ogbg-molbbbp: 0.928209 val loss: 0.902911
[Epoch 85] ogbg-molbbbp: 0.700328 test loss: 2.378823
[Epoch 86; Iter     5/   55] train: loss: 0.0027419
[Epoch 86; Iter    35/   55] train: loss: 0.0070206
[Epoch 86] ogbg-molbbbp: 0.930798 val loss: 0.849567
[Epoch 86] ogbg-molbbbp: 0.694444 test loss: 2.283906
[Epoch 87; Iter    10/   55] train: loss: 0.0034018
[Epoch 87; Iter    40/   55] train: loss: 0.0014464
[Epoch 87] ogbg-molbbbp: 0.935876 val loss: 0.715199
[Epoch 87] ogbg-molbbbp: 0.699171 test loss: 2.319095
[Epoch 88; Iter    15/   55] train: loss: 0.0004444
[Epoch 88; Iter    45/   55] train: loss: 0.0033656
[Epoch 88] ogbg-molbbbp: 0.917754 val loss: 0.927898
[Epoch 88] ogbg-molbbbp: 0.681617 test loss: 2.312587
[Epoch 89; Iter    20/   55] train: loss: 0.0007328
[Epoch 89; Iter    50/   55] train: loss: 0.0006641
[Epoch 89] ogbg-molbbbp: 0.929802 val loss: 0.781142
[Epoch 89] ogbg-molbbbp: 0.682195 test loss: 2.350796
[Epoch 90; Iter    25/   55] train: loss: 0.0007004
[Epoch 90; Iter    55/   55] train: loss: 0.0001427
[Epoch 90] ogbg-molbbbp: 0.945932 val loss: 0.633617
[Epoch 90] ogbg-molbbbp: 0.677662 test loss: 2.445348
[Epoch 91; Iter    30/   55] train: loss: 0.0002865
[Epoch 91] ogbg-molbbbp: 0.942447 val loss: 0.645646
[Epoch 91] ogbg-molbbbp: 0.676408 test loss: 2.454694
[Epoch 92; Iter     5/   55] train: loss: 0.0005367
[Epoch 92; Iter    35/   55] train: loss: 0.0003331
[Epoch 92] ogbg-molbbbp: 0.936573 val loss: 0.699612
[Epoch 92] ogbg-molbbbp: 0.688657 test loss: 2.308074
[Epoch 93; Iter    10/   55] train: loss: 0.0010781
[Epoch 93; Iter    40/   55] train: loss: 0.0005957
[Epoch 93] ogbg-molbbbp: 0.942945 val loss: 0.591938
[Epoch 93] ogbg-molbbbp: 0.701582 test loss: 1.942965
[Epoch 94; Iter    15/   55] train: loss: 0.0014830
[Epoch 94; Iter    45/   55] train: loss: 0.0163489
[Epoch 94] ogbg-molbbbp: 0.939560 val loss: 0.686792
[Epoch 94] ogbg-molbbbp: 0.705247 test loss: 2.082358
[Epoch 95; Iter    20/   55] train: loss: 0.0006605
[Epoch 95; Iter    50/   55] train: loss: 0.0030800
[Epoch 95] ogbg-molbbbp: 0.921836 val loss: 0.867195
[Epoch 95] ogbg-molbbbp: 0.705343 test loss: 2.276930
[Epoch 96; Iter    25/   55] train: loss: 0.0029701
[Epoch 96; Iter    55/   55] train: loss: 0.0005842
[Epoch 96] ogbg-molbbbp: 0.943144 val loss: 0.628458
[Epoch 96] ogbg-molbbbp: 0.700231 test loss: 2.182533
[Epoch 97; Iter    30/   55] train: loss: 0.0011484
[Epoch 97] ogbg-molbbbp: 0.896246 val loss: 1.112454
[Epoch 97] ogbg-molbbbp: 0.696663 test loss: 2.366666
[Epoch 98; Iter     5/   55] train: loss: 0.0010434
[Epoch 98; Iter    35/   55] train: loss: 0.0011498
[Epoch 98] ogbg-molbbbp: 0.914368 val loss: 1.097741
[Epoch 98] ogbg-molbbbp: 0.691165 test loss: 2.695776
[Epoch 99; Iter    10/   55] train: loss: 0.0064182
[Epoch 99; Iter    40/   55] train: loss: 0.0031029
[Epoch 99] ogbg-molbbbp: 0.935079 val loss: 0.749070
[Epoch 99] ogbg-molbbbp: 0.690008 test loss: 2.368420
[Epoch 100; Iter    15/   55] train: loss: 0.0001871
[Epoch 100; Iter    45/   55] train: loss: 0.0002188
[Epoch 100] ogbg-molbbbp: 0.935477 val loss: 0.739293
[Epoch 100] ogbg-molbbbp: 0.690008 test loss: 2.219997
[Epoch 101; Iter    20/   55] train: loss: 0.0008089
[Epoch 101; Iter    50/   55] train: loss: 0.0001716
[Epoch 101] ogbg-molbbbp: 0.912078 val loss: 0.885532
[Epoch 101] ogbg-molbbbp: 0.689333 test loss: 2.313592
[Epoch 102; Iter    25/   55] train: loss: 0.0046098
[Epoch 102; Iter    55/   55] train: loss: 0.0031414
[Epoch 102] ogbg-molbbbp: 0.916858 val loss: 0.869307
[Epoch 102] ogbg-molbbbp: 0.687211 test loss: 2.265134
[Epoch 103; Iter    30/   55] train: loss: 0.0002768
[Epoch 103] ogbg-molbbbp: 0.924923 val loss: 0.814825
[Epoch 103] ogbg-molbbbp: 0.690972 test loss: 2.303555
[Epoch 104; Iter     5/   55] train: loss: 0.0005806
[Epoch 104; Iter    35/   55] train: loss: 0.0003620
[Epoch 104] ogbg-molbbbp: 0.926815 val loss: 0.806535
[Epoch 104] ogbg-molbbbp: 0.692805 test loss: 2.235933
[Epoch 105; Iter    10/   55] train: loss: 0.0008807
[Epoch 105; Iter    40/   55] train: loss: 0.0002980
[Epoch 105] ogbg-molbbbp: 0.939162 val loss: 0.689110
[Epoch 105] ogbg-molbbbp: 0.691840 test loss: 2.147909
[Epoch 106; Iter    15/   55] train: loss: 0.0002461
[Epoch 106; Iter    45/   55] train: loss: 0.0014544
[Epoch 106] ogbg-molbbbp: 0.931992 val loss: 0.727739
[Epoch 106] ogbg-molbbbp: 0.690924 test loss: 2.150286
[Epoch 107; Iter    20/   55] train: loss: 0.0002136
[Epoch 107; Iter    50/   55] train: loss: 0.0001280
[Epoch 107] ogbg-molbbbp: 0.939759 val loss: 0.724811
[Epoch 107] ogbg-molbbbp: 0.691454 test loss: 2.193212
[Epoch 108; Iter    25/   55] train: loss: 0.0007242
[Epoch 108; Iter    55/   55] train: loss: 0.0000793
[Epoch 108] ogbg-molbbbp: 0.933984 val loss: 0.756744
[Epoch 108] ogbg-molbbbp: 0.697627 test loss: 2.309466
[Epoch 109; Iter    30/   55] train: loss: 0.0004561
[Epoch 69; Iter    40/   55] train: loss: 0.0008961
[Epoch 69] ogbg-molbbbp: 0.931992 val loss: 7.073355
[Epoch 69] ogbg-molbbbp: 0.672936 test loss: 2.174485
[Epoch 70; Iter    15/   55] train: loss: 0.0014792
[Epoch 70; Iter    45/   55] train: loss: 0.0041444
[Epoch 70] ogbg-molbbbp: 0.931694 val loss: 7.583638
[Epoch 70] ogbg-molbbbp: 0.677180 test loss: 2.175728
[Epoch 71; Iter    20/   55] train: loss: 0.0006605
[Epoch 71; Iter    50/   55] train: loss: 0.0076267
[Epoch 71] ogbg-molbbbp: 0.933486 val loss: 7.336999
[Epoch 71] ogbg-molbbbp: 0.679687 test loss: 2.048877
[Epoch 72; Iter    25/   55] train: loss: 0.0012348
[Epoch 72; Iter    55/   55] train: loss: 0.0001962
[Epoch 72] ogbg-molbbbp: 0.934482 val loss: 7.314837
[Epoch 72] ogbg-molbbbp: 0.678048 test loss: 2.141021
[Epoch 73; Iter    30/   55] train: loss: 0.0017422
[Epoch 73] ogbg-molbbbp: 0.935477 val loss: 7.035758
[Epoch 73] ogbg-molbbbp: 0.681713 test loss: 2.235269
[Epoch 74; Iter     5/   55] train: loss: 0.0005968
[Epoch 74; Iter    35/   55] train: loss: 0.0011172
[Epoch 74] ogbg-molbbbp: 0.928806 val loss: 6.742228
[Epoch 74] ogbg-molbbbp: 0.679398 test loss: 2.220434
[Epoch 75; Iter    10/   55] train: loss: 0.0009672
[Epoch 75; Iter    40/   55] train: loss: 0.0011984
[Epoch 75] ogbg-molbbbp: 0.938465 val loss: 8.208279
[Epoch 75] ogbg-molbbbp: 0.679205 test loss: 2.286783
[Epoch 76; Iter    15/   55] train: loss: 0.0025033
[Epoch 76; Iter    45/   55] train: loss: 0.0011470
[Epoch 76] ogbg-molbbbp: 0.946729 val loss: 7.169754
[Epoch 76] ogbg-molbbbp: 0.684896 test loss: 2.210276
[Epoch 77; Iter    20/   55] train: loss: 0.0018736
[Epoch 77; Iter    50/   55] train: loss: 0.0011170
[Epoch 77] ogbg-molbbbp: 0.918849 val loss: 4.474794
[Epoch 77] ogbg-molbbbp: 0.674286 test loss: 1.987004
[Epoch 78; Iter    25/   55] train: loss: 0.0012511
[Epoch 78; Iter    55/   55] train: loss: 0.0025524
[Epoch 78] ogbg-molbbbp: 0.913970 val loss: 4.759779
[Epoch 78] ogbg-molbbbp: 0.679302 test loss: 2.020230
[Epoch 79; Iter    30/   55] train: loss: 0.0012566
[Epoch 79] ogbg-molbbbp: 0.924325 val loss: 4.259736
[Epoch 79] ogbg-molbbbp: 0.682195 test loss: 2.003542
[Epoch 80; Iter     5/   55] train: loss: 0.0011472
[Epoch 80; Iter    35/   55] train: loss: 0.0006260
[Epoch 80] ogbg-molbbbp: 0.930798 val loss: 5.060781
[Epoch 80] ogbg-molbbbp: 0.692130 test loss: 2.098351
[Epoch 81; Iter    10/   55] train: loss: 0.0023449
[Epoch 81; Iter    40/   55] train: loss: 0.0032718
[Epoch 81] ogbg-molbbbp: 0.918152 val loss: 2.186386
[Epoch 81] ogbg-molbbbp: 0.680266 test loss: 2.059487
[Epoch 82; Iter    15/   55] train: loss: 0.0017120
[Epoch 82; Iter    45/   55] train: loss: 0.0004483
[Epoch 82] ogbg-molbbbp: 0.941253 val loss: 3.916956
[Epoch 82] ogbg-molbbbp: 0.690586 test loss: 2.094668
[Epoch 83; Iter    20/   55] train: loss: 0.0009038
[Epoch 83; Iter    50/   55] train: loss: 0.0005992
[Epoch 83] ogbg-molbbbp: 0.945136 val loss: 4.741768
[Epoch 83] ogbg-molbbbp: 0.694059 test loss: 2.160802
[Epoch 84; Iter    25/   55] train: loss: 0.0078648
[Epoch 84; Iter    55/   55] train: loss: 0.0002721
[Epoch 84] ogbg-molbbbp: 0.936772 val loss: 4.070194
[Epoch 84] ogbg-molbbbp: 0.697338 test loss: 2.045359
[Epoch 85; Iter    30/   55] train: loss: 0.0008555
[Epoch 85] ogbg-molbbbp: 0.925719 val loss: 4.569195
[Epoch 85] ogbg-molbbbp: 0.690779 test loss: 1.945393
[Epoch 86; Iter     5/   55] train: loss: 0.0013914
[Epoch 86; Iter    35/   55] train: loss: 0.0091630
[Epoch 86] ogbg-molbbbp: 0.931893 val loss: 4.400809
[Epoch 86] ogbg-molbbbp: 0.690201 test loss: 1.950048
[Epoch 87; Iter    10/   55] train: loss: 0.0072532
[Epoch 87; Iter    40/   55] train: loss: 0.0023146
[Epoch 87] ogbg-molbbbp: 0.935079 val loss: 4.273948
[Epoch 87] ogbg-molbbbp: 0.685957 test loss: 2.235939
[Epoch 88; Iter    15/   55] train: loss: 0.0003359
[Epoch 88; Iter    45/   55] train: loss: 0.0003443
[Epoch 88] ogbg-molbbbp: 0.937469 val loss: 4.298833
[Epoch 88] ogbg-molbbbp: 0.686632 test loss: 2.219486
[Epoch 89; Iter    20/   55] train: loss: 0.0010777
[Epoch 89; Iter    50/   55] train: loss: 0.0003380
[Epoch 89] ogbg-molbbbp: 0.933088 val loss: 4.093833
[Epoch 89] ogbg-molbbbp: 0.688561 test loss: 2.081731
[Epoch 90; Iter    25/   55] train: loss: 0.0008862
[Epoch 90; Iter    55/   55] train: loss: 0.0001134
[Epoch 90] ogbg-molbbbp: 0.933287 val loss: 4.363998
[Epoch 90] ogbg-molbbbp: 0.693866 test loss: 2.196072
[Epoch 91; Iter    30/   55] train: loss: 0.0003688
[Epoch 91] ogbg-molbbbp: 0.935577 val loss: 3.864587
[Epoch 91] ogbg-molbbbp: 0.695698 test loss: 2.099420
[Epoch 92; Iter     5/   55] train: loss: 0.0002287
[Epoch 92; Iter    35/   55] train: loss: 0.0002957
[Epoch 92] ogbg-molbbbp: 0.896644 val loss: 6.452248
[Epoch 92] ogbg-molbbbp: 0.669657 test loss: 2.333917
[Epoch 93; Iter    10/   55] train: loss: 0.0019421
[Epoch 93; Iter    40/   55] train: loss: 0.0017462
[Epoch 93] ogbg-molbbbp: 0.925022 val loss: 2.879897
[Epoch 93] ogbg-molbbbp: 0.685185 test loss: 2.151975
[Epoch 94; Iter    15/   55] train: loss: 0.0015506
[Epoch 94; Iter    45/   55] train: loss: 0.0127756
[Epoch 94] ogbg-molbbbp: 0.928408 val loss: 4.250836
[Epoch 94] ogbg-molbbbp: 0.688947 test loss: 2.246305
[Epoch 95; Iter    20/   55] train: loss: 0.0011377
[Epoch 95; Iter    50/   55] train: loss: 0.0063106
[Epoch 95] ogbg-molbbbp: 0.924624 val loss: 4.883076
[Epoch 95] ogbg-molbbbp: 0.695795 test loss: 2.112205
[Epoch 96; Iter    25/   55] train: loss: 0.0016613
[Epoch 96; Iter    55/   55] train: loss: 0.0007203
[Epoch 96] ogbg-molbbbp: 0.925620 val loss: 3.508541
[Epoch 96] ogbg-molbbbp: 0.692901 test loss: 2.080658
[Epoch 97; Iter    30/   55] train: loss: 0.0020422
[Epoch 97] ogbg-molbbbp: 0.923031 val loss: 3.785650
[Epoch 97] ogbg-molbbbp: 0.694734 test loss: 1.983532
[Epoch 98; Iter     5/   55] train: loss: 0.0009294
[Epoch 98; Iter    35/   55] train: loss: 0.0007216
[Epoch 98] ogbg-molbbbp: 0.933287 val loss: 4.031811
[Epoch 98] ogbg-molbbbp: 0.694637 test loss: 2.136951
[Epoch 99; Iter    10/   55] train: loss: 0.0011833
[Epoch 99; Iter    40/   55] train: loss: 0.0002796
[Epoch 99] ogbg-molbbbp: 0.930101 val loss: 4.938937
[Epoch 99] ogbg-molbbbp: 0.697724 test loss: 2.088378
[Epoch 100; Iter    15/   55] train: loss: 0.0003815
[Epoch 100; Iter    45/   55] train: loss: 0.0002807
[Epoch 100] ogbg-molbbbp: 0.932689 val loss: 5.270179
[Epoch 100] ogbg-molbbbp: 0.698206 test loss: 2.144995
[Epoch 101; Iter    20/   55] train: loss: 0.0001692
[Epoch 101; Iter    50/   55] train: loss: 0.0003716
[Epoch 101] ogbg-molbbbp: 0.931395 val loss: 4.961825
[Epoch 101] ogbg-molbbbp: 0.699556 test loss: 2.058003
[Epoch 102; Iter    25/   55] train: loss: 0.0085931
[Epoch 102; Iter    55/   55] train: loss: 0.0005911
[Epoch 102] ogbg-molbbbp: 0.926317 val loss: 4.451715
[Epoch 102] ogbg-molbbbp: 0.697145 test loss: 2.119390
[Epoch 103; Iter    30/   55] train: loss: 0.0001521
[Epoch 103] ogbg-molbbbp: 0.932590 val loss: 1.192443
[Epoch 103] ogbg-molbbbp: 0.695988 test loss: 2.231091
[Epoch 104; Iter     5/   55] train: loss: 0.0057424
[Epoch 104; Iter    35/   55] train: loss: 0.0002760
[Epoch 104] ogbg-molbbbp: 0.931495 val loss: 4.413381
[Epoch 104] ogbg-molbbbp: 0.699749 test loss: 2.152400
[Epoch 105; Iter    10/   55] train: loss: 0.0006592
[Epoch 105; Iter    40/   55] train: loss: 0.0002147
[Epoch 105] ogbg-molbbbp: 0.934482 val loss: 4.163682
[Epoch 105] ogbg-molbbbp: 0.698206 test loss: 2.236881
[Epoch 106; Iter    15/   55] train: loss: 0.0003153
[Epoch 106; Iter    45/   55] train: loss: 0.0032632
[Epoch 106] ogbg-molbbbp: 0.934183 val loss: 4.260451
[Epoch 106] ogbg-molbbbp: 0.694734 test loss: 2.236982
[Epoch 107; Iter    20/   55] train: loss: 0.0001346
[Epoch 107; Iter    50/   55] train: loss: 0.0001564
[Epoch 107] ogbg-molbbbp: 0.929005 val loss: 3.408980
[Epoch 107] ogbg-molbbbp: 0.693866 test loss: 2.257172
[Epoch 108; Iter    25/   55] train: loss: 0.0032420
[Epoch 108; Iter    55/   55] train: loss: 0.0006145
[Epoch 108] ogbg-molbbbp: 0.934283 val loss: 4.723252
[Epoch 108] ogbg-molbbbp: 0.680652 test loss: 2.211984
[Epoch 109; Iter    30/   55] train: loss: 0.0035890
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.950015 val loss: 0.705932
[Epoch 109] ogbg-molbbbp: 0.642554 test loss: 3.332693
[Epoch 110; Iter     5/   55] train: loss: 0.0055477
[Epoch 110; Iter    35/   55] train: loss: 0.0006368
[Epoch 110] ogbg-molbbbp: 0.952504 val loss: 0.680544
[Epoch 110] ogbg-molbbbp: 0.647377 test loss: 3.287988
[Epoch 111; Iter    10/   55] train: loss: 0.0002253
[Epoch 111; Iter    40/   55] train: loss: 0.0001198
[Epoch 111] ogbg-molbbbp: 0.951509 val loss: 0.683351
[Epoch 111] ogbg-molbbbp: 0.644869 test loss: 3.260341
[Epoch 112; Iter    15/   55] train: loss: 0.0013091
[Epoch 112; Iter    45/   55] train: loss: 0.0001403
[Epoch 112] ogbg-molbbbp: 0.952006 val loss: 0.695713
[Epoch 112] ogbg-molbbbp: 0.643711 test loss: 3.359330
[Epoch 113; Iter    20/   55] train: loss: 0.0003513
[Epoch 113; Iter    50/   55] train: loss: 0.0002029
[Epoch 113] ogbg-molbbbp: 0.953699 val loss: 0.678297
[Epoch 113] ogbg-molbbbp: 0.645930 test loss: 3.249024
[Epoch 114; Iter    25/   55] train: loss: 0.0001044
[Epoch 114; Iter    55/   55] train: loss: 0.0015600
[Epoch 114] ogbg-molbbbp: 0.949517 val loss: 0.681923
[Epoch 114] ogbg-molbbbp: 0.628086 test loss: 3.214504
[Epoch 115; Iter    30/   55] train: loss: 0.0005040
[Epoch 115] ogbg-molbbbp: 0.946928 val loss: 0.752326
[Epoch 115] ogbg-molbbbp: 0.622203 test loss: 3.387371
[Epoch 116; Iter     5/   55] train: loss: 0.0013166
[Epoch 116; Iter    35/   55] train: loss: 0.0001773
[Epoch 116] ogbg-molbbbp: 0.949915 val loss: 0.711560
[Epoch 116] ogbg-molbbbp: 0.629823 test loss: 3.344325
[Epoch 117; Iter    10/   55] train: loss: 0.0001992
[Epoch 117; Iter    40/   55] train: loss: 0.0005500
[Epoch 117] ogbg-molbbbp: 0.949418 val loss: 0.738878
[Epoch 117] ogbg-molbbbp: 0.626736 test loss: 3.415384
[Epoch 118; Iter    15/   55] train: loss: 0.0001782
[Epoch 118; Iter    45/   55] train: loss: 0.0003555
[Epoch 118] ogbg-molbbbp: 0.949119 val loss: 0.741169
[Epoch 118] ogbg-molbbbp: 0.626447 test loss: 3.437407
[Epoch 119; Iter    20/   55] train: loss: 0.0000901
[Epoch 119; Iter    50/   55] train: loss: 0.0001244
[Epoch 119] ogbg-molbbbp: 0.947824 val loss: 0.739674
[Epoch 119] ogbg-molbbbp: 0.627122 test loss: 3.453930
[Epoch 120; Iter    25/   55] train: loss: 0.0004426
[Epoch 120; Iter    55/   55] train: loss: 0.0004864
[Epoch 120] ogbg-molbbbp: 0.950911 val loss: 0.728788
[Epoch 120] ogbg-molbbbp: 0.631944 test loss: 3.465111
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 55.
Statistics on  val_best_checkpoint
mean_pred: -0.22058887779712677
std_pred: 7.94460391998291
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9388980943294498
rocauc: 0.963855421686747
ogbg-molbbbp: 0.963855421686747
BCEWithLogitsLoss: 0.38387224610362736
Statistics on  test
mean_pred: 3.247234344482422
std_pred: 5.755570888519287
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6656980714788573
rocauc: 0.6578896604938271
ogbg-molbbbp: 0.6578896604938271
BCEWithLogitsLoss: 2.037912224020277
Statistics on  train
mean_pred: 6.077600479125977
std_pred: 5.859855651855469
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9998821270227859
rocauc: 0.9992667517940884
ogbg-molbbbp: 0.9992667517940884
BCEWithLogitsLoss: 0.038521986636756496
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.940556 val loss: 0.746722
[Epoch 109] ogbg-molbbbp: 0.645930 test loss: 3.025428
[Epoch 110; Iter     5/   55] train: loss: 0.0004288
[Epoch 110; Iter    35/   55] train: loss: 0.0012740
[Epoch 110] ogbg-molbbbp: 0.940655 val loss: 0.816852
[Epoch 110] ogbg-molbbbp: 0.645640 test loss: 3.054143
[Epoch 111; Iter    10/   55] train: loss: 0.0005287
[Epoch 111; Iter    40/   55] train: loss: 0.0004581
[Epoch 111] ogbg-molbbbp: 0.941053 val loss: 0.727883
[Epoch 111] ogbg-molbbbp: 0.648534 test loss: 2.857187
[Epoch 112; Iter    15/   55] train: loss: 0.0001891
[Epoch 112; Iter    45/   55] train: loss: 0.0005168
[Epoch 112] ogbg-molbbbp: 0.943443 val loss: 0.712041
[Epoch 112] ogbg-molbbbp: 0.657890 test loss: 2.865583
[Epoch 113; Iter    20/   55] train: loss: 0.0010215
[Epoch 113; Iter    50/   55] train: loss: 0.0013297
[Epoch 113] ogbg-molbbbp: 0.941551 val loss: 0.789049
[Epoch 113] ogbg-molbbbp: 0.657504 test loss: 2.913959
[Epoch 114; Iter    25/   55] train: loss: 0.0008150
[Epoch 114; Iter    55/   55] train: loss: 0.0001098
[Epoch 114] ogbg-molbbbp: 0.943443 val loss: 0.769762
[Epoch 114] ogbg-molbbbp: 0.655478 test loss: 3.065098
[Epoch 115; Iter    30/   55] train: loss: 0.0001580
[Epoch 115] ogbg-molbbbp: 0.944140 val loss: 0.717312
[Epoch 115] ogbg-molbbbp: 0.654417 test loss: 2.953014
[Epoch 116; Iter     5/   55] train: loss: 0.0000614
[Epoch 116; Iter    35/   55] train: loss: 0.0005876
[Epoch 116] ogbg-molbbbp: 0.943543 val loss: 0.715324
[Epoch 116] ogbg-molbbbp: 0.653453 test loss: 2.905103
[Epoch 117; Iter    10/   55] train: loss: 0.0077847
[Epoch 117; Iter    40/   55] train: loss: 0.0008353
[Epoch 117] ogbg-molbbbp: 0.944240 val loss: 0.746785
[Epoch 117] ogbg-molbbbp: 0.652971 test loss: 3.063628
[Epoch 118; Iter    15/   55] train: loss: 0.0473727
[Epoch 118; Iter    45/   55] train: loss: 0.0005092
[Epoch 118] ogbg-molbbbp: 0.942248 val loss: 0.765466
[Epoch 118] ogbg-molbbbp: 0.654900 test loss: 2.952309
[Epoch 119; Iter    20/   55] train: loss: 0.0000546
[Epoch 119; Iter    50/   55] train: loss: 0.0001480
[Epoch 119] ogbg-molbbbp: 0.950314 val loss: 0.614120
[Epoch 119] ogbg-molbbbp: 0.667824 test loss: 2.826266
[Epoch 120; Iter    25/   55] train: loss: 0.0015756
[Epoch 120; Iter    55/   55] train: loss: 0.0000896
[Epoch 120] ogbg-molbbbp: 0.948223 val loss: 0.725430
[Epoch 120] ogbg-molbbbp: 0.663773 test loss: 3.044983
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 58.
Statistics on  val_best_checkpoint
mean_pred: -0.46539825201034546
std_pred: 8.261625289916992
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9484621311287268
rocauc: 0.963855421686747
ogbg-molbbbp: 0.963855421686747
BCEWithLogitsLoss: 0.40736420692077707
Statistics on  test
mean_pred: 3.3724570274353027
std_pred: 6.2986369132995605
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.7167641840774482
rocauc: 0.6713927469135803
ogbg-molbbbp: 0.6713927469135803
BCEWithLogitsLoss: 2.1747042451586043
Statistics on  train
mean_pred: 5.2990875244140625
std_pred: 6.1188483238220215
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.007789282492277297
[Epoch 109] ogbg-molbbbp: 0.923330 val loss: 0.781724
[Epoch 109] ogbg-molbbbp: 0.591146 test loss: 3.194513
[Epoch 110; Iter     5/   55] train: loss: 0.0065728
[Epoch 110; Iter    35/   55] train: loss: 0.0007981
[Epoch 110] ogbg-molbbbp: 0.926416 val loss: 0.734409
[Epoch 110] ogbg-molbbbp: 0.591049 test loss: 3.383923
[Epoch 111; Iter    10/   55] train: loss: 0.0001171
[Epoch 111; Iter    40/   55] train: loss: 0.0002263
[Epoch 111] ogbg-molbbbp: 0.921737 val loss: 0.808844
[Epoch 111] ogbg-molbbbp: 0.583912 test loss: 3.230016
[Epoch 112; Iter    15/   55] train: loss: 0.0020781
[Epoch 112; Iter    45/   55] train: loss: 0.0001657
[Epoch 112] ogbg-molbbbp: 0.923429 val loss: 0.786230
[Epoch 112] ogbg-molbbbp: 0.586806 test loss: 3.382618
[Epoch 113; Iter    20/   55] train: loss: 0.0004063
[Epoch 113; Iter    50/   55] train: loss: 0.0001888
[Epoch 113] ogbg-molbbbp: 0.924624 val loss: 0.741433
[Epoch 113] ogbg-molbbbp: 0.587191 test loss: 3.194675
[Epoch 114; Iter    25/   55] train: loss: 0.0001736
[Epoch 114; Iter    55/   55] train: loss: 0.0020808
[Epoch 114] ogbg-molbbbp: 0.917654 val loss: 0.734458
[Epoch 114] ogbg-molbbbp: 0.623553 test loss: 4.221450
[Epoch 115; Iter    30/   55] train: loss: 0.0007178
[Epoch 115] ogbg-molbbbp: 0.903415 val loss: 1.032230
[Epoch 115] ogbg-molbbbp: 0.585938 test loss: 4.589411
[Epoch 116; Iter     5/   55] train: loss: 0.0023866
[Epoch 116; Iter    35/   55] train: loss: 0.0051765
[Epoch 116] ogbg-molbbbp: 0.894255 val loss: 0.877875
[Epoch 116] ogbg-molbbbp: 0.594811 test loss: 3.466415
[Epoch 117; Iter    10/   55] train: loss: 0.0016687
[Epoch 117; Iter    40/   55] train: loss: 0.0004209
[Epoch 117] ogbg-molbbbp: 0.915762 val loss: 0.738415
[Epoch 117] ogbg-molbbbp: 0.601852 test loss: 4.403482
[Epoch 118; Iter    15/   55] train: loss: 0.0005267
[Epoch 118; Iter    45/   55] train: loss: 0.0009806
[Epoch 118] ogbg-molbbbp: 0.910087 val loss: 0.776481
[Epoch 118] ogbg-molbbbp: 0.596547 test loss: 3.966897
[Epoch 119; Iter    20/   55] train: loss: 0.0001962
[Epoch 119; Iter    50/   55] train: loss: 0.0002068
[Epoch 119] ogbg-molbbbp: 0.914767 val loss: 0.773226
[Epoch 119] ogbg-molbbbp: 0.608121 test loss: 3.751889
[Epoch 120; Iter    25/   55] train: loss: 0.0008881
[Epoch 120; Iter    55/   55] train: loss: 0.0029756
[Epoch 120] ogbg-molbbbp: 0.917555 val loss: 0.807433
[Epoch 120] ogbg-molbbbp: 0.599633 test loss: 3.814400
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 45.
Statistics on  val_best_checkpoint
mean_pred: -5.411251068115234
std_pred: 29.51382827758789
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.8700481001900385
rocauc: 0.9302997112416608
ogbg-molbbbp: 0.9302997112416608
BCEWithLogitsLoss: 0.7391839889543397
Statistics on  test
mean_pred: -3.4037015438079834
std_pred: 42.881195068359375
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.5600636105367687
rocauc: 0.5656828703703703
ogbg-molbbbp: 0.5656828703703703
BCEWithLogitsLoss: 2.3870474014963423
Statistics on  train
mean_pred: 4.984882354736328
std_pred: 5.35032844543457
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9999905203436059
rocauc: 0.9999498157121429
ogbg-molbbbp: 0.9999498157121429
BCEWithLogitsLoss: 0.00996606105896221
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.934880 val loss: 0.768145
[Epoch 109] ogbg-molbbbp: 0.676890 test loss: 2.424596
[Epoch 110; Iter     5/   55] train: loss: 0.0002391
[Epoch 110; Iter    35/   55] train: loss: 0.0002951
[Epoch 110] ogbg-molbbbp: 0.939659 val loss: 0.725321
[Epoch 110] ogbg-molbbbp: 0.676794 test loss: 2.414932
[Epoch 111; Iter    10/   55] train: loss: 0.0002509
[Epoch 111; Iter    40/   55] train: loss: 0.0004229
[Epoch 111] ogbg-molbbbp: 0.940157 val loss: 0.741489
[Epoch 111] ogbg-molbbbp: 0.682002 test loss: 2.372985
[Epoch 112; Iter    15/   55] train: loss: 0.0000812
[Epoch 112; Iter    45/   55] train: loss: 0.0003978
[Epoch 112] ogbg-molbbbp: 0.936174 val loss: 0.759886
[Epoch 112] ogbg-molbbbp: 0.681617 test loss: 2.346038
[Epoch 113; Iter    20/   55] train: loss: 0.0033034
[Epoch 113; Iter    50/   55] train: loss: 0.0006059
[Epoch 113] ogbg-molbbbp: 0.936672 val loss: 0.771505
[Epoch 113] ogbg-molbbbp: 0.672936 test loss: 2.406104
[Epoch 114; Iter    25/   55] train: loss: 0.0021619
[Epoch 114; Iter    55/   55] train: loss: 0.0022445
[Epoch 114] ogbg-molbbbp: 0.932889 val loss: 0.816494
[Epoch 114] ogbg-molbbbp: 0.672454 test loss: 2.557670
[Epoch 115; Iter    30/   55] train: loss: 0.0004682
[Epoch 115] ogbg-molbbbp: 0.924923 val loss: 0.895055
[Epoch 115] ogbg-molbbbp: 0.677373 test loss: 2.573686
[Epoch 116; Iter     5/   55] train: loss: 0.0002171
[Epoch 116; Iter    35/   55] train: loss: 0.0004185
[Epoch 116] ogbg-molbbbp: 0.934382 val loss: 0.753114
[Epoch 116] ogbg-molbbbp: 0.677276 test loss: 2.352906
[Epoch 117; Iter    10/   55] train: loss: 0.0001192
[Epoch 117; Iter    40/   55] train: loss: 0.0003116
[Epoch 117] ogbg-molbbbp: 0.937369 val loss: 0.727607
[Epoch 117] ogbg-molbbbp: 0.676890 test loss: 2.354406
[Epoch 118; Iter    15/   55] train: loss: 0.0001956
[Epoch 118; Iter    45/   55] train: loss: 0.0002361
[Epoch 118] ogbg-molbbbp: 0.930798 val loss: 0.811326
[Epoch 118] ogbg-molbbbp: 0.675347 test loss: 2.512617
[Epoch 119; Iter    20/   55] train: loss: 0.0017667
[Epoch 119; Iter    50/   55] train: loss: 0.0001566
[Epoch 119] ogbg-molbbbp: 0.941153 val loss: 0.713092
[Epoch 119] ogbg-molbbbp: 0.677951 test loss: 2.381483
[Epoch 120; Iter    25/   55] train: loss: 0.0002260
[Epoch 120; Iter    55/   55] train: loss: 0.0005919
[Epoch 120] ogbg-molbbbp: 0.946231 val loss: 0.689917
[Epoch 120] ogbg-molbbbp: 0.677566 test loss: 2.383335
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 51.
Statistics on  val_best_checkpoint
mean_pred: -3.1027283668518066
std_pred: 8.114279747009277
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9117205178595594
rocauc: 0.9533008065319128
ogbg-molbbbp: 0.9533008065319128
BCEWithLogitsLoss: 0.4139773701982839
Statistics on  test
mean_pred: 0.011886073276400566
std_pred: 6.700588226318359
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.7181089433235218
rocauc: 0.7039930555555556
ogbg-molbbbp: 0.7039930555555556
BCEWithLogitsLoss: 1.5159154364040919
Statistics on  train
mean_pred: 5.386285781860352
std_pred: 5.182162284851074
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9998181543103919
rocauc: 0.9991245629784933
ogbg-molbbbp: 0.9991245629784933
BCEWithLogitsLoss: 0.02650049219585278
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.05.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.05.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.05.yml --seed 6 --device cuda:1
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.945534 val loss: 0.844935
[Epoch 109] ogbg-molbbbp: 0.632812 test loss: 3.684877
[Epoch 110; Iter     5/   55] train: loss: 0.0006125
[Epoch 110; Iter    35/   55] train: loss: 0.0005737
[Epoch 110] ogbg-molbbbp: 0.943941 val loss: 0.597495
[Epoch 110] ogbg-molbbbp: 0.620949 test loss: 3.051515
[Epoch 111; Iter    10/   55] train: loss: 0.0002147
[Epoch 111; Iter    40/   55] train: loss: 0.0005776
[Epoch 111] ogbg-molbbbp: 0.940755 val loss: 0.723109
[Epoch 111] ogbg-molbbbp: 0.624904 test loss: 2.577294
[Epoch 112; Iter    15/   55] train: loss: 0.0001897
[Epoch 112; Iter    45/   55] train: loss: 0.0004697
[Epoch 112] ogbg-molbbbp: 0.943642 val loss: 0.787116
[Epoch 112] ogbg-molbbbp: 0.623650 test loss: 2.815914
[Epoch 113; Iter    20/   55] train: loss: 0.0004669
[Epoch 113; Iter    50/   55] train: loss: 0.0015034
[Epoch 113] ogbg-molbbbp: 0.944240 val loss: 0.639819
[Epoch 113] ogbg-molbbbp: 0.620081 test loss: 2.582711
[Epoch 114; Iter    25/   55] train: loss: 0.0005706
[Epoch 114; Iter    55/   55] train: loss: 0.0001257
[Epoch 114] ogbg-molbbbp: 0.948322 val loss: 0.614703
[Epoch 114] ogbg-molbbbp: 0.617766 test loss: 2.619739
[Epoch 115; Iter    30/   55] train: loss: 0.0001970
[Epoch 115] ogbg-molbbbp: 0.944339 val loss: 0.729395
[Epoch 115] ogbg-molbbbp: 0.622299 test loss: 2.939933
[Epoch 116; Iter     5/   55] train: loss: 0.0000836
[Epoch 116; Iter    35/   55] train: loss: 0.0026268
[Epoch 116] ogbg-molbbbp: 0.946430 val loss: 0.735531
[Epoch 116] ogbg-molbbbp: 0.623843 test loss: 2.937423
[Epoch 117; Iter    10/   55] train: loss: 0.0453026
[Epoch 117; Iter    40/   55] train: loss: 0.0015056
[Epoch 117] ogbg-molbbbp: 0.942049 val loss: 0.642303
[Epoch 117] ogbg-molbbbp: 0.624711 test loss: 2.526583
[Epoch 118; Iter    15/   55] train: loss: 0.0390270
[Epoch 118; Iter    45/   55] train: loss: 0.0003608
[Epoch 118] ogbg-molbbbp: 0.943344 val loss: 0.708381
[Epoch 118] ogbg-molbbbp: 0.626061 test loss: 2.669461
[Epoch 119; Iter    20/   55] train: loss: 0.0001503
[Epoch 119; Iter    50/   55] train: loss: 0.0001048
[Epoch 119] ogbg-molbbbp: 0.944837 val loss: 0.828931
[Epoch 119] ogbg-molbbbp: 0.623843 test loss: 2.936229
[Epoch 120; Iter    25/   55] train: loss: 0.0042707
[Epoch 120; Iter    55/   55] train: loss: 0.0000836
[Epoch 120] ogbg-molbbbp: 0.944140 val loss: 0.618111
[Epoch 120] ogbg-molbbbp: 0.618538 test loss: 2.711279
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 53.
Statistics on  val_best_checkpoint
mean_pred: -11.866683006286621
std_pred: 53.979881286621094
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9126042529496803
rocauc: 0.9579806830628299
ogbg-molbbbp: 0.9579806830628299
BCEWithLogitsLoss: 0.4515441456543548
Statistics on  test
mean_pred: -5.722265243530273
std_pred: 35.3619384765625
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6416872412641288
rocauc: 0.6086998456790123
ogbg-molbbbp: 0.6086998456790123
BCEWithLogitsLoss: 1.8824090276445662
Statistics on  train
mean_pred: 5.271833419799805
std_pred: 5.923823833465576
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.004186445870436728
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.886289 val loss: 1.791825
[Epoch 109] ogbg-molbbbp: 0.671007 test loss: 2.591276
[Epoch 110; Iter     5/   55] train: loss: 0.0012004
[Epoch 110; Iter    35/   55] train: loss: 0.0006395
[Epoch 110] ogbg-molbbbp: 0.926715 val loss: 1.897795
[Epoch 110] ogbg-molbbbp: 0.683835 test loss: 2.263009
[Epoch 111; Iter    10/   55] train: loss: 0.0003732
[Epoch 111; Iter    40/   55] train: loss: 0.0004472
[Epoch 111] ogbg-molbbbp: 0.941153 val loss: 5.431035
[Epoch 111] ogbg-molbbbp: 0.695602 test loss: 2.438322
[Epoch 112; Iter    15/   55] train: loss: 0.0031152
[Epoch 112; Iter    45/   55] train: loss: 0.0015768
[Epoch 112] ogbg-molbbbp: 0.923330 val loss: 2.607364
[Epoch 112] ogbg-molbbbp: 0.693287 test loss: 2.615134
[Epoch 113; Iter    20/   55] train: loss: 0.0093388
[Epoch 113; Iter    50/   55] train: loss: 0.0003682
[Epoch 113] ogbg-molbbbp: 0.938465 val loss: 1.394295
[Epoch 113] ogbg-molbbbp: 0.687596 test loss: 2.378315
[Epoch 114; Iter    25/   55] train: loss: 0.0332080
[Epoch 114; Iter    55/   55] train: loss: 0.0010059
[Epoch 114] ogbg-molbbbp: 0.929503 val loss: 2.453835
[Epoch 114] ogbg-molbbbp: 0.702160 test loss: 2.224986
[Epoch 115; Iter    30/   55] train: loss: 0.0004728
[Epoch 115] ogbg-molbbbp: 0.901324 val loss: 3.109813
[Epoch 115] ogbg-molbbbp: 0.682774 test loss: 2.371083
[Epoch 116; Iter     5/   55] train: loss: 0.0013699
[Epoch 116; Iter    35/   55] train: loss: 0.0010292
[Epoch 116] ogbg-molbbbp: 0.926815 val loss: 1.483535
[Epoch 116] ogbg-molbbbp: 0.689333 test loss: 2.226436
[Epoch 117; Iter    10/   55] train: loss: 0.0005313
[Epoch 117; Iter    40/   55] train: loss: 0.0008996
[Epoch 117] ogbg-molbbbp: 0.933486 val loss: 3.667298
[Epoch 117] ogbg-molbbbp: 0.696663 test loss: 2.213337
[Epoch 118; Iter    15/   55] train: loss: 0.0004898
[Epoch 118; Iter    45/   55] train: loss: 0.0005563
[Epoch 118] ogbg-molbbbp: 0.924425 val loss: 4.036271
[Epoch 118] ogbg-molbbbp: 0.700714 test loss: 2.181226
[Epoch 119; Iter    20/   55] train: loss: 0.0055629
[Epoch 119; Iter    50/   55] train: loss: 0.0004363
[Epoch 119] ogbg-molbbbp: 0.924226 val loss: 4.249179
[Epoch 119] ogbg-molbbbp: 0.698978 test loss: 2.205366
[Epoch 120; Iter    25/   55] train: loss: 0.0005197
[Epoch 120; Iter    55/   55] train: loss: 0.0012368
[Epoch 120] ogbg-molbbbp: 0.924325 val loss: 3.632989
[Epoch 120] ogbg-molbbbp: 0.698013 test loss: 2.316066
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 49.
Statistics on  val_best_checkpoint
mean_pred: -3.6680831909179688
std_pred: 14.095308303833008
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.923312171385924
rocauc: 0.9546948123070795
ogbg-molbbbp: 0.9546948123070795
BCEWithLogitsLoss: 0.4010752250573465
Statistics on  test
mean_pred: -0.6599094271659851
std_pred: 8.41032886505127
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.71108278932838
rocauc: 0.6931905864197532
ogbg-molbbbp: 0.6931905864197532
BCEWithLogitsLoss: 1.6376943503107344
Statistics on  train
mean_pred: 4.051247596740723
std_pred: 6.001477241516113
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.005490636961026626
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.930399 val loss: 0.894328
[Epoch 109] ogbg-molbbbp: 0.624711 test loss: 3.391079
[Epoch 110; Iter     5/   55] train: loss: 0.0029413
[Epoch 110; Iter    35/   55] train: loss: 0.0009796
[Epoch 110] ogbg-molbbbp: 0.928010 val loss: 0.892101
[Epoch 110] ogbg-molbbbp: 0.622685 test loss: 3.308063
[Epoch 111; Iter    10/   55] train: loss: 0.0002843
[Epoch 111; Iter    40/   55] train: loss: 0.0001264
[Epoch 111] ogbg-molbbbp: 0.929005 val loss: 0.853005
[Epoch 111] ogbg-molbbbp: 0.619502 test loss: 3.312782
[Epoch 112; Iter    15/   55] train: loss: 0.0015531
[Epoch 112; Iter    45/   55] train: loss: 0.0001626
[Epoch 112] ogbg-molbbbp: 0.930300 val loss: 0.858922
[Epoch 112] ogbg-molbbbp: 0.619406 test loss: 3.354879
[Epoch 113; Iter    20/   55] train: loss: 0.0001472
[Epoch 113; Iter    50/   55] train: loss: 0.0001392
[Epoch 113] ogbg-molbbbp: 0.932590 val loss: 0.823366
[Epoch 113] ogbg-molbbbp: 0.621528 test loss: 3.274329
[Epoch 114; Iter    25/   55] train: loss: 0.0000639
[Epoch 114; Iter    55/   55] train: loss: 0.0004025
[Epoch 114] ogbg-molbbbp: 0.930300 val loss: 0.951849
[Epoch 114] ogbg-molbbbp: 0.608507 test loss: 3.596353
[Epoch 115; Iter    30/   55] train: loss: 0.0000959
[Epoch 115] ogbg-molbbbp: 0.932590 val loss: 0.939582
[Epoch 115] ogbg-molbbbp: 0.611015 test loss: 3.649204
[Epoch 116; Iter     5/   55] train: loss: 0.0009922
[Epoch 116; Iter    35/   55] train: loss: 0.0002009
[Epoch 116] ogbg-molbbbp: 0.934183 val loss: 0.877186
[Epoch 116] ogbg-molbbbp: 0.616416 test loss: 3.491466
[Epoch 117; Iter    10/   55] train: loss: 0.0001517
[Epoch 117; Iter    40/   55] train: loss: 0.0002667
[Epoch 117] ogbg-molbbbp: 0.932291 val loss: 0.938167
[Epoch 117] ogbg-molbbbp: 0.615066 test loss: 3.623291
[Epoch 118; Iter    15/   55] train: loss: 0.0001858
[Epoch 118; Iter    45/   55] train: loss: 0.0002220
[Epoch 118] ogbg-molbbbp: 0.935876 val loss: 0.885296
[Epoch 118] ogbg-molbbbp: 0.615355 test loss: 3.613959
[Epoch 119; Iter    20/   55] train: loss: 0.0000751
[Epoch 119; Iter    50/   55] train: loss: 0.0001153
[Epoch 119] ogbg-molbbbp: 0.934183 val loss: 0.894184
[Epoch 119] ogbg-molbbbp: 0.619502 test loss: 3.550612
[Epoch 120; Iter    25/   55] train: loss: 0.0006140
[Epoch 120; Iter    55/   55] train: loss: 0.0003529
[Epoch 120] ogbg-molbbbp: 0.935776 val loss: 0.838353
[Epoch 120] ogbg-molbbbp: 0.614101 test loss: 3.446473
[Epoch 121; Iter    30/   55] train: loss: 0.0000760
[Epoch 121] ogbg-molbbbp: 0.938962 val loss: 0.840434
[Epoch 121] ogbg-molbbbp: 0.622589 test loss: 3.633865
[Epoch 122; Iter     5/   55] train: loss: 0.0001467
[Epoch 122; Iter    35/   55] train: loss: 0.0001061
[Epoch 122] ogbg-molbbbp: 0.937568 val loss: 0.803345
[Epoch 122] ogbg-molbbbp: 0.613329 test loss: 3.292357
[Epoch 123; Iter    10/   55] train: loss: 0.0002634
[Epoch 123; Iter    40/   55] train: loss: 0.0000987
[Epoch 123] ogbg-molbbbp: 0.935975 val loss: 0.842410
[Epoch 123] ogbg-molbbbp: 0.616705 test loss: 3.274878
[Epoch 124; Iter    15/   55] train: loss: 0.0003955
[Epoch 124; Iter    45/   55] train: loss: 0.0006059
[Epoch 124] ogbg-molbbbp: 0.940157 val loss: 0.833629
[Epoch 124] ogbg-molbbbp: 0.604360 test loss: 3.543339
[Epoch 125; Iter    20/   55] train: loss: 0.0000824
[Epoch 125; Iter    50/   55] train: loss: 0.0005738
[Epoch 125] ogbg-molbbbp: 0.942248 val loss: 0.749398
[Epoch 125] ogbg-molbbbp: 0.605324 test loss: 3.279284
[Epoch 126; Iter    25/   55] train: loss: 0.0000845
[Epoch 126; Iter    55/   55] train: loss: 0.0023368
[Epoch 126] ogbg-molbbbp: 0.939958 val loss: 0.814731
[Epoch 126] ogbg-molbbbp: 0.599537 test loss: 3.537799
[Epoch 127; Iter    30/   55] train: loss: 0.0000696
[Epoch 127] ogbg-molbbbp: 0.939062 val loss: 0.757112
[Epoch 127] ogbg-molbbbp: 0.606096 test loss: 3.253640
[Epoch 128; Iter     5/   55] train: loss: 0.0000611
[Epoch 128; Iter    35/   55] train: loss: 0.0002996
[Epoch 128] ogbg-molbbbp: 0.940356 val loss: 0.740222
[Epoch 128] ogbg-molbbbp: 0.604745 test loss: 3.224556
[Epoch 129; Iter    10/   55] train: loss: 0.0001297
[Epoch 129; Iter    40/   55] train: loss: 0.0001798
[Epoch 129] ogbg-molbbbp: 0.938564 val loss: 0.791507
[Epoch 129] ogbg-molbbbp: 0.609279 test loss: 3.402555
[Epoch 130; Iter    15/   55] train: loss: 0.0000483
[Epoch 130; Iter    45/   55] train: loss: 0.0003001
[Epoch 130] ogbg-molbbbp: 0.939659 val loss: 0.816392
[Epoch 130] ogbg-molbbbp: 0.606096 test loss: 3.537071
[Epoch 131; Iter    20/   55] train: loss: 0.0002483
[Epoch 131; Iter    50/   55] train: loss: 0.0005434
[Epoch 131] ogbg-molbbbp: 0.941452 val loss: 0.796051
[Epoch 131] ogbg-molbbbp: 0.608410 test loss: 3.479949
[Epoch 132; Iter    25/   55] train: loss: 0.0009211
[Epoch 132; Iter    55/   55] train: loss: 0.0002474
[Epoch 132] ogbg-molbbbp: 0.936573 val loss: 0.928642
[Epoch 132] ogbg-molbbbp: 0.592689 test loss: 4.020678
[Epoch 133; Iter    30/   55] train: loss: 0.0007350
[Epoch 133] ogbg-molbbbp: 0.937071 val loss: 0.867872
[Epoch 133] ogbg-molbbbp: 0.599826 test loss: 3.758452
[Epoch 134; Iter     5/   55] train: loss: 0.0065714
[Epoch 134; Iter    35/   55] train: loss: 0.0001630
[Epoch 134] ogbg-molbbbp: 0.938863 val loss: 0.819766
[Epoch 134] ogbg-molbbbp: 0.608314 test loss: 3.624696
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 134 epochs. Best model checkpoint was in epoch 74.
Statistics on  val_best_checkpoint
mean_pred: -4.039767265319824
std_pred: 13.039750099182129
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9152459256312737
rocauc: 0.9546948123070795
ogbg-molbbbp: 0.9546948123070795
BCEWithLogitsLoss: 0.47848417663148474
Statistics on  test
mean_pred: 0.8547669053077698
std_pred: 11.958406448364258
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6251503202254642
rocauc: 0.6222029320987654
ogbg-molbbbp: 0.6222029320987654
BCEWithLogitsLoss: 2.6432537266186307
Statistics on  train
mean_pred: 3.5021705627441406
std_pred: 7.6070027351379395
mean_targets: 0.839362382888794
std_targets: 0.36730900406837463
prcauc: 0.9997131507364869
rocauc: 0.9985920519240099
ogbg-molbbbp: 0.9985920519240099
BCEWithLogitsLoss: 0.10535608641803265
[Epoch 109] ogbg-molbbbp: 0.950712 val loss: 0.481195
[Epoch 109] ogbg-molbbbp: 0.619599 test loss: 2.619282
[Epoch 110; Iter     5/   55] train: loss: 0.0001482
[Epoch 110; Iter    35/   55] train: loss: 0.0009846
[Epoch 110] ogbg-molbbbp: 0.950115 val loss: 0.497123
[Epoch 110] ogbg-molbbbp: 0.613137 test loss: 2.654659
[Epoch 111; Iter    10/   55] train: loss: 0.0001243
[Epoch 111; Iter    40/   55] train: loss: 0.0001468
[Epoch 111] ogbg-molbbbp: 0.950612 val loss: 0.530677
[Epoch 111] ogbg-molbbbp: 0.604552 test loss: 2.923140
[Epoch 112; Iter    15/   55] train: loss: 0.0002231
[Epoch 112; Iter    45/   55] train: loss: 0.0002617
[Epoch 112] ogbg-molbbbp: 0.950612 val loss: 0.550517
[Epoch 112] ogbg-molbbbp: 0.607350 test loss: 2.945508
[Epoch 113; Iter    20/   55] train: loss: 0.0011337
[Epoch 113; Iter    50/   55] train: loss: 0.0002353
[Epoch 113] ogbg-molbbbp: 0.952803 val loss: 0.513880
[Epoch 113] ogbg-molbbbp: 0.615451 test loss: 2.797617
[Epoch 114; Iter    25/   55] train: loss: 0.0003919
[Epoch 114; Iter    55/   55] train: loss: 0.0002360
[Epoch 114] ogbg-molbbbp: 0.953898 val loss: 0.479341
[Epoch 114] ogbg-molbbbp: 0.612172 test loss: 2.717660
[Epoch 115; Iter    30/   55] train: loss: 0.0004991
[Epoch 115] ogbg-molbbbp: 0.953102 val loss: 0.498981
[Epoch 115] ogbg-molbbbp: 0.615548 test loss: 2.882166
[Epoch 116; Iter     5/   55] train: loss: 0.0005723
[Epoch 116; Iter    35/   55] train: loss: 0.0006072
[Epoch 116] ogbg-molbbbp: 0.953102 val loss: 0.445944
[Epoch 116] ogbg-molbbbp: 0.623071 test loss: 2.537932
[Epoch 117; Iter    10/   55] train: loss: 0.0000973
[Epoch 117; Iter    40/   55] train: loss: 0.0004587
[Epoch 117] ogbg-molbbbp: 0.954297 val loss: 0.454398
[Epoch 117] ogbg-molbbbp: 0.627025 test loss: 2.563118
[Epoch 118; Iter    15/   55] train: loss: 0.0001035
[Epoch 118; Iter    45/   55] train: loss: 0.0001004
[Epoch 118] ogbg-molbbbp: 0.950214 val loss: 0.492239
[Epoch 118] ogbg-molbbbp: 0.636381 test loss: 2.553905
[Epoch 119; Iter    20/   55] train: loss: 0.0009153
[Epoch 119; Iter    50/   55] train: loss: 0.0002595
[Epoch 119] ogbg-molbbbp: 0.952604 val loss: 0.478462
[Epoch 119] ogbg-molbbbp: 0.633584 test loss: 2.561108
[Epoch 120; Iter    25/   55] train: loss: 0.0001104
[Epoch 120; Iter    55/   55] train: loss: 0.0001413
[Epoch 120] ogbg-molbbbp: 0.954894 val loss: 0.504573
[Epoch 120] ogbg-molbbbp: 0.632427 test loss: 2.695529
[Epoch 121; Iter    30/   55] train: loss: 0.0002313
[Epoch 121] ogbg-molbbbp: 0.953600 val loss: 0.483482
[Epoch 121] ogbg-molbbbp: 0.634259 test loss: 2.678125
[Epoch 122; Iter     5/   55] train: loss: 0.0001093
[Epoch 122; Iter    35/   55] train: loss: 0.0010054
[Epoch 122] ogbg-molbbbp: 0.953600 val loss: 0.496674
[Epoch 122] ogbg-molbbbp: 0.635224 test loss: 2.689606
[Epoch 123; Iter    10/   55] train: loss: 0.0003699
[Epoch 123; Iter    40/   55] train: loss: 0.0005208
[Epoch 123] ogbg-molbbbp: 0.950115 val loss: 0.512926
[Epoch 123] ogbg-molbbbp: 0.633295 test loss: 2.636214
[Epoch 124; Iter    15/   55] train: loss: 0.0001360
[Epoch 124; Iter    45/   55] train: loss: 0.0002895
[Epoch 124] ogbg-molbbbp: 0.950612 val loss: 0.522074
[Epoch 124] ogbg-molbbbp: 0.633198 test loss: 2.826695
[Epoch 125; Iter    20/   55] train: loss: 0.0002034
[Epoch 125; Iter    50/   55] train: loss: 0.0005274
[Epoch 125] ogbg-molbbbp: 0.951608 val loss: 0.502072
[Epoch 125] ogbg-molbbbp: 0.627025 test loss: 2.792865
[Epoch 126; Iter    25/   55] train: loss: 0.0000870
[Epoch 126; Iter    55/   55] train: loss: 0.0094854
[Epoch 126] ogbg-molbbbp: 0.951309 val loss: 0.508627
[Epoch 126] ogbg-molbbbp: 0.627508 test loss: 2.998651
[Epoch 127; Iter    30/   55] train: loss: 0.0005670
[Epoch 127] ogbg-molbbbp: 0.950413 val loss: 0.446795
[Epoch 127] ogbg-molbbbp: 0.627411 test loss: 2.428239
[Epoch 128; Iter     5/   55] train: loss: 0.0002291
[Epoch 128; Iter    35/   55] train: loss: 0.0059749
[Epoch 128] ogbg-molbbbp: 0.951708 val loss: 0.458880
[Epoch 128] ogbg-molbbbp: 0.632523 test loss: 2.439728
[Epoch 129; Iter    10/   55] train: loss: 0.0000705
[Epoch 129; Iter    40/   55] train: loss: 0.0001301
[Epoch 129] ogbg-molbbbp: 0.953400 val loss: 0.468084
[Epoch 129] ogbg-molbbbp: 0.632234 test loss: 2.541677
[Epoch 130; Iter    15/   55] train: loss: 0.0001475
[Epoch 130; Iter    45/   55] train: loss: 0.0000769
[Epoch 130] ogbg-molbbbp: 0.954197 val loss: 0.463669
[Epoch 130] ogbg-molbbbp: 0.631366 test loss: 2.639840
[Epoch 131; Iter    20/   55] train: loss: 0.0001218
[Epoch 131; Iter    50/   55] train: loss: 0.0000933
[Epoch 131] ogbg-molbbbp: 0.953500 val loss: 0.491080
[Epoch 131] ogbg-molbbbp: 0.636285 test loss: 2.636881
[Epoch 132; Iter    25/   55] train: loss: 0.0008332
[Epoch 132; Iter    55/   55] train: loss: 0.0155884
[Epoch 132] ogbg-molbbbp: 0.955292 val loss: 0.503694
[Epoch 132] ogbg-molbbbp: 0.635899 test loss: 2.675595
[Epoch 133; Iter    30/   55] train: loss: 0.0001643
[Epoch 133] ogbg-molbbbp: 0.951608 val loss: 0.518008
[Epoch 133] ogbg-molbbbp: 0.627122 test loss: 2.793268
[Epoch 134; Iter     5/   55] train: loss: 0.0007268
[Epoch 134; Iter    35/   55] train: loss: 0.0035628
[Epoch 134] ogbg-molbbbp: 0.952106 val loss: 0.495672
[Epoch 134] ogbg-molbbbp: 0.625482 test loss: 2.759195
[Epoch 135; Iter    10/   55] train: loss: 0.0000572
[Epoch 135; Iter    40/   55] train: loss: 0.0000737
[Epoch 135] ogbg-molbbbp: 0.951807 val loss: 0.533248
[Epoch 135] ogbg-molbbbp: 0.625193 test loss: 2.948184
[Epoch 136; Iter    15/   55] train: loss: 0.0000343
[Epoch 136; Iter    45/   55] train: loss: 0.0000362
[Epoch 136] ogbg-molbbbp: 0.953600 val loss: 0.512819
[Epoch 136] ogbg-molbbbp: 0.626157 test loss: 2.987374
[Epoch 137; Iter    20/   55] train: loss: 0.0002203
[Epoch 137; Iter    50/   55] train: loss: 0.0001790
[Epoch 137] ogbg-molbbbp: 0.952604 val loss: 0.512611
[Epoch 137] ogbg-molbbbp: 0.628279 test loss: 2.843424
[Epoch 138; Iter    25/   55] train: loss: 0.0002262
[Epoch 138; Iter    55/   55] train: loss: 0.0003170
[Epoch 138] ogbg-molbbbp: 0.954894 val loss: 0.495975
[Epoch 138] ogbg-molbbbp: 0.630112 test loss: 2.834843
[Epoch 139; Iter    30/   55] train: loss: 0.0000852
[Epoch 139] ogbg-molbbbp: 0.955093 val loss: 0.508115
[Epoch 139] ogbg-molbbbp: 0.629340 test loss: 2.833677
[Epoch 140; Iter     5/   55] train: loss: 0.0001143
[Epoch 140; Iter    35/   55] train: loss: 0.0041248
[Epoch 140] ogbg-molbbbp: 0.945534 val loss: 0.724156
[Epoch 140] ogbg-molbbbp: 0.627894 test loss: 3.382041
[Epoch 141; Iter    10/   55] train: loss: 0.0009842
[Epoch 141; Iter    40/   55] train: loss: 0.0004707
[Epoch 141] ogbg-molbbbp: 0.950115 val loss: 0.635251
[Epoch 141] ogbg-molbbbp: 0.634259 test loss: 3.290230
[Epoch 142; Iter    15/   55] train: loss: 0.0004248
[Epoch 142; Iter    45/   55] train: loss: 0.0007755
[Epoch 142] ogbg-molbbbp: 0.954994 val loss: 0.475171
[Epoch 142] ogbg-molbbbp: 0.644097 test loss: 2.649998
[Epoch 143; Iter    20/   55] train: loss: 0.0000806
[Epoch 143; Iter    50/   55] train: loss: 0.0000774
[Epoch 143] ogbg-molbbbp: 0.955591 val loss: 0.480408
[Epoch 143] ogbg-molbbbp: 0.641397 test loss: 2.586689
[Epoch 144; Iter    25/   55] train: loss: 0.0001565
[Epoch 144; Iter    55/   55] train: loss: 0.0164511
[Epoch 144] ogbg-molbbbp: 0.955890 val loss: 0.473753
[Epoch 144] ogbg-molbbbp: 0.641975 test loss: 2.624572
[Epoch 145; Iter    30/   55] train: loss: 0.0000971
[Epoch 145] ogbg-molbbbp: 0.953699 val loss: 0.454422
[Epoch 145] ogbg-molbbbp: 0.642940 test loss: 2.575291
[Epoch 146; Iter     5/   55] train: loss: 0.0000695
[Epoch 146; Iter    35/   55] train: loss: 0.0000921
[Epoch 146] ogbg-molbbbp: 0.955989 val loss: 0.487421
[Epoch 146] ogbg-molbbbp: 0.640721 test loss: 2.588928
[Epoch 147; Iter    10/   55] train: loss: 0.0008494
[Epoch 147; Iter    40/   55] train: loss: 0.0006456
[Epoch 147] ogbg-molbbbp: 0.957284 val loss: 0.476703
[Epoch 147] ogbg-molbbbp: 0.641107 test loss: 2.742545
[Epoch 148; Iter    15/   55] train: loss: 0.0000569
[Epoch 148; Iter    45/   55] train: loss: 0.0000644
[Epoch 148] ogbg-molbbbp: 0.955989 val loss: 0.476252
[Epoch 148] ogbg-molbbbp: 0.639564 test loss: 2.656710
[Epoch 109] ogbg-molbbbp: 0.946132 val loss: 0.492093
[Epoch 109] ogbg-molbbbp: 0.642072 test loss: 2.206180
[Epoch 110; Iter     5/   55] train: loss: 0.0010540
[Epoch 110; Iter    35/   55] train: loss: 0.0013357
[Epoch 110] ogbg-molbbbp: 0.941352 val loss: 0.633990
[Epoch 110] ogbg-molbbbp: 0.661169 test loss: 2.352701
[Epoch 111; Iter    10/   55] train: loss: 0.0002366
[Epoch 111; Iter    40/   55] train: loss: 0.0006356
[Epoch 111] ogbg-molbbbp: 0.946430 val loss: 0.612977
[Epoch 111] ogbg-molbbbp: 0.649306 test loss: 2.451187
[Epoch 112; Iter    15/   55] train: loss: 0.0002037
[Epoch 112; Iter    45/   55] train: loss: 0.0010343
[Epoch 112] ogbg-molbbbp: 0.957782 val loss: 0.514571
[Epoch 112] ogbg-molbbbp: 0.654996 test loss: 2.464573
[Epoch 113; Iter    20/   55] train: loss: 0.0009776
[Epoch 113; Iter    50/   55] train: loss: 0.0012103
[Epoch 113] ogbg-molbbbp: 0.958777 val loss: 0.511290
[Epoch 113] ogbg-molbbbp: 0.657504 test loss: 2.562255
[Epoch 114; Iter    25/   55] train: loss: 0.0005298
[Epoch 114; Iter    55/   55] train: loss: 0.0000826
[Epoch 114] ogbg-molbbbp: 0.955989 val loss: 0.551978
[Epoch 114] ogbg-molbbbp: 0.657407 test loss: 2.662799
[Epoch 115; Iter    30/   55] train: loss: 0.0003446
[Epoch 115] ogbg-molbbbp: 0.952106 val loss: 0.577230
[Epoch 115] ogbg-molbbbp: 0.654417 test loss: 2.609099
[Epoch 116; Iter     5/   55] train: loss: 0.0001486
[Epoch 116; Iter    35/   55] train: loss: 0.0025567
[Epoch 116] ogbg-molbbbp: 0.953301 val loss: 0.547822
[Epoch 116] ogbg-molbbbp: 0.654803 test loss: 2.585275
[Epoch 117; Iter    10/   55] train: loss: 0.0911811
[Epoch 117; Iter    40/   55] train: loss: 0.0010676
[Epoch 117] ogbg-molbbbp: 0.947625 val loss: 0.719772
[Epoch 117] ogbg-molbbbp: 0.649884 test loss: 2.916935
[Epoch 118; Iter    15/   55] train: loss: 0.0882784
[Epoch 118; Iter    45/   55] train: loss: 0.0012249
[Epoch 118] ogbg-molbbbp: 0.947227 val loss: 0.675552
[Epoch 118] ogbg-molbbbp: 0.655864 test loss: 2.783994
[Epoch 119; Iter    20/   55] train: loss: 0.0002096
[Epoch 119; Iter    50/   55] train: loss: 0.0000620
[Epoch 119] ogbg-molbbbp: 0.955292 val loss: 0.599569
[Epoch 119] ogbg-molbbbp: 0.657793 test loss: 2.780481
[Epoch 120; Iter    25/   55] train: loss: 0.0297114
[Epoch 120; Iter    55/   55] train: loss: 0.0000821
[Epoch 120] ogbg-molbbbp: 0.952504 val loss: 0.593526
[Epoch 120] ogbg-molbbbp: 0.623457 test loss: 2.992704
[Epoch 121; Iter    30/   55] train: loss: 0.0124450
[Epoch 121] ogbg-molbbbp: 0.953998 val loss: 0.569710
[Epoch 121] ogbg-molbbbp: 0.633584 test loss: 2.887384
[Epoch 122; Iter     5/   55] train: loss: 0.0029581
[Epoch 122; Iter    35/   55] train: loss: 0.0004917
[Epoch 122] ogbg-molbbbp: 0.951807 val loss: 0.605358
[Epoch 122] ogbg-molbbbp: 0.629533 test loss: 2.955035
[Epoch 123; Iter    10/   55] train: loss: 0.0008941
[Epoch 123; Iter    40/   55] train: loss: 0.0001180
[Epoch 123] ogbg-molbbbp: 0.956885 val loss: 0.534317
[Epoch 123] ogbg-molbbbp: 0.641107 test loss: 2.799686
[Epoch 124; Iter    15/   55] train: loss: 0.0001266
[Epoch 124; Iter    45/   55] train: loss: 0.0002503
[Epoch 124] ogbg-molbbbp: 0.957981 val loss: 0.505765
[Epoch 124] ogbg-molbbbp: 0.638117 test loss: 2.786838
[Epoch 125; Iter    20/   55] train: loss: 0.0720885
[Epoch 125; Iter    50/   55] train: loss: 0.0003072
[Epoch 125] ogbg-molbbbp: 0.956188 val loss: 0.535395
[Epoch 125] ogbg-molbbbp: 0.638792 test loss: 2.799379
[Epoch 126; Iter    25/   55] train: loss: 0.0039645
[Epoch 126; Iter    55/   55] train: loss: 0.0050353
[Epoch 126] ogbg-molbbbp: 0.961067 val loss: 0.499453
[Epoch 126] ogbg-molbbbp: 0.640818 test loss: 2.843460
[Epoch 127; Iter    30/   55] train: loss: 0.0001620
[Epoch 127] ogbg-molbbbp: 0.954496 val loss: 0.544016
[Epoch 127] ogbg-molbbbp: 0.635610 test loss: 2.873688
[Epoch 128; Iter     5/   55] train: loss: 0.0017397
[Epoch 128; Iter    35/   55] train: loss: 0.0002512
[Epoch 128] ogbg-molbbbp: 0.950015 val loss: 0.604581
[Epoch 128] ogbg-molbbbp: 0.638407 test loss: 2.896327
[Epoch 129; Iter    10/   55] train: loss: 0.0003800
[Epoch 129; Iter    40/   55] train: loss: 0.0002022
[Epoch 129] ogbg-molbbbp: 0.953002 val loss: 0.574369
[Epoch 129] ogbg-molbbbp: 0.634549 test loss: 2.981130
[Epoch 130; Iter    15/   55] train: loss: 0.0001261
[Epoch 130; Iter    45/   55] train: loss: 0.0001889
[Epoch 130] ogbg-molbbbp: 0.953400 val loss: 0.597412
[Epoch 130] ogbg-molbbbp: 0.635224 test loss: 3.076033
[Epoch 131; Iter    20/   55] train: loss: 0.0002857
[Epoch 131; Iter    50/   55] train: loss: 0.0001194
[Epoch 131] ogbg-molbbbp: 0.949019 val loss: 0.643857
[Epoch 131] ogbg-molbbbp: 0.626254 test loss: 3.114390
[Epoch 132; Iter    25/   55] train: loss: 0.0013731
[Epoch 132; Iter    55/   55] train: loss: 0.0001064
[Epoch 132] ogbg-molbbbp: 0.951210 val loss: 0.613980
[Epoch 132] ogbg-molbbbp: 0.628569 test loss: 3.088932
[Epoch 133; Iter    30/   55] train: loss: 0.0001690
[Epoch 133] ogbg-molbbbp: 0.949915 val loss: 0.633998
[Epoch 133] ogbg-molbbbp: 0.627797 test loss: 3.127192
[Epoch 134; Iter     5/   55] train: loss: 0.0002152
[Epoch 134; Iter    35/   55] train: loss: 0.0011104
[Epoch 134] ogbg-molbbbp: 0.950214 val loss: 0.614545
[Epoch 134] ogbg-molbbbp: 0.627990 test loss: 3.056342
[Epoch 135; Iter    10/   55] train: loss: 0.0000773
[Epoch 135; Iter    40/   55] train: loss: 0.0033740
[Epoch 135] ogbg-molbbbp: 0.944937 val loss: 0.623963
[Epoch 135] ogbg-molbbbp: 0.623553 test loss: 2.940319
[Epoch 136; Iter    15/   55] train: loss: 0.0080014
[Epoch 136; Iter    45/   55] train: loss: 0.0000879
[Epoch 136] ogbg-molbbbp: 0.949318 val loss: 0.593538
[Epoch 136] ogbg-molbbbp: 0.628472 test loss: 2.865147
[Epoch 137; Iter    20/   55] train: loss: 0.0004277
[Epoch 137; Iter    50/   55] train: loss: 0.0000719
[Epoch 137] ogbg-molbbbp: 0.952903 val loss: 0.571932
[Epoch 137] ogbg-molbbbp: 0.630401 test loss: 2.890092
[Epoch 138; Iter    25/   55] train: loss: 0.0002290
[Epoch 138; Iter    55/   55] train: loss: 0.0369926
[Epoch 138] ogbg-molbbbp: 0.952504 val loss: 0.570758
[Epoch 138] ogbg-molbbbp: 0.633681 test loss: 2.792193
[Epoch 139; Iter    30/   55] train: loss: 0.0002911
[Epoch 139] ogbg-molbbbp: 0.955292 val loss: 0.536617
[Epoch 139] ogbg-molbbbp: 0.636381 test loss: 2.841444
[Epoch 140; Iter     5/   55] train: loss: 0.0058252
[Epoch 140; Iter    35/   55] train: loss: 0.0001215
[Epoch 140] ogbg-molbbbp: 0.955890 val loss: 0.545739
[Epoch 140] ogbg-molbbbp: 0.638985 test loss: 2.877214
[Epoch 141; Iter    10/   55] train: loss: 0.0001333
[Epoch 141; Iter    40/   55] train: loss: 0.0004026
[Epoch 141] ogbg-molbbbp: 0.954695 val loss: 0.559696
[Epoch 141] ogbg-molbbbp: 0.640143 test loss: 2.925833
[Epoch 142; Iter    15/   55] train: loss: 0.0002854
[Epoch 142; Iter    45/   55] train: loss: 0.0013286
[Epoch 142] ogbg-molbbbp: 0.954894 val loss: 0.571696
[Epoch 142] ogbg-molbbbp: 0.638600 test loss: 2.989696
[Epoch 143; Iter    20/   55] train: loss: 0.0000426
[Epoch 143; Iter    50/   55] train: loss: 0.0000786
[Epoch 143] ogbg-molbbbp: 0.955491 val loss: 0.548353
[Epoch 143] ogbg-molbbbp: 0.638021 test loss: 2.898426
[Epoch 144; Iter    25/   55] train: loss: 0.0001064
[Epoch 144; Iter    55/   55] train: loss: 0.1285547
[Epoch 144] ogbg-molbbbp: 0.957483 val loss: 0.558500
[Epoch 144] ogbg-molbbbp: 0.640432 test loss: 2.989513
[Epoch 145; Iter    30/   55] train: loss: 0.0034272
[Epoch 145] ogbg-molbbbp: 0.950115 val loss: 0.653484
[Epoch 145] ogbg-molbbbp: 0.634549 test loss: 3.034001
[Epoch 146; Iter     5/   55] train: loss: 0.0031086
[Epoch 146; Iter    35/   55] train: loss: 0.0002176
[Epoch 146] ogbg-molbbbp: 0.952106 val loss: 0.646073
[Epoch 146] ogbg-molbbbp: 0.637056 test loss: 3.063405
[Epoch 147; Iter    10/   55] train: loss: 0.0000973
[Epoch 147; Iter    40/   55] train: loss: 0.0005359
[Epoch 147] ogbg-molbbbp: 0.952903 val loss: 0.622820
[Epoch 147] ogbg-molbbbp: 0.638407 test loss: 2.989051
[Epoch 148; Iter    15/   55] train: loss: 0.0002206
[Epoch 148; Iter    45/   55] train: loss: 0.0000645
[Epoch 148] ogbg-molbbbp: 0.954595 val loss: 0.582041
[Epoch 148] ogbg-molbbbp: 0.638889 test loss: 2.996943
[Epoch 69; Iter    40/   55] train: loss: 0.0716489
[Epoch 69] ogbg-molbbbp: 0.965150 val loss: 0.423196
[Epoch 69] ogbg-molbbbp: 0.713156 test loss: 1.755026
[Epoch 70; Iter    15/   55] train: loss: 0.0236045
[Epoch 70; Iter    45/   55] train: loss: 0.0277191
[Epoch 70] ogbg-molbbbp: 0.962661 val loss: 0.341414
[Epoch 70] ogbg-molbbbp: 0.683835 test loss: 1.703109
[Epoch 71; Iter    20/   55] train: loss: 0.0048718
[Epoch 71; Iter    50/   55] train: loss: 0.0088770
[Epoch 71] ogbg-molbbbp: 0.960470 val loss: 0.381126
[Epoch 71] ogbg-molbbbp: 0.691744 test loss: 1.710748
[Epoch 72; Iter    25/   55] train: loss: 0.0033043
[Epoch 72; Iter    55/   55] train: loss: 0.0041191
[Epoch 72] ogbg-molbbbp: 0.960072 val loss: 0.410212
[Epoch 72] ogbg-molbbbp: 0.690104 test loss: 1.791590
[Epoch 73; Iter    30/   55] train: loss: 0.0785801
[Epoch 73] ogbg-molbbbp: 0.949418 val loss: 0.474822
[Epoch 73] ogbg-molbbbp: 0.678627 test loss: 1.890144
[Epoch 74; Iter     5/   55] train: loss: 0.0115323
[Epoch 74; Iter    35/   55] train: loss: 0.0095602
[Epoch 74] ogbg-molbbbp: 0.960968 val loss: 0.355493
[Epoch 74] ogbg-molbbbp: 0.661555 test loss: 1.880461
[Epoch 75; Iter    10/   55] train: loss: 0.0286297
[Epoch 75; Iter    40/   55] train: loss: 0.0185420
[Epoch 75] ogbg-molbbbp: 0.960271 val loss: 0.407631
[Epoch 75] ogbg-molbbbp: 0.669946 test loss: 1.961835
[Epoch 76; Iter    15/   55] train: loss: 0.0531314
[Epoch 76; Iter    45/   55] train: loss: 0.1044625
[Epoch 76] ogbg-molbbbp: 0.964752 val loss: 0.426695
[Epoch 76] ogbg-molbbbp: 0.691551 test loss: 2.072205
[Epoch 77; Iter    20/   55] train: loss: 0.0305734
[Epoch 77; Iter    50/   55] train: loss: 0.1376411
[Epoch 77] ogbg-molbbbp: 0.950214 val loss: 0.490166
[Epoch 77] ogbg-molbbbp: 0.673900 test loss: 1.859370
[Epoch 78; Iter    25/   55] train: loss: 0.0031049
[Epoch 78; Iter    55/   55] train: loss: 0.0035429
[Epoch 78] ogbg-molbbbp: 0.960171 val loss: 0.402024
[Epoch 78] ogbg-molbbbp: 0.678241 test loss: 2.097655
[Epoch 79; Iter    30/   55] train: loss: 0.0090410
[Epoch 79] ogbg-molbbbp: 0.954695 val loss: 0.559295
[Epoch 79] ogbg-molbbbp: 0.688368 test loss: 2.288113
[Epoch 80; Iter     5/   55] train: loss: 0.0069744
[Epoch 80; Iter    35/   55] train: loss: 0.0032556
[Epoch 80] ogbg-molbbbp: 0.957284 val loss: 0.452544
[Epoch 80] ogbg-molbbbp: 0.702064 test loss: 1.923259
[Epoch 81; Iter    10/   55] train: loss: 0.0840797
[Epoch 81; Iter    40/   55] train: loss: 0.1457571
[Epoch 81] ogbg-molbbbp: 0.962760 val loss: 0.371841
[Epoch 81] ogbg-molbbbp: 0.698206 test loss: 1.795393
[Epoch 82; Iter    15/   55] train: loss: 0.0028901
[Epoch 82; Iter    45/   55] train: loss: 0.0059778
[Epoch 82] ogbg-molbbbp: 0.952006 val loss: 0.418124
[Epoch 82] ogbg-molbbbp: 0.664931 test loss: 1.973646
[Epoch 83; Iter    20/   55] train: loss: 0.0657296
[Epoch 83; Iter    50/   55] train: loss: 0.0177802
[Epoch 83] ogbg-molbbbp: 0.956786 val loss: 0.416346
[Epoch 83] ogbg-molbbbp: 0.671971 test loss: 2.106480
[Epoch 84; Iter    25/   55] train: loss: 0.0636851
[Epoch 84; Iter    55/   55] train: loss: 0.0022880
[Epoch 84] ogbg-molbbbp: 0.959375 val loss: 0.381997
[Epoch 84] ogbg-molbbbp: 0.685089 test loss: 1.899805
[Epoch 85; Iter    30/   55] train: loss: 0.0829719
[Epoch 85] ogbg-molbbbp: 0.958479 val loss: 0.517738
[Epoch 85] ogbg-molbbbp: 0.710745 test loss: 2.091534
[Epoch 86; Iter     5/   55] train: loss: 0.0064167
[Epoch 86; Iter    35/   55] train: loss: 0.0499683
[Epoch 86] ogbg-molbbbp: 0.951608 val loss: 0.606931
[Epoch 86] ogbg-molbbbp: 0.724537 test loss: 2.007580
[Epoch 87; Iter    10/   55] train: loss: 0.0638436
[Epoch 87; Iter    40/   55] train: loss: 0.0075210
[Epoch 87] ogbg-molbbbp: 0.949218 val loss: 0.499325
[Epoch 87] ogbg-molbbbp: 0.686246 test loss: 1.972397
[Epoch 88; Iter    15/   55] train: loss: 0.0467502
[Epoch 88; Iter    45/   55] train: loss: 0.0017527
[Epoch 88] ogbg-molbbbp: 0.950314 val loss: 0.522230
[Epoch 88] ogbg-molbbbp: 0.703607 test loss: 2.049074
[Epoch 89; Iter    20/   55] train: loss: 0.1097981
[Epoch 89; Iter    50/   55] train: loss: 0.0195349
[Epoch 89] ogbg-molbbbp: 0.953600 val loss: 0.396279
[Epoch 89] ogbg-molbbbp: 0.684510 test loss: 1.805602
[Epoch 90; Iter    25/   55] train: loss: 0.0046104
[Epoch 90; Iter    55/   55] train: loss: 0.0017862
[Epoch 90] ogbg-molbbbp: 0.962362 val loss: 0.381944
[Epoch 90] ogbg-molbbbp: 0.701292 test loss: 1.921916
[Epoch 91; Iter    30/   55] train: loss: 0.0048293
[Epoch 91] ogbg-molbbbp: 0.957881 val loss: 0.426961
[Epoch 91] ogbg-molbbbp: 0.689622 test loss: 1.944584
[Epoch 92; Iter     5/   55] train: loss: 0.0074616
[Epoch 92; Iter    35/   55] train: loss: 0.0140710
[Epoch 92] ogbg-molbbbp: 0.954197 val loss: 0.447546
[Epoch 92] ogbg-molbbbp: 0.693769 test loss: 2.067615
[Epoch 93; Iter    10/   55] train: loss: 0.0043573
[Epoch 93; Iter    40/   55] train: loss: 0.0018822
[Epoch 93] ogbg-molbbbp: 0.953699 val loss: 0.546073
[Epoch 93] ogbg-molbbbp: 0.688561 test loss: 2.192521
[Epoch 94; Iter    15/   55] train: loss: 0.0129458
[Epoch 94; Iter    45/   55] train: loss: 0.0084625
[Epoch 94] ogbg-molbbbp: 0.956587 val loss: 0.431541
[Epoch 94] ogbg-molbbbp: 0.686053 test loss: 1.992113
[Epoch 95; Iter    20/   55] train: loss: 0.0025036
[Epoch 95; Iter    50/   55] train: loss: 0.0111858
[Epoch 95] ogbg-molbbbp: 0.952903 val loss: 0.482961
[Epoch 95] ogbg-molbbbp: 0.694348 test loss: 2.135779
[Epoch 96; Iter    25/   55] train: loss: 0.0279018
[Epoch 96; Iter    55/   55] train: loss: 0.0021514
[Epoch 96] ogbg-molbbbp: 0.955491 val loss: 0.432325
[Epoch 96] ogbg-molbbbp: 0.689140 test loss: 1.917345
[Epoch 97; Iter    30/   55] train: loss: 0.0041067
[Epoch 97] ogbg-molbbbp: 0.951907 val loss: 0.563039
[Epoch 97] ogbg-molbbbp: 0.686728 test loss: 2.242934
[Epoch 98; Iter     5/   55] train: loss: 0.0046384
[Epoch 98; Iter    35/   55] train: loss: 0.0024298
[Epoch 98] ogbg-molbbbp: 0.959176 val loss: 0.387166
[Epoch 98] ogbg-molbbbp: 0.682774 test loss: 1.925749
[Epoch 99; Iter    10/   55] train: loss: 0.0970378
[Epoch 99; Iter    40/   55] train: loss: 0.0180889
[Epoch 99] ogbg-molbbbp: 0.954695 val loss: 0.510173
[Epoch 99] ogbg-molbbbp: 0.698592 test loss: 2.109799
[Epoch 100; Iter    15/   55] train: loss: 0.0018461
[Epoch 100; Iter    45/   55] train: loss: 0.0031569
[Epoch 100] ogbg-molbbbp: 0.958976 val loss: 0.425723
[Epoch 100] ogbg-molbbbp: 0.693673 test loss: 1.931243
[Epoch 101; Iter    20/   55] train: loss: 0.1021599
[Epoch 101; Iter    50/   55] train: loss: 0.0128181
[Epoch 101] ogbg-molbbbp: 0.960769 val loss: 0.483671
[Epoch 101] ogbg-molbbbp: 0.693383 test loss: 2.179659
[Epoch 102; Iter    25/   55] train: loss: 0.0300631
[Epoch 102; Iter    55/   55] train: loss: 0.0019927
[Epoch 102] ogbg-molbbbp: 0.961366 val loss: 0.432523
[Epoch 102] ogbg-molbbbp: 0.680170 test loss: 2.035860
[Epoch 103; Iter    30/   55] train: loss: 0.0011606
[Epoch 103] ogbg-molbbbp: 0.958080 val loss: 0.467250
[Epoch 103] ogbg-molbbbp: 0.673997 test loss: 2.265918
[Epoch 104; Iter     5/   55] train: loss: 0.0054387
[Epoch 104; Iter    35/   55] train: loss: 0.0067553
[Epoch 104] ogbg-molbbbp: 0.962860 val loss: 0.394883
[Epoch 104] ogbg-molbbbp: 0.678048 test loss: 2.125207
[Epoch 105; Iter    10/   55] train: loss: 0.0030507
[Epoch 105; Iter    40/   55] train: loss: 0.0217470
[Epoch 105] ogbg-molbbbp: 0.960570 val loss: 0.458680
[Epoch 105] ogbg-molbbbp: 0.684028 test loss: 2.212278
[Epoch 106; Iter    15/   55] train: loss: 0.0014652
[Epoch 106; Iter    45/   55] train: loss: 0.0109256
[Epoch 106] ogbg-molbbbp: 0.954794 val loss: 0.515485
[Epoch 106] ogbg-molbbbp: 0.669078 test loss: 2.275990
[Epoch 107; Iter    20/   55] train: loss: 0.0015381
[Epoch 107; Iter    50/   55] train: loss: 0.0282707
[Epoch 107] ogbg-molbbbp: 0.953102 val loss: 0.479335
[Epoch 107] ogbg-molbbbp: 0.679880 test loss: 2.021689
[Epoch 108; Iter    25/   55] train: loss: 0.0104508
[Epoch 108; Iter    55/   55] train: loss: 0.1367371
[Epoch 108] ogbg-molbbbp: 0.953799 val loss: 0.586504
[Epoch 108] ogbg-molbbbp: 0.696470 test loss: 2.247551
[Epoch 109; Iter    30/   55] train: loss: 0.0026135
[Epoch 69; Iter    40/   55] train: loss: 0.0112418
[Epoch 69] ogbg-molbbbp: 0.949915 val loss: 0.526210
[Epoch 69] ogbg-molbbbp: 0.625000 test loss: 2.027904
[Epoch 70; Iter    15/   55] train: loss: 0.0989129
[Epoch 70; Iter    45/   55] train: loss: 0.0696618
[Epoch 70] ogbg-molbbbp: 0.964353 val loss: 0.384892
[Epoch 70] ogbg-molbbbp: 0.640914 test loss: 1.971848
[Epoch 71; Iter    20/   55] train: loss: 0.0535085
[Epoch 71; Iter    50/   55] train: loss: 0.2045204
[Epoch 71] ogbg-molbbbp: 0.962262 val loss: 0.422646
[Epoch 71] ogbg-molbbbp: 0.667824 test loss: 1.829180
[Epoch 72; Iter    25/   55] train: loss: 0.1450841
[Epoch 72; Iter    55/   55] train: loss: 0.2894095
[Epoch 72] ogbg-molbbbp: 0.958578 val loss: 0.424225
[Epoch 72] ogbg-molbbbp: 0.665992 test loss: 1.650537
[Epoch 73; Iter    30/   55] train: loss: 0.0996367
[Epoch 73] ogbg-molbbbp: 0.958279 val loss: 0.430178
[Epoch 73] ogbg-molbbbp: 0.631848 test loss: 1.905508
[Epoch 74; Iter     5/   55] train: loss: 0.0072786
[Epoch 74; Iter    35/   55] train: loss: 0.0331044
[Epoch 74] ogbg-molbbbp: 0.955193 val loss: 0.435035
[Epoch 74] ogbg-molbbbp: 0.639371 test loss: 1.882197
[Epoch 75; Iter    10/   55] train: loss: 0.0248690
[Epoch 75; Iter    40/   55] train: loss: 0.0880536
[Epoch 75] ogbg-molbbbp: 0.956686 val loss: 0.474159
[Epoch 75] ogbg-molbbbp: 0.632909 test loss: 1.820105
[Epoch 76; Iter    15/   55] train: loss: 0.1259567
[Epoch 76; Iter    45/   55] train: loss: 0.0780520
[Epoch 76] ogbg-molbbbp: 0.957582 val loss: 0.485646
[Epoch 76] ogbg-molbbbp: 0.671779 test loss: 1.789438
[Epoch 77; Iter    20/   55] train: loss: 0.0031291
[Epoch 77; Iter    50/   55] train: loss: 0.0049758
[Epoch 77] ogbg-molbbbp: 0.965050 val loss: 0.379404
[Epoch 77] ogbg-molbbbp: 0.676312 test loss: 1.752494
[Epoch 78; Iter    25/   55] train: loss: 0.0494426
[Epoch 78; Iter    55/   55] train: loss: 0.3750969
[Epoch 78] ogbg-molbbbp: 0.953301 val loss: 0.447530
[Epoch 78] ogbg-molbbbp: 0.635610 test loss: 1.824751
[Epoch 79; Iter    30/   55] train: loss: 0.0189604
[Epoch 79] ogbg-molbbbp: 0.954097 val loss: 0.524620
[Epoch 79] ogbg-molbbbp: 0.633005 test loss: 2.052222
[Epoch 80; Iter     5/   55] train: loss: 0.1100257
[Epoch 80; Iter    35/   55] train: loss: 0.0479065
[Epoch 80] ogbg-molbbbp: 0.956885 val loss: 0.444525
[Epoch 80] ogbg-molbbbp: 0.655961 test loss: 1.644109
[Epoch 81; Iter    10/   55] train: loss: 0.0090727
[Epoch 81; Iter    40/   55] train: loss: 0.0206689
[Epoch 81] ogbg-molbbbp: 0.950115 val loss: 0.557949
[Epoch 81] ogbg-molbbbp: 0.646508 test loss: 1.934923
[Epoch 82; Iter    15/   55] train: loss: 0.0175496
[Epoch 82; Iter    45/   55] train: loss: 0.0780118
[Epoch 82] ogbg-molbbbp: 0.951409 val loss: 0.465676
[Epoch 82] ogbg-molbbbp: 0.649402 test loss: 1.726466
[Epoch 83; Iter    20/   55] train: loss: 0.0179833
[Epoch 83; Iter    50/   55] train: loss: 0.0450212
[Epoch 83] ogbg-molbbbp: 0.950712 val loss: 0.501831
[Epoch 83] ogbg-molbbbp: 0.646123 test loss: 2.049297
[Epoch 84; Iter    25/   55] train: loss: 0.0039534
[Epoch 84; Iter    55/   55] train: loss: 0.1139684
[Epoch 84] ogbg-molbbbp: 0.943742 val loss: 0.668863
[Epoch 84] ogbg-molbbbp: 0.665606 test loss: 1.935276
[Epoch 85; Iter    30/   55] train: loss: 0.0058480
[Epoch 85] ogbg-molbbbp: 0.953600 val loss: 0.512234
[Epoch 85] ogbg-molbbbp: 0.660204 test loss: 1.973209
[Epoch 86; Iter     5/   55] train: loss: 0.0111522
[Epoch 86; Iter    35/   55] train: loss: 0.0029256
[Epoch 86] ogbg-molbbbp: 0.954496 val loss: 0.493101
[Epoch 86] ogbg-molbbbp: 0.659915 test loss: 1.899483
[Epoch 87; Iter    10/   55] train: loss: 0.0468008
[Epoch 87; Iter    40/   55] train: loss: 0.0686022
[Epoch 87] ogbg-molbbbp: 0.949816 val loss: 0.473200
[Epoch 87] ogbg-molbbbp: 0.666763 test loss: 1.606195
[Epoch 88; Iter    15/   55] train: loss: 0.0167100
[Epoch 88; Iter    45/   55] train: loss: 0.0570945
[Epoch 88] ogbg-molbbbp: 0.961167 val loss: 0.496089
[Epoch 88] ogbg-molbbbp: 0.660301 test loss: 1.926307
[Epoch 89; Iter    20/   55] train: loss: 0.0926416
[Epoch 89; Iter    50/   55] train: loss: 0.0311982
[Epoch 89] ogbg-molbbbp: 0.939659 val loss: 0.631418
[Epoch 89] ogbg-molbbbp: 0.642747 test loss: 1.965957
[Epoch 90; Iter    25/   55] train: loss: 0.0015953
[Epoch 90; Iter    55/   55] train: loss: 0.0781535
[Epoch 90] ogbg-molbbbp: 0.959574 val loss: 0.498215
[Epoch 90] ogbg-molbbbp: 0.658951 test loss: 1.941714
[Epoch 91; Iter    30/   55] train: loss: 0.0203988
[Epoch 91] ogbg-molbbbp: 0.954396 val loss: 0.529076
[Epoch 91] ogbg-molbbbp: 0.619406 test loss: 2.144700
[Epoch 92; Iter     5/   55] train: loss: 0.0056239
[Epoch 92; Iter    35/   55] train: loss: 0.0110183
[Epoch 92] ogbg-molbbbp: 0.949915 val loss: 0.497762
[Epoch 92] ogbg-molbbbp: 0.642940 test loss: 1.962233
[Epoch 93; Iter    10/   55] train: loss: 0.0102241
[Epoch 93; Iter    40/   55] train: loss: 0.0062792
[Epoch 93] ogbg-molbbbp: 0.950812 val loss: 0.500849
[Epoch 93] ogbg-molbbbp: 0.633777 test loss: 2.018051
[Epoch 94; Iter    15/   55] train: loss: 0.0118457
[Epoch 94; Iter    45/   55] train: loss: 0.0138893
[Epoch 94] ogbg-molbbbp: 0.953002 val loss: 0.533421
[Epoch 94] ogbg-molbbbp: 0.633777 test loss: 2.160982
[Epoch 95; Iter    20/   55] train: loss: 0.0728453
[Epoch 95; Iter    50/   55] train: loss: 0.0033822
[Epoch 95] ogbg-molbbbp: 0.955691 val loss: 0.458807
[Epoch 95] ogbg-molbbbp: 0.603684 test loss: 2.182034
[Epoch 96; Iter    25/   55] train: loss: 0.0711774
[Epoch 96; Iter    55/   55] train: loss: 0.0110219
[Epoch 96] ogbg-molbbbp: 0.948721 val loss: 0.555423
[Epoch 96] ogbg-molbbbp: 0.649884 test loss: 2.158071
[Epoch 97; Iter    30/   55] train: loss: 0.0197104
[Epoch 97] ogbg-molbbbp: 0.943841 val loss: 0.508270
[Epoch 97] ogbg-molbbbp: 0.631944 test loss: 2.005888
[Epoch 98; Iter     5/   55] train: loss: 0.0019425
[Epoch 98; Iter    35/   55] train: loss: 0.0157201
[Epoch 98] ogbg-molbbbp: 0.951807 val loss: 0.484958
[Epoch 98] ogbg-molbbbp: 0.654514 test loss: 1.999140
[Epoch 99; Iter    10/   55] train: loss: 0.1500742
[Epoch 99; Iter    40/   55] train: loss: 0.0020143
[Epoch 99] ogbg-molbbbp: 0.949418 val loss: 0.520203
[Epoch 99] ogbg-molbbbp: 0.662809 test loss: 1.975600
[Epoch 100; Iter    15/   55] train: loss: 0.0024502
[Epoch 100; Iter    45/   55] train: loss: 0.0083001
[Epoch 100] ogbg-molbbbp: 0.954994 val loss: 0.463244
[Epoch 100] ogbg-molbbbp: 0.652585 test loss: 2.022288
[Epoch 101; Iter    20/   55] train: loss: 0.0014954
[Epoch 101; Iter    50/   55] train: loss: 0.0780269
[Epoch 101] ogbg-molbbbp: 0.950314 val loss: 0.491690
[Epoch 101] ogbg-molbbbp: 0.648052 test loss: 2.067910
[Epoch 102; Iter    25/   55] train: loss: 0.0198197
[Epoch 102; Iter    55/   55] train: loss: 0.0079688
[Epoch 102] ogbg-molbbbp: 0.951210 val loss: 0.518044
[Epoch 102] ogbg-molbbbp: 0.636188 test loss: 2.192483
[Epoch 103; Iter    30/   55] train: loss: 0.0010349
[Epoch 103] ogbg-molbbbp: 0.944837 val loss: 0.611486
[Epoch 103] ogbg-molbbbp: 0.637056 test loss: 2.184420
[Epoch 104; Iter     5/   55] train: loss: 0.0018848
[Epoch 104; Iter    35/   55] train: loss: 0.0045269
[Epoch 104] ogbg-molbbbp: 0.944538 val loss: 0.540932
[Epoch 104] ogbg-molbbbp: 0.636381 test loss: 2.056855
[Epoch 105; Iter    10/   55] train: loss: 0.0080792
[Epoch 105; Iter    40/   55] train: loss: 0.0493480
[Epoch 105] ogbg-molbbbp: 0.949716 val loss: 0.593756
[Epoch 105] ogbg-molbbbp: 0.633873 test loss: 2.107055
[Epoch 106; Iter    15/   55] train: loss: 0.0680370
[Epoch 106; Iter    45/   55] train: loss: 0.0013484
[Epoch 106] ogbg-molbbbp: 0.945634 val loss: 0.598322
[Epoch 106] ogbg-molbbbp: 0.645062 test loss: 1.979841
[Epoch 107; Iter    20/   55] train: loss: 0.0070676
[Epoch 107; Iter    50/   55] train: loss: 0.0042699
[Epoch 107] ogbg-molbbbp: 0.945435 val loss: 0.562088
[Epoch 107] ogbg-molbbbp: 0.606964 test loss: 2.276383
[Epoch 108; Iter    25/   55] train: loss: 0.0097720
[Epoch 108; Iter    55/   55] train: loss: 0.0366057
[Epoch 108] ogbg-molbbbp: 0.945733 val loss: 0.537615
[Epoch 108] ogbg-molbbbp: 0.622975 test loss: 2.260198
[Epoch 109; Iter    30/   55] train: loss: 0.0323843
[Epoch 69; Iter    40/   55] train: loss: 0.0650941
[Epoch 69] ogbg-molbbbp: 0.944140 val loss: 0.618295
[Epoch 69] ogbg-molbbbp: 0.670235 test loss: 2.514189
[Epoch 70; Iter    15/   55] train: loss: 0.6380295
[Epoch 70; Iter    45/   55] train: loss: 0.2060047
[Epoch 70] ogbg-molbbbp: 0.944240 val loss: 0.526186
[Epoch 70] ogbg-molbbbp: 0.665606 test loss: 1.773795
[Epoch 71; Iter    20/   55] train: loss: 0.0464401
[Epoch 71; Iter    50/   55] train: loss: 0.0549491
[Epoch 71] ogbg-molbbbp: 0.967241 val loss: 0.299660
[Epoch 71] ogbg-molbbbp: 0.670235 test loss: 1.417489
[Epoch 72; Iter    25/   55] train: loss: 0.0772799
[Epoch 72; Iter    55/   55] train: loss: 0.1403587
[Epoch 72] ogbg-molbbbp: 0.961665 val loss: 0.453609
[Epoch 72] ogbg-molbbbp: 0.644579 test loss: 1.918913
[Epoch 73; Iter    30/   55] train: loss: 0.0530777
[Epoch 73] ogbg-molbbbp: 0.953699 val loss: 0.439765
[Epoch 73] ogbg-molbbbp: 0.635031 test loss: 1.759055
[Epoch 74; Iter     5/   55] train: loss: 0.0467408
[Epoch 74; Iter    35/   55] train: loss: 0.0277627
[Epoch 74] ogbg-molbbbp: 0.944240 val loss: 0.458371
[Epoch 74] ogbg-molbbbp: 0.621721 test loss: 1.750231
[Epoch 75; Iter    10/   55] train: loss: 0.0350639
[Epoch 75; Iter    40/   55] train: loss: 0.0493945
[Epoch 75] ogbg-molbbbp: 0.950314 val loss: 0.438143
[Epoch 75] ogbg-molbbbp: 0.636574 test loss: 1.718003
[Epoch 76; Iter    15/   55] train: loss: 0.1700301
[Epoch 76; Iter    45/   55] train: loss: 0.0225003
[Epoch 76] ogbg-molbbbp: 0.941850 val loss: 0.497165
[Epoch 76] ogbg-molbbbp: 0.636671 test loss: 1.812487
[Epoch 77; Iter    20/   55] train: loss: 0.0240622
[Epoch 77; Iter    50/   55] train: loss: 0.0463820
[Epoch 77] ogbg-molbbbp: 0.945235 val loss: 0.511934
[Epoch 77] ogbg-molbbbp: 0.630498 test loss: 1.802310
[Epoch 78; Iter    25/   55] train: loss: 0.1252587
[Epoch 78; Iter    55/   55] train: loss: 0.0476001
[Epoch 78] ogbg-molbbbp: 0.956388 val loss: 0.450434
[Epoch 78] ogbg-molbbbp: 0.649595 test loss: 1.933693
[Epoch 79; Iter    30/   55] train: loss: 0.0141070
[Epoch 79] ogbg-molbbbp: 0.934681 val loss: 0.689109
[Epoch 79] ogbg-molbbbp: 0.582658 test loss: 2.239114
[Epoch 80; Iter     5/   55] train: loss: 0.0445536
[Epoch 80; Iter    35/   55] train: loss: 0.0406511
[Epoch 80] ogbg-molbbbp: 0.953699 val loss: 0.381914
[Epoch 80] ogbg-molbbbp: 0.658179 test loss: 1.525696
[Epoch 81; Iter    10/   55] train: loss: 0.0106943
[Epoch 81; Iter    40/   55] train: loss: 0.0499421
[Epoch 81] ogbg-molbbbp: 0.949915 val loss: 0.415029
[Epoch 81] ogbg-molbbbp: 0.641686 test loss: 1.823184
[Epoch 82; Iter    15/   55] train: loss: 0.0605605
[Epoch 82; Iter    45/   55] train: loss: 0.0132995
[Epoch 82] ogbg-molbbbp: 0.951210 val loss: 0.494694
[Epoch 82] ogbg-molbbbp: 0.635802 test loss: 2.027093
[Epoch 83; Iter    20/   55] train: loss: 0.0646165
[Epoch 83; Iter    50/   55] train: loss: 0.0318508
[Epoch 83] ogbg-molbbbp: 0.947028 val loss: 0.491748
[Epoch 83] ogbg-molbbbp: 0.625289 test loss: 1.985561
[Epoch 84; Iter    25/   55] train: loss: 0.0915967
[Epoch 84; Iter    55/   55] train: loss: 0.0361589
[Epoch 84] ogbg-molbbbp: 0.953898 val loss: 0.486913
[Epoch 84] ogbg-molbbbp: 0.647859 test loss: 1.980808
[Epoch 85; Iter    30/   55] train: loss: 0.0070399
[Epoch 85] ogbg-molbbbp: 0.958578 val loss: 0.408254
[Epoch 85] ogbg-molbbbp: 0.664738 test loss: 1.808480
[Epoch 86; Iter     5/   55] train: loss: 0.1191566
[Epoch 86; Iter    35/   55] train: loss: 0.0484832
[Epoch 86] ogbg-molbbbp: 0.950413 val loss: 0.497046
[Epoch 86] ogbg-molbbbp: 0.640336 test loss: 1.865126
[Epoch 87; Iter    10/   55] train: loss: 0.0122922
[Epoch 87; Iter    40/   55] train: loss: 0.0069219
[Epoch 87] ogbg-molbbbp: 0.945036 val loss: 0.581041
[Epoch 87] ogbg-molbbbp: 0.639275 test loss: 2.185380
[Epoch 88; Iter    15/   55] train: loss: 0.0372957
[Epoch 88; Iter    45/   55] train: loss: 0.0089783
[Epoch 88] ogbg-molbbbp: 0.955989 val loss: 0.426829
[Epoch 88] ogbg-molbbbp: 0.648920 test loss: 1.773827
[Epoch 89; Iter    20/   55] train: loss: 0.0219845
[Epoch 89; Iter    50/   55] train: loss: 0.0596407
[Epoch 89] ogbg-molbbbp: 0.950214 val loss: 0.532366
[Epoch 89] ogbg-molbbbp: 0.636960 test loss: 2.154443
[Epoch 90; Iter    25/   55] train: loss: 0.0071308
[Epoch 90; Iter    55/   55] train: loss: 0.1153220
[Epoch 90] ogbg-molbbbp: 0.954396 val loss: 0.493655
[Epoch 90] ogbg-molbbbp: 0.636285 test loss: 2.220438
[Epoch 91; Iter    30/   55] train: loss: 0.0098144
[Epoch 91] ogbg-molbbbp: 0.956885 val loss: 0.394464
[Epoch 91] ogbg-molbbbp: 0.645351 test loss: 1.767331
[Epoch 92; Iter     5/   55] train: loss: 0.0075290
[Epoch 92; Iter    35/   55] train: loss: 0.0064362
[Epoch 92] ogbg-molbbbp: 0.951608 val loss: 0.449720
[Epoch 92] ogbg-molbbbp: 0.638214 test loss: 1.920498
[Epoch 93; Iter    10/   55] train: loss: 0.0061105
[Epoch 93; Iter    40/   55] train: loss: 0.0021991
[Epoch 93] ogbg-molbbbp: 0.941053 val loss: 0.606304
[Epoch 93] ogbg-molbbbp: 0.628472 test loss: 2.174167
[Epoch 94; Iter    15/   55] train: loss: 0.0731803
[Epoch 94; Iter    45/   55] train: loss: 0.0025182
[Epoch 94] ogbg-molbbbp: 0.949218 val loss: 0.591090
[Epoch 94] ogbg-molbbbp: 0.638117 test loss: 2.268844
[Epoch 95; Iter    20/   55] train: loss: 0.3944264
[Epoch 95; Iter    50/   55] train: loss: 0.0042802
[Epoch 95] ogbg-molbbbp: 0.944339 val loss: 0.553728
[Epoch 95] ogbg-molbbbp: 0.625772 test loss: 2.215194
[Epoch 96; Iter    25/   55] train: loss: 0.0078427
[Epoch 96; Iter    55/   55] train: loss: 0.0056776
[Epoch 96] ogbg-molbbbp: 0.955093 val loss: 0.474485
[Epoch 96] ogbg-molbbbp: 0.676987 test loss: 2.136089
[Epoch 97; Iter    30/   55] train: loss: 0.0116322
[Epoch 97] ogbg-molbbbp: 0.939759 val loss: 0.651748
[Epoch 97] ogbg-molbbbp: 0.653260 test loss: 2.190809
[Epoch 98; Iter     5/   55] train: loss: 0.0213318
[Epoch 98; Iter    35/   55] train: loss: 0.0085507
[Epoch 98] ogbg-molbbbp: 0.948223 val loss: 0.536632
[Epoch 98] ogbg-molbbbp: 0.643808 test loss: 2.076785
[Epoch 99; Iter    10/   55] train: loss: 0.0058840
[Epoch 99; Iter    40/   55] train: loss: 0.0049227
[Epoch 99] ogbg-molbbbp: 0.957184 val loss: 0.433248
[Epoch 99] ogbg-molbbbp: 0.657697 test loss: 1.923027
[Epoch 100; Iter    15/   55] train: loss: 0.0096419
[Epoch 100; Iter    45/   55] train: loss: 0.0192840
[Epoch 100] ogbg-molbbbp: 0.938863 val loss: 0.637783
[Epoch 100] ogbg-molbbbp: 0.616609 test loss: 2.557878
[Epoch 101; Iter    20/   55] train: loss: 0.1085727
[Epoch 101; Iter    50/   55] train: loss: 0.0088522
[Epoch 101] ogbg-molbbbp: 0.943144 val loss: 0.643949
[Epoch 101] ogbg-molbbbp: 0.635031 test loss: 2.359534
[Epoch 102; Iter    25/   55] train: loss: 0.1123745
[Epoch 102; Iter    55/   55] train: loss: 0.0055273
[Epoch 102] ogbg-molbbbp: 0.951509 val loss: 0.551007
[Epoch 102] ogbg-molbbbp: 0.635802 test loss: 2.356478
[Epoch 103; Iter    30/   55] train: loss: 0.0094640
[Epoch 103] ogbg-molbbbp: 0.948322 val loss: 0.507624
[Epoch 103] ogbg-molbbbp: 0.645158 test loss: 1.994379
[Epoch 104; Iter     5/   55] train: loss: 0.0745878
[Epoch 104; Iter    35/   55] train: loss: 0.0079931
[Epoch 104] ogbg-molbbbp: 0.950214 val loss: 0.486198
[Epoch 104] ogbg-molbbbp: 0.651427 test loss: 2.127552
[Epoch 105; Iter    10/   55] train: loss: 0.1552465
[Epoch 105; Iter    40/   55] train: loss: 0.0069122
[Epoch 105] ogbg-molbbbp: 0.941253 val loss: 0.642301
[Epoch 105] ogbg-molbbbp: 0.630787 test loss: 2.198387
[Epoch 106; Iter    15/   55] train: loss: 0.0189219
[Epoch 106; Iter    45/   55] train: loss: 0.0086732
[Epoch 106] ogbg-molbbbp: 0.944041 val loss: 0.633762
[Epoch 106] ogbg-molbbbp: 0.637731 test loss: 2.305326
[Epoch 107; Iter    20/   55] train: loss: 0.0102051
[Epoch 107; Iter    50/   55] train: loss: 0.0036788
[Epoch 107] ogbg-molbbbp: 0.953998 val loss: 0.434342
[Epoch 107] ogbg-molbbbp: 0.650752 test loss: 1.926317
[Epoch 108; Iter    25/   55] train: loss: 0.0012507
[Epoch 108; Iter    55/   55] train: loss: 0.0016351
[Epoch 108] ogbg-molbbbp: 0.947426 val loss: 0.608661
[Epoch 108] ogbg-molbbbp: 0.639275 test loss: 2.323313
[Epoch 109; Iter    30/   55] train: loss: 0.0209348
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.954894 val loss: 0.542405
[Epoch 109] ogbg-molbbbp: 0.680459 test loss: 2.112907
[Epoch 110; Iter     5/   55] train: loss: 0.0064640
[Epoch 110; Iter    35/   55] train: loss: 0.0027934
[Epoch 110] ogbg-molbbbp: 0.954695 val loss: 0.477511
[Epoch 110] ogbg-molbbbp: 0.680170 test loss: 2.030493
[Epoch 111; Iter    10/   55] train: loss: 0.0023918
[Epoch 111; Iter    40/   55] train: loss: 0.0031919
[Epoch 111] ogbg-molbbbp: 0.954297 val loss: 0.491210
[Epoch 111] ogbg-molbbbp: 0.687789 test loss: 2.107033
[Epoch 112; Iter    15/   55] train: loss: 0.0025080
[Epoch 112; Iter    45/   55] train: loss: 0.0038112
[Epoch 112] ogbg-molbbbp: 0.952504 val loss: 0.533774
[Epoch 112] ogbg-molbbbp: 0.690779 test loss: 2.292601
[Epoch 113; Iter    20/   55] train: loss: 0.0144800
[Epoch 113; Iter    50/   55] train: loss: 0.0025283
[Epoch 113] ogbg-molbbbp: 0.950513 val loss: 0.538382
[Epoch 113] ogbg-molbbbp: 0.684896 test loss: 2.141181
[Epoch 114; Iter    25/   55] train: loss: 0.0078027
[Epoch 114; Iter    55/   55] train: loss: 0.2984863
[Epoch 114] ogbg-molbbbp: 0.959375 val loss: 0.446482
[Epoch 114] ogbg-molbbbp: 0.691454 test loss: 2.078296
[Epoch 115; Iter    30/   55] train: loss: 0.0043800
[Epoch 115] ogbg-molbbbp: 0.954097 val loss: 0.453771
[Epoch 115] ogbg-molbbbp: 0.691165 test loss: 2.003499
[Epoch 116; Iter     5/   55] train: loss: 0.0015926
[Epoch 116; Iter    35/   55] train: loss: 0.0053413
[Epoch 116] ogbg-molbbbp: 0.947028 val loss: 0.567755
[Epoch 116] ogbg-molbbbp: 0.675733 test loss: 2.248397
[Epoch 117; Iter    10/   55] train: loss: 0.0321400
[Epoch 117; Iter    40/   55] train: loss: 0.0074114
[Epoch 117] ogbg-molbbbp: 0.953699 val loss: 0.506036
[Epoch 117] ogbg-molbbbp: 0.682292 test loss: 2.182896
[Epoch 118; Iter    15/   55] train: loss: 0.0026697
[Epoch 118; Iter    45/   55] train: loss: 0.0026539
[Epoch 118] ogbg-molbbbp: 0.951608 val loss: 0.574040
[Epoch 118] ogbg-molbbbp: 0.681520 test loss: 2.367482
[Epoch 119; Iter    20/   55] train: loss: 0.0847156
[Epoch 119; Iter    50/   55] train: loss: 0.0310283
[Epoch 119] ogbg-molbbbp: 0.951708 val loss: 0.520634
[Epoch 119] ogbg-molbbbp: 0.690490 test loss: 2.212993
[Epoch 120; Iter    25/   55] train: loss: 0.0018823
[Epoch 120; Iter    55/   55] train: loss: 0.0012214
[Epoch 120] ogbg-molbbbp: 0.954794 val loss: 0.479865
[Epoch 120] ogbg-molbbbp: 0.680266 test loss: 2.219759
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 52.
Statistics on  val_best_checkpoint
mean_pred: -0.7731286287307739
std_pred: 5.416553974151611
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.948327140452455
rocauc: 0.9722194563377476
ogbg-molbbbp: 0.9722194563377476
BCEWithLogitsLoss: 0.22333588238273347
Statistics on  test
mean_pred: 1.1432939767837524
std_pred: 4.270082950592041
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6757884428047443
rocauc: 0.6694637345679013
ogbg-molbbbp: 0.6694637345679013
BCEWithLogitsLoss: 1.188095475946154
Statistics on  train
mean_pred: 3.125520944595337
std_pred: 3.5434136390686035
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9979744764347671
rocauc: 0.9896271865015418
ogbg-molbbbp: 0.9896271865015418
BCEWithLogitsLoss: 0.12356924990361387
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.950612 val loss: 0.458649
[Epoch 109] ogbg-molbbbp: 0.590664 test loss: 2.276110
[Epoch 110; Iter     5/   55] train: loss: 0.0117051
[Epoch 110; Iter    35/   55] train: loss: 0.0601507
[Epoch 110] ogbg-molbbbp: 0.951011 val loss: 0.572203
[Epoch 110] ogbg-molbbbp: 0.661073 test loss: 2.121447
[Epoch 111; Iter    10/   55] train: loss: 0.0035314
[Epoch 111; Iter    40/   55] train: loss: 0.0015829
[Epoch 111] ogbg-molbbbp: 0.951807 val loss: 0.475161
[Epoch 111] ogbg-molbbbp: 0.648630 test loss: 2.063926
[Epoch 112; Iter    15/   55] train: loss: 0.0473916
[Epoch 112; Iter    45/   55] train: loss: 0.0019386
[Epoch 112] ogbg-molbbbp: 0.948123 val loss: 0.534620
[Epoch 112] ogbg-molbbbp: 0.643711 test loss: 2.140732
[Epoch 113; Iter    20/   55] train: loss: 0.0032004
[Epoch 113; Iter    50/   55] train: loss: 0.0621041
[Epoch 113] ogbg-molbbbp: 0.950214 val loss: 0.561995
[Epoch 113] ogbg-molbbbp: 0.631848 test loss: 2.370315
[Epoch 114; Iter    25/   55] train: loss: 0.0039374
[Epoch 114; Iter    55/   55] train: loss: 0.0017752
[Epoch 114] ogbg-molbbbp: 0.942248 val loss: 0.609024
[Epoch 114] ogbg-molbbbp: 0.652874 test loss: 2.065623
[Epoch 115; Iter    30/   55] train: loss: 0.0040273
[Epoch 115] ogbg-molbbbp: 0.951708 val loss: 0.476154
[Epoch 115] ogbg-molbbbp: 0.654032 test loss: 1.920951
[Epoch 116; Iter     5/   55] train: loss: 0.0024275
[Epoch 116; Iter    35/   55] train: loss: 0.0055425
[Epoch 116] ogbg-molbbbp: 0.950214 val loss: 0.511369
[Epoch 116] ogbg-molbbbp: 0.655382 test loss: 2.117874
[Epoch 117; Iter    10/   55] train: loss: 0.0193521
[Epoch 117; Iter    40/   55] train: loss: 0.0041569
[Epoch 117] ogbg-molbbbp: 0.952206 val loss: 0.510935
[Epoch 117] ogbg-molbbbp: 0.645544 test loss: 2.222705
[Epoch 118; Iter    15/   55] train: loss: 0.0910357
[Epoch 118; Iter    45/   55] train: loss: 0.0017761
[Epoch 118] ogbg-molbbbp: 0.952006 val loss: 0.534324
[Epoch 118] ogbg-molbbbp: 0.652874 test loss: 2.120046
[Epoch 119; Iter    20/   55] train: loss: 0.0033090
[Epoch 119; Iter    50/   55] train: loss: 0.0913206
[Epoch 119] ogbg-molbbbp: 0.950812 val loss: 0.486369
[Epoch 119] ogbg-molbbbp: 0.652199 test loss: 2.102361
[Epoch 120; Iter    25/   55] train: loss: 0.0796499
[Epoch 120; Iter    55/   55] train: loss: 0.1867776
[Epoch 120] ogbg-molbbbp: 0.957184 val loss: 0.489726
[Epoch 120] ogbg-molbbbp: 0.644965 test loss: 2.168533
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 32.
Statistics on  val_best_checkpoint
mean_pred: -0.451416552066803
std_pred: 4.535043716430664
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9502048173898439
rocauc: 0.9688340137409142
ogbg-molbbbp: 0.9688340137409142
BCEWithLogitsLoss: 0.24565320994172776
Statistics on  test
mean_pred: 1.0038633346557617
std_pred: 6.191842555999756
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6868821851248439
rocauc: 0.6865354938271605
ogbg-molbbbp: 0.6865354938271605
BCEWithLogitsLoss: 1.0349004098347254
Statistics on  train
mean_pred: 2.8244266510009766
std_pred: 2.5762248039245605
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9888852956744517
rocauc: 0.9483352756511412
ogbg-molbbbp: 0.9483352756511412
BCEWithLogitsLoss: 0.2143646881661632
[Epoch 149; Iter    20/   55] train: loss: 0.0002186
[Epoch 149; Iter    50/   55] train: loss: 0.0005723
[Epoch 149] ogbg-molbbbp: 0.956985 val loss: 0.514016
[Epoch 149] ogbg-molbbbp: 0.643036 test loss: 2.723541
[Epoch 150; Iter    25/   55] train: loss: 0.0001341
[Epoch 150; Iter    55/   55] train: loss: 0.0000366
[Epoch 150] ogbg-molbbbp: 0.954794 val loss: 0.487732
[Epoch 150] ogbg-molbbbp: 0.636960 test loss: 2.742017
[Epoch 151; Iter    30/   55] train: loss: 0.0003692
[Epoch 151] ogbg-molbbbp: 0.955392 val loss: 0.489799
[Epoch 151] ogbg-molbbbp: 0.636381 test loss: 2.716974
[Epoch 152; Iter     5/   55] train: loss: 0.0000443
[Epoch 152; Iter    35/   55] train: loss: 0.0000407
[Epoch 152] ogbg-molbbbp: 0.957782 val loss: 0.519074
[Epoch 152] ogbg-molbbbp: 0.638021 test loss: 2.801320
[Epoch 153; Iter    10/   55] train: loss: 0.0000647
[Epoch 153; Iter    40/   55] train: loss: 0.0000447
[Epoch 153] ogbg-molbbbp: 0.959275 val loss: 0.535587
[Epoch 153] ogbg-molbbbp: 0.632427 test loss: 2.780238
[Epoch 154; Iter    15/   55] train: loss: 0.0003460
[Epoch 154; Iter    45/   55] train: loss: 0.0001404
[Epoch 154] ogbg-molbbbp: 0.959375 val loss: 0.483959
[Epoch 154] ogbg-molbbbp: 0.631655 test loss: 2.774375
[Epoch 155; Iter    20/   55] train: loss: 0.0000317
[Epoch 155; Iter    50/   55] train: loss: 0.0006275
[Epoch 155] ogbg-molbbbp: 0.958678 val loss: 0.484047
[Epoch 155] ogbg-molbbbp: 0.633005 test loss: 2.790968
[Epoch 156; Iter    25/   55] train: loss: 0.0005367
[Epoch 156; Iter    55/   55] train: loss: 0.0003100
[Epoch 156] ogbg-molbbbp: 0.959375 val loss: 0.518927
[Epoch 156] ogbg-molbbbp: 0.631944 test loss: 2.825107
[Epoch 157; Iter    30/   55] train: loss: 0.0000425
[Epoch 157] ogbg-molbbbp: 0.958479 val loss: 0.497760
[Epoch 157] ogbg-molbbbp: 0.634259 test loss: 2.742620
[Epoch 158; Iter     5/   55] train: loss: 0.0000448
[Epoch 158; Iter    35/   55] train: loss: 0.0001968
[Epoch 158] ogbg-molbbbp: 0.957184 val loss: 0.494124
[Epoch 158] ogbg-molbbbp: 0.631655 test loss: 2.875414
[Epoch 159; Iter    10/   55] train: loss: 0.0000602
[Epoch 159; Iter    40/   55] train: loss: 0.0019683
[Epoch 159] ogbg-molbbbp: 0.949418 val loss: 0.703684
[Epoch 159] ogbg-molbbbp: 0.621624 test loss: 4.028808
[Epoch 160; Iter    15/   55] train: loss: 0.0000522
[Epoch 160; Iter    45/   55] train: loss: 0.0001327
[Epoch 160] ogbg-molbbbp: 0.950015 val loss: 0.573930
[Epoch 160] ogbg-molbbbp: 0.620467 test loss: 3.572523
[Epoch 161; Iter    20/   55] train: loss: 0.0000942
[Epoch 161; Iter    50/   55] train: loss: 0.0000583
[Epoch 161] ogbg-molbbbp: 0.949418 val loss: 0.570069
[Epoch 161] ogbg-molbbbp: 0.620997 test loss: 3.600805
[Epoch 162; Iter    25/   55] train: loss: 0.0007434
[Epoch 162; Iter    55/   55] train: loss: 0.0000547
[Epoch 162] ogbg-molbbbp: 0.950612 val loss: 0.717160
[Epoch 162] ogbg-molbbbp: 0.624228 test loss: 3.757911
[Epoch 163; Iter    30/   55] train: loss: 0.0000886
[Epoch 163] ogbg-molbbbp: 0.951210 val loss: 0.690790
[Epoch 163] ogbg-molbbbp: 0.626929 test loss: 3.641394
[Epoch 164; Iter     5/   55] train: loss: 0.0000665
[Epoch 164; Iter    35/   55] train: loss: 0.0002109
[Epoch 164] ogbg-molbbbp: 0.961167 val loss: 0.497359
[Epoch 164] ogbg-molbbbp: 0.635127 test loss: 3.128022
[Epoch 165; Iter    10/   55] train: loss: 0.0000739
[Epoch 165; Iter    40/   55] train: loss: 0.0001044
[Epoch 165] ogbg-molbbbp: 0.958976 val loss: 0.464020
[Epoch 165] ogbg-molbbbp: 0.632909 test loss: 3.128264
[Epoch 166; Iter    15/   55] train: loss: 0.0000563
[Epoch 166; Iter    45/   55] train: loss: 0.0004927
[Epoch 166] ogbg-molbbbp: 0.958976 val loss: 0.478873
[Epoch 166] ogbg-molbbbp: 0.631173 test loss: 3.290001
[Epoch 167; Iter    20/   55] train: loss: 0.0004322
[Epoch 167; Iter    50/   55] train: loss: 0.0000659
[Epoch 167] ogbg-molbbbp: 0.953002 val loss: 0.550554
[Epoch 167] ogbg-molbbbp: 0.631655 test loss: 3.542415
[Epoch 168; Iter    25/   55] train: loss: 0.0002561
[Epoch 168; Iter    55/   55] train: loss: 0.0089274
[Epoch 168] ogbg-molbbbp: 0.952903 val loss: 0.583614
[Epoch 168] ogbg-molbbbp: 0.633777 test loss: 3.450028
[Epoch 169; Iter    30/   55] train: loss: 0.0000227
[Epoch 169] ogbg-molbbbp: 0.952504 val loss: 0.537656
[Epoch 169] ogbg-molbbbp: 0.628569 test loss: 3.504172
[Epoch 170; Iter     5/   55] train: loss: 0.0004756
[Epoch 170; Iter    35/   55] train: loss: 0.0000599
[Epoch 170] ogbg-molbbbp: 0.958279 val loss: 0.473811
[Epoch 170] ogbg-molbbbp: 0.632330 test loss: 3.365214
[Epoch 171; Iter    10/   55] train: loss: 0.0032279
[Epoch 171; Iter    40/   55] train: loss: 0.0000832
[Epoch 171] ogbg-molbbbp: 0.956188 val loss: 0.498297
[Epoch 171] ogbg-molbbbp: 0.630594 test loss: 3.354758
[Epoch 172; Iter    15/   55] train: loss: 0.0000647
[Epoch 172; Iter    45/   55] train: loss: 0.0004914
[Epoch 172] ogbg-molbbbp: 0.958379 val loss: 0.491919
[Epoch 172] ogbg-molbbbp: 0.636092 test loss: 3.348970
[Epoch 173; Iter    20/   55] train: loss: 0.0000397
[Epoch 173; Iter    50/   55] train: loss: 0.0006441
[Epoch 173] ogbg-molbbbp: 0.956288 val loss: 0.484164
[Epoch 173] ogbg-molbbbp: 0.628858 test loss: 3.350739
[Epoch 174; Iter    25/   55] train: loss: 0.0002973
[Epoch 174; Iter    55/   55] train: loss: 0.0001701
[Epoch 174] ogbg-molbbbp: 0.959574 val loss: 0.503989
[Epoch 174] ogbg-molbbbp: 0.638021 test loss: 3.267518
[Epoch 175; Iter    30/   55] train: loss: 0.0000473
[Epoch 175] ogbg-molbbbp: 0.958877 val loss: 0.489284
[Epoch 175] ogbg-molbbbp: 0.633777 test loss: 3.286407
[Epoch 176; Iter     5/   55] train: loss: 0.0020804
[Epoch 176; Iter    35/   55] train: loss: 0.0000834
[Epoch 176] ogbg-molbbbp: 0.956388 val loss: 0.500944
[Epoch 176] ogbg-molbbbp: 0.631366 test loss: 3.421719
[Epoch 177; Iter    10/   55] train: loss: 0.0004083
[Epoch 177; Iter    40/   55] train: loss: 0.0000721
[Epoch 177] ogbg-molbbbp: 0.957782 val loss: 0.481568
[Epoch 177] ogbg-molbbbp: 0.632620 test loss: 3.219337
[Epoch 178; Iter    15/   55] train: loss: 0.0013362
[Epoch 178; Iter    45/   55] train: loss: 0.0001554
[Epoch 178] ogbg-molbbbp: 0.956686 val loss: 0.492985
[Epoch 178] ogbg-molbbbp: 0.635802 test loss: 3.199199
[Epoch 179; Iter    20/   55] train: loss: 0.0002411
[Epoch 179; Iter    50/   55] train: loss: 0.0002398
[Epoch 179] ogbg-molbbbp: 0.953500 val loss: 0.528945
[Epoch 179] ogbg-molbbbp: 0.631752 test loss: 3.385009
[Epoch 180; Iter    25/   55] train: loss: 0.0006308
[Epoch 180; Iter    55/   55] train: loss: 0.0001067
[Epoch 180] ogbg-molbbbp: 0.952703 val loss: 0.560299
[Epoch 180] ogbg-molbbbp: 0.633584 test loss: 3.502463
[Epoch 181; Iter    30/   55] train: loss: 0.0000952
[Epoch 181] ogbg-molbbbp: 0.959076 val loss: 0.499251
[Epoch 181] ogbg-molbbbp: 0.634549 test loss: 3.317789
[Epoch 182; Iter     5/   55] train: loss: 0.0000897
[Epoch 182; Iter    35/   55] train: loss: 0.0002423
[Epoch 182] ogbg-molbbbp: 0.954595 val loss: 0.517785
[Epoch 182] ogbg-molbbbp: 0.634452 test loss: 3.406368
[Epoch 183; Iter    10/   55] train: loss: 0.0001346
[Epoch 183; Iter    40/   55] train: loss: 0.0003838
[Epoch 183] ogbg-molbbbp: 0.952206 val loss: 0.547793
[Epoch 183] ogbg-molbbbp: 0.632812 test loss: 3.486957
[Epoch 184; Iter    15/   55] train: loss: 0.0000247
[Epoch 184; Iter    45/   55] train: loss: 0.0001406
[Epoch 184] ogbg-molbbbp: 0.955491 val loss: 0.519956
[Epoch 184] ogbg-molbbbp: 0.635513 test loss: 3.364389
[Epoch 185; Iter    20/   55] train: loss: 0.0001013
[Epoch 185; Iter    50/   55] train: loss: 0.0000507
[Epoch 185] ogbg-molbbbp: 0.950812 val loss: 0.610070
[Epoch 185] ogbg-molbbbp: 0.635706 test loss: 3.564167
[Epoch 186; Iter    25/   55] train: loss: 0.0000217
[Epoch 186; Iter    55/   55] train: loss: 0.0026529
[Epoch 186] ogbg-molbbbp: 0.951608 val loss: 0.654012
[Epoch 186] ogbg-molbbbp: 0.639178 test loss: 3.587117
[Epoch 187; Iter    30/   55] train: loss: 0.0001096
[Epoch 187] ogbg-molbbbp: 0.953898 val loss: 0.526619
[Epoch 187] ogbg-molbbbp: 0.635031 test loss: 3.333621
[Epoch 188; Iter     5/   55] train: loss: 0.0000255
[Epoch 188; Iter    35/   55] train: loss: 0.0000738
[Epoch 188] ogbg-molbbbp: 0.951608 val loss: 0.660620
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 149; Iter    20/   55] train: loss: 0.0001012
[Epoch 149; Iter    50/   55] train: loss: 0.0002416
[Epoch 149] ogbg-molbbbp: 0.951011 val loss: 0.599248
[Epoch 149] ogbg-molbbbp: 0.638600 test loss: 2.642247
[Epoch 150; Iter    25/   55] train: loss: 0.0014129
[Epoch 150; Iter    55/   55] train: loss: 0.0003554
[Epoch 150] ogbg-molbbbp: 0.954097 val loss: 0.580543
[Epoch 150] ogbg-molbbbp: 0.641975 test loss: 2.715833
[Epoch 151; Iter    30/   55] train: loss: 0.0000508
[Epoch 151] ogbg-molbbbp: 0.948820 val loss: 0.651992
[Epoch 151] ogbg-molbbbp: 0.637539 test loss: 2.898221
[Epoch 152; Iter     5/   55] train: loss: 0.0000698
[Epoch 152; Iter    35/   55] train: loss: 0.0000843
[Epoch 152] ogbg-molbbbp: 0.950812 val loss: 0.645863
[Epoch 152] ogbg-molbbbp: 0.638792 test loss: 2.867913
[Epoch 153; Iter    10/   55] train: loss: 0.0000450
[Epoch 153; Iter    40/   55] train: loss: 0.0001020
[Epoch 153] ogbg-molbbbp: 0.951509 val loss: 0.608850
[Epoch 153] ogbg-molbbbp: 0.641107 test loss: 2.777189
[Epoch 154; Iter    15/   55] train: loss: 0.0003596
[Epoch 154; Iter    45/   55] train: loss: 0.0002724
[Epoch 154] ogbg-molbbbp: 0.947824 val loss: 0.639316
[Epoch 154] ogbg-molbbbp: 0.641011 test loss: 2.749053
[Epoch 155; Iter    20/   55] train: loss: 0.0000997
[Epoch 155; Iter    50/   55] train: loss: 0.0016089
[Epoch 155] ogbg-molbbbp: 0.942547 val loss: 0.708414
[Epoch 155] ogbg-molbbbp: 0.640914 test loss: 2.877612
[Epoch 156; Iter    25/   55] train: loss: 0.0001518
[Epoch 156; Iter    55/   55] train: loss: 0.0000621
[Epoch 156] ogbg-molbbbp: 0.951409 val loss: 0.607002
[Epoch 156] ogbg-molbbbp: 0.642843 test loss: 2.781537
[Epoch 157; Iter    30/   55] train: loss: 0.0001424
[Epoch 157] ogbg-molbbbp: 0.952106 val loss: 0.621305
[Epoch 157] ogbg-molbbbp: 0.643422 test loss: 2.899148
[Epoch 158; Iter     5/   55] train: loss: 0.0000660
[Epoch 158; Iter    35/   55] train: loss: 0.0000325
[Epoch 158] ogbg-molbbbp: 0.953500 val loss: 0.601606
[Epoch 158] ogbg-molbbbp: 0.642265 test loss: 2.887830
[Epoch 159; Iter    10/   55] train: loss: 0.0003899
[Epoch 159; Iter    40/   55] train: loss: 0.0016667
[Epoch 159] ogbg-molbbbp: 0.951608 val loss: 0.617663
[Epoch 159] ogbg-molbbbp: 0.640529 test loss: 2.914410
[Epoch 160; Iter    15/   55] train: loss: 0.0000844
[Epoch 160; Iter    45/   55] train: loss: 0.0000493
[Epoch 160] ogbg-molbbbp: 0.953699 val loss: 0.598140
[Epoch 160] ogbg-molbbbp: 0.641879 test loss: 2.861863
[Epoch 161; Iter    20/   55] train: loss: 0.0015339
[Epoch 161; Iter    50/   55] train: loss: 0.0000620
[Epoch 161] ogbg-molbbbp: 0.950115 val loss: 0.649272
[Epoch 161] ogbg-molbbbp: 0.640432 test loss: 2.996825
[Epoch 162; Iter    25/   55] train: loss: 0.0002420
[Epoch 162; Iter    55/   55] train: loss: 0.0001479
[Epoch 162] ogbg-molbbbp: 0.951807 val loss: 0.614751
[Epoch 162] ogbg-molbbbp: 0.642747 test loss: 2.900413
[Epoch 163; Iter    30/   55] train: loss: 0.0001422
[Epoch 163] ogbg-molbbbp: 0.952604 val loss: 0.622533
[Epoch 163] ogbg-molbbbp: 0.642361 test loss: 2.944488
[Epoch 164; Iter     5/   55] train: loss: 0.0002202
[Epoch 164; Iter    35/   55] train: loss: 0.0000676
[Epoch 164] ogbg-molbbbp: 0.953400 val loss: 0.607473
[Epoch 164] ogbg-molbbbp: 0.645351 test loss: 2.980407
[Epoch 165; Iter    10/   55] train: loss: 0.0000903
[Epoch 165; Iter    40/   55] train: loss: 0.0001119
[Epoch 165] ogbg-molbbbp: 0.952206 val loss: 0.614464
[Epoch 165] ogbg-molbbbp: 0.644194 test loss: 2.961996
[Epoch 166; Iter    15/   55] train: loss: 0.0001303
[Epoch 166; Iter    45/   55] train: loss: 0.0003801
[Epoch 166] ogbg-molbbbp: 0.953600 val loss: 0.609855
[Epoch 166] ogbg-molbbbp: 0.643326 test loss: 2.970582
[Epoch 167; Iter    20/   55] train: loss: 0.0000397
[Epoch 167; Iter    50/   55] train: loss: 0.0000774
[Epoch 167] ogbg-molbbbp: 0.952803 val loss: 0.617720
[Epoch 167] ogbg-molbbbp: 0.642940 test loss: 2.956473
[Epoch 168; Iter    25/   55] train: loss: 0.0000411
[Epoch 168; Iter    55/   55] train: loss: 0.0004321
[Epoch 168] ogbg-molbbbp: 0.953898 val loss: 0.601599
[Epoch 168] ogbg-molbbbp: 0.644869 test loss: 2.895655
[Epoch 169; Iter    30/   55] train: loss: 0.0008636
[Epoch 169] ogbg-molbbbp: 0.952206 val loss: 0.638074
[Epoch 169] ogbg-molbbbp: 0.644097 test loss: 3.069699
[Epoch 170; Iter     5/   55] train: loss: 0.0000508
[Epoch 170; Iter    35/   55] train: loss: 0.0001830
[Epoch 170] ogbg-molbbbp: 0.953600 val loss: 0.609904
[Epoch 170] ogbg-molbbbp: 0.645930 test loss: 2.930635
[Epoch 171; Iter    10/   55] train: loss: 0.0003311
[Epoch 171; Iter    40/   55] train: loss: 0.0004622
[Epoch 171] ogbg-molbbbp: 0.947526 val loss: 0.691967
[Epoch 171] ogbg-molbbbp: 0.642072 test loss: 3.142625
[Epoch 172; Iter    15/   55] train: loss: 0.0013330
[Epoch 172; Iter    45/   55] train: loss: 0.0000405
[Epoch 172] ogbg-molbbbp: 0.950812 val loss: 0.628456
[Epoch 172] ogbg-molbbbp: 0.646701 test loss: 2.922896
[Epoch 173; Iter    20/   55] train: loss: 0.0001697
[Epoch 173; Iter    50/   55] train: loss: 0.0001482
[Epoch 173] ogbg-molbbbp: 0.952305 val loss: 0.622693
[Epoch 173] ogbg-molbbbp: 0.646026 test loss: 2.976521
[Epoch 174; Iter    25/   55] train: loss: 0.0000334
[Epoch 174; Iter    55/   55] train: loss: 0.0000147
[Epoch 174] ogbg-molbbbp: 0.953400 val loss: 0.606791
[Epoch 174] ogbg-molbbbp: 0.649595 test loss: 2.889624
[Epoch 175; Iter    30/   55] train: loss: 0.0000700
[Epoch 175] ogbg-molbbbp: 0.949915 val loss: 0.652535
[Epoch 175] ogbg-molbbbp: 0.648534 test loss: 3.028066
[Epoch 176; Iter     5/   55] train: loss: 0.0000413
[Epoch 176; Iter    35/   55] train: loss: 0.0003175
[Epoch 176] ogbg-molbbbp: 0.955193 val loss: 0.631958
[Epoch 176] ogbg-molbbbp: 0.647280 test loss: 2.991926
[Epoch 177; Iter    10/   55] train: loss: 0.0003751
[Epoch 177; Iter    40/   55] train: loss: 0.0003400
[Epoch 177] ogbg-molbbbp: 0.953799 val loss: 0.547330
[Epoch 177] ogbg-molbbbp: 0.642650 test loss: 2.782832
[Epoch 178; Iter    15/   55] train: loss: 0.0000996
[Epoch 178; Iter    45/   55] train: loss: 0.0001138
[Epoch 178] ogbg-molbbbp: 0.955093 val loss: 0.541970
[Epoch 178] ogbg-molbbbp: 0.642843 test loss: 2.785974
[Epoch 179; Iter    20/   55] train: loss: 0.0004307
[Epoch 179; Iter    50/   55] train: loss: 0.0000478
[Epoch 179] ogbg-molbbbp: 0.957582 val loss: 0.527731
[Epoch 179] ogbg-molbbbp: 0.644676 test loss: 2.824642
[Epoch 180; Iter    25/   55] train: loss: 0.0001233
[Epoch 180; Iter    55/   55] train: loss: 0.0000456
[Epoch 180] ogbg-molbbbp: 0.953998 val loss: 0.560764
[Epoch 180] ogbg-molbbbp: 0.644579 test loss: 2.856069
[Epoch 181; Iter    30/   55] train: loss: 0.0000851
[Epoch 181] ogbg-molbbbp: 0.957383 val loss: 0.540330
[Epoch 181] ogbg-molbbbp: 0.646026 test loss: 2.792652
[Epoch 182; Iter     5/   55] train: loss: 0.0000649
[Epoch 182; Iter    35/   55] train: loss: 0.0005401
[Epoch 182] ogbg-molbbbp: 0.956786 val loss: 0.543530
[Epoch 182] ogbg-molbbbp: 0.646316 test loss: 2.763679
[Epoch 183; Iter    10/   55] train: loss: 0.0001782
[Epoch 183; Iter    40/   55] train: loss: 0.0002726
[Epoch 183] ogbg-molbbbp: 0.951708 val loss: 0.604921
[Epoch 183] ogbg-molbbbp: 0.641590 test loss: 2.885418
[Epoch 184; Iter    15/   55] train: loss: 0.0001319
[Epoch 184; Iter    45/   55] train: loss: 0.0001297
[Epoch 184] ogbg-molbbbp: 0.953500 val loss: 0.562141
[Epoch 184] ogbg-molbbbp: 0.645062 test loss: 2.777558
[Epoch 185; Iter    20/   55] train: loss: 0.0001273
[Epoch 185; Iter    50/   55] train: loss: 0.0000393
[Epoch 185] ogbg-molbbbp: 0.952405 val loss: 0.605570
[Epoch 185] ogbg-molbbbp: 0.643326 test loss: 2.940307
[Epoch 186; Iter    25/   55] train: loss: 0.0001837
[Epoch 186; Iter    55/   55] train: loss: 0.0002170
[Epoch 186] ogbg-molbbbp: 0.953600 val loss: 0.559383
[Epoch 186] ogbg-molbbbp: 0.644194 test loss: 2.768829
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 186 epochs. Best model checkpoint was in epoch 126.
Statistics on  val_best_checkpoint
mean_pred: -1.9929347038269043
std_pred: 10.738235473632812
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9413630645088935
rocauc: 0.9610674101364134
ogbg-molbbbp: 0.9610674101364134
BCEWithLogitsLoss: 0.4994525713596626
Statistics on  test
mean_pred: 3.5561885833740234
std_pred: 8.162610054016113
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6582215621447415
rocauc: 0.6408179012345679
ogbg-molbbbp: 0.6408179012345679
BCEWithLogitsLoss: 2.843460406575884
Statistics on  train
mean_pred: 7.866189479827881
std_pred: 8.337235450744629
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.00013345621767009354
[Epoch 109] ogbg-molbbbp: 0.952903 val loss: 0.546740
[Epoch 109] ogbg-molbbbp: 0.660301 test loss: 2.220105
[Epoch 110; Iter     5/   55] train: loss: 0.0095246
[Epoch 110; Iter    35/   55] train: loss: 0.0047425
[Epoch 110] ogbg-molbbbp: 0.953898 val loss: 0.480332
[Epoch 110] ogbg-molbbbp: 0.661169 test loss: 2.033178
[Epoch 111; Iter    10/   55] train: loss: 0.0044021
[Epoch 111; Iter    40/   55] train: loss: 0.0012641
[Epoch 111] ogbg-molbbbp: 0.949617 val loss: 0.522873
[Epoch 111] ogbg-molbbbp: 0.651235 test loss: 2.120814
[Epoch 112; Iter    15/   55] train: loss: 0.0112908
[Epoch 112; Iter    45/   55] train: loss: 0.0044072
[Epoch 112] ogbg-molbbbp: 0.947028 val loss: 0.610285
[Epoch 112] ogbg-molbbbp: 0.654610 test loss: 2.309303
[Epoch 113; Iter    20/   55] train: loss: 0.0105983
[Epoch 113; Iter    50/   55] train: loss: 0.0222637
[Epoch 113] ogbg-molbbbp: 0.951110 val loss: 0.483257
[Epoch 113] ogbg-molbbbp: 0.652103 test loss: 2.019717
[Epoch 114; Iter    25/   55] train: loss: 0.0100835
[Epoch 114; Iter    55/   55] train: loss: 0.0052894
[Epoch 114] ogbg-molbbbp: 0.946829 val loss: 0.573021
[Epoch 114] ogbg-molbbbp: 0.645930 test loss: 2.230175
[Epoch 115; Iter    30/   55] train: loss: 0.0150169
[Epoch 115] ogbg-molbbbp: 0.949716 val loss: 0.546557
[Epoch 115] ogbg-molbbbp: 0.651717 test loss: 2.236024
[Epoch 116; Iter     5/   55] train: loss: 0.0137681
[Epoch 116; Iter    35/   55] train: loss: 0.0404490
[Epoch 116] ogbg-molbbbp: 0.942547 val loss: 0.633204
[Epoch 116] ogbg-molbbbp: 0.634259 test loss: 2.312287
[Epoch 117; Iter    10/   55] train: loss: 0.0626184
[Epoch 117; Iter    40/   55] train: loss: 0.0039186
[Epoch 117] ogbg-molbbbp: 0.957682 val loss: 0.511273
[Epoch 117] ogbg-molbbbp: 0.676312 test loss: 2.164862
[Epoch 118; Iter    15/   55] train: loss: 0.0038276
[Epoch 118; Iter    45/   55] train: loss: 0.0048329
[Epoch 118] ogbg-molbbbp: 0.955890 val loss: 0.473685
[Epoch 118] ogbg-molbbbp: 0.661458 test loss: 2.111975
[Epoch 119; Iter    20/   55] train: loss: 0.0915988
[Epoch 119; Iter    50/   55] train: loss: 0.0039791
[Epoch 119] ogbg-molbbbp: 0.950612 val loss: 0.609734
[Epoch 119] ogbg-molbbbp: 0.653356 test loss: 2.435317
[Epoch 120; Iter    25/   55] train: loss: 0.0595958
[Epoch 120; Iter    55/   55] train: loss: 0.0031136
[Epoch 120] ogbg-molbbbp: 0.950812 val loss: 0.550129
[Epoch 120] ogbg-molbbbp: 0.668692 test loss: 2.167000
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 38.
Statistics on  val_best_checkpoint
mean_pred: -1.9303609132766724
std_pred: 5.049192905426025
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9533436654019398
rocauc: 0.9719207408144978
ogbg-molbbbp: 0.9719207408144978
BCEWithLogitsLoss: 0.24385982086615904
Statistics on  test
mean_pred: 0.33478105068206787
std_pred: 3.8050708770751953
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.7529435537633864
rocauc: 0.7267554012345679
ogbg-molbbbp: 0.7267554012345679
BCEWithLogitsLoss: 0.9667250939777919
Statistics on  train
mean_pred: 2.0023996829986572
std_pred: 3.536742925643921
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.993463428917915
rocauc: 0.9708791729629361
ogbg-molbbbp: 0.9708791729629361
BCEWithLogitsLoss: 0.21431659216230567
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.0.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.0.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.0.yml --seed 6 --device cuda:0
All runs completed.
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.1.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.1.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.1.yml --seed 6 --device cuda:2
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 188] ogbg-molbbbp: 0.640625 test loss: 3.600088
[Epoch 189; Iter    10/   55] train: loss: 0.0001395
[Epoch 189; Iter    40/   55] train: loss: 0.0000366
[Epoch 189] ogbg-molbbbp: 0.950712 val loss: 0.585051
[Epoch 189] ogbg-molbbbp: 0.635610 test loss: 3.524795
[Epoch 190; Iter    15/   55] train: loss: 0.0001264
[Epoch 190; Iter    45/   55] train: loss: 0.0017832
[Epoch 190] ogbg-molbbbp: 0.950413 val loss: 0.621973
[Epoch 190] ogbg-molbbbp: 0.634356 test loss: 3.581524
[Epoch 191; Iter    20/   55] train: loss: 0.0002188
[Epoch 191; Iter    50/   55] train: loss: 0.0003896
[Epoch 191] ogbg-molbbbp: 0.952305 val loss: 0.538565
[Epoch 191] ogbg-molbbbp: 0.634838 test loss: 3.401384
[Epoch 192; Iter    25/   55] train: loss: 0.0006738
[Epoch 192; Iter    55/   55] train: loss: 0.0000130
[Epoch 192] ogbg-molbbbp: 0.951509 val loss: 0.542749
[Epoch 192] ogbg-molbbbp: 0.636960 test loss: 3.344257
[Epoch 193; Iter    30/   55] train: loss: 0.0000456
[Epoch 193] ogbg-molbbbp: 0.951608 val loss: 0.627683
[Epoch 193] ogbg-molbbbp: 0.636671 test loss: 3.562728
[Epoch 194; Iter     5/   55] train: loss: 0.0000403
[Epoch 194; Iter    35/   55] train: loss: 0.0011413
[Epoch 194] ogbg-molbbbp: 0.953799 val loss: 0.535816
[Epoch 194] ogbg-molbbbp: 0.633681 test loss: 3.496374
[Epoch 195; Iter    10/   55] train: loss: 0.0000519
[Epoch 195; Iter    40/   55] train: loss: 0.0000285
[Epoch 195] ogbg-molbbbp: 0.951807 val loss: 0.559732
[Epoch 195] ogbg-molbbbp: 0.637924 test loss: 3.480599
[Epoch 196; Iter    15/   55] train: loss: 0.0003217
[Epoch 196; Iter    45/   55] train: loss: 0.0012424
[Epoch 196] ogbg-molbbbp: 0.950911 val loss: 0.600025
[Epoch 196] ogbg-molbbbp: 0.634742 test loss: 3.541036
[Epoch 197; Iter    20/   55] train: loss: 0.0000120
[Epoch 197; Iter    50/   55] train: loss: 0.0000307
[Epoch 197] ogbg-molbbbp: 0.950812 val loss: 0.590766
[Epoch 197] ogbg-molbbbp: 0.634356 test loss: 3.419478
[Epoch 198; Iter    25/   55] train: loss: 0.0000375
[Epoch 198; Iter    55/   55] train: loss: 0.0000518
[Epoch 198] ogbg-molbbbp: 0.952106 val loss: 0.545437
[Epoch 198] ogbg-molbbbp: 0.636863 test loss: 3.430377
[Epoch 199; Iter    30/   55] train: loss: 0.0001231
[Epoch 199] ogbg-molbbbp: 0.951807 val loss: 0.587368
[Epoch 199] ogbg-molbbbp: 0.636960 test loss: 3.548836
[Epoch 200; Iter     5/   55] train: loss: 0.0001523
[Epoch 200; Iter    35/   55] train: loss: 0.0000719
[Epoch 200] ogbg-molbbbp: 0.956686 val loss: 0.525971
[Epoch 200] ogbg-molbbbp: 0.633584 test loss: 3.395905
[Epoch 201; Iter    10/   55] train: loss: 0.0002301
[Epoch 201; Iter    40/   55] train: loss: 0.0001860
[Epoch 201] ogbg-molbbbp: 0.953998 val loss: 0.559486
[Epoch 201] ogbg-molbbbp: 0.635802 test loss: 3.413061
[Epoch 202; Iter    15/   55] train: loss: 0.0000590
[Epoch 202; Iter    45/   55] train: loss: 0.0000256
[Epoch 202] ogbg-molbbbp: 0.955093 val loss: 0.548220
[Epoch 202] ogbg-molbbbp: 0.635706 test loss: 3.384603
[Epoch 203; Iter    20/   55] train: loss: 0.0001298
[Epoch 203; Iter    50/   55] train: loss: 0.0007457
[Epoch 203] ogbg-molbbbp: 0.955491 val loss: 0.554270
[Epoch 203] ogbg-molbbbp: 0.636960 test loss: 3.357490
[Epoch 204; Iter    25/   55] train: loss: 0.0000334
[Epoch 204; Iter    55/   55] train: loss: 0.0000564
[Epoch 204] ogbg-molbbbp: 0.952703 val loss: 0.603284
[Epoch 204] ogbg-molbbbp: 0.635610 test loss: 3.556876
[Epoch 205; Iter    30/   55] train: loss: 0.0002145
[Epoch 205] ogbg-molbbbp: 0.956388 val loss: 0.514510
[Epoch 205] ogbg-molbbbp: 0.630980 test loss: 3.372651
[Epoch 206; Iter     5/   55] train: loss: 0.0000599
[Epoch 206; Iter    35/   55] train: loss: 0.0000923
[Epoch 206] ogbg-molbbbp: 0.957981 val loss: 0.506588
[Epoch 206] ogbg-molbbbp: 0.632909 test loss: 3.312075
[Epoch 207; Iter    10/   55] train: loss: 0.0000777
[Epoch 207; Iter    40/   55] train: loss: 0.0001275
[Epoch 207] ogbg-molbbbp: 0.956885 val loss: 0.529319
[Epoch 207] ogbg-molbbbp: 0.634259 test loss: 3.409573
[Epoch 208; Iter    15/   55] train: loss: 0.0000353
[Epoch 208; Iter    45/   55] train: loss: 0.0000230
[Epoch 208] ogbg-molbbbp: 0.954994 val loss: 0.543664
[Epoch 208] ogbg-molbbbp: 0.633391 test loss: 3.422332
[Epoch 209; Iter    20/   55] train: loss: 0.0000813
[Epoch 209; Iter    50/   55] train: loss: 0.0004564
[Epoch 209] ogbg-molbbbp: 0.955491 val loss: 0.541746
[Epoch 209] ogbg-molbbbp: 0.632234 test loss: 3.333792
[Epoch 210; Iter    25/   55] train: loss: 0.0002601
[Epoch 210; Iter    55/   55] train: loss: 0.0003149
[Epoch 210] ogbg-molbbbp: 0.956089 val loss: 0.515230
[Epoch 210] ogbg-molbbbp: 0.634934 test loss: 3.345978
[Epoch 211; Iter    30/   55] train: loss: 0.0000693
[Epoch 211] ogbg-molbbbp: 0.956686 val loss: 0.537777
[Epoch 211] ogbg-molbbbp: 0.634742 test loss: 3.440103
[Epoch 212; Iter     5/   55] train: loss: 0.0002874
[Epoch 212; Iter    35/   55] train: loss: 0.0000169
[Epoch 212] ogbg-molbbbp: 0.956587 val loss: 0.518774
[Epoch 212] ogbg-molbbbp: 0.633584 test loss: 3.364667
[Epoch 213; Iter    10/   55] train: loss: 0.0007807
[Epoch 213; Iter    40/   55] train: loss: 0.0000784
[Epoch 213] ogbg-molbbbp: 0.957782 val loss: 0.497738
[Epoch 213] ogbg-molbbbp: 0.638407 test loss: 3.158617
[Epoch 214; Iter    15/   55] train: loss: 0.0001653
[Epoch 214; Iter    45/   55] train: loss: 0.0000426
[Epoch 214] ogbg-molbbbp: 0.958379 val loss: 0.493508
[Epoch 214] ogbg-molbbbp: 0.634838 test loss: 3.340881
[Epoch 215; Iter    20/   55] train: loss: 0.0000284
[Epoch 215; Iter    50/   55] train: loss: 0.0005166
[Epoch 215] ogbg-molbbbp: 0.960171 val loss: 0.501960
[Epoch 215] ogbg-molbbbp: 0.638214 test loss: 3.276816
[Epoch 216; Iter    25/   55] train: loss: 0.0021726
[Epoch 216; Iter    55/   55] train: loss: 0.0000147
[Epoch 216] ogbg-molbbbp: 0.958479 val loss: 0.489448
[Epoch 216] ogbg-molbbbp: 0.636478 test loss: 3.222580
[Epoch 217; Iter    30/   55] train: loss: 0.0011953
[Epoch 217] ogbg-molbbbp: 0.959873 val loss: 0.506787
[Epoch 217] ogbg-molbbbp: 0.638117 test loss: 3.400847
[Epoch 218; Iter     5/   55] train: loss: 0.0000256
[Epoch 218; Iter    35/   55] train: loss: 0.0000202
[Epoch 218] ogbg-molbbbp: 0.959773 val loss: 0.496189
[Epoch 218] ogbg-molbbbp: 0.637731 test loss: 3.414791
[Epoch 219; Iter    10/   55] train: loss: 0.0000463
[Epoch 219; Iter    40/   55] train: loss: 0.0006934
[Epoch 219] ogbg-molbbbp: 0.958180 val loss: 0.492385
[Epoch 219] ogbg-molbbbp: 0.637539 test loss: 3.218777
[Epoch 220; Iter    15/   55] train: loss: 0.0004316
[Epoch 220; Iter    45/   55] train: loss: 0.0001370
[Epoch 220] ogbg-molbbbp: 0.959673 val loss: 0.517922
[Epoch 220] ogbg-molbbbp: 0.639178 test loss: 3.428649
[Epoch 221; Iter    20/   55] train: loss: 0.0000389
[Epoch 221; Iter    50/   55] train: loss: 0.0004269
[Epoch 221] ogbg-molbbbp: 0.959972 val loss: 0.515243
[Epoch 221] ogbg-molbbbp: 0.639757 test loss: 3.315840
[Epoch 222; Iter    25/   55] train: loss: 0.0000176
[Epoch 222; Iter    55/   55] train: loss: 0.0002205
[Epoch 222] ogbg-molbbbp: 0.958777 val loss: 0.505250
[Epoch 222] ogbg-molbbbp: 0.636863 test loss: 3.353075
[Epoch 223; Iter    30/   55] train: loss: 0.0001588
[Epoch 223] ogbg-molbbbp: 0.958678 val loss: 0.506625
[Epoch 223] ogbg-molbbbp: 0.637442 test loss: 3.370313
[Epoch 224; Iter     5/   55] train: loss: 0.0002203
[Epoch 224; Iter    35/   55] train: loss: 0.0000301
[Epoch 224] ogbg-molbbbp: 0.959176 val loss: 0.522852
[Epoch 224] ogbg-molbbbp: 0.636285 test loss: 3.324215
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 224 epochs. Best model checkpoint was in epoch 164.
Statistics on  val_best_checkpoint
mean_pred: -14.447864532470703
std_pred: 75.62643432617188
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9311410186494242
rocauc: 0.9611669819774967
ogbg-molbbbp: 0.9611669819774967
BCEWithLogitsLoss: 0.4973594683979172
Statistics on  test
mean_pred: -4.918010234832764
std_pred: 45.74257278442383
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6562186482406083
rocauc: 0.6351273148148148
ogbg-molbbbp: 0.6351273148148148
BCEWithLogitsLoss: 3.128022185393742
Statistics on  train
mean_pred: 7.356963157653809
std_pred: 8.52157974243164
mean_targets: 0.839362382888794
std_targets: 0.36730900406837463
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.0001299780715436844
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.2.yml --seed 4 --device cuda:3
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.2.yml --seed 5 --device cuda:3
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bbbp/noise=0.2.yml --seed 6 --device cuda:3
All runs completed.
