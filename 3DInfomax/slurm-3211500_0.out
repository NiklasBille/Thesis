>>> Starting run for dataset: bace
Running SCAFF configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.7.yml --seed 6 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.8.yml --seed 6 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.8/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.8_4_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.8
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6927290
[Epoch 1] ogbg-molbace: 0.430769 val loss: 0.693569
[Epoch 1] ogbg-molbace: 0.397148 test loss: 0.693230
[Epoch 2; Iter    19/   41] train: loss: 0.6952031
[Epoch 2] ogbg-molbace: 0.429670 val loss: 0.694659
[Epoch 2] ogbg-molbace: 0.439402 test loss: 0.693252
[Epoch 3; Iter     8/   41] train: loss: 0.6980300
[Epoch 3; Iter    38/   41] train: loss: 0.6943874
[Epoch 3] ogbg-molbace: 0.427473 val loss: 0.695573
[Epoch 3] ogbg-molbace: 0.449139 test loss: 0.693333
[Epoch 4; Iter    27/   41] train: loss: 0.6930844
[Epoch 4] ogbg-molbace: 0.435897 val loss: 0.695909
[Epoch 4] ogbg-molbace: 0.453312 test loss: 0.693467
[Epoch 5; Iter    16/   41] train: loss: 0.6954031
[Epoch 5] ogbg-molbace: 0.431136 val loss: 0.696324
[Epoch 5] ogbg-molbace: 0.449139 test loss: 0.693693
[Epoch 6; Iter     5/   41] train: loss: 0.6925275
[Epoch 6; Iter    35/   41] train: loss: 0.6933805
[Epoch 6] ogbg-molbace: 0.429670 val loss: 0.696342
[Epoch 6] ogbg-molbace: 0.461137 test loss: 0.693728
[Epoch 7; Iter    24/   41] train: loss: 0.6966487
[Epoch 7] ogbg-molbace: 0.436630 val loss: 0.696925
[Epoch 7] ogbg-molbace: 0.449313 test loss: 0.693999
[Epoch 8; Iter    13/   41] train: loss: 0.6946664
[Epoch 8] ogbg-molbace: 0.443223 val loss: 0.697179
[Epoch 8] ogbg-molbace: 0.475222 test loss: 0.693782
[Epoch 9; Iter     2/   41] train: loss: 0.6927829
[Epoch 9; Iter    32/   41] train: loss: 0.6897456
[Epoch 9] ogbg-molbace: 0.432234 val loss: 0.697993
[Epoch 9] ogbg-molbace: 0.456442 test loss: 0.694640
[Epoch 10; Iter    21/   41] train: loss: 0.6913434
[Epoch 10] ogbg-molbace: 0.435531 val loss: 0.698198
[Epoch 10] ogbg-molbace: 0.465310 test loss: 0.694824
[Epoch 11; Iter    10/   41] train: loss: 0.6940972
[Epoch 11; Iter    40/   41] train: loss: 0.6894860
[Epoch 11] ogbg-molbace: 0.439560 val loss: 0.698878
[Epoch 11] ogbg-molbace: 0.476091 test loss: 0.694951
[Epoch 12; Iter    29/   41] train: loss: 0.6934273
[Epoch 12] ogbg-molbace: 0.438462 val loss: 0.699721
[Epoch 12] ogbg-molbace: 0.478352 test loss: 0.695537
[Epoch 13; Iter    18/   41] train: loss: 0.6940166
[Epoch 13] ogbg-molbace: 0.440293 val loss: 0.700601
[Epoch 13] ogbg-molbace: 0.481829 test loss: 0.695951
[Epoch 14; Iter     7/   41] train: loss: 0.6940421
[Epoch 14; Iter    37/   41] train: loss: 0.6867384
[Epoch 14] ogbg-molbace: 0.428571 val loss: 0.701695
[Epoch 14] ogbg-molbace: 0.479743 test loss: 0.696597
[Epoch 15; Iter    26/   41] train: loss: 0.6881240
[Epoch 15] ogbg-molbace: 0.427473 val loss: 0.702663
[Epoch 15] ogbg-molbace: 0.484785 test loss: 0.696974
[Epoch 16; Iter    15/   41] train: loss: 0.6942689
[Epoch 16] ogbg-molbace: 0.432601 val loss: 0.703207
[Epoch 16] ogbg-molbace: 0.495740 test loss: 0.697538
[Epoch 17; Iter     4/   41] train: loss: 0.6927571
[Epoch 17; Iter    34/   41] train: loss: 0.6883259
[Epoch 17] ogbg-molbace: 0.528205 val loss: 0.696972
[Epoch 17] ogbg-molbace: 0.611720 test loss: 0.696209
[Epoch 18; Iter    23/   41] train: loss: 0.6474306
[Epoch 18] ogbg-molbace: 0.680586 val loss: 0.679861
[Epoch 18] ogbg-molbace: 0.743001 test loss: 0.680993
[Epoch 19; Iter    12/   41] train: loss: 0.6175016
[Epoch 19] ogbg-molbace: 0.710623 val loss: 0.657612
[Epoch 19] ogbg-molbace: 0.749609 test loss: 0.685847
[Epoch 20; Iter     1/   41] train: loss: 0.5724236
[Epoch 20; Iter    31/   41] train: loss: 0.5149022
[Epoch 20] ogbg-molbace: 0.706593 val loss: 0.714616
[Epoch 20] ogbg-molbace: 0.810642 test loss: 0.665564
[Epoch 21; Iter    20/   41] train: loss: 0.4378804
[Epoch 21] ogbg-molbace: 0.594505 val loss: 0.681031
[Epoch 21] ogbg-molbace: 0.725613 test loss: 0.689480
[Epoch 22; Iter     9/   41] train: loss: 0.5697750
[Epoch 22; Iter    39/   41] train: loss: 0.3513356
[Epoch 22] ogbg-molbace: 0.709158 val loss: 0.625033
[Epoch 22] ogbg-molbace: 0.736568 test loss: 0.837734
[Epoch 23; Iter    28/   41] train: loss: 0.4636605
[Epoch 23] ogbg-molbace: 0.720513 val loss: 0.675856
[Epoch 23] ogbg-molbace: 0.806642 test loss: 0.577301
[Epoch 24; Iter    17/   41] train: loss: 0.5607677
[Epoch 24] ogbg-molbace: 0.744689 val loss: 0.908949
[Epoch 24] ogbg-molbace: 0.785776 test loss: 0.846017
[Epoch 25; Iter     6/   41] train: loss: 0.4998991
[Epoch 25; Iter    36/   41] train: loss: 0.4350850
[Epoch 25] ogbg-molbace: 0.710256 val loss: 0.839390
[Epoch 25] ogbg-molbace: 0.759520 test loss: 1.008497
[Epoch 26; Iter    25/   41] train: loss: 0.3897315
[Epoch 26] ogbg-molbace: 0.700733 val loss: 0.929364
[Epoch 26] ogbg-molbace: 0.800035 test loss: 0.858343
[Epoch 27; Iter    14/   41] train: loss: 0.3644746
[Epoch 27] ogbg-molbace: 0.643223 val loss: 0.938710
[Epoch 27] ogbg-molbace: 0.778821 test loss: 0.784683
[Epoch 28; Iter     3/   41] train: loss: 0.4022822
[Epoch 28; Iter    33/   41] train: loss: 0.4915125
[Epoch 28] ogbg-molbace: 0.737729 val loss: 0.679616
[Epoch 28] ogbg-molbace: 0.777952 test loss: 0.848452
[Epoch 29; Iter    22/   41] train: loss: 0.6041244
[Epoch 29] ogbg-molbace: 0.673993 val loss: 0.902325
[Epoch 29] ogbg-molbace: 0.780908 test loss: 0.990926
[Epoch 30; Iter    11/   41] train: loss: 0.3676251
[Epoch 30; Iter    41/   41] train: loss: 0.2871022
[Epoch 30] ogbg-molbace: 0.663370 val loss: 0.860227
[Epoch 30] ogbg-molbace: 0.780212 test loss: 0.856353
[Epoch 31; Iter    30/   41] train: loss: 0.3972546
[Epoch 31] ogbg-molbace: 0.665201 val loss: 0.890847
[Epoch 31] ogbg-molbace: 0.773605 test loss: 0.887777
[Epoch 32; Iter    19/   41] train: loss: 0.3453067
[Epoch 32] ogbg-molbace: 0.729304 val loss: 0.879522
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.7/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.7_4_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.7
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6957516
[Epoch 1] ogbg-molbace: 0.465116 val loss: 0.693456
[Epoch 1] ogbg-molbace: 0.428765 test loss: 0.693117
[Epoch 2; Iter    24/   36] train: loss: 0.6940999
[Epoch 2] ogbg-molbace: 0.468393 val loss: 0.693771
[Epoch 2] ogbg-molbace: 0.440683 test loss: 0.693520
[Epoch 3; Iter    18/   36] train: loss: 0.6929053
[Epoch 3] ogbg-molbace: 0.471882 val loss: 0.694390
[Epoch 3] ogbg-molbace: 0.468220 test loss: 0.692999
[Epoch 4; Iter    12/   36] train: loss: 0.6943447
[Epoch 4] ogbg-molbace: 0.471882 val loss: 0.693819
[Epoch 4] ogbg-molbace: 0.465511 test loss: 0.693678
[Epoch 5; Iter     6/   36] train: loss: 0.6960522
[Epoch 5; Iter    36/   36] train: loss: 0.6947464
[Epoch 5] ogbg-molbace: 0.469979 val loss: 0.693725
[Epoch 5] ogbg-molbace: 0.467588 test loss: 0.693715
[Epoch 6; Iter    30/   36] train: loss: 0.6934244
[Epoch 6] ogbg-molbace: 0.473362 val loss: 0.693240
[Epoch 6] ogbg-molbace: 0.470838 test loss: 0.693991
[Epoch 7; Iter    24/   36] train: loss: 0.6957498
[Epoch 7] ogbg-molbace: 0.468182 val loss: 0.693361
[Epoch 7] ogbg-molbace: 0.475533 test loss: 0.693843
[Epoch 8; Iter    18/   36] train: loss: 0.6929794
[Epoch 8] ogbg-molbace: 0.469873 val loss: 0.693085
[Epoch 8] ogbg-molbace: 0.471109 test loss: 0.694103
[Epoch 9; Iter    12/   36] train: loss: 0.6926658
[Epoch 9] ogbg-molbace: 0.478541 val loss: 0.691862
[Epoch 9] ogbg-molbace: 0.487811 test loss: 0.694620
[Epoch 10; Iter     6/   36] train: loss: 0.6971813
[Epoch 10; Iter    36/   36] train: loss: 0.6933757
[Epoch 10] ogbg-molbace: 0.472939 val loss: 0.692157
[Epoch 10] ogbg-molbace: 0.480137 test loss: 0.694730
[Epoch 11; Iter    30/   36] train: loss: 0.6947681
[Epoch 11] ogbg-molbace: 0.470930 val loss: 0.692246
[Epoch 11] ogbg-molbace: 0.485283 test loss: 0.694745
[Epoch 12; Iter    24/   36] train: loss: 0.6925495
[Epoch 12] ogbg-molbace: 0.481924 val loss: 0.691152
[Epoch 12] ogbg-molbace: 0.501625 test loss: 0.694936
[Epoch 13; Iter    18/   36] train: loss: 0.6948438
[Epoch 13] ogbg-molbace: 0.477801 val loss: 0.690409
[Epoch 13] ogbg-molbace: 0.500271 test loss: 0.695589
[Epoch 14; Iter    12/   36] train: loss: 0.6930928
[Epoch 14] ogbg-molbace: 0.475687 val loss: 0.690448
[Epoch 14] ogbg-molbace: 0.499368 test loss: 0.695570
[Epoch 15; Iter     6/   36] train: loss: 0.6929878
[Epoch 15; Iter    36/   36] train: loss: 0.6969072
[Epoch 15] ogbg-molbace: 0.479281 val loss: 0.689202
[Epoch 15] ogbg-molbace: 0.517515 test loss: 0.696291
[Epoch 16; Iter    30/   36] train: loss: 0.6963509
[Epoch 16] ogbg-molbace: 0.473573 val loss: 0.688520
[Epoch 16] ogbg-molbace: 0.506591 test loss: 0.697139
[Epoch 17; Iter    24/   36] train: loss: 0.6894056
[Epoch 17] ogbg-molbace: 0.480127 val loss: 0.688205
[Epoch 17] ogbg-molbace: 0.519231 test loss: 0.697170
[Epoch 18; Iter    18/   36] train: loss: 0.6922541
[Epoch 18] ogbg-molbace: 0.483510 val loss: 0.687023
[Epoch 18] ogbg-molbace: 0.528440 test loss: 0.697957
[Epoch 19; Iter    12/   36] train: loss: 0.6898551
[Epoch 19] ogbg-molbace: 0.489218 val loss: 0.686167
[Epoch 19] ogbg-molbace: 0.548303 test loss: 0.698231
[Epoch 20; Iter     6/   36] train: loss: 0.6870125
[Epoch 20; Iter    36/   36] train: loss: 0.6802954
[Epoch 20] ogbg-molbace: 0.584989 val loss: 0.679673
[Epoch 20] ogbg-molbace: 0.773474 test loss: 0.676667
[Epoch 21; Iter    30/   36] train: loss: 0.6700048
[Epoch 21] ogbg-molbace: 0.619027 val loss: 0.683694
[Epoch 21] ogbg-molbace: 0.815818 test loss: 0.633955
[Epoch 22; Iter    24/   36] train: loss: 0.6091893
[Epoch 22] ogbg-molbace: 0.605497 val loss: 0.694068
[Epoch 22] ogbg-molbace: 0.801643 test loss: 0.578428
[Epoch 23; Iter    18/   36] train: loss: 0.5659903
[Epoch 23] ogbg-molbace: 0.646300 val loss: 0.630725
[Epoch 23] ogbg-molbace: 0.750993 test loss: 0.689157
[Epoch 24; Iter    12/   36] train: loss: 0.6091851
[Epoch 24] ogbg-molbace: 0.676321 val loss: 0.618639
[Epoch 24] ogbg-molbace: 0.770314 test loss: 0.645227
[Epoch 25; Iter     6/   36] train: loss: 0.4234711
[Epoch 25; Iter    36/   36] train: loss: 0.6102633
[Epoch 25] ogbg-molbace: 0.680127 val loss: 0.611323
[Epoch 25] ogbg-molbace: 0.812929 test loss: 0.593674
[Epoch 26; Iter    30/   36] train: loss: 0.4662991
[Epoch 26] ogbg-molbace: 0.678964 val loss: 0.608105
[Epoch 26] ogbg-molbace: 0.831889 test loss: 0.614469
[Epoch 27; Iter    24/   36] train: loss: 0.5209768
[Epoch 27] ogbg-molbace: 0.702748 val loss: 0.600119
[Epoch 27] ogbg-molbace: 0.813109 test loss: 0.565081
[Epoch 28; Iter    18/   36] train: loss: 0.4958878
[Epoch 28] ogbg-molbace: 0.668710 val loss: 0.540987
[Epoch 28] ogbg-molbace: 0.790719 test loss: 0.895598
[Epoch 29; Iter    12/   36] train: loss: 0.5225581
[Epoch 29] ogbg-molbace: 0.713953 val loss: 0.543264
[Epoch 29] ogbg-molbace: 0.778440 test loss: 0.768988
[Epoch 30; Iter     6/   36] train: loss: 0.5599285
[Epoch 30; Iter    36/   36] train: loss: 0.3830597
[Epoch 30] ogbg-molbace: 0.720719 val loss: 0.620521
[Epoch 30] ogbg-molbace: 0.806338 test loss: 0.606368
[Epoch 31; Iter    30/   36] train: loss: 0.5750570
[Epoch 31] ogbg-molbace: 0.723679 val loss: 0.607221
[Epoch 31] ogbg-molbace: 0.801733 test loss: 0.571105
[Epoch 32; Iter    24/   36] train: loss: 0.3744963
[Epoch 32] ogbg-molbace: 0.700317 val loss: 0.618559
[Epoch 32] ogbg-molbace: 0.820784 test loss: 0.648221
[Epoch 33; Iter    18/   36] train: loss: 0.4096943
[Epoch 33] ogbg-molbace: 0.725899 val loss: 0.607656
[Epoch 33] ogbg-molbace: 0.810852 test loss: 0.605275
[Epoch 34; Iter    12/   36] train: loss: 0.3721424
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.8/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.8_5_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.8
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6950303
[Epoch 1] ogbg-molbace: 0.406593 val loss: 0.693259
[Epoch 1] ogbg-molbace: 0.372457 test loss: 0.692916
[Epoch 2; Iter    19/   41] train: loss: 0.6950377
[Epoch 2] ogbg-molbace: 0.447253 val loss: 0.694580
[Epoch 2] ogbg-molbace: 0.417666 test loss: 0.691552
[Epoch 3; Iter     8/   41] train: loss: 0.6960334
[Epoch 3; Iter    38/   41] train: loss: 0.6972963
[Epoch 3] ogbg-molbace: 0.431502 val loss: 0.695221
[Epoch 3] ogbg-molbace: 0.412972 test loss: 0.691748
[Epoch 4; Iter    27/   41] train: loss: 0.6938558
[Epoch 4] ogbg-molbace: 0.442857 val loss: 0.695608
[Epoch 4] ogbg-molbace: 0.424274 test loss: 0.691425
[Epoch 5; Iter    16/   41] train: loss: 0.6949973
[Epoch 5] ogbg-molbace: 0.441392 val loss: 0.695963
[Epoch 5] ogbg-molbace: 0.417145 test loss: 0.691923
[Epoch 6; Iter     5/   41] train: loss: 0.6961963
[Epoch 6; Iter    35/   41] train: loss: 0.6999989
[Epoch 6] ogbg-molbace: 0.446520 val loss: 0.696210
[Epoch 6] ogbg-molbace: 0.427056 test loss: 0.691761
[Epoch 7; Iter    24/   41] train: loss: 0.6939733
[Epoch 7] ogbg-molbace: 0.442125 val loss: 0.696963
[Epoch 7] ogbg-molbace: 0.417840 test loss: 0.692341
[Epoch 8; Iter    13/   41] train: loss: 0.6931360
[Epoch 8] ogbg-molbace: 0.420879 val loss: 0.697438
[Epoch 8] ogbg-molbace: 0.409320 test loss: 0.692982
[Epoch 9; Iter     2/   41] train: loss: 0.6941325
[Epoch 9; Iter    32/   41] train: loss: 0.6933050
[Epoch 9] ogbg-molbace: 0.428205 val loss: 0.697758
[Epoch 9] ogbg-molbace: 0.416102 test loss: 0.692804
[Epoch 10; Iter    21/   41] train: loss: 0.6952260
[Epoch 10] ogbg-molbace: 0.419414 val loss: 0.698516
[Epoch 10] ogbg-molbace: 0.410885 test loss: 0.693385
[Epoch 11; Iter    10/   41] train: loss: 0.6918857
[Epoch 11; Iter    40/   41] train: loss: 0.6938196
[Epoch 11] ogbg-molbace: 0.448718 val loss: 0.698331
[Epoch 11] ogbg-molbace: 0.441836 test loss: 0.693098
[Epoch 12; Iter    29/   41] train: loss: 0.6913506
[Epoch 12] ogbg-molbace: 0.427839 val loss: 0.699827
[Epoch 12] ogbg-molbace: 0.424622 test loss: 0.693830
[Epoch 13; Iter    18/   41] train: loss: 0.6924941
[Epoch 13] ogbg-molbace: 0.420513 val loss: 0.700463
[Epoch 13] ogbg-molbace: 0.430534 test loss: 0.694192
[Epoch 14; Iter     7/   41] train: loss: 0.6954867
[Epoch 14; Iter    37/   41] train: loss: 0.6920549
[Epoch 14] ogbg-molbace: 0.434066 val loss: 0.701485
[Epoch 14] ogbg-molbace: 0.440271 test loss: 0.694409
[Epoch 15; Iter    26/   41] train: loss: 0.6927629
[Epoch 15] ogbg-molbace: 0.428205 val loss: 0.702042
[Epoch 15] ogbg-molbace: 0.441315 test loss: 0.695451
[Epoch 16; Iter    15/   41] train: loss: 0.6844546
[Epoch 16] ogbg-molbace: 0.426374 val loss: 0.703127
[Epoch 16] ogbg-molbace: 0.446183 test loss: 0.695532
[Epoch 17; Iter     4/   41] train: loss: 0.6933329
[Epoch 17; Iter    34/   41] train: loss: 0.6949021
[Epoch 17] ogbg-molbace: 0.545788 val loss: 0.696921
[Epoch 17] ogbg-molbace: 0.602678 test loss: 0.695003
[Epoch 18; Iter    23/   41] train: loss: 0.6704170
[Epoch 18] ogbg-molbace: 0.624908 val loss: 0.686728
[Epoch 18] ogbg-molbace: 0.710311 test loss: 0.690051
[Epoch 19; Iter    12/   41] train: loss: 0.6262575
[Epoch 19] ogbg-molbace: 0.668864 val loss: 0.689252
[Epoch 19] ogbg-molbace: 0.748739 test loss: 0.695514
[Epoch 20; Iter     1/   41] train: loss: 0.5714597
[Epoch 20; Iter    31/   41] train: loss: 0.5175056
[Epoch 20] ogbg-molbace: 0.743223 val loss: 0.645782
[Epoch 20] ogbg-molbace: 0.754130 test loss: 0.683948
[Epoch 21; Iter    20/   41] train: loss: 0.4823948
[Epoch 21] ogbg-molbace: 0.756777 val loss: 0.707641
[Epoch 21] ogbg-molbace: 0.783690 test loss: 0.759231
[Epoch 22; Iter     9/   41] train: loss: 0.4359258
[Epoch 22; Iter    39/   41] train: loss: 0.5209023
[Epoch 22] ogbg-molbace: 0.682418 val loss: 0.750304
[Epoch 22] ogbg-molbace: 0.795340 test loss: 0.656327
[Epoch 23; Iter    28/   41] train: loss: 0.5123437
[Epoch 23] ogbg-molbace: 0.717582 val loss: 0.717612
[Epoch 23] ogbg-molbace: 0.790123 test loss: 0.808389
[Epoch 24; Iter    17/   41] train: loss: 0.4704634
[Epoch 24] ogbg-molbace: 0.753480 val loss: 0.814639
[Epoch 24] ogbg-molbace: 0.788211 test loss: 0.828489
[Epoch 25; Iter     6/   41] train: loss: 0.4005370
[Epoch 25; Iter    36/   41] train: loss: 0.4865997
[Epoch 25] ogbg-molbace: 0.705128 val loss: 0.822906
[Epoch 25] ogbg-molbace: 0.818466 test loss: 0.733153
[Epoch 26; Iter    25/   41] train: loss: 0.3554456
[Epoch 26] ogbg-molbace: 0.707692 val loss: 0.731633
[Epoch 26] ogbg-molbace: 0.819336 test loss: 0.773209
[Epoch 27; Iter    14/   41] train: loss: 0.4871371
[Epoch 27] ogbg-molbace: 0.715018 val loss: 0.668508
[Epoch 27] ogbg-molbace: 0.802295 test loss: 0.700438
[Epoch 28; Iter     3/   41] train: loss: 0.3050030
[Epoch 28; Iter    33/   41] train: loss: 0.5473686
[Epoch 28] ogbg-molbace: 0.712088 val loss: 1.004067
[Epoch 28] ogbg-molbace: 0.793949 test loss: 0.827717
[Epoch 29; Iter    22/   41] train: loss: 0.3282694
[Epoch 29] ogbg-molbace: 0.726374 val loss: 0.781274
[Epoch 29] ogbg-molbace: 0.777778 test loss: 0.944987
[Epoch 30; Iter    11/   41] train: loss: 0.5215871
[Epoch 30; Iter    41/   41] train: loss: 0.6341766
[Epoch 30] ogbg-molbace: 0.736630 val loss: 0.822337
[Epoch 30] ogbg-molbace: 0.808207 test loss: 0.647581
[Epoch 31; Iter    30/   41] train: loss: 0.4596748
[Epoch 31] ogbg-molbace: 0.735897 val loss: 1.050927
[Epoch 31] ogbg-molbace: 0.794471 test loss: 1.204253
[Epoch 32; Iter    19/   41] train: loss: 0.5901037
[Epoch 32] ogbg-molbace: 0.725641 val loss: 0.796150
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.7/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.7_5_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.7
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6954246
[Epoch 1] ogbg-molbace: 0.472727 val loss: 0.693937
[Epoch 1] ogbg-molbace: 0.400054 test loss: 0.692805
[Epoch 2; Iter    24/   36] train: loss: 0.6975366
[Epoch 2] ogbg-molbace: 0.448097 val loss: 0.695797
[Epoch 2] ogbg-molbace: 0.408631 test loss: 0.692373
[Epoch 3; Iter    18/   36] train: loss: 0.6957899
[Epoch 3] ogbg-molbace: 0.482030 val loss: 0.695833
[Epoch 3] ogbg-molbace: 0.443211 test loss: 0.691398
[Epoch 4; Iter    12/   36] train: loss: 0.6950893
[Epoch 4] ogbg-molbace: 0.472516 val loss: 0.695304
[Epoch 4] ogbg-molbace: 0.417660 test loss: 0.692417
[Epoch 5; Iter     6/   36] train: loss: 0.6943478
[Epoch 5; Iter    36/   36] train: loss: 0.6887620
[Epoch 5] ogbg-molbace: 0.466279 val loss: 0.695122
[Epoch 5] ogbg-molbace: 0.417479 test loss: 0.692683
[Epoch 6; Iter    30/   36] train: loss: 0.6961745
[Epoch 6] ogbg-molbace: 0.477801 val loss: 0.694763
[Epoch 6] ogbg-molbace: 0.421091 test loss: 0.692714
[Epoch 7; Iter    24/   36] train: loss: 0.6977026
[Epoch 7] ogbg-molbace: 0.481712 val loss: 0.694763
[Epoch 7] ogbg-molbace: 0.435356 test loss: 0.692240
[Epoch 8; Iter    18/   36] train: loss: 0.6921148
[Epoch 8] ogbg-molbace: 0.479070 val loss: 0.693653
[Epoch 8] ogbg-molbace: 0.432918 test loss: 0.693121
[Epoch 9; Iter    12/   36] train: loss: 0.6935070
[Epoch 9] ogbg-molbace: 0.472622 val loss: 0.693887
[Epoch 9] ogbg-molbace: 0.419646 test loss: 0.693531
[Epoch 10; Iter     6/   36] train: loss: 0.6922469
[Epoch 10; Iter    36/   36] train: loss: 0.7013277
[Epoch 10] ogbg-molbace: 0.478224 val loss: 0.693155
[Epoch 10] ogbg-molbace: 0.422445 test loss: 0.693874
[Epoch 11; Iter    30/   36] train: loss: 0.6933268
[Epoch 11] ogbg-molbace: 0.491543 val loss: 0.693015
[Epoch 11] ogbg-molbace: 0.437252 test loss: 0.693599
[Epoch 12; Iter    24/   36] train: loss: 0.6951839
[Epoch 12] ogbg-molbace: 0.489641 val loss: 0.692606
[Epoch 12] ogbg-molbace: 0.463615 test loss: 0.693159
[Epoch 13; Iter    18/   36] train: loss: 0.6924257
[Epoch 13] ogbg-molbace: 0.486047 val loss: 0.691808
[Epoch 13] ogbg-molbace: 0.433640 test loss: 0.694626
[Epoch 14; Iter    12/   36] train: loss: 0.6942793
[Epoch 14] ogbg-molbace: 0.486152 val loss: 0.691233
[Epoch 14] ogbg-molbace: 0.434724 test loss: 0.695102
[Epoch 15; Iter     6/   36] train: loss: 0.6986830
[Epoch 15; Iter    36/   36] train: loss: 0.6968724
[Epoch 15] ogbg-molbace: 0.489323 val loss: 0.690866
[Epoch 15] ogbg-molbace: 0.436891 test loss: 0.695171
[Epoch 16; Iter    30/   36] train: loss: 0.6906301
[Epoch 16] ogbg-molbace: 0.485201 val loss: 0.690169
[Epoch 16] ogbg-molbace: 0.435988 test loss: 0.695928
[Epoch 17; Iter    24/   36] train: loss: 0.6917942
[Epoch 17] ogbg-molbace: 0.498626 val loss: 0.689448
[Epoch 17] ogbg-molbace: 0.442940 test loss: 0.696190
[Epoch 18; Iter    18/   36] train: loss: 0.6906267
[Epoch 18] ogbg-molbace: 0.499894 val loss: 0.688336
[Epoch 18] ogbg-molbace: 0.442849 test loss: 0.697129
[Epoch 19; Iter    12/   36] train: loss: 0.6938077
[Epoch 19] ogbg-molbace: 0.511205 val loss: 0.688075
[Epoch 19] ogbg-molbace: 0.461538 test loss: 0.696923
[Epoch 20; Iter     6/   36] train: loss: 0.6956931
[Epoch 20; Iter    36/   36] train: loss: 0.6561066
[Epoch 20] ogbg-molbace: 0.592495 val loss: 0.674004
[Epoch 20] ogbg-molbace: 0.772571 test loss: 0.677949
[Epoch 21; Iter    30/   36] train: loss: 0.6464946
[Epoch 21] ogbg-molbace: 0.615433 val loss: 0.664327
[Epoch 21] ogbg-molbace: 0.816360 test loss: 0.634160
[Epoch 22; Iter    24/   36] train: loss: 0.5895999
[Epoch 22] ogbg-molbace: 0.632664 val loss: 0.644197
[Epoch 22] ogbg-molbace: 0.806338 test loss: 0.625893
[Epoch 23; Iter    18/   36] train: loss: 0.5705813
[Epoch 23] ogbg-molbace: 0.620930 val loss: 0.582835
[Epoch 23] ogbg-molbace: 0.781239 test loss: 0.735751
[Epoch 24; Iter    12/   36] train: loss: 0.5315210
[Epoch 24] ogbg-molbace: 0.680127 val loss: 0.560268
[Epoch 24] ogbg-molbace: 0.814283 test loss: 0.642982
[Epoch 25; Iter     6/   36] train: loss: 0.5524194
[Epoch 25; Iter    36/   36] train: loss: 0.3175666
[Epoch 25] ogbg-molbace: 0.675264 val loss: 0.590818
[Epoch 25] ogbg-molbace: 0.814464 test loss: 0.595429
[Epoch 26; Iter    30/   36] train: loss: 0.4254692
[Epoch 26] ogbg-molbace: 0.708034 val loss: 0.557286
[Epoch 26] ogbg-molbace: 0.799657 test loss: 0.627398
[Epoch 27; Iter    24/   36] train: loss: 0.5270027
[Epoch 27] ogbg-molbace: 0.686681 val loss: 0.768824
[Epoch 27] ogbg-molbace: 0.787288 test loss: 0.566108
[Epoch 28; Iter    18/   36] train: loss: 0.4291258
[Epoch 28] ogbg-molbace: 0.695877 val loss: 0.576813
[Epoch 28] ogbg-molbace: 0.797219 test loss: 0.677676
[Epoch 29; Iter    12/   36] train: loss: 0.4451495
[Epoch 29] ogbg-molbace: 0.707400 val loss: 0.516603
[Epoch 29] ogbg-molbace: 0.815728 test loss: 0.751050
[Epoch 30; Iter     6/   36] train: loss: 0.3833125
[Epoch 30; Iter    36/   36] train: loss: 0.3753177
[Epoch 30] ogbg-molbace: 0.696300 val loss: 0.544141
[Epoch 30] ogbg-molbace: 0.815637 test loss: 0.704583
[Epoch 31; Iter    30/   36] train: loss: 0.3548133
[Epoch 31] ogbg-molbace: 0.701903 val loss: 0.532172
[Epoch 31] ogbg-molbace: 0.821687 test loss: 0.681237
[Epoch 32; Iter    24/   36] train: loss: 0.4743793
[Epoch 32] ogbg-molbace: 0.661099 val loss: 0.630338
[Epoch 32] ogbg-molbace: 0.831889 test loss: 0.665077
[Epoch 33; Iter    18/   36] train: loss: 0.5531158
[Epoch 33] ogbg-molbace: 0.659514 val loss: 0.643829
[Epoch 33] ogbg-molbace: 0.815005 test loss: 0.689079
[Epoch 34; Iter    12/   36] train: loss: 0.3984171
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.6/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.6_4_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.6
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6922913
[Epoch 1] ogbg-molbace: 0.465641 val loss: 0.693673
[Epoch 1] ogbg-molbace: 0.433907 test loss: 0.693195
[Epoch 2; Iter    29/   31] train: loss: 0.6910420
[Epoch 2] ogbg-molbace: 0.445978 val loss: 0.694464
[Epoch 2] ogbg-molbace: 0.430971 test loss: 0.693847
[Epoch 3; Iter    28/   31] train: loss: 0.6933663
[Epoch 3] ogbg-molbace: 0.456703 val loss: 0.695136
[Epoch 3] ogbg-molbace: 0.461055 test loss: 0.694595
[Epoch 4; Iter    27/   31] train: loss: 0.6949117
[Epoch 4] ogbg-molbace: 0.459285 val loss: 0.695620
[Epoch 4] ogbg-molbace: 0.462961 test loss: 0.694660
[Epoch 5; Iter    26/   31] train: loss: 0.6936610
[Epoch 5] ogbg-molbace: 0.466435 val loss: 0.694817
[Epoch 5] ogbg-molbace: 0.462291 test loss: 0.694834
[Epoch 6; Iter    25/   31] train: loss: 0.6937734
[Epoch 6] ogbg-molbace: 0.459186 val loss: 0.695541
[Epoch 6] ogbg-molbace: 0.458325 test loss: 0.694652
[Epoch 7; Iter    24/   31] train: loss: 0.6936248
[Epoch 7] ogbg-molbace: 0.460179 val loss: 0.694478
[Epoch 7] ogbg-molbace: 0.459149 test loss: 0.694982
[Epoch 8; Iter    23/   31] train: loss: 0.6933005
[Epoch 8] ogbg-molbace: 0.457994 val loss: 0.694500
[Epoch 8] ogbg-molbace: 0.466619 test loss: 0.694860
[Epoch 9; Iter    22/   31] train: loss: 0.6961382
[Epoch 9] ogbg-molbace: 0.451341 val loss: 0.693565
[Epoch 9] ogbg-molbace: 0.469452 test loss: 0.694974
[Epoch 10; Iter    21/   31] train: loss: 0.6946006
[Epoch 10] ogbg-molbace: 0.447865 val loss: 0.693903
[Epoch 10] ogbg-molbace: 0.465022 test loss: 0.695018
[Epoch 11; Iter    20/   31] train: loss: 0.6964504
[Epoch 11] ogbg-molbace: 0.450546 val loss: 0.693096
[Epoch 11] ogbg-molbace: 0.476664 test loss: 0.694920
[Epoch 12; Iter    19/   31] train: loss: 0.6924362
[Epoch 12] ogbg-molbace: 0.457795 val loss: 0.693291
[Epoch 12] ogbg-molbace: 0.474655 test loss: 0.694996
[Epoch 13; Iter    18/   31] train: loss: 0.6918920
[Epoch 13] ogbg-molbace: 0.451837 val loss: 0.693610
[Epoch 13] ogbg-molbace: 0.482537 test loss: 0.694739
[Epoch 14; Iter    17/   31] train: loss: 0.6940188
[Epoch 14] ogbg-molbace: 0.455511 val loss: 0.693067
[Epoch 14] ogbg-molbace: 0.482897 test loss: 0.694771
[Epoch 15; Iter    16/   31] train: loss: 0.6903758
[Epoch 15] ogbg-molbace: 0.450149 val loss: 0.693256
[Epoch 15] ogbg-molbace: 0.487997 test loss: 0.694713
[Epoch 16; Iter    15/   31] train: loss: 0.6923183
[Epoch 16] ogbg-molbace: 0.450546 val loss: 0.692520
[Epoch 16] ogbg-molbace: 0.490058 test loss: 0.694908
[Epoch 17; Iter    14/   31] train: loss: 0.6952211
[Epoch 17] ogbg-molbace: 0.449752 val loss: 0.692485
[Epoch 17] ogbg-molbace: 0.494539 test loss: 0.694736
[Epoch 18; Iter    13/   31] train: loss: 0.6944011
[Epoch 18] ogbg-molbace: 0.446177 val loss: 0.691582
[Epoch 18] ogbg-molbace: 0.484237 test loss: 0.695207
[Epoch 19; Iter    12/   31] train: loss: 0.6931297
[Epoch 19] ogbg-molbace: 0.444687 val loss: 0.690364
[Epoch 19] ogbg-molbace: 0.493561 test loss: 0.695486
[Epoch 20; Iter    11/   31] train: loss: 0.6912597
[Epoch 20] ogbg-molbace: 0.449851 val loss: 0.691014
[Epoch 20] ogbg-molbace: 0.501648 test loss: 0.694959
[Epoch 21; Iter    10/   31] train: loss: 0.6934075
[Epoch 21] ogbg-molbace: 0.444786 val loss: 0.690830
[Epoch 21] ogbg-molbace: 0.510921 test loss: 0.695082
[Epoch 22; Iter     9/   31] train: loss: 0.6944098
[Epoch 22] ogbg-molbace: 0.443893 val loss: 0.689386
[Epoch 22] ogbg-molbace: 0.521688 test loss: 0.695043
[Epoch 23; Iter     8/   31] train: loss: 0.6866417
[Epoch 23] ogbg-molbace: 0.575670 val loss: 0.665516
[Epoch 23] ogbg-molbace: 0.682001 test loss: 0.685012
[Epoch 24; Iter     7/   31] train: loss: 0.6622651
[Epoch 24] ogbg-molbace: 0.647666 val loss: 0.620364
[Epoch 24] ogbg-molbace: 0.718628 test loss: 0.673329
[Epoch 25; Iter     6/   31] train: loss: 0.6363649
[Epoch 25] ogbg-molbace: 0.711519 val loss: 0.575344
[Epoch 25] ogbg-molbace: 0.783330 test loss: 0.647013
[Epoch 26; Iter     5/   31] train: loss: 0.5809736
[Epoch 26] ogbg-molbace: 0.750546 val loss: 0.540512
[Epoch 26] ogbg-molbace: 0.781424 test loss: 0.640391
[Epoch 27; Iter     4/   31] train: loss: 0.6634340
[Epoch 27] ogbg-molbace: 0.747964 val loss: 0.450121
[Epoch 27] ogbg-molbace: 0.756130 test loss: 0.689009
[Epoch 28; Iter     3/   31] train: loss: 0.6746523
[Epoch 28] ogbg-molbace: 0.744985 val loss: 0.559099
[Epoch 28] ogbg-molbace: 0.777045 test loss: 0.604158
[Epoch 29; Iter     2/   31] train: loss: 0.4632759
[Epoch 29] ogbg-molbace: 0.766634 val loss: 0.430018
[Epoch 29] ogbg-molbace: 0.717803 test loss: 0.706289
[Epoch 30; Iter     1/   31] train: loss: 0.4228125
[Epoch 30; Iter    31/   31] train: loss: 0.4848208
[Epoch 30] ogbg-molbace: 0.766931 val loss: 0.527884
[Epoch 30] ogbg-molbace: 0.785236 test loss: 0.611251
[Epoch 31; Iter    30/   31] train: loss: 0.6011435
[Epoch 31] ogbg-molbace: 0.780437 val loss: 0.453721
[Epoch 31] ogbg-molbace: 0.745621 test loss: 0.659489
[Epoch 32; Iter    29/   31] train: loss: 0.4774714
[Epoch 32] ogbg-molbace: 0.731281 val loss: 0.507092
[Epoch 32] ogbg-molbace: 0.768597 test loss: 0.652082
[Epoch 33; Iter    28/   31] train: loss: 0.5419827
[Epoch 33] ogbg-molbace: 0.727309 val loss: 0.492219
[Epoch 33] ogbg-molbace: 0.735937 test loss: 0.672749
[Epoch 34; Iter    27/   31] train: loss: 0.5310853
[Epoch 34] ogbg-molbace: 0.741708 val loss: 0.471983
[Epoch 34] ogbg-molbace: 0.759118 test loss: 0.677958
[Epoch 35; Iter    26/   31] train: loss: 0.4187911
[Epoch 35] ogbg-molbace: 0.740715 val loss: 0.476895
[Epoch 35] ogbg-molbace: 0.745930 test loss: 0.694742
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.6/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.6_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.6
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6937211
[Epoch 1] ogbg-molbace: 0.409732 val loss: 0.692957
[Epoch 1] ogbg-molbace: 0.498558 test loss: 0.693280
[Epoch 2; Iter    29/   31] train: loss: 0.6895218
[Epoch 2] ogbg-molbace: 0.412214 val loss: 0.693497
[Epoch 2] ogbg-molbace: 0.519215 test loss: 0.693189
[Epoch 3; Iter    28/   31] train: loss: 0.6927750
[Epoch 3] ogbg-molbace: 0.425025 val loss: 0.693762
[Epoch 3] ogbg-molbace: 0.529415 test loss: 0.693031
[Epoch 4; Iter    27/   31] train: loss: 0.6942388
[Epoch 4] ogbg-molbace: 0.429295 val loss: 0.692701
[Epoch 4] ogbg-molbace: 0.520657 test loss: 0.693572
[Epoch 5; Iter    26/   31] train: loss: 0.6932709
[Epoch 5] ogbg-molbace: 0.434260 val loss: 0.692946
[Epoch 5] ogbg-molbace: 0.532763 test loss: 0.693321
[Epoch 6; Iter    25/   31] train: loss: 0.6944118
[Epoch 6] ogbg-molbace: 0.439027 val loss: 0.693144
[Epoch 6] ogbg-molbace: 0.531630 test loss: 0.693211
[Epoch 7; Iter    24/   31] train: loss: 0.6911282
[Epoch 7] ogbg-molbace: 0.427805 val loss: 0.692572
[Epoch 7] ogbg-molbace: 0.531784 test loss: 0.693511
[Epoch 8; Iter    23/   31] train: loss: 0.6921297
[Epoch 8] ogbg-molbace: 0.444687 val loss: 0.693009
[Epoch 8] ogbg-molbace: 0.536163 test loss: 0.693189
[Epoch 9; Iter    22/   31] train: loss: 0.6915993
[Epoch 9] ogbg-molbace: 0.433466 val loss: 0.692030
[Epoch 9] ogbg-molbace: 0.535906 test loss: 0.693548
[Epoch 10; Iter    21/   31] train: loss: 0.6904463
[Epoch 10] ogbg-molbace: 0.438232 val loss: 0.692429
[Epoch 10] ogbg-molbace: 0.534669 test loss: 0.693406
[Epoch 11; Iter    20/   31] train: loss: 0.6860051
[Epoch 11] ogbg-molbace: 0.436842 val loss: 0.691778
[Epoch 11] ogbg-molbace: 0.534154 test loss: 0.693697
[Epoch 12; Iter    19/   31] train: loss: 0.6908978
[Epoch 12] ogbg-molbace: 0.438133 val loss: 0.691258
[Epoch 12] ogbg-molbace: 0.542654 test loss: 0.693670
[Epoch 13; Iter    18/   31] train: loss: 0.6897886
[Epoch 13] ogbg-molbace: 0.437835 val loss: 0.691708
[Epoch 13] ogbg-molbace: 0.542242 test loss: 0.693597
[Epoch 14; Iter    17/   31] train: loss: 0.6929994
[Epoch 14] ogbg-molbace: 0.443198 val loss: 0.691331
[Epoch 14] ogbg-molbace: 0.528333 test loss: 0.693869
[Epoch 15; Iter    16/   31] train: loss: 0.6941121
[Epoch 15] ogbg-molbace: 0.436346 val loss: 0.690659
[Epoch 15] ogbg-molbace: 0.532660 test loss: 0.693939
[Epoch 16; Iter    15/   31] train: loss: 0.6952860
[Epoch 16] ogbg-molbace: 0.436346 val loss: 0.690262
[Epoch 16] ogbg-molbace: 0.536833 test loss: 0.694092
[Epoch 17; Iter    14/   31] train: loss: 0.6908580
[Epoch 17] ogbg-molbace: 0.437438 val loss: 0.690857
[Epoch 17] ogbg-molbace: 0.540130 test loss: 0.693815
[Epoch 18; Iter    13/   31] train: loss: 0.6867669
[Epoch 18] ogbg-molbace: 0.448064 val loss: 0.690207
[Epoch 18] ogbg-molbace: 0.547599 test loss: 0.693895
[Epoch 19; Iter    12/   31] train: loss: 0.6906126
[Epoch 19] ogbg-molbace: 0.446375 val loss: 0.690436
[Epoch 19] ogbg-molbace: 0.547033 test loss: 0.693733
[Epoch 20; Iter    11/   31] train: loss: 0.6944624
[Epoch 20] ogbg-molbace: 0.459384 val loss: 0.688523
[Epoch 20] ogbg-molbace: 0.545642 test loss: 0.694496
[Epoch 21; Iter    10/   31] train: loss: 0.6939867
[Epoch 21] ogbg-molbace: 0.453724 val loss: 0.688087
[Epoch 21] ogbg-molbace: 0.549557 test loss: 0.694520
[Epoch 22; Iter     9/   31] train: loss: 0.6911855
[Epoch 22] ogbg-molbace: 0.460973 val loss: 0.688209
[Epoch 22] ogbg-molbace: 0.552854 test loss: 0.694176
[Epoch 23; Iter     8/   31] train: loss: 0.6809049
[Epoch 23] ogbg-molbace: 0.641907 val loss: 0.662500
[Epoch 23] ogbg-molbace: 0.714764 test loss: 0.681170
[Epoch 24; Iter     7/   31] train: loss: 0.6697537
[Epoch 24] ogbg-molbace: 0.692354 val loss: 0.634940
[Epoch 24] ogbg-molbace: 0.775654 test loss: 0.659717
[Epoch 25; Iter     6/   31] train: loss: 0.6337470
[Epoch 25] ogbg-molbace: 0.642006 val loss: 0.583922
[Epoch 25] ogbg-molbace: 0.730476 test loss: 0.660350
[Epoch 26; Iter     5/   31] train: loss: 0.6751978
[Epoch 26] ogbg-molbace: 0.723932 val loss: 0.544142
[Epoch 26] ogbg-molbace: 0.762363 test loss: 0.637955
[Epoch 27; Iter     4/   31] train: loss: 0.6032422
[Epoch 27] ogbg-molbace: 0.751639 val loss: 0.641515
[Epoch 27] ogbg-molbace: 0.781115 test loss: 0.581748
[Epoch 28; Iter     3/   31] train: loss: 0.6315873
[Epoch 28] ogbg-molbace: 0.680735 val loss: 0.527487
[Epoch 28] ogbg-molbace: 0.747630 test loss: 0.678490
[Epoch 29; Iter     2/   31] train: loss: 0.4955038
[Epoch 29] ogbg-molbace: 0.766733 val loss: 0.545265
[Epoch 29] ogbg-molbace: 0.749073 test loss: 0.641747
[Epoch 30; Iter     1/   31] train: loss: 0.5171532
[Epoch 30; Iter    31/   31] train: loss: 0.6335236
[Epoch 30] ogbg-molbace: 0.755313 val loss: 0.588820
[Epoch 30] ogbg-molbace: 0.787915 test loss: 0.583890
[Epoch 31; Iter    30/   31] train: loss: 0.4856516
[Epoch 31] ogbg-molbace: 0.781132 val loss: 0.446055
[Epoch 31] ogbg-molbace: 0.772151 test loss: 0.659301
[Epoch 32; Iter    29/   31] train: loss: 0.4121509
[Epoch 32] ogbg-molbace: 0.765343 val loss: 0.500644
[Epoch 32] ogbg-molbace: 0.751442 test loss: 0.642310
[Epoch 33; Iter    28/   31] train: loss: 0.5803683
[Epoch 33] ogbg-molbace: 0.738232 val loss: 0.517791
[Epoch 33] ogbg-molbace: 0.757624 test loss: 0.631852
[Epoch 34; Iter    27/   31] train: loss: 0.5437548
[Epoch 34] ogbg-molbace: 0.747964 val loss: 0.433038
[Epoch 34] ogbg-molbace: 0.758706 test loss: 0.731136
[Epoch 35; Iter    26/   31] train: loss: 0.5555139
[Epoch 35] ogbg-molbace: 0.740914 val loss: 0.536440
[Epoch 35] ogbg-molbace: 0.760251 test loss: 0.645854
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.8/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.8_6_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.8
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6937714
[Epoch 1] ogbg-molbace: 0.524542 val loss: 0.692766
[Epoch 1] ogbg-molbace: 0.444966 test loss: 0.693076
[Epoch 2; Iter    19/   41] train: loss: 0.6916298
[Epoch 2] ogbg-molbace: 0.527473 val loss: 0.691610
[Epoch 2] ogbg-molbace: 0.449661 test loss: 0.693327
[Epoch 3; Iter     8/   41] train: loss: 0.6901196
[Epoch 3; Iter    38/   41] train: loss: 0.6973189
[Epoch 3] ogbg-molbace: 0.525275 val loss: 0.691448
[Epoch 3] ogbg-molbace: 0.435750 test loss: 0.694144
[Epoch 4; Iter    27/   41] train: loss: 0.6946090
[Epoch 4] ogbg-molbace: 0.526007 val loss: 0.691496
[Epoch 4] ogbg-molbace: 0.435229 test loss: 0.694053
[Epoch 5; Iter    16/   41] train: loss: 0.6932670
[Epoch 5] ogbg-molbace: 0.521978 val loss: 0.691895
[Epoch 5] ogbg-molbace: 0.432273 test loss: 0.694522
[Epoch 6; Iter     5/   41] train: loss: 0.6914856
[Epoch 6; Iter    35/   41] train: loss: 0.6918142
[Epoch 6] ogbg-molbace: 0.524176 val loss: 0.692430
[Epoch 6] ogbg-molbace: 0.440097 test loss: 0.694714
[Epoch 7; Iter    24/   41] train: loss: 0.6926837
[Epoch 7] ogbg-molbace: 0.523810 val loss: 0.692847
[Epoch 7] ogbg-molbace: 0.447748 test loss: 0.694696
[Epoch 8; Iter    13/   41] train: loss: 0.6906585
[Epoch 8] ogbg-molbace: 0.523077 val loss: 0.693199
[Epoch 8] ogbg-molbace: 0.443749 test loss: 0.694918
[Epoch 9; Iter     2/   41] train: loss: 0.6888597
[Epoch 9; Iter    32/   41] train: loss: 0.6915891
[Epoch 9] ogbg-molbace: 0.523443 val loss: 0.693622
[Epoch 9] ogbg-molbace: 0.440097 test loss: 0.695501
[Epoch 10; Iter    21/   41] train: loss: 0.6943637
[Epoch 10] ogbg-molbace: 0.520879 val loss: 0.694820
[Epoch 10] ogbg-molbace: 0.456268 test loss: 0.695610
[Epoch 11; Iter    10/   41] train: loss: 0.6918271
[Epoch 11; Iter    40/   41] train: loss: 0.6911523
[Epoch 11] ogbg-molbace: 0.517949 val loss: 0.695369
[Epoch 11] ogbg-molbace: 0.453486 test loss: 0.696310
[Epoch 12; Iter    29/   41] train: loss: 0.6941935
[Epoch 12] ogbg-molbace: 0.525641 val loss: 0.696124
[Epoch 12] ogbg-molbace: 0.461485 test loss: 0.696371
[Epoch 13; Iter    18/   41] train: loss: 0.6933154
[Epoch 13] ogbg-molbace: 0.526007 val loss: 0.696821
[Epoch 13] ogbg-molbace: 0.466701 test loss: 0.697060
[Epoch 14; Iter     7/   41] train: loss: 0.6868996
[Epoch 14; Iter    37/   41] train: loss: 0.6904666
[Epoch 14] ogbg-molbace: 0.529304 val loss: 0.697591
[Epoch 14] ogbg-molbace: 0.466006 test loss: 0.697458
[Epoch 15; Iter    26/   41] train: loss: 0.6860691
[Epoch 15] ogbg-molbace: 0.531868 val loss: 0.698654
[Epoch 15] ogbg-molbace: 0.458355 test loss: 0.698198
[Epoch 16; Iter    15/   41] train: loss: 0.6902760
[Epoch 16] ogbg-molbace: 0.528938 val loss: 0.699825
[Epoch 16] ogbg-molbace: 0.479569 test loss: 0.698691
[Epoch 17; Iter     4/   41] train: loss: 0.6872615
[Epoch 17; Iter    34/   41] train: loss: 0.6916454
[Epoch 17] ogbg-molbace: 0.617949 val loss: 0.693574
[Epoch 17] ogbg-molbace: 0.636063 test loss: 0.697725
[Epoch 18; Iter    23/   41] train: loss: 0.6777231
[Epoch 18] ogbg-molbace: 0.742125 val loss: 0.699685
[Epoch 18] ogbg-molbace: 0.732394 test loss: 0.698114
[Epoch 19; Iter    12/   41] train: loss: 0.6326268
[Epoch 19] ogbg-molbace: 0.681685 val loss: 0.671019
[Epoch 19] ogbg-molbace: 0.788906 test loss: 0.638630
[Epoch 20; Iter     1/   41] train: loss: 0.5702552
[Epoch 20; Iter    31/   41] train: loss: 0.5351404
[Epoch 20] ogbg-molbace: 0.621612 val loss: 0.882496
[Epoch 20] ogbg-molbace: 0.699704 test loss: 0.739904
[Epoch 21; Iter    20/   41] train: loss: 0.5623946
[Epoch 21] ogbg-molbace: 0.617216 val loss: 0.876717
[Epoch 21] ogbg-molbace: 0.707703 test loss: 0.928901
[Epoch 22; Iter     9/   41] train: loss: 0.5133814
[Epoch 22; Iter    39/   41] train: loss: 0.4075597
[Epoch 22] ogbg-molbace: 0.721612 val loss: 0.833885
[Epoch 22] ogbg-molbace: 0.792036 test loss: 0.695066
[Epoch 23; Iter    28/   41] train: loss: 0.4052437
[Epoch 23] ogbg-molbace: 0.730769 val loss: 0.731228
[Epoch 23] ogbg-molbace: 0.800904 test loss: 0.732020
[Epoch 24; Iter    17/   41] train: loss: 0.3993594
[Epoch 24] ogbg-molbace: 0.695604 val loss: 0.741853
[Epoch 24] ogbg-molbace: 0.754651 test loss: 0.869904
[Epoch 25; Iter     6/   41] train: loss: 0.4191405
[Epoch 25; Iter    36/   41] train: loss: 0.3267623
[Epoch 25] ogbg-molbace: 0.723077 val loss: 0.647056
[Epoch 25] ogbg-molbace: 0.817945 test loss: 0.655226
[Epoch 26; Iter    25/   41] train: loss: 0.4697115
[Epoch 26] ogbg-molbace: 0.713187 val loss: 0.642928
[Epoch 26] ogbg-molbace: 0.750652 test loss: 0.733972
[Epoch 27; Iter    14/   41] train: loss: 0.6136090
[Epoch 27] ogbg-molbace: 0.677289 val loss: 0.804564
[Epoch 27] ogbg-molbace: 0.764041 test loss: 0.824575
[Epoch 28; Iter     3/   41] train: loss: 0.4757843
[Epoch 28; Iter    33/   41] train: loss: 0.4417647
[Epoch 28] ogbg-molbace: 0.661905 val loss: 0.913401
[Epoch 28] ogbg-molbace: 0.809598 test loss: 0.880841
[Epoch 29; Iter    22/   41] train: loss: 0.4167109
[Epoch 29] ogbg-molbace: 0.742857 val loss: 0.795708
[Epoch 29] ogbg-molbace: 0.793775 test loss: 0.782244
[Epoch 30; Iter    11/   41] train: loss: 0.3964409
[Epoch 30; Iter    41/   41] train: loss: 0.5273265
[Epoch 30] ogbg-molbace: 0.730403 val loss: 0.715148
[Epoch 30] ogbg-molbace: 0.798470 test loss: 0.822592
[Epoch 31; Iter    30/   41] train: loss: 0.4081997
[Epoch 31] ogbg-molbace: 0.636264 val loss: 1.030670
[Epoch 31] ogbg-molbace: 0.745957 test loss: 1.247607
[Epoch 32; Iter    19/   41] train: loss: 0.4636970
[Epoch 32] ogbg-molbace: 0.728205 val loss: 0.950283
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.6/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.6_5_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.6
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6952964
[Epoch 1] ogbg-molbace: 0.521648 val loss: 0.694292
[Epoch 1] ogbg-molbace: 0.417937 test loss: 0.692987
[Epoch 2; Iter    29/   31] train: loss: 0.6986051
[Epoch 2] ogbg-molbace: 0.482820 val loss: 0.696740
[Epoch 2] ogbg-molbace: 0.387029 test loss: 0.693470
[Epoch 3; Iter    28/   31] train: loss: 0.6936588
[Epoch 3] ogbg-molbace: 0.529196 val loss: 0.697891
[Epoch 3] ogbg-molbace: 0.406759 test loss: 0.693582
[Epoch 4; Iter    27/   31] train: loss: 0.6934481
[Epoch 4] ogbg-molbace: 0.558093 val loss: 0.697887
[Epoch 4] ogbg-molbace: 0.420874 test loss: 0.693133
[Epoch 5; Iter    26/   31] train: loss: 0.6957688
[Epoch 5] ogbg-molbace: 0.541410 val loss: 0.698142
[Epoch 5] ogbg-molbace: 0.412992 test loss: 0.693388
[Epoch 6; Iter    25/   31] train: loss: 0.6953778
[Epoch 6] ogbg-molbace: 0.558391 val loss: 0.697618
[Epoch 6] ogbg-molbace: 0.414692 test loss: 0.693378
[Epoch 7; Iter    24/   31] train: loss: 0.6950290
[Epoch 7] ogbg-molbace: 0.551142 val loss: 0.697471
[Epoch 7] ogbg-molbace: 0.425252 test loss: 0.693188
[Epoch 8; Iter    23/   31] train: loss: 0.6944832
[Epoch 8] ogbg-molbace: 0.551738 val loss: 0.697267
[Epoch 8] ogbg-molbace: 0.412889 test loss: 0.693390
[Epoch 9; Iter    22/   31] train: loss: 0.6982285
[Epoch 9] ogbg-molbace: 0.552036 val loss: 0.697620
[Epoch 9] ogbg-molbace: 0.415156 test loss: 0.693391
[Epoch 10; Iter    21/   31] train: loss: 0.6950737
[Epoch 10] ogbg-molbace: 0.547170 val loss: 0.697119
[Epoch 10] ogbg-molbace: 0.421080 test loss: 0.693379
[Epoch 11; Iter    20/   31] train: loss: 0.6977894
[Epoch 11] ogbg-molbace: 0.563158 val loss: 0.696261
[Epoch 11] ogbg-molbace: 0.422059 test loss: 0.693514
[Epoch 12; Iter    19/   31] train: loss: 0.6996489
[Epoch 12] ogbg-molbace: 0.572592 val loss: 0.696537
[Epoch 12] ogbg-molbace: 0.424531 test loss: 0.693299
[Epoch 13; Iter    18/   31] train: loss: 0.6939116
[Epoch 13] ogbg-molbace: 0.571003 val loss: 0.696266
[Epoch 13] ogbg-molbace: 0.425407 test loss: 0.693354
[Epoch 14; Iter    17/   31] train: loss: 0.6934553
[Epoch 14] ogbg-molbace: 0.576564 val loss: 0.696061
[Epoch 14] ogbg-molbace: 0.420565 test loss: 0.693443
[Epoch 15; Iter    16/   31] train: loss: 0.6926510
[Epoch 15] ogbg-molbace: 0.559285 val loss: 0.696396
[Epoch 15] ogbg-molbace: 0.426283 test loss: 0.693379
[Epoch 16; Iter    15/   31] train: loss: 0.6946899
[Epoch 16] ogbg-molbace: 0.562066 val loss: 0.695232
[Epoch 16] ogbg-molbace: 0.431434 test loss: 0.693557
[Epoch 17; Iter    14/   31] train: loss: 0.6947636
[Epoch 17] ogbg-molbace: 0.571797 val loss: 0.694819
[Epoch 17] ogbg-molbace: 0.427210 test loss: 0.693717
[Epoch 18; Iter    13/   31] train: loss: 0.6922098
[Epoch 18] ogbg-molbace: 0.569712 val loss: 0.695081
[Epoch 18] ogbg-molbace: 0.424737 test loss: 0.693754
[Epoch 19; Iter    12/   31] train: loss: 0.6918404
[Epoch 19] ogbg-molbace: 0.579146 val loss: 0.694823
[Epoch 19] ogbg-molbace: 0.447095 test loss: 0.693325
[Epoch 20; Iter    11/   31] train: loss: 0.6926591
[Epoch 20] ogbg-molbace: 0.581430 val loss: 0.694320
[Epoch 20] ogbg-molbace: 0.437822 test loss: 0.693646
[Epoch 21; Iter    10/   31] train: loss: 0.6936053
[Epoch 21] ogbg-molbace: 0.593843 val loss: 0.693172
[Epoch 21] ogbg-molbace: 0.458685 test loss: 0.693431
[Epoch 22; Iter     9/   31] train: loss: 0.6942869
[Epoch 22] ogbg-molbace: 0.602582 val loss: 0.693404
[Epoch 22] ogbg-molbace: 0.464867 test loss: 0.693246
[Epoch 23; Iter     8/   31] train: loss: 0.6887822
[Epoch 23] ogbg-molbace: 0.567130 val loss: 0.678863
[Epoch 23] ogbg-molbace: 0.704925 test loss: 0.674890
[Epoch 24; Iter     7/   31] train: loss: 0.6468097
[Epoch 24] ogbg-molbace: 0.672989 val loss: 0.624984
[Epoch 24] ogbg-molbace: 0.781784 test loss: 0.661009
[Epoch 25; Iter     6/   31] train: loss: 0.6535994
[Epoch 25] ogbg-molbace: 0.688779 val loss: 0.580273
[Epoch 25] ogbg-molbace: 0.772666 test loss: 0.645367
[Epoch 26; Iter     5/   31] train: loss: 0.5456930
[Epoch 26] ogbg-molbace: 0.612612 val loss: 0.612215
[Epoch 26] ogbg-molbace: 0.714197 test loss: 0.652030
[Epoch 27; Iter     4/   31] train: loss: 0.5262997
[Epoch 27] ogbg-molbace: 0.733466 val loss: 0.552245
[Epoch 27] ogbg-molbace: 0.742170 test loss: 0.646665
[Epoch 28; Iter     3/   31] train: loss: 0.5276225
[Epoch 28] ogbg-molbace: 0.755909 val loss: 0.624387
[Epoch 28] ogbg-molbace: 0.719864 test loss: 0.607369
[Epoch 29; Iter     2/   31] train: loss: 0.4897675
[Epoch 29] ogbg-molbace: 0.748064 val loss: 0.409269
[Epoch 29] ogbg-molbace: 0.770863 test loss: 0.736484
[Epoch 30; Iter     1/   31] train: loss: 0.4290884
[Epoch 30; Iter    31/   31] train: loss: 0.5020362
[Epoch 30] ogbg-molbace: 0.692949 val loss: 0.525874
[Epoch 30] ogbg-molbace: 0.748145 test loss: 0.663128
[Epoch 31; Iter    30/   31] train: loss: 0.5534670
[Epoch 31] ogbg-molbace: 0.741609 val loss: 0.489065
[Epoch 31] ogbg-molbace: 0.760664 test loss: 0.647138
[Epoch 32; Iter    29/   31] train: loss: 0.5944228
[Epoch 32] ogbg-molbace: 0.757498 val loss: 0.432787
[Epoch 32] ogbg-molbace: 0.741346 test loss: 0.743155
[Epoch 33; Iter    28/   31] train: loss: 0.4166454
[Epoch 33] ogbg-molbace: 0.785899 val loss: 0.454912
[Epoch 33] ogbg-molbace: 0.784566 test loss: 0.644500
[Epoch 34; Iter    27/   31] train: loss: 0.4568602
[Epoch 34] ogbg-molbace: 0.773386 val loss: 0.436173
[Epoch 34] ogbg-molbace: 0.783536 test loss: 0.718522
[Epoch 35; Iter    26/   31] train: loss: 0.4626994
[Epoch 35] ogbg-molbace: 0.778252 val loss: 0.532762
[Epoch 35] ogbg-molbace: 0.774366 test loss: 0.626302
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bace/scaff/train_prop=0.7/PNA_ogbg-molbace_3DInfomax_bace_scaff=0.7_6_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_scaff=0.7
logdir: runs/split/3DInfomax/bace/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6930206
[Epoch 1] ogbg-molbace: 0.526110 val loss: 0.692950
[Epoch 1] ogbg-molbace: 0.474269 test loss: 0.693650
[Epoch 2; Iter    24/   36] train: loss: 0.6883060
[Epoch 2] ogbg-molbace: 0.528964 val loss: 0.693217
[Epoch 2] ogbg-molbace: 0.503070 test loss: 0.694172
[Epoch 3; Iter    18/   36] train: loss: 0.6912462
[Epoch 3] ogbg-molbace: 0.531078 val loss: 0.692835
[Epoch 3] ogbg-molbace: 0.507042 test loss: 0.694620
[Epoch 4; Iter    12/   36] train: loss: 0.6899112
[Epoch 4] ogbg-molbace: 0.533298 val loss: 0.692872
[Epoch 4] ogbg-molbace: 0.500181 test loss: 0.694718
[Epoch 5; Iter     6/   36] train: loss: 0.6916763
[Epoch 5; Iter    36/   36] train: loss: 0.6927574
[Epoch 5] ogbg-molbace: 0.535835 val loss: 0.692046
[Epoch 5] ogbg-molbace: 0.502257 test loss: 0.695289
[Epoch 6; Iter    30/   36] train: loss: 0.6889243
[Epoch 6] ogbg-molbace: 0.535201 val loss: 0.691919
[Epoch 6] ogbg-molbace: 0.510383 test loss: 0.695209
[Epoch 7; Iter    24/   36] train: loss: 0.6928823
[Epoch 7] ogbg-molbace: 0.532770 val loss: 0.691802
[Epoch 7] ogbg-molbace: 0.496118 test loss: 0.695632
[Epoch 8; Iter    18/   36] train: loss: 0.6927649
[Epoch 8] ogbg-molbace: 0.533510 val loss: 0.691593
[Epoch 8] ogbg-molbace: 0.503160 test loss: 0.695659
[Epoch 9; Iter    12/   36] train: loss: 0.6934506
[Epoch 9] ogbg-molbace: 0.532664 val loss: 0.691387
[Epoch 9] ogbg-molbace: 0.514446 test loss: 0.695598
[Epoch 10; Iter     6/   36] train: loss: 0.6922718
[Epoch 10; Iter    36/   36] train: loss: 0.6876671
[Epoch 10] ogbg-molbace: 0.531924 val loss: 0.691056
[Epoch 10] ogbg-molbace: 0.510473 test loss: 0.696110
[Epoch 11; Iter    30/   36] train: loss: 0.6933448
[Epoch 11] ogbg-molbace: 0.535201 val loss: 0.690488
[Epoch 11] ogbg-molbace: 0.517064 test loss: 0.696285
[Epoch 12; Iter    24/   36] train: loss: 0.6910121
[Epoch 12] ogbg-molbace: 0.536998 val loss: 0.689860
[Epoch 12] ogbg-molbace: 0.510563 test loss: 0.696794
[Epoch 13; Iter    18/   36] train: loss: 0.6867887
[Epoch 13] ogbg-molbace: 0.538372 val loss: 0.689268
[Epoch 13] ogbg-molbace: 0.518057 test loss: 0.697194
[Epoch 14; Iter    12/   36] train: loss: 0.6902426
[Epoch 14] ogbg-molbace: 0.535307 val loss: 0.688737
[Epoch 14] ogbg-molbace: 0.519863 test loss: 0.697530
[Epoch 15; Iter     6/   36] train: loss: 0.6929234
[Epoch 15; Iter    36/   36] train: loss: 0.6855806
[Epoch 15] ogbg-molbace: 0.533827 val loss: 0.687879
[Epoch 15] ogbg-molbace: 0.513723 test loss: 0.698480
[Epoch 16; Iter    30/   36] train: loss: 0.6874325
[Epoch 16] ogbg-molbace: 0.539112 val loss: 0.687570
[Epoch 16] ogbg-molbace: 0.529704 test loss: 0.698277
[Epoch 17; Iter    24/   36] train: loss: 0.6926543
[Epoch 17] ogbg-molbace: 0.536575 val loss: 0.686333
[Epoch 17] ogbg-molbace: 0.525731 test loss: 0.699399
[Epoch 18; Iter    18/   36] train: loss: 0.6923649
[Epoch 18] ogbg-molbace: 0.536998 val loss: 0.685808
[Epoch 18] ogbg-molbace: 0.538371 test loss: 0.699411
[Epoch 19; Iter    12/   36] train: loss: 0.6882735
[Epoch 19] ogbg-molbace: 0.538266 val loss: 0.685246
[Epoch 19] ogbg-molbace: 0.539274 test loss: 0.699821
[Epoch 20; Iter     6/   36] train: loss: 0.6819799
[Epoch 20; Iter    36/   36] train: loss: 0.6791420
[Epoch 20] ogbg-molbace: 0.615962 val loss: 0.679052
[Epoch 20] ogbg-molbace: 0.787920 test loss: 0.668345
[Epoch 21; Iter    30/   36] train: loss: 0.6136693
[Epoch 21] ogbg-molbace: 0.594080 val loss: 0.648500
[Epoch 21] ogbg-molbace: 0.782412 test loss: 0.671370
[Epoch 22; Iter    24/   36] train: loss: 0.6612488
[Epoch 22] ogbg-molbace: 0.686786 val loss: 0.603286
[Epoch 22] ogbg-molbace: 0.774919 test loss: 0.659675
[Epoch 23; Iter    18/   36] train: loss: 0.6057537
[Epoch 23] ogbg-molbace: 0.616808 val loss: 0.693842
[Epoch 23] ogbg-molbace: 0.807151 test loss: 0.565873
[Epoch 24; Iter    12/   36] train: loss: 0.4242663
[Epoch 24] ogbg-molbace: 0.656342 val loss: 0.575096
[Epoch 24] ogbg-molbace: 0.801101 test loss: 0.659510
[Epoch 25; Iter     6/   36] train: loss: 0.4126260
[Epoch 25; Iter    36/   36] train: loss: 0.6235812
[Epoch 25] ogbg-molbace: 0.683510 val loss: 0.708845
[Epoch 25] ogbg-molbace: 0.755778 test loss: 0.601747
[Epoch 26; Iter    30/   36] train: loss: 0.5829173
[Epoch 26] ogbg-molbace: 0.689429 val loss: 0.556987
[Epoch 26] ogbg-molbace: 0.824305 test loss: 0.635049
[Epoch 27; Iter    24/   36] train: loss: 0.4031606
[Epoch 27] ogbg-molbace: 0.702008 val loss: 0.602216
[Epoch 27] ogbg-molbace: 0.829000 test loss: 0.580177
[Epoch 28; Iter    18/   36] train: loss: 0.3420616
[Epoch 28] ogbg-molbace: 0.702114 val loss: 0.686811
[Epoch 28] ogbg-molbace: 0.815096 test loss: 0.547819
[Epoch 29; Iter    12/   36] train: loss: 0.5400060
[Epoch 29] ogbg-molbace: 0.712262 val loss: 0.636232
[Epoch 29] ogbg-molbace: 0.770856 test loss: 0.592134
[Epoch 30; Iter     6/   36] train: loss: 0.6238309
[Epoch 30; Iter    36/   36] train: loss: 0.9181849
[Epoch 30] ogbg-molbace: 0.695666 val loss: 0.556365
[Epoch 30] ogbg-molbace: 0.831257 test loss: 0.644219
[Epoch 31; Iter    30/   36] train: loss: 0.3149501
[Epoch 31] ogbg-molbace: 0.699154 val loss: 0.607793
[Epoch 31] ogbg-molbace: 0.816089 test loss: 0.688990
[Epoch 32; Iter    24/   36] train: loss: 0.3833452
[Epoch 32] ogbg-molbace: 0.696934 val loss: 0.612215
[Epoch 32] ogbg-molbace: 0.753973 test loss: 0.740635
[Epoch 33; Iter    18/   36] train: loss: 0.4042786
[Epoch 33] ogbg-molbace: 0.735412 val loss: 0.568289
[Epoch 33] ogbg-molbace: 0.823312 test loss: 0.612995
[Epoch 34; Iter    12/   36] train: loss: 0.4179124
[Epoch 34] ogbg-molbace: 0.704334 val loss: 0.525533
[Epoch 34] ogbg-molbace: 0.812568 test loss: 0.942430
[Epoch 35; Iter     6/   36] train: loss: 0.7657907
[Epoch 35; Iter    36/   36] train: loss: 0.6239628
[Epoch 35] ogbg-molbace: 0.706977 val loss: 0.570870
[Epoch 35] ogbg-molbace: 0.826111 test loss: 0.614179
[Epoch 36; Iter    30/   36] train: loss: 0.6165490
[Epoch 36] ogbg-molbace: 0.711522 val loss: 0.500632
[Epoch 36] ogbg-molbace: 0.796768 test loss: 0.908600
[Epoch 37; Iter    24/   36] train: loss: 0.5951968
[Epoch 37] ogbg-molbace: 0.695455 val loss: 0.656902
[Epoch 37] ogbg-molbace: 0.821596 test loss: 0.632181
[Epoch 38; Iter    18/   36] train: loss: 0.5080923
[Epoch 38] ogbg-molbace: 0.716173 val loss: 0.547339
[Epoch 38] ogbg-molbace: 0.836403 test loss: 0.674942
[Epoch 39; Iter    12/   36] train: loss: 0.3075548
[Epoch 39] ogbg-molbace: 0.726110 val loss: 0.536697
[Epoch 39] ogbg-molbace: 0.856537 test loss: 0.905408
[Epoch 40; Iter     6/   36] train: loss: 0.3349834
[Epoch 40; Iter    36/   36] train: loss: 0.1477246
[Epoch 40] ogbg-molbace: 0.716385 val loss: 0.750817
[Epoch 40] ogbg-molbace: 0.828458 test loss: 0.536075
[Epoch 41; Iter    30/   36] train: loss: 0.4470895
[Epoch 41] ogbg-molbace: 0.712896 val loss: 0.606242
[Epoch 41] ogbg-molbace: 0.800831 test loss: 0.724559
[Epoch 42; Iter    24/   36] train: loss: 0.2834064
[Epoch 42] ogbg-molbace: 0.744926 val loss: 0.561750
[Epoch 42] ogbg-molbace: 0.825208 test loss: 0.602233
[Epoch 43; Iter    18/   36] train: loss: 0.4206548
[Epoch 43] ogbg-molbace: 0.722516 val loss: 0.684962
[Epoch 43] ogbg-molbace: 0.787288 test loss: 0.737035
[Epoch 44; Iter    12/   36] train: loss: 0.2698658
[Epoch 44] ogbg-molbace: 0.732770 val loss: 0.517820
[Epoch 44] ogbg-molbace: 0.839021 test loss: 0.876979
[Epoch 45; Iter     6/   36] train: loss: 0.2737536
[Epoch 45; Iter    36/   36] train: loss: 0.5989354
[Epoch 45] ogbg-molbace: 0.718393 val loss: 1.416259
[Epoch 45] ogbg-molbace: 0.844168 test loss: 0.458959
[Epoch 46; Iter    30/   36] train: loss: 0.4245868
[Epoch 46] ogbg-molbace: 0.702748 val loss: 0.629109
[Epoch 46] ogbg-molbace: 0.847689 test loss: 0.713064
[Epoch 47; Iter    24/   36] train: loss: 0.3724858
[Epoch 47] ogbg-molbace: 0.769556 val loss: 0.775968
[Epoch 47] ogbg-molbace: 0.855724 test loss: 0.447745
[Epoch 48; Iter    18/   36] train: loss: 0.5033616
[Epoch 48] ogbg-molbace: 0.743023 val loss: 0.673647
[Epoch 48] ogbg-molbace: 0.801282 test loss: 0.689732
[Epoch 49; Iter    12/   36] train: loss: 0.3403949
[Epoch 49] ogbg-molbace: 0.761099 val loss: 0.548216
[Epoch 49] ogbg-molbace: 0.869357 test loss: 0.592208
[Epoch 50; Iter     6/   36] train: loss: 0.3827305
[Epoch 50; Iter    36/   36] train: loss: 0.2119045
[Epoch 50] ogbg-molbace: 0.768710 val loss: 0.756849
[Epoch 50] ogbg-molbace: 0.858523 test loss: 0.431957
[Epoch 51; Iter    30/   36] train: loss: 0.2921816
[Epoch 51] ogbg-molbace: 0.753488 val loss: 0.560218
[Epoch 51] ogbg-molbace: 0.849043 test loss: 0.652696
[Epoch 52; Iter    24/   36] train: loss: 0.4830631
[Epoch 52] ogbg-molbace: 0.738795 val loss: 0.698953
[Epoch 52] ogbg-molbace: 0.817714 test loss: 0.639561
[Epoch 53; Iter    18/   36] train: loss: 0.2402019
[Epoch 53] ogbg-molbace: 0.754440 val loss: 0.643478
[Epoch 53] ogbg-molbace: 0.848682 test loss: 0.669719
[Epoch 54; Iter    12/   36] train: loss: 0.1803230
[Epoch 54] ogbg-molbace: 0.714059 val loss: 0.654598
[Epoch 54] ogbg-molbace: 0.849494 test loss: 0.540188
[Epoch 55; Iter     6/   36] train: loss: 0.3407904
[Epoch 55; Iter    36/   36] train: loss: 0.5255784
[Epoch 55] ogbg-molbace: 0.682452 val loss: 0.678854
[Epoch 55] ogbg-molbace: 0.853377 test loss: 0.725024
[Epoch 56; Iter    30/   36] train: loss: 0.2381066
[Epoch 56] ogbg-molbace: 0.741543 val loss: 0.668985
[Epoch 56] ogbg-molbace: 0.833694 test loss: 0.660654
[Epoch 57; Iter    24/   36] train: loss: 0.1059639
[Epoch 57] ogbg-molbace: 0.777484 val loss: 0.566486
[Epoch 57] ogbg-molbace: 0.868815 test loss: 0.549933
[Epoch 58; Iter    18/   36] train: loss: 0.3117779
[Epoch 58] ogbg-molbace: 0.698943 val loss: 0.668675
[Epoch 58] ogbg-molbace: 0.827465 test loss: 0.968909
[Epoch 59; Iter    12/   36] train: loss: 0.2172886
[Epoch 59] ogbg-molbace: 0.788689 val loss: 0.579541
[Epoch 59] ogbg-molbace: 0.839292 test loss: 0.669817
[Epoch 60; Iter     6/   36] train: loss: 0.1336317
[Epoch 60; Iter    36/   36] train: loss: 0.1571282
[Epoch 60] ogbg-molbace: 0.774207 val loss: 0.671084
[Epoch 60] ogbg-molbace: 0.804171 test loss: 0.695812
[Epoch 61; Iter    30/   36] train: loss: 0.2175963
[Epoch 61] ogbg-molbace: 0.754123 val loss: 0.699681
[Epoch 61] ogbg-molbace: 0.843265 test loss: 0.655933
[Epoch 62; Iter    24/   36] train: loss: 0.3105855
[Epoch 62] ogbg-molbace: 0.751057 val loss: 0.629970
[Epoch 62] ogbg-molbace: 0.854731 test loss: 0.644088
[Epoch 63; Iter    18/   36] train: loss: 0.1938933
[Epoch 63] ogbg-molbace: 0.769979 val loss: 0.892749
[Epoch 63] ogbg-molbace: 0.830896 test loss: 0.552509
[Epoch 64; Iter    12/   36] train: loss: 0.1093670
[Epoch 64] ogbg-molbace: 0.763848 val loss: 0.670805
[Epoch 64] ogbg-molbace: 0.817985 test loss: 0.985023
[Epoch 65; Iter     6/   36] train: loss: 0.4038880
[Epoch 65; Iter    36/   36] train: loss: 0.3079076
[Epoch 65] ogbg-molbace: 0.763319 val loss: 0.665248
[Epoch 65] ogbg-molbace: 0.839744 test loss: 0.785132
[Epoch 66; Iter    30/   36] train: loss: 0.3992079
[Epoch 66] ogbg-molbace: 0.794397 val loss: 0.600251
[Epoch 66] ogbg-molbace: 0.852113 test loss: 0.625338
[Epoch 67; Iter    24/   36] train: loss: 0.1509766
[Epoch 67] ogbg-molbace: 0.777378 val loss: 0.706812
[Epoch 67] ogbg-molbace: 0.844890 test loss: 0.665178
[Epoch 68; Iter    18/   36] train: loss: 0.3638147
[Epoch 68] ogbg-molbace: 0.767865 val loss: 0.785918
[Epoch 68] ogbg-molbace: 0.841549 test loss: 0.579659
[Epoch 69; Iter    12/   36] train: loss: 0.1505098
[Epoch 69] ogbg-molbace: 0.816385 val loss: 0.614821
[Epoch 69] ogbg-molbace: 0.859787 test loss: 0.782836
[Epoch 70; Iter     6/   36] train: loss: 0.3047948
[Epoch 70; Iter    36/   36] train: loss: 1.4651674
[Epoch 70] ogbg-molbace: 0.759725 val loss: 0.742646
[Epoch 70] ogbg-molbace: 0.835500 test loss: 1.055970
[Epoch 71; Iter    30/   36] train: loss: 0.4959436
[Epoch 71] ogbg-molbace: 0.796300 val loss: 0.640545
[Epoch 71] ogbg-molbace: 0.856717 test loss: 0.599395
[Epoch 72; Iter    24/   36] train: loss: 0.1806396
[Epoch 72] ogbg-molbace: 0.805180 val loss: 0.639792
[Epoch 72] ogbg-molbace: 0.859787 test loss: 0.719776
[Epoch 73; Iter    18/   36] train: loss: 0.2960913
[Epoch 73] ogbg-molbace: 0.759514 val loss: 0.758654
[Epoch 73] ogbg-molbace: 0.811394 test loss: 1.122154
[Epoch 74; Iter    12/   36] train: loss: 0.1834296
[Epoch 74] ogbg-molbace: 0.793023 val loss: 0.836791
[Epoch 74] ogbg-molbace: 0.865114 test loss: 0.532983
[Epoch 75; Iter     6/   36] train: loss: 0.1769816
[Epoch 75; Iter    36/   36] train: loss: 0.9180630
[Epoch 75] ogbg-molbace: 0.788584 val loss: 0.743704
[Epoch 75] ogbg-molbace: 0.863127 test loss: 0.747104
[Epoch 76; Iter    30/   36] train: loss: 0.2765039
[Epoch 76] ogbg-molbace: 0.792812 val loss: 0.785906
[Epoch 76] ogbg-molbace: 0.815547 test loss: 0.984512
[Epoch 77; Iter    24/   36] train: loss: 0.1169173
[Epoch 77] ogbg-molbace: 0.767230 val loss: 0.740290
[Epoch 77] ogbg-molbace: 0.820964 test loss: 0.936491
[Epoch 78; Iter    18/   36] train: loss: 0.1977764
[Epoch 78] ogbg-molbace: 0.776004 val loss: 0.907241
[Epoch 78] ogbg-molbace: 0.856717 test loss: 0.622099
[Epoch 79; Iter    12/   36] train: loss: 0.1037684
[Epoch 79] ogbg-molbace: 0.802854 val loss: 0.636366
[Epoch 79] ogbg-molbace: 0.848411 test loss: 1.077628
[Epoch 80; Iter     6/   36] train: loss: 0.2759010
[Epoch 80; Iter    36/   36] train: loss: 0.0915566
[Epoch 80] ogbg-molbace: 0.780233 val loss: 0.683652
[Epoch 80] ogbg-molbace: 0.836584 test loss: 0.813545
[Epoch 81; Iter    30/   36] train: loss: 0.0883775
[Epoch 81] ogbg-molbace: 0.749789 val loss: 0.727565
[Epoch 81] ogbg-molbace: 0.827465 test loss: 1.008767
[Epoch 82; Iter    24/   36] train: loss: 0.0631347
[Epoch 32] ogbg-molbace: 0.788732 test loss: 0.726849
[Epoch 33; Iter     8/   41] train: loss: 0.3832654
[Epoch 33; Iter    38/   41] train: loss: 0.4842205
[Epoch 33] ogbg-molbace: 0.705128 val loss: 0.933190
[Epoch 33] ogbg-molbace: 0.801774 test loss: 0.943386
[Epoch 34; Iter    27/   41] train: loss: 0.3651215
[Epoch 34] ogbg-molbace: 0.685348 val loss: 0.979617
[Epoch 34] ogbg-molbace: 0.763693 test loss: 0.827178
[Epoch 35; Iter    16/   41] train: loss: 0.3905387
[Epoch 35] ogbg-molbace: 0.671429 val loss: 0.975925
[Epoch 35] ogbg-molbace: 0.718658 test loss: 1.088127
[Epoch 36; Iter     5/   41] train: loss: 0.4010687
[Epoch 36; Iter    35/   41] train: loss: 0.3446886
[Epoch 36] ogbg-molbace: 0.743956 val loss: 0.908092
[Epoch 36] ogbg-molbace: 0.751695 test loss: 0.930742
[Epoch 37; Iter    24/   41] train: loss: 0.2595874
[Epoch 37] ogbg-molbace: 0.684249 val loss: 0.750459
[Epoch 37] ogbg-molbace: 0.798991 test loss: 0.597572
[Epoch 38; Iter    13/   41] train: loss: 0.3473659
[Epoch 38] ogbg-molbace: 0.708425 val loss: 1.395978
[Epoch 38] ogbg-molbace: 0.724396 test loss: 1.507499
[Epoch 39; Iter     2/   41] train: loss: 0.5945112
[Epoch 39; Iter    32/   41] train: loss: 0.4152804
[Epoch 39] ogbg-molbace: 0.692308 val loss: 1.415171
[Epoch 39] ogbg-molbace: 0.814293 test loss: 1.483020
[Epoch 40; Iter    21/   41] train: loss: 0.3300152
[Epoch 40] ogbg-molbace: 0.678388 val loss: 0.928880
[Epoch 40] ogbg-molbace: 0.787515 test loss: 0.774711
[Epoch 41; Iter    10/   41] train: loss: 0.3741730
[Epoch 41; Iter    40/   41] train: loss: 0.2375068
[Epoch 41] ogbg-molbace: 0.691941 val loss: 1.107865
[Epoch 41] ogbg-molbace: 0.785776 test loss: 1.008120
[Epoch 42; Iter    29/   41] train: loss: 0.2957832
[Epoch 42] ogbg-molbace: 0.687912 val loss: 1.126412
[Epoch 42] ogbg-molbace: 0.797948 test loss: 1.133571
[Epoch 43; Iter    18/   41] train: loss: 0.2433480
[Epoch 43] ogbg-molbace: 0.709158 val loss: 1.085100
[Epoch 43] ogbg-molbace: 0.795166 test loss: 1.085499
[Epoch 44; Iter     7/   41] train: loss: 0.3656789
[Epoch 44; Iter    37/   41] train: loss: 0.3513231
[Epoch 44] ogbg-molbace: 0.723077 val loss: 0.984375
[Epoch 44] ogbg-molbace: 0.764041 test loss: 0.803306
[Epoch 45; Iter    26/   41] train: loss: 0.2155602
[Epoch 45] ogbg-molbace: 0.717216 val loss: 0.853199
[Epoch 45] ogbg-molbace: 0.799165 test loss: 0.629417
[Epoch 46; Iter    15/   41] train: loss: 0.2536557
[Epoch 46] ogbg-molbace: 0.666300 val loss: 1.152447
[Epoch 46] ogbg-molbace: 0.809424 test loss: 0.748674
[Epoch 47; Iter     4/   41] train: loss: 0.2039963
[Epoch 47; Iter    34/   41] train: loss: 0.2875747
[Epoch 47] ogbg-molbace: 0.742125 val loss: 0.998678
[Epoch 47] ogbg-molbace: 0.813598 test loss: 0.984375
[Epoch 48; Iter    23/   41] train: loss: 0.2232529
[Epoch 48] ogbg-molbace: 0.660073 val loss: 0.821822
[Epoch 48] ogbg-molbace: 0.753608 test loss: 1.153119
[Epoch 49; Iter    12/   41] train: loss: 0.3359037
[Epoch 49] ogbg-molbace: 0.672894 val loss: 1.461018
[Epoch 49] ogbg-molbace: 0.758129 test loss: 1.327626
[Epoch 50; Iter     1/   41] train: loss: 0.4244674
[Epoch 50; Iter    31/   41] train: loss: 0.2925141
[Epoch 50] ogbg-molbace: 0.734799 val loss: 1.033086
[Epoch 50] ogbg-molbace: 0.804208 test loss: 0.774300
[Epoch 51; Iter    20/   41] train: loss: 0.4316849
[Epoch 51] ogbg-molbace: 0.747619 val loss: 1.017611
[Epoch 51] ogbg-molbace: 0.812207 test loss: 1.102093
[Epoch 52; Iter     9/   41] train: loss: 0.3199908
[Epoch 52; Iter    39/   41] train: loss: 0.2368694
[Epoch 52] ogbg-molbace: 0.746520 val loss: 1.256066
[Epoch 52] ogbg-molbace: 0.812902 test loss: 1.034200
[Epoch 53; Iter    28/   41] train: loss: 0.1413382
[Epoch 53] ogbg-molbace: 0.708059 val loss: 1.302282
[Epoch 53] ogbg-molbace: 0.821770 test loss: 1.133239
[Epoch 54; Iter    17/   41] train: loss: 0.4001735
[Epoch 54] ogbg-molbace: 0.745788 val loss: 1.713097
[Epoch 54] ogbg-molbace: 0.772561 test loss: 1.687428
[Epoch 55; Iter     6/   41] train: loss: 0.2688181
[Epoch 55; Iter    36/   41] train: loss: 0.1427635
[Epoch 55] ogbg-molbace: 0.699634 val loss: 1.082155
[Epoch 55] ogbg-molbace: 0.818640 test loss: 0.960397
[Epoch 56; Iter    25/   41] train: loss: 0.2339723
[Epoch 56] ogbg-molbace: 0.685714 val loss: 1.498466
[Epoch 56] ogbg-molbace: 0.797600 test loss: 1.943075
[Epoch 57; Iter    14/   41] train: loss: 0.2064912
[Epoch 57] ogbg-molbace: 0.687546 val loss: 1.529169
[Epoch 57] ogbg-molbace: 0.792036 test loss: 1.549162
[Epoch 58; Iter     3/   41] train: loss: 0.2175649
[Epoch 58; Iter    33/   41] train: loss: 0.2069963
[Epoch 58] ogbg-molbace: 0.733333 val loss: 1.064345
[Epoch 58] ogbg-molbace: 0.815336 test loss: 1.143422
[Epoch 59; Iter    22/   41] train: loss: 0.4354151
[Epoch 59] ogbg-molbace: 0.665934 val loss: 1.379384
[Epoch 59] ogbg-molbace: 0.810642 test loss: 1.026939
[Epoch 60; Iter    11/   41] train: loss: 0.1479279
[Epoch 60; Iter    41/   41] train: loss: 0.0747455
[Epoch 60] ogbg-molbace: 0.672894 val loss: 1.506148
[Epoch 60] ogbg-molbace: 0.834464 test loss: 0.920140
[Epoch 61; Iter    30/   41] train: loss: 0.2201206
[Epoch 61] ogbg-molbace: 0.701099 val loss: 1.244033
[Epoch 61] ogbg-molbace: 0.801947 test loss: 1.181083
[Epoch 62; Iter    19/   41] train: loss: 0.1109815
[Epoch 62] ogbg-molbace: 0.758974 val loss: 1.038216
[Epoch 62] ogbg-molbace: 0.822292 test loss: 1.115837
[Epoch 63; Iter     8/   41] train: loss: 0.2119472
[Epoch 63; Iter    38/   41] train: loss: 0.1581065
[Epoch 63] ogbg-molbace: 0.781319 val loss: 0.925885
[Epoch 63] ogbg-molbace: 0.797253 test loss: 1.297360
[Epoch 64; Iter    27/   41] train: loss: 0.4927990
[Epoch 64] ogbg-molbace: 0.711722 val loss: 1.502382
[Epoch 64] ogbg-molbace: 0.838289 test loss: 1.678401
[Epoch 65; Iter    16/   41] train: loss: 0.1220332
[Epoch 65] ogbg-molbace: 0.754945 val loss: 0.793598
[Epoch 65] ogbg-molbace: 0.815163 test loss: 1.085405
[Epoch 66; Iter     5/   41] train: loss: 0.2080414
[Epoch 66; Iter    35/   41] train: loss: 0.3496262
[Epoch 66] ogbg-molbace: 0.795238 val loss: 1.205306
[Epoch 66] ogbg-molbace: 0.765954 test loss: 1.790112
[Epoch 67; Iter    24/   41] train: loss: 0.2191359
[Epoch 67] ogbg-molbace: 0.737729 val loss: 1.653788
[Epoch 67] ogbg-molbace: 0.825074 test loss: 1.082036
[Epoch 68; Iter    13/   41] train: loss: 0.1576910
[Epoch 68] ogbg-molbace: 0.700366 val loss: 1.396720
[Epoch 68] ogbg-molbace: 0.826987 test loss: 1.424028
[Epoch 69; Iter     2/   41] train: loss: 0.2142936
[Epoch 69; Iter    32/   41] train: loss: 0.3693712
[Epoch 69] ogbg-molbace: 0.783150 val loss: 1.154899
[Epoch 69] ogbg-molbace: 0.818988 test loss: 1.367091
[Epoch 70; Iter    21/   41] train: loss: 0.1982600
[Epoch 70] ogbg-molbace: 0.683516 val loss: 1.299209
[Epoch 70] ogbg-molbace: 0.834464 test loss: 1.587213
[Epoch 71; Iter    10/   41] train: loss: 0.2568849
[Epoch 71; Iter    40/   41] train: loss: 0.1592791
[Epoch 71] ogbg-molbace: 0.744322 val loss: 1.105303
[Epoch 71] ogbg-molbace: 0.816206 test loss: 1.536372
[Epoch 72; Iter    29/   41] train: loss: 0.3001247
[Epoch 72] ogbg-molbace: 0.722711 val loss: 1.393548
[Epoch 72] ogbg-molbace: 0.803686 test loss: 1.634611
[Epoch 73; Iter    18/   41] train: loss: 0.0725358
[Epoch 73] ogbg-molbace: 0.698168 val loss: 1.008573
[Epoch 73] ogbg-molbace: 0.773778 test loss: 1.532981
[Epoch 74; Iter     7/   41] train: loss: 0.2077112
[Epoch 74; Iter    37/   41] train: loss: 0.1953825
[Epoch 74] ogbg-molbace: 0.743590 val loss: 1.150337
[Epoch 74] ogbg-molbace: 0.818292 test loss: 1.235358
[Epoch 75; Iter    26/   41] train: loss: 0.1174216
[Epoch 75] ogbg-molbace: 0.757509 val loss: 0.874221
[Epoch 75] ogbg-molbace: 0.793427 test loss: 1.345725
[Epoch 76; Iter    15/   41] train: loss: 0.0443959
[Epoch 76] ogbg-molbace: 0.720513 val loss: 1.547908
[Epoch 76] ogbg-molbace: 0.819857 test loss: 1.552231
[Epoch 77; Iter     4/   41] train: loss: 0.0350523
[Epoch 77; Iter    34/   41] train: loss: 0.1308917
[Epoch 77] ogbg-molbace: 0.728571 val loss: 1.218285
[Epoch 77] ogbg-molbace: 0.837246 test loss: 1.901834
[Epoch 78; Iter    23/   41] train: loss: 0.3504587
[Epoch 32] ogbg-molbace: 0.775170 test loss: 0.811893
[Epoch 33; Iter     8/   41] train: loss: 0.3382284
[Epoch 33; Iter    38/   41] train: loss: 0.3682047
[Epoch 33] ogbg-molbace: 0.746154 val loss: 0.847032
[Epoch 33] ogbg-molbace: 0.814467 test loss: 0.757076
[Epoch 34; Iter    27/   41] train: loss: 0.3620946
[Epoch 34] ogbg-molbace: 0.709890 val loss: 1.114558
[Epoch 34] ogbg-molbace: 0.749609 test loss: 1.118184
[Epoch 35; Iter    16/   41] train: loss: 0.3798922
[Epoch 35] ogbg-molbace: 0.704396 val loss: 1.038317
[Epoch 35] ogbg-molbace: 0.741089 test loss: 0.693891
[Epoch 36; Iter     5/   41] train: loss: 0.3567378
[Epoch 36; Iter    35/   41] train: loss: 0.5003220
[Epoch 36] ogbg-molbace: 0.678022 val loss: 1.065687
[Epoch 36] ogbg-molbace: 0.768562 test loss: 1.116155
[Epoch 37; Iter    24/   41] train: loss: 0.3313085
[Epoch 37] ogbg-molbace: 0.684982 val loss: 0.929916
[Epoch 37] ogbg-molbace: 0.764389 test loss: 0.588042
[Epoch 38; Iter    13/   41] train: loss: 0.3775475
[Epoch 38] ogbg-molbace: 0.634799 val loss: 0.818262
[Epoch 38] ogbg-molbace: 0.745262 test loss: 1.069723
[Epoch 39; Iter     2/   41] train: loss: 0.3149311
[Epoch 39; Iter    32/   41] train: loss: 0.2517487
[Epoch 39] ogbg-molbace: 0.708425 val loss: 0.931368
[Epoch 39] ogbg-molbace: 0.828378 test loss: 0.697325
[Epoch 40; Iter    21/   41] train: loss: 0.2404693
[Epoch 40] ogbg-molbace: 0.595971 val loss: 1.086934
[Epoch 40] ogbg-molbace: 0.793775 test loss: 0.708940
[Epoch 41; Iter    10/   41] train: loss: 0.2414226
[Epoch 41; Iter    40/   41] train: loss: 0.4066496
[Epoch 41] ogbg-molbace: 0.623443 val loss: 1.948307
[Epoch 41] ogbg-molbace: 0.756564 test loss: 1.939168
[Epoch 42; Iter    29/   41] train: loss: 0.3747616
[Epoch 42] ogbg-molbace: 0.643223 val loss: 0.990289
[Epoch 42] ogbg-molbace: 0.793601 test loss: 0.947288
[Epoch 43; Iter    18/   41] train: loss: 0.2984340
[Epoch 43] ogbg-molbace: 0.705128 val loss: 0.788756
[Epoch 43] ogbg-molbace: 0.782299 test loss: 0.841320
[Epoch 44; Iter     7/   41] train: loss: 0.3220544
[Epoch 44; Iter    37/   41] train: loss: 0.3385560
[Epoch 44] ogbg-molbace: 0.696337 val loss: 0.897714
[Epoch 44] ogbg-molbace: 0.823683 test loss: 1.180371
[Epoch 45; Iter    26/   41] train: loss: 0.2793334
[Epoch 45] ogbg-molbace: 0.671062 val loss: 1.709869
[Epoch 45] ogbg-molbace: 0.805773 test loss: 1.129922
[Epoch 46; Iter    15/   41] train: loss: 0.2139599
[Epoch 46] ogbg-molbace: 0.730769 val loss: 0.993065
[Epoch 46] ogbg-molbace: 0.801426 test loss: 1.231348
[Epoch 47; Iter     4/   41] train: loss: 0.3321202
[Epoch 47; Iter    34/   41] train: loss: 0.4637654
[Epoch 47] ogbg-molbace: 0.698168 val loss: 1.054158
[Epoch 47] ogbg-molbace: 0.825769 test loss: 1.234707
[Epoch 48; Iter    23/   41] train: loss: 0.2408220
[Epoch 48] ogbg-molbace: 0.746886 val loss: 1.000617
[Epoch 48] ogbg-molbace: 0.784733 test loss: 0.968914
[Epoch 49; Iter    12/   41] train: loss: 0.2651985
[Epoch 49] ogbg-molbace: 0.724176 val loss: 1.213507
[Epoch 49] ogbg-molbace: 0.819857 test loss: 0.959285
[Epoch 50; Iter     1/   41] train: loss: 0.3327173
[Epoch 50; Iter    31/   41] train: loss: 0.2256432
[Epoch 50] ogbg-molbace: 0.692674 val loss: 1.113583
[Epoch 50] ogbg-molbace: 0.826813 test loss: 1.362158
[Epoch 51; Iter    20/   41] train: loss: 0.3948951
[Epoch 51] ogbg-molbace: 0.684982 val loss: 1.157405
[Epoch 51] ogbg-molbace: 0.789602 test loss: 1.234604
[Epoch 52; Iter     9/   41] train: loss: 0.3461699
[Epoch 52; Iter    39/   41] train: loss: 0.3585025
[Epoch 52] ogbg-molbace: 0.695238 val loss: 0.895303
[Epoch 52] ogbg-molbace: 0.755347 test loss: 0.768231
[Epoch 53; Iter    28/   41] train: loss: 0.1695030
[Epoch 53] ogbg-molbace: 0.698535 val loss: 0.976339
[Epoch 53] ogbg-molbace: 0.791862 test loss: 1.318024
[Epoch 54; Iter    17/   41] train: loss: 0.1961433
[Epoch 54] ogbg-molbace: 0.717582 val loss: 1.378854
[Epoch 54] ogbg-molbace: 0.821075 test loss: 1.445208
[Epoch 55; Iter     6/   41] train: loss: 0.3447553
[Epoch 55; Iter    36/   41] train: loss: 0.2659015
[Epoch 55] ogbg-molbace: 0.670330 val loss: 1.369810
[Epoch 55] ogbg-molbace: 0.809251 test loss: 0.999307
[Epoch 56; Iter    25/   41] train: loss: 0.2853934
[Epoch 56] ogbg-molbace: 0.739560 val loss: 1.269954
[Epoch 56] ogbg-molbace: 0.802817 test loss: 1.288673
[Epoch 57; Iter    14/   41] train: loss: 0.3674282
[Epoch 57] ogbg-molbace: 0.716850 val loss: 1.375456
[Epoch 57] ogbg-molbace: 0.845418 test loss: 1.443662
[Epoch 58; Iter     3/   41] train: loss: 0.2655936
[Epoch 58; Iter    33/   41] train: loss: 0.2238102
[Epoch 58] ogbg-molbace: 0.712088 val loss: 1.089698
[Epoch 58] ogbg-molbace: 0.779343 test loss: 1.610223
[Epoch 59; Iter    22/   41] train: loss: 0.3767146
[Epoch 59] ogbg-molbace: 0.727839 val loss: 1.144921
[Epoch 59] ogbg-molbace: 0.790819 test loss: 1.247043
[Epoch 60; Iter    11/   41] train: loss: 0.4260295
[Epoch 60; Iter    41/   41] train: loss: 0.1620778
[Epoch 60] ogbg-molbace: 0.689011 val loss: 1.176654
[Epoch 60] ogbg-molbace: 0.759346 test loss: 1.577692
[Epoch 61; Iter    30/   41] train: loss: 0.2547365
[Epoch 61] ogbg-molbace: 0.666667 val loss: 0.918498
[Epoch 61] ogbg-molbace: 0.793427 test loss: 1.132614
[Epoch 62; Iter    19/   41] train: loss: 0.1968218
[Epoch 62] ogbg-molbace: 0.670696 val loss: 1.437571
[Epoch 62] ogbg-molbace: 0.752565 test loss: 1.645421
[Epoch 63; Iter     8/   41] train: loss: 0.2657888
[Epoch 63; Iter    38/   41] train: loss: 0.2240151
[Epoch 63] ogbg-molbace: 0.713187 val loss: 0.973923
[Epoch 63] ogbg-molbace: 0.848200 test loss: 0.718532
[Epoch 64; Iter    27/   41] train: loss: 0.2496355
[Epoch 64] ogbg-molbace: 0.758608 val loss: 1.054816
[Epoch 64] ogbg-molbace: 0.808381 test loss: 1.301527
[Epoch 65; Iter    16/   41] train: loss: 0.1306040
[Epoch 65] ogbg-molbace: 0.647619 val loss: 1.363889
[Epoch 65] ogbg-molbace: 0.802121 test loss: 1.561015
[Epoch 66; Iter     5/   41] train: loss: 0.1926614
[Epoch 66; Iter    35/   41] train: loss: 0.2850151
[Epoch 66] ogbg-molbace: 0.718681 val loss: 1.139613
[Epoch 66] ogbg-molbace: 0.819162 test loss: 1.380144
[Epoch 67; Iter    24/   41] train: loss: 0.1393800
[Epoch 67] ogbg-molbace: 0.724542 val loss: 1.336532
[Epoch 67] ogbg-molbace: 0.818466 test loss: 1.327104
[Epoch 68; Iter    13/   41] train: loss: 0.3294511
[Epoch 68] ogbg-molbace: 0.731502 val loss: 0.916428
[Epoch 68] ogbg-molbace: 0.769605 test loss: 1.608873
[Epoch 69; Iter     2/   41] train: loss: 0.1844806
[Epoch 69; Iter    32/   41] train: loss: 0.1373059
[Epoch 69] ogbg-molbace: 0.706593 val loss: 1.159662
[Epoch 69] ogbg-molbace: 0.817249 test loss: 1.323934
[Epoch 70; Iter    21/   41] train: loss: 0.1450637
[Epoch 70] ogbg-molbace: 0.700000 val loss: 1.302660
[Epoch 70] ogbg-molbace: 0.788037 test loss: 1.528361
[Epoch 71; Iter    10/   41] train: loss: 0.1370911
[Epoch 71; Iter    40/   41] train: loss: 0.1600663
[Epoch 71] ogbg-molbace: 0.712454 val loss: 1.177203
[Epoch 71] ogbg-molbace: 0.795340 test loss: 1.520476
[Epoch 72; Iter    29/   41] train: loss: 0.0720832
[Epoch 72] ogbg-molbace: 0.737729 val loss: 1.250711
[Epoch 72] ogbg-molbace: 0.771170 test loss: 1.626826
[Epoch 73; Iter    18/   41] train: loss: 0.2008254
[Epoch 73] ogbg-molbace: 0.730037 val loss: 1.105832
[Epoch 73] ogbg-molbace: 0.800730 test loss: 1.429825
[Epoch 74; Iter     7/   41] train: loss: 0.2006007
[Epoch 74; Iter    37/   41] train: loss: 0.1343438
[Epoch 74] ogbg-molbace: 0.711722 val loss: 1.552546
[Epoch 74] ogbg-molbace: 0.802817 test loss: 1.623629
[Epoch 75; Iter    26/   41] train: loss: 0.1455090
[Epoch 75] ogbg-molbace: 0.730037 val loss: 1.333399
[Epoch 75] ogbg-molbace: 0.797774 test loss: 1.973093
[Epoch 76; Iter    15/   41] train: loss: 0.0610520
[Epoch 76] ogbg-molbace: 0.726007 val loss: 1.285588
[Epoch 76] ogbg-molbace: 0.790297 test loss: 1.668063
[Epoch 77; Iter     4/   41] train: loss: 0.0933332
[Epoch 77; Iter    34/   41] train: loss: 0.0978055
[Epoch 77] ogbg-molbace: 0.713919 val loss: 1.183339
[Epoch 77] ogbg-molbace: 0.771866 test loss: 1.931118
[Epoch 78; Iter    23/   41] train: loss: 0.0535809
[Epoch 32] ogbg-molbace: 0.793079 test loss: 0.972748
[Epoch 33; Iter     8/   41] train: loss: 0.4664656
[Epoch 33; Iter    38/   41] train: loss: 0.4749136
[Epoch 33] ogbg-molbace: 0.690476 val loss: 0.893515
[Epoch 33] ogbg-molbace: 0.810989 test loss: 0.928181
[Epoch 34; Iter    27/   41] train: loss: 0.5195071
[Epoch 34] ogbg-molbace: 0.718315 val loss: 0.942915
[Epoch 34] ogbg-molbace: 0.811163 test loss: 0.855380
[Epoch 35; Iter    16/   41] train: loss: 0.4794209
[Epoch 35] ogbg-molbace: 0.690842 val loss: 1.212945
[Epoch 35] ogbg-molbace: 0.743523 test loss: 1.268775
[Epoch 36; Iter     5/   41] train: loss: 0.4227619
[Epoch 36; Iter    35/   41] train: loss: 0.4029105
[Epoch 36] ogbg-molbace: 0.732967 val loss: 0.836615
[Epoch 36] ogbg-molbace: 0.838289 test loss: 0.608944
[Epoch 37; Iter    24/   41] train: loss: 0.4754611
[Epoch 37] ogbg-molbace: 0.660806 val loss: 1.269424
[Epoch 37] ogbg-molbace: 0.782994 test loss: 0.849853
[Epoch 38; Iter    13/   41] train: loss: 0.3841422
[Epoch 38] ogbg-molbace: 0.678755 val loss: 0.835122
[Epoch 38] ogbg-molbace: 0.764563 test loss: 0.589416
[Epoch 39; Iter     2/   41] train: loss: 0.4671015
[Epoch 39; Iter    32/   41] train: loss: 0.3165196
[Epoch 39] ogbg-molbace: 0.730037 val loss: 0.894795
[Epoch 39] ogbg-molbace: 0.742480 test loss: 1.141526
[Epoch 40; Iter    21/   41] train: loss: 0.2437165
[Epoch 40] ogbg-molbace: 0.620147 val loss: 1.248324
[Epoch 40] ogbg-molbace: 0.772387 test loss: 0.853663
[Epoch 41; Iter    10/   41] train: loss: 0.6441117
[Epoch 41; Iter    40/   41] train: loss: 0.6029141
[Epoch 41] ogbg-molbace: 0.579487 val loss: 1.831505
[Epoch 41] ogbg-molbace: 0.669449 test loss: 2.044939
[Epoch 42; Iter    29/   41] train: loss: 0.4219172
[Epoch 42] ogbg-molbace: 0.707692 val loss: 0.930048
[Epoch 42] ogbg-molbace: 0.798122 test loss: 0.876903
[Epoch 43; Iter    18/   41] train: loss: 0.2873951
[Epoch 43] ogbg-molbace: 0.650549 val loss: 0.734027
[Epoch 43] ogbg-molbace: 0.747174 test loss: 0.908205
[Epoch 44; Iter     7/   41] train: loss: 0.2336060
[Epoch 44; Iter    37/   41] train: loss: 0.4077275
[Epoch 44] ogbg-molbace: 0.719414 val loss: 1.056019
[Epoch 44] ogbg-molbace: 0.686663 test loss: 1.297658
[Epoch 45; Iter    26/   41] train: loss: 0.2742249
[Epoch 45] ogbg-molbace: 0.636630 val loss: 1.524149
[Epoch 45] ogbg-molbace: 0.799339 test loss: 1.352214
[Epoch 46; Iter    15/   41] train: loss: 0.2617928
[Epoch 46] ogbg-molbace: 0.733700 val loss: 0.939507
[Epoch 46] ogbg-molbace: 0.769258 test loss: 1.002710
[Epoch 47; Iter     4/   41] train: loss: 0.4140804
[Epoch 47; Iter    34/   41] train: loss: 0.2462161
[Epoch 47] ogbg-molbace: 0.737729 val loss: 0.875470
[Epoch 47] ogbg-molbace: 0.808555 test loss: 0.765373
[Epoch 48; Iter    23/   41] train: loss: 0.2429998
[Epoch 48] ogbg-molbace: 0.690842 val loss: 1.410110
[Epoch 48] ogbg-molbace: 0.829943 test loss: 1.190477
[Epoch 49; Iter    12/   41] train: loss: 0.3144184
[Epoch 49] ogbg-molbace: 0.751282 val loss: 0.756688
[Epoch 49] ogbg-molbace: 0.769431 test loss: 1.844611
[Epoch 50; Iter     1/   41] train: loss: 0.3157371
[Epoch 50; Iter    31/   41] train: loss: 0.2412002
[Epoch 50] ogbg-molbace: 0.722711 val loss: 1.035440
[Epoch 50] ogbg-molbace: 0.813076 test loss: 1.700341
[Epoch 51; Iter    20/   41] train: loss: 0.3969741
[Epoch 51] ogbg-molbace: 0.650549 val loss: 1.163348
[Epoch 51] ogbg-molbace: 0.793949 test loss: 1.144015
[Epoch 52; Iter     9/   41] train: loss: 0.3099523
[Epoch 52; Iter    39/   41] train: loss: 0.2027982
[Epoch 52] ogbg-molbace: 0.730403 val loss: 1.101282
[Epoch 52] ogbg-molbace: 0.806295 test loss: 1.255247
[Epoch 53; Iter    28/   41] train: loss: 0.2261544
[Epoch 53] ogbg-molbace: 0.756044 val loss: 1.131873
[Epoch 53] ogbg-molbace: 0.801774 test loss: 1.244147
[Epoch 54; Iter    17/   41] train: loss: 0.2532418
[Epoch 54] ogbg-molbace: 0.749817 val loss: 0.987090
[Epoch 54] ogbg-molbace: 0.771866 test loss: 1.444569
[Epoch 55; Iter     6/   41] train: loss: 0.0922495
[Epoch 55; Iter    36/   41] train: loss: 0.2313154
[Epoch 55] ogbg-molbace: 0.682051 val loss: 1.071085
[Epoch 55] ogbg-molbace: 0.822466 test loss: 0.939378
[Epoch 56; Iter    25/   41] train: loss: 0.2165308
[Epoch 56] ogbg-molbace: 0.685714 val loss: 1.219018
[Epoch 56] ogbg-molbace: 0.795862 test loss: 1.322766
[Epoch 57; Iter    14/   41] train: loss: 0.3517072
[Epoch 57] ogbg-molbace: 0.724908 val loss: 1.206080
[Epoch 57] ogbg-molbace: 0.756216 test loss: 1.485279
[Epoch 58; Iter     3/   41] train: loss: 0.2949850
[Epoch 58; Iter    33/   41] train: loss: 0.2351948
[Epoch 58] ogbg-molbace: 0.695604 val loss: 1.303465
[Epoch 58] ogbg-molbace: 0.810468 test loss: 0.881548
[Epoch 59; Iter    22/   41] train: loss: 0.2352206
[Epoch 59] ogbg-molbace: 0.653480 val loss: 1.384245
[Epoch 59] ogbg-molbace: 0.783864 test loss: 1.502041
[Epoch 60; Iter    11/   41] train: loss: 0.5318975
[Epoch 60; Iter    41/   41] train: loss: 0.4693882
[Epoch 60] ogbg-molbace: 0.673260 val loss: 1.410474
[Epoch 60] ogbg-molbace: 0.791341 test loss: 1.703307
[Epoch 61; Iter    30/   41] train: loss: 0.4407798
[Epoch 61] ogbg-molbace: 0.698535 val loss: 1.304696
[Epoch 61] ogbg-molbace: 0.765954 test loss: 1.795703
[Epoch 62; Iter    19/   41] train: loss: 0.4237574
[Epoch 62] ogbg-molbace: 0.745788 val loss: 0.927362
[Epoch 62] ogbg-molbace: 0.814293 test loss: 1.258181
[Epoch 63; Iter     8/   41] train: loss: 0.2686975
[Epoch 63; Iter    38/   41] train: loss: 0.2084450
[Epoch 63] ogbg-molbace: 0.721978 val loss: 1.397296
[Epoch 63] ogbg-molbace: 0.786820 test loss: 1.830700
[Epoch 64; Iter    27/   41] train: loss: 0.1207911
[Epoch 64] ogbg-molbace: 0.748718 val loss: 1.125784
[Epoch 64] ogbg-molbace: 0.782473 test loss: 1.367174
[Epoch 65; Iter    16/   41] train: loss: 0.4020572
[Epoch 65] ogbg-molbace: 0.781685 val loss: 0.754225
[Epoch 65] ogbg-molbace: 0.778995 test loss: 1.508768
[Epoch 66; Iter     5/   41] train: loss: 0.1154000
[Epoch 66; Iter    35/   41] train: loss: 0.4919141
[Epoch 66] ogbg-molbace: 0.684615 val loss: 1.266428
[Epoch 66] ogbg-molbace: 0.694662 test loss: 1.644458
[Epoch 67; Iter    24/   41] train: loss: 0.3068199
[Epoch 67] ogbg-molbace: 0.728205 val loss: 1.056377
[Epoch 67] ogbg-molbace: 0.764737 test loss: 1.795126
[Epoch 68; Iter    13/   41] train: loss: 0.1980861
[Epoch 68] ogbg-molbace: 0.746886 val loss: 1.327911
[Epoch 68] ogbg-molbace: 0.761433 test loss: 2.185068
[Epoch 69; Iter     2/   41] train: loss: 0.1768414
[Epoch 69; Iter    32/   41] train: loss: 0.1910209
[Epoch 69] ogbg-molbace: 0.749084 val loss: 1.491775
[Epoch 69] ogbg-molbace: 0.757781 test loss: 1.901340
[Epoch 70; Iter    21/   41] train: loss: 0.2761658
[Epoch 70] ogbg-molbace: 0.729670 val loss: 1.208059
[Epoch 70] ogbg-molbace: 0.727352 test loss: 1.962787
[Epoch 71; Iter    10/   41] train: loss: 0.1913993
[Epoch 71; Iter    40/   41] train: loss: 0.2396839
[Epoch 71] ogbg-molbace: 0.672161 val loss: 1.439497
[Epoch 71] ogbg-molbace: 0.783168 test loss: 1.378604
[Epoch 72; Iter    29/   41] train: loss: 0.1326229
[Epoch 72] ogbg-molbace: 0.759341 val loss: 1.070875
[Epoch 72] ogbg-molbace: 0.798122 test loss: 1.661045
[Epoch 73; Iter    18/   41] train: loss: 0.1185548
[Epoch 73] ogbg-molbace: 0.772527 val loss: 1.202331
[Epoch 73] ogbg-molbace: 0.784385 test loss: 1.738998
[Epoch 74; Iter     7/   41] train: loss: 0.0818733
[Epoch 74; Iter    37/   41] train: loss: 0.6287865
[Epoch 74] ogbg-molbace: 0.747985 val loss: 1.155335
[Epoch 74] ogbg-molbace: 0.787689 test loss: 1.344679
[Epoch 75; Iter    26/   41] train: loss: 0.1770524
[Epoch 75] ogbg-molbace: 0.787546 val loss: 0.914178
[Epoch 75] ogbg-molbace: 0.779517 test loss: 1.780957
[Epoch 76; Iter    15/   41] train: loss: 0.2217532
[Epoch 76] ogbg-molbace: 0.794505 val loss: 1.044714
[Epoch 76] ogbg-molbace: 0.774300 test loss: 1.737509
[Epoch 77; Iter     4/   41] train: loss: 0.0797331
[Epoch 77; Iter    34/   41] train: loss: 0.2045431
[Epoch 77] ogbg-molbace: 0.763736 val loss: 1.337667
[Epoch 77] ogbg-molbace: 0.790819 test loss: 2.454137
[Epoch 78; Iter    23/   41] train: loss: 0.0674460
[Epoch 36; Iter    25/   31] train: loss: 0.3785626
[Epoch 36] ogbg-molbace: 0.701787 val loss: 0.718888
[Epoch 36] ogbg-molbace: 0.740676 test loss: 0.663606
[Epoch 37; Iter    24/   31] train: loss: 0.4186890
[Epoch 37] ogbg-molbace: 0.750645 val loss: 0.390870
[Epoch 37] ogbg-molbace: 0.760767 test loss: 0.773897
[Epoch 38; Iter    23/   31] train: loss: 0.3921947
[Epoch 38] ogbg-molbace: 0.750447 val loss: 0.494517
[Epoch 38] ogbg-molbace: 0.752061 test loss: 0.704199
[Epoch 39; Iter    22/   31] train: loss: 0.5188706
[Epoch 39] ogbg-molbace: 0.795035 val loss: 0.520165
[Epoch 39] ogbg-molbace: 0.775809 test loss: 0.621587
[Epoch 40; Iter    21/   31] train: loss: 0.4906254
[Epoch 40] ogbg-molbace: 0.742403 val loss: 0.424208
[Epoch 40] ogbg-molbace: 0.750567 test loss: 0.748845
[Epoch 41; Iter    20/   31] train: loss: 0.6126921
[Epoch 41] ogbg-molbace: 0.773287 val loss: 0.444749
[Epoch 41] ogbg-molbace: 0.759221 test loss: 0.680071
[Epoch 42; Iter    19/   31] train: loss: 0.5708164
[Epoch 42] ogbg-molbace: 0.787090 val loss: 0.719259
[Epoch 42] ogbg-molbace: 0.771482 test loss: 0.594037
[Epoch 43; Iter    18/   31] train: loss: 0.3827774
[Epoch 43] ogbg-molbace: 0.787388 val loss: 0.679325
[Epoch 43] ogbg-molbace: 0.768906 test loss: 0.608265
[Epoch 44; Iter    17/   31] train: loss: 0.4959973
[Epoch 44] ogbg-molbace: 0.770904 val loss: 0.495803
[Epoch 44] ogbg-molbace: 0.772254 test loss: 0.690930
[Epoch 45; Iter    16/   31] train: loss: 0.4476828
[Epoch 45] ogbg-molbace: 0.771102 val loss: 0.904432
[Epoch 45] ogbg-molbace: 0.757212 test loss: 0.669550
[Epoch 46; Iter    15/   31] train: loss: 0.6118695
[Epoch 46] ogbg-molbace: 0.770506 val loss: 0.562856
[Epoch 46] ogbg-molbace: 0.732640 test loss: 1.091429
[Epoch 47; Iter    14/   31] train: loss: 0.2053576
[Epoch 47] ogbg-molbace: 0.782026 val loss: 0.718591
[Epoch 47] ogbg-molbace: 0.769369 test loss: 0.709111
[Epoch 48; Iter    13/   31] train: loss: 0.3783658
[Epoch 48] ogbg-molbace: 0.695432 val loss: 1.152295
[Epoch 48] ogbg-molbace: 0.706264 test loss: 1.268986
[Epoch 49; Iter    12/   31] train: loss: 0.4561363
[Epoch 49] ogbg-molbace: 0.751142 val loss: 1.007240
[Epoch 49] ogbg-molbace: 0.709819 test loss: 1.117592
[Epoch 50; Iter    11/   31] train: loss: 0.4024210
[Epoch 50] ogbg-molbace: 0.711420 val loss: 0.483286
[Epoch 50] ogbg-molbace: 0.742994 test loss: 1.125611
[Epoch 51; Iter    10/   31] train: loss: 0.2899708
[Epoch 51] ogbg-molbace: 0.797517 val loss: 0.476823
[Epoch 51] ogbg-molbace: 0.736194 test loss: 0.905592
[Epoch 52; Iter     9/   31] train: loss: 0.4185380
[Epoch 52] ogbg-molbace: 0.751043 val loss: 0.391846
[Epoch 52] ogbg-molbace: 0.711982 test loss: 1.410330
[Epoch 53; Iter     8/   31] train: loss: 0.4667172
[Epoch 53] ogbg-molbace: 0.777061 val loss: 0.592065
[Epoch 53] ogbg-molbace: 0.779878 test loss: 0.759812
[Epoch 54; Iter     7/   31] train: loss: 0.6333432
[Epoch 54] ogbg-molbace: 0.764647 val loss: 3.092250
[Epoch 54] ogbg-molbace: 0.705852 test loss: 3.637965
[Epoch 55; Iter     6/   31] train: loss: 0.2463041
[Epoch 55] ogbg-molbace: 0.733366 val loss: 0.501469
[Epoch 55] ogbg-molbace: 0.753709 test loss: 0.858993
[Epoch 56; Iter     5/   31] train: loss: 0.3177459
[Epoch 56] ogbg-molbace: 0.772493 val loss: 0.578702
[Epoch 56] ogbg-molbace: 0.760303 test loss: 0.768166
[Epoch 57; Iter     4/   31] train: loss: 0.2524629
[Epoch 57] ogbg-molbace: 0.761370 val loss: 0.414744
[Epoch 57] ogbg-molbace: 0.774006 test loss: 0.849044
[Epoch 58; Iter     3/   31] train: loss: 0.2494850
[Epoch 58] ogbg-molbace: 0.764250 val loss: 0.446522
[Epoch 58] ogbg-molbace: 0.756130 test loss: 0.792372
[Epoch 59; Iter     2/   31] train: loss: 0.2423750
[Epoch 59] ogbg-molbace: 0.760377 val loss: 0.554558
[Epoch 59] ogbg-molbace: 0.778848 test loss: 0.700271
[Epoch 60; Iter     1/   31] train: loss: 0.3787224
[Epoch 60; Iter    31/   31] train: loss: 0.3605784
[Epoch 60] ogbg-molbace: 0.777358 val loss: 0.398756
[Epoch 60] ogbg-molbace: 0.786575 test loss: 0.780986
[Epoch 61; Iter    30/   31] train: loss: 0.2525431
[Epoch 61] ogbg-molbace: 0.775074 val loss: 0.589700
[Epoch 61] ogbg-molbace: 0.778642 test loss: 0.743458
[Epoch 62; Iter    29/   31] train: loss: 0.2447371
[Epoch 62] ogbg-molbace: 0.753426 val loss: 0.563558
[Epoch 62] ogbg-molbace: 0.780600 test loss: 0.756204
[Epoch 63; Iter    28/   31] train: loss: 0.2437479
[Epoch 63] ogbg-molbace: 0.677855 val loss: 0.484734
[Epoch 63] ogbg-molbace: 0.768906 test loss: 0.990515
[Epoch 64; Iter    27/   31] train: loss: 0.4611981
[Epoch 64] ogbg-molbace: 0.766733 val loss: 0.526990
[Epoch 64] ogbg-molbace: 0.779672 test loss: 1.127127
[Epoch 65; Iter    26/   31] train: loss: 0.3185555
[Epoch 65] ogbg-molbace: 0.788083 val loss: 0.534326
[Epoch 65] ogbg-molbace: 0.761951 test loss: 0.735274
[Epoch 66; Iter    25/   31] train: loss: 0.3124548
[Epoch 66] ogbg-molbace: 0.814300 val loss: 0.498116
[Epoch 66] ogbg-molbace: 0.777406 test loss: 0.826131
[Epoch 67; Iter    24/   31] train: loss: 0.1756596
[Epoch 67] ogbg-molbace: 0.797021 val loss: 0.586826
[Epoch 67] ogbg-molbace: 0.799712 test loss: 0.723728
[Epoch 68; Iter    23/   31] train: loss: 0.2042905
[Epoch 68] ogbg-molbace: 0.769116 val loss: 0.499607
[Epoch 68] ogbg-molbace: 0.822326 test loss: 0.745105
[Epoch 69; Iter    22/   31] train: loss: 0.2388110
[Epoch 69] ogbg-molbace: 0.667031 val loss: 0.720930
[Epoch 69] ogbg-molbace: 0.737740 test loss: 0.729791
[Epoch 70; Iter    21/   31] train: loss: 0.5956498
[Epoch 70] ogbg-molbace: 0.741212 val loss: 0.406981
[Epoch 70] ogbg-molbace: 0.760200 test loss: 0.923069
[Epoch 71; Iter    20/   31] train: loss: 0.7356622
[Epoch 71] ogbg-molbace: 0.834062 val loss: 0.359714
[Epoch 71] ogbg-molbace: 0.779415 test loss: 0.875460
[Epoch 72; Iter    19/   31] train: loss: 0.4642002
[Epoch 72] ogbg-molbace: 0.800993 val loss: 0.523460
[Epoch 72] ogbg-molbace: 0.787193 test loss: 0.656280
[Epoch 73; Iter    18/   31] train: loss: 0.3565424
[Epoch 73] ogbg-molbace: 0.762959 val loss: 0.627098
[Epoch 73] ogbg-molbace: 0.782454 test loss: 0.687733
[Epoch 74; Iter    17/   31] train: loss: 0.2852298
[Epoch 74] ogbg-molbace: 0.837140 val loss: 0.459963
[Epoch 74] ogbg-molbace: 0.800484 test loss: 0.704958
[Epoch 75; Iter    16/   31] train: loss: 0.1614797
[Epoch 75] ogbg-molbace: 0.836544 val loss: 0.384743
[Epoch 75] ogbg-molbace: 0.788378 test loss: 0.723116
[Epoch 76; Iter    15/   31] train: loss: 0.2055672
[Epoch 76] ogbg-molbace: 0.804568 val loss: 0.472769
[Epoch 76] ogbg-molbace: 0.807851 test loss: 0.725699
[Epoch 77; Iter    14/   31] train: loss: 0.1370942
[Epoch 77] ogbg-molbace: 0.839722 val loss: 0.431689
[Epoch 77] ogbg-molbace: 0.812590 test loss: 0.778656
[Epoch 78; Iter    13/   31] train: loss: 0.3780625
[Epoch 78] ogbg-molbace: 0.829394 val loss: 0.546849
[Epoch 78] ogbg-molbace: 0.797548 test loss: 0.747919
[Epoch 79; Iter    12/   31] train: loss: 0.2932334
[Epoch 79] ogbg-molbace: 0.818570 val loss: 0.470438
[Epoch 79] ogbg-molbace: 0.806563 test loss: 0.784062
[Epoch 80; Iter    11/   31] train: loss: 0.1804940
[Epoch 80] ogbg-molbace: 0.769911 val loss: 0.674158
[Epoch 80] ogbg-molbace: 0.790336 test loss: 0.823599
[Epoch 81; Iter    10/   31] train: loss: 0.1948146
[Epoch 81] ogbg-molbace: 0.819960 val loss: 0.390244
[Epoch 81] ogbg-molbace: 0.807851 test loss: 0.905789
[Epoch 82; Iter     9/   31] train: loss: 0.1844904
[Epoch 82] ogbg-molbace: 0.801291 val loss: 0.580874
[Epoch 82] ogbg-molbace: 0.778281 test loss: 0.843287
[Epoch 83; Iter     8/   31] train: loss: 0.1707901
[Epoch 83] ogbg-molbace: 0.759881 val loss: 0.455306
[Epoch 83] ogbg-molbace: 0.798784 test loss: 1.047798
[Epoch 84; Iter     7/   31] train: loss: 0.1976299
[Epoch 84] ogbg-molbace: 0.824528 val loss: 0.560605
[Epoch 84] ogbg-molbace: 0.798321 test loss: 0.751207
[Epoch 85; Iter     6/   31] train: loss: 0.1756394
[Epoch 85] ogbg-molbace: 0.792751 val loss: 0.393885
[Epoch 85] ogbg-molbace: 0.795384 test loss: 1.089099
[Epoch 86; Iter     5/   31] train: loss: 0.1025892
[Epoch 86] ogbg-molbace: 0.747567 val loss: 0.604912
[Epoch 86] ogbg-molbace: 0.778951 test loss: 0.930857
[Epoch 34] ogbg-molbace: 0.644609 val loss: 0.667704
[Epoch 34] ogbg-molbace: 0.792615 test loss: 0.703173
[Epoch 35; Iter     6/   36] train: loss: 0.2565666
[Epoch 35; Iter    36/   36] train: loss: 0.5665399
[Epoch 35] ogbg-molbace: 0.700317 val loss: 0.504866
[Epoch 35] ogbg-molbace: 0.823041 test loss: 0.783825
[Epoch 36; Iter    30/   36] train: loss: 0.2968932
[Epoch 36] ogbg-molbace: 0.716385 val loss: 0.658072
[Epoch 36] ogbg-molbace: 0.819700 test loss: 0.633323
[Epoch 37; Iter    24/   36] train: loss: 0.3843334
[Epoch 37] ogbg-molbace: 0.707294 val loss: 0.535567
[Epoch 37] ogbg-molbace: 0.823221 test loss: 0.769719
[Epoch 38; Iter    18/   36] train: loss: 0.3948452
[Epoch 38] ogbg-molbace: 0.733932 val loss: 0.566298
[Epoch 38] ogbg-molbace: 0.824756 test loss: 0.607818
[Epoch 39; Iter    12/   36] train: loss: 0.5498937
[Epoch 39] ogbg-molbace: 0.700106 val loss: 0.707574
[Epoch 39] ogbg-molbace: 0.766071 test loss: 0.762720
[Epoch 40; Iter     6/   36] train: loss: 0.3850824
[Epoch 40; Iter    36/   36] train: loss: 0.4712255
[Epoch 40] ogbg-molbace: 0.719133 val loss: 0.725120
[Epoch 40] ogbg-molbace: 0.794962 test loss: 0.661163
[Epoch 41; Iter    30/   36] train: loss: 0.4060478
[Epoch 41] ogbg-molbace: 0.636892 val loss: 0.933473
[Epoch 41] ogbg-molbace: 0.831076 test loss: 0.565518
[Epoch 42; Iter    24/   36] train: loss: 0.3667683
[Epoch 42] ogbg-molbace: 0.700211 val loss: 0.919344
[Epoch 42] ogbg-molbace: 0.851029 test loss: 0.438443
[Epoch 43; Iter    18/   36] train: loss: 0.4415691
[Epoch 43] ogbg-molbace: 0.756977 val loss: 0.699319
[Epoch 43] ogbg-molbace: 0.836764 test loss: 0.500235
[Epoch 44; Iter    12/   36] train: loss: 0.5394591
[Epoch 44] ogbg-molbace: 0.735095 val loss: 0.694409
[Epoch 44] ogbg-molbace: 0.852113 test loss: 0.488866
[Epoch 45; Iter     6/   36] train: loss: 0.2002605
[Epoch 45; Iter    36/   36] train: loss: 0.6474321
[Epoch 45] ogbg-molbace: 0.725264 val loss: 0.744100
[Epoch 45] ogbg-molbace: 0.801643 test loss: 0.679132
[Epoch 46; Iter    30/   36] train: loss: 0.4056691
[Epoch 46] ogbg-molbace: 0.715116 val loss: 0.674423
[Epoch 46] ogbg-molbace: 0.863308 test loss: 0.509907
[Epoch 47; Iter    24/   36] train: loss: 0.3305543
[Epoch 47] ogbg-molbace: 0.760359 val loss: 0.791828
[Epoch 47] ogbg-molbace: 0.858523 test loss: 0.462223
[Epoch 48; Iter    18/   36] train: loss: 0.2389593
[Epoch 48] ogbg-molbace: 0.738372 val loss: 0.720972
[Epoch 48] ogbg-molbace: 0.837757 test loss: 0.628608
[Epoch 49; Iter    12/   36] train: loss: 0.3651848
[Epoch 49] ogbg-molbace: 0.772939 val loss: 0.617897
[Epoch 49] ogbg-molbace: 0.858162 test loss: 0.569467
[Epoch 50; Iter     6/   36] train: loss: 0.4309680
[Epoch 50; Iter    36/   36] train: loss: 0.4950486
[Epoch 50] ogbg-molbace: 0.727590 val loss: 0.705090
[Epoch 50] ogbg-molbace: 0.844168 test loss: 0.516530
[Epoch 51; Iter    30/   36] train: loss: 0.3823373
[Epoch 51] ogbg-molbace: 0.747463 val loss: 0.529826
[Epoch 51] ogbg-molbace: 0.845612 test loss: 0.738195
[Epoch 52; Iter    24/   36] train: loss: 0.2616374
[Epoch 52] ogbg-molbace: 0.754545 val loss: 0.664533
[Epoch 52] ogbg-molbace: 0.886331 test loss: 0.681122
[Epoch 53; Iter    18/   36] train: loss: 0.3277878
[Epoch 53] ogbg-molbace: 0.738372 val loss: 0.661926
[Epoch 53] ogbg-molbace: 0.831798 test loss: 0.773320
[Epoch 54; Iter    12/   36] train: loss: 0.2818608
[Epoch 54] ogbg-molbace: 0.762262 val loss: 0.549849
[Epoch 54] ogbg-molbace: 0.845793 test loss: 0.806005
[Epoch 55; Iter     6/   36] train: loss: 0.2688297
[Epoch 55; Iter    36/   36] train: loss: 0.5735825
[Epoch 55] ogbg-molbace: 0.767970 val loss: 0.573512
[Epoch 55] ogbg-molbace: 0.836584 test loss: 0.739420
[Epoch 56; Iter    30/   36] train: loss: 0.2043386
[Epoch 56] ogbg-molbace: 0.740592 val loss: 0.567917
[Epoch 56] ogbg-molbace: 0.820874 test loss: 0.916676
[Epoch 57; Iter    24/   36] train: loss: 0.2478848
[Epoch 57] ogbg-molbace: 0.776216 val loss: 0.559525
[Epoch 57] ogbg-molbace: 0.818707 test loss: 0.795425
[Epoch 58; Iter    18/   36] train: loss: 0.2423169
[Epoch 58] ogbg-molbace: 0.772093 val loss: 0.638603
[Epoch 58] ogbg-molbace: 0.824937 test loss: 0.660214
[Epoch 59; Iter    12/   36] train: loss: 0.2289018
[Epoch 59] ogbg-molbace: 0.765328 val loss: 0.883417
[Epoch 59] ogbg-molbace: 0.801733 test loss: 0.568976
[Epoch 60; Iter     6/   36] train: loss: 0.2911591
[Epoch 60; Iter    36/   36] train: loss: 0.8370225
[Epoch 60] ogbg-molbace: 0.787421 val loss: 0.570857
[Epoch 60] ogbg-molbace: 0.842542 test loss: 0.671090
[Epoch 61; Iter    30/   36] train: loss: 0.1809768
[Epoch 61] ogbg-molbace: 0.775370 val loss: 0.583547
[Epoch 61] ogbg-molbace: 0.845161 test loss: 0.805668
[Epoch 62; Iter    24/   36] train: loss: 0.1580546
[Epoch 62] ogbg-molbace: 0.745877 val loss: 0.675608
[Epoch 62] ogbg-molbace: 0.796678 test loss: 0.806375
[Epoch 63; Iter    18/   36] train: loss: 0.2948931
[Epoch 63] ogbg-molbace: 0.797674 val loss: 0.570119
[Epoch 63] ogbg-molbace: 0.832972 test loss: 1.092376
[Epoch 64; Iter    12/   36] train: loss: 0.3382223
[Epoch 64] ogbg-molbace: 0.751268 val loss: 0.674365
[Epoch 64] ogbg-molbace: 0.839292 test loss: 0.664533
[Epoch 65; Iter     6/   36] train: loss: 0.1648384
[Epoch 65; Iter    36/   36] train: loss: 0.2743697
[Epoch 65] ogbg-molbace: 0.744503 val loss: 0.622962
[Epoch 65] ogbg-molbace: 0.835861 test loss: 0.824137
[Epoch 66; Iter    30/   36] train: loss: 0.2007898
[Epoch 66] ogbg-molbace: 0.796512 val loss: 0.530311
[Epoch 66] ogbg-molbace: 0.854280 test loss: 0.731278
[Epoch 67; Iter    24/   36] train: loss: 0.2534335
[Epoch 67] ogbg-molbace: 0.772622 val loss: 0.599995
[Epoch 67] ogbg-molbace: 0.808685 test loss: 0.753423
[Epoch 68; Iter    18/   36] train: loss: 0.1156017
[Epoch 68] ogbg-molbace: 0.786998 val loss: 0.597984
[Epoch 68] ogbg-molbace: 0.858974 test loss: 0.624329
[Epoch 69; Iter    12/   36] train: loss: 0.4213593
[Epoch 69] ogbg-molbace: 0.773679 val loss: 0.828523
[Epoch 69] ogbg-molbace: 0.848230 test loss: 0.606458
[Epoch 70; Iter     6/   36] train: loss: 0.2345544
[Epoch 70; Iter    36/   36] train: loss: 0.4263411
[Epoch 70] ogbg-molbace: 0.759197 val loss: 0.542574
[Epoch 70] ogbg-molbace: 0.840105 test loss: 0.793151
[Epoch 71; Iter    30/   36] train: loss: 0.3161146
[Epoch 71] ogbg-molbace: 0.758457 val loss: 0.671404
[Epoch 71] ogbg-molbace: 0.828187 test loss: 0.775001
[Epoch 72; Iter    24/   36] train: loss: 0.1467807
[Epoch 72] ogbg-molbace: 0.766596 val loss: 0.622409
[Epoch 72] ogbg-molbace: 0.838750 test loss: 0.825252
[Epoch 73; Iter    18/   36] train: loss: 0.1939155
[Epoch 73] ogbg-molbace: 0.798203 val loss: 0.582023
[Epoch 73] ogbg-molbace: 0.844619 test loss: 0.795042
[Epoch 74; Iter    12/   36] train: loss: 0.1877504
[Epoch 74] ogbg-molbace: 0.755180 val loss: 0.718312
[Epoch 74] ogbg-molbace: 0.844529 test loss: 0.891369
[Epoch 75; Iter     6/   36] train: loss: 0.0972476
[Epoch 75; Iter    36/   36] train: loss: 0.1514577
[Epoch 75] ogbg-molbace: 0.767653 val loss: 0.893365
[Epoch 75] ogbg-molbace: 0.833514 test loss: 0.698935
[Epoch 76; Iter    30/   36] train: loss: 0.0550210
[Epoch 76] ogbg-molbace: 0.786786 val loss: 0.877359
[Epoch 76] ogbg-molbace: 0.825208 test loss: 0.745424
[Epoch 77; Iter    24/   36] train: loss: 0.3293895
[Epoch 77] ogbg-molbace: 0.767865 val loss: 0.801715
[Epoch 77] ogbg-molbace: 0.822138 test loss: 0.824270
[Epoch 78; Iter    18/   36] train: loss: 0.1345064
[Epoch 78] ogbg-molbace: 0.786047 val loss: 0.773258
[Epoch 78] ogbg-molbace: 0.812568 test loss: 0.829481
[Epoch 79; Iter    12/   36] train: loss: 0.2280353
[Epoch 79] ogbg-molbace: 0.775264 val loss: 0.798091
[Epoch 79] ogbg-molbace: 0.827013 test loss: 0.780014
[Epoch 80; Iter     6/   36] train: loss: 0.0743522
[Epoch 80; Iter    36/   36] train: loss: 0.4149916
[Epoch 80] ogbg-molbace: 0.779598 val loss: 0.729763
[Epoch 80] ogbg-molbace: 0.838299 test loss: 0.736784
[Epoch 81; Iter    30/   36] train: loss: 0.2001863
[Epoch 81] ogbg-molbace: 0.796934 val loss: 0.633059
[Epoch 81] ogbg-molbace: 0.845973 test loss: 1.152029
[Epoch 82; Iter    24/   36] train: loss: 0.0523087
[Epoch 34] ogbg-molbace: 0.667865 val loss: 0.643073
[Epoch 34] ogbg-molbace: 0.829180 test loss: 0.628561
[Epoch 35; Iter     6/   36] train: loss: 0.4632582
[Epoch 35; Iter    36/   36] train: loss: 0.6301249
[Epoch 35] ogbg-molbace: 0.730233 val loss: 0.643687
[Epoch 35] ogbg-molbace: 0.827736 test loss: 0.558631
[Epoch 36; Iter    30/   36] train: loss: 0.4148808
[Epoch 36] ogbg-molbace: 0.672093 val loss: 0.544852
[Epoch 36] ogbg-molbace: 0.825208 test loss: 1.084493
[Epoch 37; Iter    24/   36] train: loss: 0.2775911
[Epoch 37] ogbg-molbace: 0.718922 val loss: 0.566047
[Epoch 37] ogbg-molbace: 0.824485 test loss: 0.646199
[Epoch 38; Iter    18/   36] train: loss: 0.6581807
[Epoch 38] ogbg-molbace: 0.715645 val loss: 0.538555
[Epoch 38] ogbg-molbace: 0.813561 test loss: 0.730722
[Epoch 39; Iter    12/   36] train: loss: 0.3084293
[Epoch 39] ogbg-molbace: 0.747674 val loss: 0.743855
[Epoch 39] ogbg-molbace: 0.812297 test loss: 0.562358
[Epoch 40; Iter     6/   36] train: loss: 0.4667559
[Epoch 40; Iter    36/   36] train: loss: 0.2894365
[Epoch 40] ogbg-molbace: 0.768393 val loss: 0.617192
[Epoch 40] ogbg-molbace: 0.864121 test loss: 0.622332
[Epoch 41; Iter    30/   36] train: loss: 0.5664417
[Epoch 41] ogbg-molbace: 0.738372 val loss: 0.647124
[Epoch 41] ogbg-molbace: 0.842362 test loss: 0.799460
[Epoch 42; Iter    24/   36] train: loss: 0.3714963
[Epoch 42] ogbg-molbace: 0.729281 val loss: 1.178368
[Epoch 42] ogbg-molbace: 0.857801 test loss: 0.462994
[Epoch 43; Iter    18/   36] train: loss: 0.4338920
[Epoch 43] ogbg-molbace: 0.743658 val loss: 0.821440
[Epoch 43] ogbg-molbace: 0.853106 test loss: 0.658438
[Epoch 44; Iter    12/   36] train: loss: 0.2870163
[Epoch 44] ogbg-molbace: 0.745877 val loss: 0.812002
[Epoch 44] ogbg-molbace: 0.886241 test loss: 0.380925
[Epoch 45; Iter     6/   36] train: loss: 0.2737198
[Epoch 45; Iter    36/   36] train: loss: 0.5675432
[Epoch 45] ogbg-molbace: 0.743235 val loss: 0.636159
[Epoch 45] ogbg-molbace: 0.867281 test loss: 0.482174
[Epoch 46; Iter    30/   36] train: loss: 0.4206216
[Epoch 46] ogbg-molbace: 0.686681 val loss: 0.767792
[Epoch 46] ogbg-molbace: 0.806789 test loss: 0.540342
[Epoch 47; Iter    24/   36] train: loss: 0.2908322
[Epoch 47] ogbg-molbace: 0.743763 val loss: 0.623571
[Epoch 47] ogbg-molbace: 0.826472 test loss: 1.133178
[Epoch 48; Iter    18/   36] train: loss: 0.2570621
[Epoch 48] ogbg-molbace: 0.754968 val loss: 0.605394
[Epoch 48] ogbg-molbace: 0.828548 test loss: 0.536240
[Epoch 49; Iter    12/   36] train: loss: 0.5044248
[Epoch 49] ogbg-molbace: 0.721142 val loss: 0.745809
[Epoch 49] ogbg-molbace: 0.737811 test loss: 1.027546
[Epoch 50; Iter     6/   36] train: loss: 0.4181548
[Epoch 50; Iter    36/   36] train: loss: 0.1399563
[Epoch 50] ogbg-molbace: 0.739535 val loss: 0.666996
[Epoch 50] ogbg-molbace: 0.868364 test loss: 0.516263
[Epoch 51; Iter    30/   36] train: loss: 0.3900214
[Epoch 51] ogbg-molbace: 0.778118 val loss: 0.549123
[Epoch 51] ogbg-molbace: 0.840737 test loss: 0.614441
[Epoch 52; Iter    24/   36] train: loss: 0.2934748
[Epoch 52] ogbg-molbace: 0.755180 val loss: 0.498753
[Epoch 52] ogbg-molbace: 0.853467 test loss: 0.888662
[Epoch 53; Iter    18/   36] train: loss: 0.2271714
[Epoch 53] ogbg-molbace: 0.772304 val loss: 0.635452
[Epoch 53] ogbg-molbace: 0.862405 test loss: 0.481685
[Epoch 54; Iter    12/   36] train: loss: 0.2176074
[Epoch 54] ogbg-molbace: 0.797886 val loss: 0.482378
[Epoch 54] ogbg-molbace: 0.879379 test loss: 0.706001
[Epoch 55; Iter     6/   36] train: loss: 0.1642209
[Epoch 55; Iter    36/   36] train: loss: 0.2923647
[Epoch 55] ogbg-molbace: 0.780444 val loss: 0.696857
[Epoch 55] ogbg-molbace: 0.859065 test loss: 0.470840
[Epoch 56; Iter    30/   36] train: loss: 0.1895840
[Epoch 56] ogbg-molbace: 0.783510 val loss: 0.561147
[Epoch 56] ogbg-molbace: 0.850126 test loss: 0.652290
[Epoch 57; Iter    24/   36] train: loss: 0.2935680
[Epoch 57] ogbg-molbace: 0.792283 val loss: 0.609950
[Epoch 57] ogbg-molbace: 0.856356 test loss: 0.578634
[Epoch 58; Iter    18/   36] train: loss: 0.2917945
[Epoch 58] ogbg-molbace: 0.791015 val loss: 0.747338
[Epoch 58] ogbg-molbace: 0.867642 test loss: 0.450707
[Epoch 59; Iter    12/   36] train: loss: 0.2305808
[Epoch 59] ogbg-molbace: 0.815962 val loss: 1.040857
[Epoch 59] ogbg-molbace: 0.851932 test loss: 0.473709
[Epoch 60; Iter     6/   36] train: loss: 0.2428739
[Epoch 60; Iter    36/   36] train: loss: 0.2162234
[Epoch 60] ogbg-molbace: 0.725053 val loss: 0.672585
[Epoch 60] ogbg-molbace: 0.839292 test loss: 1.053396
[Epoch 61; Iter    30/   36] train: loss: 0.1944675
[Epoch 61] ogbg-molbace: 0.739535 val loss: 1.156592
[Epoch 61] ogbg-molbace: 0.806519 test loss: 3.425603
[Epoch 62; Iter    24/   36] train: loss: 0.2732695
[Epoch 62] ogbg-molbace: 0.757822 val loss: 0.538663
[Epoch 62] ogbg-molbace: 0.853467 test loss: 0.749806
[Epoch 63; Iter    18/   36] train: loss: 0.3594546
[Epoch 63] ogbg-molbace: 0.785307 val loss: 0.901866
[Epoch 63] ogbg-molbace: 0.859606 test loss: 0.500121
[Epoch 64; Iter    12/   36] train: loss: 0.3898976
[Epoch 64] ogbg-molbace: 0.773996 val loss: 0.700610
[Epoch 64] ogbg-molbace: 0.878386 test loss: 0.505023
[Epoch 65; Iter     6/   36] train: loss: 0.4058670
[Epoch 65; Iter    36/   36] train: loss: 0.4434065
[Epoch 65] ogbg-molbace: 0.752537 val loss: 0.720289
[Epoch 65] ogbg-molbace: 0.834236 test loss: 0.756792
[Epoch 66; Iter    30/   36] train: loss: 0.1459522
[Epoch 66] ogbg-molbace: 0.761522 val loss: 0.644859
[Epoch 66] ogbg-molbace: 0.859697 test loss: 0.665446
[Epoch 67; Iter    24/   36] train: loss: 0.3060532
[Epoch 67] ogbg-molbace: 0.802114 val loss: 0.536692
[Epoch 67] ogbg-molbace: 0.832160 test loss: 1.112247
[Epoch 68; Iter    18/   36] train: loss: 0.2446156
[Epoch 68] ogbg-molbace: 0.789218 val loss: 0.708708
[Epoch 68] ogbg-molbace: 0.844800 test loss: 0.581512
[Epoch 69; Iter    12/   36] train: loss: 0.1289918
[Epoch 69] ogbg-molbace: 0.766596 val loss: 0.626414
[Epoch 69] ogbg-molbace: 0.850126 test loss: 0.765589
[Epoch 70; Iter     6/   36] train: loss: 0.1237161
[Epoch 70; Iter    36/   36] train: loss: 0.6654267
[Epoch 70] ogbg-molbace: 0.779175 val loss: 0.679461
[Epoch 70] ogbg-molbace: 0.855273 test loss: 0.643918
[Epoch 71; Iter    30/   36] train: loss: 0.1464826
[Epoch 71] ogbg-molbace: 0.797674 val loss: 0.709788
[Epoch 71] ogbg-molbace: 0.856898 test loss: 0.735453
[Epoch 72; Iter    24/   36] train: loss: 0.0966453
[Epoch 72] ogbg-molbace: 0.768605 val loss: 0.682169
[Epoch 72] ogbg-molbace: 0.826833 test loss: 0.611474
[Epoch 73; Iter    18/   36] train: loss: 0.2914342
[Epoch 73] ogbg-molbace: 0.778118 val loss: 0.803454
[Epoch 73] ogbg-molbace: 0.848772 test loss: 0.708078
[Epoch 74; Iter    12/   36] train: loss: 0.1638734
[Epoch 74] ogbg-molbace: 0.749049 val loss: 0.672429
[Epoch 74] ogbg-molbace: 0.831076 test loss: 0.903890
[Epoch 75; Iter     6/   36] train: loss: 0.2211633
[Epoch 75; Iter    36/   36] train: loss: 0.1028275
[Epoch 75] ogbg-molbace: 0.792600 val loss: 0.724878
[Epoch 75] ogbg-molbace: 0.868364 test loss: 0.580019
[Epoch 76; Iter    30/   36] train: loss: 0.0549037
[Epoch 76] ogbg-molbace: 0.792495 val loss: 0.697281
[Epoch 76] ogbg-molbace: 0.863218 test loss: 0.676347
[Epoch 77; Iter    24/   36] train: loss: 0.1773187
[Epoch 77] ogbg-molbace: 0.793763 val loss: 0.729011
[Epoch 77] ogbg-molbace: 0.861502 test loss: 0.647877
[Epoch 78; Iter    18/   36] train: loss: 0.0476058
[Epoch 78] ogbg-molbace: 0.780127 val loss: 0.744354
[Epoch 78] ogbg-molbace: 0.864753 test loss: 0.688698
[Epoch 79; Iter    12/   36] train: loss: 0.1438644
[Epoch 79] ogbg-molbace: 0.782875 val loss: 0.710930
[Epoch 79] ogbg-molbace: 0.857891 test loss: 0.730904
[Epoch 80; Iter     6/   36] train: loss: 0.0892004
[Epoch 80; Iter    36/   36] train: loss: 0.3303681
[Epoch 80] ogbg-molbace: 0.792600 val loss: 0.627259
[Epoch 80] ogbg-molbace: 0.862857 test loss: 0.836306
[Epoch 81; Iter    30/   36] train: loss: 0.0812251
[Epoch 81] ogbg-molbace: 0.794609 val loss: 0.714461
[Epoch 81] ogbg-molbace: 0.857891 test loss: 0.762731
[Epoch 82; Iter    24/   36] train: loss: 0.1060699
[Epoch 36; Iter    25/   31] train: loss: 0.5911335
[Epoch 36] ogbg-molbace: 0.743893 val loss: 0.502404
[Epoch 36] ogbg-molbace: 0.773285 test loss: 0.684556
[Epoch 37; Iter    24/   31] train: loss: 0.3824892
[Epoch 37] ogbg-molbace: 0.739424 val loss: 0.451853
[Epoch 37] ogbg-molbace: 0.741758 test loss: 0.703356
[Epoch 38; Iter    23/   31] train: loss: 0.3466721
[Epoch 38] ogbg-molbace: 0.786197 val loss: 0.408129
[Epoch 38] ogbg-molbace: 0.768545 test loss: 0.691437
[Epoch 39; Iter    22/   31] train: loss: 0.3698354
[Epoch 39] ogbg-molbace: 0.756008 val loss: 0.834198
[Epoch 39] ogbg-molbace: 0.761694 test loss: 0.610937
[Epoch 40; Iter    21/   31] train: loss: 0.4697188
[Epoch 40] ogbg-molbace: 0.747269 val loss: 0.563852
[Epoch 40] ogbg-molbace: 0.775294 test loss: 0.601238
[Epoch 41; Iter    20/   31] train: loss: 0.4839255
[Epoch 41] ogbg-molbace: 0.770804 val loss: 0.519957
[Epoch 41] ogbg-molbace: 0.765970 test loss: 0.632056
[Epoch 42; Iter    19/   31] train: loss: 0.3816597
[Epoch 42] ogbg-molbace: 0.673287 val loss: 0.569187
[Epoch 42] ogbg-molbace: 0.717391 test loss: 0.796230
[Epoch 43; Iter    18/   31] train: loss: 0.3681447
[Epoch 43] ogbg-molbace: 0.791857 val loss: 0.476773
[Epoch 43] ogbg-molbace: 0.762054 test loss: 0.722368
[Epoch 44; Iter    17/   31] train: loss: 0.6603436
[Epoch 44] ogbg-molbace: 0.766336 val loss: 0.413887
[Epoch 44] ogbg-molbace: 0.744746 test loss: 0.739367
[Epoch 45; Iter    16/   31] train: loss: 0.4050316
[Epoch 45] ogbg-molbace: 0.748759 val loss: 0.681400
[Epoch 45] ogbg-molbace: 0.718679 test loss: 0.723207
[Epoch 46; Iter    15/   31] train: loss: 0.5704563
[Epoch 46] ogbg-molbace: 0.711519 val loss: 0.515355
[Epoch 46] ogbg-molbace: 0.741964 test loss: 0.714341
[Epoch 47; Iter    14/   31] train: loss: 0.3764045
[Epoch 47] ogbg-molbace: 0.695333 val loss: 0.855160
[Epoch 47] ogbg-molbace: 0.694055 test loss: 0.742364
[Epoch 48; Iter    13/   31] train: loss: 0.5990227
[Epoch 48] ogbg-molbace: 0.803972 val loss: 0.493157
[Epoch 48] ogbg-molbace: 0.767566 test loss: 0.696496
[Epoch 49; Iter    12/   31] train: loss: 0.3787377
[Epoch 49] ogbg-molbace: 0.765243 val loss: 0.612042
[Epoch 49] ogbg-molbace: 0.766279 test loss: 0.715568
[Epoch 50; Iter    11/   31] train: loss: 0.3372124
[Epoch 50] ogbg-molbace: 0.767726 val loss: 0.365472
[Epoch 50] ogbg-molbace: 0.742170 test loss: 1.015782
[Epoch 51; Iter    10/   31] train: loss: 0.3898450
[Epoch 51] ogbg-molbace: 0.795631 val loss: 0.578005
[Epoch 51] ogbg-molbace: 0.737276 test loss: 0.780175
[Epoch 52; Iter     9/   31] train: loss: 0.4057587
[Epoch 52] ogbg-molbace: 0.832572 val loss: 0.387258
[Epoch 52] ogbg-molbace: 0.772409 test loss: 0.757002
[Epoch 53; Iter     8/   31] train: loss: 0.3768635
[Epoch 53] ogbg-molbace: 0.740318 val loss: 0.751145
[Epoch 53] ogbg-molbace: 0.761797 test loss: 0.762772
[Epoch 54; Iter     7/   31] train: loss: 0.2111222
[Epoch 54] ogbg-molbace: 0.742701 val loss: 0.582378
[Epoch 54] ogbg-molbace: 0.744539 test loss: 0.913601
[Epoch 55; Iter     6/   31] train: loss: 0.4131621
[Epoch 55] ogbg-molbace: 0.706058 val loss: 0.501806
[Epoch 55] ogbg-molbace: 0.801154 test loss: 0.651657
[Epoch 56; Iter     5/   31] train: loss: 0.3702861
[Epoch 56] ogbg-molbace: 0.787885 val loss: 0.729814
[Epoch 56] ogbg-molbace: 0.747270 test loss: 0.778942
[Epoch 57; Iter     4/   31] train: loss: 0.3424814
[Epoch 57] ogbg-molbace: 0.717577 val loss: 0.404344
[Epoch 57] ogbg-molbace: 0.759685 test loss: 1.735224
[Epoch 58; Iter     3/   31] train: loss: 0.5496494
[Epoch 58] ogbg-molbace: 0.790566 val loss: 0.545449
[Epoch 58] ogbg-molbace: 0.761230 test loss: 0.852587
[Epoch 59; Iter     2/   31] train: loss: 0.2934405
[Epoch 59] ogbg-molbace: 0.783118 val loss: 0.535738
[Epoch 59] ogbg-molbace: 0.777869 test loss: 0.884246
[Epoch 60; Iter     1/   31] train: loss: 0.1903280
[Epoch 60; Iter    31/   31] train: loss: 0.2724699
[Epoch 60] ogbg-molbace: 0.761072 val loss: 0.491205
[Epoch 60] ogbg-molbace: 0.756697 test loss: 0.888780
[Epoch 61; Iter    30/   31] train: loss: 0.2530014
[Epoch 61] ogbg-molbace: 0.820258 val loss: 0.472307
[Epoch 61] ogbg-molbace: 0.769730 test loss: 0.870440
[Epoch 62; Iter    29/   31] train: loss: 0.3401264
[Epoch 62] ogbg-molbace: 0.715492 val loss: 0.661669
[Epoch 62] ogbg-molbace: 0.776478 test loss: 0.758700
[Epoch 63; Iter    28/   31] train: loss: 0.2332037
[Epoch 63] ogbg-molbace: 0.725621 val loss: 0.465539
[Epoch 63] ogbg-molbace: 0.763239 test loss: 1.372859
[Epoch 64; Iter    27/   31] train: loss: 0.1762465
[Epoch 64] ogbg-molbace: 0.744191 val loss: 0.637938
[Epoch 64] ogbg-molbace: 0.772615 test loss: 0.743731
[Epoch 65; Iter    26/   31] train: loss: 0.2495184
[Epoch 65] ogbg-molbace: 0.770904 val loss: 0.566209
[Epoch 65] ogbg-molbace: 0.784463 test loss: 0.740719
[Epoch 66; Iter    25/   31] train: loss: 0.3126388
[Epoch 66] ogbg-molbace: 0.704866 val loss: 0.534270
[Epoch 66] ogbg-molbace: 0.763651 test loss: 0.860009
[Epoch 67; Iter    24/   31] train: loss: 0.7277091
[Epoch 67] ogbg-molbace: 0.784906 val loss: 0.460200
[Epoch 67] ogbg-molbace: 0.789821 test loss: 0.769965
[Epoch 68; Iter    23/   31] train: loss: 0.1752477
[Epoch 68] ogbg-molbace: 0.800794 val loss: 0.544876
[Epoch 68] ogbg-molbace: 0.795127 test loss: 0.699949
[Epoch 69; Iter    22/   31] train: loss: 0.2680124
[Epoch 69] ogbg-molbace: 0.820159 val loss: 0.456365
[Epoch 69] ogbg-molbace: 0.779981 test loss: 0.926178
[Epoch 70; Iter    21/   31] train: loss: 0.2670108
[Epoch 70] ogbg-molbace: 0.761072 val loss: 0.388615
[Epoch 70] ogbg-molbace: 0.792654 test loss: 1.025542
[Epoch 71; Iter    20/   31] train: loss: 0.4209493
[Epoch 71] ogbg-molbace: 0.739424 val loss: 0.436490
[Epoch 71] ogbg-molbace: 0.768803 test loss: 1.114606
[Epoch 72; Iter    19/   31] train: loss: 0.2975858
[Epoch 72] ogbg-molbace: 0.782423 val loss: 0.679139
[Epoch 72] ogbg-molbace: 0.797239 test loss: 0.728359
[Epoch 73; Iter    18/   31] train: loss: 0.1239522
[Epoch 73] ogbg-molbace: 0.770010 val loss: 0.544112
[Epoch 73] ogbg-molbace: 0.793633 test loss: 0.789776
[Epoch 74; Iter    17/   31] train: loss: 0.2156946
[Epoch 74] ogbg-molbace: 0.786594 val loss: 0.441045
[Epoch 74] ogbg-molbace: 0.781939 test loss: 0.762508
[Epoch 75; Iter    16/   31] train: loss: 0.2338630
[Epoch 75] ogbg-molbace: 0.794538 val loss: 0.530179
[Epoch 75] ogbg-molbace: 0.762827 test loss: 1.114426
[Epoch 76; Iter    15/   31] train: loss: 0.3062856
[Epoch 76] ogbg-molbace: 0.772095 val loss: 0.586791
[Epoch 76] ogbg-molbace: 0.769885 test loss: 0.924225
[Epoch 77; Iter    14/   31] train: loss: 0.1125348
[Epoch 77] ogbg-molbace: 0.770506 val loss: 0.563721
[Epoch 77] ogbg-molbace: 0.768751 test loss: 1.054425
[Epoch 78; Iter    13/   31] train: loss: 0.2747887
[Epoch 78] ogbg-molbace: 0.768620 val loss: 0.530926
[Epoch 78] ogbg-molbace: 0.788533 test loss: 0.771677
[Epoch 79; Iter    12/   31] train: loss: 0.1282368
[Epoch 79] ogbg-molbace: 0.797021 val loss: 0.403003
[Epoch 79] ogbg-molbace: 0.785751 test loss: 0.921969
[Epoch 80; Iter    11/   31] train: loss: 0.1771392
[Epoch 80] ogbg-molbace: 0.813605 val loss: 0.545796
[Epoch 80] ogbg-molbace: 0.783587 test loss: 1.032494
[Epoch 81; Iter    10/   31] train: loss: 0.2337467
[Epoch 81] ogbg-molbace: 0.803873 val loss: 0.660146
[Epoch 81] ogbg-molbace: 0.780857 test loss: 0.877395
[Epoch 82; Iter     9/   31] train: loss: 0.2641551
[Epoch 82] ogbg-molbace: 0.806455 val loss: 0.355305
[Epoch 82] ogbg-molbace: 0.785442 test loss: 1.165603
[Epoch 83; Iter     8/   31] train: loss: 0.1313689
[Epoch 83] ogbg-molbace: 0.812016 val loss: 0.569504
[Epoch 83] ogbg-molbace: 0.780033 test loss: 1.056978
[Epoch 84; Iter     7/   31] train: loss: 0.2641101
[Epoch 84] ogbg-molbace: 0.736643 val loss: 0.736955
[Epoch 84] ogbg-molbace: 0.764270 test loss: 1.192255
[Epoch 85; Iter     6/   31] train: loss: 0.5155625
[Epoch 85] ogbg-molbace: 0.780437 val loss: 0.495354
[Epoch 85] ogbg-molbace: 0.791418 test loss: 0.830351
[Epoch 86; Iter     5/   31] train: loss: 0.1210937
[Epoch 86] ogbg-molbace: 0.761768 val loss: 0.461389
[Epoch 86] ogbg-molbace: 0.782454 test loss: 1.134393
[Epoch 36; Iter    25/   31] train: loss: 0.4800310
[Epoch 36] ogbg-molbace: 0.726216 val loss: 0.719612
[Epoch 36] ogbg-molbace: 0.744900 test loss: 0.666958
[Epoch 37; Iter    24/   31] train: loss: 0.4625090
[Epoch 37] ogbg-molbace: 0.748064 val loss: 0.478364
[Epoch 37] ogbg-molbace: 0.758500 test loss: 0.655151
[Epoch 38; Iter    23/   31] train: loss: 0.8162431
[Epoch 38] ogbg-molbace: 0.714796 val loss: 0.563585
[Epoch 38] ogbg-molbace: 0.740779 test loss: 0.783007
[Epoch 39; Iter    22/   31] train: loss: 0.6254357
[Epoch 39] ogbg-molbace: 0.767925 val loss: 0.416098
[Epoch 39] ogbg-molbace: 0.709252 test loss: 0.779618
[Epoch 40; Iter    21/   31] train: loss: 0.4526339
[Epoch 40] ogbg-molbace: 0.760973 val loss: 0.415967
[Epoch 40] ogbg-molbace: 0.757830 test loss: 0.723049
[Epoch 41; Iter    20/   31] train: loss: 0.3706575
[Epoch 41] ogbg-molbace: 0.794240 val loss: 0.530817
[Epoch 41] ogbg-molbace: 0.752988 test loss: 0.624654
[Epoch 42; Iter    19/   31] train: loss: 0.3530705
[Epoch 42] ogbg-molbace: 0.805859 val loss: 0.367999
[Epoch 42] ogbg-molbace: 0.764476 test loss: 0.768835
[Epoch 43; Iter    18/   31] train: loss: 0.4247907
[Epoch 43] ogbg-molbace: 0.795531 val loss: 0.439595
[Epoch 43] ogbg-molbace: 0.775500 test loss: 0.687924
[Epoch 44; Iter    17/   31] train: loss: 0.3197933
[Epoch 44] ogbg-molbace: 0.780536 val loss: 0.377526
[Epoch 44] ogbg-molbace: 0.772872 test loss: 0.778234
[Epoch 45; Iter    16/   31] train: loss: 0.5993156
[Epoch 45] ogbg-molbace: 0.779146 val loss: 0.375155
[Epoch 45] ogbg-molbace: 0.764270 test loss: 0.912177
[Epoch 46; Iter    15/   31] train: loss: 0.3258746
[Epoch 46] ogbg-molbace: 0.748361 val loss: 0.440495
[Epoch 46] ogbg-molbace: 0.756954 test loss: 0.940241
[Epoch 47; Iter    14/   31] train: loss: 0.4196198
[Epoch 47] ogbg-molbace: 0.717080 val loss: 0.700590
[Epoch 47] ogbg-molbace: 0.774263 test loss: 0.723182
[Epoch 48; Iter    13/   31] train: loss: 0.5132380
[Epoch 48] ogbg-molbace: 0.728600 val loss: 0.455178
[Epoch 48] ogbg-molbace: 0.708273 test loss: 1.295149
[Epoch 49; Iter    12/   31] train: loss: 0.3789324
[Epoch 49] ogbg-molbace: 0.781331 val loss: 0.463135
[Epoch 49] ogbg-molbace: 0.768648 test loss: 0.682221
[Epoch 50; Iter    11/   31] train: loss: 0.2961767
[Epoch 50] ogbg-molbace: 0.757100 val loss: 0.505243
[Epoch 50] ogbg-molbace: 0.768648 test loss: 0.822646
[Epoch 51; Iter    10/   31] train: loss: 0.4019186
[Epoch 51] ogbg-molbace: 0.801390 val loss: 0.796227
[Epoch 51] ogbg-molbace: 0.769318 test loss: 0.715074
[Epoch 52; Iter     9/   31] train: loss: 0.3270646
[Epoch 52] ogbg-molbace: 0.726117 val loss: 0.577673
[Epoch 52] ogbg-molbace: 0.759788 test loss: 0.866387
[Epoch 53; Iter     8/   31] train: loss: 0.4398448
[Epoch 53] ogbg-molbace: 0.788580 val loss: 0.371862
[Epoch 53] ogbg-molbace: 0.758603 test loss: 1.188767
[Epoch 54; Iter     7/   31] train: loss: 0.4761862
[Epoch 54] ogbg-molbace: 0.753923 val loss: 0.566831
[Epoch 54] ogbg-molbace: 0.780806 test loss: 0.655261
[Epoch 55; Iter     6/   31] train: loss: 0.4160083
[Epoch 55] ogbg-molbace: 0.692651 val loss: 0.851629
[Epoch 55] ogbg-molbace: 0.749279 test loss: 0.737800
[Epoch 56; Iter     5/   31] train: loss: 0.3220672
[Epoch 56] ogbg-molbace: 0.712612 val loss: 0.443843
[Epoch 56] ogbg-molbace: 0.697558 test loss: 1.081990
[Epoch 57; Iter     4/   31] train: loss: 0.5034940
[Epoch 57] ogbg-molbace: 0.751440 val loss: 0.559669
[Epoch 57] ogbg-molbace: 0.721564 test loss: 0.790530
[Epoch 58; Iter     3/   31] train: loss: 0.2499033
[Epoch 58] ogbg-molbace: 0.761867 val loss: 0.603915
[Epoch 58] ogbg-molbace: 0.772718 test loss: 0.692855
[Epoch 59; Iter     2/   31] train: loss: 0.3446432
[Epoch 59] ogbg-molbace: 0.782920 val loss: 0.448789
[Epoch 59] ogbg-molbace: 0.774830 test loss: 0.812669
[Epoch 60; Iter     1/   31] train: loss: 0.2967709
[Epoch 60; Iter    31/   31] train: loss: 0.4537367
[Epoch 60] ogbg-molbace: 0.767825 val loss: 0.520429
[Epoch 60] ogbg-molbace: 0.804039 test loss: 0.667511
[Epoch 61; Iter    30/   31] train: loss: 0.1915517
[Epoch 61] ogbg-molbace: 0.781331 val loss: 0.392205
[Epoch 61] ogbg-molbace: 0.778281 test loss: 0.847136
[Epoch 62; Iter    29/   31] train: loss: 0.3354023
[Epoch 62] ogbg-molbace: 0.770506 val loss: 1.094215
[Epoch 62] ogbg-molbace: 0.778281 test loss: 0.871123
[Epoch 63; Iter    28/   31] train: loss: 0.4104719
[Epoch 63] ogbg-molbace: 0.707448 val loss: 0.510894
[Epoch 63] ogbg-molbace: 0.772254 test loss: 0.914765
[Epoch 64; Iter    27/   31] train: loss: 0.3308252
[Epoch 64] ogbg-molbace: 0.738828 val loss: 0.588552
[Epoch 64] ogbg-molbace: 0.754997 test loss: 0.783074
[Epoch 65; Iter    26/   31] train: loss: 0.3284937
[Epoch 65] ogbg-molbace: 0.749950 val loss: 0.418015
[Epoch 65] ogbg-molbace: 0.776427 test loss: 0.950280
[Epoch 66; Iter    25/   31] train: loss: 0.3019364
[Epoch 66] ogbg-molbace: 0.796425 val loss: 0.451518
[Epoch 66] ogbg-molbace: 0.745467 test loss: 0.793707
[Epoch 67; Iter    24/   31] train: loss: 0.2103389
[Epoch 67] ogbg-molbace: 0.800794 val loss: 0.520027
[Epoch 67] ogbg-molbace: 0.787606 test loss: 0.733492
[Epoch 68; Iter    23/   31] train: loss: 0.3360869
[Epoch 68] ogbg-molbace: 0.730586 val loss: 0.579457
[Epoch 68] ogbg-molbace: 0.792345 test loss: 0.705194
[Epoch 69; Iter    22/   31] train: loss: 0.2257786
[Epoch 69] ogbg-molbace: 0.743793 val loss: 0.445145
[Epoch 69] ogbg-molbace: 0.768803 test loss: 1.004643
[Epoch 70; Iter    21/   31] train: loss: 0.2367421
[Epoch 70] ogbg-molbace: 0.751539 val loss: 0.698789
[Epoch 70] ogbg-molbace: 0.774366 test loss: 0.739384
[Epoch 71; Iter    20/   31] train: loss: 0.1399688
[Epoch 71] ogbg-molbace: 0.782026 val loss: 0.502348
[Epoch 71] ogbg-molbace: 0.740521 test loss: 1.019701
[Epoch 72; Iter    19/   31] train: loss: 0.1386253
[Epoch 72] ogbg-molbace: 0.761768 val loss: 0.535976
[Epoch 72] ogbg-molbace: 0.773748 test loss: 0.884468
[Epoch 73; Iter    18/   31] train: loss: 0.3566191
[Epoch 73] ogbg-molbace: 0.735154 val loss: 0.688589
[Epoch 73] ogbg-molbace: 0.787915 test loss: 0.797872
[Epoch 74; Iter    17/   31] train: loss: 0.2633659
[Epoch 74] ogbg-molbace: 0.752036 val loss: 0.432713
[Epoch 74] ogbg-molbace: 0.773233 test loss: 0.939998
[Epoch 75; Iter    16/   31] train: loss: 0.2580124
[Epoch 75] ogbg-molbace: 0.735551 val loss: 0.665299
[Epoch 75] ogbg-molbace: 0.773851 test loss: 0.793326
[Epoch 76; Iter    15/   31] train: loss: 0.0846777
[Epoch 76] ogbg-molbace: 0.791162 val loss: 0.548013
[Epoch 76] ogbg-molbace: 0.781578 test loss: 1.096173
[Epoch 77; Iter    14/   31] train: loss: 0.2639360
[Epoch 77] ogbg-molbace: 0.752632 val loss: 0.421004
[Epoch 77] ogbg-molbace: 0.791366 test loss: 0.926069
[Epoch 78; Iter    13/   31] train: loss: 0.2900994
[Epoch 78] ogbg-molbace: 0.759484 val loss: 0.601693
[Epoch 78] ogbg-molbace: 0.770503 test loss: 0.884175
[Epoch 79; Iter    12/   31] train: loss: 0.3633457
[Epoch 79] ogbg-molbace: 0.761867 val loss: 0.467906
[Epoch 79] ogbg-molbace: 0.788945 test loss: 0.860958
[Epoch 80; Iter    11/   31] train: loss: 0.1589978
[Epoch 80] ogbg-molbace: 0.776564 val loss: 0.572463
[Epoch 80] ogbg-molbace: 0.786060 test loss: 0.781062
[Epoch 81; Iter    10/   31] train: loss: 0.3656740
[Epoch 81] ogbg-molbace: 0.732075 val loss: 0.491308
[Epoch 81] ogbg-molbace: 0.806151 test loss: 0.913103
[Epoch 82; Iter     9/   31] train: loss: 0.0657168
[Epoch 82] ogbg-molbace: 0.789374 val loss: 0.494877
[Epoch 82] ogbg-molbace: 0.809036 test loss: 0.791721
[Epoch 83; Iter     8/   31] train: loss: 0.1461695
[Epoch 83] ogbg-molbace: 0.788580 val loss: 0.519174
[Epoch 83] ogbg-molbace: 0.813981 test loss: 0.725863
[Epoch 84; Iter     7/   31] train: loss: 0.1415557
[Epoch 84] ogbg-molbace: 0.768520 val loss: 0.512208
[Epoch 84] ogbg-molbace: 0.806099 test loss: 0.806630
[Epoch 85; Iter     6/   31] train: loss: 0.0569085
[Epoch 85] ogbg-molbace: 0.776068 val loss: 0.536972
[Epoch 85] ogbg-molbace: 0.799454 test loss: 0.870864
[Epoch 86; Iter     5/   31] train: loss: 0.1545519
[Epoch 86] ogbg-molbace: 0.728997 val loss: 0.619228
[Epoch 86] ogbg-molbace: 0.789924 test loss: 0.837718
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 87; Iter     4/   31] train: loss: 0.1942951
[Epoch 87] ogbg-molbace: 0.775670 val loss: 0.777510
[Epoch 87] ogbg-molbace: 0.806717 test loss: 0.909481
[Epoch 88; Iter     3/   31] train: loss: 0.1664789
[Epoch 88] ogbg-molbace: 0.746773 val loss: 0.730977
[Epoch 88] ogbg-molbace: 0.794972 test loss: 0.905368
[Epoch 89; Iter     2/   31] train: loss: 0.3106739
[Epoch 89] ogbg-molbace: 0.794935 val loss: 0.814376
[Epoch 89] ogbg-molbace: 0.784824 test loss: 0.847236
[Epoch 90; Iter     1/   31] train: loss: 0.0876898
[Epoch 90; Iter    31/   31] train: loss: 0.0669014
[Epoch 90] ogbg-molbace: 0.746971 val loss: 0.533754
[Epoch 90] ogbg-molbace: 0.788430 test loss: 0.959555
[Epoch 91; Iter    30/   31] train: loss: 0.1211388
[Epoch 91] ogbg-molbace: 0.784508 val loss: 0.489952
[Epoch 91] ogbg-molbace: 0.777715 test loss: 1.049874
[Epoch 92; Iter    29/   31] train: loss: 0.2625707
[Epoch 92] ogbg-molbace: 0.772691 val loss: 0.791166
[Epoch 92] ogbg-molbace: 0.821708 test loss: 0.770280
[Epoch 93; Iter    28/   31] train: loss: 0.3405661
[Epoch 93] ogbg-molbace: 0.769215 val loss: 0.476089
[Epoch 93] ogbg-molbace: 0.794818 test loss: 1.008913
[Epoch 94; Iter    27/   31] train: loss: 0.1455975
[Epoch 94] ogbg-molbace: 0.776266 val loss: 0.571903
[Epoch 94] ogbg-molbace: 0.780033 test loss: 1.123259
[Epoch 95; Iter    26/   31] train: loss: 0.0426631
[Epoch 95] ogbg-molbace: 0.756802 val loss: 0.529043
[Epoch 95] ogbg-molbace: 0.789306 test loss: 1.096128
[Epoch 96; Iter    25/   31] train: loss: 0.1143718
[Epoch 96] ogbg-molbace: 0.759782 val loss: 0.626971
[Epoch 96] ogbg-molbace: 0.788224 test loss: 1.040093
[Epoch 97; Iter    24/   31] train: loss: 0.0751719
[Epoch 97] ogbg-molbace: 0.752632 val loss: 0.601525
[Epoch 97] ogbg-molbace: 0.775242 test loss: 1.207594
[Epoch 98; Iter    23/   31] train: loss: 0.1994984
[Epoch 98] ogbg-molbace: 0.793545 val loss: 0.699119
[Epoch 98] ogbg-molbace: 0.792654 test loss: 1.039511
[Epoch 99; Iter    22/   31] train: loss: 0.2620437
[Epoch 99] ogbg-molbace: 0.749355 val loss: 0.644645
[Epoch 99] ogbg-molbace: 0.784309 test loss: 1.196335
[Epoch 100; Iter    21/   31] train: loss: 0.1757267
[Epoch 100] ogbg-molbace: 0.769215 val loss: 0.626536
[Epoch 100] ogbg-molbace: 0.791315 test loss: 1.215832
[Epoch 101; Iter    20/   31] train: loss: 0.1692399
[Epoch 101] ogbg-molbace: 0.749553 val loss: 0.595117
[Epoch 101] ogbg-molbace: 0.790181 test loss: 1.210234
[Epoch 102; Iter    19/   31] train: loss: 0.0766077
[Epoch 102] ogbg-molbace: 0.802880 val loss: 0.632326
[Epoch 102] ogbg-molbace: 0.784051 test loss: 1.152720
[Epoch 103; Iter    18/   31] train: loss: 0.0689434
[Epoch 103] ogbg-molbace: 0.755611 val loss: 0.677488
[Epoch 103] ogbg-molbace: 0.802030 test loss: 1.197319
[Epoch 104; Iter    17/   31] train: loss: 0.3402473
[Epoch 104] ogbg-molbace: 0.750745 val loss: 0.674216
[Epoch 104] ogbg-molbace: 0.789203 test loss: 1.201703
[Epoch 105; Iter    16/   31] train: loss: 0.0715414
[Epoch 105] ogbg-molbace: 0.806058 val loss: 0.585888
[Epoch 105] ogbg-molbace: 0.772563 test loss: 1.164806
[Epoch 106; Iter    15/   31] train: loss: 0.0284575
[Epoch 106] ogbg-molbace: 0.806256 val loss: 0.549452
[Epoch 106] ogbg-molbace: 0.785494 test loss: 1.114389
[Epoch 107; Iter    14/   31] train: loss: 0.0744805
[Epoch 107] ogbg-molbace: 0.768719 val loss: 0.681464
[Epoch 107] ogbg-molbace: 0.784566 test loss: 1.239007
[Epoch 108; Iter    13/   31] train: loss: 0.1244509
[Epoch 108] ogbg-molbace: 0.748361 val loss: 0.591332
[Epoch 108] ogbg-molbace: 0.766176 test loss: 1.222084
[Epoch 109; Iter    12/   31] train: loss: 0.1440188
[Epoch 109] ogbg-molbace: 0.733565 val loss: 0.825498
[Epoch 109] ogbg-molbace: 0.800381 test loss: 1.098858
[Epoch 110; Iter    11/   31] train: loss: 0.0219263
[Epoch 110] ogbg-molbace: 0.767825 val loss: 0.647145
[Epoch 110] ogbg-molbace: 0.800072 test loss: 1.183451
[Epoch 111; Iter    10/   31] train: loss: 0.0259114
[Epoch 111] ogbg-molbace: 0.782224 val loss: 0.613026
[Epoch 111] ogbg-molbace: 0.797857 test loss: 1.102583
[Epoch 112; Iter     9/   31] train: loss: 0.0332079
[Epoch 112] ogbg-molbace: 0.772989 val loss: 0.809109
[Epoch 112] ogbg-molbace: 0.796878 test loss: 1.148286
[Epoch 113; Iter     8/   31] train: loss: 0.0452543
[Epoch 113] ogbg-molbace: 0.782324 val loss: 0.640201
[Epoch 113] ogbg-molbace: 0.796312 test loss: 1.339746
[Epoch 114; Iter     7/   31] train: loss: 0.0616245
[Epoch 114] ogbg-molbace: 0.810725 val loss: 0.660934
[Epoch 114] ogbg-molbace: 0.797909 test loss: 1.115864
[Epoch 115; Iter     6/   31] train: loss: 0.0206991
[Epoch 115] ogbg-molbace: 0.786892 val loss: 0.663355
[Epoch 115] ogbg-molbace: 0.798681 test loss: 1.275671
[Epoch 116; Iter     5/   31] train: loss: 0.0302267
[Epoch 116] ogbg-molbace: 0.740914 val loss: 0.851043
[Epoch 116] ogbg-molbace: 0.768803 test loss: 1.304199
[Epoch 117; Iter     4/   31] train: loss: 0.0458421
[Epoch 117] ogbg-molbace: 0.805065 val loss: 0.620345
[Epoch 117] ogbg-molbace: 0.788636 test loss: 1.244624
[Epoch 118; Iter     3/   31] train: loss: 0.0651944
[Epoch 118] ogbg-molbace: 0.751043 val loss: 0.734612
[Epoch 118] ogbg-molbace: 0.783278 test loss: 1.225896
[Epoch 119; Iter     2/   31] train: loss: 0.0319576
[Epoch 119] ogbg-molbace: 0.791261 val loss: 0.619483
[Epoch 119] ogbg-molbace: 0.794663 test loss: 1.277937
[Epoch 120; Iter     1/   31] train: loss: 0.0362100
[Epoch 120; Iter    31/   31] train: loss: 0.5529654
[Epoch 120] ogbg-molbace: 0.766832 val loss: 0.774351
[Epoch 120] ogbg-molbace: 0.791624 test loss: 1.373322
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 52.
Statistics on  val_best_checkpoint
mean_pred: -1.4591747522354126
std_pred: 1.9931498765945435
mean_targets: 0.12541253864765167
std_targets: 0.33173397183418274
prcauc: 0.4621198122904393
rocauc: 0.8325719960278053
ogbg-molbace: 0.8325719960278053
BCEWithLogitsLoss: 0.3872575820846991
Statistics on  test
mean_pred: -0.19698679447174072
std_pred: 2.193105936050415
mean_targets: 0.6963696479797363
std_targets: 0.4605855941772461
prcauc: 0.8750773786690313
rocauc: 0.772408819287039
ogbg-molbace: 0.772408819287039
BCEWithLogitsLoss: 0.7570024633949454
Statistics on  train
mean_pred: -0.3953532576560974
std_pred: 2.3402340412139893
mean_targets: 0.48732084035873413
std_targets: 0.5001149773597717
prcauc: 0.9423982016676964
rocauc: 0.949978105386075
ogbg-molbace: 0.949978105386075
BCEWithLogitsLoss: 0.3113842914181371
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 87; Iter     4/   31] train: loss: 0.1787611
[Epoch 87] ogbg-molbace: 0.725223 val loss: 0.593406
[Epoch 87] ogbg-molbace: 0.788327 test loss: 0.940268
[Epoch 88; Iter     3/   31] train: loss: 0.0975289
[Epoch 88] ogbg-molbace: 0.764647 val loss: 0.561483
[Epoch 88] ogbg-molbace: 0.789975 test loss: 1.004289
[Epoch 89; Iter     2/   31] train: loss: 0.0746634
[Epoch 89] ogbg-molbace: 0.757200 val loss: 0.690985
[Epoch 89] ogbg-molbace: 0.805327 test loss: 0.794643
[Epoch 90; Iter     1/   31] train: loss: 0.1748044
[Epoch 90; Iter    31/   31] train: loss: 0.5656719
[Epoch 90] ogbg-molbace: 0.795531 val loss: 0.620483
[Epoch 90] ogbg-molbace: 0.750464 test loss: 1.040393
[Epoch 91; Iter    30/   31] train: loss: 0.2038410
[Epoch 91] ogbg-molbace: 0.762761 val loss: 0.468079
[Epoch 91] ogbg-molbace: 0.779209 test loss: 1.052572
[Epoch 92; Iter    29/   31] train: loss: 0.1180138
[Epoch 92] ogbg-molbace: 0.789474 val loss: 0.518052
[Epoch 92] ogbg-molbace: 0.771585 test loss: 0.858210
[Epoch 93; Iter    28/   31] train: loss: 0.2219005
[Epoch 93] ogbg-molbace: 0.756703 val loss: 0.561564
[Epoch 93] ogbg-molbace: 0.782197 test loss: 0.972672
[Epoch 94; Iter    27/   31] train: loss: 0.1748557
[Epoch 94] ogbg-molbace: 0.743595 val loss: 0.628737
[Epoch 94] ogbg-molbace: 0.781887 test loss: 0.942532
[Epoch 95; Iter    26/   31] train: loss: 0.1357405
[Epoch 95] ogbg-molbace: 0.749255 val loss: 1.913603
[Epoch 95] ogbg-molbace: 0.750876 test loss: 2.042995
[Epoch 96; Iter    25/   31] train: loss: 0.0367268
[Epoch 96] ogbg-molbace: 0.787686 val loss: 0.549501
[Epoch 96] ogbg-molbace: 0.787606 test loss: 0.999082
[Epoch 97; Iter    24/   31] train: loss: 0.0751051
[Epoch 97] ogbg-molbace: 0.750645 val loss: 0.596749
[Epoch 97] ogbg-molbace: 0.789563 test loss: 1.105515
[Epoch 98; Iter    23/   31] train: loss: 0.0694231
[Epoch 98] ogbg-molbace: 0.721251 val loss: 0.567177
[Epoch 98] ogbg-molbace: 0.788790 test loss: 1.147020
[Epoch 99; Iter    22/   31] train: loss: 0.1325937
[Epoch 99] ogbg-molbace: 0.785601 val loss: 0.616805
[Epoch 99] ogbg-molbace: 0.790336 test loss: 0.989571
[Epoch 100; Iter    21/   31] train: loss: 0.1381351
[Epoch 100] ogbg-molbace: 0.801688 val loss: 0.765084
[Epoch 100] ogbg-molbace: 0.774212 test loss: 1.137536
[Epoch 101; Iter    20/   31] train: loss: 0.0898881
[Epoch 101] ogbg-molbace: 0.756902 val loss: 0.708025
[Epoch 101] ogbg-molbace: 0.792087 test loss: 1.059602
[Epoch 102; Iter    19/   31] train: loss: 0.0551458
[Epoch 102] ogbg-molbace: 0.758391 val loss: 0.620959
[Epoch 102] ogbg-molbace: 0.795951 test loss: 1.074444
[Epoch 103; Iter    18/   31] train: loss: 0.0451982
[Epoch 103] ogbg-molbace: 0.776365 val loss: 0.668614
[Epoch 103] ogbg-molbace: 0.763651 test loss: 1.211278
[Epoch 104; Iter    17/   31] train: loss: 0.0653374
[Epoch 104] ogbg-molbace: 0.740814 val loss: 0.796885
[Epoch 104] ogbg-molbace: 0.774985 test loss: 1.311015
[Epoch 105; Iter    16/   31] train: loss: 0.0675484
[Epoch 105] ogbg-molbace: 0.778252 val loss: 0.772790
[Epoch 105] ogbg-molbace: 0.794251 test loss: 1.042153
[Epoch 106; Iter    15/   31] train: loss: 0.0853761
[Epoch 106] ogbg-molbace: 0.755313 val loss: 0.680836
[Epoch 106] ogbg-molbace: 0.774727 test loss: 1.242887
[Epoch 107; Iter    14/   31] train: loss: 0.1037786
[Epoch 107] ogbg-molbace: 0.756902 val loss: 0.885073
[Epoch 107] ogbg-molbace: 0.796312 test loss: 1.246616
[Epoch 108; Iter    13/   31] train: loss: 0.0643425
[Epoch 108] ogbg-molbace: 0.748957 val loss: 0.749247
[Epoch 108] ogbg-molbace: 0.784566 test loss: 1.164099
[Epoch 109; Iter    12/   31] train: loss: 0.0164175
[Epoch 109] ogbg-molbace: 0.749752 val loss: 0.737911
[Epoch 109] ogbg-molbace: 0.791212 test loss: 1.201320
[Epoch 110; Iter    11/   31] train: loss: 0.0595540
[Epoch 110] ogbg-molbace: 0.739424 val loss: 0.735813
[Epoch 110] ogbg-molbace: 0.799402 test loss: 1.158059
[Epoch 111; Iter    10/   31] train: loss: 0.0353297
[Epoch 111] ogbg-molbace: 0.769315 val loss: 0.680422
[Epoch 111] ogbg-molbace: 0.797548 test loss: 1.252791
[Epoch 112; Iter     9/   31] train: loss: 0.0274217
[Epoch 112] ogbg-molbace: 0.759484 val loss: 0.703339
[Epoch 112] ogbg-molbace: 0.786369 test loss: 1.326956
[Epoch 113; Iter     8/   31] train: loss: 0.0080373
[Epoch 113] ogbg-molbace: 0.753625 val loss: 0.799263
[Epoch 113] ogbg-molbace: 0.788996 test loss: 1.268326
[Epoch 114; Iter     7/   31] train: loss: 0.0140996
[Epoch 114] ogbg-molbace: 0.762860 val loss: 0.689213
[Epoch 114] ogbg-molbace: 0.774057 test loss: 1.398292
[Epoch 115; Iter     6/   31] train: loss: 0.0243839
[Epoch 115] ogbg-molbace: 0.758788 val loss: 0.870067
[Epoch 115] ogbg-molbace: 0.786266 test loss: 1.312654
[Epoch 116; Iter     5/   31] train: loss: 0.0058141
[Epoch 116] ogbg-molbace: 0.746375 val loss: 0.746507
[Epoch 116] ogbg-molbace: 0.789048 test loss: 1.442081
[Epoch 117; Iter     4/   31] train: loss: 0.0377799
[Epoch 117] ogbg-molbace: 0.729891 val loss: 0.876338
[Epoch 117] ogbg-molbace: 0.782042 test loss: 1.316531
[Epoch 118; Iter     3/   31] train: loss: 0.0216495
[Epoch 118] ogbg-molbace: 0.728500 val loss: 0.874719
[Epoch 118] ogbg-molbace: 0.777715 test loss: 1.484275
[Epoch 119; Iter     2/   31] train: loss: 0.0189774
[Epoch 119] ogbg-molbace: 0.758292 val loss: 0.769733
[Epoch 119] ogbg-molbace: 0.790336 test loss: 1.355541
[Epoch 120; Iter     1/   31] train: loss: 0.0089954
[Epoch 120; Iter    31/   31] train: loss: 0.0049845
[Epoch 120] ogbg-molbace: 0.748858 val loss: 0.871342
[Epoch 120] ogbg-molbace: 0.782248 test loss: 1.379705
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 42.
Statistics on  val_best_checkpoint
mean_pred: -1.3915716409683228
std_pred: 1.5026605129241943
mean_targets: 0.12541253864765167
std_targets: 0.33173397183418274
prcauc: 0.37434709620345125
rocauc: 0.8058589870903674
ogbg-molbace: 0.8058589870903674
BCEWithLogitsLoss: 0.3679989522153681
Statistics on  test
mean_pred: -0.4369048774242401
std_pred: 1.7982069253921509
mean_targets: 0.6963696479797363
std_targets: 0.4605855941772461
prcauc: 0.8498855799315297
rocauc: 0.7644755821141562
ogbg-molbace: 0.7644755821141562
BCEWithLogitsLoss: 0.7688351090658795
Statistics on  train
mean_pred: -0.485862135887146
std_pred: 1.7307230234146118
mean_targets: 0.48732084035873413
std_targets: 0.5001149773597717
prcauc: 0.865032553697926
rocauc: 0.8883423344523913
ogbg-molbace: 0.8883423344523913
BCEWithLogitsLoss: 0.4389499233615014
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 82] ogbg-molbace: 0.797992 val loss: 0.732871
[Epoch 82] ogbg-molbace: 0.854460 test loss: 0.804021
[Epoch 83; Iter    18/   36] train: loss: 0.1305324
[Epoch 83] ogbg-molbace: 0.787949 val loss: 0.759897
[Epoch 83] ogbg-molbace: 0.858072 test loss: 0.762097
[Epoch 84; Iter    12/   36] train: loss: 0.1387051
[Epoch 84] ogbg-molbace: 0.790909 val loss: 0.884996
[Epoch 84] ogbg-molbace: 0.852745 test loss: 0.762985
[Epoch 85; Iter     6/   36] train: loss: 0.1427668
[Epoch 85; Iter    36/   36] train: loss: 0.0326847
[Epoch 85] ogbg-molbace: 0.770613 val loss: 0.918503
[Epoch 85] ogbg-molbace: 0.852384 test loss: 0.732731
[Epoch 86; Iter    30/   36] train: loss: 0.1074324
[Epoch 86] ogbg-molbace: 0.779493 val loss: 0.695376
[Epoch 86] ogbg-molbace: 0.844709 test loss: 0.845506
[Epoch 87; Iter    24/   36] train: loss: 0.0959361
[Epoch 87] ogbg-molbace: 0.772939 val loss: 0.636368
[Epoch 87] ogbg-molbace: 0.861683 test loss: 0.732585
[Epoch 88; Iter    18/   36] train: loss: 0.1054807
[Epoch 88] ogbg-molbace: 0.773890 val loss: 1.038273
[Epoch 88] ogbg-molbace: 0.850307 test loss: 0.727413
[Epoch 89; Iter    12/   36] train: loss: 0.0528620
[Epoch 89] ogbg-molbace: 0.800846 val loss: 0.743546
[Epoch 89] ogbg-molbace: 0.840466 test loss: 0.928602
[Epoch 90; Iter     6/   36] train: loss: 0.1455479
[Epoch 90; Iter    36/   36] train: loss: 0.0360280
[Epoch 90] ogbg-molbace: 0.798097 val loss: 0.765982
[Epoch 90] ogbg-molbace: 0.851842 test loss: 0.776775
[Epoch 91; Iter    30/   36] train: loss: 0.0445580
[Epoch 91] ogbg-molbace: 0.797674 val loss: 0.812530
[Epoch 91] ogbg-molbace: 0.848140 test loss: 0.887196
[Epoch 92; Iter    24/   36] train: loss: 0.0446963
[Epoch 92] ogbg-molbace: 0.771036 val loss: 1.444521
[Epoch 92] ogbg-molbace: 0.840285 test loss: 0.775462
[Epoch 93; Iter    18/   36] train: loss: 0.0210001
[Epoch 93] ogbg-molbace: 0.758140 val loss: 0.818220
[Epoch 93] ogbg-molbace: 0.834958 test loss: 1.244360
[Epoch 94; Iter    12/   36] train: loss: 0.1296237
[Epoch 94] ogbg-molbace: 0.787104 val loss: 0.962171
[Epoch 94] ogbg-molbace: 0.857349 test loss: 0.791331
[Epoch 95; Iter     6/   36] train: loss: 0.0401128
[Epoch 95; Iter    36/   36] train: loss: 0.0225636
[Epoch 95] ogbg-molbace: 0.776533 val loss: 0.904707
[Epoch 95] ogbg-molbace: 0.857981 test loss: 0.978624
[Epoch 96; Iter    30/   36] train: loss: 0.1691928
[Epoch 96] ogbg-molbace: 0.804440 val loss: 0.745621
[Epoch 96] ogbg-molbace: 0.824034 test loss: 1.006912
[Epoch 97; Iter    24/   36] train: loss: 0.1112026
[Epoch 97] ogbg-molbace: 0.765751 val loss: 0.935459
[Epoch 97] ogbg-molbace: 0.864030 test loss: 0.868093
[Epoch 98; Iter    18/   36] train: loss: 0.1965890
[Epoch 98] ogbg-molbace: 0.804228 val loss: 0.876904
[Epoch 98] ogbg-molbace: 0.844529 test loss: 0.629684
[Epoch 99; Iter    12/   36] train: loss: 0.1535530
[Epoch 99] ogbg-molbace: 0.786786 val loss: 0.844951
[Epoch 99] ogbg-molbace: 0.851029 test loss: 0.900894
[Epoch 100; Iter     6/   36] train: loss: 0.2504961
[Epoch 100; Iter    36/   36] train: loss: 0.0385194
[Epoch 100] ogbg-molbace: 0.809619 val loss: 0.815273
[Epoch 100] ogbg-molbace: 0.852835 test loss: 0.844019
[Epoch 101; Iter    30/   36] train: loss: 0.0443827
[Epoch 101] ogbg-molbace: 0.813848 val loss: 0.768600
[Epoch 101] ogbg-molbace: 0.850939 test loss: 0.795459
[Epoch 102; Iter    24/   36] train: loss: 0.0922015
[Epoch 102] ogbg-molbace: 0.803488 val loss: 0.861081
[Epoch 102] ogbg-molbace: 0.852925 test loss: 0.742911
[Epoch 103; Iter    18/   36] train: loss: 0.1057006
[Epoch 103] ogbg-molbace: 0.806025 val loss: 0.742418
[Epoch 103] ogbg-molbace: 0.850307 test loss: 0.804814
[Epoch 104; Iter    12/   36] train: loss: 0.0501144
[Epoch 104] ogbg-molbace: 0.806765 val loss: 0.824027
[Epoch 104] ogbg-molbace: 0.856537 test loss: 0.883240
[Epoch 105; Iter     6/   36] train: loss: 0.2300799
[Epoch 105; Iter    36/   36] train: loss: 0.0464715
[Epoch 105] ogbg-molbace: 0.792706 val loss: 0.918059
[Epoch 105] ogbg-molbace: 0.856446 test loss: 0.940221
[Epoch 106; Iter    30/   36] train: loss: 0.0239249
[Epoch 106] ogbg-molbace: 0.800211 val loss: 0.892655
[Epoch 106] ogbg-molbace: 0.854821 test loss: 0.872861
[Epoch 107; Iter    24/   36] train: loss: 0.0266665
[Epoch 107] ogbg-molbace: 0.794820 val loss: 1.011559
[Epoch 107] ogbg-molbace: 0.851390 test loss: 0.868973
[Epoch 108; Iter    18/   36] train: loss: 0.0719653
[Epoch 108] ogbg-molbace: 0.796089 val loss: 0.932991
[Epoch 108] ogbg-molbace: 0.861322 test loss: 0.831645
[Epoch 109; Iter    12/   36] train: loss: 0.1282844
[Epoch 109] ogbg-molbace: 0.807505 val loss: 0.863153
[Epoch 109] ogbg-molbace: 0.847508 test loss: 1.014345
[Epoch 110; Iter     6/   36] train: loss: 0.0099317
[Epoch 110; Iter    36/   36] train: loss: 0.0295594
[Epoch 110] ogbg-molbace: 0.800634 val loss: 0.959687
[Epoch 110] ogbg-molbace: 0.844529 test loss: 0.887581
[Epoch 111; Iter    30/   36] train: loss: 0.0512681
[Epoch 111] ogbg-molbace: 0.799894 val loss: 0.921983
[Epoch 111] ogbg-molbace: 0.855814 test loss: 0.926077
[Epoch 112; Iter    24/   36] train: loss: 0.0756599
[Epoch 112] ogbg-molbace: 0.791121 val loss: 0.883512
[Epoch 112] ogbg-molbace: 0.826020 test loss: 1.077155
[Epoch 113; Iter    18/   36] train: loss: 0.0122594
[Epoch 113] ogbg-molbace: 0.775159 val loss: 1.135893
[Epoch 113] ogbg-molbace: 0.849494 test loss: 0.949000
[Epoch 114; Iter    12/   36] train: loss: 0.0308151
[Epoch 114] ogbg-molbace: 0.808457 val loss: 0.984465
[Epoch 114] ogbg-molbace: 0.824124 test loss: 1.084347
[Epoch 115; Iter     6/   36] train: loss: 0.0369600
[Epoch 115; Iter    36/   36] train: loss: 0.0047929
[Epoch 115] ogbg-molbace: 0.802748 val loss: 0.928880
[Epoch 115] ogbg-molbace: 0.831618 test loss: 1.060856
[Epoch 116; Iter    30/   36] train: loss: 0.0302801
[Epoch 116] ogbg-molbace: 0.809408 val loss: 1.076859
[Epoch 116] ogbg-molbace: 0.836042 test loss: 1.075001
[Epoch 117; Iter    24/   36] train: loss: 0.0240139
[Epoch 117] ogbg-molbace: 0.807505 val loss: 1.078298
[Epoch 117] ogbg-molbace: 0.840556 test loss: 1.022319
[Epoch 118; Iter    18/   36] train: loss: 0.0125624
[Epoch 118] ogbg-molbace: 0.797886 val loss: 0.894798
[Epoch 118] ogbg-molbace: 0.829902 test loss: 1.182187
[Epoch 119; Iter    12/   36] train: loss: 0.0252884
[Epoch 119] ogbg-molbace: 0.788478 val loss: 1.353112
[Epoch 119] ogbg-molbace: 0.841369 test loss: 0.984916
[Epoch 120; Iter     6/   36] train: loss: 0.0390046
[Epoch 120; Iter    36/   36] train: loss: 0.8499203
[Epoch 120] ogbg-molbace: 0.800423 val loss: 0.983920
[Epoch 120] ogbg-molbace: 0.827555 test loss: 1.301785
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 59.
Statistics on  val_best_checkpoint
mean_pred: 1.0210450887680054
std_pred: 2.4488847255706787
mean_targets: 0.24229073524475098
std_targets: 0.4294162094593048
prcauc: 0.5351408335504259
rocauc: 0.8159619450317125
ogbg-molbace: 0.8159619450317125
BCEWithLogitsLoss: 1.0408571045845747
Statistics on  test
mean_pred: 1.8081451654434204
std_pred: 3.4140114784240723
mean_targets: 0.6872246265411377
std_targets: 0.4646482765674591
prcauc: 0.8994774557654752
rocauc: 0.8519321054532323
ogbg-molbace: 0.8519321054532323
BCEWithLogitsLoss: 0.4737087795510888
Statistics on  train
mean_pred: 1.424721121788025
std_pred: 3.567643165588379
mean_targets: 0.45325779914855957
std_targets: 0.49804556369781494
prcauc: 0.9432031437994451
rocauc: 0.9618091537132987
ogbg-molbace: 0.9618091537132987
BCEWithLogitsLoss: 0.39711162116792464
[Epoch 87; Iter     4/   31] train: loss: 0.1295514
[Epoch 87] ogbg-molbace: 0.767925 val loss: 0.457267
[Epoch 87] ogbg-molbace: 0.780084 test loss: 1.196341
[Epoch 88; Iter     3/   31] train: loss: 0.4539787
[Epoch 88] ogbg-molbace: 0.814002 val loss: 0.548118
[Epoch 88] ogbg-molbace: 0.787657 test loss: 0.937358
[Epoch 89; Iter     2/   31] train: loss: 0.3530567
[Epoch 89] ogbg-molbace: 0.864449 val loss: 0.347511
[Epoch 89] ogbg-molbace: 0.804760 test loss: 0.905118
[Epoch 90; Iter     1/   31] train: loss: 0.1590076
[Epoch 90; Iter    31/   31] train: loss: 0.4636091
[Epoch 90] ogbg-molbace: 0.839821 val loss: 0.513119
[Epoch 90] ogbg-molbace: 0.786318 test loss: 0.842377
[Epoch 91; Iter    30/   31] train: loss: 0.2309329
[Epoch 91] ogbg-molbace: 0.824429 val loss: 0.551271
[Epoch 91] ogbg-molbace: 0.810014 test loss: 0.890723
[Epoch 92; Iter    29/   31] train: loss: 0.4859788
[Epoch 92] ogbg-molbace: 0.841907 val loss: 1.093333
[Epoch 92] ogbg-molbace: 0.713940 test loss: 1.077878
[Epoch 93; Iter    28/   31] train: loss: 0.2786781
[Epoch 93] ogbg-molbace: 0.795333 val loss: 0.971320
[Epoch 93] ogbg-molbace: 0.749742 test loss: 1.079608
[Epoch 94; Iter    27/   31] train: loss: 0.2361625
[Epoch 94] ogbg-molbace: 0.850645 val loss: 0.544924
[Epoch 94] ogbg-molbace: 0.796827 test loss: 0.920685
[Epoch 95; Iter    26/   31] train: loss: 0.1822555
[Epoch 95] ogbg-molbace: 0.841509 val loss: 0.476210
[Epoch 95] ogbg-molbace: 0.775345 test loss: 0.816131
[Epoch 96; Iter    25/   31] train: loss: 0.3741246
[Epoch 96] ogbg-molbace: 0.847269 val loss: 0.404099
[Epoch 96] ogbg-molbace: 0.772409 test loss: 0.937541
[Epoch 97; Iter    24/   31] train: loss: 0.0833718
[Epoch 97] ogbg-molbace: 0.823138 val loss: 0.529890
[Epoch 97] ogbg-molbace: 0.803008 test loss: 0.863027
[Epoch 98; Iter    23/   31] train: loss: 0.1452223
[Epoch 98] ogbg-molbace: 0.834757 val loss: 0.433946
[Epoch 98] ogbg-molbace: 0.799248 test loss: 0.996197
[Epoch 99; Iter    22/   31] train: loss: 0.1061654
[Epoch 99] ogbg-molbace: 0.854816 val loss: 0.683684
[Epoch 99] ogbg-molbace: 0.784309 test loss: 1.021578
[Epoch 100; Iter    21/   31] train: loss: 0.0815245
[Epoch 100] ogbg-molbace: 0.820159 val loss: 0.555562
[Epoch 100] ogbg-molbace: 0.804348 test loss: 0.881322
[Epoch 101; Iter    20/   31] train: loss: 0.0628198
[Epoch 101] ogbg-molbace: 0.799404 val loss: 0.475392
[Epoch 101] ogbg-molbace: 0.796981 test loss: 1.052891
[Epoch 102; Iter    19/   31] train: loss: 0.1227259
[Epoch 102] ogbg-molbace: 0.821648 val loss: 0.643225
[Epoch 102] ogbg-molbace: 0.800072 test loss: 1.004118
[Epoch 103; Iter    18/   31] train: loss: 0.0830393
[Epoch 103] ogbg-molbace: 0.811917 val loss: 0.497208
[Epoch 103] ogbg-molbace: 0.804554 test loss: 1.129424
[Epoch 104; Iter    17/   31] train: loss: 0.3642461
[Epoch 104] ogbg-molbace: 0.814201 val loss: 0.628175
[Epoch 104] ogbg-molbace: 0.810066 test loss: 1.051001
[Epoch 105; Iter    16/   31] train: loss: 0.1023610
[Epoch 105] ogbg-molbace: 0.794638 val loss: 0.540207
[Epoch 105] ogbg-molbace: 0.801205 test loss: 1.091806
[Epoch 106; Iter    15/   31] train: loss: 0.2145626
[Epoch 106] ogbg-molbace: 0.789275 val loss: 0.698167
[Epoch 106] ogbg-molbace: 0.790851 test loss: 1.027827
[Epoch 107; Iter    14/   31] train: loss: 0.1548034
[Epoch 107] ogbg-molbace: 0.826117 val loss: 0.564147
[Epoch 107] ogbg-molbace: 0.798836 test loss: 1.040070
[Epoch 108; Iter    13/   31] train: loss: 0.0738989
[Epoch 108] ogbg-molbace: 0.784707 val loss: 0.604819
[Epoch 108] ogbg-molbace: 0.781733 test loss: 1.209319
[Epoch 109; Iter    12/   31] train: loss: 0.0357201
[Epoch 109] ogbg-molbace: 0.793942 val loss: 0.630165
[Epoch 109] ogbg-molbace: 0.808005 test loss: 1.083154
[Epoch 110; Iter    11/   31] train: loss: 0.3114699
[Epoch 110] ogbg-molbace: 0.803972 val loss: 0.582494
[Epoch 110] ogbg-molbace: 0.791778 test loss: 1.253360
[Epoch 111; Iter    10/   31] train: loss: 0.0523202
[Epoch 111] ogbg-molbace: 0.830983 val loss: 0.621106
[Epoch 111] ogbg-molbace: 0.797393 test loss: 1.049827
[Epoch 112; Iter     9/   31] train: loss: 0.2009333
[Epoch 112] ogbg-molbace: 0.804171 val loss: 0.562616
[Epoch 112] ogbg-molbace: 0.801772 test loss: 1.202926
[Epoch 113; Iter     8/   31] train: loss: 0.0548807
[Epoch 113] ogbg-molbace: 0.822244 val loss: 0.635827
[Epoch 113] ogbg-molbace: 0.779054 test loss: 1.214668
[Epoch 114; Iter     7/   31] train: loss: 0.0483229
[Epoch 114] ogbg-molbace: 0.792155 val loss: 0.681700
[Epoch 114] ogbg-molbace: 0.798887 test loss: 1.127837
[Epoch 115; Iter     6/   31] train: loss: 0.0937958
[Epoch 115] ogbg-molbace: 0.810030 val loss: 0.629839
[Epoch 115] ogbg-molbace: 0.808727 test loss: 1.103977
[Epoch 116; Iter     5/   31] train: loss: 0.0102138
[Epoch 116] ogbg-molbace: 0.790467 val loss: 0.595029
[Epoch 116] ogbg-molbace: 0.801721 test loss: 1.141280
[Epoch 117; Iter     4/   31] train: loss: 0.0489999
[Epoch 117] ogbg-molbace: 0.830089 val loss: 0.523148
[Epoch 117] ogbg-molbace: 0.795024 test loss: 1.314277
[Epoch 118; Iter     3/   31] train: loss: 0.0696201
[Epoch 118] ogbg-molbace: 0.841708 val loss: 0.776241
[Epoch 118] ogbg-molbace: 0.815526 test loss: 1.054602
[Epoch 119; Iter     2/   31] train: loss: 0.0389591
[Epoch 119] ogbg-molbace: 0.794538 val loss: 0.627006
[Epoch 119] ogbg-molbace: 0.802442 test loss: 1.145565
[Epoch 120; Iter     1/   31] train: loss: 0.0147395
[Epoch 120; Iter    31/   31] train: loss: 0.0107580
[Epoch 120] ogbg-molbace: 0.814300 val loss: 0.716943
[Epoch 120] ogbg-molbace: 0.798269 test loss: 1.327164
[Epoch 121; Iter    30/   31] train: loss: 0.0765577
[Epoch 121] ogbg-molbace: 0.818769 val loss: 0.672983
[Epoch 121] ogbg-molbace: 0.802802 test loss: 1.125585
[Epoch 122; Iter    29/   31] train: loss: 0.0318704
[Epoch 122] ogbg-molbace: 0.821053 val loss: 0.615941
[Epoch 122] ogbg-molbace: 0.798372 test loss: 1.125368
[Epoch 123; Iter    28/   31] train: loss: 0.0460940
[Epoch 123] ogbg-molbace: 0.807547 val loss: 0.683261
[Epoch 123] ogbg-molbace: 0.803884 test loss: 1.105497
[Epoch 124; Iter    27/   31] train: loss: 0.1427195
[Epoch 124] ogbg-molbace: 0.816783 val loss: 0.594865
[Epoch 124] ogbg-molbace: 0.794972 test loss: 1.172392
[Epoch 125; Iter    26/   31] train: loss: 0.0395364
[Epoch 125] ogbg-molbace: 0.808937 val loss: 0.760481
[Epoch 125] ogbg-molbace: 0.802493 test loss: 1.185834
[Epoch 126; Iter    25/   31] train: loss: 0.0939374
[Epoch 126] ogbg-molbace: 0.813505 val loss: 0.614690
[Epoch 126] ogbg-molbace: 0.796672 test loss: 1.222562
[Epoch 127; Iter    24/   31] train: loss: 0.0564139
[Epoch 127] ogbg-molbace: 0.782522 val loss: 0.734407
[Epoch 127] ogbg-molbace: 0.805533 test loss: 1.104976
[Epoch 128; Iter    23/   31] train: loss: 0.0170154
[Epoch 128] ogbg-molbace: 0.794141 val loss: 0.750555
[Epoch 128] ogbg-molbace: 0.805739 test loss: 1.157104
[Epoch 129; Iter    22/   31] train: loss: 0.0314968
[Epoch 129] ogbg-molbace: 0.802681 val loss: 0.802984
[Epoch 129] ogbg-molbace: 0.808521 test loss: 1.148475
[Epoch 130; Iter    21/   31] train: loss: 0.1474765
[Epoch 130] ogbg-molbace: 0.799603 val loss: 0.871709
[Epoch 130] ogbg-molbace: 0.808984 test loss: 1.200569
[Epoch 131; Iter    20/   31] train: loss: 0.0338663
[Epoch 131] ogbg-molbace: 0.786197 val loss: 0.715667
[Epoch 131] ogbg-molbace: 0.802751 test loss: 1.233072
[Epoch 132; Iter    19/   31] train: loss: 0.0068063
[Epoch 132] ogbg-molbace: 0.772195 val loss: 0.916826
[Epoch 132] ogbg-molbace: 0.810014 test loss: 1.171964
[Epoch 133; Iter    18/   31] train: loss: 0.0318753
[Epoch 133] ogbg-molbace: 0.765641 val loss: 0.865192
[Epoch 133] ogbg-molbace: 0.807696 test loss: 1.271115
[Epoch 134; Iter    17/   31] train: loss: 0.0433388
[Epoch 134] ogbg-molbace: 0.786892 val loss: 0.763225
[Epoch 134] ogbg-molbace: 0.795127 test loss: 1.317430
[Epoch 135; Iter    16/   31] train: loss: 0.0158896
[Epoch 135] ogbg-molbace: 0.784012 val loss: 0.758143
[Epoch 135] ogbg-molbace: 0.787193 test loss: 1.345089
[Epoch 136; Iter    15/   31] train: loss: 0.0325030
[Epoch 136] ogbg-molbace: 0.781728 val loss: 1.002723
[Epoch 136] ogbg-molbace: 0.792036 test loss: 1.303573
[Epoch 82] ogbg-molbace: 0.813319 val loss: 0.621286
[Epoch 82] ogbg-molbace: 0.843626 test loss: 0.766177
[Epoch 83; Iter    18/   36] train: loss: 0.1348217
[Epoch 83] ogbg-molbace: 0.799366 val loss: 0.680917
[Epoch 83] ogbg-molbace: 0.839112 test loss: 0.858011
[Epoch 84; Iter    12/   36] train: loss: 0.0791908
[Epoch 84] ogbg-molbace: 0.794926 val loss: 0.709676
[Epoch 84] ogbg-molbace: 0.839653 test loss: 0.934863
[Epoch 85; Iter     6/   36] train: loss: 0.2879149
[Epoch 85; Iter    36/   36] train: loss: 0.1056531
[Epoch 85] ogbg-molbace: 0.801480 val loss: 0.753931
[Epoch 85] ogbg-molbace: 0.836403 test loss: 0.771653
[Epoch 86; Iter    30/   36] train: loss: 0.1256697
[Epoch 86] ogbg-molbace: 0.798732 val loss: 0.677921
[Epoch 86] ogbg-molbace: 0.832069 test loss: 1.005233
[Epoch 87; Iter    24/   36] train: loss: 0.0499219
[Epoch 87] ogbg-molbace: 0.787421 val loss: 0.792847
[Epoch 87] ogbg-molbace: 0.850758 test loss: 0.855499
[Epoch 88; Iter    18/   36] train: loss: 0.0800939
[Epoch 88] ogbg-molbace: 0.803488 val loss: 0.795816
[Epoch 88] ogbg-molbace: 0.839834 test loss: 1.063216
[Epoch 89; Iter    12/   36] train: loss: 0.0842323
[Epoch 89] ogbg-molbace: 0.802220 val loss: 0.853240
[Epoch 89] ogbg-molbace: 0.837216 test loss: 0.834144
[Epoch 90; Iter     6/   36] train: loss: 0.0926729
[Epoch 90; Iter    36/   36] train: loss: 0.0500503
[Epoch 90] ogbg-molbace: 0.792389 val loss: 0.841779
[Epoch 90] ogbg-molbace: 0.844258 test loss: 0.842571
[Epoch 91; Iter    30/   36] train: loss: 0.1511363
[Epoch 91] ogbg-molbace: 0.791649 val loss: 0.720406
[Epoch 91] ogbg-molbace: 0.842362 test loss: 0.994040
[Epoch 92; Iter    24/   36] train: loss: 0.1622287
[Epoch 92] ogbg-molbace: 0.775793 val loss: 1.007926
[Epoch 92] ogbg-molbace: 0.844619 test loss: 0.974276
[Epoch 93; Iter    18/   36] train: loss: 0.0963841
[Epoch 93] ogbg-molbace: 0.813531 val loss: 0.945970
[Epoch 93] ogbg-molbace: 0.836222 test loss: 0.801886
[Epoch 94; Iter    12/   36] train: loss: 0.0573789
[Epoch 94] ogbg-molbace: 0.807294 val loss: 0.938750
[Epoch 94] ogbg-molbace: 0.835681 test loss: 0.932882
[Epoch 95; Iter     6/   36] train: loss: 0.1433908
[Epoch 95; Iter    36/   36] train: loss: 0.0619716
[Epoch 95] ogbg-molbace: 0.793763 val loss: 0.855595
[Epoch 95] ogbg-molbace: 0.850849 test loss: 1.024260
[Epoch 96; Iter    30/   36] train: loss: 0.0399237
[Epoch 96] ogbg-molbace: 0.786258 val loss: 0.725553
[Epoch 96] ogbg-molbace: 0.826562 test loss: 1.047343
[Epoch 97; Iter    24/   36] train: loss: 0.0412543
[Epoch 97] ogbg-molbace: 0.799260 val loss: 0.862455
[Epoch 97] ogbg-molbace: 0.841640 test loss: 0.992514
[Epoch 98; Iter    18/   36] train: loss: 0.1198490
[Epoch 98] ogbg-molbace: 0.784461 val loss: 1.064114
[Epoch 98] ogbg-molbace: 0.837938 test loss: 0.889865
[Epoch 99; Iter    12/   36] train: loss: 0.0202378
[Epoch 99] ogbg-molbace: 0.799894 val loss: 1.023952
[Epoch 99] ogbg-molbace: 0.839924 test loss: 0.997847
[Epoch 100; Iter     6/   36] train: loss: 0.0973035
[Epoch 100; Iter    36/   36] train: loss: 0.0606443
[Epoch 100] ogbg-molbace: 0.800634 val loss: 0.819643
[Epoch 100] ogbg-molbace: 0.831889 test loss: 1.007975
[Epoch 101; Iter    30/   36] train: loss: 0.1595736
[Epoch 101] ogbg-molbace: 0.790486 val loss: 1.037432
[Epoch 101] ogbg-molbace: 0.844709 test loss: 0.846393
[Epoch 102; Iter    24/   36] train: loss: 0.0774657
[Epoch 102] ogbg-molbace: 0.793235 val loss: 0.952129
[Epoch 102] ogbg-molbace: 0.833062 test loss: 1.142218
[Epoch 103; Iter    18/   36] train: loss: 0.1234852
[Epoch 103] ogbg-molbace: 0.790486 val loss: 1.103030
[Epoch 103] ogbg-molbace: 0.810220 test loss: 0.960324
[Epoch 104; Iter    12/   36] train: loss: 0.1235675
[Epoch 104] ogbg-molbace: 0.810888 val loss: 0.829111
[Epoch 104] ogbg-molbace: 0.840917 test loss: 1.158930
[Epoch 105; Iter     6/   36] train: loss: 0.0154992
[Epoch 105; Iter    36/   36] train: loss: 0.0149963
[Epoch 105] ogbg-molbace: 0.800317 val loss: 0.975816
[Epoch 105] ogbg-molbace: 0.826833 test loss: 1.294126
[Epoch 106; Iter    30/   36] train: loss: 0.0354008
[Epoch 106] ogbg-molbace: 0.819345 val loss: 0.948735
[Epoch 106] ogbg-molbace: 0.843716 test loss: 1.017532
[Epoch 107; Iter    24/   36] train: loss: 0.0191621
[Epoch 107] ogbg-molbace: 0.812156 val loss: 0.955130
[Epoch 107] ogbg-molbace: 0.836132 test loss: 1.253182
[Epoch 108; Iter    18/   36] train: loss: 0.0250461
[Epoch 108] ogbg-molbace: 0.806660 val loss: 0.959428
[Epoch 108] ogbg-molbace: 0.836313 test loss: 1.214613
[Epoch 109; Iter    12/   36] train: loss: 0.0337088
[Epoch 109] ogbg-molbace: 0.808245 val loss: 1.061500
[Epoch 109] ogbg-molbace: 0.846515 test loss: 0.991305
[Epoch 110; Iter     6/   36] train: loss: 0.0046761
[Epoch 110; Iter    36/   36] train: loss: 0.0095669
[Epoch 110] ogbg-molbace: 0.809514 val loss: 0.974243
[Epoch 110] ogbg-molbace: 0.840466 test loss: 1.107528
[Epoch 111; Iter    30/   36] train: loss: 0.0320205
[Epoch 111] ogbg-molbace: 0.812685 val loss: 1.012698
[Epoch 111] ogbg-molbace: 0.842633 test loss: 1.144885
[Epoch 112; Iter    24/   36] train: loss: 0.0118308
[Epoch 112] ogbg-molbace: 0.810677 val loss: 1.031072
[Epoch 112] ogbg-molbace: 0.841008 test loss: 1.127411
[Epoch 113; Iter    18/   36] train: loss: 0.0247527
[Epoch 113] ogbg-molbace: 0.804017 val loss: 1.011124
[Epoch 113] ogbg-molbace: 0.840737 test loss: 1.236999
[Epoch 114; Iter    12/   36] train: loss: 0.1125620
[Epoch 114] ogbg-molbace: 0.814588 val loss: 0.979191
[Epoch 114] ogbg-molbace: 0.847328 test loss: 1.177546
[Epoch 115; Iter     6/   36] train: loss: 0.0303358
[Epoch 115; Iter    36/   36] train: loss: 0.2974104
[Epoch 115] ogbg-molbace: 0.812685 val loss: 1.075824
[Epoch 115] ogbg-molbace: 0.842904 test loss: 1.280821
[Epoch 116; Iter    30/   36] train: loss: 0.0292897
[Epoch 116] ogbg-molbace: 0.800423 val loss: 1.054190
[Epoch 116] ogbg-molbace: 0.827555 test loss: 1.236445
[Epoch 117; Iter    24/   36] train: loss: 0.0378404
[Epoch 117] ogbg-molbace: 0.810994 val loss: 1.230418
[Epoch 117] ogbg-molbace: 0.826923 test loss: 1.233131
[Epoch 118; Iter    18/   36] train: loss: 0.0109335
[Epoch 118] ogbg-molbace: 0.795983 val loss: 1.049000
[Epoch 118] ogbg-molbace: 0.835139 test loss: 1.321273
[Epoch 119; Iter    12/   36] train: loss: 0.0745286
[Epoch 119] ogbg-molbace: 0.770190 val loss: 1.158618
[Epoch 119] ogbg-molbace: 0.823221 test loss: 1.210097
[Epoch 120; Iter     6/   36] train: loss: 0.0138281
[Epoch 120; Iter    36/   36] train: loss: 0.0501544
[Epoch 120] ogbg-molbace: 0.778858 val loss: 1.273526
[Epoch 120] ogbg-molbace: 0.825930 test loss: 1.321725
[Epoch 121; Iter    30/   36] train: loss: 0.0067561
[Epoch 121] ogbg-molbace: 0.777696 val loss: 1.260001
[Epoch 121] ogbg-molbace: 0.824576 test loss: 1.368494
[Epoch 122; Iter    24/   36] train: loss: 0.0050906
[Epoch 122] ogbg-molbace: 0.785095 val loss: 1.289402
[Epoch 122] ogbg-molbace: 0.818346 test loss: 1.455500
[Epoch 123; Iter    18/   36] train: loss: 0.0023424
[Epoch 123] ogbg-molbace: 0.788161 val loss: 1.265787
[Epoch 123] ogbg-molbace: 0.824847 test loss: 1.315231
[Epoch 124; Iter    12/   36] train: loss: 0.0395550
[Epoch 124] ogbg-molbace: 0.786364 val loss: 1.284341
[Epoch 124] ogbg-molbace: 0.826472 test loss: 1.322845
[Epoch 125; Iter     6/   36] train: loss: 0.0116366
[Epoch 125; Iter    36/   36] train: loss: 0.2035895
[Epoch 125] ogbg-molbace: 0.795560 val loss: 1.415544
[Epoch 125] ogbg-molbace: 0.834326 test loss: 1.328444
[Epoch 126; Iter    30/   36] train: loss: 0.0069370
[Epoch 126] ogbg-molbace: 0.787844 val loss: 1.709674
[Epoch 126] ogbg-molbace: 0.831889 test loss: 1.105201
[Epoch 127; Iter    24/   36] train: loss: 0.0248383
[Epoch 127] ogbg-molbace: 0.783721 val loss: 1.237396
[Epoch 127] ogbg-molbace: 0.828458 test loss: 1.286265
[Epoch 128; Iter    18/   36] train: loss: 0.1006612
[Epoch 128] ogbg-molbace: 0.791755 val loss: 1.220758
[Epoch 128] ogbg-molbace: 0.839021 test loss: 1.231460
[Epoch 129; Iter    12/   36] train: loss: 0.0079538
[Epoch 129] ogbg-molbace: 0.810994 val loss: 1.220031
[Epoch 129] ogbg-molbace: 0.832792 test loss: 1.135414
[Epoch 78] ogbg-molbace: 0.731502 val loss: 1.429552
[Epoch 78] ogbg-molbace: 0.786472 test loss: 1.658244
[Epoch 79; Iter    12/   41] train: loss: 0.1830130
[Epoch 79] ogbg-molbace: 0.694139 val loss: 1.475532
[Epoch 79] ogbg-molbace: 0.774648 test loss: 1.769901
[Epoch 80; Iter     1/   41] train: loss: 0.0389852
[Epoch 80; Iter    31/   41] train: loss: 0.2244993
[Epoch 80] ogbg-molbace: 0.721612 val loss: 1.398065
[Epoch 80] ogbg-molbace: 0.782125 test loss: 1.898337
[Epoch 81; Iter    20/   41] train: loss: 0.0971431
[Epoch 81] ogbg-molbace: 0.742857 val loss: 1.167267
[Epoch 81] ogbg-molbace: 0.772040 test loss: 1.935171
[Epoch 82; Iter     9/   41] train: loss: 0.0671507
[Epoch 82; Iter    39/   41] train: loss: 0.2535642
[Epoch 82] ogbg-molbace: 0.689377 val loss: 1.524123
[Epoch 82] ogbg-molbace: 0.757955 test loss: 2.054006
[Epoch 83; Iter    28/   41] train: loss: 0.0585011
[Epoch 83] ogbg-molbace: 0.748352 val loss: 1.241461
[Epoch 83] ogbg-molbace: 0.776213 test loss: 1.739823
[Epoch 84; Iter    17/   41] train: loss: 0.1643087
[Epoch 84] ogbg-molbace: 0.739194 val loss: 1.000220
[Epoch 84] ogbg-molbace: 0.754651 test loss: 1.937763
[Epoch 85; Iter     6/   41] train: loss: 0.0698991
[Epoch 85; Iter    36/   41] train: loss: 0.0391953
[Epoch 85] ogbg-molbace: 0.746154 val loss: 1.096128
[Epoch 85] ogbg-molbace: 0.788732 test loss: 1.826246
[Epoch 86; Iter    25/   41] train: loss: 0.2677195
[Epoch 86] ogbg-molbace: 0.770696 val loss: 1.672435
[Epoch 86] ogbg-molbace: 0.783516 test loss: 1.826570
[Epoch 87; Iter    14/   41] train: loss: 0.2195479
[Epoch 87] ogbg-molbace: 0.742857 val loss: 1.582279
[Epoch 87] ogbg-molbace: 0.785603 test loss: 1.711861
[Epoch 88; Iter     3/   41] train: loss: 0.1154391
[Epoch 88; Iter    33/   41] train: loss: 0.1106900
[Epoch 88] ogbg-molbace: 0.753480 val loss: 1.328844
[Epoch 88] ogbg-molbace: 0.749609 test loss: 2.178531
[Epoch 89; Iter    22/   41] train: loss: 0.2095989
[Epoch 89] ogbg-molbace: 0.734799 val loss: 1.658977
[Epoch 89] ogbg-molbace: 0.737959 test loss: 2.226197
[Epoch 90; Iter    11/   41] train: loss: 0.1494424
[Epoch 90; Iter    41/   41] train: loss: 0.4128526
[Epoch 90] ogbg-molbace: 0.682418 val loss: 1.907395
[Epoch 90] ogbg-molbace: 0.734655 test loss: 1.527421
[Epoch 91; Iter    30/   41] train: loss: 0.0544381
[Epoch 91] ogbg-molbace: 0.719780 val loss: 1.767383
[Epoch 91] ogbg-molbace: 0.740393 test loss: 1.952071
[Epoch 92; Iter    19/   41] train: loss: 0.0300667
[Epoch 92] ogbg-molbace: 0.732967 val loss: 1.979286
[Epoch 92] ogbg-molbace: 0.758998 test loss: 2.232539
[Epoch 93; Iter     8/   41] train: loss: 0.0898798
[Epoch 93; Iter    38/   41] train: loss: 0.1149811
[Epoch 93] ogbg-molbace: 0.747619 val loss: 1.704569
[Epoch 93] ogbg-molbace: 0.772735 test loss: 2.475060
[Epoch 94; Iter    27/   41] train: loss: 0.0312988
[Epoch 94] ogbg-molbace: 0.719780 val loss: 1.149458
[Epoch 94] ogbg-molbace: 0.763346 test loss: 2.168551
[Epoch 95; Iter    16/   41] train: loss: 0.1934593
[Epoch 95] ogbg-molbace: 0.746520 val loss: 1.630281
[Epoch 95] ogbg-molbace: 0.772214 test loss: 2.487044
[Epoch 96; Iter     5/   41] train: loss: 0.0737395
[Epoch 96; Iter    35/   41] train: loss: 0.0505819
[Epoch 96] ogbg-molbace: 0.736264 val loss: 1.811260
[Epoch 96] ogbg-molbace: 0.765084 test loss: 2.096929
[Epoch 97; Iter    24/   41] train: loss: 0.1396489
[Epoch 97] ogbg-molbace: 0.746154 val loss: 1.842717
[Epoch 97] ogbg-molbace: 0.750304 test loss: 2.176351
[Epoch 98; Iter    13/   41] train: loss: 0.0268919
[Epoch 98] ogbg-molbace: 0.756410 val loss: 1.588356
[Epoch 98] ogbg-molbace: 0.773431 test loss: 2.243954
[Epoch 99; Iter     2/   41] train: loss: 0.0068413
[Epoch 99; Iter    32/   41] train: loss: 0.0649880
[Epoch 99] ogbg-molbace: 0.754579 val loss: 1.645121
[Epoch 99] ogbg-molbace: 0.775170 test loss: 2.094282
[Epoch 100; Iter    21/   41] train: loss: 0.0462148
[Epoch 100] ogbg-molbace: 0.714286 val loss: 1.905123
[Epoch 100] ogbg-molbace: 0.749609 test loss: 2.168293
[Epoch 101; Iter    10/   41] train: loss: 0.0412367
[Epoch 101; Iter    40/   41] train: loss: 0.0394901
[Epoch 101] ogbg-molbace: 0.718681 val loss: 1.921205
[Epoch 101] ogbg-molbace: 0.744914 test loss: 2.171257
[Epoch 102; Iter    29/   41] train: loss: 0.0513636
[Epoch 102] ogbg-molbace: 0.727106 val loss: 2.005685
[Epoch 102] ogbg-molbace: 0.782473 test loss: 2.657849
[Epoch 103; Iter    18/   41] train: loss: 0.0487760
[Epoch 103] ogbg-molbace: 0.722344 val loss: 1.777058
[Epoch 103] ogbg-molbace: 0.765954 test loss: 2.542787
[Epoch 104; Iter     7/   41] train: loss: 0.0491805
[Epoch 104; Iter    37/   41] train: loss: 0.0209190
[Epoch 104] ogbg-molbace: 0.729304 val loss: 1.916930
[Epoch 104] ogbg-molbace: 0.757433 test loss: 2.567298
[Epoch 105; Iter    26/   41] train: loss: 0.0722149
[Epoch 105] ogbg-molbace: 0.717582 val loss: 2.166067
[Epoch 105] ogbg-molbace: 0.758477 test loss: 2.767624
[Epoch 106; Iter    15/   41] train: loss: 0.0059694
[Epoch 106] ogbg-molbace: 0.716484 val loss: 2.238788
[Epoch 106] ogbg-molbace: 0.748565 test loss: 2.929849
[Epoch 107; Iter     4/   41] train: loss: 0.0264460
[Epoch 107; Iter    34/   41] train: loss: 0.1374699
[Epoch 107] ogbg-molbace: 0.713187 val loss: 2.126680
[Epoch 107] ogbg-molbace: 0.767519 test loss: 2.152300
[Epoch 108; Iter    23/   41] train: loss: 0.1777692
[Epoch 108] ogbg-molbace: 0.751282 val loss: 1.891908
[Epoch 108] ogbg-molbace: 0.772735 test loss: 2.694352
[Epoch 109; Iter    12/   41] train: loss: 0.0194430
[Epoch 109] ogbg-molbace: 0.713187 val loss: 1.731290
[Epoch 109] ogbg-molbace: 0.754130 test loss: 2.520911
[Epoch 110; Iter     1/   41] train: loss: 0.0336000
[Epoch 110; Iter    31/   41] train: loss: 0.0169530
[Epoch 110] ogbg-molbace: 0.742857 val loss: 1.618627
[Epoch 110] ogbg-molbace: 0.762302 test loss: 2.474825
[Epoch 111; Iter    20/   41] train: loss: 0.0252511
[Epoch 111] ogbg-molbace: 0.732601 val loss: 1.530716
[Epoch 111] ogbg-molbace: 0.763693 test loss: 2.355056
[Epoch 112; Iter     9/   41] train: loss: 0.0775013
[Epoch 112; Iter    39/   41] train: loss: 0.0228487
[Epoch 112] ogbg-molbace: 0.749817 val loss: 1.625902
[Epoch 112] ogbg-molbace: 0.738132 test loss: 1.932675
[Epoch 113; Iter    28/   41] train: loss: 0.0191566
[Epoch 113] ogbg-molbace: 0.751648 val loss: 1.581692
[Epoch 113] ogbg-molbace: 0.762476 test loss: 2.514799
[Epoch 114; Iter    17/   41] train: loss: 0.0617027
[Epoch 114] ogbg-molbace: 0.772894 val loss: 1.648990
[Epoch 114] ogbg-molbace: 0.758303 test loss: 2.443950
[Epoch 115; Iter     6/   41] train: loss: 0.0126434
[Epoch 115; Iter    36/   41] train: loss: 0.0375049
[Epoch 115] ogbg-molbace: 0.761905 val loss: 1.340664
[Epoch 115] ogbg-molbace: 0.766302 test loss: 2.371385
[Epoch 116; Iter    25/   41] train: loss: 0.0287468
[Epoch 116] ogbg-molbace: 0.768132 val loss: 1.704495
[Epoch 116] ogbg-molbace: 0.756216 test loss: 2.707655
[Epoch 117; Iter    14/   41] train: loss: 0.0060621
[Epoch 117] ogbg-molbace: 0.765201 val loss: 1.520038
[Epoch 117] ogbg-molbace: 0.748913 test loss: 2.504149
[Epoch 118; Iter     3/   41] train: loss: 0.0030800
[Epoch 118; Iter    33/   41] train: loss: 0.0111946
[Epoch 118] ogbg-molbace: 0.771795 val loss: 1.474600
[Epoch 118] ogbg-molbace: 0.740741 test loss: 2.387336
[Epoch 119; Iter    22/   41] train: loss: 0.0174592
[Epoch 119] ogbg-molbace: 0.745055 val loss: 1.606840
[Epoch 119] ogbg-molbace: 0.754651 test loss: 2.705658
[Epoch 120; Iter    11/   41] train: loss: 0.0040428
[Epoch 120; Iter    41/   41] train: loss: 0.0053590
[Epoch 120] ogbg-molbace: 0.754945 val loss: 1.504728
[Epoch 120] ogbg-molbace: 0.758651 test loss: 2.468145
[Epoch 121; Iter    30/   41] train: loss: 0.0053734
[Epoch 121] ogbg-molbace: 0.753114 val loss: 1.720763
[Epoch 121] ogbg-molbace: 0.749957 test loss: 2.421961
[Epoch 122; Iter    19/   41] train: loss: 0.0045560
[Epoch 122] ogbg-molbace: 0.742857 val loss: 1.691254
[Epoch 122] ogbg-molbace: 0.747348 test loss: 2.835908
[Epoch 123; Iter     8/   41] train: loss: 0.0025031
[Epoch 123; Iter    38/   41] train: loss: 0.0431164
[Epoch 123] ogbg-molbace: 0.735531 val loss: 1.788411
[Epoch 82] ogbg-molbace: 0.771776 val loss: 0.855904
[Epoch 82] ogbg-molbace: 0.811033 test loss: 0.953358
[Epoch 83; Iter    18/   36] train: loss: 0.1404781
[Epoch 83] ogbg-molbace: 0.797357 val loss: 0.746324
[Epoch 83] ogbg-molbace: 0.849314 test loss: 0.777161
[Epoch 84; Iter    12/   36] train: loss: 0.0817541
[Epoch 84] ogbg-molbace: 0.778436 val loss: 0.726404
[Epoch 84] ogbg-molbace: 0.826562 test loss: 1.039247
[Epoch 85; Iter     6/   36] train: loss: 0.0504259
[Epoch 85; Iter    36/   36] train: loss: 0.1589112
[Epoch 85] ogbg-molbace: 0.771987 val loss: 0.738749
[Epoch 85] ogbg-molbace: 0.827104 test loss: 0.945339
[Epoch 86; Iter    30/   36] train: loss: 0.2748731
[Epoch 86] ogbg-molbace: 0.780867 val loss: 0.903307
[Epoch 86] ogbg-molbace: 0.834688 test loss: 0.859238
[Epoch 87; Iter    24/   36] train: loss: 0.0885566
[Epoch 87] ogbg-molbace: 0.770613 val loss: 0.845461
[Epoch 87] ogbg-molbace: 0.822589 test loss: 1.224646
[Epoch 88; Iter    18/   36] train: loss: 0.1247156
[Epoch 88] ogbg-molbace: 0.758034 val loss: 0.953230
[Epoch 88] ogbg-molbace: 0.851932 test loss: 0.955223
[Epoch 89; Iter    12/   36] train: loss: 0.1565899
[Epoch 89] ogbg-molbace: 0.771353 val loss: 0.780831
[Epoch 89] ogbg-molbace: 0.840376 test loss: 0.830753
[Epoch 90; Iter     6/   36] train: loss: 0.0358020
[Epoch 90; Iter    36/   36] train: loss: 0.0616312
[Epoch 90] ogbg-molbace: 0.762896 val loss: 0.848566
[Epoch 90] ogbg-molbace: 0.829632 test loss: 1.135873
[Epoch 91; Iter    30/   36] train: loss: 0.2752975
[Epoch 91] ogbg-molbace: 0.772939 val loss: 0.871359
[Epoch 91] ogbg-molbace: 0.818436 test loss: 0.898539
[Epoch 92; Iter    24/   36] train: loss: 0.0522949
[Epoch 92] ogbg-molbace: 0.776956 val loss: 0.751447
[Epoch 92] ogbg-molbace: 0.850758 test loss: 1.004860
[Epoch 93; Iter    18/   36] train: loss: 0.1810788
[Epoch 93] ogbg-molbace: 0.761945 val loss: 0.901027
[Epoch 93] ogbg-molbace: 0.824485 test loss: 0.942742
[Epoch 94; Iter    12/   36] train: loss: 0.0597290
[Epoch 94] ogbg-molbace: 0.766385 val loss: 0.706120
[Epoch 94] ogbg-molbace: 0.815728 test loss: 0.994796
[Epoch 95; Iter     6/   36] train: loss: 0.0564942
[Epoch 95; Iter    36/   36] train: loss: 0.0368776
[Epoch 95] ogbg-molbace: 0.738795 val loss: 0.983999
[Epoch 95] ogbg-molbace: 0.841188 test loss: 0.930700
[Epoch 96; Iter    30/   36] train: loss: 0.2709436
[Epoch 96] ogbg-molbace: 0.744292 val loss: 1.065261
[Epoch 96] ogbg-molbace: 0.846244 test loss: 0.828697
[Epoch 97; Iter    24/   36] train: loss: 0.1812809
[Epoch 97] ogbg-molbace: 0.750423 val loss: 0.962773
[Epoch 97] ogbg-molbace: 0.829632 test loss: 1.110297
[Epoch 98; Iter    18/   36] train: loss: 0.0973299
[Epoch 98] ogbg-molbace: 0.782135 val loss: 0.798696
[Epoch 98] ogbg-molbace: 0.833153 test loss: 1.040316
[Epoch 99; Iter    12/   36] train: loss: 0.0563119
[Epoch 99] ogbg-molbace: 0.735835 val loss: 0.976853
[Epoch 99] ogbg-molbace: 0.819700 test loss: 0.967854
[Epoch 100; Iter     6/   36] train: loss: 0.0578620
[Epoch 100; Iter    36/   36] train: loss: 0.0931081
[Epoch 100] ogbg-molbace: 0.804123 val loss: 0.756439
[Epoch 100] ogbg-molbace: 0.821325 test loss: 0.980887
[Epoch 101; Iter    30/   36] train: loss: 0.0314254
[Epoch 101] ogbg-molbace: 0.772833 val loss: 1.035902
[Epoch 101] ogbg-molbace: 0.823763 test loss: 1.019805
[Epoch 102; Iter    24/   36] train: loss: 0.1560641
[Epoch 102] ogbg-molbace: 0.784778 val loss: 0.756235
[Epoch 102] ogbg-molbace: 0.828548 test loss: 1.073922
[Epoch 103; Iter    18/   36] train: loss: 0.1056239
[Epoch 103] ogbg-molbace: 0.803277 val loss: 0.806353
[Epoch 103] ogbg-molbace: 0.828548 test loss: 1.115185
[Epoch 104; Iter    12/   36] train: loss: 0.0215724
[Epoch 104] ogbg-molbace: 0.774630 val loss: 1.127901
[Epoch 104] ogbg-molbace: 0.835771 test loss: 0.945274
[Epoch 105; Iter     6/   36] train: loss: 0.0207088
[Epoch 105; Iter    36/   36] train: loss: 0.0573445
[Epoch 105] ogbg-molbace: 0.777907 val loss: 0.896028
[Epoch 105] ogbg-molbace: 0.835049 test loss: 1.112117
[Epoch 106; Iter    30/   36] train: loss: 0.0347527
[Epoch 106] ogbg-molbace: 0.773362 val loss: 0.989080
[Epoch 106] ogbg-molbace: 0.831347 test loss: 1.065740
[Epoch 107; Iter    24/   36] train: loss: 0.0354171
[Epoch 107] ogbg-molbace: 0.767548 val loss: 1.081521
[Epoch 107] ogbg-molbace: 0.835952 test loss: 1.133301
[Epoch 108; Iter    18/   36] train: loss: 0.0303816
[Epoch 108] ogbg-molbace: 0.779281 val loss: 0.948919
[Epoch 108] ogbg-molbace: 0.836403 test loss: 1.164786
[Epoch 109; Iter    12/   36] train: loss: 0.0248952
[Epoch 109] ogbg-molbace: 0.765645 val loss: 1.122029
[Epoch 109] ogbg-molbace: 0.835861 test loss: 1.052566
[Epoch 110; Iter     6/   36] train: loss: 0.0077884
[Epoch 110; Iter    36/   36] train: loss: 0.0667448
[Epoch 110] ogbg-molbace: 0.775370 val loss: 1.115021
[Epoch 110] ogbg-molbace: 0.831257 test loss: 1.132027
[Epoch 111; Iter    30/   36] train: loss: 0.0352012
[Epoch 111] ogbg-molbace: 0.769662 val loss: 1.227709
[Epoch 111] ogbg-molbace: 0.827916 test loss: 1.081811
[Epoch 112; Iter    24/   36] train: loss: 0.0122897
[Epoch 112] ogbg-molbace: 0.756977 val loss: 1.090666
[Epoch 112] ogbg-molbace: 0.819700 test loss: 1.215417
[Epoch 113; Iter    18/   36] train: loss: 0.0108690
[Epoch 113] ogbg-molbace: 0.779915 val loss: 1.059799
[Epoch 113] ogbg-molbace: 0.831437 test loss: 1.226281
[Epoch 114; Iter    12/   36] train: loss: 0.0133194
[Epoch 114] ogbg-molbace: 0.768816 val loss: 1.154768
[Epoch 114] ogbg-molbace: 0.817804 test loss: 1.214192
[Epoch 115; Iter     6/   36] train: loss: 0.0336788
[Epoch 115; Iter    36/   36] train: loss: 0.0177336
[Epoch 115] ogbg-molbace: 0.789746 val loss: 1.043859
[Epoch 115] ogbg-molbace: 0.832972 test loss: 1.199993
[Epoch 116; Iter    30/   36] train: loss: 0.0026197
[Epoch 116] ogbg-molbace: 0.783932 val loss: 1.107096
[Epoch 116] ogbg-molbace: 0.826201 test loss: 1.187401
[Epoch 117; Iter    24/   36] train: loss: 0.0347235
[Epoch 117] ogbg-molbace: 0.769133 val loss: 1.151783
[Epoch 117] ogbg-molbace: 0.833604 test loss: 1.271670
[Epoch 118; Iter    18/   36] train: loss: 0.0461844
[Epoch 118] ogbg-molbace: 0.775476 val loss: 1.263184
[Epoch 118] ogbg-molbace: 0.829090 test loss: 1.245586
[Epoch 119; Iter    12/   36] train: loss: 0.1185677
[Epoch 119] ogbg-molbace: 0.769979 val loss: 1.214510
[Epoch 119] ogbg-molbace: 0.825479 test loss: 1.252999
[Epoch 120; Iter     6/   36] train: loss: 0.0101605
[Epoch 120; Iter    36/   36] train: loss: 0.0414841
[Epoch 120] ogbg-molbace: 0.757717 val loss: 1.120028
[Epoch 120] ogbg-molbace: 0.845161 test loss: 1.297953
[Epoch 121; Iter    30/   36] train: loss: 0.0105109
[Epoch 121] ogbg-molbace: 0.787526 val loss: 1.046562
[Epoch 121] ogbg-molbace: 0.831257 test loss: 1.244025
[Epoch 122; Iter    24/   36] train: loss: 0.2008607
[Epoch 122] ogbg-molbace: 0.773784 val loss: 1.289126
[Epoch 122] ogbg-molbace: 0.828548 test loss: 1.132308
[Epoch 123; Iter    18/   36] train: loss: 0.0709544
[Epoch 123] ogbg-molbace: 0.773996 val loss: 1.083457
[Epoch 123] ogbg-molbace: 0.826291 test loss: 1.319005
[Epoch 124; Iter    12/   36] train: loss: 0.0307549
[Epoch 124] ogbg-molbace: 0.777484 val loss: 1.170800
[Epoch 124] ogbg-molbace: 0.816811 test loss: 1.207788
[Epoch 125; Iter     6/   36] train: loss: 0.0119501
[Epoch 125; Iter    36/   36] train: loss: 0.0011219
[Epoch 125] ogbg-molbace: 0.757082 val loss: 1.224681
[Epoch 125] ogbg-molbace: 0.814825 test loss: 1.260736
[Epoch 126; Iter    30/   36] train: loss: 0.0220248
[Epoch 126] ogbg-molbace: 0.763531 val loss: 1.265172
[Epoch 126] ogbg-molbace: 0.819249 test loss: 1.199422
[Epoch 127; Iter    24/   36] train: loss: 0.0084053
[Epoch 127] ogbg-molbace: 0.783615 val loss: 1.280507
[Epoch 127] ogbg-molbace: 0.819429 test loss: 1.279248
[Epoch 128; Iter    18/   36] train: loss: 0.0026639
[Epoch 128] ogbg-molbace: 0.770402 val loss: 1.216912
[Epoch 128] ogbg-molbace: 0.823041 test loss: 1.333011
[Epoch 129; Iter    12/   36] train: loss: 0.0352988
[Epoch 129] ogbg-molbace: 0.773679 val loss: 1.345137
[Epoch 129] ogbg-molbace: 0.824666 test loss: 1.255526
[Epoch 78] ogbg-molbace: 0.750549 val loss: 1.116670
[Epoch 78] ogbg-molbace: 0.828030 test loss: 1.500146
[Epoch 79; Iter    12/   41] train: loss: 0.0697508
[Epoch 79] ogbg-molbace: 0.761172 val loss: 1.314155
[Epoch 79] ogbg-molbace: 0.793775 test loss: 1.598021
[Epoch 80; Iter     1/   41] train: loss: 0.1540671
[Epoch 80; Iter    31/   41] train: loss: 0.2811192
[Epoch 80] ogbg-molbace: 0.759341 val loss: 1.280759
[Epoch 80] ogbg-molbace: 0.807338 test loss: 1.644545
[Epoch 81; Iter    20/   41] train: loss: 0.0652270
[Epoch 81] ogbg-molbace: 0.734799 val loss: 1.561049
[Epoch 81] ogbg-molbace: 0.820553 test loss: 1.632231
[Epoch 82; Iter     9/   41] train: loss: 0.0588484
[Epoch 82; Iter    39/   41] train: loss: 0.1103839
[Epoch 82] ogbg-molbace: 0.727106 val loss: 1.581286
[Epoch 82] ogbg-molbace: 0.820553 test loss: 1.680199
[Epoch 83; Iter    28/   41] train: loss: 0.0770795
[Epoch 83] ogbg-molbace: 0.697070 val loss: 1.490943
[Epoch 83] ogbg-molbace: 0.814989 test loss: 1.531446
[Epoch 84; Iter    17/   41] train: loss: 0.0745055
[Epoch 84] ogbg-molbace: 0.721245 val loss: 1.655937
[Epoch 84] ogbg-molbace: 0.811859 test loss: 1.846313
[Epoch 85; Iter     6/   41] train: loss: 0.1161236
[Epoch 85; Iter    36/   41] train: loss: 0.2743900
[Epoch 85] ogbg-molbace: 0.680952 val loss: 1.683662
[Epoch 85] ogbg-molbace: 0.796383 test loss: 1.575288
[Epoch 86; Iter    25/   41] train: loss: 0.2174560
[Epoch 86] ogbg-molbace: 0.695971 val loss: 1.807097
[Epoch 86] ogbg-molbace: 0.818466 test loss: 2.052148
[Epoch 87; Iter    14/   41] train: loss: 0.0844972
[Epoch 87] ogbg-molbace: 0.731136 val loss: 1.709370
[Epoch 87] ogbg-molbace: 0.820727 test loss: 1.814471
[Epoch 88; Iter     3/   41] train: loss: 0.2339833
[Epoch 88; Iter    33/   41] train: loss: 0.2181617
[Epoch 88] ogbg-molbace: 0.738828 val loss: 1.408837
[Epoch 88] ogbg-molbace: 0.816901 test loss: 1.405147
[Epoch 89; Iter    22/   41] train: loss: 0.0198563
[Epoch 89] ogbg-molbace: 0.741758 val loss: 1.406590
[Epoch 89] ogbg-molbace: 0.813424 test loss: 1.311729
[Epoch 90; Iter    11/   41] train: loss: 0.0939092
[Epoch 90; Iter    41/   41] train: loss: 0.3890872
[Epoch 90] ogbg-molbace: 0.709524 val loss: 1.437650
[Epoch 90] ogbg-molbace: 0.816728 test loss: 2.091545
[Epoch 91; Iter    30/   41] train: loss: 0.0304860
[Epoch 91] ogbg-molbace: 0.720513 val loss: 1.542247
[Epoch 91] ogbg-molbace: 0.813945 test loss: 1.994434
[Epoch 92; Iter    19/   41] train: loss: 0.2217895
[Epoch 92] ogbg-molbace: 0.705495 val loss: 1.692182
[Epoch 92] ogbg-molbace: 0.825943 test loss: 1.797953
[Epoch 93; Iter     8/   41] train: loss: 0.2161878
[Epoch 93; Iter    38/   41] train: loss: 0.0930150
[Epoch 93] ogbg-molbace: 0.733700 val loss: 1.605883
[Epoch 93] ogbg-molbace: 0.816728 test loss: 2.484844
[Epoch 94; Iter    27/   41] train: loss: 0.0580491
[Epoch 94] ogbg-molbace: 0.670696 val loss: 2.131609
[Epoch 94] ogbg-molbace: 0.785081 test loss: 1.997033
[Epoch 95; Iter    16/   41] train: loss: 0.0918394
[Epoch 95] ogbg-molbace: 0.722344 val loss: 1.542280
[Epoch 95] ogbg-molbace: 0.822640 test loss: 1.258349
[Epoch 96; Iter     5/   41] train: loss: 0.0221102
[Epoch 96; Iter    35/   41] train: loss: 0.0357808
[Epoch 96] ogbg-molbace: 0.706227 val loss: 1.771715
[Epoch 96] ogbg-molbace: 0.810816 test loss: 1.984369
[Epoch 97; Iter    24/   41] train: loss: 0.0706429
[Epoch 97] ogbg-molbace: 0.682784 val loss: 1.722819
[Epoch 97] ogbg-molbace: 0.801252 test loss: 1.871623
[Epoch 98; Iter    13/   41] train: loss: 0.0674636
[Epoch 98] ogbg-molbace: 0.706227 val loss: 1.599998
[Epoch 98] ogbg-molbace: 0.814641 test loss: 1.800051
[Epoch 99; Iter     2/   41] train: loss: 0.1258664
[Epoch 99; Iter    32/   41] train: loss: 0.0771862
[Epoch 99] ogbg-molbace: 0.743223 val loss: 1.569527
[Epoch 99] ogbg-molbace: 0.811511 test loss: 1.763838
[Epoch 100; Iter    21/   41] train: loss: 0.0608724
[Epoch 100] ogbg-molbace: 0.727106 val loss: 1.336245
[Epoch 100] ogbg-molbace: 0.842984 test loss: 1.726025
[Epoch 101; Iter    10/   41] train: loss: 0.0776836
[Epoch 101; Iter    40/   41] train: loss: 0.0305430
[Epoch 101] ogbg-molbace: 0.711355 val loss: 1.790416
[Epoch 101] ogbg-molbace: 0.831855 test loss: 1.780603
[Epoch 102; Iter    29/   41] train: loss: 0.0392129
[Epoch 102] ogbg-molbace: 0.710623 val loss: 1.644066
[Epoch 102] ogbg-molbace: 0.822987 test loss: 2.101578
[Epoch 103; Iter    18/   41] train: loss: 0.0292737
[Epoch 103] ogbg-molbace: 0.726374 val loss: 1.846844
[Epoch 103] ogbg-molbace: 0.823509 test loss: 1.750810
[Epoch 104; Iter     7/   41] train: loss: 0.1310227
[Epoch 104; Iter    37/   41] train: loss: 0.0222941
[Epoch 104] ogbg-molbace: 0.694139 val loss: 1.869806
[Epoch 104] ogbg-molbace: 0.808903 test loss: 2.166918
[Epoch 105; Iter    26/   41] train: loss: 0.0057337
[Epoch 105] ogbg-molbace: 0.714652 val loss: 2.000022
[Epoch 105] ogbg-molbace: 0.816380 test loss: 1.891572
[Epoch 106; Iter    15/   41] train: loss: 0.0165718
[Epoch 106] ogbg-molbace: 0.705861 val loss: 2.139969
[Epoch 106] ogbg-molbace: 0.798818 test loss: 1.881956
[Epoch 107; Iter     4/   41] train: loss: 0.0100031
[Epoch 107; Iter    34/   41] train: loss: 0.0624056
[Epoch 107] ogbg-molbace: 0.712088 val loss: 2.107236
[Epoch 107] ogbg-molbace: 0.812033 test loss: 2.204718
[Epoch 108; Iter    23/   41] train: loss: 0.0760492
[Epoch 108] ogbg-molbace: 0.701099 val loss: 2.002397
[Epoch 108] ogbg-molbace: 0.819857 test loss: 2.129819
[Epoch 109; Iter    12/   41] train: loss: 0.0319141
[Epoch 109] ogbg-molbace: 0.694139 val loss: 2.241325
[Epoch 109] ogbg-molbace: 0.802991 test loss: 2.000243
[Epoch 110; Iter     1/   41] train: loss: 0.0401714
[Epoch 110; Iter    31/   41] train: loss: 0.0232142
[Epoch 110] ogbg-molbace: 0.721245 val loss: 2.205867
[Epoch 110] ogbg-molbace: 0.809424 test loss: 2.127808
[Epoch 111; Iter    20/   41] train: loss: 0.0509653
[Epoch 111] ogbg-molbace: 0.727106 val loss: 2.038534
[Epoch 111] ogbg-molbace: 0.815684 test loss: 2.171841
[Epoch 112; Iter     9/   41] train: loss: 0.0180652
[Epoch 112; Iter    39/   41] train: loss: 0.0088104
[Epoch 112] ogbg-molbace: 0.717582 val loss: 1.999549
[Epoch 112] ogbg-molbace: 0.806990 test loss: 2.136051
[Epoch 113; Iter    28/   41] train: loss: 0.0102229
[Epoch 113] ogbg-molbace: 0.694872 val loss: 2.194416
[Epoch 113] ogbg-molbace: 0.813424 test loss: 2.567345
[Epoch 114; Iter    17/   41] train: loss: 0.0105631
[Epoch 114] ogbg-molbace: 0.716117 val loss: 2.333522
[Epoch 114] ogbg-molbace: 0.809424 test loss: 2.170496
[Epoch 115; Iter     6/   41] train: loss: 0.0552171
[Epoch 115; Iter    36/   41] train: loss: 0.0049633
[Epoch 115] ogbg-molbace: 0.697802 val loss: 2.265074
[Epoch 115] ogbg-molbace: 0.812554 test loss: 2.254161
[Epoch 116; Iter    25/   41] train: loss: 0.0028806
[Epoch 116] ogbg-molbace: 0.712088 val loss: 2.209955
[Epoch 116] ogbg-molbace: 0.811337 test loss: 2.175353
[Epoch 117; Iter    14/   41] train: loss: 0.0419776
[Epoch 117] ogbg-molbace: 0.727473 val loss: 2.226444
[Epoch 117] ogbg-molbace: 0.806468 test loss: 2.610638
[Epoch 118; Iter     3/   41] train: loss: 0.0591916
[Epoch 118; Iter    33/   41] train: loss: 0.0098062
[Epoch 118] ogbg-molbace: 0.712454 val loss: 1.994942
[Epoch 118] ogbg-molbace: 0.808381 test loss: 2.118553
[Epoch 119; Iter    22/   41] train: loss: 0.0151686
[Epoch 119] ogbg-molbace: 0.683516 val loss: 2.045926
[Epoch 119] ogbg-molbace: 0.824378 test loss: 2.523524
[Epoch 120; Iter    11/   41] train: loss: 0.0220207
[Epoch 120; Iter    41/   41] train: loss: 0.0374794
[Epoch 120] ogbg-molbace: 0.716117 val loss: 2.303727
[Epoch 120] ogbg-molbace: 0.800556 test loss: 2.074766
[Epoch 121; Iter    30/   41] train: loss: 0.0635186
[Epoch 121] ogbg-molbace: 0.694139 val loss: 2.108404
[Epoch 121] ogbg-molbace: 0.813076 test loss: 2.487613
[Epoch 122; Iter    19/   41] train: loss: 0.0424319
[Epoch 122] ogbg-molbace: 0.676190 val loss: 2.361171
[Epoch 122] ogbg-molbace: 0.799165 test loss: 2.590871
[Epoch 123; Iter     8/   41] train: loss: 0.2023407
[Epoch 123; Iter    38/   41] train: loss: 0.0430502
[Epoch 123] ogbg-molbace: 0.716484 val loss: 2.028875
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 137; Iter    14/   31] train: loss: 0.0080969
[Epoch 137] ogbg-molbace: 0.764846 val loss: 0.870804
[Epoch 137] ogbg-molbace: 0.804296 test loss: 1.273949
[Epoch 138; Iter    13/   31] train: loss: 0.0174114
[Epoch 138] ogbg-molbace: 0.775968 val loss: 0.829261
[Epoch 138] ogbg-molbace: 0.787863 test loss: 1.575020
[Epoch 139; Iter    12/   31] train: loss: 0.1336811
[Epoch 139] ogbg-molbace: 0.737934 val loss: 1.233561
[Epoch 139] ogbg-molbace: 0.783845 test loss: 1.540108
[Epoch 140; Iter    11/   31] train: loss: 0.0994229
[Epoch 140] ogbg-molbace: 0.777855 val loss: 0.852523
[Epoch 140] ogbg-molbace: 0.788687 test loss: 1.253857
[Epoch 141; Iter    10/   31] train: loss: 0.2726067
[Epoch 141] ogbg-molbace: 0.774578 val loss: 0.750293
[Epoch 141] ogbg-molbace: 0.789203 test loss: 1.136817
[Epoch 142; Iter     9/   31] train: loss: 0.0180611
[Epoch 142] ogbg-molbace: 0.765938 val loss: 0.833560
[Epoch 142] ogbg-molbace: 0.788327 test loss: 1.196503
[Epoch 143; Iter     8/   31] train: loss: 0.0472952
[Epoch 143] ogbg-molbace: 0.777656 val loss: 0.718984
[Epoch 143] ogbg-molbace: 0.792912 test loss: 1.233568
[Epoch 144; Iter     7/   31] train: loss: 0.0279998
[Epoch 144] ogbg-molbace: 0.774181 val loss: 0.736456
[Epoch 144] ogbg-molbace: 0.793942 test loss: 1.379813
[Epoch 145; Iter     6/   31] train: loss: 0.0441050
[Epoch 145] ogbg-molbace: 0.799404 val loss: 0.917466
[Epoch 145] ogbg-molbace: 0.797445 test loss: 1.330490
[Epoch 146; Iter     5/   31] train: loss: 0.0275626
[Epoch 146] ogbg-molbace: 0.768620 val loss: 0.940574
[Epoch 146] ogbg-molbace: 0.799196 test loss: 1.345047
[Epoch 147; Iter     4/   31] train: loss: 0.1377451
[Epoch 147] ogbg-molbace: 0.796624 val loss: 0.996721
[Epoch 147] ogbg-molbace: 0.795899 test loss: 1.378423
[Epoch 148; Iter     3/   31] train: loss: 0.0122034
[Epoch 148] ogbg-molbace: 0.771599 val loss: 0.807643
[Epoch 148] ogbg-molbace: 0.796878 test loss: 1.341749
[Epoch 149; Iter     2/   31] train: loss: 0.0143582
[Epoch 149] ogbg-molbace: 0.771301 val loss: 0.796611
[Epoch 149] ogbg-molbace: 0.797909 test loss: 1.330201
Early stopping criterion based on -ogbg-molbace- that should be max reached after 149 epochs. Best model checkpoint was in epoch 89.
Statistics on  val_best_checkpoint
mean_pred: -2.762212038040161
std_pred: 3.2360856533050537
mean_targets: 0.12541253864765167
std_targets: 0.33173397183418274
prcauc: 0.5185472763930764
rocauc: 0.8644488579940417
ogbg-molbace: 0.8644488579940417
BCEWithLogitsLoss: 0.34751068360426207
Statistics on  test
mean_pred: -0.41226494312286377
std_pred: 3.751094341278076
mean_targets: 0.6963696479797363
std_targets: 0.4605855941772461
prcauc: 0.8889478067944041
rocauc: 0.8047599423037296
ogbg-molbace: 0.8047599423037296
BCEWithLogitsLoss: 0.9051175164905462
Statistics on  train
mean_pred: -1.002826452255249
std_pred: 3.8807449340820312
mean_targets: 0.48732084035873413
std_targets: 0.5001149773597717
prcauc: 0.9838230090637821
rocauc: 0.9844159003551792
ogbg-molbace: 0.9844159003551792
BCEWithLogitsLoss: 0.18284602991996274
All runs completed.
[Epoch 78] ogbg-molbace: 0.735165 val loss: 1.207148
[Epoch 78] ogbg-molbace: 0.777430 test loss: 1.707127
[Epoch 79; Iter    12/   41] train: loss: 0.2068093
[Epoch 79] ogbg-molbace: 0.755311 val loss: 1.352410
[Epoch 79] ogbg-molbace: 0.768562 test loss: 2.186914
[Epoch 80; Iter     1/   41] train: loss: 0.2397466
[Epoch 80; Iter    31/   41] train: loss: 0.0746728
[Epoch 80] ogbg-molbace: 0.753114 val loss: 1.346020
[Epoch 80] ogbg-molbace: 0.787515 test loss: 2.078289
[Epoch 81; Iter    20/   41] train: loss: 0.1809258
[Epoch 81] ogbg-molbace: 0.769963 val loss: 0.990719
[Epoch 81] ogbg-molbace: 0.771518 test loss: 1.604469
[Epoch 82; Iter     9/   41] train: loss: 0.1195644
[Epoch 82; Iter    39/   41] train: loss: 0.0980470
[Epoch 82] ogbg-molbace: 0.750549 val loss: 1.502370
[Epoch 82] ogbg-molbace: 0.781429 test loss: 1.963752
[Epoch 83; Iter    28/   41] train: loss: 0.2152847
[Epoch 83] ogbg-molbace: 0.779487 val loss: 1.101910
[Epoch 83] ogbg-molbace: 0.776039 test loss: 1.805282
[Epoch 84; Iter    17/   41] train: loss: 0.1014175
[Epoch 84] ogbg-molbace: 0.781685 val loss: 1.183921
[Epoch 84] ogbg-molbace: 0.762998 test loss: 2.108263
[Epoch 85; Iter     6/   41] train: loss: 0.2283323
[Epoch 85; Iter    36/   41] train: loss: 0.0879141
[Epoch 85] ogbg-molbace: 0.761172 val loss: 1.455877
[Epoch 85] ogbg-molbace: 0.779169 test loss: 1.951017
[Epoch 86; Iter    25/   41] train: loss: 0.0747625
[Epoch 86] ogbg-molbace: 0.777289 val loss: 1.180633
[Epoch 86] ogbg-molbace: 0.758477 test loss: 2.068766
[Epoch 87; Iter    14/   41] train: loss: 0.1134266
[Epoch 87] ogbg-molbace: 0.754579 val loss: 1.226615
[Epoch 87] ogbg-molbace: 0.768040 test loss: 2.060894
[Epoch 88; Iter     3/   41] train: loss: 0.1248626
[Epoch 88; Iter    33/   41] train: loss: 0.0422650
[Epoch 88] ogbg-molbace: 0.731502 val loss: 1.327035
[Epoch 88] ogbg-molbace: 0.777256 test loss: 1.886159
[Epoch 89; Iter    22/   41] train: loss: 0.1262465
[Epoch 89] ogbg-molbace: 0.724542 val loss: 1.632533
[Epoch 89] ogbg-molbace: 0.760563 test loss: 2.220993
[Epoch 90; Iter    11/   41] train: loss: 0.1193992
[Epoch 90; Iter    41/   41] train: loss: 0.3989879
[Epoch 90] ogbg-molbace: 0.770696 val loss: 1.460895
[Epoch 90] ogbg-molbace: 0.751521 test loss: 1.958239
[Epoch 91; Iter    30/   41] train: loss: 0.0376044
[Epoch 91] ogbg-molbace: 0.751648 val loss: 1.503594
[Epoch 91] ogbg-molbace: 0.771170 test loss: 1.970531
[Epoch 92; Iter    19/   41] train: loss: 0.1855094
[Epoch 92] ogbg-molbace: 0.752747 val loss: 1.185826
[Epoch 92] ogbg-molbace: 0.778473 test loss: 3.543078
[Epoch 93; Iter     8/   41] train: loss: 0.2779580
[Epoch 93; Iter    38/   41] train: loss: 0.0396965
[Epoch 93] ogbg-molbace: 0.794139 val loss: 1.358944
[Epoch 93] ogbg-molbace: 0.758129 test loss: 2.766575
[Epoch 94; Iter    27/   41] train: loss: 0.0858242
[Epoch 94] ogbg-molbace: 0.720513 val loss: 1.449851
[Epoch 94] ogbg-molbace: 0.760737 test loss: 2.538781
[Epoch 95; Iter    16/   41] train: loss: 0.0549335
[Epoch 95] ogbg-molbace: 0.803663 val loss: 0.919271
[Epoch 95] ogbg-molbace: 0.749783 test loss: 2.556553
[Epoch 96; Iter     5/   41] train: loss: 0.1906151
[Epoch 96; Iter    35/   41] train: loss: 0.0427793
[Epoch 96] ogbg-molbace: 0.750916 val loss: 1.215996
[Epoch 96] ogbg-molbace: 0.737611 test loss: 2.274691
[Epoch 97; Iter    24/   41] train: loss: 0.1887926
[Epoch 97] ogbg-molbace: 0.772527 val loss: 1.482666
[Epoch 97] ogbg-molbace: 0.782646 test loss: 2.021354
[Epoch 98; Iter    13/   41] train: loss: 0.1412309
[Epoch 98] ogbg-molbace: 0.791575 val loss: 1.167652
[Epoch 98] ogbg-molbace: 0.749087 test loss: 2.055255
[Epoch 99; Iter     2/   41] train: loss: 0.0052456
[Epoch 99; Iter    32/   41] train: loss: 0.0104183
[Epoch 99] ogbg-molbace: 0.760073 val loss: 1.266703
[Epoch 99] ogbg-molbace: 0.742653 test loss: 2.086151
[Epoch 100; Iter    21/   41] train: loss: 0.0250869
[Epoch 100] ogbg-molbace: 0.772527 val loss: 1.268471
[Epoch 100] ogbg-molbace: 0.748565 test loss: 2.044450
[Epoch 101; Iter    10/   41] train: loss: 0.0472155
[Epoch 101; Iter    40/   41] train: loss: 0.0488848
[Epoch 101] ogbg-molbace: 0.780952 val loss: 1.177986
[Epoch 101] ogbg-molbace: 0.745609 test loss: 2.119604
[Epoch 102; Iter    29/   41] train: loss: 0.0514317
[Epoch 102] ogbg-molbace: 0.775458 val loss: 1.453074
[Epoch 102] ogbg-molbace: 0.744914 test loss: 2.178519
[Epoch 103; Iter    18/   41] train: loss: 0.0317619
[Epoch 103] ogbg-molbace: 0.745788 val loss: 1.555403
[Epoch 103] ogbg-molbace: 0.744566 test loss: 2.277908
[Epoch 104; Iter     7/   41] train: loss: 0.0587431
[Epoch 104; Iter    37/   41] train: loss: 0.0133506
[Epoch 104] ogbg-molbace: 0.741392 val loss: 1.548875
[Epoch 104] ogbg-molbace: 0.777604 test loss: 1.903127
[Epoch 105; Iter    26/   41] train: loss: 0.0107078
[Epoch 105] ogbg-molbace: 0.764469 val loss: 1.204441
[Epoch 105] ogbg-molbace: 0.769258 test loss: 2.206781
[Epoch 106; Iter    15/   41] train: loss: 0.2212640
[Epoch 106] ogbg-molbace: 0.752747 val loss: 1.684845
[Epoch 106] ogbg-molbace: 0.779864 test loss: 2.055868
[Epoch 107; Iter     4/   41] train: loss: 0.1307790
[Epoch 107; Iter    34/   41] train: loss: 0.0125318
[Epoch 107] ogbg-molbace: 0.778388 val loss: 1.309186
[Epoch 107] ogbg-molbace: 0.754999 test loss: 2.423000
[Epoch 108; Iter    23/   41] train: loss: 0.0612731
[Epoch 108] ogbg-molbace: 0.777656 val loss: 1.467552
[Epoch 108] ogbg-molbace: 0.752391 test loss: 2.325303
[Epoch 109; Iter    12/   41] train: loss: 0.0390208
[Epoch 109] ogbg-molbace: 0.720513 val loss: 1.933592
[Epoch 109] ogbg-molbace: 0.768388 test loss: 2.210302
[Epoch 110; Iter     1/   41] train: loss: 0.0118370
[Epoch 110; Iter    31/   41] train: loss: 0.0169033
[Epoch 110] ogbg-molbace: 0.709524 val loss: 1.793901
[Epoch 110] ogbg-molbace: 0.759172 test loss: 2.202164
[Epoch 111; Iter    20/   41] train: loss: 0.0158931
[Epoch 111] ogbg-molbace: 0.727106 val loss: 1.771515
[Epoch 111] ogbg-molbace: 0.744392 test loss: 2.805608
[Epoch 112; Iter     9/   41] train: loss: 0.0163072
[Epoch 112; Iter    39/   41] train: loss: 0.0279484
[Epoch 112] ogbg-molbace: 0.736630 val loss: 1.920745
[Epoch 112] ogbg-molbace: 0.749957 test loss: 2.989846
[Epoch 113; Iter    28/   41] train: loss: 0.0119417
[Epoch 113] ogbg-molbace: 0.721978 val loss: 1.949830
[Epoch 113] ogbg-molbace: 0.752391 test loss: 2.195562
[Epoch 114; Iter    17/   41] train: loss: 0.3031251
[Epoch 114] ogbg-molbace: 0.685348 val loss: 2.404147
[Epoch 114] ogbg-molbace: 0.770127 test loss: 2.325180
[Epoch 115; Iter     6/   41] train: loss: 0.0297623
[Epoch 115; Iter    36/   41] train: loss: 0.0329870
[Epoch 115] ogbg-molbace: 0.730037 val loss: 2.226441
[Epoch 115] ogbg-molbace: 0.754477 test loss: 2.378669
[Epoch 116; Iter    25/   41] train: loss: 0.0538764
[Epoch 116] ogbg-molbace: 0.727106 val loss: 2.214718
[Epoch 116] ogbg-molbace: 0.743697 test loss: 2.461533
[Epoch 117; Iter    14/   41] train: loss: 0.0520061
[Epoch 117] ogbg-molbace: 0.746154 val loss: 2.015744
[Epoch 117] ogbg-molbace: 0.787689 test loss: 1.936254
[Epoch 118; Iter     3/   41] train: loss: 0.0137132
[Epoch 118; Iter    33/   41] train: loss: 0.0111363
[Epoch 118] ogbg-molbace: 0.753846 val loss: 1.977597
[Epoch 118] ogbg-molbace: 0.775343 test loss: 2.271036
[Epoch 119; Iter    22/   41] train: loss: 0.0211753
[Epoch 119] ogbg-molbace: 0.726007 val loss: 2.148863
[Epoch 119] ogbg-molbace: 0.767171 test loss: 2.372535
[Epoch 120; Iter    11/   41] train: loss: 0.0980842
[Epoch 120; Iter    41/   41] train: loss: 0.0140478
[Epoch 120] ogbg-molbace: 0.728938 val loss: 2.304904
[Epoch 120] ogbg-molbace: 0.772214 test loss: 2.169802
[Epoch 121; Iter    30/   41] train: loss: 0.0556540
[Epoch 121] ogbg-molbace: 0.739927 val loss: 2.187187
[Epoch 121] ogbg-molbace: 0.761954 test loss: 2.137937
[Epoch 122; Iter    19/   41] train: loss: 0.0128134
[Epoch 122] ogbg-molbace: 0.728205 val loss: 2.097241
[Epoch 122] ogbg-molbace: 0.778299 test loss: 2.063677
[Epoch 123; Iter     8/   41] train: loss: 0.0343878
[Epoch 123; Iter    38/   41] train: loss: 0.0465363
[Epoch 123] ogbg-molbace: 0.765568 val loss: 2.121521
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.797253 test loss: 2.631139
[Epoch 124; Iter    27/   41] train: loss: 0.0284454
[Epoch 124] ogbg-molbace: 0.694139 val loss: 2.323521
[Epoch 124] ogbg-molbace: 0.805077 test loss: 2.800040
[Epoch 125; Iter    16/   41] train: loss: 0.0059876
[Epoch 125] ogbg-molbace: 0.725641 val loss: 1.847531
[Epoch 125] ogbg-molbace: 0.803165 test loss: 2.477764
[Epoch 126; Iter     5/   41] train: loss: 0.0055484
[Epoch 126; Iter    35/   41] train: loss: 0.0051914
[Epoch 126] ogbg-molbace: 0.732601 val loss: 2.069236
[Epoch 126] ogbg-molbace: 0.800383 test loss: 2.410184
Early stopping criterion based on -ogbg-molbace- that should be max reached after 126 epochs. Best model checkpoint was in epoch 66.
Statistics on  val_best_checkpoint
mean_pred: -0.1149471327662468
std_pred: 3.104233741760254
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9624874525960736
rocauc: 0.7952380952380953
ogbg-molbace: 0.7952380952380953
BCEWithLogitsLoss: 1.2053057948748271
Statistics on  test
mean_pred: -2.8017802238464355
std_pred: 3.3758912086486816
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7890214819646812
rocauc: 0.7659537471744045
ogbg-molbace: 0.7659537471744045
BCEWithLogitsLoss: 1.790112003373603
Statistics on  train
mean_pred: -2.2709875106811523
std_pred: 4.198091983795166
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9700355158375189
rocauc: 0.9807334474885845
ogbg-molbace: 0.9807334474885845
BCEWithLogitsLoss: 0.19718480737107555
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 130; Iter     6/   36] train: loss: 0.0056340
[Epoch 130; Iter    36/   36] train: loss: 0.0057187
[Epoch 130] ogbg-molbace: 0.777590 val loss: 1.278257
[Epoch 130] ogbg-molbace: 0.824034 test loss: 1.306512
[Epoch 131; Iter    30/   36] train: loss: 0.0038896
[Epoch 131] ogbg-molbace: 0.783404 val loss: 1.274151
[Epoch 131] ogbg-molbace: 0.827465 test loss: 1.290831
[Epoch 132; Iter    24/   36] train: loss: 0.0064440
[Epoch 132] ogbg-molbace: 0.784989 val loss: 1.223631
[Epoch 132] ogbg-molbace: 0.825479 test loss: 1.311685
[Epoch 133; Iter    18/   36] train: loss: 0.0089851
[Epoch 133] ogbg-molbace: 0.780127 val loss: 1.227506
[Epoch 133] ogbg-molbace: 0.822770 test loss: 1.360359
[Epoch 134; Iter    12/   36] train: loss: 0.0064922
[Epoch 134] ogbg-molbace: 0.779493 val loss: 1.195536
[Epoch 134] ogbg-molbace: 0.821235 test loss: 1.343865
[Epoch 135; Iter     6/   36] train: loss: 0.0155595
[Epoch 135; Iter    36/   36] train: loss: 0.0014453
[Epoch 135] ogbg-molbace: 0.776638 val loss: 1.223535
[Epoch 135] ogbg-molbace: 0.824305 test loss: 1.367882
[Epoch 136; Iter    30/   36] train: loss: 0.0072825
[Epoch 136] ogbg-molbace: 0.779281 val loss: 1.183524
[Epoch 136] ogbg-molbace: 0.820964 test loss: 1.523909
[Epoch 137; Iter    24/   36] train: loss: 0.0129069
[Epoch 137] ogbg-molbace: 0.777167 val loss: 1.217018
[Epoch 137] ogbg-molbace: 0.829722 test loss: 1.411530
[Epoch 138; Iter    18/   36] train: loss: 0.0068105
[Epoch 138] ogbg-molbace: 0.775264 val loss: 1.322664
[Epoch 138] ogbg-molbace: 0.833875 test loss: 1.244568
[Epoch 139; Iter    12/   36] train: loss: 0.0093378
[Epoch 139] ogbg-molbace: 0.776216 val loss: 1.344208
[Epoch 139] ogbg-molbace: 0.829541 test loss: 1.224247
[Epoch 140; Iter     6/   36] train: loss: 0.0068294
[Epoch 140; Iter    36/   36] train: loss: 0.1972243
[Epoch 140] ogbg-molbace: 0.778224 val loss: 1.325243
[Epoch 140] ogbg-molbace: 0.829451 test loss: 1.336327
[Epoch 141; Iter    30/   36] train: loss: 0.0395244
[Epoch 141] ogbg-molbace: 0.770825 val loss: 1.192721
[Epoch 141] ogbg-molbace: 0.835320 test loss: 1.279804
[Epoch 142; Iter    24/   36] train: loss: 0.0040756
[Epoch 142] ogbg-molbace: 0.773784 val loss: 1.463443
[Epoch 142] ogbg-molbace: 0.820964 test loss: 1.279319
[Epoch 143; Iter    18/   36] train: loss: 0.0047652
[Epoch 143] ogbg-molbace: 0.772304 val loss: 1.269366
[Epoch 143] ogbg-molbace: 0.826923 test loss: 1.350161
[Epoch 144; Iter    12/   36] train: loss: 0.0035129
[Epoch 144] ogbg-molbace: 0.777378 val loss: 1.359789
[Epoch 144] ogbg-molbace: 0.830986 test loss: 1.346140
[Epoch 145; Iter     6/   36] train: loss: 0.0132968
[Epoch 145; Iter    36/   36] train: loss: 0.0140821
[Epoch 145] ogbg-molbace: 0.776004 val loss: 1.365628
[Epoch 145] ogbg-molbace: 0.825749 test loss: 1.363225
[Epoch 146; Iter    30/   36] train: loss: 0.0044027
[Epoch 146] ogbg-molbace: 0.775476 val loss: 1.278644
[Epoch 146] ogbg-molbace: 0.823944 test loss: 1.364980
[Epoch 147; Iter    24/   36] train: loss: 0.0037555
[Epoch 147] ogbg-molbace: 0.776638 val loss: 1.329880
[Epoch 147] ogbg-molbace: 0.824305 test loss: 1.431944
[Epoch 148; Iter    18/   36] train: loss: 0.0056690
[Epoch 148] ogbg-molbace: 0.769979 val loss: 1.412390
[Epoch 148] ogbg-molbace: 0.820061 test loss: 1.410210
[Epoch 149; Iter    12/   36] train: loss: 0.0492888
[Epoch 149] ogbg-molbace: 0.788584 val loss: 1.147331
[Epoch 149] ogbg-molbace: 0.819339 test loss: 1.353200
[Epoch 150; Iter     6/   36] train: loss: 0.0062587
[Epoch 150; Iter    36/   36] train: loss: 0.0038437
[Epoch 150] ogbg-molbace: 0.782030 val loss: 1.233709
[Epoch 150] ogbg-molbace: 0.818978 test loss: 1.426243
[Epoch 151; Iter    30/   36] train: loss: 0.0063809
[Epoch 151] ogbg-molbace: 0.781184 val loss: 1.378298
[Epoch 151] ogbg-molbace: 0.819520 test loss: 1.331623
[Epoch 152; Iter    24/   36] train: loss: 0.0033242
[Epoch 152] ogbg-molbace: 0.787209 val loss: 1.216388
[Epoch 152] ogbg-molbace: 0.816992 test loss: 1.425312
[Epoch 153; Iter    18/   36] train: loss: 0.0060469
[Epoch 153] ogbg-molbace: 0.782135 val loss: 1.391639
[Epoch 153] ogbg-molbace: 0.824124 test loss: 1.296193
[Epoch 154; Iter    12/   36] train: loss: 0.0014467
[Epoch 154] ogbg-molbace: 0.769027 val loss: 1.170373
[Epoch 154] ogbg-molbace: 0.812929 test loss: 1.444408
[Epoch 155; Iter     6/   36] train: loss: 0.0017178
[Epoch 155; Iter    36/   36] train: loss: 0.0011532
[Epoch 155] ogbg-molbace: 0.773150 val loss: 1.397560
[Epoch 155] ogbg-molbace: 0.819159 test loss: 1.373077
[Epoch 156; Iter    30/   36] train: loss: 0.0037313
[Epoch 156] ogbg-molbace: 0.781184 val loss: 1.303517
[Epoch 156] ogbg-molbace: 0.822048 test loss: 1.432185
[Epoch 157; Iter    24/   36] train: loss: 0.0016306
[Epoch 157] ogbg-molbace: 0.774736 val loss: 1.418582
[Epoch 157] ogbg-molbace: 0.823583 test loss: 1.335299
[Epoch 158; Iter    18/   36] train: loss: 0.0037341
[Epoch 158] ogbg-molbace: 0.777061 val loss: 1.216775
[Epoch 158] ogbg-molbace: 0.819971 test loss: 1.471808
[Epoch 159; Iter    12/   36] train: loss: 0.0042693
[Epoch 159] ogbg-molbace: 0.776004 val loss: 1.341064
[Epoch 159] ogbg-molbace: 0.816089 test loss: 1.379014
[Epoch 160; Iter     6/   36] train: loss: 0.0017646
[Epoch 160; Iter    36/   36] train: loss: 0.0052056
[Epoch 160] ogbg-molbace: 0.774207 val loss: 1.365488
[Epoch 160] ogbg-molbace: 0.822680 test loss: 1.442494
Early stopping criterion based on -ogbg-molbace- that should be max reached after 160 epochs. Best model checkpoint was in epoch 100.
Statistics on  val_best_checkpoint
mean_pred: -2.0874452590942383
std_pred: 4.380468368530273
mean_targets: 0.24229073524475098
std_targets: 0.4294162094593048
prcauc: 0.5127823055651319
rocauc: 0.804122621564482
ogbg-molbace: 0.804122621564482
BCEWithLogitsLoss: 0.7564390264451504
Statistics on  test
mean_pred: 0.015569879673421383
std_pred: 5.1762518882751465
mean_targets: 0.6872246265411377
std_targets: 0.4646482765674591
prcauc: 0.871324932524089
rocauc: 0.8213253882267967
ogbg-molbace: 0.8213253882267967
BCEWithLogitsLoss: 0.9808868672698736
Statistics on  train
mean_pred: -1.191636562347412
std_pred: 5.711248874664307
mean_targets: 0.45325779914855957
std_targets: 0.49804556369781494
prcauc: 0.996999103421957
rocauc: 0.9975604490500865
ogbg-molbace: 0.9975604490500865
BCEWithLogitsLoss: 0.07299489564158851
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 130; Iter     6/   36] train: loss: 0.0570879
[Epoch 130; Iter    36/   36] train: loss: 0.0074014
[Epoch 130] ogbg-molbace: 0.795137 val loss: 1.118240
[Epoch 130] ogbg-molbace: 0.824034 test loss: 1.291868
[Epoch 131; Iter    30/   36] train: loss: 0.2246648
[Epoch 131] ogbg-molbace: 0.782981 val loss: 1.214578
[Epoch 131] ogbg-molbace: 0.831437 test loss: 1.294781
[Epoch 132; Iter    24/   36] train: loss: 0.0820531
[Epoch 132] ogbg-molbace: 0.792389 val loss: 1.132029
[Epoch 132] ogbg-molbace: 0.835952 test loss: 1.409056
[Epoch 133; Iter    18/   36] train: loss: 0.0086588
[Epoch 133] ogbg-molbace: 0.795560 val loss: 1.175201
[Epoch 133] ogbg-molbace: 0.832972 test loss: 1.341078
[Epoch 134; Iter    12/   36] train: loss: 0.0314724
[Epoch 134] ogbg-molbace: 0.801057 val loss: 1.326470
[Epoch 134] ogbg-molbace: 0.839473 test loss: 1.281832
[Epoch 135; Iter     6/   36] train: loss: 0.0179800
[Epoch 135; Iter    36/   36] train: loss: 0.0023767
[Epoch 135] ogbg-molbace: 0.798414 val loss: 1.278624
[Epoch 135] ogbg-molbace: 0.837757 test loss: 1.299402
[Epoch 136; Iter    30/   36] train: loss: 0.0018278
[Epoch 136] ogbg-molbace: 0.792178 val loss: 1.234187
[Epoch 136] ogbg-molbace: 0.837938 test loss: 1.363507
[Epoch 137; Iter    24/   36] train: loss: 0.0392514
[Epoch 137] ogbg-molbace: 0.796512 val loss: 1.415290
[Epoch 137] ogbg-molbace: 0.839653 test loss: 1.250565
[Epoch 138; Iter    18/   36] train: loss: 0.0044792
[Epoch 138] ogbg-molbace: 0.797674 val loss: 1.292487
[Epoch 138] ogbg-molbace: 0.839473 test loss: 1.295971
[Epoch 139; Iter    12/   36] train: loss: 0.0024319
[Epoch 139] ogbg-molbace: 0.797992 val loss: 1.281575
[Epoch 139] ogbg-molbace: 0.839834 test loss: 1.418451
[Epoch 140; Iter     6/   36] train: loss: 0.0024714
[Epoch 140; Iter    36/   36] train: loss: 0.0009322
[Epoch 140] ogbg-molbace: 0.791649 val loss: 1.357611
[Epoch 140] ogbg-molbace: 0.833875 test loss: 1.283195
[Epoch 141; Iter    30/   36] train: loss: 0.0010090
[Epoch 141] ogbg-molbace: 0.791438 val loss: 1.302638
[Epoch 141] ogbg-molbace: 0.831437 test loss: 1.310265
[Epoch 142; Iter    24/   36] train: loss: 0.0013975
[Epoch 142] ogbg-molbace: 0.791543 val loss: 1.307566
[Epoch 142] ogbg-molbace: 0.834597 test loss: 1.322682
[Epoch 143; Iter    18/   36] train: loss: 0.0069751
[Epoch 143] ogbg-molbace: 0.791543 val loss: 1.341415
[Epoch 143] ogbg-molbace: 0.833785 test loss: 1.342949
[Epoch 144; Iter    12/   36] train: loss: 0.0017996
[Epoch 144] ogbg-molbace: 0.792706 val loss: 1.299111
[Epoch 144] ogbg-molbace: 0.830715 test loss: 1.277778
[Epoch 145; Iter     6/   36] train: loss: 0.0018567
[Epoch 145; Iter    36/   36] train: loss: 0.0048295
[Epoch 145] ogbg-molbace: 0.794820 val loss: 1.353762
[Epoch 145] ogbg-molbace: 0.834326 test loss: 1.271474
[Epoch 146; Iter    30/   36] train: loss: 0.0021013
[Epoch 146] ogbg-molbace: 0.791966 val loss: 1.298463
[Epoch 146] ogbg-molbace: 0.836222 test loss: 1.357695
[Epoch 147; Iter    24/   36] train: loss: 0.0096247
[Epoch 147] ogbg-molbace: 0.797040 val loss: 1.268787
[Epoch 147] ogbg-molbace: 0.841008 test loss: 1.380655
[Epoch 148; Iter    18/   36] train: loss: 0.0155207
[Epoch 148] ogbg-molbace: 0.797252 val loss: 1.348390
[Epoch 148] ogbg-molbace: 0.837125 test loss: 1.387673
[Epoch 149; Iter    12/   36] train: loss: 0.0044814
[Epoch 149] ogbg-molbace: 0.788161 val loss: 1.300070
[Epoch 149] ogbg-molbace: 0.834868 test loss: 1.366190
[Epoch 150; Iter     6/   36] train: loss: 0.0045248
[Epoch 150; Iter    36/   36] train: loss: 0.0756530
[Epoch 150] ogbg-molbace: 0.786575 val loss: 1.339977
[Epoch 150] ogbg-molbace: 0.838480 test loss: 1.410861
[Epoch 151; Iter    30/   36] train: loss: 0.0009637
[Epoch 151] ogbg-molbace: 0.796829 val loss: 1.512923
[Epoch 151] ogbg-molbace: 0.844800 test loss: 1.259869
[Epoch 152; Iter    24/   36] train: loss: 0.0034827
[Epoch 152] ogbg-molbace: 0.802431 val loss: 1.356083
[Epoch 152] ogbg-molbace: 0.844077 test loss: 1.285199
[Epoch 153; Iter    18/   36] train: loss: 0.0035735
[Epoch 153] ogbg-molbace: 0.805814 val loss: 1.400331
[Epoch 153] ogbg-molbace: 0.844890 test loss: 1.239873
[Epoch 154; Iter    12/   36] train: loss: 0.0038699
[Epoch 154] ogbg-molbace: 0.799577 val loss: 1.457651
[Epoch 154] ogbg-molbace: 0.839924 test loss: 1.272592
[Epoch 155; Iter     6/   36] train: loss: 0.0017914
[Epoch 155; Iter    36/   36] train: loss: 0.0445557
[Epoch 155] ogbg-molbace: 0.800846 val loss: 1.410446
[Epoch 155] ogbg-molbace: 0.842813 test loss: 1.333768
[Epoch 156; Iter    30/   36] train: loss: 0.0022516
[Epoch 156] ogbg-molbace: 0.794609 val loss: 1.475446
[Epoch 156] ogbg-molbace: 0.837306 test loss: 1.316938
[Epoch 157; Iter    24/   36] train: loss: 0.0017318
[Epoch 157] ogbg-molbace: 0.796512 val loss: 1.464542
[Epoch 157] ogbg-molbace: 0.841730 test loss: 1.324807
[Epoch 158; Iter    18/   36] train: loss: 0.0010159
[Epoch 158] ogbg-molbace: 0.787738 val loss: 1.449288
[Epoch 158] ogbg-molbace: 0.839834 test loss: 1.331590
[Epoch 159; Iter    12/   36] train: loss: 0.0307828
[Epoch 159] ogbg-molbace: 0.772727 val loss: 1.447604
[Epoch 159] ogbg-molbace: 0.844890 test loss: 1.373058
[Epoch 160; Iter     6/   36] train: loss: 0.0014243
[Epoch 160; Iter    36/   36] train: loss: 0.0014685
[Epoch 160] ogbg-molbace: 0.771247 val loss: 1.519311
[Epoch 160] ogbg-molbace: 0.841008 test loss: 1.364908
[Epoch 161; Iter    30/   36] train: loss: 0.0025505
[Epoch 161] ogbg-molbace: 0.774207 val loss: 1.504323
[Epoch 161] ogbg-molbace: 0.842723 test loss: 1.367573
[Epoch 162; Iter    24/   36] train: loss: 0.0017755
[Epoch 162] ogbg-molbace: 0.774630 val loss: 1.491615
[Epoch 162] ogbg-molbace: 0.841278 test loss: 1.373540
[Epoch 163; Iter    18/   36] train: loss: 0.0039367
[Epoch 163] ogbg-molbace: 0.778858 val loss: 1.580788
[Epoch 163] ogbg-molbace: 0.842633 test loss: 1.301183
[Epoch 164; Iter    12/   36] train: loss: 0.0012229
[Epoch 164] ogbg-molbace: 0.772093 val loss: 1.661963
[Epoch 164] ogbg-molbace: 0.838480 test loss: 1.319435
[Epoch 165; Iter     6/   36] train: loss: 0.0170072
[Epoch 165; Iter    36/   36] train: loss: 0.0138418
[Epoch 165] ogbg-molbace: 0.765751 val loss: 1.514010
[Epoch 165] ogbg-molbace: 0.840827 test loss: 1.451484
[Epoch 166; Iter    30/   36] train: loss: 0.0041658
[Epoch 166] ogbg-molbace: 0.773467 val loss: 1.523671
[Epoch 166] ogbg-molbace: 0.841188 test loss: 1.424707
Early stopping criterion based on -ogbg-molbace- that should be max reached after 166 epochs. Best model checkpoint was in epoch 106.
Statistics on  val_best_checkpoint
mean_pred: -1.4957882165908813
std_pred: 5.38517951965332
mean_targets: 0.24229073524475098
std_targets: 0.4294162094593048
prcauc: 0.5483233967494845
rocauc: 0.8193446088794927
ogbg-molbace: 0.8193446088794927
BCEWithLogitsLoss: 0.9487353153526783
Statistics on  test
mean_pred: 0.5572981834411621
std_pred: 6.55996036529541
mean_targets: 0.6872246265411377
std_targets: 0.4646482765674591
prcauc: 0.9152544356446237
rocauc: 0.8437161430119178
ogbg-molbace: 0.8437161430119178
BCEWithLogitsLoss: 1.0175320962443948
Statistics on  train
mean_pred: -0.9816762804985046
std_pred: 6.806816577911377
mean_targets: 0.45325779914855957
std_targets: 0.49804553389549255
prcauc: 0.9997956181815297
rocauc: 0.9998272884283247
ogbg-molbace: 0.9998272884283247
BCEWithLogitsLoss: 0.029833217022112675
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.767693 test loss: 2.096427
[Epoch 124; Iter    27/   41] train: loss: 0.0382754
[Epoch 124] ogbg-molbace: 0.752747 val loss: 2.181535
[Epoch 124] ogbg-molbace: 0.768910 test loss: 2.413056
[Epoch 125; Iter    16/   41] train: loss: 0.0058751
[Epoch 125] ogbg-molbace: 0.754579 val loss: 2.370986
[Epoch 125] ogbg-molbace: 0.763172 test loss: 2.403040
[Epoch 126; Iter     5/   41] train: loss: 0.0061130
[Epoch 126; Iter    35/   41] train: loss: 0.0045950
[Epoch 126] ogbg-molbace: 0.746520 val loss: 2.388715
[Epoch 126] ogbg-molbace: 0.766475 test loss: 2.389144
[Epoch 127; Iter    24/   41] train: loss: 0.0089404
[Epoch 127] ogbg-molbace: 0.754212 val loss: 2.546460
[Epoch 127] ogbg-molbace: 0.769605 test loss: 2.293691
[Epoch 128; Iter    13/   41] train: loss: 0.0196998
[Epoch 128] ogbg-molbace: 0.753846 val loss: 2.443475
[Epoch 128] ogbg-molbace: 0.777778 test loss: 2.349332
[Epoch 129; Iter     2/   41] train: loss: 0.0375029
[Epoch 129; Iter    32/   41] train: loss: 0.0020615
[Epoch 129] ogbg-molbace: 0.745788 val loss: 2.510041
[Epoch 129] ogbg-molbace: 0.771866 test loss: 2.470140
[Epoch 130; Iter    21/   41] train: loss: 0.0119905
[Epoch 130] ogbg-molbace: 0.737729 val loss: 2.350452
[Epoch 130] ogbg-molbace: 0.749435 test loss: 2.512126
[Epoch 131; Iter    10/   41] train: loss: 0.0360148
[Epoch 131; Iter    40/   41] train: loss: 0.0028627
[Epoch 131] ogbg-molbace: 0.736996 val loss: 2.450870
[Epoch 131] ogbg-molbace: 0.764563 test loss: 2.358920
[Epoch 132; Iter    29/   41] train: loss: 0.0048108
[Epoch 132] ogbg-molbace: 0.757509 val loss: 2.184934
[Epoch 132] ogbg-molbace: 0.759520 test loss: 2.286186
[Epoch 133; Iter    18/   41] train: loss: 0.0062774
[Epoch 133] ogbg-molbace: 0.757875 val loss: 2.294137
[Epoch 133] ogbg-molbace: 0.764389 test loss: 2.344980
[Epoch 134; Iter     7/   41] train: loss: 0.0065112
[Epoch 134; Iter    37/   41] train: loss: 0.0022065
[Epoch 134] ogbg-molbace: 0.753480 val loss: 2.450708
[Epoch 134] ogbg-molbace: 0.763172 test loss: 2.501675
[Epoch 135; Iter    26/   41] train: loss: 0.0089511
[Epoch 135] ogbg-molbace: 0.741758 val loss: 2.380137
[Epoch 135] ogbg-molbace: 0.765084 test loss: 2.306212
[Epoch 136; Iter    15/   41] train: loss: 0.0064184
[Epoch 136] ogbg-molbace: 0.741392 val loss: 2.290562
[Epoch 136] ogbg-molbace: 0.786646 test loss: 2.229520
[Epoch 137; Iter     4/   41] train: loss: 0.0113573
[Epoch 137; Iter    34/   41] train: loss: 0.0621149
[Epoch 137] ogbg-molbace: 0.750183 val loss: 2.374631
[Epoch 137] ogbg-molbace: 0.757433 test loss: 2.442887
[Epoch 138; Iter    23/   41] train: loss: 0.0090506
[Epoch 138] ogbg-molbace: 0.743590 val loss: 2.202404
[Epoch 138] ogbg-molbace: 0.748044 test loss: 2.538624
[Epoch 139; Iter    12/   41] train: loss: 0.0073292
[Epoch 139] ogbg-molbace: 0.752381 val loss: 1.899663
[Epoch 139] ogbg-molbace: 0.758129 test loss: 2.586624
[Epoch 140; Iter     1/   41] train: loss: 0.0063606
[Epoch 140; Iter    31/   41] train: loss: 0.0047521
[Epoch 140] ogbg-molbace: 0.754579 val loss: 2.003512
[Epoch 140] ogbg-molbace: 0.760389 test loss: 2.547196
[Epoch 141; Iter    20/   41] train: loss: 0.0072948
[Epoch 141] ogbg-molbace: 0.742491 val loss: 1.962060
[Epoch 141] ogbg-molbace: 0.755173 test loss: 2.506462
[Epoch 142; Iter     9/   41] train: loss: 0.0025447
[Epoch 142; Iter    39/   41] train: loss: 0.0041492
[Epoch 142] ogbg-molbace: 0.738828 val loss: 2.162159
[Epoch 142] ogbg-molbace: 0.750304 test loss: 2.635837
[Epoch 143; Iter    28/   41] train: loss: 0.0176597
[Epoch 143] ogbg-molbace: 0.750183 val loss: 1.983094
[Epoch 143] ogbg-molbace: 0.767693 test loss: 2.560020
[Epoch 144; Iter    17/   41] train: loss: 0.0029634
[Epoch 144] ogbg-molbace: 0.758242 val loss: 1.932862
[Epoch 144] ogbg-molbace: 0.750478 test loss: 2.521985
[Epoch 145; Iter     6/   41] train: loss: 0.0037866
[Epoch 145; Iter    36/   41] train: loss: 0.0076194
[Epoch 145] ogbg-molbace: 0.750183 val loss: 1.860263
[Epoch 145] ogbg-molbace: 0.746479 test loss: 2.589622
[Epoch 146; Iter    25/   41] train: loss: 0.0020749
[Epoch 146] ogbg-molbace: 0.727839 val loss: 2.263849
[Epoch 146] ogbg-molbace: 0.757433 test loss: 2.643047
[Epoch 147; Iter    14/   41] train: loss: 0.0028547
[Epoch 147] ogbg-molbace: 0.729304 val loss: 2.478485
[Epoch 147] ogbg-molbace: 0.772387 test loss: 2.366312
[Epoch 148; Iter     3/   41] train: loss: 0.0024645
[Epoch 148; Iter    33/   41] train: loss: 0.0043175
[Epoch 148] ogbg-molbace: 0.731136 val loss: 2.396279
[Epoch 148] ogbg-molbace: 0.769605 test loss: 2.447580
[Epoch 149; Iter    22/   41] train: loss: 0.0063163
[Epoch 149] ogbg-molbace: 0.716117 val loss: 2.393767
[Epoch 149] ogbg-molbace: 0.774648 test loss: 2.559894
[Epoch 150; Iter    11/   41] train: loss: 0.0025786
[Epoch 150; Iter    41/   41] train: loss: 0.0024892
[Epoch 150] ogbg-molbace: 0.717582 val loss: 2.409694
[Epoch 150] ogbg-molbace: 0.767693 test loss: 2.631207
[Epoch 151; Iter    30/   41] train: loss: 0.0030520
[Epoch 151] ogbg-molbace: 0.726740 val loss: 2.213907
[Epoch 151] ogbg-molbace: 0.771170 test loss: 2.471654
[Epoch 152; Iter    19/   41] train: loss: 0.0038994
[Epoch 152] ogbg-molbace: 0.730403 val loss: 2.297850
[Epoch 152] ogbg-molbace: 0.773952 test loss: 2.637613
[Epoch 153; Iter     8/   41] train: loss: 0.0048328
[Epoch 153; Iter    38/   41] train: loss: 0.0017318
[Epoch 153] ogbg-molbace: 0.734799 val loss: 2.427582
[Epoch 153] ogbg-molbace: 0.778821 test loss: 2.553575
[Epoch 154; Iter    27/   41] train: loss: 0.0015837
[Epoch 154] ogbg-molbace: 0.732967 val loss: 2.406561
[Epoch 154] ogbg-molbace: 0.776561 test loss: 2.432970
[Epoch 155; Iter    16/   41] train: loss: 0.0052394
[Epoch 155] ogbg-molbace: 0.730769 val loss: 2.541011
[Epoch 155] ogbg-molbace: 0.776387 test loss: 2.539167
Early stopping criterion based on -ogbg-molbace- that should be max reached after 155 epochs. Best model checkpoint was in epoch 95.
Statistics on  val_best_checkpoint
mean_pred: 1.3885442018508911
std_pred: 3.6810295581817627
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9466946831304849
rocauc: 0.8036630036630037
ogbg-molbace: 0.8036630036630037
BCEWithLogitsLoss: 0.9192714740832647
Statistics on  test
mean_pred: -1.0048456192016602
std_pred: 10.016844749450684
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7182553362892924
rocauc: 0.7497826464962614
ogbg-molbace: 0.7497826464962614
BCEWithLogitsLoss: 2.556553433338801
Statistics on  train
mean_pred: -1.4393705129623413
std_pred: 5.603540420532227
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9934556668593943
rocauc: 0.9959275114155252
ogbg-molbace: 0.9959275114155252
BCEWithLogitsLoss: 0.0965034241629083
[Epoch 123] ogbg-molbace: 0.752739 test loss: 2.745278
[Epoch 124; Iter    27/   41] train: loss: 0.0087642
[Epoch 124] ogbg-molbace: 0.741758 val loss: 1.896732
[Epoch 124] ogbg-molbace: 0.741089 test loss: 2.780308
[Epoch 125; Iter    16/   41] train: loss: 0.0039664
[Epoch 125] ogbg-molbace: 0.743590 val loss: 1.812450
[Epoch 125] ogbg-molbace: 0.740219 test loss: 2.770805
[Epoch 126; Iter     5/   41] train: loss: 0.0034678
[Epoch 126; Iter    35/   41] train: loss: 0.0074250
[Epoch 126] ogbg-molbace: 0.753114 val loss: 1.946717
[Epoch 126] ogbg-molbace: 0.734481 test loss: 2.690021
[Epoch 127; Iter    24/   41] train: loss: 0.0381599
[Epoch 127] ogbg-molbace: 0.739560 val loss: 2.143742
[Epoch 127] ogbg-molbace: 0.743871 test loss: 2.777088
[Epoch 128; Iter    13/   41] train: loss: 0.0027209
[Epoch 128] ogbg-molbace: 0.734432 val loss: 1.931644
[Epoch 128] ogbg-molbace: 0.732568 test loss: 2.725710
[Epoch 129; Iter     2/   41] train: loss: 0.0054575
[Epoch 129; Iter    32/   41] train: loss: 0.0260390
[Epoch 129] ogbg-molbace: 0.758974 val loss: 1.813243
[Epoch 129] ogbg-molbace: 0.743871 test loss: 2.793526
[Epoch 130; Iter    21/   41] train: loss: 0.0013728
[Epoch 130] ogbg-molbace: 0.739560 val loss: 1.909255
[Epoch 130] ogbg-molbace: 0.727526 test loss: 2.864743
[Epoch 131; Iter    10/   41] train: loss: 0.0032633
[Epoch 131; Iter    40/   41] train: loss: 0.0060081
[Epoch 131] ogbg-molbace: 0.738462 val loss: 1.694984
[Epoch 131] ogbg-molbace: 0.737611 test loss: 2.818419
[Epoch 132; Iter    29/   41] train: loss: 0.0089144
[Epoch 132] ogbg-molbace: 0.746886 val loss: 1.999377
[Epoch 132] ogbg-molbace: 0.755521 test loss: 2.735573
[Epoch 133; Iter    18/   41] train: loss: 0.0047923
[Epoch 133] ogbg-molbace: 0.742491 val loss: 2.203835
[Epoch 133] ogbg-molbace: 0.742306 test loss: 2.885458
[Epoch 134; Iter     7/   41] train: loss: 0.0099150
[Epoch 134; Iter    37/   41] train: loss: 0.0011451
[Epoch 134] ogbg-molbace: 0.731502 val loss: 2.110834
[Epoch 134] ogbg-molbace: 0.733959 test loss: 2.707063
[Epoch 135; Iter    26/   41] train: loss: 0.0029337
[Epoch 135] ogbg-molbace: 0.756044 val loss: 1.923333
[Epoch 135] ogbg-molbace: 0.729438 test loss: 2.758980
[Epoch 136; Iter    15/   41] train: loss: 0.0052028
[Epoch 136] ogbg-molbace: 0.739560 val loss: 1.763219
[Epoch 136] ogbg-molbace: 0.734829 test loss: 2.859595
[Epoch 137; Iter     4/   41] train: loss: 0.0152221
[Epoch 137; Iter    34/   41] train: loss: 0.0022850
[Epoch 137] ogbg-molbace: 0.750549 val loss: 1.950663
[Epoch 137] ogbg-molbace: 0.747174 test loss: 2.780469
[Epoch 138; Iter    23/   41] train: loss: 0.0025228
[Epoch 138] ogbg-molbace: 0.746886 val loss: 2.138529
[Epoch 138] ogbg-molbace: 0.737263 test loss: 2.931249
[Epoch 139; Iter    12/   41] train: loss: 0.0045642
[Epoch 139] ogbg-molbace: 0.750183 val loss: 2.349175
[Epoch 139] ogbg-molbace: 0.726656 test loss: 2.752704
[Epoch 140; Iter     1/   41] train: loss: 0.1236845
[Epoch 140; Iter    31/   41] train: loss: 0.0132467
[Epoch 140] ogbg-molbace: 0.748352 val loss: 1.904520
[Epoch 140] ogbg-molbace: 0.752565 test loss: 2.761554
[Epoch 141; Iter    20/   41] train: loss: 0.0217185
[Epoch 141] ogbg-molbace: 0.730403 val loss: 2.349089
[Epoch 141] ogbg-molbace: 0.745609 test loss: 2.603298
[Epoch 142; Iter     9/   41] train: loss: 0.0107254
[Epoch 142; Iter    39/   41] train: loss: 0.0024778
[Epoch 142] ogbg-molbace: 0.742491 val loss: 2.356722
[Epoch 142] ogbg-molbace: 0.741958 test loss: 2.682031
[Epoch 143; Iter    28/   41] train: loss: 0.2021331
[Epoch 143] ogbg-molbace: 0.747619 val loss: 2.282452
[Epoch 143] ogbg-molbace: 0.772387 test loss: 2.445617
[Epoch 144; Iter    17/   41] train: loss: 0.0116067
[Epoch 144] ogbg-molbace: 0.745788 val loss: 2.008462
[Epoch 144] ogbg-molbace: 0.742827 test loss: 2.688481
[Epoch 145; Iter     6/   41] train: loss: 0.0036539
[Epoch 145; Iter    36/   41] train: loss: 0.0018763
[Epoch 145] ogbg-molbace: 0.756777 val loss: 1.938991
[Epoch 145] ogbg-molbace: 0.745262 test loss: 2.607969
[Epoch 146; Iter    25/   41] train: loss: 0.0038126
[Epoch 146] ogbg-molbace: 0.763370 val loss: 1.863813
[Epoch 146] ogbg-molbace: 0.744045 test loss: 2.685245
[Epoch 147; Iter    14/   41] train: loss: 0.0042358
[Epoch 147] ogbg-molbace: 0.772161 val loss: 1.827684
[Epoch 147] ogbg-molbace: 0.745436 test loss: 2.651509
[Epoch 148; Iter     3/   41] train: loss: 0.0202028
[Epoch 148; Iter    33/   41] train: loss: 0.0027731
[Epoch 148] ogbg-molbace: 0.765934 val loss: 1.850151
[Epoch 148] ogbg-molbace: 0.746479 test loss: 2.830046
[Epoch 149; Iter    22/   41] train: loss: 0.0069532
[Epoch 149] ogbg-molbace: 0.762637 val loss: 1.772921
[Epoch 149] ogbg-molbace: 0.749957 test loss: 3.040573
[Epoch 150; Iter    11/   41] train: loss: 0.0038344
[Epoch 150; Iter    41/   41] train: loss: 0.0107001
[Epoch 150] ogbg-molbace: 0.769597 val loss: 1.805329
[Epoch 150] ogbg-molbace: 0.746131 test loss: 2.800804
[Epoch 151; Iter    30/   41] train: loss: 0.0094456
[Epoch 151] ogbg-molbace: 0.769597 val loss: 1.825710
[Epoch 151] ogbg-molbace: 0.747001 test loss: 2.852463
[Epoch 152; Iter    19/   41] train: loss: 0.0017917
[Epoch 152] ogbg-molbace: 0.765201 val loss: 1.778650
[Epoch 152] ogbg-molbace: 0.742827 test loss: 2.880304
[Epoch 153; Iter     8/   41] train: loss: 0.0018524
[Epoch 153; Iter    38/   41] train: loss: 0.0026291
[Epoch 153] ogbg-molbace: 0.762637 val loss: 1.856989
[Epoch 153] ogbg-molbace: 0.743871 test loss: 2.984333
[Epoch 154; Iter    27/   41] train: loss: 0.0060515
[Epoch 154] ogbg-molbace: 0.769231 val loss: 1.885097
[Epoch 154] ogbg-molbace: 0.743871 test loss: 2.858127
[Epoch 155; Iter    16/   41] train: loss: 0.0114802
[Epoch 155] ogbg-molbace: 0.769231 val loss: 1.809210
[Epoch 155] ogbg-molbace: 0.743349 test loss: 2.832814
[Epoch 156; Iter     5/   41] train: loss: 0.0016126
[Epoch 156; Iter    35/   41] train: loss: 0.0010776
[Epoch 156] ogbg-molbace: 0.763370 val loss: 1.783045
[Epoch 156] ogbg-molbace: 0.743523 test loss: 2.855179
[Epoch 157; Iter    24/   41] train: loss: 0.0014941
[Epoch 157] ogbg-molbace: 0.767399 val loss: 1.815085
[Epoch 157] ogbg-molbace: 0.737437 test loss: 2.936030
[Epoch 158; Iter    13/   41] train: loss: 0.0015901
[Epoch 158] ogbg-molbace: 0.770696 val loss: 1.744556
[Epoch 158] ogbg-molbace: 0.742306 test loss: 2.856384
[Epoch 159; Iter     2/   41] train: loss: 0.0011197
[Epoch 159; Iter    32/   41] train: loss: 0.0047951
[Epoch 159] ogbg-molbace: 0.765201 val loss: 1.633074
[Epoch 159] ogbg-molbace: 0.741784 test loss: 2.972369
[Epoch 160; Iter    21/   41] train: loss: 0.0023343
[Epoch 160] ogbg-molbace: 0.766667 val loss: 1.767947
[Epoch 160] ogbg-molbace: 0.746653 test loss: 2.982078
[Epoch 161; Iter    10/   41] train: loss: 0.0089095
[Epoch 161; Iter    40/   41] train: loss: 0.0058328
[Epoch 161] ogbg-molbace: 0.767033 val loss: 1.797610
[Epoch 161] ogbg-molbace: 0.734655 test loss: 3.144847
[Epoch 162; Iter    29/   41] train: loss: 0.0030501
[Epoch 162] ogbg-molbace: 0.766300 val loss: 1.747688
[Epoch 162] ogbg-molbace: 0.735176 test loss: 2.856787
[Epoch 163; Iter    18/   41] train: loss: 0.0024232
[Epoch 163] ogbg-molbace: 0.761905 val loss: 1.801127
[Epoch 163] ogbg-molbace: 0.737263 test loss: 3.016198
[Epoch 164; Iter     7/   41] train: loss: 0.0049060
[Epoch 164; Iter    37/   41] train: loss: 0.0009442
[Epoch 164] ogbg-molbace: 0.767766 val loss: 1.909881
[Epoch 164] ogbg-molbace: 0.737089 test loss: 3.066891
[Epoch 165; Iter    26/   41] train: loss: 0.0010842
[Epoch 165] ogbg-molbace: 0.758242 val loss: 1.926187
[Epoch 165] ogbg-molbace: 0.731525 test loss: 2.981493
[Epoch 166; Iter    15/   41] train: loss: 0.0010120
[Epoch 166] ogbg-molbace: 0.756410 val loss: 1.880503
[Epoch 166] ogbg-molbace: 0.735350 test loss: 2.861794
[Epoch 167; Iter     4/   41] train: loss: 0.0031951
[Epoch 167; Iter    34/   41] train: loss: 0.0015156
[Epoch 167] ogbg-molbace: 0.763004 val loss: 1.707483
[Epoch 167] ogbg-molbace: 0.732394 test loss: 3.096508
[Epoch 168; Iter    23/   41] train: loss: 0.0102870
[Epoch 168] ogbg-molbace: 0.756410 val loss: 1.836435
[Epoch 168] ogbg-molbace: 0.744914 test loss: 2.972341/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 169; Iter    12/   41] train: loss: 0.0006491
[Epoch 169] ogbg-molbace: 0.756044 val loss: 1.809787
[Epoch 169] ogbg-molbace: 0.744218 test loss: 3.001464
[Epoch 170; Iter     1/   41] train: loss: 0.0058419
[Epoch 170; Iter    31/   41] train: loss: 0.0027005
[Epoch 170] ogbg-molbace: 0.757143 val loss: 1.836539
[Epoch 170] ogbg-molbace: 0.740741 test loss: 2.864932
[Epoch 171; Iter    20/   41] train: loss: 0.0011630
[Epoch 171] ogbg-molbace: 0.762637 val loss: 1.765657
[Epoch 171] ogbg-molbace: 0.745609 test loss: 2.854869
[Epoch 172; Iter     9/   41] train: loss: 0.0076561
[Epoch 172; Iter    39/   41] train: loss: 0.0008332
[Epoch 172] ogbg-molbace: 0.765934 val loss: 1.841231
[Epoch 172] ogbg-molbace: 0.743871 test loss: 2.992015
[Epoch 173; Iter    28/   41] train: loss: 0.0006440
[Epoch 173] ogbg-molbace: 0.769231 val loss: 1.784308
[Epoch 173] ogbg-molbace: 0.740045 test loss: 2.970112
[Epoch 174; Iter    17/   41] train: loss: 0.0006331
[Epoch 174] ogbg-molbace: 0.768864 val loss: 1.735680
[Epoch 174] ogbg-molbace: 0.742306 test loss: 2.898477
Early stopping criterion based on -ogbg-molbace- that should be max reached after 174 epochs. Best model checkpoint was in epoch 114.
Statistics on  val_best_checkpoint
mean_pred: 1.762508511543274
std_pred: 5.247666358947754
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9594586152376523
rocauc: 0.772893772893773
ogbg-molbace: 0.772893772893773
BCEWithLogitsLoss: 1.6489904920260112
Statistics on  test
mean_pred: -2.6899917125701904
std_pred: 6.901976585388184
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7582204229105342
rocauc: 0.75830290384281
ogbg-molbace: 0.75830290384281
BCEWithLogitsLoss: 2.443949987490972
Statistics on  train
mean_pred: -2.020627975463867
std_pred: 7.041752338409424
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9999913554633472
rocauc: 0.999994292237443
ogbg-molbace: 0.999994292237443
BCEWithLogitsLoss: 0.015425818390212953
All runs completed.
