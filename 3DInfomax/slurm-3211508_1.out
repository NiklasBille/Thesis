>>> Starting run for dataset: bbbp
Running configs_static_noise_experiments/3DInfomax/bbbp/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/3DInfomax/bbbp/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/3DInfomax/bbbp/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/3DInfomax/bbbp/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.05/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.05_4_26-05_09-18-55
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.05
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6911833
[Epoch 1] ogbg-molbbbp: 0.592253 val loss: 0.692464
[Epoch 1] ogbg-molbbbp: 0.509838 test loss: 0.693028
[Epoch 2; Iter     5/   55] train: loss: 0.6918508
[Epoch 2; Iter    35/   55] train: loss: 0.6899440
[Epoch 2] ogbg-molbbbp: 0.594842 val loss: 0.691495
[Epoch 2] ogbg-molbbbp: 0.507716 test loss: 0.692925
[Epoch 3; Iter    10/   55] train: loss: 0.6920064
[Epoch 3; Iter    40/   55] train: loss: 0.6947237
[Epoch 3] ogbg-molbbbp: 0.591357 val loss: 0.691652
[Epoch 3] ogbg-molbbbp: 0.511188 test loss: 0.692719
[Epoch 4; Iter    15/   55] train: loss: 0.6921700
[Epoch 4; Iter    45/   55] train: loss: 0.6904904
[Epoch 4] ogbg-molbbbp: 0.574928 val loss: 0.691898
[Epoch 4] ogbg-molbbbp: 0.506848 test loss: 0.692766
[Epoch 5; Iter    20/   55] train: loss: 0.6935945
[Epoch 5; Iter    50/   55] train: loss: 0.6888319
[Epoch 5] ogbg-molbbbp: 0.580603 val loss: 0.692253
[Epoch 5] ogbg-molbbbp: 0.514371 test loss: 0.692418
[Epoch 6; Iter    25/   55] train: loss: 0.6900520
[Epoch 6; Iter    55/   55] train: loss: 0.6933067
[Epoch 6] ogbg-molbbbp: 0.621727 val loss: 0.692008
[Epoch 6] ogbg-molbbbp: 0.515625 test loss: 0.692279
[Epoch 7; Iter    30/   55] train: loss: 0.6871365
[Epoch 7] ogbg-molbbbp: 0.609778 val loss: 0.692279
[Epoch 7] ogbg-molbbbp: 0.509549 test loss: 0.692250
[Epoch 8; Iter     5/   55] train: loss: 0.6868562
[Epoch 8; Iter    35/   55] train: loss: 0.6868910
[Epoch 8] ogbg-molbbbp: 0.639251 val loss: 0.692437
[Epoch 8] ogbg-molbbbp: 0.508391 test loss: 0.692037
[Epoch 9; Iter    10/   55] train: loss: 0.6829969
[Epoch 9; Iter    40/   55] train: loss: 0.6807495
[Epoch 9] ogbg-molbbbp: 0.661655 val loss: 0.692556
[Epoch 9] ogbg-molbbbp: 0.509934 test loss: 0.691783
[Epoch 10; Iter    15/   55] train: loss: 0.6845875
[Epoch 10; Iter    45/   55] train: loss: 0.6752738
[Epoch 10] ogbg-molbbbp: 0.667629 val loss: 0.692913
[Epoch 10] ogbg-molbbbp: 0.515143 test loss: 0.691494
[Epoch 11; Iter    20/   55] train: loss: 0.6829304
[Epoch 11; Iter    50/   55] train: loss: 0.6755562
[Epoch 11] ogbg-molbbbp: 0.690630 val loss: 0.693015
[Epoch 11] ogbg-molbbbp: 0.509163 test loss: 0.691296
[Epoch 12; Iter    25/   55] train: loss: 0.6756596
[Epoch 12; Iter    55/   55] train: loss: 0.6746927
[Epoch 12] ogbg-molbbbp: 0.696704 val loss: 0.693880
[Epoch 12] ogbg-molbbbp: 0.510706 test loss: 0.691050
[Epoch 13; Iter    30/   55] train: loss: 0.6700447
[Epoch 13] ogbg-molbbbp: 0.865976 val loss: 0.646286
[Epoch 13] ogbg-molbbbp: 0.550733 test loss: 0.686437
[Epoch 14; Iter     5/   55] train: loss: 0.6595819
[Epoch 14; Iter    35/   55] train: loss: 0.5877109
[Epoch 14] ogbg-molbbbp: 0.902519 val loss: 0.483995
[Epoch 14] ogbg-molbbbp: 0.590567 test loss: 0.691820
[Epoch 15; Iter    10/   55] train: loss: 0.5587542
[Epoch 15; Iter    40/   55] train: loss: 0.6012713
[Epoch 15] ogbg-molbbbp: 0.924226 val loss: 0.418210
[Epoch 15] ogbg-molbbbp: 0.596933 test loss: 0.730975
[Epoch 16; Iter    15/   55] train: loss: 0.5463566
[Epoch 16; Iter    45/   55] train: loss: 0.3676926
[Epoch 16] ogbg-molbbbp: 0.910186 val loss: 0.465209
[Epoch 16] ogbg-molbbbp: 0.613426 test loss: 0.790103
[Epoch 17; Iter    20/   55] train: loss: 0.3462798
[Epoch 17; Iter    50/   55] train: loss: 0.5136707
[Epoch 17] ogbg-molbbbp: 0.927213 val loss: 0.392082
[Epoch 17] ogbg-molbbbp: 0.606096 test loss: 0.825987
[Epoch 18; Iter    25/   55] train: loss: 0.3483869
[Epoch 18; Iter    55/   55] train: loss: 0.4041316
[Epoch 18] ogbg-molbbbp: 0.923529 val loss: 0.414980
[Epoch 18] ogbg-molbbbp: 0.598283 test loss: 0.987124
[Epoch 19; Iter    30/   55] train: loss: 0.3415971
[Epoch 19] ogbg-molbbbp: 0.929304 val loss: 0.365014
[Epoch 19] ogbg-molbbbp: 0.599537 test loss: 1.098644
[Epoch 20; Iter     5/   55] train: loss: 0.3101082
[Epoch 20; Iter    35/   55] train: loss: 0.1878862
[Epoch 20] ogbg-molbbbp: 0.906303 val loss: 0.438158
[Epoch 20] ogbg-molbbbp: 0.592110 test loss: 0.972330
[Epoch 21; Iter    10/   55] train: loss: 0.3384713
[Epoch 21; Iter    40/   55] train: loss: 0.3751236
[Epoch 21] ogbg-molbbbp: 0.929005 val loss: 0.391293
[Epoch 21] ogbg-molbbbp: 0.622010 test loss: 0.987034
[Epoch 22; Iter    15/   55] train: loss: 0.4096291
[Epoch 22; Iter    45/   55] train: loss: 0.4065901
[Epoch 22] ogbg-molbbbp: 0.933785 val loss: 0.436978
[Epoch 22] ogbg-molbbbp: 0.616898 test loss: 1.001152
[Epoch 23; Iter    20/   55] train: loss: 0.4432585
[Epoch 23; Iter    50/   55] train: loss: 0.1830874
[Epoch 23] ogbg-molbbbp: 0.922732 val loss: 0.419157
[Epoch 23] ogbg-molbbbp: 0.610147 test loss: 1.047175
[Epoch 24; Iter    25/   55] train: loss: 0.2448823
[Epoch 24; Iter    55/   55] train: loss: 0.8811864
[Epoch 24] ogbg-molbbbp: 0.906801 val loss: 0.485103
[Epoch 24] ogbg-molbbbp: 0.608989 test loss: 1.064984
[Epoch 25; Iter    30/   55] train: loss: 0.1040429
[Epoch 25] ogbg-molbbbp: 0.921537 val loss: 0.397544
[Epoch 25] ogbg-molbbbp: 0.577546 test loss: 1.011429
[Epoch 26; Iter     5/   55] train: loss: 0.3862345
[Epoch 26; Iter    35/   55] train: loss: 0.2175345
[Epoch 26] ogbg-molbbbp: 0.906502 val loss: 0.528860
[Epoch 26] ogbg-molbbbp: 0.636863 test loss: 1.314993
[Epoch 27; Iter    10/   55] train: loss: 0.1686856
[Epoch 27; Iter    40/   55] train: loss: 0.2079120
[Epoch 27] ogbg-molbbbp: 0.916858 val loss: 0.383720
[Epoch 27] ogbg-molbbbp: 0.616223 test loss: 2.070971
[Epoch 28; Iter    15/   55] train: loss: 0.1835053
[Epoch 28; Iter    45/   55] train: loss: 0.2506375
[Epoch 28] ogbg-molbbbp: 0.946629 val loss: 0.358301
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.1/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.1_6_26-05_09-19-04
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.1
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6911128
[Epoch 1] ogbg-molbbbp: 0.528627 val loss: 0.691895
[Epoch 1] ogbg-molbbbp: 0.573688 test loss: 0.692569
[Epoch 2; Iter     5/   55] train: loss: 0.6935635
[Epoch 2; Iter    35/   55] train: loss: 0.6923097
[Epoch 2] ogbg-molbbbp: 0.422981 val loss: 0.691960
[Epoch 2] ogbg-molbbbp: 0.519001 test loss: 0.693583
[Epoch 3; Iter    10/   55] train: loss: 0.6942030
[Epoch 3; Iter    40/   55] train: loss: 0.6914742
[Epoch 3] ogbg-molbbbp: 0.415314 val loss: 0.692379
[Epoch 3] ogbg-molbbbp: 0.512539 test loss: 0.693778
[Epoch 4; Iter    15/   55] train: loss: 0.6896949
[Epoch 4; Iter    45/   55] train: loss: 0.6907088
[Epoch 4] ogbg-molbbbp: 0.403067 val loss: 0.693607
[Epoch 4] ogbg-molbbbp: 0.510513 test loss: 0.693745
[Epoch 5; Iter    20/   55] train: loss: 0.6901097
[Epoch 5; Iter    50/   55] train: loss: 0.6881664
[Epoch 5] ogbg-molbbbp: 0.406154 val loss: 0.693649
[Epoch 5] ogbg-molbbbp: 0.511863 test loss: 0.693475
[Epoch 6; Iter    25/   55] train: loss: 0.6898862
[Epoch 6; Iter    55/   55] train: loss: 0.6893981
[Epoch 6] ogbg-molbbbp: 0.418799 val loss: 0.693054
[Epoch 6] ogbg-molbbbp: 0.516397 test loss: 0.693363
[Epoch 7; Iter    30/   55] train: loss: 0.6856950
[Epoch 7] ogbg-molbbbp: 0.414916 val loss: 0.693709
[Epoch 7] ogbg-molbbbp: 0.511960 test loss: 0.693180
[Epoch 8; Iter     5/   55] train: loss: 0.6864493
[Epoch 8; Iter    35/   55] train: loss: 0.6855097
[Epoch 8] ogbg-molbbbp: 0.438614 val loss: 0.693315
[Epoch 8] ogbg-molbbbp: 0.521991 test loss: 0.692652
[Epoch 9; Iter    10/   55] train: loss: 0.6826003
[Epoch 9; Iter    40/   55] train: loss: 0.6840840
[Epoch 9] ogbg-molbbbp: 0.427561 val loss: 0.694148
[Epoch 9] ogbg-molbbbp: 0.516300 test loss: 0.692617
[Epoch 10; Iter    15/   55] train: loss: 0.6786317
[Epoch 10; Iter    45/   55] train: loss: 0.6798540
[Epoch 10] ogbg-molbbbp: 0.439112 val loss: 0.694290
[Epoch 10] ogbg-molbbbp: 0.518615 test loss: 0.692384
[Epoch 11; Iter    20/   55] train: loss: 0.6757480
[Epoch 11; Iter    50/   55] train: loss: 0.6733231
[Epoch 11] ogbg-molbbbp: 0.444389 val loss: 0.694390
[Epoch 11] ogbg-molbbbp: 0.521701 test loss: 0.692059
[Epoch 12; Iter    25/   55] train: loss: 0.6714770
[Epoch 12; Iter    55/   55] train: loss: 0.6965019
[Epoch 12] ogbg-molbbbp: 0.442796 val loss: 0.695632
[Epoch 12] ogbg-molbbbp: 0.521412 test loss: 0.691806
[Epoch 13; Iter    30/   55] train: loss: 0.6716706
[Epoch 13] ogbg-molbbbp: 0.867968 val loss: 0.654567
[Epoch 13] ogbg-molbbbp: 0.555748 test loss: 0.686216
[Epoch 14; Iter     5/   55] train: loss: 0.6447797
[Epoch 14; Iter    35/   55] train: loss: 0.6173063
[Epoch 14] ogbg-molbbbp: 0.904212 val loss: 0.538539
[Epoch 14] ogbg-molbbbp: 0.623553 test loss: 0.671796
[Epoch 15; Iter    10/   55] train: loss: 0.5906646
[Epoch 15; Iter    40/   55] train: loss: 0.5443928
[Epoch 15] ogbg-molbbbp: 0.833118 val loss: 0.570758
[Epoch 15] ogbg-molbbbp: 0.560282 test loss: 0.730120
[Epoch 16; Iter    15/   55] train: loss: 0.5230261
[Epoch 16; Iter    45/   55] train: loss: 0.4560748
[Epoch 16] ogbg-molbbbp: 0.918849 val loss: 0.479715
[Epoch 16] ogbg-molbbbp: 0.582176 test loss: 0.741738
[Epoch 17; Iter    20/   55] train: loss: 0.5093611
[Epoch 17; Iter    50/   55] train: loss: 0.3920638
[Epoch 17] ogbg-molbbbp: 0.920442 val loss: 0.419207
[Epoch 17] ogbg-molbbbp: 0.611111 test loss: 0.745777
[Epoch 18; Iter    25/   55] train: loss: 0.2965304
[Epoch 18; Iter    55/   55] train: loss: 0.3913188
[Epoch 18] ogbg-molbbbp: 0.915464 val loss: 0.515033
[Epoch 18] ogbg-molbbbp: 0.566262 test loss: 0.885342
[Epoch 19; Iter    30/   55] train: loss: 0.3147735
[Epoch 19] ogbg-molbbbp: 0.909887 val loss: 0.561865
[Epoch 19] ogbg-molbbbp: 0.584394 test loss: 0.990454
[Epoch 20; Iter     5/   55] train: loss: 0.3492917
[Epoch 20; Iter    35/   55] train: loss: 0.5608295
[Epoch 20] ogbg-molbbbp: 0.916161 val loss: 0.490806
[Epoch 20] ogbg-molbbbp: 0.596644 test loss: 0.891201
[Epoch 21; Iter    10/   55] train: loss: 0.3983331
[Epoch 21; Iter    40/   55] train: loss: 0.3158292
[Epoch 21] ogbg-molbbbp: 0.912675 val loss: 0.492753
[Epoch 21] ogbg-molbbbp: 0.597029 test loss: 1.032192
[Epoch 22; Iter    15/   55] train: loss: 0.3447008
[Epoch 22; Iter    45/   55] train: loss: 0.3427740
[Epoch 22] ogbg-molbbbp: 0.917057 val loss: 0.489751
[Epoch 22] ogbg-molbbbp: 0.590567 test loss: 0.956505
[Epoch 23; Iter    20/   55] train: loss: 0.4615994
[Epoch 23; Iter    50/   55] train: loss: 0.3452884
[Epoch 23] ogbg-molbbbp: 0.896545 val loss: 0.500117
[Epoch 23] ogbg-molbbbp: 0.579282 test loss: 1.010570
[Epoch 24; Iter    25/   55] train: loss: 0.2759682
[Epoch 24; Iter    55/   55] train: loss: 0.1101572
[Epoch 24] ogbg-molbbbp: 0.901523 val loss: 0.539117
[Epoch 24] ogbg-molbbbp: 0.588349 test loss: 1.131288
[Epoch 25; Iter    30/   55] train: loss: 0.2201798
[Epoch 25] ogbg-molbbbp: 0.909290 val loss: 0.500751
[Epoch 25] ogbg-molbbbp: 0.592593 test loss: 1.046398
[Epoch 26; Iter     5/   55] train: loss: 0.5079703
[Epoch 26; Iter    35/   55] train: loss: 0.2614577
[Epoch 26] ogbg-molbbbp: 0.938763 val loss: 0.392541
[Epoch 26] ogbg-molbbbp: 0.578800 test loss: 1.417900
[Epoch 27; Iter    10/   55] train: loss: 0.2058326
[Epoch 27; Iter    40/   55] train: loss: 0.2899298
[Epoch 27] ogbg-molbbbp: 0.895947 val loss: 0.891548
[Epoch 27] ogbg-molbbbp: 0.626640 test loss: 2.531384
[Epoch 28; Iter    15/   55] train: loss: 0.2060414
[Epoch 28; Iter    45/   55] train: loss: 0.2651512
[Epoch 28] ogbg-molbbbp: 0.904909 val loss: 0.428303
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.05/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.05_6_26-05_09-18-59
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.05
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6928933
[Epoch 1] ogbg-molbbbp: 0.615752 val loss: 0.692074
[Epoch 1] ogbg-molbbbp: 0.566840 test loss: 0.692551
[Epoch 2; Iter     5/   55] train: loss: 0.6934717
[Epoch 2; Iter    35/   55] train: loss: 0.6942495
[Epoch 2] ogbg-molbbbp: 0.471174 val loss: 0.692988
[Epoch 2] ogbg-molbbbp: 0.497878 test loss: 0.693585
[Epoch 3; Iter    10/   55] train: loss: 0.6928780
[Epoch 3; Iter    40/   55] train: loss: 0.6903037
[Epoch 3] ogbg-molbbbp: 0.454745 val loss: 0.693429
[Epoch 3] ogbg-molbbbp: 0.494599 test loss: 0.693692
[Epoch 4; Iter    15/   55] train: loss: 0.6889881
[Epoch 4; Iter    45/   55] train: loss: 0.6920808
[Epoch 4] ogbg-molbbbp: 0.466892 val loss: 0.693549
[Epoch 4] ogbg-molbbbp: 0.501736 test loss: 0.693401
[Epoch 5; Iter    20/   55] train: loss: 0.6900848
[Epoch 5; Iter    50/   55] train: loss: 0.6887857
[Epoch 5] ogbg-molbbbp: 0.470975 val loss: 0.693642
[Epoch 5] ogbg-molbbbp: 0.492477 test loss: 0.693537
[Epoch 6; Iter    25/   55] train: loss: 0.6895037
[Epoch 6; Iter    55/   55] train: loss: 0.6870921
[Epoch 6] ogbg-molbbbp: 0.493179 val loss: 0.693387
[Epoch 6] ogbg-molbbbp: 0.496721 test loss: 0.693332
[Epoch 7; Iter    30/   55] train: loss: 0.6848446
[Epoch 7] ogbg-molbbbp: 0.469880 val loss: 0.694529
[Epoch 7] ogbg-molbbbp: 0.490258 test loss: 0.693147
[Epoch 8; Iter     5/   55] train: loss: 0.6866351
[Epoch 8; Iter    35/   55] train: loss: 0.6834984
[Epoch 8] ogbg-molbbbp: 0.483222 val loss: 0.694592
[Epoch 8] ogbg-molbbbp: 0.497878 test loss: 0.692795
[Epoch 9; Iter    10/   55] train: loss: 0.6819055
[Epoch 9; Iter    40/   55] train: loss: 0.6822959
[Epoch 9] ogbg-molbbbp: 0.503336 val loss: 0.694691
[Epoch 9] ogbg-molbbbp: 0.495177 test loss: 0.692684
[Epoch 10; Iter    15/   55] train: loss: 0.6798820
[Epoch 10; Iter    45/   55] train: loss: 0.6825765
[Epoch 10] ogbg-molbbbp: 0.513691 val loss: 0.694731
[Epoch 10] ogbg-molbbbp: 0.489583 test loss: 0.692592
[Epoch 11; Iter    20/   55] train: loss: 0.6768912
[Epoch 11; Iter    50/   55] train: loss: 0.6728678
[Epoch 11] ogbg-molbbbp: 0.523350 val loss: 0.695086
[Epoch 11] ogbg-molbbbp: 0.495081 test loss: 0.692258
[Epoch 12; Iter    25/   55] train: loss: 0.6711906
[Epoch 12; Iter    55/   55] train: loss: 0.6876400
[Epoch 12] ogbg-molbbbp: 0.533008 val loss: 0.695879
[Epoch 12] ogbg-molbbbp: 0.498746 test loss: 0.692060
[Epoch 13; Iter    30/   55] train: loss: 0.6730196
[Epoch 13] ogbg-molbbbp: 0.881510 val loss: 0.639602
[Epoch 13] ogbg-molbbbp: 0.562018 test loss: 0.684618
[Epoch 14; Iter     5/   55] train: loss: 0.6482673
[Epoch 14; Iter    35/   55] train: loss: 0.6065590
[Epoch 14] ogbg-molbbbp: 0.913572 val loss: 0.501329
[Epoch 14] ogbg-molbbbp: 0.617863 test loss: 0.687250
[Epoch 15; Iter    10/   55] train: loss: 0.5766339
[Epoch 15; Iter    40/   55] train: loss: 0.5085000
[Epoch 15] ogbg-molbbbp: 0.892263 val loss: 0.494933
[Epoch 15] ogbg-molbbbp: 0.575328 test loss: 0.733521
[Epoch 16; Iter    15/   55] train: loss: 0.4925416
[Epoch 16; Iter    45/   55] train: loss: 0.4781552
[Epoch 16] ogbg-molbbbp: 0.925222 val loss: 0.423114
[Epoch 16] ogbg-molbbbp: 0.590181 test loss: 0.778172
[Epoch 17; Iter    20/   55] train: loss: 0.4431645
[Epoch 17; Iter    50/   55] train: loss: 0.4103166
[Epoch 17] ogbg-molbbbp: 0.929404 val loss: 0.386822
[Epoch 17] ogbg-molbbbp: 0.587481 test loss: 0.832647
[Epoch 18; Iter    25/   55] train: loss: 0.2565702
[Epoch 18; Iter    55/   55] train: loss: 0.4414945
[Epoch 18] ogbg-molbbbp: 0.927910 val loss: 0.438860
[Epoch 18] ogbg-molbbbp: 0.568962 test loss: 0.956735
[Epoch 19; Iter    30/   55] train: loss: 0.2879579
[Epoch 19] ogbg-molbbbp: 0.907398 val loss: 0.453926
[Epoch 19] ogbg-molbbbp: 0.570698 test loss: 1.132054
[Epoch 20; Iter     5/   55] train: loss: 0.3492567
[Epoch 20; Iter    35/   55] train: loss: 0.5311986
[Epoch 20] ogbg-molbbbp: 0.921637 val loss: 0.426839
[Epoch 20] ogbg-molbbbp: 0.580150 test loss: 1.152477
[Epoch 21; Iter    10/   55] train: loss: 0.3670389
[Epoch 21; Iter    40/   55] train: loss: 0.2823166
[Epoch 21] ogbg-molbbbp: 0.916459 val loss: 0.516418
[Epoch 21] ogbg-molbbbp: 0.590471 test loss: 1.227780
[Epoch 22; Iter    15/   55] train: loss: 0.3130204
[Epoch 22; Iter    45/   55] train: loss: 0.3320344
[Epoch 22] ogbg-molbbbp: 0.909390 val loss: 0.533991
[Epoch 22] ogbg-molbbbp: 0.607060 test loss: 1.054448
[Epoch 23; Iter    20/   55] train: loss: 0.3810665
[Epoch 23; Iter    50/   55] train: loss: 0.3397280
[Epoch 23] ogbg-molbbbp: 0.887583 val loss: 0.546582
[Epoch 23] ogbg-molbbbp: 0.597222 test loss: 0.977170
[Epoch 24; Iter    25/   55] train: loss: 0.2494398
[Epoch 24; Iter    55/   55] train: loss: 0.1425277
[Epoch 24] ogbg-molbbbp: 0.912974 val loss: 0.440435
[Epoch 24] ogbg-molbbbp: 0.596354 test loss: 1.056627
[Epoch 25; Iter    30/   55] train: loss: 0.3429186
[Epoch 25] ogbg-molbbbp: 0.894454 val loss: 0.695644
[Epoch 25] ogbg-molbbbp: 0.568866 test loss: 1.200291
[Epoch 26; Iter     5/   55] train: loss: 0.3988048
[Epoch 26; Iter    35/   55] train: loss: 0.2312090
[Epoch 26] ogbg-molbbbp: 0.920741 val loss: 0.405850
[Epoch 26] ogbg-molbbbp: 0.581983 test loss: 1.310470
[Epoch 27; Iter    10/   55] train: loss: 0.2343286
[Epoch 27; Iter    40/   55] train: loss: 0.2395954
[Epoch 27] ogbg-molbbbp: 0.932590 val loss: 0.380724
[Epoch 27] ogbg-molbbbp: 0.626543 test loss: 2.525762
[Epoch 28; Iter    15/   55] train: loss: 0.1748898
[Epoch 28; Iter    45/   55] train: loss: 0.2336647
[Epoch 28] ogbg-molbbbp: 0.921438 val loss: 0.406439
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.2/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.2_5_26-05_09-19-04
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.2
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6937250
[Epoch 1] ogbg-molbbbp: 0.598327 val loss: 0.694920
[Epoch 1] ogbg-molbbbp: 0.462867 test loss: 0.693226
[Epoch 2; Iter     5/   55] train: loss: 0.6926020
[Epoch 2; Iter    35/   55] train: loss: 0.6880221
[Epoch 2] ogbg-molbbbp: 0.524544 val loss: 0.703452
[Epoch 2] ogbg-molbbbp: 0.440586 test loss: 0.695188
[Epoch 3; Iter    10/   55] train: loss: 0.6912850
[Epoch 3; Iter    40/   55] train: loss: 0.6916616
[Epoch 3] ogbg-molbbbp: 0.517276 val loss: 0.705269
[Epoch 3] ogbg-molbbbp: 0.442998 test loss: 0.695417
[Epoch 4; Iter    15/   55] train: loss: 0.6904262
[Epoch 4; Iter    45/   55] train: loss: 0.6902776
[Epoch 4] ogbg-molbbbp: 0.531514 val loss: 0.703604
[Epoch 4] ogbg-molbbbp: 0.446952 test loss: 0.694844
[Epoch 5; Iter    20/   55] train: loss: 0.6908653
[Epoch 5; Iter    50/   55] train: loss: 0.6955771
[Epoch 5] ogbg-molbbbp: 0.525441 val loss: 0.704579
[Epoch 5] ogbg-molbbbp: 0.446084 test loss: 0.694924
[Epoch 6; Iter    25/   55] train: loss: 0.6851388
[Epoch 6; Iter    55/   55] train: loss: 0.6868735
[Epoch 6] ogbg-molbbbp: 0.534601 val loss: 0.704492
[Epoch 6] ogbg-molbbbp: 0.442612 test loss: 0.695096
[Epoch 7; Iter    30/   55] train: loss: 0.6838010
[Epoch 7] ogbg-molbbbp: 0.539978 val loss: 0.703776
[Epoch 7] ogbg-molbbbp: 0.444059 test loss: 0.694759
[Epoch 8; Iter     5/   55] train: loss: 0.6868376
[Epoch 8; Iter    35/   55] train: loss: 0.6814770
[Epoch 8] ogbg-molbbbp: 0.547844 val loss: 0.703993
[Epoch 8] ogbg-molbbbp: 0.451389 test loss: 0.694269
[Epoch 9; Iter    10/   55] train: loss: 0.6868836
[Epoch 9; Iter    40/   55] train: loss: 0.6864193
[Epoch 9] ogbg-molbbbp: 0.558100 val loss: 0.703809
[Epoch 9] ogbg-molbbbp: 0.452643 test loss: 0.694239
[Epoch 10; Iter    15/   55] train: loss: 0.6818266
[Epoch 10; Iter    45/   55] train: loss: 0.6842713
[Epoch 10] ogbg-molbbbp: 0.542866 val loss: 0.706605
[Epoch 10] ogbg-molbbbp: 0.448592 test loss: 0.694761
[Epoch 11; Iter    20/   55] train: loss: 0.6797545
[Epoch 11; Iter    50/   55] train: loss: 0.6766077
[Epoch 11] ogbg-molbbbp: 0.557702 val loss: 0.706424
[Epoch 11] ogbg-molbbbp: 0.458623 test loss: 0.694464
[Epoch 12; Iter    25/   55] train: loss: 0.6778034
[Epoch 12; Iter    55/   55] train: loss: 0.6743333
[Epoch 12] ogbg-molbbbp: 0.546450 val loss: 0.707388
[Epoch 12] ogbg-molbbbp: 0.453221 test loss: 0.694419
[Epoch 13; Iter    30/   55] train: loss: 0.6820198
[Epoch 13] ogbg-molbbbp: 0.612865 val loss: 0.729837
[Epoch 13] ogbg-molbbbp: 0.492477 test loss: 0.713066
[Epoch 14; Iter     5/   55] train: loss: 0.6628224
[Epoch 14; Iter    35/   55] train: loss: 0.6320636
[Epoch 14] ogbg-molbbbp: 0.788709 val loss: 0.689548
[Epoch 14] ogbg-molbbbp: 0.541860 test loss: 0.794125
[Epoch 15; Iter    10/   55] train: loss: 0.5946268
[Epoch 15; Iter    40/   55] train: loss: 0.5847121
[Epoch 15] ogbg-molbbbp: 0.853231 val loss: 0.619313
[Epoch 15] ogbg-molbbbp: 0.556809 test loss: 0.884056
[Epoch 16; Iter    15/   55] train: loss: 0.4428573
[Epoch 16; Iter    45/   55] train: loss: 0.5716763
[Epoch 16] ogbg-molbbbp: 0.844568 val loss: 0.609845
[Epoch 16] ogbg-molbbbp: 0.543017 test loss: 0.981618
[Epoch 17; Iter    20/   55] train: loss: 0.4624171
[Epoch 17; Iter    50/   55] train: loss: 0.5036185
[Epoch 17] ogbg-molbbbp: 0.738923 val loss: 1.884763
[Epoch 17] ogbg-molbbbp: 0.515239 test loss: 2.630895
[Epoch 18; Iter    25/   55] train: loss: 0.3625099
[Epoch 18; Iter    55/   55] train: loss: 0.3955785
[Epoch 18] ogbg-molbbbp: 0.870955 val loss: 0.733472
[Epoch 18] ogbg-molbbbp: 0.557967 test loss: 1.460580
[Epoch 19; Iter    30/   55] train: loss: 0.4267828
[Epoch 19] ogbg-molbbbp: 0.889674 val loss: 0.597142
[Epoch 19] ogbg-molbbbp: 0.583526 test loss: 2.091019
[Epoch 20; Iter     5/   55] train: loss: 0.3640945
[Epoch 20; Iter    35/   55] train: loss: 0.2544332
[Epoch 20] ogbg-molbbbp: 0.850742 val loss: 0.652078
[Epoch 20] ogbg-molbbbp: 0.574460 test loss: 1.690915
[Epoch 21; Iter    10/   55] train: loss: 0.4188940
[Epoch 21; Iter    40/   55] train: loss: 0.6076055
[Epoch 21] ogbg-molbbbp: 0.853132 val loss: 1.074669
[Epoch 21] ogbg-molbbbp: 0.556520 test loss: 2.299888
[Epoch 22; Iter    15/   55] train: loss: 0.3964592
[Epoch 22; Iter    45/   55] train: loss: 0.3894570
[Epoch 22] ogbg-molbbbp: 0.856318 val loss: 0.854161
[Epoch 22] ogbg-molbbbp: 0.565683 test loss: 2.362176
[Epoch 23; Iter    20/   55] train: loss: 0.3621351
[Epoch 23; Iter    50/   55] train: loss: 0.3407829
[Epoch 23] ogbg-molbbbp: 0.876630 val loss: 0.859148
[Epoch 23] ogbg-molbbbp: 0.554977 test loss: 2.900072
[Epoch 24; Iter    25/   55] train: loss: 0.5741713
[Epoch 24; Iter    55/   55] train: loss: 0.1822391
[Epoch 24] ogbg-molbbbp: 0.832022 val loss: 0.977624
[Epoch 24] ogbg-molbbbp: 0.550829 test loss: 2.319170
[Epoch 25; Iter    30/   55] train: loss: 0.3732740
[Epoch 25] ogbg-molbbbp: 0.815593 val loss: 1.791998
[Epoch 25] ogbg-molbbbp: 0.568094 test loss: 4.443778
[Epoch 26; Iter     5/   55] train: loss: 0.4228891
[Epoch 26; Iter    35/   55] train: loss: 0.3265617
[Epoch 26] ogbg-molbbbp: 0.879319 val loss: 1.145584
[Epoch 26] ogbg-molbbbp: 0.565104 test loss: 3.119297
[Epoch 27; Iter    10/   55] train: loss: 0.2674141
[Epoch 27; Iter    40/   55] train: loss: 0.2131128
[Epoch 27] ogbg-molbbbp: 0.860102 val loss: 2.044965
[Epoch 27] ogbg-molbbbp: 0.582272 test loss: 3.717601
[Epoch 28; Iter    15/   55] train: loss: 0.1890987
[Epoch 28; Iter    45/   55] train: loss: 0.1641367
[Epoch 28] ogbg-molbbbp: 0.875834 val loss: 1.418870
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.05/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.05_5_26-05_09-18-58
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.05
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6953789
[Epoch 1] ogbg-molbbbp: 0.841581 val loss: 0.689019
[Epoch 1] ogbg-molbbbp: 0.523245 test loss: 0.692585
[Epoch 2; Iter     5/   55] train: loss: 0.6940349
[Epoch 2; Iter    35/   55] train: loss: 0.6913426
[Epoch 2] ogbg-molbbbp: 0.785721 val loss: 0.686588
[Epoch 2] ogbg-molbbbp: 0.500772 test loss: 0.692529
[Epoch 3; Iter    10/   55] train: loss: 0.6919100
[Epoch 3; Iter    40/   55] train: loss: 0.6904301
[Epoch 3] ogbg-molbbbp: 0.769093 val loss: 0.687582
[Epoch 3] ogbg-molbbbp: 0.496046 test loss: 0.692652
[Epoch 4; Iter    15/   55] train: loss: 0.6901851
[Epoch 4; Iter    45/   55] train: loss: 0.6914852
[Epoch 4] ogbg-molbbbp: 0.771283 val loss: 0.687577
[Epoch 4] ogbg-molbbbp: 0.493056 test loss: 0.692475
[Epoch 5; Iter    20/   55] train: loss: 0.6903113
[Epoch 5; Iter    50/   55] train: loss: 0.6950211
[Epoch 5] ogbg-molbbbp: 0.780046 val loss: 0.687647
[Epoch 5] ogbg-molbbbp: 0.498650 test loss: 0.692269
[Epoch 6; Iter    25/   55] train: loss: 0.6875291
[Epoch 6; Iter    55/   55] train: loss: 0.6868259
[Epoch 6] ogbg-molbbbp: 0.786319 val loss: 0.686887
[Epoch 6] ogbg-molbbbp: 0.504051 test loss: 0.692065
[Epoch 7; Iter    30/   55] train: loss: 0.6859233
[Epoch 7] ogbg-molbbbp: 0.796176 val loss: 0.687015
[Epoch 7] ogbg-molbbbp: 0.509838 test loss: 0.691948
[Epoch 8; Iter     5/   55] train: loss: 0.6861558
[Epoch 8; Iter    35/   55] train: loss: 0.6820515
[Epoch 8] ogbg-molbbbp: 0.797969 val loss: 0.687148
[Epoch 8] ogbg-molbbbp: 0.508681 test loss: 0.691713
[Epoch 9; Iter    10/   55] train: loss: 0.6899964
[Epoch 9; Iter    40/   55] train: loss: 0.6862192
[Epoch 9] ogbg-molbbbp: 0.798068 val loss: 0.687587
[Epoch 9] ogbg-molbbbp: 0.508970 test loss: 0.691632
[Epoch 10; Iter    15/   55] train: loss: 0.6811126
[Epoch 10; Iter    45/   55] train: loss: 0.6817620
[Epoch 10] ogbg-molbbbp: 0.801055 val loss: 0.687891
[Epoch 10] ogbg-molbbbp: 0.511381 test loss: 0.691363
[Epoch 11; Iter    20/   55] train: loss: 0.6813964
[Epoch 11; Iter    50/   55] train: loss: 0.6749609
[Epoch 11] ogbg-molbbbp: 0.800856 val loss: 0.688208
[Epoch 11] ogbg-molbbbp: 0.515239 test loss: 0.691256
[Epoch 12; Iter    25/   55] train: loss: 0.6766322
[Epoch 12; Iter    55/   55] train: loss: 0.6722966
[Epoch 12] ogbg-molbbbp: 0.819675 val loss: 0.687614
[Epoch 12] ogbg-molbbbp: 0.522377 test loss: 0.690810
[Epoch 13; Iter    30/   55] train: loss: 0.6790785
[Epoch 13] ogbg-molbbbp: 0.855621 val loss: 0.649811
[Epoch 13] ogbg-molbbbp: 0.558931 test loss: 0.686825
[Epoch 14; Iter     5/   55] train: loss: 0.6602935
[Epoch 14; Iter    35/   55] train: loss: 0.6157661
[Epoch 14] ogbg-molbbbp: 0.900826 val loss: 0.499501
[Epoch 14] ogbg-molbbbp: 0.618345 test loss: 0.676090
[Epoch 15; Iter    10/   55] train: loss: 0.5618406
[Epoch 15; Iter    40/   55] train: loss: 0.5615972
[Epoch 15] ogbg-molbbbp: 0.922434 val loss: 0.430086
[Epoch 15] ogbg-molbbbp: 0.613426 test loss: 0.707194
[Epoch 16; Iter    15/   55] train: loss: 0.4263491
[Epoch 16; Iter    45/   55] train: loss: 0.4960054
[Epoch 16] ogbg-molbbbp: 0.879618 val loss: 0.477483
[Epoch 16] ogbg-molbbbp: 0.584105 test loss: 0.734671
[Epoch 17; Iter    20/   55] train: loss: 0.4047865
[Epoch 17; Iter    50/   55] train: loss: 0.4412578
[Epoch 17] ogbg-molbbbp: 0.932689 val loss: 0.388837
[Epoch 17] ogbg-molbbbp: 0.613329 test loss: 0.787827
[Epoch 18; Iter    25/   55] train: loss: 0.2935216
[Epoch 18; Iter    55/   55] train: loss: 0.4420608
[Epoch 18] ogbg-molbbbp: 0.924126 val loss: 0.413845
[Epoch 18] ogbg-molbbbp: 0.595004 test loss: 0.889978
[Epoch 19; Iter    30/   55] train: loss: 0.3763429
[Epoch 19] ogbg-molbbbp: 0.916260 val loss: 0.420476
[Epoch 19] ogbg-molbbbp: 0.621624 test loss: 0.864971
[Epoch 20; Iter     5/   55] train: loss: 0.3622575
[Epoch 20; Iter    35/   55] train: loss: 0.1976800
[Epoch 20] ogbg-molbbbp: 0.923131 val loss: 0.404203
[Epoch 20] ogbg-molbbbp: 0.610340 test loss: 0.874354
[Epoch 21; Iter    10/   55] train: loss: 0.3842442
[Epoch 21; Iter    40/   55] train: loss: 0.5635316
[Epoch 21] ogbg-molbbbp: 0.906303 val loss: 0.539600
[Epoch 21] ogbg-molbbbp: 0.616995 test loss: 1.078926
[Epoch 22; Iter    15/   55] train: loss: 0.3543645
[Epoch 22; Iter    45/   55] train: loss: 0.4540492
[Epoch 22] ogbg-molbbbp: 0.910784 val loss: 0.500583
[Epoch 22] ogbg-molbbbp: 0.608989 test loss: 0.948978
[Epoch 23; Iter    20/   55] train: loss: 0.2938647
[Epoch 23; Iter    50/   55] train: loss: 0.2348830
[Epoch 23] ogbg-molbbbp: 0.902121 val loss: 0.724623
[Epoch 23] ogbg-molbbbp: 0.616127 test loss: 1.249185
[Epoch 24; Iter    25/   55] train: loss: 0.5332071
[Epoch 24; Iter    55/   55] train: loss: 0.2214279
[Epoch 24] ogbg-molbbbp: 0.923529 val loss: 0.467728
[Epoch 24] ogbg-molbbbp: 0.617766 test loss: 1.148069
[Epoch 25; Iter    30/   55] train: loss: 0.2459959
[Epoch 25] ogbg-molbbbp: 0.921438 val loss: 0.528323
[Epoch 25] ogbg-molbbbp: 0.605903 test loss: 1.253200
[Epoch 26; Iter     5/   55] train: loss: 0.3034897
[Epoch 26; Iter    35/   55] train: loss: 0.2227031
[Epoch 26] ogbg-molbbbp: 0.906900 val loss: 0.781251
[Epoch 26] ogbg-molbbbp: 0.609279 test loss: 1.545661
[Epoch 27; Iter    10/   55] train: loss: 0.2992896
[Epoch 27; Iter    40/   55] train: loss: 0.3681670
[Epoch 27] ogbg-molbbbp: 0.930997 val loss: 0.392265
[Epoch 27] ogbg-molbbbp: 0.614294 test loss: 1.787990
[Epoch 28; Iter    15/   55] train: loss: 0.2098190
[Epoch 28; Iter    45/   55] train: loss: 0.1642969
[Epoch 28] ogbg-molbbbp: 0.911481 val loss: 0.527981
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.1/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.1_4_26-05_09-19-01
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.1
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6908906
[Epoch 1] ogbg-molbbbp: 0.547147 val loss: 0.692159
[Epoch 1] ogbg-molbbbp: 0.507137 test loss: 0.693133
[Epoch 2; Iter     5/   55] train: loss: 0.6922045
[Epoch 2; Iter    35/   55] train: loss: 0.6915106
[Epoch 2] ogbg-molbbbp: 0.616848 val loss: 0.690180
[Epoch 2] ogbg-molbbbp: 0.501543 test loss: 0.692929
[Epoch 3; Iter    10/   55] train: loss: 0.6917796
[Epoch 3; Iter    40/   55] train: loss: 0.6937359
[Epoch 3] ogbg-molbbbp: 0.623818 val loss: 0.690079
[Epoch 3] ogbg-molbbbp: 0.497106 test loss: 0.692830
[Epoch 4; Iter    15/   55] train: loss: 0.6913141
[Epoch 4; Iter    45/   55] train: loss: 0.6896843
[Epoch 4] ogbg-molbbbp: 0.622523 val loss: 0.690347
[Epoch 4] ogbg-molbbbp: 0.512924 test loss: 0.692580
[Epoch 5; Iter    20/   55] train: loss: 0.6933248
[Epoch 5; Iter    50/   55] train: loss: 0.6887803
[Epoch 5] ogbg-molbbbp: 0.601215 val loss: 0.690761
[Epoch 5] ogbg-molbbbp: 0.508873 test loss: 0.692513
[Epoch 6; Iter    25/   55] train: loss: 0.6884303
[Epoch 6; Iter    55/   55] train: loss: 0.6931431
[Epoch 6] ogbg-molbbbp: 0.641442 val loss: 0.690488
[Epoch 6] ogbg-molbbbp: 0.517072 test loss: 0.692236
[Epoch 7; Iter    30/   55] train: loss: 0.6882535
[Epoch 7] ogbg-molbbbp: 0.619337 val loss: 0.691352
[Epoch 7] ogbg-molbbbp: 0.508681 test loss: 0.692251
[Epoch 8; Iter     5/   55] train: loss: 0.6858097
[Epoch 8; Iter    35/   55] train: loss: 0.6871464
[Epoch 8] ogbg-molbbbp: 0.649009 val loss: 0.691045
[Epoch 8] ogbg-molbbbp: 0.508005 test loss: 0.692029
[Epoch 9; Iter    10/   55] train: loss: 0.6832638
[Epoch 9; Iter    40/   55] train: loss: 0.6817787
[Epoch 9] ogbg-molbbbp: 0.658767 val loss: 0.691635
[Epoch 9] ogbg-molbbbp: 0.509934 test loss: 0.691778
[Epoch 10; Iter    15/   55] train: loss: 0.6820131
[Epoch 10; Iter    45/   55] train: loss: 0.6755097
[Epoch 10] ogbg-molbbbp: 0.651598 val loss: 0.691909
[Epoch 10] ogbg-molbbbp: 0.516107 test loss: 0.691571
[Epoch 11; Iter    20/   55] train: loss: 0.6833781
[Epoch 11; Iter    50/   55] train: loss: 0.6754609
[Epoch 11] ogbg-molbbbp: 0.676491 val loss: 0.692159
[Epoch 11] ogbg-molbbbp: 0.512153 test loss: 0.691390
[Epoch 12; Iter    25/   55] train: loss: 0.6747212
[Epoch 12; Iter    55/   55] train: loss: 0.6767214
[Epoch 12] ogbg-molbbbp: 0.690132 val loss: 0.693037
[Epoch 12] ogbg-molbbbp: 0.519387 test loss: 0.691033
[Epoch 13; Iter    30/   55] train: loss: 0.6709062
[Epoch 13] ogbg-molbbbp: 0.844469 val loss: 0.658879
[Epoch 13] ogbg-molbbbp: 0.575714 test loss: 0.683916
[Epoch 14; Iter     5/   55] train: loss: 0.6651282
[Epoch 14; Iter    35/   55] train: loss: 0.6011345
[Epoch 14] ogbg-molbbbp: 0.879419 val loss: 0.542030
[Epoch 14] ogbg-molbbbp: 0.588542 test loss: 0.680867
[Epoch 15; Iter    10/   55] train: loss: 0.5820425
[Epoch 15; Iter    40/   55] train: loss: 0.6184648
[Epoch 15] ogbg-molbbbp: 0.915165 val loss: 0.471272
[Epoch 15] ogbg-molbbbp: 0.603588 test loss: 0.692881
[Epoch 16; Iter    15/   55] train: loss: 0.5427445
[Epoch 16; Iter    45/   55] train: loss: 0.3823530
[Epoch 16] ogbg-molbbbp: 0.907199 val loss: 0.486311
[Epoch 16] ogbg-molbbbp: 0.603299 test loss: 0.767616
[Epoch 17; Iter    20/   55] train: loss: 0.3593273
[Epoch 17; Iter    50/   55] train: loss: 0.5338111
[Epoch 17] ogbg-molbbbp: 0.907896 val loss: 0.454001
[Epoch 17] ogbg-molbbbp: 0.608796 test loss: 0.837390
[Epoch 18; Iter    25/   55] train: loss: 0.3919697
[Epoch 18; Iter    55/   55] train: loss: 0.3485020
[Epoch 18] ogbg-molbbbp: 0.905705 val loss: 0.481965
[Epoch 18] ogbg-molbbbp: 0.618056 test loss: 0.923201
[Epoch 19; Iter    30/   55] train: loss: 0.3200110
[Epoch 19] ogbg-molbbbp: 0.865279 val loss: 0.618414
[Epoch 19] ogbg-molbbbp: 0.551698 test loss: 1.319632
[Epoch 20; Iter     5/   55] train: loss: 0.3429214
[Epoch 20; Iter    35/   55] train: loss: 0.2063444
[Epoch 20] ogbg-molbbbp: 0.918052 val loss: 0.407492
[Epoch 20] ogbg-molbbbp: 0.597704 test loss: 0.946892
[Epoch 21; Iter    10/   55] train: loss: 0.4271255
[Epoch 21; Iter    40/   55] train: loss: 0.4642666
[Epoch 21] ogbg-molbbbp: 0.925520 val loss: 0.432786
[Epoch 21] ogbg-molbbbp: 0.631366 test loss: 0.883840
[Epoch 22; Iter    15/   55] train: loss: 0.3247892
[Epoch 22; Iter    45/   55] train: loss: 0.4642588
[Epoch 22] ogbg-molbbbp: 0.923628 val loss: 0.526506
[Epoch 22] ogbg-molbbbp: 0.625000 test loss: 0.934852
[Epoch 23; Iter    20/   55] train: loss: 0.3841627
[Epoch 23; Iter    50/   55] train: loss: 0.2450087
[Epoch 23] ogbg-molbbbp: 0.881908 val loss: 0.878837
[Epoch 23] ogbg-molbbbp: 0.606674 test loss: 1.124508
[Epoch 24; Iter    25/   55] train: loss: 0.2911364
[Epoch 24; Iter    55/   55] train: loss: 1.0919299
[Epoch 24] ogbg-molbbbp: 0.847854 val loss: 1.025486
[Epoch 24] ogbg-molbbbp: 0.625000 test loss: 1.172708
[Epoch 25; Iter    30/   55] train: loss: 0.1968673
[Epoch 25] ogbg-molbbbp: 0.886588 val loss: 0.927254
[Epoch 25] ogbg-molbbbp: 0.653646 test loss: 1.103777
[Epoch 26; Iter     5/   55] train: loss: 0.3941751
[Epoch 26; Iter    35/   55] train: loss: 0.2307875
[Epoch 26] ogbg-molbbbp: 0.922732 val loss: 0.502591
[Epoch 26] ogbg-molbbbp: 0.610629 test loss: 1.294387
[Epoch 27; Iter    10/   55] train: loss: 0.1489317
[Epoch 27; Iter    40/   55] train: loss: 0.1848130
[Epoch 27] ogbg-molbbbp: 0.946729 val loss: 0.301285
[Epoch 27] ogbg-molbbbp: 0.652681 test loss: 1.731803
[Epoch 28; Iter    15/   55] train: loss: 0.1466964
[Epoch 28; Iter    45/   55] train: loss: 0.2732939
[Epoch 28] ogbg-molbbbp: 0.910186 val loss: 0.625589
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.2/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.2_4_26-05_09-19-07
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.2
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6922309
[Epoch 1] ogbg-molbbbp: 0.639251 val loss: 0.690903
[Epoch 1] ogbg-molbbbp: 0.537230 test loss: 0.692961
[Epoch 2; Iter     5/   55] train: loss: 0.6932165
[Epoch 2; Iter    35/   55] train: loss: 0.6900982
[Epoch 2] ogbg-molbbbp: 0.740715 val loss: 0.686923
[Epoch 2] ogbg-molbbbp: 0.566262 test loss: 0.691919
[Epoch 3; Iter    10/   55] train: loss: 0.6914428
[Epoch 3; Iter    40/   55] train: loss: 0.6927221
[Epoch 3] ogbg-molbbbp: 0.740516 val loss: 0.686041
[Epoch 3] ogbg-molbbbp: 0.569444 test loss: 0.691713
[Epoch 4; Iter    15/   55] train: loss: 0.6918585
[Epoch 4; Iter    45/   55] train: loss: 0.6909277
[Epoch 4] ogbg-molbbbp: 0.741611 val loss: 0.686548
[Epoch 4] ogbg-molbbbp: 0.558449 test loss: 0.691735
[Epoch 5; Iter    20/   55] train: loss: 0.6923405
[Epoch 5; Iter    50/   55] train: loss: 0.6890645
[Epoch 5] ogbg-molbbbp: 0.739918 val loss: 0.686466
[Epoch 5] ogbg-molbbbp: 0.569252 test loss: 0.691553
[Epoch 6; Iter    25/   55] train: loss: 0.6882300
[Epoch 6; Iter    55/   55] train: loss: 0.6915478
[Epoch 6] ogbg-molbbbp: 0.747884 val loss: 0.687567
[Epoch 6] ogbg-molbbbp: 0.567226 test loss: 0.691436
[Epoch 7; Iter    30/   55] train: loss: 0.6871326
[Epoch 7] ogbg-molbbbp: 0.750672 val loss: 0.687116
[Epoch 7] ogbg-molbbbp: 0.567901 test loss: 0.691114
[Epoch 8; Iter     5/   55] train: loss: 0.6861986
[Epoch 8; Iter    35/   55] train: loss: 0.6875399
[Epoch 8] ogbg-molbbbp: 0.748979 val loss: 0.687932
[Epoch 8] ogbg-molbbbp: 0.572724 test loss: 0.691018
[Epoch 9; Iter    10/   55] train: loss: 0.6856571
[Epoch 9; Iter    40/   55] train: loss: 0.6796347
[Epoch 9] ogbg-molbbbp: 0.738325 val loss: 0.688147
[Epoch 9] ogbg-molbbbp: 0.575135 test loss: 0.690880
[Epoch 10; Iter    15/   55] train: loss: 0.6829517
[Epoch 10; Iter    45/   55] train: loss: 0.6760653
[Epoch 10] ogbg-molbbbp: 0.750075 val loss: 0.687422
[Epoch 10] ogbg-molbbbp: 0.584780 test loss: 0.690408
[Epoch 11; Iter    20/   55] train: loss: 0.6814957
[Epoch 11; Iter    50/   55] train: loss: 0.6761277
[Epoch 11] ogbg-molbbbp: 0.760032 val loss: 0.689383
[Epoch 11] ogbg-molbbbp: 0.570312 test loss: 0.690493
[Epoch 12; Iter    25/   55] train: loss: 0.6758699
[Epoch 12; Iter    55/   55] train: loss: 0.6739205
[Epoch 12] ogbg-molbbbp: 0.762123 val loss: 0.689801
[Epoch 12] ogbg-molbbbp: 0.574846 test loss: 0.690155
[Epoch 13; Iter    30/   55] train: loss: 0.6694038
[Epoch 13] ogbg-molbbbp: 0.676989 val loss: 0.701704
[Epoch 13] ogbg-molbbbp: 0.504726 test loss: 0.699993
[Epoch 14; Iter     5/   55] train: loss: 0.6661952
[Epoch 14; Iter    35/   55] train: loss: 0.6171944
[Epoch 14] ogbg-molbbbp: 0.852833 val loss: 0.575621
[Epoch 14] ogbg-molbbbp: 0.575714 test loss: 0.692926
[Epoch 15; Iter    10/   55] train: loss: 0.5875748
[Epoch 15; Iter    40/   55] train: loss: 0.6655805
[Epoch 15] ogbg-molbbbp: 0.911281 val loss: 0.475860
[Epoch 15] ogbg-molbbbp: 0.587867 test loss: 0.723154
[Epoch 16; Iter    15/   55] train: loss: 0.5805696
[Epoch 16; Iter    45/   55] train: loss: 0.4212646
[Epoch 16] ogbg-molbbbp: 0.741512 val loss: 0.758072
[Epoch 16] ogbg-molbbbp: 0.625193 test loss: 0.959774
[Epoch 17; Iter    20/   55] train: loss: 0.3866324
[Epoch 17; Iter    50/   55] train: loss: 0.4798346
[Epoch 17] ogbg-molbbbp: 0.759136 val loss: 0.855368
[Epoch 17] ogbg-molbbbp: 0.602238 test loss: 1.303009
[Epoch 18; Iter    25/   55] train: loss: 0.4247887
[Epoch 18; Iter    55/   55] train: loss: 0.3718651
[Epoch 18] ogbg-molbbbp: 0.756248 val loss: 1.220586
[Epoch 18] ogbg-molbbbp: 0.619309 test loss: 1.821783
[Epoch 19; Iter    30/   55] train: loss: 0.3729228
[Epoch 19] ogbg-molbbbp: 0.757642 val loss: 1.649808
[Epoch 19] ogbg-molbbbp: 0.616030 test loss: 2.377522
[Epoch 20; Iter     5/   55] train: loss: 0.4017621
[Epoch 20; Iter    35/   55] train: loss: 0.2941153
[Epoch 20] ogbg-molbbbp: 0.756646 val loss: 1.715598
[Epoch 20] ogbg-molbbbp: 0.613426 test loss: 2.465816
[Epoch 21; Iter    10/   55] train: loss: 0.4265420
[Epoch 21; Iter    40/   55] train: loss: 0.4924025
[Epoch 21] ogbg-molbbbp: 0.759833 val loss: 1.442979
[Epoch 21] ogbg-molbbbp: 0.613426 test loss: 2.386448
[Epoch 22; Iter    15/   55] train: loss: 0.3685576
[Epoch 22; Iter    45/   55] train: loss: 0.4105198
[Epoch 22] ogbg-molbbbp: 0.746689 val loss: 1.670483
[Epoch 22] ogbg-molbbbp: 0.612847 test loss: 2.510900
[Epoch 23; Iter    20/   55] train: loss: 0.4425962
[Epoch 23; Iter    50/   55] train: loss: 0.2884354
[Epoch 23] ogbg-molbbbp: 0.749676 val loss: 1.460560
[Epoch 23] ogbg-molbbbp: 0.612944 test loss: 2.417888
[Epoch 24; Iter    25/   55] train: loss: 0.2581673
[Epoch 24; Iter    55/   55] train: loss: 0.9955142
[Epoch 24] ogbg-molbbbp: 0.764313 val loss: 1.317321
[Epoch 24] ogbg-molbbbp: 0.621238 test loss: 2.331706
[Epoch 25; Iter    30/   55] train: loss: 0.2218415
[Epoch 25] ogbg-molbbbp: 0.740814 val loss: 1.634740
[Epoch 25] ogbg-molbbbp: 0.617188 test loss: 2.418736
[Epoch 26; Iter     5/   55] train: loss: 0.3169580
[Epoch 26; Iter    35/   55] train: loss: 0.2936873
[Epoch 26] ogbg-molbbbp: 0.811909 val loss: 1.507503
[Epoch 26] ogbg-molbbbp: 0.649209 test loss: 3.555300
[Epoch 27; Iter    10/   55] train: loss: 0.1913120
[Epoch 27; Iter    40/   55] train: loss: 0.2183875
[Epoch 27] ogbg-molbbbp: 0.773773 val loss: 1.689677
[Epoch 27] ogbg-molbbbp: 0.634066 test loss: 3.328549
[Epoch 28; Iter    15/   55] train: loss: 0.2092202
[Epoch 28; Iter    45/   55] train: loss: 0.3407242
[Epoch 28] ogbg-molbbbp: 0.917355 val loss: 0.489838
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.2/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.2_6_26-05_09-19-09
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.2
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6914902
[Epoch 1] ogbg-molbbbp: 0.577716 val loss: 0.689328
[Epoch 1] ogbg-molbbbp: 0.574074 test loss: 0.692267
[Epoch 2; Iter     5/   55] train: loss: 0.6932068
[Epoch 2; Iter    35/   55] train: loss: 0.6914584
[Epoch 2] ogbg-molbbbp: 0.526635 val loss: 0.683826
[Epoch 2] ogbg-molbbbp: 0.530093 test loss: 0.693616
[Epoch 3; Iter    10/   55] train: loss: 0.6929258
[Epoch 3; Iter    40/   55] train: loss: 0.6913837
[Epoch 3] ogbg-molbbbp: 0.525739 val loss: 0.682943
[Epoch 3] ogbg-molbbbp: 0.525463 test loss: 0.694105
[Epoch 4; Iter    15/   55] train: loss: 0.6888140
[Epoch 4; Iter    45/   55] train: loss: 0.6900592
[Epoch 4] ogbg-molbbbp: 0.527930 val loss: 0.684099
[Epoch 4] ogbg-molbbbp: 0.522859 test loss: 0.693763
[Epoch 5; Iter    20/   55] train: loss: 0.6909242
[Epoch 5; Iter    50/   55] train: loss: 0.6867200
[Epoch 5] ogbg-molbbbp: 0.525540 val loss: 0.684097
[Epoch 5] ogbg-molbbbp: 0.525174 test loss: 0.693547
[Epoch 6; Iter    25/   55] train: loss: 0.6915136
[Epoch 6; Iter    55/   55] train: loss: 0.6874639
[Epoch 6] ogbg-molbbbp: 0.540078 val loss: 0.683777
[Epoch 6] ogbg-molbbbp: 0.533951 test loss: 0.692857
[Epoch 7; Iter    30/   55] train: loss: 0.6852360
[Epoch 7] ogbg-molbbbp: 0.540376 val loss: 0.683847
[Epoch 7] ogbg-molbbbp: 0.527103 test loss: 0.693010
[Epoch 8; Iter     5/   55] train: loss: 0.6857321
[Epoch 8; Iter    35/   55] train: loss: 0.6852291
[Epoch 8] ogbg-molbbbp: 0.557104 val loss: 0.683202
[Epoch 8] ogbg-molbbbp: 0.536265 test loss: 0.692039
[Epoch 9; Iter    10/   55] train: loss: 0.6841139
[Epoch 9; Iter    40/   55] train: loss: 0.6841909
[Epoch 9] ogbg-molbbbp: 0.549836 val loss: 0.684795
[Epoch 9] ogbg-molbbbp: 0.534144 test loss: 0.691880
[Epoch 10; Iter    15/   55] train: loss: 0.6793363
[Epoch 10; Iter    45/   55] train: loss: 0.6841972
[Epoch 10] ogbg-molbbbp: 0.550334 val loss: 0.684393
[Epoch 10] ogbg-molbbbp: 0.531732 test loss: 0.691605
[Epoch 11; Iter    20/   55] train: loss: 0.6779225
[Epoch 11; Iter    50/   55] train: loss: 0.6729179
[Epoch 11] ogbg-molbbbp: 0.547446 val loss: 0.684292
[Epoch 11] ogbg-molbbbp: 0.531443 test loss: 0.691465
[Epoch 12; Iter    25/   55] train: loss: 0.6717476
[Epoch 12; Iter    55/   55] train: loss: 0.6871908
[Epoch 12] ogbg-molbbbp: 0.555910 val loss: 0.684552
[Epoch 12] ogbg-molbbbp: 0.532022 test loss: 0.691105
[Epoch 13; Iter    30/   55] train: loss: 0.6737646
[Epoch 13] ogbg-molbbbp: 0.728866 val loss: 0.694925
[Epoch 13] ogbg-molbbbp: 0.515721 test loss: 0.699274
[Epoch 14; Iter     5/   55] train: loss: 0.6529499
[Epoch 14; Iter    35/   55] train: loss: 0.6426170
[Epoch 14] ogbg-molbbbp: 0.909489 val loss: 0.530939
[Epoch 14] ogbg-molbbbp: 0.633005 test loss: 0.670244
[Epoch 15; Iter    10/   55] train: loss: 0.6027876
[Epoch 15; Iter    40/   55] train: loss: 0.5982200
[Epoch 15] ogbg-molbbbp: 0.696206 val loss: 0.930624
[Epoch 15] ogbg-molbbbp: 0.579090 test loss: 1.157453
[Epoch 16; Iter    15/   55] train: loss: 0.5250415
[Epoch 16; Iter    45/   55] train: loss: 0.4939895
[Epoch 16] ogbg-molbbbp: 0.644429 val loss: 1.231322
[Epoch 16] ogbg-molbbbp: 0.571566 test loss: 1.264289
[Epoch 17; Iter    20/   55] train: loss: 0.5281751
[Epoch 17; Iter    50/   55] train: loss: 0.4295385
[Epoch 17] ogbg-molbbbp: 0.806432 val loss: 1.346795
[Epoch 17] ogbg-molbbbp: 0.627411 test loss: 2.217231
[Epoch 18; Iter    25/   55] train: loss: 0.3424402
[Epoch 18; Iter    55/   55] train: loss: 0.3417880
[Epoch 18] ogbg-molbbbp: 0.780942 val loss: 1.644135
[Epoch 18] ogbg-molbbbp: 0.648438 test loss: 2.352994
[Epoch 19; Iter    30/   55] train: loss: 0.3231203
[Epoch 19] ogbg-molbbbp: 0.809917 val loss: 2.018108
[Epoch 19] ogbg-molbbbp: 0.676215 test loss: 3.006659
[Epoch 20; Iter     5/   55] train: loss: 0.3859230
[Epoch 20; Iter    35/   55] train: loss: 0.5681705
[Epoch 20] ogbg-molbbbp: 0.877925 val loss: 1.015175
[Epoch 20] ogbg-molbbbp: 0.626640 test loss: 1.470994
[Epoch 21; Iter    10/   55] train: loss: 0.4264148
[Epoch 21; Iter    40/   55] train: loss: 0.4166920
[Epoch 21] ogbg-molbbbp: 0.817286 val loss: 1.250225
[Epoch 21] ogbg-molbbbp: 0.630401 test loss: 1.827915
[Epoch 22; Iter    15/   55] train: loss: 0.3497199
[Epoch 22; Iter    45/   55] train: loss: 0.3710440
[Epoch 22] ogbg-molbbbp: 0.801653 val loss: 1.508006
[Epoch 22] ogbg-molbbbp: 0.624614 test loss: 2.194926
[Epoch 23; Iter    20/   55] train: loss: 0.4303517
[Epoch 23; Iter    50/   55] train: loss: 0.3435477
[Epoch 23] ogbg-molbbbp: 0.799064 val loss: 1.488595
[Epoch 23] ogbg-molbbbp: 0.631655 test loss: 2.110951
[Epoch 24; Iter    25/   55] train: loss: 0.3887844
[Epoch 24; Iter    55/   55] train: loss: 0.2866217
[Epoch 24] ogbg-molbbbp: 0.770188 val loss: 1.572087
[Epoch 24] ogbg-molbbbp: 0.626061 test loss: 2.203304
[Epoch 25; Iter    30/   55] train: loss: 0.3084300
[Epoch 25] ogbg-molbbbp: 0.793289 val loss: 1.735347
[Epoch 25] ogbg-molbbbp: 0.609761 test loss: 2.917266
[Epoch 26; Iter     5/   55] train: loss: 0.4776581
[Epoch 26; Iter    35/   55] train: loss: 0.2184950
[Epoch 26] ogbg-molbbbp: 0.859405 val loss: 1.091337
[Epoch 26] ogbg-molbbbp: 0.614969 test loss: 2.539350
[Epoch 27; Iter    10/   55] train: loss: 0.2392783
[Epoch 27; Iter    40/   55] train: loss: 0.2568631
[Epoch 27] ogbg-molbbbp: 0.883302 val loss: 0.652468
[Epoch 27] ogbg-molbbbp: 0.655768 test loss: 1.268032
[Epoch 28; Iter    15/   55] train: loss: 0.2360627
[Epoch 28; Iter    45/   55] train: loss: 0.3425947
[Epoch 28] ogbg-molbbbp: 0.881410 val loss: 1.120023
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.1/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.1_5_26-05_09-19-04
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.1
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6945624
[Epoch 1] ogbg-molbbbp: 0.765210 val loss: 0.690764
[Epoch 1] ogbg-molbbbp: 0.497203 test loss: 0.692825
[Epoch 2; Iter     5/   55] train: loss: 0.6932906
[Epoch 2; Iter    35/   55] train: loss: 0.6906857
[Epoch 2] ogbg-molbbbp: 0.676989 val loss: 0.691652
[Epoch 2] ogbg-molbbbp: 0.462577 test loss: 0.693613
[Epoch 3; Iter    10/   55] train: loss: 0.6923793
[Epoch 3; Iter    40/   55] train: loss: 0.6899964
[Epoch 3] ogbg-molbbbp: 0.666534 val loss: 0.692427
[Epoch 3] ogbg-molbbbp: 0.461806 test loss: 0.693555
[Epoch 4; Iter    15/   55] train: loss: 0.6910570
[Epoch 4; Iter    45/   55] train: loss: 0.6901880
[Epoch 4] ogbg-molbbbp: 0.682067 val loss: 0.691769
[Epoch 4] ogbg-molbbbp: 0.467207 test loss: 0.693354
[Epoch 5; Iter    20/   55] train: loss: 0.6892824
[Epoch 5; Iter    50/   55] train: loss: 0.6936045
[Epoch 5] ogbg-molbbbp: 0.682266 val loss: 0.692222
[Epoch 5] ogbg-molbbbp: 0.463927 test loss: 0.693286
[Epoch 6; Iter    25/   55] train: loss: 0.6894472
[Epoch 6; Iter    55/   55] train: loss: 0.6868544
[Epoch 6] ogbg-molbbbp: 0.686050 val loss: 0.692276
[Epoch 6] ogbg-molbbbp: 0.466628 test loss: 0.693282
[Epoch 7; Iter    30/   55] train: loss: 0.6866223
[Epoch 7] ogbg-molbbbp: 0.697302 val loss: 0.692104
[Epoch 7] ogbg-molbbbp: 0.465278 test loss: 0.693014
[Epoch 8; Iter     5/   55] train: loss: 0.6884444
[Epoch 8; Iter    35/   55] train: loss: 0.6823304
[Epoch 8] ogbg-molbbbp: 0.697700 val loss: 0.692646
[Epoch 8] ogbg-molbbbp: 0.471644 test loss: 0.692883
[Epoch 9; Iter    10/   55] train: loss: 0.6888148
[Epoch 9; Iter    40/   55] train: loss: 0.6861717
[Epoch 9] ogbg-molbbbp: 0.700090 val loss: 0.692973
[Epoch 9] ogbg-molbbbp: 0.473765 test loss: 0.692771
[Epoch 10; Iter    15/   55] train: loss: 0.6810335
[Epoch 10; Iter    45/   55] train: loss: 0.6815410
[Epoch 10] ogbg-molbbbp: 0.699094 val loss: 0.693871
[Epoch 10] ogbg-molbbbp: 0.474441 test loss: 0.692582
[Epoch 11; Iter    20/   55] train: loss: 0.6797096
[Epoch 11; Iter    50/   55] train: loss: 0.6756592
[Epoch 11] ogbg-molbbbp: 0.724286 val loss: 0.693293
[Epoch 11] ogbg-molbbbp: 0.479070 test loss: 0.692353
[Epoch 12; Iter    25/   55] train: loss: 0.6754339
[Epoch 12; Iter    55/   55] train: loss: 0.6772011
[Epoch 12] ogbg-molbbbp: 0.723987 val loss: 0.694002
[Epoch 12] ogbg-molbbbp: 0.482639 test loss: 0.692211
[Epoch 13; Iter    30/   55] train: loss: 0.6795698
[Epoch 13] ogbg-molbbbp: 0.787812 val loss: 0.673670
[Epoch 13] ogbg-molbbbp: 0.533951 test loss: 0.693805
[Epoch 14; Iter     5/   55] train: loss: 0.6628137
[Epoch 14; Iter    35/   55] train: loss: 0.6315397
[Epoch 14] ogbg-molbbbp: 0.869461 val loss: 0.566661
[Epoch 14] ogbg-molbbbp: 0.593654 test loss: 0.683402
[Epoch 15; Iter    10/   55] train: loss: 0.5960565
[Epoch 15; Iter    40/   55] train: loss: 0.5661482
[Epoch 15] ogbg-molbbbp: 0.915762 val loss: 0.460021
[Epoch 15] ogbg-molbbbp: 0.618538 test loss: 0.684311
[Epoch 16; Iter    15/   55] train: loss: 0.4226909
[Epoch 16; Iter    45/   55] train: loss: 0.5402236
[Epoch 16] ogbg-molbbbp: 0.814398 val loss: 0.554747
[Epoch 16] ogbg-molbbbp: 0.571373 test loss: 0.750447
[Epoch 17; Iter    20/   55] train: loss: 0.4308988
[Epoch 17; Iter    50/   55] train: loss: 0.4777333
[Epoch 17] ogbg-molbbbp: 0.901822 val loss: 0.485893
[Epoch 17] ogbg-molbbbp: 0.621142 test loss: 0.759538
[Epoch 18; Iter    25/   55] train: loss: 0.3591664
[Epoch 18; Iter    55/   55] train: loss: 0.4821515
[Epoch 18] ogbg-molbbbp: 0.914767 val loss: 0.564319
[Epoch 18] ogbg-molbbbp: 0.593654 test loss: 0.911747
[Epoch 19; Iter    30/   55] train: loss: 0.3791769
[Epoch 19] ogbg-molbbbp: 0.904411 val loss: 0.612226
[Epoch 19] ogbg-molbbbp: 0.611593 test loss: 0.930168
[Epoch 20; Iter     5/   55] train: loss: 0.3854831
[Epoch 20; Iter    35/   55] train: loss: 0.2504070
[Epoch 20] ogbg-molbbbp: 0.902619 val loss: 0.631116
[Epoch 20] ogbg-molbbbp: 0.608507 test loss: 0.819770
[Epoch 21; Iter    10/   55] train: loss: 0.3562114
[Epoch 21; Iter    40/   55] train: loss: 0.6044464
[Epoch 21] ogbg-molbbbp: 0.868864 val loss: 0.772599
[Epoch 21] ogbg-molbbbp: 0.578125 test loss: 1.310474
[Epoch 22; Iter    15/   55] train: loss: 0.4136662
[Epoch 22; Iter    45/   55] train: loss: 0.4252904
[Epoch 22] ogbg-molbbbp: 0.859006 val loss: 0.961146
[Epoch 22] ogbg-molbbbp: 0.584298 test loss: 1.260165
[Epoch 23; Iter    20/   55] train: loss: 0.3043106
[Epoch 23; Iter    50/   55] train: loss: 0.3362041
[Epoch 23] ogbg-molbbbp: 0.894553 val loss: 0.607283
[Epoch 23] ogbg-molbbbp: 0.576775 test loss: 1.279867
[Epoch 24; Iter    25/   55] train: loss: 0.5648292
[Epoch 24; Iter    55/   55] train: loss: 0.2365876
[Epoch 24] ogbg-molbbbp: 0.868665 val loss: 0.753759
[Epoch 24] ogbg-molbbbp: 0.589024 test loss: 1.399181
[Epoch 25; Iter    30/   55] train: loss: 0.3290374
[Epoch 25] ogbg-molbbbp: 0.874838 val loss: 0.775297
[Epoch 25] ogbg-molbbbp: 0.605517 test loss: 1.309219
[Epoch 26; Iter     5/   55] train: loss: 0.3726159
[Epoch 26; Iter    35/   55] train: loss: 0.2853074
[Epoch 26] ogbg-molbbbp: 0.767002 val loss: 5.577955
[Epoch 26] ogbg-molbbbp: 0.566647 test loss: 5.369721
[Epoch 27; Iter    10/   55] train: loss: 0.3882554
[Epoch 27; Iter    40/   55] train: loss: 0.4555385
[Epoch 27] ogbg-molbbbp: 0.738325 val loss: 1.621792
[Epoch 27] ogbg-molbbbp: 0.529996 test loss: 1.835538
[Epoch 28; Iter    15/   55] train: loss: 0.2575319
[Epoch 28; Iter    45/   55] train: loss: 0.1980015
[Epoch 28] ogbg-molbbbp: 0.843871 val loss: 1.314707
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.0/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.0_6_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.0
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6933044
[Epoch 1] ogbg-molbbbp: 0.695310 val loss: 0.691252
[Epoch 1] ogbg-molbbbp: 0.616512 test loss: 0.691880
[Epoch 2; Iter     5/   55] train: loss: 0.6939372
[Epoch 2; Iter    35/   55] train: loss: 0.6957976
[Epoch 2] ogbg-molbbbp: 0.599323 val loss: 0.691247
[Epoch 2] ogbg-molbbbp: 0.601948 test loss: 0.690859
[Epoch 3; Iter    10/   55] train: loss: 0.6924673
[Epoch 3; Iter    40/   55] train: loss: 0.6919085
[Epoch 3] ogbg-molbbbp: 0.590760 val loss: 0.691530
[Epoch 3] ogbg-molbbbp: 0.597608 test loss: 0.690960
[Epoch 4; Iter    15/   55] train: loss: 0.6910120
[Epoch 4; Iter    45/   55] train: loss: 0.6928008
[Epoch 4] ogbg-molbbbp: 0.628298 val loss: 0.690943
[Epoch 4] ogbg-molbbbp: 0.594811 test loss: 0.690907
[Epoch 5; Iter    20/   55] train: loss: 0.6927332
[Epoch 5; Iter    50/   55] train: loss: 0.6870205
[Epoch 5] ogbg-molbbbp: 0.626406 val loss: 0.691112
[Epoch 5] ogbg-molbbbp: 0.597512 test loss: 0.690758
[Epoch 6; Iter    25/   55] train: loss: 0.6896964
[Epoch 6; Iter    55/   55] train: loss: 0.6876629
[Epoch 6] ogbg-molbbbp: 0.643334 val loss: 0.690796
[Epoch 6] ogbg-molbbbp: 0.596740 test loss: 0.690553
[Epoch 7; Iter    30/   55] train: loss: 0.6871856
[Epoch 7] ogbg-molbbbp: 0.637160 val loss: 0.691671
[Epoch 7] ogbg-molbbbp: 0.597029 test loss: 0.690334
[Epoch 8; Iter     5/   55] train: loss: 0.6860068
[Epoch 8; Iter    35/   55] train: loss: 0.6842486
[Epoch 8] ogbg-molbbbp: 0.636264 val loss: 0.691991
[Epoch 8] ogbg-molbbbp: 0.596547 test loss: 0.690187
[Epoch 9; Iter    10/   55] train: loss: 0.6823841
[Epoch 9; Iter    40/   55] train: loss: 0.6840454
[Epoch 9] ogbg-molbbbp: 0.678084 val loss: 0.691409
[Epoch 9] ogbg-molbbbp: 0.597512 test loss: 0.690052
[Epoch 10; Iter    15/   55] train: loss: 0.6798117
[Epoch 10; Iter    45/   55] train: loss: 0.6816750
[Epoch 10] ogbg-molbbbp: 0.695011 val loss: 0.691449
[Epoch 10] ogbg-molbbbp: 0.597319 test loss: 0.689854
[Epoch 11; Iter    20/   55] train: loss: 0.6775602
[Epoch 11; Iter    50/   55] train: loss: 0.6733643
[Epoch 11] ogbg-molbbbp: 0.703176 val loss: 0.691896
[Epoch 11] ogbg-molbbbp: 0.601177 test loss: 0.689674
[Epoch 12; Iter    25/   55] train: loss: 0.6707716
[Epoch 12; Iter    55/   55] train: loss: 0.6917852
[Epoch 12] ogbg-molbbbp: 0.723688 val loss: 0.692180
[Epoch 12] ogbg-molbbbp: 0.591242 test loss: 0.689678
[Epoch 13; Iter    30/   55] train: loss: 0.6720517
[Epoch 13] ogbg-molbbbp: 0.909788 val loss: 0.618938
[Epoch 13] ogbg-molbbbp: 0.589410 test loss: 0.681408
[Epoch 14; Iter     5/   55] train: loss: 0.6405759
[Epoch 14; Iter    35/   55] train: loss: 0.5845066
[Epoch 14] ogbg-molbbbp: 0.921836 val loss: 0.520144
[Epoch 14] ogbg-molbbbp: 0.630980 test loss: 0.673151
[Epoch 15; Iter    10/   55] train: loss: 0.5625376
[Epoch 15; Iter    40/   55] train: loss: 0.4956192
[Epoch 15] ogbg-molbbbp: 0.827641 val loss: 0.544368
[Epoch 15] ogbg-molbbbp: 0.567226 test loss: 0.715977
[Epoch 16; Iter    15/   55] train: loss: 0.4627948
[Epoch 16; Iter    45/   55] train: loss: 0.4202855
[Epoch 16] ogbg-molbbbp: 0.918550 val loss: 0.396962
[Epoch 16] ogbg-molbbbp: 0.609857 test loss: 0.770395
[Epoch 17; Iter    20/   55] train: loss: 0.3951970
[Epoch 17; Iter    50/   55] train: loss: 0.3908048
[Epoch 17] ogbg-molbbbp: 0.936672 val loss: 0.364304
[Epoch 17] ogbg-molbbbp: 0.629147 test loss: 0.807455
[Epoch 18; Iter    25/   55] train: loss: 0.2450227
[Epoch 18; Iter    55/   55] train: loss: 0.3560490
[Epoch 18] ogbg-molbbbp: 0.934382 val loss: 0.386032
[Epoch 18] ogbg-molbbbp: 0.612365 test loss: 0.858108
[Epoch 19; Iter    30/   55] train: loss: 0.2673239
[Epoch 19] ogbg-molbbbp: 0.926715 val loss: 0.395154
[Epoch 19] ogbg-molbbbp: 0.642554 test loss: 0.903718
[Epoch 20; Iter     5/   55] train: loss: 0.3572958
[Epoch 20; Iter    35/   55] train: loss: 0.4357879
[Epoch 20] ogbg-molbbbp: 0.911779 val loss: 0.447077
[Epoch 20] ogbg-molbbbp: 0.647762 test loss: 0.957493
[Epoch 21; Iter    10/   55] train: loss: 0.3412524
[Epoch 21; Iter    40/   55] train: loss: 0.2764200
[Epoch 21] ogbg-molbbbp: 0.918152 val loss: 0.441610
[Epoch 21] ogbg-molbbbp: 0.650077 test loss: 1.161492
[Epoch 22; Iter    15/   55] train: loss: 0.2309872
[Epoch 22; Iter    45/   55] train: loss: 0.2230658
[Epoch 22] ogbg-molbbbp: 0.927213 val loss: 0.415797
[Epoch 22] ogbg-molbbbp: 0.666763 test loss: 1.018156
[Epoch 23; Iter    20/   55] train: loss: 0.4258675
[Epoch 23; Iter    50/   55] train: loss: 0.3631783
[Epoch 23] ogbg-molbbbp: 0.917156 val loss: 0.512343
[Epoch 23] ogbg-molbbbp: 0.668113 test loss: 1.087129
[Epoch 24; Iter    25/   55] train: loss: 0.2344359
[Epoch 24; Iter    55/   55] train: loss: 0.0907512
[Epoch 24] ogbg-molbbbp: 0.924325 val loss: 0.419536
[Epoch 24] ogbg-molbbbp: 0.662133 test loss: 1.006609
[Epoch 25; Iter    30/   55] train: loss: 0.2783074
[Epoch 25] ogbg-molbbbp: 0.909190 val loss: 0.482252
[Epoch 25] ogbg-molbbbp: 0.657986 test loss: 1.106448
[Epoch 26; Iter     5/   55] train: loss: 0.2608490
[Epoch 26; Iter    35/   55] train: loss: 0.2423488
[Epoch 26] ogbg-molbbbp: 0.949915 val loss: 0.317056
[Epoch 26] ogbg-molbbbp: 0.642940 test loss: 1.065169
[Epoch 27; Iter    10/   55] train: loss: 0.2034440
[Epoch 27; Iter    40/   55] train: loss: 0.1193041
[Epoch 27] ogbg-molbbbp: 0.947127 val loss: 0.316486
[Epoch 27] ogbg-molbbbp: 0.662230 test loss: 1.172294
[Epoch 28; Iter    15/   55] train: loss: 0.1444989
[Epoch 28; Iter    45/   55] train: loss: 0.2597054
[Epoch 28] ogbg-molbbbp: 0.942049 val loss: 0.313065
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.0/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.0_5_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.0
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6939895
[Epoch 1] ogbg-molbbbp: 0.830827 val loss: 0.689182
[Epoch 1] ogbg-molbbbp: 0.512249 test loss: 0.692677
[Epoch 2; Iter     5/   55] train: loss: 0.6939717
[Epoch 2; Iter    35/   55] train: loss: 0.6921082
[Epoch 2] ogbg-molbbbp: 0.773374 val loss: 0.686356
[Epoch 2] ogbg-molbbbp: 0.486208 test loss: 0.692856
[Epoch 3; Iter    10/   55] train: loss: 0.6924200
[Epoch 3; Iter    40/   55] train: loss: 0.6918980
[Epoch 3] ogbg-molbbbp: 0.773673 val loss: 0.686571
[Epoch 3] ogbg-molbbbp: 0.488715 test loss: 0.692731
[Epoch 4; Iter    15/   55] train: loss: 0.6910273
[Epoch 4; Iter    45/   55] train: loss: 0.6919813
[Epoch 4] ogbg-molbbbp: 0.772478 val loss: 0.686993
[Epoch 4] ogbg-molbbbp: 0.490837 test loss: 0.692563
[Epoch 5; Iter    20/   55] train: loss: 0.6905544
[Epoch 5; Iter    50/   55] train: loss: 0.6956550
[Epoch 5] ogbg-molbbbp: 0.776860 val loss: 0.687049
[Epoch 5] ogbg-molbbbp: 0.495274 test loss: 0.692327
[Epoch 6; Iter    25/   55] train: loss: 0.6895805
[Epoch 6; Iter    55/   55] train: loss: 0.6868336
[Epoch 6] ogbg-molbbbp: 0.790202 val loss: 0.686084
[Epoch 6] ogbg-molbbbp: 0.498650 test loss: 0.692149
[Epoch 7; Iter    30/   55] train: loss: 0.6861193
[Epoch 7] ogbg-molbbbp: 0.792990 val loss: 0.686561
[Epoch 7] ogbg-molbbbp: 0.503376 test loss: 0.691911
[Epoch 8; Iter     5/   55] train: loss: 0.6861213
[Epoch 8; Iter    35/   55] train: loss: 0.6816033
[Epoch 8] ogbg-molbbbp: 0.808025 val loss: 0.686092
[Epoch 8] ogbg-molbbbp: 0.498071 test loss: 0.691909
[Epoch 9; Iter    10/   55] train: loss: 0.6904274
[Epoch 9; Iter    40/   55] train: loss: 0.6869763
[Epoch 9] ogbg-molbbbp: 0.815394 val loss: 0.685809
[Epoch 9] ogbg-molbbbp: 0.497782 test loss: 0.691648
[Epoch 10; Iter    15/   55] train: loss: 0.6823931
[Epoch 10; Iter    45/   55] train: loss: 0.6818923
[Epoch 10] ogbg-molbbbp: 0.827342 val loss: 0.685753
[Epoch 10] ogbg-molbbbp: 0.502797 test loss: 0.691316
[Epoch 11; Iter    20/   55] train: loss: 0.6808157
[Epoch 11; Iter    50/   55] train: loss: 0.6750900
[Epoch 11] ogbg-molbbbp: 0.821567 val loss: 0.685839
[Epoch 11] ogbg-molbbbp: 0.508777 test loss: 0.691265
[Epoch 12; Iter    25/   55] train: loss: 0.6761420
[Epoch 12; Iter    55/   55] train: loss: 0.6714888
[Epoch 12] ogbg-molbbbp: 0.836005 val loss: 0.685724
[Epoch 12] ogbg-molbbbp: 0.511671 test loss: 0.690679
[Epoch 13; Iter    30/   55] train: loss: 0.6795565
[Epoch 13] ogbg-molbbbp: 0.909589 val loss: 0.619825
[Epoch 13] ogbg-molbbbp: 0.597319 test loss: 0.680014
[Epoch 14; Iter     5/   55] train: loss: 0.6645089
[Epoch 14; Iter    35/   55] train: loss: 0.6055999
[Epoch 14] ogbg-molbbbp: 0.903117 val loss: 0.496318
[Epoch 14] ogbg-molbbbp: 0.621142 test loss: 0.674686
[Epoch 15; Iter    10/   55] train: loss: 0.5603158
[Epoch 15; Iter    40/   55] train: loss: 0.5461428
[Epoch 15] ogbg-molbbbp: 0.929005 val loss: 0.406838
[Epoch 15] ogbg-molbbbp: 0.614390 test loss: 0.707443
[Epoch 16; Iter    15/   55] train: loss: 0.3915764
[Epoch 16; Iter    45/   55] train: loss: 0.5014724
[Epoch 16] ogbg-molbbbp: 0.881211 val loss: 0.459789
[Epoch 16] ogbg-molbbbp: 0.622106 test loss: 0.717702
[Epoch 17; Iter    20/   55] train: loss: 0.3617046
[Epoch 17; Iter    50/   55] train: loss: 0.4312295
[Epoch 17] ogbg-molbbbp: 0.942846 val loss: 0.345251
[Epoch 17] ogbg-molbbbp: 0.640818 test loss: 0.744355
[Epoch 18; Iter    25/   55] train: loss: 0.2733384
[Epoch 18; Iter    55/   55] train: loss: 0.5353193
[Epoch 18] ogbg-molbbbp: 0.934980 val loss: 0.326853
[Epoch 18] ogbg-molbbbp: 0.636478 test loss: 0.837754
[Epoch 19; Iter    30/   55] train: loss: 0.3205720
[Epoch 19] ogbg-molbbbp: 0.937568 val loss: 0.371831
[Epoch 19] ogbg-molbbbp: 0.642168 test loss: 0.962408
[Epoch 20; Iter     5/   55] train: loss: 0.2664358
[Epoch 20; Iter    35/   55] train: loss: 0.2193703
[Epoch 20] ogbg-molbbbp: 0.926715 val loss: 0.404407
[Epoch 20] ogbg-molbbbp: 0.628954 test loss: 0.990321
[Epoch 21; Iter    10/   55] train: loss: 0.2864611
[Epoch 21; Iter    40/   55] train: loss: 0.4969093
[Epoch 21] ogbg-molbbbp: 0.912875 val loss: 0.488184
[Epoch 21] ogbg-molbbbp: 0.648823 test loss: 1.024659
[Epoch 22; Iter    15/   55] train: loss: 0.4593271
[Epoch 22; Iter    45/   55] train: loss: 0.3132461
[Epoch 22] ogbg-molbbbp: 0.906104 val loss: 0.445152
[Epoch 22] ogbg-molbbbp: 0.627025 test loss: 0.963695
[Epoch 23; Iter    20/   55] train: loss: 0.2186852
[Epoch 23; Iter    50/   55] train: loss: 0.3147523
[Epoch 23] ogbg-molbbbp: 0.914767 val loss: 0.499366
[Epoch 23] ogbg-molbbbp: 0.643808 test loss: 1.141949
[Epoch 24; Iter    25/   55] train: loss: 0.4004529
[Epoch 24; Iter    55/   55] train: loss: 0.2501616
[Epoch 24] ogbg-molbbbp: 0.936374 val loss: 0.344709
[Epoch 24] ogbg-molbbbp: 0.653935 test loss: 1.021820
[Epoch 25; Iter    30/   55] train: loss: 0.2225169
[Epoch 25] ogbg-molbbbp: 0.920741 val loss: 0.441956
[Epoch 25] ogbg-molbbbp: 0.657215 test loss: 1.212585
[Epoch 26; Iter     5/   55] train: loss: 0.2353809
[Epoch 26; Iter    35/   55] train: loss: 0.2379824
[Epoch 26] ogbg-molbbbp: 0.892164 val loss: 0.784647
[Epoch 26] ogbg-molbbbp: 0.629437 test loss: 1.363748
[Epoch 27; Iter    10/   55] train: loss: 0.3116715
[Epoch 27; Iter    40/   55] train: loss: 0.2066592
[Epoch 27] ogbg-molbbbp: 0.904013 val loss: 0.505961
[Epoch 27] ogbg-molbbbp: 0.585455 test loss: 2.499735
[Epoch 28; Iter    15/   55] train: loss: 0.2153786
[Epoch 28; Iter    45/   55] train: loss: 0.1423854
[Epoch 28] ogbg-molbbbp: 0.940456 val loss: 0.521450
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/bbbp/noise=0.0/PNA_ogbg-molbbbp_3DInfomax_bbbp_static_noise=0.0_4_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bbbp/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_static_noise=0.0
logdir: runs/static_noise/3DInfomax/bbbp/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6913464
[Epoch 1] ogbg-molbbbp: 0.629593 val loss: 0.692138
[Epoch 1] ogbg-molbbbp: 0.530478 test loss: 0.692881
[Epoch 2; Iter     5/   55] train: loss: 0.6924450
[Epoch 2; Iter    35/   55] train: loss: 0.6924824
[Epoch 2] ogbg-molbbbp: 0.613462 val loss: 0.691002
[Epoch 2] ogbg-molbbbp: 0.515143 test loss: 0.692894
[Epoch 3; Iter    10/   55] train: loss: 0.6921225
[Epoch 3; Iter    40/   55] train: loss: 0.6933986
[Epoch 3] ogbg-molbbbp: 0.603007 val loss: 0.691752
[Epoch 3] ogbg-molbbbp: 0.516011 test loss: 0.692909
[Epoch 4; Iter    15/   55] train: loss: 0.6924711
[Epoch 4; Iter    45/   55] train: loss: 0.6902018
[Epoch 4] ogbg-molbbbp: 0.610973 val loss: 0.691429
[Epoch 4] ogbg-molbbbp: 0.518326 test loss: 0.692703
[Epoch 5; Iter    20/   55] train: loss: 0.6925603
[Epoch 5; Iter    50/   55] train: loss: 0.6885024
[Epoch 5] ogbg-molbbbp: 0.613263 val loss: 0.691657
[Epoch 5] ogbg-molbbbp: 0.525656 test loss: 0.692495
[Epoch 6; Iter    25/   55] train: loss: 0.6895051
[Epoch 6; Iter    55/   55] train: loss: 0.6959885
[Epoch 6] ogbg-molbbbp: 0.655681 val loss: 0.690845
[Epoch 6] ogbg-molbbbp: 0.521701 test loss: 0.692290
[Epoch 7; Iter    30/   55] train: loss: 0.6887948
[Epoch 7] ogbg-molbbbp: 0.632082 val loss: 0.691679
[Epoch 7] ogbg-molbbbp: 0.521894 test loss: 0.692351
[Epoch 8; Iter     5/   55] train: loss: 0.6872915
[Epoch 8; Iter    35/   55] train: loss: 0.6889781
[Epoch 8] ogbg-molbbbp: 0.654386 val loss: 0.691745
[Epoch 8] ogbg-molbbbp: 0.517747 test loss: 0.692137
[Epoch 9; Iter    10/   55] train: loss: 0.6852095
[Epoch 9; Iter    40/   55] train: loss: 0.6817462
[Epoch 9] ogbg-molbbbp: 0.690232 val loss: 0.691016
[Epoch 9] ogbg-molbbbp: 0.521798 test loss: 0.691721
[Epoch 10; Iter    15/   55] train: loss: 0.6857153
[Epoch 10; Iter    45/   55] train: loss: 0.6750189
[Epoch 10] ogbg-molbbbp: 0.700388 val loss: 0.691413
[Epoch 10] ogbg-molbbbp: 0.527392 test loss: 0.691401
[Epoch 11; Iter    20/   55] train: loss: 0.6843321
[Epoch 11; Iter    50/   55] train: loss: 0.6771483
[Epoch 11] ogbg-molbbbp: 0.713631 val loss: 0.691309
[Epoch 11] ogbg-molbbbp: 0.525367 test loss: 0.691283
[Epoch 12; Iter    25/   55] train: loss: 0.6766505
[Epoch 12; Iter    55/   55] train: loss: 0.6733553
[Epoch 12] ogbg-molbbbp: 0.741113 val loss: 0.691088
[Epoch 12] ogbg-molbbbp: 0.535880 test loss: 0.690884
[Epoch 13; Iter    30/   55] train: loss: 0.6700225
[Epoch 13] ogbg-molbbbp: 0.910286 val loss: 0.633183
[Epoch 13] ogbg-molbbbp: 0.571181 test loss: 0.683596
[Epoch 14; Iter     5/   55] train: loss: 0.6582623
[Epoch 14; Iter    35/   55] train: loss: 0.5814320
[Epoch 14] ogbg-molbbbp: 0.932490 val loss: 0.458012
[Epoch 14] ogbg-molbbbp: 0.612365 test loss: 0.694631
[Epoch 15; Iter    10/   55] train: loss: 0.5379752
[Epoch 15; Iter    40/   55] train: loss: 0.5629935
[Epoch 15] ogbg-molbbbp: 0.883700 val loss: 0.491861
[Epoch 15] ogbg-molbbbp: 0.585166 test loss: 0.718064
[Epoch 16; Iter    15/   55] train: loss: 0.5313541
[Epoch 16; Iter    45/   55] train: loss: 0.3602386
[Epoch 16] ogbg-molbbbp: 0.905407 val loss: 0.449643
[Epoch 16] ogbg-molbbbp: 0.630401 test loss: 0.751998
[Epoch 17; Iter    20/   55] train: loss: 0.3298573
[Epoch 17; Iter    50/   55] train: loss: 0.4915681
[Epoch 17] ogbg-molbbbp: 0.921836 val loss: 0.390951
[Epoch 17] ogbg-molbbbp: 0.641011 test loss: 0.803316
[Epoch 18; Iter    25/   55] train: loss: 0.3394468
[Epoch 18; Iter    55/   55] train: loss: 0.4385184
[Epoch 18] ogbg-molbbbp: 0.928408 val loss: 0.392896
[Epoch 18] ogbg-molbbbp: 0.629919 test loss: 0.934098
[Epoch 19; Iter    30/   55] train: loss: 0.2721257
[Epoch 19] ogbg-molbbbp: 0.910883 val loss: 0.390621
[Epoch 19] ogbg-molbbbp: 0.621817 test loss: 0.865611
[Epoch 20; Iter     5/   55] train: loss: 0.2562791
[Epoch 20; Iter    35/   55] train: loss: 0.1669187
[Epoch 20] ogbg-molbbbp: 0.928707 val loss: 0.406891
[Epoch 20] ogbg-molbbbp: 0.621335 test loss: 1.017080
[Epoch 21; Iter    10/   55] train: loss: 0.3246138
[Epoch 21; Iter    40/   55] train: loss: 0.3853703
[Epoch 21] ogbg-molbbbp: 0.926715 val loss: 0.420816
[Epoch 21] ogbg-molbbbp: 0.656732 test loss: 1.010888
[Epoch 22; Iter    15/   55] train: loss: 0.2952636
[Epoch 22; Iter    45/   55] train: loss: 0.4191974
[Epoch 22] ogbg-molbbbp: 0.913870 val loss: 0.578856
[Epoch 22] ogbg-molbbbp: 0.649306 test loss: 1.109523
[Epoch 23; Iter    20/   55] train: loss: 0.4136929
[Epoch 23; Iter    50/   55] train: loss: 0.1968569
[Epoch 23] ogbg-molbbbp: 0.925919 val loss: 0.438271
[Epoch 23] ogbg-molbbbp: 0.653742 test loss: 1.007326
[Epoch 24; Iter    25/   55] train: loss: 0.2421927
[Epoch 24; Iter    55/   55] train: loss: 0.6207256
[Epoch 24] ogbg-molbbbp: 0.894255 val loss: 0.493529
[Epoch 24] ogbg-molbbbp: 0.642168 test loss: 0.947955
[Epoch 25; Iter    30/   55] train: loss: 0.1594835
[Epoch 25] ogbg-molbbbp: 0.933984 val loss: 0.405837
[Epoch 25] ogbg-molbbbp: 0.646894 test loss: 1.011268
[Epoch 26; Iter     5/   55] train: loss: 0.2715015
[Epoch 26; Iter    35/   55] train: loss: 0.1775868
[Epoch 26] ogbg-molbbbp: 0.870955 val loss: 0.630487
[Epoch 26] ogbg-molbbbp: 0.611400 test loss: 0.986745
[Epoch 27; Iter    10/   55] train: loss: 0.1797710
[Epoch 27; Iter    40/   55] train: loss: 0.1301559
[Epoch 27] ogbg-molbbbp: 0.936374 val loss: 0.443043
[Epoch 27] ogbg-molbbbp: 0.693673 test loss: 0.944695
[Epoch 28; Iter    15/   55] train: loss: 0.1630806
[Epoch 28; Iter    45/   55] train: loss: 0.2820918
[Epoch 28] ogbg-molbbbp: 0.942348 val loss: 0.388722
[Epoch 28] ogbg-molbbbp: 0.668113 test loss: 1.219803
[Epoch 29; Iter    20/   55] train: loss: 0.1422901
[Epoch 29; Iter    50/   55] train: loss: 0.2894166
[Epoch 29] ogbg-molbbbp: 0.955292 val loss: 0.285470
[Epoch 29] ogbg-molbbbp: 0.588927 test loss: 1.097936
[Epoch 30; Iter    25/   55] train: loss: 0.2105554
[Epoch 30; Iter    55/   55] train: loss: 0.3220263
[Epoch 30] ogbg-molbbbp: 0.819476 val loss: 1.003361
[Epoch 30] ogbg-molbbbp: 0.598090 test loss: 1.535712
[Epoch 31; Iter    30/   55] train: loss: 0.3642843
[Epoch 31] ogbg-molbbbp: 0.950314 val loss: 0.457584
[Epoch 31] ogbg-molbbbp: 0.662326 test loss: 2.653225
[Epoch 32; Iter     5/   55] train: loss: 0.2378492
[Epoch 32; Iter    35/   55] train: loss: 0.2318125
[Epoch 32] ogbg-molbbbp: 0.959574 val loss: 0.295441
[Epoch 32] ogbg-molbbbp: 0.673225 test loss: 1.269373
[Epoch 33; Iter    10/   55] train: loss: 0.1535690
[Epoch 33; Iter    40/   55] train: loss: 0.3282406
[Epoch 33] ogbg-molbbbp: 0.942447 val loss: 0.578322
[Epoch 33] ogbg-molbbbp: 0.632620 test loss: 1.813906
[Epoch 34; Iter    15/   55] train: loss: 0.1882285
[Epoch 34; Iter    45/   55] train: loss: 0.2521783
[Epoch 34] ogbg-molbbbp: 0.961466 val loss: 0.291078
[Epoch 34] ogbg-molbbbp: 0.640818 test loss: 1.501707
[Epoch 35; Iter    20/   55] train: loss: 0.0521577
[Epoch 35; Iter    50/   55] train: loss: 0.1631275
[Epoch 35] ogbg-molbbbp: 0.957582 val loss: 0.268446
[Epoch 35] ogbg-molbbbp: 0.651235 test loss: 1.294850
[Epoch 36; Iter    25/   55] train: loss: 0.1673064
[Epoch 36; Iter    55/   55] train: loss: 0.4042691
[Epoch 36] ogbg-molbbbp: 0.944937 val loss: 0.447847
[Epoch 36] ogbg-molbbbp: 0.630112 test loss: 1.831751
[Epoch 37; Iter    30/   55] train: loss: 0.0439868
[Epoch 37] ogbg-molbbbp: 0.895748 val loss: 0.807805
[Epoch 37] ogbg-molbbbp: 0.615451 test loss: 1.864112
[Epoch 38; Iter     5/   55] train: loss: 0.0181634
[Epoch 38; Iter    35/   55] train: loss: 0.1335494
[Epoch 38] ogbg-molbbbp: 0.944738 val loss: 0.392856
[Epoch 38] ogbg-molbbbp: 0.657118 test loss: 1.507746
[Epoch 39; Iter    10/   55] train: loss: 0.0291297
[Epoch 39; Iter    40/   55] train: loss: 0.1772266
[Epoch 39] ogbg-molbbbp: 0.938265 val loss: 0.515258
[Epoch 39] ogbg-molbbbp: 0.597222 test loss: 2.036730
[Epoch 40; Iter    15/   55] train: loss: 0.0541401
[Epoch 40; Iter    45/   55] train: loss: 0.1243363
[Epoch 40] ogbg-molbbbp: 0.942647 val loss: 0.581597
[Epoch 40] ogbg-molbbbp: 0.636188 test loss: 1.933622
[Epoch 41; Iter    20/   55] train: loss: 0.0358981
[Epoch 41; Iter    50/   55] train: loss: 0.0982766
[Epoch 41] ogbg-molbbbp: 0.948621 val loss: 0.417863
[Epoch 41] ogbg-molbbbp: 0.657600 test loss: 1.801553
[Epoch 42; Iter    25/   55] train: loss: 0.0871777
[Epoch 42; Iter    55/   55] train: loss: 0.3587483
[Epoch 42] ogbg-molbbbp: 0.946729 val loss: 0.486081
[Epoch 42] ogbg-molbbbp: 0.631655 test loss: 2.006292
[Epoch 43; Iter    30/   55] train: loss: 0.1959598
[Epoch 43] ogbg-molbbbp: 0.950513 val loss: 0.387353
[Epoch 43] ogbg-molbbbp: 0.654514 test loss: 1.562255
[Epoch 44; Iter     5/   55] train: loss: 0.1856179
[Epoch 44; Iter    35/   55] train: loss: 0.0345799
[Epoch 44] ogbg-molbbbp: 0.943045 val loss: 0.571702
[Epoch 44] ogbg-molbbbp: 0.632523 test loss: 2.239787
[Epoch 45; Iter    10/   55] train: loss: 0.0172600
[Epoch 45; Iter    40/   55] train: loss: 0.0372873
[Epoch 45] ogbg-molbbbp: 0.937867 val loss: 0.548893
[Epoch 45] ogbg-molbbbp: 0.602141 test loss: 2.183696
[Epoch 46; Iter    15/   55] train: loss: 0.0413391
[Epoch 46; Iter    45/   55] train: loss: 0.0107692
[Epoch 46] ogbg-molbbbp: 0.953600 val loss: 0.412788
[Epoch 46] ogbg-molbbbp: 0.646412 test loss: 1.913067
[Epoch 47; Iter    20/   55] train: loss: 0.0617122
[Epoch 47; Iter    50/   55] train: loss: 0.0232092
[Epoch 47] ogbg-molbbbp: 0.938664 val loss: 0.609290
[Epoch 47] ogbg-molbbbp: 0.653260 test loss: 1.996084
[Epoch 48; Iter    25/   55] train: loss: 0.0101858
[Epoch 48; Iter    55/   55] train: loss: 0.0167385
[Epoch 48] ogbg-molbbbp: 0.935577 val loss: 0.591678
[Epoch 48] ogbg-molbbbp: 0.659915 test loss: 1.977236
[Epoch 49; Iter    30/   55] train: loss: 0.0080086
[Epoch 49] ogbg-molbbbp: 0.947426 val loss: 0.497029
[Epoch 49] ogbg-molbbbp: 0.682388 test loss: 1.938691
[Epoch 50; Iter     5/   55] train: loss: 0.1053946
[Epoch 50; Iter    35/   55] train: loss: 0.0272308
[Epoch 50] ogbg-molbbbp: 0.946829 val loss: 0.518718
[Epoch 50] ogbg-molbbbp: 0.631173 test loss: 2.079543
[Epoch 51; Iter    10/   55] train: loss: 0.0617884
[Epoch 51; Iter    40/   55] train: loss: 0.0470813
[Epoch 51] ogbg-molbbbp: 0.958877 val loss: 0.430385
[Epoch 51] ogbg-molbbbp: 0.667438 test loss: 1.661281
[Epoch 52; Iter    15/   55] train: loss: 0.0225836
[Epoch 52; Iter    45/   55] train: loss: 0.0213721
[Epoch 52] ogbg-molbbbp: 0.960072 val loss: 0.362934
[Epoch 52] ogbg-molbbbp: 0.626736 test loss: 1.988240
[Epoch 53; Iter    20/   55] train: loss: 0.0114567
[Epoch 53; Iter    50/   55] train: loss: 0.0179764
[Epoch 53] ogbg-molbbbp: 0.952803 val loss: 0.438555
[Epoch 53] ogbg-molbbbp: 0.676698 test loss: 1.466070
[Epoch 54; Iter    25/   55] train: loss: 0.0360178
[Epoch 54; Iter    55/   55] train: loss: 0.0070085
[Epoch 54] ogbg-molbbbp: 0.951907 val loss: 0.385496
[Epoch 54] ogbg-molbbbp: 0.665027 test loss: 1.609945
[Epoch 55; Iter    30/   55] train: loss: 0.0231309
[Epoch 55] ogbg-molbbbp: 0.949716 val loss: 0.450079
[Epoch 55] ogbg-molbbbp: 0.666184 test loss: 1.800942
[Epoch 56; Iter     5/   55] train: loss: 0.0332864
[Epoch 56; Iter    35/   55] train: loss: 0.0104138
[Epoch 56] ogbg-molbbbp: 0.949218 val loss: 0.439442
[Epoch 56] ogbg-molbbbp: 0.679398 test loss: 1.805860
[Epoch 57; Iter    10/   55] train: loss: 0.0043736
[Epoch 57; Iter    40/   55] train: loss: 0.0542175
[Epoch 57] ogbg-molbbbp: 0.931893 val loss: 0.572429
[Epoch 57] ogbg-molbbbp: 0.636092 test loss: 2.027585
[Epoch 58; Iter    15/   55] train: loss: 0.1038256
[Epoch 58; Iter    45/   55] train: loss: 0.0063326
[Epoch 58] ogbg-molbbbp: 0.942647 val loss: 0.594669
[Epoch 58] ogbg-molbbbp: 0.692130 test loss: 1.985550
[Epoch 59; Iter    20/   55] train: loss: 0.0171304
[Epoch 59; Iter    50/   55] train: loss: 0.0880786
[Epoch 59] ogbg-molbbbp: 0.937668 val loss: 0.665107
[Epoch 59] ogbg-molbbbp: 0.655382 test loss: 2.725449
[Epoch 60; Iter    25/   55] train: loss: 0.0062558
[Epoch 60; Iter    55/   55] train: loss: 0.3140031
[Epoch 60] ogbg-molbbbp: 0.925321 val loss: 0.836713
[Epoch 60] ogbg-molbbbp: 0.651331 test loss: 2.364736
[Epoch 61; Iter    30/   55] train: loss: 0.0868793
[Epoch 61] ogbg-molbbbp: 0.936174 val loss: 0.828769
[Epoch 61] ogbg-molbbbp: 0.631462 test loss: 3.058591
[Epoch 62; Iter     5/   55] train: loss: 0.1189522
[Epoch 62; Iter    35/   55] train: loss: 0.0747549
[Epoch 62] ogbg-molbbbp: 0.934083 val loss: 0.661248
[Epoch 62] ogbg-molbbbp: 0.625772 test loss: 2.531189
[Epoch 63; Iter    10/   55] train: loss: 0.0151562
[Epoch 63; Iter    40/   55] train: loss: 0.0126837
[Epoch 63] ogbg-molbbbp: 0.943642 val loss: 0.616453
[Epoch 63] ogbg-molbbbp: 0.675540 test loss: 2.120942
[Epoch 64; Iter    15/   55] train: loss: 0.0096101
[Epoch 64; Iter    45/   55] train: loss: 0.0311996
[Epoch 64] ogbg-molbbbp: 0.937369 val loss: 0.684089
[Epoch 64] ogbg-molbbbp: 0.652006 test loss: 2.338250
[Epoch 65; Iter    20/   55] train: loss: 0.0035279
[Epoch 65; Iter    50/   55] train: loss: 0.0018549
[Epoch 65] ogbg-molbbbp: 0.938962 val loss: 0.627161
[Epoch 65] ogbg-molbbbp: 0.651813 test loss: 2.193426
[Epoch 66; Iter    25/   55] train: loss: 0.0293190
[Epoch 66; Iter    55/   55] train: loss: 0.0037723
[Epoch 66] ogbg-molbbbp: 0.938962 val loss: 0.621490
[Epoch 66] ogbg-molbbbp: 0.642747 test loss: 2.333397
[Epoch 67; Iter    30/   55] train: loss: 0.0009917
[Epoch 67] ogbg-molbbbp: 0.940655 val loss: 0.599825
[Epoch 67] ogbg-molbbbp: 0.657504 test loss: 2.214246
[Epoch 68; Iter     5/   55] train: loss: 0.0022594
[Epoch 68; Iter    35/   55] train: loss: 0.0021364
[Epoch 68] ogbg-molbbbp: 0.934980 val loss: 0.661289
[Epoch 68] ogbg-molbbbp: 0.650752 test loss: 2.428994
[Epoch 69; Iter    10/   55] train: loss: 0.0014034
[Epoch 28] ogbg-molbbbp: 0.579379 test loss: 1.495471
[Epoch 29; Iter    20/   55] train: loss: 0.2163417
[Epoch 29; Iter    50/   55] train: loss: 0.3109863
[Epoch 29] ogbg-molbbbp: 0.935876 val loss: 0.375688
[Epoch 29] ogbg-molbbbp: 0.635224 test loss: 1.186556
[Epoch 30; Iter    25/   55] train: loss: 0.1114260
[Epoch 30; Iter    55/   55] train: loss: 0.0924565
[Epoch 30] ogbg-molbbbp: 0.930001 val loss: 0.889951
[Epoch 30] ogbg-molbbbp: 0.616416 test loss: 3.036672
[Epoch 31; Iter    30/   55] train: loss: 0.2866006
[Epoch 31] ogbg-molbbbp: 0.930897 val loss: 0.404104
[Epoch 31] ogbg-molbbbp: 0.582272 test loss: 2.084195
[Epoch 32; Iter     5/   55] train: loss: 0.1261245
[Epoch 32; Iter    35/   55] train: loss: 0.0811847
[Epoch 32] ogbg-molbbbp: 0.922434 val loss: 0.468523
[Epoch 32] ogbg-molbbbp: 0.623360 test loss: 1.851679
[Epoch 33; Iter    10/   55] train: loss: 0.0764597
[Epoch 33; Iter    40/   55] train: loss: 0.2365466
[Epoch 33] ogbg-molbbbp: 0.951210 val loss: 0.426067
[Epoch 33] ogbg-molbbbp: 0.632812 test loss: 1.582594
[Epoch 34; Iter    15/   55] train: loss: 0.1542063
[Epoch 34; Iter    45/   55] train: loss: 0.0520685
[Epoch 34] ogbg-molbbbp: 0.941352 val loss: 0.429540
[Epoch 34] ogbg-molbbbp: 0.631752 test loss: 2.448659
[Epoch 35; Iter    20/   55] train: loss: 0.1205393
[Epoch 35; Iter    50/   55] train: loss: 0.0893556
[Epoch 35] ogbg-molbbbp: 0.942945 val loss: 0.389841
[Epoch 35] ogbg-molbbbp: 0.657407 test loss: 2.121779
[Epoch 36; Iter    25/   55] train: loss: 0.1228632
[Epoch 36; Iter    55/   55] train: loss: 0.1301259
[Epoch 36] ogbg-molbbbp: 0.958678 val loss: 0.305409
[Epoch 36] ogbg-molbbbp: 0.653742 test loss: 1.762034
[Epoch 37; Iter    30/   55] train: loss: 0.3416861
[Epoch 37] ogbg-molbbbp: 0.939659 val loss: 0.390614
[Epoch 37] ogbg-molbbbp: 0.592110 test loss: 1.739988
[Epoch 38; Iter     5/   55] train: loss: 0.0802283
[Epoch 38; Iter    35/   55] train: loss: 0.0898112
[Epoch 38] ogbg-molbbbp: 0.954197 val loss: 0.379805
[Epoch 38] ogbg-molbbbp: 0.632234 test loss: 1.694379
[Epoch 39; Iter    10/   55] train: loss: 0.0331819
[Epoch 39; Iter    40/   55] train: loss: 0.1699203
[Epoch 39] ogbg-molbbbp: 0.963955 val loss: 0.296103
[Epoch 39] ogbg-molbbbp: 0.622685 test loss: 1.672532
[Epoch 40; Iter    15/   55] train: loss: 0.0477942
[Epoch 40; Iter    45/   55] train: loss: 0.0390712
[Epoch 40] ogbg-molbbbp: 0.955790 val loss: 0.373617
[Epoch 40] ogbg-molbbbp: 0.625482 test loss: 1.902706
[Epoch 41; Iter    20/   55] train: loss: 0.0438126
[Epoch 41; Iter    50/   55] train: loss: 0.0121033
[Epoch 41] ogbg-molbbbp: 0.951309 val loss: 0.359600
[Epoch 41] ogbg-molbbbp: 0.639371 test loss: 1.976757
[Epoch 42; Iter    25/   55] train: loss: 0.0448101
[Epoch 42; Iter    55/   55] train: loss: 0.1752379
[Epoch 42] ogbg-molbbbp: 0.944638 val loss: 0.441556
[Epoch 42] ogbg-molbbbp: 0.644965 test loss: 2.033095
[Epoch 43; Iter    30/   55] train: loss: 0.0194634
[Epoch 43] ogbg-molbbbp: 0.957782 val loss: 0.392913
[Epoch 43] ogbg-molbbbp: 0.640336 test loss: 2.645426
[Epoch 44; Iter     5/   55] train: loss: 0.0138261
[Epoch 44; Iter    35/   55] train: loss: 0.0510613
[Epoch 44] ogbg-molbbbp: 0.920641 val loss: 0.688158
[Epoch 44] ogbg-molbbbp: 0.622106 test loss: 1.972194
[Epoch 45; Iter    10/   55] train: loss: 0.0108822
[Epoch 45; Iter    40/   55] train: loss: 0.0123424
[Epoch 45] ogbg-molbbbp: 0.950911 val loss: 0.406365
[Epoch 45] ogbg-molbbbp: 0.617766 test loss: 1.989430
[Epoch 46; Iter    15/   55] train: loss: 0.0458216
[Epoch 46; Iter    45/   55] train: loss: 0.0157067
[Epoch 46] ogbg-molbbbp: 0.954595 val loss: 0.356321
[Epoch 46] ogbg-molbbbp: 0.617766 test loss: 2.012727
[Epoch 47; Iter    20/   55] train: loss: 0.0053287
[Epoch 47; Iter    50/   55] train: loss: 0.0194878
[Epoch 47] ogbg-molbbbp: 0.951409 val loss: 0.412719
[Epoch 47] ogbg-molbbbp: 0.662519 test loss: 1.930041
[Epoch 48; Iter    25/   55] train: loss: 0.0088857
[Epoch 48; Iter    55/   55] train: loss: 0.1531889
[Epoch 48] ogbg-molbbbp: 0.951110 val loss: 0.576263
[Epoch 48] ogbg-molbbbp: 0.614390 test loss: 2.542128
[Epoch 49; Iter    30/   55] train: loss: 0.0643044
[Epoch 49] ogbg-molbbbp: 0.954994 val loss: 0.455499
[Epoch 49] ogbg-molbbbp: 0.657022 test loss: 2.333254
[Epoch 50; Iter     5/   55] train: loss: 0.0101872
[Epoch 50; Iter    35/   55] train: loss: 0.0354057
[Epoch 50] ogbg-molbbbp: 0.959574 val loss: 0.345692
[Epoch 50] ogbg-molbbbp: 0.656250 test loss: 1.884297
[Epoch 51; Iter    10/   55] train: loss: 0.0151144
[Epoch 51; Iter    40/   55] train: loss: 0.0198212
[Epoch 51] ogbg-molbbbp: 0.949019 val loss: 0.430729
[Epoch 51] ogbg-molbbbp: 0.633873 test loss: 2.152547
[Epoch 52; Iter    15/   55] train: loss: 0.0074630
[Epoch 52; Iter    45/   55] train: loss: 0.0029237
[Epoch 52] ogbg-molbbbp: 0.950612 val loss: 0.423960
[Epoch 52] ogbg-molbbbp: 0.656443 test loss: 1.959196
[Epoch 53; Iter    20/   55] train: loss: 0.0210048
[Epoch 53; Iter    50/   55] train: loss: 0.0491849
[Epoch 53] ogbg-molbbbp: 0.958180 val loss: 0.578426
[Epoch 53] ogbg-molbbbp: 0.643036 test loss: 2.587626
[Epoch 54; Iter    25/   55] train: loss: 0.0200238
[Epoch 54; Iter    55/   55] train: loss: 0.2787376
[Epoch 54] ogbg-molbbbp: 0.947028 val loss: 0.506864
[Epoch 54] ogbg-molbbbp: 0.609182 test loss: 2.422032
[Epoch 55; Iter    30/   55] train: loss: 0.0091418
[Epoch 55] ogbg-molbbbp: 0.932092 val loss: 0.769075
[Epoch 55] ogbg-molbbbp: 0.630305 test loss: 2.587114
[Epoch 56; Iter     5/   55] train: loss: 0.0290035
[Epoch 56; Iter    35/   55] train: loss: 0.0174812
[Epoch 56] ogbg-molbbbp: 0.948223 val loss: 0.513851
[Epoch 56] ogbg-molbbbp: 0.631076 test loss: 2.510753
[Epoch 57; Iter    10/   55] train: loss: 0.0177651
[Epoch 57; Iter    40/   55] train: loss: 0.0315328
[Epoch 57] ogbg-molbbbp: 0.949816 val loss: 0.499271
[Epoch 57] ogbg-molbbbp: 0.632330 test loss: 2.845026
[Epoch 58; Iter    15/   55] train: loss: 0.0136254
[Epoch 58; Iter    45/   55] train: loss: 0.0300320
[Epoch 58] ogbg-molbbbp: 0.943543 val loss: 0.468238
[Epoch 58] ogbg-molbbbp: 0.630305 test loss: 2.248484
[Epoch 59; Iter    20/   55] train: loss: 0.0755203
[Epoch 59; Iter    50/   55] train: loss: 0.0194062
[Epoch 59] ogbg-molbbbp: 0.952604 val loss: 0.473750
[Epoch 59] ogbg-molbbbp: 0.618731 test loss: 2.773123
[Epoch 60; Iter    25/   55] train: loss: 0.0014998
[Epoch 60; Iter    55/   55] train: loss: 0.2867892
[Epoch 60] ogbg-molbbbp: 0.949418 val loss: 0.523021
[Epoch 60] ogbg-molbbbp: 0.627411 test loss: 3.079591
[Epoch 61; Iter    30/   55] train: loss: 0.0636882
[Epoch 61] ogbg-molbbbp: 0.950314 val loss: 0.550510
[Epoch 61] ogbg-molbbbp: 0.617959 test loss: 3.142292
[Epoch 62; Iter     5/   55] train: loss: 0.0969604
[Epoch 62; Iter    35/   55] train: loss: 0.0193283
[Epoch 62] ogbg-molbbbp: 0.949019 val loss: 0.526828
[Epoch 62] ogbg-molbbbp: 0.662133 test loss: 2.672759
[Epoch 63; Iter    10/   55] train: loss: 0.0056542
[Epoch 63; Iter    40/   55] train: loss: 0.0178707
[Epoch 63] ogbg-molbbbp: 0.951907 val loss: 0.518822
[Epoch 63] ogbg-molbbbp: 0.628665 test loss: 2.960256
[Epoch 64; Iter    15/   55] train: loss: 0.0886735
[Epoch 64; Iter    45/   55] train: loss: 0.0254885
[Epoch 64] ogbg-molbbbp: 0.952703 val loss: 0.468046
[Epoch 64] ogbg-molbbbp: 0.632234 test loss: 2.504503
[Epoch 65; Iter    20/   55] train: loss: 0.0015166
[Epoch 65; Iter    50/   55] train: loss: 0.0041641
[Epoch 65] ogbg-molbbbp: 0.959673 val loss: 0.505137
[Epoch 65] ogbg-molbbbp: 0.635127 test loss: 3.199580
[Epoch 66; Iter    25/   55] train: loss: 0.0032514
[Epoch 66; Iter    55/   55] train: loss: 0.0052657
[Epoch 66] ogbg-molbbbp: 0.956388 val loss: 0.582476
[Epoch 66] ogbg-molbbbp: 0.629919 test loss: 2.975149
[Epoch 67; Iter    30/   55] train: loss: 0.0026287
[Epoch 67] ogbg-molbbbp: 0.953799 val loss: 0.533357
[Epoch 67] ogbg-molbbbp: 0.617091 test loss: 3.295526
[Epoch 68; Iter     5/   55] train: loss: 0.0009773
[Epoch 68; Iter    35/   55] train: loss: 0.0006549
[Epoch 68] ogbg-molbbbp: 0.947028 val loss: 0.523326
[Epoch 68] ogbg-molbbbp: 0.629823 test loss: 2.630405
[Epoch 69; Iter    10/   55] train: loss: 0.0042980
[Epoch 28] ogbg-molbbbp: 0.609568 test loss: 1.941109
[Epoch 29; Iter    20/   55] train: loss: 0.1966651
[Epoch 29; Iter    50/   55] train: loss: 0.2373475
[Epoch 29] ogbg-molbbbp: 0.847556 val loss: 4.015496
[Epoch 29] ogbg-molbbbp: 0.541956 test loss: 11.080541
[Epoch 30; Iter    25/   55] train: loss: 0.1775158
[Epoch 30; Iter    55/   55] train: loss: 0.5095639
[Epoch 30] ogbg-molbbbp: 0.847456 val loss: 1.056126
[Epoch 30] ogbg-molbbbp: 0.564140 test loss: 2.277003
[Epoch 31; Iter    30/   55] train: loss: 0.3230152
[Epoch 31] ogbg-molbbbp: 0.845465 val loss: 0.858790
[Epoch 31] ogbg-molbbbp: 0.574653 test loss: 1.303995
[Epoch 32; Iter     5/   55] train: loss: 0.1639545
[Epoch 32; Iter    35/   55] train: loss: 0.0989381
[Epoch 32] ogbg-molbbbp: 0.882605 val loss: 1.216226
[Epoch 32] ogbg-molbbbp: 0.584298 test loss: 2.353462
[Epoch 33; Iter    10/   55] train: loss: 0.1375972
[Epoch 33; Iter    40/   55] train: loss: 0.3833679
[Epoch 33] ogbg-molbbbp: 0.933486 val loss: 0.329794
[Epoch 33] ogbg-molbbbp: 0.633777 test loss: 1.066903
[Epoch 34; Iter    15/   55] train: loss: 0.1661084
[Epoch 34; Iter    45/   55] train: loss: 0.1668575
[Epoch 34] ogbg-molbbbp: 0.946331 val loss: 0.352688
[Epoch 34] ogbg-molbbbp: 0.621624 test loss: 1.666153
[Epoch 35; Iter    20/   55] train: loss: 0.2312445
[Epoch 35; Iter    50/   55] train: loss: 0.2564323
[Epoch 35] ogbg-molbbbp: 0.930200 val loss: 0.389479
[Epoch 35] ogbg-molbbbp: 0.632716 test loss: 1.423261
[Epoch 36; Iter    25/   55] train: loss: 0.2772269
[Epoch 36; Iter    55/   55] train: loss: 0.0568907
[Epoch 36] ogbg-molbbbp: 0.936274 val loss: 0.356509
[Epoch 36] ogbg-molbbbp: 0.615162 test loss: 1.471329
[Epoch 37; Iter    30/   55] train: loss: 0.0673070
[Epoch 37] ogbg-molbbbp: 0.794982 val loss: 1.126322
[Epoch 37] ogbg-molbbbp: 0.585262 test loss: 1.567543
[Epoch 38; Iter     5/   55] train: loss: 0.0817446
[Epoch 38; Iter    35/   55] train: loss: 0.0399252
[Epoch 38] ogbg-molbbbp: 0.929503 val loss: 0.616788
[Epoch 38] ogbg-molbbbp: 0.586323 test loss: 1.899717
[Epoch 39; Iter    10/   55] train: loss: 0.0900694
[Epoch 39; Iter    40/   55] train: loss: 0.0913999
[Epoch 39] ogbg-molbbbp: 0.927810 val loss: 0.379122
[Epoch 39] ogbg-molbbbp: 0.619695 test loss: 1.203231
[Epoch 40; Iter    15/   55] train: loss: 0.0377925
[Epoch 40; Iter    45/   55] train: loss: 0.1276795
[Epoch 40] ogbg-molbbbp: 0.928707 val loss: 0.479348
[Epoch 40] ogbg-molbbbp: 0.567708 test loss: 2.590054
[Epoch 41; Iter    20/   55] train: loss: 0.0124389
[Epoch 41; Iter    50/   55] train: loss: 0.0317541
[Epoch 41] ogbg-molbbbp: 0.908294 val loss: 0.553716
[Epoch 41] ogbg-molbbbp: 0.607253 test loss: 1.734811
[Epoch 42; Iter    25/   55] train: loss: 0.0373389
[Epoch 42; Iter    55/   55] train: loss: 0.2047121
[Epoch 42] ogbg-molbbbp: 0.904511 val loss: 0.688312
[Epoch 42] ogbg-molbbbp: 0.539062 test loss: 2.375356
[Epoch 43; Iter    30/   55] train: loss: 0.0308947
[Epoch 43] ogbg-molbbbp: 0.947526 val loss: 0.564078
[Epoch 43] ogbg-molbbbp: 0.594522 test loss: 2.123703
[Epoch 44; Iter     5/   55] train: loss: 0.0302501
[Epoch 44; Iter    35/   55] train: loss: 0.0607158
[Epoch 44] ogbg-molbbbp: 0.948023 val loss: 0.340210
[Epoch 44] ogbg-molbbbp: 0.636478 test loss: 1.799044
[Epoch 45; Iter    10/   55] train: loss: 0.0669193
[Epoch 45; Iter    40/   55] train: loss: 0.0238790
[Epoch 45] ogbg-molbbbp: 0.930499 val loss: 0.501497
[Epoch 45] ogbg-molbbbp: 0.619985 test loss: 2.098394
[Epoch 46; Iter    15/   55] train: loss: 0.0197979
[Epoch 46; Iter    45/   55] train: loss: 0.0180095
[Epoch 46] ogbg-molbbbp: 0.942647 val loss: 0.386391
[Epoch 46] ogbg-molbbbp: 0.614873 test loss: 1.901269
[Epoch 47; Iter    20/   55] train: loss: 0.0088118
[Epoch 47; Iter    50/   55] train: loss: 0.0864944
[Epoch 47] ogbg-molbbbp: 0.929901 val loss: 0.552654
[Epoch 47] ogbg-molbbbp: 0.620467 test loss: 2.097435
[Epoch 48; Iter    25/   55] train: loss: 0.0473342
[Epoch 48; Iter    55/   55] train: loss: 0.0045006
[Epoch 48] ogbg-molbbbp: 0.911879 val loss: 0.688484
[Epoch 48] ogbg-molbbbp: 0.617284 test loss: 2.170879
[Epoch 49; Iter    30/   55] train: loss: 0.0129234
[Epoch 49] ogbg-molbbbp: 0.919845 val loss: 0.826399
[Epoch 49] ogbg-molbbbp: 0.632330 test loss: 2.642013
[Epoch 50; Iter     5/   55] train: loss: 0.0724732
[Epoch 50; Iter    35/   55] train: loss: 0.0062340
[Epoch 50] ogbg-molbbbp: 0.945335 val loss: 0.424702
[Epoch 50] ogbg-molbbbp: 0.575424 test loss: 2.141917
[Epoch 51; Iter    10/   55] train: loss: 0.0217625
[Epoch 51; Iter    40/   55] train: loss: 0.0054575
[Epoch 51] ogbg-molbbbp: 0.919944 val loss: 0.533270
[Epoch 51] ogbg-molbbbp: 0.589603 test loss: 1.906631
[Epoch 52; Iter    15/   55] train: loss: 0.0234964
[Epoch 52; Iter    45/   55] train: loss: 0.0127362
[Epoch 52] ogbg-molbbbp: 0.937867 val loss: 0.776606
[Epoch 52] ogbg-molbbbp: 0.633970 test loss: 2.424090
[Epoch 53; Iter    20/   55] train: loss: 0.0154227
[Epoch 53; Iter    50/   55] train: loss: 0.0022249
[Epoch 53] ogbg-molbbbp: 0.919546 val loss: 0.616516
[Epoch 53] ogbg-molbbbp: 0.619020 test loss: 1.963536
[Epoch 54; Iter    25/   55] train: loss: 0.0174677
[Epoch 54; Iter    55/   55] train: loss: 0.1740740
[Epoch 54] ogbg-molbbbp: 0.919148 val loss: 0.623848
[Epoch 54] ogbg-molbbbp: 0.597512 test loss: 2.283380
[Epoch 55; Iter    30/   55] train: loss: 0.0066484
[Epoch 55] ogbg-molbbbp: 0.960570 val loss: 0.466047
[Epoch 55] ogbg-molbbbp: 0.588059 test loss: 2.863680
[Epoch 56; Iter     5/   55] train: loss: 0.0015316
[Epoch 56; Iter    35/   55] train: loss: 0.0366375
[Epoch 56] ogbg-molbbbp: 0.956487 val loss: 0.625975
[Epoch 56] ogbg-molbbbp: 0.648052 test loss: 2.388896
[Epoch 57; Iter    10/   55] train: loss: 0.0125172
[Epoch 57; Iter    40/   55] train: loss: 0.0108447
[Epoch 57] ogbg-molbbbp: 0.898636 val loss: 0.966206
[Epoch 57] ogbg-molbbbp: 0.623457 test loss: 2.109431
[Epoch 58; Iter    15/   55] train: loss: 0.0469321
[Epoch 58; Iter    45/   55] train: loss: 0.0206351
[Epoch 58] ogbg-molbbbp: 0.954496 val loss: 0.483900
[Epoch 58] ogbg-molbbbp: 0.647569 test loss: 2.476561
[Epoch 59; Iter    20/   55] train: loss: 0.0007700
[Epoch 59; Iter    50/   55] train: loss: 0.0208953
[Epoch 59] ogbg-molbbbp: 0.945235 val loss: 0.509639
[Epoch 59] ogbg-molbbbp: 0.635513 test loss: 2.028891
[Epoch 60; Iter    25/   55] train: loss: 0.0033629
[Epoch 60; Iter    55/   55] train: loss: 0.0012601
[Epoch 60] ogbg-molbbbp: 0.953500 val loss: 0.661032
[Epoch 60] ogbg-molbbbp: 0.619309 test loss: 3.325650
[Epoch 61; Iter    30/   55] train: loss: 0.0448760
[Epoch 61] ogbg-molbbbp: 0.944339 val loss: 0.469566
[Epoch 61] ogbg-molbbbp: 0.576678 test loss: 2.569602
[Epoch 62; Iter     5/   55] train: loss: 0.0029120
[Epoch 62; Iter    35/   55] train: loss: 0.0212026
[Epoch 62] ogbg-molbbbp: 0.930499 val loss: 0.710069
[Epoch 62] ogbg-molbbbp: 0.576292 test loss: 3.124645
[Epoch 63; Iter    10/   55] train: loss: 0.0260876
[Epoch 63; Iter    40/   55] train: loss: 0.0034783
[Epoch 63] ogbg-molbbbp: 0.955392 val loss: 0.512502
[Epoch 63] ogbg-molbbbp: 0.637635 test loss: 2.306574
[Epoch 64; Iter    15/   55] train: loss: 0.0027929
[Epoch 64; Iter    45/   55] train: loss: 0.0027721
[Epoch 64] ogbg-molbbbp: 0.953002 val loss: 0.486616
[Epoch 64] ogbg-molbbbp: 0.602527 test loss: 2.631264
[Epoch 65; Iter    20/   55] train: loss: 0.0013991
[Epoch 65; Iter    50/   55] train: loss: 0.0011364
[Epoch 65] ogbg-molbbbp: 0.955193 val loss: 0.632457
[Epoch 65] ogbg-molbbbp: 0.615837 test loss: 2.663561
[Epoch 66; Iter    25/   55] train: loss: 0.0025922
[Epoch 66; Iter    55/   55] train: loss: 0.0012516
[Epoch 66] ogbg-molbbbp: 0.949816 val loss: 0.513770
[Epoch 66] ogbg-molbbbp: 0.616802 test loss: 2.580418
[Epoch 67; Iter    30/   55] train: loss: 0.0012751
[Epoch 67] ogbg-molbbbp: 0.948521 val loss: 0.523416
[Epoch 67] ogbg-molbbbp: 0.609375 test loss: 2.684992
[Epoch 68; Iter     5/   55] train: loss: 0.0020659
[Epoch 68; Iter    35/   55] train: loss: 0.0010779
[Epoch 68] ogbg-molbbbp: 0.953600 val loss: 0.563340
[Epoch 68] ogbg-molbbbp: 0.620853 test loss: 2.623402
[Epoch 69; Iter    10/   55] train: loss: 0.0008045
[Epoch 28] ogbg-molbbbp: 0.627990 test loss: 1.350391
[Epoch 29; Iter    20/   55] train: loss: 0.2570800
[Epoch 29; Iter    50/   55] train: loss: 0.2465430
[Epoch 29] ogbg-molbbbp: 0.934581 val loss: 0.368306
[Epoch 29] ogbg-molbbbp: 0.613137 test loss: 1.278202
[Epoch 30; Iter    25/   55] train: loss: 0.2236568
[Epoch 30; Iter    55/   55] train: loss: 0.1883753
[Epoch 30] ogbg-molbbbp: 0.935577 val loss: 0.683367
[Epoch 30] ogbg-molbbbp: 0.590953 test loss: 1.731706
[Epoch 31; Iter    30/   55] train: loss: 0.5534188
[Epoch 31] ogbg-molbbbp: 0.925022 val loss: 0.577162
[Epoch 31] ogbg-molbbbp: 0.603009 test loss: 2.596647
[Epoch 32; Iter     5/   55] train: loss: 0.1117118
[Epoch 32; Iter    35/   55] train: loss: 0.1376458
[Epoch 32] ogbg-molbbbp: 0.939062 val loss: 0.409267
[Epoch 32] ogbg-molbbbp: 0.646219 test loss: 4.154983
[Epoch 33; Iter    10/   55] train: loss: 0.1388225
[Epoch 33; Iter    40/   55] train: loss: 0.2242045
[Epoch 33] ogbg-molbbbp: 0.934083 val loss: 0.703308
[Epoch 33] ogbg-molbbbp: 0.672164 test loss: 1.944780
[Epoch 34; Iter    15/   55] train: loss: 0.2262014
[Epoch 34; Iter    45/   55] train: loss: 0.1675476
[Epoch 34] ogbg-molbbbp: 0.941452 val loss: 0.399221
[Epoch 34] ogbg-molbbbp: 0.671200 test loss: 1.243159
[Epoch 35; Iter    20/   55] train: loss: 0.0820011
[Epoch 35; Iter    50/   55] train: loss: 0.3094064
[Epoch 35] ogbg-molbbbp: 0.927810 val loss: 0.562779
[Epoch 35] ogbg-molbbbp: 0.671875 test loss: 1.610857
[Epoch 36; Iter    25/   55] train: loss: 0.2428542
[Epoch 36; Iter    55/   55] train: loss: 0.0229417
[Epoch 36] ogbg-molbbbp: 0.950115 val loss: 0.347988
[Epoch 36] ogbg-molbbbp: 0.679784 test loss: 1.385068
[Epoch 37; Iter    30/   55] train: loss: 0.1133794
[Epoch 37] ogbg-molbbbp: 0.947227 val loss: 0.307167
[Epoch 37] ogbg-molbbbp: 0.666956 test loss: 1.190643
[Epoch 38; Iter     5/   55] train: loss: 0.0508323
[Epoch 38; Iter    35/   55] train: loss: 0.1709416
[Epoch 38] ogbg-molbbbp: 0.938465 val loss: 0.385006
[Epoch 38] ogbg-molbbbp: 0.661169 test loss: 1.395038
[Epoch 39; Iter    10/   55] train: loss: 0.0763301
[Epoch 39; Iter    40/   55] train: loss: 0.0481655
[Epoch 39] ogbg-molbbbp: 0.946231 val loss: 0.364540
[Epoch 39] ogbg-molbbbp: 0.667245 test loss: 1.328172
[Epoch 40; Iter    15/   55] train: loss: 0.1092143
[Epoch 40; Iter    45/   55] train: loss: 0.0974406
[Epoch 40] ogbg-molbbbp: 0.949517 val loss: 0.463016
[Epoch 40] ogbg-molbbbp: 0.634742 test loss: 2.377997
[Epoch 41; Iter    20/   55] train: loss: 0.0591812
[Epoch 41; Iter    50/   55] train: loss: 0.0196327
[Epoch 41] ogbg-molbbbp: 0.936274 val loss: 0.901925
[Epoch 41] ogbg-molbbbp: 0.655478 test loss: 1.854069
[Epoch 42; Iter    25/   55] train: loss: 0.0461879
[Epoch 42; Iter    55/   55] train: loss: 0.0896333
[Epoch 42] ogbg-molbbbp: 0.941750 val loss: 0.479597
[Epoch 42] ogbg-molbbbp: 0.631173 test loss: 1.763542
[Epoch 43; Iter    30/   55] train: loss: 0.0755578
[Epoch 43] ogbg-molbbbp: 0.941850 val loss: 0.452632
[Epoch 43] ogbg-molbbbp: 0.647859 test loss: 1.939829
[Epoch 44; Iter     5/   55] train: loss: 0.0219770
[Epoch 44; Iter    35/   55] train: loss: 0.0895395
[Epoch 44] ogbg-molbbbp: 0.956985 val loss: 0.445335
[Epoch 44] ogbg-molbbbp: 0.650270 test loss: 2.125449
[Epoch 45; Iter    10/   55] train: loss: 0.0631590
[Epoch 45; Iter    40/   55] train: loss: 0.0179745
[Epoch 45] ogbg-molbbbp: 0.956985 val loss: 0.421391
[Epoch 45] ogbg-molbbbp: 0.668789 test loss: 1.931565
[Epoch 46; Iter    15/   55] train: loss: 0.1052639
[Epoch 46; Iter    45/   55] train: loss: 0.0484893
[Epoch 46] ogbg-molbbbp: 0.936772 val loss: 0.644623
[Epoch 46] ogbg-molbbbp: 0.643326 test loss: 2.263898
[Epoch 47; Iter    20/   55] train: loss: 0.0379508
[Epoch 47; Iter    50/   55] train: loss: 0.0432880
[Epoch 47] ogbg-molbbbp: 0.957483 val loss: 0.416581
[Epoch 47] ogbg-molbbbp: 0.661555 test loss: 2.179244
[Epoch 48; Iter    25/   55] train: loss: 0.0554988
[Epoch 48; Iter    55/   55] train: loss: 0.4192612
[Epoch 48] ogbg-molbbbp: 0.946430 val loss: 0.398478
[Epoch 48] ogbg-molbbbp: 0.684510 test loss: 1.490319
[Epoch 49; Iter    30/   55] train: loss: 0.0467004
[Epoch 49] ogbg-molbbbp: 0.943244 val loss: 0.547232
[Epoch 49] ogbg-molbbbp: 0.699267 test loss: 1.886084
[Epoch 50; Iter     5/   55] train: loss: 0.0131481
[Epoch 50; Iter    35/   55] train: loss: 0.0685499
[Epoch 50] ogbg-molbbbp: 0.963158 val loss: 0.370152
[Epoch 50] ogbg-molbbbp: 0.655575 test loss: 2.157013
[Epoch 51; Iter    10/   55] train: loss: 0.0250640
[Epoch 51; Iter    40/   55] train: loss: 0.0181126
[Epoch 51] ogbg-molbbbp: 0.962561 val loss: 0.366331
[Epoch 51] ogbg-molbbbp: 0.674576 test loss: 1.922116
[Epoch 52; Iter    15/   55] train: loss: 0.0339325
[Epoch 52; Iter    45/   55] train: loss: 0.0096952
[Epoch 52] ogbg-molbbbp: 0.928109 val loss: 0.594453
[Epoch 52] ogbg-molbbbp: 0.669560 test loss: 1.789458
[Epoch 53; Iter    20/   55] train: loss: 0.1871931
[Epoch 53; Iter    50/   55] train: loss: 0.0108375
[Epoch 53] ogbg-molbbbp: 0.951011 val loss: 0.395920
[Epoch 53] ogbg-molbbbp: 0.676215 test loss: 1.890917
[Epoch 54; Iter    25/   55] train: loss: 0.0261743
[Epoch 54; Iter    55/   55] train: loss: 0.0143037
[Epoch 54] ogbg-molbbbp: 0.942945 val loss: 0.546350
[Epoch 54] ogbg-molbbbp: 0.661362 test loss: 2.174120
[Epoch 55; Iter    30/   55] train: loss: 0.0486451
[Epoch 55] ogbg-molbbbp: 0.949119 val loss: 0.481235
[Epoch 55] ogbg-molbbbp: 0.666570 test loss: 2.194202
[Epoch 56; Iter     5/   55] train: loss: 0.0982137
[Epoch 56; Iter    35/   55] train: loss: 0.0834061
[Epoch 56] ogbg-molbbbp: 0.962661 val loss: 0.386978
[Epoch 56] ogbg-molbbbp: 0.669946 test loss: 2.408369
[Epoch 57; Iter    10/   55] train: loss: 0.2176667
[Epoch 57; Iter    40/   55] train: loss: 0.0824123
[Epoch 57] ogbg-molbbbp: 0.962959 val loss: 0.378482
[Epoch 57] ogbg-molbbbp: 0.692515 test loss: 1.931179
[Epoch 58; Iter    15/   55] train: loss: 0.0081738
[Epoch 58; Iter    45/   55] train: loss: 0.0722482
[Epoch 58] ogbg-molbbbp: 0.946928 val loss: 0.784050
[Epoch 58] ogbg-molbbbp: 0.673032 test loss: 2.480010
[Epoch 59; Iter    20/   55] train: loss: 0.0537805
[Epoch 59; Iter    50/   55] train: loss: 0.0109475
[Epoch 59] ogbg-molbbbp: 0.961665 val loss: 0.381563
[Epoch 59] ogbg-molbbbp: 0.675733 test loss: 1.888672
[Epoch 60; Iter    25/   55] train: loss: 0.0112227
[Epoch 60; Iter    55/   55] train: loss: 0.0727219
[Epoch 60] ogbg-molbbbp: 0.964652 val loss: 0.430381
[Epoch 60] ogbg-molbbbp: 0.672647 test loss: 2.504265
[Epoch 61; Iter    30/   55] train: loss: 0.0855998
[Epoch 61] ogbg-molbbbp: 0.956388 val loss: 0.444802
[Epoch 61] ogbg-molbbbp: 0.672454 test loss: 2.226289
[Epoch 62; Iter     5/   55] train: loss: 0.0106924
[Epoch 62; Iter    35/   55] train: loss: 0.0032979
[Epoch 62] ogbg-molbbbp: 0.957582 val loss: 0.439371
[Epoch 62] ogbg-molbbbp: 0.682388 test loss: 2.323933
[Epoch 63; Iter    10/   55] train: loss: 0.0133837
[Epoch 63; Iter    40/   55] train: loss: 0.0301062
[Epoch 63] ogbg-molbbbp: 0.958877 val loss: 0.446045
[Epoch 63] ogbg-molbbbp: 0.683160 test loss: 2.389169
[Epoch 64; Iter    15/   55] train: loss: 0.0022888
[Epoch 64; Iter    45/   55] train: loss: 0.0162397
[Epoch 64] ogbg-molbbbp: 0.957682 val loss: 0.443330
[Epoch 64] ogbg-molbbbp: 0.687211 test loss: 2.195964
[Epoch 65; Iter    20/   55] train: loss: 0.0009024
[Epoch 65; Iter    50/   55] train: loss: 0.0024131
[Epoch 65] ogbg-molbbbp: 0.957284 val loss: 0.455417
[Epoch 65] ogbg-molbbbp: 0.679012 test loss: 2.365147
[Epoch 66; Iter    25/   55] train: loss: 0.0010999
[Epoch 66; Iter    55/   55] train: loss: 0.0120405
[Epoch 66] ogbg-molbbbp: 0.960470 val loss: 0.453104
[Epoch 66] ogbg-molbbbp: 0.684896 test loss: 2.438441
[Epoch 67; Iter    30/   55] train: loss: 0.0106967
[Epoch 67] ogbg-molbbbp: 0.960868 val loss: 0.443417
[Epoch 67] ogbg-molbbbp: 0.680266 test loss: 2.400657
[Epoch 68; Iter     5/   55] train: loss: 0.0014552
[Epoch 68; Iter    35/   55] train: loss: 0.0003544
[Epoch 68] ogbg-molbbbp: 0.960072 val loss: 0.454606
[Epoch 68] ogbg-molbbbp: 0.680170 test loss: 2.441771
[Epoch 69; Iter    10/   55] train: loss: 0.0007382
[Epoch 28] ogbg-molbbbp: 0.639082 test loss: 1.685814
[Epoch 29; Iter    20/   55] train: loss: 0.2160431
[Epoch 29; Iter    50/   55] train: loss: 0.3223197
[Epoch 29] ogbg-molbbbp: 0.792592 val loss: 1.171409
[Epoch 29] ogbg-molbbbp: 0.584684 test loss: 1.557525
[Epoch 30; Iter    25/   55] train: loss: 0.3051260
[Epoch 30; Iter    55/   55] train: loss: 0.4076111
[Epoch 30] ogbg-molbbbp: 0.910883 val loss: 0.524777
[Epoch 30] ogbg-molbbbp: 0.648630 test loss: 1.588614
[Epoch 31; Iter    30/   55] train: loss: 0.3786654
[Epoch 31] ogbg-molbbbp: 0.886588 val loss: 0.614010
[Epoch 31] ogbg-molbbbp: 0.612365 test loss: 1.637463
[Epoch 32; Iter     5/   55] train: loss: 0.2527352
[Epoch 32; Iter    35/   55] train: loss: 0.2553605
[Epoch 32] ogbg-molbbbp: 0.945136 val loss: 0.776377
[Epoch 32] ogbg-molbbbp: 0.646798 test loss: 2.895827
[Epoch 33; Iter    10/   55] train: loss: 0.1743269
[Epoch 33; Iter    40/   55] train: loss: 0.2293147
[Epoch 33] ogbg-molbbbp: 0.948820 val loss: 0.348692
[Epoch 33] ogbg-molbbbp: 0.650270 test loss: 2.076519
[Epoch 34; Iter    15/   55] train: loss: 0.0613008
[Epoch 34; Iter    45/   55] train: loss: 0.2200177
[Epoch 34] ogbg-molbbbp: 0.921338 val loss: 1.040257
[Epoch 34] ogbg-molbbbp: 0.621238 test loss: 2.349774
[Epoch 35; Iter    20/   55] train: loss: 0.1243130
[Epoch 35; Iter    50/   55] train: loss: 0.1268224
[Epoch 35] ogbg-molbbbp: 0.930499 val loss: 0.591316
[Epoch 35] ogbg-molbbbp: 0.654900 test loss: 2.106783
[Epoch 36; Iter    25/   55] train: loss: 0.1265568
[Epoch 36; Iter    55/   55] train: loss: 0.5249084
[Epoch 36] ogbg-molbbbp: 0.945932 val loss: 0.506088
[Epoch 36] ogbg-molbbbp: 0.662712 test loss: 1.617074
[Epoch 37; Iter    30/   55] train: loss: 0.1009810
[Epoch 37] ogbg-molbbbp: 0.921936 val loss: 0.771920
[Epoch 37] ogbg-molbbbp: 0.622975 test loss: 1.585976
[Epoch 38; Iter     5/   55] train: loss: 0.0422933
[Epoch 38; Iter    35/   55] train: loss: 0.0944226
[Epoch 38] ogbg-molbbbp: 0.928109 val loss: 0.736414
[Epoch 38] ogbg-molbbbp: 0.620563 test loss: 2.120360
[Epoch 39; Iter    10/   55] train: loss: 0.0353384
[Epoch 39; Iter    40/   55] train: loss: 0.2038993
[Epoch 39] ogbg-molbbbp: 0.918949 val loss: 0.472772
[Epoch 39] ogbg-molbbbp: 0.628279 test loss: 1.621502
[Epoch 40; Iter    15/   55] train: loss: 0.0284537
[Epoch 40; Iter    45/   55] train: loss: 0.0943595
[Epoch 40] ogbg-molbbbp: 0.953799 val loss: 0.449329
[Epoch 40] ogbg-molbbbp: 0.653646 test loss: 1.590757
[Epoch 41; Iter    20/   55] train: loss: 0.0074851
[Epoch 41; Iter    50/   55] train: loss: 0.0385047
[Epoch 41] ogbg-molbbbp: 0.944140 val loss: 0.455004
[Epoch 41] ogbg-molbbbp: 0.608989 test loss: 2.032322
[Epoch 42; Iter    25/   55] train: loss: 0.0161033
[Epoch 42; Iter    55/   55] train: loss: 0.0212587
[Epoch 42] ogbg-molbbbp: 0.901125 val loss: 1.901938
[Epoch 42] ogbg-molbbbp: 0.667342 test loss: 2.586700
[Epoch 43; Iter    30/   55] train: loss: 0.1275898
[Epoch 43] ogbg-molbbbp: 0.947227 val loss: 0.938651
[Epoch 43] ogbg-molbbbp: 0.617959 test loss: 2.253601
[Epoch 44; Iter     5/   55] train: loss: 0.2820206
[Epoch 44; Iter    35/   55] train: loss: 0.0190306
[Epoch 44] ogbg-molbbbp: 0.937568 val loss: 0.707175
[Epoch 44] ogbg-molbbbp: 0.648727 test loss: 2.060752
[Epoch 45; Iter    10/   55] train: loss: 0.0105105
[Epoch 45; Iter    40/   55] train: loss: 0.0181758
[Epoch 45] ogbg-molbbbp: 0.927113 val loss: 0.908102
[Epoch 45] ogbg-molbbbp: 0.638214 test loss: 2.615594
[Epoch 46; Iter    15/   55] train: loss: 0.0140012
[Epoch 46; Iter    45/   55] train: loss: 0.0644070
[Epoch 46] ogbg-molbbbp: 0.928209 val loss: 0.556073
[Epoch 46] ogbg-molbbbp: 0.622203 test loss: 2.097833
[Epoch 47; Iter    20/   55] train: loss: 0.0261683
[Epoch 47; Iter    50/   55] train: loss: 0.0955306
[Epoch 47] ogbg-molbbbp: 0.904610 val loss: 1.527872
[Epoch 47] ogbg-molbbbp: 0.672936 test loss: 2.312982
[Epoch 48; Iter    25/   55] train: loss: 0.0143985
[Epoch 48; Iter    55/   55] train: loss: 0.0024839
[Epoch 48] ogbg-molbbbp: 0.937668 val loss: 0.596677
[Epoch 48] ogbg-molbbbp: 0.638985 test loss: 1.974376
[Epoch 49; Iter    30/   55] train: loss: 0.0052297
[Epoch 49] ogbg-molbbbp: 0.939460 val loss: 0.493026
[Epoch 49] ogbg-molbbbp: 0.658854 test loss: 2.284835
[Epoch 50; Iter     5/   55] train: loss: 0.0316512
[Epoch 50; Iter    35/   55] train: loss: 0.0068310
[Epoch 50] ogbg-molbbbp: 0.943344 val loss: 0.902322
[Epoch 50] ogbg-molbbbp: 0.608025 test loss: 2.280657
[Epoch 51; Iter    10/   55] train: loss: 0.0126819
[Epoch 51; Iter    40/   55] train: loss: 0.0578991
[Epoch 51] ogbg-molbbbp: 0.941253 val loss: 0.660331
[Epoch 51] ogbg-molbbbp: 0.639757 test loss: 1.943803
[Epoch 52; Iter    15/   55] train: loss: 0.0530781
[Epoch 52; Iter    45/   55] train: loss: 0.0139584
[Epoch 52] ogbg-molbbbp: 0.935677 val loss: 0.818285
[Epoch 52] ogbg-molbbbp: 0.609471 test loss: 2.612239
[Epoch 53; Iter    20/   55] train: loss: 0.0151413
[Epoch 53; Iter    50/   55] train: loss: 0.0456275
[Epoch 53] ogbg-molbbbp: 0.918252 val loss: 0.709590
[Epoch 53] ogbg-molbbbp: 0.610436 test loss: 2.513481
[Epoch 54; Iter    25/   55] train: loss: 0.0445321
[Epoch 54; Iter    55/   55] train: loss: 0.0071259
[Epoch 54] ogbg-molbbbp: 0.921737 val loss: 0.974123
[Epoch 54] ogbg-molbbbp: 0.641590 test loss: 3.229503
[Epoch 55; Iter    30/   55] train: loss: 0.0058422
[Epoch 55] ogbg-molbbbp: 0.933785 val loss: 0.625562
[Epoch 55] ogbg-molbbbp: 0.615451 test loss: 2.621187
[Epoch 56; Iter     5/   55] train: loss: 0.1034834
[Epoch 56; Iter    35/   55] train: loss: 0.0461603
[Epoch 56] ogbg-molbbbp: 0.934283 val loss: 0.630800
[Epoch 56] ogbg-molbbbp: 0.633488 test loss: 2.171253
[Epoch 57; Iter    10/   55] train: loss: 0.0161902
[Epoch 57; Iter    40/   55] train: loss: 0.0221843
[Epoch 57] ogbg-molbbbp: 0.931992 val loss: 0.747702
[Epoch 57] ogbg-molbbbp: 0.634452 test loss: 2.027496
[Epoch 58; Iter    15/   55] train: loss: 0.0323329
[Epoch 58; Iter    45/   55] train: loss: 0.0415186
[Epoch 58] ogbg-molbbbp: 0.938564 val loss: 0.628038
[Epoch 58] ogbg-molbbbp: 0.623650 test loss: 2.255653
[Epoch 59; Iter    20/   55] train: loss: 0.0702795
[Epoch 59; Iter    50/   55] train: loss: 0.0207262
[Epoch 59] ogbg-molbbbp: 0.941253 val loss: 0.702549
[Epoch 59] ogbg-molbbbp: 0.637153 test loss: 2.633384
[Epoch 60; Iter    25/   55] train: loss: 0.0048100
[Epoch 60; Iter    55/   55] train: loss: 0.1407312
[Epoch 60] ogbg-molbbbp: 0.922931 val loss: 0.534512
[Epoch 60] ogbg-molbbbp: 0.615066 test loss: 2.131523
[Epoch 61; Iter    30/   55] train: loss: 0.0178143
[Epoch 61] ogbg-molbbbp: 0.927113 val loss: 0.900933
[Epoch 61] ogbg-molbbbp: 0.601562 test loss: 3.565031
[Epoch 62; Iter     5/   55] train: loss: 0.0415463
[Epoch 62; Iter    35/   55] train: loss: 0.0185724
[Epoch 62] ogbg-molbbbp: 0.948223 val loss: 0.661908
[Epoch 62] ogbg-molbbbp: 0.611979 test loss: 2.121739
[Epoch 63; Iter    10/   55] train: loss: 0.0194490
[Epoch 63; Iter    40/   55] train: loss: 0.0064232
[Epoch 63] ogbg-molbbbp: 0.928408 val loss: 0.646846
[Epoch 63] ogbg-molbbbp: 0.591821 test loss: 2.970527
[Epoch 64; Iter    15/   55] train: loss: 0.0250318
[Epoch 64; Iter    45/   55] train: loss: 0.0608396
[Epoch 64] ogbg-molbbbp: 0.949119 val loss: 0.548409
[Epoch 64] ogbg-molbbbp: 0.612172 test loss: 2.208128
[Epoch 65; Iter    20/   55] train: loss: 0.0320638
[Epoch 65; Iter    50/   55] train: loss: 0.0014816
[Epoch 65] ogbg-molbbbp: 0.951509 val loss: 0.550052
[Epoch 65] ogbg-molbbbp: 0.599730 test loss: 2.612130
[Epoch 66; Iter    25/   55] train: loss: 0.1040783
[Epoch 66; Iter    55/   55] train: loss: 0.0007048
[Epoch 66] ogbg-molbbbp: 0.951509 val loss: 0.511307
[Epoch 66] ogbg-molbbbp: 0.588349 test loss: 3.219343
[Epoch 67; Iter    30/   55] train: loss: 0.0012433
[Epoch 67] ogbg-molbbbp: 0.948721 val loss: 0.525709
[Epoch 67] ogbg-molbbbp: 0.590567 test loss: 3.195154
[Epoch 68; Iter     5/   55] train: loss: 0.0021212
[Epoch 68; Iter    35/   55] train: loss: 0.0025438
[Epoch 68] ogbg-molbbbp: 0.950314 val loss: 0.582193
[Epoch 68] ogbg-molbbbp: 0.598862 test loss: 3.300063
[Epoch 69; Iter    10/   55] train: loss: 0.0010967
[Epoch 28] ogbg-molbbbp: 0.623264 test loss: 1.075822
[Epoch 29; Iter    20/   55] train: loss: 0.1913733
[Epoch 29; Iter    50/   55] train: loss: 0.2634065
[Epoch 29] ogbg-molbbbp: 0.926815 val loss: 0.529368
[Epoch 29] ogbg-molbbbp: 0.643615 test loss: 1.635501
[Epoch 30; Iter    25/   55] train: loss: 0.1884369
[Epoch 30; Iter    55/   55] train: loss: 0.5086876
[Epoch 30] ogbg-molbbbp: 0.931096 val loss: 0.377582
[Epoch 30] ogbg-molbbbp: 0.683256 test loss: 0.953597
[Epoch 31; Iter    30/   55] train: loss: 0.3204290
[Epoch 31] ogbg-molbbbp: 0.909390 val loss: 0.672673
[Epoch 31] ogbg-molbbbp: 0.620660 test loss: 1.743978
[Epoch 32; Iter     5/   55] train: loss: 0.1819089
[Epoch 32; Iter    35/   55] train: loss: 0.0900922
[Epoch 32] ogbg-molbbbp: 0.806532 val loss: 1.345656
[Epoch 32] ogbg-molbbbp: 0.610340 test loss: 1.630775
[Epoch 33; Iter    10/   55] train: loss: 0.1429010
[Epoch 33; Iter    40/   55] train: loss: 0.2823355
[Epoch 33] ogbg-molbbbp: 0.848551 val loss: 1.997617
[Epoch 33] ogbg-molbbbp: 0.662809 test loss: 2.061362
[Epoch 34; Iter    15/   55] train: loss: 0.3247513
[Epoch 34; Iter    45/   55] train: loss: 0.3670394
[Epoch 34] ogbg-molbbbp: 0.907996 val loss: 0.514462
[Epoch 34] ogbg-molbbbp: 0.586420 test loss: 2.596007
[Epoch 35; Iter    20/   55] train: loss: 0.1551854
[Epoch 35; Iter    50/   55] train: loss: 0.2434178
[Epoch 35] ogbg-molbbbp: 0.887384 val loss: 2.042202
[Epoch 35] ogbg-molbbbp: 0.685378 test loss: 1.380766
[Epoch 36; Iter    25/   55] train: loss: 0.1532997
[Epoch 36; Iter    55/   55] train: loss: 0.2784796
[Epoch 36] ogbg-molbbbp: 0.918451 val loss: 0.558829
[Epoch 36] ogbg-molbbbp: 0.632041 test loss: 1.549091
[Epoch 37; Iter    30/   55] train: loss: 0.1232658
[Epoch 37] ogbg-molbbbp: 0.932590 val loss: 0.459055
[Epoch 37] ogbg-molbbbp: 0.644676 test loss: 1.552252
[Epoch 38; Iter     5/   55] train: loss: 0.1704305
[Epoch 38; Iter    35/   55] train: loss: 0.2098608
[Epoch 38] ogbg-molbbbp: 0.941850 val loss: 0.400292
[Epoch 38] ogbg-molbbbp: 0.654610 test loss: 1.594297
[Epoch 39; Iter    10/   55] train: loss: 0.0678424
[Epoch 39; Iter    40/   55] train: loss: 0.0611363
[Epoch 39] ogbg-molbbbp: 0.937270 val loss: 0.414934
[Epoch 39] ogbg-molbbbp: 0.633198 test loss: 1.533267
[Epoch 40; Iter    15/   55] train: loss: 0.0966024
[Epoch 40; Iter    45/   55] train: loss: 0.2757320
[Epoch 40] ogbg-molbbbp: 0.955093 val loss: 0.377101
[Epoch 40] ogbg-molbbbp: 0.672261 test loss: 1.668563
[Epoch 41; Iter    20/   55] train: loss: 0.1177435
[Epoch 41; Iter    50/   55] train: loss: 0.1831650
[Epoch 41] ogbg-molbbbp: 0.937469 val loss: 0.715488
[Epoch 41] ogbg-molbbbp: 0.664931 test loss: 2.365144
[Epoch 42; Iter    25/   55] train: loss: 0.2122515
[Epoch 42; Iter    55/   55] train: loss: 0.0178691
[Epoch 42] ogbg-molbbbp: 0.924226 val loss: 0.757375
[Epoch 42] ogbg-molbbbp: 0.670042 test loss: 2.013202
[Epoch 43; Iter    30/   55] train: loss: 0.0122989
[Epoch 43] ogbg-molbbbp: 0.935179 val loss: 0.628352
[Epoch 43] ogbg-molbbbp: 0.617670 test loss: 2.392315
[Epoch 44; Iter     5/   55] train: loss: 0.0312464
[Epoch 44; Iter    35/   55] train: loss: 0.0324724
[Epoch 44] ogbg-molbbbp: 0.929204 val loss: 0.693224
[Epoch 44] ogbg-molbbbp: 0.614390 test loss: 2.730483
[Epoch 45; Iter    10/   55] train: loss: 0.0124950
[Epoch 45; Iter    40/   55] train: loss: 0.0216630
[Epoch 45] ogbg-molbbbp: 0.933187 val loss: 0.670068
[Epoch 45] ogbg-molbbbp: 0.675154 test loss: 2.511840
[Epoch 46; Iter    15/   55] train: loss: 0.1102463
[Epoch 46; Iter    45/   55] train: loss: 0.1041291
[Epoch 46] ogbg-molbbbp: 0.932789 val loss: 0.635039
[Epoch 46] ogbg-molbbbp: 0.629630 test loss: 2.272913
[Epoch 47; Iter    20/   55] train: loss: 0.0557318
[Epoch 47; Iter    50/   55] train: loss: 0.1637206
[Epoch 47] ogbg-molbbbp: 0.933984 val loss: 0.587366
[Epoch 47] ogbg-molbbbp: 0.632234 test loss: 2.073181
[Epoch 48; Iter    25/   55] train: loss: 0.0501127
[Epoch 48; Iter    55/   55] train: loss: 0.0197395
[Epoch 48] ogbg-molbbbp: 0.951708 val loss: 0.486235
[Epoch 48] ogbg-molbbbp: 0.663773 test loss: 1.947626
[Epoch 49; Iter    30/   55] train: loss: 0.0277035
[Epoch 49] ogbg-molbbbp: 0.943443 val loss: 0.555448
[Epoch 49] ogbg-molbbbp: 0.665413 test loss: 2.035820
[Epoch 50; Iter     5/   55] train: loss: 0.0288054
[Epoch 50; Iter    35/   55] train: loss: 0.0109765
[Epoch 50] ogbg-molbbbp: 0.926416 val loss: 0.779520
[Epoch 50] ogbg-molbbbp: 0.583623 test loss: 2.793055
[Epoch 51; Iter    10/   55] train: loss: 0.0250679
[Epoch 51; Iter    40/   55] train: loss: 0.0311732
[Epoch 51] ogbg-molbbbp: 0.931694 val loss: 0.773168
[Epoch 51] ogbg-molbbbp: 0.642843 test loss: 2.538635
[Epoch 52; Iter    15/   55] train: loss: 0.0540557
[Epoch 52; Iter    45/   55] train: loss: 0.0273412
[Epoch 52] ogbg-molbbbp: 0.941551 val loss: 0.621482
[Epoch 52] ogbg-molbbbp: 0.657407 test loss: 2.132618
[Epoch 53; Iter    20/   55] train: loss: 0.0082744
[Epoch 53; Iter    50/   55] train: loss: 0.0090504
[Epoch 53] ogbg-molbbbp: 0.941850 val loss: 0.591199
[Epoch 53] ogbg-molbbbp: 0.635513 test loss: 2.153024
[Epoch 54; Iter    25/   55] train: loss: 0.0095421
[Epoch 54; Iter    55/   55] train: loss: 0.4098549
[Epoch 54] ogbg-molbbbp: 0.941053 val loss: 0.644590
[Epoch 54] ogbg-molbbbp: 0.636092 test loss: 2.369445
[Epoch 55; Iter    30/   55] train: loss: 0.0312284
[Epoch 55] ogbg-molbbbp: 0.942846 val loss: 0.541455
[Epoch 55] ogbg-molbbbp: 0.665413 test loss: 1.869588
[Epoch 56; Iter     5/   55] train: loss: 0.0213359
[Epoch 56; Iter    35/   55] train: loss: 0.1689569
[Epoch 56] ogbg-molbbbp: 0.944937 val loss: 0.614079
[Epoch 56] ogbg-molbbbp: 0.686728 test loss: 2.161912
[Epoch 57; Iter    10/   55] train: loss: 0.0191682
[Epoch 57; Iter    40/   55] train: loss: 0.0040362
[Epoch 57] ogbg-molbbbp: 0.953699 val loss: 0.481465
[Epoch 57] ogbg-molbbbp: 0.696566 test loss: 1.749659
[Epoch 58; Iter    15/   55] train: loss: 0.0199535
[Epoch 58; Iter    45/   55] train: loss: 0.0178695
[Epoch 58] ogbg-molbbbp: 0.952504 val loss: 0.565673
[Epoch 58] ogbg-molbbbp: 0.671971 test loss: 2.526304
[Epoch 59; Iter    20/   55] train: loss: 0.0022498
[Epoch 59; Iter    50/   55] train: loss: 0.0066034
[Epoch 59] ogbg-molbbbp: 0.955790 val loss: 0.521755
[Epoch 59] ogbg-molbbbp: 0.678530 test loss: 2.228045
[Epoch 60; Iter    25/   55] train: loss: 0.0074917
[Epoch 60; Iter    55/   55] train: loss: 0.0098974
[Epoch 60] ogbg-molbbbp: 0.956188 val loss: 0.517809
[Epoch 60] ogbg-molbbbp: 0.681327 test loss: 2.241356
[Epoch 61; Iter    30/   55] train: loss: 0.0077352
[Epoch 61] ogbg-molbbbp: 0.955193 val loss: 0.577337
[Epoch 61] ogbg-molbbbp: 0.678530 test loss: 2.251303
[Epoch 62; Iter     5/   55] train: loss: 0.0017154
[Epoch 62; Iter    35/   55] train: loss: 0.0028506
[Epoch 62] ogbg-molbbbp: 0.949119 val loss: 0.551918
[Epoch 62] ogbg-molbbbp: 0.691551 test loss: 2.400151
[Epoch 63; Iter    10/   55] train: loss: 0.0027434
[Epoch 63; Iter    40/   55] train: loss: 0.0030520
[Epoch 63] ogbg-molbbbp: 0.948621 val loss: 0.660098
[Epoch 63] ogbg-molbbbp: 0.649113 test loss: 2.868639
[Epoch 64; Iter    15/   55] train: loss: 0.0008632
[Epoch 64; Iter    45/   55] train: loss: 0.0007159
[Epoch 64] ogbg-molbbbp: 0.953500 val loss: 0.564683
[Epoch 64] ogbg-molbbbp: 0.669464 test loss: 2.485318
[Epoch 65; Iter    20/   55] train: loss: 0.0003898
[Epoch 65; Iter    50/   55] train: loss: 0.0019124
[Epoch 65] ogbg-molbbbp: 0.950214 val loss: 0.578191
[Epoch 65] ogbg-molbbbp: 0.669560 test loss: 2.531986
[Epoch 66; Iter    25/   55] train: loss: 0.0042512
[Epoch 66; Iter    55/   55] train: loss: 0.0041322
[Epoch 66] ogbg-molbbbp: 0.953699 val loss: 0.554278
[Epoch 66] ogbg-molbbbp: 0.658468 test loss: 2.623151
[Epoch 67; Iter    30/   55] train: loss: 0.0008542
[Epoch 67] ogbg-molbbbp: 0.954097 val loss: 0.569025
[Epoch 67] ogbg-molbbbp: 0.676119 test loss: 2.500319
[Epoch 68; Iter     5/   55] train: loss: 0.0041204
[Epoch 68; Iter    35/   55] train: loss: 0.0020593
[Epoch 68] ogbg-molbbbp: 0.955193 val loss: 0.553007
[Epoch 68] ogbg-molbbbp: 0.674672 test loss: 2.449703
[Epoch 69; Iter    10/   55] train: loss: 0.0010235
[Epoch 28] ogbg-molbbbp: 0.602334 test loss: 1.770827
[Epoch 29; Iter    20/   55] train: loss: 0.3116508
[Epoch 29; Iter    50/   55] train: loss: 0.2921562
[Epoch 29] ogbg-molbbbp: 0.947526 val loss: 0.366896
[Epoch 29] ogbg-molbbbp: 0.592496 test loss: 1.549855
[Epoch 30; Iter    25/   55] train: loss: 0.1492112
[Epoch 30; Iter    55/   55] train: loss: 0.4621429
[Epoch 30] ogbg-molbbbp: 0.933586 val loss: 0.384357
[Epoch 30] ogbg-molbbbp: 0.662616 test loss: 1.087424
[Epoch 31; Iter    30/   55] train: loss: 0.2807289
[Epoch 31] ogbg-molbbbp: 0.912775 val loss: 0.544021
[Epoch 31] ogbg-molbbbp: 0.602527 test loss: 1.676503
[Epoch 32; Iter     5/   55] train: loss: 0.2014583
[Epoch 32; Iter    35/   55] train: loss: 0.1544428
[Epoch 32] ogbg-molbbbp: 0.947326 val loss: 0.300505
[Epoch 32] ogbg-molbbbp: 0.685764 test loss: 1.027826
[Epoch 33; Iter    10/   55] train: loss: 0.1493923
[Epoch 33; Iter    40/   55] train: loss: 0.1231038
[Epoch 33] ogbg-molbbbp: 0.888280 val loss: 0.636985
[Epoch 33] ogbg-molbbbp: 0.630112 test loss: 1.343699
[Epoch 34; Iter    15/   55] train: loss: 0.3023559
[Epoch 34; Iter    45/   55] train: loss: 0.2826967
[Epoch 34] ogbg-molbbbp: 0.901424 val loss: 0.527051
[Epoch 34] ogbg-molbbbp: 0.675540 test loss: 1.224585
[Epoch 35; Iter    20/   55] train: loss: 0.1305603
[Epoch 35; Iter    50/   55] train: loss: 0.1991820
[Epoch 35] ogbg-molbbbp: 0.886189 val loss: 0.479036
[Epoch 35] ogbg-molbbbp: 0.695023 test loss: 0.939216
[Epoch 36; Iter    25/   55] train: loss: 0.3271925
[Epoch 36; Iter    55/   55] train: loss: 0.1558473
[Epoch 36] ogbg-molbbbp: 0.875535 val loss: 1.012849
[Epoch 36] ogbg-molbbbp: 0.636478 test loss: 1.851238
[Epoch 37; Iter    30/   55] train: loss: 0.0741196
[Epoch 37] ogbg-molbbbp: 0.914070 val loss: 0.840172
[Epoch 37] ogbg-molbbbp: 0.711998 test loss: 1.778129
[Epoch 38; Iter     5/   55] train: loss: 0.0872393
[Epoch 38; Iter    35/   55] train: loss: 0.1242431
[Epoch 38] ogbg-molbbbp: 0.888380 val loss: 2.231846
[Epoch 38] ogbg-molbbbp: 0.668499 test loss: 3.116020
[Epoch 39; Iter    10/   55] train: loss: 0.1056369
[Epoch 39; Iter    40/   55] train: loss: 0.0965024
[Epoch 39] ogbg-molbbbp: 0.828736 val loss: 3.538817
[Epoch 39] ogbg-molbbbp: 0.623650 test loss: 5.496465
[Epoch 40; Iter    15/   55] train: loss: 0.0295967
[Epoch 40; Iter    45/   55] train: loss: 0.0803664
[Epoch 40] ogbg-molbbbp: 0.921438 val loss: 2.035824
[Epoch 40] ogbg-molbbbp: 0.688657 test loss: 3.793076
[Epoch 41; Iter    20/   55] train: loss: 0.0388478
[Epoch 41; Iter    50/   55] train: loss: 0.0276294
[Epoch 41] ogbg-molbbbp: 0.917057 val loss: 2.388628
[Epoch 41] ogbg-molbbbp: 0.640432 test loss: 3.138670
[Epoch 42; Iter    25/   55] train: loss: 0.1846935
[Epoch 42; Iter    55/   55] train: loss: 0.0143827
[Epoch 42] ogbg-molbbbp: 0.868665 val loss: 2.519892
[Epoch 42] ogbg-molbbbp: 0.665316 test loss: 5.284519
[Epoch 43; Iter    30/   55] train: loss: 0.0221346
[Epoch 43] ogbg-molbbbp: 0.868167 val loss: 1.389318
[Epoch 43] ogbg-molbbbp: 0.673418 test loss: 2.211059
[Epoch 44; Iter     5/   55] train: loss: 0.0857786
[Epoch 44; Iter    35/   55] train: loss: 0.0469175
[Epoch 44] ogbg-molbbbp: 0.911082 val loss: 1.396458
[Epoch 44] ogbg-molbbbp: 0.662423 test loss: 3.171045
[Epoch 45; Iter    10/   55] train: loss: 0.0674590
[Epoch 45; Iter    40/   55] train: loss: 0.0320016
[Epoch 45] ogbg-molbbbp: 0.895450 val loss: 1.709465
[Epoch 45] ogbg-molbbbp: 0.676987 test loss: 3.236312
[Epoch 46; Iter    15/   55] train: loss: 0.0242696
[Epoch 46; Iter    45/   55] train: loss: 0.0769259
[Epoch 46] ogbg-molbbbp: 0.892462 val loss: 1.592947
[Epoch 46] ogbg-molbbbp: 0.670814 test loss: 3.059044
[Epoch 47; Iter    20/   55] train: loss: 0.0317317
[Epoch 47; Iter    50/   55] train: loss: 0.1664647
[Epoch 47] ogbg-molbbbp: 0.898835 val loss: 0.890898
[Epoch 47] ogbg-molbbbp: 0.712288 test loss: 1.983358
[Epoch 48; Iter    25/   55] train: loss: 0.0262133
[Epoch 48; Iter    55/   55] train: loss: 0.0178840
[Epoch 48] ogbg-molbbbp: 0.914667 val loss: 2.510142
[Epoch 48] ogbg-molbbbp: 0.704958 test loss: 4.559248
[Epoch 49; Iter    30/   55] train: loss: 0.0112806
[Epoch 49] ogbg-molbbbp: 0.931793 val loss: 0.562737
[Epoch 49] ogbg-molbbbp: 0.666377 test loss: 1.796010
[Epoch 50; Iter     5/   55] train: loss: 0.0110086
[Epoch 50; Iter    35/   55] train: loss: 0.0084554
[Epoch 50] ogbg-molbbbp: 0.919247 val loss: 1.007713
[Epoch 50] ogbg-molbbbp: 0.676698 test loss: 2.789111
[Epoch 51; Iter    10/   55] train: loss: 0.0127606
[Epoch 51; Iter    40/   55] train: loss: 0.0120849
[Epoch 51] ogbg-molbbbp: 0.927113 val loss: 0.593391
[Epoch 51] ogbg-molbbbp: 0.671296 test loss: 1.822629
[Epoch 52; Iter    15/   55] train: loss: 0.0251402
[Epoch 52; Iter    45/   55] train: loss: 0.0382444
[Epoch 52] ogbg-molbbbp: 0.923131 val loss: 0.903663
[Epoch 52] ogbg-molbbbp: 0.690394 test loss: 2.542844
[Epoch 53; Iter    20/   55] train: loss: 0.0019150
[Epoch 53; Iter    50/   55] train: loss: 0.0039353
[Epoch 53] ogbg-molbbbp: 0.926914 val loss: 0.656841
[Epoch 53] ogbg-molbbbp: 0.686825 test loss: 2.067061
[Epoch 54; Iter    25/   55] train: loss: 0.0116474
[Epoch 54; Iter    55/   55] train: loss: 0.2928882
[Epoch 54] ogbg-molbbbp: 0.878721 val loss: 1.178725
[Epoch 54] ogbg-molbbbp: 0.649402 test loss: 2.226280
[Epoch 55; Iter    30/   55] train: loss: 0.0157510
[Epoch 55] ogbg-molbbbp: 0.903415 val loss: 1.339214
[Epoch 55] ogbg-molbbbp: 0.656539 test loss: 2.816261
[Epoch 56; Iter     5/   55] train: loss: 0.0248240
[Epoch 56; Iter    35/   55] train: loss: 0.0162039
[Epoch 56] ogbg-molbbbp: 0.933486 val loss: 2.023064
[Epoch 56] ogbg-molbbbp: 0.682774 test loss: 3.214389
[Epoch 57; Iter    10/   55] train: loss: 0.0534959
[Epoch 57; Iter    40/   55] train: loss: 0.0074442
[Epoch 57] ogbg-molbbbp: 0.934283 val loss: 3.584849
[Epoch 57] ogbg-molbbbp: 0.657697 test loss: 2.220423
[Epoch 58; Iter    15/   55] train: loss: 0.0939279
[Epoch 58; Iter    45/   55] train: loss: 0.0153540
[Epoch 58] ogbg-molbbbp: 0.882704 val loss: 1.316040
[Epoch 58] ogbg-molbbbp: 0.646412 test loss: 1.990907
[Epoch 59; Iter    20/   55] train: loss: 0.0034538
[Epoch 59; Iter    50/   55] train: loss: 0.0117414
[Epoch 59] ogbg-molbbbp: 0.947725 val loss: 0.538996
[Epoch 59] ogbg-molbbbp: 0.668789 test loss: 2.083723
[Epoch 60; Iter    25/   55] train: loss: 0.0086169
[Epoch 60; Iter    55/   55] train: loss: 0.0471248
[Epoch 60] ogbg-molbbbp: 0.942547 val loss: 0.596475
[Epoch 60] ogbg-molbbbp: 0.669560 test loss: 2.336145
[Epoch 61; Iter    30/   55] train: loss: 0.0166293
[Epoch 61] ogbg-molbbbp: 0.892263 val loss: 1.283883
[Epoch 61] ogbg-molbbbp: 0.624904 test loss: 2.278845
[Epoch 62; Iter     5/   55] train: loss: 0.0089259
[Epoch 62; Iter    35/   55] train: loss: 0.0065893
[Epoch 62] ogbg-molbbbp: 0.918152 val loss: 1.724678
[Epoch 62] ogbg-molbbbp: 0.671971 test loss: 2.803797
[Epoch 63; Iter    10/   55] train: loss: 0.0217885
[Epoch 63; Iter    40/   55] train: loss: 0.0046002
[Epoch 63] ogbg-molbbbp: 0.928209 val loss: 1.829703
[Epoch 63] ogbg-molbbbp: 0.663677 test loss: 2.152849
[Epoch 64; Iter    15/   55] train: loss: 0.0077758
[Epoch 64; Iter    45/   55] train: loss: 0.0065628
[Epoch 64] ogbg-molbbbp: 0.929005 val loss: 1.459987
[Epoch 64] ogbg-molbbbp: 0.669367 test loss: 2.538125
[Epoch 65; Iter    20/   55] train: loss: 0.0005351
[Epoch 65; Iter    50/   55] train: loss: 0.0016020
[Epoch 65] ogbg-molbbbp: 0.922533 val loss: 1.241352
[Epoch 65] ogbg-molbbbp: 0.647859 test loss: 2.395479
[Epoch 66; Iter    25/   55] train: loss: 0.0017026
[Epoch 66; Iter    55/   55] train: loss: 0.0008337
[Epoch 66] ogbg-molbbbp: 0.927014 val loss: 0.729758
[Epoch 66] ogbg-molbbbp: 0.660012 test loss: 2.703273
[Epoch 67; Iter    30/   55] train: loss: 0.0005511
[Epoch 67] ogbg-molbbbp: 0.931196 val loss: 0.723039
[Epoch 67] ogbg-molbbbp: 0.662905 test loss: 2.470197
[Epoch 68; Iter     5/   55] train: loss: 0.0014564
[Epoch 68; Iter    35/   55] train: loss: 0.0011595
[Epoch 68] ogbg-molbbbp: 0.937768 val loss: 0.641158
[Epoch 68] ogbg-molbbbp: 0.665799 test loss: 2.422712
[Epoch 69; Iter    10/   55] train: loss: 0.0016149
[Epoch 28] ogbg-molbbbp: 0.602527 test loss: 2.223389
[Epoch 29; Iter    20/   55] train: loss: 0.2305940
[Epoch 29; Iter    50/   55] train: loss: 0.2264207
[Epoch 29] ogbg-molbbbp: 0.959673 val loss: 0.269448
[Epoch 29] ogbg-molbbbp: 0.639950 test loss: 0.980168
[Epoch 30; Iter    25/   55] train: loss: 0.1597455
[Epoch 30; Iter    55/   55] train: loss: 0.4409404
[Epoch 30] ogbg-molbbbp: 0.924525 val loss: 0.736004
[Epoch 30] ogbg-molbbbp: 0.655575 test loss: 5.522854
[Epoch 31; Iter    30/   55] train: loss: 0.2860855
[Epoch 31] ogbg-molbbbp: 0.952206 val loss: 0.312003
[Epoch 31] ogbg-molbbbp: 0.657215 test loss: 1.002712
[Epoch 32; Iter     5/   55] train: loss: 0.2550064
[Epoch 32; Iter    35/   55] train: loss: 0.1110921
[Epoch 32] ogbg-molbbbp: 0.949617 val loss: 0.375465
[Epoch 32] ogbg-molbbbp: 0.606096 test loss: 2.005014
[Epoch 33; Iter    10/   55] train: loss: 0.1360985
[Epoch 33; Iter    40/   55] train: loss: 0.1994213
[Epoch 33] ogbg-molbbbp: 0.935477 val loss: 0.447242
[Epoch 33] ogbg-molbbbp: 0.651331 test loss: 1.622352
[Epoch 34; Iter    15/   55] train: loss: 0.1401655
[Epoch 34; Iter    45/   55] train: loss: 0.2317153
[Epoch 34] ogbg-molbbbp: 0.953600 val loss: 0.511020
[Epoch 34] ogbg-molbbbp: 0.656829 test loss: 2.277841
[Epoch 35; Iter    20/   55] train: loss: 0.0879300
[Epoch 35; Iter    50/   55] train: loss: 0.2522513
[Epoch 35] ogbg-molbbbp: 0.947526 val loss: 0.378081
[Epoch 35] ogbg-molbbbp: 0.667728 test loss: 1.358068
[Epoch 36; Iter    25/   55] train: loss: 0.2366606
[Epoch 36; Iter    55/   55] train: loss: 0.3890768
[Epoch 36] ogbg-molbbbp: 0.939361 val loss: 0.426208
[Epoch 36] ogbg-molbbbp: 0.621046 test loss: 2.394853
[Epoch 37; Iter    30/   55] train: loss: 0.0695113
[Epoch 37] ogbg-molbbbp: 0.921438 val loss: 0.488247
[Epoch 37] ogbg-molbbbp: 0.630208 test loss: 2.634946
[Epoch 38; Iter     5/   55] train: loss: 0.0473426
[Epoch 38; Iter    35/   55] train: loss: 0.1819857
[Epoch 38] ogbg-molbbbp: 0.928109 val loss: 0.561526
[Epoch 38] ogbg-molbbbp: 0.652103 test loss: 2.108588
[Epoch 39; Iter    10/   55] train: loss: 0.0305206
[Epoch 39; Iter    40/   55] train: loss: 0.0850495
[Epoch 39] ogbg-molbbbp: 0.946430 val loss: 0.395002
[Epoch 39] ogbg-molbbbp: 0.631752 test loss: 1.559352
[Epoch 40; Iter    15/   55] train: loss: 0.0309222
[Epoch 40; Iter    45/   55] train: loss: 0.1099868
[Epoch 40] ogbg-molbbbp: 0.957184 val loss: 0.311413
[Epoch 40] ogbg-molbbbp: 0.653935 test loss: 1.646094
[Epoch 41; Iter    20/   55] train: loss: 0.0390340
[Epoch 41; Iter    50/   55] train: loss: 0.0570703
[Epoch 41] ogbg-molbbbp: 0.948521 val loss: 0.664490
[Epoch 41] ogbg-molbbbp: 0.689043 test loss: 2.285950
[Epoch 42; Iter    25/   55] train: loss: 0.0345324
[Epoch 42; Iter    55/   55] train: loss: 0.0186205
[Epoch 42] ogbg-molbbbp: 0.947127 val loss: 0.590110
[Epoch 42] ogbg-molbbbp: 0.658758 test loss: 2.305067
[Epoch 43; Iter    30/   55] train: loss: 0.0460387
[Epoch 43] ogbg-molbbbp: 0.957782 val loss: 0.331221
[Epoch 43] ogbg-molbbbp: 0.632716 test loss: 1.878769
[Epoch 44; Iter     5/   55] train: loss: 0.3859759
[Epoch 44; Iter    35/   55] train: loss: 0.0252526
[Epoch 44] ogbg-molbbbp: 0.953102 val loss: 0.379750
[Epoch 44] ogbg-molbbbp: 0.663098 test loss: 1.605358
[Epoch 45; Iter    10/   55] train: loss: 0.0261506
[Epoch 45; Iter    40/   55] train: loss: 0.0092263
[Epoch 45] ogbg-molbbbp: 0.934083 val loss: 0.570364
[Epoch 45] ogbg-molbbbp: 0.644387 test loss: 1.876508
[Epoch 46; Iter    15/   55] train: loss: 0.0049577
[Epoch 46; Iter    45/   55] train: loss: 0.0130462
[Epoch 46] ogbg-molbbbp: 0.957184 val loss: 0.342836
[Epoch 46] ogbg-molbbbp: 0.684221 test loss: 1.441266
[Epoch 47; Iter    20/   55] train: loss: 0.0278671
[Epoch 47; Iter    50/   55] train: loss: 0.0097548
[Epoch 47] ogbg-molbbbp: 0.950214 val loss: 0.489415
[Epoch 47] ogbg-molbbbp: 0.685475 test loss: 2.058571
[Epoch 48; Iter    25/   55] train: loss: 0.0128557
[Epoch 48; Iter    55/   55] train: loss: 0.0432197
[Epoch 48] ogbg-molbbbp: 0.955790 val loss: 0.412708
[Epoch 48] ogbg-molbbbp: 0.645737 test loss: 1.976227
[Epoch 49; Iter    30/   55] train: loss: 0.0541522
[Epoch 49] ogbg-molbbbp: 0.950214 val loss: 0.419514
[Epoch 49] ogbg-molbbbp: 0.633488 test loss: 1.878980
[Epoch 50; Iter     5/   55] train: loss: 0.0150728
[Epoch 50; Iter    35/   55] train: loss: 0.0325003
[Epoch 50] ogbg-molbbbp: 0.969332 val loss: 0.291960
[Epoch 50] ogbg-molbbbp: 0.636960 test loss: 2.051179
[Epoch 51; Iter    10/   55] train: loss: 0.0079857
[Epoch 51; Iter    40/   55] train: loss: 0.0354584
[Epoch 51] ogbg-molbbbp: 0.951708 val loss: 0.498666
[Epoch 51] ogbg-molbbbp: 0.654900 test loss: 2.167117
[Epoch 52; Iter    15/   55] train: loss: 0.0057172
[Epoch 52; Iter    45/   55] train: loss: 0.1081815
[Epoch 52] ogbg-molbbbp: 0.956487 val loss: 0.464392
[Epoch 52] ogbg-molbbbp: 0.593846 test loss: 3.048003
[Epoch 53; Iter    20/   55] train: loss: 0.0933886
[Epoch 53; Iter    50/   55] train: loss: 0.0179391
[Epoch 53] ogbg-molbbbp: 0.958180 val loss: 0.455876
[Epoch 53] ogbg-molbbbp: 0.665799 test loss: 1.977400
[Epoch 54; Iter    25/   55] train: loss: 0.0157511
[Epoch 54; Iter    55/   55] train: loss: 0.0081375
[Epoch 54] ogbg-molbbbp: 0.964453 val loss: 0.351379
[Epoch 54] ogbg-molbbbp: 0.644869 test loss: 1.925924
[Epoch 55; Iter    30/   55] train: loss: 0.0027627
[Epoch 55] ogbg-molbbbp: 0.955093 val loss: 0.467265
[Epoch 55] ogbg-molbbbp: 0.621914 test loss: 2.464636
[Epoch 56; Iter     5/   55] train: loss: 0.0218715
[Epoch 56; Iter    35/   55] train: loss: 0.0212612
[Epoch 56] ogbg-molbbbp: 0.954595 val loss: 0.452701
[Epoch 56] ogbg-molbbbp: 0.655189 test loss: 1.978438
[Epoch 57; Iter    10/   55] train: loss: 0.0058830
[Epoch 57; Iter    40/   55] train: loss: 0.1259953
[Epoch 57] ogbg-molbbbp: 0.954297 val loss: 0.478437
[Epoch 57] ogbg-molbbbp: 0.644290 test loss: 2.191472
[Epoch 58; Iter    15/   55] train: loss: 0.0420549
[Epoch 58; Iter    45/   55] train: loss: 0.0081606
[Epoch 58] ogbg-molbbbp: 0.951509 val loss: 0.470235
[Epoch 58] ogbg-molbbbp: 0.644676 test loss: 2.104608
[Epoch 59; Iter    20/   55] train: loss: 0.0460262
[Epoch 59; Iter    50/   55] train: loss: 0.0668569
[Epoch 59] ogbg-molbbbp: 0.945435 val loss: 0.587149
[Epoch 59] ogbg-molbbbp: 0.633391 test loss: 2.310200
[Epoch 60; Iter    25/   55] train: loss: 0.1093171
[Epoch 60; Iter    55/   55] train: loss: 0.1063651
[Epoch 60] ogbg-molbbbp: 0.944439 val loss: 0.509989
[Epoch 60] ogbg-molbbbp: 0.636478 test loss: 2.166078
[Epoch 61; Iter    30/   55] train: loss: 0.1108450
[Epoch 61] ogbg-molbbbp: 0.939759 val loss: 1.099684
[Epoch 61] ogbg-molbbbp: 0.644869 test loss: 2.483215
[Epoch 62; Iter     5/   55] train: loss: 0.1365944
[Epoch 62; Iter    35/   55] train: loss: 0.2496004
[Epoch 62] ogbg-molbbbp: 0.954297 val loss: 0.576803
[Epoch 62] ogbg-molbbbp: 0.635802 test loss: 2.662091
[Epoch 63; Iter    10/   55] train: loss: 0.0509689
[Epoch 63; Iter    40/   55] train: loss: 0.0165675
[Epoch 63] ogbg-molbbbp: 0.965249 val loss: 0.429527
[Epoch 63] ogbg-molbbbp: 0.662519 test loss: 2.316607
[Epoch 64; Iter    15/   55] train: loss: 0.0030686
[Epoch 64; Iter    45/   55] train: loss: 0.0520372
[Epoch 64] ogbg-molbbbp: 0.959673 val loss: 0.404172
[Epoch 64] ogbg-molbbbp: 0.642458 test loss: 1.953188
[Epoch 65; Iter    20/   55] train: loss: 0.0033457
[Epoch 65; Iter    50/   55] train: loss: 0.0039378
[Epoch 65] ogbg-molbbbp: 0.964851 val loss: 0.393825
[Epoch 65] ogbg-molbbbp: 0.644965 test loss: 2.000273
[Epoch 66; Iter    25/   55] train: loss: 0.0400641
[Epoch 66; Iter    55/   55] train: loss: 0.0008724
[Epoch 66] ogbg-molbbbp: 0.958080 val loss: 0.431829
[Epoch 66] ogbg-molbbbp: 0.638696 test loss: 2.093038
[Epoch 67; Iter    30/   55] train: loss: 0.0011642
[Epoch 67] ogbg-molbbbp: 0.961964 val loss: 0.380887
[Epoch 67] ogbg-molbbbp: 0.643133 test loss: 2.028779
[Epoch 68; Iter     5/   55] train: loss: 0.0018735
[Epoch 68; Iter    35/   55] train: loss: 0.0030660
[Epoch 68] ogbg-molbbbp: 0.960769 val loss: 0.452048
[Epoch 68] ogbg-molbbbp: 0.641397 test loss: 2.231078
[Epoch 69; Iter    10/   55] train: loss: 0.0012985
[Epoch 28] ogbg-molbbbp: 0.582562 test loss: 1.779375
[Epoch 29; Iter    20/   55] train: loss: 0.2957041
[Epoch 29; Iter    50/   55] train: loss: 0.3698316
[Epoch 29] ogbg-molbbbp: 0.696605 val loss: 2.480902
[Epoch 29] ogbg-molbbbp: 0.552566 test loss: 2.279656
[Epoch 30; Iter    25/   55] train: loss: 0.1711333
[Epoch 30; Iter    55/   55] train: loss: 0.1293057
[Epoch 30] ogbg-molbbbp: 0.925819 val loss: 0.546374
[Epoch 30] ogbg-molbbbp: 0.553723 test loss: 3.943658
[Epoch 31; Iter    30/   55] train: loss: 0.3887211
[Epoch 31] ogbg-molbbbp: 0.904809 val loss: 1.503606
[Epoch 31] ogbg-molbbbp: 0.569734 test loss: 12.374908
[Epoch 32; Iter     5/   55] train: loss: 0.1366364
[Epoch 32; Iter    35/   55] train: loss: 0.1436882
[Epoch 32] ogbg-molbbbp: 0.851439 val loss: 1.145688
[Epoch 32] ogbg-molbbbp: 0.576292 test loss: 1.808547
[Epoch 33; Iter    10/   55] train: loss: 0.0830512
[Epoch 33; Iter    40/   55] train: loss: 0.0841700
[Epoch 33] ogbg-molbbbp: 0.927810 val loss: 0.628864
[Epoch 33] ogbg-molbbbp: 0.565683 test loss: 2.542335
[Epoch 34; Iter    15/   55] train: loss: 0.3108591
[Epoch 34; Iter    45/   55] train: loss: 0.0495193
[Epoch 34] ogbg-molbbbp: 0.796276 val loss: 2.064233
[Epoch 34] ogbg-molbbbp: 0.545428 test loss: 2.593210
[Epoch 35; Iter    20/   55] train: loss: 0.1098460
[Epoch 35; Iter    50/   55] train: loss: 0.0905656
[Epoch 35] ogbg-molbbbp: 0.851240 val loss: 1.295985
[Epoch 35] ogbg-molbbbp: 0.575617 test loss: 2.390699
[Epoch 36; Iter    25/   55] train: loss: 0.3799682
[Epoch 36; Iter    55/   55] train: loss: 0.0226622
[Epoch 36] ogbg-molbbbp: 0.902519 val loss: 0.926641
[Epoch 36] ogbg-molbbbp: 0.629437 test loss: 2.113291
[Epoch 37; Iter    30/   55] train: loss: 0.0448856
[Epoch 37] ogbg-molbbbp: 0.890371 val loss: 0.650012
[Epoch 37] ogbg-molbbbp: 0.605228 test loss: 1.646123
[Epoch 38; Iter     5/   55] train: loss: 0.0347964
[Epoch 38; Iter    35/   55] train: loss: 0.0292335
[Epoch 38] ogbg-molbbbp: 0.923131 val loss: 0.600983
[Epoch 38] ogbg-molbbbp: 0.548322 test loss: 2.451090
[Epoch 39; Iter    10/   55] train: loss: 0.0524858
[Epoch 39; Iter    40/   55] train: loss: 0.0912308
[Epoch 39] ogbg-molbbbp: 0.938066 val loss: 0.450265
[Epoch 39] ogbg-molbbbp: 0.629437 test loss: 1.874427
[Epoch 40; Iter    15/   55] train: loss: 0.1206273
[Epoch 40; Iter    45/   55] train: loss: 0.0757424
[Epoch 40] ogbg-molbbbp: 0.923230 val loss: 0.538779
[Epoch 40] ogbg-molbbbp: 0.603684 test loss: 2.201856
[Epoch 41; Iter    20/   55] train: loss: 0.0313580
[Epoch 41; Iter    50/   55] train: loss: 0.0335584
[Epoch 41] ogbg-molbbbp: 0.924226 val loss: 0.594855
[Epoch 41] ogbg-molbbbp: 0.584877 test loss: 3.813497
[Epoch 42; Iter    25/   55] train: loss: 0.1947032
[Epoch 42; Iter    55/   55] train: loss: 0.0829960
[Epoch 42] ogbg-molbbbp: 0.929404 val loss: 0.522135
[Epoch 42] ogbg-molbbbp: 0.637153 test loss: 2.588594
[Epoch 43; Iter    30/   55] train: loss: 0.0097934
[Epoch 43] ogbg-molbbbp: 0.941253 val loss: 0.436720
[Epoch 43] ogbg-molbbbp: 0.602816 test loss: 2.749428
[Epoch 44; Iter     5/   55] train: loss: 0.0298614
[Epoch 44; Iter    35/   55] train: loss: 0.0850043
[Epoch 44] ogbg-molbbbp: 0.904511 val loss: 2.446025
[Epoch 44] ogbg-molbbbp: 0.610532 test loss: 3.025822
[Epoch 45; Iter    10/   55] train: loss: 0.0695566
[Epoch 45; Iter    40/   55] train: loss: 0.0122293
[Epoch 45] ogbg-molbbbp: 0.943244 val loss: 0.462753
[Epoch 45] ogbg-molbbbp: 0.603684 test loss: 2.545034
[Epoch 46; Iter    15/   55] train: loss: 0.0086433
[Epoch 46; Iter    45/   55] train: loss: 0.1231188
[Epoch 46] ogbg-molbbbp: 0.945235 val loss: 0.431137
[Epoch 46] ogbg-molbbbp: 0.603106 test loss: 3.888178
[Epoch 47; Iter    20/   55] train: loss: 0.0096269
[Epoch 47; Iter    50/   55] train: loss: 0.0222683
[Epoch 47] ogbg-molbbbp: 0.925520 val loss: 0.794649
[Epoch 47] ogbg-molbbbp: 0.628762 test loss: 2.418465
[Epoch 48; Iter    25/   55] train: loss: 0.0354064
[Epoch 48; Iter    55/   55] train: loss: 0.3197932
[Epoch 48] ogbg-molbbbp: 0.915364 val loss: 0.674343
[Epoch 48] ogbg-molbbbp: 0.611400 test loss: 3.582259
[Epoch 49; Iter    30/   55] train: loss: 0.0408885
[Epoch 49] ogbg-molbbbp: 0.887982 val loss: 0.985783
[Epoch 49] ogbg-molbbbp: 0.626833 test loss: 2.717794
[Epoch 50; Iter     5/   55] train: loss: 0.0279457
[Epoch 50; Iter    35/   55] train: loss: 0.0391715
[Epoch 50] ogbg-molbbbp: 0.877726 val loss: 1.389920
[Epoch 50] ogbg-molbbbp: 0.622975 test loss: 2.717005
[Epoch 51; Iter    10/   55] train: loss: 0.0183956
[Epoch 51; Iter    40/   55] train: loss: 0.0268321
[Epoch 51] ogbg-molbbbp: 0.928507 val loss: 0.773546
[Epoch 51] ogbg-molbbbp: 0.630305 test loss: 2.895917
[Epoch 52; Iter    15/   55] train: loss: 0.0156125
[Epoch 52; Iter    45/   55] train: loss: 0.0035517
[Epoch 52] ogbg-molbbbp: 0.937967 val loss: 0.541859
[Epoch 52] ogbg-molbbbp: 0.637539 test loss: 3.838351
[Epoch 53; Iter    20/   55] train: loss: 0.0307843
[Epoch 53; Iter    50/   55] train: loss: 0.0194256
[Epoch 53] ogbg-molbbbp: 0.924724 val loss: 0.714419
[Epoch 53] ogbg-molbbbp: 0.612944 test loss: 2.968366
[Epoch 54; Iter    25/   55] train: loss: 0.0147133
[Epoch 54; Iter    55/   55] train: loss: 0.0050768
[Epoch 54] ogbg-molbbbp: 0.928010 val loss: 0.999054
[Epoch 54] ogbg-molbbbp: 0.625386 test loss: 3.643431
[Epoch 55; Iter    30/   55] train: loss: 0.0031500
[Epoch 55] ogbg-molbbbp: 0.916459 val loss: 0.743733
[Epoch 55] ogbg-molbbbp: 0.627604 test loss: 2.782283
[Epoch 56; Iter     5/   55] train: loss: 0.0123828
[Epoch 56; Iter    35/   55] train: loss: 0.0171482
[Epoch 56] ogbg-molbbbp: 0.911281 val loss: 0.969806
[Epoch 56] ogbg-molbbbp: 0.641300 test loss: 2.964271
[Epoch 57; Iter    10/   55] train: loss: 0.0137462
[Epoch 57; Iter    40/   55] train: loss: 0.0158286
[Epoch 57] ogbg-molbbbp: 0.923728 val loss: 1.116802
[Epoch 57] ogbg-molbbbp: 0.635995 test loss: 3.447788
[Epoch 58; Iter    15/   55] train: loss: 0.0079061
[Epoch 58; Iter    45/   55] train: loss: 0.0080768
[Epoch 58] ogbg-molbbbp: 0.939958 val loss: 0.567421
[Epoch 58] ogbg-molbbbp: 0.638696 test loss: 2.589672
[Epoch 59; Iter    20/   55] train: loss: 0.0341963
[Epoch 59; Iter    50/   55] train: loss: 0.0104664
[Epoch 59] ogbg-molbbbp: 0.940058 val loss: 0.828341
[Epoch 59] ogbg-molbbbp: 0.628472 test loss: 3.298023
[Epoch 60; Iter    25/   55] train: loss: 0.0028695
[Epoch 60; Iter    55/   55] train: loss: 0.1249279
[Epoch 60] ogbg-molbbbp: 0.942248 val loss: 0.675490
[Epoch 60] ogbg-molbbbp: 0.633681 test loss: 3.017666
[Epoch 61; Iter    30/   55] train: loss: 0.0031774
[Epoch 61] ogbg-molbbbp: 0.938863 val loss: 0.679153
[Epoch 61] ogbg-molbbbp: 0.634838 test loss: 2.798616
[Epoch 62; Iter     5/   55] train: loss: 0.0024147
[Epoch 62; Iter    35/   55] train: loss: 0.0031394
[Epoch 62] ogbg-molbbbp: 0.926416 val loss: 0.953543
[Epoch 62] ogbg-molbbbp: 0.622782 test loss: 3.107720
[Epoch 63; Iter    10/   55] train: loss: 0.0083535
[Epoch 63; Iter    40/   55] train: loss: 0.0104093
[Epoch 63] ogbg-molbbbp: 0.946132 val loss: 0.632672
[Epoch 63] ogbg-molbbbp: 0.622203 test loss: 3.038572
[Epoch 64; Iter    15/   55] train: loss: 0.0037209
[Epoch 64; Iter    45/   55] train: loss: 0.0034485
[Epoch 64] ogbg-molbbbp: 0.945235 val loss: 0.526777
[Epoch 64] ogbg-molbbbp: 0.607832 test loss: 2.594495
[Epoch 65; Iter    20/   55] train: loss: 0.0010760
[Epoch 65; Iter    50/   55] train: loss: 0.0011202
[Epoch 65] ogbg-molbbbp: 0.936573 val loss: 0.760619
[Epoch 65] ogbg-molbbbp: 0.622589 test loss: 2.902529
[Epoch 66; Iter    25/   55] train: loss: 0.0005688
[Epoch 66; Iter    55/   55] train: loss: 0.0016925
[Epoch 66] ogbg-molbbbp: 0.940257 val loss: 0.764486
[Epoch 66] ogbg-molbbbp: 0.633295 test loss: 3.017647
[Epoch 67; Iter    30/   55] train: loss: 0.0016795
[Epoch 67] ogbg-molbbbp: 0.946530 val loss: 0.662934
[Epoch 67] ogbg-molbbbp: 0.630208 test loss: 2.927690
[Epoch 68; Iter     5/   55] train: loss: 0.0007903
[Epoch 68; Iter    35/   55] train: loss: 0.0006770
[Epoch 68] ogbg-molbbbp: 0.940157 val loss: 0.759016
[Epoch 68] ogbg-molbbbp: 0.626061 test loss: 3.051804
[Epoch 69; Iter    10/   55] train: loss: 0.0006246
[Epoch 28] ogbg-molbbbp: 0.680652 test loss: 0.906819
[Epoch 29; Iter    20/   55] train: loss: 0.1635112
[Epoch 29; Iter    50/   55] train: loss: 0.2605624
[Epoch 29] ogbg-molbbbp: 0.952106 val loss: 0.358593
[Epoch 29] ogbg-molbbbp: 0.679784 test loss: 1.047861
[Epoch 30; Iter    25/   55] train: loss: 0.2213949
[Epoch 30; Iter    55/   55] train: loss: 0.1191623
[Epoch 30] ogbg-molbbbp: 0.907697 val loss: 0.685432
[Epoch 30] ogbg-molbbbp: 0.615258 test loss: 3.947702
[Epoch 31; Iter    30/   55] train: loss: 0.3673165
[Epoch 31] ogbg-molbbbp: 0.956786 val loss: 0.274597
[Epoch 31] ogbg-molbbbp: 0.668017 test loss: 1.022313
[Epoch 32; Iter     5/   55] train: loss: 0.1673569
[Epoch 32; Iter    35/   55] train: loss: 0.1564163
[Epoch 32] ogbg-molbbbp: 0.925122 val loss: 0.531841
[Epoch 32] ogbg-molbbbp: 0.673322 test loss: 1.068480
[Epoch 33; Iter    10/   55] train: loss: 0.1005599
[Epoch 33; Iter    40/   55] train: loss: 0.1011002
[Epoch 33] ogbg-molbbbp: 0.940157 val loss: 0.379342
[Epoch 33] ogbg-molbbbp: 0.694155 test loss: 1.262496
[Epoch 34; Iter    15/   55] train: loss: 0.3391555
[Epoch 34; Iter    45/   55] train: loss: 0.1398440
[Epoch 34] ogbg-molbbbp: 0.939361 val loss: 0.536903
[Epoch 34] ogbg-molbbbp: 0.659336 test loss: 1.318961
[Epoch 35; Iter    20/   55] train: loss: 0.1386367
[Epoch 35; Iter    50/   55] train: loss: 0.1531029
[Epoch 35] ogbg-molbbbp: 0.952504 val loss: 0.299168
[Epoch 35] ogbg-molbbbp: 0.695988 test loss: 1.222383
[Epoch 36; Iter    25/   55] train: loss: 0.2111064
[Epoch 36; Iter    55/   55] train: loss: 0.0989173
[Epoch 36] ogbg-molbbbp: 0.955193 val loss: 0.341727
[Epoch 36] ogbg-molbbbp: 0.712481 test loss: 1.156164
[Epoch 37; Iter    30/   55] train: loss: 0.1862798
[Epoch 37] ogbg-molbbbp: 0.954396 val loss: 0.362509
[Epoch 37] ogbg-molbbbp: 0.729649 test loss: 1.161827
[Epoch 38; Iter     5/   55] train: loss: 0.0665828
[Epoch 38; Iter    35/   55] train: loss: 0.1178106
[Epoch 38] ogbg-molbbbp: 0.960868 val loss: 0.300158
[Epoch 38] ogbg-molbbbp: 0.710455 test loss: 1.188312
[Epoch 39; Iter    10/   55] train: loss: 0.0569090
[Epoch 39; Iter    40/   55] train: loss: 0.1803047
[Epoch 39] ogbg-molbbbp: 0.960669 val loss: 0.313312
[Epoch 39] ogbg-molbbbp: 0.649209 test loss: 1.324616
[Epoch 40; Iter    15/   55] train: loss: 0.1731405
[Epoch 40; Iter    45/   55] train: loss: 0.1080066
[Epoch 40] ogbg-molbbbp: 0.971921 val loss: 0.253767
[Epoch 40] ogbg-molbbbp: 0.681906 test loss: 1.358097
[Epoch 41; Iter    20/   55] train: loss: 0.3399099
[Epoch 41; Iter    50/   55] train: loss: 0.0549597
[Epoch 41] ogbg-molbbbp: 0.951807 val loss: 0.404063
[Epoch 41] ogbg-molbbbp: 0.676890 test loss: 1.487686
[Epoch 42; Iter    25/   55] train: loss: 0.0445981
[Epoch 42; Iter    55/   55] train: loss: 0.1538762
[Epoch 42] ogbg-molbbbp: 0.958379 val loss: 0.359437
[Epoch 42] ogbg-molbbbp: 0.689815 test loss: 1.412450
[Epoch 43; Iter    30/   55] train: loss: 0.0746311
[Epoch 43] ogbg-molbbbp: 0.953400 val loss: 0.489899
[Epoch 43] ogbg-molbbbp: 0.700810 test loss: 1.469603
[Epoch 44; Iter     5/   55] train: loss: 0.1454038
[Epoch 44; Iter    35/   55] train: loss: 0.2660872
[Epoch 44] ogbg-molbbbp: 0.955591 val loss: 0.375769
[Epoch 44] ogbg-molbbbp: 0.688657 test loss: 1.388255
[Epoch 45; Iter    10/   55] train: loss: 0.0704900
[Epoch 45; Iter    40/   55] train: loss: 0.0617848
[Epoch 45] ogbg-molbbbp: 0.941850 val loss: 0.453443
[Epoch 45] ogbg-molbbbp: 0.689718 test loss: 1.631110
[Epoch 46; Iter    15/   55] train: loss: 0.2431055
[Epoch 46; Iter    45/   55] train: loss: 0.0530971
[Epoch 46] ogbg-molbbbp: 0.942846 val loss: 0.622693
[Epoch 46] ogbg-molbbbp: 0.663484 test loss: 1.906337
[Epoch 47; Iter    20/   55] train: loss: 0.1724647
[Epoch 47; Iter    50/   55] train: loss: 0.1054578
[Epoch 47] ogbg-molbbbp: 0.966643 val loss: 0.306623
[Epoch 47] ogbg-molbbbp: 0.705343 test loss: 1.495603
[Epoch 48; Iter    25/   55] train: loss: 0.0630511
[Epoch 48; Iter    55/   55] train: loss: 0.4240972
[Epoch 48] ogbg-molbbbp: 0.948820 val loss: 0.410207
[Epoch 48] ogbg-molbbbp: 0.699363 test loss: 1.479107
[Epoch 49; Iter    30/   55] train: loss: 0.0597667
[Epoch 49] ogbg-molbbbp: 0.944638 val loss: 0.397009
[Epoch 49] ogbg-molbbbp: 0.634549 test loss: 1.664447
[Epoch 50; Iter     5/   55] train: loss: 0.0609136
[Epoch 50; Iter    35/   55] train: loss: 0.0405494
[Epoch 50] ogbg-molbbbp: 0.964254 val loss: 0.302625
[Epoch 50] ogbg-molbbbp: 0.679302 test loss: 1.341342
[Epoch 51; Iter    10/   55] train: loss: 0.0381987
[Epoch 51; Iter    40/   55] train: loss: 0.0695963
[Epoch 51] ogbg-molbbbp: 0.965648 val loss: 0.358796
[Epoch 51] ogbg-molbbbp: 0.651813 test loss: 1.682046
[Epoch 52; Iter    15/   55] train: loss: 0.1800359
[Epoch 52; Iter    45/   55] train: loss: 0.0252045
[Epoch 52] ogbg-molbbbp: 0.971323 val loss: 0.265441
[Epoch 52] ogbg-molbbbp: 0.689622 test loss: 1.359250
[Epoch 53; Iter    20/   55] train: loss: 0.0760674
[Epoch 53; Iter    50/   55] train: loss: 0.1128288
[Epoch 53] ogbg-molbbbp: 0.940556 val loss: 0.462082
[Epoch 53] ogbg-molbbbp: 0.612654 test loss: 1.564023
[Epoch 54; Iter    25/   55] train: loss: 0.0404074
[Epoch 54; Iter    55/   55] train: loss: 0.1332806
[Epoch 54] ogbg-molbbbp: 0.963955 val loss: 0.552286
[Epoch 54] ogbg-molbbbp: 0.685860 test loss: 2.303450
[Epoch 55; Iter    30/   55] train: loss: 0.0352468
[Epoch 55] ogbg-molbbbp: 0.919048 val loss: 1.726315
[Epoch 55] ogbg-molbbbp: 0.598573 test loss: 4.533130
[Epoch 56; Iter     5/   55] train: loss: 0.0319391
[Epoch 56; Iter    35/   55] train: loss: 0.0518112
[Epoch 56] ogbg-molbbbp: 0.944339 val loss: 0.415390
[Epoch 56] ogbg-molbbbp: 0.671779 test loss: 1.460437
[Epoch 57; Iter    10/   55] train: loss: 0.0787432
[Epoch 57; Iter    40/   55] train: loss: 0.0898047
[Epoch 57] ogbg-molbbbp: 0.947227 val loss: 0.419818
[Epoch 57] ogbg-molbbbp: 0.686535 test loss: 1.422579
[Epoch 58; Iter    15/   55] train: loss: 0.0862474
[Epoch 58; Iter    45/   55] train: loss: 0.0463484
[Epoch 58] ogbg-molbbbp: 0.959474 val loss: 0.365743
[Epoch 58] ogbg-molbbbp: 0.628086 test loss: 1.737765
[Epoch 59; Iter    20/   55] train: loss: 0.1409850
[Epoch 59; Iter    50/   55] train: loss: 0.3715048
[Epoch 59] ogbg-molbbbp: 0.939659 val loss: 0.504210
[Epoch 59] ogbg-molbbbp: 0.654417 test loss: 1.457916
[Epoch 60; Iter    25/   55] train: loss: 0.0197581
[Epoch 60; Iter    55/   55] train: loss: 0.1548738
[Epoch 60] ogbg-molbbbp: 0.957483 val loss: 0.391497
[Epoch 60] ogbg-molbbbp: 0.646701 test loss: 1.524960
[Epoch 61; Iter    30/   55] train: loss: 0.0543414
[Epoch 61] ogbg-molbbbp: 0.964951 val loss: 0.384862
[Epoch 61] ogbg-molbbbp: 0.699267 test loss: 1.678944
[Epoch 62; Iter     5/   55] train: loss: 0.0892379
[Epoch 62; Iter    35/   55] train: loss: 0.0404483
[Epoch 62] ogbg-molbbbp: 0.955292 val loss: 0.417959
[Epoch 62] ogbg-molbbbp: 0.694830 test loss: 1.669573
[Epoch 63; Iter    10/   55] train: loss: 0.0186102
[Epoch 63; Iter    40/   55] train: loss: 0.0731815
[Epoch 63] ogbg-molbbbp: 0.947924 val loss: 0.447192
[Epoch 63] ogbg-molbbbp: 0.667921 test loss: 1.726521
[Epoch 64; Iter    15/   55] train: loss: 0.0850982
[Epoch 64; Iter    45/   55] train: loss: 0.1016696
[Epoch 64] ogbg-molbbbp: 0.948920 val loss: 0.460650
[Epoch 64] ogbg-molbbbp: 0.661458 test loss: 1.776184
[Epoch 65; Iter    20/   55] train: loss: 0.0122513
[Epoch 65; Iter    50/   55] train: loss: 0.0826102
[Epoch 65] ogbg-molbbbp: 0.947326 val loss: 0.559203
[Epoch 65] ogbg-molbbbp: 0.649498 test loss: 2.044603
[Epoch 66; Iter    25/   55] train: loss: 0.0090796
[Epoch 66; Iter    55/   55] train: loss: 0.0725779
[Epoch 66] ogbg-molbbbp: 0.944538 val loss: 0.534521
[Epoch 66] ogbg-molbbbp: 0.648052 test loss: 2.184768
[Epoch 67; Iter    30/   55] train: loss: 0.1209513
[Epoch 67] ogbg-molbbbp: 0.934482 val loss: 0.561674
[Epoch 67] ogbg-molbbbp: 0.666088 test loss: 1.689866
[Epoch 68; Iter     5/   55] train: loss: 0.0150638
[Epoch 68; Iter    35/   55] train: loss: 0.0033525
[Epoch 68] ogbg-molbbbp: 0.944538 val loss: 0.564944
[Epoch 68] ogbg-molbbbp: 0.653839 test loss: 2.084568
[Epoch 69; Iter    10/   55] train: loss: 0.0935184
[Epoch 28] ogbg-molbbbp: 0.662326 test loss: 1.460753
[Epoch 29; Iter    20/   55] train: loss: 0.2768273
[Epoch 29; Iter    50/   55] train: loss: 0.2521923
[Epoch 29] ogbg-molbbbp: 0.953400 val loss: 0.347728
[Epoch 29] ogbg-molbbbp: 0.661844 test loss: 1.173740
[Epoch 30; Iter    25/   55] train: loss: 0.1868336
[Epoch 30; Iter    55/   55] train: loss: 0.4261582
[Epoch 30] ogbg-molbbbp: 0.953898 val loss: 0.269670
[Epoch 30] ogbg-molbbbp: 0.712384 test loss: 0.834125
[Epoch 31; Iter    30/   55] train: loss: 0.3716076
[Epoch 31] ogbg-molbbbp: 0.920343 val loss: 0.717598
[Epoch 31] ogbg-molbbbp: 0.665799 test loss: 1.593430
[Epoch 32; Iter     5/   55] train: loss: 0.1230837
[Epoch 32; Iter    35/   55] train: loss: 0.1156369
[Epoch 32] ogbg-molbbbp: 0.955691 val loss: 0.399095
[Epoch 32] ogbg-molbbbp: 0.660783 test loss: 1.308427
[Epoch 33; Iter    10/   55] train: loss: 0.1488551
[Epoch 33; Iter    40/   55] train: loss: 0.2184542
[Epoch 33] ogbg-molbbbp: 0.961067 val loss: 0.286040
[Epoch 33] ogbg-molbbbp: 0.679398 test loss: 1.177032
[Epoch 34; Iter    15/   55] train: loss: 0.3171338
[Epoch 34; Iter    45/   55] train: loss: 0.3135313
[Epoch 34] ogbg-molbbbp: 0.942447 val loss: 0.417118
[Epoch 34] ogbg-molbbbp: 0.683160 test loss: 1.838748
[Epoch 35; Iter    20/   55] train: loss: 0.2680695
[Epoch 35; Iter    50/   55] train: loss: 0.2914730
[Epoch 35] ogbg-molbbbp: 0.954794 val loss: 0.379015
[Epoch 35] ogbg-molbbbp: 0.689525 test loss: 1.162524
[Epoch 36; Iter    25/   55] train: loss: 0.2739728
[Epoch 36; Iter    55/   55] train: loss: 0.1814406
[Epoch 36] ogbg-molbbbp: 0.957582 val loss: 0.318868
[Epoch 36] ogbg-molbbbp: 0.678434 test loss: 1.189020
[Epoch 37; Iter    30/   55] train: loss: 0.1919309
[Epoch 37] ogbg-molbbbp: 0.957782 val loss: 0.311739
[Epoch 37] ogbg-molbbbp: 0.690490 test loss: 1.288032
[Epoch 38; Iter     5/   55] train: loss: 0.1527071
[Epoch 38; Iter    35/   55] train: loss: 0.1488594
[Epoch 38] ogbg-molbbbp: 0.937071 val loss: 0.512858
[Epoch 38] ogbg-molbbbp: 0.587770 test loss: 1.806436
[Epoch 39; Iter    10/   55] train: loss: 0.0433916
[Epoch 39; Iter    40/   55] train: loss: 0.1135607
[Epoch 39] ogbg-molbbbp: 0.940356 val loss: 0.359926
[Epoch 39] ogbg-molbbbp: 0.665027 test loss: 1.189434
[Epoch 40; Iter    15/   55] train: loss: 0.0550592
[Epoch 40; Iter    45/   55] train: loss: 0.3606410
[Epoch 40] ogbg-molbbbp: 0.946530 val loss: 0.468665
[Epoch 40] ogbg-molbbbp: 0.670235 test loss: 1.411962
[Epoch 41; Iter    20/   55] train: loss: 0.1126280
[Epoch 41; Iter    50/   55] train: loss: 0.1098897
[Epoch 41] ogbg-molbbbp: 0.950911 val loss: 0.405544
[Epoch 41] ogbg-molbbbp: 0.643326 test loss: 1.456634
[Epoch 42; Iter    25/   55] train: loss: 0.0859375
[Epoch 42; Iter    55/   55] train: loss: 0.0569468
[Epoch 42] ogbg-molbbbp: 0.951509 val loss: 0.477190
[Epoch 42] ogbg-molbbbp: 0.658083 test loss: 1.449110
[Epoch 43; Iter    30/   55] train: loss: 0.1041100
[Epoch 43] ogbg-molbbbp: 0.943344 val loss: 0.373493
[Epoch 43] ogbg-molbbbp: 0.629147 test loss: 1.468902
[Epoch 44; Iter     5/   55] train: loss: 0.0794619
[Epoch 44; Iter    35/   55] train: loss: 0.0461316
[Epoch 44] ogbg-molbbbp: 0.958479 val loss: 0.334896
[Epoch 44] ogbg-molbbbp: 0.703414 test loss: 1.208968
[Epoch 45; Iter    10/   55] train: loss: 0.0701701
[Epoch 45; Iter    40/   55] train: loss: 0.1294822
[Epoch 45] ogbg-molbbbp: 0.950413 val loss: 0.478536
[Epoch 45] ogbg-molbbbp: 0.660204 test loss: 1.790267
[Epoch 46; Iter    15/   55] train: loss: 0.3264340
[Epoch 46; Iter    45/   55] train: loss: 0.1815443
[Epoch 46] ogbg-molbbbp: 0.931793 val loss: 0.603256
[Epoch 46] ogbg-molbbbp: 0.615548 test loss: 1.518623
[Epoch 47; Iter    20/   55] train: loss: 0.1714602
[Epoch 47; Iter    50/   55] train: loss: 0.0964706
[Epoch 47] ogbg-molbbbp: 0.955491 val loss: 0.393504
[Epoch 47] ogbg-molbbbp: 0.675444 test loss: 1.542326
[Epoch 48; Iter    25/   55] train: loss: 0.0693112
[Epoch 48; Iter    55/   55] train: loss: 0.0237528
[Epoch 48] ogbg-molbbbp: 0.939958 val loss: 0.492199
[Epoch 48] ogbg-molbbbp: 0.640239 test loss: 1.502648
[Epoch 49; Iter    30/   55] train: loss: 0.2615927
[Epoch 49] ogbg-molbbbp: 0.958080 val loss: 0.311697
[Epoch 49] ogbg-molbbbp: 0.644579 test loss: 1.346641
[Epoch 50; Iter     5/   55] train: loss: 0.1884886
[Epoch 50; Iter    35/   55] train: loss: 0.0280187
[Epoch 50] ogbg-molbbbp: 0.944240 val loss: 0.564448
[Epoch 50] ogbg-molbbbp: 0.644772 test loss: 1.693669
[Epoch 51; Iter    10/   55] train: loss: 0.0419298
[Epoch 51; Iter    40/   55] train: loss: 0.1500777
[Epoch 51] ogbg-molbbbp: 0.928507 val loss: 0.638549
[Epoch 51] ogbg-molbbbp: 0.650463 test loss: 1.953326
[Epoch 52; Iter    15/   55] train: loss: 0.0454353
[Epoch 52; Iter    45/   55] train: loss: 0.0256071
[Epoch 52] ogbg-molbbbp: 0.939659 val loss: 0.663002
[Epoch 52] ogbg-molbbbp: 0.616512 test loss: 2.252864
[Epoch 53; Iter    20/   55] train: loss: 0.0857460
[Epoch 53; Iter    50/   55] train: loss: 0.0432008
[Epoch 53] ogbg-molbbbp: 0.945435 val loss: 0.457228
[Epoch 53] ogbg-molbbbp: 0.682774 test loss: 1.342272
[Epoch 54; Iter    25/   55] train: loss: 0.0314429
[Epoch 54; Iter    55/   55] train: loss: 0.2300527
[Epoch 54] ogbg-molbbbp: 0.939859 val loss: 0.485700
[Epoch 54] ogbg-molbbbp: 0.664931 test loss: 1.541967
[Epoch 55; Iter    30/   55] train: loss: 0.0702576
[Epoch 55] ogbg-molbbbp: 0.948820 val loss: 0.590881
[Epoch 55] ogbg-molbbbp: 0.670718 test loss: 1.962589
[Epoch 56; Iter     5/   55] train: loss: 0.0324113
[Epoch 56; Iter    35/   55] train: loss: 0.0223525
[Epoch 56] ogbg-molbbbp: 0.944339 val loss: 0.612227
[Epoch 56] ogbg-molbbbp: 0.646508 test loss: 2.063884
[Epoch 57; Iter    10/   55] train: loss: 0.0485494
[Epoch 57; Iter    40/   55] train: loss: 0.0541496
[Epoch 57] ogbg-molbbbp: 0.941551 val loss: 0.554507
[Epoch 57] ogbg-molbbbp: 0.697145 test loss: 1.558551
[Epoch 58; Iter    15/   55] train: loss: 0.0803598
[Epoch 58; Iter    45/   55] train: loss: 0.0478031
[Epoch 58] ogbg-molbbbp: 0.935179 val loss: 0.590601
[Epoch 58] ogbg-molbbbp: 0.648148 test loss: 1.856727
[Epoch 59; Iter    20/   55] train: loss: 0.0698822
[Epoch 59; Iter    50/   55] train: loss: 0.0934681
[Epoch 59] ogbg-molbbbp: 0.941153 val loss: 0.575321
[Epoch 59] ogbg-molbbbp: 0.634549 test loss: 2.036544
[Epoch 60; Iter    25/   55] train: loss: 0.1035742
[Epoch 60; Iter    55/   55] train: loss: 0.2299813
[Epoch 60] ogbg-molbbbp: 0.934681 val loss: 0.654727
[Epoch 60] ogbg-molbbbp: 0.642458 test loss: 2.115040
[Epoch 61; Iter    30/   55] train: loss: 0.0401036
[Epoch 61] ogbg-molbbbp: 0.951807 val loss: 0.462904
[Epoch 61] ogbg-molbbbp: 0.641975 test loss: 1.976124
[Epoch 62; Iter     5/   55] train: loss: 0.1108263
[Epoch 62; Iter    35/   55] train: loss: 0.1820256
[Epoch 62] ogbg-molbbbp: 0.951807 val loss: 0.472089
[Epoch 62] ogbg-molbbbp: 0.669657 test loss: 1.825904
[Epoch 63; Iter    10/   55] train: loss: 0.0383861
[Epoch 63; Iter    40/   55] train: loss: 0.0185101
[Epoch 63] ogbg-molbbbp: 0.942846 val loss: 0.578446
[Epoch 63] ogbg-molbbbp: 0.652971 test loss: 1.977288
[Epoch 64; Iter    15/   55] train: loss: 0.0092226
[Epoch 64; Iter    45/   55] train: loss: 0.0220219
[Epoch 64] ogbg-molbbbp: 0.951409 val loss: 0.538655
[Epoch 64] ogbg-molbbbp: 0.664834 test loss: 2.191948
[Epoch 65; Iter    20/   55] train: loss: 0.0514100
[Epoch 65; Iter    50/   55] train: loss: 0.0565042
[Epoch 65] ogbg-molbbbp: 0.942746 val loss: 0.670773
[Epoch 65] ogbg-molbbbp: 0.623843 test loss: 2.475327
[Epoch 66; Iter    25/   55] train: loss: 0.0087741
[Epoch 66; Iter    55/   55] train: loss: 0.1398799
[Epoch 66] ogbg-molbbbp: 0.957881 val loss: 0.412489
[Epoch 66] ogbg-molbbbp: 0.671875 test loss: 1.729329
[Epoch 67; Iter    30/   55] train: loss: 0.0327625
[Epoch 67] ogbg-molbbbp: 0.951210 val loss: 0.388749
[Epoch 67] ogbg-molbbbp: 0.638117 test loss: 1.753141
[Epoch 68; Iter     5/   55] train: loss: 0.0481476
[Epoch 68; Iter    35/   55] train: loss: 0.1329612
[Epoch 68] ogbg-molbbbp: 0.931196 val loss: 0.771165
[Epoch 68] ogbg-molbbbp: 0.649209 test loss: 2.027656
[Epoch 69; Iter    10/   55] train: loss: 0.0375313
[Epoch 28] ogbg-molbbbp: 0.654900 test loss: 1.302014
[Epoch 29; Iter    20/   55] train: loss: 0.1446466
[Epoch 29; Iter    50/   55] train: loss: 0.1983028
[Epoch 29] ogbg-molbbbp: 0.968436 val loss: 0.236065
[Epoch 29] ogbg-molbbbp: 0.658275 test loss: 0.927058
[Epoch 30; Iter    25/   55] train: loss: 0.1966054
[Epoch 30; Iter    55/   55] train: loss: 0.2038235
[Epoch 30] ogbg-molbbbp: 0.884596 val loss: 0.741779
[Epoch 30] ogbg-molbbbp: 0.619888 test loss: 1.147000
[Epoch 31; Iter    30/   55] train: loss: 0.2635919
[Epoch 31] ogbg-molbbbp: 0.956388 val loss: 0.287272
[Epoch 31] ogbg-molbbbp: 0.664545 test loss: 1.088848
[Epoch 32; Iter     5/   55] train: loss: 0.2019038
[Epoch 32; Iter    35/   55] train: loss: 0.2709963
[Epoch 32] ogbg-molbbbp: 0.934083 val loss: 0.333872
[Epoch 32] ogbg-molbbbp: 0.682485 test loss: 0.957774
[Epoch 33; Iter    10/   55] train: loss: 0.1305458
[Epoch 33; Iter    40/   55] train: loss: 0.2842813
[Epoch 33] ogbg-molbbbp: 0.954197 val loss: 0.327395
[Epoch 33] ogbg-molbbbp: 0.683256 test loss: 1.062326
[Epoch 34; Iter    15/   55] train: loss: 0.1217569
[Epoch 34; Iter    45/   55] train: loss: 0.2770430
[Epoch 34] ogbg-molbbbp: 0.959474 val loss: 0.304442
[Epoch 34] ogbg-molbbbp: 0.707369 test loss: 1.125139
[Epoch 35; Iter    20/   55] train: loss: 0.2087261
[Epoch 35; Iter    50/   55] train: loss: 0.1955938
[Epoch 35] ogbg-molbbbp: 0.958180 val loss: 0.559668
[Epoch 35] ogbg-molbbbp: 0.690779 test loss: 1.370830
[Epoch 36; Iter    25/   55] train: loss: 0.3848635
[Epoch 36; Iter    55/   55] train: loss: 0.8887540
[Epoch 36] ogbg-molbbbp: 0.952604 val loss: 0.311321
[Epoch 36] ogbg-molbbbp: 0.672261 test loss: 1.090297
[Epoch 37; Iter    30/   55] train: loss: 0.1095858
[Epoch 37] ogbg-molbbbp: 0.925421 val loss: 0.575220
[Epoch 37] ogbg-molbbbp: 0.655575 test loss: 1.374546
[Epoch 38; Iter     5/   55] train: loss: 0.1312350
[Epoch 38; Iter    35/   55] train: loss: 0.1611690
[Epoch 38] ogbg-molbbbp: 0.967639 val loss: 0.257501
[Epoch 38] ogbg-molbbbp: 0.705343 test loss: 1.270616
[Epoch 39; Iter    10/   55] train: loss: 0.0955983
[Epoch 39; Iter    40/   55] train: loss: 0.0789426
[Epoch 39] ogbg-molbbbp: 0.951907 val loss: 0.420253
[Epoch 39] ogbg-molbbbp: 0.641782 test loss: 1.469737
[Epoch 40; Iter    15/   55] train: loss: 0.0601522
[Epoch 40; Iter    45/   55] train: loss: 0.2293773
[Epoch 40] ogbg-molbbbp: 0.948820 val loss: 0.442122
[Epoch 40] ogbg-molbbbp: 0.683256 test loss: 1.314139
[Epoch 41; Iter    20/   55] train: loss: 0.1987004
[Epoch 41; Iter    50/   55] train: loss: 0.2337875
[Epoch 41] ogbg-molbbbp: 0.952405 val loss: 0.398970
[Epoch 41] ogbg-molbbbp: 0.691647 test loss: 1.463062
[Epoch 42; Iter    25/   55] train: loss: 0.1453734
[Epoch 42; Iter    55/   55] train: loss: 0.3244591
[Epoch 42] ogbg-molbbbp: 0.948322 val loss: 0.363596
[Epoch 42] ogbg-molbbbp: 0.657697 test loss: 1.402244
[Epoch 43; Iter    30/   55] train: loss: 0.1597260
[Epoch 43] ogbg-molbbbp: 0.933088 val loss: 0.539138
[Epoch 43] ogbg-molbbbp: 0.657118 test loss: 1.250415
[Epoch 44; Iter     5/   55] train: loss: 0.1251010
[Epoch 44; Iter    35/   55] train: loss: 0.0593350
[Epoch 44] ogbg-molbbbp: 0.958777 val loss: 0.306133
[Epoch 44] ogbg-molbbbp: 0.722126 test loss: 1.103652
[Epoch 45; Iter    10/   55] train: loss: 0.0435265
[Epoch 45; Iter    40/   55] train: loss: 0.1058443
[Epoch 45] ogbg-molbbbp: 0.957383 val loss: 0.308350
[Epoch 45] ogbg-molbbbp: 0.692901 test loss: 1.183801
[Epoch 46; Iter    15/   55] train: loss: 0.1079394
[Epoch 46; Iter    45/   55] train: loss: 0.1406935
[Epoch 46] ogbg-molbbbp: 0.949716 val loss: 0.426531
[Epoch 46] ogbg-molbbbp: 0.707176 test loss: 1.221809
[Epoch 47; Iter    20/   55] train: loss: 0.2082053
[Epoch 47; Iter    50/   55] train: loss: 0.1497315
[Epoch 47] ogbg-molbbbp: 0.951608 val loss: 0.323603
[Epoch 47] ogbg-molbbbp: 0.645351 test loss: 1.385092
[Epoch 48; Iter    25/   55] train: loss: 0.1466619
[Epoch 48; Iter    55/   55] train: loss: 0.0704396
[Epoch 48] ogbg-molbbbp: 0.959574 val loss: 0.325172
[Epoch 48] ogbg-molbbbp: 0.670428 test loss: 1.401683
[Epoch 49; Iter    30/   55] train: loss: 0.0323640
[Epoch 49] ogbg-molbbbp: 0.969431 val loss: 0.312288
[Epoch 49] ogbg-molbbbp: 0.694927 test loss: 1.330754
[Epoch 50; Iter     5/   55] train: loss: 0.0330969
[Epoch 50; Iter    35/   55] train: loss: 0.0365025
[Epoch 50] ogbg-molbbbp: 0.957085 val loss: 0.304770
[Epoch 50] ogbg-molbbbp: 0.692612 test loss: 1.164952
[Epoch 51; Iter    10/   55] train: loss: 0.0994605
[Epoch 51; Iter    40/   55] train: loss: 0.0617082
[Epoch 51] ogbg-molbbbp: 0.956288 val loss: 0.379020
[Epoch 51] ogbg-molbbbp: 0.695505 test loss: 1.535804
[Epoch 52; Iter    15/   55] train: loss: 0.0172126
[Epoch 52; Iter    45/   55] train: loss: 0.1270802
[Epoch 52] ogbg-molbbbp: 0.957483 val loss: 0.308494
[Epoch 52] ogbg-molbbbp: 0.636767 test loss: 1.265660
[Epoch 53; Iter    20/   55] train: loss: 0.1615512
[Epoch 53; Iter    50/   55] train: loss: 0.1113639
[Epoch 53] ogbg-molbbbp: 0.958379 val loss: 0.334019
[Epoch 53] ogbg-molbbbp: 0.696373 test loss: 1.367445
[Epoch 54; Iter    25/   55] train: loss: 0.1197026
[Epoch 54; Iter    55/   55] train: loss: 0.0130297
[Epoch 54] ogbg-molbbbp: 0.959873 val loss: 0.355791
[Epoch 54] ogbg-molbbbp: 0.681424 test loss: 1.404416
[Epoch 55; Iter    30/   55] train: loss: 0.0757267
[Epoch 55] ogbg-molbbbp: 0.958479 val loss: 0.378403
[Epoch 55] ogbg-molbbbp: 0.686535 test loss: 1.547970
[Epoch 56; Iter     5/   55] train: loss: 0.0898003
[Epoch 56; Iter    35/   55] train: loss: 0.0393642
[Epoch 56] ogbg-molbbbp: 0.961565 val loss: 0.380853
[Epoch 56] ogbg-molbbbp: 0.681038 test loss: 1.662343
[Epoch 57; Iter    10/   55] train: loss: 0.1531141
[Epoch 57; Iter    40/   55] train: loss: 0.2233798
[Epoch 57] ogbg-molbbbp: 0.957981 val loss: 0.401785
[Epoch 57] ogbg-molbbbp: 0.668113 test loss: 1.662511
[Epoch 58; Iter    15/   55] train: loss: 0.1766652
[Epoch 58; Iter    45/   55] train: loss: 0.1468991
[Epoch 58] ogbg-molbbbp: 0.914070 val loss: 1.795905
[Epoch 58] ogbg-molbbbp: 0.644965 test loss: 1.823129
[Epoch 59; Iter    20/   55] train: loss: 0.5884742
[Epoch 59; Iter    50/   55] train: loss: 0.2497533
[Epoch 59] ogbg-molbbbp: 0.959375 val loss: 0.281504
[Epoch 59] ogbg-molbbbp: 0.661169 test loss: 1.203241
[Epoch 60; Iter    25/   55] train: loss: 0.2402237
[Epoch 60; Iter    55/   55] train: loss: 0.2264549
[Epoch 60] ogbg-molbbbp: 0.960072 val loss: 0.296678
[Epoch 60] ogbg-molbbbp: 0.665413 test loss: 1.224149
[Epoch 61; Iter    30/   55] train: loss: 0.2729545
[Epoch 61] ogbg-molbbbp: 0.952106 val loss: 0.333289
[Epoch 61] ogbg-molbbbp: 0.688754 test loss: 1.226841
[Epoch 62; Iter     5/   55] train: loss: 0.1432707
[Epoch 62; Iter    35/   55] train: loss: 0.2042551
[Epoch 62] ogbg-molbbbp: 0.962163 val loss: 0.313645
[Epoch 62] ogbg-molbbbp: 0.678627 test loss: 1.267595
[Epoch 63; Iter    10/   55] train: loss: 0.0970747
[Epoch 63; Iter    40/   55] train: loss: 0.0498994
[Epoch 63] ogbg-molbbbp: 0.959474 val loss: 0.325342
[Epoch 63] ogbg-molbbbp: 0.691744 test loss: 1.355476
[Epoch 64; Iter    15/   55] train: loss: 0.1350020
[Epoch 64; Iter    45/   55] train: loss: 0.2290002
[Epoch 64] ogbg-molbbbp: 0.958080 val loss: 0.331873
[Epoch 64] ogbg-molbbbp: 0.671489 test loss: 1.287275
[Epoch 65; Iter    20/   55] train: loss: 0.1282439
[Epoch 65; Iter    50/   55] train: loss: 0.0266965
[Epoch 65] ogbg-molbbbp: 0.930499 val loss: 0.481850
[Epoch 65] ogbg-molbbbp: 0.627218 test loss: 1.759358
[Epoch 66; Iter    25/   55] train: loss: 0.2461819
[Epoch 66; Iter    55/   55] train: loss: 0.0203933
[Epoch 66] ogbg-molbbbp: 0.954894 val loss: 0.330939
[Epoch 66] ogbg-molbbbp: 0.670910 test loss: 1.414478
[Epoch 67; Iter    30/   55] train: loss: 0.0227184
[Epoch 67] ogbg-molbbbp: 0.950712 val loss: 0.443993
[Epoch 67] ogbg-molbbbp: 0.638021 test loss: 1.550700
[Epoch 68; Iter     5/   55] train: loss: 0.1072467
[Epoch 68; Iter    35/   55] train: loss: 0.1170347
[Epoch 68] ogbg-molbbbp: 0.958777 val loss: 0.331023
[Epoch 68] ogbg-molbbbp: 0.673611 test loss: 1.410351
[Epoch 69; Iter    10/   55] train: loss: 0.0381903
[Epoch 69; Iter    40/   55] train: loss: 0.0004666
[Epoch 69] ogbg-molbbbp: 0.935278 val loss: 0.667683
[Epoch 69] ogbg-molbbbp: 0.654707 test loss: 2.387799
[Epoch 70; Iter    15/   55] train: loss: 0.0021535
[Epoch 70; Iter    45/   55] train: loss: 0.0008125
[Epoch 70] ogbg-molbbbp: 0.936174 val loss: 0.666493
[Epoch 70] ogbg-molbbbp: 0.651524 test loss: 2.447938
[Epoch 71; Iter    20/   55] train: loss: 0.0005222
[Epoch 71; Iter    50/   55] train: loss: 0.0034820
[Epoch 71] ogbg-molbbbp: 0.935278 val loss: 0.687878
[Epoch 71] ogbg-molbbbp: 0.645737 test loss: 2.534278
[Epoch 72; Iter    25/   55] train: loss: 0.0007598
[Epoch 72; Iter    55/   55] train: loss: 0.0002032
[Epoch 72] ogbg-molbbbp: 0.935577 val loss: 0.671984
[Epoch 72] ogbg-molbbbp: 0.645737 test loss: 2.527655
[Epoch 73; Iter    30/   55] train: loss: 0.0005379
[Epoch 73] ogbg-molbbbp: 0.934083 val loss: 0.688550
[Epoch 73] ogbg-molbbbp: 0.650174 test loss: 2.423343
[Epoch 74; Iter     5/   55] train: loss: 0.0004054
[Epoch 74; Iter    35/   55] train: loss: 0.0005589
[Epoch 74] ogbg-molbbbp: 0.936473 val loss: 0.667192
[Epoch 74] ogbg-molbbbp: 0.649209 test loss: 2.479342
[Epoch 75; Iter    10/   55] train: loss: 0.0006066
[Epoch 75; Iter    40/   55] train: loss: 0.0005906
[Epoch 75] ogbg-molbbbp: 0.935577 val loss: 0.694393
[Epoch 75] ogbg-molbbbp: 0.646316 test loss: 2.536572
[Epoch 76; Iter    15/   55] train: loss: 0.0025882
[Epoch 76; Iter    45/   55] train: loss: 0.0017421
[Epoch 76] ogbg-molbbbp: 0.938664 val loss: 0.710413
[Epoch 76] ogbg-molbbbp: 0.646219 test loss: 2.599291
[Epoch 77; Iter    20/   55] train: loss: 0.0021279
[Epoch 77; Iter    50/   55] train: loss: 0.0009427
[Epoch 77] ogbg-molbbbp: 0.941551 val loss: 0.624565
[Epoch 77] ogbg-molbbbp: 0.642650 test loss: 2.610103
[Epoch 78; Iter    25/   55] train: loss: 0.0003670
[Epoch 78; Iter    55/   55] train: loss: 0.0007231
[Epoch 78] ogbg-molbbbp: 0.942547 val loss: 0.673623
[Epoch 78] ogbg-molbbbp: 0.654707 test loss: 2.550528
[Epoch 79; Iter    30/   55] train: loss: 0.0003340
[Epoch 79] ogbg-molbbbp: 0.941850 val loss: 0.697809
[Epoch 79] ogbg-molbbbp: 0.653742 test loss: 2.693522
[Epoch 80; Iter     5/   55] train: loss: 0.0003609
[Epoch 80; Iter    35/   55] train: loss: 0.0009522
[Epoch 80] ogbg-molbbbp: 0.928607 val loss: 0.802541
[Epoch 80] ogbg-molbbbp: 0.635320 test loss: 2.854534
[Epoch 81; Iter    10/   55] train: loss: 0.0019485
[Epoch 81; Iter    40/   55] train: loss: 0.0014090
[Epoch 81] ogbg-molbbbp: 0.936274 val loss: 0.767608
[Epoch 81] ogbg-molbbbp: 0.632620 test loss: 3.036878
[Epoch 82; Iter    15/   55] train: loss: 0.0006837
[Epoch 82; Iter    45/   55] train: loss: 0.0003900
[Epoch 82] ogbg-molbbbp: 0.938365 val loss: 0.728230
[Epoch 82] ogbg-molbbbp: 0.639853 test loss: 2.787083
[Epoch 83; Iter    20/   55] train: loss: 0.0003391
[Epoch 83; Iter    50/   55] train: loss: 0.0001770
[Epoch 83] ogbg-molbbbp: 0.943941 val loss: 0.665664
[Epoch 83] ogbg-molbbbp: 0.631752 test loss: 3.006288
[Epoch 84; Iter    25/   55] train: loss: 0.0154792
[Epoch 84; Iter    55/   55] train: loss: 0.0004669
[Epoch 84] ogbg-molbbbp: 0.936174 val loss: 0.711077
[Epoch 84] ogbg-molbbbp: 0.634549 test loss: 2.811989
[Epoch 85; Iter    30/   55] train: loss: 0.0017829
[Epoch 85] ogbg-molbbbp: 0.936374 val loss: 0.672611
[Epoch 85] ogbg-molbbbp: 0.631462 test loss: 2.488566
[Epoch 86; Iter     5/   55] train: loss: 0.0004947
[Epoch 86; Iter    35/   55] train: loss: 0.0016490
[Epoch 86] ogbg-molbbbp: 0.932689 val loss: 0.759979
[Epoch 86] ogbg-molbbbp: 0.630691 test loss: 2.871631
[Epoch 87; Iter    10/   55] train: loss: 0.0009956
[Epoch 87; Iter    40/   55] train: loss: 0.0009801
[Epoch 87] ogbg-molbbbp: 0.935677 val loss: 0.757901
[Epoch 87] ogbg-molbbbp: 0.636092 test loss: 2.905783
[Epoch 88; Iter    15/   55] train: loss: 0.0003609
[Epoch 88; Iter    45/   55] train: loss: 0.0021091
[Epoch 88] ogbg-molbbbp: 0.935079 val loss: 0.742771
[Epoch 88] ogbg-molbbbp: 0.637249 test loss: 2.868479
[Epoch 89; Iter    20/   55] train: loss: 0.0004116
[Epoch 89; Iter    50/   55] train: loss: 0.0006929
[Epoch 89] ogbg-molbbbp: 0.934183 val loss: 0.855715
[Epoch 89] ogbg-molbbbp: 0.624228 test loss: 3.426481
[Epoch 90; Iter    25/   55] train: loss: 0.0029884
[Epoch 90; Iter    55/   55] train: loss: 0.0009945
[Epoch 90] ogbg-molbbbp: 0.938564 val loss: 0.697353
[Epoch 90] ogbg-molbbbp: 0.636767 test loss: 2.524352
[Epoch 91; Iter    30/   55] train: loss: 0.0004900
[Epoch 91] ogbg-molbbbp: 0.934681 val loss: 0.719600
[Epoch 91] ogbg-molbbbp: 0.637635 test loss: 2.684824
[Epoch 92; Iter     5/   55] train: loss: 0.0024607
[Epoch 92; Iter    35/   55] train: loss: 0.0001459
[Epoch 92] ogbg-molbbbp: 0.940556 val loss: 0.719161
[Epoch 92] ogbg-molbbbp: 0.631944 test loss: 3.025638
[Epoch 93; Iter    10/   55] train: loss: 0.0016890
[Epoch 93; Iter    40/   55] train: loss: 0.0010825
[Epoch 93] ogbg-molbbbp: 0.937170 val loss: 0.696649
[Epoch 93] ogbg-molbbbp: 0.631848 test loss: 2.903296
[Epoch 94; Iter    15/   55] train: loss: 0.0055128
[Epoch 94; Iter    45/   55] train: loss: 0.0025538
[Epoch 94] ogbg-molbbbp: 0.943144 val loss: 0.627891
[Epoch 94] ogbg-molbbbp: 0.636671 test loss: 2.699863
[Epoch 95; Iter    20/   55] train: loss: 0.0026773
[Epoch 95; Iter    50/   55] train: loss: 0.0010108
[Epoch 95] ogbg-molbbbp: 0.942149 val loss: 0.636984
[Epoch 95] ogbg-molbbbp: 0.665606 test loss: 2.652867
[Epoch 96; Iter    25/   55] train: loss: 0.0021366
[Epoch 96; Iter    55/   55] train: loss: 0.0006031
[Epoch 96] ogbg-molbbbp: 0.942447 val loss: 0.658006
[Epoch 96] ogbg-molbbbp: 0.652199 test loss: 2.754265
[Epoch 97; Iter    30/   55] train: loss: 0.0012838
[Epoch 97] ogbg-molbbbp: 0.934482 val loss: 0.743724
[Epoch 97] ogbg-molbbbp: 0.621142 test loss: 3.121280
[Epoch 98; Iter     5/   55] train: loss: 0.0004838
[Epoch 98; Iter    35/   55] train: loss: 0.0006356
[Epoch 98] ogbg-molbbbp: 0.949119 val loss: 0.598483
[Epoch 98] ogbg-molbbbp: 0.664159 test loss: 2.404794
[Epoch 99; Iter    10/   55] train: loss: 0.0107138
[Epoch 99; Iter    40/   55] train: loss: 0.0006917
[Epoch 99] ogbg-molbbbp: 0.945036 val loss: 0.556093
[Epoch 99] ogbg-molbbbp: 0.656636 test loss: 2.614151
[Epoch 100; Iter    15/   55] train: loss: 0.0001783
[Epoch 100; Iter    45/   55] train: loss: 0.0003061
[Epoch 100] ogbg-molbbbp: 0.945634 val loss: 0.591918
[Epoch 100] ogbg-molbbbp: 0.656443 test loss: 2.532445
[Epoch 101; Iter    20/   55] train: loss: 0.0004013
[Epoch 101; Iter    50/   55] train: loss: 0.0001517
[Epoch 101] ogbg-molbbbp: 0.944439 val loss: 0.610802
[Epoch 101] ogbg-molbbbp: 0.647762 test loss: 2.840583
[Epoch 102; Iter    25/   55] train: loss: 0.0277954
[Epoch 102; Iter    55/   55] train: loss: 0.0004859
[Epoch 102] ogbg-molbbbp: 0.942647 val loss: 0.616156
[Epoch 102] ogbg-molbbbp: 0.651042 test loss: 2.743738
[Epoch 103; Iter    30/   55] train: loss: 0.0002135
[Epoch 103] ogbg-molbbbp: 0.934880 val loss: 0.667751
[Epoch 103] ogbg-molbbbp: 0.652681 test loss: 2.452229
[Epoch 104; Iter     5/   55] train: loss: 0.0066347
[Epoch 104; Iter    35/   55] train: loss: 0.0003793
[Epoch 104] ogbg-molbbbp: 0.947625 val loss: 0.621379
[Epoch 104] ogbg-molbbbp: 0.651717 test loss: 2.783269
[Epoch 105; Iter    10/   55] train: loss: 0.0005722
[Epoch 105; Iter    40/   55] train: loss: 0.0002426
[Epoch 105] ogbg-molbbbp: 0.941850 val loss: 0.653786
[Epoch 105] ogbg-molbbbp: 0.652585 test loss: 2.677292
[Epoch 106; Iter    15/   55] train: loss: 0.0001766
[Epoch 106; Iter    45/   55] train: loss: 0.0015754
[Epoch 106] ogbg-molbbbp: 0.940854 val loss: 0.657887
[Epoch 106] ogbg-molbbbp: 0.650077 test loss: 2.680563
[Epoch 107; Iter    20/   55] train: loss: 0.0000932
[Epoch 107; Iter    50/   55] train: loss: 0.0000718
[Epoch 107] ogbg-molbbbp: 0.942348 val loss: 0.644498
[Epoch 107] ogbg-molbbbp: 0.650656 test loss: 2.618582
[Epoch 108; Iter    25/   55] train: loss: 0.0004184
[Epoch 108; Iter    55/   55] train: loss: 0.0001719
[Epoch 108] ogbg-molbbbp: 0.949617 val loss: 0.575482
[Epoch 108] ogbg-molbbbp: 0.658661 test loss: 2.559692
[Epoch 109; Iter    30/   55] train: loss: 0.0006764
[Epoch 69; Iter    40/   55] train: loss: 0.0012099
[Epoch 69] ogbg-molbbbp: 0.949716 val loss: 0.529354
[Epoch 69] ogbg-molbbbp: 0.619309 test loss: 2.546898
[Epoch 70; Iter    15/   55] train: loss: 0.0039045
[Epoch 70; Iter    45/   55] train: loss: 0.0054241
[Epoch 70] ogbg-molbbbp: 0.949816 val loss: 0.545538
[Epoch 70] ogbg-molbbbp: 0.613137 test loss: 2.676481
[Epoch 71; Iter    20/   55] train: loss: 0.0066389
[Epoch 71; Iter    50/   55] train: loss: 0.0544765
[Epoch 71] ogbg-molbbbp: 0.951509 val loss: 0.542727
[Epoch 71] ogbg-molbbbp: 0.609954 test loss: 2.769566
[Epoch 72; Iter    25/   55] train: loss: 0.0009466
[Epoch 72; Iter    55/   55] train: loss: 0.0026255
[Epoch 72] ogbg-molbbbp: 0.949716 val loss: 0.555485
[Epoch 72] ogbg-molbbbp: 0.607446 test loss: 2.781973
[Epoch 73; Iter    30/   55] train: loss: 0.0006806
[Epoch 73] ogbg-molbbbp: 0.951409 val loss: 0.592791
[Epoch 73] ogbg-molbbbp: 0.614873 test loss: 2.850915
[Epoch 74; Iter     5/   55] train: loss: 0.0002738
[Epoch 74; Iter    35/   55] train: loss: 0.0013050
[Epoch 74] ogbg-molbbbp: 0.945136 val loss: 0.653106
[Epoch 74] ogbg-molbbbp: 0.611786 test loss: 3.045945
[Epoch 75; Iter    10/   55] train: loss: 0.0023372
[Epoch 75; Iter    40/   55] train: loss: 0.0022010
[Epoch 75] ogbg-molbbbp: 0.946331 val loss: 0.621562
[Epoch 75] ogbg-molbbbp: 0.607735 test loss: 2.931019
[Epoch 76; Iter    15/   55] train: loss: 0.0029160
[Epoch 76; Iter    45/   55] train: loss: 0.0017978
[Epoch 76] ogbg-molbbbp: 0.947924 val loss: 0.585761
[Epoch 76] ogbg-molbbbp: 0.611400 test loss: 2.838727
[Epoch 77; Iter    20/   55] train: loss: 0.0003275
[Epoch 77; Iter    50/   55] train: loss: 0.0003995
[Epoch 77] ogbg-molbbbp: 0.949218 val loss: 0.596989
[Epoch 77] ogbg-molbbbp: 0.613426 test loss: 2.868182
[Epoch 78; Iter    25/   55] train: loss: 0.0002856
[Epoch 78; Iter    55/   55] train: loss: 0.0027566
[Epoch 78] ogbg-molbbbp: 0.953799 val loss: 0.540527
[Epoch 78] ogbg-molbbbp: 0.606192 test loss: 3.040034
[Epoch 79; Iter    30/   55] train: loss: 0.0020990
[Epoch 79] ogbg-molbbbp: 0.947824 val loss: 0.545743
[Epoch 79] ogbg-molbbbp: 0.617670 test loss: 2.631718
[Epoch 80; Iter     5/   55] train: loss: 0.0013047
[Epoch 80; Iter    35/   55] train: loss: 0.0073549
[Epoch 80] ogbg-molbbbp: 0.942547 val loss: 0.575721
[Epoch 80] ogbg-molbbbp: 0.619888 test loss: 2.603218
[Epoch 81; Iter    10/   55] train: loss: 0.0621992
[Epoch 81; Iter    40/   55] train: loss: 0.0070600
[Epoch 81] ogbg-molbbbp: 0.948422 val loss: 0.481572
[Epoch 81] ogbg-molbbbp: 0.606771 test loss: 2.409161
[Epoch 82; Iter    15/   55] train: loss: 0.0008312
[Epoch 82; Iter    45/   55] train: loss: 0.0015071
[Epoch 82] ogbg-molbbbp: 0.948223 val loss: 0.506269
[Epoch 82] ogbg-molbbbp: 0.588156 test loss: 3.283481
[Epoch 83; Iter    20/   55] train: loss: 0.0021476
[Epoch 83; Iter    50/   55] train: loss: 0.0003299
[Epoch 83] ogbg-molbbbp: 0.947824 val loss: 0.539280
[Epoch 83] ogbg-molbbbp: 0.597126 test loss: 3.106272
[Epoch 84; Iter    25/   55] train: loss: 0.0030497
[Epoch 84; Iter    55/   55] train: loss: 0.0018231
[Epoch 84] ogbg-molbbbp: 0.949119 val loss: 0.534631
[Epoch 84] ogbg-molbbbp: 0.640143 test loss: 2.616032
[Epoch 85; Iter    30/   55] train: loss: 0.0006188
[Epoch 85] ogbg-molbbbp: 0.884696 val loss: 1.132222
[Epoch 85] ogbg-molbbbp: 0.625193 test loss: 2.262360
[Epoch 86; Iter     5/   55] train: loss: 0.0412173
[Epoch 86; Iter    35/   55] train: loss: 0.0014963
[Epoch 86] ogbg-molbbbp: 0.944339 val loss: 0.603009
[Epoch 86] ogbg-molbbbp: 0.626833 test loss: 3.482360
[Epoch 87; Iter    10/   55] train: loss: 0.0027194
[Epoch 87; Iter    40/   55] train: loss: 0.0033852
[Epoch 87] ogbg-molbbbp: 0.951309 val loss: 0.477905
[Epoch 87] ogbg-molbbbp: 0.615066 test loss: 2.663885
[Epoch 88; Iter    15/   55] train: loss: 0.0044381
[Epoch 88; Iter    45/   55] train: loss: 0.0009081
[Epoch 88] ogbg-molbbbp: 0.957383 val loss: 0.544886
[Epoch 88] ogbg-molbbbp: 0.634549 test loss: 2.573564
[Epoch 89; Iter    20/   55] train: loss: 0.0040492
[Epoch 89; Iter    50/   55] train: loss: 0.0002938
[Epoch 89] ogbg-molbbbp: 0.939460 val loss: 0.627344
[Epoch 89] ogbg-molbbbp: 0.656732 test loss: 2.540822
[Epoch 90; Iter    25/   55] train: loss: 0.0007146
[Epoch 90; Iter    55/   55] train: loss: 0.0211526
[Epoch 90] ogbg-molbbbp: 0.931992 val loss: 0.704003
[Epoch 90] ogbg-molbbbp: 0.595100 test loss: 2.935836
[Epoch 91; Iter    30/   55] train: loss: 0.0011111
[Epoch 91] ogbg-molbbbp: 0.942547 val loss: 0.587158
[Epoch 91] ogbg-molbbbp: 0.610822 test loss: 2.691615
[Epoch 92; Iter     5/   55] train: loss: 0.0006435
[Epoch 92; Iter    35/   55] train: loss: 0.0002863
[Epoch 92] ogbg-molbbbp: 0.947326 val loss: 0.595491
[Epoch 92] ogbg-molbbbp: 0.614776 test loss: 2.696060
[Epoch 93; Iter    10/   55] train: loss: 0.0005285
[Epoch 93; Iter    40/   55] train: loss: 0.0004191
[Epoch 93] ogbg-molbbbp: 0.943144 val loss: 0.581468
[Epoch 93] ogbg-molbbbp: 0.609279 test loss: 2.789419
[Epoch 94; Iter    15/   55] train: loss: 0.0001326
[Epoch 94; Iter    45/   55] train: loss: 0.0010743
[Epoch 94] ogbg-molbbbp: 0.946829 val loss: 0.533746
[Epoch 94] ogbg-molbbbp: 0.631559 test loss: 2.672041
[Epoch 95; Iter    20/   55] train: loss: 0.0006543
[Epoch 95; Iter    50/   55] train: loss: 0.0005846
[Epoch 95] ogbg-molbbbp: 0.948920 val loss: 0.541389
[Epoch 95] ogbg-molbbbp: 0.638696 test loss: 2.574525
[Epoch 96; Iter    25/   55] train: loss: 0.0011662
[Epoch 96; Iter    55/   55] train: loss: 0.0090667
[Epoch 96] ogbg-molbbbp: 0.943045 val loss: 0.572335
[Epoch 96] ogbg-molbbbp: 0.630305 test loss: 2.719641
[Epoch 97; Iter    30/   55] train: loss: 0.0014178
[Epoch 97] ogbg-molbbbp: 0.947028 val loss: 0.566222
[Epoch 97] ogbg-molbbbp: 0.639178 test loss: 2.551240
[Epoch 98; Iter     5/   55] train: loss: 0.0003117
[Epoch 98; Iter    35/   55] train: loss: 0.0055329
[Epoch 98] ogbg-molbbbp: 0.945733 val loss: 0.553121
[Epoch 98] ogbg-molbbbp: 0.627797 test loss: 2.704723
[Epoch 99; Iter    10/   55] train: loss: 0.0001976
[Epoch 99; Iter    40/   55] train: loss: 0.0012129
[Epoch 99] ogbg-molbbbp: 0.945036 val loss: 0.575474
[Epoch 99] ogbg-molbbbp: 0.630208 test loss: 2.772745
[Epoch 100; Iter    15/   55] train: loss: 0.0003055
[Epoch 100; Iter    45/   55] train: loss: 0.0078825
[Epoch 100] ogbg-molbbbp: 0.946132 val loss: 0.603513
[Epoch 100] ogbg-molbbbp: 0.627508 test loss: 2.899995
[Epoch 101; Iter    20/   55] train: loss: 0.0002576
[Epoch 101; Iter    50/   55] train: loss: 0.0011015
[Epoch 101] ogbg-molbbbp: 0.946032 val loss: 0.599486
[Epoch 101] ogbg-molbbbp: 0.629533 test loss: 2.871407
[Epoch 102; Iter    25/   55] train: loss: 0.0075098
[Epoch 102; Iter    55/   55] train: loss: 0.0005447
[Epoch 102] ogbg-molbbbp: 0.955890 val loss: 0.488645
[Epoch 102] ogbg-molbbbp: 0.644579 test loss: 2.459980
[Epoch 103; Iter    30/   55] train: loss: 0.0000951
[Epoch 103] ogbg-molbbbp: 0.959076 val loss: 0.523124
[Epoch 103] ogbg-molbbbp: 0.644772 test loss: 2.502806
[Epoch 104; Iter     5/   55] train: loss: 0.0001010
[Epoch 104; Iter    35/   55] train: loss: 0.0019684
[Epoch 104] ogbg-molbbbp: 0.957184 val loss: 0.536750
[Epoch 104] ogbg-molbbbp: 0.621528 test loss: 2.602836
[Epoch 105; Iter    10/   55] train: loss: 0.0004417
[Epoch 105; Iter    40/   55] train: loss: 0.0002530
[Epoch 105] ogbg-molbbbp: 0.955890 val loss: 0.502812
[Epoch 105] ogbg-molbbbp: 0.623071 test loss: 2.650617
[Epoch 106; Iter    15/   55] train: loss: 0.0002098
[Epoch 106; Iter    45/   55] train: loss: 0.0004767
[Epoch 106] ogbg-molbbbp: 0.955591 val loss: 0.519338
[Epoch 106] ogbg-molbbbp: 0.619599 test loss: 2.713759
[Epoch 107; Iter    20/   55] train: loss: 0.0121995
[Epoch 107; Iter    50/   55] train: loss: 0.0023379
[Epoch 107] ogbg-molbbbp: 0.951110 val loss: 0.530292
[Epoch 107] ogbg-molbbbp: 0.608121 test loss: 2.868102
[Epoch 108; Iter    25/   55] train: loss: 0.0005124
[Epoch 108; Iter    55/   55] train: loss: 0.0220308
[Epoch 108] ogbg-molbbbp: 0.955989 val loss: 0.487237
[Epoch 108] ogbg-molbbbp: 0.620370 test loss: 2.689584
[Epoch 109; Iter    30/   55] train: loss: 0.0002075
[Epoch 69; Iter    40/   55] train: loss: 0.0055390
[Epoch 69] ogbg-molbbbp: 0.952903 val loss: 0.431920
[Epoch 69] ogbg-molbbbp: 0.618056 test loss: 2.603430
[Epoch 70; Iter    15/   55] train: loss: 0.1177525
[Epoch 70; Iter    45/   55] train: loss: 0.0030525
[Epoch 70] ogbg-molbbbp: 0.950214 val loss: 0.509955
[Epoch 70] ogbg-molbbbp: 0.654225 test loss: 2.604141
[Epoch 71; Iter    20/   55] train: loss: 0.0038607
[Epoch 71; Iter    50/   55] train: loss: 0.0026828
[Epoch 71] ogbg-molbbbp: 0.947326 val loss: 0.592823
[Epoch 71] ogbg-molbbbp: 0.629726 test loss: 2.727563
[Epoch 72; Iter    25/   55] train: loss: 0.0005686
[Epoch 72; Iter    55/   55] train: loss: 0.1650956
[Epoch 72] ogbg-molbbbp: 0.955989 val loss: 0.496837
[Epoch 72] ogbg-molbbbp: 0.628762 test loss: 2.765158
[Epoch 73; Iter    30/   55] train: loss: 0.0055822
[Epoch 73] ogbg-molbbbp: 0.955491 val loss: 0.484969
[Epoch 73] ogbg-molbbbp: 0.624711 test loss: 2.577731
[Epoch 74; Iter     5/   55] train: loss: 0.0021758
[Epoch 74; Iter    35/   55] train: loss: 0.0008157
[Epoch 74] ogbg-molbbbp: 0.958976 val loss: 0.469068
[Epoch 74] ogbg-molbbbp: 0.628665 test loss: 2.688681
[Epoch 75; Iter    10/   55] train: loss: 0.0007502
[Epoch 75; Iter    40/   55] train: loss: 0.0383748
[Epoch 75] ogbg-molbbbp: 0.960769 val loss: 0.420221
[Epoch 75] ogbg-molbbbp: 0.638214 test loss: 2.680827
[Epoch 76; Iter    15/   55] train: loss: 0.0041817
[Epoch 76; Iter    45/   55] train: loss: 0.0012365
[Epoch 76] ogbg-molbbbp: 0.959176 val loss: 0.414218
[Epoch 76] ogbg-molbbbp: 0.633391 test loss: 2.512164
[Epoch 77; Iter    20/   55] train: loss: 0.0031664
[Epoch 77; Iter    50/   55] train: loss: 0.0095101
[Epoch 77] ogbg-molbbbp: 0.959773 val loss: 0.423165
[Epoch 77] ogbg-molbbbp: 0.643904 test loss: 2.385315
[Epoch 78; Iter    25/   55] train: loss: 0.0019349
[Epoch 78; Iter    55/   55] train: loss: 0.0144938
[Epoch 78] ogbg-molbbbp: 0.961864 val loss: 0.393085
[Epoch 78] ogbg-molbbbp: 0.638696 test loss: 2.576860
[Epoch 79; Iter    30/   55] train: loss: 0.0014457
[Epoch 79] ogbg-molbbbp: 0.959176 val loss: 0.508465
[Epoch 79] ogbg-molbbbp: 0.632330 test loss: 3.133718
[Epoch 80; Iter     5/   55] train: loss: 0.0027600
[Epoch 80; Iter    35/   55] train: loss: 0.0010007
[Epoch 80] ogbg-molbbbp: 0.958578 val loss: 0.487138
[Epoch 80] ogbg-molbbbp: 0.631366 test loss: 3.122040
[Epoch 81; Iter    10/   55] train: loss: 0.0006733
[Epoch 81; Iter    40/   55] train: loss: 0.0025853
[Epoch 81] ogbg-molbbbp: 0.957483 val loss: 0.510187
[Epoch 81] ogbg-molbbbp: 0.632812 test loss: 3.176077
[Epoch 82; Iter    15/   55] train: loss: 0.0005068
[Epoch 82; Iter    45/   55] train: loss: 0.0003033
[Epoch 82] ogbg-molbbbp: 0.956686 val loss: 0.462474
[Epoch 82] ogbg-molbbbp: 0.638889 test loss: 2.648061
[Epoch 83; Iter    20/   55] train: loss: 0.0016600
[Epoch 83; Iter    50/   55] train: loss: 0.0005398
[Epoch 83] ogbg-molbbbp: 0.956786 val loss: 0.568459
[Epoch 83] ogbg-molbbbp: 0.643519 test loss: 3.202582
[Epoch 84; Iter    25/   55] train: loss: 0.0009695
[Epoch 84; Iter    55/   55] train: loss: 0.0008462
[Epoch 84] ogbg-molbbbp: 0.957085 val loss: 0.532216
[Epoch 84] ogbg-molbbbp: 0.627508 test loss: 3.112623
[Epoch 85; Iter    30/   55] train: loss: 0.0007312
[Epoch 85] ogbg-molbbbp: 0.955890 val loss: 0.550927
[Epoch 85] ogbg-molbbbp: 0.629051 test loss: 3.134819
[Epoch 86; Iter     5/   55] train: loss: 0.0026527
[Epoch 86; Iter    35/   55] train: loss: 0.0005546
[Epoch 86] ogbg-molbbbp: 0.960669 val loss: 0.521072
[Epoch 86] ogbg-molbbbp: 0.626640 test loss: 3.037238
[Epoch 87; Iter    10/   55] train: loss: 0.0387456
[Epoch 87; Iter    40/   55] train: loss: 0.0015677
[Epoch 87] ogbg-molbbbp: 0.957383 val loss: 0.589248
[Epoch 87] ogbg-molbbbp: 0.643711 test loss: 3.421251
[Epoch 88; Iter    15/   55] train: loss: 0.0012145
[Epoch 88; Iter    45/   55] train: loss: 0.0020926
[Epoch 88] ogbg-molbbbp: 0.951708 val loss: 0.671106
[Epoch 88] ogbg-molbbbp: 0.642168 test loss: 3.294335
[Epoch 89; Iter    20/   55] train: loss: 0.0049106
[Epoch 89; Iter    50/   55] train: loss: 0.0016340
[Epoch 89] ogbg-molbbbp: 0.954794 val loss: 0.587701
[Epoch 89] ogbg-molbbbp: 0.636478 test loss: 3.346863
[Epoch 90; Iter    25/   55] train: loss: 0.0005415
[Epoch 90; Iter    55/   55] train: loss: 0.0008261
[Epoch 90] ogbg-molbbbp: 0.958678 val loss: 0.561747
[Epoch 90] ogbg-molbbbp: 0.630015 test loss: 3.358430
[Epoch 91; Iter    30/   55] train: loss: 0.0003685
[Epoch 91] ogbg-molbbbp: 0.958279 val loss: 0.566740
[Epoch 91] ogbg-molbbbp: 0.637346 test loss: 3.431558
[Epoch 92; Iter     5/   55] train: loss: 0.0013193
[Epoch 92; Iter    35/   55] train: loss: 0.0021809
[Epoch 92] ogbg-molbbbp: 0.957881 val loss: 0.551057
[Epoch 92] ogbg-molbbbp: 0.646894 test loss: 3.109615
[Epoch 93; Iter    10/   55] train: loss: 0.0014900
[Epoch 93; Iter    40/   55] train: loss: 0.0007490
[Epoch 93] ogbg-molbbbp: 0.957383 val loss: 0.613893
[Epoch 93] ogbg-molbbbp: 0.651138 test loss: 3.584715
[Epoch 94; Iter    15/   55] train: loss: 0.0008526
[Epoch 94; Iter    45/   55] train: loss: 0.0616407
[Epoch 94] ogbg-molbbbp: 0.949218 val loss: 0.628411
[Epoch 94] ogbg-molbbbp: 0.638407 test loss: 2.774893
[Epoch 95; Iter    20/   55] train: loss: 0.2204137
[Epoch 95; Iter    50/   55] train: loss: 0.0179826
[Epoch 95] ogbg-molbbbp: 0.955790 val loss: 0.613762
[Epoch 95] ogbg-molbbbp: 0.626833 test loss: 3.590988
[Epoch 96; Iter    25/   55] train: loss: 0.0005793
[Epoch 96; Iter    55/   55] train: loss: 0.0014650
[Epoch 96] ogbg-molbbbp: 0.962262 val loss: 0.571232
[Epoch 96] ogbg-molbbbp: 0.617959 test loss: 3.862778
[Epoch 97; Iter    30/   55] train: loss: 0.0205024
[Epoch 97] ogbg-molbbbp: 0.960769 val loss: 0.557306
[Epoch 97] ogbg-molbbbp: 0.610822 test loss: 3.308044
[Epoch 98; Iter     5/   55] train: loss: 0.0040782
[Epoch 98; Iter    35/   55] train: loss: 0.0030081
[Epoch 98] ogbg-molbbbp: 0.959873 val loss: 0.496964
[Epoch 98] ogbg-molbbbp: 0.629147 test loss: 3.109598
[Epoch 99; Iter    10/   55] train: loss: 0.0003260
[Epoch 99; Iter    40/   55] train: loss: 0.0013991
[Epoch 99] ogbg-molbbbp: 0.961067 val loss: 0.537341
[Epoch 99] ogbg-molbbbp: 0.629823 test loss: 3.388547
[Epoch 100; Iter    15/   55] train: loss: 0.0042075
[Epoch 100; Iter    45/   55] train: loss: 0.0029802
[Epoch 100] ogbg-molbbbp: 0.960570 val loss: 0.522808
[Epoch 100] ogbg-molbbbp: 0.629147 test loss: 3.412591
[Epoch 101; Iter    20/   55] train: loss: 0.0005510
[Epoch 101; Iter    50/   55] train: loss: 0.0003317
[Epoch 101] ogbg-molbbbp: 0.958678 val loss: 0.567727
[Epoch 101] ogbg-molbbbp: 0.632234 test loss: 3.479907
[Epoch 102; Iter    25/   55] train: loss: 0.0007276
[Epoch 102; Iter    55/   55] train: loss: 0.0003424
[Epoch 102] ogbg-molbbbp: 0.960968 val loss: 0.497701
[Epoch 102] ogbg-molbbbp: 0.622396 test loss: 3.304983
[Epoch 103; Iter    30/   55] train: loss: 0.0017775
[Epoch 103] ogbg-molbbbp: 0.960072 val loss: 0.524998
[Epoch 103] ogbg-molbbbp: 0.625193 test loss: 3.369856
[Epoch 104; Iter     5/   55] train: loss: 0.0003204
[Epoch 104; Iter    35/   55] train: loss: 0.0004609
[Epoch 104] ogbg-molbbbp: 0.959873 val loss: 0.560046
[Epoch 104] ogbg-molbbbp: 0.629340 test loss: 3.545915
[Epoch 105; Iter    10/   55] train: loss: 0.0014880
[Epoch 105; Iter    40/   55] train: loss: 0.0001131
[Epoch 105] ogbg-molbbbp: 0.957483 val loss: 0.588991
[Epoch 105] ogbg-molbbbp: 0.629051 test loss: 3.619312
[Epoch 106; Iter    15/   55] train: loss: 0.0004927
[Epoch 106; Iter    45/   55] train: loss: 0.0002098
[Epoch 106] ogbg-molbbbp: 0.958279 val loss: 0.570900
[Epoch 106] ogbg-molbbbp: 0.636285 test loss: 3.501505
[Epoch 107; Iter    20/   55] train: loss: 0.0007856
[Epoch 107; Iter    50/   55] train: loss: 0.0000506
[Epoch 107] ogbg-molbbbp: 0.958578 val loss: 0.576847
[Epoch 107] ogbg-molbbbp: 0.634356 test loss: 3.445554
[Epoch 108; Iter    25/   55] train: loss: 0.0001360
[Epoch 108; Iter    55/   55] train: loss: 0.0002136
[Epoch 108] ogbg-molbbbp: 0.956188 val loss: 0.575021
[Epoch 108] ogbg-molbbbp: 0.636092 test loss: 3.434720
[Epoch 109; Iter    30/   55] train: loss: 0.0001393
[Epoch 69; Iter    40/   55] train: loss: 0.0019538
[Epoch 69] ogbg-molbbbp: 0.961964 val loss: 0.447571
[Epoch 69] ogbg-molbbbp: 0.677083 test loss: 2.464760
[Epoch 70; Iter    15/   55] train: loss: 0.0187679
[Epoch 70; Iter    45/   55] train: loss: 0.0007089
[Epoch 70] ogbg-molbbbp: 0.956388 val loss: 0.489566
[Epoch 70] ogbg-molbbbp: 0.676890 test loss: 2.570974
[Epoch 71; Iter    20/   55] train: loss: 0.0005248
[Epoch 71; Iter    50/   55] train: loss: 0.0006031
[Epoch 71] ogbg-molbbbp: 0.959076 val loss: 0.472439
[Epoch 71] ogbg-molbbbp: 0.680266 test loss: 2.570864
[Epoch 72; Iter    25/   55] train: loss: 0.0004449
[Epoch 72; Iter    55/   55] train: loss: 0.0138398
[Epoch 72] ogbg-molbbbp: 0.959176 val loss: 0.470784
[Epoch 72] ogbg-molbbbp: 0.680748 test loss: 2.509366
[Epoch 73; Iter    30/   55] train: loss: 0.0013857
[Epoch 73] ogbg-molbbbp: 0.959972 val loss: 0.458693
[Epoch 73] ogbg-molbbbp: 0.677180 test loss: 2.546006
[Epoch 74; Iter     5/   55] train: loss: 0.0008266
[Epoch 74; Iter    35/   55] train: loss: 0.0005955
[Epoch 74] ogbg-molbbbp: 0.958877 val loss: 0.473569
[Epoch 74] ogbg-molbbbp: 0.679012 test loss: 2.554975
[Epoch 75; Iter    10/   55] train: loss: 0.0005261
[Epoch 75; Iter    40/   55] train: loss: 0.0008969
[Epoch 75] ogbg-molbbbp: 0.955691 val loss: 0.497597
[Epoch 75] ogbg-molbbbp: 0.677855 test loss: 2.455933
[Epoch 76; Iter    15/   55] train: loss: 0.0020313
[Epoch 76; Iter    45/   55] train: loss: 0.0014950
[Epoch 76] ogbg-molbbbp: 0.951509 val loss: 0.508607
[Epoch 76] ogbg-molbbbp: 0.684028 test loss: 2.443562
[Epoch 77; Iter    20/   55] train: loss: 0.0037616
[Epoch 77; Iter    50/   55] train: loss: 0.0035528
[Epoch 77] ogbg-molbbbp: 0.958678 val loss: 0.522756
[Epoch 77] ogbg-molbbbp: 0.667052 test loss: 3.046466
[Epoch 78; Iter    25/   55] train: loss: 0.1487293
[Epoch 78; Iter    55/   55] train: loss: 0.0098817
[Epoch 78] ogbg-molbbbp: 0.949816 val loss: 0.620080
[Epoch 78] ogbg-molbbbp: 0.670814 test loss: 2.845063
[Epoch 79; Iter    30/   55] train: loss: 0.0004055
[Epoch 79] ogbg-molbbbp: 0.960470 val loss: 0.526183
[Epoch 79] ogbg-molbbbp: 0.668017 test loss: 2.880283
[Epoch 80; Iter     5/   55] train: loss: 0.0180176
[Epoch 80; Iter    35/   55] train: loss: 0.0006743
[Epoch 80] ogbg-molbbbp: 0.960470 val loss: 0.474968
[Epoch 80] ogbg-molbbbp: 0.678337 test loss: 2.676145
[Epoch 81; Iter    10/   55] train: loss: 0.0010796
[Epoch 81; Iter    40/   55] train: loss: 0.0008360
[Epoch 81] ogbg-molbbbp: 0.963258 val loss: 0.438627
[Epoch 81] ogbg-molbbbp: 0.676601 test loss: 2.520875
[Epoch 82; Iter    15/   55] train: loss: 0.0003202
[Epoch 82; Iter    45/   55] train: loss: 0.0012183
[Epoch 82] ogbg-molbbbp: 0.959673 val loss: 0.465554
[Epoch 82] ogbg-molbbbp: 0.665895 test loss: 2.739988
[Epoch 83; Iter    20/   55] train: loss: 0.0029910
[Epoch 83; Iter    50/   55] train: loss: 0.0006555
[Epoch 83] ogbg-molbbbp: 0.958180 val loss: 0.471785
[Epoch 83] ogbg-molbbbp: 0.663002 test loss: 2.753741
[Epoch 84; Iter    25/   55] train: loss: 0.0026250
[Epoch 84; Iter    55/   55] train: loss: 0.0090978
[Epoch 84] ogbg-molbbbp: 0.952504 val loss: 0.562907
[Epoch 84] ogbg-molbbbp: 0.656732 test loss: 2.904228
[Epoch 85; Iter    30/   55] train: loss: 0.0062769
[Epoch 85] ogbg-molbbbp: 0.952106 val loss: 0.550158
[Epoch 85] ogbg-molbbbp: 0.651427 test loss: 2.715105
[Epoch 86; Iter     5/   55] train: loss: 0.0071964
[Epoch 86; Iter    35/   55] train: loss: 0.0020666
[Epoch 86] ogbg-molbbbp: 0.962262 val loss: 0.446371
[Epoch 86] ogbg-molbbbp: 0.677566 test loss: 2.556887
[Epoch 87; Iter    10/   55] train: loss: 0.0018764
[Epoch 87; Iter    40/   55] train: loss: 0.0003570
[Epoch 87] ogbg-molbbbp: 0.959375 val loss: 0.482608
[Epoch 87] ogbg-molbbbp: 0.665123 test loss: 2.610581
[Epoch 88; Iter    15/   55] train: loss: 0.0006466
[Epoch 88; Iter    45/   55] train: loss: 0.0023034
[Epoch 88] ogbg-molbbbp: 0.961167 val loss: 0.494823
[Epoch 88] ogbg-molbbbp: 0.659240 test loss: 2.879571
[Epoch 89; Iter    20/   55] train: loss: 0.0006399
[Epoch 89; Iter    50/   55] train: loss: 0.0003837
[Epoch 89] ogbg-molbbbp: 0.960271 val loss: 0.502782
[Epoch 89] ogbg-molbbbp: 0.664448 test loss: 2.789464
[Epoch 90; Iter    25/   55] train: loss: 0.0005454
[Epoch 90; Iter    55/   55] train: loss: 0.0019963
[Epoch 90] ogbg-molbbbp: 0.961366 val loss: 0.506310
[Epoch 90] ogbg-molbbbp: 0.664255 test loss: 2.702796
[Epoch 91; Iter    30/   55] train: loss: 0.0003048
[Epoch 91] ogbg-molbbbp: 0.960868 val loss: 0.505707
[Epoch 91] ogbg-molbbbp: 0.666281 test loss: 2.679684
[Epoch 92; Iter     5/   55] train: loss: 0.0005781
[Epoch 92; Iter    35/   55] train: loss: 0.0005587
[Epoch 92] ogbg-molbbbp: 0.960470 val loss: 0.517522
[Epoch 92] ogbg-molbbbp: 0.670235 test loss: 2.650780
[Epoch 93; Iter    10/   55] train: loss: 0.0007072
[Epoch 93; Iter    40/   55] train: loss: 0.0002986
[Epoch 93] ogbg-molbbbp: 0.960669 val loss: 0.529032
[Epoch 93] ogbg-molbbbp: 0.675637 test loss: 2.780544
[Epoch 94; Iter    15/   55] train: loss: 0.0033348
[Epoch 94; Iter    45/   55] train: loss: 0.0005938
[Epoch 94] ogbg-molbbbp: 0.960072 val loss: 0.501210
[Epoch 94] ogbg-molbbbp: 0.650752 test loss: 2.807720
[Epoch 95; Iter    20/   55] train: loss: 0.2602322
[Epoch 95; Iter    50/   55] train: loss: 0.0080560
[Epoch 95] ogbg-molbbbp: 0.959773 val loss: 0.508092
[Epoch 95] ogbg-molbbbp: 0.634066 test loss: 2.989098
[Epoch 96; Iter    25/   55] train: loss: 0.0012764
[Epoch 96; Iter    55/   55] train: loss: 0.0011992
[Epoch 96] ogbg-molbbbp: 0.960570 val loss: 0.474149
[Epoch 96] ogbg-molbbbp: 0.646123 test loss: 2.832586
[Epoch 97; Iter    30/   55] train: loss: 0.0145666
[Epoch 97] ogbg-molbbbp: 0.953600 val loss: 0.558869
[Epoch 97] ogbg-molbbbp: 0.625579 test loss: 2.933574
[Epoch 98; Iter     5/   55] train: loss: 0.0003586
[Epoch 98; Iter    35/   55] train: loss: 0.0049609
[Epoch 98] ogbg-molbbbp: 0.955890 val loss: 0.571670
[Epoch 98] ogbg-molbbbp: 0.636478 test loss: 3.062011
[Epoch 99; Iter    10/   55] train: loss: 0.0004915
[Epoch 99; Iter    40/   55] train: loss: 0.0002735
[Epoch 99] ogbg-molbbbp: 0.956089 val loss: 0.541098
[Epoch 99] ogbg-molbbbp: 0.641397 test loss: 2.928719
[Epoch 100; Iter    15/   55] train: loss: 0.0006611
[Epoch 100; Iter    45/   55] train: loss: 0.0008855
[Epoch 100] ogbg-molbbbp: 0.955491 val loss: 0.541313
[Epoch 100] ogbg-molbbbp: 0.644097 test loss: 2.902998
[Epoch 101; Iter    20/   55] train: loss: 0.0007905
[Epoch 101; Iter    50/   55] train: loss: 0.0004383
[Epoch 101] ogbg-molbbbp: 0.956587 val loss: 0.558336
[Epoch 101] ogbg-molbbbp: 0.638889 test loss: 3.080042
[Epoch 102; Iter    25/   55] train: loss: 0.0004164
[Epoch 102; Iter    55/   55] train: loss: 0.0073552
[Epoch 102] ogbg-molbbbp: 0.953002 val loss: 0.604065
[Epoch 102] ogbg-molbbbp: 0.648823 test loss: 3.105313
[Epoch 103; Iter    30/   55] train: loss: 0.0108334
[Epoch 103] ogbg-molbbbp: 0.957483 val loss: 0.529593
[Epoch 103] ogbg-molbbbp: 0.649981 test loss: 3.044372
[Epoch 104; Iter     5/   55] train: loss: 0.0007175
[Epoch 104; Iter    35/   55] train: loss: 0.0006794
[Epoch 104] ogbg-molbbbp: 0.956388 val loss: 0.561311
[Epoch 104] ogbg-molbbbp: 0.648534 test loss: 3.203918
[Epoch 105; Iter    10/   55] train: loss: 0.0012981
[Epoch 105; Iter    40/   55] train: loss: 0.0008643
[Epoch 105] ogbg-molbbbp: 0.955193 val loss: 0.601895
[Epoch 105] ogbg-molbbbp: 0.639468 test loss: 3.327038
[Epoch 106; Iter    15/   55] train: loss: 0.0001529
[Epoch 106; Iter    45/   55] train: loss: 0.0001942
[Epoch 106] ogbg-molbbbp: 0.958180 val loss: 0.538916
[Epoch 106] ogbg-molbbbp: 0.652199 test loss: 3.044489
[Epoch 107; Iter    20/   55] train: loss: 0.0004707
[Epoch 107; Iter    50/   55] train: loss: 0.0001473
[Epoch 107] ogbg-molbbbp: 0.958578 val loss: 0.516535
[Epoch 107] ogbg-molbbbp: 0.647280 test loss: 3.123194
[Epoch 108; Iter    25/   55] train: loss: 0.0001518
[Epoch 108; Iter    55/   55] train: loss: 0.0004903
[Epoch 108] ogbg-molbbbp: 0.957682 val loss: 0.552562
[Epoch 108] ogbg-molbbbp: 0.644097 test loss: 3.200528
[Epoch 109; Iter    30/   55] train: loss: 0.0001659
[Epoch 69; Iter    40/   55] train: loss: 0.0013414
[Epoch 69] ogbg-molbbbp: 0.956487 val loss: 0.517943
[Epoch 69] ogbg-molbbbp: 0.638792 test loss: 2.917841
[Epoch 70; Iter    15/   55] train: loss: 0.0109833
[Epoch 70; Iter    45/   55] train: loss: 0.0009392
[Epoch 70] ogbg-molbbbp: 0.951907 val loss: 0.576781
[Epoch 70] ogbg-molbbbp: 0.643711 test loss: 2.949080
[Epoch 71; Iter    20/   55] train: loss: 0.0006142
[Epoch 71; Iter    50/   55] train: loss: 0.0005125
[Epoch 71] ogbg-molbbbp: 0.952206 val loss: 0.516202
[Epoch 71] ogbg-molbbbp: 0.649884 test loss: 2.675247
[Epoch 72; Iter    25/   55] train: loss: 0.0008424
[Epoch 72; Iter    55/   55] train: loss: 0.0107972
[Epoch 72] ogbg-molbbbp: 0.954595 val loss: 0.495488
[Epoch 72] ogbg-molbbbp: 0.644579 test loss: 2.720602
[Epoch 73; Iter    30/   55] train: loss: 0.0012365
[Epoch 73] ogbg-molbbbp: 0.953600 val loss: 0.528360
[Epoch 73] ogbg-molbbbp: 0.645544 test loss: 2.837512
[Epoch 74; Iter     5/   55] train: loss: 0.0003041
[Epoch 74; Iter    35/   55] train: loss: 0.0003626
[Epoch 74] ogbg-molbbbp: 0.949816 val loss: 0.555259
[Epoch 74] ogbg-molbbbp: 0.648148 test loss: 2.770735
[Epoch 75; Iter    10/   55] train: loss: 0.0003319
[Epoch 75; Iter    40/   55] train: loss: 0.0008488
[Epoch 75] ogbg-molbbbp: 0.946629 val loss: 0.803280
[Epoch 75] ogbg-molbbbp: 0.643133 test loss: 3.262159
[Epoch 76; Iter    15/   55] train: loss: 0.0023008
[Epoch 76; Iter    45/   55] train: loss: 0.0085658
[Epoch 76] ogbg-molbbbp: 0.906104 val loss: 1.189978
[Epoch 76] ogbg-molbbbp: 0.608893 test loss: 3.225827
[Epoch 77; Iter    20/   55] train: loss: 0.0012758
[Epoch 77; Iter    50/   55] train: loss: 0.0053507
[Epoch 77] ogbg-molbbbp: 0.954994 val loss: 0.782694
[Epoch 77] ogbg-molbbbp: 0.653646 test loss: 3.170288
[Epoch 78; Iter    25/   55] train: loss: 0.0177207
[Epoch 78; Iter    55/   55] train: loss: 0.0090380
[Epoch 78] ogbg-molbbbp: 0.959873 val loss: 0.512209
[Epoch 78] ogbg-molbbbp: 0.646026 test loss: 2.982348
[Epoch 79; Iter    30/   55] train: loss: 0.0081253
[Epoch 79] ogbg-molbbbp: 0.951608 val loss: 0.607034
[Epoch 79] ogbg-molbbbp: 0.690297 test loss: 2.582855
[Epoch 80; Iter     5/   55] train: loss: 0.1588082
[Epoch 80; Iter    35/   55] train: loss: 0.0019700
[Epoch 80] ogbg-molbbbp: 0.926815 val loss: 0.823550
[Epoch 80] ogbg-molbbbp: 0.640529 test loss: 2.907021
[Epoch 81; Iter    10/   55] train: loss: 0.0039669
[Epoch 81; Iter    40/   55] train: loss: 0.0070804
[Epoch 81] ogbg-molbbbp: 0.956786 val loss: 0.485972
[Epoch 81] ogbg-molbbbp: 0.624132 test loss: 3.294400
[Epoch 82; Iter    15/   55] train: loss: 0.0009451
[Epoch 82; Iter    45/   55] train: loss: 0.0013169
[Epoch 82] ogbg-molbbbp: 0.899831 val loss: 1.434340
[Epoch 82] ogbg-molbbbp: 0.595583 test loss: 2.934101
[Epoch 83; Iter    20/   55] train: loss: 0.0087631
[Epoch 83; Iter    50/   55] train: loss: 0.0005822
[Epoch 83] ogbg-molbbbp: 0.953600 val loss: 0.551230
[Epoch 83] ogbg-molbbbp: 0.648534 test loss: 3.025497
[Epoch 84; Iter    25/   55] train: loss: 0.0044035
[Epoch 84; Iter    55/   55] train: loss: 0.0017902
[Epoch 84] ogbg-molbbbp: 0.945733 val loss: 0.653120
[Epoch 84] ogbg-molbbbp: 0.654321 test loss: 2.741017
[Epoch 85; Iter    30/   55] train: loss: 0.0011052
[Epoch 85] ogbg-molbbbp: 0.955292 val loss: 0.490955
[Epoch 85] ogbg-molbbbp: 0.652874 test loss: 2.746724
[Epoch 86; Iter     5/   55] train: loss: 0.0035470
[Epoch 86; Iter    35/   55] train: loss: 0.0067896
[Epoch 86] ogbg-molbbbp: 0.898935 val loss: 1.336334
[Epoch 86] ogbg-molbbbp: 0.600212 test loss: 3.283538
[Epoch 87; Iter    10/   55] train: loss: 0.0017102
[Epoch 87; Iter    40/   55] train: loss: 0.0002797
[Epoch 87] ogbg-molbbbp: 0.869660 val loss: 2.781119
[Epoch 87] ogbg-molbbbp: 0.614101 test loss: 3.579633
[Epoch 88; Iter    15/   55] train: loss: 0.0007071
[Epoch 88; Iter    45/   55] train: loss: 0.0014567
[Epoch 88] ogbg-molbbbp: 0.853729 val loss: 2.871055
[Epoch 88] ogbg-molbbbp: 0.614873 test loss: 3.521912
[Epoch 89; Iter    20/   55] train: loss: 0.0004535
[Epoch 89; Iter    50/   55] train: loss: 0.0008143
[Epoch 89] ogbg-molbbbp: 0.933187 val loss: 0.703865
[Epoch 89] ogbg-molbbbp: 0.631752 test loss: 2.641282
[Epoch 90; Iter    25/   55] train: loss: 0.0005731
[Epoch 90; Iter    55/   55] train: loss: 0.0095980
[Epoch 90] ogbg-molbbbp: 0.838096 val loss: 4.247920
[Epoch 90] ogbg-molbbbp: 0.609664 test loss: 4.011364
[Epoch 91; Iter    30/   55] train: loss: 0.0007441
[Epoch 91] ogbg-molbbbp: 0.947028 val loss: 0.521171
[Epoch 91] ogbg-molbbbp: 0.640336 test loss: 2.677954
[Epoch 92; Iter     5/   55] train: loss: 0.0003927
[Epoch 92; Iter    35/   55] train: loss: 0.0005512
[Epoch 92] ogbg-molbbbp: 0.950214 val loss: 0.503903
[Epoch 92] ogbg-molbbbp: 0.647280 test loss: 2.799231
[Epoch 93; Iter    10/   55] train: loss: 0.0004076
[Epoch 93; Iter    40/   55] train: loss: 0.0003769
[Epoch 93] ogbg-molbbbp: 0.938962 val loss: 0.657065
[Epoch 93] ogbg-molbbbp: 0.644869 test loss: 2.740086
[Epoch 94; Iter    15/   55] train: loss: 0.0001855
[Epoch 94; Iter    45/   55] train: loss: 0.0002475
[Epoch 94] ogbg-molbbbp: 0.953699 val loss: 0.508707
[Epoch 94] ogbg-molbbbp: 0.646219 test loss: 2.898776
[Epoch 95; Iter    20/   55] train: loss: 0.1071399
[Epoch 95; Iter    50/   55] train: loss: 0.0010101
[Epoch 95] ogbg-molbbbp: 0.945833 val loss: 0.669568
[Epoch 95] ogbg-molbbbp: 0.642072 test loss: 3.048618
[Epoch 96; Iter    25/   55] train: loss: 0.0011989
[Epoch 96; Iter    55/   55] train: loss: 0.0009613
[Epoch 96] ogbg-molbbbp: 0.954097 val loss: 0.575623
[Epoch 96] ogbg-molbbbp: 0.636671 test loss: 3.135589
[Epoch 97; Iter    30/   55] train: loss: 0.0167503
[Epoch 97] ogbg-molbbbp: 0.899731 val loss: 1.720413
[Epoch 97] ogbg-molbbbp: 0.630112 test loss: 3.512083
[Epoch 98; Iter     5/   55] train: loss: 0.0006282
[Epoch 98; Iter    35/   55] train: loss: 0.0082925
[Epoch 98] ogbg-molbbbp: 0.954894 val loss: 0.543106
[Epoch 98] ogbg-molbbbp: 0.659336 test loss: 3.010687
[Epoch 99; Iter    10/   55] train: loss: 0.0003149
[Epoch 99; Iter    40/   55] train: loss: 0.0002006
[Epoch 99] ogbg-molbbbp: 0.956985 val loss: 0.494161
[Epoch 99] ogbg-molbbbp: 0.654996 test loss: 2.948948
[Epoch 100; Iter    15/   55] train: loss: 0.0018557
[Epoch 100; Iter    45/   55] train: loss: 0.0014898
[Epoch 100] ogbg-molbbbp: 0.957383 val loss: 0.493572
[Epoch 100] ogbg-molbbbp: 0.656057 test loss: 2.925219
[Epoch 101; Iter    20/   55] train: loss: 0.0012849
[Epoch 101; Iter    50/   55] train: loss: 0.0001520
[Epoch 101] ogbg-molbbbp: 0.955392 val loss: 0.533251
[Epoch 101] ogbg-molbbbp: 0.655864 test loss: 2.954634
[Epoch 102; Iter    25/   55] train: loss: 0.0002745
[Epoch 102; Iter    55/   55] train: loss: 0.0001104
[Epoch 102] ogbg-molbbbp: 0.928507 val loss: 1.218103
[Epoch 102] ogbg-molbbbp: 0.637539 test loss: 2.986731
[Epoch 103; Iter    30/   55] train: loss: 0.0032775
[Epoch 103] ogbg-molbbbp: 0.884098 val loss: 2.177787
[Epoch 103] ogbg-molbbbp: 0.627411 test loss: 3.396010
[Epoch 104; Iter     5/   55] train: loss: 0.0002940
[Epoch 104; Iter    35/   55] train: loss: 0.0008353
[Epoch 104] ogbg-molbbbp: 0.958976 val loss: 0.536758
[Epoch 104] ogbg-molbbbp: 0.653549 test loss: 3.118758
[Epoch 105; Iter    10/   55] train: loss: 0.0008235
[Epoch 105; Iter    40/   55] train: loss: 0.0008684
[Epoch 105] ogbg-molbbbp: 0.957085 val loss: 0.618645
[Epoch 105] ogbg-molbbbp: 0.657215 test loss: 3.156113
[Epoch 106; Iter    15/   55] train: loss: 0.0004589
[Epoch 106; Iter    45/   55] train: loss: 0.0002873
[Epoch 106] ogbg-molbbbp: 0.956089 val loss: 0.543585
[Epoch 106] ogbg-molbbbp: 0.654128 test loss: 3.100327
[Epoch 107; Iter    20/   55] train: loss: 0.0012348
[Epoch 107; Iter    50/   55] train: loss: 0.0001669
[Epoch 107] ogbg-molbbbp: 0.959375 val loss: 0.473777
[Epoch 107] ogbg-molbbbp: 0.663194 test loss: 2.926432
[Epoch 108; Iter    25/   55] train: loss: 0.0001880
[Epoch 108; Iter    55/   55] train: loss: 0.0007083
[Epoch 108] ogbg-molbbbp: 0.957881 val loss: 0.541513
[Epoch 108] ogbg-molbbbp: 0.660590 test loss: 3.053377
[Epoch 109; Iter    30/   55] train: loss: 0.0001494
[Epoch 69; Iter    40/   55] train: loss: 0.0012666
[Epoch 69] ogbg-molbbbp: 0.952305 val loss: 0.578551
[Epoch 69] ogbg-molbbbp: 0.602238 test loss: 3.169585
[Epoch 70; Iter    15/   55] train: loss: 0.0013933
[Epoch 70; Iter    45/   55] train: loss: 0.0029591
[Epoch 70] ogbg-molbbbp: 0.956089 val loss: 0.506002
[Epoch 70] ogbg-molbbbp: 0.604938 test loss: 2.958675
[Epoch 71; Iter    20/   55] train: loss: 0.0006305
[Epoch 71; Iter    50/   55] train: loss: 0.0031547
[Epoch 71] ogbg-molbbbp: 0.956487 val loss: 0.483593
[Epoch 71] ogbg-molbbbp: 0.603974 test loss: 2.988351
[Epoch 72; Iter    25/   55] train: loss: 0.0006489
[Epoch 72; Iter    55/   55] train: loss: 0.0003257
[Epoch 72] ogbg-molbbbp: 0.956786 val loss: 0.473448
[Epoch 72] ogbg-molbbbp: 0.604745 test loss: 2.999912
[Epoch 73; Iter    30/   55] train: loss: 0.0016534
[Epoch 73] ogbg-molbbbp: 0.956885 val loss: 0.482649
[Epoch 73] ogbg-molbbbp: 0.607446 test loss: 2.914752
[Epoch 74; Iter     5/   55] train: loss: 0.0008717
[Epoch 74; Iter    35/   55] train: loss: 0.0017367
[Epoch 74] ogbg-molbbbp: 0.955292 val loss: 0.514174
[Epoch 74] ogbg-molbbbp: 0.605806 test loss: 3.064393
[Epoch 75; Iter    10/   55] train: loss: 0.0009316
[Epoch 75; Iter    40/   55] train: loss: 0.0010853
[Epoch 75] ogbg-molbbbp: 0.955491 val loss: 0.523254
[Epoch 75] ogbg-molbbbp: 0.604360 test loss: 3.064453
[Epoch 76; Iter    15/   55] train: loss: 0.0035419
[Epoch 76; Iter    45/   55] train: loss: 0.0006118
[Epoch 76] ogbg-molbbbp: 0.955790 val loss: 0.542738
[Epoch 76] ogbg-molbbbp: 0.603974 test loss: 3.074623
[Epoch 77; Iter    20/   55] train: loss: 0.0033244
[Epoch 77; Iter    50/   55] train: loss: 0.0001624
[Epoch 77] ogbg-molbbbp: 0.952206 val loss: 0.526172
[Epoch 77] ogbg-molbbbp: 0.589024 test loss: 3.259922
[Epoch 78; Iter    25/   55] train: loss: 0.0014126
[Epoch 78; Iter    55/   55] train: loss: 0.0009283
[Epoch 78] ogbg-molbbbp: 0.955790 val loss: 0.449342
[Epoch 78] ogbg-molbbbp: 0.602334 test loss: 2.876740
[Epoch 79; Iter    30/   55] train: loss: 0.0003567
[Epoch 79] ogbg-molbbbp: 0.956288 val loss: 0.466800
[Epoch 79] ogbg-molbbbp: 0.612847 test loss: 2.913983
[Epoch 80; Iter     5/   55] train: loss: 0.0001441
[Epoch 80; Iter    35/   55] train: loss: 0.0002466
[Epoch 80] ogbg-molbbbp: 0.935876 val loss: 0.762444
[Epoch 80] ogbg-molbbbp: 0.600791 test loss: 3.203808
[Epoch 81; Iter    10/   55] train: loss: 0.0013460
[Epoch 81; Iter    40/   55] train: loss: 0.0068239
[Epoch 81] ogbg-molbbbp: 0.947924 val loss: 0.567263
[Epoch 81] ogbg-molbbbp: 0.610147 test loss: 3.140440
[Epoch 82; Iter    15/   55] train: loss: 0.0006566
[Epoch 82; Iter    45/   55] train: loss: 0.0003609
[Epoch 82] ogbg-molbbbp: 0.952305 val loss: 0.523611
[Epoch 82] ogbg-molbbbp: 0.607542 test loss: 3.060927
[Epoch 83; Iter    20/   55] train: loss: 0.0016413
[Epoch 83; Iter    50/   55] train: loss: 0.0004295
[Epoch 83] ogbg-molbbbp: 0.950513 val loss: 0.596051
[Epoch 83] ogbg-molbbbp: 0.595679 test loss: 3.425650
[Epoch 84; Iter    25/   55] train: loss: 0.0899169
[Epoch 84; Iter    55/   55] train: loss: 0.0003201
[Epoch 84] ogbg-molbbbp: 0.941750 val loss: 0.672219
[Epoch 84] ogbg-molbbbp: 0.602913 test loss: 3.384629
[Epoch 85; Iter    30/   55] train: loss: 0.0275325
[Epoch 85] ogbg-molbbbp: 0.946928 val loss: 0.618668
[Epoch 85] ogbg-molbbbp: 0.598187 test loss: 3.045867
[Epoch 86; Iter     5/   55] train: loss: 0.0004656
[Epoch 86; Iter    35/   55] train: loss: 0.0055629
[Epoch 86] ogbg-molbbbp: 0.944041 val loss: 0.625684
[Epoch 86] ogbg-molbbbp: 0.589120 test loss: 3.213475
[Epoch 87; Iter    10/   55] train: loss: 0.0029298
[Epoch 87; Iter    40/   55] train: loss: 0.0013526
[Epoch 87] ogbg-molbbbp: 0.947227 val loss: 0.619067
[Epoch 87] ogbg-molbbbp: 0.595968 test loss: 3.007837
[Epoch 88; Iter    15/   55] train: loss: 0.0001584
[Epoch 88; Iter    45/   55] train: loss: 0.0008181
[Epoch 88] ogbg-molbbbp: 0.948820 val loss: 0.606383
[Epoch 88] ogbg-molbbbp: 0.601659 test loss: 3.158883
[Epoch 89; Iter    20/   55] train: loss: 0.0002831
[Epoch 89; Iter    50/   55] train: loss: 0.0002726
[Epoch 89] ogbg-molbbbp: 0.949418 val loss: 0.574885
[Epoch 89] ogbg-molbbbp: 0.600598 test loss: 3.033337
[Epoch 90; Iter    25/   55] train: loss: 0.0004075
[Epoch 90; Iter    55/   55] train: loss: 0.0003805
[Epoch 90] ogbg-molbbbp: 0.948920 val loss: 0.594084
[Epoch 90] ogbg-molbbbp: 0.600791 test loss: 3.056854
[Epoch 91; Iter    30/   55] train: loss: 0.0003324
[Epoch 91] ogbg-molbbbp: 0.947127 val loss: 0.619835
[Epoch 91] ogbg-molbbbp: 0.599055 test loss: 3.191412
[Epoch 92; Iter     5/   55] train: loss: 0.0001637
[Epoch 92; Iter    35/   55] train: loss: 0.0004867
[Epoch 92] ogbg-molbbbp: 0.949019 val loss: 0.551490
[Epoch 92] ogbg-molbbbp: 0.602431 test loss: 3.030292
[Epoch 93; Iter    10/   55] train: loss: 0.0003945
[Epoch 93; Iter    40/   55] train: loss: 0.0001626
[Epoch 93] ogbg-molbbbp: 0.950612 val loss: 0.541396
[Epoch 93] ogbg-molbbbp: 0.593943 test loss: 2.874971
[Epoch 94; Iter    15/   55] train: loss: 0.0006207
[Epoch 94; Iter    45/   55] train: loss: 0.0055014
[Epoch 94] ogbg-molbbbp: 0.948521 val loss: 0.657612
[Epoch 94] ogbg-molbbbp: 0.611015 test loss: 2.854927
[Epoch 95; Iter    20/   55] train: loss: 0.0008075
[Epoch 95; Iter    50/   55] train: loss: 0.0009542
[Epoch 95] ogbg-molbbbp: 0.950214 val loss: 0.588512
[Epoch 95] ogbg-molbbbp: 0.610340 test loss: 2.834639
[Epoch 96; Iter    25/   55] train: loss: 0.0037961
[Epoch 96; Iter    55/   55] train: loss: 0.0002842
[Epoch 96] ogbg-molbbbp: 0.952903 val loss: 0.531671
[Epoch 96] ogbg-molbbbp: 0.615451 test loss: 2.786689
[Epoch 97; Iter    30/   55] train: loss: 0.0007131
[Epoch 97] ogbg-molbbbp: 0.953201 val loss: 0.571472
[Epoch 97] ogbg-molbbbp: 0.612847 test loss: 2.839010
[Epoch 98; Iter     5/   55] train: loss: 0.0005856
[Epoch 98; Iter    35/   55] train: loss: 0.0003990
[Epoch 98] ogbg-molbbbp: 0.952405 val loss: 0.561336
[Epoch 98] ogbg-molbbbp: 0.607060 test loss: 2.786504
[Epoch 99; Iter    10/   55] train: loss: 0.0010284
[Epoch 99; Iter    40/   55] train: loss: 0.0008023
[Epoch 99] ogbg-molbbbp: 0.953301 val loss: 0.537552
[Epoch 99] ogbg-molbbbp: 0.604552 test loss: 2.864833
[Epoch 100; Iter    15/   55] train: loss: 0.0002530
[Epoch 100; Iter    45/   55] train: loss: 0.0003685
[Epoch 100] ogbg-molbbbp: 0.951509 val loss: 0.574426
[Epoch 100] ogbg-molbbbp: 0.606771 test loss: 2.905809
[Epoch 101; Iter    20/   55] train: loss: 0.0005093
[Epoch 101; Iter    50/   55] train: loss: 0.0001900
[Epoch 101] ogbg-molbbbp: 0.953201 val loss: 0.558072
[Epoch 101] ogbg-molbbbp: 0.609086 test loss: 2.734042
[Epoch 102; Iter    25/   55] train: loss: 0.0007287
[Epoch 102; Iter    55/   55] train: loss: 0.0007872
[Epoch 102] ogbg-molbbbp: 0.953102 val loss: 0.588683
[Epoch 102] ogbg-molbbbp: 0.603781 test loss: 2.871641
[Epoch 103; Iter    30/   55] train: loss: 0.0001606
[Epoch 103] ogbg-molbbbp: 0.952504 val loss: 0.598358
[Epoch 103] ogbg-molbbbp: 0.608989 test loss: 2.804178
[Epoch 104; Iter     5/   55] train: loss: 0.0002145
[Epoch 104; Iter    35/   55] train: loss: 0.0001343
[Epoch 104] ogbg-molbbbp: 0.951907 val loss: 0.606668
[Epoch 104] ogbg-molbbbp: 0.607446 test loss: 2.880329
[Epoch 105; Iter    10/   55] train: loss: 0.0003280
[Epoch 105; Iter    40/   55] train: loss: 0.0003063
[Epoch 105] ogbg-molbbbp: 0.951608 val loss: 0.608569
[Epoch 105] ogbg-molbbbp: 0.609086 test loss: 2.903313
[Epoch 106; Iter    15/   55] train: loss: 0.0000950
[Epoch 106; Iter    45/   55] train: loss: 0.0021755
[Epoch 106] ogbg-molbbbp: 0.950612 val loss: 0.596442
[Epoch 106] ogbg-molbbbp: 0.609086 test loss: 2.890875
[Epoch 107; Iter    20/   55] train: loss: 0.0001065
[Epoch 107; Iter    50/   55] train: loss: 0.0001465
[Epoch 107] ogbg-molbbbp: 0.952106 val loss: 0.616455
[Epoch 107] ogbg-molbbbp: 0.606867 test loss: 2.876204
[Epoch 108; Iter    25/   55] train: loss: 0.0003467
[Epoch 108; Iter    55/   55] train: loss: 0.0002346
[Epoch 108] ogbg-molbbbp: 0.951110 val loss: 0.559730
[Epoch 108] ogbg-molbbbp: 0.602141 test loss: 2.847505
[Epoch 109; Iter    30/   55] train: loss: 0.0001562
[Epoch 69; Iter    40/   55] train: loss: 0.0006733
[Epoch 69] ogbg-molbbbp: 0.962561 val loss: 0.443191
[Epoch 69] ogbg-molbbbp: 0.647184 test loss: 2.096930
[Epoch 70; Iter    15/   55] train: loss: 0.0017504
[Epoch 70; Iter    45/   55] train: loss: 0.0012979
[Epoch 70] ogbg-molbbbp: 0.962362 val loss: 0.454228
[Epoch 70] ogbg-molbbbp: 0.643615 test loss: 2.174341
[Epoch 71; Iter    20/   55] train: loss: 0.0006522
[Epoch 71; Iter    50/   55] train: loss: 0.0116921
[Epoch 71] ogbg-molbbbp: 0.962262 val loss: 0.440186
[Epoch 71] ogbg-molbbbp: 0.643133 test loss: 2.175044
[Epoch 72; Iter    25/   55] train: loss: 0.0010690
[Epoch 72; Iter    55/   55] train: loss: 0.0002859
[Epoch 72] ogbg-molbbbp: 0.961964 val loss: 0.423646
[Epoch 72] ogbg-molbbbp: 0.646798 test loss: 2.153755
[Epoch 73; Iter    30/   55] train: loss: 0.0013588
[Epoch 73] ogbg-molbbbp: 0.962362 val loss: 0.498032
[Epoch 73] ogbg-molbbbp: 0.642940 test loss: 2.328226
[Epoch 74; Iter     5/   55] train: loss: 0.0005072
[Epoch 74; Iter    35/   55] train: loss: 0.0006932
[Epoch 74] ogbg-molbbbp: 0.962860 val loss: 0.471287
[Epoch 74] ogbg-molbbbp: 0.646701 test loss: 2.239980
[Epoch 75; Iter    10/   55] train: loss: 0.0003989
[Epoch 75; Iter    40/   55] train: loss: 0.0014875
[Epoch 75] ogbg-molbbbp: 0.964453 val loss: 0.473034
[Epoch 75] ogbg-molbbbp: 0.648823 test loss: 2.270206
[Epoch 76; Iter    15/   55] train: loss: 0.0014049
[Epoch 76; Iter    45/   55] train: loss: 0.0007003
[Epoch 76] ogbg-molbbbp: 0.963855 val loss: 0.457840
[Epoch 76] ogbg-molbbbp: 0.647666 test loss: 2.285258
[Epoch 77; Iter    20/   55] train: loss: 0.0075956
[Epoch 77; Iter    50/   55] train: loss: 0.0003410
[Epoch 77] ogbg-molbbbp: 0.959773 val loss: 0.481795
[Epoch 77] ogbg-molbbbp: 0.638600 test loss: 2.440970
[Epoch 78; Iter    25/   55] train: loss: 0.0006149
[Epoch 78; Iter    55/   55] train: loss: 0.0006159
[Epoch 78] ogbg-molbbbp: 0.955989 val loss: 0.482796
[Epoch 78] ogbg-molbbbp: 0.639371 test loss: 2.312571
[Epoch 79; Iter    30/   55] train: loss: 0.0006340
[Epoch 79] ogbg-molbbbp: 0.954595 val loss: 0.496834
[Epoch 79] ogbg-molbbbp: 0.636863 test loss: 2.362376
[Epoch 80; Iter     5/   55] train: loss: 0.0001939
[Epoch 80; Iter    35/   55] train: loss: 0.0011411
[Epoch 80] ogbg-molbbbp: 0.955392 val loss: 0.536923
[Epoch 80] ogbg-molbbbp: 0.632427 test loss: 2.731230
[Epoch 81; Iter    10/   55] train: loss: 0.0011228
[Epoch 81; Iter    40/   55] train: loss: 0.0016810
[Epoch 81] ogbg-molbbbp: 0.955691 val loss: 0.531780
[Epoch 81] ogbg-molbbbp: 0.609182 test loss: 2.609586
[Epoch 82; Iter    15/   55] train: loss: 0.0010673
[Epoch 82; Iter    45/   55] train: loss: 0.0009180
[Epoch 82] ogbg-molbbbp: 0.962959 val loss: 0.522860
[Epoch 82] ogbg-molbbbp: 0.631076 test loss: 2.672200
[Epoch 83; Iter    20/   55] train: loss: 0.0009842
[Epoch 83; Iter    50/   55] train: loss: 0.0002292
[Epoch 83] ogbg-molbbbp: 0.959275 val loss: 0.588187
[Epoch 83] ogbg-molbbbp: 0.621914 test loss: 2.638637
[Epoch 84; Iter    25/   55] train: loss: 0.0081310
[Epoch 84; Iter    55/   55] train: loss: 0.0002296
[Epoch 84] ogbg-molbbbp: 0.966046 val loss: 0.425770
[Epoch 84] ogbg-molbbbp: 0.658083 test loss: 2.301093
[Epoch 85; Iter    30/   55] train: loss: 0.0008953
[Epoch 85] ogbg-molbbbp: 0.961067 val loss: 0.486850
[Epoch 85] ogbg-molbbbp: 0.640818 test loss: 2.388420
[Epoch 86; Iter     5/   55] train: loss: 0.0005402
[Epoch 86; Iter    35/   55] train: loss: 0.0009515
[Epoch 86] ogbg-molbbbp: 0.958976 val loss: 0.467822
[Epoch 86] ogbg-molbbbp: 0.632234 test loss: 2.479206
[Epoch 87; Iter    10/   55] train: loss: 0.0034168
[Epoch 87; Iter    40/   55] train: loss: 0.0039898
[Epoch 87] ogbg-molbbbp: 0.961167 val loss: 0.539216
[Epoch 87] ogbg-molbbbp: 0.651235 test loss: 2.531022
[Epoch 88; Iter    15/   55] train: loss: 0.0005651
[Epoch 88; Iter    45/   55] train: loss: 0.0004561
[Epoch 88] ogbg-molbbbp: 0.962661 val loss: 0.533231
[Epoch 88] ogbg-molbbbp: 0.643615 test loss: 2.575541
[Epoch 89; Iter    20/   55] train: loss: 0.0002348
[Epoch 89; Iter    50/   55] train: loss: 0.0002302
[Epoch 89] ogbg-molbbbp: 0.961565 val loss: 0.489065
[Epoch 89] ogbg-molbbbp: 0.643711 test loss: 2.418540
[Epoch 90; Iter    25/   55] train: loss: 0.0008797
[Epoch 90; Iter    55/   55] train: loss: 0.0001041
[Epoch 90] ogbg-molbbbp: 0.962461 val loss: 0.500820
[Epoch 90] ogbg-molbbbp: 0.640818 test loss: 2.550426
[Epoch 91; Iter    30/   55] train: loss: 0.0008119
[Epoch 91] ogbg-molbbbp: 0.963258 val loss: 0.490322
[Epoch 91] ogbg-molbbbp: 0.645351 test loss: 2.466688
[Epoch 92; Iter     5/   55] train: loss: 0.0000820
[Epoch 92; Iter    35/   55] train: loss: 0.0001722
[Epoch 92] ogbg-molbbbp: 0.964154 val loss: 0.482187
[Epoch 92] ogbg-molbbbp: 0.643519 test loss: 2.639863
[Epoch 93; Iter    10/   55] train: loss: 0.0009952
[Epoch 93; Iter    40/   55] train: loss: 0.0001589
[Epoch 93] ogbg-molbbbp: 0.962860 val loss: 0.463042
[Epoch 93] ogbg-molbbbp: 0.640914 test loss: 2.562356
[Epoch 94; Iter    15/   55] train: loss: 0.0002869
[Epoch 94; Iter    45/   55] train: loss: 0.0021243
[Epoch 94] ogbg-molbbbp: 0.963457 val loss: 0.499474
[Epoch 94] ogbg-molbbbp: 0.641204 test loss: 2.705527
[Epoch 95; Iter    20/   55] train: loss: 0.0009265
[Epoch 95; Iter    50/   55] train: loss: 0.0027770
[Epoch 95] ogbg-molbbbp: 0.962063 val loss: 0.525719
[Epoch 95] ogbg-molbbbp: 0.642843 test loss: 2.595404
[Epoch 96; Iter    25/   55] train: loss: 0.0007234
[Epoch 96; Iter    55/   55] train: loss: 0.0003829
[Epoch 96] ogbg-molbbbp: 0.965150 val loss: 0.480143
[Epoch 96] ogbg-molbbbp: 0.644869 test loss: 2.627075
[Epoch 97; Iter    30/   55] train: loss: 0.0005799
[Epoch 97] ogbg-molbbbp: 0.961466 val loss: 0.471528
[Epoch 97] ogbg-molbbbp: 0.641493 test loss: 2.477262
[Epoch 98; Iter     5/   55] train: loss: 0.0002463
[Epoch 98; Iter    35/   55] train: loss: 0.0013464
[Epoch 98] ogbg-molbbbp: 0.962063 val loss: 0.541297
[Epoch 98] ogbg-molbbbp: 0.635127 test loss: 2.644807
[Epoch 99; Iter    10/   55] train: loss: 0.0010204
[Epoch 99; Iter    40/   55] train: loss: 0.0002663
[Epoch 99] ogbg-molbbbp: 0.963557 val loss: 0.483831
[Epoch 99] ogbg-molbbbp: 0.638696 test loss: 2.500788
[Epoch 100; Iter    15/   55] train: loss: 0.0002666
[Epoch 100; Iter    45/   55] train: loss: 0.0002089
[Epoch 100] ogbg-molbbbp: 0.962262 val loss: 0.544643
[Epoch 100] ogbg-molbbbp: 0.635031 test loss: 2.637485
[Epoch 101; Iter    20/   55] train: loss: 0.0001376
[Epoch 101; Iter    50/   55] train: loss: 0.0001263
[Epoch 101] ogbg-molbbbp: 0.963059 val loss: 0.504024
[Epoch 101] ogbg-molbbbp: 0.636574 test loss: 2.571018
[Epoch 102; Iter    25/   55] train: loss: 0.0009554
[Epoch 102; Iter    55/   55] train: loss: 0.0002379
[Epoch 102] ogbg-molbbbp: 0.962959 val loss: 0.541836
[Epoch 102] ogbg-molbbbp: 0.634742 test loss: 2.666376
[Epoch 103; Iter    30/   55] train: loss: 0.0000911
[Epoch 103] ogbg-molbbbp: 0.965349 val loss: 0.485325
[Epoch 103] ogbg-molbbbp: 0.636671 test loss: 2.691570
[Epoch 104; Iter     5/   55] train: loss: 0.0001472
[Epoch 104; Iter    35/   55] train: loss: 0.0003956
[Epoch 104] ogbg-molbbbp: 0.966643 val loss: 0.443026
[Epoch 104] ogbg-molbbbp: 0.633488 test loss: 2.654727
[Epoch 105; Iter    10/   55] train: loss: 0.0004720
[Epoch 105; Iter    40/   55] train: loss: 0.0002052
[Epoch 105] ogbg-molbbbp: 0.967141 val loss: 0.450970
[Epoch 105] ogbg-molbbbp: 0.634452 test loss: 2.767877
[Epoch 106; Iter    15/   55] train: loss: 0.0001946
[Epoch 106; Iter    45/   55] train: loss: 0.0012433
[Epoch 106] ogbg-molbbbp: 0.963258 val loss: 0.485607
[Epoch 106] ogbg-molbbbp: 0.633198 test loss: 2.722901
[Epoch 107; Iter    20/   55] train: loss: 0.0005638
[Epoch 107; Iter    50/   55] train: loss: 0.0006677
[Epoch 107] ogbg-molbbbp: 0.960171 val loss: 0.475570
[Epoch 107] ogbg-molbbbp: 0.657022 test loss: 2.393260
[Epoch 108; Iter    25/   55] train: loss: 0.0032088
[Epoch 108; Iter    55/   55] train: loss: 0.0005460
[Epoch 108] ogbg-molbbbp: 0.958678 val loss: 0.518117
[Epoch 108] ogbg-molbbbp: 0.652488 test loss: 2.926462
[Epoch 109; Iter    30/   55] train: loss: 0.0004502
[Epoch 69; Iter    40/   55] train: loss: 0.0016581
[Epoch 69] ogbg-molbbbp: 0.950612 val loss: 0.641036
[Epoch 69] ogbg-molbbbp: 0.665413 test loss: 2.641073
[Epoch 70; Iter    15/   55] train: loss: 0.0018762
[Epoch 70; Iter    45/   55] train: loss: 0.0032264
[Epoch 70] ogbg-molbbbp: 0.953500 val loss: 0.581272
[Epoch 70] ogbg-molbbbp: 0.670139 test loss: 2.571285
[Epoch 71; Iter    20/   55] train: loss: 0.0057375
[Epoch 71; Iter    50/   55] train: loss: 0.0420970
[Epoch 71] ogbg-molbbbp: 0.954496 val loss: 0.591131
[Epoch 71] ogbg-molbbbp: 0.662423 test loss: 2.686475
[Epoch 72; Iter    25/   55] train: loss: 0.0012221
[Epoch 72; Iter    55/   55] train: loss: 0.0021673
[Epoch 72] ogbg-molbbbp: 0.956089 val loss: 0.557499
[Epoch 72] ogbg-molbbbp: 0.684992 test loss: 2.464858
[Epoch 73; Iter    30/   55] train: loss: 0.0012169
[Epoch 73] ogbg-molbbbp: 0.956188 val loss: 0.543837
[Epoch 73] ogbg-molbbbp: 0.684703 test loss: 2.388490
[Epoch 74; Iter     5/   55] train: loss: 0.0015683
[Epoch 74; Iter    35/   55] train: loss: 0.0042673
[Epoch 74] ogbg-molbbbp: 0.954097 val loss: 0.667114
[Epoch 74] ogbg-molbbbp: 0.668885 test loss: 2.681010
[Epoch 75; Iter    10/   55] train: loss: 0.0030122
[Epoch 75; Iter    40/   55] train: loss: 0.0015475
[Epoch 75] ogbg-molbbbp: 0.954396 val loss: 0.582478
[Epoch 75] ogbg-molbbbp: 0.655382 test loss: 2.718796
[Epoch 76; Iter    15/   55] train: loss: 0.0045223
[Epoch 76; Iter    45/   55] train: loss: 0.0030462
[Epoch 76] ogbg-molbbbp: 0.949019 val loss: 0.692456
[Epoch 76] ogbg-molbbbp: 0.639082 test loss: 2.850090
[Epoch 77; Iter    20/   55] train: loss: 0.0004670
[Epoch 77; Iter    50/   55] train: loss: 0.0005302
[Epoch 77] ogbg-molbbbp: 0.953600 val loss: 0.590669
[Epoch 77] ogbg-molbbbp: 0.647280 test loss: 2.755997
[Epoch 78; Iter    25/   55] train: loss: 0.0003145
[Epoch 78; Iter    55/   55] train: loss: 0.0056012
[Epoch 78] ogbg-molbbbp: 0.952903 val loss: 0.745814
[Epoch 78] ogbg-molbbbp: 0.633777 test loss: 3.627593
[Epoch 79; Iter    30/   55] train: loss: 0.0011848
[Epoch 79] ogbg-molbbbp: 0.952803 val loss: 0.628041
[Epoch 79] ogbg-molbbbp: 0.657215 test loss: 2.646625
[Epoch 80; Iter     5/   55] train: loss: 0.0048233
[Epoch 80; Iter    35/   55] train: loss: 0.0006659
[Epoch 80] ogbg-molbbbp: 0.950413 val loss: 0.636394
[Epoch 80] ogbg-molbbbp: 0.666956 test loss: 2.468242
[Epoch 81; Iter    10/   55] train: loss: 0.0035581
[Epoch 81; Iter    40/   55] train: loss: 0.0060734
[Epoch 81] ogbg-molbbbp: 0.954396 val loss: 0.692091
[Epoch 81] ogbg-molbbbp: 0.642072 test loss: 2.715247
[Epoch 82; Iter    15/   55] train: loss: 0.0063418
[Epoch 82; Iter    45/   55] train: loss: 0.0006367
[Epoch 82] ogbg-molbbbp: 0.956089 val loss: 0.597712
[Epoch 82] ogbg-molbbbp: 0.661073 test loss: 2.560544
[Epoch 83; Iter    20/   55] train: loss: 0.0057348
[Epoch 83; Iter    50/   55] train: loss: 0.0010442
[Epoch 83] ogbg-molbbbp: 0.960570 val loss: 0.548552
[Epoch 83] ogbg-molbbbp: 0.661362 test loss: 2.869869
[Epoch 84; Iter    25/   55] train: loss: 0.1671780
[Epoch 84; Iter    55/   55] train: loss: 0.0025125
[Epoch 84] ogbg-molbbbp: 0.945733 val loss: 0.695812
[Epoch 84] ogbg-molbbbp: 0.653260 test loss: 2.705871
[Epoch 85; Iter    30/   55] train: loss: 0.0016797
[Epoch 85] ogbg-molbbbp: 0.955591 val loss: 0.514673
[Epoch 85] ogbg-molbbbp: 0.675154 test loss: 2.150279
[Epoch 86; Iter     5/   55] train: loss: 0.0060900
[Epoch 86; Iter    35/   55] train: loss: 0.0040852
[Epoch 86] ogbg-molbbbp: 0.952803 val loss: 0.626632
[Epoch 86] ogbg-molbbbp: 0.633102 test loss: 2.828051
[Epoch 87; Iter    10/   55] train: loss: 0.0019753
[Epoch 87; Iter    40/   55] train: loss: 0.0021445
[Epoch 87] ogbg-molbbbp: 0.958479 val loss: 0.546618
[Epoch 87] ogbg-molbbbp: 0.673708 test loss: 2.402240
[Epoch 88; Iter    15/   55] train: loss: 0.0022219
[Epoch 88; Iter    45/   55] train: loss: 0.0028357
[Epoch 88] ogbg-molbbbp: 0.953201 val loss: 0.613093
[Epoch 88] ogbg-molbbbp: 0.648052 test loss: 2.697290
[Epoch 89; Iter    20/   55] train: loss: 0.0043488
[Epoch 89; Iter    50/   55] train: loss: 0.0010042
[Epoch 89] ogbg-molbbbp: 0.952106 val loss: 0.615985
[Epoch 89] ogbg-molbbbp: 0.650270 test loss: 2.604670
[Epoch 90; Iter    25/   55] train: loss: 0.0008833
[Epoch 90; Iter    55/   55] train: loss: 0.0113000
[Epoch 90] ogbg-molbbbp: 0.950314 val loss: 0.601536
[Epoch 90] ogbg-molbbbp: 0.652778 test loss: 2.511651
[Epoch 91; Iter    30/   55] train: loss: 0.0021534
[Epoch 91] ogbg-molbbbp: 0.955989 val loss: 0.598383
[Epoch 91] ogbg-molbbbp: 0.649113 test loss: 2.714928
[Epoch 92; Iter     5/   55] train: loss: 0.0006076
[Epoch 92; Iter    35/   55] train: loss: 0.0011745
[Epoch 92] ogbg-molbbbp: 0.952206 val loss: 0.610390
[Epoch 92] ogbg-molbbbp: 0.644483 test loss: 2.662371
[Epoch 93; Iter    10/   55] train: loss: 0.0013612
[Epoch 93; Iter    40/   55] train: loss: 0.0004011
[Epoch 93] ogbg-molbbbp: 0.954097 val loss: 0.658172
[Epoch 93] ogbg-molbbbp: 0.651524 test loss: 2.938210
[Epoch 94; Iter    15/   55] train: loss: 0.0013990
[Epoch 94; Iter    45/   55] train: loss: 0.0025012
[Epoch 94] ogbg-molbbbp: 0.953102 val loss: 0.687019
[Epoch 94] ogbg-molbbbp: 0.656829 test loss: 3.011646
[Epoch 95; Iter    20/   55] train: loss: 0.0012945
[Epoch 95; Iter    50/   55] train: loss: 0.0006424
[Epoch 95] ogbg-molbbbp: 0.953898 val loss: 0.645071
[Epoch 95] ogbg-molbbbp: 0.648341 test loss: 2.861901
[Epoch 96; Iter    25/   55] train: loss: 0.0008237
[Epoch 96; Iter    55/   55] train: loss: 0.0056177
[Epoch 96] ogbg-molbbbp: 0.954197 val loss: 0.676047
[Epoch 96] ogbg-molbbbp: 0.659626 test loss: 2.897319
[Epoch 97; Iter    30/   55] train: loss: 0.0030917
[Epoch 97] ogbg-molbbbp: 0.956587 val loss: 0.616698
[Epoch 97] ogbg-molbbbp: 0.659915 test loss: 2.790831
[Epoch 98; Iter     5/   55] train: loss: 0.0003882
[Epoch 98; Iter    35/   55] train: loss: 0.0027650
[Epoch 98] ogbg-molbbbp: 0.957085 val loss: 0.586582
[Epoch 98] ogbg-molbbbp: 0.670621 test loss: 2.733712
[Epoch 99; Iter    10/   55] train: loss: 0.0002230
[Epoch 99; Iter    40/   55] train: loss: 0.0019433
[Epoch 99] ogbg-molbbbp: 0.956288 val loss: 0.651069
[Epoch 99] ogbg-molbbbp: 0.647473 test loss: 3.100989
[Epoch 100; Iter    15/   55] train: loss: 0.0006450
[Epoch 100; Iter    45/   55] train: loss: 0.0020884
[Epoch 100] ogbg-molbbbp: 0.954894 val loss: 0.638263
[Epoch 100] ogbg-molbbbp: 0.645062 test loss: 3.039264
[Epoch 101; Iter    20/   55] train: loss: 0.0013971
[Epoch 101; Iter    50/   55] train: loss: 0.0012738
[Epoch 101] ogbg-molbbbp: 0.954794 val loss: 0.641066
[Epoch 101] ogbg-molbbbp: 0.642843 test loss: 3.064111
[Epoch 102; Iter    25/   55] train: loss: 0.0033964
[Epoch 102; Iter    55/   55] train: loss: 0.0002424
[Epoch 102] ogbg-molbbbp: 0.954994 val loss: 0.687121
[Epoch 102] ogbg-molbbbp: 0.646894 test loss: 3.107315
[Epoch 103; Iter    30/   55] train: loss: 0.0004252
[Epoch 103] ogbg-molbbbp: 0.956388 val loss: 0.655164
[Epoch 103] ogbg-molbbbp: 0.641782 test loss: 3.035038
[Epoch 104; Iter     5/   55] train: loss: 0.0004527
[Epoch 104; Iter    35/   55] train: loss: 0.0019787
[Epoch 104] ogbg-molbbbp: 0.954097 val loss: 0.697574
[Epoch 104] ogbg-molbbbp: 0.633584 test loss: 3.220571
[Epoch 105; Iter    10/   55] train: loss: 0.0030235
[Epoch 105; Iter    40/   55] train: loss: 0.0001994
[Epoch 105] ogbg-molbbbp: 0.951509 val loss: 0.794273
[Epoch 105] ogbg-molbbbp: 0.620370 test loss: 3.742272
[Epoch 106; Iter    15/   55] train: loss: 0.0001868
[Epoch 106; Iter    45/   55] train: loss: 0.0005427
[Epoch 106] ogbg-molbbbp: 0.958877 val loss: 0.593433
[Epoch 106] ogbg-molbbbp: 0.653067 test loss: 3.020166
[Epoch 107; Iter    20/   55] train: loss: 0.0011580
[Epoch 107; Iter    50/   55] train: loss: 0.0010893
[Epoch 107] ogbg-molbbbp: 0.954894 val loss: 0.659179
[Epoch 107] ogbg-molbbbp: 0.631944 test loss: 3.272637
[Epoch 108; Iter    25/   55] train: loss: 0.0016687
[Epoch 108; Iter    55/   55] train: loss: 0.2702540
[Epoch 108] ogbg-molbbbp: 0.961565 val loss: 0.478101
[Epoch 108] ogbg-molbbbp: 0.625965 test loss: 2.683734
[Epoch 109; Iter    30/   55] train: loss: 0.0006582
[Epoch 69; Iter    40/   55] train: loss: 0.0021278
[Epoch 69] ogbg-molbbbp: 0.935278 val loss: 0.647878
[Epoch 69] ogbg-molbbbp: 0.667631 test loss: 2.347951
[Epoch 70; Iter    15/   55] train: loss: 0.0012305
[Epoch 70; Iter    45/   55] train: loss: 0.0037932
[Epoch 70] ogbg-molbbbp: 0.948223 val loss: 0.544338
[Epoch 70] ogbg-molbbbp: 0.668885 test loss: 2.069759
[Epoch 71; Iter    20/   55] train: loss: 0.0102986
[Epoch 71; Iter    50/   55] train: loss: 0.1433917
[Epoch 71] ogbg-molbbbp: 0.948621 val loss: 0.552950
[Epoch 71] ogbg-molbbbp: 0.670042 test loss: 2.220831
[Epoch 72; Iter    25/   55] train: loss: 0.0020925
[Epoch 72; Iter    55/   55] train: loss: 0.0056398
[Epoch 72] ogbg-molbbbp: 0.948920 val loss: 0.546632
[Epoch 72] ogbg-molbbbp: 0.669464 test loss: 2.300529
[Epoch 73; Iter    30/   55] train: loss: 0.0007521
[Epoch 73] ogbg-molbbbp: 0.944837 val loss: 0.621200
[Epoch 73] ogbg-molbbbp: 0.670525 test loss: 2.320707
[Epoch 74; Iter     5/   55] train: loss: 0.0004246
[Epoch 74; Iter    35/   55] train: loss: 0.0079150
[Epoch 74] ogbg-molbbbp: 0.937071 val loss: 1.102649
[Epoch 74] ogbg-molbbbp: 0.682870 test loss: 2.193433
[Epoch 75; Iter    10/   55] train: loss: 0.0051495
[Epoch 75; Iter    40/   55] train: loss: 0.0024757
[Epoch 75] ogbg-molbbbp: 0.938763 val loss: 1.289871
[Epoch 75] ogbg-molbbbp: 0.654900 test loss: 2.479664
[Epoch 76; Iter    15/   55] train: loss: 0.0035344
[Epoch 76; Iter    45/   55] train: loss: 0.0013522
[Epoch 76] ogbg-molbbbp: 0.936174 val loss: 1.101892
[Epoch 76] ogbg-molbbbp: 0.660397 test loss: 2.302797
[Epoch 77; Iter    20/   55] train: loss: 0.0004590
[Epoch 77; Iter    50/   55] train: loss: 0.0003366
[Epoch 77] ogbg-molbbbp: 0.937668 val loss: 1.040937
[Epoch 77] ogbg-molbbbp: 0.656829 test loss: 2.329589
[Epoch 78; Iter    25/   55] train: loss: 0.0002487
[Epoch 78; Iter    55/   55] train: loss: 0.0907544
[Epoch 78] ogbg-molbbbp: 0.942746 val loss: 1.869580
[Epoch 78] ogbg-molbbbp: 0.675444 test loss: 2.286237
[Epoch 79; Iter    30/   55] train: loss: 0.0069825
[Epoch 79] ogbg-molbbbp: 0.930200 val loss: 2.262581
[Epoch 79] ogbg-molbbbp: 0.645448 test loss: 2.320680
[Epoch 80; Iter     5/   55] train: loss: 0.0032434
[Epoch 80; Iter    35/   55] train: loss: 0.0091363
[Epoch 80] ogbg-molbbbp: 0.928109 val loss: 2.137026
[Epoch 80] ogbg-molbbbp: 0.647569 test loss: 2.446097
[Epoch 81; Iter    10/   55] train: loss: 0.0003889
[Epoch 81; Iter    40/   55] train: loss: 0.0014319
[Epoch 81] ogbg-molbbbp: 0.931893 val loss: 2.632551
[Epoch 81] ogbg-molbbbp: 0.646219 test loss: 2.387366
[Epoch 82; Iter    15/   55] train: loss: 0.0006529
[Epoch 82; Iter    45/   55] train: loss: 0.0004191
[Epoch 82] ogbg-molbbbp: 0.932689 val loss: 2.442712
[Epoch 82] ogbg-molbbbp: 0.658951 test loss: 2.410433
[Epoch 83; Iter    20/   55] train: loss: 0.0021953
[Epoch 83; Iter    50/   55] train: loss: 0.0003806
[Epoch 83] ogbg-molbbbp: 0.936473 val loss: 1.877286
[Epoch 83] ogbg-molbbbp: 0.653646 test loss: 2.536854
[Epoch 84; Iter    25/   55] train: loss: 0.0004677
[Epoch 84; Iter    55/   55] train: loss: 0.0018937
[Epoch 84] ogbg-molbbbp: 0.927313 val loss: 3.076785
[Epoch 84] ogbg-molbbbp: 0.615451 test loss: 2.810301
[Epoch 85; Iter    30/   55] train: loss: 0.0006383
[Epoch 85] ogbg-molbbbp: 0.931295 val loss: 2.547475
[Epoch 85] ogbg-molbbbp: 0.652392 test loss: 2.495512
[Epoch 86; Iter     5/   55] train: loss: 0.0005784
[Epoch 86; Iter    35/   55] train: loss: 0.0022458
[Epoch 86] ogbg-molbbbp: 0.908294 val loss: 2.415778
[Epoch 86] ogbg-molbbbp: 0.646605 test loss: 2.482873
[Epoch 87; Iter    10/   55] train: loss: 0.0035836
[Epoch 87; Iter    40/   55] train: loss: 0.0028497
[Epoch 87] ogbg-molbbbp: 0.937469 val loss: 2.602439
[Epoch 87] ogbg-molbbbp: 0.646605 test loss: 2.572427
[Epoch 88; Iter    15/   55] train: loss: 0.0075610
[Epoch 88; Iter    45/   55] train: loss: 0.0176898
[Epoch 88] ogbg-molbbbp: 0.927213 val loss: 3.851865
[Epoch 88] ogbg-molbbbp: 0.654321 test loss: 2.325330
[Epoch 89; Iter    20/   55] train: loss: 0.0073512
[Epoch 89; Iter    50/   55] train: loss: 0.0011035
[Epoch 89] ogbg-molbbbp: 0.935378 val loss: 3.168550
[Epoch 89] ogbg-molbbbp: 0.675251 test loss: 2.365307
[Epoch 90; Iter    25/   55] train: loss: 0.0010418
[Epoch 90; Iter    55/   55] train: loss: 0.1080656
[Epoch 90] ogbg-molbbbp: 0.935677 val loss: 2.795810
[Epoch 90] ogbg-molbbbp: 0.682870 test loss: 2.293870
[Epoch 91; Iter    30/   55] train: loss: 0.0068248
[Epoch 91] ogbg-molbbbp: 0.936871 val loss: 2.779766
[Epoch 91] ogbg-molbbbp: 0.665413 test loss: 2.286536
[Epoch 92; Iter     5/   55] train: loss: 0.0009673
[Epoch 92; Iter    35/   55] train: loss: 0.0013359
[Epoch 92] ogbg-molbbbp: 0.939361 val loss: 3.019652
[Epoch 92] ogbg-molbbbp: 0.674479 test loss: 2.096271
[Epoch 93; Iter    10/   55] train: loss: 0.0009353
[Epoch 93; Iter    40/   55] train: loss: 0.0002105
[Epoch 93] ogbg-molbbbp: 0.939958 val loss: 2.816342
[Epoch 93] ogbg-molbbbp: 0.679012 test loss: 2.059029
[Epoch 94; Iter    15/   55] train: loss: 0.0012412
[Epoch 94; Iter    45/   55] train: loss: 0.0010032
[Epoch 94] ogbg-molbbbp: 0.937071 val loss: 2.828346
[Epoch 94] ogbg-molbbbp: 0.681134 test loss: 2.118523
[Epoch 95; Iter    20/   55] train: loss: 0.0002579
[Epoch 95; Iter    50/   55] train: loss: 0.0010557
[Epoch 95] ogbg-molbbbp: 0.939162 val loss: 2.668800
[Epoch 95] ogbg-molbbbp: 0.682099 test loss: 1.987624
[Epoch 96; Iter    25/   55] train: loss: 0.0010249
[Epoch 96; Iter    55/   55] train: loss: 0.0028511
[Epoch 96] ogbg-molbbbp: 0.939859 val loss: 2.515773
[Epoch 96] ogbg-molbbbp: 0.679109 test loss: 2.064789
[Epoch 97; Iter    30/   55] train: loss: 0.0011601
[Epoch 97] ogbg-molbbbp: 0.941352 val loss: 2.555905
[Epoch 97] ogbg-molbbbp: 0.675444 test loss: 2.062638
[Epoch 98; Iter     5/   55] train: loss: 0.0003266
[Epoch 98; Iter    35/   55] train: loss: 0.0060751
[Epoch 98] ogbg-molbbbp: 0.938265 val loss: 2.484405
[Epoch 98] ogbg-molbbbp: 0.676312 test loss: 2.390307
[Epoch 99; Iter    10/   55] train: loss: 0.0002896
[Epoch 99; Iter    40/   55] train: loss: 0.0061856
[Epoch 99] ogbg-molbbbp: 0.910983 val loss: 3.571140
[Epoch 99] ogbg-molbbbp: 0.630015 test loss: 2.690123
[Epoch 100; Iter    15/   55] train: loss: 0.0005316
[Epoch 100; Iter    45/   55] train: loss: 0.0097119
[Epoch 100] ogbg-molbbbp: 0.926914 val loss: 2.324278
[Epoch 100] ogbg-molbbbp: 0.643711 test loss: 2.511227
[Epoch 101; Iter    20/   55] train: loss: 0.0003233
[Epoch 101; Iter    50/   55] train: loss: 0.0015887
[Epoch 101] ogbg-molbbbp: 0.926217 val loss: 2.725003
[Epoch 101] ogbg-molbbbp: 0.653453 test loss: 2.347634
[Epoch 102; Iter    25/   55] train: loss: 0.0063458
[Epoch 102; Iter    55/   55] train: loss: 0.0024392
[Epoch 102] ogbg-molbbbp: 0.930598 val loss: 3.062024
[Epoch 102] ogbg-molbbbp: 0.645062 test loss: 2.412714
[Epoch 103; Iter    30/   55] train: loss: 0.0002054
[Epoch 103] ogbg-molbbbp: 0.934183 val loss: 2.776644
[Epoch 103] ogbg-molbbbp: 0.658951 test loss: 2.234136
[Epoch 104; Iter     5/   55] train: loss: 0.0004051
[Epoch 104; Iter    35/   55] train: loss: 0.0006881
[Epoch 104] ogbg-molbbbp: 0.935677 val loss: 2.447682
[Epoch 104] ogbg-molbbbp: 0.666474 test loss: 2.207914
[Epoch 105; Iter    10/   55] train: loss: 0.0010676
[Epoch 105; Iter    40/   55] train: loss: 0.0002515
[Epoch 105] ogbg-molbbbp: 0.939162 val loss: 2.468891
[Epoch 105] ogbg-molbbbp: 0.665316 test loss: 2.384080
[Epoch 106; Iter    15/   55] train: loss: 0.0003746
[Epoch 106; Iter    45/   55] train: loss: 0.0007094
[Epoch 106] ogbg-molbbbp: 0.939859 val loss: 2.534997
[Epoch 106] ogbg-molbbbp: 0.663966 test loss: 2.445462
[Epoch 107; Iter    20/   55] train: loss: 0.0017517
[Epoch 107; Iter    50/   55] train: loss: 0.0029899
[Epoch 107] ogbg-molbbbp: 0.938365 val loss: 2.827290
[Epoch 107] ogbg-molbbbp: 0.667438 test loss: 2.321367
[Epoch 108; Iter    25/   55] train: loss: 0.0002215
[Epoch 108; Iter    55/   55] train: loss: 0.0392596
[Epoch 108] ogbg-molbbbp: 0.934482 val loss: 2.512762
[Epoch 108] ogbg-molbbbp: 0.672550 test loss: 2.642204
[Epoch 109; Iter    30/   55] train: loss: 0.0003952
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.940356 val loss: 0.663224
[Epoch 109] ogbg-molbbbp: 0.651331 test loss: 2.708541
[Epoch 110; Iter     5/   55] train: loss: 0.0003467
[Epoch 110; Iter    35/   55] train: loss: 0.0002078
[Epoch 110] ogbg-molbbbp: 0.943045 val loss: 0.618702
[Epoch 110] ogbg-molbbbp: 0.650945 test loss: 2.529424
[Epoch 111; Iter    10/   55] train: loss: 0.0004181
[Epoch 111; Iter    40/   55] train: loss: 0.0007536
[Epoch 111] ogbg-molbbbp: 0.943244 val loss: 0.610888
[Epoch 111] ogbg-molbbbp: 0.651427 test loss: 2.488436
[Epoch 112; Iter    15/   55] train: loss: 0.0002920
[Epoch 112; Iter    45/   55] train: loss: 0.0004026
[Epoch 112] ogbg-molbbbp: 0.944140 val loss: 0.608142
[Epoch 112] ogbg-molbbbp: 0.651331 test loss: 2.549343
[Epoch 113; Iter    20/   55] train: loss: 0.0276364
[Epoch 113; Iter    50/   55] train: loss: 0.0008001
[Epoch 113] ogbg-molbbbp: 0.942846 val loss: 0.681953
[Epoch 113] ogbg-molbbbp: 0.670139 test loss: 2.530881
[Epoch 114; Iter    25/   55] train: loss: 0.0009293
[Epoch 114; Iter    55/   55] train: loss: 0.0006118
[Epoch 114] ogbg-molbbbp: 0.945733 val loss: 0.583531
[Epoch 114] ogbg-molbbbp: 0.656057 test loss: 2.451412
[Epoch 115; Iter    30/   55] train: loss: 0.0025712
[Epoch 115] ogbg-molbbbp: 0.943742 val loss: 0.639271
[Epoch 115] ogbg-molbbbp: 0.663966 test loss: 2.599290
[Epoch 116; Iter     5/   55] train: loss: 0.0003502
[Epoch 116; Iter    35/   55] train: loss: 0.0003406
[Epoch 116] ogbg-molbbbp: 0.938863 val loss: 0.632281
[Epoch 116] ogbg-molbbbp: 0.672454 test loss: 2.577454
[Epoch 117; Iter    10/   55] train: loss: 0.0004427
[Epoch 117; Iter    40/   55] train: loss: 0.0003264
[Epoch 117] ogbg-molbbbp: 0.938465 val loss: 0.643195
[Epoch 117] ogbg-molbbbp: 0.666763 test loss: 2.646368
[Epoch 118; Iter    15/   55] train: loss: 0.0002855
[Epoch 118; Iter    45/   55] train: loss: 0.0006824
[Epoch 118] ogbg-molbbbp: 0.937369 val loss: 0.677687
[Epoch 118] ogbg-molbbbp: 0.666570 test loss: 2.732087
[Epoch 119; Iter    20/   55] train: loss: 0.0028823
[Epoch 119; Iter    50/   55] train: loss: 0.0003094
[Epoch 119] ogbg-molbbbp: 0.940157 val loss: 0.644272
[Epoch 119] ogbg-molbbbp: 0.671489 test loss: 2.510910
[Epoch 120; Iter    25/   55] train: loss: 0.0001758
[Epoch 120; Iter    55/   55] train: loss: 0.0001265
[Epoch 120] ogbg-molbbbp: 0.942447 val loss: 0.656436
[Epoch 120] ogbg-molbbbp: 0.673515 test loss: 2.485795
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 34.
Statistics on  val_best_checkpoint
mean_pred: -0.2464466691017151
std_pred: 6.090885639190674
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9311176436521993
rocauc: 0.9614656975007467
ogbg-molbbbp: 0.9614656975007467
BCEWithLogitsLoss: 0.29107796187911716
Statistics on  test
mean_pred: 2.1364941596984863
std_pred: 4.075181007385254
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6786432222102801
rocauc: 0.6408179012345678
ogbg-molbbbp: 0.6408179012345678
BCEWithLogitsLoss: 1.5017070259366716
Statistics on  train
mean_pred: 2.92152738571167
std_pred: 2.8319079875946045
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9991159550818994
rocauc: 0.9953468013092523
ogbg-molbbbp: 0.9953468013092523
BCEWithLogitsLoss: 0.09497397487813776
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.952106 val loss: 0.613914
[Epoch 109] ogbg-molbbbp: 0.624035 test loss: 2.659228
[Epoch 110; Iter     5/   55] train: loss: 0.0003121
[Epoch 110; Iter    35/   55] train: loss: 0.0004125
[Epoch 110] ogbg-molbbbp: 0.956487 val loss: 0.466455
[Epoch 110] ogbg-molbbbp: 0.618538 test loss: 2.723364
[Epoch 111; Iter    10/   55] train: loss: 0.0001329
[Epoch 111; Iter    40/   55] train: loss: 0.0006028
[Epoch 111] ogbg-molbbbp: 0.953898 val loss: 0.507809
[Epoch 111] ogbg-molbbbp: 0.616705 test loss: 2.733402
[Epoch 112; Iter    15/   55] train: loss: 0.0002757
[Epoch 112; Iter    45/   55] train: loss: 0.0004446
[Epoch 112] ogbg-molbbbp: 0.952106 val loss: 0.523291
[Epoch 112] ogbg-molbbbp: 0.614390 test loss: 2.738271
[Epoch 113; Iter    20/   55] train: loss: 0.0004418
[Epoch 113; Iter    50/   55] train: loss: 0.0010025
[Epoch 113] ogbg-molbbbp: 0.948123 val loss: 0.574106
[Epoch 113] ogbg-molbbbp: 0.609761 test loss: 2.761939
[Epoch 114; Iter    25/   55] train: loss: 0.0011631
[Epoch 114; Iter    55/   55] train: loss: 0.0000221
[Epoch 114] ogbg-molbbbp: 0.950513 val loss: 0.561677
[Epoch 114] ogbg-molbbbp: 0.605131 test loss: 2.881605
[Epoch 115; Iter    30/   55] train: loss: 0.0002852
[Epoch 115] ogbg-molbbbp: 0.949418 val loss: 0.590992
[Epoch 115] ogbg-molbbbp: 0.607639 test loss: 2.907160
[Epoch 116; Iter     5/   55] train: loss: 0.0000955
[Epoch 116; Iter    35/   55] train: loss: 0.0016265
[Epoch 116] ogbg-molbbbp: 0.950115 val loss: 0.599004
[Epoch 116] ogbg-molbbbp: 0.611690 test loss: 2.899553
[Epoch 117; Iter    10/   55] train: loss: 0.0107528
[Epoch 117; Iter    40/   55] train: loss: 0.0007966
[Epoch 117] ogbg-molbbbp: 0.949915 val loss: 0.578511
[Epoch 117] ogbg-molbbbp: 0.607735 test loss: 2.920590
[Epoch 118; Iter    15/   55] train: loss: 0.0487170
[Epoch 118; Iter    45/   55] train: loss: 0.0001633
[Epoch 118] ogbg-molbbbp: 0.952803 val loss: 0.573014
[Epoch 118] ogbg-molbbbp: 0.611690 test loss: 2.849623
[Epoch 119; Iter    20/   55] train: loss: 0.0001727
[Epoch 119; Iter    50/   55] train: loss: 0.0000658
[Epoch 119] ogbg-molbbbp: 0.952604 val loss: 0.630557
[Epoch 119] ogbg-molbbbp: 0.612461 test loss: 3.008885
[Epoch 120; Iter    25/   55] train: loss: 0.0037611
[Epoch 120; Iter    55/   55] train: loss: 0.0000792
[Epoch 120] ogbg-molbbbp: 0.947824 val loss: 0.590329
[Epoch 120] ogbg-molbbbp: 0.589699 test loss: 3.173163
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 55.
Statistics on  val_best_checkpoint
mean_pred: -10.560150146484375
std_pred: 49.6703987121582
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.93254332382032
rocauc: 0.9605695509309967
ogbg-molbbbp: 0.9605695509309967
BCEWithLogitsLoss: 0.4660474255714299
Statistics on  test
mean_pred: -1.0637078285217285
std_pred: 20.369291305541992
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6160658145998676
rocauc: 0.5880594135802469
ogbg-molbbbp: 0.5880594135802469
BCEWithLogitsLoss: 2.8636803116117204
Statistics on  train
mean_pred: 5.4748430252075195
std_pred: 6.764364719390869
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9999492783468349
rocauc: 0.99971562236881
ogbg-molbbbp: 0.99971562236881
BCEWithLogitsLoss: 0.01342459186509421
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.957782 val loss: 0.565764
[Epoch 109] ogbg-molbbbp: 0.634934 test loss: 3.478353
[Epoch 110; Iter     5/   55] train: loss: 0.0214680
[Epoch 110; Iter    35/   55] train: loss: 0.0002409
[Epoch 110] ogbg-molbbbp: 0.959972 val loss: 0.560041
[Epoch 110] ogbg-molbbbp: 0.635995 test loss: 3.511734
[Epoch 111; Iter    10/   55] train: loss: 0.0001618
[Epoch 111; Iter    40/   55] train: loss: 0.0002167
[Epoch 111] ogbg-molbbbp: 0.959375 val loss: 0.585604
[Epoch 111] ogbg-molbbbp: 0.636574 test loss: 3.540173
[Epoch 112; Iter    15/   55] train: loss: 0.0011293
[Epoch 112; Iter    45/   55] train: loss: 0.0001567
[Epoch 112] ogbg-molbbbp: 0.960669 val loss: 0.567247
[Epoch 112] ogbg-molbbbp: 0.633584 test loss: 3.619979
[Epoch 113; Iter    20/   55] train: loss: 0.0011949
[Epoch 113; Iter    50/   55] train: loss: 0.0001746
[Epoch 113] ogbg-molbbbp: 0.961167 val loss: 0.579684
[Epoch 113] ogbg-molbbbp: 0.636767 test loss: 3.575441
[Epoch 114; Iter    25/   55] train: loss: 0.0001574
[Epoch 114; Iter    55/   55] train: loss: 0.0019705
[Epoch 114] ogbg-molbbbp: 0.955093 val loss: 0.627708
[Epoch 114] ogbg-molbbbp: 0.652681 test loss: 3.298232
[Epoch 115; Iter    30/   55] train: loss: 0.0020571
[Epoch 115] ogbg-molbbbp: 0.954396 val loss: 0.586705
[Epoch 115] ogbg-molbbbp: 0.651717 test loss: 3.387832
[Epoch 116; Iter     5/   55] train: loss: 0.0022084
[Epoch 116; Iter    35/   55] train: loss: 0.0001436
[Epoch 116] ogbg-molbbbp: 0.955193 val loss: 0.611248
[Epoch 116] ogbg-molbbbp: 0.654128 test loss: 3.422330
[Epoch 117; Iter    10/   55] train: loss: 0.0006297
[Epoch 117; Iter    40/   55] train: loss: 0.0002476
[Epoch 117] ogbg-molbbbp: 0.956089 val loss: 0.655951
[Epoch 117] ogbg-molbbbp: 0.644772 test loss: 3.698316
[Epoch 118; Iter    15/   55] train: loss: 0.0002589
[Epoch 118; Iter    45/   55] train: loss: 0.0003107
[Epoch 118] ogbg-molbbbp: 0.955890 val loss: 0.627429
[Epoch 118] ogbg-molbbbp: 0.642168 test loss: 3.660373
[Epoch 119; Iter    20/   55] train: loss: 0.0001721
[Epoch 119; Iter    50/   55] train: loss: 0.0002787
[Epoch 119] ogbg-molbbbp: 0.956188 val loss: 0.612577
[Epoch 119] ogbg-molbbbp: 0.644001 test loss: 3.598571
[Epoch 120; Iter    25/   55] train: loss: 0.0004528
[Epoch 120; Iter    55/   55] train: loss: 0.0006436
[Epoch 120] ogbg-molbbbp: 0.952604 val loss: 0.762888
[Epoch 120] ogbg-molbbbp: 0.638214 test loss: 3.939182
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 39.
Statistics on  val_best_checkpoint
mean_pred: -1.0125813484191895
std_pred: 8.615242958068848
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9395076497791468
rocauc: 0.9639549935278303
ogbg-molbbbp: 0.9639549935278303
BCEWithLogitsLoss: 0.29610290484769003
Statistics on  test
mean_pred: 1.4011666774749756
std_pred: 8.02972412109375
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6329020737707444
rocauc: 0.6226851851851851
ogbg-molbbbp: 0.6226851851851851
BCEWithLogitsLoss: 1.6725324988365173
Statistics on  train
mean_pred: 3.149312973022461
std_pred: 3.580644369125366
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9999404448246844
rocauc: 0.9996933182408734
ogbg-molbbbp: 0.9996933182408734
BCEWithLogitsLoss: 0.038077999431301246
[Epoch 109] ogbg-molbbbp: 0.957582 val loss: 0.549497
[Epoch 109] ogbg-molbbbp: 0.645544 test loss: 3.242438
[Epoch 110; Iter     5/   55] train: loss: 0.0027002
[Epoch 110; Iter    35/   55] train: loss: 0.0031079
[Epoch 110] ogbg-molbbbp: 0.958976 val loss: 0.527971
[Epoch 110] ogbg-molbbbp: 0.651138 test loss: 3.153798
[Epoch 111; Iter    10/   55] train: loss: 0.0001446
[Epoch 111; Iter    40/   55] train: loss: 0.0003629
[Epoch 111] ogbg-molbbbp: 0.960968 val loss: 0.516847
[Epoch 111] ogbg-molbbbp: 0.652006 test loss: 3.136714
[Epoch 112; Iter    15/   55] train: loss: 0.0009529
[Epoch 112; Iter    45/   55] train: loss: 0.0001797
[Epoch 112] ogbg-molbbbp: 0.961067 val loss: 0.512881
[Epoch 112] ogbg-molbbbp: 0.649595 test loss: 3.181276
[Epoch 113; Iter    20/   55] train: loss: 0.0001881
[Epoch 113; Iter    50/   55] train: loss: 0.0002130
[Epoch 113] ogbg-molbbbp: 0.960370 val loss: 0.517261
[Epoch 113] ogbg-molbbbp: 0.651524 test loss: 3.243430
[Epoch 114; Iter    25/   55] train: loss: 0.0005103
[Epoch 114; Iter    55/   55] train: loss: 0.0285796
[Epoch 114] ogbg-molbbbp: 0.949915 val loss: 0.586029
[Epoch 114] ogbg-molbbbp: 0.663773 test loss: 3.029487
[Epoch 115; Iter    30/   55] train: loss: 0.0017589
[Epoch 115] ogbg-molbbbp: 0.949915 val loss: 0.633456
[Epoch 115] ogbg-molbbbp: 0.662230 test loss: 3.297434
[Epoch 116; Iter     5/   55] train: loss: 0.0026301
[Epoch 116; Iter    35/   55] train: loss: 0.0004511
[Epoch 116] ogbg-molbbbp: 0.951210 val loss: 0.603308
[Epoch 116] ogbg-molbbbp: 0.659915 test loss: 3.174413
[Epoch 117; Iter    10/   55] train: loss: 0.0016839
[Epoch 117; Iter    40/   55] train: loss: 0.0004496
[Epoch 117] ogbg-molbbbp: 0.953500 val loss: 0.590182
[Epoch 117] ogbg-molbbbp: 0.661844 test loss: 3.118755
[Epoch 118; Iter    15/   55] train: loss: 0.0002117
[Epoch 118; Iter    45/   55] train: loss: 0.0003993
[Epoch 118] ogbg-molbbbp: 0.954496 val loss: 0.571443
[Epoch 118] ogbg-molbbbp: 0.665799 test loss: 3.056686
[Epoch 119; Iter    20/   55] train: loss: 0.0001392
[Epoch 119; Iter    50/   55] train: loss: 0.0001926
[Epoch 119] ogbg-molbbbp: 0.954197 val loss: 0.599302
[Epoch 119] ogbg-molbbbp: 0.668789 test loss: 3.136844
[Epoch 120; Iter    25/   55] train: loss: 0.0026660
[Epoch 120; Iter    55/   55] train: loss: 0.0012747
[Epoch 120] ogbg-molbbbp: 0.957184 val loss: 0.554298
[Epoch 120] ogbg-molbbbp: 0.654707 test loss: 3.227260
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 60.
Statistics on  val_best_checkpoint
mean_pred: -3.92795467376709
std_pred: 11.626015663146973
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9268157439830831
rocauc: 0.9646519964154137
ogbg-molbbbp: 0.9646519964154137
BCEWithLogitsLoss: 0.4303806019680841
Statistics on  test
mean_pred: 1.8639014959335327
std_pred: 8.656953811645508
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6901585536813271
rocauc: 0.6726466049382717
ogbg-molbbbp: 0.6726466049382717
BCEWithLogitsLoss: 2.504264627184187
Statistics on  train
mean_pred: 5.0540947914123535
std_pred: 6.652057647705078
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.006023433550514958
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.958180 val loss: 0.571022
[Epoch 109] ogbg-molbbbp: 0.638310 test loss: 2.664232
[Epoch 110; Iter     5/   55] train: loss: 0.0011075
[Epoch 110; Iter    35/   55] train: loss: 0.0002694
[Epoch 110] ogbg-molbbbp: 0.958479 val loss: 0.559041
[Epoch 110] ogbg-molbbbp: 0.635802 test loss: 2.533244
[Epoch 111; Iter    10/   55] train: loss: 0.0004394
[Epoch 111; Iter    40/   55] train: loss: 0.0002495
[Epoch 111] ogbg-molbbbp: 0.960570 val loss: 0.535992
[Epoch 111] ogbg-molbbbp: 0.629823 test loss: 2.595390
[Epoch 112; Iter    15/   55] train: loss: 0.0001693
[Epoch 112; Iter    45/   55] train: loss: 0.0008883
[Epoch 112] ogbg-molbbbp: 0.957483 val loss: 0.516798
[Epoch 112] ogbg-molbbbp: 0.627411 test loss: 2.655461
[Epoch 113; Iter    20/   55] train: loss: 0.0021167
[Epoch 113; Iter    50/   55] train: loss: 0.0011388
[Epoch 113] ogbg-molbbbp: 0.961366 val loss: 0.604432
[Epoch 113] ogbg-molbbbp: 0.647859 test loss: 2.787215
[Epoch 114; Iter    25/   55] train: loss: 0.0052470
[Epoch 114; Iter    55/   55] train: loss: 0.0012950
[Epoch 114] ogbg-molbbbp: 0.956089 val loss: 0.588801
[Epoch 114] ogbg-molbbbp: 0.641975 test loss: 2.558002
[Epoch 115; Iter    30/   55] train: loss: 0.0010787
[Epoch 115] ogbg-molbbbp: 0.965548 val loss: 0.396137
[Epoch 115] ogbg-molbbbp: 0.626929 test loss: 2.760520
[Epoch 116; Iter     5/   55] train: loss: 0.0013028
[Epoch 116; Iter    35/   55] train: loss: 0.0029759
[Epoch 116] ogbg-molbbbp: 0.967440 val loss: 0.425954
[Epoch 116] ogbg-molbbbp: 0.630883 test loss: 2.558108
[Epoch 117; Iter    10/   55] train: loss: 0.0008676
[Epoch 117; Iter    40/   55] train: loss: 0.0006212
[Epoch 117] ogbg-molbbbp: 0.966544 val loss: 0.398852
[Epoch 117] ogbg-molbbbp: 0.635320 test loss: 2.507961
[Epoch 118; Iter    15/   55] train: loss: 0.0005097
[Epoch 118; Iter    45/   55] train: loss: 0.0003355
[Epoch 118] ogbg-molbbbp: 0.964851 val loss: 0.401545
[Epoch 118] ogbg-molbbbp: 0.630401 test loss: 2.662553
[Epoch 119; Iter    20/   55] train: loss: 0.0019954
[Epoch 119; Iter    50/   55] train: loss: 0.0010270
[Epoch 119] ogbg-molbbbp: 0.963955 val loss: 0.446140
[Epoch 119] ogbg-molbbbp: 0.629147 test loss: 2.743450
[Epoch 120; Iter    25/   55] train: loss: 0.0004560
[Epoch 120; Iter    55/   55] train: loss: 0.0008091
[Epoch 120] ogbg-molbbbp: 0.964353 val loss: 0.473456
[Epoch 120] ogbg-molbbbp: 0.640529 test loss: 2.840335
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 50.
Statistics on  val_best_checkpoint
mean_pred: -2.3932065963745117
std_pred: 11.87755012512207
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9432285072826725
rocauc: 0.9693318729463308
ogbg-molbbbp: 0.9693318729463308
BCEWithLogitsLoss: 0.2919602715410292
Statistics on  test
mean_pred: 0.7498410940170288
std_pred: 12.692505836486816
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6562120576006234
rocauc: 0.63695987654321
ogbg-molbbbp: 0.63695987654321
BCEWithLogitsLoss: 2.0511786256517683
Statistics on  train
mean_pred: 5.990293502807617
std_pred: 4.997492790222168
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9999989332452905
rocauc: 0.999994423968016
ogbg-molbbbp: 0.999994423968016
BCEWithLogitsLoss: 0.020291756550696764
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.930897 val loss: 2.917454
[Epoch 109] ogbg-molbbbp: 0.663291 test loss: 2.921543
[Epoch 110; Iter     5/   55] train: loss: 0.0001525
[Epoch 110; Iter    35/   55] train: loss: 0.0001625
[Epoch 110] ogbg-molbbbp: 0.928906 val loss: 3.137465
[Epoch 110] ogbg-molbbbp: 0.657504 test loss: 2.789963
[Epoch 111; Iter    10/   55] train: loss: 0.0010062
[Epoch 111; Iter    40/   55] train: loss: 0.0003609
[Epoch 111] ogbg-molbbbp: 0.932490 val loss: 2.801115
[Epoch 111] ogbg-molbbbp: 0.659144 test loss: 2.813571
[Epoch 112; Iter    15/   55] train: loss: 0.0002645
[Epoch 112; Iter    45/   55] train: loss: 0.0012222
[Epoch 112] ogbg-molbbbp: 0.933785 val loss: 2.498143
[Epoch 112] ogbg-molbbbp: 0.664931 test loss: 2.721467
[Epoch 113; Iter    20/   55] train: loss: 0.0003649
[Epoch 113; Iter    50/   55] train: loss: 0.0012476
[Epoch 113] ogbg-molbbbp: 0.934083 val loss: 2.738923
[Epoch 113] ogbg-molbbbp: 0.663194 test loss: 2.857775
[Epoch 114; Iter    25/   55] train: loss: 0.0007419
[Epoch 114; Iter    55/   55] train: loss: 0.0000545
[Epoch 114] ogbg-molbbbp: 0.937967 val loss: 2.385345
[Epoch 114] ogbg-molbbbp: 0.664738 test loss: 2.946357
[Epoch 115; Iter    30/   55] train: loss: 0.0002841
[Epoch 115] ogbg-molbbbp: 0.937867 val loss: 2.212175
[Epoch 115] ogbg-molbbbp: 0.671007 test loss: 2.768529
[Epoch 116; Iter     5/   55] train: loss: 0.0000934
[Epoch 116; Iter    35/   55] train: loss: 0.0022189
[Epoch 116] ogbg-molbbbp: 0.936871 val loss: 2.152678
[Epoch 116] ogbg-molbbbp: 0.663387 test loss: 2.819861
[Epoch 117; Iter    10/   55] train: loss: 0.0239048
[Epoch 117; Iter    40/   55] train: loss: 0.0010344
[Epoch 117] ogbg-molbbbp: 0.944738 val loss: 2.532710
[Epoch 117] ogbg-molbbbp: 0.673032 test loss: 2.785388
[Epoch 118; Iter    15/   55] train: loss: 0.0273926
[Epoch 118; Iter    45/   55] train: loss: 0.0011554
[Epoch 118] ogbg-molbbbp: 0.943742 val loss: 2.549927
[Epoch 118] ogbg-molbbbp: 0.672164 test loss: 2.649889
[Epoch 119; Iter    20/   55] train: loss: 0.0001486
[Epoch 119; Iter    50/   55] train: loss: 0.0002210
[Epoch 119] ogbg-molbbbp: 0.945435 val loss: 2.289806
[Epoch 119] ogbg-molbbbp: 0.678627 test loss: 2.586667
[Epoch 120; Iter    25/   55] train: loss: 0.0033704
[Epoch 120; Iter    55/   55] train: loss: 0.0001545
[Epoch 120] ogbg-molbbbp: 0.941651 val loss: 2.457142
[Epoch 120] ogbg-molbbbp: 0.673418 test loss: 2.653180
[Epoch 121; Iter    30/   55] train: loss: 0.0016190
[Epoch 121] ogbg-molbbbp: 0.943841 val loss: 2.436741
[Epoch 121] ogbg-molbbbp: 0.676505 test loss: 2.576401
[Epoch 122; Iter     5/   55] train: loss: 0.0015576
[Epoch 122; Iter    35/   55] train: loss: 0.0003038
[Epoch 122] ogbg-molbbbp: 0.941352 val loss: 2.437077
[Epoch 122] ogbg-molbbbp: 0.682967 test loss: 2.566009
[Epoch 123; Iter    10/   55] train: loss: 0.0009155
[Epoch 123; Iter    40/   55] train: loss: 0.0000725
[Epoch 123] ogbg-molbbbp: 0.942945 val loss: 2.309152
[Epoch 123] ogbg-molbbbp: 0.685089 test loss: 2.445403
[Epoch 124; Iter    15/   55] train: loss: 0.0001453
[Epoch 124; Iter    45/   55] train: loss: 0.0002136
[Epoch 124] ogbg-molbbbp: 0.939162 val loss: 2.775144
[Epoch 124] ogbg-molbbbp: 0.676987 test loss: 2.610417
[Epoch 125; Iter    20/   55] train: loss: 0.0193340
[Epoch 125; Iter    50/   55] train: loss: 0.0002173
[Epoch 125] ogbg-molbbbp: 0.939659 val loss: 2.863536
[Epoch 125] ogbg-molbbbp: 0.668981 test loss: 2.872098
[Epoch 126; Iter    25/   55] train: loss: 0.0011140
[Epoch 126; Iter    55/   55] train: loss: 0.0003634
[Epoch 126] ogbg-molbbbp: 0.944339 val loss: 3.048366
[Epoch 126] ogbg-molbbbp: 0.670428 test loss: 3.043964
[Epoch 127; Iter    30/   55] train: loss: 0.0001826
[Epoch 127] ogbg-molbbbp: 0.940556 val loss: 3.510475
[Epoch 127] ogbg-molbbbp: 0.662326 test loss: 2.942892
[Epoch 128; Iter     5/   55] train: loss: 0.0015760
[Epoch 128; Iter    35/   55] train: loss: 0.0002257
[Epoch 128] ogbg-molbbbp: 0.941153 val loss: 3.728910
[Epoch 128] ogbg-molbbbp: 0.658758 test loss: 2.933567
[Epoch 129; Iter    10/   55] train: loss: 0.0001065
[Epoch 129; Iter    40/   55] train: loss: 0.0001989
[Epoch 129] ogbg-molbbbp: 0.941153 val loss: 3.554438
[Epoch 129] ogbg-molbbbp: 0.661362 test loss: 2.937385
[Epoch 130; Iter    15/   55] train: loss: 0.0000995
[Epoch 130; Iter    45/   55] train: loss: 0.0005508
[Epoch 130] ogbg-molbbbp: 0.940954 val loss: 3.370943
[Epoch 130] ogbg-molbbbp: 0.663098 test loss: 2.908313
[Epoch 131; Iter    20/   55] train: loss: 0.0006117
[Epoch 131; Iter    50/   55] train: loss: 0.0002308
[Epoch 131] ogbg-molbbbp: 0.939361 val loss: 3.519476
[Epoch 131] ogbg-molbbbp: 0.665895 test loss: 2.900336
[Epoch 132; Iter    25/   55] train: loss: 0.0009720
[Epoch 132; Iter    55/   55] train: loss: 0.0009072
[Epoch 132] ogbg-molbbbp: 0.941551 val loss: 3.281441
[Epoch 132] ogbg-molbbbp: 0.660590 test loss: 2.824726
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 132 epochs. Best model checkpoint was in epoch 72.
Statistics on  val_best_checkpoint
mean_pred: -4.80112361907959
std_pred: 12.961512565612793
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9089278304044816
rocauc: 0.9489196455242458
ogbg-molbbbp: 0.9489196455242458
BCEWithLogitsLoss: 0.5466323516198567
Statistics on  test
mean_pred: 0.6856209635734558
std_pred: 9.403738975524902
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6871316107395596
rocauc: 0.6694637345679012
ogbg-molbbbp: 0.6694637345679012
BCEWithLogitsLoss: 2.3005291734422957
Statistics on  train
mean_pred: 6.276303768157959
std_pred: 7.450326442718506
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.0031899557840502396
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.1.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.1.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.1.yml --seed 6 --device cuda:2
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.950314 val loss: 0.569959
[Epoch 109] ogbg-molbbbp: 0.606578 test loss: 2.773646
[Epoch 110; Iter     5/   55] train: loss: 0.0003262
[Epoch 110; Iter    35/   55] train: loss: 0.0007424
[Epoch 110] ogbg-molbbbp: 0.951409 val loss: 0.568473
[Epoch 110] ogbg-molbbbp: 0.605999 test loss: 2.772718
[Epoch 111; Iter    10/   55] train: loss: 0.0001082
[Epoch 111; Iter    40/   55] train: loss: 0.0000889
[Epoch 111] ogbg-molbbbp: 0.950812 val loss: 0.616615
[Epoch 111] ogbg-molbbbp: 0.603781 test loss: 2.904421
[Epoch 112; Iter    15/   55] train: loss: 0.0002178
[Epoch 112; Iter    45/   55] train: loss: 0.0003613
[Epoch 112] ogbg-molbbbp: 0.950214 val loss: 0.624914
[Epoch 112] ogbg-molbbbp: 0.606481 test loss: 2.825222
[Epoch 113; Iter    20/   55] train: loss: 0.0061847
[Epoch 113; Iter    50/   55] train: loss: 0.0004697
[Epoch 113] ogbg-molbbbp: 0.954496 val loss: 0.631951
[Epoch 113] ogbg-molbbbp: 0.608314 test loss: 2.893284
[Epoch 114; Iter    25/   55] train: loss: 0.0005432
[Epoch 114; Iter    55/   55] train: loss: 0.0040529
[Epoch 114] ogbg-molbbbp: 0.952903 val loss: 0.600701
[Epoch 114] ogbg-molbbbp: 0.613715 test loss: 2.861745
[Epoch 115; Iter    30/   55] train: loss: 0.0001967
[Epoch 115] ogbg-molbbbp: 0.954197 val loss: 0.613277
[Epoch 115] ogbg-molbbbp: 0.613040 test loss: 2.893135
[Epoch 116; Iter     5/   55] train: loss: 0.0001398
[Epoch 116; Iter    35/   55] train: loss: 0.0006023
[Epoch 116] ogbg-molbbbp: 0.955691 val loss: 0.547376
[Epoch 116] ogbg-molbbbp: 0.613812 test loss: 2.734032
[Epoch 117; Iter    10/   55] train: loss: 0.0001151
[Epoch 117; Iter    40/   55] train: loss: 0.0001998
[Epoch 117] ogbg-molbbbp: 0.954994 val loss: 0.570514
[Epoch 117] ogbg-molbbbp: 0.613233 test loss: 2.876158
[Epoch 118; Iter    15/   55] train: loss: 0.0001768
[Epoch 118; Iter    45/   55] train: loss: 0.0002972
[Epoch 118] ogbg-molbbbp: 0.953301 val loss: 0.573282
[Epoch 118] ogbg-molbbbp: 0.609954 test loss: 2.971942
[Epoch 119; Iter    20/   55] train: loss: 0.0007562
[Epoch 119; Iter    50/   55] train: loss: 0.0002649
[Epoch 119] ogbg-molbbbp: 0.949617 val loss: 0.674782
[Epoch 119] ogbg-molbbbp: 0.614873 test loss: 3.170334
[Epoch 120; Iter    25/   55] train: loss: 0.0002899
[Epoch 120; Iter    55/   55] train: loss: 0.0008813
[Epoch 120] ogbg-molbbbp: 0.956188 val loss: 0.655976
[Epoch 120] ogbg-molbbbp: 0.615258 test loss: 3.000510
[Epoch 121; Iter    30/   55] train: loss: 0.0004409
[Epoch 121] ogbg-molbbbp: 0.956388 val loss: 0.567661
[Epoch 121] ogbg-molbbbp: 0.615355 test loss: 2.908300
[Epoch 122; Iter     5/   55] train: loss: 0.0001441
[Epoch 122; Iter    35/   55] train: loss: 0.0027728
[Epoch 122] ogbg-molbbbp: 0.956188 val loss: 0.622219
[Epoch 122] ogbg-molbbbp: 0.616995 test loss: 2.978348
[Epoch 123; Iter    10/   55] train: loss: 0.0002538
[Epoch 123; Iter    40/   55] train: loss: 0.0017656
[Epoch 123] ogbg-molbbbp: 0.950513 val loss: 0.589704
[Epoch 123] ogbg-molbbbp: 0.622685 test loss: 3.041884
[Epoch 124; Iter    15/   55] train: loss: 0.0003472
[Epoch 124; Iter    45/   55] train: loss: 0.0004306
[Epoch 124] ogbg-molbbbp: 0.953102 val loss: 0.590329
[Epoch 124] ogbg-molbbbp: 0.625096 test loss: 2.911866
[Epoch 125; Iter    20/   55] train: loss: 0.0004542
[Epoch 125; Iter    50/   55] train: loss: 0.0005706
[Epoch 125] ogbg-molbbbp: 0.952305 val loss: 0.549248
[Epoch 125] ogbg-molbbbp: 0.625193 test loss: 2.839026
[Epoch 126; Iter    25/   55] train: loss: 0.0001480
[Epoch 126; Iter    55/   55] train: loss: 0.0576180
[Epoch 126] ogbg-molbbbp: 0.953201 val loss: 0.561109
[Epoch 126] ogbg-molbbbp: 0.623457 test loss: 2.849125
[Epoch 127; Iter    30/   55] train: loss: 0.0004622
[Epoch 127] ogbg-molbbbp: 0.953002 val loss: 0.606047
[Epoch 127] ogbg-molbbbp: 0.626350 test loss: 2.820980
[Epoch 128; Iter     5/   55] train: loss: 0.0002510
[Epoch 128; Iter    35/   55] train: loss: 0.0030324
[Epoch 128] ogbg-molbbbp: 0.954097 val loss: 0.599035
[Epoch 128] ogbg-molbbbp: 0.626254 test loss: 2.809064
[Epoch 129; Iter    10/   55] train: loss: 0.0001671
[Epoch 129; Iter    40/   55] train: loss: 0.0001052
[Epoch 129] ogbg-molbbbp: 0.953799 val loss: 0.617341
[Epoch 129] ogbg-molbbbp: 0.625482 test loss: 2.875534
[Epoch 130; Iter    15/   55] train: loss: 0.0002526
[Epoch 130; Iter    45/   55] train: loss: 0.0001748
[Epoch 130] ogbg-molbbbp: 0.953400 val loss: 0.618876
[Epoch 130] ogbg-molbbbp: 0.622685 test loss: 3.014931
[Epoch 131; Iter    20/   55] train: loss: 0.0001365
[Epoch 131; Iter    50/   55] train: loss: 0.0002307
[Epoch 131] ogbg-molbbbp: 0.954097 val loss: 0.659226
[Epoch 131] ogbg-molbbbp: 0.623939 test loss: 3.011970
[Epoch 132; Iter    25/   55] train: loss: 0.0005807
[Epoch 132; Iter    55/   55] train: loss: 0.0029023
[Epoch 132] ogbg-molbbbp: 0.954695 val loss: 0.667095
[Epoch 132] ogbg-molbbbp: 0.624518 test loss: 2.994716
[Epoch 133; Iter    30/   55] train: loss: 0.0001613
[Epoch 133] ogbg-molbbbp: 0.951807 val loss: 0.516082
[Epoch 133] ogbg-molbbbp: 0.627411 test loss: 2.789057
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 133 epochs. Best model checkpoint was in epoch 73.
Statistics on  val_best_checkpoint
mean_pred: -3.85190486907959
std_pred: 13.805866241455078
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9246591486212569
rocauc: 0.9568853928109131
ogbg-molbbbp: 0.9568853928109131
BCEWithLogitsLoss: 0.4826491948749338
Statistics on  test
mean_pred: 2.491657018661499
std_pred: 10.035820960998535
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.679174995155954
rocauc: 0.607445987654321
ogbg-molbbbp: 0.607445987654321
BCEWithLogitsLoss: 2.9147523811885288
Statistics on  train
mean_pred: 6.775483131408691
std_pred: 7.377865791320801
mean_targets: 0.839362382888794
std_targets: 0.36730900406837463
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.0004168329916534607
[Epoch 109] ogbg-molbbbp: 0.954396 val loss: 0.580394
[Epoch 109] ogbg-molbbbp: 0.686053 test loss: 2.431423
[Epoch 110; Iter     5/   55] train: loss: 0.0099553
[Epoch 110; Iter    35/   55] train: loss: 0.0078966
[Epoch 110] ogbg-molbbbp: 0.951409 val loss: 0.780384
[Epoch 110] ogbg-molbbbp: 0.647184 test loss: 3.351207
[Epoch 111; Iter    10/   55] train: loss: 0.0004166
[Epoch 111; Iter    40/   55] train: loss: 0.0039558
[Epoch 111] ogbg-molbbbp: 0.954595 val loss: 0.710149
[Epoch 111] ogbg-molbbbp: 0.665123 test loss: 3.156251
[Epoch 112; Iter    15/   55] train: loss: 0.0003228
[Epoch 112; Iter    45/   55] train: loss: 0.0005408
[Epoch 112] ogbg-molbbbp: 0.955093 val loss: 0.670086
[Epoch 112] ogbg-molbbbp: 0.650367 test loss: 3.199287
[Epoch 113; Iter    20/   55] train: loss: 0.0021107
[Epoch 113; Iter    50/   55] train: loss: 0.0042786
[Epoch 113] ogbg-molbbbp: 0.953600 val loss: 0.709344
[Epoch 113] ogbg-molbbbp: 0.655285 test loss: 3.262506
[Epoch 114; Iter    25/   55] train: loss: 0.0007300
[Epoch 114; Iter    55/   55] train: loss: 0.0007625
[Epoch 114] ogbg-molbbbp: 0.955491 val loss: 0.688282
[Epoch 114] ogbg-molbbbp: 0.664352 test loss: 3.213633
[Epoch 115; Iter    30/   55] train: loss: 0.0005523
[Epoch 115] ogbg-molbbbp: 0.954595 val loss: 0.669782
[Epoch 115] ogbg-molbbbp: 0.661458 test loss: 3.171307
[Epoch 116; Iter     5/   55] train: loss: 0.0002400
[Epoch 116; Iter    35/   55] train: loss: 0.0062532
[Epoch 116] ogbg-molbbbp: 0.953998 val loss: 0.672254
[Epoch 116] ogbg-molbbbp: 0.660880 test loss: 3.183858
[Epoch 117; Iter    10/   55] train: loss: 0.0397403
[Epoch 117; Iter    40/   55] train: loss: 0.0025663
[Epoch 117] ogbg-molbbbp: 0.951907 val loss: 0.728617
[Epoch 117] ogbg-molbbbp: 0.653260 test loss: 3.339469
[Epoch 118; Iter    15/   55] train: loss: 0.0606379
[Epoch 118; Iter    45/   55] train: loss: 0.0009967
[Epoch 118] ogbg-molbbbp: 0.950712 val loss: 0.722853
[Epoch 118] ogbg-molbbbp: 0.638792 test loss: 3.380062
[Epoch 119; Iter    20/   55] train: loss: 0.0003917
[Epoch 119; Iter    50/   55] train: loss: 0.0001557
[Epoch 119] ogbg-molbbbp: 0.955292 val loss: 0.665531
[Epoch 119] ogbg-molbbbp: 0.656250 test loss: 3.236663
[Epoch 120; Iter    25/   55] train: loss: 0.0044862
[Epoch 120; Iter    55/   55] train: loss: 0.0016453
[Epoch 120] ogbg-molbbbp: 0.953102 val loss: 0.720282
[Epoch 120] ogbg-molbbbp: 0.653549 test loss: 3.315391
[Epoch 121; Iter    30/   55] train: loss: 0.0006231
[Epoch 121] ogbg-molbbbp: 0.956388 val loss: 0.623607
[Epoch 121] ogbg-molbbbp: 0.655575 test loss: 3.086148
[Epoch 122; Iter     5/   55] train: loss: 0.0017552
[Epoch 122; Iter    35/   55] train: loss: 0.0003019
[Epoch 122] ogbg-molbbbp: 0.951708 val loss: 0.720548
[Epoch 122] ogbg-molbbbp: 0.649981 test loss: 3.268577
[Epoch 123; Iter    10/   55] train: loss: 0.0013478
[Epoch 123; Iter    40/   55] train: loss: 0.0001819
[Epoch 123] ogbg-molbbbp: 0.953002 val loss: 0.658683
[Epoch 123] ogbg-molbbbp: 0.648727 test loss: 3.038234
[Epoch 124; Iter    15/   55] train: loss: 0.0002678
[Epoch 124; Iter    45/   55] train: loss: 0.0006508
[Epoch 124] ogbg-molbbbp: 0.945435 val loss: 0.799926
[Epoch 124] ogbg-molbbbp: 0.623650 test loss: 3.493687
[Epoch 125; Iter    20/   55] train: loss: 0.0205747
[Epoch 125; Iter    50/   55] train: loss: 0.0002515
[Epoch 125] ogbg-molbbbp: 0.949915 val loss: 0.726460
[Epoch 125] ogbg-molbbbp: 0.665027 test loss: 3.007727
[Epoch 126; Iter    25/   55] train: loss: 0.0019768
[Epoch 126; Iter    55/   55] train: loss: 0.0006791
[Epoch 126] ogbg-molbbbp: 0.949716 val loss: 0.763320
[Epoch 126] ogbg-molbbbp: 0.661748 test loss: 3.289129
[Epoch 127; Iter    30/   55] train: loss: 0.0003523
[Epoch 127] ogbg-molbbbp: 0.949418 val loss: 0.697872
[Epoch 127] ogbg-molbbbp: 0.640818 test loss: 3.305364
[Epoch 128; Iter     5/   55] train: loss: 0.0005699
[Epoch 128; Iter    35/   55] train: loss: 0.0003893
[Epoch 128] ogbg-molbbbp: 0.952504 val loss: 0.688209
[Epoch 128] ogbg-molbbbp: 0.657407 test loss: 3.215167
[Epoch 129; Iter    10/   55] train: loss: 0.0001689
[Epoch 129; Iter    40/   55] train: loss: 0.0003976
[Epoch 129] ogbg-molbbbp: 0.949617 val loss: 0.760317
[Epoch 129] ogbg-molbbbp: 0.650656 test loss: 3.436070
[Epoch 130; Iter    15/   55] train: loss: 0.0002530
[Epoch 130; Iter    45/   55] train: loss: 0.0006711
[Epoch 130] ogbg-molbbbp: 0.951011 val loss: 0.770384
[Epoch 130] ogbg-molbbbp: 0.653935 test loss: 3.494751
[Epoch 131; Iter    20/   55] train: loss: 0.0016898
[Epoch 131; Iter    50/   55] train: loss: 0.0003288
[Epoch 131] ogbg-molbbbp: 0.946331 val loss: 0.836420
[Epoch 131] ogbg-molbbbp: 0.646701 test loss: 3.583516
[Epoch 132; Iter    25/   55] train: loss: 0.0009663
[Epoch 132; Iter    55/   55] train: loss: 0.0003224
[Epoch 132] ogbg-molbbbp: 0.946629 val loss: 0.806698
[Epoch 132] ogbg-molbbbp: 0.646123 test loss: 3.567251
[Epoch 133; Iter    30/   55] train: loss: 0.0000732
[Epoch 133] ogbg-molbbbp: 0.948322 val loss: 0.811340
[Epoch 133] ogbg-molbbbp: 0.653356 test loss: 3.454405
[Epoch 134; Iter     5/   55] train: loss: 0.0000776
[Epoch 134; Iter    35/   55] train: loss: 0.0004416
[Epoch 134] ogbg-molbbbp: 0.949019 val loss: 0.759650
[Epoch 134] ogbg-molbbbp: 0.648438 test loss: 3.403734
[Epoch 135; Iter    10/   55] train: loss: 0.0003641
[Epoch 135; Iter    40/   55] train: loss: 0.0059136
[Epoch 135] ogbg-molbbbp: 0.944041 val loss: 0.993477
[Epoch 135] ogbg-molbbbp: 0.645833 test loss: 3.709506
[Epoch 136; Iter    15/   55] train: loss: 0.0076020
[Epoch 136; Iter    45/   55] train: loss: 0.0003020
[Epoch 136] ogbg-molbbbp: 0.943144 val loss: 0.933847
[Epoch 136] ogbg-molbbbp: 0.670139 test loss: 3.212703
[Epoch 137; Iter    20/   55] train: loss: 0.0034479
[Epoch 137; Iter    50/   55] train: loss: 0.0003153
[Epoch 137] ogbg-molbbbp: 0.941750 val loss: 0.980438
[Epoch 137] ogbg-molbbbp: 0.657600 test loss: 3.563592
[Epoch 138; Iter    25/   55] train: loss: 0.0029031
[Epoch 138; Iter    55/   55] train: loss: 0.1241769
[Epoch 138] ogbg-molbbbp: 0.946032 val loss: 0.843357
[Epoch 138] ogbg-molbbbp: 0.661265 test loss: 3.208883
[Epoch 139; Iter    30/   55] train: loss: 0.0005960
[Epoch 139] ogbg-molbbbp: 0.939361 val loss: 0.928269
[Epoch 139] ogbg-molbbbp: 0.647666 test loss: 3.331976
[Epoch 140; Iter     5/   55] train: loss: 0.0090283
[Epoch 140; Iter    35/   55] train: loss: 0.0003842
[Epoch 140] ogbg-molbbbp: 0.941253 val loss: 0.913282
[Epoch 140] ogbg-molbbbp: 0.644869 test loss: 3.403231
[Epoch 141; Iter    10/   55] train: loss: 0.0007061
[Epoch 141; Iter    40/   55] train: loss: 0.0019464
[Epoch 141] ogbg-molbbbp: 0.944837 val loss: 0.881904
[Epoch 141] ogbg-molbbbp: 0.652585 test loss: 3.360966
[Epoch 142; Iter    15/   55] train: loss: 0.0004242
[Epoch 142; Iter    45/   55] train: loss: 0.0010512
[Epoch 142] ogbg-molbbbp: 0.942248 val loss: 0.936352
[Epoch 142] ogbg-molbbbp: 0.652199 test loss: 3.499864
[Epoch 143; Iter    20/   55] train: loss: 0.0002791
[Epoch 143; Iter    50/   55] train: loss: 0.0003283
[Epoch 143] ogbg-molbbbp: 0.946829 val loss: 0.880293
[Epoch 143] ogbg-molbbbp: 0.654610 test loss: 3.452826
[Epoch 144; Iter    25/   55] train: loss: 0.0003944
[Epoch 144; Iter    55/   55] train: loss: 0.0307561
[Epoch 144] ogbg-molbbbp: 0.949119 val loss: 0.822848
[Epoch 144] ogbg-molbbbp: 0.661265 test loss: 3.335599
[Epoch 145; Iter    30/   55] train: loss: 0.0009273
[Epoch 145] ogbg-molbbbp: 0.947526 val loss: 0.842285
[Epoch 145] ogbg-molbbbp: 0.654803 test loss: 3.428348
[Epoch 146; Iter     5/   55] train: loss: 0.0010072
[Epoch 146; Iter    35/   55] train: loss: 0.0003455
[Epoch 146] ogbg-molbbbp: 0.947127 val loss: 0.858464
[Epoch 146] ogbg-molbbbp: 0.654032 test loss: 3.467455
[Epoch 147; Iter    10/   55] train: loss: 0.0002763
[Epoch 147; Iter    40/   55] train: loss: 0.0003909
[Epoch 147] ogbg-molbbbp: 0.945833 val loss: 0.861616
[Epoch 147] ogbg-molbbbp: 0.657118 test loss: 3.424602
[Epoch 148; Iter    15/   55] train: loss: 0.0003507
[Epoch 148; Iter    45/   55] train: loss: 0.0001157
[Epoch 148] ogbg-molbbbp: 0.946629 val loss: 0.841202
[Epoch 148] ogbg-molbbbp: 0.657022 test loss: 3.412142
[Epoch 69; Iter    40/   55] train: loss: 0.0413583
[Epoch 69] ogbg-molbbbp: 0.948721 val loss: 0.423636
[Epoch 69] ogbg-molbbbp: 0.662326 test loss: 1.701772
[Epoch 70; Iter    15/   55] train: loss: 0.1544138
[Epoch 70; Iter    45/   55] train: loss: 0.0633121
[Epoch 70] ogbg-molbbbp: 0.951409 val loss: 0.566889
[Epoch 70] ogbg-molbbbp: 0.692805 test loss: 1.981816
[Epoch 71; Iter    20/   55] train: loss: 0.0147621
[Epoch 71; Iter    50/   55] train: loss: 0.0094963
[Epoch 71] ogbg-molbbbp: 0.959076 val loss: 0.440347
[Epoch 71] ogbg-molbbbp: 0.646701 test loss: 1.812810
[Epoch 72; Iter    25/   55] train: loss: 0.0431703
[Epoch 72; Iter    55/   55] train: loss: 0.5906502
[Epoch 72] ogbg-molbbbp: 0.949119 val loss: 0.627293
[Epoch 72] ogbg-molbbbp: 0.640818 test loss: 1.976257
[Epoch 73; Iter    30/   55] train: loss: 0.0261278
[Epoch 73] ogbg-molbbbp: 0.953898 val loss: 0.535809
[Epoch 73] ogbg-molbbbp: 0.629726 test loss: 2.086327
[Epoch 74; Iter     5/   55] train: loss: 0.0237918
[Epoch 74; Iter    35/   55] train: loss: 0.0156517
[Epoch 74] ogbg-molbbbp: 0.948223 val loss: 0.529386
[Epoch 74] ogbg-molbbbp: 0.661844 test loss: 1.884723
[Epoch 75; Iter    10/   55] train: loss: 0.0585395
[Epoch 75; Iter    40/   55] train: loss: 0.0657585
[Epoch 75] ogbg-molbbbp: 0.954695 val loss: 0.543990
[Epoch 75] ogbg-molbbbp: 0.656539 test loss: 2.004140
[Epoch 76; Iter    15/   55] train: loss: 0.1462236
[Epoch 76; Iter    45/   55] train: loss: 0.0106434
[Epoch 76] ogbg-molbbbp: 0.955093 val loss: 0.438766
[Epoch 76] ogbg-molbbbp: 0.676890 test loss: 1.706638
[Epoch 77; Iter    20/   55] train: loss: 0.0297339
[Epoch 77; Iter    50/   55] train: loss: 0.0340284
[Epoch 77] ogbg-molbbbp: 0.946231 val loss: 0.632992
[Epoch 77] ogbg-molbbbp: 0.676987 test loss: 2.076371
[Epoch 78; Iter    25/   55] train: loss: 0.0898645
[Epoch 78; Iter    55/   55] train: loss: 0.0711930
[Epoch 78] ogbg-molbbbp: 0.952504 val loss: 0.504802
[Epoch 78] ogbg-molbbbp: 0.683160 test loss: 2.030776
[Epoch 79; Iter    30/   55] train: loss: 0.0421533
[Epoch 79] ogbg-molbbbp: 0.954097 val loss: 0.506657
[Epoch 79] ogbg-molbbbp: 0.662809 test loss: 1.877920
[Epoch 80; Iter     5/   55] train: loss: 0.0372895
[Epoch 80; Iter    35/   55] train: loss: 0.0243015
[Epoch 80] ogbg-molbbbp: 0.939659 val loss: 0.702474
[Epoch 80] ogbg-molbbbp: 0.665220 test loss: 2.304802
[Epoch 81; Iter    10/   55] train: loss: 0.0183720
[Epoch 81; Iter    40/   55] train: loss: 0.0208733
[Epoch 81] ogbg-molbbbp: 0.949617 val loss: 0.474059
[Epoch 81] ogbg-molbbbp: 0.659529 test loss: 1.714946
[Epoch 82; Iter    15/   55] train: loss: 0.1214375
[Epoch 82; Iter    45/   55] train: loss: 0.0100338
[Epoch 82] ogbg-molbbbp: 0.953301 val loss: 0.472809
[Epoch 82] ogbg-molbbbp: 0.670235 test loss: 1.848219
[Epoch 83; Iter    20/   55] train: loss: 0.1003826
[Epoch 83; Iter    50/   55] train: loss: 0.0124473
[Epoch 83] ogbg-molbbbp: 0.952903 val loss: 0.530348
[Epoch 83] ogbg-molbbbp: 0.653549 test loss: 2.092538
[Epoch 84; Iter    25/   55] train: loss: 0.1107397
[Epoch 84; Iter    55/   55] train: loss: 0.0180778
[Epoch 84] ogbg-molbbbp: 0.948621 val loss: 0.601391
[Epoch 84] ogbg-molbbbp: 0.673129 test loss: 2.050532
[Epoch 85; Iter    30/   55] train: loss: 0.0046117
[Epoch 85] ogbg-molbbbp: 0.950812 val loss: 0.551591
[Epoch 85] ogbg-molbbbp: 0.673515 test loss: 2.060011
[Epoch 86; Iter     5/   55] train: loss: 0.0882525
[Epoch 86; Iter    35/   55] train: loss: 0.0094466
[Epoch 86] ogbg-molbbbp: 0.946430 val loss: 0.559205
[Epoch 86] ogbg-molbbbp: 0.676215 test loss: 1.945005
[Epoch 87; Iter    10/   55] train: loss: 0.0133280
[Epoch 87; Iter    40/   55] train: loss: 0.0060403
[Epoch 87] ogbg-molbbbp: 0.951907 val loss: 0.579786
[Epoch 87] ogbg-molbbbp: 0.680941 test loss: 2.109192
[Epoch 88; Iter    15/   55] train: loss: 0.0256756
[Epoch 88; Iter    45/   55] train: loss: 0.0062201
[Epoch 88] ogbg-molbbbp: 0.954396 val loss: 0.495489
[Epoch 88] ogbg-molbbbp: 0.680266 test loss: 1.968778
[Epoch 89; Iter    20/   55] train: loss: 0.0034773
[Epoch 89; Iter    50/   55] train: loss: 0.0636992
[Epoch 89] ogbg-molbbbp: 0.949119 val loss: 0.618514
[Epoch 89] ogbg-molbbbp: 0.682099 test loss: 2.243668
[Epoch 90; Iter    25/   55] train: loss: 0.0039273
[Epoch 90; Iter    55/   55] train: loss: 0.2384054
[Epoch 90] ogbg-molbbbp: 0.946829 val loss: 0.636163
[Epoch 90] ogbg-molbbbp: 0.663966 test loss: 2.313138
[Epoch 91; Iter    30/   55] train: loss: 0.0067689
[Epoch 91] ogbg-molbbbp: 0.954297 val loss: 0.465937
[Epoch 91] ogbg-molbbbp: 0.659336 test loss: 2.107414
[Epoch 92; Iter     5/   55] train: loss: 0.0065769
[Epoch 92; Iter    35/   55] train: loss: 0.0055423
[Epoch 92] ogbg-molbbbp: 0.956786 val loss: 0.461075
[Epoch 92] ogbg-molbbbp: 0.656057 test loss: 2.078648
[Epoch 93; Iter    10/   55] train: loss: 0.0039450
[Epoch 93; Iter    40/   55] train: loss: 0.0057860
[Epoch 93] ogbg-molbbbp: 0.954097 val loss: 0.565754
[Epoch 93] ogbg-molbbbp: 0.679012 test loss: 2.190441
[Epoch 94; Iter    15/   55] train: loss: 0.0230771
[Epoch 94; Iter    45/   55] train: loss: 0.0013193
[Epoch 94] ogbg-molbbbp: 0.944638 val loss: 0.616381
[Epoch 94] ogbg-molbbbp: 0.656250 test loss: 2.182368
[Epoch 95; Iter    20/   55] train: loss: 0.3076943
[Epoch 95; Iter    50/   55] train: loss: 0.0072515
[Epoch 95] ogbg-molbbbp: 0.956587 val loss: 0.542939
[Epoch 95] ogbg-molbbbp: 0.638503 test loss: 2.479555
[Epoch 96; Iter    25/   55] train: loss: 0.0093921
[Epoch 96; Iter    55/   55] train: loss: 0.0053282
[Epoch 96] ogbg-molbbbp: 0.949418 val loss: 0.543959
[Epoch 96] ogbg-molbbbp: 0.682774 test loss: 2.233292
[Epoch 97; Iter    30/   55] train: loss: 0.0055419
[Epoch 97] ogbg-molbbbp: 0.948422 val loss: 0.586215
[Epoch 97] ogbg-molbbbp: 0.648727 test loss: 2.489655
[Epoch 98; Iter     5/   55] train: loss: 0.0372031
[Epoch 98; Iter    35/   55] train: loss: 0.0075170
[Epoch 98] ogbg-molbbbp: 0.945932 val loss: 0.566232
[Epoch 98] ogbg-molbbbp: 0.672840 test loss: 2.175673
[Epoch 99; Iter    10/   55] train: loss: 0.0026587
[Epoch 99; Iter    40/   55] train: loss: 0.0016880
[Epoch 99] ogbg-molbbbp: 0.949915 val loss: 0.542947
[Epoch 99] ogbg-molbbbp: 0.668403 test loss: 2.228630
[Epoch 100; Iter    15/   55] train: loss: 0.0083391
[Epoch 100; Iter    45/   55] train: loss: 0.0189792
[Epoch 100] ogbg-molbbbp: 0.949318 val loss: 0.590477
[Epoch 100] ogbg-molbbbp: 0.661265 test loss: 2.372774
[Epoch 101; Iter    20/   55] train: loss: 0.0232177
[Epoch 101; Iter    50/   55] train: loss: 0.0064908
[Epoch 101] ogbg-molbbbp: 0.936672 val loss: 0.766938
[Epoch 101] ogbg-molbbbp: 0.644869 test loss: 2.665240
[Epoch 102; Iter    25/   55] train: loss: 0.1032348
[Epoch 102; Iter    55/   55] train: loss: 0.0009752
[Epoch 102] ogbg-molbbbp: 0.931295 val loss: 0.836500
[Epoch 102] ogbg-molbbbp: 0.651427 test loss: 2.762303
[Epoch 103; Iter    30/   55] train: loss: 0.0196083
[Epoch 103] ogbg-molbbbp: 0.947426 val loss: 0.517275
[Epoch 103] ogbg-molbbbp: 0.659336 test loss: 2.069903
[Epoch 104; Iter     5/   55] train: loss: 0.0516325
[Epoch 104; Iter    35/   55] train: loss: 0.0288559
[Epoch 104] ogbg-molbbbp: 0.945833 val loss: 0.547909
[Epoch 104] ogbg-molbbbp: 0.652778 test loss: 2.363136
[Epoch 105; Iter    10/   55] train: loss: 0.1413815
[Epoch 105; Iter    40/   55] train: loss: 0.0029780
[Epoch 105] ogbg-molbbbp: 0.939560 val loss: 0.678915
[Epoch 105] ogbg-molbbbp: 0.665123 test loss: 2.487509
[Epoch 106; Iter    15/   55] train: loss: 0.0355009
[Epoch 106; Iter    45/   55] train: loss: 0.0029007
[Epoch 106] ogbg-molbbbp: 0.934681 val loss: 0.654561
[Epoch 106] ogbg-molbbbp: 0.642458 test loss: 2.304483
[Epoch 107; Iter    20/   55] train: loss: 0.0265898
[Epoch 107; Iter    50/   55] train: loss: 0.0032839
[Epoch 107] ogbg-molbbbp: 0.950712 val loss: 0.450958
[Epoch 107] ogbg-molbbbp: 0.682099 test loss: 1.755814
[Epoch 108; Iter    25/   55] train: loss: 0.0029066
[Epoch 108; Iter    55/   55] train: loss: 0.0010042
[Epoch 108] ogbg-molbbbp: 0.944937 val loss: 0.607756
[Epoch 108] ogbg-molbbbp: 0.673611 test loss: 2.172452
[Epoch 109; Iter    30/   55] train: loss: 0.0274234
[Epoch 109] ogbg-molbbbp: 0.956985 val loss: 0.530661
[Epoch 109] ogbg-molbbbp: 0.659626 test loss: 3.048927
[Epoch 110; Iter     5/   55] train: loss: 0.0032757
[Epoch 110; Iter    35/   55] train: loss: 0.0002657
[Epoch 110] ogbg-molbbbp: 0.959076 val loss: 0.471280
[Epoch 110] ogbg-molbbbp: 0.661651 test loss: 2.920311
[Epoch 111; Iter    10/   55] train: loss: 0.0000955
[Epoch 111; Iter    40/   55] train: loss: 0.0001806
[Epoch 111] ogbg-molbbbp: 0.958379 val loss: 0.468043
[Epoch 111] ogbg-molbbbp: 0.663870 test loss: 2.891160
[Epoch 112; Iter    15/   55] train: loss: 0.0011494
[Epoch 112; Iter    45/   55] train: loss: 0.0001516
[Epoch 112] ogbg-molbbbp: 0.957881 val loss: 0.500116
[Epoch 112] ogbg-molbbbp: 0.661169 test loss: 2.952588
[Epoch 113; Iter    20/   55] train: loss: 0.0001707
[Epoch 113; Iter    50/   55] train: loss: 0.0002190
[Epoch 113] ogbg-molbbbp: 0.958976 val loss: 0.459145
[Epoch 113] ogbg-molbbbp: 0.660976 test loss: 2.926777
[Epoch 114; Iter    25/   55] train: loss: 0.0001050
[Epoch 114; Iter    55/   55] train: loss: 0.0001712
[Epoch 114] ogbg-molbbbp: 0.958678 val loss: 0.532708
[Epoch 114] ogbg-molbbbp: 0.663966 test loss: 3.100342
[Epoch 115; Iter    30/   55] train: loss: 0.0001985
[Epoch 115] ogbg-molbbbp: 0.959873 val loss: 0.496085
[Epoch 115] ogbg-molbbbp: 0.659819 test loss: 3.067990
[Epoch 116; Iter     5/   55] train: loss: 0.0006570
[Epoch 116; Iter    35/   55] train: loss: 0.0001377
[Epoch 116] ogbg-molbbbp: 0.959773 val loss: 0.476676
[Epoch 116] ogbg-molbbbp: 0.657407 test loss: 3.010799
[Epoch 117; Iter    10/   55] train: loss: 0.0002118
[Epoch 117; Iter    40/   55] train: loss: 0.0003182
[Epoch 117] ogbg-molbbbp: 0.957483 val loss: 0.560334
[Epoch 117] ogbg-molbbbp: 0.657407 test loss: 3.124096
[Epoch 118; Iter    15/   55] train: loss: 0.0001478
[Epoch 118; Iter    45/   55] train: loss: 0.0001990
[Epoch 118] ogbg-molbbbp: 0.959375 val loss: 0.475848
[Epoch 118] ogbg-molbbbp: 0.657407 test loss: 3.010059
[Epoch 119; Iter    20/   55] train: loss: 0.0001139
[Epoch 119; Iter    50/   55] train: loss: 0.0000966
[Epoch 119] ogbg-molbbbp: 0.952206 val loss: 0.594006
[Epoch 119] ogbg-molbbbp: 0.667824 test loss: 2.910471
[Epoch 120; Iter    25/   55] train: loss: 0.0002999
[Epoch 120; Iter    55/   55] train: loss: 0.0003166
[Epoch 120] ogbg-molbbbp: 0.955392 val loss: 0.610599
[Epoch 120] ogbg-molbbbp: 0.661362 test loss: 3.167259
[Epoch 121; Iter    30/   55] train: loss: 0.0000955
[Epoch 121] ogbg-molbbbp: 0.956487 val loss: 0.586209
[Epoch 121] ogbg-molbbbp: 0.655189 test loss: 3.292493
[Epoch 122; Iter     5/   55] train: loss: 0.0003015
[Epoch 122; Iter    35/   55] train: loss: 0.0003425
[Epoch 122] ogbg-molbbbp: 0.955491 val loss: 0.554377
[Epoch 122] ogbg-molbbbp: 0.659336 test loss: 3.165218
[Epoch 123; Iter    10/   55] train: loss: 0.0001504
[Epoch 123; Iter    40/   55] train: loss: 0.0000843
[Epoch 123] ogbg-molbbbp: 0.954496 val loss: 0.634106
[Epoch 123] ogbg-molbbbp: 0.655864 test loss: 3.280786
[Epoch 124; Iter    15/   55] train: loss: 0.0001472
[Epoch 124; Iter    45/   55] train: loss: 0.0007095
[Epoch 124] ogbg-molbbbp: 0.952006 val loss: 0.724831
[Epoch 124] ogbg-molbbbp: 0.652681 test loss: 3.458638
[Epoch 125; Iter    20/   55] train: loss: 0.0001097
[Epoch 125; Iter    50/   55] train: loss: 0.0002230
[Epoch 125] ogbg-molbbbp: 0.954297 val loss: 0.554714
[Epoch 125] ogbg-molbbbp: 0.649016 test loss: 3.278607
[Epoch 126; Iter    25/   55] train: loss: 0.0001868
[Epoch 126; Iter    55/   55] train: loss: 0.0020060
[Epoch 126] ogbg-molbbbp: 0.944339 val loss: 0.726583
[Epoch 126] ogbg-molbbbp: 0.648534 test loss: 3.171888
[Epoch 127; Iter    30/   55] train: loss: 0.0001618
[Epoch 127] ogbg-molbbbp: 0.953201 val loss: 0.540795
[Epoch 127] ogbg-molbbbp: 0.650752 test loss: 3.122724
[Epoch 128; Iter     5/   55] train: loss: 0.0000700
[Epoch 128; Iter    35/   55] train: loss: 0.0001661
[Epoch 128] ogbg-molbbbp: 0.952903 val loss: 0.572122
[Epoch 128] ogbg-molbbbp: 0.647087 test loss: 3.269505
[Epoch 129; Iter    10/   55] train: loss: 0.0001476
[Epoch 129; Iter    40/   55] train: loss: 0.0001280
[Epoch 129] ogbg-molbbbp: 0.952006 val loss: 0.585270
[Epoch 129] ogbg-molbbbp: 0.645158 test loss: 3.310587
[Epoch 130; Iter    15/   55] train: loss: 0.0001028
[Epoch 130; Iter    45/   55] train: loss: 0.0002562
[Epoch 130] ogbg-molbbbp: 0.952803 val loss: 0.561861
[Epoch 130] ogbg-molbbbp: 0.645255 test loss: 3.251165
[Epoch 131; Iter    20/   55] train: loss: 0.0001079
[Epoch 131; Iter    50/   55] train: loss: 0.0003250
[Epoch 131] ogbg-molbbbp: 0.953400 val loss: 0.534012
[Epoch 131] ogbg-molbbbp: 0.648823 test loss: 3.154138
[Epoch 132; Iter    25/   55] train: loss: 0.0000824
[Epoch 132; Iter    55/   55] train: loss: 0.0011678
[Epoch 132] ogbg-molbbbp: 0.951807 val loss: 0.555303
[Epoch 132] ogbg-molbbbp: 0.651910 test loss: 3.180055
[Epoch 133; Iter    30/   55] train: loss: 0.0000866
[Epoch 133] ogbg-molbbbp: 0.953400 val loss: 0.530804
[Epoch 133] ogbg-molbbbp: 0.650367 test loss: 3.095447
[Epoch 134; Iter     5/   55] train: loss: 0.0003211
[Epoch 134; Iter    35/   55] train: loss: 0.0000563
[Epoch 134] ogbg-molbbbp: 0.953201 val loss: 0.525609
[Epoch 134] ogbg-molbbbp: 0.654803 test loss: 3.076277
[Epoch 135; Iter    10/   55] train: loss: 0.0002471
[Epoch 135; Iter    40/   55] train: loss: 0.0003094
[Epoch 135] ogbg-molbbbp: 0.950612 val loss: 0.579765
[Epoch 135] ogbg-molbbbp: 0.647184 test loss: 3.280941
[Epoch 136; Iter    15/   55] train: loss: 0.0001284
[Epoch 136; Iter    45/   55] train: loss: 0.0009552
[Epoch 136] ogbg-molbbbp: 0.953699 val loss: 0.523686
[Epoch 136] ogbg-molbbbp: 0.654900 test loss: 3.081005
[Epoch 137; Iter    20/   55] train: loss: 0.0001124
[Epoch 137; Iter    50/   55] train: loss: 0.0001119
[Epoch 137] ogbg-molbbbp: 0.953998 val loss: 0.534344
[Epoch 137] ogbg-molbbbp: 0.654514 test loss: 3.147454
[Epoch 138; Iter    25/   55] train: loss: 0.0002102
[Epoch 138; Iter    55/   55] train: loss: 0.0000942
[Epoch 138] ogbg-molbbbp: 0.946231 val loss: 0.679866
[Epoch 138] ogbg-molbbbp: 0.652778 test loss: 3.257609
[Epoch 139; Iter    30/   55] train: loss: 0.0004408
[Epoch 139] ogbg-molbbbp: 0.948820 val loss: 0.654247
[Epoch 139] ogbg-molbbbp: 0.653742 test loss: 3.345163
[Epoch 140; Iter     5/   55] train: loss: 0.0002906
[Epoch 140; Iter    35/   55] train: loss: 0.0002253
[Epoch 140] ogbg-molbbbp: 0.948621 val loss: 0.657366
[Epoch 140] ogbg-molbbbp: 0.654514 test loss: 3.381413
[Epoch 141; Iter    10/   55] train: loss: 0.0004483
[Epoch 141; Iter    40/   55] train: loss: 0.0000956
[Epoch 141] ogbg-molbbbp: 0.948223 val loss: 0.616190
[Epoch 141] ogbg-molbbbp: 0.658468 test loss: 3.272430
[Epoch 142; Iter    15/   55] train: loss: 0.0003406
[Epoch 142; Iter    45/   55] train: loss: 0.0013754
[Epoch 142] ogbg-molbbbp: 0.945136 val loss: 0.666191
[Epoch 142] ogbg-molbbbp: 0.662519 test loss: 3.292223
[Epoch 143; Iter    20/   55] train: loss: 0.0001028
[Epoch 143; Iter    50/   55] train: loss: 0.0003724
[Epoch 143] ogbg-molbbbp: 0.927910 val loss: 0.857046
[Epoch 143] ogbg-molbbbp: 0.657022 test loss: 3.266674
[Epoch 144; Iter    25/   55] train: loss: 0.0001772
[Epoch 144; Iter    55/   55] train: loss: 0.0000481
[Epoch 144] ogbg-molbbbp: 0.941053 val loss: 0.721787
[Epoch 144] ogbg-molbbbp: 0.655864 test loss: 3.285321
[Epoch 145; Iter    30/   55] train: loss: 0.0003009
[Epoch 145] ogbg-molbbbp: 0.934482 val loss: 0.859911
[Epoch 145] ogbg-molbbbp: 0.649113 test loss: 3.543620
[Epoch 146; Iter     5/   55] train: loss: 0.0001432
[Epoch 146; Iter    35/   55] train: loss: 0.0003380
[Epoch 146] ogbg-molbbbp: 0.946530 val loss: 0.674401
[Epoch 146] ogbg-molbbbp: 0.651717 test loss: 3.452278
[Epoch 147; Iter    10/   55] train: loss: 0.0001886
[Epoch 147; Iter    40/   55] train: loss: 0.0004059
[Epoch 147] ogbg-molbbbp: 0.938564 val loss: 0.750619
[Epoch 147] ogbg-molbbbp: 0.650270 test loss: 3.334559
[Epoch 148; Iter    15/   55] train: loss: 0.0003337
[Epoch 148; Iter    45/   55] train: loss: 0.0002018
[Epoch 148] ogbg-molbbbp: 0.947924 val loss: 0.614149
[Epoch 148] ogbg-molbbbp: 0.658275 test loss: 3.285516
[Epoch 69; Iter    40/   55] train: loss: 0.0080285
[Epoch 69] ogbg-molbbbp: 0.949816 val loss: 0.426931
[Epoch 69] ogbg-molbbbp: 0.650077 test loss: 1.898928
[Epoch 70; Iter    15/   55] train: loss: 0.0633974
[Epoch 70; Iter    45/   55] train: loss: 0.0646269
[Epoch 70] ogbg-molbbbp: 0.929802 val loss: 0.639114
[Epoch 70] ogbg-molbbbp: 0.629630 test loss: 2.059343
[Epoch 71; Iter    20/   55] train: loss: 0.0240673
[Epoch 71; Iter    50/   55] train: loss: 0.1325531
[Epoch 71] ogbg-molbbbp: 0.958877 val loss: 0.501498
[Epoch 71] ogbg-molbbbp: 0.676022 test loss: 2.250377
[Epoch 72; Iter    25/   55] train: loss: 0.1467570
[Epoch 72; Iter    55/   55] train: loss: 0.0509727
[Epoch 72] ogbg-molbbbp: 0.924027 val loss: 0.714149
[Epoch 72] ogbg-molbbbp: 0.657504 test loss: 2.277847
[Epoch 73; Iter    30/   55] train: loss: 0.1099052
[Epoch 73] ogbg-molbbbp: 0.946829 val loss: 0.571309
[Epoch 73] ogbg-molbbbp: 0.660687 test loss: 2.030083
[Epoch 74; Iter     5/   55] train: loss: 0.0336408
[Epoch 74; Iter    35/   55] train: loss: 0.0122267
[Epoch 74] ogbg-molbbbp: 0.952106 val loss: 0.441356
[Epoch 74] ogbg-molbbbp: 0.684992 test loss: 1.678973
[Epoch 75; Iter    10/   55] train: loss: 0.0338536
[Epoch 75; Iter    40/   55] train: loss: 0.0517125
[Epoch 75] ogbg-molbbbp: 0.943344 val loss: 0.814027
[Epoch 75] ogbg-molbbbp: 0.638407 test loss: 2.237512
[Epoch 76; Iter    15/   55] train: loss: 0.0728647
[Epoch 76; Iter    45/   55] train: loss: 0.0822321
[Epoch 76] ogbg-molbbbp: 0.953998 val loss: 0.564944
[Epoch 76] ogbg-molbbbp: 0.639178 test loss: 2.176914
[Epoch 77; Iter    20/   55] train: loss: 0.0183157
[Epoch 77; Iter    50/   55] train: loss: 0.0067167
[Epoch 77] ogbg-molbbbp: 0.935577 val loss: 0.643723
[Epoch 77] ogbg-molbbbp: 0.680941 test loss: 2.020704
[Epoch 78; Iter    25/   55] train: loss: 0.0535957
[Epoch 78; Iter    55/   55] train: loss: 0.4032736
[Epoch 78] ogbg-molbbbp: 0.950911 val loss: 0.386896
[Epoch 78] ogbg-molbbbp: 0.652199 test loss: 1.681373
[Epoch 79; Iter    30/   55] train: loss: 0.0214465
[Epoch 79] ogbg-molbbbp: 0.953301 val loss: 0.492800
[Epoch 79] ogbg-molbbbp: 0.651138 test loss: 2.081966
[Epoch 80; Iter     5/   55] train: loss: 0.1475234
[Epoch 80; Iter    35/   55] train: loss: 0.0246394
[Epoch 80] ogbg-molbbbp: 0.951110 val loss: 0.590436
[Epoch 80] ogbg-molbbbp: 0.626350 test loss: 2.174757
[Epoch 81; Iter    10/   55] train: loss: 0.0223811
[Epoch 81; Iter    40/   55] train: loss: 0.0149781
[Epoch 81] ogbg-molbbbp: 0.945534 val loss: 0.624730
[Epoch 81] ogbg-molbbbp: 0.653453 test loss: 2.143121
[Epoch 82; Iter    15/   55] train: loss: 0.0140349
[Epoch 82; Iter    45/   55] train: loss: 0.0519478
[Epoch 82] ogbg-molbbbp: 0.938166 val loss: 0.679989
[Epoch 82] ogbg-molbbbp: 0.637731 test loss: 2.466254
[Epoch 83; Iter    20/   55] train: loss: 0.0193724
[Epoch 83; Iter    50/   55] train: loss: 0.0764371
[Epoch 83] ogbg-molbbbp: 0.941352 val loss: 0.714362
[Epoch 83] ogbg-molbbbp: 0.652874 test loss: 2.254732
[Epoch 84; Iter    25/   55] train: loss: 0.0614220
[Epoch 84; Iter    55/   55] train: loss: 0.0783608
[Epoch 84] ogbg-molbbbp: 0.933586 val loss: 0.720953
[Epoch 84] ogbg-molbbbp: 0.641397 test loss: 2.300795
[Epoch 85; Iter    30/   55] train: loss: 0.0042999
[Epoch 85] ogbg-molbbbp: 0.955491 val loss: 0.462661
[Epoch 85] ogbg-molbbbp: 0.647859 test loss: 1.897895
[Epoch 86; Iter     5/   55] train: loss: 0.0075892
[Epoch 86; Iter    35/   55] train: loss: 0.0054275
[Epoch 86] ogbg-molbbbp: 0.941352 val loss: 0.568051
[Epoch 86] ogbg-molbbbp: 0.637442 test loss: 2.248379
[Epoch 87; Iter    10/   55] train: loss: 0.0284381
[Epoch 87; Iter    40/   55] train: loss: 0.1246279
[Epoch 87] ogbg-molbbbp: 0.952903 val loss: 0.487339
[Epoch 87] ogbg-molbbbp: 0.654900 test loss: 2.015095
[Epoch 88; Iter    15/   55] train: loss: 0.0091956
[Epoch 88; Iter    45/   55] train: loss: 0.0419162
[Epoch 88] ogbg-molbbbp: 0.950115 val loss: 0.623001
[Epoch 88] ogbg-molbbbp: 0.669560 test loss: 2.342856
[Epoch 89; Iter    20/   55] train: loss: 0.1035229
[Epoch 89; Iter    50/   55] train: loss: 0.1505503
[Epoch 89] ogbg-molbbbp: 0.933685 val loss: 0.799353
[Epoch 89] ogbg-molbbbp: 0.627315 test loss: 2.170659
[Epoch 90; Iter    25/   55] train: loss: 0.0153477
[Epoch 90; Iter    55/   55] train: loss: 0.2142667
[Epoch 90] ogbg-molbbbp: 0.914169 val loss: 1.227731
[Epoch 90] ogbg-molbbbp: 0.577739 test loss: 2.884050
[Epoch 91; Iter    30/   55] train: loss: 0.0277115
[Epoch 91] ogbg-molbbbp: 0.939759 val loss: 0.570594
[Epoch 91] ogbg-molbbbp: 0.627604 test loss: 2.037723
[Epoch 92; Iter     5/   55] train: loss: 0.0564882
[Epoch 92; Iter    35/   55] train: loss: 0.0138527
[Epoch 92] ogbg-molbbbp: 0.947028 val loss: 0.612852
[Epoch 92] ogbg-molbbbp: 0.611979 test loss: 2.232009
[Epoch 93; Iter    10/   55] train: loss: 0.0089847
[Epoch 93; Iter    40/   55] train: loss: 0.0161031
[Epoch 93] ogbg-molbbbp: 0.934780 val loss: 0.538702
[Epoch 93] ogbg-molbbbp: 0.670718 test loss: 1.780377
[Epoch 94; Iter    15/   55] train: loss: 0.0158835
[Epoch 94; Iter    45/   55] train: loss: 0.0604997
[Epoch 94] ogbg-molbbbp: 0.939162 val loss: 0.628100
[Epoch 94] ogbg-molbbbp: 0.626543 test loss: 2.453229
[Epoch 95; Iter    20/   55] train: loss: 0.0918387
[Epoch 95; Iter    50/   55] train: loss: 0.0043954
[Epoch 95] ogbg-molbbbp: 0.950612 val loss: 0.470350
[Epoch 95] ogbg-molbbbp: 0.672550 test loss: 1.795329
[Epoch 96; Iter    25/   55] train: loss: 0.1033124
[Epoch 96; Iter    55/   55] train: loss: 0.0315697
[Epoch 96] ogbg-molbbbp: 0.942647 val loss: 0.687872
[Epoch 96] ogbg-molbbbp: 0.634163 test loss: 2.352293
[Epoch 97; Iter    30/   55] train: loss: 0.0522105
[Epoch 97] ogbg-molbbbp: 0.949418 val loss: 0.500083
[Epoch 97] ogbg-molbbbp: 0.652006 test loss: 1.936298
[Epoch 98; Iter     5/   55] train: loss: 0.0043210
[Epoch 98; Iter    35/   55] train: loss: 0.0266890
[Epoch 98] ogbg-molbbbp: 0.947924 val loss: 0.541797
[Epoch 98] ogbg-molbbbp: 0.666570 test loss: 2.148652
[Epoch 99; Iter    10/   55] train: loss: 0.1248360
[Epoch 99; Iter    40/   55] train: loss: 0.0051759
[Epoch 99] ogbg-molbbbp: 0.939162 val loss: 0.724909
[Epoch 99] ogbg-molbbbp: 0.643519 test loss: 2.556205
[Epoch 100; Iter    15/   55] train: loss: 0.0044922
[Epoch 100; Iter    45/   55] train: loss: 0.0093953
[Epoch 100] ogbg-molbbbp: 0.945235 val loss: 0.620731
[Epoch 100] ogbg-molbbbp: 0.662230 test loss: 2.376385
[Epoch 101; Iter    20/   55] train: loss: 0.0027728
[Epoch 101; Iter    50/   55] train: loss: 0.1318536
[Epoch 101] ogbg-molbbbp: 0.935876 val loss: 0.695576
[Epoch 101] ogbg-molbbbp: 0.636863 test loss: 2.278637
[Epoch 102; Iter    25/   55] train: loss: 0.0318157
[Epoch 102; Iter    55/   55] train: loss: 0.0074520
[Epoch 102] ogbg-molbbbp: 0.944638 val loss: 0.565717
[Epoch 102] ogbg-molbbbp: 0.642361 test loss: 2.584232
[Epoch 103; Iter    30/   55] train: loss: 0.0197771
[Epoch 103] ogbg-molbbbp: 0.945932 val loss: 0.653461
[Epoch 103] ogbg-molbbbp: 0.655864 test loss: 2.566121
[Epoch 104; Iter     5/   55] train: loss: 0.0110439
[Epoch 104; Iter    35/   55] train: loss: 0.0163578
[Epoch 104] ogbg-molbbbp: 0.945235 val loss: 0.611461
[Epoch 104] ogbg-molbbbp: 0.643519 test loss: 2.447320
[Epoch 105; Iter    10/   55] train: loss: 0.0597473
[Epoch 105; Iter    40/   55] train: loss: 0.0989891
[Epoch 105] ogbg-molbbbp: 0.932192 val loss: 0.822846
[Epoch 105] ogbg-molbbbp: 0.626543 test loss: 2.649779
[Epoch 106; Iter    15/   55] train: loss: 0.2918175
[Epoch 106; Iter    45/   55] train: loss: 0.0184937
[Epoch 106] ogbg-molbbbp: 0.949716 val loss: 0.559372
[Epoch 106] ogbg-molbbbp: 0.613233 test loss: 2.569385
[Epoch 107; Iter    20/   55] train: loss: 0.0071854
[Epoch 107; Iter    50/   55] train: loss: 0.0053771
[Epoch 107] ogbg-molbbbp: 0.942049 val loss: 0.556660
[Epoch 107] ogbg-molbbbp: 0.665220 test loss: 1.981317
[Epoch 108; Iter    25/   55] train: loss: 0.0086652
[Epoch 108; Iter    55/   55] train: loss: 0.0996813
[Epoch 108] ogbg-molbbbp: 0.926715 val loss: 0.760360
[Epoch 108] ogbg-molbbbp: 0.615741 test loss: 2.398760
[Epoch 109; Iter    30/   55] train: loss: 0.0240476
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 149; Iter    20/   55] train: loss: 0.0003010
[Epoch 149; Iter    50/   55] train: loss: 0.0002007
[Epoch 149] ogbg-molbbbp: 0.947127 val loss: 0.806537
[Epoch 149] ogbg-molbbbp: 0.654225 test loss: 3.324593
[Epoch 150; Iter    25/   55] train: loss: 0.0011039
[Epoch 150; Iter    55/   55] train: loss: 0.0006115
[Epoch 150] ogbg-molbbbp: 0.947127 val loss: 0.804937
[Epoch 150] ogbg-molbbbp: 0.660397 test loss: 3.285867
[Epoch 151; Iter    30/   55] train: loss: 0.0004740
[Epoch 151] ogbg-molbbbp: 0.944041 val loss: 0.874069
[Epoch 151] ogbg-molbbbp: 0.652006 test loss: 3.480715
[Epoch 152; Iter     5/   55] train: loss: 0.0000947
[Epoch 152; Iter    35/   55] train: loss: 0.0001596
[Epoch 152] ogbg-molbbbp: 0.944937 val loss: 0.857457
[Epoch 152] ogbg-molbbbp: 0.655768 test loss: 3.495881
[Epoch 153; Iter    10/   55] train: loss: 0.0001404
[Epoch 153; Iter    40/   55] train: loss: 0.0001362
[Epoch 153] ogbg-molbbbp: 0.944837 val loss: 0.840711
[Epoch 153] ogbg-molbbbp: 0.652392 test loss: 3.477346
[Epoch 154; Iter    15/   55] train: loss: 0.0001170
[Epoch 154; Iter    45/   55] train: loss: 0.0012712
[Epoch 154] ogbg-molbbbp: 0.942846 val loss: 0.880757
[Epoch 154] ogbg-molbbbp: 0.656829 test loss: 3.429904
[Epoch 155; Iter    20/   55] train: loss: 0.0001239
[Epoch 155; Iter    50/   55] train: loss: 0.0086144
[Epoch 155] ogbg-molbbbp: 0.941352 val loss: 0.918894
[Epoch 155] ogbg-molbbbp: 0.651235 test loss: 3.592219
[Epoch 156; Iter    25/   55] train: loss: 0.0004586
[Epoch 156; Iter    55/   55] train: loss: 0.0001696
[Epoch 156] ogbg-molbbbp: 0.946530 val loss: 0.836334
[Epoch 156] ogbg-molbbbp: 0.656250 test loss: 3.443109
[Epoch 157; Iter    30/   55] train: loss: 0.0003160
[Epoch 157] ogbg-molbbbp: 0.946430 val loss: 0.845132
[Epoch 157] ogbg-molbbbp: 0.653839 test loss: 3.537101
[Epoch 158; Iter     5/   55] train: loss: 0.0001217
[Epoch 158; Iter    35/   55] train: loss: 0.0000882
[Epoch 158] ogbg-molbbbp: 0.946530 val loss: 0.837247
[Epoch 158] ogbg-molbbbp: 0.656057 test loss: 3.528505
[Epoch 159; Iter    10/   55] train: loss: 0.0006977
[Epoch 159; Iter    40/   55] train: loss: 0.0011617
[Epoch 159] ogbg-molbbbp: 0.946928 val loss: 0.827001
[Epoch 159] ogbg-molbbbp: 0.652681 test loss: 3.500125
[Epoch 160; Iter    15/   55] train: loss: 0.0000724
[Epoch 160; Iter    45/   55] train: loss: 0.0000555
[Epoch 160] ogbg-molbbbp: 0.948820 val loss: 0.809837
[Epoch 160] ogbg-molbbbp: 0.654128 test loss: 3.461569
[Epoch 161; Iter    20/   55] train: loss: 0.0014459
[Epoch 161; Iter    50/   55] train: loss: 0.0000522
[Epoch 161] ogbg-molbbbp: 0.944339 val loss: 0.918735
[Epoch 161] ogbg-molbbbp: 0.647666 test loss: 3.726721
[Epoch 162; Iter    25/   55] train: loss: 0.0003011
[Epoch 162; Iter    55/   55] train: loss: 0.0001261
[Epoch 162] ogbg-molbbbp: 0.945634 val loss: 0.866848
[Epoch 162] ogbg-molbbbp: 0.652778 test loss: 3.580547
[Epoch 163; Iter    30/   55] train: loss: 0.0001939
[Epoch 163] ogbg-molbbbp: 0.944937 val loss: 0.880170
[Epoch 163] ogbg-molbbbp: 0.651717 test loss: 3.627226
[Epoch 164; Iter     5/   55] train: loss: 0.0001398
[Epoch 164; Iter    35/   55] train: loss: 0.0002242
[Epoch 164] ogbg-molbbbp: 0.946231 val loss: 0.876201
[Epoch 164] ogbg-molbbbp: 0.652199 test loss: 3.683048
[Epoch 165; Iter    10/   55] train: loss: 0.0001745
[Epoch 165; Iter    40/   55] train: loss: 0.0001183
[Epoch 165] ogbg-molbbbp: 0.945335 val loss: 0.882149
[Epoch 165] ogbg-molbbbp: 0.650752 test loss: 3.656395
[Epoch 166; Iter    15/   55] train: loss: 0.0002538
[Epoch 166; Iter    45/   55] train: loss: 0.0003113
[Epoch 166] ogbg-molbbbp: 0.943642 val loss: 0.906388
[Epoch 166] ogbg-molbbbp: 0.651524 test loss: 3.646406
[Epoch 167; Iter    20/   55] train: loss: 0.0001314
[Epoch 167; Iter    50/   55] train: loss: 0.0000802
[Epoch 167] ogbg-molbbbp: 0.944937 val loss: 0.869075
[Epoch 167] ogbg-molbbbp: 0.654032 test loss: 3.583826
[Epoch 168; Iter    25/   55] train: loss: 0.0000936
[Epoch 168; Iter    55/   55] train: loss: 0.0009281
[Epoch 168] ogbg-molbbbp: 0.945335 val loss: 0.858445
[Epoch 168] ogbg-molbbbp: 0.657215 test loss: 3.540930
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 168 epochs. Best model checkpoint was in epoch 108.
Statistics on  val_best_checkpoint
mean_pred: -1.4621238708496094
std_pred: 10.485870361328125
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9333912897705303
rocauc: 0.9615652693418301
ogbg-molbbbp: 0.9615652693418301
BCEWithLogitsLoss: 0.4781005159020424
Statistics on  test
mean_pred: 3.394465446472168
std_pred: 7.553685188293457
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6604176046000615
rocauc: 0.6259645061728395
ogbg-molbbbp: 0.6259645061728395
BCEWithLogitsLoss: 2.6837335995265414
Statistics on  train
mean_pred: 6.483189582824707
std_pred: 6.9996232986450195
mean_targets: 0.839362382888794
std_targets: 0.36730900406837463
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.0017543318046426232
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.05.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.05.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.05.yml --seed 6 --device cuda:1
All runs completed.
[Epoch 69; Iter    40/   55] train: loss: 0.1157965
[Epoch 69] ogbg-molbbbp: 0.954595 val loss: 0.408087
[Epoch 69] ogbg-molbbbp: 0.661748 test loss: 1.491764
[Epoch 70; Iter    15/   55] train: loss: 0.0356486
[Epoch 70; Iter    45/   55] train: loss: 0.0473602
[Epoch 70] ogbg-molbbbp: 0.944538 val loss: 0.488138
[Epoch 70] ogbg-molbbbp: 0.649402 test loss: 1.681928
[Epoch 71; Iter    20/   55] train: loss: 0.0127842
[Epoch 71; Iter    50/   55] train: loss: 0.0466113
[Epoch 71] ogbg-molbbbp: 0.942945 val loss: 0.475717
[Epoch 71] ogbg-molbbbp: 0.666860 test loss: 1.614166
[Epoch 72; Iter    25/   55] train: loss: 0.0144881
[Epoch 72; Iter    55/   55] train: loss: 0.0091474
[Epoch 72] ogbg-molbbbp: 0.951110 val loss: 0.445799
[Epoch 72] ogbg-molbbbp: 0.650849 test loss: 1.694611
[Epoch 73; Iter    30/   55] train: loss: 0.0571119
[Epoch 73] ogbg-molbbbp: 0.934581 val loss: 0.533320
[Epoch 73] ogbg-molbbbp: 0.639950 test loss: 1.865185
[Epoch 74; Iter     5/   55] train: loss: 0.0146487
[Epoch 74; Iter    35/   55] train: loss: 0.0252190
[Epoch 74] ogbg-molbbbp: 0.931694 val loss: 0.547328
[Epoch 74] ogbg-molbbbp: 0.638985 test loss: 1.751467
[Epoch 75; Iter    10/   55] train: loss: 0.0304929
[Epoch 75; Iter    40/   55] train: loss: 0.0523976
[Epoch 75] ogbg-molbbbp: 0.939560 val loss: 0.629483
[Epoch 75] ogbg-molbbbp: 0.669174 test loss: 2.479738
[Epoch 76; Iter    15/   55] train: loss: 0.1750793
[Epoch 76; Iter    45/   55] train: loss: 0.0498835
[Epoch 76] ogbg-molbbbp: 0.949019 val loss: 0.484793
[Epoch 76] ogbg-molbbbp: 0.655864 test loss: 1.762592
[Epoch 77; Iter    20/   55] train: loss: 0.0735437
[Epoch 77; Iter    50/   55] train: loss: 0.1373983
[Epoch 77] ogbg-molbbbp: 0.932291 val loss: 0.615317
[Epoch 77] ogbg-molbbbp: 0.627990 test loss: 2.006440
[Epoch 78; Iter    25/   55] train: loss: 0.0101749
[Epoch 78; Iter    55/   55] train: loss: 0.0057085
[Epoch 78] ogbg-molbbbp: 0.959773 val loss: 0.401169
[Epoch 78] ogbg-molbbbp: 0.637828 test loss: 1.966576
[Epoch 79; Iter    30/   55] train: loss: 0.0602339
[Epoch 79] ogbg-molbbbp: 0.955292 val loss: 0.456461
[Epoch 79] ogbg-molbbbp: 0.660880 test loss: 1.915424
[Epoch 80; Iter     5/   55] train: loss: 0.0157310
[Epoch 80; Iter    35/   55] train: loss: 0.0652172
[Epoch 80] ogbg-molbbbp: 0.952703 val loss: 0.371442
[Epoch 80] ogbg-molbbbp: 0.620563 test loss: 1.602555
[Epoch 81; Iter    10/   55] train: loss: 0.1499484
[Epoch 81; Iter    40/   55] train: loss: 0.0544232
[Epoch 81] ogbg-molbbbp: 0.952206 val loss: 0.351471
[Epoch 81] ogbg-molbbbp: 0.651138 test loss: 1.522816
[Epoch 82; Iter    15/   55] train: loss: 0.0150457
[Epoch 82; Iter    45/   55] train: loss: 0.0441339
[Epoch 82] ogbg-molbbbp: 0.931992 val loss: 0.625114
[Epoch 82] ogbg-molbbbp: 0.631269 test loss: 1.975246
[Epoch 83; Iter    20/   55] train: loss: 0.0767686
[Epoch 83; Iter    50/   55] train: loss: 0.0261496
[Epoch 83] ogbg-molbbbp: 0.941352 val loss: 0.408302
[Epoch 83] ogbg-molbbbp: 0.592978 test loss: 1.835697
[Epoch 84; Iter    25/   55] train: loss: 0.1489407
[Epoch 84; Iter    55/   55] train: loss: 0.0061610
[Epoch 84] ogbg-molbbbp: 0.952006 val loss: 0.488472
[Epoch 84] ogbg-molbbbp: 0.638214 test loss: 2.266447
[Epoch 85; Iter    30/   55] train: loss: 0.1613584
[Epoch 85] ogbg-molbbbp: 0.949915 val loss: 0.502445
[Epoch 85] ogbg-molbbbp: 0.639660 test loss: 2.019004
[Epoch 86; Iter     5/   55] train: loss: 0.0079130
[Epoch 86; Iter    35/   55] train: loss: 0.1439564
[Epoch 86] ogbg-molbbbp: 0.950413 val loss: 0.505268
[Epoch 86] ogbg-molbbbp: 0.630208 test loss: 2.045012
[Epoch 87; Iter    10/   55] train: loss: 0.0417993
[Epoch 87; Iter    40/   55] train: loss: 0.0078963
[Epoch 87] ogbg-molbbbp: 0.947028 val loss: 0.510869
[Epoch 87] ogbg-molbbbp: 0.626061 test loss: 2.183392
[Epoch 88; Iter    15/   55] train: loss: 0.0435113
[Epoch 88; Iter    45/   55] train: loss: 0.0063941
[Epoch 88] ogbg-molbbbp: 0.942746 val loss: 0.569682
[Epoch 88] ogbg-molbbbp: 0.629823 test loss: 2.165981
[Epoch 89; Iter    20/   55] train: loss: 0.1397833
[Epoch 89; Iter    50/   55] train: loss: 0.0074003
[Epoch 89] ogbg-molbbbp: 0.943045 val loss: 0.557913
[Epoch 89] ogbg-molbbbp: 0.632523 test loss: 2.105812
[Epoch 90; Iter    25/   55] train: loss: 0.0208082
[Epoch 90; Iter    55/   55] train: loss: 0.0025455
[Epoch 90] ogbg-molbbbp: 0.948820 val loss: 0.471742
[Epoch 90] ogbg-molbbbp: 0.628762 test loss: 2.125656
[Epoch 91; Iter    30/   55] train: loss: 0.0029760
[Epoch 91] ogbg-molbbbp: 0.937867 val loss: 0.555919
[Epoch 91] ogbg-molbbbp: 0.614776 test loss: 2.105407
[Epoch 92; Iter     5/   55] train: loss: 0.0061711
[Epoch 92; Iter    35/   55] train: loss: 0.0083709
[Epoch 92] ogbg-molbbbp: 0.947426 val loss: 0.501596
[Epoch 92] ogbg-molbbbp: 0.615258 test loss: 2.224165
[Epoch 93; Iter    10/   55] train: loss: 0.0039615
[Epoch 93; Iter    40/   55] train: loss: 0.0036822
[Epoch 93] ogbg-molbbbp: 0.947426 val loss: 0.555045
[Epoch 93] ogbg-molbbbp: 0.625096 test loss: 2.288871
[Epoch 94; Iter    15/   55] train: loss: 0.0213724
[Epoch 94; Iter    45/   55] train: loss: 0.0166682
[Epoch 94] ogbg-molbbbp: 0.947824 val loss: 0.489333
[Epoch 94] ogbg-molbbbp: 0.634066 test loss: 2.093600
[Epoch 95; Iter    20/   55] train: loss: 0.0166886
[Epoch 95; Iter    50/   55] train: loss: 0.0071281
[Epoch 95] ogbg-molbbbp: 0.945235 val loss: 0.536387
[Epoch 95] ogbg-molbbbp: 0.629919 test loss: 2.253603
[Epoch 96; Iter    25/   55] train: loss: 0.0271157
[Epoch 96; Iter    55/   55] train: loss: 0.0045042
[Epoch 96] ogbg-molbbbp: 0.942547 val loss: 0.496009
[Epoch 96] ogbg-molbbbp: 0.644097 test loss: 2.027751
[Epoch 97; Iter    30/   55] train: loss: 0.0031494
[Epoch 97] ogbg-molbbbp: 0.941352 val loss: 0.679492
[Epoch 97] ogbg-molbbbp: 0.648534 test loss: 2.471188
[Epoch 98; Iter     5/   55] train: loss: 0.0076451
[Epoch 98; Iter    35/   55] train: loss: 0.0098510
[Epoch 98] ogbg-molbbbp: 0.950015 val loss: 0.484526
[Epoch 98] ogbg-molbbbp: 0.619695 test loss: 2.138109
[Epoch 99; Iter    10/   55] train: loss: 0.0949242
[Epoch 99; Iter    40/   55] train: loss: 0.0288756
[Epoch 99] ogbg-molbbbp: 0.945036 val loss: 0.586204
[Epoch 99] ogbg-molbbbp: 0.646798 test loss: 2.345728
[Epoch 100; Iter    15/   55] train: loss: 0.0085145
[Epoch 100; Iter    45/   55] train: loss: 0.0062348
[Epoch 100] ogbg-molbbbp: 0.954197 val loss: 0.470293
[Epoch 100] ogbg-molbbbp: 0.638021 test loss: 2.064904
[Epoch 101; Iter    20/   55] train: loss: 0.1004284
[Epoch 101; Iter    50/   55] train: loss: 0.0163074
[Epoch 101] ogbg-molbbbp: 0.945235 val loss: 0.603638
[Epoch 101] ogbg-molbbbp: 0.648341 test loss: 2.320944
[Epoch 102; Iter    25/   55] train: loss: 0.0517829
[Epoch 102; Iter    55/   55] train: loss: 0.0107095
[Epoch 102] ogbg-molbbbp: 0.951210 val loss: 0.453980
[Epoch 102] ogbg-molbbbp: 0.629726 test loss: 2.059039
[Epoch 103; Iter    30/   55] train: loss: 0.0022328
[Epoch 103] ogbg-molbbbp: 0.948621 val loss: 0.518763
[Epoch 103] ogbg-molbbbp: 0.619888 test loss: 2.274752
[Epoch 104; Iter     5/   55] train: loss: 0.0116825
[Epoch 104; Iter    35/   55] train: loss: 0.0172919
[Epoch 104] ogbg-molbbbp: 0.948322 val loss: 0.516533
[Epoch 104] ogbg-molbbbp: 0.625482 test loss: 2.261301
[Epoch 105; Iter    10/   55] train: loss: 0.0032352
[Epoch 105; Iter    40/   55] train: loss: 0.0625710
[Epoch 105] ogbg-molbbbp: 0.949617 val loss: 0.526731
[Epoch 105] ogbg-molbbbp: 0.636671 test loss: 2.383368
[Epoch 106; Iter    15/   55] train: loss: 0.0086009
[Epoch 106; Iter    45/   55] train: loss: 0.0375505
[Epoch 106] ogbg-molbbbp: 0.945733 val loss: 0.560726
[Epoch 106] ogbg-molbbbp: 0.628086 test loss: 2.260062
[Epoch 107; Iter    20/   55] train: loss: 0.0030890
[Epoch 107; Iter    50/   55] train: loss: 0.0536966
[Epoch 107] ogbg-molbbbp: 0.951807 val loss: 0.517039
[Epoch 107] ogbg-molbbbp: 0.635224 test loss: 2.321077
[Epoch 108; Iter    25/   55] train: loss: 0.0039434
[Epoch 108; Iter    55/   55] train: loss: 0.1202603
[Epoch 108] ogbg-molbbbp: 0.947227 val loss: 0.578203
[Epoch 108] ogbg-molbbbp: 0.634549 test loss: 2.391416
[Epoch 109; Iter    30/   55] train: loss: 0.0026413
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 149; Iter    20/   55] train: loss: 0.0007142
[Epoch 149; Iter    50/   55] train: loss: 0.0005201
[Epoch 149] ogbg-molbbbp: 0.944439 val loss: 0.683335
[Epoch 149] ogbg-molbbbp: 0.662809 test loss: 3.346332
[Epoch 150; Iter    25/   55] train: loss: 0.0073969
[Epoch 150; Iter    55/   55] train: loss: 0.0002552
[Epoch 150] ogbg-molbbbp: 0.943941 val loss: 0.692062
[Epoch 150] ogbg-molbbbp: 0.660976 test loss: 3.318940
[Epoch 151; Iter    30/   55] train: loss: 0.0000849
[Epoch 151] ogbg-molbbbp: 0.937867 val loss: 0.810348
[Epoch 151] ogbg-molbbbp: 0.658275 test loss: 3.246130
[Epoch 152; Iter     5/   55] train: loss: 0.0002569
[Epoch 152; Iter    35/   55] train: loss: 0.0000649
[Epoch 152] ogbg-molbbbp: 0.948023 val loss: 0.590665
[Epoch 152] ogbg-molbbbp: 0.662326 test loss: 3.133523
[Epoch 153; Iter    10/   55] train: loss: 0.0002390
[Epoch 153; Iter    40/   55] train: loss: 0.0001403
[Epoch 153] ogbg-molbbbp: 0.946430 val loss: 0.689508
[Epoch 153] ogbg-molbbbp: 0.656636 test loss: 3.392338
[Epoch 154; Iter    15/   55] train: loss: 0.0001517
[Epoch 154; Iter    45/   55] train: loss: 0.0000839
[Epoch 154] ogbg-molbbbp: 0.902519 val loss: 2.000788
[Epoch 154] ogbg-molbbbp: 0.641011 test loss: 3.587726
[Epoch 155; Iter    20/   55] train: loss: 0.0001215
[Epoch 155; Iter    50/   55] train: loss: 0.0000807
[Epoch 155] ogbg-molbbbp: 0.950115 val loss: 0.650575
[Epoch 155] ogbg-molbbbp: 0.659819 test loss: 3.525291
[Epoch 156; Iter    25/   55] train: loss: 0.0326566
[Epoch 156; Iter    55/   55] train: loss: 0.0001827
[Epoch 156] ogbg-molbbbp: 0.950214 val loss: 0.618856
[Epoch 156] ogbg-molbbbp: 0.653067 test loss: 3.438776
[Epoch 157; Iter    30/   55] train: loss: 0.0001145
[Epoch 157] ogbg-molbbbp: 0.934780 val loss: 0.935390
[Epoch 157] ogbg-molbbbp: 0.648438 test loss: 3.554533
[Epoch 158; Iter     5/   55] train: loss: 0.0001708
[Epoch 158; Iter    35/   55] train: loss: 0.0001177
[Epoch 158] ogbg-molbbbp: 0.947127 val loss: 0.658264
[Epoch 158] ogbg-molbbbp: 0.640914 test loss: 3.563553
[Epoch 159; Iter    10/   55] train: loss: 0.0039308
[Epoch 159; Iter    40/   55] train: loss: 0.0005025
[Epoch 159] ogbg-molbbbp: 0.949218 val loss: 0.609838
[Epoch 159] ogbg-molbbbp: 0.643519 test loss: 3.454666
[Epoch 160; Iter    15/   55] train: loss: 0.0009518
[Epoch 160; Iter    45/   55] train: loss: 0.0000879
[Epoch 160] ogbg-molbbbp: 0.907996 val loss: 1.740298
[Epoch 160] ogbg-molbbbp: 0.639853 test loss: 3.429927
[Epoch 161; Iter    20/   55] train: loss: 0.0010485
[Epoch 161; Iter    50/   55] train: loss: 0.0001811
[Epoch 161] ogbg-molbbbp: 0.903814 val loss: 1.903001
[Epoch 161] ogbg-molbbbp: 0.640046 test loss: 3.472310
[Epoch 162; Iter    25/   55] train: loss: 0.0000944
[Epoch 162; Iter    55/   55] train: loss: 0.0002719
[Epoch 162] ogbg-molbbbp: 0.949517 val loss: 0.604862
[Epoch 162] ogbg-molbbbp: 0.650559 test loss: 3.442935
[Epoch 163; Iter    30/   55] train: loss: 0.0001209
[Epoch 163] ogbg-molbbbp: 0.929304 val loss: 1.159910
[Epoch 163] ogbg-molbbbp: 0.647280 test loss: 3.426165
[Epoch 164; Iter     5/   55] train: loss: 0.0002561
[Epoch 164; Iter    35/   55] train: loss: 0.0000411
[Epoch 164] ogbg-molbbbp: 0.929404 val loss: 1.219282
[Epoch 164] ogbg-molbbbp: 0.650077 test loss: 3.479938
[Epoch 165; Iter    10/   55] train: loss: 0.0000716
[Epoch 165; Iter    40/   55] train: loss: 0.0000832
[Epoch 165] ogbg-molbbbp: 0.947625 val loss: 0.646255
[Epoch 165] ogbg-molbbbp: 0.652006 test loss: 3.513866
[Epoch 166; Iter    15/   55] train: loss: 0.0002818
[Epoch 166; Iter    45/   55] train: loss: 0.0003136
[Epoch 166] ogbg-molbbbp: 0.938564 val loss: 0.781654
[Epoch 166] ogbg-molbbbp: 0.651331 test loss: 3.601970
[Epoch 167; Iter    20/   55] train: loss: 0.0003311
[Epoch 167; Iter    50/   55] train: loss: 0.0000288
[Epoch 167] ogbg-molbbbp: 0.931495 val loss: 1.083579
[Epoch 167] ogbg-molbbbp: 0.650849 test loss: 3.436358
[Epoch 168; Iter    25/   55] train: loss: 0.0005343
[Epoch 168; Iter    55/   55] train: loss: 0.0001490
[Epoch 168] ogbg-molbbbp: 0.906104 val loss: 1.847427
[Epoch 168] ogbg-molbbbp: 0.634742 test loss: 3.578774
[Epoch 169; Iter    30/   55] train: loss: 0.0001732
[Epoch 169] ogbg-molbbbp: 0.948521 val loss: 0.648402
[Epoch 169] ogbg-molbbbp: 0.653742 test loss: 3.499502
[Epoch 170; Iter     5/   55] train: loss: 0.0000871
[Epoch 170; Iter    35/   55] train: loss: 0.0001349
[Epoch 170] ogbg-molbbbp: 0.932092 val loss: 1.235705
[Epoch 170] ogbg-molbbbp: 0.645930 test loss: 3.794606
[Epoch 171; Iter    10/   55] train: loss: 0.0013357
[Epoch 171; Iter    40/   55] train: loss: 0.0001632
[Epoch 171] ogbg-molbbbp: 0.922135 val loss: 1.541856
[Epoch 171] ogbg-molbbbp: 0.640625 test loss: 3.671029
[Epoch 172; Iter    15/   55] train: loss: 0.0005763
[Epoch 172; Iter    45/   55] train: loss: 0.0003057
[Epoch 172] ogbg-molbbbp: 0.935577 val loss: 1.053829
[Epoch 172] ogbg-molbbbp: 0.643904 test loss: 3.606099
[Epoch 173; Iter    20/   55] train: loss: 0.0001270
[Epoch 173; Iter    50/   55] train: loss: 0.0007566
[Epoch 173] ogbg-molbbbp: 0.918749 val loss: 1.564823
[Epoch 173] ogbg-molbbbp: 0.639564 test loss: 3.609578
[Epoch 174; Iter    25/   55] train: loss: 0.0003731
[Epoch 174; Iter    55/   55] train: loss: 0.0002125
[Epoch 174] ogbg-molbbbp: 0.927711 val loss: 1.383839
[Epoch 174] ogbg-molbbbp: 0.632620 test loss: 3.734208
[Epoch 175; Iter    30/   55] train: loss: 0.0000816
[Epoch 175] ogbg-molbbbp: 0.918252 val loss: 1.677726
[Epoch 175] ogbg-molbbbp: 0.622492 test loss: 3.701154
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 175 epochs. Best model checkpoint was in epoch 115.
Statistics on  val_best_checkpoint
mean_pred: -3.0895566940307617
std_pred: 29.087289810180664
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9278200925932822
rocauc: 0.9598725480434134
ogbg-molbbbp: 0.9598725480434134
BCEWithLogitsLoss: 0.49608482314007624
Statistics on  test
mean_pred: -2.2106592655181885
std_pred: 73.0383529663086
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6974076362346739
rocauc: 0.6598186728395061
ogbg-molbbbp: 0.6598186728395061
BCEWithLogitsLoss: 3.067990303039551
Statistics on  train
mean_pred: 7.0495734214782715
std_pred: 8.11971664428711
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 1.0
rocauc: 1.0
ogbg-molbbbp: 1.0
BCEWithLogitsLoss: 0.000232628732919693
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.2.yml --seed 4 --device cuda:3
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.2.yml --seed 5 --device cuda:3
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.2.yml --seed 6 --device cuda:3
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.944240 val loss: 0.624432
[Epoch 109] ogbg-molbbbp: 0.675251 test loss: 2.297485
[Epoch 110; Iter     5/   55] train: loss: 0.0069638
[Epoch 110; Iter    35/   55] train: loss: 0.0016277
[Epoch 110] ogbg-molbbbp: 0.944538 val loss: 0.543650
[Epoch 110] ogbg-molbbbp: 0.669753 test loss: 2.119464
[Epoch 111; Iter    10/   55] train: loss: 0.0010011
[Epoch 111; Iter    40/   55] train: loss: 0.0009905
[Epoch 111] ogbg-molbbbp: 0.942149 val loss: 0.582186
[Epoch 111] ogbg-molbbbp: 0.668210 test loss: 2.152203
[Epoch 112; Iter    15/   55] train: loss: 0.0034036
[Epoch 112; Iter    45/   55] train: loss: 0.0018590
[Epoch 112] ogbg-molbbbp: 0.941750 val loss: 0.656456
[Epoch 112] ogbg-molbbbp: 0.678723 test loss: 2.371150
[Epoch 113; Iter    20/   55] train: loss: 0.0233495
[Epoch 113; Iter    50/   55] train: loss: 0.0180538
[Epoch 113] ogbg-molbbbp: 0.938863 val loss: 0.575714
[Epoch 113] ogbg-molbbbp: 0.673997 test loss: 2.103981
[Epoch 114; Iter    25/   55] train: loss: 0.0015838
[Epoch 114; Iter    55/   55] train: loss: 0.0080639
[Epoch 114] ogbg-molbbbp: 0.940755 val loss: 0.640693
[Epoch 114] ogbg-molbbbp: 0.665799 test loss: 2.361652
[Epoch 115; Iter    30/   55] train: loss: 0.0165870
[Epoch 115] ogbg-molbbbp: 0.941750 val loss: 0.617290
[Epoch 115] ogbg-molbbbp: 0.668499 test loss: 2.365674
[Epoch 116; Iter     5/   55] train: loss: 0.0022984
[Epoch 116; Iter    35/   55] train: loss: 0.0158194
[Epoch 116] ogbg-molbbbp: 0.934382 val loss: 0.686076
[Epoch 116] ogbg-molbbbp: 0.658854 test loss: 2.383077
[Epoch 117; Iter    10/   55] train: loss: 0.0646324
[Epoch 117; Iter    40/   55] train: loss: 0.0020701
[Epoch 117] ogbg-molbbbp: 0.940655 val loss: 0.647534
[Epoch 117] ogbg-molbbbp: 0.672550 test loss: 2.349280
[Epoch 118; Iter    15/   55] train: loss: 0.0018004
[Epoch 118; Iter    45/   55] train: loss: 0.0024019
[Epoch 118] ogbg-molbbbp: 0.946231 val loss: 0.562752
[Epoch 118] ogbg-molbbbp: 0.669464 test loss: 2.286236
[Epoch 119; Iter    20/   55] train: loss: 0.0672669
[Epoch 119; Iter    50/   55] train: loss: 0.0018879
[Epoch 119] ogbg-molbbbp: 0.941153 val loss: 0.703835
[Epoch 119] ogbg-molbbbp: 0.657118 test loss: 2.493542
[Epoch 120; Iter    25/   55] train: loss: 0.0301185
[Epoch 120; Iter    55/   55] train: loss: 0.0020584
[Epoch 120] ogbg-molbbbp: 0.943244 val loss: 0.671477
[Epoch 120] ogbg-molbbbp: 0.667245 test loss: 2.442385
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 40.
Statistics on  val_best_checkpoint
mean_pred: -0.8149829506874084
std_pred: 6.078700065612793
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9565923547000457
rocauc: 0.9719207408144976
ogbg-molbbbp: 0.9719207408144976
BCEWithLogitsLoss: 0.2537672274879047
Statistics on  test
mean_pred: 1.8421497344970703
std_pred: 4.32622766494751
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.692652354315798
rocauc: 0.6819058641975309
ogbg-molbbbp: 0.6819058641975309
BCEWithLogitsLoss: 1.3580974084990365
Statistics on  train
mean_pred: 3.29396653175354
std_pred: 3.2721714973449707
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9974875837780266
rocauc: 0.9876658172511277
ogbg-molbbbp: 0.9876658172511277
BCEWithLogitsLoss: 0.1076399032365192
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.935975 val loss: 0.638227
[Epoch 109] ogbg-molbbbp: 0.646123 test loss: 2.202115
[Epoch 110; Iter     5/   55] train: loss: 0.0038292
[Epoch 110; Iter    35/   55] train: loss: 0.0082900
[Epoch 110] ogbg-molbbbp: 0.931395 val loss: 0.624984
[Epoch 110] ogbg-molbbbp: 0.635802 test loss: 2.113864
[Epoch 111; Iter    10/   55] train: loss: 0.0039010
[Epoch 111; Iter    40/   55] train: loss: 0.0018606
[Epoch 111] ogbg-molbbbp: 0.942647 val loss: 0.591177
[Epoch 111] ogbg-molbbbp: 0.660687 test loss: 2.214737
[Epoch 112; Iter    15/   55] train: loss: 0.0046519
[Epoch 112; Iter    45/   55] train: loss: 0.0050192
[Epoch 112] ogbg-molbbbp: 0.942049 val loss: 0.601359
[Epoch 112] ogbg-molbbbp: 0.646026 test loss: 2.277916
[Epoch 113; Iter    20/   55] train: loss: 0.0107073
[Epoch 113; Iter    50/   55] train: loss: 0.0041698
[Epoch 113] ogbg-molbbbp: 0.938962 val loss: 0.659529
[Epoch 113] ogbg-molbbbp: 0.638214 test loss: 2.308250
[Epoch 114; Iter    25/   55] train: loss: 0.0300775
[Epoch 114; Iter    55/   55] train: loss: 0.2590482
[Epoch 114] ogbg-molbbbp: 0.939460 val loss: 0.644640
[Epoch 114] ogbg-molbbbp: 0.653646 test loss: 2.313752
[Epoch 115; Iter    30/   55] train: loss: 0.0038916
[Epoch 115] ogbg-molbbbp: 0.946032 val loss: 0.523798
[Epoch 115] ogbg-molbbbp: 0.652585 test loss: 2.111871
[Epoch 116; Iter     5/   55] train: loss: 0.0014116
[Epoch 116; Iter    35/   55] train: loss: 0.0038831
[Epoch 116] ogbg-molbbbp: 0.938465 val loss: 0.644700
[Epoch 116] ogbg-molbbbp: 0.629726 test loss: 2.417217
[Epoch 117; Iter    10/   55] train: loss: 0.0978563
[Epoch 117; Iter    40/   55] train: loss: 0.0050907
[Epoch 117] ogbg-molbbbp: 0.941352 val loss: 0.643185
[Epoch 117] ogbg-molbbbp: 0.649016 test loss: 2.367752
[Epoch 118; Iter    15/   55] train: loss: 0.0027645
[Epoch 118; Iter    45/   55] train: loss: 0.0019276
[Epoch 118] ogbg-molbbbp: 0.938066 val loss: 0.666123
[Epoch 118] ogbg-molbbbp: 0.631462 test loss: 2.527893
[Epoch 119; Iter    20/   55] train: loss: 0.0550652
[Epoch 119; Iter    50/   55] train: loss: 0.0319576
[Epoch 119] ogbg-molbbbp: 0.946231 val loss: 0.588460
[Epoch 119] ogbg-molbbbp: 0.650656 test loss: 2.326057
[Epoch 120; Iter    25/   55] train: loss: 0.0090764
[Epoch 120; Iter    55/   55] train: loss: 0.0010857
[Epoch 120] ogbg-molbbbp: 0.944937 val loss: 0.550639
[Epoch 120] ogbg-molbbbp: 0.637731 test loss: 2.270240
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 49.
Statistics on  val_best_checkpoint
mean_pred: -0.04833042249083519
std_pred: 6.151402950286865
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9534505164637735
rocauc: 0.9694314447874142
ogbg-molbbbp: 0.9694314447874142
BCEWithLogitsLoss: 0.31228756265980856
Statistics on  test
mean_pred: 1.7550634145736694
std_pred: 4.465105056762695
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.7164285834433872
rocauc: 0.6949266975308641
ogbg-molbbbp: 0.6949266975308641
BCEWithLogitsLoss: 1.3307540927614485
Statistics on  train
mean_pred: 4.553627967834473
std_pred: 4.289630889892578
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9985762473152502
rocauc: 0.9927204902447321
ogbg-molbbbp: 0.9927204902447321
BCEWithLogitsLoss: 0.08813102313063362
[Epoch 109] ogbg-molbbbp: 0.936473 val loss: 0.480766
[Epoch 109] ogbg-molbbbp: 0.581019 test loss: 2.193384
[Epoch 110; Iter     5/   55] train: loss: 0.0408779
[Epoch 110; Iter    35/   55] train: loss: 0.1236812
[Epoch 110] ogbg-molbbbp: 0.954097 val loss: 0.561767
[Epoch 110] ogbg-molbbbp: 0.608893 test loss: 2.599226
[Epoch 111; Iter    10/   55] train: loss: 0.0057282
[Epoch 111; Iter    40/   55] train: loss: 0.0031820
[Epoch 111] ogbg-molbbbp: 0.951708 val loss: 0.557045
[Epoch 111] ogbg-molbbbp: 0.640721 test loss: 2.334418
[Epoch 112; Iter    15/   55] train: loss: 0.0510457
[Epoch 112; Iter    45/   55] train: loss: 0.0033237
[Epoch 112] ogbg-molbbbp: 0.942945 val loss: 0.681376
[Epoch 112] ogbg-molbbbp: 0.634934 test loss: 2.667924
[Epoch 113; Iter    20/   55] train: loss: 0.0064737
[Epoch 113; Iter    50/   55] train: loss: 0.0967859
[Epoch 113] ogbg-molbbbp: 0.962860 val loss: 0.576388
[Epoch 113] ogbg-molbbbp: 0.617188 test loss: 2.980194
[Epoch 114; Iter    25/   55] train: loss: 0.0021778
[Epoch 114; Iter    55/   55] train: loss: 0.0047339
[Epoch 114] ogbg-molbbbp: 0.939759 val loss: 0.847054
[Epoch 114] ogbg-molbbbp: 0.581694 test loss: 3.315471
[Epoch 115; Iter    30/   55] train: loss: 0.0074123
[Epoch 115] ogbg-molbbbp: 0.952703 val loss: 0.530597
[Epoch 115] ogbg-molbbbp: 0.628472 test loss: 2.355106
[Epoch 116; Iter     5/   55] train: loss: 0.0055279
[Epoch 116; Iter    35/   55] train: loss: 0.0435787
[Epoch 116] ogbg-molbbbp: 0.948621 val loss: 0.626499
[Epoch 116] ogbg-molbbbp: 0.640432 test loss: 2.556476
[Epoch 117; Iter    10/   55] train: loss: 0.0279010
[Epoch 117; Iter    40/   55] train: loss: 0.0154675
[Epoch 117] ogbg-molbbbp: 0.952106 val loss: 0.603449
[Epoch 117] ogbg-molbbbp: 0.641397 test loss: 2.445776
[Epoch 118; Iter    15/   55] train: loss: 0.1460751
[Epoch 118; Iter    45/   55] train: loss: 0.0047399
[Epoch 118] ogbg-molbbbp: 0.953002 val loss: 0.553699
[Epoch 118] ogbg-molbbbp: 0.649595 test loss: 2.169356
[Epoch 119; Iter    20/   55] train: loss: 0.0031611
[Epoch 119; Iter    50/   55] train: loss: 0.1013861
[Epoch 119] ogbg-molbbbp: 0.957682 val loss: 0.482767
[Epoch 119] ogbg-molbbbp: 0.652296 test loss: 2.234039
[Epoch 120; Iter    25/   55] train: loss: 0.0822513
[Epoch 120; Iter    55/   55] train: loss: 0.1040576
[Epoch 120] ogbg-molbbbp: 0.949318 val loss: 0.576373
[Epoch 120] ogbg-molbbbp: 0.639660 test loss: 2.317684
[Epoch 121; Iter    30/   55] train: loss: 0.0531603
[Epoch 121] ogbg-molbbbp: 0.946231 val loss: 0.509712
[Epoch 121] ogbg-molbbbp: 0.654996 test loss: 2.051103
[Epoch 122; Iter     5/   55] train: loss: 0.0796894
[Epoch 122; Iter    35/   55] train: loss: 0.0019792
[Epoch 122] ogbg-molbbbp: 0.946132 val loss: 0.622612
[Epoch 122] ogbg-molbbbp: 0.651235 test loss: 2.528623
[Epoch 123; Iter    10/   55] train: loss: 0.0125971
[Epoch 123; Iter    40/   55] train: loss: 0.0051307
[Epoch 123] ogbg-molbbbp: 0.947824 val loss: 0.532392
[Epoch 123] ogbg-molbbbp: 0.648148 test loss: 2.225197
[Epoch 124; Iter    15/   55] train: loss: 0.0085299
[Epoch 124; Iter    45/   55] train: loss: 0.0015761
[Epoch 124] ogbg-molbbbp: 0.940755 val loss: 0.642352
[Epoch 124] ogbg-molbbbp: 0.654417 test loss: 2.306476
[Epoch 125; Iter    20/   55] train: loss: 0.1118523
[Epoch 125; Iter    50/   55] train: loss: 0.0347266
[Epoch 125] ogbg-molbbbp: 0.947924 val loss: 0.570210
[Epoch 125] ogbg-molbbbp: 0.646991 test loss: 2.298112
[Epoch 126; Iter    25/   55] train: loss: 0.0986483
[Epoch 126; Iter    55/   55] train: loss: 0.0015954
[Epoch 126] ogbg-molbbbp: 0.946331 val loss: 0.617383
[Epoch 126] ogbg-molbbbp: 0.660108 test loss: 2.345233
[Epoch 127; Iter    30/   55] train: loss: 0.0016056
[Epoch 127] ogbg-molbbbp: 0.948820 val loss: 0.589793
[Epoch 127] ogbg-molbbbp: 0.662133 test loss: 2.382343
[Epoch 128; Iter     5/   55] train: loss: 0.0016427
[Epoch 128; Iter    35/   55] train: loss: 0.0010147
[Epoch 128] ogbg-molbbbp: 0.946132 val loss: 0.566122
[Epoch 128] ogbg-molbbbp: 0.650463 test loss: 2.287645
[Epoch 129; Iter    10/   55] train: loss: 0.0039864
[Epoch 129; Iter    40/   55] train: loss: 0.0064567
[Epoch 129] ogbg-molbbbp: 0.947625 val loss: 0.569212
[Epoch 129] ogbg-molbbbp: 0.654321 test loss: 2.336884
[Epoch 130; Iter    15/   55] train: loss: 0.0430136
[Epoch 130; Iter    45/   55] train: loss: 0.0031360
[Epoch 130] ogbg-molbbbp: 0.940755 val loss: 0.706718
[Epoch 130] ogbg-molbbbp: 0.643711 test loss: 2.515561
[Epoch 131; Iter    20/   55] train: loss: 0.0209089
[Epoch 131; Iter    50/   55] train: loss: 0.0018052
[Epoch 131] ogbg-molbbbp: 0.940157 val loss: 0.692737
[Epoch 131] ogbg-molbbbp: 0.622878 test loss: 2.637695
[Epoch 132; Iter    25/   55] train: loss: 0.0045457
[Epoch 132; Iter    55/   55] train: loss: 0.0015928
[Epoch 132] ogbg-molbbbp: 0.933287 val loss: 0.857656
[Epoch 132] ogbg-molbbbp: 0.633777 test loss: 2.674340
[Epoch 133; Iter    30/   55] train: loss: 0.0009900
[Epoch 133] ogbg-molbbbp: 0.946530 val loss: 0.728891
[Epoch 133] ogbg-molbbbp: 0.625000 test loss: 3.003992
[Epoch 134; Iter     5/   55] train: loss: 0.0005813
[Epoch 134; Iter    35/   55] train: loss: 0.0963425
[Epoch 134] ogbg-molbbbp: 0.955193 val loss: 0.553661
[Epoch 134] ogbg-molbbbp: 0.630787 test loss: 2.576223
[Epoch 135; Iter    10/   55] train: loss: 0.0035737
[Epoch 135; Iter    40/   55] train: loss: 0.0030857
[Epoch 135] ogbg-molbbbp: 0.951309 val loss: 0.671377
[Epoch 135] ogbg-molbbbp: 0.631944 test loss: 2.813553
[Epoch 136; Iter    15/   55] train: loss: 0.0060794
[Epoch 136; Iter    45/   55] train: loss: 0.1753115
[Epoch 136] ogbg-molbbbp: 0.932590 val loss: 0.705398
[Epoch 136] ogbg-molbbbp: 0.640721 test loss: 2.584365
[Epoch 137; Iter    20/   55] train: loss: 0.0063063
[Epoch 137; Iter    50/   55] train: loss: 0.0021394
[Epoch 137] ogbg-molbbbp: 0.939460 val loss: 0.687281
[Epoch 137] ogbg-molbbbp: 0.628376 test loss: 2.686844
[Epoch 138; Iter    25/   55] train: loss: 0.0682306
[Epoch 138; Iter    55/   55] train: loss: 0.0113158
[Epoch 138] ogbg-molbbbp: 0.938465 val loss: 0.708474
[Epoch 138] ogbg-molbbbp: 0.616127 test loss: 2.574957
[Epoch 139; Iter    30/   55] train: loss: 0.0017619
[Epoch 139] ogbg-molbbbp: 0.944240 val loss: 0.651012
[Epoch 139] ogbg-molbbbp: 0.627990 test loss: 2.688269
[Epoch 140; Iter     5/   55] train: loss: 0.0104305
[Epoch 140; Iter    35/   55] train: loss: 0.0556998
[Epoch 140] ogbg-molbbbp: 0.948422 val loss: 0.595128
[Epoch 140] ogbg-molbbbp: 0.624904 test loss: 2.652763
[Epoch 141; Iter    10/   55] train: loss: 0.0051936
[Epoch 141; Iter    40/   55] train: loss: 0.0239783
[Epoch 141] ogbg-molbbbp: 0.938763 val loss: 0.698859
[Epoch 141] ogbg-molbbbp: 0.620853 test loss: 2.739287
[Epoch 142; Iter    15/   55] train: loss: 0.0011885
[Epoch 142; Iter    45/   55] train: loss: 0.0055910
[Epoch 142] ogbg-molbbbp: 0.933287 val loss: 0.676133
[Epoch 142] ogbg-molbbbp: 0.632812 test loss: 2.638025
[Epoch 143; Iter    20/   55] train: loss: 0.0013160
[Epoch 143; Iter    50/   55] train: loss: 0.0132353
[Epoch 143] ogbg-molbbbp: 0.932391 val loss: 0.688109
[Epoch 143] ogbg-molbbbp: 0.624711 test loss: 2.779395
[Epoch 144; Iter    25/   55] train: loss: 0.0029106
[Epoch 144; Iter    55/   55] train: loss: 0.0526411
[Epoch 144] ogbg-molbbbp: 0.940058 val loss: 0.687518
[Epoch 144] ogbg-molbbbp: 0.626254 test loss: 2.912852
[Epoch 145; Iter    30/   55] train: loss: 0.0088002
[Epoch 145] ogbg-molbbbp: 0.933685 val loss: 0.654587
[Epoch 145] ogbg-molbbbp: 0.627025 test loss: 2.658102
[Epoch 146; Iter     5/   55] train: loss: 0.0008394
[Epoch 146; Iter    35/   55] train: loss: 0.0025963
[Epoch 146] ogbg-molbbbp: 0.929404 val loss: 0.874947
[Epoch 146] ogbg-molbbbp: 0.625000 test loss: 2.698499
[Epoch 147; Iter    10/   55] train: loss: 0.0273033
[Epoch 147; Iter    40/   55] train: loss: 0.0521288
[Epoch 147] ogbg-molbbbp: 0.933586 val loss: 0.756598
[Epoch 147] ogbg-molbbbp: 0.627218 test loss: 2.870330
[Epoch 148; Iter    15/   55] train: loss: 0.0007219
[Epoch 148; Iter    45/   55] train: loss: 0.0012562
[Epoch 148] ogbg-molbbbp: 0.935378 val loss: 0.682953
[Epoch 148] ogbg-molbbbp: 0.629533 test loss: 2.687365
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 149; Iter    20/   55] train: loss: 0.0006192
[Epoch 149; Iter    50/   55] train: loss: 0.0012720
[Epoch 149] ogbg-molbbbp: 0.936672 val loss: 0.622244
[Epoch 149] ogbg-molbbbp: 0.631559 test loss: 2.477952
[Epoch 150; Iter    25/   55] train: loss: 0.0102301
[Epoch 150; Iter    55/   55] train: loss: 0.0005105
[Epoch 150] ogbg-molbbbp: 0.934482 val loss: 0.595375
[Epoch 150] ogbg-molbbbp: 0.634452 test loss: 2.409189
[Epoch 151; Iter    30/   55] train: loss: 0.0220997
[Epoch 151] ogbg-molbbbp: 0.931893 val loss: 0.765551
[Epoch 151] ogbg-molbbbp: 0.620756 test loss: 2.885192
[Epoch 152; Iter     5/   55] train: loss: 0.0007853
[Epoch 152; Iter    35/   55] train: loss: 0.0026508
[Epoch 152] ogbg-molbbbp: 0.937967 val loss: 0.709187
[Epoch 152] ogbg-molbbbp: 0.622878 test loss: 2.806766
[Epoch 153; Iter    10/   55] train: loss: 0.0221465
[Epoch 153; Iter    40/   55] train: loss: 0.0017896
[Epoch 153] ogbg-molbbbp: 0.938465 val loss: 0.671189
[Epoch 153] ogbg-molbbbp: 0.627894 test loss: 2.688144
[Epoch 154; Iter    15/   55] train: loss: 0.0209504
[Epoch 154; Iter    45/   55] train: loss: 0.0020423
[Epoch 154] ogbg-molbbbp: 0.937369 val loss: 0.702366
[Epoch 154] ogbg-molbbbp: 0.630208 test loss: 2.654565
[Epoch 155; Iter    20/   55] train: loss: 0.0007924
[Epoch 155; Iter    50/   55] train: loss: 0.0047919
[Epoch 155] ogbg-molbbbp: 0.938265 val loss: 0.755881
[Epoch 155] ogbg-molbbbp: 0.613619 test loss: 2.919545
[Epoch 156; Iter    25/   55] train: loss: 0.0016287
[Epoch 156; Iter    55/   55] train: loss: 0.0357713
[Epoch 156] ogbg-molbbbp: 0.938166 val loss: 0.670137
[Epoch 156] ogbg-molbbbp: 0.632330 test loss: 2.596612
[Epoch 157; Iter    30/   55] train: loss: 0.0259114
[Epoch 157] ogbg-molbbbp: 0.941850 val loss: 0.686775
[Epoch 157] ogbg-molbbbp: 0.620949 test loss: 2.790323
[Epoch 158; Iter     5/   55] train: loss: 0.0358474
[Epoch 158; Iter    35/   55] train: loss: 0.0248626
[Epoch 158] ogbg-molbbbp: 0.941950 val loss: 0.727114
[Epoch 158] ogbg-molbbbp: 0.629823 test loss: 2.902069
[Epoch 159; Iter    10/   55] train: loss: 0.0792764
[Epoch 159; Iter    40/   55] train: loss: 0.0360330
[Epoch 159] ogbg-molbbbp: 0.941153 val loss: 0.680033
[Epoch 159] ogbg-molbbbp: 0.645062 test loss: 2.568329
[Epoch 160; Iter    15/   55] train: loss: 0.0243400
[Epoch 160; Iter    45/   55] train: loss: 0.0006600
[Epoch 160] ogbg-molbbbp: 0.942149 val loss: 0.652016
[Epoch 160] ogbg-molbbbp: 0.632234 test loss: 2.612943
[Epoch 161; Iter    20/   55] train: loss: 0.0017091
[Epoch 161; Iter    50/   55] train: loss: 0.0191888
[Epoch 161] ogbg-molbbbp: 0.940655 val loss: 0.690435
[Epoch 161] ogbg-molbbbp: 0.634549 test loss: 2.767904
[Epoch 162; Iter    25/   55] train: loss: 0.0755069
[Epoch 162; Iter    55/   55] train: loss: 0.0003095
[Epoch 162] ogbg-molbbbp: 0.940157 val loss: 0.695775
[Epoch 162] ogbg-molbbbp: 0.634645 test loss: 2.862653
[Epoch 163; Iter    30/   55] train: loss: 0.0008057
[Epoch 163] ogbg-molbbbp: 0.942149 val loss: 0.700084
[Epoch 163] ogbg-molbbbp: 0.627894 test loss: 2.912926
[Epoch 164; Iter     5/   55] train: loss: 0.0022796
[Epoch 164; Iter    35/   55] train: loss: 0.0017837
[Epoch 164] ogbg-molbbbp: 0.939859 val loss: 0.673947
[Epoch 164] ogbg-molbbbp: 0.631076 test loss: 2.736344
[Epoch 165; Iter    10/   55] train: loss: 0.0005990
[Epoch 165; Iter    40/   55] train: loss: 0.0010974
[Epoch 165] ogbg-molbbbp: 0.942149 val loss: 0.672933
[Epoch 165] ogbg-molbbbp: 0.630305 test loss: 2.760354
[Epoch 166; Iter    15/   55] train: loss: 0.0205208
[Epoch 166; Iter    45/   55] train: loss: 0.0021992
[Epoch 166] ogbg-molbbbp: 0.938962 val loss: 0.752438
[Epoch 166] ogbg-molbbbp: 0.635995 test loss: 2.797546
[Epoch 167; Iter    20/   55] train: loss: 0.0013038
[Epoch 167; Iter    50/   55] train: loss: 0.0373222
[Epoch 167] ogbg-molbbbp: 0.941153 val loss: 0.637064
[Epoch 167] ogbg-molbbbp: 0.639660 test loss: 2.652976
[Epoch 168; Iter    25/   55] train: loss: 0.0005969
[Epoch 168; Iter    55/   55] train: loss: 0.0011775
[Epoch 168] ogbg-molbbbp: 0.940058 val loss: 0.676315
[Epoch 168] ogbg-molbbbp: 0.635995 test loss: 2.803928
[Epoch 169; Iter    30/   55] train: loss: 0.0201670
[Epoch 169] ogbg-molbbbp: 0.941750 val loss: 0.709489
[Epoch 169] ogbg-molbbbp: 0.633777 test loss: 2.911282
[Epoch 170; Iter     5/   55] train: loss: 0.0126248
[Epoch 170; Iter    35/   55] train: loss: 0.0010173
[Epoch 170] ogbg-molbbbp: 0.943344 val loss: 0.662253
[Epoch 170] ogbg-molbbbp: 0.636478 test loss: 2.744073
[Epoch 171; Iter    10/   55] train: loss: 0.0009213
[Epoch 171; Iter    40/   55] train: loss: 0.0563374
[Epoch 171] ogbg-molbbbp: 0.941153 val loss: 0.815119
[Epoch 171] ogbg-molbbbp: 0.633584 test loss: 3.109026
[Epoch 172; Iter    15/   55] train: loss: 0.0077189
[Epoch 172; Iter    45/   55] train: loss: 0.0156904
[Epoch 172] ogbg-molbbbp: 0.943841 val loss: 0.696275
[Epoch 172] ogbg-molbbbp: 0.634452 test loss: 2.823444
[Epoch 173; Iter    20/   55] train: loss: 0.0005857
[Epoch 173; Iter    50/   55] train: loss: 0.0065135
[Epoch 173] ogbg-molbbbp: 0.939859 val loss: 0.754219
[Epoch 173] ogbg-molbbbp: 0.635610 test loss: 2.886591
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 173 epochs. Best model checkpoint was in epoch 113.
Statistics on  val_best_checkpoint
mean_pred: -8.422119140625
std_pred: 107.56065368652344
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9357138828009662
rocauc: 0.9628597032759136
ogbg-molbbbp: 0.9628597032759136
BCEWithLogitsLoss: 0.5763876502002988
Statistics on  test
mean_pred: 0.9540039300918579
std_pred: 67.38677978515625
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.654264781094765
rocauc: 0.6171875
ogbg-molbbbp: 0.6171875
BCEWithLogitsLoss: 2.980193563870021
Statistics on  train
mean_pred: 7.545321464538574
std_pred: 6.710911750793457
mean_targets: 0.839362382888794
std_targets: 0.36730900406837463
prcauc: 0.9997918406682801
rocauc: 0.9988973396751404
ogbg-molbbbp: 0.9988973396751404
BCEWithLogitsLoss: 0.032380743707868864
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.0.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.0.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bbbp/noise=0.0.yml --seed 6 --device cuda:0
All runs completed.
