>>> Starting run for dataset: bace
Running configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/bace/noise=0.2/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.2_4_26-05_10-07-29
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.2
logdir: runs/static_noise/GraphCL/bace/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6946530
[Epoch 1] ogbg-molbace: 0.408425 val loss: 0.694871
[Epoch 1] ogbg-molbace: 0.295775 test loss: 0.695546
[Epoch 2; Iter    19/   41] train: loss: 0.6905311
[Epoch 2] ogbg-molbace: 0.398901 val loss: 0.695745
[Epoch 2] ogbg-molbace: 0.298035 test loss: 0.700044
[Epoch 3; Iter     8/   41] train: loss: 0.6880651
[Epoch 3; Iter    38/   41] train: loss: 0.6965904
[Epoch 3] ogbg-molbace: 0.409890 val loss: 0.693999
[Epoch 3] ogbg-molbace: 0.298209 test loss: 0.701369
[Epoch 4; Iter    27/   41] train: loss: 0.6951241
[Epoch 4] ogbg-molbace: 0.408425 val loss: 0.693641
[Epoch 4] ogbg-molbace: 0.305164 test loss: 0.701026
[Epoch 5; Iter    16/   41] train: loss: 0.6949344
[Epoch 5] ogbg-molbace: 0.413919 val loss: 0.694052
[Epoch 5] ogbg-molbace: 0.296470 test loss: 0.701376
[Epoch 6; Iter     5/   41] train: loss: 0.7001895
[Epoch 6; Iter    35/   41] train: loss: 0.6937670
[Epoch 6] ogbg-molbace: 0.415751 val loss: 0.694703
[Epoch 6] ogbg-molbace: 0.305164 test loss: 0.701550
[Epoch 7; Iter    24/   41] train: loss: 0.6938801
[Epoch 7] ogbg-molbace: 0.421245 val loss: 0.694727
[Epoch 7] ogbg-molbace: 0.306034 test loss: 0.701470
[Epoch 8; Iter    13/   41] train: loss: 0.6895895
[Epoch 8] ogbg-molbace: 0.417949 val loss: 0.695629
[Epoch 8] ogbg-molbace: 0.303773 test loss: 0.701951
[Epoch 9; Iter     2/   41] train: loss: 0.6936268
[Epoch 9; Iter    32/   41] train: loss: 0.6979018
[Epoch 9] ogbg-molbace: 0.411355 val loss: 0.696751
[Epoch 9] ogbg-molbace: 0.303425 test loss: 0.702371
[Epoch 10; Iter    21/   41] train: loss: 0.6937490
[Epoch 10] ogbg-molbace: 0.418315 val loss: 0.696607
[Epoch 10] ogbg-molbace: 0.311424 test loss: 0.701906
[Epoch 11; Iter    10/   41] train: loss: 0.6977723
[Epoch 11; Iter    40/   41] train: loss: 0.6906564
[Epoch 11] ogbg-molbace: 0.416117 val loss: 0.698921
[Epoch 11] ogbg-molbace: 0.306034 test loss: 0.702972
[Epoch 12; Iter    29/   41] train: loss: 0.6994846
[Epoch 12] ogbg-molbace: 0.418315 val loss: 0.698949
[Epoch 12] ogbg-molbace: 0.309859 test loss: 0.702597
[Epoch 13; Iter    18/   41] train: loss: 0.6926369
[Epoch 13] ogbg-molbace: 0.430037 val loss: 0.700394
[Epoch 13] ogbg-molbace: 0.303947 test loss: 0.703853
[Epoch 14; Iter     7/   41] train: loss: 0.6933898
[Epoch 14; Iter    37/   41] train: loss: 0.6858670
[Epoch 14] ogbg-molbace: 0.420879 val loss: 0.700719
[Epoch 14] ogbg-molbace: 0.308990 test loss: 0.703597
[Epoch 15; Iter    26/   41] train: loss: 0.6932017
[Epoch 15] ogbg-molbace: 0.430403 val loss: 0.701650
[Epoch 15] ogbg-molbace: 0.310555 test loss: 0.703945
[Epoch 16; Iter    15/   41] train: loss: 0.6866879
[Epoch 16] ogbg-molbace: 0.423077 val loss: 0.703464
[Epoch 16] ogbg-molbace: 0.316641 test loss: 0.704673
[Epoch 17; Iter     4/   41] train: loss: 0.6963089
[Epoch 17; Iter    34/   41] train: loss: 0.6880153
[Epoch 17] ogbg-molbace: 0.547253 val loss: 0.696599
[Epoch 17] ogbg-molbace: 0.487915 test loss: 0.700118
[Epoch 18; Iter    23/   41] train: loss: 0.6781450
[Epoch 18] ogbg-molbace: 0.692674 val loss: 0.724236
[Epoch 18] ogbg-molbace: 0.705964 test loss: 0.720089
[Epoch 19; Iter    12/   41] train: loss: 0.6618362
[Epoch 19] ogbg-molbace: 0.661905 val loss: 0.694188
[Epoch 19] ogbg-molbace: 0.720396 test loss: 0.731829
[Epoch 20; Iter     1/   41] train: loss: 0.6558442
[Epoch 20; Iter    31/   41] train: loss: 0.6196188
[Epoch 20] ogbg-molbace: 0.633700 val loss: 0.628696
[Epoch 20] ogbg-molbace: 0.730308 test loss: 0.691112
[Epoch 21; Iter    20/   41] train: loss: 0.5416435
[Epoch 21] ogbg-molbace: 0.647253 val loss: 0.584625
[Epoch 21] ogbg-molbace: 0.758825 test loss: 0.648618
[Epoch 22; Iter     9/   41] train: loss: 0.6813377
[Epoch 22; Iter    39/   41] train: loss: 0.4918486
[Epoch 22] ogbg-molbace: 0.630769 val loss: 0.684180
[Epoch 22] ogbg-molbace: 0.704225 test loss: 0.850857
[Epoch 23; Iter    28/   41] train: loss: 0.6157664
[Epoch 23] ogbg-molbace: 0.684249 val loss: 0.583374
[Epoch 23] ogbg-molbace: 0.749609 test loss: 0.781089
[Epoch 24; Iter    17/   41] train: loss: 0.7356310
[Epoch 24] ogbg-molbace: 0.665568 val loss: 0.707400
[Epoch 24] ogbg-molbace: 0.738480 test loss: 0.867697
[Epoch 25; Iter     6/   41] train: loss: 0.5108435
[Epoch 25; Iter    36/   41] train: loss: 0.5692053
[Epoch 25] ogbg-molbace: 0.675824 val loss: 0.554235
[Epoch 25] ogbg-molbace: 0.764737 test loss: 0.684003
[Epoch 26; Iter    25/   41] train: loss: 0.6237693
[Epoch 26] ogbg-molbace: 0.692308 val loss: 0.567066
[Epoch 26] ogbg-molbace: 0.747174 test loss: 0.733021
[Epoch 27; Iter    14/   41] train: loss: 0.5134406
[Epoch 27] ogbg-molbace: 0.716850 val loss: 0.584714
[Epoch 27] ogbg-molbace: 0.776561 test loss: 0.714608
[Epoch 28; Iter     3/   41] train: loss: 0.6099362
[Epoch 28; Iter    33/   41] train: loss: 0.5109347
[Epoch 28] ogbg-molbace: 0.678022 val loss: 0.805552
[Epoch 28] ogbg-molbace: 0.737785 test loss: 1.000195
[Epoch 29; Iter    22/   41] train: loss: 0.6681656
[Epoch 29] ogbg-molbace: 0.640659 val loss: 0.560760
[Epoch 29] ogbg-molbace: 0.716571 test loss: 0.805349
[Epoch 30; Iter    11/   41] train: loss: 0.6423541
[Epoch 30; Iter    41/   41] train: loss: 0.4032789
[Epoch 30] ogbg-molbace: 0.707326 val loss: 0.691225
[Epoch 30] ogbg-molbace: 0.769605 test loss: 0.843726
[Epoch 31; Iter    30/   41] train: loss: 0.6025141
[Epoch 31] ogbg-molbace: 0.638095 val loss: 0.793602
[Epoch 31] ogbg-molbace: 0.705269 test loss: 1.020429
[Epoch 32; Iter    19/   41] train: loss: 0.5669554
[Epoch 32] ogbg-molbace: 0.725275 val loss: 0.547840
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/bace/noise=0.05/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.05_6_26-05_10-07-25
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.05
logdir: runs/static_noise/GraphCL/bace/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6966178
[Epoch 1] ogbg-molbace: 0.441758 val loss: 0.691435
[Epoch 1] ogbg-molbace: 0.320118 test loss: 0.692698
[Epoch 2; Iter    19/   41] train: loss: 0.7021371
[Epoch 2] ogbg-molbace: 0.435897 val loss: 0.687140
[Epoch 2] ogbg-molbace: 0.324813 test loss: 0.692706
[Epoch 3; Iter     8/   41] train: loss: 0.6943965
[Epoch 3; Iter    38/   41] train: loss: 0.6978461
[Epoch 3] ogbg-molbace: 0.453846 val loss: 0.688656
[Epoch 3] ogbg-molbace: 0.345505 test loss: 0.694557
[Epoch 4; Iter    27/   41] train: loss: 0.6970946
[Epoch 4] ogbg-molbace: 0.454579 val loss: 0.688275
[Epoch 4] ogbg-molbace: 0.337159 test loss: 0.694300
[Epoch 5; Iter    16/   41] train: loss: 0.6978875
[Epoch 5] ogbg-molbace: 0.453114 val loss: 0.688557
[Epoch 5] ogbg-molbace: 0.346375 test loss: 0.694464
[Epoch 6; Iter     5/   41] train: loss: 0.6975085
[Epoch 6; Iter    35/   41] train: loss: 0.6958655
[Epoch 6] ogbg-molbace: 0.456410 val loss: 0.689773
[Epoch 6] ogbg-molbace: 0.345679 test loss: 0.694879
[Epoch 7; Iter    24/   41] train: loss: 0.7000876
[Epoch 7] ogbg-molbace: 0.456044 val loss: 0.689678
[Epoch 7] ogbg-molbace: 0.338898 test loss: 0.694944
[Epoch 8; Iter    13/   41] train: loss: 0.7020119
[Epoch 8] ogbg-molbace: 0.455678 val loss: 0.690392
[Epoch 8] ogbg-molbace: 0.343071 test loss: 0.695357
[Epoch 9; Iter     2/   41] train: loss: 0.6944592
[Epoch 9; Iter    32/   41] train: loss: 0.6973000
[Epoch 9] ogbg-molbace: 0.456777 val loss: 0.691756
[Epoch 9] ogbg-molbace: 0.353852 test loss: 0.695745
[Epoch 10; Iter    21/   41] train: loss: 0.6973948
[Epoch 10] ogbg-molbace: 0.466667 val loss: 0.691577
[Epoch 10] ogbg-molbace: 0.381325 test loss: 0.695297
[Epoch 11; Iter    10/   41] train: loss: 0.6912754
[Epoch 11; Iter    40/   41] train: loss: 0.6965667
[Epoch 11] ogbg-molbace: 0.467766 val loss: 0.692610
[Epoch 11] ogbg-molbace: 0.366371 test loss: 0.696142
[Epoch 12; Iter    29/   41] train: loss: 0.6950688
[Epoch 12] ogbg-molbace: 0.462637 val loss: 0.694514
[Epoch 12] ogbg-molbace: 0.368632 test loss: 0.697035
[Epoch 13; Iter    18/   41] train: loss: 0.6967564
[Epoch 13] ogbg-molbace: 0.479853 val loss: 0.694384
[Epoch 13] ogbg-molbace: 0.387933 test loss: 0.696665
[Epoch 14; Iter     7/   41] train: loss: 0.6911957
[Epoch 14; Iter    37/   41] train: loss: 0.6902815
[Epoch 14] ogbg-molbace: 0.469231 val loss: 0.694788
[Epoch 14] ogbg-molbace: 0.376630 test loss: 0.696673
[Epoch 15; Iter    26/   41] train: loss: 0.6931930
[Epoch 15] ogbg-molbace: 0.487179 val loss: 0.696174
[Epoch 15] ogbg-molbace: 0.397496 test loss: 0.697233
[Epoch 16; Iter    15/   41] train: loss: 0.6896007
[Epoch 16] ogbg-molbace: 0.493407 val loss: 0.697606
[Epoch 16] ogbg-molbace: 0.415754 test loss: 0.697962
[Epoch 17; Iter     4/   41] train: loss: 0.6878234
[Epoch 17; Iter    34/   41] train: loss: 0.6960523
[Epoch 17] ogbg-molbace: 0.620513 val loss: 0.691163
[Epoch 17] ogbg-molbace: 0.648061 test loss: 0.690738
[Epoch 18; Iter    23/   41] train: loss: 0.6721644
[Epoch 18] ogbg-molbace: 0.663004 val loss: 0.695010
[Epoch 18] ogbg-molbace: 0.753608 test loss: 0.685333
[Epoch 19; Iter    12/   41] train: loss: 0.6544091
[Epoch 19] ogbg-molbace: 0.680952 val loss: 0.662270
[Epoch 19] ogbg-molbace: 0.774822 test loss: 0.676146
[Epoch 20; Iter     1/   41] train: loss: 0.5990274
[Epoch 20; Iter    31/   41] train: loss: 0.5765555
[Epoch 20] ogbg-molbace: 0.680220 val loss: 0.604949
[Epoch 20] ogbg-molbace: 0.763867 test loss: 0.657900
[Epoch 21; Iter    20/   41] train: loss: 0.6070397
[Epoch 21] ogbg-molbace: 0.566667 val loss: 0.852035
[Epoch 21] ogbg-molbace: 0.658842 test loss: 0.933718
[Epoch 22; Iter     9/   41] train: loss: 0.6047428
[Epoch 22; Iter    39/   41] train: loss: 0.4622318
[Epoch 22] ogbg-molbace: 0.679487 val loss: 0.620934
[Epoch 22] ogbg-molbace: 0.772735 test loss: 0.700299
[Epoch 23; Iter    28/   41] train: loss: 0.4870054
[Epoch 23] ogbg-molbace: 0.665201 val loss: 0.951117
[Epoch 23] ogbg-molbace: 0.763172 test loss: 1.047043
[Epoch 24; Iter    17/   41] train: loss: 0.4644448
[Epoch 24] ogbg-molbace: 0.697802 val loss: 0.629252
[Epoch 24] ogbg-molbace: 0.789080 test loss: 0.719180
[Epoch 25; Iter     6/   41] train: loss: 0.5626037
[Epoch 25; Iter    36/   41] train: loss: 0.4039483
[Epoch 25] ogbg-molbace: 0.663736 val loss: 0.768583
[Epoch 25] ogbg-molbace: 0.735003 test loss: 1.007340
[Epoch 26; Iter    25/   41] train: loss: 0.5692403
[Epoch 26] ogbg-molbace: 0.696337 val loss: 0.770885
[Epoch 26] ogbg-molbace: 0.757781 test loss: 0.946541
[Epoch 27; Iter    14/   41] train: loss: 0.5859873
[Epoch 27] ogbg-molbace: 0.693040 val loss: 0.543465
[Epoch 27] ogbg-molbace: 0.776387 test loss: 0.749504
[Epoch 28; Iter     3/   41] train: loss: 0.5995038
[Epoch 28; Iter    33/   41] train: loss: 0.5062551
[Epoch 28] ogbg-molbace: 0.652015 val loss: 0.848747
[Epoch 28] ogbg-molbace: 0.750478 test loss: 1.133587
[Epoch 29; Iter    22/   41] train: loss: 0.5587968
[Epoch 29] ogbg-molbace: 0.679853 val loss: 0.644902
[Epoch 29] ogbg-molbace: 0.783168 test loss: 0.817787
[Epoch 30; Iter    11/   41] train: loss: 0.5466895
[Epoch 30; Iter    41/   41] train: loss: 0.4955885
[Epoch 30] ogbg-molbace: 0.633333 val loss: 0.803231
[Epoch 30] ogbg-molbace: 0.741784 test loss: 1.057217
[Epoch 31; Iter    30/   41] train: loss: 0.4596307
[Epoch 31] ogbg-molbace: 0.612454 val loss: 1.343801
[Epoch 31] ogbg-molbace: 0.737089 test loss: 1.516336
[Epoch 32; Iter    19/   41] train: loss: 0.4838865
[Epoch 32] ogbg-molbace: 0.728571 val loss: 0.806075
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/bace/noise=0.1/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.1_4_26-05_10-07-27
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.1
logdir: runs/static_noise/GraphCL/bace/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6929551
[Epoch 1] ogbg-molbace: 0.400000 val loss: 0.694898
[Epoch 1] ogbg-molbace: 0.299252 test loss: 0.695394
[Epoch 2; Iter    19/   41] train: loss: 0.6934130
[Epoch 2] ogbg-molbace: 0.382051 val loss: 0.696422
[Epoch 2] ogbg-molbace: 0.298383 test loss: 0.699391
[Epoch 3; Iter     8/   41] train: loss: 0.6895469
[Epoch 3; Iter    38/   41] train: loss: 0.6957334
[Epoch 3] ogbg-molbace: 0.386813 val loss: 0.694589
[Epoch 3] ogbg-molbace: 0.299078 test loss: 0.700223
[Epoch 4; Iter    27/   41] train: loss: 0.6940492
[Epoch 4] ogbg-molbace: 0.389744 val loss: 0.695445
[Epoch 4] ogbg-molbace: 0.294557 test loss: 0.700564
[Epoch 5; Iter    16/   41] train: loss: 0.6966285
[Epoch 5] ogbg-molbace: 0.392674 val loss: 0.695266
[Epoch 5] ogbg-molbace: 0.301513 test loss: 0.700448
[Epoch 6; Iter     5/   41] train: loss: 0.6977325
[Epoch 6; Iter    35/   41] train: loss: 0.6929164
[Epoch 6] ogbg-molbace: 0.388278 val loss: 0.695769
[Epoch 6] ogbg-molbace: 0.303078 test loss: 0.700576
[Epoch 7; Iter    24/   41] train: loss: 0.6934001
[Epoch 7] ogbg-molbace: 0.392308 val loss: 0.696365
[Epoch 7] ogbg-molbace: 0.303599 test loss: 0.700848
[Epoch 8; Iter    13/   41] train: loss: 0.6905686
[Epoch 8] ogbg-molbace: 0.390476 val loss: 0.697595
[Epoch 8] ogbg-molbace: 0.297687 test loss: 0.701486
[Epoch 9; Iter     2/   41] train: loss: 0.6943312
[Epoch 9; Iter    32/   41] train: loss: 0.6960752
[Epoch 9] ogbg-molbace: 0.394139 val loss: 0.697823
[Epoch 9] ogbg-molbace: 0.301165 test loss: 0.701162
[Epoch 10; Iter    21/   41] train: loss: 0.6945145
[Epoch 10] ogbg-molbace: 0.393773 val loss: 0.698101
[Epoch 10] ogbg-molbace: 0.304643 test loss: 0.701075
[Epoch 11; Iter    10/   41] train: loss: 0.6962134
[Epoch 11; Iter    40/   41] train: loss: 0.6929615
[Epoch 11] ogbg-molbace: 0.391941 val loss: 0.699465
[Epoch 11] ogbg-molbace: 0.302208 test loss: 0.701619
[Epoch 12; Iter    29/   41] train: loss: 0.6974005
[Epoch 12] ogbg-molbace: 0.402198 val loss: 0.700005
[Epoch 12] ogbg-molbace: 0.302730 test loss: 0.701545
[Epoch 13; Iter    18/   41] train: loss: 0.6962897
[Epoch 13] ogbg-molbace: 0.406227 val loss: 0.701621
[Epoch 13] ogbg-molbace: 0.300122 test loss: 0.702871
[Epoch 14; Iter     7/   41] train: loss: 0.6930054
[Epoch 14; Iter    37/   41] train: loss: 0.6867956
[Epoch 14] ogbg-molbace: 0.399267 val loss: 0.702092
[Epoch 14] ogbg-molbace: 0.301339 test loss: 0.702760
[Epoch 15; Iter    26/   41] train: loss: 0.6934614
[Epoch 15] ogbg-molbace: 0.408059 val loss: 0.703027
[Epoch 15] ogbg-molbace: 0.306729 test loss: 0.703082
[Epoch 16; Iter    15/   41] train: loss: 0.6898048
[Epoch 16] ogbg-molbace: 0.410256 val loss: 0.704119
[Epoch 16] ogbg-molbace: 0.303599 test loss: 0.703862
[Epoch 17; Iter     4/   41] train: loss: 0.6930323
[Epoch 17; Iter    34/   41] train: loss: 0.6865262
[Epoch 17] ogbg-molbace: 0.610623 val loss: 0.691469
[Epoch 17] ogbg-molbace: 0.594853 test loss: 0.700547
[Epoch 18; Iter    23/   41] train: loss: 0.6669387
[Epoch 18] ogbg-molbace: 0.656777 val loss: 0.703323
[Epoch 18] ogbg-molbace: 0.705964 test loss: 0.708046
[Epoch 19; Iter    12/   41] train: loss: 0.6496191
[Epoch 19] ogbg-molbace: 0.590476 val loss: 0.713522
[Epoch 19] ogbg-molbace: 0.679360 test loss: 0.754708
[Epoch 20; Iter     1/   41] train: loss: 0.6141219
[Epoch 20; Iter    31/   41] train: loss: 0.6010569
[Epoch 20] ogbg-molbace: 0.617582 val loss: 0.640301
[Epoch 20] ogbg-molbace: 0.727526 test loss: 0.712498
[Epoch 21; Iter    20/   41] train: loss: 0.5253052
[Epoch 21] ogbg-molbace: 0.633333 val loss: 0.669691
[Epoch 21] ogbg-molbace: 0.740915 test loss: 0.786576
[Epoch 22; Iter     9/   41] train: loss: 0.6504620
[Epoch 22; Iter    39/   41] train: loss: 0.4625349
[Epoch 22] ogbg-molbace: 0.620147 val loss: 0.722702
[Epoch 22] ogbg-molbace: 0.724222 test loss: 0.873584
[Epoch 23; Iter    28/   41] train: loss: 0.5548608
[Epoch 23] ogbg-molbace: 0.641758 val loss: 0.609650
[Epoch 23] ogbg-molbace: 0.721266 test loss: 0.851209
[Epoch 24; Iter    17/   41] train: loss: 0.6852106
[Epoch 24] ogbg-molbace: 0.638828 val loss: 0.865650
[Epoch 24] ogbg-molbace: 0.746479 test loss: 0.961085
[Epoch 25; Iter     6/   41] train: loss: 0.4847573
[Epoch 25; Iter    36/   41] train: loss: 0.5678506
[Epoch 25] ogbg-molbace: 0.645788 val loss: 0.808506
[Epoch 25] ogbg-molbace: 0.742653 test loss: 0.973121
[Epoch 26; Iter    25/   41] train: loss: 0.5369331
[Epoch 26] ogbg-molbace: 0.564835 val loss: 0.866953
[Epoch 26] ogbg-molbace: 0.685446 test loss: 1.040570
[Epoch 27; Iter    14/   41] train: loss: 0.5111030
[Epoch 27] ogbg-molbace: 0.593773 val loss: 1.062786
[Epoch 27] ogbg-molbace: 0.684750 test loss: 1.361914
[Epoch 28; Iter     3/   41] train: loss: 0.4758765
[Epoch 28; Iter    33/   41] train: loss: 0.5504127
[Epoch 28] ogbg-molbace: 0.639927 val loss: 0.997261
[Epoch 28] ogbg-molbace: 0.762476 test loss: 1.210783
[Epoch 29; Iter    22/   41] train: loss: 0.6122648
[Epoch 29] ogbg-molbace: 0.567399 val loss: 0.896892
[Epoch 29] ogbg-molbace: 0.647366 test loss: 1.421002
[Epoch 30; Iter    11/   41] train: loss: 0.5683015
[Epoch 30; Iter    41/   41] train: loss: 0.4976580
[Epoch 30] ogbg-molbace: 0.660073 val loss: 1.021912
[Epoch 30] ogbg-molbace: 0.797948 test loss: 1.089410
[Epoch 31; Iter    30/   41] train: loss: 0.5495533
[Epoch 31] ogbg-molbace: 0.577656 val loss: 1.131109
[Epoch 31] ogbg-molbace: 0.717440 test loss: 1.350913
[Epoch 32; Iter    19/   41] train: loss: 0.5455981
[Epoch 32] ogbg-molbace: 0.684615 val loss: 0.709471
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/bace/noise=0.2/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.2_6_26-05_10-07-28
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.2
logdir: runs/static_noise/GraphCL/bace/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6930896
[Epoch 1] ogbg-molbace: 0.391941 val loss: 0.691148
[Epoch 1] ogbg-molbace: 0.290210 test loss: 0.692868
[Epoch 2; Iter    19/   41] train: loss: 0.6996338
[Epoch 2] ogbg-molbace: 0.416117 val loss: 0.689663
[Epoch 2] ogbg-molbace: 0.278386 test loss: 0.695710
[Epoch 3; Iter     8/   41] train: loss: 0.6973317
[Epoch 3; Iter    38/   41] train: loss: 0.6986034
[Epoch 3] ogbg-molbace: 0.439194 val loss: 0.701271
[Epoch 3] ogbg-molbace: 0.255260 test loss: 0.704444
[Epoch 4; Iter    27/   41] train: loss: 0.6945436
[Epoch 4] ogbg-molbace: 0.439560 val loss: 0.700572
[Epoch 4] ogbg-molbace: 0.246566 test loss: 0.703707
[Epoch 5; Iter    16/   41] train: loss: 0.6983398
[Epoch 5] ogbg-molbace: 0.445055 val loss: 0.698415
[Epoch 5] ogbg-molbace: 0.271431 test loss: 0.702935
[Epoch 6; Iter     5/   41] train: loss: 0.6980006
[Epoch 6; Iter    35/   41] train: loss: 0.6964801
[Epoch 6] ogbg-molbace: 0.438828 val loss: 0.701669
[Epoch 6] ogbg-molbace: 0.251608 test loss: 0.703739
[Epoch 7; Iter    24/   41] train: loss: 0.6929793
[Epoch 7] ogbg-molbace: 0.439927 val loss: 0.704187
[Epoch 7] ogbg-molbace: 0.247087 test loss: 0.705280
[Epoch 8; Iter    13/   41] train: loss: 0.7016120
[Epoch 8] ogbg-molbace: 0.438462 val loss: 0.704902
[Epoch 8] ogbg-molbace: 0.250913 test loss: 0.706039
[Epoch 9; Iter     2/   41] train: loss: 0.6926603
[Epoch 9; Iter    32/   41] train: loss: 0.6950805
[Epoch 9] ogbg-molbace: 0.447619 val loss: 0.703969
[Epoch 9] ogbg-molbace: 0.260129 test loss: 0.705371
[Epoch 10; Iter    21/   41] train: loss: 0.6976501
[Epoch 10] ogbg-molbace: 0.442857 val loss: 0.704038
[Epoch 10] ogbg-molbace: 0.273344 test loss: 0.706191
[Epoch 11; Iter    10/   41] train: loss: 0.6956710
[Epoch 11; Iter    40/   41] train: loss: 0.6960821
[Epoch 11] ogbg-molbace: 0.443223 val loss: 0.707545
[Epoch 11] ogbg-molbace: 0.259085 test loss: 0.707590
[Epoch 12; Iter    29/   41] train: loss: 0.6937546
[Epoch 12] ogbg-molbace: 0.442125 val loss: 0.705677
[Epoch 12] ogbg-molbace: 0.251956 test loss: 0.706108
[Epoch 13; Iter    18/   41] train: loss: 0.6958023
[Epoch 13] ogbg-molbace: 0.446886 val loss: 0.711974
[Epoch 13] ogbg-molbace: 0.264997 test loss: 0.709742
[Epoch 14; Iter     7/   41] train: loss: 0.6898123
[Epoch 14; Iter    37/   41] train: loss: 0.6952744
[Epoch 14] ogbg-molbace: 0.446154 val loss: 0.711614
[Epoch 14] ogbg-molbace: 0.266910 test loss: 0.709451
[Epoch 15; Iter    26/   41] train: loss: 0.6931974
[Epoch 15] ogbg-molbace: 0.453846 val loss: 0.709498
[Epoch 15] ogbg-molbace: 0.272822 test loss: 0.708478
[Epoch 16; Iter    15/   41] train: loss: 0.6877595
[Epoch 16] ogbg-molbace: 0.451648 val loss: 0.714790
[Epoch 16] ogbg-molbace: 0.277691 test loss: 0.711414
[Epoch 17; Iter     4/   41] train: loss: 0.6883582
[Epoch 17; Iter    34/   41] train: loss: 0.6962225
[Epoch 17] ogbg-molbace: 0.544322 val loss: 0.709188
[Epoch 17] ogbg-molbace: 0.436446 test loss: 0.703035
[Epoch 18; Iter    23/   41] train: loss: 0.6699808
[Epoch 18] ogbg-molbace: 0.655311 val loss: 0.714523
[Epoch 18] ogbg-molbace: 0.723352 test loss: 0.693832
[Epoch 19; Iter    12/   41] train: loss: 0.6621693
[Epoch 19] ogbg-molbace: 0.640659 val loss: 0.705029
[Epoch 19] ogbg-molbace: 0.721266 test loss: 0.717996
[Epoch 20; Iter     1/   41] train: loss: 0.6356825
[Epoch 20; Iter    31/   41] train: loss: 0.5908708
[Epoch 20] ogbg-molbace: 0.568498 val loss: 0.631117
[Epoch 20] ogbg-molbace: 0.663363 test loss: 0.713016
[Epoch 21; Iter    20/   41] train: loss: 0.6477140
[Epoch 21] ogbg-molbace: 0.586813 val loss: 1.070846
[Epoch 21] ogbg-molbace: 0.696575 test loss: 1.027072
[Epoch 22; Iter     9/   41] train: loss: 0.6398751
[Epoch 22; Iter    39/   41] train: loss: 0.5270509
[Epoch 22] ogbg-molbace: 0.576557 val loss: 0.903620
[Epoch 22] ogbg-molbace: 0.671709 test loss: 0.990059
[Epoch 23; Iter    28/   41] train: loss: 0.5497823
[Epoch 23] ogbg-molbace: 0.656410 val loss: 0.707486
[Epoch 23] ogbg-molbace: 0.738132 test loss: 0.854880
[Epoch 24; Iter    17/   41] train: loss: 0.5108263
[Epoch 24] ogbg-molbace: 0.661538 val loss: 0.753033
[Epoch 24] ogbg-molbace: 0.802469 test loss: 0.774179
[Epoch 25; Iter     6/   41] train: loss: 0.6034600
[Epoch 25; Iter    36/   41] train: loss: 0.4920201
[Epoch 25] ogbg-molbace: 0.634799 val loss: 1.237435
[Epoch 25] ogbg-molbace: 0.754130 test loss: 1.294650
[Epoch 26; Iter    25/   41] train: loss: 0.6260768
[Epoch 26] ogbg-molbace: 0.579121 val loss: 1.019719
[Epoch 26] ogbg-molbace: 0.695705 test loss: 1.159597
[Epoch 27; Iter    14/   41] train: loss: 0.5965461
[Epoch 27] ogbg-molbace: 0.583150 val loss: 0.645368
[Epoch 27] ogbg-molbace: 0.731699 test loss: 0.852973
[Epoch 28; Iter     3/   41] train: loss: 0.5918822
[Epoch 28; Iter    33/   41] train: loss: 0.5349786
[Epoch 28] ogbg-molbace: 0.510989 val loss: 1.133499
[Epoch 28] ogbg-molbace: 0.603721 test loss: 1.354092
[Epoch 29; Iter    22/   41] train: loss: 0.5329726
[Epoch 29] ogbg-molbace: 0.583516 val loss: 0.795247
[Epoch 29] ogbg-molbace: 0.669797 test loss: 1.042830
[Epoch 30; Iter    11/   41] train: loss: 0.6711439
[Epoch 30; Iter    41/   41] train: loss: 0.7147400
[Epoch 30] ogbg-molbace: 0.540293 val loss: 1.132273
[Epoch 30] ogbg-molbace: 0.635194 test loss: 1.333518
[Epoch 31; Iter    30/   41] train: loss: 0.5780333
[Epoch 31] ogbg-molbace: 0.579487 val loss: 1.867639
[Epoch 31] ogbg-molbace: 0.665797 test loss: 1.825082
[Epoch 32; Iter    19/   41] train: loss: 0.5440327
[Epoch 32] ogbg-molbace: 0.608791 val loss: 1.044660
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/bace/noise=0.1/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.1_6_26-05_10-07-27
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.1
logdir: runs/static_noise/GraphCL/bace/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6963573
[Epoch 1] ogbg-molbace: 0.430037 val loss: 0.691170
[Epoch 1] ogbg-molbace: 0.312120 test loss: 0.692729
[Epoch 2; Iter    19/   41] train: loss: 0.7010972
[Epoch 2] ogbg-molbace: 0.423810 val loss: 0.687366
[Epoch 2] ogbg-molbace: 0.307773 test loss: 0.693624
[Epoch 3; Iter     8/   41] train: loss: 0.6921756
[Epoch 3; Iter    38/   41] train: loss: 0.6997860
[Epoch 3] ogbg-molbace: 0.450549 val loss: 0.692426
[Epoch 3] ogbg-molbace: 0.318032 test loss: 0.697934
[Epoch 4; Iter    27/   41] train: loss: 0.6984392
[Epoch 4] ogbg-molbace: 0.452015 val loss: 0.692544
[Epoch 4] ogbg-molbace: 0.313511 test loss: 0.697478
[Epoch 5; Iter    16/   41] train: loss: 0.6976532
[Epoch 5] ogbg-molbace: 0.453846 val loss: 0.692401
[Epoch 5] ogbg-molbace: 0.324118 test loss: 0.697759
[Epoch 6; Iter     5/   41] train: loss: 0.6951097
[Epoch 6; Iter    35/   41] train: loss: 0.6963459
[Epoch 6] ogbg-molbace: 0.450183 val loss: 0.693450
[Epoch 6] ogbg-molbace: 0.303252 test loss: 0.697598
[Epoch 7; Iter    24/   41] train: loss: 0.6979109
[Epoch 7] ogbg-molbace: 0.451648 val loss: 0.695148
[Epoch 7] ogbg-molbace: 0.304295 test loss: 0.698895
[Epoch 8; Iter    13/   41] train: loss: 0.7020288
[Epoch 8] ogbg-molbace: 0.445055 val loss: 0.695719
[Epoch 8] ogbg-molbace: 0.312120 test loss: 0.699138
[Epoch 9; Iter     2/   41] train: loss: 0.6930954
[Epoch 9; Iter    32/   41] train: loss: 0.6941767
[Epoch 9] ogbg-molbace: 0.454579 val loss: 0.696135
[Epoch 9] ogbg-molbace: 0.306034 test loss: 0.699199
[Epoch 10; Iter    21/   41] train: loss: 0.6977822
[Epoch 10] ogbg-molbace: 0.459707 val loss: 0.696619
[Epoch 10] ogbg-molbace: 0.333333 test loss: 0.699625
[Epoch 11; Iter    10/   41] train: loss: 0.6898083
[Epoch 11; Iter    40/   41] train: loss: 0.6967794
[Epoch 11] ogbg-molbace: 0.454212 val loss: 0.697358
[Epoch 11] ogbg-molbace: 0.302382 test loss: 0.699531
[Epoch 12; Iter    29/   41] train: loss: 0.6943422
[Epoch 12] ogbg-molbace: 0.461905 val loss: 0.698714
[Epoch 12] ogbg-molbace: 0.303773 test loss: 0.700224
[Epoch 13; Iter    18/   41] train: loss: 0.6958750
[Epoch 13] ogbg-molbace: 0.467766 val loss: 0.699997
[Epoch 13] ogbg-molbace: 0.331768 test loss: 0.700968
[Epoch 14; Iter     7/   41] train: loss: 0.6922327
[Epoch 14; Iter    37/   41] train: loss: 0.6912004
[Epoch 14] ogbg-molbace: 0.468864 val loss: 0.700089
[Epoch 14] ogbg-molbace: 0.329856 test loss: 0.700670
[Epoch 15; Iter    26/   41] train: loss: 0.6927072
[Epoch 15] ogbg-molbace: 0.472527 val loss: 0.702260
[Epoch 15] ogbg-molbace: 0.346896 test loss: 0.701754
[Epoch 16; Iter    15/   41] train: loss: 0.6886687
[Epoch 16] ogbg-molbace: 0.472161 val loss: 0.705108
[Epoch 16] ogbg-molbace: 0.356460 test loss: 0.703199
[Epoch 17; Iter     4/   41] train: loss: 0.6878332
[Epoch 17; Iter    34/   41] train: loss: 0.6941730
[Epoch 17] ogbg-molbace: 0.571062 val loss: 0.695542
[Epoch 17] ogbg-molbace: 0.601461 test loss: 0.693015
[Epoch 18; Iter    23/   41] train: loss: 0.6736702
[Epoch 18] ogbg-molbace: 0.667766 val loss: 0.699974
[Epoch 18] ogbg-molbace: 0.749957 test loss: 0.686891
[Epoch 19; Iter    12/   41] train: loss: 0.6505084
[Epoch 19] ogbg-molbace: 0.623077 val loss: 0.625507
[Epoch 19] ogbg-molbace: 0.750826 test loss: 0.675827
[Epoch 20; Iter     1/   41] train: loss: 0.6227164
[Epoch 20; Iter    31/   41] train: loss: 0.5537660
[Epoch 20] ogbg-molbace: 0.603663 val loss: 0.651858
[Epoch 20] ogbg-molbace: 0.737959 test loss: 0.674461
[Epoch 21; Iter    20/   41] train: loss: 0.6305469
[Epoch 21] ogbg-molbace: 0.553480 val loss: 0.883568
[Epoch 21] ogbg-molbace: 0.688750 test loss: 0.926769
[Epoch 22; Iter     9/   41] train: loss: 0.5770209
[Epoch 22; Iter    39/   41] train: loss: 0.4997432
[Epoch 22] ogbg-molbace: 0.605495 val loss: 0.687607
[Epoch 22] ogbg-molbace: 0.738654 test loss: 0.798270
[Epoch 23; Iter    28/   41] train: loss: 0.4707964
[Epoch 23] ogbg-molbace: 0.667399 val loss: 0.973834
[Epoch 23] ogbg-molbace: 0.739871 test loss: 1.320729
[Epoch 24; Iter    17/   41] train: loss: 0.4337793
[Epoch 24] ogbg-molbace: 0.639927 val loss: 0.818628
[Epoch 24] ogbg-molbace: 0.759520 test loss: 1.588989
[Epoch 25; Iter     6/   41] train: loss: 0.5493577
[Epoch 25; Iter    36/   41] train: loss: 0.4454675
[Epoch 25] ogbg-molbace: 0.632234 val loss: 1.054180
[Epoch 25] ogbg-molbace: 0.685272 test loss: 2.226844
[Epoch 26; Iter    25/   41] train: loss: 0.5199613
[Epoch 26] ogbg-molbace: 0.631136 val loss: 0.929767
[Epoch 26] ogbg-molbace: 0.687880 test loss: 2.104500
[Epoch 27; Iter    14/   41] train: loss: 0.5609655
[Epoch 27] ogbg-molbace: 0.694139 val loss: 0.557723
[Epoch 27] ogbg-molbace: 0.767345 test loss: 1.615209
[Epoch 28; Iter     3/   41] train: loss: 0.6235068
[Epoch 28; Iter    33/   41] train: loss: 0.4344929
[Epoch 28] ogbg-molbace: 0.557875 val loss: 1.043230
[Epoch 28] ogbg-molbace: 0.638845 test loss: 2.420369
[Epoch 29; Iter    22/   41] train: loss: 0.5988513
[Epoch 29] ogbg-molbace: 0.615751 val loss: 0.833820
[Epoch 29] ogbg-molbace: 0.716745 test loss: 1.960010
[Epoch 30; Iter    11/   41] train: loss: 0.6135797
[Epoch 30; Iter    41/   41] train: loss: 0.7092318
[Epoch 30] ogbg-molbace: 0.607326 val loss: 1.001866
[Epoch 30] ogbg-molbace: 0.710659 test loss: 2.136033
[Epoch 31; Iter    30/   41] train: loss: 0.5085709
[Epoch 31] ogbg-molbace: 0.538828 val loss: 1.712738
[Epoch 31] ogbg-molbace: 0.611024 test loss: 2.746701
[Epoch 32; Iter    19/   41] train: loss: 0.5132341
[Epoch 32] ogbg-molbace: 0.630037 val loss: 0.940995
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/bace/noise=0.05/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.05_5_26-05_10-07-25
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.05
logdir: runs/static_noise/GraphCL/bace/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6949780
[Epoch 1] ogbg-molbace: 0.432234 val loss: 0.692136
[Epoch 1] ogbg-molbace: 0.483742 test loss: 0.693321
[Epoch 2; Iter    19/   41] train: loss: 0.6956625
[Epoch 2] ogbg-molbace: 0.422711 val loss: 0.690396
[Epoch 2] ogbg-molbace: 0.542167 test loss: 0.692034
[Epoch 3; Iter     8/   41] train: loss: 0.6947325
[Epoch 3; Iter    38/   41] train: loss: 0.6971090
[Epoch 3] ogbg-molbace: 0.427839 val loss: 0.691508
[Epoch 3] ogbg-molbace: 0.517301 test loss: 0.691535
[Epoch 4; Iter    27/   41] train: loss: 0.6930040
[Epoch 4] ogbg-molbace: 0.420513 val loss: 0.692247
[Epoch 4] ogbg-molbace: 0.520083 test loss: 0.691418
[Epoch 5; Iter    16/   41] train: loss: 0.6932056
[Epoch 5] ogbg-molbace: 0.429670 val loss: 0.693404
[Epoch 5] ogbg-molbace: 0.518692 test loss: 0.692482
[Epoch 6; Iter     5/   41] train: loss: 0.6946265
[Epoch 6; Iter    35/   41] train: loss: 0.6984015
[Epoch 6] ogbg-molbace: 0.425275 val loss: 0.692792
[Epoch 6] ogbg-molbace: 0.524257 test loss: 0.691757
[Epoch 7; Iter    24/   41] train: loss: 0.6951135
[Epoch 7] ogbg-molbace: 0.432601 val loss: 0.693597
[Epoch 7] ogbg-molbace: 0.525300 test loss: 0.692502
[Epoch 8; Iter    13/   41] train: loss: 0.6916364
[Epoch 8] ogbg-molbace: 0.427839 val loss: 0.694533
[Epoch 8] ogbg-molbace: 0.524257 test loss: 0.692887
[Epoch 9; Iter     2/   41] train: loss: 0.6942556
[Epoch 9; Iter    32/   41] train: loss: 0.6907576
[Epoch 9] ogbg-molbace: 0.427839 val loss: 0.694987
[Epoch 9] ogbg-molbace: 0.540428 test loss: 0.692627
[Epoch 10; Iter    21/   41] train: loss: 0.6919522
[Epoch 10] ogbg-molbace: 0.433700 val loss: 0.696203
[Epoch 10] ogbg-molbace: 0.542167 test loss: 0.692869
[Epoch 11; Iter    10/   41] train: loss: 0.6940238
[Epoch 11; Iter    40/   41] train: loss: 0.6945961
[Epoch 11] ogbg-molbace: 0.436264 val loss: 0.697516
[Epoch 11] ogbg-molbace: 0.538341 test loss: 0.694157
[Epoch 12; Iter    29/   41] train: loss: 0.6928762
[Epoch 12] ogbg-molbace: 0.441392 val loss: 0.697424
[Epoch 12] ogbg-molbace: 0.552600 test loss: 0.693291
[Epoch 13; Iter    18/   41] train: loss: 0.6951259
[Epoch 13] ogbg-molbace: 0.445788 val loss: 0.698627
[Epoch 13] ogbg-molbace: 0.557121 test loss: 0.693993
[Epoch 14; Iter     7/   41] train: loss: 0.6904895
[Epoch 14; Iter    37/   41] train: loss: 0.6878769
[Epoch 14] ogbg-molbace: 0.447985 val loss: 0.699856
[Epoch 14] ogbg-molbace: 0.575552 test loss: 0.694029
[Epoch 15; Iter    26/   41] train: loss: 0.6937315
[Epoch 15] ogbg-molbace: 0.444322 val loss: 0.700944
[Epoch 15] ogbg-molbace: 0.567727 test loss: 0.695043
[Epoch 16; Iter    15/   41] train: loss: 0.6858312
[Epoch 16] ogbg-molbace: 0.459707 val loss: 0.702531
[Epoch 16] ogbg-molbace: 0.587550 test loss: 0.695314
[Epoch 17; Iter     4/   41] train: loss: 0.6921248
[Epoch 17; Iter    34/   41] train: loss: 0.6955411
[Epoch 17] ogbg-molbace: 0.576190 val loss: 0.699725
[Epoch 17] ogbg-molbace: 0.681273 test loss: 0.692620
[Epoch 18; Iter    23/   41] train: loss: 0.6714295
[Epoch 18] ogbg-molbace: 0.687179 val loss: 0.697119
[Epoch 18] ogbg-molbace: 0.765258 test loss: 0.694729
[Epoch 19; Iter    12/   41] train: loss: 0.6426921
[Epoch 19] ogbg-molbace: 0.636996 val loss: 0.689138
[Epoch 19] ogbg-molbace: 0.732916 test loss: 0.710007
[Epoch 20; Iter     1/   41] train: loss: 0.6104389
[Epoch 20; Iter    31/   41] train: loss: 0.5548922
[Epoch 20] ogbg-molbace: 0.656410 val loss: 0.676345
[Epoch 20] ogbg-molbace: 0.763693 test loss: 0.741601
[Epoch 21; Iter    20/   41] train: loss: 0.5552587
[Epoch 21] ogbg-molbace: 0.677656 val loss: 0.786901
[Epoch 21] ogbg-molbace: 0.764041 test loss: 0.797164
[Epoch 22; Iter     9/   41] train: loss: 0.4688529
[Epoch 22; Iter    39/   41] train: loss: 0.4792995
[Epoch 22] ogbg-molbace: 0.650916 val loss: 0.746458
[Epoch 22] ogbg-molbace: 0.764910 test loss: 0.787646
[Epoch 23; Iter    28/   41] train: loss: 0.5243523
[Epoch 23] ogbg-molbace: 0.691575 val loss: 0.759239
[Epoch 23] ogbg-molbace: 0.801078 test loss: 0.810164
[Epoch 24; Iter    17/   41] train: loss: 0.5218978
[Epoch 24] ogbg-molbace: 0.636264 val loss: 0.872069
[Epoch 24] ogbg-molbace: 0.742306 test loss: 1.014627
[Epoch 25; Iter     6/   41] train: loss: 0.5523633
[Epoch 25; Iter    36/   41] train: loss: 0.5382293
[Epoch 25] ogbg-molbace: 0.642491 val loss: 0.683392
[Epoch 25] ogbg-molbace: 0.772561 test loss: 0.840733
[Epoch 26; Iter    25/   41] train: loss: 0.4707695
[Epoch 26] ogbg-molbace: 0.673626 val loss: 0.628050
[Epoch 26] ogbg-molbace: 0.795166 test loss: 0.763770
[Epoch 27; Iter    14/   41] train: loss: 0.5207596
[Epoch 27] ogbg-molbace: 0.687912 val loss: 0.643137
[Epoch 27] ogbg-molbace: 0.772214 test loss: 0.856356
[Epoch 28; Iter     3/   41] train: loss: 0.4704569
[Epoch 28; Iter    33/   41] train: loss: 0.4940917
[Epoch 28] ogbg-molbace: 0.646886 val loss: 0.887208
[Epoch 28] ogbg-molbace: 0.788037 test loss: 1.035210
[Epoch 29; Iter    22/   41] train: loss: 0.3998007
[Epoch 29] ogbg-molbace: 0.693407 val loss: 0.934743
[Epoch 29] ogbg-molbace: 0.805251 test loss: 1.033235
[Epoch 30; Iter    11/   41] train: loss: 0.5497605
[Epoch 30; Iter    41/   41] train: loss: 0.5426471
[Epoch 30] ogbg-molbace: 0.689377 val loss: 0.753050
[Epoch 30] ogbg-molbace: 0.790123 test loss: 0.832083
[Epoch 31; Iter    30/   41] train: loss: 0.4880379
[Epoch 31] ogbg-molbace: 0.693407 val loss: 0.913929
[Epoch 31] ogbg-molbace: 0.798991 test loss: 1.007909
[Epoch 32; Iter    19/   41] train: loss: 0.7249597
[Epoch 32] ogbg-molbace: 0.645788 val loss: 0.762744
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/bace/noise=0.05/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.05_4_26-05_10-07-25
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.05
logdir: runs/static_noise/GraphCL/bace/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6945302
[Epoch 1] ogbg-molbace: 0.399634 val loss: 0.694591
[Epoch 1] ogbg-molbace: 0.317336 test loss: 0.695124
[Epoch 2; Iter    19/   41] train: loss: 0.6938980
[Epoch 2] ogbg-molbace: 0.370330 val loss: 0.696426
[Epoch 2] ogbg-molbace: 0.306903 test loss: 0.698766
[Epoch 3; Iter     8/   41] train: loss: 0.6891720
[Epoch 3; Iter    38/   41] train: loss: 0.6951566
[Epoch 3] ogbg-molbace: 0.365201 val loss: 0.694960
[Epoch 3] ogbg-molbace: 0.304469 test loss: 0.699336
[Epoch 4; Iter    27/   41] train: loss: 0.6986001
[Epoch 4] ogbg-molbace: 0.371062 val loss: 0.695531
[Epoch 4] ogbg-molbace: 0.302382 test loss: 0.699538
[Epoch 5; Iter    16/   41] train: loss: 0.6956629
[Epoch 5] ogbg-molbace: 0.378755 val loss: 0.695847
[Epoch 5] ogbg-molbace: 0.308990 test loss: 0.699390
[Epoch 6; Iter     5/   41] train: loss: 0.6990639
[Epoch 6; Iter    35/   41] train: loss: 0.6935178
[Epoch 6] ogbg-molbace: 0.368132 val loss: 0.695799
[Epoch 6] ogbg-molbace: 0.309338 test loss: 0.699670
[Epoch 7; Iter    24/   41] train: loss: 0.6950009
[Epoch 7] ogbg-molbace: 0.377289 val loss: 0.696619
[Epoch 7] ogbg-molbace: 0.309859 test loss: 0.699801
[Epoch 8; Iter    13/   41] train: loss: 0.6912615
[Epoch 8] ogbg-molbace: 0.369231 val loss: 0.697247
[Epoch 8] ogbg-molbace: 0.304643 test loss: 0.700508
[Epoch 9; Iter     2/   41] train: loss: 0.6972011
[Epoch 9; Iter    32/   41] train: loss: 0.6971704
[Epoch 9] ogbg-molbace: 0.370696 val loss: 0.697761
[Epoch 9] ogbg-molbace: 0.313858 test loss: 0.700035
[Epoch 10; Iter    21/   41] train: loss: 0.6959769
[Epoch 10] ogbg-molbace: 0.373626 val loss: 0.697940
[Epoch 10] ogbg-molbace: 0.311076 test loss: 0.700148
[Epoch 11; Iter    10/   41] train: loss: 0.6987960
[Epoch 11; Iter    40/   41] train: loss: 0.6912299
[Epoch 11] ogbg-molbace: 0.365201 val loss: 0.699451
[Epoch 11] ogbg-molbace: 0.318379 test loss: 0.700415
[Epoch 12; Iter    29/   41] train: loss: 0.6978691
[Epoch 12] ogbg-molbace: 0.380952 val loss: 0.700080
[Epoch 12] ogbg-molbace: 0.310381 test loss: 0.700672
[Epoch 13; Iter    18/   41] train: loss: 0.6949490
[Epoch 13] ogbg-molbace: 0.390842 val loss: 0.701218
[Epoch 13] ogbg-molbace: 0.310207 test loss: 0.701383
[Epoch 14; Iter     7/   41] train: loss: 0.6964052
[Epoch 14; Iter    37/   41] train: loss: 0.6893693
[Epoch 14] ogbg-molbace: 0.381319 val loss: 0.701814
[Epoch 14] ogbg-molbace: 0.314728 test loss: 0.701538
[Epoch 15; Iter    26/   41] train: loss: 0.6925291
[Epoch 15] ogbg-molbace: 0.389744 val loss: 0.703128
[Epoch 15] ogbg-molbace: 0.314902 test loss: 0.701935
[Epoch 16; Iter    15/   41] train: loss: 0.6899304
[Epoch 16] ogbg-molbace: 0.388645 val loss: 0.704244
[Epoch 16] ogbg-molbace: 0.314380 test loss: 0.702530
[Epoch 17; Iter     4/   41] train: loss: 0.6939102
[Epoch 17; Iter    34/   41] train: loss: 0.6868020
[Epoch 17] ogbg-molbace: 0.699267 val loss: 0.686487
[Epoch 17] ogbg-molbace: 0.728047 test loss: 0.690478
[Epoch 18; Iter    23/   41] train: loss: 0.6637917
[Epoch 18] ogbg-molbace: 0.687546 val loss: 0.689946
[Epoch 18] ogbg-molbace: 0.748739 test loss: 0.696660
[Epoch 19; Iter    12/   41] train: loss: 0.6360679
[Epoch 19] ogbg-molbace: 0.671795 val loss: 0.663819
[Epoch 19] ogbg-molbace: 0.753608 test loss: 0.703370
[Epoch 20; Iter     1/   41] train: loss: 0.6187137
[Epoch 20; Iter    31/   41] train: loss: 0.5568945
[Epoch 20] ogbg-molbace: 0.661905 val loss: 0.593683
[Epoch 20] ogbg-molbace: 0.769605 test loss: 0.614760
[Epoch 21; Iter    20/   41] train: loss: 0.4866785
[Epoch 21] ogbg-molbace: 0.638828 val loss: 0.651151
[Epoch 21] ogbg-molbace: 0.751869 test loss: 0.795717
[Epoch 22; Iter     9/   41] train: loss: 0.6165382
[Epoch 22; Iter    39/   41] train: loss: 0.4377517
[Epoch 22] ogbg-molbace: 0.673626 val loss: 0.693600
[Epoch 22] ogbg-molbace: 0.764389 test loss: 0.828646
[Epoch 23; Iter    28/   41] train: loss: 0.5318801
[Epoch 23] ogbg-molbace: 0.715751 val loss: 0.542272
[Epoch 23] ogbg-molbace: 0.789950 test loss: 0.681881
[Epoch 24; Iter    17/   41] train: loss: 0.5897359
[Epoch 24] ogbg-molbace: 0.693040 val loss: 0.648491
[Epoch 24] ogbg-molbace: 0.779517 test loss: 0.772875
[Epoch 25; Iter     6/   41] train: loss: 0.4243112
[Epoch 25; Iter    36/   41] train: loss: 0.5087332
[Epoch 25] ogbg-molbace: 0.683150 val loss: 0.650698
[Epoch 25] ogbg-molbace: 0.768040 test loss: 0.863641
[Epoch 26; Iter    25/   41] train: loss: 0.5258979
[Epoch 26] ogbg-molbace: 0.601099 val loss: 0.699141
[Epoch 26] ogbg-molbace: 0.684577 test loss: 0.845001
[Epoch 27; Iter    14/   41] train: loss: 0.4665464
[Epoch 27] ogbg-molbace: 0.638095 val loss: 0.875656
[Epoch 27] ogbg-molbace: 0.722831 test loss: 1.106347
[Epoch 28; Iter     3/   41] train: loss: 0.5788443
[Epoch 28; Iter    33/   41] train: loss: 0.4902788
[Epoch 28] ogbg-molbace: 0.691575 val loss: 0.755694
[Epoch 28] ogbg-molbace: 0.758129 test loss: 0.967333
[Epoch 29; Iter    22/   41] train: loss: 0.5557589
[Epoch 29] ogbg-molbace: 0.636264 val loss: 0.887485
[Epoch 29] ogbg-molbace: 0.725961 test loss: 1.226239
[Epoch 30; Iter    11/   41] train: loss: 0.5579953
[Epoch 30; Iter    41/   41] train: loss: 0.4335810
[Epoch 30] ogbg-molbace: 0.680220 val loss: 0.866425
[Epoch 30] ogbg-molbace: 0.768388 test loss: 0.977749
[Epoch 31; Iter    30/   41] train: loss: 0.5014572
[Epoch 31] ogbg-molbace: 0.630403 val loss: 1.034036
[Epoch 31] ogbg-molbace: 0.725265 test loss: 1.245913
[Epoch 32; Iter    19/   41] train: loss: 0.4974005
[Epoch 32] ogbg-molbace: 0.726374 val loss: 0.667755
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/bace/noise=0.1/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.1_5_26-05_10-07-28
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.1
logdir: runs/static_noise/GraphCL/bace/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6934643
[Epoch 1] ogbg-molbace: 0.438095 val loss: 0.691816
[Epoch 1] ogbg-molbace: 0.482351 test loss: 0.693160
[Epoch 2; Iter    19/   41] train: loss: 0.6958437
[Epoch 2] ogbg-molbace: 0.423443 val loss: 0.688532
[Epoch 2] ogbg-molbace: 0.508955 test loss: 0.691763
[Epoch 3; Iter     8/   41] train: loss: 0.6946903
[Epoch 3; Iter    38/   41] train: loss: 0.6952707
[Epoch 3] ogbg-molbace: 0.419048 val loss: 0.688787
[Epoch 3] ogbg-molbace: 0.419058 test loss: 0.694657
[Epoch 4; Iter    27/   41] train: loss: 0.6931023
[Epoch 4] ogbg-molbace: 0.414286 val loss: 0.690879
[Epoch 4] ogbg-molbace: 0.417840 test loss: 0.695388
[Epoch 5; Iter    16/   41] train: loss: 0.6937793
[Epoch 5] ogbg-molbace: 0.410256 val loss: 0.692134
[Epoch 5] ogbg-molbace: 0.402713 test loss: 0.697361
[Epoch 6; Iter     5/   41] train: loss: 0.6952078
[Epoch 6; Iter    35/   41] train: loss: 0.6975999
[Epoch 6] ogbg-molbace: 0.416850 val loss: 0.690334
[Epoch 6] ogbg-molbace: 0.418884 test loss: 0.695438
[Epoch 7; Iter    24/   41] train: loss: 0.6929535
[Epoch 7] ogbg-molbace: 0.418315 val loss: 0.690584
[Epoch 7] ogbg-molbace: 0.423231 test loss: 0.695194
[Epoch 8; Iter    13/   41] train: loss: 0.6912565
[Epoch 8] ogbg-molbace: 0.416484 val loss: 0.691036
[Epoch 8] ogbg-molbace: 0.418536 test loss: 0.695735
[Epoch 9; Iter     2/   41] train: loss: 0.6917976
[Epoch 9; Iter    32/   41] train: loss: 0.6921201
[Epoch 9] ogbg-molbace: 0.417582 val loss: 0.692271
[Epoch 9] ogbg-molbace: 0.426535 test loss: 0.695716
[Epoch 10; Iter    21/   41] train: loss: 0.6921803
[Epoch 10] ogbg-molbace: 0.420513 val loss: 0.692190
[Epoch 10] ogbg-molbace: 0.429143 test loss: 0.695578
[Epoch 11; Iter    10/   41] train: loss: 0.6939309
[Epoch 11; Iter    40/   41] train: loss: 0.6925529
[Epoch 11] ogbg-molbace: 0.414652 val loss: 0.694888
[Epoch 11] ogbg-molbace: 0.407407 test loss: 0.697957
[Epoch 12; Iter    29/   41] train: loss: 0.6922657
[Epoch 12] ogbg-molbace: 0.419414 val loss: 0.695027
[Epoch 12] ogbg-molbace: 0.424100 test loss: 0.697615
[Epoch 13; Iter    18/   41] train: loss: 0.6946513
[Epoch 13] ogbg-molbace: 0.428571 val loss: 0.691206
[Epoch 13] ogbg-molbace: 0.468266 test loss: 0.693731
[Epoch 14; Iter     7/   41] train: loss: 0.6904634
[Epoch 14; Iter    37/   41] train: loss: 0.6885330
[Epoch 14] ogbg-molbace: 0.419048 val loss: 0.697196
[Epoch 14] ogbg-molbace: 0.438185 test loss: 0.697537
[Epoch 15; Iter    26/   41] train: loss: 0.6916651
[Epoch 15] ogbg-molbace: 0.419048 val loss: 0.698733
[Epoch 15] ogbg-molbace: 0.426013 test loss: 0.698934
[Epoch 16; Iter    15/   41] train: loss: 0.6863034
[Epoch 16] ogbg-molbace: 0.413919 val loss: 0.702634
[Epoch 16] ogbg-molbace: 0.426187 test loss: 0.700682
[Epoch 17; Iter     4/   41] train: loss: 0.6906871
[Epoch 17; Iter    34/   41] train: loss: 0.6940988
[Epoch 17] ogbg-molbace: 0.471795 val loss: 0.698014
[Epoch 17] ogbg-molbace: 0.621805 test loss: 0.694858
[Epoch 18; Iter    23/   41] train: loss: 0.6740961
[Epoch 18] ogbg-molbace: 0.675458 val loss: 0.698073
[Epoch 18] ogbg-molbace: 0.773431 test loss: 0.691525
[Epoch 19; Iter    12/   41] train: loss: 0.6432603
[Epoch 19] ogbg-molbace: 0.661172 val loss: 0.649992
[Epoch 19] ogbg-molbace: 0.760911 test loss: 0.675068
[Epoch 20; Iter     1/   41] train: loss: 0.6215452
[Epoch 20; Iter    31/   41] train: loss: 0.5894226
[Epoch 20] ogbg-molbace: 0.668864 val loss: 0.626973
[Epoch 20] ogbg-molbace: 0.774996 test loss: 0.669603
[Epoch 21; Iter    20/   41] train: loss: 0.5930267
[Epoch 21] ogbg-molbace: 0.672161 val loss: 0.745503
[Epoch 21] ogbg-molbace: 0.770649 test loss: 0.749020
[Epoch 22; Iter     9/   41] train: loss: 0.5102376
[Epoch 22; Iter    39/   41] train: loss: 0.5498378
[Epoch 22] ogbg-molbace: 0.598168 val loss: 0.738664
[Epoch 22] ogbg-molbace: 0.728221 test loss: 0.806516
[Epoch 23; Iter    28/   41] train: loss: 0.5050921
[Epoch 23] ogbg-molbace: 0.678755 val loss: 0.780328
[Epoch 23] ogbg-molbace: 0.786994 test loss: 0.861102
[Epoch 24; Iter    17/   41] train: loss: 0.5106734
[Epoch 24] ogbg-molbace: 0.615385 val loss: 0.702885
[Epoch 24] ogbg-molbace: 0.735176 test loss: 0.863082
[Epoch 25; Iter     6/   41] train: loss: 0.5412003
[Epoch 25; Iter    36/   41] train: loss: 0.5485969
[Epoch 25] ogbg-molbace: 0.598168 val loss: 0.722692
[Epoch 25] ogbg-molbace: 0.745436 test loss: 0.869957
[Epoch 26; Iter    25/   41] train: loss: 0.5087914
[Epoch 26] ogbg-molbace: 0.656777 val loss: 0.636218
[Epoch 26] ogbg-molbace: 0.762650 test loss: 0.794580
[Epoch 27; Iter    14/   41] train: loss: 0.6133921
[Epoch 27] ogbg-molbace: 0.654945 val loss: 0.661887
[Epoch 27] ogbg-molbace: 0.767693 test loss: 0.752149
[Epoch 28; Iter     3/   41] train: loss: 0.3972010
[Epoch 28; Iter    33/   41] train: loss: 0.5190350
[Epoch 28] ogbg-molbace: 0.657509 val loss: 0.673758
[Epoch 28] ogbg-molbace: 0.773431 test loss: 0.800852
[Epoch 29; Iter    22/   41] train: loss: 0.4833164
[Epoch 29] ogbg-molbace: 0.688278 val loss: 0.891637
[Epoch 29] ogbg-molbace: 0.776039 test loss: 0.993998
[Epoch 30; Iter    11/   41] train: loss: 0.5883940
[Epoch 30; Iter    41/   41] train: loss: 0.4967625
[Epoch 30] ogbg-molbace: 0.709890 val loss: 0.643802
[Epoch 30] ogbg-molbace: 0.788906 test loss: 0.728164
[Epoch 31; Iter    30/   41] train: loss: 0.4526926
[Epoch 31] ogbg-molbace: 0.687179 val loss: 0.674513
[Epoch 31] ogbg-molbace: 0.786994 test loss: 0.673918
[Epoch 32; Iter    19/   41] train: loss: 0.6407889
[Epoch 32] ogbg-molbace: 0.626007 val loss: 0.672602
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/bace/noise=0.2/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.2_5_26-05_10-07-31
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.2
logdir: runs/static_noise/GraphCL/bace/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6949667
[Epoch 1] ogbg-molbace: 0.439927 val loss: 0.691361
[Epoch 1] ogbg-molbace: 0.501652 test loss: 0.692807
[Epoch 2; Iter    19/   41] train: loss: 0.6943505
[Epoch 2] ogbg-molbace: 0.404762 val loss: 0.685756
[Epoch 2] ogbg-molbace: 0.489828 test loss: 0.690596
[Epoch 3; Iter     8/   41] train: loss: 0.6954476
[Epoch 3; Iter    38/   41] train: loss: 0.6955317
[Epoch 3] ogbg-molbace: 0.381685 val loss: 0.689422
[Epoch 3] ogbg-molbace: 0.394540 test loss: 0.697114
[Epoch 4; Iter    27/   41] train: loss: 0.6967378
[Epoch 4] ogbg-molbace: 0.380586 val loss: 0.692148
[Epoch 4] ogbg-molbace: 0.383585 test loss: 0.699722
[Epoch 5; Iter    16/   41] train: loss: 0.6948492
[Epoch 5] ogbg-molbace: 0.378755 val loss: 0.699405
[Epoch 5] ogbg-molbace: 0.370892 test loss: 0.704110
[Epoch 6; Iter     5/   41] train: loss: 0.6939648
[Epoch 6; Iter    35/   41] train: loss: 0.6965867
[Epoch 6] ogbg-molbace: 0.382418 val loss: 0.694740
[Epoch 6] ogbg-molbace: 0.379238 test loss: 0.701433
[Epoch 7; Iter    24/   41] train: loss: 0.6955287
[Epoch 7] ogbg-molbace: 0.378755 val loss: 0.692934
[Epoch 7] ogbg-molbace: 0.386020 test loss: 0.699624
[Epoch 8; Iter    13/   41] train: loss: 0.6896327
[Epoch 8] ogbg-molbace: 0.380586 val loss: 0.698260
[Epoch 8] ogbg-molbace: 0.372805 test loss: 0.703977
[Epoch 9; Iter     2/   41] train: loss: 0.6944321
[Epoch 9; Iter    32/   41] train: loss: 0.6928332
[Epoch 9] ogbg-molbace: 0.380220 val loss: 0.697903
[Epoch 9] ogbg-molbace: 0.377326 test loss: 0.701888
[Epoch 10; Iter    21/   41] train: loss: 0.6942457
[Epoch 10] ogbg-molbace: 0.377289 val loss: 0.697712
[Epoch 10] ogbg-molbace: 0.384977 test loss: 0.701144
[Epoch 11; Iter    10/   41] train: loss: 0.6941341
[Epoch 11; Iter    40/   41] train: loss: 0.6928916
[Epoch 11] ogbg-molbace: 0.382051 val loss: 0.693821
[Epoch 11] ogbg-molbace: 0.390019 test loss: 0.699727
[Epoch 12; Iter    29/   41] train: loss: 0.6937248
[Epoch 12] ogbg-molbace: 0.383516 val loss: 0.700686
[Epoch 12] ogbg-molbace: 0.378369 test loss: 0.703882
[Epoch 13; Iter    18/   41] train: loss: 0.6924059
[Epoch 13] ogbg-molbace: 0.380952 val loss: 0.695928
[Epoch 13] ogbg-molbace: 0.396627 test loss: 0.699737
[Epoch 14; Iter     7/   41] train: loss: 0.6898165
[Epoch 14; Iter    37/   41] train: loss: 0.6896603
[Epoch 14] ogbg-molbace: 0.379487 val loss: 0.702098
[Epoch 14] ogbg-molbace: 0.387759 test loss: 0.702892
[Epoch 15; Iter    26/   41] train: loss: 0.6931875
[Epoch 15] ogbg-molbace: 0.379121 val loss: 0.698841
[Epoch 15] ogbg-molbace: 0.403756 test loss: 0.700676
[Epoch 16; Iter    15/   41] train: loss: 0.6839843
[Epoch 16] ogbg-molbace: 0.380952 val loss: 0.705984
[Epoch 16] ogbg-molbace: 0.380108 test loss: 0.705634
[Epoch 17; Iter     4/   41] train: loss: 0.6909257
[Epoch 17; Iter    34/   41] train: loss: 0.6970875
[Epoch 17] ogbg-molbace: 0.400000 val loss: 0.699978
[Epoch 17] ogbg-molbace: 0.546861 test loss: 0.693287
[Epoch 18; Iter    23/   41] train: loss: 0.6803269
[Epoch 18] ogbg-molbace: 0.607326 val loss: 0.702689
[Epoch 18] ogbg-molbace: 0.742827 test loss: 0.689188
[Epoch 19; Iter    12/   41] train: loss: 0.6584627
[Epoch 19] ogbg-molbace: 0.636996 val loss: 0.652835
[Epoch 19] ogbg-molbace: 0.762824 test loss: 0.647532
[Epoch 20; Iter     1/   41] train: loss: 0.6298058
[Epoch 20; Iter    31/   41] train: loss: 0.5975159
[Epoch 20] ogbg-molbace: 0.648718 val loss: 0.576334
[Epoch 20] ogbg-molbace: 0.771344 test loss: 0.635333
[Epoch 21; Iter    20/   41] train: loss: 0.5905295
[Epoch 21] ogbg-molbace: 0.663370 val loss: 0.573300
[Epoch 21] ogbg-molbace: 0.773431 test loss: 0.650252
[Epoch 22; Iter     9/   41] train: loss: 0.5380198
[Epoch 22; Iter    39/   41] train: loss: 0.5673865
[Epoch 22] ogbg-molbace: 0.649817 val loss: 0.615683
[Epoch 22] ogbg-molbace: 0.773952 test loss: 0.686637
[Epoch 23; Iter    28/   41] train: loss: 0.5548695
[Epoch 23] ogbg-molbace: 0.661172 val loss: 0.607625
[Epoch 23] ogbg-molbace: 0.785081 test loss: 0.710891
[Epoch 24; Iter    17/   41] train: loss: 0.5289481
[Epoch 24] ogbg-molbace: 0.653480 val loss: 0.616905
[Epoch 24] ogbg-molbace: 0.770649 test loss: 0.701039
[Epoch 25; Iter     6/   41] train: loss: 0.5008156
[Epoch 25; Iter    36/   41] train: loss: 0.6256920
[Epoch 25] ogbg-molbace: 0.663370 val loss: 0.630819
[Epoch 25] ogbg-molbace: 0.789950 test loss: 0.657455
[Epoch 26; Iter    25/   41] train: loss: 0.4953767
[Epoch 26] ogbg-molbace: 0.662637 val loss: 0.561044
[Epoch 26] ogbg-molbace: 0.770649 test loss: 0.688744
[Epoch 27; Iter    14/   41] train: loss: 0.5498462
[Epoch 27] ogbg-molbace: 0.669231 val loss: 0.628094
[Epoch 27] ogbg-molbace: 0.789776 test loss: 0.682571
[Epoch 28; Iter     3/   41] train: loss: 0.5455687
[Epoch 28; Iter    33/   41] train: loss: 0.4711458
[Epoch 28] ogbg-molbace: 0.675092 val loss: 0.548232
[Epoch 28] ogbg-molbace: 0.780212 test loss: 0.665639
[Epoch 29; Iter    22/   41] train: loss: 0.5789163
[Epoch 29] ogbg-molbace: 0.663370 val loss: 0.574117
[Epoch 29] ogbg-molbace: 0.784385 test loss: 0.701055
[Epoch 30; Iter    11/   41] train: loss: 0.6550182
[Epoch 30; Iter    41/   41] train: loss: 0.3667331
[Epoch 30] ogbg-molbace: 0.672894 val loss: 0.679789
[Epoch 30] ogbg-molbace: 0.789428 test loss: 0.701125
[Epoch 31; Iter    30/   41] train: loss: 0.5969489
[Epoch 31] ogbg-molbace: 0.687179 val loss: 0.578054
[Epoch 31] ogbg-molbace: 0.772561 test loss: 0.695907
[Epoch 32; Iter    19/   41] train: loss: 0.7078755
[Epoch 32] ogbg-molbace: 0.663736 val loss: 0.553167
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/bace/noise=0.0/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.0_6_26-05_10-06-59
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.0
logdir: runs/static_noise/GraphCL/bace/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6950312
[Epoch 1] ogbg-molbace: 0.390842 val loss: 0.693616
[Epoch 1] ogbg-molbace: 0.308294 test loss: 0.693569
[Epoch 2; Iter    19/   41] train: loss: 0.7015147
[Epoch 2] ogbg-molbace: 0.383150 val loss: 0.694402
[Epoch 2] ogbg-molbace: 0.298035 test loss: 0.695208
[Epoch 3; Iter     8/   41] train: loss: 0.6928485
[Epoch 3; Iter    38/   41] train: loss: 0.7026150
[Epoch 3] ogbg-molbace: 0.381685 val loss: 0.695068
[Epoch 3] ogbg-molbace: 0.304643 test loss: 0.696302
[Epoch 4; Iter    27/   41] train: loss: 0.6963117
[Epoch 4] ogbg-molbace: 0.366667 val loss: 0.695214
[Epoch 4] ogbg-molbace: 0.300122 test loss: 0.696418
[Epoch 5; Iter    16/   41] train: loss: 0.6983405
[Epoch 5] ogbg-molbace: 0.372894 val loss: 0.695499
[Epoch 5] ogbg-molbace: 0.309511 test loss: 0.696708
[Epoch 6; Iter     5/   41] train: loss: 0.6953483
[Epoch 6; Iter    35/   41] train: loss: 0.6975852
[Epoch 6] ogbg-molbace: 0.376190 val loss: 0.695585
[Epoch 6] ogbg-molbace: 0.305164 test loss: 0.696479
[Epoch 7; Iter    24/   41] train: loss: 0.6994967
[Epoch 7] ogbg-molbace: 0.379121 val loss: 0.696618
[Epoch 7] ogbg-molbace: 0.306555 test loss: 0.696888
[Epoch 8; Iter    13/   41] train: loss: 0.6998385
[Epoch 8] ogbg-molbace: 0.374725 val loss: 0.696639
[Epoch 8] ogbg-molbace: 0.305338 test loss: 0.696956
[Epoch 9; Iter     2/   41] train: loss: 0.6927477
[Epoch 9; Iter    32/   41] train: loss: 0.6953177
[Epoch 9] ogbg-molbace: 0.388278 val loss: 0.697411
[Epoch 9] ogbg-molbace: 0.313685 test loss: 0.697470
[Epoch 10; Iter    21/   41] train: loss: 0.6960940
[Epoch 10] ogbg-molbace: 0.376557 val loss: 0.698159
[Epoch 10] ogbg-molbace: 0.318206 test loss: 0.697161
[Epoch 11; Iter    10/   41] train: loss: 0.6949072
[Epoch 11; Iter    40/   41] train: loss: 0.6967743
[Epoch 11] ogbg-molbace: 0.379121 val loss: 0.698883
[Epoch 11] ogbg-molbace: 0.315597 test loss: 0.698364
[Epoch 12; Iter    29/   41] train: loss: 0.6955029
[Epoch 12] ogbg-molbace: 0.379487 val loss: 0.699180
[Epoch 12] ogbg-molbace: 0.323596 test loss: 0.698249
[Epoch 13; Iter    18/   41] train: loss: 0.6949422
[Epoch 13] ogbg-molbace: 0.395604 val loss: 0.700263
[Epoch 13] ogbg-molbace: 0.332290 test loss: 0.698399
[Epoch 14; Iter     7/   41] train: loss: 0.6895126
[Epoch 14; Iter    37/   41] train: loss: 0.6925826
[Epoch 14] ogbg-molbace: 0.394139 val loss: 0.701040
[Epoch 14] ogbg-molbace: 0.333159 test loss: 0.699072
[Epoch 15; Iter    26/   41] train: loss: 0.6897358
[Epoch 15] ogbg-molbace: 0.402930 val loss: 0.701895
[Epoch 15] ogbg-molbace: 0.342201 test loss: 0.698796
[Epoch 16; Iter    15/   41] train: loss: 0.6896077
[Epoch 16] ogbg-molbace: 0.405861 val loss: 0.703070
[Epoch 16] ogbg-molbace: 0.343592 test loss: 0.699651
[Epoch 17; Iter     4/   41] train: loss: 0.6878887
[Epoch 17; Iter    34/   41] train: loss: 0.6983106
[Epoch 17] ogbg-molbace: 0.638462 val loss: 0.695647
[Epoch 17] ogbg-molbace: 0.665623 test loss: 0.690611
[Epoch 18; Iter    23/   41] train: loss: 0.6668013
[Epoch 18] ogbg-molbace: 0.669231 val loss: 0.687927
[Epoch 18] ogbg-molbace: 0.757086 test loss: 0.674837
[Epoch 19; Iter    12/   41] train: loss: 0.6310849
[Epoch 19] ogbg-molbace: 0.655678 val loss: 0.686492
[Epoch 19] ogbg-molbace: 0.742653 test loss: 0.709502
[Epoch 20; Iter     1/   41] train: loss: 0.5587086
[Epoch 20; Iter    31/   41] train: loss: 0.5533006
[Epoch 20] ogbg-molbace: 0.633700 val loss: 0.764389
[Epoch 20] ogbg-molbace: 0.761954 test loss: 0.784464
[Epoch 21; Iter    20/   41] train: loss: 0.5678983
[Epoch 21] ogbg-molbace: 0.620879 val loss: 0.862770
[Epoch 21] ogbg-molbace: 0.727700 test loss: 0.916149
[Epoch 22; Iter     9/   41] train: loss: 0.5027606
[Epoch 22; Iter    39/   41] train: loss: 0.4658118
[Epoch 22] ogbg-molbace: 0.674725 val loss: 0.719713
[Epoch 22] ogbg-molbace: 0.764041 test loss: 0.618284
[Epoch 23; Iter    28/   41] train: loss: 0.4154097
[Epoch 23] ogbg-molbace: 0.699634 val loss: 0.745083
[Epoch 23] ogbg-molbace: 0.787167 test loss: 0.840309
[Epoch 24; Iter    17/   41] train: loss: 0.3310628
[Epoch 24] ogbg-molbace: 0.678022 val loss: 0.854424
[Epoch 24] ogbg-molbace: 0.793427 test loss: 0.910190
[Epoch 25; Iter     6/   41] train: loss: 0.3499763
[Epoch 25; Iter    36/   41] train: loss: 0.2955730
[Epoch 25] ogbg-molbace: 0.675458 val loss: 0.758316
[Epoch 25] ogbg-molbace: 0.787863 test loss: 0.853016
[Epoch 26; Iter    25/   41] train: loss: 0.4223655
[Epoch 26] ogbg-molbace: 0.694872 val loss: 0.880113
[Epoch 26] ogbg-molbace: 0.767866 test loss: 1.073811
[Epoch 27; Iter    14/   41] train: loss: 0.4835436
[Epoch 27] ogbg-molbace: 0.695604 val loss: 0.675150
[Epoch 27] ogbg-molbace: 0.794818 test loss: 0.662657
[Epoch 28; Iter     3/   41] train: loss: 0.4393473
[Epoch 28; Iter    33/   41] train: loss: 0.4775278
[Epoch 28] ogbg-molbace: 0.670330 val loss: 0.724400
[Epoch 28] ogbg-molbace: 0.757955 test loss: 0.941781
[Epoch 29; Iter    22/   41] train: loss: 0.5146201
[Epoch 29] ogbg-molbace: 0.688278 val loss: 0.625472
[Epoch 29] ogbg-molbace: 0.762128 test loss: 0.741384
[Epoch 30; Iter    11/   41] train: loss: 0.4588378
[Epoch 30; Iter    41/   41] train: loss: 0.5497862
[Epoch 30] ogbg-molbace: 0.642857 val loss: 0.782079
[Epoch 30] ogbg-molbace: 0.779343 test loss: 0.755651
[Epoch 31; Iter    30/   41] train: loss: 0.4355943
[Epoch 31] ogbg-molbace: 0.610989 val loss: 1.532749
[Epoch 31] ogbg-molbace: 0.751348 test loss: 1.682485
[Epoch 32; Iter    19/   41] train: loss: 0.3911993
[Epoch 32] ogbg-molbace: 0.716117 val loss: 0.750523
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/bace/noise=0.0/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.0_4_26-05_10-06-59
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.0
logdir: runs/static_noise/GraphCL/bace/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6946846
[Epoch 1] ogbg-molbace: 0.402930 val loss: 0.692686
[Epoch 1] ogbg-molbace: 0.399061 test loss: 0.693830
[Epoch 2; Iter    19/   41] train: loss: 0.6947215
[Epoch 2] ogbg-molbace: 0.359341 val loss: 0.692525
[Epoch 2] ogbg-molbace: 0.390193 test loss: 0.695428
[Epoch 3; Iter     8/   41] train: loss: 0.6890867
[Epoch 3; Iter    38/   41] train: loss: 0.6967059
[Epoch 3] ogbg-molbace: 0.366300 val loss: 0.692163
[Epoch 3] ogbg-molbace: 0.382368 test loss: 0.695824
[Epoch 4; Iter    27/   41] train: loss: 0.6968171
[Epoch 4] ogbg-molbace: 0.361172 val loss: 0.692548
[Epoch 4] ogbg-molbace: 0.383585 test loss: 0.695948
[Epoch 5; Iter    16/   41] train: loss: 0.6947884
[Epoch 5] ogbg-molbace: 0.367033 val loss: 0.692152
[Epoch 5] ogbg-molbace: 0.390193 test loss: 0.695624
[Epoch 6; Iter     5/   41] train: loss: 0.7019798
[Epoch 6; Iter    35/   41] train: loss: 0.6907607
[Epoch 6] ogbg-molbace: 0.364103 val loss: 0.692974
[Epoch 6] ogbg-molbace: 0.383759 test loss: 0.696172
[Epoch 7; Iter    24/   41] train: loss: 0.6940095
[Epoch 7] ogbg-molbace: 0.372894 val loss: 0.693015
[Epoch 7] ogbg-molbace: 0.395236 test loss: 0.695967
[Epoch 8; Iter    13/   41] train: loss: 0.6914994
[Epoch 8] ogbg-molbace: 0.363736 val loss: 0.694182
[Epoch 8] ogbg-molbace: 0.387063 test loss: 0.696765
[Epoch 9; Iter     2/   41] train: loss: 0.6980399
[Epoch 9; Iter    32/   41] train: loss: 0.6966116
[Epoch 9] ogbg-molbace: 0.364469 val loss: 0.694568
[Epoch 9] ogbg-molbace: 0.386889 test loss: 0.696485
[Epoch 10; Iter    21/   41] train: loss: 0.6957610
[Epoch 10] ogbg-molbace: 0.368498 val loss: 0.694874
[Epoch 10] ogbg-molbace: 0.398366 test loss: 0.696535
[Epoch 11; Iter    10/   41] train: loss: 0.6995579
[Epoch 11; Iter    40/   41] train: loss: 0.6941286
[Epoch 11] ogbg-molbace: 0.371062 val loss: 0.696089
[Epoch 11] ogbg-molbace: 0.396279 test loss: 0.697022
[Epoch 12; Iter    29/   41] train: loss: 0.6982664
[Epoch 12] ogbg-molbace: 0.372894 val loss: 0.696794
[Epoch 12] ogbg-molbace: 0.404104 test loss: 0.697138
[Epoch 13; Iter    18/   41] train: loss: 0.6940095
[Epoch 13] ogbg-molbace: 0.380220 val loss: 0.697883
[Epoch 13] ogbg-molbace: 0.403930 test loss: 0.697782
[Epoch 14; Iter     7/   41] train: loss: 0.6958494
[Epoch 14; Iter    37/   41] train: loss: 0.6903545
[Epoch 14] ogbg-molbace: 0.383150 val loss: 0.698198
[Epoch 14] ogbg-molbace: 0.404104 test loss: 0.697857
[Epoch 15; Iter    26/   41] train: loss: 0.6947518
[Epoch 15] ogbg-molbace: 0.385348 val loss: 0.699027
[Epoch 15] ogbg-molbace: 0.411233 test loss: 0.698149
[Epoch 16; Iter    15/   41] train: loss: 0.6920192
[Epoch 16] ogbg-molbace: 0.389011 val loss: 0.700167
[Epoch 16] ogbg-molbace: 0.417319 test loss: 0.698587
[Epoch 17; Iter     4/   41] train: loss: 0.6936738
[Epoch 17; Iter    34/   41] train: loss: 0.6849753
[Epoch 17] ogbg-molbace: 0.661538 val loss: 0.680783
[Epoch 17] ogbg-molbace: 0.742653 test loss: 0.686086
[Epoch 18; Iter    23/   41] train: loss: 0.6545444
[Epoch 18] ogbg-molbace: 0.674359 val loss: 0.681607
[Epoch 18] ogbg-molbace: 0.760911 test loss: 0.683911
[Epoch 19; Iter    12/   41] train: loss: 0.6116589
[Epoch 19] ogbg-molbace: 0.669231 val loss: 0.649174
[Epoch 19] ogbg-molbace: 0.756912 test loss: 0.684262
[Epoch 20; Iter     1/   41] train: loss: 0.5737045
[Epoch 20; Iter    31/   41] train: loss: 0.5188594
[Epoch 20] ogbg-molbace: 0.677656 val loss: 0.629087
[Epoch 20] ogbg-molbace: 0.783864 test loss: 0.610843
[Epoch 21; Iter    20/   41] train: loss: 0.4748307
[Epoch 21] ogbg-molbace: 0.662637 val loss: 0.836232
[Epoch 21] ogbg-molbace: 0.783516 test loss: 0.818938
[Epoch 22; Iter     9/   41] train: loss: 0.5596997
[Epoch 22; Iter    39/   41] train: loss: 0.3438788
[Epoch 22] ogbg-molbace: 0.672527 val loss: 0.717183
[Epoch 22] ogbg-molbace: 0.769084 test loss: 0.819382
[Epoch 23; Iter    28/   41] train: loss: 0.4405135
[Epoch 23] ogbg-molbace: 0.721612 val loss: 0.648041
[Epoch 23] ogbg-molbace: 0.784907 test loss: 0.716864
[Epoch 24; Iter    17/   41] train: loss: 0.5653491
[Epoch 24] ogbg-molbace: 0.712454 val loss: 0.692394
[Epoch 24] ogbg-molbace: 0.798122 test loss: 0.684356
[Epoch 25; Iter     6/   41] train: loss: 0.4259747
[Epoch 25; Iter    36/   41] train: loss: 0.4918485
[Epoch 25] ogbg-molbace: 0.691209 val loss: 0.942795
[Epoch 25] ogbg-molbace: 0.804730 test loss: 0.947914
[Epoch 26; Iter    25/   41] train: loss: 0.4408578
[Epoch 26] ogbg-molbace: 0.645788 val loss: 0.814016
[Epoch 26] ogbg-molbace: 0.775517 test loss: 0.884437
[Epoch 27; Iter    14/   41] train: loss: 0.3231972
[Epoch 27] ogbg-molbace: 0.648718 val loss: 0.834537
[Epoch 27] ogbg-molbace: 0.774126 test loss: 0.761511
[Epoch 28; Iter     3/   41] train: loss: 0.4116565
[Epoch 28; Iter    33/   41] train: loss: 0.4338376
[Epoch 28] ogbg-molbace: 0.731136 val loss: 0.756094
[Epoch 28] ogbg-molbace: 0.798296 test loss: 0.890204
[Epoch 29; Iter    22/   41] train: loss: 0.5851269
[Epoch 29] ogbg-molbace: 0.672161 val loss: 0.702919
[Epoch 29] ogbg-molbace: 0.790471 test loss: 0.894697
[Epoch 30; Iter    11/   41] train: loss: 0.4034610
[Epoch 30; Iter    41/   41] train: loss: 0.3163770
[Epoch 30] ogbg-molbace: 0.631136 val loss: 1.224884
[Epoch 30] ogbg-molbace: 0.761085 test loss: 1.247691
[Epoch 31; Iter    30/   41] train: loss: 0.3575637
[Epoch 31] ogbg-molbace: 0.639194 val loss: 1.092287
[Epoch 31] ogbg-molbace: 0.771170 test loss: 1.021075
[Epoch 32; Iter    19/   41] train: loss: 0.3953187
[Epoch 32] ogbg-molbace: 0.728938 val loss: 0.689394
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/bace/noise=0.0/PNA_ogbg-molbace_GraphCL_bace_static_noise=0.0_5_26-05_10-06-59
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_static_noise=0.0
logdir: runs/static_noise/GraphCL/bace/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6941631
[Epoch 1] ogbg-molbace: 0.440659 val loss: 0.692166
[Epoch 1] ogbg-molbace: 0.429491 test loss: 0.693840
[Epoch 2; Iter    19/   41] train: loss: 0.6973654
[Epoch 2] ogbg-molbace: 0.406227 val loss: 0.691402
[Epoch 2] ogbg-molbace: 0.444792 test loss: 0.694443
[Epoch 3; Iter     8/   41] train: loss: 0.6944934
[Epoch 3; Iter    38/   41] train: loss: 0.6957358
[Epoch 3] ogbg-molbace: 0.435165 val loss: 0.690996
[Epoch 3] ogbg-molbace: 0.444792 test loss: 0.694651
[Epoch 4; Iter    27/   41] train: loss: 0.6940734
[Epoch 4] ogbg-molbace: 0.442125 val loss: 0.691196
[Epoch 4] ogbg-molbace: 0.446705 test loss: 0.694732
[Epoch 5; Iter    16/   41] train: loss: 0.6944140
[Epoch 5] ogbg-molbace: 0.431868 val loss: 0.691992
[Epoch 5] ogbg-molbace: 0.459572 test loss: 0.694731
[Epoch 6; Iter     5/   41] train: loss: 0.6944993
[Epoch 6; Iter    35/   41] train: loss: 0.6972181
[Epoch 6] ogbg-molbace: 0.441392 val loss: 0.692018
[Epoch 6] ogbg-molbace: 0.459920 test loss: 0.694895
[Epoch 7; Iter    24/   41] train: loss: 0.6930083
[Epoch 7] ogbg-molbace: 0.426374 val loss: 0.693080
[Epoch 7] ogbg-molbace: 0.459920 test loss: 0.695155
[Epoch 8; Iter    13/   41] train: loss: 0.6900389
[Epoch 8] ogbg-molbace: 0.432601 val loss: 0.693074
[Epoch 8] ogbg-molbace: 0.456268 test loss: 0.695385
[Epoch 9; Iter     2/   41] train: loss: 0.6933948
[Epoch 9; Iter    32/   41] train: loss: 0.6909602
[Epoch 9] ogbg-molbace: 0.456410 val loss: 0.693950
[Epoch 9] ogbg-molbace: 0.462354 test loss: 0.695565
[Epoch 10; Iter    21/   41] train: loss: 0.6918141
[Epoch 10] ogbg-molbace: 0.475458 val loss: 0.694257
[Epoch 10] ogbg-molbace: 0.485481 test loss: 0.695550
[Epoch 11; Iter    10/   41] train: loss: 0.6945540
[Epoch 11; Iter    40/   41] train: loss: 0.6928239
[Epoch 11] ogbg-molbace: 0.454579 val loss: 0.695062
[Epoch 11] ogbg-molbace: 0.488785 test loss: 0.695739
[Epoch 12; Iter    29/   41] train: loss: 0.6920391
[Epoch 12] ogbg-molbace: 0.488278 val loss: 0.695949
[Epoch 12] ogbg-molbace: 0.516606 test loss: 0.695838
[Epoch 13; Iter    18/   41] train: loss: 0.6947280
[Epoch 13] ogbg-molbace: 0.499634 val loss: 0.696641
[Epoch 13] ogbg-molbace: 0.526865 test loss: 0.695879
[Epoch 14; Iter     7/   41] train: loss: 0.6912009
[Epoch 14; Iter    37/   41] train: loss: 0.6882623
[Epoch 14] ogbg-molbace: 0.523443 val loss: 0.698040
[Epoch 14] ogbg-molbace: 0.536602 test loss: 0.696256
[Epoch 15; Iter    26/   41] train: loss: 0.6939883
[Epoch 15] ogbg-molbace: 0.521978 val loss: 0.698858
[Epoch 15] ogbg-molbace: 0.548079 test loss: 0.696868
[Epoch 16; Iter    15/   41] train: loss: 0.6843122
[Epoch 16] ogbg-molbace: 0.539560 val loss: 0.699656
[Epoch 16] ogbg-molbace: 0.566684 test loss: 0.697214
[Epoch 17; Iter     4/   41] train: loss: 0.6893942
[Epoch 17; Iter    34/   41] train: loss: 0.6943185
[Epoch 17] ogbg-molbace: 0.622344 val loss: 0.695543
[Epoch 17] ogbg-molbace: 0.739350 test loss: 0.689008
[Epoch 18; Iter    23/   41] train: loss: 0.6675870
[Epoch 18] ogbg-molbace: 0.674359 val loss: 0.697613
[Epoch 18] ogbg-molbace: 0.774648 test loss: 0.691867
[Epoch 19; Iter    12/   41] train: loss: 0.6294688
[Epoch 19] ogbg-molbace: 0.634066 val loss: 0.736852
[Epoch 19] ogbg-molbace: 0.728047 test loss: 0.747801
[Epoch 20; Iter     1/   41] train: loss: 0.5960811
[Epoch 20; Iter    31/   41] train: loss: 0.5263762
[Epoch 20] ogbg-molbace: 0.679121 val loss: 0.658170
[Epoch 20] ogbg-molbace: 0.769084 test loss: 0.674200
[Epoch 21; Iter    20/   41] train: loss: 0.5247238
[Epoch 21] ogbg-molbace: 0.688278 val loss: 1.034972
[Epoch 21] ogbg-molbace: 0.747174 test loss: 0.965905
[Epoch 22; Iter     9/   41] train: loss: 0.3979757
[Epoch 22; Iter    39/   41] train: loss: 0.4531927
[Epoch 22] ogbg-molbace: 0.631868 val loss: 0.759481
[Epoch 22] ogbg-molbace: 0.773778 test loss: 0.742023
[Epoch 23; Iter    28/   41] train: loss: 0.5383353
[Epoch 23] ogbg-molbace: 0.708425 val loss: 0.697014
[Epoch 23] ogbg-molbace: 0.806816 test loss: 0.673656
[Epoch 24; Iter    17/   41] train: loss: 0.4872547
[Epoch 24] ogbg-molbace: 0.692308 val loss: 0.834786
[Epoch 24] ogbg-molbace: 0.777256 test loss: 0.949473
[Epoch 25; Iter     6/   41] train: loss: 0.4095693
[Epoch 25; Iter    36/   41] train: loss: 0.4600287
[Epoch 25] ogbg-molbace: 0.620513 val loss: 0.799441
[Epoch 25] ogbg-molbace: 0.770822 test loss: 0.735524
[Epoch 26; Iter    25/   41] train: loss: 0.3381007
[Epoch 26] ogbg-molbace: 0.682051 val loss: 0.747623
[Epoch 26] ogbg-molbace: 0.779343 test loss: 0.623725
[Epoch 27; Iter    14/   41] train: loss: 0.5240496
[Epoch 27] ogbg-molbace: 0.721245 val loss: 0.708536
[Epoch 27] ogbg-molbace: 0.799687 test loss: 0.667350
[Epoch 28; Iter     3/   41] train: loss: 0.3390129
[Epoch 28; Iter    33/   41] train: loss: 0.5475276
[Epoch 28] ogbg-molbace: 0.664835 val loss: 0.879600
[Epoch 28] ogbg-molbace: 0.781951 test loss: 0.837073
[Epoch 29; Iter    22/   41] train: loss: 0.3272292
[Epoch 29] ogbg-molbace: 0.712088 val loss: 0.821904
[Epoch 29] ogbg-molbace: 0.801947 test loss: 0.906377
[Epoch 30; Iter    11/   41] train: loss: 0.4589972
[Epoch 30; Iter    41/   41] train: loss: 0.7516398
[Epoch 30] ogbg-molbace: 0.724542 val loss: 0.925979
[Epoch 30] ogbg-molbace: 0.799861 test loss: 0.753726
[Epoch 31; Iter    30/   41] train: loss: 0.3966761
[Epoch 31] ogbg-molbace: 0.704029 val loss: 0.920356
[Epoch 31] ogbg-molbace: 0.801600 test loss: 0.947973
[Epoch 32; Iter    19/   41] train: loss: 0.6199533
[Epoch 32] ogbg-molbace: 0.707326 val loss: 1.011373
[Epoch 32] ogbg-molbace: 0.797427 test loss: 0.797338
[Epoch 33; Iter     8/   41] train: loss: 0.4253961
[Epoch 33; Iter    38/   41] train: loss: 0.5146912
[Epoch 33] ogbg-molbace: 0.627839 val loss: 1.421874
[Epoch 33] ogbg-molbace: 0.735872 test loss: 1.645275
[Epoch 34; Iter    27/   41] train: loss: 0.5361418
[Epoch 34] ogbg-molbace: 0.535531 val loss: 0.928427
[Epoch 34] ogbg-molbace: 0.649800 test loss: 1.241435
[Epoch 35; Iter    16/   41] train: loss: 0.4315059
[Epoch 35] ogbg-molbace: 0.569963 val loss: 0.604383
[Epoch 35] ogbg-molbace: 0.633455 test loss: 1.075795
[Epoch 36; Iter     5/   41] train: loss: 0.3430600
[Epoch 36; Iter    35/   41] train: loss: 0.4292618
[Epoch 36] ogbg-molbace: 0.695238 val loss: 1.721942
[Epoch 36] ogbg-molbace: 0.803860 test loss: 1.797777
[Epoch 37; Iter    24/   41] train: loss: 0.3288790
[Epoch 37] ogbg-molbace: 0.638828 val loss: 1.068754
[Epoch 37] ogbg-molbace: 0.713963 test loss: 1.351606
[Epoch 38; Iter    13/   41] train: loss: 0.3369296
[Epoch 38] ogbg-molbace: 0.550916 val loss: 0.959340
[Epoch 38] ogbg-molbace: 0.636237 test loss: 1.965409
[Epoch 39; Iter     2/   41] train: loss: 0.2744115
[Epoch 39; Iter    32/   41] train: loss: 0.3674288
[Epoch 39] ogbg-molbace: 0.715751 val loss: 0.973762
[Epoch 39] ogbg-molbace: 0.822466 test loss: 0.879619
[Epoch 40; Iter    21/   41] train: loss: 0.2363954
[Epoch 40] ogbg-molbace: 0.645055 val loss: 1.064866
[Epoch 40] ogbg-molbace: 0.753608 test loss: 1.101474
[Epoch 41; Iter    10/   41] train: loss: 0.4035791
[Epoch 41; Iter    40/   41] train: loss: 0.3775324
[Epoch 41] ogbg-molbace: 0.601099 val loss: 2.979261
[Epoch 41] ogbg-molbace: 0.684577 test loss: 3.304152
[Epoch 42; Iter    29/   41] train: loss: 0.3181682
[Epoch 42] ogbg-molbace: 0.615751 val loss: 1.348366
[Epoch 42] ogbg-molbace: 0.672057 test loss: 1.900512
[Epoch 43; Iter    18/   41] train: loss: 0.2723254
[Epoch 43] ogbg-molbace: 0.692674 val loss: 1.480970
[Epoch 43] ogbg-molbace: 0.747870 test loss: 1.794639
[Epoch 44; Iter     7/   41] train: loss: 0.2037731
[Epoch 44; Iter    37/   41] train: loss: 0.2857065
[Epoch 44] ogbg-molbace: 0.689377 val loss: 0.992362
[Epoch 44] ogbg-molbace: 0.767866 test loss: 0.893782
[Epoch 45; Iter    26/   41] train: loss: 0.2152712
[Epoch 45] ogbg-molbace: 0.634066 val loss: 1.507346
[Epoch 45] ogbg-molbace: 0.711528 test loss: 2.187647
[Epoch 46; Iter    15/   41] train: loss: 0.2157275
[Epoch 46] ogbg-molbace: 0.607326 val loss: 2.214821
[Epoch 46] ogbg-molbace: 0.651713 test loss: 2.646997
[Epoch 47; Iter     4/   41] train: loss: 0.1282729
[Epoch 47; Iter    34/   41] train: loss: 0.1098889
[Epoch 47] ogbg-molbace: 0.637363 val loss: 2.454142
[Epoch 47] ogbg-molbace: 0.724396 test loss: 3.148760
[Epoch 48; Iter    23/   41] train: loss: 0.1032830
[Epoch 48] ogbg-molbace: 0.602930 val loss: 1.945800
[Epoch 48] ogbg-molbace: 0.743349 test loss: 2.017487
[Epoch 49; Iter    12/   41] train: loss: 0.0720877
[Epoch 49] ogbg-molbace: 0.639194 val loss: 2.756218
[Epoch 49] ogbg-molbace: 0.668753 test loss: 3.867214
[Epoch 50; Iter     1/   41] train: loss: 0.0704716
[Epoch 50; Iter    31/   41] train: loss: 0.0167753
[Epoch 50] ogbg-molbace: 0.676190 val loss: 1.188613
[Epoch 50] ogbg-molbace: 0.779169 test loss: 1.669614
[Epoch 51; Iter    20/   41] train: loss: 0.0320821
[Epoch 51] ogbg-molbace: 0.579121 val loss: 3.642796
[Epoch 51] ogbg-molbace: 0.693445 test loss: 4.562878
[Epoch 52; Iter     9/   41] train: loss: 0.0946348
[Epoch 52; Iter    39/   41] train: loss: 0.2058794
[Epoch 52] ogbg-molbace: 0.617216 val loss: 1.718511
[Epoch 52] ogbg-molbace: 0.720223 test loss: 1.930745
[Epoch 53; Iter    28/   41] train: loss: 0.1126283
[Epoch 53] ogbg-molbace: 0.639927 val loss: 1.414286
[Epoch 53] ogbg-molbace: 0.680577 test loss: 1.707723
[Epoch 54; Iter    17/   41] train: loss: 0.1557791
[Epoch 54] ogbg-molbace: 0.672894 val loss: 1.493892
[Epoch 54] ogbg-molbace: 0.729960 test loss: 1.486299
[Epoch 55; Iter     6/   41] train: loss: 0.1889779
[Epoch 55; Iter    36/   41] train: loss: 0.0333212
[Epoch 55] ogbg-molbace: 0.594139 val loss: 4.781537
[Epoch 55] ogbg-molbace: 0.546340 test loss: 5.529928
[Epoch 56; Iter    25/   41] train: loss: 0.0747983
[Epoch 56] ogbg-molbace: 0.612454 val loss: 2.343728
[Epoch 56] ogbg-molbace: 0.631021 test loss: 3.686131
[Epoch 57; Iter    14/   41] train: loss: 0.0193119
[Epoch 57] ogbg-molbace: 0.652015 val loss: 1.870395
[Epoch 57] ogbg-molbace: 0.725091 test loss: 2.891441
[Epoch 58; Iter     3/   41] train: loss: 0.0295843
[Epoch 58; Iter    33/   41] train: loss: 0.0920795
[Epoch 58] ogbg-molbace: 0.673993 val loss: 1.739807
[Epoch 58] ogbg-molbace: 0.719005 test loss: 2.882441
[Epoch 59; Iter    22/   41] train: loss: 0.0469696
[Epoch 59] ogbg-molbace: 0.564103 val loss: 1.819577
[Epoch 59] ogbg-molbace: 0.634846 test loss: 2.269217
[Epoch 60; Iter    11/   41] train: loss: 0.0439276
[Epoch 60; Iter    41/   41] train: loss: 0.0061550
[Epoch 60] ogbg-molbace: 0.638462 val loss: 4.505777
[Epoch 60] ogbg-molbace: 0.616762 test loss: 5.388640
[Epoch 61; Iter    30/   41] train: loss: 0.0572968
[Epoch 61] ogbg-molbace: 0.640659 val loss: 1.519675
[Epoch 61] ogbg-molbace: 0.697096 test loss: 2.144764
[Epoch 62; Iter    19/   41] train: loss: 0.0290236
[Epoch 62] ogbg-molbace: 0.635165 val loss: 1.649292
[Epoch 62] ogbg-molbace: 0.742306 test loss: 2.467086
[Epoch 63; Iter     8/   41] train: loss: 0.0155547
[Epoch 63; Iter    38/   41] train: loss: 0.0246886
[Epoch 63] ogbg-molbace: 0.590110 val loss: 1.728332
[Epoch 63] ogbg-molbace: 0.692227 test loss: 2.509078
[Epoch 64; Iter    27/   41] train: loss: 0.0327188
[Epoch 64] ogbg-molbace: 0.590842 val loss: 3.048000
[Epoch 64] ogbg-molbace: 0.705616 test loss: 3.884272
[Epoch 65; Iter    16/   41] train: loss: 0.0262931
[Epoch 65] ogbg-molbace: 0.635165 val loss: 2.901172
[Epoch 65] ogbg-molbace: 0.701791 test loss: 3.247725
[Epoch 66; Iter     5/   41] train: loss: 0.0483245
[Epoch 66; Iter    35/   41] train: loss: 0.1482543
[Epoch 66] ogbg-molbace: 0.620513 val loss: 2.554383
[Epoch 66] ogbg-molbace: 0.752565 test loss: 3.045745
[Epoch 67; Iter    24/   41] train: loss: 0.1165789
[Epoch 67] ogbg-molbace: 0.621612 val loss: 2.523678
[Epoch 67] ogbg-molbace: 0.743175 test loss: 2.706544
[Epoch 68; Iter    13/   41] train: loss: 0.1200759
[Epoch 68] ogbg-molbace: 0.580952 val loss: 2.017671
[Epoch 68] ogbg-molbace: 0.688924 test loss: 3.493812
[Epoch 69; Iter     2/   41] train: loss: 0.0560577
[Epoch 69; Iter    32/   41] train: loss: 0.0786292
[Epoch 69] ogbg-molbace: 0.600733 val loss: 2.239331
[Epoch 69] ogbg-molbace: 0.678317 test loss: 2.882342
[Epoch 70; Iter    21/   41] train: loss: 0.0570430
[Epoch 70] ogbg-molbace: 0.586081 val loss: 3.721112
[Epoch 70] ogbg-molbace: 0.610503 test loss: 5.784251
[Epoch 71; Iter    10/   41] train: loss: 0.1078234
[Epoch 71; Iter    40/   41] train: loss: 0.1099357
[Epoch 71] ogbg-molbace: 0.646154 val loss: 1.960150
[Epoch 71] ogbg-molbace: 0.693966 test loss: 1.957555
[Epoch 72; Iter    29/   41] train: loss: 0.1053819
[Epoch 72] ogbg-molbace: 0.605128 val loss: 1.960383
[Epoch 72] ogbg-molbace: 0.625978 test loss: 3.118240
[Epoch 73; Iter    18/   41] train: loss: 0.0238631
[Epoch 73] ogbg-molbace: 0.605128 val loss: 3.175641
[Epoch 73] ogbg-molbace: 0.700748 test loss: 4.160835
[Epoch 74; Iter     7/   41] train: loss: 0.0489810
[Epoch 74; Iter    37/   41] train: loss: 0.0815010
[Epoch 74] ogbg-molbace: 0.588278 val loss: 2.509240
[Epoch 74] ogbg-molbace: 0.719005 test loss: 3.811425
[Epoch 75; Iter    26/   41] train: loss: 0.0178267
[Epoch 75] ogbg-molbace: 0.629670 val loss: 2.144866
[Epoch 75] ogbg-molbace: 0.752391 test loss: 2.428345
[Epoch 76; Iter    15/   41] train: loss: 0.0102926
[Epoch 76] ogbg-molbace: 0.617582 val loss: 2.444008
[Epoch 76] ogbg-molbace: 0.667014 test loss: 3.882585
[Epoch 77; Iter     4/   41] train: loss: 0.0055931
[Epoch 77; Iter    34/   41] train: loss: 0.0039605
[Epoch 77] ogbg-molbace: 0.634799 val loss: 1.840006
[Epoch 77] ogbg-molbace: 0.693271 test loss: 2.410182
[Epoch 78; Iter    23/   41] train: loss: 0.1148330
[Epoch 32] ogbg-molbace: 0.755173 test loss: 0.725657
[Epoch 33; Iter     8/   41] train: loss: 0.4808474
[Epoch 33; Iter    38/   41] train: loss: 0.5309687
[Epoch 33] ogbg-molbace: 0.700366 val loss: 0.913380
[Epoch 33] ogbg-molbace: 0.744914 test loss: 1.077283
[Epoch 34; Iter    27/   41] train: loss: 0.5608610
[Epoch 34] ogbg-molbace: 0.615385 val loss: 0.571007
[Epoch 34] ogbg-molbace: 0.650148 test loss: 0.838444
[Epoch 35; Iter    16/   41] train: loss: 0.5508070
[Epoch 35] ogbg-molbace: 0.683150 val loss: 0.570938
[Epoch 35] ogbg-molbace: 0.683881 test loss: 0.781267
[Epoch 36; Iter     5/   41] train: loss: 0.4692871
[Epoch 36; Iter    35/   41] train: loss: 0.4669022
[Epoch 36] ogbg-molbace: 0.701465 val loss: 0.774332
[Epoch 36] ogbg-molbace: 0.772040 test loss: 0.869726
[Epoch 37; Iter    24/   41] train: loss: 0.3772946
[Epoch 37] ogbg-molbace: 0.664835 val loss: 0.795744
[Epoch 37] ogbg-molbace: 0.720570 test loss: 0.926834
[Epoch 38; Iter    13/   41] train: loss: 0.3627436
[Epoch 38] ogbg-molbace: 0.703663 val loss: 0.843786
[Epoch 38] ogbg-molbace: 0.783342 test loss: 0.814333
[Epoch 39; Iter     2/   41] train: loss: 0.2967470
[Epoch 39; Iter    32/   41] train: loss: 0.2590467
[Epoch 39] ogbg-molbace: 0.642125 val loss: 1.138358
[Epoch 39] ogbg-molbace: 0.745088 test loss: 1.296615
[Epoch 40; Iter    21/   41] train: loss: 0.2218354
[Epoch 40] ogbg-molbace: 0.670696 val loss: 0.930877
[Epoch 40] ogbg-molbace: 0.593462 test loss: 1.578002
[Epoch 41; Iter    10/   41] train: loss: 0.3146436
[Epoch 41; Iter    40/   41] train: loss: 0.3863838
[Epoch 41] ogbg-molbace: 0.605495 val loss: 1.317380
[Epoch 41] ogbg-molbace: 0.665102 test loss: 1.479421
[Epoch 42; Iter    29/   41] train: loss: 0.3319243
[Epoch 42] ogbg-molbace: 0.676923 val loss: 1.178817
[Epoch 42] ogbg-molbace: 0.692575 test loss: 1.410061
[Epoch 43; Iter    18/   41] train: loss: 0.1439968
[Epoch 43] ogbg-molbace: 0.706960 val loss: 1.143296
[Epoch 43] ogbg-molbace: 0.784038 test loss: 1.138600
[Epoch 44; Iter     7/   41] train: loss: 0.3243696
[Epoch 44; Iter    37/   41] train: loss: 0.1789535
[Epoch 44] ogbg-molbace: 0.723443 val loss: 0.829307
[Epoch 44] ogbg-molbace: 0.715875 test loss: 1.311694
[Epoch 45; Iter    26/   41] train: loss: 0.1353711
[Epoch 45] ogbg-molbace: 0.668864 val loss: 1.702954
[Epoch 45] ogbg-molbace: 0.703356 test loss: 2.015379
[Epoch 46; Iter    15/   41] train: loss: 0.2119420
[Epoch 46] ogbg-molbace: 0.640293 val loss: 1.509761
[Epoch 46] ogbg-molbace: 0.730482 test loss: 1.593259
[Epoch 47; Iter     4/   41] train: loss: 0.0888000
[Epoch 47; Iter    34/   41] train: loss: 0.0670413
[Epoch 47] ogbg-molbace: 0.561905 val loss: 3.847628
[Epoch 47] ogbg-molbace: 0.580247 test loss: 3.969517
[Epoch 48; Iter    23/   41] train: loss: 0.2679685
[Epoch 48] ogbg-molbace: 0.655678 val loss: 1.002925
[Epoch 48] ogbg-molbace: 0.658668 test loss: 1.922763
[Epoch 49; Iter    12/   41] train: loss: 0.0839634
[Epoch 49] ogbg-molbace: 0.638462 val loss: 1.922581
[Epoch 49] ogbg-molbace: 0.665797 test loss: 1.934640
[Epoch 50; Iter     1/   41] train: loss: 0.3635853
[Epoch 50; Iter    31/   41] train: loss: 0.0653627
[Epoch 50] ogbg-molbace: 0.706227 val loss: 1.178373
[Epoch 50] ogbg-molbace: 0.762650 test loss: 1.103949
[Epoch 51; Iter    20/   41] train: loss: 0.0593306
[Epoch 51] ogbg-molbace: 0.653480 val loss: 1.227871
[Epoch 51] ogbg-molbace: 0.714137 test loss: 1.948916
[Epoch 52; Iter     9/   41] train: loss: 0.0306788
[Epoch 52; Iter    39/   41] train: loss: 0.2067526
[Epoch 52] ogbg-molbace: 0.664103 val loss: 1.855589
[Epoch 52] ogbg-molbace: 0.709790 test loss: 2.435924
[Epoch 53; Iter    28/   41] train: loss: 0.0072510
[Epoch 53] ogbg-molbace: 0.605128 val loss: 3.457011
[Epoch 53] ogbg-molbace: 0.657103 test loss: 3.966577
[Epoch 54; Iter    17/   41] train: loss: 0.0292787
[Epoch 54] ogbg-molbace: 0.615751 val loss: 1.838299
[Epoch 54] ogbg-molbace: 0.652234 test loss: 2.796202
[Epoch 55; Iter     6/   41] train: loss: 0.0744556
[Epoch 55; Iter    36/   41] train: loss: 0.0065480
[Epoch 55] ogbg-molbace: 0.649817 val loss: 1.637161
[Epoch 55] ogbg-molbace: 0.689793 test loss: 2.854518
[Epoch 56; Iter    25/   41] train: loss: 0.0065437
[Epoch 56] ogbg-molbace: 0.643223 val loss: 1.524572
[Epoch 56] ogbg-molbace: 0.710137 test loss: 2.004131
[Epoch 57; Iter    14/   41] train: loss: 0.0038969
[Epoch 57] ogbg-molbace: 0.636996 val loss: 1.490823
[Epoch 57] ogbg-molbace: 0.713963 test loss: 2.062255
[Epoch 58; Iter     3/   41] train: loss: 0.0020784
[Epoch 58; Iter    33/   41] train: loss: 0.0092596
[Epoch 58] ogbg-molbace: 0.658242 val loss: 1.497763
[Epoch 58] ogbg-molbace: 0.715354 test loss: 2.348928
[Epoch 59; Iter    22/   41] train: loss: 0.0048301
[Epoch 59] ogbg-molbace: 0.675092 val loss: 1.801694
[Epoch 59] ogbg-molbace: 0.686837 test loss: 3.140926
[Epoch 60; Iter    11/   41] train: loss: 0.0119420
[Epoch 60; Iter    41/   41] train: loss: 0.0011675
[Epoch 60] ogbg-molbace: 0.662637 val loss: 1.286509
[Epoch 60] ogbg-molbace: 0.707007 test loss: 2.205514
[Epoch 61; Iter    30/   41] train: loss: 0.0013992
[Epoch 61] ogbg-molbace: 0.678022 val loss: 1.272525
[Epoch 61] ogbg-molbace: 0.738306 test loss: 1.809355
[Epoch 62; Iter    19/   41] train: loss: 0.0008904
[Epoch 62] ogbg-molbace: 0.664469 val loss: 1.307826
[Epoch 62] ogbg-molbace: 0.724396 test loss: 2.190114
[Epoch 63; Iter     8/   41] train: loss: 0.0009356
[Epoch 63; Iter    38/   41] train: loss: 0.0008287
[Epoch 63] ogbg-molbace: 0.654945 val loss: 1.422951
[Epoch 63] ogbg-molbace: 0.715354 test loss: 2.300827
[Epoch 64; Iter    27/   41] train: loss: 0.0007253
[Epoch 64] ogbg-molbace: 0.655678 val loss: 1.420099
[Epoch 64] ogbg-molbace: 0.708920 test loss: 2.090420
[Epoch 65; Iter    16/   41] train: loss: 0.0008888
[Epoch 65] ogbg-molbace: 0.658608 val loss: 1.406272
[Epoch 65] ogbg-molbace: 0.727004 test loss: 2.116933
[Epoch 66; Iter     5/   41] train: loss: 0.0020212
[Epoch 66; Iter    35/   41] train: loss: 0.0091801
[Epoch 66] ogbg-molbace: 0.657509 val loss: 1.450546
[Epoch 66] ogbg-molbace: 0.731699 test loss: 2.005526
[Epoch 67; Iter    24/   41] train: loss: 0.0027696
[Epoch 67] ogbg-molbace: 0.662637 val loss: 1.826140
[Epoch 67] ogbg-molbace: 0.731003 test loss: 2.244182
[Epoch 68; Iter    13/   41] train: loss: 0.0010504
[Epoch 68] ogbg-molbace: 0.647253 val loss: 1.610718
[Epoch 68] ogbg-molbace: 0.707007 test loss: 2.186369
[Epoch 69; Iter     2/   41] train: loss: 0.0023170
[Epoch 69; Iter    32/   41] train: loss: 0.0044222
[Epoch 69] ogbg-molbace: 0.659707 val loss: 1.585749
[Epoch 69] ogbg-molbace: 0.713267 test loss: 2.149477
[Epoch 70; Iter    21/   41] train: loss: 0.0004996
[Epoch 70] ogbg-molbace: 0.663370 val loss: 1.786993
[Epoch 70] ogbg-molbace: 0.715528 test loss: 2.668080
[Epoch 71; Iter    10/   41] train: loss: 0.0017632
[Epoch 71; Iter    40/   41] train: loss: 0.0017527
[Epoch 71] ogbg-molbace: 0.673993 val loss: 1.554180
[Epoch 71] ogbg-molbace: 0.736568 test loss: 2.258163
[Epoch 72; Iter    29/   41] train: loss: 0.0005266
[Epoch 72] ogbg-molbace: 0.669597 val loss: 1.555557
[Epoch 72] ogbg-molbace: 0.733959 test loss: 2.308543
[Epoch 73; Iter    18/   41] train: loss: 0.0002115
[Epoch 73] ogbg-molbace: 0.666667 val loss: 1.578117
[Epoch 73] ogbg-molbace: 0.726135 test loss: 2.304147
[Epoch 74; Iter     7/   41] train: loss: 0.0026722
[Epoch 74; Iter    37/   41] train: loss: 0.0005243
[Epoch 74] ogbg-molbace: 0.672161 val loss: 1.618680
[Epoch 74] ogbg-molbace: 0.738480 test loss: 2.345796
[Epoch 75; Iter    26/   41] train: loss: 0.0002444
[Epoch 75] ogbg-molbace: 0.675092 val loss: 1.618963
[Epoch 75] ogbg-molbace: 0.738132 test loss: 2.383735
[Epoch 76; Iter    15/   41] train: loss: 0.0002683
[Epoch 76] ogbg-molbace: 0.683150 val loss: 1.470635
[Epoch 76] ogbg-molbace: 0.743175 test loss: 2.205488
[Epoch 77; Iter     4/   41] train: loss: 0.0006778
[Epoch 77; Iter    34/   41] train: loss: 0.0002840
[Epoch 77] ogbg-molbace: 0.675092 val loss: 1.535410
[Epoch 77] ogbg-molbace: 0.743001 test loss: 2.158827
[Epoch 78; Iter    23/   41] train: loss: 0.0100386
[Epoch 32] ogbg-molbace: 0.786472 test loss: 0.865708
[Epoch 33; Iter     8/   41] train: loss: 0.4011537
[Epoch 33; Iter    38/   41] train: loss: 0.4346235
[Epoch 33] ogbg-molbace: 0.671062 val loss: 1.336811
[Epoch 33] ogbg-molbace: 0.766997 test loss: 1.580907
[Epoch 34; Iter    27/   41] train: loss: 0.5531488
[Epoch 34] ogbg-molbace: 0.586813 val loss: 0.733945
[Epoch 34] ogbg-molbace: 0.668753 test loss: 1.118514
[Epoch 35; Iter    16/   41] train: loss: 0.4017407
[Epoch 35] ogbg-molbace: 0.616117 val loss: 0.628075
[Epoch 35] ogbg-molbace: 0.689098 test loss: 1.051039
[Epoch 36; Iter     5/   41] train: loss: 0.3606474
[Epoch 36; Iter    35/   41] train: loss: 0.3945260
[Epoch 36] ogbg-molbace: 0.716484 val loss: 0.574674
[Epoch 36] ogbg-molbace: 0.712050 test loss: 0.893309
[Epoch 37; Iter    24/   41] train: loss: 0.3496150
[Epoch 37] ogbg-molbace: 0.525641 val loss: 3.150861
[Epoch 37] ogbg-molbace: 0.717093 test loss: 1.327267
[Epoch 38; Iter    13/   41] train: loss: 0.3054907
[Epoch 38] ogbg-molbace: 0.700733 val loss: 1.022441
[Epoch 38] ogbg-molbace: 0.710833 test loss: 1.812551
[Epoch 39; Iter     2/   41] train: loss: 0.4199135
[Epoch 39; Iter    32/   41] train: loss: 0.5470295
[Epoch 39] ogbg-molbace: 0.738462 val loss: 0.791773
[Epoch 39] ogbg-molbace: 0.790819 test loss: 0.891920
[Epoch 40; Iter    21/   41] train: loss: 0.1793578
[Epoch 40] ogbg-molbace: 0.728938 val loss: 1.122465
[Epoch 40] ogbg-molbace: 0.803512 test loss: 1.232381
[Epoch 41; Iter    10/   41] train: loss: 0.2794555
[Epoch 41; Iter    40/   41] train: loss: 0.4547746
[Epoch 41] ogbg-molbace: 0.768132 val loss: 0.933848
[Epoch 41] ogbg-molbace: 0.777604 test loss: 0.859135
[Epoch 42; Iter    29/   41] train: loss: 0.4636483
[Epoch 42] ogbg-molbace: 0.695971 val loss: 1.664194
[Epoch 42] ogbg-molbace: 0.752391 test loss: 2.264848
[Epoch 43; Iter    18/   41] train: loss: 0.2424221
[Epoch 43] ogbg-molbace: 0.649817 val loss: 1.115275
[Epoch 43] ogbg-molbace: 0.804208 test loss: 1.268912
[Epoch 44; Iter     7/   41] train: loss: 0.4067841
[Epoch 44; Iter    37/   41] train: loss: 0.3105349
[Epoch 44] ogbg-molbace: 0.682418 val loss: 1.080409
[Epoch 44] ogbg-molbace: 0.784733 test loss: 1.341218
[Epoch 45; Iter    26/   41] train: loss: 0.1497548
[Epoch 45] ogbg-molbace: 0.765934 val loss: 0.743577
[Epoch 45] ogbg-molbace: 0.815336 test loss: 1.219600
[Epoch 46; Iter    15/   41] train: loss: 0.2673305
[Epoch 46] ogbg-molbace: 0.586447 val loss: 1.293470
[Epoch 46] ogbg-molbace: 0.767693 test loss: 1.141645
[Epoch 47; Iter     4/   41] train: loss: 0.1458429
[Epoch 47; Iter    34/   41] train: loss: 0.1142899
[Epoch 47] ogbg-molbace: 0.669231 val loss: 1.833759
[Epoch 47] ogbg-molbace: 0.784211 test loss: 1.871786
[Epoch 48; Iter    23/   41] train: loss: 0.1780637
[Epoch 48] ogbg-molbace: 0.618315 val loss: 1.359244
[Epoch 48] ogbg-molbace: 0.739871 test loss: 0.851017
[Epoch 49; Iter    12/   41] train: loss: 0.1670691
[Epoch 49] ogbg-molbace: 0.624542 val loss: 1.170557
[Epoch 49] ogbg-molbace: 0.779864 test loss: 1.164987
[Epoch 50; Iter     1/   41] train: loss: 0.1436955
[Epoch 50; Iter    31/   41] train: loss: 0.0688670
[Epoch 50] ogbg-molbace: 0.685348 val loss: 1.529538
[Epoch 50] ogbg-molbace: 0.803512 test loss: 0.945855
[Epoch 51; Iter    20/   41] train: loss: 0.0370827
[Epoch 51] ogbg-molbace: 0.624908 val loss: 1.076623
[Epoch 51] ogbg-molbace: 0.791688 test loss: 1.376472
[Epoch 52; Iter     9/   41] train: loss: 0.1654130
[Epoch 52; Iter    39/   41] train: loss: 0.0781798
[Epoch 52] ogbg-molbace: 0.619048 val loss: 1.274955
[Epoch 52] ogbg-molbace: 0.788385 test loss: 1.652603
[Epoch 53; Iter    28/   41] train: loss: 0.0202386
[Epoch 53] ogbg-molbace: 0.680220 val loss: 1.334408
[Epoch 53] ogbg-molbace: 0.794471 test loss: 1.255247
[Epoch 54; Iter    17/   41] train: loss: 0.0131856
[Epoch 54] ogbg-molbace: 0.682784 val loss: 1.415683
[Epoch 54] ogbg-molbace: 0.740393 test loss: 1.429751
[Epoch 55; Iter     6/   41] train: loss: 0.0926222
[Epoch 55; Iter    36/   41] train: loss: 0.0035285
[Epoch 55] ogbg-molbace: 0.717949 val loss: 1.252056
[Epoch 55] ogbg-molbace: 0.798991 test loss: 1.387477
[Epoch 56; Iter    25/   41] train: loss: 0.0124195
[Epoch 56] ogbg-molbace: 0.674359 val loss: 1.618375
[Epoch 56] ogbg-molbace: 0.622674 test loss: 2.429820
[Epoch 57; Iter    14/   41] train: loss: 0.0089434
[Epoch 57] ogbg-molbace: 0.679121 val loss: 1.706390
[Epoch 57] ogbg-molbace: 0.793775 test loss: 0.975371
[Epoch 58; Iter     3/   41] train: loss: 0.0060949
[Epoch 58; Iter    33/   41] train: loss: 0.0723360
[Epoch 58] ogbg-molbace: 0.643223 val loss: 1.942779
[Epoch 58] ogbg-molbace: 0.720049 test loss: 2.540724
[Epoch 59; Iter    22/   41] train: loss: 0.2106815
[Epoch 59] ogbg-molbace: 0.652381 val loss: 2.655950
[Epoch 59] ogbg-molbace: 0.738654 test loss: 2.519969
[Epoch 60; Iter    11/   41] train: loss: 0.1679209
[Epoch 60; Iter    41/   41] train: loss: 0.2945016
[Epoch 60] ogbg-molbace: 0.704029 val loss: 1.378637
[Epoch 60] ogbg-molbace: 0.796209 test loss: 1.469359
[Epoch 61; Iter    30/   41] train: loss: 0.0709559
[Epoch 61] ogbg-molbace: 0.688645 val loss: 2.480046
[Epoch 61] ogbg-molbace: 0.819857 test loss: 1.855616
[Epoch 62; Iter    19/   41] train: loss: 0.0457240
[Epoch 62] ogbg-molbace: 0.664103 val loss: 1.294439
[Epoch 62] ogbg-molbace: 0.846288 test loss: 1.489706
[Epoch 63; Iter     8/   41] train: loss: 0.0401309
[Epoch 63; Iter    38/   41] train: loss: 0.0546021
[Epoch 63] ogbg-molbace: 0.620513 val loss: 1.928302
[Epoch 63] ogbg-molbace: 0.744045 test loss: 1.705265
[Epoch 64; Iter    27/   41] train: loss: 0.0563041
[Epoch 64] ogbg-molbace: 0.676557 val loss: 1.210554
[Epoch 64] ogbg-molbace: 0.794992 test loss: 1.996005
[Epoch 65; Iter    16/   41] train: loss: 0.0226531
[Epoch 65] ogbg-molbace: 0.669231 val loss: 1.916127
[Epoch 65] ogbg-molbace: 0.714832 test loss: 2.815072
[Epoch 66; Iter     5/   41] train: loss: 0.0134817
[Epoch 66; Iter    35/   41] train: loss: 0.1394330
[Epoch 66] ogbg-molbace: 0.704396 val loss: 1.761482
[Epoch 66] ogbg-molbace: 0.775343 test loss: 1.412382
[Epoch 67; Iter    24/   41] train: loss: 0.0771552
[Epoch 67] ogbg-molbace: 0.698535 val loss: 1.377839
[Epoch 67] ogbg-molbace: 0.767866 test loss: 1.476981
[Epoch 68; Iter    13/   41] train: loss: 0.0102801
[Epoch 68] ogbg-molbace: 0.659707 val loss: 1.848036
[Epoch 68] ogbg-molbace: 0.723700 test loss: 2.041785
[Epoch 69; Iter     2/   41] train: loss: 0.0386288
[Epoch 69; Iter    32/   41] train: loss: 0.0952070
[Epoch 69] ogbg-molbace: 0.677289 val loss: 1.654020
[Epoch 69] ogbg-molbace: 0.692923 test loss: 2.597683
[Epoch 70; Iter    21/   41] train: loss: 0.0106642
[Epoch 70] ogbg-molbace: 0.667033 val loss: 2.081689
[Epoch 70] ogbg-molbace: 0.688750 test loss: 3.164710
[Epoch 71; Iter    10/   41] train: loss: 0.1109080
[Epoch 71; Iter    40/   41] train: loss: 0.0911131
[Epoch 71] ogbg-molbace: 0.662271 val loss: 2.061179
[Epoch 71] ogbg-molbace: 0.679360 test loss: 2.985135
[Epoch 72; Iter    29/   41] train: loss: 0.0373459
[Epoch 72] ogbg-molbace: 0.649451 val loss: 2.234338
[Epoch 72] ogbg-molbace: 0.753086 test loss: 2.593428
[Epoch 73; Iter    18/   41] train: loss: 0.0048550
[Epoch 73] ogbg-molbace: 0.683150 val loss: 1.558345
[Epoch 73] ogbg-molbace: 0.719527 test loss: 2.166562
[Epoch 74; Iter     7/   41] train: loss: 0.0088696
[Epoch 74; Iter    37/   41] train: loss: 0.0075710
[Epoch 74] ogbg-molbace: 0.661172 val loss: 2.113577
[Epoch 74] ogbg-molbace: 0.737785 test loss: 2.268401
[Epoch 75; Iter    26/   41] train: loss: 0.0015493
[Epoch 75] ogbg-molbace: 0.669231 val loss: 2.074629
[Epoch 75] ogbg-molbace: 0.729438 test loss: 2.093904
[Epoch 76; Iter    15/   41] train: loss: 0.0015381
[Epoch 76] ogbg-molbace: 0.673260 val loss: 1.845718
[Epoch 76] ogbg-molbace: 0.752217 test loss: 1.663153
[Epoch 77; Iter     4/   41] train: loss: 0.0029945
[Epoch 77; Iter    34/   41] train: loss: 0.0009217
[Epoch 77] ogbg-molbace: 0.671795 val loss: 1.871298
[Epoch 77] ogbg-molbace: 0.725439 test loss: 2.005917
[Epoch 78; Iter    23/   41] train: loss: 0.0383312
[Epoch 32] ogbg-molbace: 0.719527 test loss: 1.248638
[Epoch 33; Iter     8/   41] train: loss: 0.5879255
[Epoch 33; Iter    38/   41] train: loss: 0.5607612
[Epoch 33] ogbg-molbace: 0.609524 val loss: 1.448940
[Epoch 33] ogbg-molbace: 0.762998 test loss: 1.474734
[Epoch 34; Iter    27/   41] train: loss: 0.3838566
[Epoch 34] ogbg-molbace: 0.587912 val loss: 1.204629
[Epoch 34] ogbg-molbace: 0.757955 test loss: 1.214057
[Epoch 35; Iter    16/   41] train: loss: 0.4836779
[Epoch 35] ogbg-molbace: 0.602198 val loss: 1.153830
[Epoch 35] ogbg-molbace: 0.725961 test loss: 1.157274
[Epoch 36; Iter     5/   41] train: loss: 0.4494362
[Epoch 36; Iter    35/   41] train: loss: 0.5106558
[Epoch 36] ogbg-molbace: 0.595971 val loss: 1.377456
[Epoch 36] ogbg-molbace: 0.631890 test loss: 1.646821
[Epoch 37; Iter    24/   41] train: loss: 0.3603519
[Epoch 37] ogbg-molbace: 0.609158 val loss: 0.685454
[Epoch 37] ogbg-molbace: 0.649452 test loss: 0.960599
[Epoch 38; Iter    13/   41] train: loss: 0.3513344
[Epoch 38] ogbg-molbace: 0.579121 val loss: 0.808782
[Epoch 38] ogbg-molbace: 0.740045 test loss: 0.972764
[Epoch 39; Iter     2/   41] train: loss: 0.4109002
[Epoch 39; Iter    32/   41] train: loss: 0.2946303
[Epoch 39] ogbg-molbace: 0.645421 val loss: 0.677986
[Epoch 39] ogbg-molbace: 0.655712 test loss: 1.008837
[Epoch 40; Iter    21/   41] train: loss: 0.2188270
[Epoch 40] ogbg-molbace: 0.551282 val loss: 0.759733
[Epoch 40] ogbg-molbace: 0.617979 test loss: 0.932454
[Epoch 41; Iter    10/   41] train: loss: 0.1513941
[Epoch 41; Iter    40/   41] train: loss: 0.2242184
[Epoch 41] ogbg-molbace: 0.620879 val loss: 0.825672
[Epoch 41] ogbg-molbace: 0.694314 test loss: 1.204893
[Epoch 42; Iter    29/   41] train: loss: 0.2900847
[Epoch 42] ogbg-molbace: 0.636996 val loss: 0.974712
[Epoch 42] ogbg-molbace: 0.728047 test loss: 0.921135
[Epoch 43; Iter    18/   41] train: loss: 0.2950744
[Epoch 43] ogbg-molbace: 0.623810 val loss: 0.909782
[Epoch 43] ogbg-molbace: 0.735872 test loss: 0.706539
[Epoch 44; Iter     7/   41] train: loss: 0.2786839
[Epoch 44; Iter    37/   41] train: loss: 0.2844791
[Epoch 44] ogbg-molbace: 0.572894 val loss: 1.090283
[Epoch 44] ogbg-molbace: 0.643019 test loss: 2.350942
[Epoch 45; Iter    26/   41] train: loss: 0.1042493
[Epoch 45] ogbg-molbace: 0.638462 val loss: 0.842878
[Epoch 45] ogbg-molbace: 0.636411 test loss: 1.927689
[Epoch 46; Iter    15/   41] train: loss: 0.0377592
[Epoch 46] ogbg-molbace: 0.628571 val loss: 1.117570
[Epoch 46] ogbg-molbace: 0.654321 test loss: 4.622009
[Epoch 47; Iter     4/   41] train: loss: 0.0679970
[Epoch 47; Iter    34/   41] train: loss: 0.0453074
[Epoch 47] ogbg-molbace: 0.623077 val loss: 1.039589
[Epoch 47] ogbg-molbace: 0.673970 test loss: 5.363637
[Epoch 48; Iter    23/   41] train: loss: 0.0217710
[Epoch 48] ogbg-molbace: 0.612454 val loss: 0.996744
[Epoch 48] ogbg-molbace: 0.695879 test loss: 3.370028
[Epoch 49; Iter    12/   41] train: loss: 0.0158307
[Epoch 49] ogbg-molbace: 0.615018 val loss: 1.025559
[Epoch 49] ogbg-molbace: 0.685794 test loss: 3.838580
[Epoch 50; Iter     1/   41] train: loss: 0.0077831
[Epoch 50; Iter    31/   41] train: loss: 0.0062278
[Epoch 50] ogbg-molbace: 0.587546 val loss: 1.184464
[Epoch 50] ogbg-molbace: 0.663885 test loss: 3.990842
[Epoch 51; Iter    20/   41] train: loss: 0.0149181
[Epoch 51] ogbg-molbace: 0.601832 val loss: 1.104554
[Epoch 51] ogbg-molbace: 0.677274 test loss: 3.324750
[Epoch 52; Iter     9/   41] train: loss: 0.0098423
[Epoch 52; Iter    39/   41] train: loss: 0.0032875
[Epoch 52] ogbg-molbace: 0.603297 val loss: 1.170599
[Epoch 52] ogbg-molbace: 0.669275 test loss: 3.116071
[Epoch 53; Iter    28/   41] train: loss: 0.0031387
[Epoch 53] ogbg-molbace: 0.599634 val loss: 1.165432
[Epoch 53] ogbg-molbace: 0.670492 test loss: 3.338731
[Epoch 54; Iter    17/   41] train: loss: 0.0051316
[Epoch 54] ogbg-molbace: 0.605861 val loss: 1.199037
[Epoch 54] ogbg-molbace: 0.677621 test loss: 3.278247
[Epoch 55; Iter     6/   41] train: loss: 0.0076631
[Epoch 55; Iter    36/   41] train: loss: 0.0028549
[Epoch 55] ogbg-molbace: 0.602198 val loss: 1.203531
[Epoch 55] ogbg-molbace: 0.680403 test loss: 2.677006
[Epoch 56; Iter    25/   41] train: loss: 0.0079818
[Epoch 56] ogbg-molbace: 0.596703 val loss: 1.192639
[Epoch 56] ogbg-molbace: 0.674839 test loss: 3.134463
[Epoch 57; Iter    14/   41] train: loss: 0.0036169
[Epoch 57] ogbg-molbace: 0.597070 val loss: 1.255339
[Epoch 57] ogbg-molbace: 0.675535 test loss: 3.062370
[Epoch 58; Iter     3/   41] train: loss: 0.0018104
[Epoch 58; Iter    33/   41] train: loss: 0.0045692
[Epoch 58] ogbg-molbace: 0.606960 val loss: 1.378588
[Epoch 58] ogbg-molbace: 0.679186 test loss: 2.669057
[Epoch 59; Iter    22/   41] train: loss: 0.0021681
[Epoch 59] ogbg-molbace: 0.607326 val loss: 1.242973
[Epoch 59] ogbg-molbace: 0.675187 test loss: 2.721166
[Epoch 60; Iter    11/   41] train: loss: 0.0027570
[Epoch 60; Iter    41/   41] train: loss: 0.0012668
[Epoch 60] ogbg-molbace: 0.616117 val loss: 1.197228
[Epoch 60] ogbg-molbace: 0.661276 test loss: 2.381888
[Epoch 61; Iter    30/   41] train: loss: 0.0109479
[Epoch 61] ogbg-molbace: 0.615385 val loss: 1.197255
[Epoch 61] ogbg-molbace: 0.678665 test loss: 2.824567
[Epoch 62; Iter    19/   41] train: loss: 0.0015560
[Epoch 62] ogbg-molbace: 0.612454 val loss: 1.345321
[Epoch 62] ogbg-molbace: 0.685620 test loss: 3.534354
[Epoch 63; Iter     8/   41] train: loss: 0.0132014
[Epoch 63; Iter    38/   41] train: loss: 0.0260705
[Epoch 63] ogbg-molbace: 0.590842 val loss: 1.836267
[Epoch 63] ogbg-molbace: 0.630673 test loss: 8.181429
[Epoch 64; Iter    27/   41] train: loss: 0.5308270
[Epoch 64] ogbg-molbace: 0.646520 val loss: 1.906666
[Epoch 64] ogbg-molbace: 0.721788 test loss: 2.639367
[Epoch 65; Iter    16/   41] train: loss: 0.3935612
[Epoch 65] ogbg-molbace: 0.586081 val loss: 0.620349
[Epoch 65] ogbg-molbace: 0.640584 test loss: 1.752068
[Epoch 66; Iter     5/   41] train: loss: 0.3561732
[Epoch 66; Iter    35/   41] train: loss: 0.1863888
[Epoch 66] ogbg-molbace: 0.626007 val loss: 0.596625
[Epoch 66] ogbg-molbace: 0.693271 test loss: 1.321843
[Epoch 67; Iter    24/   41] train: loss: 0.2024965
[Epoch 67] ogbg-molbace: 0.630769 val loss: 1.205724
[Epoch 67] ogbg-molbace: 0.748913 test loss: 1.259567
[Epoch 68; Iter    13/   41] train: loss: 0.1141764
[Epoch 68] ogbg-molbace: 0.656044 val loss: 1.294910
[Epoch 68] ogbg-molbace: 0.742480 test loss: 1.506556
[Epoch 69; Iter     2/   41] train: loss: 0.0267135
[Epoch 69; Iter    32/   41] train: loss: 0.0735705
[Epoch 69] ogbg-molbace: 0.649084 val loss: 0.927824
[Epoch 69] ogbg-molbace: 0.732568 test loss: 2.952232
[Epoch 70; Iter    21/   41] train: loss: 0.0273539
[Epoch 70] ogbg-molbace: 0.613553 val loss: 0.954517
[Epoch 70] ogbg-molbace: 0.665971 test loss: 3.575596
[Epoch 71; Iter    10/   41] train: loss: 0.0159952
[Epoch 71; Iter    40/   41] train: loss: 0.0220311
[Epoch 71] ogbg-molbace: 0.665934 val loss: 0.780097
[Epoch 71] ogbg-molbace: 0.697444 test loss: 3.138193
[Epoch 72; Iter    29/   41] train: loss: 0.0294061
[Epoch 72] ogbg-molbace: 0.639560 val loss: 0.958841
[Epoch 72] ogbg-molbace: 0.694488 test loss: 3.363658
[Epoch 73; Iter    18/   41] train: loss: 0.0094637
[Epoch 73] ogbg-molbace: 0.604762 val loss: 1.084222
[Epoch 73] ogbg-molbace: 0.668753 test loss: 5.104250
[Epoch 74; Iter     7/   41] train: loss: 0.0214557
[Epoch 74; Iter    37/   41] train: loss: 0.0063150
[Epoch 74] ogbg-molbace: 0.623077 val loss: 0.978857
[Epoch 74] ogbg-molbace: 0.701791 test loss: 3.286339
[Epoch 75; Iter    26/   41] train: loss: 0.0041105
[Epoch 75] ogbg-molbace: 0.638462 val loss: 1.032333
[Epoch 75] ogbg-molbace: 0.721092 test loss: 2.882939
[Epoch 76; Iter    15/   41] train: loss: 0.0045687
[Epoch 76] ogbg-molbace: 0.641026 val loss: 1.144738
[Epoch 76] ogbg-molbace: 0.723352 test loss: 2.660434
[Epoch 77; Iter     4/   41] train: loss: 0.0027774
[Epoch 77; Iter    34/   41] train: loss: 0.0040252
[Epoch 77] ogbg-molbace: 0.636264 val loss: 1.163035
[Epoch 77] ogbg-molbace: 0.716397 test loss: 2.461320
[Epoch 78; Iter    23/   41] train: loss: 0.0028323
[Epoch 32] ogbg-molbace: 0.781429 test loss: 1.005066
[Epoch 33; Iter     8/   41] train: loss: 0.4574844
[Epoch 33; Iter    38/   41] train: loss: 0.4673090
[Epoch 33] ogbg-molbace: 0.705495 val loss: 0.875252
[Epoch 33] ogbg-molbace: 0.783168 test loss: 1.004741
[Epoch 34; Iter    27/   41] train: loss: 0.3877441
[Epoch 34] ogbg-molbace: 0.665934 val loss: 1.074124
[Epoch 34] ogbg-molbace: 0.766128 test loss: 1.242856
[Epoch 35; Iter    16/   41] train: loss: 0.3599502
[Epoch 35] ogbg-molbace: 0.740293 val loss: 0.850414
[Epoch 35] ogbg-molbace: 0.805251 test loss: 0.966039
[Epoch 36; Iter     5/   41] train: loss: 0.3929058
[Epoch 36; Iter    35/   41] train: loss: 0.3954563
[Epoch 36] ogbg-molbace: 0.727473 val loss: 0.984008
[Epoch 36] ogbg-molbace: 0.812380 test loss: 1.440448
[Epoch 37; Iter    24/   41] train: loss: 0.3709195
[Epoch 37] ogbg-molbace: 0.638828 val loss: 0.782215
[Epoch 37] ogbg-molbace: 0.756564 test loss: 0.731544
[Epoch 38; Iter    13/   41] train: loss: 0.3206551
[Epoch 38] ogbg-molbace: 0.579487 val loss: 1.078626
[Epoch 38] ogbg-molbace: 0.724048 test loss: 1.218053
[Epoch 39; Iter     2/   41] train: loss: 0.3824117
[Epoch 39; Iter    32/   41] train: loss: 0.4821406
[Epoch 39] ogbg-molbace: 0.662637 val loss: 0.738426
[Epoch 39] ogbg-molbace: 0.696922 test loss: 1.094470
[Epoch 40; Iter    21/   41] train: loss: 0.2142264
[Epoch 40] ogbg-molbace: 0.546520 val loss: 1.641711
[Epoch 40] ogbg-molbace: 0.685794 test loss: 2.000530
[Epoch 41; Iter    10/   41] train: loss: 0.2995096
[Epoch 41; Iter    40/   41] train: loss: 0.3170116
[Epoch 41] ogbg-molbace: 0.695604 val loss: 0.955813
[Epoch 41] ogbg-molbace: 0.679186 test loss: 1.402811
[Epoch 42; Iter    29/   41] train: loss: 0.4381865
[Epoch 42] ogbg-molbace: 0.631868 val loss: 1.150054
[Epoch 42] ogbg-molbace: 0.808381 test loss: 1.708595
[Epoch 43; Iter    18/   41] train: loss: 0.3095991
[Epoch 43] ogbg-molbace: 0.696703 val loss: 0.817613
[Epoch 43] ogbg-molbace: 0.782299 test loss: 1.085797
[Epoch 44; Iter     7/   41] train: loss: 0.2169565
[Epoch 44; Iter    37/   41] train: loss: 0.2781783
[Epoch 44] ogbg-molbace: 0.606227 val loss: 0.734723
[Epoch 44] ogbg-molbace: 0.713267 test loss: 1.063989
[Epoch 45; Iter    26/   41] train: loss: 0.1510618
[Epoch 45] ogbg-molbace: 0.697070 val loss: 1.095555
[Epoch 45] ogbg-molbace: 0.809424 test loss: 1.059898
[Epoch 46; Iter    15/   41] train: loss: 0.2644621
[Epoch 46] ogbg-molbace: 0.707326 val loss: 0.988646
[Epoch 46] ogbg-molbace: 0.753434 test loss: 1.118951
[Epoch 47; Iter     4/   41] train: loss: 0.1375419
[Epoch 47; Iter    34/   41] train: loss: 0.2911301
[Epoch 47] ogbg-molbace: 0.602198 val loss: 1.623063
[Epoch 47] ogbg-molbace: 0.697792 test loss: 1.773071
[Epoch 48; Iter    23/   41] train: loss: 0.1403878
[Epoch 48] ogbg-molbace: 0.676923 val loss: 1.504047
[Epoch 48] ogbg-molbace: 0.776039 test loss: 1.520672
[Epoch 49; Iter    12/   41] train: loss: 0.1154231
[Epoch 49] ogbg-molbace: 0.635165 val loss: 1.264934
[Epoch 49] ogbg-molbace: 0.675361 test loss: 1.726848
[Epoch 50; Iter     1/   41] train: loss: 0.0945154
[Epoch 50; Iter    31/   41] train: loss: 0.0722903
[Epoch 50] ogbg-molbace: 0.624542 val loss: 2.027134
[Epoch 50] ogbg-molbace: 0.787341 test loss: 1.721439
[Epoch 51; Iter    20/   41] train: loss: 0.1009878
[Epoch 51] ogbg-molbace: 0.614286 val loss: 2.072260
[Epoch 51] ogbg-molbace: 0.741784 test loss: 2.262816
[Epoch 52; Iter     9/   41] train: loss: 0.0971288
[Epoch 52; Iter    39/   41] train: loss: 0.1704084
[Epoch 52] ogbg-molbace: 0.668132 val loss: 1.452636
[Epoch 52] ogbg-molbace: 0.720049 test loss: 2.244273
[Epoch 53; Iter    28/   41] train: loss: 0.0575860
[Epoch 53] ogbg-molbace: 0.669597 val loss: 1.579239
[Epoch 53] ogbg-molbace: 0.785081 test loss: 1.429690
[Epoch 54; Iter    17/   41] train: loss: 0.0490354
[Epoch 54] ogbg-molbace: 0.657875 val loss: 1.597213
[Epoch 54] ogbg-molbace: 0.763867 test loss: 1.536476
[Epoch 55; Iter     6/   41] train: loss: 0.0519523
[Epoch 55; Iter    36/   41] train: loss: 0.0524388
[Epoch 55] ogbg-molbace: 0.676923 val loss: 1.745591
[Epoch 55] ogbg-molbace: 0.783690 test loss: 1.630914
[Epoch 56; Iter    25/   41] train: loss: 0.0291866
[Epoch 56] ogbg-molbace: 0.654945 val loss: 1.955796
[Epoch 56] ogbg-molbace: 0.777952 test loss: 1.657479
[Epoch 57; Iter    14/   41] train: loss: 0.0157704
[Epoch 57] ogbg-molbace: 0.651648 val loss: 2.126043
[Epoch 57] ogbg-molbace: 0.774996 test loss: 2.512721
[Epoch 58; Iter     3/   41] train: loss: 0.0184442
[Epoch 58; Iter    33/   41] train: loss: 0.0267915
[Epoch 58] ogbg-molbace: 0.685714 val loss: 2.476063
[Epoch 58] ogbg-molbace: 0.776908 test loss: 2.815302
[Epoch 59; Iter    22/   41] train: loss: 0.0101835
[Epoch 59] ogbg-molbace: 0.665934 val loss: 2.100648
[Epoch 59] ogbg-molbace: 0.743871 test loss: 1.688842
[Epoch 60; Iter    11/   41] train: loss: 0.0181663
[Epoch 60; Iter    41/   41] train: loss: 0.0080804
[Epoch 60] ogbg-molbace: 0.678388 val loss: 2.406179
[Epoch 60] ogbg-molbace: 0.762824 test loss: 1.394035
[Epoch 61; Iter    30/   41] train: loss: 0.0562001
[Epoch 61] ogbg-molbace: 0.652381 val loss: 2.171272
[Epoch 61] ogbg-molbace: 0.704225 test loss: 3.574703
[Epoch 62; Iter    19/   41] train: loss: 0.0455770
[Epoch 62] ogbg-molbace: 0.669231 val loss: 2.031017
[Epoch 62] ogbg-molbace: 0.729786 test loss: 1.778557
[Epoch 63; Iter     8/   41] train: loss: 0.5175335
[Epoch 63; Iter    38/   41] train: loss: 0.1620885
[Epoch 63] ogbg-molbace: 0.639560 val loss: 1.969595
[Epoch 63] ogbg-molbace: 0.763867 test loss: 2.499606
[Epoch 64; Iter    27/   41] train: loss: 0.1746512
[Epoch 64] ogbg-molbace: 0.647253 val loss: 2.004490
[Epoch 64] ogbg-molbace: 0.717614 test loss: 2.048835
[Epoch 65; Iter    16/   41] train: loss: 0.0663062
[Epoch 65] ogbg-molbace: 0.592674 val loss: 1.668204
[Epoch 65] ogbg-molbace: 0.740045 test loss: 1.690326
[Epoch 66; Iter     5/   41] train: loss: 0.1552248
[Epoch 66; Iter    35/   41] train: loss: 0.0791296
[Epoch 66] ogbg-molbace: 0.638828 val loss: 1.689879
[Epoch 66] ogbg-molbace: 0.773083 test loss: 1.395709
[Epoch 67; Iter    24/   41] train: loss: 0.0352696
[Epoch 67] ogbg-molbace: 0.686447 val loss: 1.896623
[Epoch 67] ogbg-molbace: 0.764215 test loss: 1.764052
[Epoch 68; Iter    13/   41] train: loss: 0.1457711
[Epoch 68] ogbg-molbace: 0.636264 val loss: 1.625583
[Epoch 68] ogbg-molbace: 0.782299 test loss: 1.773691
[Epoch 69; Iter     2/   41] train: loss: 0.0221711
[Epoch 69; Iter    32/   41] train: loss: 0.0173244
[Epoch 69] ogbg-molbace: 0.632601 val loss: 2.139170
[Epoch 69] ogbg-molbace: 0.770127 test loss: 1.916311
[Epoch 70; Iter    21/   41] train: loss: 0.0385053
[Epoch 70] ogbg-molbace: 0.632234 val loss: 1.876039
[Epoch 70] ogbg-molbace: 0.764910 test loss: 1.723279
[Epoch 71; Iter    10/   41] train: loss: 0.0039342
[Epoch 71; Iter    40/   41] train: loss: 0.0053638
[Epoch 71] ogbg-molbace: 0.654579 val loss: 1.949615
[Epoch 71] ogbg-molbace: 0.740567 test loss: 1.757157
[Epoch 72; Iter    29/   41] train: loss: 0.0157646
[Epoch 72] ogbg-molbace: 0.641026 val loss: 1.926776
[Epoch 72] ogbg-molbace: 0.735524 test loss: 1.977610
[Epoch 73; Iter    18/   41] train: loss: 0.0153299
[Epoch 73] ogbg-molbace: 0.623443 val loss: 2.120188
[Epoch 73] ogbg-molbace: 0.702139 test loss: 2.643986
[Epoch 74; Iter     7/   41] train: loss: 0.0106724
[Epoch 74; Iter    37/   41] train: loss: 0.0071290
[Epoch 74] ogbg-molbace: 0.657509 val loss: 2.713678
[Epoch 74] ogbg-molbace: 0.752565 test loss: 3.326565
[Epoch 75; Iter    26/   41] train: loss: 0.0049869
[Epoch 75] ogbg-molbace: 0.671795 val loss: 1.756348
[Epoch 75] ogbg-molbace: 0.734133 test loss: 2.809184
[Epoch 76; Iter    15/   41] train: loss: 0.0099798
[Epoch 76] ogbg-molbace: 0.639560 val loss: 2.271192
[Epoch 76] ogbg-molbace: 0.732220 test loss: 2.229221
[Epoch 77; Iter     4/   41] train: loss: 0.0049443
[Epoch 77; Iter    34/   41] train: loss: 0.0029603
[Epoch 77] ogbg-molbace: 0.644322 val loss: 2.276285
[Epoch 77] ogbg-molbace: 0.743349 test loss: 2.431646
[Epoch 78; Iter    23/   41] train: loss: 0.0027078
[Epoch 32] ogbg-molbace: 0.757607 test loss: 0.710204
[Epoch 33; Iter     8/   41] train: loss: 0.6500964
[Epoch 33; Iter    38/   41] train: loss: 0.6738394
[Epoch 33] ogbg-molbace: 0.660073 val loss: 0.663029
[Epoch 33] ogbg-molbace: 0.787863 test loss: 0.693173
[Epoch 34; Iter    27/   41] train: loss: 0.5521711
[Epoch 34] ogbg-molbace: 0.681685 val loss: 0.604542
[Epoch 34] ogbg-molbace: 0.783516 test loss: 0.655876
[Epoch 35; Iter    16/   41] train: loss: 0.4626167
[Epoch 35] ogbg-molbace: 0.612088 val loss: 0.538062
[Epoch 35] ogbg-molbace: 0.665449 test loss: 0.778042
[Epoch 36; Iter     5/   41] train: loss: 0.3962270
[Epoch 36; Iter    35/   41] train: loss: 0.3607287
[Epoch 36] ogbg-molbace: 0.643590 val loss: 0.534025
[Epoch 36] ogbg-molbace: 0.718832 test loss: 0.822157
[Epoch 37; Iter    24/   41] train: loss: 0.5258358
[Epoch 37] ogbg-molbace: 0.730403 val loss: 1.144129
[Epoch 37] ogbg-molbace: 0.780908 test loss: 1.135407
[Epoch 38; Iter    13/   41] train: loss: 0.4089098
[Epoch 38] ogbg-molbace: 0.677656 val loss: 0.918720
[Epoch 38] ogbg-molbace: 0.774648 test loss: 0.732508
[Epoch 39; Iter     2/   41] train: loss: 0.4223520
[Epoch 39; Iter    32/   41] train: loss: 0.3652216
[Epoch 39] ogbg-molbace: 0.624908 val loss: 0.789030
[Epoch 39] ogbg-molbace: 0.733612 test loss: 0.699559
[Epoch 40; Iter    21/   41] train: loss: 0.2837769
[Epoch 40] ogbg-molbace: 0.582051 val loss: 0.806148
[Epoch 40] ogbg-molbace: 0.608764 test loss: 1.284941
[Epoch 41; Iter    10/   41] train: loss: 0.5062724
[Epoch 41; Iter    40/   41] train: loss: 0.4989287
[Epoch 41] ogbg-molbace: 0.697802 val loss: 0.469087
[Epoch 41] ogbg-molbace: 0.764563 test loss: 0.805367
[Epoch 42; Iter    29/   41] train: loss: 0.3695911
[Epoch 42] ogbg-molbace: 0.668498 val loss: 1.109482
[Epoch 42] ogbg-molbace: 0.739697 test loss: 0.910715
[Epoch 43; Iter    18/   41] train: loss: 0.2853053
[Epoch 43] ogbg-molbace: 0.721245 val loss: 1.138647
[Epoch 43] ogbg-molbace: 0.702660 test loss: 1.272474
[Epoch 44; Iter     7/   41] train: loss: 0.3023426
[Epoch 44; Iter    37/   41] train: loss: 0.1819617
[Epoch 44] ogbg-molbace: 0.739194 val loss: 1.551979
[Epoch 44] ogbg-molbace: 0.815163 test loss: 1.383192
[Epoch 45; Iter    26/   41] train: loss: 0.2989268
[Epoch 45] ogbg-molbace: 0.717582 val loss: 1.430871
[Epoch 45] ogbg-molbace: 0.837246 test loss: 1.272094
[Epoch 46; Iter    15/   41] train: loss: 0.0916957
[Epoch 46] ogbg-molbace: 0.581685 val loss: 0.950065
[Epoch 46] ogbg-molbace: 0.753260 test loss: 1.039079
[Epoch 47; Iter     4/   41] train: loss: 0.1120607
[Epoch 47; Iter    34/   41] train: loss: 0.0523594
[Epoch 47] ogbg-molbace: 0.619780 val loss: 2.117761
[Epoch 47] ogbg-molbace: 0.663711 test loss: 1.902076
[Epoch 48; Iter    23/   41] train: loss: 0.0535678
[Epoch 48] ogbg-molbace: 0.615751 val loss: 1.184739
[Epoch 48] ogbg-molbace: 0.731525 test loss: 1.350664
[Epoch 49; Iter    12/   41] train: loss: 0.1471096
[Epoch 49] ogbg-molbace: 0.705861 val loss: 2.073921
[Epoch 49] ogbg-molbace: 0.792906 test loss: 1.664428
[Epoch 50; Iter     1/   41] train: loss: 0.1665166
[Epoch 50; Iter    31/   41] train: loss: 0.0855222
[Epoch 50] ogbg-molbace: 0.621612 val loss: 1.716686
[Epoch 50] ogbg-molbace: 0.721614 test loss: 1.224542
[Epoch 51; Iter    20/   41] train: loss: 0.1087556
[Epoch 51] ogbg-molbace: 0.636264 val loss: 0.868741
[Epoch 51] ogbg-molbace: 0.724570 test loss: 1.266059
[Epoch 52; Iter     9/   41] train: loss: 0.2323370
[Epoch 52; Iter    39/   41] train: loss: 0.1088659
[Epoch 52] ogbg-molbace: 0.654945 val loss: 2.159203
[Epoch 52] ogbg-molbace: 0.770649 test loss: 1.349085
[Epoch 53; Iter    28/   41] train: loss: 0.0112724
[Epoch 53] ogbg-molbace: 0.629670 val loss: 2.337036
[Epoch 53] ogbg-molbace: 0.762476 test loss: 1.428724
[Epoch 54; Iter    17/   41] train: loss: 0.0139661
[Epoch 54] ogbg-molbace: 0.671795 val loss: 2.713948
[Epoch 54] ogbg-molbace: 0.783168 test loss: 1.524265
[Epoch 55; Iter     6/   41] train: loss: 0.0079203
[Epoch 55; Iter    36/   41] train: loss: 0.0074347
[Epoch 55] ogbg-molbace: 0.656044 val loss: 1.845851
[Epoch 55] ogbg-molbace: 0.798644 test loss: 1.153657
[Epoch 56; Iter    25/   41] train: loss: 0.0087735
[Epoch 56] ogbg-molbace: 0.645421 val loss: 2.649072
[Epoch 56] ogbg-molbace: 0.783516 test loss: 1.585211
[Epoch 57; Iter    14/   41] train: loss: 0.0030986
[Epoch 57] ogbg-molbace: 0.641758 val loss: 1.981879
[Epoch 57] ogbg-molbace: 0.765084 test loss: 1.299866
[Epoch 58; Iter     3/   41] train: loss: 0.0030474
[Epoch 58; Iter    33/   41] train: loss: 0.0030112
[Epoch 58] ogbg-molbace: 0.651282 val loss: 2.421749
[Epoch 58] ogbg-molbace: 0.760216 test loss: 1.576191
[Epoch 59; Iter    22/   41] train: loss: 0.0052232
[Epoch 59] ogbg-molbace: 0.661172 val loss: 2.139035
[Epoch 59] ogbg-molbace: 0.775865 test loss: 1.481875
[Epoch 60; Iter    11/   41] train: loss: 0.0017261
[Epoch 60; Iter    41/   41] train: loss: 0.0028292
[Epoch 60] ogbg-molbace: 0.649817 val loss: 2.146121
[Epoch 60] ogbg-molbace: 0.773431 test loss: 1.489898
[Epoch 61; Iter    30/   41] train: loss: 0.0014023
[Epoch 61] ogbg-molbace: 0.650549 val loss: 2.109508
[Epoch 61] ogbg-molbace: 0.772214 test loss: 1.476023
[Epoch 62; Iter    19/   41] train: loss: 0.0051173
[Epoch 62] ogbg-molbace: 0.641026 val loss: 2.259737
[Epoch 62] ogbg-molbace: 0.762824 test loss: 1.607063
[Epoch 63; Iter     8/   41] train: loss: 0.0026553
[Epoch 63; Iter    38/   41] train: loss: 0.0204833
[Epoch 63] ogbg-molbace: 0.676557 val loss: 2.140611
[Epoch 63] ogbg-molbace: 0.725265 test loss: 1.734817
[Epoch 64; Iter    27/   41] train: loss: 0.0312461
[Epoch 64] ogbg-molbace: 0.652747 val loss: 2.081155
[Epoch 64] ogbg-molbace: 0.763867 test loss: 1.476916
[Epoch 65; Iter    16/   41] train: loss: 0.0048599
[Epoch 65] ogbg-molbace: 0.651648 val loss: 2.641483
[Epoch 65] ogbg-molbace: 0.753782 test loss: 2.083655
[Epoch 66; Iter     5/   41] train: loss: 0.0093101
[Epoch 66; Iter    35/   41] train: loss: 0.0689025
[Epoch 66] ogbg-molbace: 0.667033 val loss: 2.248924
[Epoch 66] ogbg-molbace: 0.754651 test loss: 1.621472
[Epoch 67; Iter    24/   41] train: loss: 0.1207890
[Epoch 67] ogbg-molbace: 0.634432 val loss: 3.225239
[Epoch 67] ogbg-molbace: 0.743697 test loss: 1.763590
[Epoch 68; Iter    13/   41] train: loss: 0.1285935
[Epoch 68] ogbg-molbace: 0.712454 val loss: 2.076219
[Epoch 68] ogbg-molbace: 0.745957 test loss: 1.835412
[Epoch 69; Iter     2/   41] train: loss: 0.2112032
[Epoch 69; Iter    32/   41] train: loss: 0.2425620
[Epoch 69] ogbg-molbace: 0.627106 val loss: 1.408292
[Epoch 69] ogbg-molbace: 0.727700 test loss: 1.123231
[Epoch 70; Iter    21/   41] train: loss: 0.0867353
[Epoch 70] ogbg-molbace: 0.675458 val loss: 0.985030
[Epoch 70] ogbg-molbace: 0.739524 test loss: 1.187457
[Epoch 71; Iter    10/   41] train: loss: 0.0228833
[Epoch 71; Iter    40/   41] train: loss: 0.0243484
[Epoch 71] ogbg-molbace: 0.660440 val loss: 1.213292
[Epoch 71] ogbg-molbace: 0.766649 test loss: 0.987044
[Epoch 72; Iter    29/   41] train: loss: 0.0079309
[Epoch 72] ogbg-molbace: 0.637363 val loss: 1.271027
[Epoch 72] ogbg-molbace: 0.727700 test loss: 1.086156
[Epoch 73; Iter    18/   41] train: loss: 0.0042477
[Epoch 73] ogbg-molbace: 0.650916 val loss: 1.285474
[Epoch 73] ogbg-molbace: 0.760042 test loss: 1.074055
[Epoch 74; Iter     7/   41] train: loss: 0.0043393
[Epoch 74; Iter    37/   41] train: loss: 0.0053349
[Epoch 74] ogbg-molbace: 0.650549 val loss: 1.300419
[Epoch 74] ogbg-molbace: 0.752565 test loss: 1.082156
[Epoch 75; Iter    26/   41] train: loss: 0.0117207
[Epoch 75] ogbg-molbace: 0.661172 val loss: 1.190764
[Epoch 75] ogbg-molbace: 0.757607 test loss: 1.070375
[Epoch 76; Iter    15/   41] train: loss: 0.0027311
[Epoch 76] ogbg-molbace: 0.660440 val loss: 1.243453
[Epoch 76] ogbg-molbace: 0.751174 test loss: 1.116781
[Epoch 77; Iter     4/   41] train: loss: 0.0025880
[Epoch 77; Iter    34/   41] train: loss: 0.0039737
[Epoch 77] ogbg-molbace: 0.660440 val loss: 1.313695
[Epoch 77] ogbg-molbace: 0.748565 test loss: 1.161971
[Epoch 78; Iter    23/   41] train: loss: 0.0032291
[Epoch 32] ogbg-molbace: 0.730482 test loss: 0.946895
[Epoch 33; Iter     8/   41] train: loss: 0.5519645
[Epoch 33; Iter    38/   41] train: loss: 0.6668704
[Epoch 33] ogbg-molbace: 0.616484 val loss: 1.026008
[Epoch 33] ogbg-molbace: 0.751695 test loss: 1.077912
[Epoch 34; Iter    27/   41] train: loss: 0.4318714
[Epoch 34] ogbg-molbace: 0.622711 val loss: 0.856085
[Epoch 34] ogbg-molbace: 0.742306 test loss: 0.956792
[Epoch 35; Iter    16/   41] train: loss: 0.4073146
[Epoch 35] ogbg-molbace: 0.737363 val loss: 0.751351
[Epoch 35] ogbg-molbace: 0.791341 test loss: 0.797154
[Epoch 36; Iter     5/   41] train: loss: 0.4430854
[Epoch 36; Iter    35/   41] train: loss: 0.3744533
[Epoch 36] ogbg-molbace: 0.617582 val loss: 0.696993
[Epoch 36] ogbg-molbace: 0.716223 test loss: 1.110779
[Epoch 37; Iter    24/   41] train: loss: 0.4029825
[Epoch 37] ogbg-molbace: 0.713187 val loss: 0.764308
[Epoch 37] ogbg-molbace: 0.796731 test loss: 1.248413
[Epoch 38; Iter    13/   41] train: loss: 0.3195774
[Epoch 38] ogbg-molbace: 0.624176 val loss: 1.198390
[Epoch 38] ogbg-molbace: 0.705269 test loss: 0.738444
[Epoch 39; Iter     2/   41] train: loss: 0.5466774
[Epoch 39; Iter    32/   41] train: loss: 0.5931783
[Epoch 39] ogbg-molbace: 0.741026 val loss: 0.594208
[Epoch 39] ogbg-molbace: 0.712572 test loss: 0.980241
[Epoch 40; Iter    21/   41] train: loss: 0.3124785
[Epoch 40] ogbg-molbace: 0.628205 val loss: 1.245375
[Epoch 40] ogbg-molbace: 0.692227 test loss: 1.332893
[Epoch 41; Iter    10/   41] train: loss: 0.4097992
[Epoch 41; Iter    40/   41] train: loss: 0.7517692
[Epoch 41] ogbg-molbace: 0.660073 val loss: 0.585250
[Epoch 41] ogbg-molbace: 0.694140 test loss: 1.140168
[Epoch 42; Iter    29/   41] train: loss: 0.4356509
[Epoch 42] ogbg-molbace: 0.613553 val loss: 1.267438
[Epoch 42] ogbg-molbace: 0.682316 test loss: 0.912985
[Epoch 43; Iter    18/   41] train: loss: 0.2302723
[Epoch 43] ogbg-molbace: 0.715385 val loss: 0.733083
[Epoch 43] ogbg-molbace: 0.818292 test loss: 0.991082
[Epoch 44; Iter     7/   41] train: loss: 0.2828129
[Epoch 44; Iter    37/   41] train: loss: 0.2375427
[Epoch 44] ogbg-molbace: 0.698168 val loss: 1.306887
[Epoch 44] ogbg-molbace: 0.728047 test loss: 1.445873
[Epoch 45; Iter    26/   41] train: loss: 0.2078506
[Epoch 45] ogbg-molbace: 0.704762 val loss: 1.164706
[Epoch 45] ogbg-molbace: 0.786124 test loss: 0.834014
[Epoch 46; Iter    15/   41] train: loss: 0.2141721
[Epoch 46] ogbg-molbace: 0.675458 val loss: 1.075640
[Epoch 46] ogbg-molbace: 0.788385 test loss: 1.251933
[Epoch 47; Iter     4/   41] train: loss: 0.1411631
[Epoch 47; Iter    34/   41] train: loss: 0.0885581
[Epoch 47] ogbg-molbace: 0.595238 val loss: 1.214416
[Epoch 47] ogbg-molbace: 0.745436 test loss: 1.212495
[Epoch 48; Iter    23/   41] train: loss: 0.0509163
[Epoch 48] ogbg-molbace: 0.675458 val loss: 1.231741
[Epoch 48] ogbg-molbace: 0.745088 test loss: 1.374276
[Epoch 49; Iter    12/   41] train: loss: 0.3730542
[Epoch 49] ogbg-molbace: 0.635531 val loss: 1.179236
[Epoch 49] ogbg-molbace: 0.718484 test loss: 1.099470
[Epoch 50; Iter     1/   41] train: loss: 0.0895466
[Epoch 50; Iter    31/   41] train: loss: 0.0810877
[Epoch 50] ogbg-molbace: 0.682051 val loss: 2.024997
[Epoch 50] ogbg-molbace: 0.740567 test loss: 1.696210
[Epoch 51; Iter    20/   41] train: loss: 0.1837609
[Epoch 51] ogbg-molbace: 0.634432 val loss: 1.524381
[Epoch 51] ogbg-molbace: 0.785603 test loss: 1.441190
[Epoch 52; Iter     9/   41] train: loss: 0.2531651
[Epoch 52; Iter    39/   41] train: loss: 0.1058393
[Epoch 52] ogbg-molbace: 0.712454 val loss: 1.333168
[Epoch 52] ogbg-molbace: 0.774474 test loss: 1.293754
[Epoch 53; Iter    28/   41] train: loss: 0.0829208
[Epoch 53] ogbg-molbace: 0.650183 val loss: 1.238090
[Epoch 53] ogbg-molbace: 0.694662 test loss: 1.873301
[Epoch 54; Iter    17/   41] train: loss: 0.0272271
[Epoch 54] ogbg-molbace: 0.635531 val loss: 1.313921
[Epoch 54] ogbg-molbace: 0.746827 test loss: 1.969010
[Epoch 55; Iter     6/   41] train: loss: 0.0179441
[Epoch 55; Iter    36/   41] train: loss: 0.0145343
[Epoch 55] ogbg-molbace: 0.713553 val loss: 1.579971
[Epoch 55] ogbg-molbace: 0.781255 test loss: 1.039468
[Epoch 56; Iter    25/   41] train: loss: 0.0231449
[Epoch 56] ogbg-molbace: 0.647619 val loss: 1.405376
[Epoch 56] ogbg-molbace: 0.747696 test loss: 1.878420
[Epoch 57; Iter    14/   41] train: loss: 0.0052805
[Epoch 57] ogbg-molbace: 0.682418 val loss: 1.387997
[Epoch 57] ogbg-molbace: 0.769084 test loss: 1.712691
[Epoch 58; Iter     3/   41] train: loss: 0.0063645
[Epoch 58; Iter    33/   41] train: loss: 0.0062083
[Epoch 58] ogbg-molbace: 0.712821 val loss: 1.415490
[Epoch 58] ogbg-molbace: 0.799687 test loss: 2.090556
[Epoch 59; Iter    22/   41] train: loss: 0.0207883
[Epoch 59] ogbg-molbace: 0.690842 val loss: 1.390745
[Epoch 59] ogbg-molbace: 0.775170 test loss: 2.102697
[Epoch 60; Iter    11/   41] train: loss: 0.0031712
[Epoch 60; Iter    41/   41] train: loss: 0.0048133
[Epoch 60] ogbg-molbace: 0.690476 val loss: 1.410504
[Epoch 60] ogbg-molbace: 0.773257 test loss: 2.098267
[Epoch 61; Iter    30/   41] train: loss: 0.0025839
[Epoch 61] ogbg-molbace: 0.691941 val loss: 1.393221
[Epoch 61] ogbg-molbace: 0.776734 test loss: 2.086807
[Epoch 62; Iter    19/   41] train: loss: 0.0082905
[Epoch 62] ogbg-molbace: 0.682051 val loss: 1.569437
[Epoch 62] ogbg-molbace: 0.775517 test loss: 1.947503
[Epoch 63; Iter     8/   41] train: loss: 0.0080832
[Epoch 63; Iter    38/   41] train: loss: 0.1123377
[Epoch 63] ogbg-molbace: 0.691575 val loss: 2.337245
[Epoch 63] ogbg-molbace: 0.785429 test loss: 1.031827
[Epoch 64; Iter    27/   41] train: loss: 0.0277812
[Epoch 64] ogbg-molbace: 0.685714 val loss: 1.663415
[Epoch 64] ogbg-molbace: 0.755695 test loss: 1.751207
[Epoch 65; Iter    16/   41] train: loss: 0.0316143
[Epoch 65] ogbg-molbace: 0.676190 val loss: 2.079495
[Epoch 65] ogbg-molbace: 0.729091 test loss: 3.038479
[Epoch 66; Iter     5/   41] train: loss: 0.0336166
[Epoch 66; Iter    35/   41] train: loss: 0.0957610
[Epoch 66] ogbg-molbace: 0.621612 val loss: 1.178415
[Epoch 66] ogbg-molbace: 0.692227 test loss: 1.464754
[Epoch 67; Iter    24/   41] train: loss: 0.1135190
[Epoch 67] ogbg-molbace: 0.650183 val loss: 4.038847
[Epoch 67] ogbg-molbace: 0.741089 test loss: 1.748808
[Epoch 68; Iter    13/   41] train: loss: 0.1984783
[Epoch 68] ogbg-molbace: 0.637363 val loss: 2.105262
[Epoch 68] ogbg-molbace: 0.705616 test loss: 2.345569
[Epoch 69; Iter     2/   41] train: loss: 0.0887325
[Epoch 69; Iter    32/   41] train: loss: 0.2329773
[Epoch 69] ogbg-molbace: 0.652015 val loss: 1.068331
[Epoch 69] ogbg-molbace: 0.775343 test loss: 1.469767
[Epoch 70; Iter    21/   41] train: loss: 0.0472258
[Epoch 70] ogbg-molbace: 0.632234 val loss: 1.551595
[Epoch 70] ogbg-molbace: 0.719179 test loss: 2.028996
[Epoch 71; Iter    10/   41] train: loss: 0.0230771
[Epoch 71; Iter    40/   41] train: loss: 0.0247378
[Epoch 71] ogbg-molbace: 0.605495 val loss: 1.631573
[Epoch 71] ogbg-molbace: 0.739176 test loss: 1.630132
[Epoch 72; Iter    29/   41] train: loss: 0.0063691
[Epoch 72] ogbg-molbace: 0.622344 val loss: 1.462938
[Epoch 72] ogbg-molbace: 0.742827 test loss: 1.449139
[Epoch 73; Iter    18/   41] train: loss: 0.0051421
[Epoch 73] ogbg-molbace: 0.642857 val loss: 1.419803
[Epoch 73] ogbg-molbace: 0.754825 test loss: 1.706396
[Epoch 74; Iter     7/   41] train: loss: 0.0053505
[Epoch 74; Iter    37/   41] train: loss: 0.0030299
[Epoch 74] ogbg-molbace: 0.638828 val loss: 1.357944
[Epoch 74] ogbg-molbace: 0.761607 test loss: 1.463219
[Epoch 75; Iter    26/   41] train: loss: 0.0140178
[Epoch 75] ogbg-molbace: 0.636630 val loss: 1.446509
[Epoch 75] ogbg-molbace: 0.743349 test loss: 1.858511
[Epoch 76; Iter    15/   41] train: loss: 0.0034244
[Epoch 76] ogbg-molbace: 0.642125 val loss: 1.420691
[Epoch 76] ogbg-molbace: 0.761781 test loss: 1.584527
[Epoch 77; Iter     4/   41] train: loss: 0.0029441
[Epoch 77; Iter    34/   41] train: loss: 0.0040030
[Epoch 77] ogbg-molbace: 0.635897 val loss: 1.437955
[Epoch 77] ogbg-molbace: 0.753260 test loss: 1.588086
[Epoch 78; Iter    23/   41] train: loss: 0.0019283
[Epoch 32] ogbg-molbace: 0.730482 test loss: 1.093708
[Epoch 33; Iter     8/   41] train: loss: 0.5320138
[Epoch 33; Iter    38/   41] train: loss: 0.5982394
[Epoch 33] ogbg-molbace: 0.604762 val loss: 1.035065
[Epoch 33] ogbg-molbace: 0.770475 test loss: 1.098220
[Epoch 34; Iter    27/   41] train: loss: 0.5207918
[Epoch 34] ogbg-molbace: 0.628938 val loss: 0.974717
[Epoch 34] ogbg-molbace: 0.773257 test loss: 1.232349
[Epoch 35; Iter    16/   41] train: loss: 0.4561903
[Epoch 35] ogbg-molbace: 0.648352 val loss: 0.920423
[Epoch 35] ogbg-molbace: 0.777430 test loss: 1.121798
[Epoch 36; Iter     5/   41] train: loss: 0.3152355
[Epoch 36; Iter    35/   41] train: loss: 0.4228045
[Epoch 36] ogbg-molbace: 0.549084 val loss: 1.307286
[Epoch 36] ogbg-molbace: 0.652061 test loss: 1.777334
[Epoch 37; Iter    24/   41] train: loss: 0.4162444
[Epoch 37] ogbg-molbace: 0.713553 val loss: 0.839335
[Epoch 37] ogbg-molbace: 0.777430 test loss: 0.879289
[Epoch 38; Iter    13/   41] train: loss: 0.3583824
[Epoch 38] ogbg-molbace: 0.710989 val loss: 1.080665
[Epoch 38] ogbg-molbace: 0.732916 test loss: 1.480895
[Epoch 39; Iter     2/   41] train: loss: 0.4235837
[Epoch 39; Iter    32/   41] train: loss: 0.4074799
[Epoch 39] ogbg-molbace: 0.721612 val loss: 0.948831
[Epoch 39] ogbg-molbace: 0.802991 test loss: 0.854753
[Epoch 40; Iter    21/   41] train: loss: 0.3059212
[Epoch 40] ogbg-molbace: 0.668498 val loss: 1.281986
[Epoch 40] ogbg-molbace: 0.720049 test loss: 0.948828
[Epoch 41; Iter    10/   41] train: loss: 0.6326844
[Epoch 41; Iter    40/   41] train: loss: 0.4264429
[Epoch 41] ogbg-molbace: 0.674725 val loss: 0.738253
[Epoch 41] ogbg-molbace: 0.781255 test loss: 0.734172
[Epoch 42; Iter    29/   41] train: loss: 0.2989114
[Epoch 42] ogbg-molbace: 0.653114 val loss: 0.961413
[Epoch 42] ogbg-molbace: 0.722309 test loss: 0.646200
[Epoch 43; Iter    18/   41] train: loss: 0.3162551
[Epoch 43] ogbg-molbace: 0.615018 val loss: 0.748187
[Epoch 43] ogbg-molbace: 0.709268 test loss: 1.238609
[Epoch 44; Iter     7/   41] train: loss: 0.4316027
[Epoch 44; Iter    37/   41] train: loss: 0.3443331
[Epoch 44] ogbg-molbace: 0.738828 val loss: 0.852106
[Epoch 44] ogbg-molbace: 0.757781 test loss: 1.079464
[Epoch 45; Iter    26/   41] train: loss: 0.3615643
[Epoch 45] ogbg-molbace: 0.687546 val loss: 1.392794
[Epoch 45] ogbg-molbace: 0.778473 test loss: 0.982848
[Epoch 46; Iter    15/   41] train: loss: 0.1397542
[Epoch 46] ogbg-molbace: 0.641026 val loss: 1.436280
[Epoch 46] ogbg-molbace: 0.767693 test loss: 1.596968
[Epoch 47; Iter     4/   41] train: loss: 0.2035294
[Epoch 47; Iter    34/   41] train: loss: 0.1410070
[Epoch 47] ogbg-molbace: 0.724176 val loss: 0.909114
[Epoch 47] ogbg-molbace: 0.760216 test loss: 0.855330
[Epoch 48; Iter    23/   41] train: loss: 0.1011570
[Epoch 48] ogbg-molbace: 0.620879 val loss: 1.677513
[Epoch 48] ogbg-molbace: 0.740393 test loss: 1.576625
[Epoch 49; Iter    12/   41] train: loss: 0.1556811
[Epoch 49] ogbg-molbace: 0.695971 val loss: 0.975113
[Epoch 49] ogbg-molbace: 0.788037 test loss: 1.570724
[Epoch 50; Iter     1/   41] train: loss: 0.1257476
[Epoch 50; Iter    31/   41] train: loss: 0.1501030
[Epoch 50] ogbg-molbace: 0.638095 val loss: 1.531814
[Epoch 50] ogbg-molbace: 0.686837 test loss: 1.489159
[Epoch 51; Iter    20/   41] train: loss: 0.2069895
[Epoch 51] ogbg-molbace: 0.661905 val loss: 1.356733
[Epoch 51] ogbg-molbace: 0.792906 test loss: 1.514194
[Epoch 52; Iter     9/   41] train: loss: 0.1706451
[Epoch 52; Iter    39/   41] train: loss: 0.1093390
[Epoch 52] ogbg-molbace: 0.713187 val loss: 1.460354
[Epoch 52] ogbg-molbace: 0.724570 test loss: 1.991993
[Epoch 53; Iter    28/   41] train: loss: 0.0698241
[Epoch 53] ogbg-molbace: 0.680952 val loss: 1.046652
[Epoch 53] ogbg-molbace: 0.724917 test loss: 1.121275
[Epoch 54; Iter    17/   41] train: loss: 0.0734015
[Epoch 54] ogbg-molbace: 0.683883 val loss: 1.648208
[Epoch 54] ogbg-molbace: 0.761433 test loss: 1.529149
[Epoch 55; Iter     6/   41] train: loss: 0.0349351
[Epoch 55; Iter    36/   41] train: loss: 0.2259246
[Epoch 55] ogbg-molbace: 0.651648 val loss: 1.640153
[Epoch 55] ogbg-molbace: 0.624935 test loss: 1.381704
[Epoch 56; Iter    25/   41] train: loss: 0.0273936
[Epoch 56] ogbg-molbace: 0.660073 val loss: 1.100856
[Epoch 56] ogbg-molbace: 0.742132 test loss: 0.973647
[Epoch 57; Iter    14/   41] train: loss: 0.0159519
[Epoch 57] ogbg-molbace: 0.666667 val loss: 1.015990
[Epoch 57] ogbg-molbace: 0.821944 test loss: 1.670880
[Epoch 58; Iter     3/   41] train: loss: 0.0181307
[Epoch 58; Iter    33/   41] train: loss: 0.0999563
[Epoch 58] ogbg-molbace: 0.656777 val loss: 1.627974
[Epoch 58] ogbg-molbace: 0.756912 test loss: 1.788111
[Epoch 59; Iter    22/   41] train: loss: 0.0717857
[Epoch 59] ogbg-molbace: 0.635897 val loss: 2.637784
[Epoch 59] ogbg-molbace: 0.705095 test loss: 2.893847
[Epoch 60; Iter    11/   41] train: loss: 0.0534457
[Epoch 60; Iter    41/   41] train: loss: 0.6505327
[Epoch 60] ogbg-molbace: 0.652381 val loss: 1.952760
[Epoch 60] ogbg-molbace: 0.725787 test loss: 2.226285
[Epoch 61; Iter    30/   41] train: loss: 0.0203164
[Epoch 61] ogbg-molbace: 0.686813 val loss: 1.634495
[Epoch 61] ogbg-molbace: 0.798122 test loss: 1.023459
[Epoch 62; Iter    19/   41] train: loss: 0.3009910
[Epoch 62] ogbg-molbace: 0.695238 val loss: 1.983095
[Epoch 62] ogbg-molbace: 0.810816 test loss: 1.560917
[Epoch 63; Iter     8/   41] train: loss: 0.0482367
[Epoch 63; Iter    38/   41] train: loss: 0.1137474
[Epoch 63] ogbg-molbace: 0.712821 val loss: 1.860743
[Epoch 63] ogbg-molbace: 0.602852 test loss: 2.370197
[Epoch 64; Iter    27/   41] train: loss: 0.3238993
[Epoch 64] ogbg-molbace: 0.678388 val loss: 2.417163
[Epoch 64] ogbg-molbace: 0.793775 test loss: 1.061432
[Epoch 65; Iter    16/   41] train: loss: 0.0839775
[Epoch 65] ogbg-molbace: 0.666300 val loss: 1.510191
[Epoch 65] ogbg-molbace: 0.754999 test loss: 1.310705
[Epoch 66; Iter     5/   41] train: loss: 0.0608428
[Epoch 66; Iter    35/   41] train: loss: 0.0608181
[Epoch 66] ogbg-molbace: 0.644322 val loss: 1.994537
[Epoch 66] ogbg-molbace: 0.723352 test loss: 1.356048
[Epoch 67; Iter    24/   41] train: loss: 0.0977970
[Epoch 67] ogbg-molbace: 0.711355 val loss: 1.576656
[Epoch 67] ogbg-molbace: 0.782646 test loss: 1.388841
[Epoch 68; Iter    13/   41] train: loss: 0.0144849
[Epoch 68] ogbg-molbace: 0.660806 val loss: 1.460932
[Epoch 68] ogbg-molbace: 0.761259 test loss: 1.648783
[Epoch 69; Iter     2/   41] train: loss: 0.0128603
[Epoch 69; Iter    32/   41] train: loss: 0.0035882
[Epoch 69] ogbg-molbace: 0.669963 val loss: 1.829882
[Epoch 69] ogbg-molbace: 0.745088 test loss: 1.909063
[Epoch 70; Iter    21/   41] train: loss: 0.0139761
[Epoch 70] ogbg-molbace: 0.675092 val loss: 1.565581
[Epoch 70] ogbg-molbace: 0.741610 test loss: 1.742670
[Epoch 71; Iter    10/   41] train: loss: 0.0064618
[Epoch 71; Iter    40/   41] train: loss: 0.0036706
[Epoch 71] ogbg-molbace: 0.683516 val loss: 1.653673
[Epoch 71] ogbg-molbace: 0.753782 test loss: 1.653194
[Epoch 72; Iter    29/   41] train: loss: 0.0071699
[Epoch 72] ogbg-molbace: 0.676923 val loss: 1.619859
[Epoch 72] ogbg-molbace: 0.748565 test loss: 1.672834
[Epoch 73; Iter    18/   41] train: loss: 0.0016721
[Epoch 73] ogbg-molbace: 0.677289 val loss: 1.683118
[Epoch 73] ogbg-molbace: 0.742653 test loss: 1.830145
[Epoch 74; Iter     7/   41] train: loss: 0.0011195
[Epoch 74; Iter    37/   41] train: loss: 0.0016530
[Epoch 74] ogbg-molbace: 0.673626 val loss: 1.560835
[Epoch 74] ogbg-molbace: 0.744392 test loss: 1.648038
[Epoch 75; Iter    26/   41] train: loss: 0.0078888
[Epoch 75] ogbg-molbace: 0.672161 val loss: 1.657674
[Epoch 75] ogbg-molbace: 0.738306 test loss: 1.889763
[Epoch 76; Iter    15/   41] train: loss: 0.0014711
[Epoch 76] ogbg-molbace: 0.683150 val loss: 1.612877
[Epoch 76] ogbg-molbace: 0.752043 test loss: 1.545330
[Epoch 77; Iter     4/   41] train: loss: 0.0009853
[Epoch 77; Iter    34/   41] train: loss: 0.0016042
[Epoch 77] ogbg-molbace: 0.674725 val loss: 1.646095
[Epoch 77] ogbg-molbace: 0.740567 test loss: 1.560927
[Epoch 78; Iter    23/   41] train: loss: 0.0009539
[Epoch 32] ogbg-molbace: 0.727178 test loss: 1.953953
[Epoch 33; Iter     8/   41] train: loss: 0.4539258
[Epoch 33; Iter    38/   41] train: loss: 0.6047828
[Epoch 33] ogbg-molbace: 0.646520 val loss: 1.245274
[Epoch 33] ogbg-molbace: 0.761259 test loss: 2.056015
[Epoch 34; Iter    27/   41] train: loss: 0.3650611
[Epoch 34] ogbg-molbace: 0.601099 val loss: 1.297692
[Epoch 34] ogbg-molbace: 0.715875 test loss: 2.116690
[Epoch 35; Iter    16/   41] train: loss: 0.4806177
[Epoch 35] ogbg-molbace: 0.621612 val loss: 1.090317
[Epoch 35] ogbg-molbace: 0.691880 test loss: 2.019774
[Epoch 36; Iter     5/   41] train: loss: 0.2917040
[Epoch 36; Iter    35/   41] train: loss: 0.5058166
[Epoch 36] ogbg-molbace: 0.669963 val loss: 0.693619
[Epoch 36] ogbg-molbace: 0.758998 test loss: 1.549123
[Epoch 37; Iter    24/   41] train: loss: 0.4870356
[Epoch 37] ogbg-molbace: 0.589744 val loss: 0.814342
[Epoch 37] ogbg-molbace: 0.676404 test loss: 1.327368
[Epoch 38; Iter    13/   41] train: loss: 0.3511460
[Epoch 38] ogbg-molbace: 0.515018 val loss: 1.038627
[Epoch 38] ogbg-molbace: 0.540776 test loss: 4.139438
[Epoch 39; Iter     2/   41] train: loss: 0.3310859
[Epoch 39; Iter    32/   41] train: loss: 0.4094578
[Epoch 39] ogbg-molbace: 0.700733 val loss: 0.802690
[Epoch 39] ogbg-molbace: 0.784907 test loss: 1.220180
[Epoch 40; Iter    21/   41] train: loss: 0.2545841
[Epoch 40] ogbg-molbace: 0.611355 val loss: 1.695281
[Epoch 40] ogbg-molbace: 0.767519 test loss: 2.217942
[Epoch 41; Iter    10/   41] train: loss: 0.2925850
[Epoch 41; Iter    40/   41] train: loss: 0.4685968
[Epoch 41] ogbg-molbace: 0.600000 val loss: 1.207921
[Epoch 41] ogbg-molbace: 0.710311 test loss: 1.719498
[Epoch 42; Iter    29/   41] train: loss: 0.5905401
[Epoch 42] ogbg-molbace: 0.567399 val loss: 0.963107
[Epoch 42] ogbg-molbace: 0.665971 test loss: 1.586740
[Epoch 43; Iter    18/   41] train: loss: 0.1876039
[Epoch 43] ogbg-molbace: 0.586813 val loss: 1.624057
[Epoch 43] ogbg-molbace: 0.683707 test loss: 5.311203
[Epoch 44; Iter     7/   41] train: loss: 0.2963443
[Epoch 44; Iter    37/   41] train: loss: 0.4842672
[Epoch 44] ogbg-molbace: 0.671795 val loss: 1.278697
[Epoch 44] ogbg-molbace: 0.787689 test loss: 1.635128
[Epoch 45; Iter    26/   41] train: loss: 0.2984210
[Epoch 45] ogbg-molbace: 0.673260 val loss: 1.231422
[Epoch 45] ogbg-molbace: 0.813250 test loss: 0.999836
[Epoch 46; Iter    15/   41] train: loss: 0.2084225
[Epoch 46] ogbg-molbace: 0.649084 val loss: 1.445709
[Epoch 46] ogbg-molbace: 0.783342 test loss: 1.974411
[Epoch 47; Iter     4/   41] train: loss: 0.1331162
[Epoch 47; Iter    34/   41] train: loss: 0.0656833
[Epoch 47] ogbg-molbace: 0.636630 val loss: 1.297068
[Epoch 47] ogbg-molbace: 0.738480 test loss: 1.830558
[Epoch 48; Iter    23/   41] train: loss: 0.0696477
[Epoch 48] ogbg-molbace: 0.651282 val loss: 1.590139
[Epoch 48] ogbg-molbace: 0.744218 test loss: 2.101461
[Epoch 49; Iter    12/   41] train: loss: 0.0874947
[Epoch 49] ogbg-molbace: 0.649084 val loss: 1.443882
[Epoch 49] ogbg-molbace: 0.756738 test loss: 2.011812
[Epoch 50; Iter     1/   41] train: loss: 0.0117761
[Epoch 50; Iter    31/   41] train: loss: 0.0281384
[Epoch 50] ogbg-molbace: 0.653114 val loss: 1.415691
[Epoch 50] ogbg-molbace: 0.795166 test loss: 2.243302
[Epoch 51; Iter    20/   41] train: loss: 0.0162264
[Epoch 51] ogbg-molbace: 0.720513 val loss: 1.107401
[Epoch 51] ogbg-molbace: 0.756390 test loss: 2.073680
[Epoch 52; Iter     9/   41] train: loss: 0.0665054
[Epoch 52; Iter    39/   41] train: loss: 0.0188467
[Epoch 52] ogbg-molbace: 0.642125 val loss: 1.502040
[Epoch 52] ogbg-molbace: 0.706486 test loss: 2.155125
[Epoch 53; Iter    28/   41] train: loss: 0.0244460
[Epoch 53] ogbg-molbace: 0.656410 val loss: 1.504791
[Epoch 53] ogbg-molbace: 0.791688 test loss: 1.768618
[Epoch 54; Iter    17/   41] train: loss: 0.1082372
[Epoch 54] ogbg-molbace: 0.572161 val loss: 1.434478
[Epoch 54] ogbg-molbace: 0.699183 test loss: 2.401821
[Epoch 55; Iter     6/   41] train: loss: 0.0974956
[Epoch 55; Iter    36/   41] train: loss: 0.0568608
[Epoch 55] ogbg-molbace: 0.583883 val loss: 3.007595
[Epoch 55] ogbg-molbace: 0.687706 test loss: 3.318394
[Epoch 56; Iter    25/   41] train: loss: 0.2994683
[Epoch 56] ogbg-molbace: 0.661905 val loss: 1.474538
[Epoch 56] ogbg-molbace: 0.780908 test loss: 2.584809
[Epoch 57; Iter    14/   41] train: loss: 0.0800729
[Epoch 57] ogbg-molbace: 0.563736 val loss: 2.654573
[Epoch 57] ogbg-molbace: 0.668232 test loss: 3.802229
[Epoch 58; Iter     3/   41] train: loss: 0.1280157
[Epoch 58; Iter    33/   41] train: loss: 0.3114687
[Epoch 58] ogbg-molbace: 0.675092 val loss: 1.728655
[Epoch 58] ogbg-molbace: 0.689271 test loss: 2.424339
[Epoch 59; Iter    22/   41] train: loss: 0.0470088
[Epoch 59] ogbg-molbace: 0.619414 val loss: 1.446272
[Epoch 59] ogbg-molbace: 0.729612 test loss: 2.317490
[Epoch 60; Iter    11/   41] train: loss: 0.1581465
[Epoch 60; Iter    41/   41] train: loss: 0.0071669
[Epoch 60] ogbg-molbace: 0.679853 val loss: 1.846723
[Epoch 60] ogbg-molbace: 0.710485 test loss: 3.422019
[Epoch 61; Iter    30/   41] train: loss: 0.0354049
[Epoch 61] ogbg-molbace: 0.665201 val loss: 1.453035
[Epoch 61] ogbg-molbace: 0.738480 test loss: 3.050456
[Epoch 62; Iter    19/   41] train: loss: 0.0062890
[Epoch 62] ogbg-molbace: 0.650916 val loss: 1.893763
[Epoch 62] ogbg-molbace: 0.749087 test loss: 3.120208
[Epoch 63; Iter     8/   41] train: loss: 0.1599169
[Epoch 63; Iter    38/   41] train: loss: 0.0500631
[Epoch 63] ogbg-molbace: 0.627473 val loss: 1.336633
[Epoch 63] ogbg-molbace: 0.779169 test loss: 2.026291
[Epoch 64; Iter    27/   41] train: loss: 0.0759320
[Epoch 64] ogbg-molbace: 0.657143 val loss: 1.554033
[Epoch 64] ogbg-molbace: 0.742480 test loss: 2.451903
[Epoch 65; Iter    16/   41] train: loss: 0.0113516
[Epoch 65] ogbg-molbace: 0.614286 val loss: 2.399065
[Epoch 65] ogbg-molbace: 0.749087 test loss: 2.368420
[Epoch 66; Iter     5/   41] train: loss: 0.0634939
[Epoch 66; Iter    35/   41] train: loss: 0.0515208
[Epoch 66] ogbg-molbace: 0.651648 val loss: 1.728963
[Epoch 66] ogbg-molbace: 0.760216 test loss: 2.563219
[Epoch 67; Iter    24/   41] train: loss: 0.0711277
[Epoch 67] ogbg-molbace: 0.608791 val loss: 2.224915
[Epoch 67] ogbg-molbace: 0.731351 test loss: 2.693175
[Epoch 68; Iter    13/   41] train: loss: 0.0787776
[Epoch 68] ogbg-molbace: 0.666667 val loss: 2.696839
[Epoch 68] ogbg-molbace: 0.743175 test loss: 3.702793
[Epoch 69; Iter     2/   41] train: loss: 0.0083819
[Epoch 69; Iter    32/   41] train: loss: 0.0075718
[Epoch 69] ogbg-molbace: 0.632234 val loss: 1.573659
[Epoch 69] ogbg-molbace: 0.766475 test loss: 2.750057
[Epoch 70; Iter    21/   41] train: loss: 0.0078756
[Epoch 70] ogbg-molbace: 0.623443 val loss: 1.452692
[Epoch 70] ogbg-molbace: 0.747348 test loss: 2.468888
[Epoch 71; Iter    10/   41] train: loss: 0.0018291
[Epoch 71; Iter    40/   41] train: loss: 0.0028591
[Epoch 71] ogbg-molbace: 0.647619 val loss: 1.623757
[Epoch 71] ogbg-molbace: 0.793079 test loss: 2.613127
[Epoch 72; Iter    29/   41] train: loss: 0.0078262
[Epoch 72] ogbg-molbace: 0.649451 val loss: 1.682206
[Epoch 72] ogbg-molbace: 0.777952 test loss: 2.834015
[Epoch 73; Iter    18/   41] train: loss: 0.0789600
[Epoch 73] ogbg-molbace: 0.610989 val loss: 2.468236
[Epoch 73] ogbg-molbace: 0.808207 test loss: 2.295410
[Epoch 74; Iter     7/   41] train: loss: 0.2899452
[Epoch 74; Iter    37/   41] train: loss: 0.0379551
[Epoch 74] ogbg-molbace: 0.687179 val loss: 2.263773
[Epoch 74] ogbg-molbace: 0.742827 test loss: 2.716137
[Epoch 75; Iter    26/   41] train: loss: 0.1732863
[Epoch 75] ogbg-molbace: 0.612454 val loss: 3.330054
[Epoch 75] ogbg-molbace: 0.739350 test loss: 3.422562
[Epoch 76; Iter    15/   41] train: loss: 0.0176852
[Epoch 76] ogbg-molbace: 0.662271 val loss: 1.699029
[Epoch 76] ogbg-molbace: 0.796731 test loss: 1.535301
[Epoch 77; Iter     4/   41] train: loss: 0.0095684
[Epoch 77; Iter    34/   41] train: loss: 0.0054782
[Epoch 77] ogbg-molbace: 0.658974 val loss: 2.348847
[Epoch 77] ogbg-molbace: 0.792732 test loss: 2.656519
[Epoch 78; Iter    23/   41] train: loss: 0.0107047
[Epoch 32] ogbg-molbace: 0.792036 test loss: 0.809749
[Epoch 33; Iter     8/   41] train: loss: 0.3620505
[Epoch 33; Iter    38/   41] train: loss: 0.3937176
[Epoch 33] ogbg-molbace: 0.719780 val loss: 1.119813
[Epoch 33] ogbg-molbace: 0.801774 test loss: 1.284502
[Epoch 34; Iter    27/   41] train: loss: 0.4436733
[Epoch 34] ogbg-molbace: 0.706960 val loss: 0.885338
[Epoch 34] ogbg-molbace: 0.800035 test loss: 1.019031
[Epoch 35; Iter    16/   41] train: loss: 0.2793368
[Epoch 35] ogbg-molbace: 0.702198 val loss: 0.856802
[Epoch 35] ogbg-molbace: 0.735350 test loss: 1.010058
[Epoch 36; Iter     5/   41] train: loss: 0.4420201
[Epoch 36; Iter    35/   41] train: loss: 0.3722818
[Epoch 36] ogbg-molbace: 0.684615 val loss: 1.420504
[Epoch 36] ogbg-molbace: 0.755695 test loss: 0.977667
[Epoch 37; Iter    24/   41] train: loss: 0.2469556
[Epoch 37] ogbg-molbace: 0.690476 val loss: 0.642670
[Epoch 37] ogbg-molbace: 0.805773 test loss: 0.547337
[Epoch 38; Iter    13/   41] train: loss: 0.3220268
[Epoch 38] ogbg-molbace: 0.741026 val loss: 1.157952
[Epoch 38] ogbg-molbace: 0.807512 test loss: 1.326463
[Epoch 39; Iter     2/   41] train: loss: 0.3832110
[Epoch 39; Iter    32/   41] train: loss: 0.4024355
[Epoch 39] ogbg-molbace: 0.711355 val loss: 1.118765
[Epoch 39] ogbg-molbace: 0.821596 test loss: 1.069943
[Epoch 40; Iter    21/   41] train: loss: 0.2790284
[Epoch 40] ogbg-molbace: 0.713553 val loss: 0.879039
[Epoch 40] ogbg-molbace: 0.855503 test loss: 0.757382
[Epoch 41; Iter    10/   41] train: loss: 0.4623299
[Epoch 41; Iter    40/   41] train: loss: 0.1909890
[Epoch 41] ogbg-molbace: 0.736264 val loss: 1.171608
[Epoch 41] ogbg-molbace: 0.810642 test loss: 0.864825
[Epoch 42; Iter    29/   41] train: loss: 0.3780653
[Epoch 42] ogbg-molbace: 0.725641 val loss: 0.782356
[Epoch 42] ogbg-molbace: 0.825074 test loss: 0.719572
[Epoch 43; Iter    18/   41] train: loss: 0.2358595
[Epoch 43] ogbg-molbace: 0.707692 val loss: 0.992124
[Epoch 43] ogbg-molbace: 0.844549 test loss: 0.677572
[Epoch 44; Iter     7/   41] train: loss: 0.3653361
[Epoch 44; Iter    37/   41] train: loss: 0.3222233
[Epoch 44] ogbg-molbace: 0.778388 val loss: 0.727126
[Epoch 44] ogbg-molbace: 0.795166 test loss: 0.651203
[Epoch 45; Iter    26/   41] train: loss: 0.2733331
[Epoch 45] ogbg-molbace: 0.753114 val loss: 0.692341
[Epoch 45] ogbg-molbace: 0.813598 test loss: 0.890538
[Epoch 46; Iter    15/   41] train: loss: 0.2719332
[Epoch 46] ogbg-molbace: 0.692308 val loss: 1.285047
[Epoch 46] ogbg-molbace: 0.825769 test loss: 1.181457
[Epoch 47; Iter     4/   41] train: loss: 0.2520310
[Epoch 47; Iter    34/   41] train: loss: 0.2722585
[Epoch 47] ogbg-molbace: 0.749084 val loss: 0.944652
[Epoch 47] ogbg-molbace: 0.815858 test loss: 0.797008
[Epoch 48; Iter    23/   41] train: loss: 0.2691644
[Epoch 48] ogbg-molbace: 0.657509 val loss: 0.972318
[Epoch 48] ogbg-molbace: 0.801947 test loss: 0.893930
[Epoch 49; Iter    12/   41] train: loss: 0.3816583
[Epoch 49] ogbg-molbace: 0.758608 val loss: 0.821694
[Epoch 49] ogbg-molbace: 0.810989 test loss: 0.779163
[Epoch 50; Iter     1/   41] train: loss: 0.3982593
[Epoch 50; Iter    31/   41] train: loss: 0.3762581
[Epoch 50] ogbg-molbace: 0.762271 val loss: 0.943228
[Epoch 50] ogbg-molbace: 0.805599 test loss: 0.977538
[Epoch 51; Iter    20/   41] train: loss: 0.4240212
[Epoch 51] ogbg-molbace: 0.712088 val loss: 1.091427
[Epoch 51] ogbg-molbace: 0.793775 test loss: 1.183946
[Epoch 52; Iter     9/   41] train: loss: 0.2549265
[Epoch 52; Iter    39/   41] train: loss: 0.3104953
[Epoch 52] ogbg-molbace: 0.763004 val loss: 1.033159
[Epoch 52] ogbg-molbace: 0.798644 test loss: 0.644496
[Epoch 53; Iter    28/   41] train: loss: 0.1309527
[Epoch 53] ogbg-molbace: 0.756410 val loss: 1.017917
[Epoch 53] ogbg-molbace: 0.842810 test loss: 0.663000
[Epoch 54; Iter    17/   41] train: loss: 0.4615006
[Epoch 54] ogbg-molbace: 0.739927 val loss: 1.194477
[Epoch 54] ogbg-molbace: 0.774648 test loss: 1.287451
[Epoch 55; Iter     6/   41] train: loss: 0.2849522
[Epoch 55; Iter    36/   41] train: loss: 0.2305767
[Epoch 55] ogbg-molbace: 0.712088 val loss: 1.196908
[Epoch 55] ogbg-molbace: 0.842288 test loss: 0.982273
[Epoch 56; Iter    25/   41] train: loss: 0.2192322
[Epoch 56] ogbg-molbace: 0.721245 val loss: 1.004792
[Epoch 56] ogbg-molbace: 0.799513 test loss: 0.905245
[Epoch 57; Iter    14/   41] train: loss: 0.2450780
[Epoch 57] ogbg-molbace: 0.710623 val loss: 1.092337
[Epoch 57] ogbg-molbace: 0.833073 test loss: 0.864782
[Epoch 58; Iter     3/   41] train: loss: 0.2616720
[Epoch 58; Iter    33/   41] train: loss: 0.3483783
[Epoch 58] ogbg-molbace: 0.716850 val loss: 1.024212
[Epoch 58] ogbg-molbace: 0.780908 test loss: 1.382300
[Epoch 59; Iter    22/   41] train: loss: 0.4448687
[Epoch 59] ogbg-molbace: 0.736264 val loss: 0.969319
[Epoch 59] ogbg-molbace: 0.829595 test loss: 1.161225
[Epoch 60; Iter    11/   41] train: loss: 0.3107994
[Epoch 60; Iter    41/   41] train: loss: 0.0658163
[Epoch 60] ogbg-molbace: 0.745788 val loss: 1.351229
[Epoch 60] ogbg-molbace: 0.832551 test loss: 1.146755
[Epoch 61; Iter    30/   41] train: loss: 0.1868380
[Epoch 61] ogbg-molbace: 0.747985 val loss: 1.132993
[Epoch 61] ogbg-molbace: 0.812380 test loss: 0.783054
[Epoch 62; Iter    19/   41] train: loss: 0.1524297
[Epoch 62] ogbg-molbace: 0.752015 val loss: 1.188851
[Epoch 62] ogbg-molbace: 0.830290 test loss: 0.847467
[Epoch 63; Iter     8/   41] train: loss: 0.2611747
[Epoch 63; Iter    38/   41] train: loss: 0.2569570
[Epoch 63] ogbg-molbace: 0.756777 val loss: 1.043574
[Epoch 63] ogbg-molbace: 0.806295 test loss: 0.960514
[Epoch 64; Iter    27/   41] train: loss: 0.4477177
[Epoch 64] ogbg-molbace: 0.756777 val loss: 0.953314
[Epoch 64] ogbg-molbace: 0.830117 test loss: 0.754816
[Epoch 65; Iter    16/   41] train: loss: 0.2248435
[Epoch 65] ogbg-molbace: 0.729304 val loss: 1.217808
[Epoch 65] ogbg-molbace: 0.810816 test loss: 1.384029
[Epoch 66; Iter     5/   41] train: loss: 0.1590987
[Epoch 66; Iter    35/   41] train: loss: 0.3583245
[Epoch 66] ogbg-molbace: 0.780220 val loss: 1.211556
[Epoch 66] ogbg-molbace: 0.799339 test loss: 0.737721
[Epoch 67; Iter    24/   41] train: loss: 0.1781667
[Epoch 67] ogbg-molbace: 0.797802 val loss: 0.890527
[Epoch 67] ogbg-molbace: 0.823857 test loss: 0.702887
[Epoch 68; Iter    13/   41] train: loss: 0.1510113
[Epoch 68] ogbg-molbace: 0.701099 val loss: 1.491028
[Epoch 68] ogbg-molbace: 0.823857 test loss: 2.354469
[Epoch 69; Iter     2/   41] train: loss: 0.2422836
[Epoch 69; Iter    32/   41] train: loss: 0.2734334
[Epoch 69] ogbg-molbace: 0.747619 val loss: 1.284808
[Epoch 69] ogbg-molbace: 0.825596 test loss: 0.897630
[Epoch 70; Iter    21/   41] train: loss: 0.3356890
[Epoch 70] ogbg-molbace: 0.748352 val loss: 1.151226
[Epoch 70] ogbg-molbace: 0.826291 test loss: 1.113953
[Epoch 71; Iter    10/   41] train: loss: 0.2409789
[Epoch 71; Iter    40/   41] train: loss: 0.1244326
[Epoch 71] ogbg-molbace: 0.799634 val loss: 0.635215
[Epoch 71] ogbg-molbace: 0.812554 test loss: 0.723136
[Epoch 72; Iter    29/   41] train: loss: 0.2170946
[Epoch 72] ogbg-molbace: 0.686813 val loss: 2.014324
[Epoch 72] ogbg-molbace: 0.828552 test loss: 1.341555
[Epoch 73; Iter    18/   41] train: loss: 0.0921099
[Epoch 73] ogbg-molbace: 0.705861 val loss: 0.920974
[Epoch 73] ogbg-molbace: 0.768910 test loss: 1.640924
[Epoch 74; Iter     7/   41] train: loss: 0.1608022
[Epoch 74; Iter    37/   41] train: loss: 0.2697340
[Epoch 74] ogbg-molbace: 0.747985 val loss: 1.362909
[Epoch 74] ogbg-molbace: 0.807686 test loss: 0.958616
[Epoch 75; Iter    26/   41] train: loss: 0.1409686
[Epoch 75] ogbg-molbace: 0.745421 val loss: 1.148197
[Epoch 75] ogbg-molbace: 0.800904 test loss: 1.277409
[Epoch 76; Iter    15/   41] train: loss: 0.0549665
[Epoch 76] ogbg-molbace: 0.766667 val loss: 1.163849
[Epoch 76] ogbg-molbace: 0.795862 test loss: 1.159340
[Epoch 77; Iter     4/   41] train: loss: 0.0364140
[Epoch 77; Iter    34/   41] train: loss: 0.0749471
[Epoch 77] ogbg-molbace: 0.756777 val loss: 0.968278
[Epoch 77] ogbg-molbace: 0.793427 test loss: 1.115385
[Epoch 78; Iter    23/   41] train: loss: 0.3533304
[Epoch 32] ogbg-molbace: 0.772909 test loss: 1.392841
[Epoch 33; Iter     8/   41] train: loss: 0.6361281
[Epoch 33; Iter    38/   41] train: loss: 0.6096950
[Epoch 33] ogbg-molbace: 0.658608 val loss: 0.846697
[Epoch 33] ogbg-molbace: 0.801426 test loss: 0.827739
[Epoch 34; Iter    27/   41] train: loss: 0.5419636
[Epoch 34] ogbg-molbace: 0.718315 val loss: 0.864469
[Epoch 34] ogbg-molbace: 0.814119 test loss: 1.007071
[Epoch 35; Iter    16/   41] train: loss: 0.3936158
[Epoch 35] ogbg-molbace: 0.680952 val loss: 0.972315
[Epoch 35] ogbg-molbace: 0.792732 test loss: 1.035091
[Epoch 36; Iter     5/   41] train: loss: 0.4430000
[Epoch 36; Iter    35/   41] train: loss: 0.3764144
[Epoch 36] ogbg-molbace: 0.705495 val loss: 0.829568
[Epoch 36] ogbg-molbace: 0.839680 test loss: 0.982195
[Epoch 37; Iter    24/   41] train: loss: 0.4959471
[Epoch 37] ogbg-molbace: 0.688278 val loss: 0.782517
[Epoch 37] ogbg-molbace: 0.822640 test loss: 0.702124
[Epoch 38; Iter    13/   41] train: loss: 0.3895800
[Epoch 38] ogbg-molbace: 0.742491 val loss: 0.809706
[Epoch 38] ogbg-molbace: 0.803860 test loss: 0.647537
[Epoch 39; Iter     2/   41] train: loss: 0.5700966
[Epoch 39; Iter    32/   41] train: loss: 0.4439177
[Epoch 39] ogbg-molbace: 0.772894 val loss: 0.681897
[Epoch 39] ogbg-molbace: 0.776213 test loss: 0.706781
[Epoch 40; Iter    21/   41] train: loss: 0.2652986
[Epoch 40] ogbg-molbace: 0.751282 val loss: 0.842334
[Epoch 40] ogbg-molbace: 0.830812 test loss: 0.867272
[Epoch 41; Iter    10/   41] train: loss: 0.5771459
[Epoch 41; Iter    40/   41] train: loss: 0.4940590
[Epoch 41] ogbg-molbace: 0.695604 val loss: 1.037421
[Epoch 41] ogbg-molbace: 0.811163 test loss: 0.874388
[Epoch 42; Iter    29/   41] train: loss: 0.3825164
[Epoch 42] ogbg-molbace: 0.743223 val loss: 0.690563
[Epoch 42] ogbg-molbace: 0.773952 test loss: 0.811899
[Epoch 43; Iter    18/   41] train: loss: 0.3576148
[Epoch 43] ogbg-molbace: 0.693040 val loss: 0.729967
[Epoch 43] ogbg-molbace: 0.734655 test loss: 1.211549
[Epoch 44; Iter     7/   41] train: loss: 0.3520736
[Epoch 44; Iter    37/   41] train: loss: 0.4242525
[Epoch 44] ogbg-molbace: 0.730403 val loss: 0.807695
[Epoch 44] ogbg-molbace: 0.796557 test loss: 0.712638
[Epoch 45; Iter    26/   41] train: loss: 0.2847494
[Epoch 45] ogbg-molbace: 0.670330 val loss: 1.256141
[Epoch 45] ogbg-molbace: 0.801426 test loss: 1.271245
[Epoch 46; Iter    15/   41] train: loss: 0.3130619
[Epoch 46] ogbg-molbace: 0.709890 val loss: 0.996351
[Epoch 46] ogbg-molbace: 0.807686 test loss: 0.820035
[Epoch 47; Iter     4/   41] train: loss: 0.2909724
[Epoch 47; Iter    34/   41] train: loss: 0.2382556
[Epoch 47] ogbg-molbace: 0.763736 val loss: 0.655124
[Epoch 47] ogbg-molbace: 0.812902 test loss: 0.646804
[Epoch 48; Iter    23/   41] train: loss: 0.2212663
[Epoch 48] ogbg-molbace: 0.752747 val loss: 1.248031
[Epoch 48] ogbg-molbace: 0.818988 test loss: 1.031132
[Epoch 49; Iter    12/   41] train: loss: 0.3964114
[Epoch 49] ogbg-molbace: 0.756044 val loss: 0.946193
[Epoch 49] ogbg-molbace: 0.833246 test loss: 0.948992
[Epoch 50; Iter     1/   41] train: loss: 0.3914080
[Epoch 50; Iter    31/   41] train: loss: 0.3126180
[Epoch 50] ogbg-molbace: 0.719414 val loss: 1.089300
[Epoch 50] ogbg-molbace: 0.825422 test loss: 0.652088
[Epoch 51; Iter    20/   41] train: loss: 0.3527275
[Epoch 51] ogbg-molbace: 0.679121 val loss: 1.264526
[Epoch 51] ogbg-molbace: 0.820901 test loss: 1.082978
[Epoch 52; Iter     9/   41] train: loss: 0.3450890
[Epoch 52; Iter    39/   41] train: loss: 0.2085010
[Epoch 52] ogbg-molbace: 0.741392 val loss: 1.107656
[Epoch 52] ogbg-molbace: 0.835855 test loss: 0.694401
[Epoch 53; Iter    28/   41] train: loss: 0.2910205
[Epoch 53] ogbg-molbace: 0.790110 val loss: 1.177997
[Epoch 53] ogbg-molbace: 0.790645 test loss: 1.141011
[Epoch 54; Iter    17/   41] train: loss: 0.2155414
[Epoch 54] ogbg-molbace: 0.752747 val loss: 0.923430
[Epoch 54] ogbg-molbace: 0.805773 test loss: 1.040678
[Epoch 55; Iter     6/   41] train: loss: 0.1358785
[Epoch 55; Iter    36/   41] train: loss: 0.2630475
[Epoch 55] ogbg-molbace: 0.678755 val loss: 1.014548
[Epoch 55] ogbg-molbace: 0.801600 test loss: 0.741145
[Epoch 56; Iter    25/   41] train: loss: 0.2470646
[Epoch 56] ogbg-molbace: 0.728938 val loss: 1.034132
[Epoch 56] ogbg-molbace: 0.816728 test loss: 0.770598
[Epoch 57; Iter    14/   41] train: loss: 0.3604282
[Epoch 57] ogbg-molbace: 0.734432 val loss: 1.191010
[Epoch 57] ogbg-molbace: 0.813772 test loss: 0.962382
[Epoch 58; Iter     3/   41] train: loss: 0.3853139
[Epoch 58; Iter    33/   41] train: loss: 0.1847769
[Epoch 58] ogbg-molbace: 0.704029 val loss: 1.189999
[Epoch 58] ogbg-molbace: 0.845070 test loss: 0.942263
[Epoch 59; Iter    22/   41] train: loss: 0.2089462
[Epoch 59] ogbg-molbace: 0.691209 val loss: 0.972432
[Epoch 59] ogbg-molbace: 0.798644 test loss: 0.937573
[Epoch 60; Iter    11/   41] train: loss: 0.4249853
[Epoch 60; Iter    41/   41] train: loss: 0.5678830
[Epoch 60] ogbg-molbace: 0.751282 val loss: 0.969474
[Epoch 60] ogbg-molbace: 0.814641 test loss: 1.043802
[Epoch 61; Iter    30/   41] train: loss: 0.3282930
[Epoch 61] ogbg-molbace: 0.708425 val loss: 1.205404
[Epoch 61] ogbg-molbace: 0.796035 test loss: 1.016296
[Epoch 62; Iter    19/   41] train: loss: 0.2726347
[Epoch 62] ogbg-molbace: 0.738462 val loss: 1.047919
[Epoch 62] ogbg-molbace: 0.815510 test loss: 1.027623
[Epoch 63; Iter     8/   41] train: loss: 0.1607568
[Epoch 63; Iter    38/   41] train: loss: 0.2383620
[Epoch 63] ogbg-molbace: 0.709890 val loss: 1.575717
[Epoch 63] ogbg-molbace: 0.790819 test loss: 1.064288
[Epoch 64; Iter    27/   41] train: loss: 0.1911372
[Epoch 64] ogbg-molbace: 0.724908 val loss: 1.075772
[Epoch 64] ogbg-molbace: 0.815684 test loss: 0.715471
[Epoch 65; Iter    16/   41] train: loss: 0.4681217
[Epoch 65] ogbg-molbace: 0.787546 val loss: 0.828038
[Epoch 65] ogbg-molbace: 0.804208 test loss: 0.752173
[Epoch 66; Iter     5/   41] train: loss: 0.1154337
[Epoch 66; Iter    35/   41] train: loss: 0.3283191
[Epoch 66] ogbg-molbace: 0.693407 val loss: 1.276124
[Epoch 66] ogbg-molbace: 0.786472 test loss: 1.213837
[Epoch 67; Iter    24/   41] train: loss: 0.3948512
[Epoch 67] ogbg-molbace: 0.687546 val loss: 1.227575
[Epoch 67] ogbg-molbace: 0.795166 test loss: 1.023200
[Epoch 68; Iter    13/   41] train: loss: 0.1863836
[Epoch 68] ogbg-molbace: 0.702930 val loss: 1.710090
[Epoch 68] ogbg-molbace: 0.748913 test loss: 1.879018
[Epoch 69; Iter     2/   41] train: loss: 0.1849210
[Epoch 69; Iter    32/   41] train: loss: 0.1907131
[Epoch 69] ogbg-molbace: 0.751648 val loss: 1.201110
[Epoch 69] ogbg-molbace: 0.797079 test loss: 0.885810
[Epoch 70; Iter    21/   41] train: loss: 0.2612190
[Epoch 70] ogbg-molbace: 0.691941 val loss: 1.354071
[Epoch 70] ogbg-molbace: 0.772909 test loss: 1.146191
[Epoch 71; Iter    10/   41] train: loss: 0.1535434
[Epoch 71; Iter    40/   41] train: loss: 0.2420447
[Epoch 71] ogbg-molbace: 0.693407 val loss: 1.541857
[Epoch 71] ogbg-molbace: 0.824378 test loss: 0.959907
[Epoch 72; Iter    29/   41] train: loss: 0.1637999
[Epoch 72] ogbg-molbace: 0.743590 val loss: 1.057524
[Epoch 72] ogbg-molbace: 0.797774 test loss: 1.049368
[Epoch 73; Iter    18/   41] train: loss: 0.1002485
[Epoch 73] ogbg-molbace: 0.769963 val loss: 1.055225
[Epoch 73] ogbg-molbace: 0.792906 test loss: 1.237872
[Epoch 74; Iter     7/   41] train: loss: 0.1321370
[Epoch 74; Iter    37/   41] train: loss: 0.4067953
[Epoch 74] ogbg-molbace: 0.712088 val loss: 1.401088
[Epoch 74] ogbg-molbace: 0.768214 test loss: 1.227190
[Epoch 75; Iter    26/   41] train: loss: 0.2224472
[Epoch 75] ogbg-molbace: 0.708425 val loss: 1.173939
[Epoch 75] ogbg-molbace: 0.783864 test loss: 1.249416
[Epoch 76; Iter    15/   41] train: loss: 0.1605660
[Epoch 76] ogbg-molbace: 0.746886 val loss: 1.089177
[Epoch 76] ogbg-molbace: 0.790645 test loss: 1.063711
[Epoch 77; Iter     4/   41] train: loss: 0.0510027
[Epoch 77; Iter    34/   41] train: loss: 0.1642260
[Epoch 77] ogbg-molbace: 0.739927 val loss: 1.330349
[Epoch 77] ogbg-molbace: 0.802295 test loss: 1.268045
[Epoch 78; Iter    23/   41] train: loss: 0.0805399
[Epoch 32] ogbg-molbace: 0.754825 test loss: 0.998809
[Epoch 33; Iter     8/   41] train: loss: 0.3509471
[Epoch 33; Iter    38/   41] train: loss: 0.4138108
[Epoch 33] ogbg-molbace: 0.731136 val loss: 0.899247
[Epoch 33] ogbg-molbace: 0.807686 test loss: 0.967965
[Epoch 34; Iter    27/   41] train: loss: 0.3385219
[Epoch 34] ogbg-molbace: 0.656044 val loss: 1.474414
[Epoch 34] ogbg-molbace: 0.773257 test loss: 1.549633
[Epoch 35; Iter    16/   41] train: loss: 0.4415939
[Epoch 35] ogbg-molbace: 0.760806 val loss: 0.704046
[Epoch 35] ogbg-molbace: 0.801078 test loss: 0.789512
[Epoch 36; Iter     5/   41] train: loss: 0.3323199
[Epoch 36; Iter    35/   41] train: loss: 0.5010393
[Epoch 36] ogbg-molbace: 0.763004 val loss: 0.778846
[Epoch 36] ogbg-molbace: 0.816554 test loss: 1.059705
[Epoch 37; Iter    24/   41] train: loss: 0.4128405
[Epoch 37] ogbg-molbace: 0.715018 val loss: 0.766823
[Epoch 37] ogbg-molbace: 0.778126 test loss: 0.826490
[Epoch 38; Iter    13/   41] train: loss: 0.3559017
[Epoch 38] ogbg-molbace: 0.727473 val loss: 0.654209
[Epoch 38] ogbg-molbace: 0.811163 test loss: 0.851752
[Epoch 39; Iter     2/   41] train: loss: 0.3419325
[Epoch 39; Iter    32/   41] train: loss: 0.2786582
[Epoch 39] ogbg-molbace: 0.761905 val loss: 0.763111
[Epoch 39] ogbg-molbace: 0.788037 test loss: 0.810640
[Epoch 40; Iter    21/   41] train: loss: 0.2558460
[Epoch 40] ogbg-molbace: 0.618681 val loss: 0.839921
[Epoch 40] ogbg-molbace: 0.814641 test loss: 0.882563
[Epoch 41; Iter    10/   41] train: loss: 0.2206735
[Epoch 41; Iter    40/   41] train: loss: 0.5278370
[Epoch 41] ogbg-molbace: 0.760806 val loss: 1.010323
[Epoch 41] ogbg-molbace: 0.787863 test loss: 1.090797
[Epoch 42; Iter    29/   41] train: loss: 0.3585219
[Epoch 42] ogbg-molbace: 0.705128 val loss: 0.988814
[Epoch 42] ogbg-molbace: 0.798818 test loss: 1.183560
[Epoch 43; Iter    18/   41] train: loss: 0.3515219
[Epoch 43] ogbg-molbace: 0.696337 val loss: 0.830178
[Epoch 43] ogbg-molbace: 0.747870 test loss: 0.802023
[Epoch 44; Iter     7/   41] train: loss: 0.3609696
[Epoch 44; Iter    37/   41] train: loss: 0.3585179
[Epoch 44] ogbg-molbace: 0.767766 val loss: 0.718972
[Epoch 44] ogbg-molbace: 0.830464 test loss: 0.817267
[Epoch 45; Iter    26/   41] train: loss: 0.2383975
[Epoch 45] ogbg-molbace: 0.728571 val loss: 1.340301
[Epoch 45] ogbg-molbace: 0.837941 test loss: 1.634761
[Epoch 46; Iter    15/   41] train: loss: 0.3601436
[Epoch 46] ogbg-molbace: 0.755678 val loss: 1.057398
[Epoch 46] ogbg-molbace: 0.805077 test loss: 1.125832
[Epoch 47; Iter     4/   41] train: loss: 0.4617167
[Epoch 47; Iter    34/   41] train: loss: 0.4142144
[Epoch 47] ogbg-molbace: 0.717216 val loss: 1.018388
[Epoch 47] ogbg-molbace: 0.836202 test loss: 0.882375
[Epoch 48; Iter    23/   41] train: loss: 0.2563684
[Epoch 48] ogbg-molbace: 0.771062 val loss: 0.823049
[Epoch 48] ogbg-molbace: 0.803165 test loss: 1.090596
[Epoch 49; Iter    12/   41] train: loss: 0.3548733
[Epoch 49] ogbg-molbace: 0.717216 val loss: 0.974915
[Epoch 49] ogbg-molbace: 0.816032 test loss: 0.795132
[Epoch 50; Iter     1/   41] train: loss: 0.3511704
[Epoch 50; Iter    31/   41] train: loss: 0.3127769
[Epoch 50] ogbg-molbace: 0.750916 val loss: 0.882291
[Epoch 50] ogbg-molbace: 0.795688 test loss: 1.414882
[Epoch 51; Iter    20/   41] train: loss: 0.3680032
[Epoch 51] ogbg-molbace: 0.725641 val loss: 0.863551
[Epoch 51] ogbg-molbace: 0.807860 test loss: 0.982341
[Epoch 52; Iter     9/   41] train: loss: 0.3818888
[Epoch 52; Iter    39/   41] train: loss: 0.3029363
[Epoch 52] ogbg-molbace: 0.721245 val loss: 1.003220
[Epoch 52] ogbg-molbace: 0.793601 test loss: 1.020138
[Epoch 53; Iter    28/   41] train: loss: 0.1612584
[Epoch 53] ogbg-molbace: 0.740659 val loss: 1.094331
[Epoch 53] ogbg-molbace: 0.814467 test loss: 0.937914
[Epoch 54; Iter    17/   41] train: loss: 0.3835732
[Epoch 54] ogbg-molbace: 0.790842 val loss: 0.866414
[Epoch 54] ogbg-molbace: 0.827334 test loss: 0.939288
[Epoch 55; Iter     6/   41] train: loss: 0.5214929
[Epoch 55; Iter    36/   41] train: loss: 0.2665672
[Epoch 55] ogbg-molbace: 0.717949 val loss: 1.222221
[Epoch 55] ogbg-molbace: 0.800730 test loss: 1.272241
[Epoch 56; Iter    25/   41] train: loss: 0.2319091
[Epoch 56] ogbg-molbace: 0.744322 val loss: 1.269027
[Epoch 56] ogbg-molbace: 0.747870 test loss: 0.877501
[Epoch 57; Iter    14/   41] train: loss: 0.3689456
[Epoch 57] ogbg-molbace: 0.775824 val loss: 1.119794
[Epoch 57] ogbg-molbace: 0.803860 test loss: 1.142858
[Epoch 58; Iter     3/   41] train: loss: 0.3101985
[Epoch 58; Iter    33/   41] train: loss: 0.2649927
[Epoch 58] ogbg-molbace: 0.739560 val loss: 1.213122
[Epoch 58] ogbg-molbace: 0.784038 test loss: 1.263226
[Epoch 59; Iter    22/   41] train: loss: 0.2972901
[Epoch 59] ogbg-molbace: 0.742125 val loss: 0.976446
[Epoch 59] ogbg-molbace: 0.778821 test loss: 0.918360
[Epoch 60; Iter    11/   41] train: loss: 0.4691603
[Epoch 60; Iter    41/   41] train: loss: 0.1862651
[Epoch 60] ogbg-molbace: 0.670330 val loss: 1.001057
[Epoch 60] ogbg-molbace: 0.806816 test loss: 0.856162
[Epoch 61; Iter    30/   41] train: loss: 0.2792954
[Epoch 61] ogbg-molbace: 0.704762 val loss: 0.960960
[Epoch 61] ogbg-molbace: 0.786646 test loss: 1.330852
[Epoch 62; Iter    19/   41] train: loss: 0.2010244
[Epoch 62] ogbg-molbace: 0.715018 val loss: 1.362956
[Epoch 62] ogbg-molbace: 0.803339 test loss: 0.893822
[Epoch 63; Iter     8/   41] train: loss: 0.2897456
[Epoch 63; Iter    38/   41] train: loss: 0.1951061
[Epoch 63] ogbg-molbace: 0.699634 val loss: 1.231885
[Epoch 63] ogbg-molbace: 0.820553 test loss: 1.038037
[Epoch 64; Iter    27/   41] train: loss: 0.2591322
[Epoch 64] ogbg-molbace: 0.750916 val loss: 0.848023
[Epoch 64] ogbg-molbace: 0.829769 test loss: 0.740387
[Epoch 65; Iter    16/   41] train: loss: 0.1753936
[Epoch 65] ogbg-molbace: 0.705128 val loss: 1.070312
[Epoch 65] ogbg-molbace: 0.807338 test loss: 1.052312
[Epoch 66; Iter     5/   41] train: loss: 0.1507813
[Epoch 66; Iter    35/   41] train: loss: 0.2683054
[Epoch 66] ogbg-molbace: 0.727106 val loss: 0.924754
[Epoch 66] ogbg-molbace: 0.808381 test loss: 1.275341
[Epoch 67; Iter    24/   41] train: loss: 0.2572788
[Epoch 67] ogbg-molbace: 0.738828 val loss: 1.053365
[Epoch 67] ogbg-molbace: 0.810816 test loss: 1.255247
[Epoch 68; Iter    13/   41] train: loss: 0.5325435
[Epoch 68] ogbg-molbace: 0.726007 val loss: 1.043962
[Epoch 68] ogbg-molbace: 0.763346 test loss: 1.506471
[Epoch 69; Iter     2/   41] train: loss: 0.2422640
[Epoch 69; Iter    32/   41] train: loss: 0.3839411
[Epoch 69] ogbg-molbace: 0.751282 val loss: 1.077334
[Epoch 69] ogbg-molbace: 0.836376 test loss: 1.059637
[Epoch 70; Iter    21/   41] train: loss: 0.1335892
[Epoch 70] ogbg-molbace: 0.685348 val loss: 1.092696
[Epoch 70] ogbg-molbace: 0.796035 test loss: 1.273912
[Epoch 71; Iter    10/   41] train: loss: 0.2272616
[Epoch 71; Iter    40/   41] train: loss: 0.1771084
[Epoch 71] ogbg-molbace: 0.726740 val loss: 0.898383
[Epoch 71] ogbg-molbace: 0.822640 test loss: 1.039852
[Epoch 72; Iter    29/   41] train: loss: 0.0997613
[Epoch 72] ogbg-molbace: 0.737729 val loss: 0.985360
[Epoch 72] ogbg-molbace: 0.793775 test loss: 1.042647
[Epoch 73; Iter    18/   41] train: loss: 0.2716444
[Epoch 73] ogbg-molbace: 0.747253 val loss: 0.857197
[Epoch 73] ogbg-molbace: 0.809077 test loss: 1.231347
[Epoch 74; Iter     7/   41] train: loss: 0.2917707
[Epoch 74; Iter    37/   41] train: loss: 0.3620298
[Epoch 74] ogbg-molbace: 0.705495 val loss: 1.377121
[Epoch 74] ogbg-molbace: 0.812728 test loss: 1.015072
[Epoch 75; Iter    26/   41] train: loss: 0.2453335
[Epoch 75] ogbg-molbace: 0.680586 val loss: 1.428141
[Epoch 75] ogbg-molbace: 0.809077 test loss: 1.368800
[Epoch 76; Iter    15/   41] train: loss: 0.1827686
[Epoch 76] ogbg-molbace: 0.712821 val loss: 1.210700
[Epoch 76] ogbg-molbace: 0.834637 test loss: 1.139766
[Epoch 77; Iter     4/   41] train: loss: 0.1168718
[Epoch 77; Iter    34/   41] train: loss: 0.0620904
[Epoch 77] ogbg-molbace: 0.692674 val loss: 1.117361
[Epoch 77] ogbg-molbace: 0.727004 test loss: 1.861236
[Epoch 78; Iter    23/   41] train: loss: 0.1817841
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.673993 val loss: 1.701816
[Epoch 78] ogbg-molbace: 0.748044 test loss: 1.735779
[Epoch 79; Iter    12/   41] train: loss: 0.0013831
[Epoch 79] ogbg-molbace: 0.661905 val loss: 1.838390
[Epoch 79] ogbg-molbace: 0.728917 test loss: 2.036389
[Epoch 80; Iter     1/   41] train: loss: 0.0046832
[Epoch 80; Iter    31/   41] train: loss: 0.0008248
[Epoch 80] ogbg-molbace: 0.671429 val loss: 1.654786
[Epoch 80] ogbg-molbace: 0.756216 test loss: 1.829100
[Epoch 81; Iter    20/   41] train: loss: 0.0017205
[Epoch 81] ogbg-molbace: 0.655311 val loss: 1.659774
[Epoch 81] ogbg-molbace: 0.763693 test loss: 1.864688
[Epoch 82; Iter     9/   41] train: loss: 0.0012508
[Epoch 82; Iter    39/   41] train: loss: 0.0058818
[Epoch 82] ogbg-molbace: 0.655678 val loss: 1.795544
[Epoch 82] ogbg-molbace: 0.757086 test loss: 1.878110
[Epoch 83; Iter    28/   41] train: loss: 0.0006161
[Epoch 83] ogbg-molbace: 0.658974 val loss: 1.889118
[Epoch 83] ogbg-molbace: 0.738828 test loss: 2.143179
[Epoch 84; Iter    17/   41] train: loss: 0.0007534
[Epoch 84] ogbg-molbace: 0.664835 val loss: 1.818929
[Epoch 84] ogbg-molbace: 0.744914 test loss: 1.778848
[Epoch 85; Iter     6/   41] train: loss: 0.0087698
[Epoch 85; Iter    36/   41] train: loss: 0.0032831
[Epoch 85] ogbg-molbace: 0.663370 val loss: 1.824902
[Epoch 85] ogbg-molbace: 0.746827 test loss: 1.760818
[Epoch 86; Iter    25/   41] train: loss: 0.0031735
[Epoch 86] ogbg-molbace: 0.657875 val loss: 1.918526
[Epoch 86] ogbg-molbace: 0.744566 test loss: 1.739230
[Epoch 87; Iter    14/   41] train: loss: 0.0008655
[Epoch 87] ogbg-molbace: 0.665201 val loss: 1.968712
[Epoch 87] ogbg-molbace: 0.750826 test loss: 1.694373
[Epoch 88; Iter     3/   41] train: loss: 0.0003257
[Epoch 88; Iter    33/   41] train: loss: 0.0008454
[Epoch 88] ogbg-molbace: 0.668864 val loss: 2.033952
[Epoch 88] ogbg-molbace: 0.752391 test loss: 1.499660
[Epoch 89; Iter    22/   41] train: loss: 0.0022792
[Epoch 89] ogbg-molbace: 0.669963 val loss: 1.968045
[Epoch 89] ogbg-molbace: 0.753434 test loss: 1.617594
[Epoch 90; Iter    11/   41] train: loss: 0.0005482
[Epoch 90; Iter    41/   41] train: loss: 0.0051977
[Epoch 90] ogbg-molbace: 0.663004 val loss: 2.001071
[Epoch 90] ogbg-molbace: 0.745609 test loss: 1.653263
[Epoch 91; Iter    30/   41] train: loss: 0.0004177
[Epoch 91] ogbg-molbace: 0.676923 val loss: 1.734339
[Epoch 91] ogbg-molbace: 0.764737 test loss: 1.722623
[Epoch 92; Iter    19/   41] train: loss: 0.0008008
[Epoch 92] ogbg-molbace: 0.668132 val loss: 1.856651
[Epoch 92] ogbg-molbace: 0.748392 test loss: 1.774303
[Epoch 93; Iter     8/   41] train: loss: 0.0008367
[Epoch 93; Iter    38/   41] train: loss: 0.0004885
[Epoch 93] ogbg-molbace: 0.667399 val loss: 1.942413
[Epoch 93] ogbg-molbace: 0.748913 test loss: 1.730512
[Epoch 94; Iter    27/   41] train: loss: 0.0004249
[Epoch 94] ogbg-molbace: 0.655311 val loss: 2.128727
[Epoch 94] ogbg-molbace: 0.758825 test loss: 1.940270
[Epoch 95; Iter    16/   41] train: loss: 0.0011142
[Epoch 95] ogbg-molbace: 0.653480 val loss: 2.050211
[Epoch 95] ogbg-molbace: 0.754825 test loss: 1.790243
[Epoch 96; Iter     5/   41] train: loss: 0.0003781
[Epoch 96; Iter    35/   41] train: loss: 0.0003225
[Epoch 96] ogbg-molbace: 0.663370 val loss: 2.032225
[Epoch 96] ogbg-molbace: 0.751869 test loss: 1.614443
[Epoch 97; Iter    24/   41] train: loss: 0.0004998
[Epoch 97] ogbg-molbace: 0.662637 val loss: 2.056564
[Epoch 97] ogbg-molbace: 0.739871 test loss: 1.690472
[Epoch 98; Iter    13/   41] train: loss: 0.0021649
[Epoch 98] ogbg-molbace: 0.672527 val loss: 2.042564
[Epoch 98] ogbg-molbace: 0.749435 test loss: 1.585066
[Epoch 99; Iter     2/   41] train: loss: 0.0005540
[Epoch 99; Iter    32/   41] train: loss: 0.0006297
[Epoch 99] ogbg-molbace: 0.671429 val loss: 2.010553
[Epoch 99] ogbg-molbace: 0.753434 test loss: 1.657397
[Epoch 100; Iter    21/   41] train: loss: 0.0007903
[Epoch 100] ogbg-molbace: 0.668498 val loss: 2.032233
[Epoch 100] ogbg-molbace: 0.748044 test loss: 1.565616
[Epoch 101; Iter    10/   41] train: loss: 0.0010609
[Epoch 101; Iter    40/   41] train: loss: 0.0014157
[Epoch 101] ogbg-molbace: 0.667399 val loss: 2.036208
[Epoch 101] ogbg-molbace: 0.752565 test loss: 1.677469
[Epoch 102; Iter    29/   41] train: loss: 0.0005006
[Epoch 102] ogbg-molbace: 0.665934 val loss: 2.092405
[Epoch 102] ogbg-molbace: 0.751174 test loss: 1.598791
[Epoch 103; Iter    18/   41] train: loss: 0.0003073
[Epoch 103] ogbg-molbace: 0.660806 val loss: 2.114529
[Epoch 103] ogbg-molbace: 0.749087 test loss: 1.588737
[Epoch 104; Iter     7/   41] train: loss: 0.0003062
[Epoch 104; Iter    37/   41] train: loss: 0.0003555
[Epoch 104] ogbg-molbace: 0.660806 val loss: 2.079092
[Epoch 104] ogbg-molbace: 0.753782 test loss: 1.609842
[Epoch 105; Iter    26/   41] train: loss: 0.0007700
[Epoch 105] ogbg-molbace: 0.663370 val loss: 2.124466
[Epoch 105] ogbg-molbace: 0.753956 test loss: 1.491975
[Epoch 106; Iter    15/   41] train: loss: 0.0035094
[Epoch 106] ogbg-molbace: 0.662637 val loss: 2.108410
[Epoch 106] ogbg-molbace: 0.749609 test loss: 1.634670
[Epoch 107; Iter     4/   41] train: loss: 0.0004288
[Epoch 107; Iter    34/   41] train: loss: 0.0002484
[Epoch 107] ogbg-molbace: 0.666300 val loss: 2.177168
[Epoch 107] ogbg-molbace: 0.754304 test loss: 1.476068
[Epoch 108; Iter    23/   41] train: loss: 0.0029228
[Epoch 108] ogbg-molbace: 0.662637 val loss: 2.140028
[Epoch 108] ogbg-molbace: 0.755173 test loss: 1.717376
[Epoch 109; Iter    12/   41] train: loss: 0.0012111
[Epoch 109] ogbg-molbace: 0.660440 val loss: 2.007251
[Epoch 109] ogbg-molbace: 0.753260 test loss: 1.638550
[Epoch 110; Iter     1/   41] train: loss: 0.0004167
[Epoch 110; Iter    31/   41] train: loss: 0.0002627
[Epoch 110] ogbg-molbace: 0.661172 val loss: 2.011698
[Epoch 110] ogbg-molbace: 0.750826 test loss: 1.740487
[Epoch 111; Iter    20/   41] train: loss: 0.0001802
[Epoch 111] ogbg-molbace: 0.665568 val loss: 2.101657
[Epoch 111] ogbg-molbace: 0.745609 test loss: 1.693874
[Epoch 112; Iter     9/   41] train: loss: 0.0003117
[Epoch 112; Iter    39/   41] train: loss: 0.0001963
[Epoch 112] ogbg-molbace: 0.661172 val loss: 2.158475
[Epoch 112] ogbg-molbace: 0.744740 test loss: 1.842671
[Epoch 113; Iter    28/   41] train: loss: 0.0002980
[Epoch 113] ogbg-molbace: 0.662271 val loss: 2.002381
[Epoch 113] ogbg-molbace: 0.758651 test loss: 1.306404
[Epoch 114; Iter    17/   41] train: loss: 0.0007774
[Epoch 114] ogbg-molbace: 0.660806 val loss: 2.138720
[Epoch 114] ogbg-molbace: 0.749087 test loss: 1.374366
[Epoch 115; Iter     6/   41] train: loss: 0.0002221
[Epoch 115; Iter    36/   41] train: loss: 0.0007530
[Epoch 115] ogbg-molbace: 0.656044 val loss: 2.192097
[Epoch 115] ogbg-molbace: 0.746305 test loss: 1.498534
[Epoch 116; Iter    25/   41] train: loss: 0.0005474
[Epoch 116] ogbg-molbace: 0.652747 val loss: 2.205933
[Epoch 116] ogbg-molbace: 0.744740 test loss: 1.457968
[Epoch 117; Iter    14/   41] train: loss: 0.0010430
[Epoch 117] ogbg-molbace: 0.657143 val loss: 2.188032
[Epoch 117] ogbg-molbace: 0.749957 test loss: 1.460567
[Epoch 118; Iter     3/   41] train: loss: 0.0003123
[Epoch 118; Iter    33/   41] train: loss: 0.0006870
[Epoch 118] ogbg-molbace: 0.657875 val loss: 2.180797
[Epoch 118] ogbg-molbace: 0.748218 test loss: 1.613814
[Epoch 119; Iter    22/   41] train: loss: 0.0006596
[Epoch 119] ogbg-molbace: 0.657875 val loss: 2.192528
[Epoch 119] ogbg-molbace: 0.744218 test loss: 1.448205
[Epoch 120; Iter    11/   41] train: loss: 0.0001753
[Epoch 120; Iter    41/   41] train: loss: 0.0004769
[Epoch 120] ogbg-molbace: 0.660073 val loss: 2.253925
[Epoch 120] ogbg-molbace: 0.750478 test loss: 1.429129
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 44.
Statistics on  val_best_checkpoint
mean_pred: 0.9983112215995789
std_pred: 3.240950345993042
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9497838351332183
rocauc: 0.7388278388278389
ogbg-molbace: 0.7388278388278389
BCEWithLogitsLoss: 0.8521062036355337
Statistics on  test
mean_pred: -1.114294409751892
std_pred: 3.1414191722869873
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7635243938600216
rocauc: 0.7577812554338376
ogbg-molbace: 0.7577812554338376
BCEWithLogitsLoss: 1.0794642915328343
Statistics on  train
mean_pred: -0.10611333698034286
std_pred: 3.335458278656006
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9477411288633578
rocauc: 0.96525399543379
ogbg-molbace: 0.96525399543379
BCEWithLogitsLoss: 0.2659707879874764
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.641758 val loss: 1.147306
[Epoch 78] ogbg-molbace: 0.710311 test loss: 2.994034
[Epoch 79; Iter    12/   41] train: loss: 0.0025275
[Epoch 79] ogbg-molbace: 0.638828 val loss: 1.033571
[Epoch 79] ogbg-molbace: 0.714832 test loss: 3.161932
[Epoch 80; Iter     1/   41] train: loss: 0.0127977
[Epoch 80; Iter    31/   41] train: loss: 0.0043116
[Epoch 80] ogbg-molbace: 0.638462 val loss: 1.087337
[Epoch 80] ogbg-molbace: 0.726830 test loss: 2.777406
[Epoch 81; Iter    20/   41] train: loss: 0.0025241
[Epoch 81] ogbg-molbace: 0.636264 val loss: 1.059574
[Epoch 81] ogbg-molbace: 0.718310 test loss: 2.639339
[Epoch 82; Iter     9/   41] train: loss: 0.0020219
[Epoch 82; Iter    39/   41] train: loss: 0.0032815
[Epoch 82] ogbg-molbace: 0.631136 val loss: 1.060241
[Epoch 82] ogbg-molbace: 0.725613 test loss: 2.804028
[Epoch 83; Iter    28/   41] train: loss: 0.0014029
[Epoch 83] ogbg-molbace: 0.641392 val loss: 1.215124
[Epoch 83] ogbg-molbace: 0.724917 test loss: 2.331148
[Epoch 84; Iter    17/   41] train: loss: 0.0023330
[Epoch 84] ogbg-molbace: 0.629304 val loss: 1.055866
[Epoch 84] ogbg-molbace: 0.723874 test loss: 2.794421
[Epoch 85; Iter     6/   41] train: loss: 0.0015562
[Epoch 85; Iter    36/   41] train: loss: 0.0010178
[Epoch 85] ogbg-molbace: 0.637729 val loss: 0.976454
[Epoch 85] ogbg-molbace: 0.713267 test loss: 2.521666
[Epoch 86; Iter    25/   41] train: loss: 0.0024650
[Epoch 86] ogbg-molbace: 0.645788 val loss: 1.169235
[Epoch 86] ogbg-molbace: 0.731873 test loss: 2.107111
[Epoch 87; Iter    14/   41] train: loss: 0.0147342
[Epoch 87] ogbg-molbace: 0.645055 val loss: 0.947346
[Epoch 87] ogbg-molbace: 0.723874 test loss: 2.348919
[Epoch 88; Iter     3/   41] train: loss: 0.0029114
[Epoch 88; Iter    33/   41] train: loss: 0.0017108
[Epoch 88] ogbg-molbace: 0.645055 val loss: 0.982426
[Epoch 88] ogbg-molbace: 0.725961 test loss: 2.334059
[Epoch 89; Iter    22/   41] train: loss: 0.0084219
[Epoch 89] ogbg-molbace: 0.649451 val loss: 1.004854
[Epoch 89] ogbg-molbace: 0.731351 test loss: 2.144479
[Epoch 90; Iter    11/   41] train: loss: 0.0086352
[Epoch 90; Iter    41/   41] train: loss: 0.0096297
[Epoch 90] ogbg-molbace: 0.629670 val loss: 1.006945
[Epoch 90] ogbg-molbace: 0.723700 test loss: 2.527175
[Epoch 91; Iter    30/   41] train: loss: 0.0023940
[Epoch 91] ogbg-molbace: 0.639194 val loss: 1.110142
[Epoch 91] ogbg-molbace: 0.724048 test loss: 2.309111
[Epoch 92; Iter    19/   41] train: loss: 0.0015027
[Epoch 92] ogbg-molbace: 0.645788 val loss: 1.139074
[Epoch 92] ogbg-molbace: 0.728047 test loss: 2.104652
[Epoch 93; Iter     8/   41] train: loss: 0.0006973
[Epoch 93; Iter    38/   41] train: loss: 0.0113836
[Epoch 93] ogbg-molbace: 0.645788 val loss: 1.022947
[Epoch 93] ogbg-molbace: 0.727352 test loss: 2.544714
[Epoch 94; Iter    27/   41] train: loss: 0.0011491
[Epoch 94] ogbg-molbace: 0.636996 val loss: 0.927102
[Epoch 94] ogbg-molbace: 0.720918 test loss: 3.104715
[Epoch 95; Iter    16/   41] train: loss: 0.0227868
[Epoch 95] ogbg-molbace: 0.624176 val loss: 1.003595
[Epoch 95] ogbg-molbace: 0.717788 test loss: 3.313132
[Epoch 96; Iter     5/   41] train: loss: 0.0010096
[Epoch 96; Iter    35/   41] train: loss: 0.0088653
[Epoch 96] ogbg-molbace: 0.627473 val loss: 1.017413
[Epoch 96] ogbg-molbace: 0.720223 test loss: 3.159547
[Epoch 97; Iter    24/   41] train: loss: 0.0145161
[Epoch 97] ogbg-molbace: 0.637363 val loss: 1.047626
[Epoch 97] ogbg-molbace: 0.716049 test loss: 2.720667
[Epoch 98; Iter    13/   41] train: loss: 0.0019798
[Epoch 98] ogbg-molbace: 0.638095 val loss: 1.043026
[Epoch 98] ogbg-molbace: 0.720570 test loss: 2.716128
[Epoch 99; Iter     2/   41] train: loss: 0.0006368
[Epoch 99; Iter    32/   41] train: loss: 0.0020640
[Epoch 99] ogbg-molbace: 0.626374 val loss: 1.078625
[Epoch 99] ogbg-molbace: 0.714484 test loss: 2.871110
[Epoch 100; Iter    21/   41] train: loss: 0.0015131
[Epoch 100] ogbg-molbace: 0.631502 val loss: 1.009187
[Epoch 100] ogbg-molbace: 0.714137 test loss: 2.970301
[Epoch 101; Iter    10/   41] train: loss: 0.0003965
[Epoch 101; Iter    40/   41] train: loss: 0.0008701
[Epoch 101] ogbg-molbace: 0.640659 val loss: 1.090712
[Epoch 101] ogbg-molbace: 0.720223 test loss: 2.542855
[Epoch 102; Iter    29/   41] train: loss: 0.0020703
[Epoch 102] ogbg-molbace: 0.638828 val loss: 1.046595
[Epoch 102] ogbg-molbace: 0.719527 test loss: 2.493732
[Epoch 103; Iter    18/   41] train: loss: 0.0014161
[Epoch 103] ogbg-molbace: 0.634799 val loss: 1.021868
[Epoch 103] ogbg-molbace: 0.715354 test loss: 2.762311
[Epoch 104; Iter     7/   41] train: loss: 0.0005370
[Epoch 104; Iter    37/   41] train: loss: 0.0005520
[Epoch 104] ogbg-molbace: 0.638828 val loss: 1.210253
[Epoch 104] ogbg-molbace: 0.728221 test loss: 2.145101
[Epoch 105; Iter    26/   41] train: loss: 0.0005431
[Epoch 105] ogbg-molbace: 0.638462 val loss: 1.063940
[Epoch 105] ogbg-molbace: 0.725961 test loss: 2.514806
[Epoch 106; Iter    15/   41] train: loss: 0.0013737
[Epoch 106] ogbg-molbace: 0.642125 val loss: 1.235233
[Epoch 106] ogbg-molbace: 0.728743 test loss: 2.335705
[Epoch 107; Iter     4/   41] train: loss: 0.0004760
[Epoch 107; Iter    34/   41] train: loss: 0.0023432
[Epoch 107] ogbg-molbace: 0.624176 val loss: 1.044456
[Epoch 107] ogbg-molbace: 0.705269 test loss: 2.769588
[Epoch 108; Iter    23/   41] train: loss: 0.0013337
[Epoch 108] ogbg-molbace: 0.638462 val loss: 1.391230
[Epoch 108] ogbg-molbace: 0.722135 test loss: 2.105539
[Epoch 109; Iter    12/   41] train: loss: 0.0018164
[Epoch 109] ogbg-molbace: 0.644689 val loss: 1.330641
[Epoch 109] ogbg-molbace: 0.742480 test loss: 1.813878
[Epoch 110; Iter     1/   41] train: loss: 0.0009327
[Epoch 110; Iter    31/   41] train: loss: 0.0019683
[Epoch 110] ogbg-molbace: 0.638828 val loss: 1.369811
[Epoch 110] ogbg-molbace: 0.740219 test loss: 1.748890
[Epoch 111; Iter    20/   41] train: loss: 0.0017396
[Epoch 111] ogbg-molbace: 0.616850 val loss: 1.297433
[Epoch 111] ogbg-molbace: 0.704573 test loss: 2.407631
[Epoch 112; Iter     9/   41] train: loss: 0.0014276
[Epoch 112; Iter    39/   41] train: loss: 0.0008222
[Epoch 112] ogbg-molbace: 0.606960 val loss: 1.253890
[Epoch 112] ogbg-molbace: 0.699357 test loss: 2.811879
[Epoch 113; Iter    28/   41] train: loss: 0.0006479
[Epoch 113] ogbg-molbace: 0.604396 val loss: 1.212580
[Epoch 113] ogbg-molbace: 0.701095 test loss: 2.827695
[Epoch 114; Iter    17/   41] train: loss: 0.0010125
[Epoch 114] ogbg-molbace: 0.604762 val loss: 1.183145
[Epoch 114] ogbg-molbace: 0.699009 test loss: 3.070465
[Epoch 115; Iter     6/   41] train: loss: 0.0011090
[Epoch 115; Iter    36/   41] train: loss: 0.0022361
[Epoch 115] ogbg-molbace: 0.604396 val loss: 1.135611
[Epoch 115] ogbg-molbace: 0.704051 test loss: 2.928424
[Epoch 116; Iter    25/   41] train: loss: 0.0005215
[Epoch 116] ogbg-molbace: 0.602198 val loss: 1.163560
[Epoch 116] ogbg-molbace: 0.709616 test loss: 2.574221
[Epoch 117; Iter    14/   41] train: loss: 0.0005236
[Epoch 117] ogbg-molbace: 0.609158 val loss: 1.174401
[Epoch 117] ogbg-molbace: 0.713441 test loss: 2.468021
[Epoch 118; Iter     3/   41] train: loss: 0.0006946
[Epoch 118; Iter    33/   41] train: loss: 0.0003733
[Epoch 118] ogbg-molbace: 0.610623 val loss: 1.154053
[Epoch 118] ogbg-molbace: 0.714658 test loss: 2.429857
[Epoch 119; Iter    22/   41] train: loss: 0.0017406
[Epoch 119] ogbg-molbace: 0.598901 val loss: 1.160898
[Epoch 119] ogbg-molbace: 0.695531 test loss: 3.160974
[Epoch 120; Iter    11/   41] train: loss: 0.0008424
[Epoch 120; Iter    41/   41] train: loss: 0.0013295
[Epoch 120] ogbg-molbace: 0.597436 val loss: 1.205118
[Epoch 120] ogbg-molbace: 0.690489 test loss: 3.205834
[Epoch 121; Iter    30/   41] train: loss: 0.0003905
[Epoch 121] ogbg-molbace: 0.611722 val loss: 1.273836
[Epoch 121] ogbg-molbace: 0.707877 test loss: 2.742632
[Epoch 122; Iter    19/   41] train: loss: 0.0005106
[Epoch 122] ogbg-molbace: 0.610623 val loss: 1.218018
[Epoch 122] ogbg-molbace: 0.711007 test loss: 2.550642
[Epoch 123; Iter     8/   41] train: loss: 0.0003115
[Epoch 123; Iter    38/   41] train: loss: 0.0008477
[Epoch 123] ogbg-molbace: 0.609158 val loss: 1.188022
[Epoch 78] ogbg-molbace: 0.587912 val loss: 2.665131
[Epoch 78] ogbg-molbace: 0.618849 test loss: 4.463812
[Epoch 79; Iter    12/   41] train: loss: 0.0211455
[Epoch 79] ogbg-molbace: 0.649451 val loss: 1.755230
[Epoch 79] ogbg-molbace: 0.716049 test loss: 1.822538
[Epoch 80; Iter     1/   41] train: loss: 0.0174383
[Epoch 80; Iter    31/   41] train: loss: 0.1657911
[Epoch 80] ogbg-molbace: 0.612821 val loss: 2.260799
[Epoch 80] ogbg-molbace: 0.698835 test loss: 3.188951
[Epoch 81; Iter    20/   41] train: loss: 0.0123225
[Epoch 81] ogbg-molbace: 0.609890 val loss: 2.937311
[Epoch 81] ogbg-molbace: 0.601113 test loss: 4.740247
[Epoch 82; Iter     9/   41] train: loss: 0.0030219
[Epoch 82; Iter    39/   41] train: loss: 0.0059309
[Epoch 82] ogbg-molbace: 0.634066 val loss: 2.119275
[Epoch 82] ogbg-molbace: 0.712572 test loss: 2.195522
[Epoch 83; Iter    28/   41] train: loss: 0.0146729
[Epoch 83] ogbg-molbace: 0.632234 val loss: 2.237113
[Epoch 83] ogbg-molbace: 0.683012 test loss: 3.982539
[Epoch 84; Iter    17/   41] train: loss: 0.0032665
[Epoch 84] ogbg-molbace: 0.631136 val loss: 1.788451
[Epoch 84] ogbg-molbace: 0.655190 test loss: 3.141190
[Epoch 85; Iter     6/   41] train: loss: 0.0008002
[Epoch 85; Iter    36/   41] train: loss: 0.0028152
[Epoch 85] ogbg-molbace: 0.634799 val loss: 2.273626
[Epoch 85] ogbg-molbace: 0.641454 test loss: 3.930775
[Epoch 86; Iter    25/   41] train: loss: 0.0078145
[Epoch 86] ogbg-molbace: 0.608791 val loss: 2.300837
[Epoch 86] ogbg-molbace: 0.663189 test loss: 4.057768
[Epoch 87; Iter    14/   41] train: loss: 0.0113085
[Epoch 87] ogbg-molbace: 0.610623 val loss: 3.322648
[Epoch 87] ogbg-molbace: 0.643366 test loss: 5.054745
[Epoch 88; Iter     3/   41] train: loss: 0.0207303
[Epoch 88; Iter    33/   41] train: loss: 0.0077897
[Epoch 88] ogbg-molbace: 0.627839 val loss: 1.892214
[Epoch 88] ogbg-molbace: 0.677621 test loss: 3.418615
[Epoch 89; Iter    22/   41] train: loss: 0.0003512
[Epoch 89] ogbg-molbace: 0.614652 val loss: 2.551776
[Epoch 89] ogbg-molbace: 0.615719 test loss: 4.330888
[Epoch 90; Iter    11/   41] train: loss: 0.0007401
[Epoch 90; Iter    41/   41] train: loss: 0.3251523
[Epoch 90] ogbg-molbace: 0.628205 val loss: 2.346956
[Epoch 90] ogbg-molbace: 0.654147 test loss: 3.877126
[Epoch 91; Iter    30/   41] train: loss: 0.0371826
[Epoch 91] ogbg-molbace: 0.602930 val loss: 2.550216
[Epoch 91] ogbg-molbace: 0.617458 test loss: 4.677329
[Epoch 92; Iter    19/   41] train: loss: 0.0240690
[Epoch 92] ogbg-molbace: 0.586813 val loss: 2.777624
[Epoch 92] ogbg-molbace: 0.645801 test loss: 4.625876
[Epoch 93; Iter     8/   41] train: loss: 0.0085473
[Epoch 93; Iter    38/   41] train: loss: 0.0005418
[Epoch 93] ogbg-molbace: 0.610989 val loss: 2.595686
[Epoch 93] ogbg-molbace: 0.640584 test loss: 4.240397
[Epoch 94; Iter    27/   41] train: loss: 0.0249403
[Epoch 94] ogbg-molbace: 0.611355 val loss: 2.897505
[Epoch 94] ogbg-molbace: 0.633107 test loss: 4.640760
[Epoch 95; Iter    16/   41] train: loss: 0.0011409
[Epoch 95] ogbg-molbace: 0.590476 val loss: 4.559689
[Epoch 95] ogbg-molbace: 0.599374 test loss: 6.299393
[Epoch 96; Iter     5/   41] train: loss: 0.0036875
[Epoch 96; Iter    35/   41] train: loss: 0.0085676
[Epoch 96] ogbg-molbace: 0.595238 val loss: 3.521266
[Epoch 96] ogbg-molbace: 0.626674 test loss: 4.978352
[Epoch 97; Iter    24/   41] train: loss: 0.0014640
[Epoch 97] ogbg-molbace: 0.607326 val loss: 3.276288
[Epoch 97] ogbg-molbace: 0.657973 test loss: 4.545421
[Epoch 98; Iter    13/   41] train: loss: 0.0025573
[Epoch 98] ogbg-molbace: 0.594505 val loss: 3.317180
[Epoch 98] ogbg-molbace: 0.642323 test loss: 4.392419
[Epoch 99; Iter     2/   41] train: loss: 0.0078296
[Epoch 99; Iter    32/   41] train: loss: 0.0014758
[Epoch 99] ogbg-molbace: 0.599634 val loss: 3.311558
[Epoch 99] ogbg-molbace: 0.657103 test loss: 4.395677
[Epoch 100; Iter    21/   41] train: loss: 0.0003730
[Epoch 100] ogbg-molbace: 0.615018 val loss: 3.337515
[Epoch 100] ogbg-molbace: 0.691184 test loss: 4.937792
[Epoch 101; Iter    10/   41] train: loss: 0.0006175
[Epoch 101; Iter    40/   41] train: loss: 0.0003893
[Epoch 101] ogbg-molbace: 0.616117 val loss: 3.326952
[Epoch 101] ogbg-molbace: 0.683533 test loss: 4.783793
[Epoch 102; Iter    29/   41] train: loss: 0.0001709
[Epoch 102] ogbg-molbace: 0.612088 val loss: 3.479207
[Epoch 102] ogbg-molbace: 0.676926 test loss: 4.929479
[Epoch 103; Iter    18/   41] train: loss: 0.0002238
[Epoch 103] ogbg-molbace: 0.606593 val loss: 3.050078
[Epoch 103] ogbg-molbace: 0.647713 test loss: 4.457006
[Epoch 104; Iter     7/   41] train: loss: 0.0143188
[Epoch 104; Iter    37/   41] train: loss: 0.0002585
[Epoch 104] ogbg-molbace: 0.608425 val loss: 3.252691
[Epoch 104] ogbg-molbace: 0.668405 test loss: 4.679412
[Epoch 105; Iter    26/   41] train: loss: 0.0013676
[Epoch 105] ogbg-molbace: 0.605861 val loss: 3.459564
[Epoch 105] ogbg-molbace: 0.669970 test loss: 4.922576
[Epoch 106; Iter    15/   41] train: loss: 0.0001918
[Epoch 106] ogbg-molbace: 0.598535 val loss: 3.328765
[Epoch 106] ogbg-molbace: 0.644062 test loss: 4.959617
[Epoch 107; Iter     4/   41] train: loss: 0.0017803
[Epoch 107; Iter    34/   41] train: loss: 0.0048766
[Epoch 107] ogbg-molbace: 0.625641 val loss: 2.449537
[Epoch 107] ogbg-molbace: 0.633977 test loss: 3.629867
[Epoch 108; Iter    23/   41] train: loss: 0.0134325
[Epoch 108] ogbg-molbace: 0.620879 val loss: 2.821427
[Epoch 108] ogbg-molbace: 0.664580 test loss: 3.994533
[Epoch 109; Iter    12/   41] train: loss: 0.0004470
[Epoch 109] ogbg-molbace: 0.636630 val loss: 2.856583
[Epoch 109] ogbg-molbace: 0.689619 test loss: 3.954262
[Epoch 110; Iter     1/   41] train: loss: 0.0111201
[Epoch 110; Iter    31/   41] train: loss: 0.0019087
[Epoch 110] ogbg-molbace: 0.632967 val loss: 2.408336
[Epoch 110] ogbg-molbace: 0.683186 test loss: 3.243471
[Epoch 111; Iter    20/   41] train: loss: 0.0009100
[Epoch 111] ogbg-molbace: 0.634432 val loss: 2.606303
[Epoch 111] ogbg-molbace: 0.676752 test loss: 3.761706
[Epoch 112; Iter     9/   41] train: loss: 0.0001377
[Epoch 112; Iter    39/   41] train: loss: 0.0003238
[Epoch 112] ogbg-molbace: 0.633333 val loss: 2.541854
[Epoch 112] ogbg-molbace: 0.677795 test loss: 3.510454
[Epoch 113; Iter    28/   41] train: loss: 0.0004179
[Epoch 113] ogbg-molbace: 0.631868 val loss: 2.752840
[Epoch 113] ogbg-molbace: 0.683012 test loss: 3.960155
[Epoch 114; Iter    17/   41] train: loss: 0.0003113
[Epoch 114] ogbg-molbace: 0.632601 val loss: 2.669273
[Epoch 114] ogbg-molbace: 0.681273 test loss: 3.804455
[Epoch 115; Iter     6/   41] train: loss: 0.0023921
[Epoch 115; Iter    36/   41] train: loss: 0.0004723
[Epoch 115] ogbg-molbace: 0.628938 val loss: 2.735604
[Epoch 115] ogbg-molbace: 0.677969 test loss: 3.917040
[Epoch 116; Iter    25/   41] train: loss: 0.0001938
[Epoch 116] ogbg-molbace: 0.634432 val loss: 2.579211
[Epoch 116] ogbg-molbace: 0.695531 test loss: 3.481784
[Epoch 117; Iter    14/   41] train: loss: 0.0004484
[Epoch 117] ogbg-molbace: 0.634432 val loss: 2.838085
[Epoch 117] ogbg-molbace: 0.685446 test loss: 3.952665
[Epoch 118; Iter     3/   41] train: loss: 0.0002833
[Epoch 118; Iter    33/   41] train: loss: 0.0008045
[Epoch 118] ogbg-molbace: 0.630769 val loss: 2.661920
[Epoch 118] ogbg-molbace: 0.681968 test loss: 3.546567
[Epoch 119; Iter    22/   41] train: loss: 0.0002027
[Epoch 119] ogbg-molbace: 0.627839 val loss: 2.832457
[Epoch 119] ogbg-molbace: 0.676578 test loss: 3.955256
[Epoch 120; Iter    11/   41] train: loss: 0.0001222
[Epoch 120; Iter    41/   41] train: loss: 0.0010903
[Epoch 120] ogbg-molbace: 0.632601 val loss: 2.726058
[Epoch 120] ogbg-molbace: 0.693271 test loss: 3.766260
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 39.
Statistics on  val_best_checkpoint
mean_pred: 3.873417615890503
std_pred: 2.1991875171661377
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9416143605661027
rocauc: 0.7157509157509158
ogbg-molbace: 0.7157509157509158
BCEWithLogitsLoss: 0.9737619124352932
Statistics on  test
mean_pred: 1.408715844154358
std_pred: 3.0849506855010986
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.8268703823703881
rocauc: 0.8224656581464093
ogbg-molbace: 0.8224656581464093
BCEWithLogitsLoss: 0.8796190321445465
Statistics on  train
mean_pred: 2.4271864891052246
std_pred: 2.864704132080078
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.9245048991893741
rocauc: 0.9472602739726026
ogbg-molbace: 0.9472602739726026
BCEWithLogitsLoss: 0.8869916585887351
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.670330 val loss: 1.979526
[Epoch 78] ogbg-molbace: 0.728395 test loss: 2.628388
[Epoch 79; Iter    12/   41] train: loss: 0.0011517
[Epoch 79] ogbg-molbace: 0.663004 val loss: 1.871735
[Epoch 79] ogbg-molbace: 0.717962 test loss: 2.508729
[Epoch 80; Iter     1/   41] train: loss: 0.0037999
[Epoch 80; Iter    31/   41] train: loss: 0.0150450
[Epoch 80] ogbg-molbace: 0.663370 val loss: 2.041173
[Epoch 80] ogbg-molbace: 0.728395 test loss: 2.758244
[Epoch 81; Iter    20/   41] train: loss: 0.0010392
[Epoch 81] ogbg-molbace: 0.667033 val loss: 2.079836
[Epoch 81] ogbg-molbace: 0.712398 test loss: 2.832737
[Epoch 82; Iter     9/   41] train: loss: 0.0007094
[Epoch 82; Iter    39/   41] train: loss: 0.0013732
[Epoch 82] ogbg-molbace: 0.685714 val loss: 1.949735
[Epoch 82] ogbg-molbace: 0.724396 test loss: 2.497969
[Epoch 83; Iter    28/   41] train: loss: 0.0038419
[Epoch 83] ogbg-molbace: 0.657875 val loss: 2.047189
[Epoch 83] ogbg-molbace: 0.699704 test loss: 2.558124
[Epoch 84; Iter    17/   41] train: loss: 0.0008009
[Epoch 84] ogbg-molbace: 0.656044 val loss: 2.127717
[Epoch 84] ogbg-molbace: 0.696053 test loss: 2.683639
[Epoch 85; Iter     6/   41] train: loss: 0.0007431
[Epoch 85; Iter    36/   41] train: loss: 0.0028487
[Epoch 85] ogbg-molbace: 0.664103 val loss: 2.107227
[Epoch 85] ogbg-molbace: 0.704399 test loss: 2.675843
[Epoch 86; Iter    25/   41] train: loss: 0.0015824
[Epoch 86] ogbg-molbace: 0.681319 val loss: 2.208230
[Epoch 86] ogbg-molbace: 0.728569 test loss: 2.283894
[Epoch 87; Iter    14/   41] train: loss: 0.0666054
[Epoch 87] ogbg-molbace: 0.657143 val loss: 2.511062
[Epoch 87] ogbg-molbace: 0.752913 test loss: 1.423892
[Epoch 88; Iter     3/   41] train: loss: 0.0818084
[Epoch 88; Iter    33/   41] train: loss: 0.0619398
[Epoch 88] ogbg-molbace: 0.624176 val loss: 2.480228
[Epoch 88] ogbg-molbace: 0.727700 test loss: 2.409114
[Epoch 89; Iter    22/   41] train: loss: 0.0046300
[Epoch 89] ogbg-molbace: 0.660806 val loss: 2.635583
[Epoch 89] ogbg-molbace: 0.747348 test loss: 2.253747
[Epoch 90; Iter    11/   41] train: loss: 0.0008215
[Epoch 90; Iter    41/   41] train: loss: 0.4080205
[Epoch 90] ogbg-molbace: 0.658974 val loss: 2.475906
[Epoch 90] ogbg-molbace: 0.718484 test loss: 2.578094
[Epoch 91; Iter    30/   41] train: loss: 0.0101948
[Epoch 91] ogbg-molbace: 0.647253 val loss: 2.210668
[Epoch 91] ogbg-molbace: 0.653625 test loss: 2.259670
[Epoch 92; Iter    19/   41] train: loss: 0.0692633
[Epoch 92] ogbg-molbace: 0.672527 val loss: 1.975407
[Epoch 92] ogbg-molbace: 0.708399 test loss: 2.348644
[Epoch 93; Iter     8/   41] train: loss: 0.0923073
[Epoch 93; Iter    38/   41] train: loss: 0.0068224
[Epoch 93] ogbg-molbace: 0.672894 val loss: 2.234728
[Epoch 93] ogbg-molbace: 0.696748 test loss: 2.571059
[Epoch 94; Iter    27/   41] train: loss: 0.0065177
[Epoch 94] ogbg-molbace: 0.665934 val loss: 2.743615
[Epoch 94] ogbg-molbace: 0.712746 test loss: 2.424458
[Epoch 95; Iter    16/   41] train: loss: 0.0018376
[Epoch 95] ogbg-molbace: 0.655678 val loss: 2.391101
[Epoch 95] ogbg-molbace: 0.737263 test loss: 1.957250
[Epoch 96; Iter     5/   41] train: loss: 0.0032672
[Epoch 96; Iter    35/   41] train: loss: 0.0042932
[Epoch 96] ogbg-molbace: 0.652381 val loss: 2.429896
[Epoch 96] ogbg-molbace: 0.700922 test loss: 2.465330
[Epoch 97; Iter    24/   41] train: loss: 0.0020478
[Epoch 97] ogbg-molbace: 0.656044 val loss: 2.616155
[Epoch 97] ogbg-molbace: 0.709616 test loss: 2.571158
[Epoch 98; Iter    13/   41] train: loss: 0.0024316
[Epoch 98] ogbg-molbace: 0.678755 val loss: 2.576925
[Epoch 98] ogbg-molbace: 0.735176 test loss: 2.413498
[Epoch 99; Iter     2/   41] train: loss: 0.0062067
[Epoch 99; Iter    32/   41] train: loss: 0.0023353
[Epoch 99] ogbg-molbace: 0.662271 val loss: 2.656665
[Epoch 99] ogbg-molbace: 0.717440 test loss: 2.557060
[Epoch 100; Iter    21/   41] train: loss: 0.0029097
[Epoch 100] ogbg-molbace: 0.685714 val loss: 2.351667
[Epoch 100] ogbg-molbace: 0.739871 test loss: 2.387429
[Epoch 101; Iter    10/   41] train: loss: 0.0003637
[Epoch 101; Iter    40/   41] train: loss: 0.0006497
[Epoch 101] ogbg-molbace: 0.676190 val loss: 2.353173
[Epoch 101] ogbg-molbace: 0.724917 test loss: 2.289143
[Epoch 102; Iter    29/   41] train: loss: 0.0049890
[Epoch 102] ogbg-molbace: 0.672894 val loss: 2.417563
[Epoch 102] ogbg-molbace: 0.726830 test loss: 2.422696
[Epoch 103; Iter    18/   41] train: loss: 0.0006885
[Epoch 103] ogbg-molbace: 0.675092 val loss: 2.550844
[Epoch 103] ogbg-molbace: 0.731003 test loss: 2.176287
[Epoch 104; Iter     7/   41] train: loss: 0.0381878
[Epoch 104; Iter    37/   41] train: loss: 0.0006800
[Epoch 104] ogbg-molbace: 0.659707 val loss: 2.538733
[Epoch 104] ogbg-molbace: 0.714658 test loss: 2.832704
[Epoch 105; Iter    26/   41] train: loss: 0.0053316
[Epoch 105] ogbg-molbace: 0.668864 val loss: 2.402664
[Epoch 105] ogbg-molbace: 0.717962 test loss: 2.330413
[Epoch 106; Iter    15/   41] train: loss: 0.0007997
[Epoch 106] ogbg-molbace: 0.689011 val loss: 2.209339
[Epoch 106] ogbg-molbace: 0.710137 test loss: 2.312716
[Epoch 107; Iter     4/   41] train: loss: 0.0004739
[Epoch 107; Iter    34/   41] train: loss: 0.0003043
[Epoch 107] ogbg-molbace: 0.683150 val loss: 2.209398
[Epoch 107] ogbg-molbace: 0.703704 test loss: 2.611915
[Epoch 108; Iter    23/   41] train: loss: 0.0014331
[Epoch 108] ogbg-molbace: 0.676190 val loss: 2.292314
[Epoch 108] ogbg-molbace: 0.702139 test loss: 2.498600
[Epoch 109; Iter    12/   41] train: loss: 0.0013775
[Epoch 109] ogbg-molbace: 0.672894 val loss: 2.389045
[Epoch 109] ogbg-molbace: 0.708572 test loss: 2.482303
[Epoch 110; Iter     1/   41] train: loss: 0.0029702
[Epoch 110; Iter    31/   41] train: loss: 0.0020688
[Epoch 110] ogbg-molbace: 0.675824 val loss: 2.373741
[Epoch 110] ogbg-molbace: 0.720918 test loss: 2.258342
[Epoch 111; Iter    20/   41] train: loss: 0.0007256
[Epoch 111] ogbg-molbace: 0.685348 val loss: 2.323141
[Epoch 111] ogbg-molbace: 0.724048 test loss: 2.422948
[Epoch 112; Iter     9/   41] train: loss: 0.0002740
[Epoch 112; Iter    39/   41] train: loss: 0.0009108
[Epoch 112] ogbg-molbace: 0.678755 val loss: 2.360181
[Epoch 112] ogbg-molbace: 0.720049 test loss: 2.241797
[Epoch 113; Iter    28/   41] train: loss: 0.0002647
[Epoch 113] ogbg-molbace: 0.679121 val loss: 2.381504
[Epoch 113] ogbg-molbace: 0.716049 test loss: 2.737731
[Epoch 114; Iter    17/   41] train: loss: 0.0001168
[Epoch 114] ogbg-molbace: 0.678755 val loss: 2.328624
[Epoch 114] ogbg-molbace: 0.714484 test loss: 2.441803
[Epoch 115; Iter     6/   41] train: loss: 0.0021300
[Epoch 115; Iter    36/   41] train: loss: 0.0001840
[Epoch 115] ogbg-molbace: 0.675092 val loss: 2.397958
[Epoch 115] ogbg-molbace: 0.712050 test loss: 2.622361
[Epoch 116; Iter    25/   41] train: loss: 0.0002252
[Epoch 116] ogbg-molbace: 0.675824 val loss: 2.464292
[Epoch 116] ogbg-molbace: 0.722657 test loss: 2.410643
[Epoch 117; Iter    14/   41] train: loss: 0.0002759
[Epoch 117] ogbg-molbace: 0.679121 val loss: 2.485619
[Epoch 117] ogbg-molbace: 0.716397 test loss: 2.494722
[Epoch 118; Iter     3/   41] train: loss: 0.0003844
[Epoch 118; Iter    33/   41] train: loss: 0.0006263
[Epoch 118] ogbg-molbace: 0.672894 val loss: 2.483219
[Epoch 118] ogbg-molbace: 0.720744 test loss: 2.176239
[Epoch 119; Iter    22/   41] train: loss: 0.0003135
[Epoch 119] ogbg-molbace: 0.672894 val loss: 2.427246
[Epoch 119] ogbg-molbace: 0.708572 test loss: 2.665376
[Epoch 120; Iter    11/   41] train: loss: 0.0002351
[Epoch 120; Iter    41/   41] train: loss: 0.0050782
[Epoch 120] ogbg-molbace: 0.675092 val loss: 2.395304
[Epoch 120] ogbg-molbace: 0.719179 test loss: 2.439601
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 41.
Statistics on  val_best_checkpoint
mean_pred: 3.7061243057250977
std_pred: 3.067425012588501
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9374423851599025
rocauc: 0.7681318681318682
ogbg-molbace: 0.7681318681318682
BCEWithLogitsLoss: 0.9338481649756432
Statistics on  test
mean_pred: 2.06204891204834
std_pred: 4.584066390991211
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7521878940674225
rocauc: 0.777603894974787
ogbg-molbace: 0.777603894974787
BCEWithLogitsLoss: 0.8591346989075342
Statistics on  train
mean_pred: 2.267090320587158
std_pred: 5.37351131439209
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.8835896690162097
rocauc: 0.9064611872146118
ogbg-molbace: 0.9064611872146118
BCEWithLogitsLoss: 0.7793415297822255
[Epoch 78] ogbg-molbace: 0.659707 val loss: 1.732797
[Epoch 78] ogbg-molbace: 0.726656 test loss: 2.808210
[Epoch 79; Iter    12/   41] train: loss: 0.0002983
[Epoch 79] ogbg-molbace: 0.667766 val loss: 1.522100
[Epoch 79] ogbg-molbace: 0.735524 test loss: 2.385193
[Epoch 80; Iter     1/   41] train: loss: 0.0005693
[Epoch 80; Iter    31/   41] train: loss: 0.0070658
[Epoch 80] ogbg-molbace: 0.669597 val loss: 1.557902
[Epoch 80] ogbg-molbace: 0.740045 test loss: 2.543165
[Epoch 81; Iter    20/   41] train: loss: 0.0005079
[Epoch 81] ogbg-molbace: 0.672527 val loss: 1.434741
[Epoch 81] ogbg-molbace: 0.732394 test loss: 2.404072
[Epoch 82; Iter     9/   41] train: loss: 0.0002687
[Epoch 82; Iter    39/   41] train: loss: 0.0002826
[Epoch 82] ogbg-molbace: 0.668132 val loss: 1.544719
[Epoch 82] ogbg-molbace: 0.731003 test loss: 2.413198
[Epoch 83; Iter    28/   41] train: loss: 0.0010262
[Epoch 83] ogbg-molbace: 0.668132 val loss: 1.561756
[Epoch 83] ogbg-molbace: 0.728917 test loss: 2.409471
[Epoch 84; Iter    17/   41] train: loss: 0.0003021
[Epoch 84] ogbg-molbace: 0.663736 val loss: 1.638145
[Epoch 84] ogbg-molbace: 0.720223 test loss: 2.619434
[Epoch 85; Iter     6/   41] train: loss: 0.0001733
[Epoch 85; Iter    36/   41] train: loss: 0.0003900
[Epoch 85] ogbg-molbace: 0.671429 val loss: 1.600625
[Epoch 85] ogbg-molbace: 0.733090 test loss: 2.445459
[Epoch 86; Iter    25/   41] train: loss: 0.0006043
[Epoch 86] ogbg-molbace: 0.671795 val loss: 1.726777
[Epoch 86] ogbg-molbace: 0.732916 test loss: 2.588253
[Epoch 87; Iter    14/   41] train: loss: 0.1251902
[Epoch 87] ogbg-molbace: 0.647619 val loss: 1.299867
[Epoch 87] ogbg-molbace: 0.742132 test loss: 1.763741
[Epoch 88; Iter     3/   41] train: loss: 0.0138196
[Epoch 88; Iter    33/   41] train: loss: 0.0218493
[Epoch 88] ogbg-molbace: 0.664103 val loss: 1.381714
[Epoch 88] ogbg-molbace: 0.713267 test loss: 2.706356
[Epoch 89; Iter    22/   41] train: loss: 0.0022588
[Epoch 89] ogbg-molbace: 0.674359 val loss: 1.414636
[Epoch 89] ogbg-molbace: 0.727352 test loss: 2.563862
[Epoch 90; Iter    11/   41] train: loss: 0.0002900
[Epoch 90; Iter    41/   41] train: loss: 0.0928310
[Epoch 90] ogbg-molbace: 0.636996 val loss: 1.347081
[Epoch 90] ogbg-molbace: 0.697966 test loss: 2.342743
[Epoch 91; Iter    30/   41] train: loss: 0.1301834
[Epoch 91] ogbg-molbace: 0.666667 val loss: 2.276827
[Epoch 91] ogbg-molbace: 0.728395 test loss: 3.011270
[Epoch 92; Iter    19/   41] train: loss: 0.0388856
[Epoch 92] ogbg-molbace: 0.668864 val loss: 2.812923
[Epoch 92] ogbg-molbace: 0.733438 test loss: 2.053092
[Epoch 93; Iter     8/   41] train: loss: 0.1832513
[Epoch 93; Iter    38/   41] train: loss: 0.0669881
[Epoch 93] ogbg-molbace: 0.730403 val loss: 3.588955
[Epoch 93] ogbg-molbace: 0.565641 test loss: 3.805192
[Epoch 94; Iter    27/   41] train: loss: 0.1447400
[Epoch 94] ogbg-molbace: 0.664103 val loss: 2.682782
[Epoch 94] ogbg-molbace: 0.783516 test loss: 1.529473
[Epoch 95; Iter    16/   41] train: loss: 0.1541571
[Epoch 95] ogbg-molbace: 0.626374 val loss: 1.699972
[Epoch 95] ogbg-molbace: 0.722831 test loss: 2.660322
[Epoch 96; Iter     5/   41] train: loss: 0.0462219
[Epoch 96; Iter    35/   41] train: loss: 0.0758432
[Epoch 96] ogbg-molbace: 0.641758 val loss: 1.916521
[Epoch 96] ogbg-molbace: 0.749957 test loss: 2.275351
[Epoch 97; Iter    24/   41] train: loss: 0.0272165
[Epoch 97] ogbg-molbace: 0.657143 val loss: 2.527339
[Epoch 97] ogbg-molbace: 0.747001 test loss: 3.438693
[Epoch 98; Iter    13/   41] train: loss: 0.0261605
[Epoch 98] ogbg-molbace: 0.649817 val loss: 2.367430
[Epoch 98] ogbg-molbace: 0.753956 test loss: 2.965615
[Epoch 99; Iter     2/   41] train: loss: 0.0179169
[Epoch 99; Iter    32/   41] train: loss: 0.0035351
[Epoch 99] ogbg-molbace: 0.638095 val loss: 2.682995
[Epoch 99] ogbg-molbace: 0.750130 test loss: 3.002528
[Epoch 100; Iter    21/   41] train: loss: 0.0056035
[Epoch 100] ogbg-molbace: 0.641026 val loss: 3.064579
[Epoch 100] ogbg-molbace: 0.722831 test loss: 3.803148
[Epoch 101; Iter    10/   41] train: loss: 0.0009880
[Epoch 101; Iter    40/   41] train: loss: 0.0019026
[Epoch 101] ogbg-molbace: 0.643590 val loss: 2.613876
[Epoch 101] ogbg-molbace: 0.738306 test loss: 2.890874
[Epoch 102; Iter    29/   41] train: loss: 0.0016790
[Epoch 102] ogbg-molbace: 0.639927 val loss: 2.969006
[Epoch 102] ogbg-molbace: 0.728221 test loss: 3.554406
[Epoch 103; Iter    18/   41] train: loss: 0.0018337
[Epoch 103] ogbg-molbace: 0.647253 val loss: 2.522668
[Epoch 103] ogbg-molbace: 0.728047 test loss: 3.051435
[Epoch 104; Iter     7/   41] train: loss: 0.0417042
[Epoch 104; Iter    37/   41] train: loss: 0.0014115
[Epoch 104] ogbg-molbace: 0.638828 val loss: 2.738600
[Epoch 104] ogbg-molbace: 0.728047 test loss: 3.259400
[Epoch 105; Iter    26/   41] train: loss: 0.0028698
[Epoch 105] ogbg-molbace: 0.652747 val loss: 2.748468
[Epoch 105] ogbg-molbace: 0.737785 test loss: 3.185125
[Epoch 106; Iter    15/   41] train: loss: 0.0010035
[Epoch 106] ogbg-molbace: 0.662271 val loss: 2.912303
[Epoch 106] ogbg-molbace: 0.743871 test loss: 3.080211
[Epoch 107; Iter     4/   41] train: loss: 0.0011031
[Epoch 107; Iter    34/   41] train: loss: 0.0019079
[Epoch 107] ogbg-molbace: 0.638462 val loss: 2.979611
[Epoch 107] ogbg-molbace: 0.727352 test loss: 3.560893
[Epoch 108; Iter    23/   41] train: loss: 0.0016156
[Epoch 108] ogbg-molbace: 0.641026 val loss: 2.921279
[Epoch 108] ogbg-molbace: 0.724917 test loss: 3.590043
[Epoch 109; Iter    12/   41] train: loss: 0.0017297
[Epoch 109] ogbg-molbace: 0.642857 val loss: 2.895256
[Epoch 109] ogbg-molbace: 0.735003 test loss: 3.403500
[Epoch 110; Iter     1/   41] train: loss: 0.0054502
[Epoch 110; Iter    31/   41] train: loss: 0.0020701
[Epoch 110] ogbg-molbace: 0.646886 val loss: 2.921498
[Epoch 110] ogbg-molbace: 0.732916 test loss: 3.450862
[Epoch 111; Iter    20/   41] train: loss: 0.0021874
[Epoch 111] ogbg-molbace: 0.646520 val loss: 2.670816
[Epoch 111] ogbg-molbace: 0.740219 test loss: 3.152415
[Epoch 112; Iter     9/   41] train: loss: 0.0003638
[Epoch 112; Iter    39/   41] train: loss: 0.0015371
[Epoch 112] ogbg-molbace: 0.650183 val loss: 2.942379
[Epoch 112] ogbg-molbace: 0.744392 test loss: 3.212700
[Epoch 113; Iter    28/   41] train: loss: 0.0004044
[Epoch 113] ogbg-molbace: 0.646886 val loss: 2.876603
[Epoch 113] ogbg-molbace: 0.734481 test loss: 3.577696
[Epoch 114; Iter    17/   41] train: loss: 0.0006774
[Epoch 114] ogbg-molbace: 0.647985 val loss: 2.941472
[Epoch 114] ogbg-molbace: 0.730482 test loss: 3.584650
[Epoch 115; Iter     6/   41] train: loss: 0.0036455
[Epoch 115; Iter    36/   41] train: loss: 0.0011574
[Epoch 115] ogbg-molbace: 0.643223 val loss: 2.947325
[Epoch 115] ogbg-molbace: 0.734829 test loss: 3.566727
[Epoch 116; Iter    25/   41] train: loss: 0.0004081
[Epoch 116] ogbg-molbace: 0.651282 val loss: 2.631308
[Epoch 116] ogbg-molbace: 0.736220 test loss: 3.084008
[Epoch 117; Iter    14/   41] train: loss: 0.0005359
[Epoch 117] ogbg-molbace: 0.636630 val loss: 3.185529
[Epoch 117] ogbg-molbace: 0.727178 test loss: 3.961824
[Epoch 118; Iter     3/   41] train: loss: 0.0009125
[Epoch 118; Iter    33/   41] train: loss: 0.0006706
[Epoch 118] ogbg-molbace: 0.641392 val loss: 3.111362
[Epoch 118] ogbg-molbace: 0.731525 test loss: 3.689420
[Epoch 119; Iter    22/   41] train: loss: 0.0007141
[Epoch 119] ogbg-molbace: 0.636996 val loss: 3.135553
[Epoch 119] ogbg-molbace: 0.727700 test loss: 3.907893
[Epoch 120; Iter    11/   41] train: loss: 0.0005010
[Epoch 120; Iter    41/   41] train: loss: 0.0048078
[Epoch 120] ogbg-molbace: 0.638828 val loss: 3.376623
[Epoch 120] ogbg-molbace: 0.731525 test loss: 4.234166
[Epoch 121; Iter    30/   41] train: loss: 0.0008869
[Epoch 121] ogbg-molbace: 0.645788 val loss: 2.606221
[Epoch 121] ogbg-molbace: 0.735176 test loss: 3.254611
[Epoch 122; Iter    19/   41] train: loss: 0.0015586
[Epoch 122] ogbg-molbace: 0.644689 val loss: 2.504953
[Epoch 122] ogbg-molbace: 0.734481 test loss: 3.142983
[Epoch 123; Iter     8/   41] train: loss: 0.0012276
[Epoch 123; Iter    38/   41] train: loss: 0.0002357
[Epoch 123] ogbg-molbace: 0.642125 val loss: 3.020221
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.619780 val loss: 1.901428
[Epoch 78] ogbg-molbace: 0.761607 test loss: 2.574837
[Epoch 79; Iter    12/   41] train: loss: 0.0573654
[Epoch 79] ogbg-molbace: 0.630403 val loss: 2.429560
[Epoch 79] ogbg-molbace: 0.685272 test loss: 4.098629
[Epoch 80; Iter     1/   41] train: loss: 0.0501144
[Epoch 80; Iter    31/   41] train: loss: 0.0091985
[Epoch 80] ogbg-molbace: 0.660440 val loss: 2.184306
[Epoch 80] ogbg-molbace: 0.753260 test loss: 3.478958
[Epoch 81; Iter    20/   41] train: loss: 0.0093121
[Epoch 81] ogbg-molbace: 0.636264 val loss: 2.444975
[Epoch 81] ogbg-molbace: 0.743871 test loss: 3.739226
[Epoch 82; Iter     9/   41] train: loss: 0.0014719
[Epoch 82; Iter    39/   41] train: loss: 0.0022918
[Epoch 82] ogbg-molbace: 0.661172 val loss: 2.598101
[Epoch 82] ogbg-molbace: 0.774822 test loss: 3.567727
[Epoch 83; Iter    28/   41] train: loss: 0.0014003
[Epoch 83] ogbg-molbace: 0.660440 val loss: 2.211320
[Epoch 83] ogbg-molbace: 0.770649 test loss: 3.349483
[Epoch 84; Iter    17/   41] train: loss: 0.0011842
[Epoch 84] ogbg-molbace: 0.649451 val loss: 2.371995
[Epoch 84] ogbg-molbace: 0.750652 test loss: 3.575655
[Epoch 85; Iter     6/   41] train: loss: 0.0019581
[Epoch 85; Iter    36/   41] train: loss: 0.0010390
[Epoch 85] ogbg-molbace: 0.659341 val loss: 2.285282
[Epoch 85] ogbg-molbace: 0.760911 test loss: 3.426421
[Epoch 86; Iter    25/   41] train: loss: 0.0009554
[Epoch 86] ogbg-molbace: 0.653846 val loss: 2.241971
[Epoch 86] ogbg-molbace: 0.760563 test loss: 3.306170
[Epoch 87; Iter    14/   41] train: loss: 0.0048121
[Epoch 87] ogbg-molbace: 0.645055 val loss: 2.650396
[Epoch 87] ogbg-molbace: 0.761433 test loss: 3.457485
[Epoch 88; Iter     3/   41] train: loss: 0.0011075
[Epoch 88; Iter    33/   41] train: loss: 0.0009358
[Epoch 88] ogbg-molbace: 0.647253 val loss: 2.614014
[Epoch 88] ogbg-molbace: 0.754130 test loss: 3.600872
[Epoch 89; Iter    22/   41] train: loss: 0.0051824
[Epoch 89] ogbg-molbace: 0.658974 val loss: 2.445538
[Epoch 89] ogbg-molbace: 0.771344 test loss: 3.354226
[Epoch 90; Iter    11/   41] train: loss: 0.0051268
[Epoch 90; Iter    41/   41] train: loss: 0.0566640
[Epoch 90] ogbg-molbace: 0.656777 val loss: 2.502373
[Epoch 90] ogbg-molbace: 0.791688 test loss: 3.426168
[Epoch 91; Iter    30/   41] train: loss: 0.0011739
[Epoch 91] ogbg-molbace: 0.652015 val loss: 2.631953
[Epoch 91] ogbg-molbace: 0.768388 test loss: 3.569997
[Epoch 92; Iter    19/   41] train: loss: 0.0011351
[Epoch 92] ogbg-molbace: 0.661905 val loss: 2.266736
[Epoch 92] ogbg-molbace: 0.774126 test loss: 3.238124
[Epoch 93; Iter     8/   41] train: loss: 0.0005404
[Epoch 93; Iter    38/   41] train: loss: 0.0038327
[Epoch 93] ogbg-molbace: 0.648718 val loss: 2.381895
[Epoch 93] ogbg-molbace: 0.766128 test loss: 3.407051
[Epoch 94; Iter    27/   41] train: loss: 0.0012035
[Epoch 94] ogbg-molbace: 0.653114 val loss: 2.565592
[Epoch 94] ogbg-molbace: 0.765606 test loss: 3.620909
[Epoch 95; Iter    16/   41] train: loss: 0.0240574
[Epoch 95] ogbg-molbace: 0.641026 val loss: 2.602030
[Epoch 95] ogbg-molbace: 0.754130 test loss: 3.535839
[Epoch 96; Iter     5/   41] train: loss: 0.0003923
[Epoch 96; Iter    35/   41] train: loss: 0.0066542
[Epoch 96] ogbg-molbace: 0.654579 val loss: 2.336133
[Epoch 96] ogbg-molbace: 0.762128 test loss: 3.241898
[Epoch 97; Iter    24/   41] train: loss: 0.0090194
[Epoch 97] ogbg-molbace: 0.656044 val loss: 2.369040
[Epoch 97] ogbg-molbace: 0.767345 test loss: 3.217334
[Epoch 98; Iter    13/   41] train: loss: 0.0010286
[Epoch 98] ogbg-molbace: 0.667033 val loss: 2.213463
[Epoch 98] ogbg-molbace: 0.773083 test loss: 3.106031
[Epoch 99; Iter     2/   41] train: loss: 0.0002467
[Epoch 99; Iter    32/   41] train: loss: 0.0025478
[Epoch 99] ogbg-molbace: 0.663370 val loss: 2.166917
[Epoch 99] ogbg-molbace: 0.777430 test loss: 3.024485
[Epoch 100; Iter    21/   41] train: loss: 0.0010568
[Epoch 100] ogbg-molbace: 0.655678 val loss: 2.215763
[Epoch 100] ogbg-molbace: 0.769431 test loss: 3.174773
[Epoch 101; Iter    10/   41] train: loss: 0.0002515
[Epoch 101; Iter    40/   41] train: loss: 0.0005443
[Epoch 101] ogbg-molbace: 0.667033 val loss: 2.055065
[Epoch 101] ogbg-molbace: 0.780212 test loss: 3.025120
[Epoch 102; Iter    29/   41] train: loss: 0.0012846
[Epoch 102] ogbg-molbace: 0.662271 val loss: 2.203341
[Epoch 102] ogbg-molbace: 0.780212 test loss: 3.150897
[Epoch 103; Iter    18/   41] train: loss: 0.0007217
[Epoch 103] ogbg-molbace: 0.662271 val loss: 2.314686
[Epoch 103] ogbg-molbace: 0.773952 test loss: 3.312922
[Epoch 104; Iter     7/   41] train: loss: 0.0002415
[Epoch 104; Iter    37/   41] train: loss: 0.0003737
[Epoch 104] ogbg-molbace: 0.665568 val loss: 2.083636
[Epoch 104] ogbg-molbace: 0.785081 test loss: 2.953630
[Epoch 105; Iter    26/   41] train: loss: 0.0001487
[Epoch 105] ogbg-molbace: 0.665934 val loss: 2.355757
[Epoch 105] ogbg-molbace: 0.780908 test loss: 3.369982
[Epoch 106; Iter    15/   41] train: loss: 0.0005674
[Epoch 106] ogbg-molbace: 0.671429 val loss: 1.997436
[Epoch 106] ogbg-molbace: 0.790819 test loss: 2.804421
[Epoch 107; Iter     4/   41] train: loss: 0.0003385
[Epoch 107; Iter    34/   41] train: loss: 0.0011868
[Epoch 107] ogbg-molbace: 0.657875 val loss: 2.343920
[Epoch 107] ogbg-molbace: 0.750478 test loss: 3.538758
[Epoch 108; Iter    23/   41] train: loss: 0.0008505
[Epoch 108] ogbg-molbace: 0.672161 val loss: 1.928091
[Epoch 108] ogbg-molbace: 0.776387 test loss: 2.715111
[Epoch 109; Iter    12/   41] train: loss: 0.0004114
[Epoch 109] ogbg-molbace: 0.641392 val loss: 3.110586
[Epoch 109] ogbg-molbace: 0.721440 test loss: 4.399388
[Epoch 110; Iter     1/   41] train: loss: 0.0004726
[Epoch 110; Iter    31/   41] train: loss: 0.0009242
[Epoch 110] ogbg-molbace: 0.632234 val loss: 3.160262
[Epoch 110] ogbg-molbace: 0.728569 test loss: 4.487045
[Epoch 111; Iter    20/   41] train: loss: 0.0001793
[Epoch 111] ogbg-molbace: 0.630769 val loss: 3.027053
[Epoch 111] ogbg-molbace: 0.736915 test loss: 4.311985
[Epoch 112; Iter     9/   41] train: loss: 0.0009780
[Epoch 112; Iter    39/   41] train: loss: 0.0002925
[Epoch 112] ogbg-molbace: 0.628938 val loss: 2.730747
[Epoch 112] ogbg-molbace: 0.738480 test loss: 4.031725
[Epoch 113; Iter    28/   41] train: loss: 0.0003904
[Epoch 113] ogbg-molbace: 0.636996 val loss: 2.949157
[Epoch 113] ogbg-molbace: 0.747522 test loss: 4.172574
[Epoch 114; Iter    17/   41] train: loss: 0.0024102
[Epoch 114] ogbg-molbace: 0.645788 val loss: 2.762274
[Epoch 114] ogbg-molbace: 0.744045 test loss: 4.092000
[Epoch 115; Iter     6/   41] train: loss: 0.0027572
[Epoch 115; Iter    36/   41] train: loss: 0.0029050
[Epoch 115] ogbg-molbace: 0.657509 val loss: 2.285038
[Epoch 115] ogbg-molbace: 0.741784 test loss: 3.645228
[Epoch 116; Iter    25/   41] train: loss: 0.0002549
[Epoch 116] ogbg-molbace: 0.659707 val loss: 2.364385
[Epoch 116] ogbg-molbace: 0.742132 test loss: 3.716845
[Epoch 117; Iter    14/   41] train: loss: 0.0005912
[Epoch 117] ogbg-molbace: 0.664835 val loss: 2.315656
[Epoch 117] ogbg-molbace: 0.750478 test loss: 3.605691
[Epoch 118; Iter     3/   41] train: loss: 0.0003384
[Epoch 118; Iter    33/   41] train: loss: 0.0005269
[Epoch 118] ogbg-molbace: 0.664835 val loss: 2.288079
[Epoch 118] ogbg-molbace: 0.754130 test loss: 3.515781
[Epoch 119; Iter    22/   41] train: loss: 0.0008913
[Epoch 119] ogbg-molbace: 0.663004 val loss: 2.543060
[Epoch 119] ogbg-molbace: 0.753782 test loss: 3.803944
[Epoch 120; Iter    11/   41] train: loss: 0.0002441
[Epoch 120; Iter    41/   41] train: loss: 0.0005586
[Epoch 120] ogbg-molbace: 0.660806 val loss: 2.355766
[Epoch 120] ogbg-molbace: 0.757086 test loss: 3.523277
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 51.
Statistics on  val_best_checkpoint
mean_pred: 0.5596510767936707
std_pred: 3.4741010665893555
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9479416691911235
rocauc: 0.7205128205128205
ogbg-molbace: 0.7205128205128205
BCEWithLogitsLoss: 1.1074012617270153
Statistics on  test
mean_pred: -2.335329055786133
std_pred: 4.462714672088623
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7728455930939439
rocauc: 0.7563901930099113
ogbg-molbace: 0.7563901930099113
BCEWithLogitsLoss: 2.07367975016435
Statistics on  train
mean_pred: -0.27674227952957153
std_pred: 5.925525665283203
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.9997573581293437
rocauc: 0.9998373287671233
ogbg-molbace: 0.9998373287671233
BCEWithLogitsLoss: 0.04949767760387281
[Epoch 78] ogbg-molbace: 0.663736 val loss: 3.017489
[Epoch 78] ogbg-molbace: 0.750826 test loss: 2.864463
[Epoch 79; Iter    12/   41] train: loss: 0.0047275
[Epoch 79] ogbg-molbace: 0.651282 val loss: 2.441826
[Epoch 79] ogbg-molbace: 0.737089 test loss: 2.828974
[Epoch 80; Iter     1/   41] train: loss: 0.0136798
[Epoch 80; Iter    31/   41] train: loss: 0.0030630
[Epoch 80] ogbg-molbace: 0.660440 val loss: 2.339000
[Epoch 80] ogbg-molbace: 0.760042 test loss: 2.440017
[Epoch 81; Iter    20/   41] train: loss: 0.0028303
[Epoch 81] ogbg-molbace: 0.661172 val loss: 2.827173
[Epoch 81] ogbg-molbace: 0.751695 test loss: 2.682579
[Epoch 82; Iter     9/   41] train: loss: 0.0008605
[Epoch 82; Iter    39/   41] train: loss: 0.0014311
[Epoch 82] ogbg-molbace: 0.656044 val loss: 2.361370
[Epoch 82] ogbg-molbace: 0.758303 test loss: 2.629365
[Epoch 83; Iter    28/   41] train: loss: 0.0007257
[Epoch 83] ogbg-molbace: 0.664103 val loss: 2.221688
[Epoch 83] ogbg-molbace: 0.768910 test loss: 2.254190
[Epoch 84; Iter    17/   41] train: loss: 0.0012487
[Epoch 84] ogbg-molbace: 0.660440 val loss: 2.294817
[Epoch 84] ogbg-molbace: 0.755695 test loss: 2.431433
[Epoch 85; Iter     6/   41] train: loss: 0.0017842
[Epoch 85; Iter    36/   41] train: loss: 0.0007669
[Epoch 85] ogbg-molbace: 0.681319 val loss: 2.321895
[Epoch 85] ogbg-molbace: 0.770127 test loss: 2.287533
[Epoch 86; Iter    25/   41] train: loss: 0.0029678
[Epoch 86] ogbg-molbace: 0.668498 val loss: 2.341676
[Epoch 86] ogbg-molbace: 0.747696 test loss: 2.530233
[Epoch 87; Iter    14/   41] train: loss: 0.0531225
[Epoch 87] ogbg-molbace: 0.671062 val loss: 2.244731
[Epoch 87] ogbg-molbace: 0.753608 test loss: 2.345357
[Epoch 88; Iter     3/   41] train: loss: 0.0015619
[Epoch 88; Iter    33/   41] train: loss: 0.0257907
[Epoch 88] ogbg-molbace: 0.694505 val loss: 3.009633
[Epoch 88] ogbg-molbace: 0.697966 test loss: 4.285547
[Epoch 89; Iter    22/   41] train: loss: 0.1076285
[Epoch 89] ogbg-molbace: 0.549817 val loss: 4.236804
[Epoch 89] ogbg-molbace: 0.590332 test loss: 4.863704
[Epoch 90; Iter    11/   41] train: loss: 1.2390983
[Epoch 90; Iter    41/   41] train: loss: 0.4535000
[Epoch 90] ogbg-molbace: 0.643223 val loss: 2.730304
[Epoch 90] ogbg-molbace: 0.760042 test loss: 2.619652
[Epoch 91; Iter    30/   41] train: loss: 0.1739220
[Epoch 91] ogbg-molbace: 0.649817 val loss: 1.849075
[Epoch 91] ogbg-molbace: 0.772214 test loss: 1.222562
[Epoch 92; Iter    19/   41] train: loss: 0.0236840
[Epoch 92] ogbg-molbace: 0.656777 val loss: 2.026381
[Epoch 92] ogbg-molbace: 0.750826 test loss: 1.945057
[Epoch 93; Iter     8/   41] train: loss: 0.0458786
[Epoch 93; Iter    38/   41] train: loss: 0.1319830
[Epoch 93] ogbg-molbace: 0.645421 val loss: 2.240612
[Epoch 93] ogbg-molbace: 0.757260 test loss: 1.717738
[Epoch 94; Iter    27/   41] train: loss: 0.0059872
[Epoch 94] ogbg-molbace: 0.647985 val loss: 2.129381
[Epoch 94] ogbg-molbace: 0.746653 test loss: 2.182459
[Epoch 95; Iter    16/   41] train: loss: 0.1106965
[Epoch 95] ogbg-molbace: 0.649084 val loss: 2.017863
[Epoch 95] ogbg-molbace: 0.739002 test loss: 2.266323
[Epoch 96; Iter     5/   41] train: loss: 0.0215925
[Epoch 96; Iter    35/   41] train: loss: 0.0322172
[Epoch 96] ogbg-molbace: 0.665201 val loss: 2.072239
[Epoch 96] ogbg-molbace: 0.737437 test loss: 1.830669
[Epoch 97; Iter    24/   41] train: loss: 0.0391108
[Epoch 97] ogbg-molbace: 0.661905 val loss: 2.161665
[Epoch 97] ogbg-molbace: 0.724917 test loss: 2.046025
[Epoch 98; Iter    13/   41] train: loss: 0.0039895
[Epoch 98] ogbg-molbace: 0.663004 val loss: 2.224373
[Epoch 98] ogbg-molbace: 0.728743 test loss: 2.011324
[Epoch 99; Iter     2/   41] train: loss: 0.0011617
[Epoch 99; Iter    32/   41] train: loss: 0.0271931
[Epoch 99] ogbg-molbace: 0.675458 val loss: 2.219099
[Epoch 99] ogbg-molbace: 0.707181 test loss: 1.953244
[Epoch 100; Iter    21/   41] train: loss: 0.0022848
[Epoch 100] ogbg-molbace: 0.658974 val loss: 2.347485
[Epoch 100] ogbg-molbace: 0.702139 test loss: 2.816911
[Epoch 101; Iter    10/   41] train: loss: 0.0024204
[Epoch 101; Iter    40/   41] train: loss: 0.0014015
[Epoch 101] ogbg-molbace: 0.663736 val loss: 2.611202
[Epoch 101] ogbg-molbace: 0.733438 test loss: 2.145360
[Epoch 102; Iter    29/   41] train: loss: 0.0065023
[Epoch 102] ogbg-molbace: 0.690476 val loss: 2.335546
[Epoch 102] ogbg-molbace: 0.741958 test loss: 2.439748
[Epoch 103; Iter    18/   41] train: loss: 0.0022895
[Epoch 103] ogbg-molbace: 0.672527 val loss: 2.263671
[Epoch 103] ogbg-molbace: 0.729091 test loss: 2.534607
[Epoch 104; Iter     7/   41] train: loss: 0.0006736
[Epoch 104; Iter    37/   41] train: loss: 0.0008458
[Epoch 104] ogbg-molbace: 0.682051 val loss: 2.229623
[Epoch 104] ogbg-molbace: 0.744566 test loss: 2.149918
[Epoch 105; Iter    26/   41] train: loss: 0.0004229
[Epoch 105] ogbg-molbace: 0.667399 val loss: 2.454568
[Epoch 105] ogbg-molbace: 0.723874 test loss: 2.851707
[Epoch 106; Iter    15/   41] train: loss: 0.0035390
[Epoch 106] ogbg-molbace: 0.675824 val loss: 2.399772
[Epoch 106] ogbg-molbace: 0.726656 test loss: 2.268551
[Epoch 107; Iter     4/   41] train: loss: 0.0006499
[Epoch 107; Iter    34/   41] train: loss: 0.0018078
[Epoch 107] ogbg-molbace: 0.668864 val loss: 2.665361
[Epoch 107] ogbg-molbace: 0.728917 test loss: 2.869143
[Epoch 108; Iter    23/   41] train: loss: 0.0009056
[Epoch 108] ogbg-molbace: 0.657509 val loss: 2.751027
[Epoch 108] ogbg-molbace: 0.729438 test loss: 2.179913
[Epoch 109; Iter    12/   41] train: loss: 0.2311856
[Epoch 109] ogbg-molbace: 0.680220 val loss: 2.873034
[Epoch 109] ogbg-molbace: 0.745262 test loss: 2.399557
[Epoch 110; Iter     1/   41] train: loss: 0.1401366
[Epoch 110; Iter    31/   41] train: loss: 0.0952965
[Epoch 110] ogbg-molbace: 0.695238 val loss: 1.916623
[Epoch 110] ogbg-molbace: 0.731003 test loss: 1.862729
[Epoch 111; Iter    20/   41] train: loss: 0.2709779
[Epoch 111] ogbg-molbace: 0.691941 val loss: 2.044791
[Epoch 111] ogbg-molbace: 0.709616 test loss: 3.291797
[Epoch 112; Iter     9/   41] train: loss: 0.0704097
[Epoch 112; Iter    39/   41] train: loss: 0.0354833
[Epoch 112] ogbg-molbace: 0.624176 val loss: 2.197935
[Epoch 112] ogbg-molbace: 0.700574 test loss: 1.793067
[Epoch 113; Iter    28/   41] train: loss: 0.0392330
[Epoch 113] ogbg-molbace: 0.674359 val loss: 1.581963
[Epoch 113] ogbg-molbace: 0.740567 test loss: 1.961119
[Epoch 114; Iter    17/   41] train: loss: 0.0077228
[Epoch 114] ogbg-molbace: 0.672161 val loss: 2.311978
[Epoch 114] ogbg-molbace: 0.759694 test loss: 2.837793
[Epoch 115; Iter     6/   41] train: loss: 0.0039741
[Epoch 115; Iter    36/   41] train: loss: 0.0329987
[Epoch 115] ogbg-molbace: 0.686081 val loss: 2.336873
[Epoch 115] ogbg-molbace: 0.766128 test loss: 2.291700
[Epoch 116; Iter    25/   41] train: loss: 0.0107407
[Epoch 116] ogbg-molbace: 0.674359 val loss: 2.006541
[Epoch 116] ogbg-molbace: 0.749957 test loss: 1.915261
[Epoch 117; Iter    14/   41] train: loss: 0.0032907
[Epoch 117] ogbg-molbace: 0.673260 val loss: 2.131518
[Epoch 117] ogbg-molbace: 0.755869 test loss: 1.886108
[Epoch 118; Iter     3/   41] train: loss: 0.0033644
[Epoch 118; Iter    33/   41] train: loss: 0.0013272
[Epoch 118] ogbg-molbace: 0.680952 val loss: 2.061261
[Epoch 118] ogbg-molbace: 0.754130 test loss: 1.774945
[Epoch 119; Iter    22/   41] train: loss: 0.0117209
[Epoch 119] ogbg-molbace: 0.669231 val loss: 2.291265
[Epoch 119] ogbg-molbace: 0.753782 test loss: 2.303214
[Epoch 120; Iter    11/   41] train: loss: 0.0087785
[Epoch 120; Iter    41/   41] train: loss: 0.0004556
[Epoch 120] ogbg-molbace: 0.669963 val loss: 2.302860
[Epoch 120] ogbg-molbace: 0.755347 test loss: 2.099528
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 35.
Statistics on  val_best_checkpoint
mean_pred: -0.31337061524391174
std_pred: 1.4332900047302246
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9540788074332536
rocauc: 0.7402930402930403
ogbg-molbace: 0.7402930402930403
BCEWithLogitsLoss: 0.8504137496153513
Statistics on  test
mean_pred: -1.0356895923614502
std_pred: 1.9687633514404297
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7677066160616048
rocauc: 0.8052512606503217
ogbg-molbace: 0.8052512606503217
BCEWithLogitsLoss: 0.9660393285254637
Statistics on  train
mean_pred: 0.7920772433280945
std_pred: 2.0157933235168457
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.8209341805191824
rocauc: 0.8881078767123287
ogbg-molbace: 0.8881078767123287
BCEWithLogitsLoss: 0.575142139341773
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.637729 val loss: 1.434309
[Epoch 78] ogbg-molbace: 0.755695 test loss: 1.704531
[Epoch 79; Iter    12/   41] train: loss: 0.0038665
[Epoch 79] ogbg-molbace: 0.637729 val loss: 1.492929
[Epoch 79] ogbg-molbace: 0.751174 test loss: 1.983047
[Epoch 80; Iter     1/   41] train: loss: 0.0057496
[Epoch 80; Iter    31/   41] train: loss: 0.0051089
[Epoch 80] ogbg-molbace: 0.655678 val loss: 1.361975
[Epoch 80] ogbg-molbace: 0.762476 test loss: 1.848435
[Epoch 81; Iter    20/   41] train: loss: 0.0021835
[Epoch 81] ogbg-molbace: 0.635531 val loss: 1.359761
[Epoch 81] ogbg-molbace: 0.744045 test loss: 1.995063
[Epoch 82; Iter     9/   41] train: loss: 0.0043171
[Epoch 82; Iter    39/   41] train: loss: 0.0081879
[Epoch 82] ogbg-molbace: 0.638462 val loss: 1.407517
[Epoch 82] ogbg-molbace: 0.753434 test loss: 1.906724
[Epoch 83; Iter    28/   41] train: loss: 0.0023903
[Epoch 83] ogbg-molbace: 0.653480 val loss: 1.337526
[Epoch 83] ogbg-molbace: 0.762650 test loss: 1.751748
[Epoch 84; Iter    17/   41] train: loss: 0.0020860
[Epoch 84] ogbg-molbace: 0.659707 val loss: 1.316182
[Epoch 84] ogbg-molbace: 0.744392 test loss: 1.660422
[Epoch 85; Iter     6/   41] train: loss: 0.0218094
[Epoch 85; Iter    36/   41] train: loss: 0.0030844
[Epoch 85] ogbg-molbace: 0.652015 val loss: 1.393107
[Epoch 85] ogbg-molbace: 0.752391 test loss: 1.742119
[Epoch 86; Iter    25/   41] train: loss: 0.0034377
[Epoch 86] ogbg-molbace: 0.653114 val loss: 1.425024
[Epoch 86] ogbg-molbace: 0.759346 test loss: 1.651213
[Epoch 87; Iter    14/   41] train: loss: 0.0018977
[Epoch 87] ogbg-molbace: 0.650183 val loss: 1.423832
[Epoch 87] ogbg-molbace: 0.749261 test loss: 1.841817
[Epoch 88; Iter     3/   41] train: loss: 0.0008148
[Epoch 88; Iter    33/   41] train: loss: 0.0017166
[Epoch 88] ogbg-molbace: 0.648352 val loss: 1.478854
[Epoch 88] ogbg-molbace: 0.756390 test loss: 1.926310
[Epoch 89; Iter    22/   41] train: loss: 0.0043193
[Epoch 89] ogbg-molbace: 0.643223 val loss: 1.543017
[Epoch 89] ogbg-molbace: 0.751174 test loss: 2.407207
[Epoch 90; Iter    11/   41] train: loss: 0.0014031
[Epoch 90; Iter    41/   41] train: loss: 0.0217763
[Epoch 90] ogbg-molbace: 0.645421 val loss: 1.505597
[Epoch 90] ogbg-molbace: 0.755869 test loss: 2.238707
[Epoch 91; Iter    30/   41] train: loss: 0.0008110
[Epoch 91] ogbg-molbace: 0.643590 val loss: 1.490203
[Epoch 91] ogbg-molbace: 0.740567 test loss: 2.376303
[Epoch 92; Iter    19/   41] train: loss: 0.0011794
[Epoch 92] ogbg-molbace: 0.642857 val loss: 1.590283
[Epoch 92] ogbg-molbace: 0.736568 test loss: 2.581791
[Epoch 93; Iter     8/   41] train: loss: 0.0011926
[Epoch 93; Iter    38/   41] train: loss: 0.0005894
[Epoch 93] ogbg-molbace: 0.640659 val loss: 1.505966
[Epoch 93] ogbg-molbace: 0.743349 test loss: 2.105236
[Epoch 94; Iter    27/   41] train: loss: 0.0008144
[Epoch 94] ogbg-molbace: 0.640293 val loss: 1.568143
[Epoch 94] ogbg-molbace: 0.745436 test loss: 2.280525
[Epoch 95; Iter    16/   41] train: loss: 0.0011078
[Epoch 95] ogbg-molbace: 0.645421 val loss: 1.596173
[Epoch 95] ogbg-molbace: 0.745783 test loss: 2.420659
[Epoch 96; Iter     5/   41] train: loss: 0.0006389
[Epoch 96; Iter    35/   41] train: loss: 0.0006976
[Epoch 96] ogbg-molbace: 0.643223 val loss: 1.556822
[Epoch 96] ogbg-molbace: 0.742827 test loss: 2.284178
[Epoch 97; Iter    24/   41] train: loss: 0.0006886
[Epoch 97] ogbg-molbace: 0.654579 val loss: 1.569521
[Epoch 97] ogbg-molbace: 0.762998 test loss: 2.011914
[Epoch 98; Iter    13/   41] train: loss: 0.0056353
[Epoch 98] ogbg-molbace: 0.643956 val loss: 1.562901
[Epoch 98] ogbg-molbace: 0.751000 test loss: 2.014720
[Epoch 99; Iter     2/   41] train: loss: 0.0008441
[Epoch 99; Iter    32/   41] train: loss: 0.0009066
[Epoch 99] ogbg-molbace: 0.639560 val loss: 1.556077
[Epoch 99] ogbg-molbace: 0.750826 test loss: 2.152797
[Epoch 100; Iter    21/   41] train: loss: 0.0016092
[Epoch 100] ogbg-molbace: 0.643956 val loss: 1.540626
[Epoch 100] ogbg-molbace: 0.748044 test loss: 1.974815
[Epoch 101; Iter    10/   41] train: loss: 0.0009813
[Epoch 101; Iter    40/   41] train: loss: 0.0022678
[Epoch 101] ogbg-molbace: 0.644322 val loss: 1.613125
[Epoch 101] ogbg-molbace: 0.749087 test loss: 2.240681
[Epoch 102; Iter    29/   41] train: loss: 0.0010304
[Epoch 102] ogbg-molbace: 0.642857 val loss: 1.603126
[Epoch 102] ogbg-molbace: 0.748565 test loss: 2.045568
[Epoch 103; Iter    18/   41] train: loss: 0.0007186
[Epoch 103] ogbg-molbace: 0.644689 val loss: 1.589066
[Epoch 103] ogbg-molbace: 0.746479 test loss: 2.047561
[Epoch 104; Iter     7/   41] train: loss: 0.0005612
[Epoch 104; Iter    37/   41] train: loss: 0.0005386
[Epoch 104] ogbg-molbace: 0.648718 val loss: 1.651626
[Epoch 104] ogbg-molbace: 0.750304 test loss: 2.386093
[Epoch 105; Iter    26/   41] train: loss: 0.0006079
[Epoch 105] ogbg-molbace: 0.639927 val loss: 1.659996
[Epoch 105] ogbg-molbace: 0.746827 test loss: 2.253548
[Epoch 106; Iter    15/   41] train: loss: 0.0022157
[Epoch 106] ogbg-molbace: 0.640293 val loss: 1.658350
[Epoch 106] ogbg-molbace: 0.747522 test loss: 2.330571
[Epoch 107; Iter     4/   41] train: loss: 0.0006226
[Epoch 107; Iter    34/   41] train: loss: 0.0005584
[Epoch 107] ogbg-molbace: 0.643956 val loss: 1.626477
[Epoch 107] ogbg-molbace: 0.761085 test loss: 1.987160
[Epoch 108; Iter    23/   41] train: loss: 0.0022616
[Epoch 108] ogbg-molbace: 0.643590 val loss: 1.677201
[Epoch 108] ogbg-molbace: 0.752565 test loss: 2.319515
[Epoch 109; Iter    12/   41] train: loss: 0.0016096
[Epoch 109] ogbg-molbace: 0.638828 val loss: 1.695708
[Epoch 109] ogbg-molbace: 0.742306 test loss: 2.409850
[Epoch 110; Iter     1/   41] train: loss: 0.0005867
[Epoch 110; Iter    31/   41] train: loss: 0.0004888
[Epoch 110] ogbg-molbace: 0.639194 val loss: 1.748112
[Epoch 110] ogbg-molbace: 0.742827 test loss: 2.533204
[Epoch 111; Iter    20/   41] train: loss: 0.0004771
[Epoch 111] ogbg-molbace: 0.658242 val loss: 1.640562
[Epoch 111] ogbg-molbace: 0.759172 test loss: 1.829661
[Epoch 112; Iter     9/   41] train: loss: 0.0007551
[Epoch 112; Iter    39/   41] train: loss: 0.0005988
[Epoch 112] ogbg-molbace: 0.648718 val loss: 1.755246
[Epoch 112] ogbg-molbace: 0.751869 test loss: 2.143305
[Epoch 113; Iter    28/   41] train: loss: 0.0013119
[Epoch 113] ogbg-molbace: 0.616850 val loss: 2.450520
[Epoch 113] ogbg-molbace: 0.740219 test loss: 3.123630
[Epoch 114; Iter    17/   41] train: loss: 0.0212518
[Epoch 114] ogbg-molbace: 0.627106 val loss: 2.158224
[Epoch 114] ogbg-molbace: 0.746479 test loss: 2.450036
[Epoch 115; Iter     6/   41] train: loss: 0.0003572
[Epoch 115; Iter    36/   41] train: loss: 0.0018670
[Epoch 115] ogbg-molbace: 0.628205 val loss: 2.059679
[Epoch 115] ogbg-molbace: 0.752913 test loss: 2.138398
[Epoch 116; Iter    25/   41] train: loss: 0.0022539
[Epoch 116] ogbg-molbace: 0.624542 val loss: 2.050165
[Epoch 116] ogbg-molbace: 0.755347 test loss: 2.160372
[Epoch 117; Iter    14/   41] train: loss: 0.0023535
[Epoch 117] ogbg-molbace: 0.624542 val loss: 2.064002
[Epoch 117] ogbg-molbace: 0.753608 test loss: 2.233620
[Epoch 118; Iter     3/   41] train: loss: 0.0003691
[Epoch 118; Iter    33/   41] train: loss: 0.0008317
[Epoch 118] ogbg-molbace: 0.626740 val loss: 2.106086
[Epoch 118] ogbg-molbace: 0.756390 test loss: 2.502648
[Epoch 119; Iter    22/   41] train: loss: 0.0006302
[Epoch 119] ogbg-molbace: 0.627839 val loss: 1.969663
[Epoch 119] ogbg-molbace: 0.759520 test loss: 2.162178
[Epoch 120; Iter    11/   41] train: loss: 0.0003828
[Epoch 120; Iter    41/   41] train: loss: 0.0021390
[Epoch 120] ogbg-molbace: 0.632234 val loss: 1.981613
[Epoch 120] ogbg-molbace: 0.763519 test loss: 2.089760
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 39.
Statistics on  val_best_checkpoint
mean_pred: 1.2490562200546265
std_pred: 1.8858314752578735
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9513662051534102
rocauc: 0.7410256410256411
ogbg-molbace: 0.7410256410256411
BCEWithLogitsLoss: 0.5942078431447347
Statistics on  test
mean_pred: -0.6611148715019226
std_pred: 2.320175886154175
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --seed 6 --device cuda:1
All runs completed.
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7074080052863508
rocauc: 0.7125717266562337
ogbg-molbace: 0.7125717266562337
BCEWithLogitsLoss: 0.9802410155534744
Statistics on  train
mean_pred: 0.7448639273643494
std_pred: 2.508617877960205
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.8939605027441242
rocauc: 0.9288641552511415
ogbg-molbace: 0.9288641552511415
BCEWithLogitsLoss: 0.47751492334575185
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --seed 6 --device cuda:2
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.660073 val loss: 1.305245
[Epoch 78] ogbg-molbace: 0.754130 test loss: 1.125918
[Epoch 79; Iter    12/   41] train: loss: 0.0032486
[Epoch 79] ogbg-molbace: 0.655678 val loss: 1.266942
[Epoch 79] ogbg-molbace: 0.738828 test loss: 1.146778
[Epoch 80; Iter     1/   41] train: loss: 0.0052908
[Epoch 80; Iter    31/   41] train: loss: 0.0067297
[Epoch 80] ogbg-molbace: 0.640659 val loss: 1.336422
[Epoch 80] ogbg-molbace: 0.728221 test loss: 1.246839
[Epoch 81; Iter    20/   41] train: loss: 0.0044819
[Epoch 81] ogbg-molbace: 0.644322 val loss: 1.350285
[Epoch 81] ogbg-molbace: 0.741958 test loss: 1.136465
[Epoch 82; Iter     9/   41] train: loss: 0.0035178
[Epoch 82; Iter    39/   41] train: loss: 0.0088704
[Epoch 82] ogbg-molbace: 0.642125 val loss: 1.373133
[Epoch 82] ogbg-molbace: 0.746131 test loss: 1.138263
[Epoch 83; Iter    28/   41] train: loss: 0.0024910
[Epoch 83] ogbg-molbace: 0.642857 val loss: 1.324459
[Epoch 83] ogbg-molbace: 0.747522 test loss: 1.158961
[Epoch 84; Iter    17/   41] train: loss: 0.0034115
[Epoch 84] ogbg-molbace: 0.652015 val loss: 1.339230
[Epoch 84] ogbg-molbace: 0.752913 test loss: 1.103220
[Epoch 85; Iter     6/   41] train: loss: 0.0164926
[Epoch 85; Iter    36/   41] train: loss: 0.0034567
[Epoch 85] ogbg-molbace: 0.650549 val loss: 1.409554
[Epoch 85] ogbg-molbace: 0.747522 test loss: 1.138923
[Epoch 86; Iter    25/   41] train: loss: 0.0060423
[Epoch 86] ogbg-molbace: 0.656044 val loss: 1.489851
[Epoch 86] ogbg-molbace: 0.755521 test loss: 1.190029
[Epoch 87; Iter    14/   41] train: loss: 0.0013320
[Epoch 87] ogbg-molbace: 0.657143 val loss: 1.341058
[Epoch 87] ogbg-molbace: 0.752739 test loss: 1.142573
[Epoch 88; Iter     3/   41] train: loss: 0.0009996
[Epoch 88; Iter    33/   41] train: loss: 0.0017408
[Epoch 88] ogbg-molbace: 0.659707 val loss: 1.451319
[Epoch 88] ogbg-molbace: 0.758651 test loss: 1.215678
[Epoch 89; Iter    22/   41] train: loss: 0.0021622
[Epoch 89] ogbg-molbace: 0.664835 val loss: 1.481219
[Epoch 89] ogbg-molbace: 0.762476 test loss: 1.296934
[Epoch 90; Iter    11/   41] train: loss: 0.0014317
[Epoch 90; Iter    41/   41] train: loss: 0.0091280
[Epoch 90] ogbg-molbace: 0.654579 val loss: 1.400090
[Epoch 90] ogbg-molbace: 0.750478 test loss: 1.229260
[Epoch 91; Iter    30/   41] train: loss: 0.0009741
[Epoch 91] ogbg-molbace: 0.663736 val loss: 1.379139
[Epoch 91] ogbg-molbace: 0.766128 test loss: 1.184499
[Epoch 92; Iter    19/   41] train: loss: 0.0015290
[Epoch 92] ogbg-molbace: 0.661172 val loss: 1.397144
[Epoch 92] ogbg-molbace: 0.760042 test loss: 1.218332
[Epoch 93; Iter     8/   41] train: loss: 0.0013725
[Epoch 93; Iter    38/   41] train: loss: 0.0008397
[Epoch 93] ogbg-molbace: 0.661538 val loss: 1.489878
[Epoch 93] ogbg-molbace: 0.762824 test loss: 1.257594
[Epoch 94; Iter    27/   41] train: loss: 0.0007006
[Epoch 94] ogbg-molbace: 0.658242 val loss: 1.392852
[Epoch 94] ogbg-molbace: 0.757955 test loss: 1.195947
[Epoch 95; Iter    16/   41] train: loss: 0.0011788
[Epoch 95] ogbg-molbace: 0.659707 val loss: 1.387878
[Epoch 95] ogbg-molbace: 0.755173 test loss: 1.225415
[Epoch 96; Iter     5/   41] train: loss: 0.0009061
[Epoch 96; Iter    35/   41] train: loss: 0.0005364
[Epoch 96] ogbg-molbace: 0.667766 val loss: 1.434737
[Epoch 96] ogbg-molbace: 0.768214 test loss: 1.228780
[Epoch 97; Iter    24/   41] train: loss: 0.0006144
[Epoch 97] ogbg-molbace: 0.659707 val loss: 1.603716
[Epoch 97] ogbg-molbace: 0.742132 test loss: 1.297419
[Epoch 98; Iter    13/   41] train: loss: 0.0034282
[Epoch 98] ogbg-molbace: 0.659707 val loss: 1.672599
[Epoch 98] ogbg-molbace: 0.746131 test loss: 1.331262
[Epoch 99; Iter     2/   41] train: loss: 0.0010214
[Epoch 99; Iter    32/   41] train: loss: 0.0008219
[Epoch 99] ogbg-molbace: 0.661538 val loss: 1.593213
[Epoch 99] ogbg-molbace: 0.749783 test loss: 1.285657
[Epoch 100; Iter    21/   41] train: loss: 0.0016561
[Epoch 100] ogbg-molbace: 0.663370 val loss: 1.692584
[Epoch 100] ogbg-molbace: 0.747001 test loss: 1.357472
[Epoch 101; Iter    10/   41] train: loss: 0.0028519
[Epoch 101; Iter    40/   41] train: loss: 0.0023049
[Epoch 101] ogbg-molbace: 0.660806 val loss: 1.611946
[Epoch 101] ogbg-molbace: 0.747348 test loss: 1.306087
[Epoch 102; Iter    29/   41] train: loss: 0.0005423
[Epoch 102] ogbg-molbace: 0.662637 val loss: 1.690948
[Epoch 102] ogbg-molbace: 0.751000 test loss: 1.351259
[Epoch 103; Iter    18/   41] train: loss: 0.0006155
[Epoch 103] ogbg-molbace: 0.663370 val loss: 1.708803
[Epoch 103] ogbg-molbace: 0.753086 test loss: 1.365894
[Epoch 104; Iter     7/   41] train: loss: 0.0004922
[Epoch 104; Iter    37/   41] train: loss: 0.0005251
[Epoch 104] ogbg-molbace: 0.657875 val loss: 1.617391
[Epoch 104] ogbg-molbace: 0.743349 test loss: 1.309522
[Epoch 105; Iter    26/   41] train: loss: 0.0006785
[Epoch 105] ogbg-molbace: 0.660440 val loss: 1.650277
[Epoch 105] ogbg-molbace: 0.744914 test loss: 1.341819
[Epoch 106; Iter    15/   41] train: loss: 0.0021833
[Epoch 106] ogbg-molbace: 0.659707 val loss: 1.567904
[Epoch 106] ogbg-molbace: 0.748392 test loss: 1.277561
[Epoch 107; Iter     4/   41] train: loss: 0.0005635
[Epoch 107; Iter    34/   41] train: loss: 0.0004252
[Epoch 107] ogbg-molbace: 0.659707 val loss: 1.695489
[Epoch 107] ogbg-molbace: 0.751348 test loss: 1.341969
[Epoch 108; Iter    23/   41] train: loss: 0.0027561
[Epoch 108] ogbg-molbace: 0.661172 val loss: 1.611976
[Epoch 108] ogbg-molbace: 0.751348 test loss: 1.317261
[Epoch 109; Iter    12/   41] train: loss: 0.0010826
[Epoch 109] ogbg-molbace: 0.662637 val loss: 1.603022
[Epoch 109] ogbg-molbace: 0.748044 test loss: 1.282235
[Epoch 110; Iter     1/   41] train: loss: 0.0010400
[Epoch 110; Iter    31/   41] train: loss: 0.0003840
[Epoch 110] ogbg-molbace: 0.658608 val loss: 1.551772
[Epoch 110] ogbg-molbace: 0.742653 test loss: 1.262647
[Epoch 111; Iter    20/   41] train: loss: 0.0006168
[Epoch 111] ogbg-molbace: 0.651648 val loss: 1.591412
[Epoch 111] ogbg-molbace: 0.743523 test loss: 1.257286
[Epoch 112; Iter     9/   41] train: loss: 0.0005952
[Epoch 112; Iter    39/   41] train: loss: 0.0003043
[Epoch 112] ogbg-molbace: 0.652381 val loss: 1.576064
[Epoch 112] ogbg-molbace: 0.740915 test loss: 1.275848
[Epoch 113; Iter    28/   41] train: loss: 0.0003096
[Epoch 113] ogbg-molbace: 0.647985 val loss: 1.416900
[Epoch 113] ogbg-molbace: 0.749261 test loss: 1.224863
[Epoch 114; Iter    17/   41] train: loss: 0.0033895
[Epoch 114] ogbg-molbace: 0.641392 val loss: 1.373665
[Epoch 114] ogbg-molbace: 0.739871 test loss: 1.220343
[Epoch 115; Iter     6/   41] train: loss: 0.0003802
[Epoch 115; Iter    36/   41] train: loss: 0.0026806
[Epoch 115] ogbg-molbace: 0.644322 val loss: 1.409802
[Epoch 115] ogbg-molbace: 0.740567 test loss: 1.222424
[Epoch 116; Iter    25/   41] train: loss: 0.0018980
[Epoch 116] ogbg-molbace: 0.644689 val loss: 1.448958
[Epoch 116] ogbg-molbace: 0.744045 test loss: 1.235968
[Epoch 117; Iter    14/   41] train: loss: 0.0019039
[Epoch 117] ogbg-molbace: 0.643223 val loss: 1.476950
[Epoch 117] ogbg-molbace: 0.739176 test loss: 1.238634
[Epoch 118; Iter     3/   41] train: loss: 0.0003757
[Epoch 118; Iter    33/   41] train: loss: 0.0015854
[Epoch 118] ogbg-molbace: 0.648352 val loss: 1.437514
[Epoch 118] ogbg-molbace: 0.740915 test loss: 1.253455
[Epoch 119; Iter    22/   41] train: loss: 0.0004667
[Epoch 119] ogbg-molbace: 0.649084 val loss: 1.524925
[Epoch 119] ogbg-molbace: 0.743697 test loss: 1.247842
[Epoch 120; Iter    11/   41] train: loss: 0.0003294
[Epoch 120; Iter    41/   41] train: loss: 0.0006296
[Epoch 120] ogbg-molbace: 0.656044 val loss: 1.618675
[Epoch 120] ogbg-molbace: 0.754130 test loss: 1.305188
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 44.
Statistics on  val_best_checkpoint
mean_pred: 6.3774495124816895
std_pred: 2.786492109298706
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9512408398437977
rocauc: 0.7391941391941392
ogbg-molbace: 0.7391941391941392
BCEWithLogitsLoss: 1.5519785747552912
Statistics on  test
mean_pred: 4.895089149475098
std_pred: 3.561654806137085
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.8039644971320692
rocauc: 0.8151625804207965
ogbg-molbace: 0.8151625804207965
BCEWithLogitsLoss: 1.383192146352182
Statistics on  train
mean_pred: 0.08808377385139465
std_pred: 4.054502010345459
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9958051265168999
rocauc: 0.9976883561643836
ogbg-molbace: 0.9976883561643836
BCEWithLogitsLoss: 0.11875990541969858
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.711702 test loss: 2.448681
[Epoch 124; Iter    27/   41] train: loss: 0.0005345
[Epoch 124] ogbg-molbace: 0.616117 val loss: 1.273994
[Epoch 124] ogbg-molbace: 0.719179 test loss: 2.338052
[Epoch 125; Iter    16/   41] train: loss: 0.0006340
[Epoch 125] ogbg-molbace: 0.609890 val loss: 1.215273
[Epoch 125] ogbg-molbace: 0.711876 test loss: 2.536434
[Epoch 126; Iter     5/   41] train: loss: 0.0008612
[Epoch 126; Iter    35/   41] train: loss: 0.0004655
[Epoch 126] ogbg-molbace: 0.612454 val loss: 1.260447
[Epoch 126] ogbg-molbace: 0.717614 test loss: 2.420973
[Epoch 127; Iter    24/   41] train: loss: 0.0336576
[Epoch 127] ogbg-molbace: 0.632601 val loss: 1.295501
[Epoch 127] ogbg-molbace: 0.758825 test loss: 1.753085
[Epoch 128; Iter    13/   41] train: loss: 0.0002727
[Epoch 128] ogbg-molbace: 0.635531 val loss: 1.237822
[Epoch 128] ogbg-molbace: 0.749435 test loss: 1.810930
[Epoch 129; Iter     2/   41] train: loss: 0.0007987
[Epoch 129; Iter    32/   41] train: loss: 0.0010598
[Epoch 129] ogbg-molbace: 0.627839 val loss: 1.107670
[Epoch 129] ogbg-molbace: 0.731177 test loss: 2.410845
[Epoch 130; Iter    21/   41] train: loss: 0.0003641
[Epoch 130] ogbg-molbace: 0.625641 val loss: 1.322795
[Epoch 130] ogbg-molbace: 0.745088 test loss: 1.954782
[Epoch 131; Iter    10/   41] train: loss: 0.0002600
[Epoch 131; Iter    40/   41] train: loss: 0.0004688
[Epoch 131] ogbg-molbace: 0.630403 val loss: 1.270764
[Epoch 131] ogbg-molbace: 0.723352 test loss: 2.647196
Early stopping criterion based on -ogbg-molbace- that should be max reached after 131 epochs. Best model checkpoint was in epoch 71.
Statistics on  val_best_checkpoint
mean_pred: 4.060181617736816
std_pred: 4.249401092529297
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9243941159610276
rocauc: 0.6659340659340659
ogbg-molbace: 0.6659340659340659
BCEWithLogitsLoss: 0.7800969953338305
Statistics on  test
mean_pred: -3.533240795135498
std_pred: 14.495793342590332
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7239974159264351
rocauc: 0.6974439227960354
ogbg-molbace: 0.6974439227960354
BCEWithLogitsLoss: 3.138193150361379
Statistics on  train
mean_pred: -1.3405152559280396
std_pred: 6.216704368591309
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 1.0
rocauc: 1.0
ogbg-molbace: 1.0
BCEWithLogitsLoss: 0.010321620874470326
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.737263 test loss: 3.543364
[Epoch 124; Iter    27/   41] train: loss: 0.0003675
[Epoch 124] ogbg-molbace: 0.639194 val loss: 2.685340
[Epoch 124] ogbg-molbace: 0.733438 test loss: 3.296628
[Epoch 125; Iter    16/   41] train: loss: 0.0003391
[Epoch 125] ogbg-molbace: 0.641026 val loss: 3.103147
[Epoch 125] ogbg-molbace: 0.731351 test loss: 3.609855
[Epoch 126; Iter     5/   41] train: loss: 0.0006131
[Epoch 126; Iter    35/   41] train: loss: 0.0002278
[Epoch 126] ogbg-molbace: 0.636996 val loss: 3.342049
[Epoch 126] ogbg-molbace: 0.731699 test loss: 3.949942
[Epoch 127; Iter    24/   41] train: loss: 0.0005783
[Epoch 127] ogbg-molbace: 0.641758 val loss: 2.370112
[Epoch 127] ogbg-molbace: 0.736568 test loss: 2.944977
[Epoch 128; Iter    13/   41] train: loss: 0.0004347
[Epoch 128] ogbg-molbace: 0.631502 val loss: 3.275097
[Epoch 128] ogbg-molbace: 0.727873 test loss: 4.134709
[Epoch 129; Iter     2/   41] train: loss: 0.0003157
[Epoch 129; Iter    32/   41] train: loss: 0.0005530
[Epoch 129] ogbg-molbace: 0.632234 val loss: 3.391369
[Epoch 129] ogbg-molbace: 0.727526 test loss: 4.220133
[Epoch 130; Iter    21/   41] train: loss: 0.0009313
[Epoch 130] ogbg-molbace: 0.639560 val loss: 3.118069
[Epoch 130] ogbg-molbace: 0.729960 test loss: 3.708499
[Epoch 131; Iter    10/   41] train: loss: 0.0009408
[Epoch 131; Iter    40/   41] train: loss: 0.0002589
[Epoch 131] ogbg-molbace: 0.635897 val loss: 3.255801
[Epoch 131] ogbg-molbace: 0.731003 test loss: 3.829385
[Epoch 132; Iter    29/   41] train: loss: 0.0007008
[Epoch 132] ogbg-molbace: 0.641026 val loss: 2.836294
[Epoch 132] ogbg-molbace: 0.730134 test loss: 3.419754
[Epoch 133; Iter    18/   41] train: loss: 0.0002109
[Epoch 133] ogbg-molbace: 0.648352 val loss: 2.541293
[Epoch 133] ogbg-molbace: 0.737611 test loss: 3.031439
[Epoch 134; Iter     7/   41] train: loss: 0.0003317
[Epoch 134; Iter    37/   41] train: loss: 0.0004826
[Epoch 134] ogbg-molbace: 0.642125 val loss: 3.318849
[Epoch 134] ogbg-molbace: 0.736046 test loss: 3.878601
[Epoch 135; Iter    26/   41] train: loss: 0.0002873
[Epoch 135] ogbg-molbace: 0.639927 val loss: 2.874634
[Epoch 135] ogbg-molbace: 0.730656 test loss: 3.481904
[Epoch 136; Iter    15/   41] train: loss: 0.0030704
[Epoch 136] ogbg-molbace: 0.636264 val loss: 2.615720
[Epoch 136] ogbg-molbace: 0.729786 test loss: 3.202740
[Epoch 137; Iter     4/   41] train: loss: 0.0011617
[Epoch 137; Iter    34/   41] train: loss: 0.0004368
[Epoch 137] ogbg-molbace: 0.639194 val loss: 3.108288
[Epoch 137] ogbg-molbace: 0.736046 test loss: 3.594754
[Epoch 138; Iter    23/   41] train: loss: 0.0001927
[Epoch 138] ogbg-molbace: 0.643590 val loss: 2.693623
[Epoch 138] ogbg-molbace: 0.735350 test loss: 3.261404
[Epoch 139; Iter    12/   41] train: loss: 0.0004460
[Epoch 139] ogbg-molbace: 0.644689 val loss: 3.314711
[Epoch 139] ogbg-molbace: 0.739176 test loss: 3.782069
[Epoch 140; Iter     1/   41] train: loss: 0.0021873
[Epoch 140; Iter    31/   41] train: loss: 0.0026979
[Epoch 140] ogbg-molbace: 0.646154 val loss: 3.180781
[Epoch 140] ogbg-molbace: 0.740219 test loss: 3.377773
[Epoch 141; Iter    20/   41] train: loss: 0.0003967
[Epoch 141] ogbg-molbace: 0.636996 val loss: 4.655725
[Epoch 141] ogbg-molbace: 0.728569 test loss: 4.815546
[Epoch 142; Iter     9/   41] train: loss: 0.0007149
[Epoch 142; Iter    39/   41] train: loss: 0.0006054
[Epoch 142] ogbg-molbace: 0.634066 val loss: 4.167624
[Epoch 142] ogbg-molbace: 0.727873 test loss: 4.443154
[Epoch 143; Iter    28/   41] train: loss: 0.0003670
[Epoch 143] ogbg-molbace: 0.644689 val loss: 4.417786
[Epoch 143] ogbg-molbace: 0.743523 test loss: 4.461589
[Epoch 144; Iter    17/   41] train: loss: 0.0002860
[Epoch 144] ogbg-molbace: 0.643590 val loss: 3.603130
[Epoch 144] ogbg-molbace: 0.744914 test loss: 3.671711
[Epoch 145; Iter     6/   41] train: loss: 0.0008816
[Epoch 145; Iter    36/   41] train: loss: 0.0010816
[Epoch 145] ogbg-molbace: 0.642491 val loss: 4.369747
[Epoch 145] ogbg-molbace: 0.741784 test loss: 4.500261
[Epoch 146; Iter    25/   41] train: loss: 0.0003449
[Epoch 146] ogbg-molbace: 0.641758 val loss: 3.187047
[Epoch 146] ogbg-molbace: 0.739176 test loss: 3.504230
[Epoch 147; Iter    14/   41] train: loss: 0.0002891
[Epoch 147] ogbg-molbace: 0.645055 val loss: 4.361686
[Epoch 147] ogbg-molbace: 0.743871 test loss: 4.360538
[Epoch 148; Iter     3/   41] train: loss: 0.0008866
[Epoch 148; Iter    33/   41] train: loss: 0.0028152
[Epoch 148] ogbg-molbace: 0.634066 val loss: 3.198350
[Epoch 148] ogbg-molbace: 0.736568 test loss: 3.116186
[Epoch 149; Iter    22/   41] train: loss: 0.0001616
[Epoch 149] ogbg-molbace: 0.634432 val loss: 3.941253
[Epoch 149] ogbg-molbace: 0.733090 test loss: 3.948587
[Epoch 150; Iter    11/   41] train: loss: 0.0002168
[Epoch 150; Iter    41/   41] train: loss: 0.0003172
[Epoch 150] ogbg-molbace: 0.637363 val loss: 4.018415
[Epoch 150] ogbg-molbace: 0.740393 test loss: 3.818632
[Epoch 151; Iter    30/   41] train: loss: 0.0023742
[Epoch 151] ogbg-molbace: 0.639927 val loss: 3.802833
[Epoch 151] ogbg-molbace: 0.742306 test loss: 3.915247
[Epoch 152; Iter    19/   41] train: loss: 0.0006704
[Epoch 152] ogbg-molbace: 0.646886 val loss: 2.905032
[Epoch 152] ogbg-molbace: 0.748565 test loss: 2.969337
[Epoch 153; Iter     8/   41] train: loss: 0.0001870
[Epoch 153; Iter    38/   41] train: loss: 0.0002929
[Epoch 153] ogbg-molbace: 0.641758 val loss: 3.718413
[Epoch 153] ogbg-molbace: 0.744045 test loss: 3.670590
Early stopping criterion based on -ogbg-molbace- that should be max reached after 153 epochs. Best model checkpoint was in epoch 93.
Statistics on  val_best_checkpoint
mean_pred: 16.43134880065918
std_pred: 9.912980079650879
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9461982353848097
rocauc: 0.7304029304029304
ogbg-molbace: 0.7304029304029304
BCEWithLogitsLoss: 3.5889551752819293
Statistics on  test
mean_pred: 8.261195182800293
std_pred: 9.774724006652832
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.5471140661707165
rocauc: 0.565640758129021
ogbg-molbace: 0.565640758129021
BCEWithLogitsLoss: 3.8051922538628182
Statistics on  train
mean_pred: 3.128380537033081
std_pred: 11.293335914611816
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9769725111836338
rocauc: 0.9845490867579909
ogbg-molbace: 0.9845490867579909
BCEWithLogitsLoss: 0.5936813539848095
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --seed 4 --device cuda:3
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --seed 5 --device cuda:3
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --seed 6 --device cuda:3
All runs completed.
[Epoch 78] ogbg-molbace: 0.731136 val loss: 1.277059
[Epoch 78] ogbg-molbace: 0.798991 test loss: 1.583571
[Epoch 79; Iter    12/   41] train: loss: 0.1888224
[Epoch 79] ogbg-molbace: 0.758608 val loss: 1.197859
[Epoch 79] ogbg-molbace: 0.795340 test loss: 1.321342
[Epoch 80; Iter     1/   41] train: loss: 0.1837237
[Epoch 80; Iter    31/   41] train: loss: 0.3294458
[Epoch 80] ogbg-molbace: 0.755311 val loss: 1.159808
[Epoch 80] ogbg-molbace: 0.803339 test loss: 1.494484
[Epoch 81; Iter    20/   41] train: loss: 0.1062149
[Epoch 81] ogbg-molbace: 0.778755 val loss: 1.333810
[Epoch 81] ogbg-molbace: 0.793079 test loss: 1.635874
[Epoch 82; Iter     9/   41] train: loss: 0.0248926
[Epoch 82; Iter    39/   41] train: loss: 0.1091166
[Epoch 82] ogbg-molbace: 0.784249 val loss: 0.953307
[Epoch 82] ogbg-molbace: 0.793949 test loss: 1.076469
[Epoch 83; Iter    28/   41] train: loss: 0.1003161
[Epoch 83] ogbg-molbace: 0.788645 val loss: 0.964753
[Epoch 83] ogbg-molbace: 0.807512 test loss: 1.606729
[Epoch 84; Iter    17/   41] train: loss: 0.0675719
[Epoch 84] ogbg-molbace: 0.772527 val loss: 1.354176
[Epoch 84] ogbg-molbace: 0.779343 test loss: 1.573954
[Epoch 85; Iter     6/   41] train: loss: 0.1655110
[Epoch 85; Iter    36/   41] train: loss: 0.1802382
[Epoch 85] ogbg-molbace: 0.718315 val loss: 1.646047
[Epoch 85] ogbg-molbace: 0.787863 test loss: 1.632540
[Epoch 86; Iter    25/   41] train: loss: 0.0878490
[Epoch 86] ogbg-molbace: 0.777289 val loss: 1.050616
[Epoch 86] ogbg-molbace: 0.782299 test loss: 1.449618
[Epoch 87; Iter    14/   41] train: loss: 0.0812474
[Epoch 87] ogbg-molbace: 0.766667 val loss: 1.329799
[Epoch 87] ogbg-molbace: 0.789428 test loss: 1.256598
[Epoch 88; Iter     3/   41] train: loss: 0.1749398
[Epoch 88; Iter    33/   41] train: loss: 0.1703115
[Epoch 88] ogbg-molbace: 0.752747 val loss: 1.315495
[Epoch 88] ogbg-molbace: 0.798818 test loss: 1.232125
[Epoch 89; Iter    22/   41] train: loss: 0.0367419
[Epoch 89] ogbg-molbace: 0.767766 val loss: 1.070078
[Epoch 89] ogbg-molbace: 0.767345 test loss: 1.170666
[Epoch 90; Iter    11/   41] train: loss: 0.1072720
[Epoch 90; Iter    41/   41] train: loss: 0.3931206
[Epoch 90] ogbg-molbace: 0.753114 val loss: 1.312322
[Epoch 90] ogbg-molbace: 0.815336 test loss: 1.342217
[Epoch 91; Iter    30/   41] train: loss: 0.0782411
[Epoch 91] ogbg-molbace: 0.794505 val loss: 0.898642
[Epoch 91] ogbg-molbace: 0.779690 test loss: 1.258187
[Epoch 92; Iter    19/   41] train: loss: 0.1674617
[Epoch 92] ogbg-molbace: 0.756410 val loss: 1.659145
[Epoch 92] ogbg-molbace: 0.797948 test loss: 1.595380
[Epoch 93; Iter     8/   41] train: loss: 0.1608466
[Epoch 93; Iter    38/   41] train: loss: 0.0836053
[Epoch 93] ogbg-molbace: 0.775824 val loss: 1.172312
[Epoch 93] ogbg-molbace: 0.757607 test loss: 1.809823
[Epoch 94; Iter    27/   41] train: loss: 0.1386995
[Epoch 94] ogbg-molbace: 0.761172 val loss: 1.027422
[Epoch 94] ogbg-molbace: 0.780386 test loss: 1.232045
[Epoch 95; Iter    16/   41] train: loss: 0.0558291
[Epoch 95] ogbg-molbace: 0.797436 val loss: 0.651349
[Epoch 95] ogbg-molbace: 0.800730 test loss: 1.306056
[Epoch 96; Iter     5/   41] train: loss: 0.0397591
[Epoch 96; Iter    35/   41] train: loss: 0.0493704
[Epoch 96] ogbg-molbace: 0.712088 val loss: 1.551351
[Epoch 96] ogbg-molbace: 0.779690 test loss: 1.464423
[Epoch 97; Iter    24/   41] train: loss: 0.1953859
[Epoch 97] ogbg-molbace: 0.750916 val loss: 1.099377
[Epoch 97] ogbg-molbace: 0.794992 test loss: 1.365563
[Epoch 98; Iter    13/   41] train: loss: 0.0598023
[Epoch 98] ogbg-molbace: 0.734066 val loss: 1.697431
[Epoch 98] ogbg-molbace: 0.765084 test loss: 1.677449
[Epoch 99; Iter     2/   41] train: loss: 0.1361055
[Epoch 99; Iter    32/   41] train: loss: 0.0856260
[Epoch 99] ogbg-molbace: 0.752015 val loss: 1.556649
[Epoch 99] ogbg-molbace: 0.787515 test loss: 1.322188
[Epoch 100; Iter    21/   41] train: loss: 0.0947341
[Epoch 100] ogbg-molbace: 0.780586 val loss: 1.168713
[Epoch 100] ogbg-molbace: 0.794818 test loss: 1.715310
[Epoch 101; Iter    10/   41] train: loss: 0.0506106
[Epoch 101; Iter    40/   41] train: loss: 0.0553243
[Epoch 101] ogbg-molbace: 0.761172 val loss: 1.276361
[Epoch 101] ogbg-molbace: 0.788732 test loss: 1.719609
[Epoch 102; Iter    29/   41] train: loss: 0.0636968
[Epoch 102] ogbg-molbace: 0.769597 val loss: 1.281732
[Epoch 102] ogbg-molbace: 0.779690 test loss: 1.716531
[Epoch 103; Iter    18/   41] train: loss: 0.0429326
[Epoch 103] ogbg-molbace: 0.778022 val loss: 1.144243
[Epoch 103] ogbg-molbace: 0.786472 test loss: 1.644726
[Epoch 104; Iter     7/   41] train: loss: 0.0842869
[Epoch 104; Iter    37/   41] train: loss: 0.0129226
[Epoch 104] ogbg-molbace: 0.760073 val loss: 1.239608
[Epoch 104] ogbg-molbace: 0.784559 test loss: 1.641995
[Epoch 105; Iter    26/   41] train: loss: 0.0186326
[Epoch 105] ogbg-molbace: 0.779853 val loss: 1.240817
[Epoch 105] ogbg-molbace: 0.784211 test loss: 1.731549
[Epoch 106; Iter    15/   41] train: loss: 0.0216199
[Epoch 106] ogbg-molbace: 0.791209 val loss: 1.200908
[Epoch 106] ogbg-molbace: 0.756738 test loss: 1.870829
[Epoch 107; Iter     4/   41] train: loss: 0.0323808
[Epoch 107; Iter    34/   41] train: loss: 0.0137916
[Epoch 107] ogbg-molbace: 0.771429 val loss: 1.454705
[Epoch 107] ogbg-molbace: 0.780212 test loss: 1.804951
[Epoch 108; Iter    23/   41] train: loss: 0.0679540
[Epoch 108] ogbg-molbace: 0.770330 val loss: 1.300747
[Epoch 108] ogbg-molbace: 0.779343 test loss: 1.925555
[Epoch 109; Iter    12/   41] train: loss: 0.0365315
[Epoch 109] ogbg-molbace: 0.802930 val loss: 1.127937
[Epoch 109] ogbg-molbace: 0.769779 test loss: 2.078825
[Epoch 110; Iter     1/   41] train: loss: 0.0895016
[Epoch 110; Iter    31/   41] train: loss: 0.0367141
[Epoch 110] ogbg-molbace: 0.758242 val loss: 1.320133
[Epoch 110] ogbg-molbace: 0.777430 test loss: 1.924914
[Epoch 111; Iter    20/   41] train: loss: 0.0776355
[Epoch 111] ogbg-molbace: 0.773626 val loss: 1.267775
[Epoch 111] ogbg-molbace: 0.761954 test loss: 2.039732
[Epoch 112; Iter     9/   41] train: loss: 0.0353651
[Epoch 112; Iter    39/   41] train: loss: 0.0099506
[Epoch 112] ogbg-molbace: 0.767399 val loss: 1.355878
[Epoch 112] ogbg-molbace: 0.749783 test loss: 1.982842
[Epoch 113; Iter    28/   41] train: loss: 0.0412686
[Epoch 113] ogbg-molbace: 0.760806 val loss: 1.242190
[Epoch 113] ogbg-molbace: 0.786994 test loss: 1.849397
[Epoch 114; Iter    17/   41] train: loss: 0.0239931
[Epoch 114] ogbg-molbace: 0.772527 val loss: 1.308355
[Epoch 114] ogbg-molbace: 0.773431 test loss: 1.818791
[Epoch 115; Iter     6/   41] train: loss: 0.1026210
[Epoch 115; Iter    36/   41] train: loss: 0.0185481
[Epoch 115] ogbg-molbace: 0.763370 val loss: 1.428551
[Epoch 115] ogbg-molbace: 0.775343 test loss: 1.915031
[Epoch 116; Iter    25/   41] train: loss: 0.0149387
[Epoch 116] ogbg-molbace: 0.769963 val loss: 1.343271
[Epoch 116] ogbg-molbace: 0.760737 test loss: 1.980366
[Epoch 117; Iter    14/   41] train: loss: 0.0348907
[Epoch 117] ogbg-molbace: 0.743223 val loss: 1.500274
[Epoch 117] ogbg-molbace: 0.751174 test loss: 2.250581
[Epoch 118; Iter     3/   41] train: loss: 0.0192886
[Epoch 118; Iter    33/   41] train: loss: 0.0159892
[Epoch 118] ogbg-molbace: 0.745421 val loss: 1.267092
[Epoch 118] ogbg-molbace: 0.753434 test loss: 2.032948
[Epoch 119; Iter    22/   41] train: loss: 0.0127636
[Epoch 119] ogbg-molbace: 0.789377 val loss: 1.257353
[Epoch 119] ogbg-molbace: 0.745436 test loss: 1.855282
[Epoch 120; Iter    11/   41] train: loss: 0.0566482
[Epoch 120; Iter    41/   41] train: loss: 0.1063163
[Epoch 120] ogbg-molbace: 0.772894 val loss: 1.422987
[Epoch 120] ogbg-molbace: 0.742653 test loss: 2.533800
[Epoch 121; Iter    30/   41] train: loss: 0.0578108
[Epoch 121] ogbg-molbace: 0.737363 val loss: 1.314733
[Epoch 121] ogbg-molbace: 0.745609 test loss: 2.180472
[Epoch 122; Iter    19/   41] train: loss: 0.1318484
[Epoch 122] ogbg-molbace: 0.787912 val loss: 0.900430
[Epoch 122] ogbg-molbace: 0.747001 test loss: 2.156812
[Epoch 123; Iter     8/   41] train: loss: 0.1591693
[Epoch 123; Iter    38/   41] train: loss: 0.0385263
[Epoch 123] ogbg-molbace: 0.786813 val loss: 1.119026
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.689744 val loss: 1.581657
[Epoch 78] ogbg-molbace: 0.799339 test loss: 1.649514
[Epoch 79; Iter    12/   41] train: loss: 0.2298115
[Epoch 79] ogbg-molbace: 0.754945 val loss: 1.281910
[Epoch 79] ogbg-molbace: 0.789428 test loss: 1.679822
[Epoch 80; Iter     1/   41] train: loss: 0.3089251
[Epoch 80; Iter    31/   41] train: loss: 0.0955339
[Epoch 80] ogbg-molbace: 0.747619 val loss: 1.548649
[Epoch 80] ogbg-molbace: 0.790645 test loss: 1.563951
[Epoch 81; Iter    20/   41] train: loss: 0.0993991
[Epoch 81] ogbg-molbace: 0.756410 val loss: 1.063693
[Epoch 81] ogbg-molbace: 0.790123 test loss: 1.454416
[Epoch 82; Iter     9/   41] train: loss: 0.0937556
[Epoch 82; Iter    39/   41] train: loss: 0.1715696
[Epoch 82] ogbg-molbace: 0.740293 val loss: 1.491860
[Epoch 82] ogbg-molbace: 0.782125 test loss: 1.740168
[Epoch 83; Iter    28/   41] train: loss: 0.3427618
[Epoch 83] ogbg-molbace: 0.764835 val loss: 1.227967
[Epoch 83] ogbg-molbace: 0.796383 test loss: 1.696179
[Epoch 84; Iter    17/   41] train: loss: 0.0774818
[Epoch 84] ogbg-molbace: 0.771062 val loss: 1.279745
[Epoch 84] ogbg-molbace: 0.770649 test loss: 1.770895
[Epoch 85; Iter     6/   41] train: loss: 0.2806364
[Epoch 85; Iter    36/   41] train: loss: 0.1612932
[Epoch 85] ogbg-molbace: 0.721612 val loss: 1.330973
[Epoch 85] ogbg-molbace: 0.780560 test loss: 1.424422
[Epoch 86; Iter    25/   41] train: loss: 0.0485655
[Epoch 86] ogbg-molbace: 0.734799 val loss: 1.357224
[Epoch 86] ogbg-molbace: 0.813945 test loss: 1.647407
[Epoch 87; Iter    14/   41] train: loss: 0.0803255
[Epoch 87] ogbg-molbace: 0.736996 val loss: 1.458672
[Epoch 87] ogbg-molbace: 0.807338 test loss: 1.661911
[Epoch 88; Iter     3/   41] train: loss: 0.1522116
[Epoch 88; Iter    33/   41] train: loss: 0.0917358
[Epoch 88] ogbg-molbace: 0.709524 val loss: 1.501613
[Epoch 88] ogbg-molbace: 0.793775 test loss: 1.867737
[Epoch 89; Iter    22/   41] train: loss: 0.1191871
[Epoch 89] ogbg-molbace: 0.756410 val loss: 1.508170
[Epoch 89] ogbg-molbace: 0.783168 test loss: 1.735969
[Epoch 90; Iter    11/   41] train: loss: 0.0648132
[Epoch 90; Iter    41/   41] train: loss: 0.2099299
[Epoch 90] ogbg-molbace: 0.747985 val loss: 1.434776
[Epoch 90] ogbg-molbace: 0.765606 test loss: 1.935355
[Epoch 91; Iter    30/   41] train: loss: 0.0114235
[Epoch 91] ogbg-molbace: 0.728938 val loss: 1.611405
[Epoch 91] ogbg-molbace: 0.770301 test loss: 1.972173
[Epoch 92; Iter    19/   41] train: loss: 0.1354939
[Epoch 92] ogbg-molbace: 0.727473 val loss: 1.303184
[Epoch 92] ogbg-molbace: 0.782125 test loss: 1.660543
[Epoch 93; Iter     8/   41] train: loss: 0.1327517
[Epoch 93; Iter    38/   41] train: loss: 0.1207277
[Epoch 93] ogbg-molbace: 0.735165 val loss: 1.552937
[Epoch 93] ogbg-molbace: 0.782820 test loss: 2.325123
[Epoch 94; Iter    27/   41] train: loss: 0.1487816
[Epoch 94] ogbg-molbace: 0.744322 val loss: 1.578678
[Epoch 94] ogbg-molbace: 0.784038 test loss: 1.763909
[Epoch 95; Iter    16/   41] train: loss: 0.0756934
[Epoch 95] ogbg-molbace: 0.721612 val loss: 1.367456
[Epoch 95] ogbg-molbace: 0.795514 test loss: 2.071979
[Epoch 96; Iter     5/   41] train: loss: 0.2134907
[Epoch 96; Iter    35/   41] train: loss: 0.0467332
[Epoch 96] ogbg-molbace: 0.776557 val loss: 1.041875
[Epoch 96] ogbg-molbace: 0.771518 test loss: 1.603532
[Epoch 97; Iter    24/   41] train: loss: 0.1893810
[Epoch 97] ogbg-molbace: 0.765201 val loss: 1.690382
[Epoch 97] ogbg-molbace: 0.802643 test loss: 1.458915
[Epoch 98; Iter    13/   41] train: loss: 0.0383141
[Epoch 98] ogbg-molbace: 0.739927 val loss: 1.580210
[Epoch 98] ogbg-molbace: 0.782994 test loss: 2.093466
[Epoch 99; Iter     2/   41] train: loss: 0.0087845
[Epoch 99; Iter    32/   41] train: loss: 0.0407119
[Epoch 99] ogbg-molbace: 0.723443 val loss: 1.536442
[Epoch 99] ogbg-molbace: 0.776039 test loss: 2.224727
[Epoch 100; Iter    21/   41] train: loss: 0.0719668
[Epoch 100] ogbg-molbace: 0.736630 val loss: 1.673628
[Epoch 100] ogbg-molbace: 0.772561 test loss: 2.059714
[Epoch 101; Iter    10/   41] train: loss: 0.0278274
[Epoch 101; Iter    40/   41] train: loss: 0.0576348
[Epoch 101] ogbg-molbace: 0.745421 val loss: 1.450862
[Epoch 101] ogbg-molbace: 0.771518 test loss: 2.087665
[Epoch 102; Iter    29/   41] train: loss: 0.0213501
[Epoch 102] ogbg-molbace: 0.734799 val loss: 1.826720
[Epoch 102] ogbg-molbace: 0.767866 test loss: 2.071949
[Epoch 103; Iter    18/   41] train: loss: 0.0182019
[Epoch 103] ogbg-molbace: 0.713187 val loss: 2.001509
[Epoch 103] ogbg-molbace: 0.770996 test loss: 2.523855
[Epoch 104; Iter     7/   41] train: loss: 0.0409792
[Epoch 104; Iter    37/   41] train: loss: 0.0334111
[Epoch 104] ogbg-molbace: 0.759341 val loss: 1.358745
[Epoch 104] ogbg-molbace: 0.767693 test loss: 2.231396
[Epoch 105; Iter    26/   41] train: loss: 0.0188714
[Epoch 105] ogbg-molbace: 0.760073 val loss: 1.603169
[Epoch 105] ogbg-molbace: 0.760216 test loss: 2.538006
[Epoch 106; Iter    15/   41] train: loss: 0.2186791
[Epoch 106] ogbg-molbace: 0.723443 val loss: 1.913844
[Epoch 106] ogbg-molbace: 0.774996 test loss: 2.303982
[Epoch 107; Iter     4/   41] train: loss: 0.0590145
[Epoch 107; Iter    34/   41] train: loss: 0.0221068
[Epoch 107] ogbg-molbace: 0.732234 val loss: 1.786740
[Epoch 107] ogbg-molbace: 0.772909 test loss: 2.385272
[Epoch 108; Iter    23/   41] train: loss: 0.0747040
[Epoch 108] ogbg-molbace: 0.733700 val loss: 1.807878
[Epoch 108] ogbg-molbace: 0.775691 test loss: 2.282963
[Epoch 109; Iter    12/   41] train: loss: 0.0486397
[Epoch 109] ogbg-molbace: 0.722711 val loss: 1.719134
[Epoch 109] ogbg-molbace: 0.770649 test loss: 2.085851
[Epoch 110; Iter     1/   41] train: loss: 0.0256088
[Epoch 110; Iter    31/   41] train: loss: 0.0352244
[Epoch 110] ogbg-molbace: 0.715385 val loss: 2.040853
[Epoch 110] ogbg-molbace: 0.785081 test loss: 2.200899
[Epoch 111; Iter    20/   41] train: loss: 0.0238130
[Epoch 111] ogbg-molbace: 0.733700 val loss: 1.702300
[Epoch 111] ogbg-molbace: 0.775691 test loss: 2.700422
[Epoch 112; Iter     9/   41] train: loss: 0.0139013
[Epoch 112; Iter    39/   41] train: loss: 0.0303888
[Epoch 112] ogbg-molbace: 0.697436 val loss: 2.209105
[Epoch 112] ogbg-molbace: 0.747001 test loss: 2.523818
[Epoch 113; Iter    28/   41] train: loss: 0.0068351
[Epoch 113] ogbg-molbace: 0.720879 val loss: 2.082072
[Epoch 113] ogbg-molbace: 0.767519 test loss: 2.424954
[Epoch 114; Iter    17/   41] train: loss: 0.1072370
[Epoch 114] ogbg-molbace: 0.712454 val loss: 2.365364
[Epoch 114] ogbg-molbace: 0.764563 test loss: 2.438608
[Epoch 115; Iter     6/   41] train: loss: 0.0482884
[Epoch 115; Iter    36/   41] train: loss: 0.0530067
[Epoch 115] ogbg-molbace: 0.722344 val loss: 2.158429
[Epoch 115] ogbg-molbace: 0.758303 test loss: 2.413643
[Epoch 116; Iter    25/   41] train: loss: 0.0486713
[Epoch 116] ogbg-molbace: 0.718315 val loss: 1.807362
[Epoch 116] ogbg-molbace: 0.770301 test loss: 2.349764
[Epoch 117; Iter    14/   41] train: loss: 0.0286985
[Epoch 117] ogbg-molbace: 0.709158 val loss: 2.638823
[Epoch 117] ogbg-molbace: 0.760737 test loss: 2.496310
[Epoch 118; Iter     3/   41] train: loss: 0.0119973
[Epoch 118; Iter    33/   41] train: loss: 0.0400607
[Epoch 118] ogbg-molbace: 0.727839 val loss: 2.183184
[Epoch 118] ogbg-molbace: 0.757955 test loss: 2.236189
[Epoch 119; Iter    22/   41] train: loss: 0.1644646
[Epoch 119] ogbg-molbace: 0.706960 val loss: 2.434640
[Epoch 119] ogbg-molbace: 0.761607 test loss: 2.501258
[Epoch 120; Iter    11/   41] train: loss: 0.0606194
[Epoch 120; Iter    41/   41] train: loss: 0.0261813
[Epoch 120] ogbg-molbace: 0.698901 val loss: 2.338000
[Epoch 120] ogbg-molbace: 0.746653 test loss: 2.307286
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 53.
Statistics on  val_best_checkpoint
mean_pred: 0.13835254311561584
std_pred: 2.1907958984375
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9535363011607559
rocauc: 0.7901098901098901
ogbg-molbace: 0.7901098901098901
BCEWithLogitsLoss: 1.1779974748690922
Statistics on  test
mean_pred: -1.7703815698623657
std_pred: 3.354417324066162
[Epoch 78] ogbg-molbace: 0.768864 val loss: 1.303476
[Epoch 78] ogbg-molbace: 0.802469 test loss: 1.293550
[Epoch 79; Iter    12/   41] train: loss: 0.3068069
[Epoch 79] ogbg-molbace: 0.659341 val loss: 1.524201
[Epoch 79] ogbg-molbace: 0.790123 test loss: 1.292804
[Epoch 80; Iter     1/   41] train: loss: 0.1007878
[Epoch 80; Iter    31/   41] train: loss: 0.2056185
[Epoch 80] ogbg-molbace: 0.693040 val loss: 1.353844
[Epoch 80] ogbg-molbace: 0.758303 test loss: 1.567832
[Epoch 81; Iter    20/   41] train: loss: 0.1122316
[Epoch 81] ogbg-molbace: 0.730403 val loss: 1.094278
[Epoch 81] ogbg-molbace: 0.802295 test loss: 1.130297
[Epoch 82; Iter     9/   41] train: loss: 0.2544503
[Epoch 82; Iter    39/   41] train: loss: 0.1698514
[Epoch 82] ogbg-molbace: 0.742491 val loss: 1.190785
[Epoch 82] ogbg-molbace: 0.814989 test loss: 1.501656
[Epoch 83; Iter    28/   41] train: loss: 0.1579283
[Epoch 83] ogbg-molbace: 0.790476 val loss: 1.000845
[Epoch 83] ogbg-molbace: 0.777952 test loss: 1.234709
[Epoch 84; Iter    17/   41] train: loss: 0.1036754
[Epoch 84] ogbg-molbace: 0.724908 val loss: 0.979987
[Epoch 84] ogbg-molbace: 0.786646 test loss: 1.203413
[Epoch 85; Iter     6/   41] train: loss: 0.1896342
[Epoch 85; Iter    36/   41] train: loss: 0.1734254
[Epoch 85] ogbg-molbace: 0.716484 val loss: 1.195086
[Epoch 85] ogbg-molbace: 0.802469 test loss: 1.308424
[Epoch 86; Iter    25/   41] train: loss: 0.1365872
[Epoch 86] ogbg-molbace: 0.757143 val loss: 1.389410
[Epoch 86] ogbg-molbace: 0.822466 test loss: 1.301654
[Epoch 87; Iter    14/   41] train: loss: 0.1565847
[Epoch 87] ogbg-molbace: 0.777656 val loss: 1.165114
[Epoch 87] ogbg-molbace: 0.798296 test loss: 1.601433
[Epoch 88; Iter     3/   41] train: loss: 0.3906658
[Epoch 88; Iter    33/   41] train: loss: 0.2426504
[Epoch 88] ogbg-molbace: 0.773993 val loss: 1.131514
[Epoch 88] ogbg-molbace: 0.815858 test loss: 1.295986
[Epoch 89; Iter    22/   41] train: loss: 0.1545976
[Epoch 89] ogbg-molbace: 0.772161 val loss: 1.092102
[Epoch 89] ogbg-molbace: 0.817423 test loss: 1.103056
[Epoch 90; Iter    11/   41] train: loss: 0.2693748
[Epoch 90; Iter    41/   41] train: loss: 0.4998174
[Epoch 90] ogbg-molbace: 0.732601 val loss: 1.370445
[Epoch 90] ogbg-molbace: 0.786472 test loss: 1.242304
[Epoch 91; Iter    30/   41] train: loss: 0.1541754
[Epoch 91] ogbg-molbace: 0.728205 val loss: 1.145077
[Epoch 91] ogbg-molbace: 0.823857 test loss: 0.972258
[Epoch 92; Iter    19/   41] train: loss: 0.0857361
[Epoch 92] ogbg-molbace: 0.708425 val loss: 1.741912
[Epoch 92] ogbg-molbace: 0.798470 test loss: 1.510744
[Epoch 93; Iter     8/   41] train: loss: 0.0766452
[Epoch 93; Iter    38/   41] train: loss: 0.2270294
[Epoch 93] ogbg-molbace: 0.748352 val loss: 1.742335
[Epoch 93] ogbg-molbace: 0.788732 test loss: 1.772593
[Epoch 94; Iter    27/   41] train: loss: 0.0371498
[Epoch 94] ogbg-molbace: 0.734066 val loss: 1.745339
[Epoch 94] ogbg-molbace: 0.752391 test loss: 2.166459
[Epoch 95; Iter    16/   41] train: loss: 0.2513996
[Epoch 95] ogbg-molbace: 0.761172 val loss: 1.678200
[Epoch 95] ogbg-molbace: 0.770127 test loss: 1.442629
[Epoch 96; Iter     5/   41] train: loss: 0.1779168
[Epoch 96; Iter    35/   41] train: loss: 0.3229096
[Epoch 96] ogbg-molbace: 0.790842 val loss: 1.383072
[Epoch 96] ogbg-molbace: 0.788732 test loss: 1.617797
[Epoch 97; Iter    24/   41] train: loss: 0.2380256
[Epoch 97] ogbg-molbace: 0.752747 val loss: 1.628272
[Epoch 97] ogbg-molbace: 0.792906 test loss: 1.117642
[Epoch 98; Iter    13/   41] train: loss: 0.0577287
[Epoch 98] ogbg-molbace: 0.742857 val loss: 1.498348
[Epoch 98] ogbg-molbace: 0.819857 test loss: 1.843892
[Epoch 99; Iter     2/   41] train: loss: 0.0549262
[Epoch 99; Iter    32/   41] train: loss: 0.1217922
[Epoch 99] ogbg-molbace: 0.755311 val loss: 1.396112
[Epoch 99] ogbg-molbace: 0.812728 test loss: 1.939183
[Epoch 100; Iter    21/   41] train: loss: 0.2232973
[Epoch 100] ogbg-molbace: 0.733700 val loss: 1.383686
[Epoch 100] ogbg-molbace: 0.805773 test loss: 1.624942
[Epoch 101; Iter    10/   41] train: loss: 0.1301196
[Epoch 101; Iter    40/   41] train: loss: 0.0551688
[Epoch 101] ogbg-molbace: 0.768864 val loss: 1.476028
[Epoch 101] ogbg-molbace: 0.795688 test loss: 1.589837
[Epoch 102; Iter    29/   41] train: loss: 0.1440809
[Epoch 102] ogbg-molbace: 0.727473 val loss: 1.973021
[Epoch 102] ogbg-molbace: 0.788732 test loss: 1.848622
[Epoch 103; Iter    18/   41] train: loss: 0.0856367
[Epoch 103] ogbg-molbace: 0.772527 val loss: 1.401106
[Epoch 103] ogbg-molbace: 0.815163 test loss: 1.556406
[Epoch 104; Iter     7/   41] train: loss: 0.1028829
[Epoch 104; Iter    37/   41] train: loss: 0.0683338
[Epoch 104] ogbg-molbace: 0.778755 val loss: 1.251092
[Epoch 104] ogbg-molbace: 0.792558 test loss: 1.446812
[Epoch 105; Iter    26/   41] train: loss: 0.2065382
[Epoch 105] ogbg-molbace: 0.715751 val loss: 1.872696
[Epoch 105] ogbg-molbace: 0.775343 test loss: 1.921737
[Epoch 106; Iter    15/   41] train: loss: 0.0778524
[Epoch 106] ogbg-molbace: 0.711722 val loss: 1.937982
[Epoch 106] ogbg-molbace: 0.790123 test loss: 1.691082
[Epoch 107; Iter     4/   41] train: loss: 0.0742792
[Epoch 107; Iter    34/   41] train: loss: 0.1468927
[Epoch 107] ogbg-molbace: 0.751648 val loss: 1.716124
[Epoch 107] ogbg-molbace: 0.793253 test loss: 1.609776
[Epoch 108; Iter    23/   41] train: loss: 0.1929346
[Epoch 108] ogbg-molbace: 0.757143 val loss: 1.668366
[Epoch 108] ogbg-molbace: 0.792210 test loss: 1.887591
[Epoch 109; Iter    12/   41] train: loss: 0.0266605
[Epoch 109] ogbg-molbace: 0.734799 val loss: 1.876939
[Epoch 109] ogbg-molbace: 0.797253 test loss: 1.839095
[Epoch 110; Iter     1/   41] train: loss: 0.0432884
[Epoch 110; Iter    31/   41] train: loss: 0.0555287
[Epoch 110] ogbg-molbace: 0.726374 val loss: 1.657850
[Epoch 110] ogbg-molbace: 0.809077 test loss: 1.802778
[Epoch 111; Iter    20/   41] train: loss: 0.0303614
[Epoch 111] ogbg-molbace: 0.761172 val loss: 1.611287
[Epoch 111] ogbg-molbace: 0.781951 test loss: 1.907969
[Epoch 112; Iter     9/   41] train: loss: 0.1719108
[Epoch 112; Iter    39/   41] train: loss: 0.0649861
[Epoch 112] ogbg-molbace: 0.737729 val loss: 1.691455
[Epoch 112] ogbg-molbace: 0.776561 test loss: 1.779912
[Epoch 113; Iter    28/   41] train: loss: 0.0229757
[Epoch 113] ogbg-molbace: 0.761905 val loss: 1.600103
[Epoch 113] ogbg-molbace: 0.792558 test loss: 1.708178
[Epoch 114; Iter    17/   41] train: loss: 0.0849926
[Epoch 114] ogbg-molbace: 0.743956 val loss: 1.892748
[Epoch 114] ogbg-molbace: 0.802295 test loss: 1.958144
[Epoch 115; Iter     6/   41] train: loss: 0.0153130
[Epoch 115; Iter    36/   41] train: loss: 0.0348966
[Epoch 115] ogbg-molbace: 0.750916 val loss: 1.798449
[Epoch 115] ogbg-molbace: 0.798644 test loss: 1.616676
[Epoch 116; Iter    25/   41] train: loss: 0.0451157
[Epoch 116] ogbg-molbace: 0.741026 val loss: 1.630218
[Epoch 116] ogbg-molbace: 0.786820 test loss: 1.874298
[Epoch 117; Iter    14/   41] train: loss: 0.0348525
[Epoch 117] ogbg-molbace: 0.745788 val loss: 1.784454
[Epoch 117] ogbg-molbace: 0.793253 test loss: 1.751268
[Epoch 118; Iter     3/   41] train: loss: 0.0181313
[Epoch 118; Iter    33/   41] train: loss: 0.0371167
[Epoch 118] ogbg-molbace: 0.755678 val loss: 1.774710
[Epoch 118] ogbg-molbace: 0.775865 test loss: 1.787790
[Epoch 119; Iter    22/   41] train: loss: 0.0519356
[Epoch 119] ogbg-molbace: 0.743223 val loss: 1.869178
[Epoch 119] ogbg-molbace: 0.788211 test loss: 2.282164
[Epoch 120; Iter    11/   41] train: loss: 0.0191931
[Epoch 120; Iter    41/   41] train: loss: 0.0053281
[Epoch 120] ogbg-molbace: 0.732601 val loss: 1.751970
[Epoch 120] ogbg-molbace: 0.807686 test loss: 1.964129
[Epoch 121; Iter    30/   41] train: loss: 0.0058255
[Epoch 121] ogbg-molbace: 0.735897 val loss: 2.137523
[Epoch 121] ogbg-molbace: 0.796731 test loss: 2.578450
[Epoch 122; Iter    19/   41] train: loss: 0.0513835
[Epoch 122] ogbg-molbace: 0.719048 val loss: 1.399818
[Epoch 122] ogbg-molbace: 0.810989 test loss: 1.901788
[Epoch 123; Iter     8/   41] train: loss: 0.0151116
[Epoch 123; Iter    38/   41] train: loss: 0.0367800
[Epoch 123] ogbg-molbace: 0.736630 val loss: 1.625178
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7567710286327205
rocauc: 0.7906451051990958
ogbg-molbace: 0.7906451051990958
BCEWithLogitsLoss: 1.1410112157464027
Statistics on  train
mean_pred: -1.1564477682113647
std_pred: 3.224738836288452
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9420859495120202
rocauc: 0.9614982876712329
ogbg-molbace: 0.9614982876712329
BCEWithLogitsLoss: 0.2467620345513995
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.800904 test loss: 2.056716
[Epoch 124; Iter    27/   41] train: loss: 0.0379526
[Epoch 124] ogbg-molbace: 0.723810 val loss: 1.710651
[Epoch 124] ogbg-molbace: 0.772387 test loss: 2.103534
[Epoch 125; Iter    16/   41] train: loss: 0.0110698
[Epoch 125] ogbg-molbace: 0.715385 val loss: 1.844270
[Epoch 125] ogbg-molbace: 0.808381 test loss: 1.960587
[Epoch 126; Iter     5/   41] train: loss: 0.0604898
[Epoch 126; Iter    35/   41] train: loss: 0.0444101
[Epoch 126] ogbg-molbace: 0.739194 val loss: 2.113328
[Epoch 126] ogbg-molbace: 0.796905 test loss: 2.113406
[Epoch 127; Iter    24/   41] train: loss: 0.0507090
[Epoch 127] ogbg-molbace: 0.744689 val loss: 1.676450
[Epoch 127] ogbg-molbace: 0.808729 test loss: 1.657278
[Epoch 128; Iter    13/   41] train: loss: 0.0222186
[Epoch 128] ogbg-molbace: 0.735165 val loss: 2.315574
[Epoch 128] ogbg-molbace: 0.788732 test loss: 1.701055
[Epoch 129; Iter     2/   41] train: loss: 0.0097312
[Epoch 129; Iter    32/   41] train: loss: 0.0331512
[Epoch 129] ogbg-molbace: 0.745421 val loss: 1.719507
[Epoch 129] ogbg-molbace: 0.812380 test loss: 1.856239
[Epoch 130; Iter    21/   41] train: loss: 0.0238917
[Epoch 130] ogbg-molbace: 0.727473 val loss: 2.103209
[Epoch 130] ogbg-molbace: 0.794297 test loss: 1.904030
[Epoch 131; Iter    10/   41] train: loss: 0.0117917
[Epoch 131; Iter    40/   41] train: loss: 0.0534050
[Epoch 131] ogbg-molbace: 0.702564 val loss: 1.986004
[Epoch 131] ogbg-molbace: 0.770649 test loss: 2.655702
[Epoch 132; Iter    29/   41] train: loss: 0.1084497
[Epoch 132] ogbg-molbace: 0.727473 val loss: 1.978726
[Epoch 132] ogbg-molbace: 0.799339 test loss: 1.695720
[Epoch 133; Iter    18/   41] train: loss: 0.0295883
[Epoch 133] ogbg-molbace: 0.726007 val loss: 1.956436
[Epoch 133] ogbg-molbace: 0.784211 test loss: 2.174856
[Epoch 134; Iter     7/   41] train: loss: 0.0131806
[Epoch 134; Iter    37/   41] train: loss: 0.0041140
[Epoch 134] ogbg-molbace: 0.729304 val loss: 2.012278
[Epoch 134] ogbg-molbace: 0.800556 test loss: 1.853338
[Epoch 135; Iter    26/   41] train: loss: 0.0062929
[Epoch 135] ogbg-molbace: 0.728571 val loss: 1.929080
[Epoch 135] ogbg-molbace: 0.801774 test loss: 1.956214
[Epoch 136; Iter    15/   41] train: loss: 0.0140096
[Epoch 136] ogbg-molbace: 0.738095 val loss: 1.792474
[Epoch 136] ogbg-molbace: 0.796035 test loss: 1.898213
[Epoch 137; Iter     4/   41] train: loss: 0.0101290
[Epoch 137; Iter    34/   41] train: loss: 0.0060911
[Epoch 137] ogbg-molbace: 0.739560 val loss: 2.082387
[Epoch 137] ogbg-molbace: 0.796035 test loss: 1.938658
[Epoch 138; Iter    23/   41] train: loss: 0.0042712
[Epoch 138] ogbg-molbace: 0.743223 val loss: 2.029525
[Epoch 138] ogbg-molbace: 0.789950 test loss: 1.928388
[Epoch 139; Iter    12/   41] train: loss: 0.0072810
[Epoch 139] ogbg-molbace: 0.753114 val loss: 2.010033
[Epoch 139] ogbg-molbace: 0.791341 test loss: 1.999891
[Epoch 140; Iter     1/   41] train: loss: 0.1030929
[Epoch 140; Iter    31/   41] train: loss: 0.0070033
[Epoch 140] ogbg-molbace: 0.736264 val loss: 2.278268
[Epoch 140] ogbg-molbace: 0.789254 test loss: 1.710651
[Epoch 141; Iter    20/   41] train: loss: 0.0300369
[Epoch 141] ogbg-molbace: 0.715751 val loss: 1.926347
[Epoch 141] ogbg-molbace: 0.790297 test loss: 1.891323
[Epoch 142; Iter     9/   41] train: loss: 0.0024091
[Epoch 142; Iter    39/   41] train: loss: 0.0047070
[Epoch 142] ogbg-molbace: 0.742491 val loss: 2.279325
[Epoch 142] ogbg-molbace: 0.779169 test loss: 2.124871
[Epoch 143; Iter    28/   41] train: loss: 0.0383219
[Epoch 143] ogbg-molbace: 0.721612 val loss: 2.360150
[Epoch 143] ogbg-molbace: 0.775343 test loss: 2.303375
[Epoch 144; Iter    17/   41] train: loss: 0.0708297
[Epoch 144] ogbg-molbace: 0.727106 val loss: 2.306047
[Epoch 144] ogbg-molbace: 0.784907 test loss: 2.272413
[Epoch 145; Iter     6/   41] train: loss: 0.0049088
[Epoch 145; Iter    36/   41] train: loss: 0.0040272
[Epoch 145] ogbg-molbace: 0.736996 val loss: 2.187054
[Epoch 145] ogbg-molbace: 0.777604 test loss: 2.430750
[Epoch 146; Iter    25/   41] train: loss: 0.0145004
[Epoch 146] ogbg-molbace: 0.713919 val loss: 2.282579
[Epoch 146] ogbg-molbace: 0.781951 test loss: 2.330154
[Epoch 147; Iter    14/   41] train: loss: 0.0070468
[Epoch 147] ogbg-molbace: 0.730769 val loss: 2.445090
[Epoch 147] ogbg-molbace: 0.798296 test loss: 2.186191
[Epoch 148; Iter     3/   41] train: loss: 0.0125714
[Epoch 148; Iter    33/   41] train: loss: 0.0658704
[Epoch 148] ogbg-molbace: 0.743223 val loss: 2.199742
[Epoch 148] ogbg-molbace: 0.785081 test loss: 2.089352
[Epoch 149; Iter    22/   41] train: loss: 0.0064451
[Epoch 149] ogbg-molbace: 0.732601 val loss: 2.254881
[Epoch 149] ogbg-molbace: 0.788037 test loss: 2.169072
[Epoch 150; Iter    11/   41] train: loss: 0.0175022
[Epoch 150; Iter    41/   41] train: loss: 0.0050303
[Epoch 150] ogbg-molbace: 0.758242 val loss: 2.137410
[Epoch 150] ogbg-molbace: 0.786472 test loss: 2.012474
[Epoch 151; Iter    30/   41] train: loss: 0.0041008
[Epoch 151] ogbg-molbace: 0.739927 val loss: 2.175182
[Epoch 151] ogbg-molbace: 0.788732 test loss: 2.106606
[Epoch 152; Iter    19/   41] train: loss: 0.0031277
[Epoch 152] ogbg-molbace: 0.746886 val loss: 2.348395
[Epoch 152] ogbg-molbace: 0.780908 test loss: 1.968474
[Epoch 153; Iter     8/   41] train: loss: 0.0058618
[Epoch 153; Iter    38/   41] train: loss: 0.0209768
[Epoch 153] ogbg-molbace: 0.749451 val loss: 2.275397
[Epoch 153] ogbg-molbace: 0.785429 test loss: 2.227582
[Epoch 154; Iter    27/   41] train: loss: 0.0053171
[Epoch 154] ogbg-molbace: 0.734432 val loss: 2.257186
[Epoch 154] ogbg-molbace: 0.793601 test loss: 1.840437
[Epoch 155; Iter    16/   41] train: loss: 0.0792619
[Epoch 155] ogbg-molbace: 0.738828 val loss: 2.443433
[Epoch 155] ogbg-molbace: 0.781255 test loss: 2.054145
[Epoch 156; Iter     5/   41] train: loss: 0.0072845
[Epoch 156; Iter    35/   41] train: loss: 0.0030128
[Epoch 156] ogbg-molbace: 0.723077 val loss: 2.782863
[Epoch 156] ogbg-molbace: 0.775517 test loss: 2.421833
Early stopping criterion based on -ogbg-molbace- that should be max reached after 156 epochs. Best model checkpoint was in epoch 96.
Statistics on  val_best_checkpoint
mean_pred: 1.369286298751831
std_pred: 4.489266395568848
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9561738523062454
rocauc: 0.7908424908424909
ogbg-molbace: 0.7908424908424909
BCEWithLogitsLoss: 1.383072167634964
Statistics on  test
mean_pred: -1.535839557647705
std_pred: 6.010002613067627
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7736045772442706
rocauc: 0.7887323943661971
ogbg-molbace: 0.7887323943661971
BCEWithLogitsLoss: 1.6177971276144187
Statistics on  train
mean_pred: -0.7037322521209717
std_pred: 5.145493030548096
mean_targets: 0.39669421315193176
std_targets: 0.489413857460022
prcauc: 0.9788979769725283
rocauc: 0.9845348173515981
ogbg-molbace: 0.9845348173515981
BCEWithLogitsLoss: 0.16450581672351536
[Epoch 123] ogbg-molbace: 0.746827 test loss: 1.975460
[Epoch 124; Iter    27/   41] train: loss: 0.0352074
[Epoch 124] ogbg-molbace: 0.806593 val loss: 1.187153
[Epoch 124] ogbg-molbace: 0.747522 test loss: 2.098765
[Epoch 125; Iter    16/   41] train: loss: 0.0416448
[Epoch 125] ogbg-molbace: 0.789744 val loss: 1.192727
[Epoch 125] ogbg-molbace: 0.756390 test loss: 2.064565
[Epoch 126; Iter     5/   41] train: loss: 0.0063154
[Epoch 126; Iter    35/   41] train: loss: 0.0037151
[Epoch 126] ogbg-molbace: 0.773626 val loss: 1.265305
[Epoch 126] ogbg-molbace: 0.762998 test loss: 1.896155
[Epoch 127; Iter    24/   41] train: loss: 0.0150322
[Epoch 127] ogbg-molbace: 0.785714 val loss: 1.153510
[Epoch 127] ogbg-molbace: 0.747174 test loss: 2.046099
[Epoch 128; Iter    13/   41] train: loss: 0.0106661
[Epoch 128] ogbg-molbace: 0.789011 val loss: 1.291082
[Epoch 128] ogbg-molbace: 0.758825 test loss: 2.094174
[Epoch 129; Iter     2/   41] train: loss: 0.0049097
[Epoch 129; Iter    32/   41] train: loss: 0.0106655
[Epoch 129] ogbg-molbace: 0.788278 val loss: 1.228156
[Epoch 129] ogbg-molbace: 0.750304 test loss: 2.176528
[Epoch 130; Iter    21/   41] train: loss: 0.0189047
[Epoch 130] ogbg-molbace: 0.776923 val loss: 1.244928
[Epoch 130] ogbg-molbace: 0.753260 test loss: 2.065542
[Epoch 131; Iter    10/   41] train: loss: 0.0346298
[Epoch 131; Iter    40/   41] train: loss: 0.0263400
[Epoch 131] ogbg-molbace: 0.772894 val loss: 1.329081
[Epoch 131] ogbg-molbace: 0.746479 test loss: 2.022393
[Epoch 132; Iter    29/   41] train: loss: 0.0105995
[Epoch 132] ogbg-molbace: 0.784249 val loss: 1.328478
[Epoch 132] ogbg-molbace: 0.756390 test loss: 2.152624
[Epoch 133; Iter    18/   41] train: loss: 0.0197089
[Epoch 133] ogbg-molbace: 0.792308 val loss: 1.299959
[Epoch 133] ogbg-molbace: 0.760389 test loss: 2.029575
[Epoch 134; Iter     7/   41] train: loss: 0.0127030
[Epoch 134; Iter    37/   41] train: loss: 0.0076461
[Epoch 134] ogbg-molbace: 0.789011 val loss: 1.208291
[Epoch 134] ogbg-molbace: 0.751695 test loss: 2.050622
[Epoch 135; Iter    26/   41] train: loss: 0.0054638
[Epoch 135] ogbg-molbace: 0.781319 val loss: 1.397512
[Epoch 135] ogbg-molbace: 0.750652 test loss: 2.166034
[Epoch 136; Iter    15/   41] train: loss: 0.0411399
[Epoch 136] ogbg-molbace: 0.773626 val loss: 1.663394
[Epoch 136] ogbg-molbace: 0.759346 test loss: 2.040112
[Epoch 137; Iter     4/   41] train: loss: 0.0157697
[Epoch 137; Iter    34/   41] train: loss: 0.0120747
[Epoch 137] ogbg-molbace: 0.774359 val loss: 1.394571
[Epoch 137] ogbg-molbace: 0.761433 test loss: 1.976341
[Epoch 138; Iter    23/   41] train: loss: 0.0039572
[Epoch 138] ogbg-molbace: 0.790476 val loss: 1.201200
[Epoch 138] ogbg-molbace: 0.759520 test loss: 2.055768
[Epoch 139; Iter    12/   41] train: loss: 0.0029010
[Epoch 139] ogbg-molbace: 0.761172 val loss: 1.480515
[Epoch 139] ogbg-molbace: 0.768040 test loss: 1.971701
[Epoch 140; Iter     1/   41] train: loss: 0.0163184
[Epoch 140; Iter    31/   41] train: loss: 0.0971119
[Epoch 140] ogbg-molbace: 0.774359 val loss: 1.366480
[Epoch 140] ogbg-molbace: 0.768040 test loss: 1.976516
[Epoch 141; Iter    20/   41] train: loss: 0.0347992
[Epoch 141] ogbg-molbace: 0.764103 val loss: 1.588885
[Epoch 141] ogbg-molbace: 0.770475 test loss: 2.088649
[Epoch 142; Iter     9/   41] train: loss: 0.0182645
[Epoch 142; Iter    39/   41] train: loss: 0.0118867
[Epoch 142] ogbg-molbace: 0.767033 val loss: 1.557176
[Epoch 142] ogbg-molbace: 0.772387 test loss: 2.270466
[Epoch 143; Iter    28/   41] train: loss: 0.0041405
[Epoch 143] ogbg-molbace: 0.781685 val loss: 1.422046
[Epoch 143] ogbg-molbace: 0.767519 test loss: 2.212938
[Epoch 144; Iter    17/   41] train: loss: 0.0017586
[Epoch 144] ogbg-molbace: 0.774359 val loss: 1.472366
[Epoch 144] ogbg-molbace: 0.759346 test loss: 2.202670
[Epoch 145; Iter     6/   41] train: loss: 0.0073957
[Epoch 145; Iter    36/   41] train: loss: 0.0079041
[Epoch 145] ogbg-molbace: 0.783150 val loss: 1.439900
[Epoch 145] ogbg-molbace: 0.752739 test loss: 2.254752
[Epoch 146; Iter    25/   41] train: loss: 0.0024000
[Epoch 146] ogbg-molbace: 0.784615 val loss: 1.366225
[Epoch 146] ogbg-molbace: 0.760737 test loss: 2.252799
[Epoch 147; Iter    14/   41] train: loss: 0.0609788
[Epoch 147] ogbg-molbace: 0.786447 val loss: 1.433075
[Epoch 147] ogbg-molbace: 0.755521 test loss: 2.129462
[Epoch 148; Iter     3/   41] train: loss: 0.0210367
[Epoch 148; Iter    33/   41] train: loss: 0.0283290
[Epoch 148] ogbg-molbace: 0.774359 val loss: 1.608703
[Epoch 148] ogbg-molbace: 0.753608 test loss: 2.145330
[Epoch 149; Iter    22/   41] train: loss: 0.0052453
[Epoch 149] ogbg-molbace: 0.775458 val loss: 1.545909
[Epoch 149] ogbg-molbace: 0.749783 test loss: 2.281364
[Epoch 150; Iter    11/   41] train: loss: 0.0015508
[Epoch 150; Iter    41/   41] train: loss: 0.0033154
[Epoch 150] ogbg-molbace: 0.775092 val loss: 1.450070
[Epoch 150] ogbg-molbace: 0.757086 test loss: 2.199235
[Epoch 151; Iter    30/   41] train: loss: 0.0060586
[Epoch 151] ogbg-molbace: 0.772161 val loss: 1.526374
[Epoch 151] ogbg-molbace: 0.758998 test loss: 2.261145
[Epoch 152; Iter    19/   41] train: loss: 0.0351375
[Epoch 152] ogbg-molbace: 0.783150 val loss: 1.482531
[Epoch 152] ogbg-molbace: 0.752217 test loss: 2.350374
[Epoch 153; Iter     8/   41] train: loss: 0.0039324
[Epoch 153; Iter    38/   41] train: loss: 0.0012867
[Epoch 153] ogbg-molbace: 0.784982 val loss: 1.453902
[Epoch 153] ogbg-molbace: 0.749957 test loss: 2.260659
[Epoch 154; Iter    27/   41] train: loss: 0.0032385
[Epoch 154] ogbg-molbace: 0.784249 val loss: 1.503936
[Epoch 154] ogbg-molbace: 0.750652 test loss: 2.320690
[Epoch 155; Iter    16/   41] train: loss: 0.0123431
[Epoch 155] ogbg-molbace: 0.783516 val loss: 1.441912
[Epoch 155] ogbg-molbace: 0.748913 test loss: 2.189884
[Epoch 156; Iter     5/   41] train: loss: 0.0029637
[Epoch 156; Iter    35/   41] train: loss: 0.0333276
[Epoch 156] ogbg-molbace: 0.782051 val loss: 1.482743
[Epoch 156] ogbg-molbace: 0.750478 test loss: 2.226790
[Epoch 157; Iter    24/   41] train: loss: 0.0062894
[Epoch 157] ogbg-molbace: 0.782784 val loss: 1.493984
[Epoch 157] ogbg-molbace: 0.754999 test loss: 2.246201
[Epoch 158; Iter    13/   41] train: loss: 0.0088693
[Epoch 158] ogbg-molbace: 0.778388 val loss: 1.532137
[Epoch 158] ogbg-molbace: 0.755173 test loss: 2.296790
[Epoch 159; Iter     2/   41] train: loss: 0.0034788
[Epoch 159; Iter    32/   41] train: loss: 0.0229933
[Epoch 159] ogbg-molbace: 0.766300 val loss: 1.468395
[Epoch 159] ogbg-molbace: 0.773431 test loss: 1.993577
[Epoch 160; Iter    21/   41] train: loss: 0.0126691
[Epoch 160] ogbg-molbace: 0.771062 val loss: 1.445859
[Epoch 160] ogbg-molbace: 0.772040 test loss: 2.215309
[Epoch 161; Iter    10/   41] train: loss: 0.0041695
[Epoch 161; Iter    40/   41] train: loss: 0.0034520
[Epoch 161] ogbg-molbace: 0.775458 val loss: 1.536948
[Epoch 161] ogbg-molbace: 0.766475 test loss: 2.238850
[Epoch 162; Iter    29/   41] train: loss: 0.0029702
[Epoch 162] ogbg-molbace: 0.775458 val loss: 1.547613
[Epoch 162] ogbg-molbace: 0.763867 test loss: 2.248315
[Epoch 163; Iter    18/   41] train: loss: 0.0014139
[Epoch 163] ogbg-molbace: 0.776557 val loss: 1.542546
[Epoch 163] ogbg-molbace: 0.760737 test loss: 2.155747
[Epoch 164; Iter     7/   41] train: loss: 0.0040763
[Epoch 164; Iter    37/   41] train: loss: 0.0035648
[Epoch 164] ogbg-molbace: 0.777656 val loss: 1.658133
[Epoch 164] ogbg-molbace: 0.762476 test loss: 2.249472
[Epoch 165; Iter    26/   41] train: loss: 0.0033070
[Epoch 165] ogbg-molbace: 0.778755 val loss: 1.590871
[Epoch 165] ogbg-molbace: 0.755173 test loss: 2.344323
[Epoch 166; Iter    15/   41] train: loss: 0.0083676
[Epoch 166] ogbg-molbace: 0.769963 val loss: 1.654957
[Epoch 166] ogbg-molbace: 0.752565 test loss: 2.237297
[Epoch 167; Iter     4/   41] train: loss: 0.0087777
[Epoch 167; Iter    34/   41] train: loss: 0.0085984
[Epoch 167] ogbg-molbace: 0.768864 val loss: 1.602577
[Epoch 167] ogbg-molbace: 0.753608 test loss: 2.305082
[Epoch 168; Iter    23/   41] train: loss: 0.0012518
[Epoch 168] ogbg-molbace: 0.775458 val loss: 1.555703
[Epoch 168] ogbg-molbace: 0.752913 test loss: 2.310748/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 169; Iter    12/   41] train: loss: 0.0011365
[Epoch 169] ogbg-molbace: 0.773993 val loss: 1.590384
[Epoch 169] ogbg-molbace: 0.758998 test loss: 2.401204
[Epoch 170; Iter     1/   41] train: loss: 0.0078410
[Epoch 170; Iter    31/   41] train: loss: 0.0044423
[Epoch 170] ogbg-molbace: 0.775458 val loss: 1.525554
[Epoch 170] ogbg-molbace: 0.755869 test loss: 2.289711
[Epoch 171; Iter    20/   41] train: loss: 0.0019522
[Epoch 171] ogbg-molbace: 0.779487 val loss: 1.398944
[Epoch 171] ogbg-molbace: 0.762824 test loss: 2.223596
[Epoch 172; Iter     9/   41] train: loss: 0.0015076
[Epoch 172; Iter    39/   41] train: loss: 0.0015584
[Epoch 172] ogbg-molbace: 0.781685 val loss: 1.519163
[Epoch 172] ogbg-molbace: 0.752217 test loss: 2.221709
[Epoch 173; Iter    28/   41] train: loss: 0.0022328
[Epoch 173] ogbg-molbace: 0.775824 val loss: 1.580892
[Epoch 173] ogbg-molbace: 0.749783 test loss: 2.330979
[Epoch 174; Iter    17/   41] train: loss: 0.0048961
[Epoch 174] ogbg-molbace: 0.772527 val loss: 1.480356
[Epoch 174] ogbg-molbace: 0.746305 test loss: 2.315791
[Epoch 175; Iter     6/   41] train: loss: 0.0012208
[Epoch 175; Iter    36/   41] train: loss: 0.0018112
[Epoch 175] ogbg-molbace: 0.782051 val loss: 1.519817
[Epoch 175] ogbg-molbace: 0.751174 test loss: 2.302902
[Epoch 176; Iter    25/   41] train: loss: 0.0013919
[Epoch 176] ogbg-molbace: 0.780586 val loss: 1.437092
[Epoch 176] ogbg-molbace: 0.755521 test loss: 2.292688
[Epoch 177; Iter    14/   41] train: loss: 0.0008065
[Epoch 177] ogbg-molbace: 0.774725 val loss: 1.516769
[Epoch 177] ogbg-molbace: 0.747348 test loss: 2.346308
[Epoch 178; Iter     3/   41] train: loss: 0.0036596
[Epoch 178; Iter    33/   41] train: loss: 0.0164634
[Epoch 178] ogbg-molbace: 0.779853 val loss: 1.558263
[Epoch 178] ogbg-molbace: 0.749435 test loss: 2.421557
[Epoch 179; Iter    22/   41] train: loss: 0.0087189
[Epoch 179] ogbg-molbace: 0.781319 val loss: 1.477725
[Epoch 179] ogbg-molbace: 0.750130 test loss: 2.314935
[Epoch 180; Iter    11/   41] train: loss: 0.0012987
[Epoch 180; Iter    41/   41] train: loss: 0.0023609
[Epoch 180] ogbg-molbace: 0.780220 val loss: 1.552693
[Epoch 180] ogbg-molbace: 0.748913 test loss: 2.332007
[Epoch 181; Iter    30/   41] train: loss: 0.0007524
[Epoch 181] ogbg-molbace: 0.776557 val loss: 1.559237
[Epoch 181] ogbg-molbace: 0.746479 test loss: 2.337135
[Epoch 182; Iter    19/   41] train: loss: 0.0069886
[Epoch 182] ogbg-molbace: 0.771429 val loss: 1.614786
[Epoch 182] ogbg-molbace: 0.755695 test loss: 2.310204
[Epoch 183; Iter     8/   41] train: loss: 0.0205053
[Epoch 183; Iter    38/   41] train: loss: 0.0008979
[Epoch 183] ogbg-molbace: 0.770330 val loss: 1.583422
[Epoch 183] ogbg-molbace: 0.756216 test loss: 2.304506
[Epoch 184; Iter    27/   41] train: loss: 0.0240932
[Epoch 184] ogbg-molbace: 0.777289 val loss: 1.607149
[Epoch 184] ogbg-molbace: 0.749087 test loss: 2.441100
Early stopping criterion based on -ogbg-molbace- that should be max reached after 184 epochs. Best model checkpoint was in epoch 124.
Statistics on  val_best_checkpoint
mean_pred: 0.4920310080051422
std_pred: 4.608753204345703
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9651341393707301
rocauc: 0.8065934065934066
ogbg-molbace: 0.8065934065934066
BCEWithLogitsLoss: 1.1871534585952759
Statistics on  test
mean_pred: -1.818593144416809
std_pred: 6.644918918609619
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7289981617319305
rocauc: 0.7475221700573813
ogbg-molbace: 0.7475221700573813
BCEWithLogitsLoss: 2.098765100042025
Statistics on  train
mean_pred: -1.7033231258392334
std_pred: 6.627303600311279
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9997922965155868
rocauc: 0.9998544520547945
ogbg-molbace: 0.9998544520547945
BCEWithLogitsLoss: 0.0205017720042478
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --seed 6 --device cuda:0
All runs completed.
