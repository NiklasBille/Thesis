>>> Starting run for dataset: sider
Running configs_static_noise_experiments/GraphCL/sider/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/GraphCL/sider/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/GraphCL/sider/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/GraphCL/sider/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/sider/noise=0.05/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.05_6_26-05_10-50-07
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.05
logdir: runs/static_noise/GraphCL/sider/noise=0.05
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6928000
[Epoch 1] ogbg-molsider: 0.474686 val loss: 0.692767
[Epoch 1] ogbg-molsider: 0.484440 test loss: 0.692262
[Epoch 2; Iter    24/   36] train: loss: 0.6937519
[Epoch 2] ogbg-molsider: 0.477310 val loss: 0.692478
[Epoch 2] ogbg-molsider: 0.490417 test loss: 0.691809
[Epoch 3; Iter    18/   36] train: loss: 0.6935679
[Epoch 3] ogbg-molsider: 0.483635 val loss: 0.692202
[Epoch 3] ogbg-molsider: 0.500160 test loss: 0.691453
[Epoch 4; Iter    12/   36] train: loss: 0.6933407
[Epoch 4] ogbg-molsider: 0.485505 val loss: 0.692100
[Epoch 4] ogbg-molsider: 0.507762 test loss: 0.691348
[Epoch 5; Iter     6/   36] train: loss: 0.6929553
[Epoch 5; Iter    36/   36] train: loss: 0.6923963
[Epoch 5] ogbg-molsider: 0.482627 val loss: 0.691948
[Epoch 5] ogbg-molsider: 0.503626 test loss: 0.691092
[Epoch 6; Iter    30/   36] train: loss: 0.6931731
[Epoch 6] ogbg-molsider: 0.484238 val loss: 0.691888
[Epoch 6] ogbg-molsider: 0.502921 test loss: 0.691070
[Epoch 7; Iter    24/   36] train: loss: 0.6927304
[Epoch 7] ogbg-molsider: 0.484979 val loss: 0.692046
[Epoch 7] ogbg-molsider: 0.505196 test loss: 0.691388
[Epoch 8; Iter    18/   36] train: loss: 0.6928567
[Epoch 8] ogbg-molsider: 0.480161 val loss: 0.691508
[Epoch 8] ogbg-molsider: 0.503868 test loss: 0.690589
[Epoch 9; Iter    12/   36] train: loss: 0.6931652
[Epoch 9] ogbg-molsider: 0.483202 val loss: 0.691579
[Epoch 9] ogbg-molsider: 0.504241 test loss: 0.690723
[Epoch 10; Iter     6/   36] train: loss: 0.6928152
[Epoch 10; Iter    36/   36] train: loss: 0.6935404
[Epoch 10] ogbg-molsider: 0.481597 val loss: 0.691597
[Epoch 10] ogbg-molsider: 0.505143 test loss: 0.690836
[Epoch 11; Iter    30/   36] train: loss: 0.6925538
[Epoch 11] ogbg-molsider: 0.484415 val loss: 0.691391
[Epoch 11] ogbg-molsider: 0.506540 test loss: 0.690591
[Epoch 12; Iter    24/   36] train: loss: 0.6924664
[Epoch 12] ogbg-molsider: 0.484684 val loss: 0.691289
[Epoch 12] ogbg-molsider: 0.507985 test loss: 0.690514
[Epoch 13; Iter    18/   36] train: loss: 0.6921154
[Epoch 13] ogbg-molsider: 0.485851 val loss: 0.691202
[Epoch 13] ogbg-molsider: 0.505565 test loss: 0.690435
[Epoch 14; Iter    12/   36] train: loss: 0.6922092
[Epoch 14] ogbg-molsider: 0.484894 val loss: 0.690848
[Epoch 14] ogbg-molsider: 0.505392 test loss: 0.690006
[Epoch 15; Iter     6/   36] train: loss: 0.6913907
[Epoch 15; Iter    36/   36] train: loss: 0.6916249
[Epoch 15] ogbg-molsider: 0.485272 val loss: 0.690847
[Epoch 15] ogbg-molsider: 0.504944 test loss: 0.690016
[Epoch 16; Iter    30/   36] train: loss: 0.6924663
[Epoch 16] ogbg-molsider: 0.483311 val loss: 0.690687
[Epoch 16] ogbg-molsider: 0.508213 test loss: 0.689947
[Epoch 17; Iter    24/   36] train: loss: 0.6915482
[Epoch 17] ogbg-molsider: 0.487115 val loss: 0.690446
[Epoch 17] ogbg-molsider: 0.507014 test loss: 0.689701
[Epoch 18; Iter    18/   36] train: loss: 0.6917786
[Epoch 18] ogbg-molsider: 0.484025 val loss: 0.690174
[Epoch 18] ogbg-molsider: 0.507343 test loss: 0.689355
[Epoch 19; Iter    12/   36] train: loss: 0.6910805
[Epoch 19] ogbg-molsider: 0.487369 val loss: 0.689943
[Epoch 19] ogbg-molsider: 0.507842 test loss: 0.689093
[Epoch 20; Iter     6/   36] train: loss: 0.6904938
[Epoch 20; Iter    36/   36] train: loss: 0.6872056
[Epoch 20] ogbg-molsider: 0.521089 val loss: 0.682410
[Epoch 20] ogbg-molsider: 0.548105 test loss: 0.681741
[Epoch 21; Iter    30/   36] train: loss: 0.6756861
[Epoch 21] ogbg-molsider: 0.520415 val loss: 0.659377
[Epoch 21] ogbg-molsider: 0.557311 test loss: 0.659362
[Epoch 22; Iter    24/   36] train: loss: 0.6625661
[Epoch 22] ogbg-molsider: 0.521541 val loss: 0.635517
[Epoch 22] ogbg-molsider: 0.570563 test loss: 0.635186
[Epoch 23; Iter    18/   36] train: loss: 0.6315784
[Epoch 23] ogbg-molsider: 0.522252 val loss: 0.615061
[Epoch 23] ogbg-molsider: 0.569264 test loss: 0.613176
[Epoch 24; Iter    12/   36] train: loss: 0.6166221
[Epoch 24] ogbg-molsider: 0.538413 val loss: 0.558103
[Epoch 24] ogbg-molsider: 0.580243 test loss: 0.555149
[Epoch 25; Iter     6/   36] train: loss: 0.5801529
[Epoch 25; Iter    36/   36] train: loss: 0.5867494
[Epoch 25] ogbg-molsider: 0.550578 val loss: 0.535591
[Epoch 25] ogbg-molsider: 0.580172 test loss: 0.531050
[Epoch 26; Iter    30/   36] train: loss: 0.5679240
[Epoch 26] ogbg-molsider: 0.527054 val loss: 0.555598
[Epoch 26] ogbg-molsider: 0.583041 test loss: 0.551084
[Epoch 27; Iter    24/   36] train: loss: 0.5267351
[Epoch 27] ogbg-molsider: 0.555239 val loss: 0.507898
[Epoch 27] ogbg-molsider: 0.585538 test loss: 0.514922
[Epoch 28; Iter    18/   36] train: loss: 0.4764363
[Epoch 28] ogbg-molsider: 0.559446 val loss: 0.482894
[Epoch 28] ogbg-molsider: 0.590290 test loss: 0.492477
[Epoch 29; Iter    12/   36] train: loss: 0.5614548
[Epoch 29] ogbg-molsider: 0.552901 val loss: 0.498709
[Epoch 29] ogbg-molsider: 0.583797 test loss: 0.498365
[Epoch 30; Iter     6/   36] train: loss: 0.5118034
[Epoch 30; Iter    36/   36] train: loss: 0.4858335
[Epoch 30] ogbg-molsider: 0.561528 val loss: 0.489959
[Epoch 30] ogbg-molsider: 0.585669 test loss: 0.496763
[Epoch 31; Iter    30/   36] train: loss: 0.5071134
[Epoch 31] ogbg-molsider: 0.567782 val loss: 0.480934
[Epoch 31] ogbg-molsider: 0.598851 test loss: 0.484807
[Epoch 32; Iter    24/   36] train: loss: 0.4936076
[Epoch 32] ogbg-molsider: 0.576214 val loss: 0.485809
[Epoch 32] ogbg-molsider: 0.595397 test loss: 0.490509
[Epoch 33; Iter    18/   36] train: loss: 0.5274612
[Epoch 33] ogbg-molsider: 0.571267 val loss: 0.482366
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/sider/noise=0.1/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.1_5_26-05_10-50-11
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.1
logdir: runs/static_noise/GraphCL/sider/noise=0.1
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6928868
[Epoch 1] ogbg-molsider: 0.482463 val loss: 0.693056
[Epoch 1] ogbg-molsider: 0.530733 test loss: 0.692667
[Epoch 2; Iter    24/   36] train: loss: 0.6928979
[Epoch 2] ogbg-molsider: 0.487326 val loss: 0.692601
[Epoch 2] ogbg-molsider: 0.524961 test loss: 0.691770
[Epoch 3; Iter    18/   36] train: loss: 0.6929217
[Epoch 3] ogbg-molsider: 0.499840 val loss: 0.692305
[Epoch 3] ogbg-molsider: 0.526212 test loss: 0.691123
[Epoch 4; Iter    12/   36] train: loss: 0.6935714
[Epoch 4] ogbg-molsider: 0.501627 val loss: 0.692061
[Epoch 4] ogbg-molsider: 0.529036 test loss: 0.691068
[Epoch 5; Iter     6/   36] train: loss: 0.6927169
[Epoch 5; Iter    36/   36] train: loss: 0.6927363
[Epoch 5] ogbg-molsider: 0.497570 val loss: 0.691899
[Epoch 5] ogbg-molsider: 0.525081 test loss: 0.690752
[Epoch 6; Iter    30/   36] train: loss: 0.6928137
[Epoch 6] ogbg-molsider: 0.502869 val loss: 0.691869
[Epoch 6] ogbg-molsider: 0.523600 test loss: 0.690842
[Epoch 7; Iter    24/   36] train: loss: 0.6932243
[Epoch 7] ogbg-molsider: 0.502003 val loss: 0.691797
[Epoch 7] ogbg-molsider: 0.526312 test loss: 0.690817
[Epoch 8; Iter    18/   36] train: loss: 0.6928803
[Epoch 8] ogbg-molsider: 0.503802 val loss: 0.691738
[Epoch 8] ogbg-molsider: 0.521870 test loss: 0.690614
[Epoch 9; Iter    12/   36] train: loss: 0.6925620
[Epoch 9] ogbg-molsider: 0.499237 val loss: 0.691500
[Epoch 9] ogbg-molsider: 0.522415 test loss: 0.690418
[Epoch 10; Iter     6/   36] train: loss: 0.6931134
[Epoch 10; Iter    36/   36] train: loss: 0.6926913
[Epoch 10] ogbg-molsider: 0.499458 val loss: 0.691549
[Epoch 10] ogbg-molsider: 0.522766 test loss: 0.690455
[Epoch 11; Iter    30/   36] train: loss: 0.6926221
[Epoch 11] ogbg-molsider: 0.502908 val loss: 0.691597
[Epoch 11] ogbg-molsider: 0.523490 test loss: 0.690523
[Epoch 12; Iter    24/   36] train: loss: 0.6923489
[Epoch 12] ogbg-molsider: 0.501657 val loss: 0.691228
[Epoch 12] ogbg-molsider: 0.520989 test loss: 0.690198
[Epoch 13; Iter    18/   36] train: loss: 0.6921337
[Epoch 13] ogbg-molsider: 0.505145 val loss: 0.691114
[Epoch 13] ogbg-molsider: 0.526322 test loss: 0.690204
[Epoch 14; Iter    12/   36] train: loss: 0.6926554
[Epoch 14] ogbg-molsider: 0.504961 val loss: 0.690997
[Epoch 14] ogbg-molsider: 0.525755 test loss: 0.690069
[Epoch 15; Iter     6/   36] train: loss: 0.6921147
[Epoch 15; Iter    36/   36] train: loss: 0.6921432
[Epoch 15] ogbg-molsider: 0.504315 val loss: 0.690750
[Epoch 15] ogbg-molsider: 0.523692 test loss: 0.689823
[Epoch 16; Iter    30/   36] train: loss: 0.6921554
[Epoch 16] ogbg-molsider: 0.498601 val loss: 0.690415
[Epoch 16] ogbg-molsider: 0.525149 test loss: 0.689517
[Epoch 17; Iter    24/   36] train: loss: 0.6915076
[Epoch 17] ogbg-molsider: 0.499468 val loss: 0.690445
[Epoch 17] ogbg-molsider: 0.527171 test loss: 0.689618
[Epoch 18; Iter    18/   36] train: loss: 0.6916505
[Epoch 18] ogbg-molsider: 0.499563 val loss: 0.690155
[Epoch 18] ogbg-molsider: 0.527850 test loss: 0.689278
[Epoch 19; Iter    12/   36] train: loss: 0.6915512
[Epoch 19] ogbg-molsider: 0.504464 val loss: 0.689869
[Epoch 19] ogbg-molsider: 0.525534 test loss: 0.688912
[Epoch 20; Iter     6/   36] train: loss: 0.6903263
[Epoch 20; Iter    36/   36] train: loss: 0.6857492
[Epoch 20] ogbg-molsider: 0.533035 val loss: 0.685531
[Epoch 20] ogbg-molsider: 0.541696 test loss: 0.685503
[Epoch 21; Iter    30/   36] train: loss: 0.6788773
[Epoch 21] ogbg-molsider: 0.530027 val loss: 0.665539
[Epoch 21] ogbg-molsider: 0.575504 test loss: 0.664936
[Epoch 22; Iter    24/   36] train: loss: 0.6602569
[Epoch 22] ogbg-molsider: 0.529742 val loss: 0.662088
[Epoch 22] ogbg-molsider: 0.558916 test loss: 0.659930
[Epoch 23; Iter    18/   36] train: loss: 0.6348847
[Epoch 23] ogbg-molsider: 0.539885 val loss: 0.653259
[Epoch 23] ogbg-molsider: 0.551818 test loss: 0.674974
[Epoch 24; Iter    12/   36] train: loss: 0.6293538
[Epoch 24] ogbg-molsider: 0.525955 val loss: 0.638829
[Epoch 24] ogbg-molsider: 0.542284 test loss: 0.646060
[Epoch 25; Iter     6/   36] train: loss: 0.5814133
[Epoch 25; Iter    36/   36] train: loss: 0.5523344
[Epoch 25] ogbg-molsider: 0.513136 val loss: 0.570381
[Epoch 25] ogbg-molsider: 0.557178 test loss: 0.569409
[Epoch 26; Iter    30/   36] train: loss: 0.5700640
[Epoch 26] ogbg-molsider: 0.532853 val loss: 0.524640
[Epoch 26] ogbg-molsider: 0.573376 test loss: 0.526665
[Epoch 27; Iter    24/   36] train: loss: 0.5409249
[Epoch 27] ogbg-molsider: 0.528530 val loss: 0.510277
[Epoch 27] ogbg-molsider: 0.569354 test loss: 0.510427
[Epoch 28; Iter    18/   36] train: loss: 0.5387068
[Epoch 28] ogbg-molsider: 0.545716 val loss: 0.504704
[Epoch 28] ogbg-molsider: 0.572674 test loss: 0.506226
[Epoch 29; Iter    12/   36] train: loss: 0.5081249
[Epoch 29] ogbg-molsider: 0.562256 val loss: 0.491152
[Epoch 29] ogbg-molsider: 0.574776 test loss: 0.496318
[Epoch 30; Iter     6/   36] train: loss: 0.5155643
[Epoch 30; Iter    36/   36] train: loss: 0.4595831
[Epoch 30] ogbg-molsider: 0.528999 val loss: 0.505308
[Epoch 30] ogbg-molsider: 0.558522 test loss: 0.507337
[Epoch 31; Iter    30/   36] train: loss: 0.5210049
[Epoch 31] ogbg-molsider: 0.542474 val loss: 0.515639
[Epoch 31] ogbg-molsider: 0.567459 test loss: 0.512944
[Epoch 32; Iter    24/   36] train: loss: 0.4868469
[Epoch 32] ogbg-molsider: 0.548369 val loss: 0.494925
[Epoch 32] ogbg-molsider: 0.582587 test loss: 0.501413
[Epoch 33; Iter    18/   36] train: loss: 0.5001968
[Epoch 33] ogbg-molsider: 0.553682 val loss: 0.486503
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/sider/noise=0.05/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.05_4_26-05_10-50-07
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.05
logdir: runs/static_noise/GraphCL/sider/noise=0.05
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932566
[Epoch 1] ogbg-molsider: 0.526489 val loss: 0.693140
[Epoch 1] ogbg-molsider: 0.485793 test loss: 0.693386
[Epoch 2; Iter    24/   36] train: loss: 0.6932172
[Epoch 2] ogbg-molsider: 0.505648 val loss: 0.692815
[Epoch 2] ogbg-molsider: 0.492170 test loss: 0.693316
[Epoch 3; Iter    18/   36] train: loss: 0.6928416
[Epoch 3] ogbg-molsider: 0.507441 val loss: 0.692676
[Epoch 3] ogbg-molsider: 0.495118 test loss: 0.693218
[Epoch 4; Iter    12/   36] train: loss: 0.6935157
[Epoch 4] ogbg-molsider: 0.503149 val loss: 0.692778
[Epoch 4] ogbg-molsider: 0.495262 test loss: 0.693507
[Epoch 5; Iter     6/   36] train: loss: 0.6931186
[Epoch 5; Iter    36/   36] train: loss: 0.6935863
[Epoch 5] ogbg-molsider: 0.506808 val loss: 0.692606
[Epoch 5] ogbg-molsider: 0.494360 test loss: 0.693281
[Epoch 6; Iter    30/   36] train: loss: 0.6936179
[Epoch 6] ogbg-molsider: 0.505235 val loss: 0.692456
[Epoch 6] ogbg-molsider: 0.496298 test loss: 0.692960
[Epoch 7; Iter    24/   36] train: loss: 0.6930497
[Epoch 7] ogbg-molsider: 0.504699 val loss: 0.692390
[Epoch 7] ogbg-molsider: 0.495339 test loss: 0.693011
[Epoch 8; Iter    18/   36] train: loss: 0.6929032
[Epoch 8] ogbg-molsider: 0.506675 val loss: 0.692407
[Epoch 8] ogbg-molsider: 0.495247 test loss: 0.693025
[Epoch 9; Iter    12/   36] train: loss: 0.6935679
[Epoch 9] ogbg-molsider: 0.507009 val loss: 0.692220
[Epoch 9] ogbg-molsider: 0.496619 test loss: 0.692838
[Epoch 10; Iter     6/   36] train: loss: 0.6930581
[Epoch 10; Iter    36/   36] train: loss: 0.6930303
[Epoch 10] ogbg-molsider: 0.505500 val loss: 0.692225
[Epoch 10] ogbg-molsider: 0.495547 test loss: 0.692878
[Epoch 11; Iter    30/   36] train: loss: 0.6927571
[Epoch 11] ogbg-molsider: 0.507420 val loss: 0.691923
[Epoch 11] ogbg-molsider: 0.495921 test loss: 0.692548
[Epoch 12; Iter    24/   36] train: loss: 0.6924160
[Epoch 12] ogbg-molsider: 0.506379 val loss: 0.691832
[Epoch 12] ogbg-molsider: 0.494576 test loss: 0.692456
[Epoch 13; Iter    18/   36] train: loss: 0.6921973
[Epoch 13] ogbg-molsider: 0.507246 val loss: 0.691701
[Epoch 13] ogbg-molsider: 0.495800 test loss: 0.692294
[Epoch 14; Iter    12/   36] train: loss: 0.6921043
[Epoch 14] ogbg-molsider: 0.506145 val loss: 0.691480
[Epoch 14] ogbg-molsider: 0.495550 test loss: 0.692051
[Epoch 15; Iter     6/   36] train: loss: 0.6924990
[Epoch 15; Iter    36/   36] train: loss: 0.6918641
[Epoch 15] ogbg-molsider: 0.509748 val loss: 0.691307
[Epoch 15] ogbg-molsider: 0.495246 test loss: 0.691897
[Epoch 16; Iter    30/   36] train: loss: 0.6916683
[Epoch 16] ogbg-molsider: 0.505862 val loss: 0.691088
[Epoch 16] ogbg-molsider: 0.492870 test loss: 0.691769
[Epoch 17; Iter    24/   36] train: loss: 0.6917460
[Epoch 17] ogbg-molsider: 0.509855 val loss: 0.690823
[Epoch 17] ogbg-molsider: 0.495207 test loss: 0.691440
[Epoch 18; Iter    18/   36] train: loss: 0.6914758
[Epoch 18] ogbg-molsider: 0.509989 val loss: 0.690593
[Epoch 18] ogbg-molsider: 0.495416 test loss: 0.691201
[Epoch 19; Iter    12/   36] train: loss: 0.6912255
[Epoch 19] ogbg-molsider: 0.507735 val loss: 0.690296
[Epoch 19] ogbg-molsider: 0.494792 test loss: 0.690941
[Epoch 20; Iter     6/   36] train: loss: 0.6911961
[Epoch 20; Iter    36/   36] train: loss: 0.6862366
[Epoch 20] ogbg-molsider: 0.525500 val loss: 0.679762
[Epoch 20] ogbg-molsider: 0.574224 test loss: 0.679087
[Epoch 21; Iter    30/   36] train: loss: 0.6737435
[Epoch 21] ogbg-molsider: 0.524335 val loss: 0.671287
[Epoch 21] ogbg-molsider: 0.564249 test loss: 0.669127
[Epoch 22; Iter    24/   36] train: loss: 0.6580437
[Epoch 22] ogbg-molsider: 0.520099 val loss: 0.640649
[Epoch 22] ogbg-molsider: 0.559550 test loss: 0.641520
[Epoch 23; Iter    18/   36] train: loss: 0.6311350
[Epoch 23] ogbg-molsider: 0.520854 val loss: 0.617407
[Epoch 23] ogbg-molsider: 0.568852 test loss: 0.617676
[Epoch 24; Iter    12/   36] train: loss: 0.6154665
[Epoch 24] ogbg-molsider: 0.524707 val loss: 0.644780
[Epoch 24] ogbg-molsider: 0.557860 test loss: 0.656727
[Epoch 25; Iter     6/   36] train: loss: 0.5864964
[Epoch 25; Iter    36/   36] train: loss: 0.5700610
[Epoch 25] ogbg-molsider: 0.533153 val loss: 0.549858
[Epoch 25] ogbg-molsider: 0.593534 test loss: 0.549902
[Epoch 26; Iter    30/   36] train: loss: 0.5393324
[Epoch 26] ogbg-molsider: 0.516187 val loss: 0.522479
[Epoch 26] ogbg-molsider: 0.571135 test loss: 0.522349
[Epoch 27; Iter    24/   36] train: loss: 0.5271764
[Epoch 27] ogbg-molsider: 0.521511 val loss: 0.508539
[Epoch 27] ogbg-molsider: 0.579651 test loss: 0.510881
[Epoch 28; Iter    18/   36] train: loss: 0.5039065
[Epoch 28] ogbg-molsider: 0.529083 val loss: 0.499683
[Epoch 28] ogbg-molsider: 0.584968 test loss: 0.499976
[Epoch 29; Iter    12/   36] train: loss: 0.5146413
[Epoch 29] ogbg-molsider: 0.537957 val loss: 0.497786
[Epoch 29] ogbg-molsider: 0.570593 test loss: 0.502495
[Epoch 30; Iter     6/   36] train: loss: 0.5216893
[Epoch 30; Iter    36/   36] train: loss: 0.5167825
[Epoch 30] ogbg-molsider: 0.550996 val loss: 0.485990
[Epoch 30] ogbg-molsider: 0.603229 test loss: 0.487396
[Epoch 31; Iter    30/   36] train: loss: 0.4633508
[Epoch 31] ogbg-molsider: 0.548498 val loss: 0.488026
[Epoch 31] ogbg-molsider: 0.604365 test loss: 0.492521
[Epoch 32; Iter    24/   36] train: loss: 0.5502990
[Epoch 32] ogbg-molsider: 0.523813 val loss: 0.497750
[Epoch 32] ogbg-molsider: 0.587939 test loss: 0.499355
[Epoch 33; Iter    18/   36] train: loss: 0.4876812
[Epoch 33] ogbg-molsider: 0.544786 val loss: 0.491020
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/sider/noise=0.1/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.1_4_26-05_10-50-12
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.1
logdir: runs/static_noise/GraphCL/sider/noise=0.1
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6931806
[Epoch 1] ogbg-molsider: 0.526017 val loss: 0.693094
[Epoch 1] ogbg-molsider: 0.483864 test loss: 0.693335
[Epoch 2; Iter    24/   36] train: loss: 0.6930164
[Epoch 2] ogbg-molsider: 0.508932 val loss: 0.692627
[Epoch 2] ogbg-molsider: 0.489755 test loss: 0.693133
[Epoch 3; Iter    18/   36] train: loss: 0.6927903
[Epoch 3] ogbg-molsider: 0.512785 val loss: 0.692440
[Epoch 3] ogbg-molsider: 0.493772 test loss: 0.692971
[Epoch 4; Iter    12/   36] train: loss: 0.6935170
[Epoch 4] ogbg-molsider: 0.505851 val loss: 0.692571
[Epoch 4] ogbg-molsider: 0.491718 test loss: 0.693313
[Epoch 5; Iter     6/   36] train: loss: 0.6931267
[Epoch 5; Iter    36/   36] train: loss: 0.6934208
[Epoch 5] ogbg-molsider: 0.508463 val loss: 0.692450
[Epoch 5] ogbg-molsider: 0.491109 test loss: 0.693143
[Epoch 6; Iter    30/   36] train: loss: 0.6934556
[Epoch 6] ogbg-molsider: 0.507923 val loss: 0.692285
[Epoch 6] ogbg-molsider: 0.494645 test loss: 0.692788
[Epoch 7; Iter    24/   36] train: loss: 0.6931667
[Epoch 7] ogbg-molsider: 0.509009 val loss: 0.692260
[Epoch 7] ogbg-molsider: 0.492794 test loss: 0.692885
[Epoch 8; Iter    18/   36] train: loss: 0.6928092
[Epoch 8] ogbg-molsider: 0.509596 val loss: 0.692186
[Epoch 8] ogbg-molsider: 0.491674 test loss: 0.692822
[Epoch 9; Iter    12/   36] train: loss: 0.6936143
[Epoch 9] ogbg-molsider: 0.509490 val loss: 0.692020
[Epoch 9] ogbg-molsider: 0.491164 test loss: 0.692673
[Epoch 10; Iter     6/   36] train: loss: 0.6929184
[Epoch 10; Iter    36/   36] train: loss: 0.6929538
[Epoch 10] ogbg-molsider: 0.507133 val loss: 0.691983
[Epoch 10] ogbg-molsider: 0.493074 test loss: 0.692647
[Epoch 11; Iter    30/   36] train: loss: 0.6927106
[Epoch 11] ogbg-molsider: 0.510979 val loss: 0.691780
[Epoch 11] ogbg-molsider: 0.492049 test loss: 0.692401
[Epoch 12; Iter    24/   36] train: loss: 0.6924139
[Epoch 12] ogbg-molsider: 0.510790 val loss: 0.691672
[Epoch 12] ogbg-molsider: 0.491203 test loss: 0.692341
[Epoch 13; Iter    18/   36] train: loss: 0.6920585
[Epoch 13] ogbg-molsider: 0.510726 val loss: 0.691462
[Epoch 13] ogbg-molsider: 0.493624 test loss: 0.692034
[Epoch 14; Iter    12/   36] train: loss: 0.6918113
[Epoch 14] ogbg-molsider: 0.511480 val loss: 0.691303
[Epoch 14] ogbg-molsider: 0.493029 test loss: 0.691862
[Epoch 15; Iter     6/   36] train: loss: 0.6925529
[Epoch 15; Iter    36/   36] train: loss: 0.6917385
[Epoch 15] ogbg-molsider: 0.512710 val loss: 0.691159
[Epoch 15] ogbg-molsider: 0.490372 test loss: 0.691773
[Epoch 16; Iter    30/   36] train: loss: 0.6917695
[Epoch 16] ogbg-molsider: 0.511423 val loss: 0.690929
[Epoch 16] ogbg-molsider: 0.491220 test loss: 0.691659
[Epoch 17; Iter    24/   36] train: loss: 0.6919376
[Epoch 17] ogbg-molsider: 0.514264 val loss: 0.690682
[Epoch 17] ogbg-molsider: 0.491646 test loss: 0.691293
[Epoch 18; Iter    18/   36] train: loss: 0.6915001
[Epoch 18] ogbg-molsider: 0.513885 val loss: 0.690400
[Epoch 18] ogbg-molsider: 0.494283 test loss: 0.691004
[Epoch 19; Iter    12/   36] train: loss: 0.6914209
[Epoch 19] ogbg-molsider: 0.514164 val loss: 0.690143
[Epoch 19] ogbg-molsider: 0.492995 test loss: 0.690804
[Epoch 20; Iter     6/   36] train: loss: 0.6911079
[Epoch 20; Iter    36/   36] train: loss: 0.6869748
[Epoch 20] ogbg-molsider: 0.524485 val loss: 0.683587
[Epoch 20] ogbg-molsider: 0.561256 test loss: 0.684607
[Epoch 21; Iter    30/   36] train: loss: 0.6743712
[Epoch 21] ogbg-molsider: 0.520630 val loss: 0.682894
[Epoch 21] ogbg-molsider: 0.543817 test loss: 0.680800
[Epoch 22; Iter    24/   36] train: loss: 0.6587517
[Epoch 22] ogbg-molsider: 0.514263 val loss: 0.670757
[Epoch 22] ogbg-molsider: 0.537029 test loss: 0.668383
[Epoch 23; Iter    18/   36] train: loss: 0.6339196
[Epoch 23] ogbg-molsider: 0.521212 val loss: 0.742700
[Epoch 23] ogbg-molsider: 0.535352 test loss: 0.761828
[Epoch 24; Iter    12/   36] train: loss: 0.6169735
[Epoch 24] ogbg-molsider: 0.533386 val loss: 0.683964
[Epoch 24] ogbg-molsider: 0.553957 test loss: 0.667952
[Epoch 25; Iter     6/   36] train: loss: 0.5913910
[Epoch 25; Iter    36/   36] train: loss: 0.5732236
[Epoch 25] ogbg-molsider: 0.563812 val loss: 0.601293
[Epoch 25] ogbg-molsider: 0.543042 test loss: 0.571255
[Epoch 26; Iter    30/   36] train: loss: 0.5429223
[Epoch 26] ogbg-molsider: 0.534901 val loss: 0.621911
[Epoch 26] ogbg-molsider: 0.549854 test loss: 0.589481
[Epoch 27; Iter    24/   36] train: loss: 0.5316013
[Epoch 27] ogbg-molsider: 0.560686 val loss: 0.613181
[Epoch 27] ogbg-molsider: 0.552912 test loss: 0.580892
[Epoch 28; Iter    18/   36] train: loss: 0.5042811
[Epoch 28] ogbg-molsider: 0.556914 val loss: 0.650243
[Epoch 28] ogbg-molsider: 0.551627 test loss: 0.629546
[Epoch 29; Iter    12/   36] train: loss: 0.5235560
[Epoch 29] ogbg-molsider: 0.564603 val loss: 0.659164
[Epoch 29] ogbg-molsider: 0.562210 test loss: 0.622573
[Epoch 30; Iter     6/   36] train: loss: 0.5227501
[Epoch 30; Iter    36/   36] train: loss: 0.5194553
[Epoch 30] ogbg-molsider: 0.558611 val loss: 0.664756
[Epoch 30] ogbg-molsider: 0.566559 test loss: 0.625803
[Epoch 31; Iter    30/   36] train: loss: 0.4646615
[Epoch 31] ogbg-molsider: 0.555222 val loss: 0.645200
[Epoch 31] ogbg-molsider: 0.565558 test loss: 0.593830
[Epoch 32; Iter    24/   36] train: loss: 0.5518011
[Epoch 32] ogbg-molsider: 0.526948 val loss: 0.651933
[Epoch 32] ogbg-molsider: 0.561251 test loss: 0.596340
[Epoch 33; Iter    18/   36] train: loss: 0.4919127
[Epoch 33] ogbg-molsider: 0.562717 val loss: 0.667347
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/sider/noise=0.1/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.1_6_26-05_10-50-14
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.1
logdir: runs/static_noise/GraphCL/sider/noise=0.1
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6929265
[Epoch 1] ogbg-molsider: 0.472571 val loss: 0.692594
[Epoch 1] ogbg-molsider: 0.487799 test loss: 0.692013
[Epoch 2; Iter    24/   36] train: loss: 0.6938208
[Epoch 2] ogbg-molsider: 0.478840 val loss: 0.692014
[Epoch 2] ogbg-molsider: 0.495819 test loss: 0.691136
[Epoch 3; Iter    18/   36] train: loss: 0.6935161
[Epoch 3] ogbg-molsider: 0.489462 val loss: 0.691919
[Epoch 3] ogbg-molsider: 0.503715 test loss: 0.690976
[Epoch 4; Iter    12/   36] train: loss: 0.6931599
[Epoch 4] ogbg-molsider: 0.488973 val loss: 0.691955
[Epoch 4] ogbg-molsider: 0.504789 test loss: 0.691012
[Epoch 5; Iter     6/   36] train: loss: 0.6930652
[Epoch 5; Iter    36/   36] train: loss: 0.6927198
[Epoch 5] ogbg-molsider: 0.488291 val loss: 0.691776
[Epoch 5] ogbg-molsider: 0.505669 test loss: 0.690717
[Epoch 6; Iter    30/   36] train: loss: 0.6934596
[Epoch 6] ogbg-molsider: 0.490329 val loss: 0.691601
[Epoch 6] ogbg-molsider: 0.505224 test loss: 0.690567
[Epoch 7; Iter    24/   36] train: loss: 0.6928400
[Epoch 7] ogbg-molsider: 0.489734 val loss: 0.691862
[Epoch 7] ogbg-molsider: 0.503978 test loss: 0.691053
[Epoch 8; Iter    18/   36] train: loss: 0.6929209
[Epoch 8] ogbg-molsider: 0.487954 val loss: 0.691304
[Epoch 8] ogbg-molsider: 0.506201 test loss: 0.690092
[Epoch 9; Iter    12/   36] train: loss: 0.6930819
[Epoch 9] ogbg-molsider: 0.488580 val loss: 0.691540
[Epoch 9] ogbg-molsider: 0.503163 test loss: 0.690540
[Epoch 10; Iter     6/   36] train: loss: 0.6927117
[Epoch 10; Iter    36/   36] train: loss: 0.6936629
[Epoch 10] ogbg-molsider: 0.490501 val loss: 0.691315
[Epoch 10] ogbg-molsider: 0.506664 test loss: 0.690378
[Epoch 11; Iter    30/   36] train: loss: 0.6927883
[Epoch 11] ogbg-molsider: 0.490843 val loss: 0.691242
[Epoch 11] ogbg-molsider: 0.505267 test loss: 0.690255
[Epoch 12; Iter    24/   36] train: loss: 0.6924075
[Epoch 12] ogbg-molsider: 0.490901 val loss: 0.691184
[Epoch 12] ogbg-molsider: 0.505735 test loss: 0.690311
[Epoch 13; Iter    18/   36] train: loss: 0.6921816
[Epoch 13] ogbg-molsider: 0.490767 val loss: 0.691275
[Epoch 13] ogbg-molsider: 0.502237 test loss: 0.690432
[Epoch 14; Iter    12/   36] train: loss: 0.6922325
[Epoch 14] ogbg-molsider: 0.491479 val loss: 0.690731
[Epoch 14] ogbg-molsider: 0.504923 test loss: 0.689746
[Epoch 15; Iter     6/   36] train: loss: 0.6916241
[Epoch 15; Iter    36/   36] train: loss: 0.6915129
[Epoch 15] ogbg-molsider: 0.491400 val loss: 0.690679
[Epoch 15] ogbg-molsider: 0.505867 test loss: 0.689654
[Epoch 16; Iter    30/   36] train: loss: 0.6925153
[Epoch 16] ogbg-molsider: 0.490996 val loss: 0.690638
[Epoch 16] ogbg-molsider: 0.505522 test loss: 0.689774
[Epoch 17; Iter    24/   36] train: loss: 0.6919108
[Epoch 17] ogbg-molsider: 0.492503 val loss: 0.690279
[Epoch 17] ogbg-molsider: 0.505120 test loss: 0.689361
[Epoch 18; Iter    18/   36] train: loss: 0.6915515
[Epoch 18] ogbg-molsider: 0.492214 val loss: 0.690152
[Epoch 18] ogbg-molsider: 0.505335 test loss: 0.689212
[Epoch 19; Iter    12/   36] train: loss: 0.6912426
[Epoch 19] ogbg-molsider: 0.493717 val loss: 0.689769
[Epoch 19] ogbg-molsider: 0.506270 test loss: 0.688730
[Epoch 20; Iter     6/   36] train: loss: 0.6905258
[Epoch 20; Iter    36/   36] train: loss: 0.6877103
[Epoch 20] ogbg-molsider: 0.513383 val loss: 0.684561
[Epoch 20] ogbg-molsider: 0.539654 test loss: 0.684361
[Epoch 21; Iter    30/   36] train: loss: 0.6762856
[Epoch 21] ogbg-molsider: 0.520344 val loss: 0.665333
[Epoch 21] ogbg-molsider: 0.551011 test loss: 0.666573
[Epoch 22; Iter    24/   36] train: loss: 0.6627471
[Epoch 22] ogbg-molsider: 0.530634 val loss: 0.664397
[Epoch 22] ogbg-molsider: 0.545152 test loss: 0.668190
[Epoch 23; Iter    18/   36] train: loss: 0.6297505
[Epoch 23] ogbg-molsider: 0.513662 val loss: 0.635062
[Epoch 23] ogbg-molsider: 0.558880 test loss: 0.629261
[Epoch 24; Iter    12/   36] train: loss: 0.6139173
[Epoch 24] ogbg-molsider: 0.529395 val loss: 0.581579
[Epoch 24] ogbg-molsider: 0.571266 test loss: 0.579396
[Epoch 25; Iter     6/   36] train: loss: 0.5774401
[Epoch 25; Iter    36/   36] train: loss: 0.5834499
[Epoch 25] ogbg-molsider: 0.538300 val loss: 0.564064
[Epoch 25] ogbg-molsider: 0.555967 test loss: 0.561938
[Epoch 26; Iter    30/   36] train: loss: 0.5666928
[Epoch 26] ogbg-molsider: 0.518177 val loss: 0.551816
[Epoch 26] ogbg-molsider: 0.567038 test loss: 0.546261
[Epoch 27; Iter    24/   36] train: loss: 0.5238957
[Epoch 27] ogbg-molsider: 0.531664 val loss: 0.533870
[Epoch 27] ogbg-molsider: 0.544819 test loss: 0.549841
[Epoch 28; Iter    18/   36] train: loss: 0.4912940
[Epoch 28] ogbg-molsider: 0.532314 val loss: 0.501792
[Epoch 28] ogbg-molsider: 0.573493 test loss: 0.505381
[Epoch 29; Iter    12/   36] train: loss: 0.5590622
[Epoch 29] ogbg-molsider: 0.529760 val loss: 0.502566
[Epoch 29] ogbg-molsider: 0.584174 test loss: 0.499196
[Epoch 30; Iter     6/   36] train: loss: 0.5211662
[Epoch 30; Iter    36/   36] train: loss: 0.4874019
[Epoch 30] ogbg-molsider: 0.528689 val loss: 0.505043
[Epoch 30] ogbg-molsider: 0.567884 test loss: 0.510951
[Epoch 31; Iter    30/   36] train: loss: 0.5123202
[Epoch 31] ogbg-molsider: 0.521915 val loss: 0.491229
[Epoch 31] ogbg-molsider: 0.568456 test loss: 0.498577
[Epoch 32; Iter    24/   36] train: loss: 0.5057571
[Epoch 32] ogbg-molsider: 0.535551 val loss: 0.485399
[Epoch 32] ogbg-molsider: 0.592329 test loss: 0.485779
[Epoch 33; Iter    18/   36] train: loss: 0.5289153
[Epoch 33] ogbg-molsider: 0.521424 val loss: 0.508834
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/sider/noise=0.05/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.05_5_26-05_10-50-07
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.05
logdir: runs/static_noise/GraphCL/sider/noise=0.05
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6928420
[Epoch 1] ogbg-molsider: 0.473876 val loss: 0.693051
[Epoch 1] ogbg-molsider: 0.525761 test loss: 0.692712
[Epoch 2; Iter    24/   36] train: loss: 0.6927859
[Epoch 2] ogbg-molsider: 0.481617 val loss: 0.692716
[Epoch 2] ogbg-molsider: 0.517603 test loss: 0.692050
[Epoch 3; Iter    18/   36] train: loss: 0.6929751
[Epoch 3] ogbg-molsider: 0.488293 val loss: 0.692573
[Epoch 3] ogbg-molsider: 0.515734 test loss: 0.691629
[Epoch 4; Iter    12/   36] train: loss: 0.6938624
[Epoch 4] ogbg-molsider: 0.492544 val loss: 0.692434
[Epoch 4] ogbg-molsider: 0.519084 test loss: 0.691678
[Epoch 5; Iter     6/   36] train: loss: 0.6929009
[Epoch 5; Iter    36/   36] train: loss: 0.6926812
[Epoch 5] ogbg-molsider: 0.479810 val loss: 0.692184
[Epoch 5] ogbg-molsider: 0.515883 test loss: 0.691207
[Epoch 6; Iter    30/   36] train: loss: 0.6928489
[Epoch 6] ogbg-molsider: 0.485205 val loss: 0.692315
[Epoch 6] ogbg-molsider: 0.517756 test loss: 0.691461
[Epoch 7; Iter    24/   36] train: loss: 0.6930580
[Epoch 7] ogbg-molsider: 0.487693 val loss: 0.692208
[Epoch 7] ogbg-molsider: 0.518164 test loss: 0.691407
[Epoch 8; Iter    18/   36] train: loss: 0.6929122
[Epoch 8] ogbg-molsider: 0.492129 val loss: 0.692088
[Epoch 8] ogbg-molsider: 0.513249 test loss: 0.691143
[Epoch 9; Iter    12/   36] train: loss: 0.6925013
[Epoch 9] ogbg-molsider: 0.483315 val loss: 0.691936
[Epoch 9] ogbg-molsider: 0.515057 test loss: 0.691075
[Epoch 10; Iter     6/   36] train: loss: 0.6931930
[Epoch 10; Iter    36/   36] train: loss: 0.6928304
[Epoch 10] ogbg-molsider: 0.483708 val loss: 0.691821
[Epoch 10] ogbg-molsider: 0.515676 test loss: 0.690997
[Epoch 11; Iter    30/   36] train: loss: 0.6928845
[Epoch 11] ogbg-molsider: 0.489967 val loss: 0.691942
[Epoch 11] ogbg-molsider: 0.515540 test loss: 0.691017
[Epoch 12; Iter    24/   36] train: loss: 0.6924216
[Epoch 12] ogbg-molsider: 0.484241 val loss: 0.691723
[Epoch 12] ogbg-molsider: 0.518058 test loss: 0.690822
[Epoch 13; Iter    18/   36] train: loss: 0.6922716
[Epoch 13] ogbg-molsider: 0.491305 val loss: 0.691465
[Epoch 13] ogbg-molsider: 0.515586 test loss: 0.690754
[Epoch 14; Iter    12/   36] train: loss: 0.6925360
[Epoch 14] ogbg-molsider: 0.485630 val loss: 0.691308
[Epoch 14] ogbg-molsider: 0.517242 test loss: 0.690576
[Epoch 15; Iter     6/   36] train: loss: 0.6923130
[Epoch 15; Iter    36/   36] train: loss: 0.6918468
[Epoch 15] ogbg-molsider: 0.490239 val loss: 0.691107
[Epoch 15] ogbg-molsider: 0.515771 test loss: 0.690356
[Epoch 16; Iter    30/   36] train: loss: 0.6922482
[Epoch 16] ogbg-molsider: 0.482838 val loss: 0.690756
[Epoch 16] ogbg-molsider: 0.515906 test loss: 0.690048
[Epoch 17; Iter    24/   36] train: loss: 0.6915820
[Epoch 17] ogbg-molsider: 0.483731 val loss: 0.690621
[Epoch 17] ogbg-molsider: 0.517979 test loss: 0.689929
[Epoch 18; Iter    18/   36] train: loss: 0.6918992
[Epoch 18] ogbg-molsider: 0.480681 val loss: 0.690415
[Epoch 18] ogbg-molsider: 0.517745 test loss: 0.689742
[Epoch 19; Iter    12/   36] train: loss: 0.6915982
[Epoch 19] ogbg-molsider: 0.484855 val loss: 0.690159
[Epoch 19] ogbg-molsider: 0.517879 test loss: 0.689400
[Epoch 20; Iter     6/   36] train: loss: 0.6903497
[Epoch 20; Iter    36/   36] train: loss: 0.6857069
[Epoch 20] ogbg-molsider: 0.521497 val loss: 0.686097
[Epoch 20] ogbg-molsider: 0.564068 test loss: 0.685350
[Epoch 21; Iter    30/   36] train: loss: 0.6796960
[Epoch 21] ogbg-molsider: 0.541994 val loss: 0.669742
[Epoch 21] ogbg-molsider: 0.546649 test loss: 0.669344
[Epoch 22; Iter    24/   36] train: loss: 0.6595075
[Epoch 22] ogbg-molsider: 0.514138 val loss: 0.647352
[Epoch 22] ogbg-molsider: 0.561798 test loss: 0.643101
[Epoch 23; Iter    18/   36] train: loss: 0.6342993
[Epoch 23] ogbg-molsider: 0.528896 val loss: 0.625500
[Epoch 23] ogbg-molsider: 0.586002 test loss: 0.625903
[Epoch 24; Iter    12/   36] train: loss: 0.6269735
[Epoch 24] ogbg-molsider: 0.552251 val loss: 0.583111
[Epoch 24] ogbg-molsider: 0.580071 test loss: 0.584467
[Epoch 25; Iter     6/   36] train: loss: 0.5852987
[Epoch 25; Iter    36/   36] train: loss: 0.5582234
[Epoch 25] ogbg-molsider: 0.549051 val loss: 0.602158
[Epoch 25] ogbg-molsider: 0.556289 test loss: 0.573134
[Epoch 26; Iter    30/   36] train: loss: 0.5726489
[Epoch 26] ogbg-molsider: 0.529727 val loss: 0.548339
[Epoch 26] ogbg-molsider: 0.577460 test loss: 0.555644
[Epoch 27; Iter    24/   36] train: loss: 0.5344801
[Epoch 27] ogbg-molsider: 0.523651 val loss: 0.512359
[Epoch 27] ogbg-molsider: 0.564157 test loss: 0.514774
[Epoch 28; Iter    18/   36] train: loss: 0.5365828
[Epoch 28] ogbg-molsider: 0.533402 val loss: 0.505619
[Epoch 28] ogbg-molsider: 0.586414 test loss: 0.503273
[Epoch 29; Iter    12/   36] train: loss: 0.5149897
[Epoch 29] ogbg-molsider: 0.542714 val loss: 0.496887
[Epoch 29] ogbg-molsider: 0.590730 test loss: 0.493574
[Epoch 30; Iter     6/   36] train: loss: 0.5003455
[Epoch 30; Iter    36/   36] train: loss: 0.4673506
[Epoch 30] ogbg-molsider: 0.537516 val loss: 0.489427
[Epoch 30] ogbg-molsider: 0.576783 test loss: 0.491025
[Epoch 31; Iter    30/   36] train: loss: 0.5036454
[Epoch 31] ogbg-molsider: 0.537990 val loss: 0.499432
[Epoch 31] ogbg-molsider: 0.570806 test loss: 0.501642
[Epoch 32; Iter    24/   36] train: loss: 0.4859441
[Epoch 32] ogbg-molsider: 0.532156 val loss: 0.503079
[Epoch 32] ogbg-molsider: 0.592086 test loss: 0.505915
[Epoch 33; Iter    18/   36] train: loss: 0.4875302
[Epoch 33] ogbg-molsider: 0.547917 val loss: 0.493065
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/sider/noise=0.2/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.2_5_26-05_10-50-17
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.2
logdir: runs/static_noise/GraphCL/sider/noise=0.2
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6930929
[Epoch 1] ogbg-molsider: 0.489173 val loss: 0.693081
[Epoch 1] ogbg-molsider: 0.528989 test loss: 0.692635
[Epoch 2; Iter    24/   36] train: loss: 0.6930505
[Epoch 2] ogbg-molsider: 0.497697 val loss: 0.692655
[Epoch 2] ogbg-molsider: 0.529366 test loss: 0.691688
[Epoch 3; Iter    18/   36] train: loss: 0.6927109
[Epoch 3] ogbg-molsider: 0.515598 val loss: 0.692869
[Epoch 3] ogbg-molsider: 0.516861 test loss: 0.692186
[Epoch 4; Iter    12/   36] train: loss: 0.6932384
[Epoch 4] ogbg-molsider: 0.515838 val loss: 0.692660
[Epoch 4] ogbg-molsider: 0.509157 test loss: 0.692669
[Epoch 5; Iter     6/   36] train: loss: 0.6925829
[Epoch 5; Iter    36/   36] train: loss: 0.6927764
[Epoch 5] ogbg-molsider: 0.514634 val loss: 0.692884
[Epoch 5] ogbg-molsider: 0.510331 test loss: 0.692901
[Epoch 6; Iter    30/   36] train: loss: 0.6926678
[Epoch 6] ogbg-molsider: 0.515351 val loss: 0.692681
[Epoch 6] ogbg-molsider: 0.509509 test loss: 0.692600
[Epoch 7; Iter    24/   36] train: loss: 0.6930227
[Epoch 7] ogbg-molsider: 0.513867 val loss: 0.692501
[Epoch 7] ogbg-molsider: 0.508812 test loss: 0.692496
[Epoch 8; Iter    18/   36] train: loss: 0.6926239
[Epoch 8] ogbg-molsider: 0.513380 val loss: 0.692673
[Epoch 8] ogbg-molsider: 0.510691 test loss: 0.692807
[Epoch 9; Iter    12/   36] train: loss: 0.6921502
[Epoch 9] ogbg-molsider: 0.513272 val loss: 0.692532
[Epoch 9] ogbg-molsider: 0.510226 test loss: 0.692720
[Epoch 10; Iter     6/   36] train: loss: 0.6928068
[Epoch 10; Iter    36/   36] train: loss: 0.6928055
[Epoch 10] ogbg-molsider: 0.514984 val loss: 0.691992
[Epoch 10] ogbg-molsider: 0.510283 test loss: 0.692051
[Epoch 11; Iter    30/   36] train: loss: 0.6925724
[Epoch 11] ogbg-molsider: 0.517144 val loss: 0.692277
[Epoch 11] ogbg-molsider: 0.508752 test loss: 0.692059
[Epoch 12; Iter    24/   36] train: loss: 0.6925194
[Epoch 12] ogbg-molsider: 0.516512 val loss: 0.691751
[Epoch 12] ogbg-molsider: 0.510039 test loss: 0.691584
[Epoch 13; Iter    18/   36] train: loss: 0.6919423
[Epoch 13] ogbg-molsider: 0.516597 val loss: 0.692149
[Epoch 13] ogbg-molsider: 0.509556 test loss: 0.692132
[Epoch 14; Iter    12/   36] train: loss: 0.6928044
[Epoch 14] ogbg-molsider: 0.513598 val loss: 0.691842
[Epoch 14] ogbg-molsider: 0.509732 test loss: 0.692006
[Epoch 15; Iter     6/   36] train: loss: 0.6923211
[Epoch 15; Iter    36/   36] train: loss: 0.6917506
[Epoch 15] ogbg-molsider: 0.518969 val loss: 0.691277
[Epoch 15] ogbg-molsider: 0.507173 test loss: 0.691209
[Epoch 16; Iter    30/   36] train: loss: 0.6920401
[Epoch 16] ogbg-molsider: 0.513144 val loss: 0.690968
[Epoch 16] ogbg-molsider: 0.507350 test loss: 0.691135
[Epoch 17; Iter    24/   36] train: loss: 0.6914876
[Epoch 17] ogbg-molsider: 0.508054 val loss: 0.692250
[Epoch 17] ogbg-molsider: 0.509104 test loss: 0.692852
[Epoch 18; Iter    18/   36] train: loss: 0.6916553
[Epoch 18] ogbg-molsider: 0.508228 val loss: 0.691508
[Epoch 18] ogbg-molsider: 0.508205 test loss: 0.692093
[Epoch 19; Iter    12/   36] train: loss: 0.6914786
[Epoch 19] ogbg-molsider: 0.512748 val loss: 0.691161
[Epoch 19] ogbg-molsider: 0.510868 test loss: 0.691592
[Epoch 20; Iter     6/   36] train: loss: 0.6902631
[Epoch 20; Iter    36/   36] train: loss: 0.6856293
[Epoch 20] ogbg-molsider: 0.509465 val loss: 0.694067
[Epoch 20] ogbg-molsider: 0.521904 test loss: 0.699575
[Epoch 21; Iter    30/   36] train: loss: 0.6787232
[Epoch 21] ogbg-molsider: 0.490121 val loss: 0.674984
[Epoch 21] ogbg-molsider: 0.540849 test loss: 0.668105
[Epoch 22; Iter    24/   36] train: loss: 0.6569180
[Epoch 22] ogbg-molsider: 0.513454 val loss: 0.703611
[Epoch 22] ogbg-molsider: 0.544614 test loss: 0.706290
[Epoch 23; Iter    18/   36] train: loss: 0.6363942
[Epoch 23] ogbg-molsider: 0.500385 val loss: 0.706506
[Epoch 23] ogbg-molsider: 0.546347 test loss: 0.716408
[Epoch 24; Iter    12/   36] train: loss: 0.6368665
[Epoch 24] ogbg-molsider: 0.512927 val loss: 0.658118
[Epoch 24] ogbg-molsider: 0.529736 test loss: 0.673011
[Epoch 25; Iter     6/   36] train: loss: 0.5848432
[Epoch 25; Iter    36/   36] train: loss: 0.5480765
[Epoch 25] ogbg-molsider: 0.504166 val loss: 0.604319
[Epoch 25] ogbg-molsider: 0.534326 test loss: 0.606298
[Epoch 26; Iter    30/   36] train: loss: 0.5783988
[Epoch 26] ogbg-molsider: 0.518983 val loss: 0.595187
[Epoch 26] ogbg-molsider: 0.540124 test loss: 0.607606
[Epoch 27; Iter    24/   36] train: loss: 0.5336319
[Epoch 27] ogbg-molsider: 0.488597 val loss: 0.563078
[Epoch 27] ogbg-molsider: 0.541503 test loss: 0.554618
[Epoch 28; Iter    18/   36] train: loss: 0.5357289
[Epoch 28] ogbg-molsider: 0.496610 val loss: 0.599423
[Epoch 28] ogbg-molsider: 0.522713 test loss: 0.591816
[Epoch 29; Iter    12/   36] train: loss: 0.5037912
[Epoch 29] ogbg-molsider: 0.502549 val loss: 0.591620
[Epoch 29] ogbg-molsider: 0.545230 test loss: 0.590887
[Epoch 30; Iter     6/   36] train: loss: 0.5014986
[Epoch 30; Iter    36/   36] train: loss: 0.4611262
[Epoch 30] ogbg-molsider: 0.502563 val loss: 0.597110
[Epoch 30] ogbg-molsider: 0.541376 test loss: 0.570463
[Epoch 31; Iter    30/   36] train: loss: 0.5143170
[Epoch 31] ogbg-molsider: 0.503879 val loss: 0.620266
[Epoch 31] ogbg-molsider: 0.535971 test loss: 0.603958
[Epoch 32; Iter    24/   36] train: loss: 0.4854318
[Epoch 32] ogbg-molsider: 0.512996 val loss: 0.590018
[Epoch 32] ogbg-molsider: 0.566605 test loss: 0.559011
[Epoch 33; Iter    18/   36] train: loss: 0.4979769
[Epoch 33] ogbg-molsider: 0.521317 val loss: 0.571033
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/sider/noise=0.2/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.2_4_26-05_10-50-17
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.2
logdir: runs/static_noise/GraphCL/sider/noise=0.2
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932629
[Epoch 1] ogbg-molsider: 0.525953 val loss: 0.693061
[Epoch 1] ogbg-molsider: 0.484356 test loss: 0.693302
[Epoch 2; Iter    24/   36] train: loss: 0.6930309
[Epoch 2] ogbg-molsider: 0.509512 val loss: 0.692476
[Epoch 2] ogbg-molsider: 0.488594 test loss: 0.693055
[Epoch 3; Iter    18/   36] train: loss: 0.6928338
[Epoch 3] ogbg-molsider: 0.506207 val loss: 0.692314
[Epoch 3] ogbg-molsider: 0.492205 test loss: 0.692976
[Epoch 4; Iter    12/   36] train: loss: 0.6937038
[Epoch 4] ogbg-molsider: 0.499009 val loss: 0.692485
[Epoch 4] ogbg-molsider: 0.491434 test loss: 0.693353
[Epoch 5; Iter     6/   36] train: loss: 0.6929002
[Epoch 5; Iter    36/   36] train: loss: 0.6936058
[Epoch 5] ogbg-molsider: 0.499239 val loss: 0.692457
[Epoch 5] ogbg-molsider: 0.492115 test loss: 0.693247
[Epoch 6; Iter    30/   36] train: loss: 0.6936058
[Epoch 6] ogbg-molsider: 0.499546 val loss: 0.692139
[Epoch 6] ogbg-molsider: 0.494741 test loss: 0.692678
[Epoch 7; Iter    24/   36] train: loss: 0.6930638
[Epoch 7] ogbg-molsider: 0.499689 val loss: 0.692218
[Epoch 7] ogbg-molsider: 0.490786 test loss: 0.692954
[Epoch 8; Iter    18/   36] train: loss: 0.6927834
[Epoch 8] ogbg-molsider: 0.501779 val loss: 0.692080
[Epoch 8] ogbg-molsider: 0.490075 test loss: 0.692811
[Epoch 9; Iter    12/   36] train: loss: 0.6934974
[Epoch 9] ogbg-molsider: 0.499911 val loss: 0.691887
[Epoch 9] ogbg-molsider: 0.494766 test loss: 0.692588
[Epoch 10; Iter     6/   36] train: loss: 0.6929269
[Epoch 10; Iter    36/   36] train: loss: 0.6930236
[Epoch 10] ogbg-molsider: 0.501737 val loss: 0.691885
[Epoch 10] ogbg-molsider: 0.491771 test loss: 0.692643
[Epoch 11; Iter    30/   36] train: loss: 0.6925166
[Epoch 11] ogbg-molsider: 0.502335 val loss: 0.691590
[Epoch 11] ogbg-molsider: 0.492174 test loss: 0.692265
[Epoch 12; Iter    24/   36] train: loss: 0.6923532
[Epoch 12] ogbg-molsider: 0.499716 val loss: 0.691561
[Epoch 12] ogbg-molsider: 0.490475 test loss: 0.692318
[Epoch 13; Iter    18/   36] train: loss: 0.6921173
[Epoch 13] ogbg-molsider: 0.499784 val loss: 0.691282
[Epoch 13] ogbg-molsider: 0.492208 test loss: 0.691940
[Epoch 14; Iter    12/   36] train: loss: 0.6920329
[Epoch 14] ogbg-molsider: 0.502838 val loss: 0.691211
[Epoch 14] ogbg-molsider: 0.494411 test loss: 0.691874
[Epoch 15; Iter     6/   36] train: loss: 0.6926872
[Epoch 15; Iter    36/   36] train: loss: 0.6916527
[Epoch 15] ogbg-molsider: 0.500829 val loss: 0.691065
[Epoch 15] ogbg-molsider: 0.491621 test loss: 0.691750
[Epoch 16; Iter    30/   36] train: loss: 0.6918469
[Epoch 16] ogbg-molsider: 0.501279 val loss: 0.690847
[Epoch 16] ogbg-molsider: 0.490546 test loss: 0.691633
[Epoch 17; Iter    24/   36] train: loss: 0.6918456
[Epoch 17] ogbg-molsider: 0.504184 val loss: 0.690523
[Epoch 17] ogbg-molsider: 0.491997 test loss: 0.691203
[Epoch 18; Iter    18/   36] train: loss: 0.6917106
[Epoch 18] ogbg-molsider: 0.503619 val loss: 0.690264
[Epoch 18] ogbg-molsider: 0.494135 test loss: 0.690895
[Epoch 19; Iter    12/   36] train: loss: 0.6916050
[Epoch 19] ogbg-molsider: 0.504228 val loss: 0.690105
[Epoch 19] ogbg-molsider: 0.492217 test loss: 0.690873
[Epoch 20; Iter     6/   36] train: loss: 0.6913213
[Epoch 20; Iter    36/   36] train: loss: 0.6865446
[Epoch 20] ogbg-molsider: 0.527327 val loss: 0.688573
[Epoch 20] ogbg-molsider: 0.559395 test loss: 0.690424
[Epoch 21; Iter    30/   36] train: loss: 0.6731545
[Epoch 21] ogbg-molsider: 0.510509 val loss: 0.680484
[Epoch 21] ogbg-molsider: 0.530546 test loss: 0.680598
[Epoch 22; Iter    24/   36] train: loss: 0.6597386
[Epoch 22] ogbg-molsider: 0.514908 val loss: 0.682694
[Epoch 22] ogbg-molsider: 0.533287 test loss: 0.687301
[Epoch 23; Iter    18/   36] train: loss: 0.6325610
[Epoch 23] ogbg-molsider: 0.511302 val loss: 0.682707
[Epoch 23] ogbg-molsider: 0.544442 test loss: 0.687738
[Epoch 24; Iter    12/   36] train: loss: 0.6140737
[Epoch 24] ogbg-molsider: 0.499993 val loss: 0.629275
[Epoch 24] ogbg-molsider: 0.544292 test loss: 0.619695
[Epoch 25; Iter     6/   36] train: loss: 0.5901282
[Epoch 25; Iter    36/   36] train: loss: 0.5772138
[Epoch 25] ogbg-molsider: 0.515613 val loss: 0.563791
[Epoch 25] ogbg-molsider: 0.540280 test loss: 0.555348
[Epoch 26; Iter    30/   36] train: loss: 0.5395963
[Epoch 26] ogbg-molsider: 0.533101 val loss: 1.195166
[Epoch 26] ogbg-molsider: 0.546756 test loss: 1.406966
[Epoch 27; Iter    24/   36] train: loss: 0.5243873
[Epoch 27] ogbg-molsider: 0.540118 val loss: 0.831513
[Epoch 27] ogbg-molsider: 0.537629 test loss: 0.838991
[Epoch 28; Iter    18/   36] train: loss: 0.5111520
[Epoch 28] ogbg-molsider: 0.524714 val loss: 0.794416
[Epoch 28] ogbg-molsider: 0.545387 test loss: 0.704658
[Epoch 29; Iter    12/   36] train: loss: 0.5262805
[Epoch 29] ogbg-molsider: 0.521485 val loss: 0.682673
[Epoch 29] ogbg-molsider: 0.534026 test loss: 0.670771
[Epoch 30; Iter     6/   36] train: loss: 0.5314710
[Epoch 30; Iter    36/   36] train: loss: 0.5269513
[Epoch 30] ogbg-molsider: 0.524674 val loss: 0.510109
[Epoch 30] ogbg-molsider: 0.563998 test loss: 0.503704
[Epoch 31; Iter    30/   36] train: loss: 0.4714009
[Epoch 31] ogbg-molsider: 0.536963 val loss: 0.522165
[Epoch 31] ogbg-molsider: 0.560183 test loss: 0.507230
[Epoch 32; Iter    24/   36] train: loss: 0.5480386
[Epoch 32] ogbg-molsider: 0.509726 val loss: 0.546779
[Epoch 32] ogbg-molsider: 0.559763 test loss: 0.534733
[Epoch 33; Iter    18/   36] train: loss: 0.4950085
[Epoch 33] ogbg-molsider: 0.540055 val loss: 1.387934
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/sider/noise=0.2/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.2_6_26-05_10-50-20
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.2
logdir: runs/static_noise/GraphCL/sider/noise=0.2
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6927496
[Epoch 1] ogbg-molsider: 0.472108 val loss: 0.692361
[Epoch 1] ogbg-molsider: 0.489410 test loss: 0.691652
[Epoch 2; Iter    24/   36] train: loss: 0.6937664
[Epoch 2] ogbg-molsider: 0.486034 val loss: 0.691340
[Epoch 2] ogbg-molsider: 0.501404 test loss: 0.690088
[Epoch 3; Iter    18/   36] train: loss: 0.6936335
[Epoch 3] ogbg-molsider: 0.485238 val loss: 0.692375
[Epoch 3] ogbg-molsider: 0.500807 test loss: 0.691369
[Epoch 4; Iter    12/   36] train: loss: 0.6934088
[Epoch 4] ogbg-molsider: 0.488709 val loss: 0.692809
[Epoch 4] ogbg-molsider: 0.499687 test loss: 0.692021
[Epoch 5; Iter     6/   36] train: loss: 0.6931030
[Epoch 5; Iter    36/   36] train: loss: 0.6927692
[Epoch 5] ogbg-molsider: 0.489127 val loss: 0.693091
[Epoch 5] ogbg-molsider: 0.501213 test loss: 0.692334
[Epoch 6; Iter    30/   36] train: loss: 0.6934203
[Epoch 6] ogbg-molsider: 0.490114 val loss: 0.692315
[Epoch 6] ogbg-molsider: 0.501726 test loss: 0.691204
[Epoch 7; Iter    24/   36] train: loss: 0.6930045
[Epoch 7] ogbg-molsider: 0.487225 val loss: 0.692499
[Epoch 7] ogbg-molsider: 0.501336 test loss: 0.691578
[Epoch 8; Iter    18/   36] train: loss: 0.6930236
[Epoch 8] ogbg-molsider: 0.488395 val loss: 0.691838
[Epoch 8] ogbg-molsider: 0.502801 test loss: 0.690522
[Epoch 9; Iter    12/   36] train: loss: 0.6931832
[Epoch 9] ogbg-molsider: 0.489060 val loss: 0.692342
[Epoch 9] ogbg-molsider: 0.501645 test loss: 0.691457
[Epoch 10; Iter     6/   36] train: loss: 0.6927491
[Epoch 10; Iter    36/   36] train: loss: 0.6929167
[Epoch 10] ogbg-molsider: 0.489232 val loss: 0.691972
[Epoch 10] ogbg-molsider: 0.501344 test loss: 0.691079
[Epoch 11; Iter    30/   36] train: loss: 0.6930478
[Epoch 11] ogbg-molsider: 0.489183 val loss: 0.691940
[Epoch 11] ogbg-molsider: 0.501884 test loss: 0.690939
[Epoch 12; Iter    24/   36] train: loss: 0.6923748
[Epoch 12] ogbg-molsider: 0.491017 val loss: 0.691927
[Epoch 12] ogbg-molsider: 0.501470 test loss: 0.691106
[Epoch 13; Iter    18/   36] train: loss: 0.6920391
[Epoch 13] ogbg-molsider: 0.490508 val loss: 0.692019
[Epoch 13] ogbg-molsider: 0.501016 test loss: 0.691176
[Epoch 14; Iter    12/   36] train: loss: 0.6919178
[Epoch 14] ogbg-molsider: 0.492122 val loss: 0.691233
[Epoch 14] ogbg-molsider: 0.500813 test loss: 0.690238
[Epoch 15; Iter     6/   36] train: loss: 0.6913303
[Epoch 15; Iter    36/   36] train: loss: 0.6920158
[Epoch 15] ogbg-molsider: 0.492969 val loss: 0.691417
[Epoch 15] ogbg-molsider: 0.502513 test loss: 0.690563
[Epoch 16; Iter    30/   36] train: loss: 0.6920885
[Epoch 16] ogbg-molsider: 0.490237 val loss: 0.691385
[Epoch 16] ogbg-molsider: 0.500750 test loss: 0.690544
[Epoch 17; Iter    24/   36] train: loss: 0.6919185
[Epoch 17] ogbg-molsider: 0.491933 val loss: 0.690836
[Epoch 17] ogbg-molsider: 0.501625 test loss: 0.689920
[Epoch 18; Iter    18/   36] train: loss: 0.6916260
[Epoch 18] ogbg-molsider: 0.490930 val loss: 0.690802
[Epoch 18] ogbg-molsider: 0.501136 test loss: 0.689815
[Epoch 19; Iter    12/   36] train: loss: 0.6910458
[Epoch 19] ogbg-molsider: 0.492407 val loss: 0.690089
[Epoch 19] ogbg-molsider: 0.502508 test loss: 0.688794
[Epoch 20; Iter     6/   36] train: loss: 0.6907631
[Epoch 20; Iter    36/   36] train: loss: 0.6874719
[Epoch 20] ogbg-molsider: 0.526577 val loss: 0.681507
[Epoch 20] ogbg-molsider: 0.529706 test loss: 0.681940
[Epoch 21; Iter    30/   36] train: loss: 0.6770176
[Epoch 21] ogbg-molsider: 0.518346 val loss: 0.655800
[Epoch 21] ogbg-molsider: 0.564557 test loss: 0.654368
[Epoch 22; Iter    24/   36] train: loss: 0.6640244
[Epoch 22] ogbg-molsider: 0.529858 val loss: 0.636264
[Epoch 22] ogbg-molsider: 0.550850 test loss: 0.633741
[Epoch 23; Iter    18/   36] train: loss: 0.6345972
[Epoch 23] ogbg-molsider: 0.530060 val loss: 0.653365
[Epoch 23] ogbg-molsider: 0.553330 test loss: 0.656364
[Epoch 24; Iter    12/   36] train: loss: 0.6179525
[Epoch 24] ogbg-molsider: 0.517626 val loss: 0.569275
[Epoch 24] ogbg-molsider: 0.564840 test loss: 0.566480
[Epoch 25; Iter     6/   36] train: loss: 0.5797314
[Epoch 25; Iter    36/   36] train: loss: 0.5910757
[Epoch 25] ogbg-molsider: 0.511235 val loss: 0.541165
[Epoch 25] ogbg-molsider: 0.562249 test loss: 0.542579
[Epoch 26; Iter    30/   36] train: loss: 0.5649620
[Epoch 26] ogbg-molsider: 0.496060 val loss: 0.543792
[Epoch 26] ogbg-molsider: 0.554125 test loss: 0.541272
[Epoch 27; Iter    24/   36] train: loss: 0.5300685
[Epoch 27] ogbg-molsider: 0.507675 val loss: 0.516203
[Epoch 27] ogbg-molsider: 0.568291 test loss: 0.522026
[Epoch 28; Iter    18/   36] train: loss: 0.4979738
[Epoch 28] ogbg-molsider: 0.536826 val loss: 0.498232
[Epoch 28] ogbg-molsider: 0.565714 test loss: 0.509230
[Epoch 29; Iter    12/   36] train: loss: 0.5668300
[Epoch 29] ogbg-molsider: 0.503064 val loss: 0.505423
[Epoch 29] ogbg-molsider: 0.563469 test loss: 0.511425
[Epoch 30; Iter     6/   36] train: loss: 0.5142914
[Epoch 30; Iter    36/   36] train: loss: 0.4834899
[Epoch 30] ogbg-molsider: 0.523023 val loss: 0.577292
[Epoch 30] ogbg-molsider: 0.565292 test loss: 0.552958
[Epoch 31; Iter    30/   36] train: loss: 0.5131522
[Epoch 31] ogbg-molsider: 0.543876 val loss: 1.041201
[Epoch 31] ogbg-molsider: 0.549362 test loss: 1.025489
[Epoch 32; Iter    24/   36] train: loss: 0.4970766
[Epoch 32] ogbg-molsider: 0.549939 val loss: 1.075543
[Epoch 32] ogbg-molsider: 0.549373 test loss: 1.018856
[Epoch 33; Iter    18/   36] train: loss: 0.5116051
[Epoch 33] ogbg-molsider: 0.515969 val loss: 1.775762
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/sider/noise=0.0/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.0_6_26-05_10-49-42
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.0
logdir: runs/static_noise/GraphCL/sider/noise=0.0
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932251
[Epoch 1] ogbg-molsider: 0.475188 val loss: 0.693091
[Epoch 1] ogbg-molsider: 0.487468 test loss: 0.692665
[Epoch 2; Iter    24/   36] train: loss: 0.6936162
[Epoch 2] ogbg-molsider: 0.483259 val loss: 0.693333
[Epoch 2] ogbg-molsider: 0.494448 test loss: 0.692781
[Epoch 3; Iter    18/   36] train: loss: 0.6937852
[Epoch 3] ogbg-molsider: 0.481916 val loss: 0.693251
[Epoch 3] ogbg-molsider: 0.495852 test loss: 0.692527
[Epoch 4; Iter    12/   36] train: loss: 0.6931652
[Epoch 4] ogbg-molsider: 0.484816 val loss: 0.693293
[Epoch 4] ogbg-molsider: 0.499551 test loss: 0.692624
[Epoch 5; Iter     6/   36] train: loss: 0.6927382
[Epoch 5; Iter    36/   36] train: loss: 0.6923722
[Epoch 5] ogbg-molsider: 0.483999 val loss: 0.693191
[Epoch 5] ogbg-molsider: 0.499833 test loss: 0.692471
[Epoch 6; Iter    30/   36] train: loss: 0.6933881
[Epoch 6] ogbg-molsider: 0.483554 val loss: 0.693122
[Epoch 6] ogbg-molsider: 0.497578 test loss: 0.692398
[Epoch 7; Iter    24/   36] train: loss: 0.6925113
[Epoch 7] ogbg-molsider: 0.483826 val loss: 0.693102
[Epoch 7] ogbg-molsider: 0.498720 test loss: 0.692464
[Epoch 8; Iter    18/   36] train: loss: 0.6927406
[Epoch 8] ogbg-molsider: 0.480618 val loss: 0.692852
[Epoch 8] ogbg-molsider: 0.498981 test loss: 0.692081
[Epoch 9; Iter    12/   36] train: loss: 0.6931607
[Epoch 9] ogbg-molsider: 0.481298 val loss: 0.692794
[Epoch 9] ogbg-molsider: 0.498189 test loss: 0.692018
[Epoch 10; Iter     6/   36] train: loss: 0.6927510
[Epoch 10; Iter    36/   36] train: loss: 0.6932729
[Epoch 10] ogbg-molsider: 0.483696 val loss: 0.692735
[Epoch 10] ogbg-molsider: 0.497800 test loss: 0.692060
[Epoch 11; Iter    30/   36] train: loss: 0.6922941
[Epoch 11] ogbg-molsider: 0.483690 val loss: 0.692607
[Epoch 11] ogbg-molsider: 0.499632 test loss: 0.691933
[Epoch 12; Iter    24/   36] train: loss: 0.6922039
[Epoch 12] ogbg-molsider: 0.484378 val loss: 0.692515
[Epoch 12] ogbg-molsider: 0.501044 test loss: 0.691892
[Epoch 13; Iter    18/   36] train: loss: 0.6923359
[Epoch 13] ogbg-molsider: 0.484329 val loss: 0.692347
[Epoch 13] ogbg-molsider: 0.500183 test loss: 0.691708
[Epoch 14; Iter    12/   36] train: loss: 0.6920405
[Epoch 14] ogbg-molsider: 0.482742 val loss: 0.692055
[Epoch 14] ogbg-molsider: 0.498248 test loss: 0.691322
[Epoch 15; Iter     6/   36] train: loss: 0.6918349
[Epoch 15; Iter    36/   36] train: loss: 0.6916741
[Epoch 15] ogbg-molsider: 0.482693 val loss: 0.692006
[Epoch 15] ogbg-molsider: 0.499932 test loss: 0.691260
[Epoch 16; Iter    30/   36] train: loss: 0.6923392
[Epoch 16] ogbg-molsider: 0.486074 val loss: 0.691824
[Epoch 16] ogbg-molsider: 0.500256 test loss: 0.691167
[Epoch 17; Iter    24/   36] train: loss: 0.6917476
[Epoch 17] ogbg-molsider: 0.482482 val loss: 0.691651
[Epoch 17] ogbg-molsider: 0.501051 test loss: 0.690935
[Epoch 18; Iter    18/   36] train: loss: 0.6917873
[Epoch 18] ogbg-molsider: 0.481579 val loss: 0.691432
[Epoch 18] ogbg-molsider: 0.499520 test loss: 0.690706
[Epoch 19; Iter    12/   36] train: loss: 0.6910781
[Epoch 19] ogbg-molsider: 0.483040 val loss: 0.691175
[Epoch 19] ogbg-molsider: 0.499059 test loss: 0.690396
[Epoch 20; Iter     6/   36] train: loss: 0.6901195
[Epoch 20; Iter    36/   36] train: loss: 0.6872858
[Epoch 20] ogbg-molsider: 0.532703 val loss: 0.680850
[Epoch 20] ogbg-molsider: 0.557857 test loss: 0.679399
[Epoch 21; Iter    30/   36] train: loss: 0.6758711
[Epoch 21] ogbg-molsider: 0.518390 val loss: 0.650811
[Epoch 21] ogbg-molsider: 0.560651 test loss: 0.651349
[Epoch 22; Iter    24/   36] train: loss: 0.6622298
[Epoch 22] ogbg-molsider: 0.531238 val loss: 0.621855
[Epoch 22] ogbg-molsider: 0.570501 test loss: 0.620218
[Epoch 23; Iter    18/   36] train: loss: 0.6309107
[Epoch 23] ogbg-molsider: 0.527992 val loss: 0.587983
[Epoch 23] ogbg-molsider: 0.577106 test loss: 0.590271
[Epoch 24; Iter    12/   36] train: loss: 0.6134252
[Epoch 24] ogbg-molsider: 0.541577 val loss: 0.555987
[Epoch 24] ogbg-molsider: 0.590502 test loss: 0.559049
[Epoch 25; Iter     6/   36] train: loss: 0.5730899
[Epoch 25; Iter    36/   36] train: loss: 0.5890972
[Epoch 25] ogbg-molsider: 0.550515 val loss: 0.532913
[Epoch 25] ogbg-molsider: 0.559815 test loss: 0.539503
[Epoch 26; Iter    30/   36] train: loss: 0.5630614
[Epoch 26] ogbg-molsider: 0.539935 val loss: 0.530809
[Epoch 26] ogbg-molsider: 0.589484 test loss: 0.533135
[Epoch 27; Iter    24/   36] train: loss: 0.5240155
[Epoch 27] ogbg-molsider: 0.569708 val loss: 0.501522
[Epoch 27] ogbg-molsider: 0.577891 test loss: 0.518668
[Epoch 28; Iter    18/   36] train: loss: 0.4704752
[Epoch 28] ogbg-molsider: 0.556601 val loss: 0.489122
[Epoch 28] ogbg-molsider: 0.581335 test loss: 0.498574
[Epoch 29; Iter    12/   36] train: loss: 0.5589181
[Epoch 29] ogbg-molsider: 0.573898 val loss: 0.486062
[Epoch 29] ogbg-molsider: 0.601157 test loss: 0.495226
[Epoch 30; Iter     6/   36] train: loss: 0.5131509
[Epoch 30; Iter    36/   36] train: loss: 0.4826609
[Epoch 30] ogbg-molsider: 0.582567 val loss: 0.481786
[Epoch 30] ogbg-molsider: 0.592026 test loss: 0.495194
[Epoch 31; Iter    30/   36] train: loss: 0.5102567
[Epoch 31] ogbg-molsider: 0.562999 val loss: 0.486401
[Epoch 31] ogbg-molsider: 0.602747 test loss: 0.496858
[Epoch 32; Iter    24/   36] train: loss: 0.4893579
[Epoch 32] ogbg-molsider: 0.581744 val loss: 0.478141
[Epoch 32] ogbg-molsider: 0.611555 test loss: 0.489153
[Epoch 33; Iter    18/   36] train: loss: 0.5150605
[Epoch 33] ogbg-molsider: 0.575777 val loss: 0.482730
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/sider/noise=0.0/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.0_4_26-05_10-49-41
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.0
logdir: runs/static_noise/GraphCL/sider/noise=0.0
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6934346
[Epoch 1] ogbg-molsider: 0.517062 val loss: 0.693269
[Epoch 1] ogbg-molsider: 0.487229 test loss: 0.693540
[Epoch 2; Iter    24/   36] train: loss: 0.6932150
[Epoch 2] ogbg-molsider: 0.496588 val loss: 0.693226
[Epoch 2] ogbg-molsider: 0.494216 test loss: 0.693742
[Epoch 3; Iter    18/   36] train: loss: 0.6929246
[Epoch 3] ogbg-molsider: 0.494929 val loss: 0.693120
[Epoch 3] ogbg-molsider: 0.499175 test loss: 0.693727
[Epoch 4; Iter    12/   36] train: loss: 0.6936165
[Epoch 4] ogbg-molsider: 0.491360 val loss: 0.693248
[Epoch 4] ogbg-molsider: 0.501199 test loss: 0.693993
[Epoch 5; Iter     6/   36] train: loss: 0.6930395
[Epoch 5; Iter    36/   36] train: loss: 0.6938283
[Epoch 5] ogbg-molsider: 0.493785 val loss: 0.693021
[Epoch 5] ogbg-molsider: 0.501167 test loss: 0.693669
[Epoch 6; Iter    30/   36] train: loss: 0.6937210
[Epoch 6] ogbg-molsider: 0.497214 val loss: 0.692776
[Epoch 6] ogbg-molsider: 0.502237 test loss: 0.693334
[Epoch 7; Iter    24/   36] train: loss: 0.6931588
[Epoch 7] ogbg-molsider: 0.492556 val loss: 0.692751
[Epoch 7] ogbg-molsider: 0.500532 test loss: 0.693388
[Epoch 8; Iter    18/   36] train: loss: 0.6926011
[Epoch 8] ogbg-molsider: 0.493893 val loss: 0.692808
[Epoch 8] ogbg-molsider: 0.498751 test loss: 0.693443
[Epoch 9; Iter    12/   36] train: loss: 0.6935994
[Epoch 9] ogbg-molsider: 0.494922 val loss: 0.692633
[Epoch 9] ogbg-molsider: 0.498371 test loss: 0.693348
[Epoch 10; Iter     6/   36] train: loss: 0.6931671
[Epoch 10; Iter    36/   36] train: loss: 0.6930419
[Epoch 10] ogbg-molsider: 0.492844 val loss: 0.692702
[Epoch 10] ogbg-molsider: 0.501177 test loss: 0.693414
[Epoch 11; Iter    30/   36] train: loss: 0.6924747
[Epoch 11] ogbg-molsider: 0.495599 val loss: 0.692310
[Epoch 11] ogbg-molsider: 0.500103 test loss: 0.692999
[Epoch 12; Iter    24/   36] train: loss: 0.6924058
[Epoch 12] ogbg-molsider: 0.494151 val loss: 0.692222
[Epoch 12] ogbg-molsider: 0.500191 test loss: 0.692899
[Epoch 13; Iter    18/   36] train: loss: 0.6919406
[Epoch 13] ogbg-molsider: 0.494657 val loss: 0.692163
[Epoch 13] ogbg-molsider: 0.497504 test loss: 0.692801
[Epoch 14; Iter    12/   36] train: loss: 0.6922175
[Epoch 14] ogbg-molsider: 0.494748 val loss: 0.691822
[Epoch 14] ogbg-molsider: 0.500956 test loss: 0.692424
[Epoch 15; Iter     6/   36] train: loss: 0.6922642
[Epoch 15; Iter    36/   36] train: loss: 0.6918569
[Epoch 15] ogbg-molsider: 0.494233 val loss: 0.691641
[Epoch 15] ogbg-molsider: 0.497670 test loss: 0.692290
[Epoch 16; Iter    30/   36] train: loss: 0.6916514
[Epoch 16] ogbg-molsider: 0.493241 val loss: 0.691514
[Epoch 16] ogbg-molsider: 0.498376 test loss: 0.692222
[Epoch 17; Iter    24/   36] train: loss: 0.6917756
[Epoch 17] ogbg-molsider: 0.496347 val loss: 0.691206
[Epoch 17] ogbg-molsider: 0.500581 test loss: 0.691859
[Epoch 18; Iter    18/   36] train: loss: 0.6913220
[Epoch 18] ogbg-molsider: 0.496930 val loss: 0.690971
[Epoch 18] ogbg-molsider: 0.498736 test loss: 0.691616
[Epoch 19; Iter    12/   36] train: loss: 0.6915759
[Epoch 19] ogbg-molsider: 0.495092 val loss: 0.690679
[Epoch 19] ogbg-molsider: 0.498381 test loss: 0.691408
[Epoch 20; Iter     6/   36] train: loss: 0.6911590
[Epoch 20; Iter    36/   36] train: loss: 0.6859812
[Epoch 20] ogbg-molsider: 0.523212 val loss: 0.679632
[Epoch 20] ogbg-molsider: 0.567177 test loss: 0.679582
[Epoch 21; Iter    30/   36] train: loss: 0.6719510
[Epoch 21] ogbg-molsider: 0.529366 val loss: 0.668470
[Epoch 21] ogbg-molsider: 0.560364 test loss: 0.667669
[Epoch 22; Iter    24/   36] train: loss: 0.6578662
[Epoch 22] ogbg-molsider: 0.527395 val loss: 0.616824
[Epoch 22] ogbg-molsider: 0.560309 test loss: 0.618345
[Epoch 23; Iter    18/   36] train: loss: 0.6296055
[Epoch 23] ogbg-molsider: 0.545563 val loss: 0.597652
[Epoch 23] ogbg-molsider: 0.569389 test loss: 0.609850
[Epoch 24; Iter    12/   36] train: loss: 0.6154273
[Epoch 24] ogbg-molsider: 0.540302 val loss: 0.573624
[Epoch 24] ogbg-molsider: 0.578993 test loss: 0.581587
[Epoch 25; Iter     6/   36] train: loss: 0.5911117
[Epoch 25; Iter    36/   36] train: loss: 0.5668362
[Epoch 25] ogbg-molsider: 0.532083 val loss: 0.542490
[Epoch 25] ogbg-molsider: 0.574028 test loss: 0.544566
[Epoch 26; Iter    30/   36] train: loss: 0.5350565
[Epoch 26] ogbg-molsider: 0.543953 val loss: 0.501791
[Epoch 26] ogbg-molsider: 0.590497 test loss: 0.510996
[Epoch 27; Iter    24/   36] train: loss: 0.5206698
[Epoch 27] ogbg-molsider: 0.548912 val loss: 0.499007
[Epoch 27] ogbg-molsider: 0.574523 test loss: 0.511997
[Epoch 28; Iter    18/   36] train: loss: 0.5145035
[Epoch 28] ogbg-molsider: 0.553118 val loss: 0.484822
[Epoch 28] ogbg-molsider: 0.590743 test loss: 0.496456
[Epoch 29; Iter    12/   36] train: loss: 0.5277827
[Epoch 29] ogbg-molsider: 0.566481 val loss: 0.482683
[Epoch 29] ogbg-molsider: 0.590578 test loss: 0.492127
[Epoch 30; Iter     6/   36] train: loss: 0.5182301
[Epoch 30; Iter    36/   36] train: loss: 0.5050404
[Epoch 30] ogbg-molsider: 0.564688 val loss: 0.484159
[Epoch 30] ogbg-molsider: 0.610031 test loss: 0.488898
[Epoch 31; Iter    30/   36] train: loss: 0.4642969
[Epoch 31] ogbg-molsider: 0.570587 val loss: 0.481559
[Epoch 31] ogbg-molsider: 0.601890 test loss: 0.496772
[Epoch 32; Iter    24/   36] train: loss: 0.5458062
[Epoch 32] ogbg-molsider: 0.554737 val loss: 0.493074
[Epoch 32] ogbg-molsider: 0.597334 test loss: 0.502236
[Epoch 33; Iter    18/   36] train: loss: 0.4886954
[Epoch 33] ogbg-molsider: 0.558825 val loss: 0.487001
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/sider/noise=0.0/PNA_ogbg-molsider_GraphCL_sider_static_noise=0.0_5_26-05_10-49-42
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/sider/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_static_noise=0.0
logdir: runs/static_noise/GraphCL/sider/noise=0.0
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6927666
[Epoch 1] ogbg-molsider: 0.470907 val loss: 0.693026
[Epoch 1] ogbg-molsider: 0.518256 test loss: 0.692729
[Epoch 2; Iter    24/   36] train: loss: 0.6928043
[Epoch 2] ogbg-molsider: 0.467259 val loss: 0.692918
[Epoch 2] ogbg-molsider: 0.509426 test loss: 0.692458
[Epoch 3; Iter    18/   36] train: loss: 0.6928941
[Epoch 3] ogbg-molsider: 0.466317 val loss: 0.692948
[Epoch 3] ogbg-molsider: 0.503969 test loss: 0.692369
[Epoch 4; Iter    12/   36] train: loss: 0.6940182
[Epoch 4] ogbg-molsider: 0.472369 val loss: 0.692835
[Epoch 4] ogbg-molsider: 0.502085 test loss: 0.692439
[Epoch 5; Iter     6/   36] train: loss: 0.6927276
[Epoch 5; Iter    36/   36] train: loss: 0.6929785
[Epoch 5] ogbg-molsider: 0.465347 val loss: 0.692665
[Epoch 5] ogbg-molsider: 0.504436 test loss: 0.692044
[Epoch 6; Iter    30/   36] train: loss: 0.6928712
[Epoch 6] ogbg-molsider: 0.464059 val loss: 0.692736
[Epoch 6] ogbg-molsider: 0.502695 test loss: 0.692245
[Epoch 7; Iter    24/   36] train: loss: 0.6931918
[Epoch 7] ogbg-molsider: 0.464694 val loss: 0.692641
[Epoch 7] ogbg-molsider: 0.503673 test loss: 0.692137
[Epoch 8; Iter    18/   36] train: loss: 0.6928407
[Epoch 8] ogbg-molsider: 0.472480 val loss: 0.692578
[Epoch 8] ogbg-molsider: 0.507325 test loss: 0.691990
[Epoch 9; Iter    12/   36] train: loss: 0.6926391
[Epoch 9] ogbg-molsider: 0.467280 val loss: 0.692309
[Epoch 9] ogbg-molsider: 0.503701 test loss: 0.691826
[Epoch 10; Iter     6/   36] train: loss: 0.6930600
[Epoch 10; Iter    36/   36] train: loss: 0.6928003
[Epoch 10] ogbg-molsider: 0.466410 val loss: 0.692248
[Epoch 10] ogbg-molsider: 0.502825 test loss: 0.691801
[Epoch 11; Iter    30/   36] train: loss: 0.6928711
[Epoch 11] ogbg-molsider: 0.472666 val loss: 0.692319
[Epoch 11] ogbg-molsider: 0.502607 test loss: 0.691763
[Epoch 12; Iter    24/   36] train: loss: 0.6927555
[Epoch 12] ogbg-molsider: 0.467929 val loss: 0.691996
[Epoch 12] ogbg-molsider: 0.503249 test loss: 0.691521
[Epoch 13; Iter    18/   36] train: loss: 0.6920398
[Epoch 13] ogbg-molsider: 0.470612 val loss: 0.691888
[Epoch 13] ogbg-molsider: 0.502339 test loss: 0.691520
[Epoch 14; Iter    12/   36] train: loss: 0.6925406
[Epoch 14] ogbg-molsider: 0.468067 val loss: 0.691736
[Epoch 14] ogbg-molsider: 0.502421 test loss: 0.691432
[Epoch 15; Iter     6/   36] train: loss: 0.6923411
[Epoch 15; Iter    36/   36] train: loss: 0.6922120
[Epoch 15] ogbg-molsider: 0.472721 val loss: 0.691465
[Epoch 15] ogbg-molsider: 0.501804 test loss: 0.691101
[Epoch 16; Iter    30/   36] train: loss: 0.6922432
[Epoch 16] ogbg-molsider: 0.468274 val loss: 0.691176
[Epoch 16] ogbg-molsider: 0.501560 test loss: 0.690806
[Epoch 17; Iter    24/   36] train: loss: 0.6914683
[Epoch 17] ogbg-molsider: 0.466829 val loss: 0.691068
[Epoch 17] ogbg-molsider: 0.502384 test loss: 0.690731
[Epoch 18; Iter    18/   36] train: loss: 0.6916913
[Epoch 18] ogbg-molsider: 0.469502 val loss: 0.690821
[Epoch 18] ogbg-molsider: 0.504158 test loss: 0.690475
[Epoch 19; Iter    12/   36] train: loss: 0.6914520
[Epoch 19] ogbg-molsider: 0.467819 val loss: 0.690582
[Epoch 19] ogbg-molsider: 0.504164 test loss: 0.690178
[Epoch 20; Iter     6/   36] train: loss: 0.6904233
[Epoch 20; Iter    36/   36] train: loss: 0.6852675
[Epoch 20] ogbg-molsider: 0.556428 val loss: 0.681383
[Epoch 20] ogbg-molsider: 0.540792 test loss: 0.681370
[Epoch 21; Iter    30/   36] train: loss: 0.6782297
[Epoch 21] ogbg-molsider: 0.526708 val loss: 0.661628
[Epoch 21] ogbg-molsider: 0.567245 test loss: 0.660192
[Epoch 22; Iter    24/   36] train: loss: 0.6581158
[Epoch 22] ogbg-molsider: 0.526854 val loss: 0.630513
[Epoch 22] ogbg-molsider: 0.561822 test loss: 0.629877
[Epoch 23; Iter    18/   36] train: loss: 0.6328252
[Epoch 23] ogbg-molsider: 0.536460 val loss: 0.597797
[Epoch 23] ogbg-molsider: 0.581449 test loss: 0.599655
[Epoch 24; Iter    12/   36] train: loss: 0.6301340
[Epoch 24] ogbg-molsider: 0.541864 val loss: 0.558747
[Epoch 24] ogbg-molsider: 0.575883 test loss: 0.561569
[Epoch 25; Iter     6/   36] train: loss: 0.5865523
[Epoch 25; Iter    36/   36] train: loss: 0.5560640
[Epoch 25] ogbg-molsider: 0.528946 val loss: 0.555613
[Epoch 25] ogbg-molsider: 0.567873 test loss: 0.556904
[Epoch 26; Iter    30/   36] train: loss: 0.5714481
[Epoch 26] ogbg-molsider: 0.547959 val loss: 0.522184
[Epoch 26] ogbg-molsider: 0.586042 test loss: 0.527968
[Epoch 27; Iter    24/   36] train: loss: 0.5317004
[Epoch 27] ogbg-molsider: 0.556506 val loss: 0.551537
[Epoch 27] ogbg-molsider: 0.578084 test loss: 0.522468
[Epoch 28; Iter    18/   36] train: loss: 0.5323139
[Epoch 28] ogbg-molsider: 0.570439 val loss: 0.495576
[Epoch 28] ogbg-molsider: 0.600412 test loss: 0.504854
[Epoch 29; Iter    12/   36] train: loss: 0.4946818
[Epoch 29] ogbg-molsider: 0.579151 val loss: 0.479828
[Epoch 29] ogbg-molsider: 0.602051 test loss: 0.488982
[Epoch 30; Iter     6/   36] train: loss: 0.4960810
[Epoch 30; Iter    36/   36] train: loss: 0.4559035
[Epoch 30] ogbg-molsider: 0.551406 val loss: 0.489279
[Epoch 30] ogbg-molsider: 0.595604 test loss: 0.499380
[Epoch 31; Iter    30/   36] train: loss: 0.5034415
[Epoch 31] ogbg-molsider: 0.565892 val loss: 0.497058
[Epoch 31] ogbg-molsider: 0.581490 test loss: 0.507726
[Epoch 32; Iter    24/   36] train: loss: 0.4795765
[Epoch 32] ogbg-molsider: 0.556922 val loss: 0.485797
[Epoch 32] ogbg-molsider: 0.611765 test loss: 0.493485
[Epoch 33; Iter    18/   36] train: loss: 0.4925794
[Epoch 33] ogbg-molsider: 0.561708 val loss: 0.485353
[Epoch 33] ogbg-molsider: 0.583092 test loss: 0.493028
[Epoch 34; Iter    12/   36] train: loss: 0.5067698
[Epoch 34] ogbg-molsider: 0.597460 val loss: 0.475798
[Epoch 34] ogbg-molsider: 0.577381 test loss: 0.493491
[Epoch 35; Iter     6/   36] train: loss: 0.5044770
[Epoch 35; Iter    36/   36] train: loss: 0.5132710
[Epoch 35] ogbg-molsider: 0.584005 val loss: 0.489765
[Epoch 35] ogbg-molsider: 0.592647 test loss: 0.505503
[Epoch 36; Iter    30/   36] train: loss: 0.4765377
[Epoch 36] ogbg-molsider: 0.595539 val loss: 0.482093
[Epoch 36] ogbg-molsider: 0.607242 test loss: 0.498175
[Epoch 37; Iter    24/   36] train: loss: 0.5041363
[Epoch 37] ogbg-molsider: 0.588522 val loss: 0.485974
[Epoch 37] ogbg-molsider: 0.597339 test loss: 0.500349
[Epoch 38; Iter    18/   36] train: loss: 0.4828921
[Epoch 38] ogbg-molsider: 0.585330 val loss: 0.478050
[Epoch 38] ogbg-molsider: 0.594492 test loss: 0.493862
[Epoch 39; Iter    12/   36] train: loss: 0.5249252
[Epoch 39] ogbg-molsider: 0.590693 val loss: 0.481958
[Epoch 39] ogbg-molsider: 0.576832 test loss: 0.507083
[Epoch 40; Iter     6/   36] train: loss: 0.5026757
[Epoch 40; Iter    36/   36] train: loss: 0.4703741
[Epoch 40] ogbg-molsider: 0.595095 val loss: 0.554313
[Epoch 40] ogbg-molsider: 0.594843 test loss: 0.563573
[Epoch 41; Iter    30/   36] train: loss: 0.4792831
[Epoch 41] ogbg-molsider: 0.556360 val loss: 0.548719
[Epoch 41] ogbg-molsider: 0.521837 test loss: 0.553882
[Epoch 42; Iter    24/   36] train: loss: 0.4766196
[Epoch 42] ogbg-molsider: 0.532293 val loss: 0.529349
[Epoch 42] ogbg-molsider: 0.603801 test loss: 0.580307
[Epoch 43; Iter    18/   36] train: loss: 0.4831638
[Epoch 43] ogbg-molsider: 0.567310 val loss: 0.716045
[Epoch 43] ogbg-molsider: 0.574027 test loss: 0.607580
[Epoch 44; Iter    12/   36] train: loss: 0.4519635
[Epoch 44] ogbg-molsider: 0.589817 val loss: 0.496482
[Epoch 44] ogbg-molsider: 0.605029 test loss: 0.504175
[Epoch 45; Iter     6/   36] train: loss: 0.4286978
[Epoch 45; Iter    36/   36] train: loss: 0.4465943
[Epoch 45] ogbg-molsider: 0.576416 val loss: 0.513406
[Epoch 45] ogbg-molsider: 0.580473 test loss: 0.546356
[Epoch 46; Iter    30/   36] train: loss: 0.4903682
[Epoch 46] ogbg-molsider: 0.572493 val loss: 0.671629
[Epoch 46] ogbg-molsider: 0.555783 test loss: 0.871552
[Epoch 47; Iter    24/   36] train: loss: 0.4671039
[Epoch 47] ogbg-molsider: 0.563192 val loss: 0.503912
[Epoch 47] ogbg-molsider: 0.583390 test loss: 0.520551
[Epoch 48; Iter    18/   36] train: loss: 0.5449232
[Epoch 48] ogbg-molsider: 0.571276 val loss: 0.491424
[Epoch 48] ogbg-molsider: 0.563783 test loss: 0.527972
[Epoch 49; Iter    12/   36] train: loss: 0.4421075
[Epoch 49] ogbg-molsider: 0.569879 val loss: 0.534290
[Epoch 49] ogbg-molsider: 0.602129 test loss: 0.562400
[Epoch 50; Iter     6/   36] train: loss: 0.4515703
[Epoch 50; Iter    36/   36] train: loss: 0.4536832
[Epoch 50] ogbg-molsider: 0.585388 val loss: 0.552614
[Epoch 50] ogbg-molsider: 0.590205 test loss: 0.614650
[Epoch 51; Iter    30/   36] train: loss: 0.4439495
[Epoch 51] ogbg-molsider: 0.560291 val loss: 0.602995
[Epoch 51] ogbg-molsider: 0.547618 test loss: 0.694274
[Epoch 52; Iter    24/   36] train: loss: 0.4196041
[Epoch 52] ogbg-molsider: 0.570671 val loss: 0.519930
[Epoch 52] ogbg-molsider: 0.588076 test loss: 0.529160
[Epoch 53; Iter    18/   36] train: loss: 0.4437590
[Epoch 53] ogbg-molsider: 0.587145 val loss: 0.567386
[Epoch 53] ogbg-molsider: 0.569759 test loss: 0.633799
[Epoch 54; Iter    12/   36] train: loss: 0.4252403
[Epoch 54] ogbg-molsider: 0.574606 val loss: 0.553870
[Epoch 54] ogbg-molsider: 0.539730 test loss: 0.624275
[Epoch 55; Iter     6/   36] train: loss: 0.3968642
[Epoch 55; Iter    36/   36] train: loss: 0.4326644
[Epoch 55] ogbg-molsider: 0.590378 val loss: 0.533945
[Epoch 55] ogbg-molsider: 0.552124 test loss: 0.601193
[Epoch 56; Iter    30/   36] train: loss: 0.3856048
[Epoch 56] ogbg-molsider: 0.560414 val loss: 0.557323
[Epoch 56] ogbg-molsider: 0.573802 test loss: 0.583186
[Epoch 57; Iter    24/   36] train: loss: 0.3756810
[Epoch 57] ogbg-molsider: 0.588351 val loss: 0.558324
[Epoch 57] ogbg-molsider: 0.580207 test loss: 0.619259
[Epoch 58; Iter    18/   36] train: loss: 0.3833808
[Epoch 58] ogbg-molsider: 0.578034 val loss: 0.581711
[Epoch 58] ogbg-molsider: 0.559257 test loss: 0.621651
[Epoch 59; Iter    12/   36] train: loss: 0.3333005
[Epoch 59] ogbg-molsider: 0.554039 val loss: 0.567643
[Epoch 59] ogbg-molsider: 0.575799 test loss: 0.568716
[Epoch 60; Iter     6/   36] train: loss: 0.3900263
[Epoch 60; Iter    36/   36] train: loss: 0.3774062
[Epoch 60] ogbg-molsider: 0.569244 val loss: 0.574457
[Epoch 60] ogbg-molsider: 0.567450 test loss: 0.598694
[Epoch 61; Iter    30/   36] train: loss: 0.3661131
[Epoch 61] ogbg-molsider: 0.572802 val loss: 0.592736
[Epoch 61] ogbg-molsider: 0.550293 test loss: 0.702386
[Epoch 62; Iter    24/   36] train: loss: 0.3872154
[Epoch 62] ogbg-molsider: 0.573616 val loss: 0.596566
[Epoch 62] ogbg-molsider: 0.578989 test loss: 0.649913
[Epoch 63; Iter    18/   36] train: loss: 0.3927975
[Epoch 63] ogbg-molsider: 0.581390 val loss: 0.619862
[Epoch 63] ogbg-molsider: 0.573562 test loss: 0.740202
[Epoch 64; Iter    12/   36] train: loss: 0.3354212
[Epoch 64] ogbg-molsider: 0.586028 val loss: 0.758458
[Epoch 64] ogbg-molsider: 0.552942 test loss: 0.850193
[Epoch 65; Iter     6/   36] train: loss: 0.3728669
[Epoch 65; Iter    36/   36] train: loss: 0.3704561
[Epoch 65] ogbg-molsider: 0.612886 val loss: 0.551266
[Epoch 65] ogbg-molsider: 0.589225 test loss: 0.546748
[Epoch 66; Iter    30/   36] train: loss: 0.3262373
[Epoch 66] ogbg-molsider: 0.589525 val loss: 0.618879
[Epoch 66] ogbg-molsider: 0.576542 test loss: 0.630116
[Epoch 67; Iter    24/   36] train: loss: 0.3409136
[Epoch 67] ogbg-molsider: 0.568099 val loss: 0.569154
[Epoch 67] ogbg-molsider: 0.548568 test loss: 0.636339
[Epoch 68; Iter    18/   36] train: loss: 0.3288877
[Epoch 68] ogbg-molsider: 0.582843 val loss: 0.578799
[Epoch 68] ogbg-molsider: 0.596979 test loss: 0.577285
[Epoch 69; Iter    12/   36] train: loss: 0.3258574
[Epoch 69] ogbg-molsider: 0.574067 val loss: 0.589811
[Epoch 69] ogbg-molsider: 0.599355 test loss: 0.614186
[Epoch 70; Iter     6/   36] train: loss: 0.3191475
[Epoch 70; Iter    36/   36] train: loss: 0.3392189
[Epoch 70] ogbg-molsider: 0.578866 val loss: 0.584224
[Epoch 70] ogbg-molsider: 0.595757 test loss: 0.622539
[Epoch 71; Iter    30/   36] train: loss: 0.3529268
[Epoch 71] ogbg-molsider: 0.587662 val loss: 0.539857
[Epoch 71] ogbg-molsider: 0.604125 test loss: 0.550801
[Epoch 72; Iter    24/   36] train: loss: 0.3348951
[Epoch 72] ogbg-molsider: 0.616864 val loss: 0.556043
[Epoch 72] ogbg-molsider: 0.595111 test loss: 0.591538
[Epoch 73; Iter    18/   36] train: loss: 0.3240382
[Epoch 73] ogbg-molsider: 0.579489 val loss: 0.567947
[Epoch 73] ogbg-molsider: 0.601384 test loss: 0.570959
[Epoch 74; Iter    12/   36] train: loss: 0.3240062
[Epoch 74] ogbg-molsider: 0.581840 val loss: 0.598240
[Epoch 74] ogbg-molsider: 0.583444 test loss: 0.666053
[Epoch 75; Iter     6/   36] train: loss: 0.3371150
[Epoch 75; Iter    36/   36] train: loss: 0.3677396
[Epoch 75] ogbg-molsider: 0.569904 val loss: 0.630611
[Epoch 75] ogbg-molsider: 0.587789 test loss: 0.692007
[Epoch 76; Iter    30/   36] train: loss: 0.3498732
[Epoch 76] ogbg-molsider: 0.585661 val loss: 0.642741
[Epoch 76] ogbg-molsider: 0.564732 test loss: 0.727895
[Epoch 77; Iter    24/   36] train: loss: 0.3153139
[Epoch 77] ogbg-molsider: 0.570437 val loss: 0.573857
[Epoch 77] ogbg-molsider: 0.592879 test loss: 0.607611
[Epoch 78; Iter    18/   36] train: loss: 0.3205713
[Epoch 78] ogbg-molsider: 0.578762 val loss: 0.584282
[Epoch 78] ogbg-molsider: 0.592647 test loss: 0.614980
[Epoch 79; Iter    12/   36] train: loss: 0.3121366
[Epoch 79] ogbg-molsider: 0.592360 val loss: 0.623822
[Epoch 79] ogbg-molsider: 0.564631 test loss: 0.735774
[Epoch 80; Iter     6/   36] train: loss: 0.3100622
[Epoch 80; Iter    36/   36] train: loss: 0.3267995
[Epoch 80] ogbg-molsider: 0.590750 val loss: 0.621551
[Epoch 80] ogbg-molsider: 0.579263 test loss: 0.703954
[Epoch 33] ogbg-molsider: 0.577461 test loss: 0.628125
[Epoch 34; Iter    12/   36] train: loss: 0.4839970
[Epoch 34] ogbg-molsider: 0.557752 val loss: 0.670950
[Epoch 34] ogbg-molsider: 0.536785 test loss: 0.638108
[Epoch 35; Iter     6/   36] train: loss: 0.5467702
[Epoch 35; Iter    36/   36] train: loss: 0.4910613
[Epoch 35] ogbg-molsider: 0.550380 val loss: 0.654428
[Epoch 35] ogbg-molsider: 0.553688 test loss: 0.612056
[Epoch 36; Iter    30/   36] train: loss: 0.4620476
[Epoch 36] ogbg-molsider: 0.591358 val loss: 0.717915
[Epoch 36] ogbg-molsider: 0.566437 test loss: 0.717695
[Epoch 37; Iter    24/   36] train: loss: 0.5542769
[Epoch 37] ogbg-molsider: 0.559711 val loss: 0.696833
[Epoch 37] ogbg-molsider: 0.550284 test loss: 0.676589
[Epoch 38; Iter    18/   36] train: loss: 0.5160586
[Epoch 38] ogbg-molsider: 0.568364 val loss: 0.643316
[Epoch 38] ogbg-molsider: 0.559559 test loss: 0.587713
[Epoch 39; Iter    12/   36] train: loss: 0.5006890
[Epoch 39] ogbg-molsider: 0.558853 val loss: 0.553645
[Epoch 39] ogbg-molsider: 0.560383 test loss: 0.511940
[Epoch 40; Iter     6/   36] train: loss: 0.4778854
[Epoch 40; Iter    36/   36] train: loss: 0.5113636
[Epoch 40] ogbg-molsider: 0.573373 val loss: 0.519428
[Epoch 40] ogbg-molsider: 0.553022 test loss: 0.512420
[Epoch 41; Iter    30/   36] train: loss: 0.5290746
[Epoch 41] ogbg-molsider: 0.596660 val loss: 0.478623
[Epoch 41] ogbg-molsider: 0.574445 test loss: 0.494440
[Epoch 42; Iter    24/   36] train: loss: 0.4922685
[Epoch 42] ogbg-molsider: 0.551223 val loss: 0.525038
[Epoch 42] ogbg-molsider: 0.567993 test loss: 0.561469
[Epoch 43; Iter    18/   36] train: loss: 0.4928749
[Epoch 43] ogbg-molsider: 0.507666 val loss: 0.601395
[Epoch 43] ogbg-molsider: 0.516437 test loss: 0.726864
[Epoch 44; Iter    12/   36] train: loss: 0.4886194
[Epoch 44] ogbg-molsider: 0.583058 val loss: 0.503798
[Epoch 44] ogbg-molsider: 0.553499 test loss: 0.528896
[Epoch 45; Iter     6/   36] train: loss: 0.4517361
[Epoch 45; Iter    36/   36] train: loss: 0.4474224
[Epoch 45] ogbg-molsider: 0.510708 val loss: 1.430129
[Epoch 45] ogbg-molsider: 0.525076 test loss: 1.431539
[Epoch 46; Iter    30/   36] train: loss: 0.5148127
[Epoch 46] ogbg-molsider: 0.560316 val loss: 0.521010
[Epoch 46] ogbg-molsider: 0.566736 test loss: 0.553222
[Epoch 47; Iter    24/   36] train: loss: 0.4692297
[Epoch 47] ogbg-molsider: 0.560634 val loss: 0.520521
[Epoch 47] ogbg-molsider: 0.551746 test loss: 0.527886
[Epoch 48; Iter    18/   36] train: loss: 0.4490984
[Epoch 48] ogbg-molsider: 0.543751 val loss: 1.361608
[Epoch 48] ogbg-molsider: 0.523762 test loss: 1.099305
[Epoch 49; Iter    12/   36] train: loss: 0.4647041
[Epoch 49] ogbg-molsider: 0.585342 val loss: 0.735575
[Epoch 49] ogbg-molsider: 0.541944 test loss: 0.775793
[Epoch 50; Iter     6/   36] train: loss: 0.4662117
[Epoch 50; Iter    36/   36] train: loss: 0.4761040
[Epoch 50] ogbg-molsider: 0.573366 val loss: 0.635377
[Epoch 50] ogbg-molsider: 0.567569 test loss: 0.641046
[Epoch 51; Iter    30/   36] train: loss: 0.4323961
[Epoch 51] ogbg-molsider: 0.568791 val loss: 0.722536
[Epoch 51] ogbg-molsider: 0.545344 test loss: 0.829361
[Epoch 52; Iter    24/   36] train: loss: 0.4536622
[Epoch 52] ogbg-molsider: 0.608707 val loss: 0.641782
[Epoch 52] ogbg-molsider: 0.561109 test loss: 0.624019
[Epoch 53; Iter    18/   36] train: loss: 0.3803367
[Epoch 53] ogbg-molsider: 0.602118 val loss: 0.763529
[Epoch 53] ogbg-molsider: 0.569061 test loss: 0.820623
[Epoch 54; Iter    12/   36] train: loss: 0.4014996
[Epoch 54] ogbg-molsider: 0.593485 val loss: 0.818910
[Epoch 54] ogbg-molsider: 0.549590 test loss: 0.925165
[Epoch 55; Iter     6/   36] train: loss: 0.3720289
[Epoch 55; Iter    36/   36] train: loss: 0.3595672
[Epoch 55] ogbg-molsider: 0.626330 val loss: 0.667367
[Epoch 55] ogbg-molsider: 0.573721 test loss: 0.718590
[Epoch 56; Iter    30/   36] train: loss: 0.3949233
[Epoch 56] ogbg-molsider: 0.613723 val loss: 0.818291
[Epoch 56] ogbg-molsider: 0.546966 test loss: 1.034340
[Epoch 57; Iter    24/   36] train: loss: 0.3518720
[Epoch 57] ogbg-molsider: 0.584482 val loss: 0.815512
[Epoch 57] ogbg-molsider: 0.557816 test loss: 0.855186
[Epoch 58; Iter    18/   36] train: loss: 0.3486015
[Epoch 58] ogbg-molsider: 0.608091 val loss: 0.610808
[Epoch 58] ogbg-molsider: 0.576040 test loss: 0.636682
[Epoch 59; Iter    12/   36] train: loss: 0.3702864
[Epoch 59] ogbg-molsider: 0.594398 val loss: 0.881856
[Epoch 59] ogbg-molsider: 0.568983 test loss: 1.017641
[Epoch 60; Iter     6/   36] train: loss: 0.3903488
[Epoch 60; Iter    36/   36] train: loss: 0.3329734
[Epoch 60] ogbg-molsider: 0.596390 val loss: 0.676931
[Epoch 60] ogbg-molsider: 0.559338 test loss: 0.788089
[Epoch 61; Iter    30/   36] train: loss: 0.3245771
[Epoch 61] ogbg-molsider: 0.615343 val loss: 0.608392
[Epoch 61] ogbg-molsider: 0.594750 test loss: 0.667819
[Epoch 62; Iter    24/   36] train: loss: 0.3446664
[Epoch 62] ogbg-molsider: 0.617687 val loss: 0.552750
[Epoch 62] ogbg-molsider: 0.590972 test loss: 0.583766
[Epoch 63; Iter    18/   36] train: loss: 0.3576426
[Epoch 63] ogbg-molsider: 0.616249 val loss: 0.606590
[Epoch 63] ogbg-molsider: 0.568472 test loss: 0.683978
[Epoch 64; Iter    12/   36] train: loss: 0.3468241
[Epoch 64] ogbg-molsider: 0.589594 val loss: 0.711879
[Epoch 64] ogbg-molsider: 0.575766 test loss: 0.795081
[Epoch 65; Iter     6/   36] train: loss: 0.3281384
[Epoch 65; Iter    36/   36] train: loss: 0.4702712
[Epoch 65] ogbg-molsider: 0.597517 val loss: 0.657296
[Epoch 65] ogbg-molsider: 0.588643 test loss: 0.703915
[Epoch 66; Iter    30/   36] train: loss: 0.3277107
[Epoch 66] ogbg-molsider: 0.599978 val loss: 0.642486
[Epoch 66] ogbg-molsider: 0.583663 test loss: 0.738134
[Epoch 67; Iter    24/   36] train: loss: 0.3405113
[Epoch 67] ogbg-molsider: 0.599258 val loss: 0.662076
[Epoch 67] ogbg-molsider: 0.580488 test loss: 0.724968
[Epoch 68; Iter    18/   36] train: loss: 0.3342706
[Epoch 68] ogbg-molsider: 0.614309 val loss: 0.688970
[Epoch 68] ogbg-molsider: 0.579821 test loss: 0.770101
[Epoch 69; Iter    12/   36] train: loss: 0.2964116
[Epoch 69] ogbg-molsider: 0.589252 val loss: 0.629672
[Epoch 69] ogbg-molsider: 0.581803 test loss: 0.653173
[Epoch 70; Iter     6/   36] train: loss: 0.3017508
[Epoch 70; Iter    36/   36] train: loss: 0.3134512
[Epoch 70] ogbg-molsider: 0.595822 val loss: 0.891886
[Epoch 70] ogbg-molsider: 0.581682 test loss: 1.031975
[Epoch 71; Iter    30/   36] train: loss: 0.3258353
[Epoch 71] ogbg-molsider: 0.605598 val loss: 0.610076
[Epoch 71] ogbg-molsider: 0.582689 test loss: 0.712189
[Epoch 72; Iter    24/   36] train: loss: 0.3038178
[Epoch 72] ogbg-molsider: 0.612534 val loss: 1.150662
[Epoch 72] ogbg-molsider: 0.592424 test loss: 0.956771
[Epoch 73; Iter    18/   36] train: loss: 0.2841740
[Epoch 73] ogbg-molsider: 0.603139 val loss: 0.820175
[Epoch 73] ogbg-molsider: 0.579861 test loss: 0.929789
[Epoch 74; Iter    12/   36] train: loss: 0.2699871
[Epoch 74] ogbg-molsider: 0.623681 val loss: 0.751786
[Epoch 74] ogbg-molsider: 0.586356 test loss: 0.825896
[Epoch 75; Iter     6/   36] train: loss: 0.2751633
[Epoch 75; Iter    36/   36] train: loss: 0.2826538
[Epoch 75] ogbg-molsider: 0.605683 val loss: 0.717778
[Epoch 75] ogbg-molsider: 0.595698 test loss: 0.777477
[Epoch 76; Iter    30/   36] train: loss: 0.3088587
[Epoch 76] ogbg-molsider: 0.606130 val loss: 0.768573
[Epoch 76] ogbg-molsider: 0.591317 test loss: 0.901121
[Epoch 77; Iter    24/   36] train: loss: 0.2765518
[Epoch 77] ogbg-molsider: 0.610900 val loss: 0.696445
[Epoch 77] ogbg-molsider: 0.597060 test loss: 0.771718
[Epoch 78; Iter    18/   36] train: loss: 0.2723641
[Epoch 78] ogbg-molsider: 0.601118 val loss: 0.752695
[Epoch 78] ogbg-molsider: 0.593919 test loss: 0.824624
[Epoch 79; Iter    12/   36] train: loss: 0.2311127
[Epoch 79] ogbg-molsider: 0.601512 val loss: 0.707388
[Epoch 79] ogbg-molsider: 0.596223 test loss: 0.787452
[Epoch 80; Iter     6/   36] train: loss: 0.2532997
[Epoch 80; Iter    36/   36] train: loss: 0.2607539
[Epoch 80] ogbg-molsider: 0.601128 val loss: 0.740503
[Epoch 80] ogbg-molsider: 0.589492 test loss: 0.822768
[Epoch 33] ogbg-molsider: 0.591104 test loss: 0.496526
[Epoch 34; Iter    12/   36] train: loss: 0.4931545
[Epoch 34] ogbg-molsider: 0.550739 val loss: 0.494599
[Epoch 34] ogbg-molsider: 0.587064 test loss: 0.502326
[Epoch 35; Iter     6/   36] train: loss: 0.5420672
[Epoch 35; Iter    36/   36] train: loss: 0.4796748
[Epoch 35] ogbg-molsider: 0.553559 val loss: 0.493572
[Epoch 35] ogbg-molsider: 0.588533 test loss: 0.497820
[Epoch 36; Iter    30/   36] train: loss: 0.4546924
[Epoch 36] ogbg-molsider: 0.576843 val loss: 0.501649
[Epoch 36] ogbg-molsider: 0.578870 test loss: 0.508955
[Epoch 37; Iter    24/   36] train: loss: 0.5302959
[Epoch 37] ogbg-molsider: 0.544617 val loss: 0.495211
[Epoch 37] ogbg-molsider: 0.576710 test loss: 0.500286
[Epoch 38; Iter    18/   36] train: loss: 0.5065804
[Epoch 38] ogbg-molsider: 0.553908 val loss: 0.489296
[Epoch 38] ogbg-molsider: 0.578405 test loss: 0.493161
[Epoch 39; Iter    12/   36] train: loss: 0.4934339
[Epoch 39] ogbg-molsider: 0.555661 val loss: 0.489707
[Epoch 39] ogbg-molsider: 0.587787 test loss: 0.494369
[Epoch 40; Iter     6/   36] train: loss: 0.4632750
[Epoch 40; Iter    36/   36] train: loss: 0.5026768
[Epoch 40] ogbg-molsider: 0.559111 val loss: 0.500888
[Epoch 40] ogbg-molsider: 0.551482 test loss: 0.504401
[Epoch 41; Iter    30/   36] train: loss: 0.5420316
[Epoch 41] ogbg-molsider: 0.590243 val loss: 0.488097
[Epoch 41] ogbg-molsider: 0.568906 test loss: 0.500093
[Epoch 42; Iter    24/   36] train: loss: 0.5042883
[Epoch 42] ogbg-molsider: 0.547287 val loss: 0.521821
[Epoch 42] ogbg-molsider: 0.573330 test loss: 0.534117
[Epoch 43; Iter    18/   36] train: loss: 0.4798081
[Epoch 43] ogbg-molsider: 0.536168 val loss: 0.561848
[Epoch 43] ogbg-molsider: 0.580841 test loss: 0.613102
[Epoch 44; Iter    12/   36] train: loss: 0.4643425
[Epoch 44] ogbg-molsider: 0.558500 val loss: 0.506530
[Epoch 44] ogbg-molsider: 0.596192 test loss: 0.504074
[Epoch 45; Iter     6/   36] train: loss: 0.4773766
[Epoch 45; Iter    36/   36] train: loss: 0.4804008
[Epoch 45] ogbg-molsider: 0.521000 val loss: 0.659815
[Epoch 45] ogbg-molsider: 0.581799 test loss: 0.585445
[Epoch 46; Iter    30/   36] train: loss: 0.4889081
[Epoch 46] ogbg-molsider: 0.569771 val loss: 0.525911
[Epoch 46] ogbg-molsider: 0.592166 test loss: 0.529803
[Epoch 47; Iter    24/   36] train: loss: 0.4797549
[Epoch 47] ogbg-molsider: 0.572678 val loss: 0.612926
[Epoch 47] ogbg-molsider: 0.572775 test loss: 0.528663
[Epoch 48; Iter    18/   36] train: loss: 0.4496341
[Epoch 48] ogbg-molsider: 0.589326 val loss: 0.490664
[Epoch 48] ogbg-molsider: 0.559569 test loss: 0.519504
[Epoch 49; Iter    12/   36] train: loss: 0.4796282
[Epoch 49] ogbg-molsider: 0.580659 val loss: 0.509459
[Epoch 49] ogbg-molsider: 0.583848 test loss: 0.523061
[Epoch 50; Iter     6/   36] train: loss: 0.4530174
[Epoch 50; Iter    36/   36] train: loss: 0.5065770
[Epoch 50] ogbg-molsider: 0.552250 val loss: 0.519848
[Epoch 50] ogbg-molsider: 0.567975 test loss: 0.556512
[Epoch 51; Iter    30/   36] train: loss: 0.4720766
[Epoch 51] ogbg-molsider: 0.513164 val loss: 0.554677
[Epoch 51] ogbg-molsider: 0.584529 test loss: 0.558091
[Epoch 52; Iter    24/   36] train: loss: 0.4555170
[Epoch 52] ogbg-molsider: 0.582399 val loss: 0.512630
[Epoch 52] ogbg-molsider: 0.573093 test loss: 0.522522
[Epoch 53; Iter    18/   36] train: loss: 0.4013370
[Epoch 53] ogbg-molsider: 0.543166 val loss: 0.547459
[Epoch 53] ogbg-molsider: 0.565760 test loss: 0.557688
[Epoch 54; Iter    12/   36] train: loss: 0.3877672
[Epoch 54] ogbg-molsider: 0.503694 val loss: 0.582797
[Epoch 54] ogbg-molsider: 0.588705 test loss: 0.605645
[Epoch 55; Iter     6/   36] train: loss: 0.4073238
[Epoch 55; Iter    36/   36] train: loss: 0.4014038
[Epoch 55] ogbg-molsider: 0.556265 val loss: 0.534779
[Epoch 55] ogbg-molsider: 0.571380 test loss: 0.541939
[Epoch 56; Iter    30/   36] train: loss: 0.4148118
[Epoch 56] ogbg-molsider: 0.551571 val loss: 0.532620
[Epoch 56] ogbg-molsider: 0.559546 test loss: 0.549802
[Epoch 57; Iter    24/   36] train: loss: 0.3727452
[Epoch 57] ogbg-molsider: 0.566486 val loss: 0.545780
[Epoch 57] ogbg-molsider: 0.579533 test loss: 0.536646
[Epoch 58; Iter    18/   36] train: loss: 0.3556589
[Epoch 58] ogbg-molsider: 0.528616 val loss: 0.568431
[Epoch 58] ogbg-molsider: 0.572282 test loss: 0.580777
[Epoch 59; Iter    12/   36] train: loss: 0.3780918
[Epoch 59] ogbg-molsider: 0.576954 val loss: 0.619723
[Epoch 59] ogbg-molsider: 0.576862 test loss: 0.783775
[Epoch 60; Iter     6/   36] train: loss: 0.3925347
[Epoch 60; Iter    36/   36] train: loss: 0.3374776
[Epoch 60] ogbg-molsider: 0.566294 val loss: 0.585854
[Epoch 60] ogbg-molsider: 0.565855 test loss: 0.599508
[Epoch 61; Iter    30/   36] train: loss: 0.3404265
[Epoch 61] ogbg-molsider: 0.571407 val loss: 0.561719
[Epoch 61] ogbg-molsider: 0.565552 test loss: 0.585840
[Epoch 62; Iter    24/   36] train: loss: 0.3555219
[Epoch 62] ogbg-molsider: 0.569276 val loss: 0.564712
[Epoch 62] ogbg-molsider: 0.593435 test loss: 0.581267
[Epoch 63; Iter    18/   36] train: loss: 0.3610861
[Epoch 63] ogbg-molsider: 0.565718 val loss: 0.581627
[Epoch 63] ogbg-molsider: 0.582484 test loss: 0.595429
[Epoch 64; Iter    12/   36] train: loss: 0.3717460
[Epoch 64] ogbg-molsider: 0.544593 val loss: 0.635713
[Epoch 64] ogbg-molsider: 0.581429 test loss: 0.700270
[Epoch 65; Iter     6/   36] train: loss: 0.3408993
[Epoch 65; Iter    36/   36] train: loss: 0.4922881
[Epoch 65] ogbg-molsider: 0.563993 val loss: 0.810386
[Epoch 65] ogbg-molsider: 0.597820 test loss: 0.659560
[Epoch 66; Iter    30/   36] train: loss: 0.3334802
[Epoch 66] ogbg-molsider: 0.567123 val loss: 7.687000
[Epoch 66] ogbg-molsider: 0.565344 test loss: 11.213361
[Epoch 67; Iter    24/   36] train: loss: 0.3534103
[Epoch 67] ogbg-molsider: 0.588528 val loss: 0.964423
[Epoch 67] ogbg-molsider: 0.578443 test loss: 1.003343
[Epoch 68; Iter    18/   36] train: loss: 0.3542331
[Epoch 68] ogbg-molsider: 0.537123 val loss: 0.603656
[Epoch 68] ogbg-molsider: 0.566964 test loss: 0.619266
[Epoch 69; Iter    12/   36] train: loss: 0.3169045
[Epoch 69] ogbg-molsider: 0.555427 val loss: 0.599540
[Epoch 69] ogbg-molsider: 0.598345 test loss: 0.595558
[Epoch 70; Iter     6/   36] train: loss: 0.3156208
[Epoch 70; Iter    36/   36] train: loss: 0.3217562
[Epoch 70] ogbg-molsider: 0.566359 val loss: 0.619859
[Epoch 70] ogbg-molsider: 0.592187 test loss: 0.596120
[Epoch 71; Iter    30/   36] train: loss: 0.3335518
[Epoch 71] ogbg-molsider: 0.588405 val loss: 0.589466
[Epoch 71] ogbg-molsider: 0.584004 test loss: 0.627743
[Epoch 72; Iter    24/   36] train: loss: 0.3153844
[Epoch 72] ogbg-molsider: 0.584224 val loss: 0.781275
[Epoch 72] ogbg-molsider: 0.581561 test loss: 0.709806
[Epoch 73; Iter    18/   36] train: loss: 0.3024222
[Epoch 73] ogbg-molsider: 0.574720 val loss: 2.925656
[Epoch 73] ogbg-molsider: 0.604688 test loss: 1.565462
[Epoch 74; Iter    12/   36] train: loss: 0.2822883
[Epoch 74] ogbg-molsider: 0.576711 val loss: 15.073905
[Epoch 74] ogbg-molsider: 0.589639 test loss: 13.909915
[Epoch 75; Iter     6/   36] train: loss: 0.2923640
[Epoch 75; Iter    36/   36] train: loss: 0.3196207
[Epoch 75] ogbg-molsider: 0.571638 val loss: 9.183738
[Epoch 75] ogbg-molsider: 0.592121 test loss: 5.358603
[Epoch 76; Iter    30/   36] train: loss: 0.3373254
[Epoch 76] ogbg-molsider: 0.603185 val loss: 15.046058
[Epoch 76] ogbg-molsider: 0.587953 test loss: 15.508810
[Epoch 77; Iter    24/   36] train: loss: 0.3304586
[Epoch 77] ogbg-molsider: 0.578414 val loss: 9.914263
[Epoch 77] ogbg-molsider: 0.576610 test loss: 6.975630
[Epoch 78; Iter    18/   36] train: loss: 0.2959866
[Epoch 78] ogbg-molsider: 0.584065 val loss: 24.943752
[Epoch 78] ogbg-molsider: 0.591866 test loss: 38.430464
[Epoch 79; Iter    12/   36] train: loss: 0.2780200
[Epoch 79] ogbg-molsider: 0.603512 val loss: 5.253667
[Epoch 79] ogbg-molsider: 0.600872 test loss: 3.441926
[Epoch 80; Iter     6/   36] train: loss: 0.2788540
[Epoch 80; Iter    36/   36] train: loss: 0.2894271
[Epoch 80] ogbg-molsider: 0.600033 val loss: 15.833739
[Epoch 80] ogbg-molsider: 0.566760 test loss: 18.438239
[Epoch 33] ogbg-molsider: 0.578066 test loss: 0.494631
[Epoch 34; Iter    12/   36] train: loss: 0.5028487
[Epoch 34] ogbg-molsider: 0.575222 val loss: 0.495868
[Epoch 34] ogbg-molsider: 0.569777 test loss: 0.517809
[Epoch 35; Iter     6/   36] train: loss: 0.5577927
[Epoch 35; Iter    36/   36] train: loss: 0.5282868
[Epoch 35] ogbg-molsider: 0.552472 val loss: 0.485193
[Epoch 35] ogbg-molsider: 0.583004 test loss: 0.494121
[Epoch 36; Iter    30/   36] train: loss: 0.4935181
[Epoch 36] ogbg-molsider: 0.556497 val loss: 0.483985
[Epoch 36] ogbg-molsider: 0.567330 test loss: 0.497057
[Epoch 37; Iter    24/   36] train: loss: 0.4635551
[Epoch 37] ogbg-molsider: 0.547525 val loss: 0.491302
[Epoch 37] ogbg-molsider: 0.581082 test loss: 0.502052
[Epoch 38; Iter    18/   36] train: loss: 0.4966551
[Epoch 38] ogbg-molsider: 0.550641 val loss: 0.479447
[Epoch 38] ogbg-molsider: 0.584113 test loss: 0.489999
[Epoch 39; Iter    12/   36] train: loss: 0.4960541
[Epoch 39] ogbg-molsider: 0.559171 val loss: 0.480048
[Epoch 39] ogbg-molsider: 0.575218 test loss: 0.493759
[Epoch 40; Iter     6/   36] train: loss: 0.4755753
[Epoch 40; Iter    36/   36] train: loss: 0.5288585
[Epoch 40] ogbg-molsider: 0.554547 val loss: 0.523878
[Epoch 40] ogbg-molsider: 0.556994 test loss: 0.542667
[Epoch 41; Iter    30/   36] train: loss: 0.5142316
[Epoch 41] ogbg-molsider: 0.572034 val loss: 0.500596
[Epoch 41] ogbg-molsider: 0.575908 test loss: 0.519972
[Epoch 42; Iter    24/   36] train: loss: 0.4979739
[Epoch 42] ogbg-molsider: 0.558820 val loss: 0.506442
[Epoch 42] ogbg-molsider: 0.553417 test loss: 0.524861
[Epoch 43; Iter    18/   36] train: loss: 0.4750948
[Epoch 43] ogbg-molsider: 0.571690 val loss: 3.377977
[Epoch 43] ogbg-molsider: 0.593130 test loss: 3.587797
[Epoch 44; Iter    12/   36] train: loss: 0.4828463
[Epoch 44] ogbg-molsider: 0.599831 val loss: 0.478489
[Epoch 44] ogbg-molsider: 0.579938 test loss: 0.500396
[Epoch 45; Iter     6/   36] train: loss: 0.4878806
[Epoch 45; Iter    36/   36] train: loss: 0.4393323
[Epoch 45] ogbg-molsider: 0.571396 val loss: 8.932899
[Epoch 45] ogbg-molsider: 0.593883 test loss: 7.358119
[Epoch 46; Iter    30/   36] train: loss: 0.5086451
[Epoch 46] ogbg-molsider: 0.582908 val loss: 0.481804
[Epoch 46] ogbg-molsider: 0.576387 test loss: 0.505720
[Epoch 47; Iter    24/   36] train: loss: 0.4652725
[Epoch 47] ogbg-molsider: 0.585796 val loss: 0.600420
[Epoch 47] ogbg-molsider: 0.570956 test loss: 0.643911
[Epoch 48; Iter    18/   36] train: loss: 0.4674944
[Epoch 48] ogbg-molsider: 0.569785 val loss: 0.912254
[Epoch 48] ogbg-molsider: 0.584704 test loss: 0.697033
[Epoch 49; Iter    12/   36] train: loss: 0.4680942
[Epoch 49] ogbg-molsider: 0.564474 val loss: 0.503130
[Epoch 49] ogbg-molsider: 0.552766 test loss: 0.520087
[Epoch 50; Iter     6/   36] train: loss: 0.4498886
[Epoch 50; Iter    36/   36] train: loss: 0.4744187
[Epoch 50] ogbg-molsider: 0.583213 val loss: 0.485611
[Epoch 50] ogbg-molsider: 0.581281 test loss: 0.512178
[Epoch 51; Iter    30/   36] train: loss: 0.4502165
[Epoch 51] ogbg-molsider: 0.591251 val loss: 0.499319
[Epoch 51] ogbg-molsider: 0.579653 test loss: 0.524608
[Epoch 52; Iter    24/   36] train: loss: 0.4064665
[Epoch 52] ogbg-molsider: 0.605494 val loss: 0.556966
[Epoch 52] ogbg-molsider: 0.600948 test loss: 0.578741
[Epoch 53; Iter    18/   36] train: loss: 0.4134136
[Epoch 53] ogbg-molsider: 0.568745 val loss: 0.498313
[Epoch 53] ogbg-molsider: 0.620269 test loss: 0.493571
[Epoch 54; Iter    12/   36] train: loss: 0.4106834
[Epoch 54] ogbg-molsider: 0.567008 val loss: 0.559086
[Epoch 54] ogbg-molsider: 0.576686 test loss: 0.553377
[Epoch 55; Iter     6/   36] train: loss: 0.3873043
[Epoch 55; Iter    36/   36] train: loss: 0.4187056
[Epoch 55] ogbg-molsider: 0.588855 val loss: 0.495811
[Epoch 55] ogbg-molsider: 0.613972 test loss: 0.491142
[Epoch 56; Iter    30/   36] train: loss: 0.3724863
[Epoch 56] ogbg-molsider: 0.583625 val loss: 0.603089
[Epoch 56] ogbg-molsider: 0.570041 test loss: 0.590683
[Epoch 57; Iter    24/   36] train: loss: 0.3870415
[Epoch 57] ogbg-molsider: 0.578806 val loss: 0.505679
[Epoch 57] ogbg-molsider: 0.609210 test loss: 0.540533
[Epoch 58; Iter    18/   36] train: loss: 0.4150282
[Epoch 58] ogbg-molsider: 0.597091 val loss: 0.565602
[Epoch 58] ogbg-molsider: 0.569582 test loss: 0.541551
[Epoch 59; Iter    12/   36] train: loss: 0.3484895
[Epoch 59] ogbg-molsider: 0.592739 val loss: 0.535455
[Epoch 59] ogbg-molsider: 0.570939 test loss: 0.557181
[Epoch 60; Iter     6/   36] train: loss: 0.3367391
[Epoch 60; Iter    36/   36] train: loss: 0.3894750
[Epoch 60] ogbg-molsider: 0.618549 val loss: 0.534407
[Epoch 60] ogbg-molsider: 0.591269 test loss: 0.597391
[Epoch 61; Iter    30/   36] train: loss: 0.3704709
[Epoch 61] ogbg-molsider: 0.605679 val loss: 0.504584
[Epoch 61] ogbg-molsider: 0.609391 test loss: 0.507408
[Epoch 62; Iter    24/   36] train: loss: 0.3462802
[Epoch 62] ogbg-molsider: 0.604198 val loss: 0.560983
[Epoch 62] ogbg-molsider: 0.593957 test loss: 0.567857
[Epoch 63; Iter    18/   36] train: loss: 0.3644783
[Epoch 63] ogbg-molsider: 0.576483 val loss: 0.574688
[Epoch 63] ogbg-molsider: 0.595926 test loss: 0.600400
[Epoch 64; Iter    12/   36] train: loss: 0.3283977
[Epoch 64] ogbg-molsider: 0.605133 val loss: 0.524640
[Epoch 64] ogbg-molsider: 0.586684 test loss: 0.546462
[Epoch 65; Iter     6/   36] train: loss: 0.2891418
[Epoch 65; Iter    36/   36] train: loss: 0.3147971
[Epoch 65] ogbg-molsider: 0.608795 val loss: 0.524601
[Epoch 65] ogbg-molsider: 0.607723 test loss: 0.516375
[Epoch 66; Iter    30/   36] train: loss: 0.3233026
[Epoch 66] ogbg-molsider: 0.611813 val loss: 0.571842
[Epoch 66] ogbg-molsider: 0.592680 test loss: 0.601990
[Epoch 67; Iter    24/   36] train: loss: 0.3141563
[Epoch 67] ogbg-molsider: 0.611751 val loss: 0.524532
[Epoch 67] ogbg-molsider: 0.608100 test loss: 0.542354
[Epoch 68; Iter    18/   36] train: loss: 0.3202114
[Epoch 68] ogbg-molsider: 0.620366 val loss: 0.547263
[Epoch 68] ogbg-molsider: 0.590536 test loss: 0.573301
[Epoch 69; Iter    12/   36] train: loss: 0.3346898
[Epoch 69] ogbg-molsider: 0.583036 val loss: 0.664878
[Epoch 69] ogbg-molsider: 0.616657 test loss: 0.650700
[Epoch 70; Iter     6/   36] train: loss: 0.3315750
[Epoch 70; Iter    36/   36] train: loss: 0.2962708
[Epoch 70] ogbg-molsider: 0.597018 val loss: 0.554684
[Epoch 70] ogbg-molsider: 0.603576 test loss: 0.557865
[Epoch 71; Iter    30/   36] train: loss: 0.3165675
[Epoch 71] ogbg-molsider: 0.600800 val loss: 0.605053
[Epoch 71] ogbg-molsider: 0.614168 test loss: 0.596011
[Epoch 72; Iter    24/   36] train: loss: 0.3229465
[Epoch 72] ogbg-molsider: 0.622999 val loss: 0.546968
[Epoch 72] ogbg-molsider: 0.601803 test loss: 0.559949
[Epoch 73; Iter    18/   36] train: loss: 0.2873173
[Epoch 73] ogbg-molsider: 0.575671 val loss: 0.612279
[Epoch 73] ogbg-molsider: 0.577230 test loss: 0.627277
[Epoch 74; Iter    12/   36] train: loss: 0.2911572
[Epoch 74] ogbg-molsider: 0.571365 val loss: 0.579284
[Epoch 74] ogbg-molsider: 0.632812 test loss: 0.565937
[Epoch 75; Iter     6/   36] train: loss: 0.3443816
[Epoch 75; Iter    36/   36] train: loss: 0.3448248
[Epoch 75] ogbg-molsider: 0.572513 val loss: 0.619882
[Epoch 75] ogbg-molsider: 0.590690 test loss: 0.599067
[Epoch 76; Iter    30/   36] train: loss: 0.2754491
[Epoch 76] ogbg-molsider: 0.578637 val loss: 0.605222
[Epoch 76] ogbg-molsider: 0.589215 test loss: 0.590927
[Epoch 77; Iter    24/   36] train: loss: 0.2975910
[Epoch 77] ogbg-molsider: 0.580419 val loss: 0.602551
[Epoch 77] ogbg-molsider: 0.588178 test loss: 0.586420
[Epoch 78; Iter    18/   36] train: loss: 0.2841230
[Epoch 78] ogbg-molsider: 0.585380 val loss: 0.608451
[Epoch 78] ogbg-molsider: 0.596208 test loss: 0.586449
[Epoch 79; Iter    12/   36] train: loss: 0.2630614
[Epoch 79] ogbg-molsider: 0.597954 val loss: 0.576884
[Epoch 79] ogbg-molsider: 0.600047 test loss: 0.576248
[Epoch 80; Iter     6/   36] train: loss: 0.3031992
[Epoch 80; Iter    36/   36] train: loss: 0.2754010
[Epoch 80] ogbg-molsider: 0.591526 val loss: 0.602480
[Epoch 80] ogbg-molsider: 0.605799 test loss: 0.602867
[Epoch 33] ogbg-molsider: 0.574613 test loss: 0.509481
[Epoch 34; Iter    12/   36] train: loss: 0.5024689
[Epoch 34] ogbg-molsider: 0.552582 val loss: 0.516224
[Epoch 34] ogbg-molsider: 0.564710 test loss: 0.535927
[Epoch 35; Iter     6/   36] train: loss: 0.5086950
[Epoch 35; Iter    36/   36] train: loss: 0.5370409
[Epoch 35] ogbg-molsider: 0.553729 val loss: 3.888689
[Epoch 35] ogbg-molsider: 0.568789 test loss: 4.235379
[Epoch 36; Iter    30/   36] train: loss: 0.4835199
[Epoch 36] ogbg-molsider: 0.548521 val loss: 0.632733
[Epoch 36] ogbg-molsider: 0.577899 test loss: 0.612029
[Epoch 37; Iter    24/   36] train: loss: 0.4915722
[Epoch 37] ogbg-molsider: 0.551587 val loss: 0.730880
[Epoch 37] ogbg-molsider: 0.564903 test loss: 0.643341
[Epoch 38; Iter    18/   36] train: loss: 0.4858198
[Epoch 38] ogbg-molsider: 0.565149 val loss: 1.903368
[Epoch 38] ogbg-molsider: 0.559974 test loss: 1.302899
[Epoch 39; Iter    12/   36] train: loss: 0.5244928
[Epoch 39] ogbg-molsider: 0.565748 val loss: 3.742888
[Epoch 39] ogbg-molsider: 0.565170 test loss: 3.094362
[Epoch 40; Iter     6/   36] train: loss: 0.5232447
[Epoch 40; Iter    36/   36] train: loss: 0.4961903
[Epoch 40] ogbg-molsider: 0.583244 val loss: 1.228805
[Epoch 40] ogbg-molsider: 0.578501 test loss: 1.039039
[Epoch 41; Iter    30/   36] train: loss: 0.4936367
[Epoch 41] ogbg-molsider: 0.574259 val loss: 2.845087
[Epoch 41] ogbg-molsider: 0.562019 test loss: 1.633813
[Epoch 42; Iter    24/   36] train: loss: 0.4370870
[Epoch 42] ogbg-molsider: 0.585661 val loss: 0.527835
[Epoch 42] ogbg-molsider: 0.591433 test loss: 0.625006
[Epoch 43; Iter    18/   36] train: loss: 0.4658282
[Epoch 43] ogbg-molsider: 0.595342 val loss: 1.341394
[Epoch 43] ogbg-molsider: 0.606522 test loss: 0.966897
[Epoch 44; Iter    12/   36] train: loss: 0.4435672
[Epoch 44] ogbg-molsider: 0.569661 val loss: 0.530939
[Epoch 44] ogbg-molsider: 0.567691 test loss: 0.539140
[Epoch 45; Iter     6/   36] train: loss: 0.4283028
[Epoch 45; Iter    36/   36] train: loss: 0.4395587
[Epoch 45] ogbg-molsider: 0.583738 val loss: 0.613133
[Epoch 45] ogbg-molsider: 0.570982 test loss: 0.790156
[Epoch 46; Iter    30/   36] train: loss: 0.4561227
[Epoch 46] ogbg-molsider: 0.574659 val loss: 0.513779
[Epoch 46] ogbg-molsider: 0.611960 test loss: 0.523862
[Epoch 47; Iter    24/   36] train: loss: 0.4314358
[Epoch 47] ogbg-molsider: 0.591407 val loss: 0.880987
[Epoch 47] ogbg-molsider: 0.572702 test loss: 0.959611
[Epoch 48; Iter    18/   36] train: loss: 0.5035774
[Epoch 48] ogbg-molsider: 0.605146 val loss: 0.493973
[Epoch 48] ogbg-molsider: 0.596021 test loss: 0.519258
[Epoch 49; Iter    12/   36] train: loss: 0.4035545
[Epoch 49] ogbg-molsider: 0.562269 val loss: 0.588506
[Epoch 49] ogbg-molsider: 0.582636 test loss: 0.673382
[Epoch 50; Iter     6/   36] train: loss: 0.4247988
[Epoch 50; Iter    36/   36] train: loss: 0.4665616
[Epoch 50] ogbg-molsider: 0.603537 val loss: 0.746601
[Epoch 50] ogbg-molsider: 0.607036 test loss: 0.594638
[Epoch 51; Iter    30/   36] train: loss: 0.4047167
[Epoch 51] ogbg-molsider: 0.565272 val loss: 1.347934
[Epoch 51] ogbg-molsider: 0.614510 test loss: 1.648627
[Epoch 52; Iter    24/   36] train: loss: 0.4164271
[Epoch 52] ogbg-molsider: 0.594864 val loss: 2.297092
[Epoch 52] ogbg-molsider: 0.605436 test loss: 2.027224
[Epoch 53; Iter    18/   36] train: loss: 0.3799230
[Epoch 53] ogbg-molsider: 0.602260 val loss: 1.095222
[Epoch 53] ogbg-molsider: 0.562972 test loss: 1.002305
[Epoch 54; Iter    12/   36] train: loss: 0.3754699
[Epoch 54] ogbg-molsider: 0.594272 val loss: 2.384776
[Epoch 54] ogbg-molsider: 0.586282 test loss: 2.357479
[Epoch 55; Iter     6/   36] train: loss: 0.3583062
[Epoch 55; Iter    36/   36] train: loss: 0.4452437
[Epoch 55] ogbg-molsider: 0.619658 val loss: 0.789148
[Epoch 55] ogbg-molsider: 0.582016 test loss: 0.682629
[Epoch 56; Iter    30/   36] train: loss: 0.3512666
[Epoch 56] ogbg-molsider: 0.594708 val loss: 2.204326
[Epoch 56] ogbg-molsider: 0.582079 test loss: 2.606086
[Epoch 57; Iter    24/   36] train: loss: 0.3469204
[Epoch 57] ogbg-molsider: 0.590513 val loss: 0.848214
[Epoch 57] ogbg-molsider: 0.602632 test loss: 0.704955
[Epoch 58; Iter    18/   36] train: loss: 0.3645688
[Epoch 58] ogbg-molsider: 0.596938 val loss: 2.470351
[Epoch 58] ogbg-molsider: 0.599210 test loss: 3.297258
[Epoch 59; Iter    12/   36] train: loss: 0.3183859
[Epoch 59] ogbg-molsider: 0.579420 val loss: 1.936337
[Epoch 59] ogbg-molsider: 0.589931 test loss: 2.657685
[Epoch 60; Iter     6/   36] train: loss: 0.3758731
[Epoch 60; Iter    36/   36] train: loss: 0.3445139
[Epoch 60] ogbg-molsider: 0.547471 val loss: 7.482490
[Epoch 60] ogbg-molsider: 0.533019 test loss: 12.321584
[Epoch 61; Iter    30/   36] train: loss: 0.3834875
[Epoch 61] ogbg-molsider: 0.553939 val loss: 3.939080
[Epoch 61] ogbg-molsider: 0.570264 test loss: 5.845920
[Epoch 62; Iter    24/   36] train: loss: 0.3502240
[Epoch 62] ogbg-molsider: 0.589731 val loss: 1.038491
[Epoch 62] ogbg-molsider: 0.585213 test loss: 1.100494
[Epoch 63; Iter    18/   36] train: loss: 0.3679210
[Epoch 63] ogbg-molsider: 0.596039 val loss: 3.761689
[Epoch 63] ogbg-molsider: 0.586058 test loss: 5.392581
[Epoch 64; Iter    12/   36] train: loss: 0.3294005
[Epoch 64] ogbg-molsider: 0.584534 val loss: 4.290279
[Epoch 64] ogbg-molsider: 0.593116 test loss: 5.991270
[Epoch 65; Iter     6/   36] train: loss: 0.3663009
[Epoch 65; Iter    36/   36] train: loss: 0.3354325
[Epoch 65] ogbg-molsider: 0.581240 val loss: 3.864796
[Epoch 65] ogbg-molsider: 0.571106 test loss: 5.192731
[Epoch 66; Iter    30/   36] train: loss: 0.2963192
[Epoch 66] ogbg-molsider: 0.569232 val loss: 4.176601
[Epoch 66] ogbg-molsider: 0.600226 test loss: 6.128069
[Epoch 67; Iter    24/   36] train: loss: 0.3116549
[Epoch 67] ogbg-molsider: 0.576437 val loss: 3.038136
[Epoch 67] ogbg-molsider: 0.604725 test loss: 4.029495
[Epoch 68; Iter    18/   36] train: loss: 0.3037755
[Epoch 68] ogbg-molsider: 0.560541 val loss: 3.695875
[Epoch 68] ogbg-molsider: 0.603474 test loss: 5.415431
[Epoch 69; Iter    12/   36] train: loss: 0.3008327
[Epoch 69] ogbg-molsider: 0.586781 val loss: 2.641582
[Epoch 69] ogbg-molsider: 0.572496 test loss: 3.986008
[Epoch 70; Iter     6/   36] train: loss: 0.2964329
[Epoch 70; Iter    36/   36] train: loss: 0.3574950
[Epoch 70] ogbg-molsider: 0.581130 val loss: 1.348837
[Epoch 70] ogbg-molsider: 0.601682 test loss: 2.089667
[Epoch 71; Iter    30/   36] train: loss: 0.3157823
[Epoch 71] ogbg-molsider: 0.576045 val loss: 1.794126
[Epoch 71] ogbg-molsider: 0.597172 test loss: 2.669190
[Epoch 72; Iter    24/   36] train: loss: 0.3042345
[Epoch 72] ogbg-molsider: 0.561620 val loss: 4.892350
[Epoch 72] ogbg-molsider: 0.588317 test loss: 7.748461
[Epoch 73; Iter    18/   36] train: loss: 0.3155041
[Epoch 73] ogbg-molsider: 0.593256 val loss: 1.949100
[Epoch 73] ogbg-molsider: 0.608235 test loss: 3.057001
[Epoch 74; Iter    12/   36] train: loss: 0.3057262
[Epoch 74] ogbg-molsider: 0.580173 val loss: 1.475865
[Epoch 74] ogbg-molsider: 0.561641 test loss: 2.464081
[Epoch 75; Iter     6/   36] train: loss: 0.2896829
[Epoch 75; Iter    36/   36] train: loss: 0.2993392
[Epoch 75] ogbg-molsider: 0.581505 val loss: 7.163650
[Epoch 75] ogbg-molsider: 0.579842 test loss: 14.893637
[Epoch 76; Iter    30/   36] train: loss: 0.2955031
[Epoch 76] ogbg-molsider: 0.558776 val loss: 0.856659
[Epoch 76] ogbg-molsider: 0.581055 test loss: 1.064337
[Epoch 77; Iter    24/   36] train: loss: 0.2729054
[Epoch 77] ogbg-molsider: 0.583004 val loss: 0.824718
[Epoch 77] ogbg-molsider: 0.600613 test loss: 1.051282
[Epoch 78; Iter    18/   36] train: loss: 0.2864934
[Epoch 78] ogbg-molsider: 0.574065 val loss: 0.879823
[Epoch 78] ogbg-molsider: 0.579737 test loss: 1.285687
[Epoch 79; Iter    12/   36] train: loss: 0.2808469
[Epoch 79] ogbg-molsider: 0.588345 val loss: 0.849357
[Epoch 79] ogbg-molsider: 0.593631 test loss: 1.160266
[Epoch 80; Iter     6/   36] train: loss: 0.2751534
[Epoch 80; Iter    36/   36] train: loss: 0.3092296
[Epoch 80] ogbg-molsider: 0.557581 val loss: 1.083677
[Epoch 80] ogbg-molsider: 0.581577 test loss: 1.525559
[Epoch 33] ogbg-molsider: 0.564885 test loss: 0.557526
[Epoch 34; Iter    12/   36] train: loss: 0.5189940
[Epoch 34] ogbg-molsider: 0.499362 val loss: 0.672419
[Epoch 34] ogbg-molsider: 0.560798 test loss: 0.737855
[Epoch 35; Iter     6/   36] train: loss: 0.5595964
[Epoch 35; Iter    36/   36] train: loss: 0.5047923
[Epoch 35] ogbg-molsider: 0.517181 val loss: 0.738885
[Epoch 35] ogbg-molsider: 0.543248 test loss: 0.951007
[Epoch 36; Iter    30/   36] train: loss: 0.4853509
[Epoch 36] ogbg-molsider: 0.501664 val loss: 0.771642
[Epoch 36] ogbg-molsider: 0.537818 test loss: 1.039159
[Epoch 37; Iter    24/   36] train: loss: 0.4706142
[Epoch 37] ogbg-molsider: 0.499464 val loss: 0.682225
[Epoch 37] ogbg-molsider: 0.578983 test loss: 0.963877
[Epoch 38; Iter    18/   36] train: loss: 0.4968893
[Epoch 38] ogbg-molsider: 0.508045 val loss: 0.689171
[Epoch 38] ogbg-molsider: 0.578464 test loss: 1.014882
[Epoch 39; Iter    12/   36] train: loss: 0.5126872
[Epoch 39] ogbg-molsider: 0.527927 val loss: 1.061460
[Epoch 39] ogbg-molsider: 0.564934 test loss: 1.257980
[Epoch 40; Iter     6/   36] train: loss: 0.4797317
[Epoch 40; Iter    36/   36] train: loss: 0.5312054
[Epoch 40] ogbg-molsider: 0.532671 val loss: 2.268527
[Epoch 40] ogbg-molsider: 0.550703 test loss: 2.997604
[Epoch 41; Iter    30/   36] train: loss: 0.5069129
[Epoch 41] ogbg-molsider: 0.534556 val loss: 0.845875
[Epoch 41] ogbg-molsider: 0.571461 test loss: 0.547562
[Epoch 42; Iter    24/   36] train: loss: 0.4866025
[Epoch 42] ogbg-molsider: 0.552496 val loss: 1.013851
[Epoch 42] ogbg-molsider: 0.526781 test loss: 0.821588
[Epoch 43; Iter    18/   36] train: loss: 0.4665123
[Epoch 43] ogbg-molsider: 0.523690 val loss: 0.569253
[Epoch 43] ogbg-molsider: 0.570552 test loss: 0.549875
[Epoch 44; Iter    12/   36] train: loss: 0.4231973
[Epoch 44] ogbg-molsider: 0.522919 val loss: 0.610674
[Epoch 44] ogbg-molsider: 0.529878 test loss: 0.612502
[Epoch 45; Iter     6/   36] train: loss: 0.4676546
[Epoch 45; Iter    36/   36] train: loss: 0.4421904
[Epoch 45] ogbg-molsider: 0.511116 val loss: 0.733538
[Epoch 45] ogbg-molsider: 0.544472 test loss: 0.724718
[Epoch 46; Iter    30/   36] train: loss: 0.4672176
[Epoch 46] ogbg-molsider: 0.549307 val loss: 0.643498
[Epoch 46] ogbg-molsider: 0.573818 test loss: 0.726928
[Epoch 47; Iter    24/   36] train: loss: 0.4268904
[Epoch 47] ogbg-molsider: 0.549405 val loss: 0.677187
[Epoch 47] ogbg-molsider: 0.530577 test loss: 0.765501
[Epoch 48; Iter    18/   36] train: loss: 0.4195068
[Epoch 48] ogbg-molsider: 0.544557 val loss: 0.679650
[Epoch 48] ogbg-molsider: 0.515258 test loss: 0.737451
[Epoch 49; Iter    12/   36] train: loss: 0.4119743
[Epoch 49] ogbg-molsider: 0.512993 val loss: 0.835536
[Epoch 49] ogbg-molsider: 0.562215 test loss: 0.720461
[Epoch 50; Iter     6/   36] train: loss: 0.4011675
[Epoch 50; Iter    36/   36] train: loss: 0.4143757
[Epoch 50] ogbg-molsider: 0.519067 val loss: 0.875667
[Epoch 50] ogbg-molsider: 0.555113 test loss: 0.736473
[Epoch 51; Iter    30/   36] train: loss: 0.3839414
[Epoch 51] ogbg-molsider: 0.497954 val loss: 0.962645
[Epoch 51] ogbg-molsider: 0.553576 test loss: 0.811591
[Epoch 52; Iter    24/   36] train: loss: 0.3492676
[Epoch 52] ogbg-molsider: 0.525854 val loss: 5.127809
[Epoch 52] ogbg-molsider: 0.530038 test loss: 5.637374
[Epoch 53; Iter    18/   36] train: loss: 0.3765721
[Epoch 53] ogbg-molsider: 0.540333 val loss: 0.603792
[Epoch 53] ogbg-molsider: 0.544441 test loss: 0.639200
[Epoch 54; Iter    12/   36] train: loss: 0.3873826
[Epoch 54] ogbg-molsider: 0.519382 val loss: 0.659186
[Epoch 54] ogbg-molsider: 0.549385 test loss: 0.697266
[Epoch 55; Iter     6/   36] train: loss: 0.3461997
[Epoch 55; Iter    36/   36] train: loss: 0.3880362
[Epoch 55] ogbg-molsider: 0.528602 val loss: 0.563184
[Epoch 55] ogbg-molsider: 0.580858 test loss: 0.568598
[Epoch 56; Iter    30/   36] train: loss: 0.3141284
[Epoch 56] ogbg-molsider: 0.514008 val loss: 0.685289
[Epoch 56] ogbg-molsider: 0.560244 test loss: 0.725076
[Epoch 57; Iter    24/   36] train: loss: 0.3384565
[Epoch 57] ogbg-molsider: 0.519439 val loss: 0.578621
[Epoch 57] ogbg-molsider: 0.590214 test loss: 0.602497
[Epoch 58; Iter    18/   36] train: loss: 0.3821918
[Epoch 58] ogbg-molsider: 0.531264 val loss: 0.626940
[Epoch 58] ogbg-molsider: 0.558092 test loss: 0.702549
[Epoch 59; Iter    12/   36] train: loss: 0.3210087
[Epoch 59] ogbg-molsider: 0.527108 val loss: 0.853087
[Epoch 59] ogbg-molsider: 0.545190 test loss: 1.310378
[Epoch 60; Iter     6/   36] train: loss: 0.3220975
[Epoch 60; Iter    36/   36] train: loss: 0.3838820
[Epoch 60] ogbg-molsider: 0.511976 val loss: 0.634787
[Epoch 60] ogbg-molsider: 0.556007 test loss: 0.705280
[Epoch 61; Iter    30/   36] train: loss: 0.3481271
[Epoch 61] ogbg-molsider: 0.524770 val loss: 0.725779
[Epoch 61] ogbg-molsider: 0.567101 test loss: 0.707222
[Epoch 62; Iter    24/   36] train: loss: 0.3276557
[Epoch 62] ogbg-molsider: 0.520861 val loss: 1.633962
[Epoch 62] ogbg-molsider: 0.586744 test loss: 1.182407
[Epoch 63; Iter    18/   36] train: loss: 0.3290057
[Epoch 63] ogbg-molsider: 0.500240 val loss: 1.869088
[Epoch 63] ogbg-molsider: 0.545884 test loss: 1.299464
[Epoch 64; Iter    12/   36] train: loss: 0.2988037
[Epoch 64] ogbg-molsider: 0.527060 val loss: 0.616717
[Epoch 64] ogbg-molsider: 0.561635 test loss: 0.623630
[Epoch 65; Iter     6/   36] train: loss: 0.2793829
[Epoch 65; Iter    36/   36] train: loss: 0.3106732
[Epoch 65] ogbg-molsider: 0.538267 val loss: 0.619909
[Epoch 65] ogbg-molsider: 0.564316 test loss: 0.635692
[Epoch 66; Iter    30/   36] train: loss: 0.3065869
[Epoch 66] ogbg-molsider: 0.538203 val loss: 0.693294
[Epoch 66] ogbg-molsider: 0.568660 test loss: 0.797388
[Epoch 67; Iter    24/   36] train: loss: 0.2890984
[Epoch 67] ogbg-molsider: 0.524898 val loss: 0.667665
[Epoch 67] ogbg-molsider: 0.576135 test loss: 0.687933
[Epoch 68; Iter    18/   36] train: loss: 0.2952566
[Epoch 68] ogbg-molsider: 0.529366 val loss: 0.933437
[Epoch 68] ogbg-molsider: 0.566114 test loss: 0.838011
[Epoch 69; Iter    12/   36] train: loss: 0.3105213
[Epoch 69] ogbg-molsider: 0.524658 val loss: 0.646771
[Epoch 69] ogbg-molsider: 0.549711 test loss: 0.692979
[Epoch 70; Iter     6/   36] train: loss: 0.2879993
[Epoch 70; Iter    36/   36] train: loss: 0.2897082
[Epoch 70] ogbg-molsider: 0.545821 val loss: 0.739999
[Epoch 70] ogbg-molsider: 0.553348 test loss: 0.808835
[Epoch 71; Iter    30/   36] train: loss: 0.2882714
[Epoch 71] ogbg-molsider: 0.515757 val loss: 0.644598
[Epoch 71] ogbg-molsider: 0.572256 test loss: 0.646055
[Epoch 72; Iter    24/   36] train: loss: 0.2839350
[Epoch 72] ogbg-molsider: 0.536685 val loss: 1.703511
[Epoch 72] ogbg-molsider: 0.567176 test loss: 1.614935
[Epoch 73; Iter    18/   36] train: loss: 0.2617252
[Epoch 73] ogbg-molsider: 0.541384 val loss: 0.641810
[Epoch 73] ogbg-molsider: 0.558527 test loss: 0.703316
[Epoch 74; Iter    12/   36] train: loss: 0.2396721
[Epoch 74] ogbg-molsider: 0.527637 val loss: 0.952124
[Epoch 74] ogbg-molsider: 0.576041 test loss: 1.363628
[Epoch 75; Iter     6/   36] train: loss: 0.2814824
[Epoch 75; Iter    36/   36] train: loss: 0.2720510
[Epoch 75] ogbg-molsider: 0.533056 val loss: 0.696270
[Epoch 75] ogbg-molsider: 0.573948 test loss: 0.713175
[Epoch 76; Iter    30/   36] train: loss: 0.2573602
[Epoch 76] ogbg-molsider: 0.520215 val loss: 0.860072
[Epoch 76] ogbg-molsider: 0.583281 test loss: 0.932389
[Epoch 77; Iter    24/   36] train: loss: 0.2789395
[Epoch 77] ogbg-molsider: 0.515191 val loss: 0.796986
[Epoch 77] ogbg-molsider: 0.563367 test loss: 0.783160
[Epoch 78; Iter    18/   36] train: loss: 0.2606159
[Epoch 78] ogbg-molsider: 0.519703 val loss: 0.712940
[Epoch 78] ogbg-molsider: 0.558763 test loss: 0.956989
[Epoch 79; Iter    12/   36] train: loss: 0.2240784
[Epoch 79] ogbg-molsider: 0.517263 val loss: 0.712401
[Epoch 79] ogbg-molsider: 0.575280 test loss: 0.865239
[Epoch 80; Iter     6/   36] train: loss: 0.2468381
[Epoch 80; Iter    36/   36] train: loss: 0.2439855
[Epoch 80] ogbg-molsider: 0.525003 val loss: 0.734064
[Epoch 80] ogbg-molsider: 0.565953 test loss: 0.888839
[Epoch 33] ogbg-molsider: 0.593162 test loss: 0.496230
[Epoch 34; Iter    12/   36] train: loss: 0.5034366
[Epoch 34] ogbg-molsider: 0.539523 val loss: 0.484527
[Epoch 34] ogbg-molsider: 0.593272 test loss: 0.486784
[Epoch 35; Iter     6/   36] train: loss: 0.5453494
[Epoch 35; Iter    36/   36] train: loss: 0.5220161
[Epoch 35] ogbg-molsider: 0.559671 val loss: 0.499636
[Epoch 35] ogbg-molsider: 0.560506 test loss: 0.506455
[Epoch 36; Iter    30/   36] train: loss: 0.4824426
[Epoch 36] ogbg-molsider: 0.550925 val loss: 0.487786
[Epoch 36] ogbg-molsider: 0.592028 test loss: 0.488153
[Epoch 37; Iter    24/   36] train: loss: 0.4623833
[Epoch 37] ogbg-molsider: 0.558188 val loss: 0.489890
[Epoch 37] ogbg-molsider: 0.592316 test loss: 0.497286
[Epoch 38; Iter    18/   36] train: loss: 0.4885163
[Epoch 38] ogbg-molsider: 0.551277 val loss: 0.491322
[Epoch 38] ogbg-molsider: 0.581326 test loss: 0.499195
[Epoch 39; Iter    12/   36] train: loss: 0.5198227
[Epoch 39] ogbg-molsider: 0.537999 val loss: 0.500875
[Epoch 39] ogbg-molsider: 0.592910 test loss: 0.509165
[Epoch 40; Iter     6/   36] train: loss: 0.4929839
[Epoch 40; Iter    36/   36] train: loss: 0.5622450
[Epoch 40] ogbg-molsider: 0.518278 val loss: 0.597427
[Epoch 40] ogbg-molsider: 0.577768 test loss: 0.532715
[Epoch 41; Iter    30/   36] train: loss: 0.5228524
[Epoch 41] ogbg-molsider: 0.585313 val loss: 0.481511
[Epoch 41] ogbg-molsider: 0.591193 test loss: 0.502953
[Epoch 42; Iter    24/   36] train: loss: 0.4929305
[Epoch 42] ogbg-molsider: 0.588281 val loss: 0.479741
[Epoch 42] ogbg-molsider: 0.593535 test loss: 0.492325
[Epoch 43; Iter    18/   36] train: loss: 0.4788318
[Epoch 43] ogbg-molsider: 0.538216 val loss: 0.533228
[Epoch 43] ogbg-molsider: 0.568890 test loss: 0.557147
[Epoch 44; Iter    12/   36] train: loss: 0.4831629
[Epoch 44] ogbg-molsider: 0.546734 val loss: 0.509090
[Epoch 44] ogbg-molsider: 0.613722 test loss: 0.519487
[Epoch 45; Iter     6/   36] train: loss: 0.4789136
[Epoch 45; Iter    36/   36] train: loss: 0.4614739
[Epoch 45] ogbg-molsider: 0.560938 val loss: 0.534721
[Epoch 45] ogbg-molsider: 0.568598 test loss: 0.543196
[Epoch 46; Iter    30/   36] train: loss: 0.5070369
[Epoch 46] ogbg-molsider: 0.565266 val loss: 0.622255
[Epoch 46] ogbg-molsider: 0.573600 test loss: 0.626827
[Epoch 47; Iter    24/   36] train: loss: 0.4894539
[Epoch 47] ogbg-molsider: 0.477099 val loss: 15.881330
[Epoch 47] ogbg-molsider: 0.539264 test loss: 13.017663
[Epoch 48; Iter    18/   36] train: loss: 0.5262572
[Epoch 48] ogbg-molsider: 0.552140 val loss: 0.490737
[Epoch 48] ogbg-molsider: 0.613283 test loss: 0.493846
[Epoch 49; Iter    12/   36] train: loss: 0.5017072
[Epoch 49] ogbg-molsider: 0.541884 val loss: 1.451184
[Epoch 49] ogbg-molsider: 0.605108 test loss: 1.661304
[Epoch 50; Iter     6/   36] train: loss: 0.4979999
[Epoch 50; Iter    36/   36] train: loss: 0.5507227
[Epoch 50] ogbg-molsider: 0.558696 val loss: 0.503002
[Epoch 50] ogbg-molsider: 0.576845 test loss: 0.527348
[Epoch 51; Iter    30/   36] train: loss: 0.5000504
[Epoch 51] ogbg-molsider: 0.531790 val loss: 0.537095
[Epoch 51] ogbg-molsider: 0.575676 test loss: 0.526432
[Epoch 52; Iter    24/   36] train: loss: 0.4533675
[Epoch 52] ogbg-molsider: 0.584268 val loss: 0.512780
[Epoch 52] ogbg-molsider: 0.575879 test loss: 0.507441
[Epoch 53; Iter    18/   36] train: loss: 0.4567888
[Epoch 53] ogbg-molsider: 0.552106 val loss: 0.518124
[Epoch 53] ogbg-molsider: 0.590487 test loss: 0.532496
[Epoch 54; Iter    12/   36] train: loss: 0.4506202
[Epoch 54] ogbg-molsider: 0.567322 val loss: 0.544526
[Epoch 54] ogbg-molsider: 0.558988 test loss: 0.578613
[Epoch 55; Iter     6/   36] train: loss: 0.4595558
[Epoch 55; Iter    36/   36] train: loss: 0.4691242
[Epoch 55] ogbg-molsider: 0.572768 val loss: 0.527474
[Epoch 55] ogbg-molsider: 0.584878 test loss: 0.511166
[Epoch 56; Iter    30/   36] train: loss: 0.4291907
[Epoch 56] ogbg-molsider: 0.568558 val loss: 0.534623
[Epoch 56] ogbg-molsider: 0.541324 test loss: 0.538591
[Epoch 57; Iter    24/   36] train: loss: 0.4304692
[Epoch 57] ogbg-molsider: 0.551383 val loss: 0.508378
[Epoch 57] ogbg-molsider: 0.614435 test loss: 0.515403
[Epoch 58; Iter    18/   36] train: loss: 0.4306093
[Epoch 58] ogbg-molsider: 0.548650 val loss: 0.535638
[Epoch 58] ogbg-molsider: 0.573300 test loss: 0.553433
[Epoch 59; Iter    12/   36] train: loss: 0.3728609
[Epoch 59] ogbg-molsider: 0.593824 val loss: 0.546445
[Epoch 59] ogbg-molsider: 0.569021 test loss: 0.573103
[Epoch 60; Iter     6/   36] train: loss: 0.3612331
[Epoch 60; Iter    36/   36] train: loss: 0.4219599
[Epoch 60] ogbg-molsider: 0.564912 val loss: 0.540299
[Epoch 60] ogbg-molsider: 0.585932 test loss: 0.559688
[Epoch 61; Iter    30/   36] train: loss: 0.4611185
[Epoch 61] ogbg-molsider: 0.590157 val loss: 0.565159
[Epoch 61] ogbg-molsider: 0.545612 test loss: 0.597230
[Epoch 62; Iter    24/   36] train: loss: 0.4156791
[Epoch 62] ogbg-molsider: 0.577334 val loss: 0.513430
[Epoch 62] ogbg-molsider: 0.583560 test loss: 0.550995
[Epoch 63; Iter    18/   36] train: loss: 0.4106409
[Epoch 63] ogbg-molsider: 0.595386 val loss: 0.522203
[Epoch 63] ogbg-molsider: 0.558214 test loss: 0.566570
[Epoch 64; Iter    12/   36] train: loss: 0.3670826
[Epoch 64] ogbg-molsider: 0.634044 val loss: 0.527151
[Epoch 64] ogbg-molsider: 0.553828 test loss: 0.559006
[Epoch 65; Iter     6/   36] train: loss: 0.3135706
[Epoch 65; Iter    36/   36] train: loss: 0.3557886
[Epoch 65] ogbg-molsider: 0.611623 val loss: 0.528146
[Epoch 65] ogbg-molsider: 0.556459 test loss: 0.578133
[Epoch 66; Iter    30/   36] train: loss: 0.3338821
[Epoch 66] ogbg-molsider: 0.590992 val loss: 0.582839
[Epoch 66] ogbg-molsider: 0.573359 test loss: 0.598488
[Epoch 67; Iter    24/   36] train: loss: 0.3351547
[Epoch 67] ogbg-molsider: 0.619503 val loss: 0.515254
[Epoch 67] ogbg-molsider: 0.561266 test loss: 0.566970
[Epoch 68; Iter    18/   36] train: loss: 0.3423686
[Epoch 68] ogbg-molsider: 0.601816 val loss: 0.541272
[Epoch 68] ogbg-molsider: 0.561268 test loss: 0.594338
[Epoch 69; Iter    12/   36] train: loss: 0.3425804
[Epoch 69] ogbg-molsider: 0.591676 val loss: 0.552084
[Epoch 69] ogbg-molsider: 0.578646 test loss: 0.591236
[Epoch 70; Iter     6/   36] train: loss: 0.3530589
[Epoch 70; Iter    36/   36] train: loss: 0.3135916
[Epoch 70] ogbg-molsider: 0.611761 val loss: 0.533668
[Epoch 70] ogbg-molsider: 0.572493 test loss: 0.575167
[Epoch 71; Iter    30/   36] train: loss: 0.3252878
[Epoch 71] ogbg-molsider: 0.622849 val loss: 0.577506
[Epoch 71] ogbg-molsider: 0.565541 test loss: 0.627434
[Epoch 72; Iter    24/   36] train: loss: 0.3309299
[Epoch 72] ogbg-molsider: 0.640900 val loss: 0.621911
[Epoch 72] ogbg-molsider: 0.565174 test loss: 0.817372
[Epoch 73; Iter    18/   36] train: loss: 0.3148414
[Epoch 73] ogbg-molsider: 0.623760 val loss: 0.534983
[Epoch 73] ogbg-molsider: 0.587863 test loss: 0.590959
[Epoch 74; Iter    12/   36] train: loss: 0.2853351
[Epoch 74] ogbg-molsider: 0.624621 val loss: 0.649502
[Epoch 74] ogbg-molsider: 0.569443 test loss: 0.823615
[Epoch 75; Iter     6/   36] train: loss: 0.3311415
[Epoch 75; Iter    36/   36] train: loss: 0.3382133
[Epoch 75] ogbg-molsider: 0.634115 val loss: 0.540896
[Epoch 75] ogbg-molsider: 0.588499 test loss: 0.676807
[Epoch 76; Iter    30/   36] train: loss: 0.3115415
[Epoch 76] ogbg-molsider: 0.610445 val loss: 0.600690
[Epoch 76] ogbg-molsider: 0.570297 test loss: 0.621461
[Epoch 77; Iter    24/   36] train: loss: 0.3206022
[Epoch 77] ogbg-molsider: 0.616794 val loss: 0.542047
[Epoch 77] ogbg-molsider: 0.618259 test loss: 0.579850
[Epoch 78; Iter    18/   36] train: loss: 0.3227717
[Epoch 78] ogbg-molsider: 0.608900 val loss: 0.602651
[Epoch 78] ogbg-molsider: 0.580201 test loss: 0.691734
[Epoch 79; Iter    12/   36] train: loss: 0.2738947
[Epoch 79] ogbg-molsider: 0.625019 val loss: 0.559554
[Epoch 79] ogbg-molsider: 0.588524 test loss: 0.615405
[Epoch 80; Iter     6/   36] train: loss: 0.3130956
[Epoch 80; Iter    36/   36] train: loss: 0.2994449
[Epoch 80] ogbg-molsider: 0.619063 val loss: 0.619008
[Epoch 80] ogbg-molsider: 0.578073 test loss: 0.664553
[Epoch 33] ogbg-molsider: 0.530951 test loss: 1.587268
[Epoch 34; Iter    12/   36] train: loss: 0.4908987
[Epoch 34] ogbg-molsider: 0.534514 val loss: 2.192057
[Epoch 34] ogbg-molsider: 0.532319 test loss: 3.081903
[Epoch 35; Iter     6/   36] train: loss: 0.5499069
[Epoch 35; Iter    36/   36] train: loss: 0.4927637
[Epoch 35] ogbg-molsider: 0.549042 val loss: 0.913941
[Epoch 35] ogbg-molsider: 0.539098 test loss: 1.140389
[Epoch 36; Iter    30/   36] train: loss: 0.4500973
[Epoch 36] ogbg-molsider: 0.547771 val loss: 1.555058
[Epoch 36] ogbg-molsider: 0.549476 test loss: 2.156259
[Epoch 37; Iter    24/   36] train: loss: 0.5745658
[Epoch 37] ogbg-molsider: 0.529951 val loss: 0.679069
[Epoch 37] ogbg-molsider: 0.533120 test loss: 0.751808
[Epoch 38; Iter    18/   36] train: loss: 0.5120829
[Epoch 38] ogbg-molsider: 0.557163 val loss: 0.526727
[Epoch 38] ogbg-molsider: 0.548179 test loss: 0.526450
[Epoch 39; Iter    12/   36] train: loss: 0.5084683
[Epoch 39] ogbg-molsider: 0.541461 val loss: 0.538316
[Epoch 39] ogbg-molsider: 0.540931 test loss: 0.531904
[Epoch 40; Iter     6/   36] train: loss: 0.4514739
[Epoch 40; Iter    36/   36] train: loss: 0.5076350
[Epoch 40] ogbg-molsider: 0.530105 val loss: 0.577407
[Epoch 40] ogbg-molsider: 0.514880 test loss: 0.559860
[Epoch 41; Iter    30/   36] train: loss: 0.5147766
[Epoch 41] ogbg-molsider: 0.584388 val loss: 0.602429
[Epoch 41] ogbg-molsider: 0.506783 test loss: 0.638002
[Epoch 42; Iter    24/   36] train: loss: 0.4996494
[Epoch 42] ogbg-molsider: 0.536711 val loss: 0.919468
[Epoch 42] ogbg-molsider: 0.513454 test loss: 0.940214
[Epoch 43; Iter    18/   36] train: loss: 0.5003445
[Epoch 43] ogbg-molsider: 0.526857 val loss: 0.610012
[Epoch 43] ogbg-molsider: 0.519349 test loss: 0.604642
[Epoch 44; Iter    12/   36] train: loss: 0.4418306
[Epoch 44] ogbg-molsider: 0.508074 val loss: 0.674655
[Epoch 44] ogbg-molsider: 0.565340 test loss: 0.690627
[Epoch 45; Iter     6/   36] train: loss: 0.4527867
[Epoch 45; Iter    36/   36] train: loss: 0.4495317
[Epoch 45] ogbg-molsider: 0.567995 val loss: 0.726260
[Epoch 45] ogbg-molsider: 0.527224 test loss: 0.821726
[Epoch 46; Iter    30/   36] train: loss: 0.4359226
[Epoch 46] ogbg-molsider: 0.522670 val loss: 0.836885
[Epoch 46] ogbg-molsider: 0.513856 test loss: 0.824875
[Epoch 47; Iter    24/   36] train: loss: 0.4867256
[Epoch 47] ogbg-molsider: 0.545667 val loss: 0.895865
[Epoch 47] ogbg-molsider: 0.506510 test loss: 1.006175
[Epoch 48; Iter    18/   36] train: loss: 0.4187139
[Epoch 48] ogbg-molsider: 0.543593 val loss: 0.597096
[Epoch 48] ogbg-molsider: 0.534250 test loss: 0.609354
[Epoch 49; Iter    12/   36] train: loss: 0.4826209
[Epoch 49] ogbg-molsider: 0.543685 val loss: 0.893852
[Epoch 49] ogbg-molsider: 0.508573 test loss: 0.907967
[Epoch 50; Iter     6/   36] train: loss: 0.4363458
[Epoch 50; Iter    36/   36] train: loss: 0.5119553
[Epoch 50] ogbg-molsider: 0.505105 val loss: 2.115705
[Epoch 50] ogbg-molsider: 0.510690 test loss: 2.289050
[Epoch 51; Iter    30/   36] train: loss: 0.3890072
[Epoch 51] ogbg-molsider: 0.541472 val loss: 1.223810
[Epoch 51] ogbg-molsider: 0.506649 test loss: 1.316393
[Epoch 52; Iter    24/   36] train: loss: 0.4415035
[Epoch 52] ogbg-molsider: 0.566785 val loss: 1.043018
[Epoch 52] ogbg-molsider: 0.511319 test loss: 1.154778
[Epoch 53; Iter    18/   36] train: loss: 0.3553606
[Epoch 53] ogbg-molsider: 0.551420 val loss: 1.255171
[Epoch 53] ogbg-molsider: 0.512035 test loss: 1.297203
[Epoch 54; Iter    12/   36] train: loss: 0.3540289
[Epoch 54] ogbg-molsider: 0.574562 val loss: 1.048178
[Epoch 54] ogbg-molsider: 0.515694 test loss: 1.157710
[Epoch 55; Iter     6/   36] train: loss: 0.3688766
[Epoch 55; Iter    36/   36] train: loss: 0.3254122
[Epoch 55] ogbg-molsider: 0.568956 val loss: 1.435371
[Epoch 55] ogbg-molsider: 0.516313 test loss: 1.642760
[Epoch 56; Iter    30/   36] train: loss: 0.3780439
[Epoch 56] ogbg-molsider: 0.578932 val loss: 1.273823
[Epoch 56] ogbg-molsider: 0.519907 test loss: 1.454770
[Epoch 57; Iter    24/   36] train: loss: 0.3499386
[Epoch 57] ogbg-molsider: 0.552397 val loss: 1.376212
[Epoch 57] ogbg-molsider: 0.521618 test loss: 1.468297
[Epoch 58; Iter    18/   36] train: loss: 0.3355811
[Epoch 58] ogbg-molsider: 0.576097 val loss: 1.207916
[Epoch 58] ogbg-molsider: 0.534806 test loss: 1.275015
[Epoch 59; Iter    12/   36] train: loss: 0.3763728
[Epoch 59] ogbg-molsider: 0.562734 val loss: 1.668290
[Epoch 59] ogbg-molsider: 0.507364 test loss: 1.883044
[Epoch 60; Iter     6/   36] train: loss: 0.3822882
[Epoch 60; Iter    36/   36] train: loss: 0.3350771
[Epoch 60] ogbg-molsider: 0.571077 val loss: 1.318466
[Epoch 60] ogbg-molsider: 0.520770 test loss: 1.451834
[Epoch 61; Iter    30/   36] train: loss: 0.3208446
[Epoch 61] ogbg-molsider: 0.561881 val loss: 1.400118
[Epoch 61] ogbg-molsider: 0.545547 test loss: 1.486756
[Epoch 62; Iter    24/   36] train: loss: 0.3378218
[Epoch 62] ogbg-molsider: 0.560168 val loss: 0.994688
[Epoch 62] ogbg-molsider: 0.531538 test loss: 1.069885
[Epoch 63; Iter    18/   36] train: loss: 0.3574082
[Epoch 63] ogbg-molsider: 0.578456 val loss: 1.567099
[Epoch 63] ogbg-molsider: 0.537764 test loss: 1.743254
[Epoch 64; Iter    12/   36] train: loss: 0.3507353
[Epoch 64] ogbg-molsider: 0.579988 val loss: 1.237787
[Epoch 64] ogbg-molsider: 0.538803 test loss: 1.328462
[Epoch 65; Iter     6/   36] train: loss: 0.3227234
[Epoch 65; Iter    36/   36] train: loss: 0.4900433
[Epoch 65] ogbg-molsider: 0.545373 val loss: 1.538991
[Epoch 65] ogbg-molsider: 0.544461 test loss: 1.637964
[Epoch 66; Iter    30/   36] train: loss: 0.3301767
[Epoch 66] ogbg-molsider: 0.559312 val loss: 1.109020
[Epoch 66] ogbg-molsider: 0.524636 test loss: 1.169450
[Epoch 67; Iter    24/   36] train: loss: 0.3291208
[Epoch 67] ogbg-molsider: 0.568248 val loss: 1.535744
[Epoch 67] ogbg-molsider: 0.515381 test loss: 1.757036
[Epoch 68; Iter    18/   36] train: loss: 0.3391760
[Epoch 68] ogbg-molsider: 0.553657 val loss: 1.121127
[Epoch 68] ogbg-molsider: 0.540834 test loss: 1.168543
[Epoch 69; Iter    12/   36] train: loss: 0.3048871
[Epoch 69] ogbg-molsider: 0.559415 val loss: 1.477984
[Epoch 69] ogbg-molsider: 0.519099 test loss: 1.635153
[Epoch 70; Iter     6/   36] train: loss: 0.2988945
[Epoch 70; Iter    36/   36] train: loss: 0.3018732
[Epoch 70] ogbg-molsider: 0.547487 val loss: 1.294611
[Epoch 70] ogbg-molsider: 0.521641 test loss: 1.328988
[Epoch 71; Iter    30/   36] train: loss: 0.3113285
[Epoch 71] ogbg-molsider: 0.575118 val loss: 1.133061
[Epoch 71] ogbg-molsider: 0.531265 test loss: 1.243570
[Epoch 72; Iter    24/   36] train: loss: 0.2940178
[Epoch 72] ogbg-molsider: 0.566350 val loss: 1.430125
[Epoch 72] ogbg-molsider: 0.529994 test loss: 1.566197
[Epoch 73; Iter    18/   36] train: loss: 0.2940689
[Epoch 73] ogbg-molsider: 0.568283 val loss: 1.567883
[Epoch 73] ogbg-molsider: 0.517799 test loss: 1.664088
[Epoch 74; Iter    12/   36] train: loss: 0.2675422
[Epoch 74] ogbg-molsider: 0.582071 val loss: 1.151450
[Epoch 74] ogbg-molsider: 0.543118 test loss: 1.260554
[Epoch 75; Iter     6/   36] train: loss: 0.2937587
[Epoch 75; Iter    36/   36] train: loss: 0.2774267
[Epoch 75] ogbg-molsider: 0.560732 val loss: 1.211705
[Epoch 75] ogbg-molsider: 0.541996 test loss: 1.264452
[Epoch 76; Iter    30/   36] train: loss: 0.3150008
[Epoch 76] ogbg-molsider: 0.571080 val loss: 1.522252
[Epoch 76] ogbg-molsider: 0.533689 test loss: 1.649256
[Epoch 77; Iter    24/   36] train: loss: 0.2830276
[Epoch 77] ogbg-molsider: 0.565457 val loss: 1.478945
[Epoch 77] ogbg-molsider: 0.527474 test loss: 1.606440
[Epoch 78; Iter    18/   36] train: loss: 0.2765355
[Epoch 78] ogbg-molsider: 0.561100 val loss: 1.490185
[Epoch 78] ogbg-molsider: 0.525816 test loss: 1.661919
[Epoch 79; Iter    12/   36] train: loss: 0.2475786
[Epoch 79] ogbg-molsider: 0.563666 val loss: 1.439547
[Epoch 79] ogbg-molsider: 0.525267 test loss: 1.581298
[Epoch 80; Iter     6/   36] train: loss: 0.2467035
[Epoch 80; Iter    36/   36] train: loss: 0.2496066
[Epoch 80] ogbg-molsider: 0.557720 val loss: 1.505866
[Epoch 80] ogbg-molsider: 0.525198 test loss: 1.640685
[Epoch 33] ogbg-molsider: 0.543573 test loss: 1.631799
[Epoch 34; Iter    12/   36] train: loss: 0.4999704
[Epoch 34] ogbg-molsider: 0.551251 val loss: 3.152824
[Epoch 34] ogbg-molsider: 0.541146 test loss: 2.797244
[Epoch 35; Iter     6/   36] train: loss: 0.5126693
[Epoch 35; Iter    36/   36] train: loss: 0.5275081
[Epoch 35] ogbg-molsider: 0.554340 val loss: 7.488971
[Epoch 35] ogbg-molsider: 0.553114 test loss: 7.424958
[Epoch 36; Iter    30/   36] train: loss: 0.4826627
[Epoch 36] ogbg-molsider: 0.543461 val loss: 5.697167
[Epoch 36] ogbg-molsider: 0.547623 test loss: 4.972735
[Epoch 37; Iter    24/   36] train: loss: 0.4976426
[Epoch 37] ogbg-molsider: 0.539090 val loss: 4.670503
[Epoch 37] ogbg-molsider: 0.531610 test loss: 3.889781
[Epoch 38; Iter    18/   36] train: loss: 0.4931314
[Epoch 38] ogbg-molsider: 0.553992 val loss: 5.348742
[Epoch 38] ogbg-molsider: 0.536136 test loss: 4.705045
[Epoch 39; Iter    12/   36] train: loss: 0.5237208
[Epoch 39] ogbg-molsider: 0.541731 val loss: 2.671380
[Epoch 39] ogbg-molsider: 0.537730 test loss: 2.406296
[Epoch 40; Iter     6/   36] train: loss: 0.5008584
[Epoch 40; Iter    36/   36] train: loss: 0.5028335
[Epoch 40] ogbg-molsider: 0.522104 val loss: 1.175236
[Epoch 40] ogbg-molsider: 0.569867 test loss: 1.240016
[Epoch 41; Iter    30/   36] train: loss: 0.4892637
[Epoch 41] ogbg-molsider: 0.526006 val loss: 0.624165
[Epoch 41] ogbg-molsider: 0.533336 test loss: 0.650926
[Epoch 42; Iter    24/   36] train: loss: 0.4316719
[Epoch 42] ogbg-molsider: 0.541334 val loss: 0.710762
[Epoch 42] ogbg-molsider: 0.559465 test loss: 0.615037
[Epoch 43; Iter    18/   36] train: loss: 0.4512550
[Epoch 43] ogbg-molsider: 0.557962 val loss: 1.254301
[Epoch 43] ogbg-molsider: 0.576303 test loss: 0.977094
[Epoch 44; Iter    12/   36] train: loss: 0.4185678
[Epoch 44] ogbg-molsider: 0.543401 val loss: 0.733849
[Epoch 44] ogbg-molsider: 0.553621 test loss: 0.736470
[Epoch 45; Iter     6/   36] train: loss: 0.4302109
[Epoch 45; Iter    36/   36] train: loss: 0.4709609
[Epoch 45] ogbg-molsider: 0.551882 val loss: 4.842025
[Epoch 45] ogbg-molsider: 0.537834 test loss: 3.168044
[Epoch 46; Iter    30/   36] train: loss: 0.4435565
[Epoch 46] ogbg-molsider: 0.563360 val loss: 8.725906
[Epoch 46] ogbg-molsider: 0.573011 test loss: 9.719629
[Epoch 47; Iter    24/   36] train: loss: 0.4017096
[Epoch 47] ogbg-molsider: 0.547741 val loss: 3.302748
[Epoch 47] ogbg-molsider: 0.543923 test loss: 2.630448
[Epoch 48; Iter    18/   36] train: loss: 0.4850954
[Epoch 48] ogbg-molsider: 0.556890 val loss: 2.390050
[Epoch 48] ogbg-molsider: 0.574255 test loss: 1.179367
[Epoch 49; Iter    12/   36] train: loss: 0.3725491
[Epoch 49] ogbg-molsider: 0.552861 val loss: 5.970521
[Epoch 49] ogbg-molsider: 0.575477 test loss: 2.724698
[Epoch 50; Iter     6/   36] train: loss: 0.3991746
[Epoch 50; Iter    36/   36] train: loss: 0.4308435
[Epoch 50] ogbg-molsider: 0.558262 val loss: 2.715381
[Epoch 50] ogbg-molsider: 0.600585 test loss: 2.516483
[Epoch 51; Iter    30/   36] train: loss: 0.3788309
[Epoch 51] ogbg-molsider: 0.570260 val loss: 1.554670
[Epoch 51] ogbg-molsider: 0.584919 test loss: 0.901360
[Epoch 52; Iter    24/   36] train: loss: 0.3767404
[Epoch 52] ogbg-molsider: 0.564640 val loss: 4.688152
[Epoch 52] ogbg-molsider: 0.591956 test loss: 5.868242
[Epoch 53; Iter    18/   36] train: loss: 0.3653863
[Epoch 53] ogbg-molsider: 0.565129 val loss: 12.190638
[Epoch 53] ogbg-molsider: 0.550163 test loss: 8.576054
[Epoch 54; Iter    12/   36] train: loss: 0.3736488
[Epoch 54] ogbg-molsider: 0.557934 val loss: 8.517295
[Epoch 54] ogbg-molsider: 0.545021 test loss: 6.545596
[Epoch 55; Iter     6/   36] train: loss: 0.3678819
[Epoch 55; Iter    36/   36] train: loss: 0.4634067
[Epoch 55] ogbg-molsider: 0.560536 val loss: 5.147057
[Epoch 55] ogbg-molsider: 0.587593 test loss: 3.247733
[Epoch 56; Iter    30/   36] train: loss: 0.3412716
[Epoch 56] ogbg-molsider: 0.547319 val loss: 12.965482
[Epoch 56] ogbg-molsider: 0.574196 test loss: 9.980162
[Epoch 57; Iter    24/   36] train: loss: 0.3476698
[Epoch 57] ogbg-molsider: 0.553821 val loss: 5.837181
[Epoch 57] ogbg-molsider: 0.604818 test loss: 5.159420
[Epoch 58; Iter    18/   36] train: loss: 0.3581096
[Epoch 58] ogbg-molsider: 0.565637 val loss: 25.029453
[Epoch 58] ogbg-molsider: 0.574918 test loss: 39.892173
[Epoch 59; Iter    12/   36] train: loss: 0.2877516
[Epoch 59] ogbg-molsider: 0.555815 val loss: 16.326802
[Epoch 59] ogbg-molsider: 0.593380 test loss: 16.514744
[Epoch 60; Iter     6/   36] train: loss: 0.3746341
[Epoch 60; Iter    36/   36] train: loss: 0.3505703
[Epoch 60] ogbg-molsider: 0.557480 val loss: 26.606681
[Epoch 60] ogbg-molsider: 0.604157 test loss: 30.698535
[Epoch 61; Iter    30/   36] train: loss: 0.3637542
[Epoch 61] ogbg-molsider: 0.583736 val loss: 32.386222
[Epoch 61] ogbg-molsider: 0.572603 test loss: 54.301404
[Epoch 62; Iter    24/   36] train: loss: 0.3420568
[Epoch 62] ogbg-molsider: 0.571388 val loss: 15.886848
[Epoch 62] ogbg-molsider: 0.589857 test loss: 21.133990
[Epoch 63; Iter    18/   36] train: loss: 0.3415088
[Epoch 63] ogbg-molsider: 0.563382 val loss: 37.213253
[Epoch 63] ogbg-molsider: 0.593077 test loss: 57.375370
[Epoch 64; Iter    12/   36] train: loss: 0.3113605
[Epoch 64] ogbg-molsider: 0.554567 val loss: 7.934741
[Epoch 64] ogbg-molsider: 0.592644 test loss: 6.592833
[Epoch 65; Iter     6/   36] train: loss: 0.3475420
[Epoch 65; Iter    36/   36] train: loss: 0.3280989
[Epoch 65] ogbg-molsider: 0.583945 val loss: 25.931514
[Epoch 65] ogbg-molsider: 0.578056 test loss: 49.327961
[Epoch 66; Iter    30/   36] train: loss: 0.2913787
[Epoch 66] ogbg-molsider: 0.540545 val loss: 29.738097
[Epoch 66] ogbg-molsider: 0.591161 test loss: 48.539335
[Epoch 67; Iter    24/   36] train: loss: 0.2849052
[Epoch 67] ogbg-molsider: 0.570950 val loss: 36.111989
[Epoch 67] ogbg-molsider: 0.589055 test loss: 61.909126
[Epoch 68; Iter    18/   36] train: loss: 0.2808901
[Epoch 68] ogbg-molsider: 0.570680 val loss: 27.572171
[Epoch 68] ogbg-molsider: 0.585974 test loss: 50.926801
[Epoch 69; Iter    12/   36] train: loss: 0.2826873
[Epoch 69] ogbg-molsider: 0.554771 val loss: 24.375709
[Epoch 69] ogbg-molsider: 0.580831 test loss: 43.665077
[Epoch 70; Iter     6/   36] train: loss: 0.2782734
[Epoch 70; Iter    36/   36] train: loss: 0.3463717
[Epoch 70] ogbg-molsider: 0.564548 val loss: 36.065076
[Epoch 70] ogbg-molsider: 0.580491 test loss: 65.111562
[Epoch 71; Iter    30/   36] train: loss: 0.3012924
[Epoch 71] ogbg-molsider: 0.577349 val loss: 29.248550
[Epoch 71] ogbg-molsider: 0.575341 test loss: 48.224956
[Epoch 72; Iter    24/   36] train: loss: 0.2885846
[Epoch 72] ogbg-molsider: 0.548330 val loss: 17.285857
[Epoch 72] ogbg-molsider: 0.580043 test loss: 31.873263
[Epoch 73; Iter    18/   36] train: loss: 0.2939248
[Epoch 73] ogbg-molsider: 0.552650 val loss: 20.827792
[Epoch 73] ogbg-molsider: 0.577736 test loss: 39.575323
[Epoch 74; Iter    12/   36] train: loss: 0.2980615
[Epoch 74] ogbg-molsider: 0.581001 val loss: 23.739410
[Epoch 74] ogbg-molsider: 0.577210 test loss: 43.216910
[Epoch 75; Iter     6/   36] train: loss: 0.2742527
[Epoch 75; Iter    36/   36] train: loss: 0.2879417
[Epoch 75] ogbg-molsider: 0.540264 val loss: 9.752960
[Epoch 75] ogbg-molsider: 0.575504 test loss: 10.735987
[Epoch 76; Iter    30/   36] train: loss: 0.2839636
[Epoch 76] ogbg-molsider: 0.570307 val loss: 20.351684
[Epoch 76] ogbg-molsider: 0.582372 test loss: 39.097941
[Epoch 77; Iter    24/   36] train: loss: 0.2668298
[Epoch 77] ogbg-molsider: 0.566637 val loss: 24.292913
[Epoch 77] ogbg-molsider: 0.579461 test loss: 44.238625
[Epoch 78; Iter    18/   36] train: loss: 0.2734767
[Epoch 78] ogbg-molsider: 0.538570 val loss: 17.337084
[Epoch 78] ogbg-molsider: 0.587753 test loss: 29.017462
[Epoch 79; Iter    12/   36] train: loss: 0.2551458
[Epoch 79] ogbg-molsider: 0.561995 val loss: 21.608698
[Epoch 79] ogbg-molsider: 0.569358 test loss: 35.610199
[Epoch 80; Iter     6/   36] train: loss: 0.2608946
[Epoch 80; Iter    36/   36] train: loss: 0.2809335
[Epoch 80] ogbg-molsider: 0.566995 val loss: 17.936357
[Epoch 80] ogbg-molsider: 0.566597 test loss: 33.720118
[Epoch 33] ogbg-molsider: 0.597269 test loss: 0.495408
[Epoch 34; Iter    12/   36] train: loss: 0.4855094
[Epoch 34] ogbg-molsider: 0.545883 val loss: 0.487516
[Epoch 34] ogbg-molsider: 0.599926 test loss: 0.491746
[Epoch 35; Iter     6/   36] train: loss: 0.5457836
[Epoch 35; Iter    36/   36] train: loss: 0.4709366
[Epoch 35] ogbg-molsider: 0.571093 val loss: 0.483016
[Epoch 35] ogbg-molsider: 0.605163 test loss: 0.495103
[Epoch 36; Iter    30/   36] train: loss: 0.4503605
[Epoch 36] ogbg-molsider: 0.595608 val loss: 0.480553
[Epoch 36] ogbg-molsider: 0.614611 test loss: 0.495085
[Epoch 37; Iter    24/   36] train: loss: 0.5510948
[Epoch 37] ogbg-molsider: 0.573803 val loss: 0.489325
[Epoch 37] ogbg-molsider: 0.601215 test loss: 0.501233
[Epoch 38; Iter    18/   36] train: loss: 0.5003145
[Epoch 38] ogbg-molsider: 0.584592 val loss: 0.481215
[Epoch 38] ogbg-molsider: 0.601619 test loss: 0.491162
[Epoch 39; Iter    12/   36] train: loss: 0.4917309
[Epoch 39] ogbg-molsider: 0.573819 val loss: 0.485845
[Epoch 39] ogbg-molsider: 0.615600 test loss: 0.500927
[Epoch 40; Iter     6/   36] train: loss: 0.4781269
[Epoch 40; Iter    36/   36] train: loss: 0.5223055
[Epoch 40] ogbg-molsider: 0.595465 val loss: 0.482254
[Epoch 40] ogbg-molsider: 0.583847 test loss: 0.501872
[Epoch 41; Iter    30/   36] train: loss: 0.5209562
[Epoch 41] ogbg-molsider: 0.591817 val loss: 0.480690
[Epoch 41] ogbg-molsider: 0.585193 test loss: 0.493567
[Epoch 42; Iter    24/   36] train: loss: 0.4933637
[Epoch 42] ogbg-molsider: 0.565229 val loss: 0.499526
[Epoch 42] ogbg-molsider: 0.621681 test loss: 0.516668
[Epoch 43; Iter    18/   36] train: loss: 0.4956894
[Epoch 43] ogbg-molsider: 0.578042 val loss: 0.491394
[Epoch 43] ogbg-molsider: 0.591142 test loss: 0.518961
[Epoch 44; Iter    12/   36] train: loss: 0.4915101
[Epoch 44] ogbg-molsider: 0.566135 val loss: 0.495485
[Epoch 44] ogbg-molsider: 0.620170 test loss: 0.498509
[Epoch 45; Iter     6/   36] train: loss: 0.4845520
[Epoch 45; Iter    36/   36] train: loss: 0.4560557
[Epoch 45] ogbg-molsider: 0.550328 val loss: 0.583356
[Epoch 45] ogbg-molsider: 0.602772 test loss: 0.583211
[Epoch 46; Iter    30/   36] train: loss: 0.4899889
[Epoch 46] ogbg-molsider: 0.567540 val loss: 0.478761
[Epoch 46] ogbg-molsider: 0.605336 test loss: 0.495696
[Epoch 47; Iter    24/   36] train: loss: 0.5040683
[Epoch 47] ogbg-molsider: 0.583461 val loss: 0.478013
[Epoch 47] ogbg-molsider: 0.613870 test loss: 0.501175
[Epoch 48; Iter    18/   36] train: loss: 0.4659994
[Epoch 48] ogbg-molsider: 0.567864 val loss: 0.492933
[Epoch 48] ogbg-molsider: 0.630218 test loss: 0.509765
[Epoch 49; Iter    12/   36] train: loss: 0.4710050
[Epoch 49] ogbg-molsider: 0.616625 val loss: 0.476180
[Epoch 49] ogbg-molsider: 0.581022 test loss: 0.507298
[Epoch 50; Iter     6/   36] train: loss: 0.5163227
[Epoch 50; Iter    36/   36] train: loss: 0.5507436
[Epoch 50] ogbg-molsider: 0.559917 val loss: 0.541392
[Epoch 50] ogbg-molsider: 0.565030 test loss: 0.592893
[Epoch 51; Iter    30/   36] train: loss: 0.5342759
[Epoch 51] ogbg-molsider: 0.568473 val loss: 0.489315
[Epoch 51] ogbg-molsider: 0.602984 test loss: 0.507088
[Epoch 52; Iter    24/   36] train: loss: 0.4769329
[Epoch 52] ogbg-molsider: 0.607291 val loss: 0.479800
[Epoch 52] ogbg-molsider: 0.612312 test loss: 0.498962
[Epoch 53; Iter    18/   36] train: loss: 0.4703044
[Epoch 53] ogbg-molsider: 0.614629 val loss: 0.484077
[Epoch 53] ogbg-molsider: 0.598853 test loss: 0.508321
[Epoch 54; Iter    12/   36] train: loss: 0.4824056
[Epoch 54] ogbg-molsider: 0.619371 val loss: 0.477412
[Epoch 54] ogbg-molsider: 0.612237 test loss: 0.496302
[Epoch 55; Iter     6/   36] train: loss: 0.4743524
[Epoch 55; Iter    36/   36] train: loss: 0.4458753
[Epoch 55] ogbg-molsider: 0.598636 val loss: 0.492770
[Epoch 55] ogbg-molsider: 0.597261 test loss: 0.519252
[Epoch 56; Iter    30/   36] train: loss: 0.4436585
[Epoch 56] ogbg-molsider: 0.614874 val loss: 0.487318
[Epoch 56] ogbg-molsider: 0.596780 test loss: 0.516127
[Epoch 57; Iter    24/   36] train: loss: 0.4372514
[Epoch 57] ogbg-molsider: 0.615036 val loss: 0.488649
[Epoch 57] ogbg-molsider: 0.601476 test loss: 0.515024
[Epoch 58; Iter    18/   36] train: loss: 0.4362156
[Epoch 58] ogbg-molsider: 0.619332 val loss: 0.479417
[Epoch 58] ogbg-molsider: 0.590012 test loss: 0.515144
[Epoch 59; Iter    12/   36] train: loss: 0.4472038
[Epoch 59] ogbg-molsider: 0.609108 val loss: 0.597371
[Epoch 59] ogbg-molsider: 0.578096 test loss: 0.676001
[Epoch 60; Iter     6/   36] train: loss: 0.4645629
[Epoch 60; Iter    36/   36] train: loss: 0.4006911
[Epoch 60] ogbg-molsider: 0.583137 val loss: 0.525751
[Epoch 60] ogbg-molsider: 0.606162 test loss: 0.558744
[Epoch 61; Iter    30/   36] train: loss: 0.4046558
[Epoch 61] ogbg-molsider: 0.619061 val loss: 0.499260
[Epoch 61] ogbg-molsider: 0.589346 test loss: 0.538981
[Epoch 62; Iter    24/   36] train: loss: 0.4267903
[Epoch 62] ogbg-molsider: 0.574489 val loss: 0.534113
[Epoch 62] ogbg-molsider: 0.565495 test loss: 0.598689
[Epoch 63; Iter    18/   36] train: loss: 0.4726930
[Epoch 63] ogbg-molsider: 0.617241 val loss: 0.501471
[Epoch 63] ogbg-molsider: 0.572726 test loss: 0.536705
[Epoch 64; Iter    12/   36] train: loss: 0.4088888
[Epoch 64] ogbg-molsider: 0.593525 val loss: 0.524086
[Epoch 64] ogbg-molsider: 0.594073 test loss: 0.574022
[Epoch 65; Iter     6/   36] train: loss: 0.4058783
[Epoch 65; Iter    36/   36] train: loss: 0.5466138
[Epoch 65] ogbg-molsider: 0.555232 val loss: 0.960379
[Epoch 65] ogbg-molsider: 0.607318 test loss: 1.450139
[Epoch 66; Iter    30/   36] train: loss: 0.4455681
[Epoch 66] ogbg-molsider: 0.557575 val loss: 0.598469
[Epoch 66] ogbg-molsider: 0.612807 test loss: 1.012332
[Epoch 67; Iter    24/   36] train: loss: 0.4843024
[Epoch 67] ogbg-molsider: 0.611542 val loss: 0.488067
[Epoch 67] ogbg-molsider: 0.619128 test loss: 0.508920
[Epoch 68; Iter    18/   36] train: loss: 0.4905331
[Epoch 68] ogbg-molsider: 0.572491 val loss: 0.519515
[Epoch 68] ogbg-molsider: 0.612331 test loss: 0.533426
[Epoch 69; Iter    12/   36] train: loss: 0.4549685
[Epoch 69] ogbg-molsider: 0.606957 val loss: 0.483101
[Epoch 69] ogbg-molsider: 0.602531 test loss: 0.510033
[Epoch 70; Iter     6/   36] train: loss: 0.4070316
[Epoch 70; Iter    36/   36] train: loss: 0.4768788
[Epoch 70] ogbg-molsider: 0.614897 val loss: 0.486800
[Epoch 70] ogbg-molsider: 0.613626 test loss: 0.499511
[Epoch 71; Iter    30/   36] train: loss: 0.4522054
[Epoch 71] ogbg-molsider: 0.602299 val loss: 0.508454
[Epoch 71] ogbg-molsider: 0.588359 test loss: 0.546850
[Epoch 72; Iter    24/   36] train: loss: 0.4242524
[Epoch 72] ogbg-molsider: 0.567059 val loss: 0.586137
[Epoch 72] ogbg-molsider: 0.589006 test loss: 0.617753
[Epoch 73; Iter    18/   36] train: loss: 0.4219133
[Epoch 73] ogbg-molsider: 0.587548 val loss: 0.522590
[Epoch 73] ogbg-molsider: 0.606159 test loss: 0.556201
[Epoch 74; Iter    12/   36] train: loss: 0.4123028
[Epoch 74] ogbg-molsider: 0.604109 val loss: 0.505975
[Epoch 74] ogbg-molsider: 0.602180 test loss: 0.520712
[Epoch 75; Iter     6/   36] train: loss: 0.4254484
[Epoch 75; Iter    36/   36] train: loss: 0.5173570
[Epoch 75] ogbg-molsider: 0.576720 val loss: 0.500068
[Epoch 75] ogbg-molsider: 0.597348 test loss: 0.515165
[Epoch 76; Iter    30/   36] train: loss: 0.4924740
[Epoch 76] ogbg-molsider: 0.607808 val loss: 0.496551
[Epoch 76] ogbg-molsider: 0.598870 test loss: 0.529050
[Epoch 77; Iter    24/   36] train: loss: 0.4671274
[Epoch 77] ogbg-molsider: 0.601181 val loss: 0.511286
[Epoch 77] ogbg-molsider: 0.616590 test loss: 0.547355
[Epoch 78; Iter    18/   36] train: loss: 0.4352700
[Epoch 78] ogbg-molsider: 0.624642 val loss: 0.488377
[Epoch 78] ogbg-molsider: 0.602654 test loss: 0.523030
[Epoch 79; Iter    12/   36] train: loss: 0.3837354
[Epoch 79] ogbg-molsider: 0.616559 val loss: 0.505220
[Epoch 79] ogbg-molsider: 0.615186 test loss: 0.527532
[Epoch 80; Iter     6/   36] train: loss: 0.4300856
[Epoch 80; Iter    36/   36] train: loss: 0.3923671
[Epoch 80] ogbg-molsider: 0.620326 val loss: 0.519463
[Epoch 80] ogbg-molsider: 0.582251 test loss: 0.596590
[Epoch 33] ogbg-molsider: 0.606712 test loss: 0.490195
[Epoch 34; Iter    12/   36] train: loss: 0.5031945
[Epoch 34] ogbg-molsider: 0.593583 val loss: 0.478628
[Epoch 34] ogbg-molsider: 0.590959 test loss: 0.500398
[Epoch 35; Iter     6/   36] train: loss: 0.4920425
[Epoch 35; Iter    36/   36] train: loss: 0.5246127
[Epoch 35] ogbg-molsider: 0.575065 val loss: 0.491257
[Epoch 35] ogbg-molsider: 0.606498 test loss: 0.502237
[Epoch 36; Iter    30/   36] train: loss: 0.4686493
[Epoch 36] ogbg-molsider: 0.572235 val loss: 0.485523
[Epoch 36] ogbg-molsider: 0.610795 test loss: 0.499547
[Epoch 37; Iter    24/   36] train: loss: 0.4951315
[Epoch 37] ogbg-molsider: 0.587925 val loss: 0.478505
[Epoch 37] ogbg-molsider: 0.606757 test loss: 0.489643
[Epoch 38; Iter    18/   36] train: loss: 0.4733518
[Epoch 38] ogbg-molsider: 0.598528 val loss: 0.478354
[Epoch 38] ogbg-molsider: 0.605020 test loss: 0.492548
[Epoch 39; Iter    12/   36] train: loss: 0.5238388
[Epoch 39] ogbg-molsider: 0.575512 val loss: 0.481452
[Epoch 39] ogbg-molsider: 0.616218 test loss: 0.487645
[Epoch 40; Iter     6/   36] train: loss: 0.5347266
[Epoch 40; Iter    36/   36] train: loss: 0.4678488
[Epoch 40] ogbg-molsider: 0.569997 val loss: 0.495380
[Epoch 40] ogbg-molsider: 0.617531 test loss: 0.511129
[Epoch 41; Iter    30/   36] train: loss: 0.5242606
[Epoch 41] ogbg-molsider: 0.596609 val loss: 0.500509
[Epoch 41] ogbg-molsider: 0.590839 test loss: 0.498990
[Epoch 42; Iter    24/   36] train: loss: 0.4496145
[Epoch 42] ogbg-molsider: 0.609504 val loss: 0.477110
[Epoch 42] ogbg-molsider: 0.589959 test loss: 0.499520
[Epoch 43; Iter    18/   36] train: loss: 0.5101535
[Epoch 43] ogbg-molsider: 0.590858 val loss: 0.504617
[Epoch 43] ogbg-molsider: 0.580978 test loss: 0.542957
[Epoch 44; Iter    12/   36] train: loss: 0.4904064
[Epoch 44] ogbg-molsider: 0.588330 val loss: 0.496654
[Epoch 44] ogbg-molsider: 0.594215 test loss: 0.511181
[Epoch 45; Iter     6/   36] train: loss: 0.4574571
[Epoch 45; Iter    36/   36] train: loss: 0.4633884
[Epoch 45] ogbg-molsider: 0.638854 val loss: 0.471715
[Epoch 45] ogbg-molsider: 0.601123 test loss: 0.501997
[Epoch 46; Iter    30/   36] train: loss: 0.4725792
[Epoch 46] ogbg-molsider: 0.571182 val loss: 0.498117
[Epoch 46] ogbg-molsider: 0.605853 test loss: 0.498383
[Epoch 47; Iter    24/   36] train: loss: 0.4406542
[Epoch 47] ogbg-molsider: 0.570251 val loss: 0.507002
[Epoch 47] ogbg-molsider: 0.568368 test loss: 0.538610
[Epoch 48; Iter    18/   36] train: loss: 0.5400532
[Epoch 48] ogbg-molsider: 0.579483 val loss: 0.491358
[Epoch 48] ogbg-molsider: 0.586949 test loss: 0.500966
[Epoch 49; Iter    12/   36] train: loss: 0.4518847
[Epoch 49] ogbg-molsider: 0.589166 val loss: 0.505515
[Epoch 49] ogbg-molsider: 0.588149 test loss: 0.537116
[Epoch 50; Iter     6/   36] train: loss: 0.4611495
[Epoch 50; Iter    36/   36] train: loss: 0.4840206
[Epoch 50] ogbg-molsider: 0.588857 val loss: 0.486558
[Epoch 50] ogbg-molsider: 0.599366 test loss: 0.509247
[Epoch 51; Iter    30/   36] train: loss: 0.4401727
[Epoch 51] ogbg-molsider: 0.572385 val loss: 0.500866
[Epoch 51] ogbg-molsider: 0.600137 test loss: 0.513700
[Epoch 52; Iter    24/   36] train: loss: 0.4337056
[Epoch 52] ogbg-molsider: 0.582231 val loss: 0.507363
[Epoch 52] ogbg-molsider: 0.602448 test loss: 0.512839
[Epoch 53; Iter    18/   36] train: loss: 0.4651290
[Epoch 53] ogbg-molsider: 0.561519 val loss: 0.509679
[Epoch 53] ogbg-molsider: 0.561713 test loss: 0.532291
[Epoch 54; Iter    12/   36] train: loss: 0.4339281
[Epoch 54] ogbg-molsider: 0.592614 val loss: 0.484137
[Epoch 54] ogbg-molsider: 0.600337 test loss: 0.502936
[Epoch 55; Iter     6/   36] train: loss: 0.4115709
[Epoch 55; Iter    36/   36] train: loss: 0.5028076
[Epoch 55] ogbg-molsider: 0.597897 val loss: 0.528540
[Epoch 55] ogbg-molsider: 0.593210 test loss: 0.566393
[Epoch 56; Iter    30/   36] train: loss: 0.4459999
[Epoch 56] ogbg-molsider: 0.632431 val loss: 0.489036
[Epoch 56] ogbg-molsider: 0.584710 test loss: 0.516261
[Epoch 57; Iter    24/   36] train: loss: 0.3985216
[Epoch 57] ogbg-molsider: 0.613908 val loss: 0.491334
[Epoch 57] ogbg-molsider: 0.585468 test loss: 0.527107
[Epoch 58; Iter    18/   36] train: loss: 0.4309219
[Epoch 58] ogbg-molsider: 0.589173 val loss: 0.509007
[Epoch 58] ogbg-molsider: 0.608003 test loss: 0.510909
[Epoch 59; Iter    12/   36] train: loss: 0.4184252
[Epoch 59] ogbg-molsider: 0.604708 val loss: 0.570265
[Epoch 59] ogbg-molsider: 0.601549 test loss: 0.553290
[Epoch 60; Iter     6/   36] train: loss: 0.4391380
[Epoch 60; Iter    36/   36] train: loss: 0.4693292
[Epoch 60] ogbg-molsider: 0.608869 val loss: 0.520091
[Epoch 60] ogbg-molsider: 0.588408 test loss: 0.552202
[Epoch 61; Iter    30/   36] train: loss: 0.4508210
[Epoch 61] ogbg-molsider: 0.605415 val loss: 0.498844
[Epoch 61] ogbg-molsider: 0.600602 test loss: 0.534928
[Epoch 62; Iter    24/   36] train: loss: 0.4162078
[Epoch 62] ogbg-molsider: 0.592026 val loss: 0.534308
[Epoch 62] ogbg-molsider: 0.602566 test loss: 0.569833
[Epoch 63; Iter    18/   36] train: loss: 0.4532910
[Epoch 63] ogbg-molsider: 0.638218 val loss: 0.503855
[Epoch 63] ogbg-molsider: 0.587172 test loss: 0.536326
[Epoch 64; Iter    12/   36] train: loss: 0.3988233
[Epoch 64] ogbg-molsider: 0.580439 val loss: 0.518636
[Epoch 64] ogbg-molsider: 0.608151 test loss: 0.541684
[Epoch 65; Iter     6/   36] train: loss: 0.4035671
[Epoch 65; Iter    36/   36] train: loss: 0.4145777
[Epoch 65] ogbg-molsider: 0.602153 val loss: 0.505660
[Epoch 65] ogbg-molsider: 0.590232 test loss: 0.541305
[Epoch 66; Iter    30/   36] train: loss: 0.3816334
[Epoch 66] ogbg-molsider: 0.610158 val loss: 0.523749
[Epoch 66] ogbg-molsider: 0.596466 test loss: 0.560773
[Epoch 67; Iter    24/   36] train: loss: 0.3780785
[Epoch 67] ogbg-molsider: 0.609097 val loss: 0.528272
[Epoch 67] ogbg-molsider: 0.604189 test loss: 0.561675
[Epoch 68; Iter    18/   36] train: loss: 0.3484277
[Epoch 68] ogbg-molsider: 0.619174 val loss: 0.513465
[Epoch 68] ogbg-molsider: 0.604498 test loss: 0.556018
[Epoch 69; Iter    12/   36] train: loss: 0.3631628
[Epoch 69] ogbg-molsider: 0.641688 val loss: 0.496158
[Epoch 69] ogbg-molsider: 0.614015 test loss: 0.532723
[Epoch 70; Iter     6/   36] train: loss: 0.3313563
[Epoch 70; Iter    36/   36] train: loss: 0.4058430
[Epoch 70] ogbg-molsider: 0.604505 val loss: 0.550281
[Epoch 70] ogbg-molsider: 0.593222 test loss: 0.587184
[Epoch 71; Iter    30/   36] train: loss: 0.3788905
[Epoch 71] ogbg-molsider: 0.628757 val loss: 0.502379
[Epoch 71] ogbg-molsider: 0.590353 test loss: 0.575813
[Epoch 72; Iter    24/   36] train: loss: 0.3860427
[Epoch 72] ogbg-molsider: 0.618052 val loss: 0.517969
[Epoch 72] ogbg-molsider: 0.619728 test loss: 0.553279
[Epoch 73; Iter    18/   36] train: loss: 0.3805830
[Epoch 73] ogbg-molsider: 0.635601 val loss: 0.521108
[Epoch 73] ogbg-molsider: 0.584868 test loss: 0.610193
[Epoch 74; Iter    12/   36] train: loss: 0.3795007
[Epoch 74] ogbg-molsider: 0.597034 val loss: 0.554091
[Epoch 74] ogbg-molsider: 0.568588 test loss: 0.598084
[Epoch 75; Iter     6/   36] train: loss: 0.3298406
[Epoch 75; Iter    36/   36] train: loss: 0.3784290
[Epoch 75] ogbg-molsider: 0.585269 val loss: 0.589093
[Epoch 75] ogbg-molsider: 0.591498 test loss: 0.629915
[Epoch 76; Iter    30/   36] train: loss: 0.3531488
[Epoch 76] ogbg-molsider: 0.612237 val loss: 0.540809
[Epoch 76] ogbg-molsider: 0.570520 test loss: 0.583560
[Epoch 77; Iter    24/   36] train: loss: 0.3589747
[Epoch 77] ogbg-molsider: 0.606433 val loss: 0.538368
[Epoch 77] ogbg-molsider: 0.597461 test loss: 0.572123
[Epoch 78; Iter    18/   36] train: loss: 0.3715202
[Epoch 78] ogbg-molsider: 0.608338 val loss: 0.575246
[Epoch 78] ogbg-molsider: 0.587965 test loss: 0.611182
[Epoch 79; Iter    12/   36] train: loss: 0.3404821
[Epoch 79] ogbg-molsider: 0.616287 val loss: 0.577266
[Epoch 79] ogbg-molsider: 0.580156 test loss: 0.622808
[Epoch 80; Iter     6/   36] train: loss: 0.3342263
[Epoch 80; Iter    36/   36] train: loss: 0.3618311
[Epoch 80] ogbg-molsider: 0.622643 val loss: 0.550144
[Epoch 80] ogbg-molsider: 0.573749 test loss: 0.607837
[Epoch 33] ogbg-molsider: 0.611953 test loss: 0.494842
[Epoch 34; Iter    12/   36] train: loss: 0.5000725
[Epoch 34] ogbg-molsider: 0.578457 val loss: 0.490188
[Epoch 34] ogbg-molsider: 0.597721 test loss: 0.510233
[Epoch 35; Iter     6/   36] train: loss: 0.5462330
[Epoch 35; Iter    36/   36] train: loss: 0.5072761
[Epoch 35] ogbg-molsider: 0.587578 val loss: 0.491399
[Epoch 35] ogbg-molsider: 0.590006 test loss: 0.504726
[Epoch 36; Iter    30/   36] train: loss: 0.4811558
[Epoch 36] ogbg-molsider: 0.564890 val loss: 0.482126
[Epoch 36] ogbg-molsider: 0.592217 test loss: 0.491413
[Epoch 37; Iter    24/   36] train: loss: 0.4630575
[Epoch 37] ogbg-molsider: 0.575578 val loss: 0.489023
[Epoch 37] ogbg-molsider: 0.603729 test loss: 0.504952
[Epoch 38; Iter    18/   36] train: loss: 0.4957457
[Epoch 38] ogbg-molsider: 0.580506 val loss: 0.479880
[Epoch 38] ogbg-molsider: 0.618810 test loss: 0.491871
[Epoch 39; Iter    12/   36] train: loss: 0.5037125
[Epoch 39] ogbg-molsider: 0.589661 val loss: 0.484394
[Epoch 39] ogbg-molsider: 0.618633 test loss: 0.502284
[Epoch 40; Iter     6/   36] train: loss: 0.4773421
[Epoch 40; Iter    36/   36] train: loss: 0.5184150
[Epoch 40] ogbg-molsider: 0.541803 val loss: 0.502964
[Epoch 40] ogbg-molsider: 0.600158 test loss: 0.513022
[Epoch 41; Iter    30/   36] train: loss: 0.5156377
[Epoch 41] ogbg-molsider: 0.597964 val loss: 0.475261
[Epoch 41] ogbg-molsider: 0.593451 test loss: 0.497801
[Epoch 42; Iter    24/   36] train: loss: 0.4747066
[Epoch 42] ogbg-molsider: 0.594815 val loss: 0.483298
[Epoch 42] ogbg-molsider: 0.617878 test loss: 0.495276
[Epoch 43; Iter    18/   36] train: loss: 0.4947161
[Epoch 43] ogbg-molsider: 0.606171 val loss: 0.473198
[Epoch 43] ogbg-molsider: 0.608603 test loss: 0.490139
[Epoch 44; Iter    12/   36] train: loss: 0.4618759
[Epoch 44] ogbg-molsider: 0.600496 val loss: 0.485198
[Epoch 44] ogbg-molsider: 0.608218 test loss: 0.511112
[Epoch 45; Iter     6/   36] train: loss: 0.4816950
[Epoch 45; Iter    36/   36] train: loss: 0.4514899
[Epoch 45] ogbg-molsider: 0.591721 val loss: 0.764961
[Epoch 45] ogbg-molsider: 0.580366 test loss: 0.630946
[Epoch 46; Iter    30/   36] train: loss: 0.5126225
[Epoch 46] ogbg-molsider: 0.583790 val loss: 0.495453
[Epoch 46] ogbg-molsider: 0.625647 test loss: 0.503108
[Epoch 47; Iter    24/   36] train: loss: 0.4630098
[Epoch 47] ogbg-molsider: 0.580978 val loss: 0.489505
[Epoch 47] ogbg-molsider: 0.603840 test loss: 0.507207
[Epoch 48; Iter    18/   36] train: loss: 0.4991953
[Epoch 48] ogbg-molsider: 0.580553 val loss: 0.495281
[Epoch 48] ogbg-molsider: 0.618838 test loss: 0.506728
[Epoch 49; Iter    12/   36] train: loss: 0.4749456
[Epoch 49] ogbg-molsider: 0.588780 val loss: 0.479939
[Epoch 49] ogbg-molsider: 0.605835 test loss: 0.489594
[Epoch 50; Iter     6/   36] train: loss: 0.4886106
[Epoch 50; Iter    36/   36] train: loss: 0.5259469
[Epoch 50] ogbg-molsider: 0.586167 val loss: 0.501547
[Epoch 50] ogbg-molsider: 0.621065 test loss: 0.513745
[Epoch 51; Iter    30/   36] train: loss: 0.5406562
[Epoch 51] ogbg-molsider: 0.606074 val loss: 0.490386
[Epoch 51] ogbg-molsider: 0.615865 test loss: 0.516443
[Epoch 52; Iter    24/   36] train: loss: 0.4501581
[Epoch 52] ogbg-molsider: 0.633883 val loss: 0.473437
[Epoch 52] ogbg-molsider: 0.593531 test loss: 0.497737
[Epoch 53; Iter    18/   36] train: loss: 0.4598140
[Epoch 53] ogbg-molsider: 0.598583 val loss: 0.492689
[Epoch 53] ogbg-molsider: 0.595510 test loss: 0.521228
[Epoch 54; Iter    12/   36] train: loss: 0.4295415
[Epoch 54] ogbg-molsider: 0.604881 val loss: 0.480076
[Epoch 54] ogbg-molsider: 0.588359 test loss: 0.507933
[Epoch 55; Iter     6/   36] train: loss: 0.4674853
[Epoch 55; Iter    36/   36] train: loss: 0.5273484
[Epoch 55] ogbg-molsider: 0.563110 val loss: 3.290264
[Epoch 55] ogbg-molsider: 0.569687 test loss: 6.501547
[Epoch 56; Iter    30/   36] train: loss: 0.4719634
[Epoch 56] ogbg-molsider: 0.553233 val loss: 0.727841
[Epoch 56] ogbg-molsider: 0.564310 test loss: 0.584373
[Epoch 57; Iter    24/   36] train: loss: 0.4941161
[Epoch 57] ogbg-molsider: 0.566382 val loss: 0.507049
[Epoch 57] ogbg-molsider: 0.609805 test loss: 0.531503
[Epoch 58; Iter    18/   36] train: loss: 0.4947456
[Epoch 58] ogbg-molsider: 0.570011 val loss: 0.502642
[Epoch 58] ogbg-molsider: 0.576494 test loss: 0.528665
[Epoch 59; Iter    12/   36] train: loss: 0.4320258
[Epoch 59] ogbg-molsider: 0.573157 val loss: 0.502321
[Epoch 59] ogbg-molsider: 0.608127 test loss: 0.512446
[Epoch 60; Iter     6/   36] train: loss: 0.4104364
[Epoch 60; Iter    36/   36] train: loss: 0.4554821
[Epoch 60] ogbg-molsider: 0.623255 val loss: 0.535940
[Epoch 60] ogbg-molsider: 0.594349 test loss: 0.582283
[Epoch 61; Iter    30/   36] train: loss: 0.4497009
[Epoch 61] ogbg-molsider: 0.600080 val loss: 0.496376
[Epoch 61] ogbg-molsider: 0.584225 test loss: 0.531138
[Epoch 62; Iter    24/   36] train: loss: 0.3812849
[Epoch 62] ogbg-molsider: 0.585207 val loss: 0.511809
[Epoch 62] ogbg-molsider: 0.578026 test loss: 0.535818
[Epoch 63; Iter    18/   36] train: loss: 0.4533871
[Epoch 63] ogbg-molsider: 0.595058 val loss: 0.494750
[Epoch 63] ogbg-molsider: 0.584105 test loss: 0.516480
[Epoch 64; Iter    12/   36] train: loss: 0.4219020
[Epoch 64] ogbg-molsider: 0.594689 val loss: 0.514490
[Epoch 64] ogbg-molsider: 0.583239 test loss: 0.542697
[Epoch 65; Iter     6/   36] train: loss: 0.3828372
[Epoch 65; Iter    36/   36] train: loss: 0.3995831
[Epoch 65] ogbg-molsider: 0.587045 val loss: 0.512201
[Epoch 65] ogbg-molsider: 0.584292 test loss: 0.556780
[Epoch 66; Iter    30/   36] train: loss: 0.3819422
[Epoch 66] ogbg-molsider: 0.598965 val loss: 0.513904
[Epoch 66] ogbg-molsider: 0.592919 test loss: 0.552324
[Epoch 67; Iter    24/   36] train: loss: 0.3883826
[Epoch 67] ogbg-molsider: 0.603302 val loss: 0.506020
[Epoch 67] ogbg-molsider: 0.585004 test loss: 0.543520
[Epoch 68; Iter    18/   36] train: loss: 0.3878300
[Epoch 68] ogbg-molsider: 0.630078 val loss: 0.487419
[Epoch 68] ogbg-molsider: 0.589260 test loss: 0.524470
[Epoch 69; Iter    12/   36] train: loss: 0.3943435
[Epoch 69] ogbg-molsider: 0.602582 val loss: 0.526709
[Epoch 69] ogbg-molsider: 0.584828 test loss: 0.563331
[Epoch 70; Iter     6/   36] train: loss: 0.3759248
[Epoch 70; Iter    36/   36] train: loss: 0.4168877
[Epoch 70] ogbg-molsider: 0.607219 val loss: 0.502137
[Epoch 70] ogbg-molsider: 0.585987 test loss: 0.527981
[Epoch 71; Iter    30/   36] train: loss: 0.4023128
[Epoch 71] ogbg-molsider: 0.617553 val loss: 0.510600
[Epoch 71] ogbg-molsider: 0.581973 test loss: 0.570182
[Epoch 72; Iter    24/   36] train: loss: 0.3932273
[Epoch 72] ogbg-molsider: 0.591995 val loss: 0.527928
[Epoch 72] ogbg-molsider: 0.575100 test loss: 0.569668
[Epoch 73; Iter    18/   36] train: loss: 0.4229764
[Epoch 73] ogbg-molsider: 0.617968 val loss: 0.517121
[Epoch 73] ogbg-molsider: 0.576625 test loss: 0.592722
[Epoch 74; Iter    12/   36] train: loss: 0.3592120
[Epoch 74] ogbg-molsider: 0.593871 val loss: 0.525291
[Epoch 74] ogbg-molsider: 0.588357 test loss: 0.564588
[Epoch 75; Iter     6/   36] train: loss: 0.3923704
[Epoch 75; Iter    36/   36] train: loss: 0.4299344
[Epoch 75] ogbg-molsider: 0.625689 val loss: 0.528369
[Epoch 75] ogbg-molsider: 0.569834 test loss: 0.584172
[Epoch 76; Iter    30/   36] train: loss: 0.3697838
[Epoch 76] ogbg-molsider: 0.612250 val loss: 0.510224
[Epoch 76] ogbg-molsider: 0.562424 test loss: 0.563350
[Epoch 77; Iter    24/   36] train: loss: 0.3817094
[Epoch 77] ogbg-molsider: 0.582635 val loss: 0.526027
[Epoch 77] ogbg-molsider: 0.564351 test loss: 0.550763
[Epoch 78; Iter    18/   36] train: loss: 0.4080902
[Epoch 78] ogbg-molsider: 0.612968 val loss: 0.562519
[Epoch 78] ogbg-molsider: 0.574783 test loss: 0.644307
[Epoch 79; Iter    12/   36] train: loss: 0.3588941
[Epoch 79] ogbg-molsider: 0.610829 val loss: 0.532294
[Epoch 79] ogbg-molsider: 0.586671 test loss: 0.583064
[Epoch 80; Iter     6/   36] train: loss: 0.3911848
[Epoch 80; Iter    36/   36] train: loss: 0.3833808
[Epoch 80] ogbg-molsider: 0.634671 val loss: 0.529055
[Epoch 80] ogbg-molsider: 0.582061 test loss: 0.609251
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2781645
[Epoch 81] ogbg-molsider: 0.595399 val loss: 0.783891
[Epoch 81] ogbg-molsider: 0.591991 test loss: 0.898633
[Epoch 82; Iter    24/   36] train: loss: 0.2492391
[Epoch 82] ogbg-molsider: 0.605059 val loss: 0.737265
[Epoch 82] ogbg-molsider: 0.583317 test loss: 0.823798
[Epoch 83; Iter    18/   36] train: loss: 0.2620935
[Epoch 83] ogbg-molsider: 0.602146 val loss: 0.784109
[Epoch 83] ogbg-molsider: 0.601462 test loss: 0.859418
[Epoch 84; Iter    12/   36] train: loss: 0.2373406
[Epoch 84] ogbg-molsider: 0.595653 val loss: 0.740138
[Epoch 84] ogbg-molsider: 0.583491 test loss: 0.824763
[Epoch 85; Iter     6/   36] train: loss: 0.2613245
[Epoch 85; Iter    36/   36] train: loss: 0.2654411
[Epoch 85] ogbg-molsider: 0.601928 val loss: 0.878370
[Epoch 85] ogbg-molsider: 0.589071 test loss: 0.818564
[Epoch 86; Iter    30/   36] train: loss: 0.2233177
[Epoch 86] ogbg-molsider: 0.604634 val loss: 0.786072
[Epoch 86] ogbg-molsider: 0.599997 test loss: 0.893056
[Epoch 87; Iter    24/   36] train: loss: 0.2257046
[Epoch 87] ogbg-molsider: 0.585024 val loss: 0.733551
[Epoch 87] ogbg-molsider: 0.601933 test loss: 0.769969
[Epoch 88; Iter    18/   36] train: loss: 0.1826290
[Epoch 88] ogbg-molsider: 0.594517 val loss: 0.874995
[Epoch 88] ogbg-molsider: 0.598795 test loss: 0.869577
[Epoch 89; Iter    12/   36] train: loss: 0.2479315
[Epoch 89] ogbg-molsider: 0.600146 val loss: 0.973689
[Epoch 89] ogbg-molsider: 0.600070 test loss: 0.870037
[Epoch 90; Iter     6/   36] train: loss: 0.2108384
[Epoch 90; Iter    36/   36] train: loss: 0.2429080
[Epoch 90] ogbg-molsider: 0.589163 val loss: 1.007476
[Epoch 90] ogbg-molsider: 0.592728 test loss: 0.914986
[Epoch 91; Iter    30/   36] train: loss: 0.2461350
[Epoch 91] ogbg-molsider: 0.592851 val loss: 1.036697
[Epoch 91] ogbg-molsider: 0.589954 test loss: 1.021192
[Epoch 92; Iter    24/   36] train: loss: 0.2155547
[Epoch 92] ogbg-molsider: 0.589666 val loss: 0.818136
[Epoch 92] ogbg-molsider: 0.595723 test loss: 0.903078
[Epoch 93; Iter    18/   36] train: loss: 0.1973731
[Epoch 93] ogbg-molsider: 0.596591 val loss: 1.378502
[Epoch 93] ogbg-molsider: 0.585623 test loss: 1.150505
[Epoch 94; Iter    12/   36] train: loss: 0.2176536
[Epoch 94] ogbg-molsider: 0.594144 val loss: 0.911021
[Epoch 94] ogbg-molsider: 0.594108 test loss: 0.973676
[Epoch 95; Iter     6/   36] train: loss: 0.2185343
[Epoch 95; Iter    36/   36] train: loss: 0.2514499
[Epoch 95] ogbg-molsider: 0.574779 val loss: 0.743829
[Epoch 95] ogbg-molsider: 0.590806 test loss: 0.764411
[Epoch 96; Iter    30/   36] train: loss: 0.2660306
[Epoch 96] ogbg-molsider: 0.593777 val loss: 1.315486
[Epoch 96] ogbg-molsider: 0.586073 test loss: 1.154461
[Epoch 97; Iter    24/   36] train: loss: 0.2113810
[Epoch 97] ogbg-molsider: 0.601901 val loss: 1.278858
[Epoch 97] ogbg-molsider: 0.574461 test loss: 1.053834
[Epoch 98; Iter    18/   36] train: loss: 0.2361517
[Epoch 98] ogbg-molsider: 0.577615 val loss: 0.938302
[Epoch 98] ogbg-molsider: 0.584257 test loss: 0.871907
[Epoch 99; Iter    12/   36] train: loss: 0.2238266
[Epoch 99] ogbg-molsider: 0.590033 val loss: 1.246692
[Epoch 99] ogbg-molsider: 0.571210 test loss: 1.110255
[Epoch 100; Iter     6/   36] train: loss: 0.1723657
[Epoch 100; Iter    36/   36] train: loss: 0.2377252
[Epoch 100] ogbg-molsider: 0.585121 val loss: 0.866851
[Epoch 100] ogbg-molsider: 0.583304 test loss: 0.940813
[Epoch 101; Iter    30/   36] train: loss: 0.1680130
[Epoch 101] ogbg-molsider: 0.603134 val loss: 1.119428
[Epoch 101] ogbg-molsider: 0.584669 test loss: 1.044999
[Epoch 102; Iter    24/   36] train: loss: 0.1751374
[Epoch 102] ogbg-molsider: 0.587978 val loss: 0.955513
[Epoch 102] ogbg-molsider: 0.585576 test loss: 0.928832
[Epoch 103; Iter    18/   36] train: loss: 0.1721484
[Epoch 103] ogbg-molsider: 0.590041 val loss: 1.098139
[Epoch 103] ogbg-molsider: 0.582586 test loss: 0.967218
[Epoch 104; Iter    12/   36] train: loss: 0.1701158
[Epoch 104] ogbg-molsider: 0.586761 val loss: 1.321624
[Epoch 104] ogbg-molsider: 0.586143 test loss: 1.108725
[Epoch 105; Iter     6/   36] train: loss: 0.1500082
[Epoch 105; Iter    36/   36] train: loss: 0.2116431
[Epoch 105] ogbg-molsider: 0.599726 val loss: 1.034500
[Epoch 105] ogbg-molsider: 0.579857 test loss: 0.977076
[Epoch 106; Iter    30/   36] train: loss: 0.1551871
[Epoch 106] ogbg-molsider: 0.592173 val loss: 1.243734
[Epoch 106] ogbg-molsider: 0.582221 test loss: 1.089479
[Epoch 107; Iter    24/   36] train: loss: 0.1579372
[Epoch 107] ogbg-molsider: 0.580111 val loss: 0.880077
[Epoch 107] ogbg-molsider: 0.583207 test loss: 0.905746
[Epoch 108; Iter    18/   36] train: loss: 0.1454019
[Epoch 108] ogbg-molsider: 0.585217 val loss: 0.928540
[Epoch 108] ogbg-molsider: 0.583959 test loss: 0.966210
[Epoch 109; Iter    12/   36] train: loss: 0.1584567
[Epoch 109] ogbg-molsider: 0.587430 val loss: 1.160562
[Epoch 109] ogbg-molsider: 0.578251 test loss: 1.025595
[Epoch 110; Iter     6/   36] train: loss: 0.1866421
[Epoch 110; Iter    36/   36] train: loss: 0.1660509
[Epoch 110] ogbg-molsider: 0.586828 val loss: 0.912050
[Epoch 110] ogbg-molsider: 0.588485 test loss: 0.959655
[Epoch 111; Iter    30/   36] train: loss: 0.1799922
[Epoch 111] ogbg-molsider: 0.592180 val loss: 1.071193
[Epoch 111] ogbg-molsider: 0.586566 test loss: 1.034699
[Epoch 112; Iter    24/   36] train: loss: 0.1422164
[Epoch 112] ogbg-molsider: 0.586988 val loss: 0.828566
[Epoch 112] ogbg-molsider: 0.587705 test loss: 0.890728
[Epoch 113; Iter    18/   36] train: loss: 0.1392697
[Epoch 113] ogbg-molsider: 0.582881 val loss: 1.016179
[Epoch 113] ogbg-molsider: 0.585727 test loss: 0.929812
[Epoch 114; Iter    12/   36] train: loss: 0.1730855
[Epoch 114] ogbg-molsider: 0.576894 val loss: 0.870752
[Epoch 114] ogbg-molsider: 0.582430 test loss: 0.961505
[Epoch 115; Iter     6/   36] train: loss: 0.1504893
[Epoch 115; Iter    36/   36] train: loss: 0.1394185
[Epoch 115] ogbg-molsider: 0.565817 val loss: 0.868423
[Epoch 115] ogbg-molsider: 0.581995 test loss: 0.938180
[Epoch 116; Iter    30/   36] train: loss: 0.1540937
[Epoch 116] ogbg-molsider: 0.575765 val loss: 0.837552
[Epoch 116] ogbg-molsider: 0.574676 test loss: 0.906077
[Epoch 117; Iter    24/   36] train: loss: 0.1725028
[Epoch 117] ogbg-molsider: 0.578603 val loss: 0.879202
[Epoch 117] ogbg-molsider: 0.580705 test loss: 0.987713
[Epoch 118; Iter    18/   36] train: loss: 0.1248382
[Epoch 118] ogbg-molsider: 0.575976 val loss: 1.397853
[Epoch 118] ogbg-molsider: 0.583478 test loss: 0.973039
[Epoch 119; Iter    12/   36] train: loss: 0.1588320
[Epoch 119] ogbg-molsider: 0.580508 val loss: 0.861105
[Epoch 119] ogbg-molsider: 0.583391 test loss: 0.921782
[Epoch 120; Iter     6/   36] train: loss: 0.1678766
[Epoch 120; Iter    36/   36] train: loss: 0.2926086
[Epoch 120] ogbg-molsider: 0.585205 val loss: 0.985649
[Epoch 120] ogbg-molsider: 0.578952 test loss: 1.001343
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 55.
Statistics on  val_best_checkpoint
mean_pred: -0.6859635710716248
std_pred: 2.1425318717956543
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.674777950299488
rocauc: 0.6263295812108659
ogbg-molsider: 0.6263295812108659
OGBNanLabelBCEWithLogitsLoss: 0.667367136478424
Statistics on  test
mean_pred: -0.7220414280891418
std_pred: 2.162313938140869
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6124295717990574
rocauc: 0.5737214718785627
ogbg-molsider: 0.5737214718785627
OGBNanLabelBCEWithLogitsLoss: 0.7185901284217835
Statistics on  train
mean_pred: 0.39942076802253723
std_pred: 2.3597524166107178
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.811879782097448
rocauc: 0.8608636653435731
ogbg-molsider: 0.8608636653435731
OGBNanLabelBCEWithLogitsLoss: 0.36568471292654675
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2379990
[Epoch 81] ogbg-molsider: 0.563785 val loss: 0.802792
[Epoch 81] ogbg-molsider: 0.600174 test loss: 1.077774
[Epoch 82; Iter    24/   36] train: loss: 0.2657160
[Epoch 82] ogbg-molsider: 0.575699 val loss: 0.994411
[Epoch 82] ogbg-molsider: 0.584471 test loss: 1.129379
[Epoch 83; Iter    18/   36] train: loss: 0.2314757
[Epoch 83] ogbg-molsider: 0.563540 val loss: 0.779053
[Epoch 83] ogbg-molsider: 0.607464 test loss: 0.973776
[Epoch 84; Iter    12/   36] train: loss: 0.2274293
[Epoch 84] ogbg-molsider: 0.568353 val loss: 1.267511
[Epoch 84] ogbg-molsider: 0.593139 test loss: 1.044977
[Epoch 85; Iter     6/   36] train: loss: 0.2743869
[Epoch 85; Iter    36/   36] train: loss: 0.2425517
[Epoch 85] ogbg-molsider: 0.564828 val loss: 1.389492
[Epoch 85] ogbg-molsider: 0.604070 test loss: 1.131593
[Epoch 86; Iter    30/   36] train: loss: 0.2718028
[Epoch 86] ogbg-molsider: 0.566545 val loss: 0.816681
[Epoch 86] ogbg-molsider: 0.587334 test loss: 1.092657
[Epoch 87; Iter    24/   36] train: loss: 0.2381328
[Epoch 87] ogbg-molsider: 0.571111 val loss: 0.826089
[Epoch 87] ogbg-molsider: 0.602250 test loss: 1.055302
[Epoch 88; Iter    18/   36] train: loss: 0.2209522
[Epoch 88] ogbg-molsider: 0.558482 val loss: 0.823400
[Epoch 88] ogbg-molsider: 0.596636 test loss: 1.057025
[Epoch 89; Iter    12/   36] train: loss: 0.2052537
[Epoch 89] ogbg-molsider: 0.556459 val loss: 0.811956
[Epoch 89] ogbg-molsider: 0.601270 test loss: 1.186547
[Epoch 90; Iter     6/   36] train: loss: 0.2058096
[Epoch 90; Iter    36/   36] train: loss: 0.2440563
[Epoch 90] ogbg-molsider: 0.562280 val loss: 1.387610
[Epoch 90] ogbg-molsider: 0.587253 test loss: 1.819782
[Epoch 91; Iter    30/   36] train: loss: 0.2447108
[Epoch 91] ogbg-molsider: 0.563411 val loss: 0.805629
[Epoch 91] ogbg-molsider: 0.598826 test loss: 1.190757
[Epoch 92; Iter    24/   36] train: loss: 0.2238778
[Epoch 92] ogbg-molsider: 0.555995 val loss: 1.341162
[Epoch 92] ogbg-molsider: 0.603517 test loss: 1.270152
[Epoch 93; Iter    18/   36] train: loss: 0.2459305
[Epoch 93] ogbg-molsider: 0.551939 val loss: 0.952483
[Epoch 93] ogbg-molsider: 0.600795 test loss: 1.340599
[Epoch 94; Iter    12/   36] train: loss: 0.1821189
[Epoch 94] ogbg-molsider: 0.553960 val loss: 1.124774
[Epoch 94] ogbg-molsider: 0.602586 test loss: 1.862459
[Epoch 95; Iter     6/   36] train: loss: 0.1914613
[Epoch 95; Iter    36/   36] train: loss: 0.2266158
[Epoch 95] ogbg-molsider: 0.531769 val loss: 1.786673
[Epoch 95] ogbg-molsider: 0.597386 test loss: 2.949096
[Epoch 96; Iter    30/   36] train: loss: 0.2585220
[Epoch 96] ogbg-molsider: 0.555221 val loss: 0.934232
[Epoch 96] ogbg-molsider: 0.603946 test loss: 1.235362
[Epoch 97; Iter    24/   36] train: loss: 0.2080948
[Epoch 97] ogbg-molsider: 0.557619 val loss: 1.089584
[Epoch 97] ogbg-molsider: 0.596986 test loss: 1.993616
[Epoch 98; Iter    18/   36] train: loss: 0.2281121
[Epoch 98] ogbg-molsider: 0.561748 val loss: 0.938387
[Epoch 98] ogbg-molsider: 0.608757 test loss: 1.562830
[Epoch 99; Iter    12/   36] train: loss: 0.2001788
[Epoch 99] ogbg-molsider: 0.564047 val loss: 1.333001
[Epoch 99] ogbg-molsider: 0.595688 test loss: 2.323213
[Epoch 100; Iter     6/   36] train: loss: 0.1852752
[Epoch 100; Iter    36/   36] train: loss: 0.2003478
[Epoch 100] ogbg-molsider: 0.558620 val loss: 1.368328
[Epoch 100] ogbg-molsider: 0.591106 test loss: 2.343048
[Epoch 101; Iter    30/   36] train: loss: 0.2158190
[Epoch 101] ogbg-molsider: 0.560622 val loss: 1.088000
[Epoch 101] ogbg-molsider: 0.602614 test loss: 1.581403
[Epoch 102; Iter    24/   36] train: loss: 0.2169581
[Epoch 102] ogbg-molsider: 0.558537 val loss: 1.839288
[Epoch 102] ogbg-molsider: 0.596514 test loss: 2.977349
[Epoch 103; Iter    18/   36] train: loss: 0.1731817
[Epoch 103] ogbg-molsider: 0.558814 val loss: 2.166778
[Epoch 103] ogbg-molsider: 0.589231 test loss: 3.226403
[Epoch 104; Iter    12/   36] train: loss: 0.1924132
[Epoch 104] ogbg-molsider: 0.551746 val loss: 1.241048
[Epoch 104] ogbg-molsider: 0.589580 test loss: 2.128376
[Epoch 105; Iter     6/   36] train: loss: 0.2144378
[Epoch 105; Iter    36/   36] train: loss: 0.2171374
[Epoch 105] ogbg-molsider: 0.551716 val loss: 1.874087
[Epoch 105] ogbg-molsider: 0.592900 test loss: 2.852229
[Epoch 106; Iter    30/   36] train: loss: 0.1900014
[Epoch 106] ogbg-molsider: 0.544061 val loss: 1.483799
[Epoch 106] ogbg-molsider: 0.600896 test loss: 2.299796
[Epoch 107; Iter    24/   36] train: loss: 0.1553365
[Epoch 107] ogbg-molsider: 0.551815 val loss: 2.750508
[Epoch 107] ogbg-molsider: 0.596939 test loss: 4.151735
[Epoch 108; Iter    18/   36] train: loss: 0.1775190
[Epoch 108] ogbg-molsider: 0.550135 val loss: 2.201526
[Epoch 108] ogbg-molsider: 0.590219 test loss: 3.397584
[Epoch 109; Iter    12/   36] train: loss: 0.1854704
[Epoch 109] ogbg-molsider: 0.539871 val loss: 1.313676
[Epoch 109] ogbg-molsider: 0.602095 test loss: 2.043928
[Epoch 110; Iter     6/   36] train: loss: 0.1620459
[Epoch 110; Iter    36/   36] train: loss: 0.2153396
[Epoch 110] ogbg-molsider: 0.557461 val loss: 1.858658
[Epoch 110] ogbg-molsider: 0.595305 test loss: 2.723428
[Epoch 111; Iter    30/   36] train: loss: 0.2182763
[Epoch 111] ogbg-molsider: 0.550067 val loss: 2.633219
[Epoch 111] ogbg-molsider: 0.597940 test loss: 4.212021
[Epoch 112; Iter    24/   36] train: loss: 0.1668300
[Epoch 112] ogbg-molsider: 0.539452 val loss: 1.289407
[Epoch 112] ogbg-molsider: 0.597848 test loss: 1.995008
[Epoch 113; Iter    18/   36] train: loss: 0.1706892
[Epoch 113] ogbg-molsider: 0.548823 val loss: 2.035337
[Epoch 113] ogbg-molsider: 0.590321 test loss: 3.191326
[Epoch 114; Iter    12/   36] train: loss: 0.1568554
[Epoch 114] ogbg-molsider: 0.553418 val loss: 2.125116
[Epoch 114] ogbg-molsider: 0.584934 test loss: 2.984768
[Epoch 115; Iter     6/   36] train: loss: 0.1512932
[Epoch 115; Iter    36/   36] train: loss: 0.2073699
[Epoch 115] ogbg-molsider: 0.548744 val loss: 1.875372
[Epoch 115] ogbg-molsider: 0.590981 test loss: 2.759672
[Epoch 116; Iter    30/   36] train: loss: 0.1717453
[Epoch 116] ogbg-molsider: 0.552332 val loss: 1.808435
[Epoch 116] ogbg-molsider: 0.592038 test loss: 1.984140
[Epoch 117; Iter    24/   36] train: loss: 0.1828269
[Epoch 117] ogbg-molsider: 0.551709 val loss: 1.740545
[Epoch 117] ogbg-molsider: 0.586588 test loss: 2.427659
[Epoch 118; Iter    18/   36] train: loss: 0.1547920
[Epoch 118] ogbg-molsider: 0.551841 val loss: 1.821914
[Epoch 118] ogbg-molsider: 0.582107 test loss: 2.429846
[Epoch 119; Iter    12/   36] train: loss: 0.1578182
[Epoch 119] ogbg-molsider: 0.547873 val loss: 2.451269
[Epoch 119] ogbg-molsider: 0.586228 test loss: 3.481720
[Epoch 120; Iter     6/   36] train: loss: 0.1493546
[Epoch 120; Iter    36/   36] train: loss: 0.1758009
[Epoch 120] ogbg-molsider: 0.550504 val loss: 2.460849
[Epoch 120] ogbg-molsider: 0.580106 test loss: 3.744519
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 55.
Statistics on  val_best_checkpoint
mean_pred: 1.2737928628921509
std_pred: 7.509760856628418
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6482036157606835
rocauc: 0.6196583199387568
ogbg-molsider: 0.6196583199387568
OGBNanLabelBCEWithLogitsLoss: 0.7891483187675477
Statistics on  test
mean_pred: 1.2153891324996948
std_pred: 2.7571544647216797
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6261269104608238
rocauc: 0.5820160211001416
ogbg-molsider: 0.5820160211001416
OGBNanLabelBCEWithLogitsLoss: 0.682629132270813
Statistics on  train
mean_pred: 0.5465347170829773
std_pred: 2.542062759399414
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8220645780826898
rocauc: 0.8692524395747336
ogbg-molsider: 0.8692524395747336
OGBNanLabelBCEWithLogitsLoss: 0.3489033290081554
[Epoch 81; Iter    30/   36] train: loss: 0.1909289
[Epoch 81] ogbg-molsider: 0.504476 val loss: 0.836334
[Epoch 81] ogbg-molsider: 0.557634 test loss: 0.975420
[Epoch 82; Iter    24/   36] train: loss: 0.2337777
[Epoch 82] ogbg-molsider: 0.520223 val loss: 0.797515
[Epoch 82] ogbg-molsider: 0.562448 test loss: 1.018729
[Epoch 83; Iter    18/   36] train: loss: 0.2280827
[Epoch 83] ogbg-molsider: 0.517975 val loss: 0.761287
[Epoch 83] ogbg-molsider: 0.563642 test loss: 0.826204
[Epoch 84; Iter    12/   36] train: loss: 0.2212098
[Epoch 84] ogbg-molsider: 0.522900 val loss: 0.945598
[Epoch 84] ogbg-molsider: 0.564887 test loss: 1.028795
[Epoch 85; Iter     6/   36] train: loss: 0.2188328
[Epoch 85; Iter    36/   36] train: loss: 0.2194114
[Epoch 85] ogbg-molsider: 0.514730 val loss: 0.965830
[Epoch 85] ogbg-molsider: 0.562203 test loss: 1.275385
[Epoch 86; Iter    30/   36] train: loss: 0.2311784
[Epoch 86] ogbg-molsider: 0.509447 val loss: 0.788071
[Epoch 86] ogbg-molsider: 0.566503 test loss: 0.944603
[Epoch 87; Iter    24/   36] train: loss: 0.2164144
[Epoch 87] ogbg-molsider: 0.515109 val loss: 0.898212
[Epoch 87] ogbg-molsider: 0.574213 test loss: 1.020554
[Epoch 88; Iter    18/   36] train: loss: 0.2222495
[Epoch 88] ogbg-molsider: 0.523890 val loss: 0.863035
[Epoch 88] ogbg-molsider: 0.561000 test loss: 1.032171
[Epoch 89; Iter    12/   36] train: loss: 0.2237009
[Epoch 89] ogbg-molsider: 0.505444 val loss: 1.377774
[Epoch 89] ogbg-molsider: 0.549803 test loss: 1.427775
[Epoch 90; Iter     6/   36] train: loss: 0.1927838
[Epoch 90; Iter    36/   36] train: loss: 0.2248165
[Epoch 90] ogbg-molsider: 0.518860 val loss: 0.978830
[Epoch 90] ogbg-molsider: 0.558233 test loss: 1.341937
[Epoch 91; Iter    30/   36] train: loss: 0.2099127
[Epoch 91] ogbg-molsider: 0.518635 val loss: 1.077051
[Epoch 91] ogbg-molsider: 0.553057 test loss: 1.229807
[Epoch 92; Iter    24/   36] train: loss: 0.2221059
[Epoch 92] ogbg-molsider: 0.520642 val loss: 1.067993
[Epoch 92] ogbg-molsider: 0.551208 test loss: 1.339181
[Epoch 93; Iter    18/   36] train: loss: 0.1581815
[Epoch 93] ogbg-molsider: 0.497608 val loss: 1.186145
[Epoch 93] ogbg-molsider: 0.544102 test loss: 1.956832
[Epoch 94; Iter    12/   36] train: loss: 0.1924794
[Epoch 94] ogbg-molsider: 0.523675 val loss: 1.069594
[Epoch 94] ogbg-molsider: 0.553001 test loss: 1.395134
[Epoch 95; Iter     6/   36] train: loss: 0.2201376
[Epoch 95; Iter    36/   36] train: loss: 0.1901772
[Epoch 95] ogbg-molsider: 0.511764 val loss: 1.050597
[Epoch 95] ogbg-molsider: 0.551678 test loss: 1.353407
[Epoch 96; Iter    30/   36] train: loss: 0.2331900
[Epoch 96] ogbg-molsider: 0.505436 val loss: 1.419565
[Epoch 96] ogbg-molsider: 0.558040 test loss: 1.906045
[Epoch 97; Iter    24/   36] train: loss: 0.2168582
[Epoch 97] ogbg-molsider: 0.506464 val loss: 1.173767
[Epoch 97] ogbg-molsider: 0.555185 test loss: 1.552810
[Epoch 98; Iter    18/   36] train: loss: 0.2118795
[Epoch 98] ogbg-molsider: 0.520569 val loss: 1.341403
[Epoch 98] ogbg-molsider: 0.558016 test loss: 1.593845
[Epoch 99; Iter    12/   36] train: loss: 0.2147314
[Epoch 99] ogbg-molsider: 0.495386 val loss: 1.265326
[Epoch 99] ogbg-molsider: 0.558453 test loss: 1.680848
[Epoch 100; Iter     6/   36] train: loss: 0.1471014
[Epoch 100; Iter    36/   36] train: loss: 0.1781666
[Epoch 100] ogbg-molsider: 0.502218 val loss: 1.308825
[Epoch 100] ogbg-molsider: 0.550568 test loss: 1.955738
[Epoch 101; Iter    30/   36] train: loss: 0.1536036
[Epoch 101] ogbg-molsider: 0.501526 val loss: 1.073443
[Epoch 101] ogbg-molsider: 0.547815 test loss: 1.549945
[Epoch 102; Iter    24/   36] train: loss: 0.1969881
[Epoch 102] ogbg-molsider: 0.506128 val loss: 1.060312
[Epoch 102] ogbg-molsider: 0.569114 test loss: 1.340805
[Epoch 103; Iter    18/   36] train: loss: 0.1999632
[Epoch 103] ogbg-molsider: 0.508359 val loss: 1.290137
[Epoch 103] ogbg-molsider: 0.559389 test loss: 2.184891
[Epoch 104; Iter    12/   36] train: loss: 0.1580097
[Epoch 104] ogbg-molsider: 0.505111 val loss: 1.166347
[Epoch 104] ogbg-molsider: 0.550341 test loss: 1.444675
[Epoch 105; Iter     6/   36] train: loss: 0.1464093
[Epoch 105; Iter    36/   36] train: loss: 0.1903809
[Epoch 105] ogbg-molsider: 0.506593 val loss: 2.173865
[Epoch 105] ogbg-molsider: 0.540193 test loss: 2.840503
[Epoch 106; Iter    30/   36] train: loss: 0.1881359
[Epoch 106] ogbg-molsider: 0.503155 val loss: 1.399648
[Epoch 106] ogbg-molsider: 0.564851 test loss: 1.608778
[Epoch 107; Iter    24/   36] train: loss: 0.1622696
[Epoch 107] ogbg-molsider: 0.508257 val loss: 2.416098
[Epoch 107] ogbg-molsider: 0.543666 test loss: 2.929182
[Epoch 108; Iter    18/   36] train: loss: 0.1763227
[Epoch 108] ogbg-molsider: 0.505253 val loss: 1.575991
[Epoch 108] ogbg-molsider: 0.546378 test loss: 2.156945
[Epoch 109; Iter    12/   36] train: loss: 0.1745407
[Epoch 109] ogbg-molsider: 0.505977 val loss: 1.224560
[Epoch 109] ogbg-molsider: 0.554851 test loss: 1.659732
[Epoch 110; Iter     6/   36] train: loss: 0.1669603
[Epoch 110; Iter    36/   36] train: loss: 0.1647675
[Epoch 110] ogbg-molsider: 0.507136 val loss: 1.383738
[Epoch 110] ogbg-molsider: 0.554895 test loss: 1.655018
[Epoch 111; Iter    30/   36] train: loss: 0.1384913
[Epoch 111] ogbg-molsider: 0.506899 val loss: 2.589857
[Epoch 111] ogbg-molsider: 0.530061 test loss: 4.317764
[Epoch 112; Iter    24/   36] train: loss: 0.1618364
[Epoch 112] ogbg-molsider: 0.508507 val loss: 1.300961
[Epoch 112] ogbg-molsider: 0.554481 test loss: 1.568120
[Epoch 113; Iter    18/   36] train: loss: 0.1326842
[Epoch 113] ogbg-molsider: 0.502625 val loss: 1.623090
[Epoch 113] ogbg-molsider: 0.529093 test loss: 4.383924
[Epoch 114; Iter    12/   36] train: loss: 0.1435158
[Epoch 114] ogbg-molsider: 0.500338 val loss: 1.574357
[Epoch 114] ogbg-molsider: 0.549223 test loss: 2.008752
[Epoch 115; Iter     6/   36] train: loss: 0.1236938
[Epoch 115; Iter    36/   36] train: loss: 0.1488387
[Epoch 115] ogbg-molsider: 0.507239 val loss: 2.080911
[Epoch 115] ogbg-molsider: 0.528552 test loss: 4.854575
[Epoch 116; Iter    30/   36] train: loss: 0.1241036
[Epoch 116] ogbg-molsider: 0.499830 val loss: 1.599017
[Epoch 116] ogbg-molsider: 0.542768 test loss: 2.634865
[Epoch 117; Iter    24/   36] train: loss: 0.1374573
[Epoch 117] ogbg-molsider: 0.496147 val loss: 1.510421
[Epoch 117] ogbg-molsider: 0.553257 test loss: 1.922090
[Epoch 118; Iter    18/   36] train: loss: 0.1695427
[Epoch 118] ogbg-molsider: 0.507389 val loss: 2.712872
[Epoch 118] ogbg-molsider: 0.545807 test loss: 2.963698
[Epoch 119; Iter    12/   36] train: loss: 0.1267643
[Epoch 119] ogbg-molsider: 0.494747 val loss: 1.816338
[Epoch 119] ogbg-molsider: 0.546781 test loss: 2.916184
[Epoch 120; Iter     6/   36] train: loss: 0.1678861
[Epoch 120; Iter    36/   36] train: loss: 0.1264561
[Epoch 120] ogbg-molsider: 0.514425 val loss: 5.547144
[Epoch 120] ogbg-molsider: 0.530655 test loss: 6.801192
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 42.
Statistics on  val_best_checkpoint
mean_pred: -0.4575868546962738
std_pred: 3.9449455738067627
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6275160490280386
rocauc: 0.5524962876683839
ogbg-molsider: 0.5524962876683839
OGBNanLabelBCEWithLogitsLoss: 1.0138509750366211
Statistics on  test
mean_pred: -0.3288557231426239
std_pred: 3.756648063659668
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.5984818993369084
rocauc: 0.5267812345043221
ogbg-molsider: 0.5267812345043221
OGBNanLabelBCEWithLogitsLoss: 0.821588146686554
Statistics on  train
mean_pred: 0.5278186798095703
std_pred: 1.901322841644287
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.7346703539026385
rocauc: 0.7768466556282824
ogbg-molsider: 0.7768466556282824
OGBNanLabelBCEWithLogitsLoss: 0.4454532191157341
[Epoch 81; Iter    30/   36] train: loss: 0.2934680
[Epoch 81] ogbg-molsider: 0.563846 val loss: 1.343063
[Epoch 81] ogbg-molsider: 0.527633 test loss: 1.468808
[Epoch 82; Iter    24/   36] train: loss: 0.2419557
[Epoch 82] ogbg-molsider: 0.561510 val loss: 1.511003
[Epoch 82] ogbg-molsider: 0.524100 test loss: 1.633780
[Epoch 83; Iter    18/   36] train: loss: 0.2407446
[Epoch 83] ogbg-molsider: 0.569760 val loss: 1.353796
[Epoch 83] ogbg-molsider: 0.527906 test loss: 1.529645
[Epoch 84; Iter    12/   36] train: loss: 0.2247954
[Epoch 84] ogbg-molsider: 0.569393 val loss: 1.347172
[Epoch 84] ogbg-molsider: 0.521253 test loss: 1.485057
[Epoch 85; Iter     6/   36] train: loss: 0.2593465
[Epoch 85; Iter    36/   36] train: loss: 0.2512768
[Epoch 85] ogbg-molsider: 0.563260 val loss: 1.600364
[Epoch 85] ogbg-molsider: 0.527942 test loss: 1.759713
[Epoch 86; Iter    30/   36] train: loss: 0.2137204
[Epoch 86] ogbg-molsider: 0.574304 val loss: 1.551494
[Epoch 86] ogbg-molsider: 0.522581 test loss: 1.653893
[Epoch 87; Iter    24/   36] train: loss: 0.2240975
[Epoch 87] ogbg-molsider: 0.575097 val loss: 1.519606
[Epoch 87] ogbg-molsider: 0.523629 test loss: 1.653063
[Epoch 88; Iter    18/   36] train: loss: 0.1858985
[Epoch 88] ogbg-molsider: 0.557636 val loss: 1.700708
[Epoch 88] ogbg-molsider: 0.509996 test loss: 1.832055
[Epoch 89; Iter    12/   36] train: loss: 0.2387469
[Epoch 89] ogbg-molsider: 0.579226 val loss: 1.412549
[Epoch 89] ogbg-molsider: 0.518436 test loss: 1.554411
[Epoch 90; Iter     6/   36] train: loss: 0.2011808
[Epoch 90; Iter    36/   36] train: loss: 0.2243519
[Epoch 90] ogbg-molsider: 0.563998 val loss: 1.460311
[Epoch 90] ogbg-molsider: 0.518218 test loss: 1.538545
[Epoch 91; Iter    30/   36] train: loss: 0.2264042
[Epoch 91] ogbg-molsider: 0.573720 val loss: 1.468469
[Epoch 91] ogbg-molsider: 0.527283 test loss: 1.596672
[Epoch 92; Iter    24/   36] train: loss: 0.2114145
[Epoch 92] ogbg-molsider: 0.560015 val loss: 1.396903
[Epoch 92] ogbg-molsider: 0.522528 test loss: 1.510554
[Epoch 93; Iter    18/   36] train: loss: 0.2032810
[Epoch 93] ogbg-molsider: 0.566172 val loss: 1.567882
[Epoch 93] ogbg-molsider: 0.531349 test loss: 1.616517
[Epoch 94; Iter    12/   36] train: loss: 0.2113043
[Epoch 94] ogbg-molsider: 0.569771 val loss: 1.513440
[Epoch 94] ogbg-molsider: 0.518946 test loss: 1.645946
[Epoch 95; Iter     6/   36] train: loss: 0.2097510
[Epoch 95; Iter    36/   36] train: loss: 0.2427704
[Epoch 95] ogbg-molsider: 0.562284 val loss: 1.621164
[Epoch 95] ogbg-molsider: 0.519721 test loss: 1.759594
[Epoch 96; Iter    30/   36] train: loss: 0.2507762
[Epoch 96] ogbg-molsider: 0.568086 val loss: 1.805067
[Epoch 96] ogbg-molsider: 0.513874 test loss: 1.879507
[Epoch 97; Iter    24/   36] train: loss: 0.2145107
[Epoch 97] ogbg-molsider: 0.550590 val loss: 1.765587
[Epoch 97] ogbg-molsider: 0.530415 test loss: 1.955459
[Epoch 98; Iter    18/   36] train: loss: 0.2239331
[Epoch 98] ogbg-molsider: 0.558840 val loss: 1.634782
[Epoch 98] ogbg-molsider: 0.531667 test loss: 1.807903
[Epoch 99; Iter    12/   36] train: loss: 0.2015609
[Epoch 99] ogbg-molsider: 0.565815 val loss: 1.925969
[Epoch 99] ogbg-molsider: 0.528368 test loss: 2.059140
[Epoch 100; Iter     6/   36] train: loss: 0.1793967
[Epoch 100; Iter    36/   36] train: loss: 0.2295049
[Epoch 100] ogbg-molsider: 0.579002 val loss: 1.656594
[Epoch 100] ogbg-molsider: 0.532172 test loss: 1.770095
[Epoch 101; Iter    30/   36] train: loss: 0.1835444
[Epoch 101] ogbg-molsider: 0.560677 val loss: 2.109747
[Epoch 101] ogbg-molsider: 0.514922 test loss: 2.367525
[Epoch 102; Iter    24/   36] train: loss: 0.1864221
[Epoch 102] ogbg-molsider: 0.568423 val loss: 1.650714
[Epoch 102] ogbg-molsider: 0.528023 test loss: 1.797115
[Epoch 103; Iter    18/   36] train: loss: 0.1758583
[Epoch 103] ogbg-molsider: 0.566145 val loss: 1.737607
[Epoch 103] ogbg-molsider: 0.529686 test loss: 1.850018
[Epoch 104; Iter    12/   36] train: loss: 0.1617126
[Epoch 104] ogbg-molsider: 0.563460 val loss: 1.791954
[Epoch 104] ogbg-molsider: 0.524030 test loss: 1.885292
[Epoch 105; Iter     6/   36] train: loss: 0.1499365
[Epoch 105; Iter    36/   36] train: loss: 0.2015195
[Epoch 105] ogbg-molsider: 0.561490 val loss: 1.772616
[Epoch 105] ogbg-molsider: 0.520185 test loss: 1.927639
[Epoch 106; Iter    30/   36] train: loss: 0.1391707
[Epoch 106] ogbg-molsider: 0.564918 val loss: 1.633657
[Epoch 106] ogbg-molsider: 0.530394 test loss: 1.753499
[Epoch 107; Iter    24/   36] train: loss: 0.1483254
[Epoch 107] ogbg-molsider: 0.562890 val loss: 1.522837
[Epoch 107] ogbg-molsider: 0.528507 test loss: 1.594752
[Epoch 108; Iter    18/   36] train: loss: 0.1250229
[Epoch 108] ogbg-molsider: 0.560368 val loss: 1.997376
[Epoch 108] ogbg-molsider: 0.511292 test loss: 2.133061
[Epoch 109; Iter    12/   36] train: loss: 0.1493701
[Epoch 109] ogbg-molsider: 0.559348 val loss: 1.861131
[Epoch 109] ogbg-molsider: 0.521062 test loss: 2.018485
[Epoch 110; Iter     6/   36] train: loss: 0.1843418
[Epoch 110; Iter    36/   36] train: loss: 0.1425133
[Epoch 110] ogbg-molsider: 0.557064 val loss: 1.814720
[Epoch 110] ogbg-molsider: 0.524463 test loss: 1.968451
[Epoch 111; Iter    30/   36] train: loss: 0.1692132
[Epoch 111] ogbg-molsider: 0.560433 val loss: 1.824152
[Epoch 111] ogbg-molsider: 0.523621 test loss: 2.015020
[Epoch 112; Iter    24/   36] train: loss: 0.1476104
[Epoch 112] ogbg-molsider: 0.566570 val loss: 1.995559
[Epoch 112] ogbg-molsider: 0.515520 test loss: 2.131629
[Epoch 113; Iter    18/   36] train: loss: 0.1414244
[Epoch 113] ogbg-molsider: 0.562826 val loss: 1.952491
[Epoch 113] ogbg-molsider: 0.520214 test loss: 2.131621
[Epoch 114; Iter    12/   36] train: loss: 0.1539168
[Epoch 114] ogbg-molsider: 0.552513 val loss: 1.752484
[Epoch 114] ogbg-molsider: 0.540257 test loss: 1.915577
[Epoch 115; Iter     6/   36] train: loss: 0.1517865
[Epoch 115; Iter    36/   36] train: loss: 0.1360566
[Epoch 115] ogbg-molsider: 0.569188 val loss: 1.626090
[Epoch 115] ogbg-molsider: 0.528990 test loss: 1.764329
[Epoch 116; Iter    30/   36] train: loss: 0.1504876
[Epoch 116] ogbg-molsider: 0.561961 val loss: 1.986084
[Epoch 116] ogbg-molsider: 0.521814 test loss: 2.211802
[Epoch 117; Iter    24/   36] train: loss: 0.1675995
[Epoch 117] ogbg-molsider: 0.569747 val loss: 1.953366
[Epoch 117] ogbg-molsider: 0.522811 test loss: 2.155898
[Epoch 118; Iter    18/   36] train: loss: 0.1253276
[Epoch 118] ogbg-molsider: 0.562765 val loss: 1.866305
[Epoch 118] ogbg-molsider: 0.524354 test loss: 2.020098
[Epoch 119; Iter    12/   36] train: loss: 0.1529477
[Epoch 119] ogbg-molsider: 0.557501 val loss: 2.114768
[Epoch 119] ogbg-molsider: 0.523933 test loss: 2.300788
[Epoch 120; Iter     6/   36] train: loss: 0.1800900
[Epoch 120; Iter    36/   36] train: loss: 0.2751387
[Epoch 120] ogbg-molsider: 0.551814 val loss: 2.093969
[Epoch 120] ogbg-molsider: 0.524459 test loss: 2.291108
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 41.
Statistics on  val_best_checkpoint
mean_pred: -0.4453290104866028
std_pred: 1.6757800579071045
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6484401226054562
rocauc: 0.5843883549937278
ogbg-molsider: 0.5843883549937278
OGBNanLabelBCEWithLogitsLoss: 0.6024293303489685
Statistics on  test
mean_pred: -0.5095874667167664
std_pred: 1.7396519184112549
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.5854456248249315
rocauc: 0.5067830263783117
ogbg-molsider: 0.5067830263783117
OGBNanLabelBCEWithLogitsLoss: 0.638001811504364
Statistics on  train
mean_pred: 0.40574130415916443
std_pred: 1.630920648574829
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.7110015430596475
rocauc: 0.7436939334943297
ogbg-molsider: 0.7436939334943297
OGBNanLabelBCEWithLogitsLoss: 0.4662481149037679
[Epoch 81; Iter    30/   36] train: loss: 0.3260706
[Epoch 81] ogbg-molsider: 0.598978 val loss: 9.329406
[Epoch 81] ogbg-molsider: 0.597213 test loss: 6.681852
[Epoch 82; Iter    24/   36] train: loss: 0.2592831
[Epoch 82] ogbg-molsider: 0.594517 val loss: 4.858014
[Epoch 82] ogbg-molsider: 0.593256 test loss: 3.156202
[Epoch 83; Iter    18/   36] train: loss: 0.2715510
[Epoch 83] ogbg-molsider: 0.607138 val loss: 15.226007
[Epoch 83] ogbg-molsider: 0.592783 test loss: 16.044511
[Epoch 84; Iter    12/   36] train: loss: 0.2383731
[Epoch 84] ogbg-molsider: 0.598604 val loss: 10.591403
[Epoch 84] ogbg-molsider: 0.592318 test loss: 8.923770
[Epoch 85; Iter     6/   36] train: loss: 0.2706847
[Epoch 85; Iter    36/   36] train: loss: 0.2740121
[Epoch 85] ogbg-molsider: 0.597313 val loss: 4.562189
[Epoch 85] ogbg-molsider: 0.600996 test loss: 3.827673
[Epoch 86; Iter    30/   36] train: loss: 0.2376143
[Epoch 86] ogbg-molsider: 0.583358 val loss: 6.340422
[Epoch 86] ogbg-molsider: 0.598413 test loss: 4.440089
[Epoch 87; Iter    24/   36] train: loss: 0.2252202
[Epoch 87] ogbg-molsider: 0.586933 val loss: 1.534531
[Epoch 87] ogbg-molsider: 0.604400 test loss: 0.650875
[Epoch 88; Iter    18/   36] train: loss: 0.2044471
[Epoch 88] ogbg-molsider: 0.591545 val loss: 10.010380
[Epoch 88] ogbg-molsider: 0.594773 test loss: 8.095614
[Epoch 89; Iter    12/   36] train: loss: 0.2586235
[Epoch 89] ogbg-molsider: 0.588738 val loss: 4.953912
[Epoch 89] ogbg-molsider: 0.597870 test loss: 3.281767
[Epoch 90; Iter     6/   36] train: loss: 0.2089666
[Epoch 90; Iter    36/   36] train: loss: 0.2383574
[Epoch 90] ogbg-molsider: 0.584476 val loss: 0.728772
[Epoch 90] ogbg-molsider: 0.596743 test loss: 0.703751
[Epoch 91; Iter    30/   36] train: loss: 0.2618530
[Epoch 91] ogbg-molsider: 0.595677 val loss: 2.897145
[Epoch 91] ogbg-molsider: 0.590197 test loss: 2.052548
[Epoch 92; Iter    24/   36] train: loss: 0.2201447
[Epoch 92] ogbg-molsider: 0.593099 val loss: 5.774660
[Epoch 92] ogbg-molsider: 0.610454 test loss: 3.256009
[Epoch 93; Iter    18/   36] train: loss: 0.2148450
[Epoch 93] ogbg-molsider: 0.580975 val loss: 10.960690
[Epoch 93] ogbg-molsider: 0.596174 test loss: 9.745888
[Epoch 94; Iter    12/   36] train: loss: 0.2319089
[Epoch 94] ogbg-molsider: 0.599938 val loss: 11.831376
[Epoch 94] ogbg-molsider: 0.593007 test loss: 11.037661
[Epoch 95; Iter     6/   36] train: loss: 0.2364894
[Epoch 95; Iter    36/   36] train: loss: 0.2967047
[Epoch 95] ogbg-molsider: 0.579574 val loss: 13.123199
[Epoch 95] ogbg-molsider: 0.592620 test loss: 12.511320
[Epoch 96; Iter    30/   36] train: loss: 0.2849182
[Epoch 96] ogbg-molsider: 0.590627 val loss: 10.571868
[Epoch 96] ogbg-molsider: 0.599617 test loss: 10.103089
[Epoch 97; Iter    24/   36] train: loss: 0.2348392
[Epoch 97] ogbg-molsider: 0.593230 val loss: 7.296357
[Epoch 97] ogbg-molsider: 0.603007 test loss: 6.110927
[Epoch 98; Iter    18/   36] train: loss: 0.2361374
[Epoch 98] ogbg-molsider: 0.585161 val loss: 3.368956
[Epoch 98] ogbg-molsider: 0.592212 test loss: 1.375819
[Epoch 99; Iter    12/   36] train: loss: 0.2360064
[Epoch 99] ogbg-molsider: 0.599111 val loss: 8.417502
[Epoch 99] ogbg-molsider: 0.600458 test loss: 6.093617
[Epoch 100; Iter     6/   36] train: loss: 0.1770137
[Epoch 100; Iter    36/   36] train: loss: 0.2524176
[Epoch 100] ogbg-molsider: 0.585368 val loss: 18.251706
[Epoch 100] ogbg-molsider: 0.592716 test loss: 21.469842
[Epoch 101; Iter    30/   36] train: loss: 0.2140116
[Epoch 101] ogbg-molsider: 0.599148 val loss: 9.505846
[Epoch 101] ogbg-molsider: 0.600282 test loss: 7.694570
[Epoch 102; Iter    24/   36] train: loss: 0.2088182
[Epoch 102] ogbg-molsider: 0.582254 val loss: 13.537427
[Epoch 102] ogbg-molsider: 0.602085 test loss: 12.089003
[Epoch 103; Iter    18/   36] train: loss: 0.1963782
[Epoch 103] ogbg-molsider: 0.579350 val loss: 5.387618
[Epoch 103] ogbg-molsider: 0.594588 test loss: 2.812826
[Epoch 104; Iter    12/   36] train: loss: 0.1872119
[Epoch 104] ogbg-molsider: 0.582161 val loss: 5.713756
[Epoch 104] ogbg-molsider: 0.609341 test loss: 3.228208
[Epoch 105; Iter     6/   36] train: loss: 0.1796791
[Epoch 105; Iter    36/   36] train: loss: 0.2416314
[Epoch 105] ogbg-molsider: 0.590627 val loss: 7.162029
[Epoch 105] ogbg-molsider: 0.590072 test loss: 3.537149
[Epoch 106; Iter    30/   36] train: loss: 0.1838655
[Epoch 106] ogbg-molsider: 0.585856 val loss: 7.694291
[Epoch 106] ogbg-molsider: 0.594902 test loss: 4.203574
[Epoch 107; Iter    24/   36] train: loss: 0.1940745
[Epoch 107] ogbg-molsider: 0.592822 val loss: 0.696524
[Epoch 107] ogbg-molsider: 0.597452 test loss: 0.743857
[Epoch 108; Iter    18/   36] train: loss: 0.1552694
[Epoch 108] ogbg-molsider: 0.591498 val loss: 5.689472
[Epoch 108] ogbg-molsider: 0.595023 test loss: 2.687841
[Epoch 109; Iter    12/   36] train: loss: 0.1645291
[Epoch 109] ogbg-molsider: 0.592050 val loss: 2.773103
[Epoch 109] ogbg-molsider: 0.603621 test loss: 0.936792
[Epoch 110; Iter     6/   36] train: loss: 0.2016433
[Epoch 110; Iter    36/   36] train: loss: 0.1866033
[Epoch 110] ogbg-molsider: 0.592659 val loss: 3.402604
[Epoch 110] ogbg-molsider: 0.601172 test loss: 1.302880
[Epoch 111; Iter    30/   36] train: loss: 0.1930183
[Epoch 111] ogbg-molsider: 0.589413 val loss: 2.130974
[Epoch 111] ogbg-molsider: 0.597760 test loss: 0.859339
[Epoch 112; Iter    24/   36] train: loss: 0.1639413
[Epoch 112] ogbg-molsider: 0.579865 val loss: 0.847235
[Epoch 112] ogbg-molsider: 0.612510 test loss: 0.735075
[Epoch 113; Iter    18/   36] train: loss: 0.1669706
[Epoch 113] ogbg-molsider: 0.594743 val loss: 2.412498
[Epoch 113] ogbg-molsider: 0.605765 test loss: 1.102101
[Epoch 114; Iter    12/   36] train: loss: 0.1806496
[Epoch 114] ogbg-molsider: 0.586626 val loss: 1.494508
[Epoch 114] ogbg-molsider: 0.614051 test loss: 0.733571
[Epoch 115; Iter     6/   36] train: loss: 0.1701377
[Epoch 115; Iter    36/   36] train: loss: 0.1764866
[Epoch 115] ogbg-molsider: 0.588970 val loss: 2.955509
[Epoch 115] ogbg-molsider: 0.605385 test loss: 1.489080
[Epoch 116; Iter    30/   36] train: loss: 0.1669724
[Epoch 116] ogbg-molsider: 0.592695 val loss: 4.952260
[Epoch 116] ogbg-molsider: 0.611852 test loss: 2.731120
[Epoch 117; Iter    24/   36] train: loss: 0.1847358
[Epoch 117] ogbg-molsider: 0.598405 val loss: 2.412730
[Epoch 117] ogbg-molsider: 0.607590 test loss: 1.413341
[Epoch 118; Iter    18/   36] train: loss: 0.1559001
[Epoch 118] ogbg-molsider: 0.595541 val loss: 4.161438
[Epoch 118] ogbg-molsider: 0.604230 test loss: 2.225090
[Epoch 119; Iter    12/   36] train: loss: 0.1764926
[Epoch 119] ogbg-molsider: 0.597650 val loss: 4.412865
[Epoch 119] ogbg-molsider: 0.598576 test loss: 1.988912
[Epoch 120; Iter     6/   36] train: loss: 0.1879442
[Epoch 120; Iter    36/   36] train: loss: 0.2916214
[Epoch 120] ogbg-molsider: 0.591288 val loss: 4.322943
[Epoch 120] ogbg-molsider: 0.603125 test loss: 2.021997
[Epoch 121; Iter    30/   36] train: loss: 0.1653553
[Epoch 121] ogbg-molsider: 0.590715 val loss: 4.719483
[Epoch 121] ogbg-molsider: 0.604040 test loss: 2.613675
[Epoch 122; Iter    24/   36] train: loss: 0.1690323
[Epoch 122] ogbg-molsider: 0.583045 val loss: 3.418604
[Epoch 122] ogbg-molsider: 0.597842 test loss: 1.524390
[Epoch 123; Iter    18/   36] train: loss: 0.2031714
[Epoch 123] ogbg-molsider: 0.591934 val loss: 3.679619
[Epoch 123] ogbg-molsider: 0.599063 test loss: 2.053681
[Epoch 124; Iter    12/   36] train: loss: 0.1802967
[Epoch 124] ogbg-molsider: 0.586612 val loss: 2.776236
[Epoch 124] ogbg-molsider: 0.606395 test loss: 1.279598
[Epoch 125; Iter     6/   36] train: loss: 0.1561061
[Epoch 125; Iter    36/   36] train: loss: 0.1401545
[Epoch 125] ogbg-molsider: 0.576627 val loss: 1.384115
[Epoch 125] ogbg-molsider: 0.612642 test loss: 1.034660
[Epoch 126; Iter    30/   36] train: loss: 0.1440287
[Epoch 126] ogbg-molsider: 0.591010 val loss: 6.451370
[Epoch 126] ogbg-molsider: 0.594115 test loss: 5.269836
[Epoch 127; Iter    24/   36] train: loss: 0.1663517
[Epoch 127] ogbg-molsider: 0.592219 val loss: 7.693847
[Epoch 127] ogbg-molsider: 0.605275 test loss: 5.902838
[Epoch 128; Iter    18/   36] train: loss: 0.1490822[Epoch 81; Iter    30/   36] train: loss: 0.2216503
[Epoch 81] ogbg-molsider: 0.570254 val loss: 0.652110
[Epoch 81] ogbg-molsider: 0.603903 test loss: 0.611549
[Epoch 82; Iter    24/   36] train: loss: 0.2601460
[Epoch 82] ogbg-molsider: 0.588677 val loss: 0.612771
[Epoch 82] ogbg-molsider: 0.611167 test loss: 0.593776
[Epoch 83; Iter    18/   36] train: loss: 0.2562849
[Epoch 83] ogbg-molsider: 0.577372 val loss: 0.619155
[Epoch 83] ogbg-molsider: 0.607756 test loss: 0.609128
[Epoch 84; Iter    12/   36] train: loss: 0.2537621
[Epoch 84] ogbg-molsider: 0.581787 val loss: 0.649845
[Epoch 84] ogbg-molsider: 0.604891 test loss: 0.625398
[Epoch 85; Iter     6/   36] train: loss: 0.2555551
[Epoch 85; Iter    36/   36] train: loss: 0.2523499
[Epoch 85] ogbg-molsider: 0.579022 val loss: 0.623072
[Epoch 85] ogbg-molsider: 0.592892 test loss: 0.621811
[Epoch 86; Iter    30/   36] train: loss: 0.2787327
[Epoch 86] ogbg-molsider: 0.582169 val loss: 0.612198
[Epoch 86] ogbg-molsider: 0.604117 test loss: 0.609399
[Epoch 87; Iter    24/   36] train: loss: 0.2623734
[Epoch 87] ogbg-molsider: 0.589753 val loss: 0.658582
[Epoch 87] ogbg-molsider: 0.598080 test loss: 0.651973
[Epoch 88; Iter    18/   36] train: loss: 0.2637334
[Epoch 88] ogbg-molsider: 0.585764 val loss: 0.685341
[Epoch 88] ogbg-molsider: 0.596678 test loss: 0.691659
[Epoch 89; Iter    12/   36] train: loss: 0.2527856
[Epoch 89] ogbg-molsider: 0.588348 val loss: 0.623976
[Epoch 89] ogbg-molsider: 0.602843 test loss: 0.639736
[Epoch 90; Iter     6/   36] train: loss: 0.2352061
[Epoch 90; Iter    36/   36] train: loss: 0.2699657
[Epoch 90] ogbg-molsider: 0.600632 val loss: 0.641553
[Epoch 90] ogbg-molsider: 0.595254 test loss: 0.640666
[Epoch 91; Iter    30/   36] train: loss: 0.2455011
[Epoch 91] ogbg-molsider: 0.572950 val loss: 0.691214
[Epoch 91] ogbg-molsider: 0.600548 test loss: 0.678877
[Epoch 92; Iter    24/   36] train: loss: 0.2607049
[Epoch 92] ogbg-molsider: 0.582897 val loss: 0.681034
[Epoch 92] ogbg-molsider: 0.587253 test loss: 0.691518
[Epoch 93; Iter    18/   36] train: loss: 0.1972506
[Epoch 93] ogbg-molsider: 0.585492 val loss: 0.670093
[Epoch 93] ogbg-molsider: 0.601265 test loss: 0.678017
[Epoch 94; Iter    12/   36] train: loss: 0.2365614
[Epoch 94] ogbg-molsider: 0.588242 val loss: 0.664721
[Epoch 94] ogbg-molsider: 0.602861 test loss: 0.664830
[Epoch 95; Iter     6/   36] train: loss: 0.2506239
[Epoch 95; Iter    36/   36] train: loss: 0.2343860
[Epoch 95] ogbg-molsider: 0.580827 val loss: 0.675422
[Epoch 95] ogbg-molsider: 0.600755 test loss: 0.707340
[Epoch 96; Iter    30/   36] train: loss: 0.2680079
[Epoch 96] ogbg-molsider: 0.571916 val loss: 0.683766
[Epoch 96] ogbg-molsider: 0.591195 test loss: 0.658280
[Epoch 97; Iter    24/   36] train: loss: 0.2706474
[Epoch 97] ogbg-molsider: 0.582706 val loss: 0.715955
[Epoch 97] ogbg-molsider: 0.594046 test loss: 0.747283
[Epoch 98; Iter    18/   36] train: loss: 0.2420301
[Epoch 98] ogbg-molsider: 0.570314 val loss: 0.765760
[Epoch 98] ogbg-molsider: 0.585553 test loss: 0.793615
[Epoch 99; Iter    12/   36] train: loss: 0.2686419
[Epoch 99] ogbg-molsider: 0.588593 val loss: 0.684398
[Epoch 99] ogbg-molsider: 0.598182 test loss: 0.681231
[Epoch 100; Iter     6/   36] train: loss: 0.1848294
[Epoch 100; Iter    36/   36] train: loss: 0.2592164
[Epoch 100] ogbg-molsider: 0.576097 val loss: 0.725054
[Epoch 100] ogbg-molsider: 0.587134 test loss: 0.721131
[Epoch 101; Iter    30/   36] train: loss: 0.1933191
[Epoch 101] ogbg-molsider: 0.568332 val loss: 0.711717
[Epoch 101] ogbg-molsider: 0.592213 test loss: 0.736665
[Epoch 102; Iter    24/   36] train: loss: 0.2198424
[Epoch 102] ogbg-molsider: 0.588315 val loss: 0.730495
[Epoch 102] ogbg-molsider: 0.585843 test loss: 0.754457
[Epoch 103; Iter    18/   36] train: loss: 0.2024908
[Epoch 103] ogbg-molsider: 0.576267 val loss: 0.709935
[Epoch 103] ogbg-molsider: 0.584608 test loss: 0.741132
[Epoch 104; Iter    12/   36] train: loss: 0.2010858
[Epoch 104] ogbg-molsider: 0.590125 val loss: 0.746836
[Epoch 104] ogbg-molsider: 0.582794 test loss: 0.782149
[Epoch 105; Iter     6/   36] train: loss: 0.1790322
[Epoch 105; Iter    36/   36] train: loss: 0.2226436
[Epoch 105] ogbg-molsider: 0.586452 val loss: 0.717270
[Epoch 105] ogbg-molsider: 0.587854 test loss: 0.759367
[Epoch 106; Iter    30/   36] train: loss: 0.2251349
[Epoch 106] ogbg-molsider: 0.582415 val loss: 0.724462
[Epoch 106] ogbg-molsider: 0.586263 test loss: 0.733324
[Epoch 107; Iter    24/   36] train: loss: 0.1972931
[Epoch 107] ogbg-molsider: 0.582801 val loss: 0.720211
[Epoch 107] ogbg-molsider: 0.589396 test loss: 0.723338
[Epoch 108; Iter    18/   36] train: loss: 0.1780543
[Epoch 108] ogbg-molsider: 0.585816 val loss: 0.719710
[Epoch 108] ogbg-molsider: 0.588115 test loss: 0.745179
[Epoch 109; Iter    12/   36] train: loss: 0.1869775
[Epoch 109] ogbg-molsider: 0.572699 val loss: 0.751412
[Epoch 109] ogbg-molsider: 0.585840 test loss: 0.775958
[Epoch 110; Iter     6/   36] train: loss: 0.1972306
[Epoch 110; Iter    36/   36] train: loss: 0.1744666
[Epoch 110] ogbg-molsider: 0.584110 val loss: 0.766968
[Epoch 110] ogbg-molsider: 0.569841 test loss: 0.811273
[Epoch 111; Iter    30/   36] train: loss: 0.1648400
[Epoch 111] ogbg-molsider: 0.579256 val loss: 0.730712
[Epoch 111] ogbg-molsider: 0.585209 test loss: 0.748022
[Epoch 112; Iter    24/   36] train: loss: 0.2127177
[Epoch 112] ogbg-molsider: 0.578676 val loss: 0.744517
[Epoch 112] ogbg-molsider: 0.579721 test loss: 0.757895
[Epoch 113; Iter    18/   36] train: loss: 0.1559830
[Epoch 113] ogbg-molsider: 0.576926 val loss: 0.878558
[Epoch 113] ogbg-molsider: 0.574380 test loss: 0.819932
[Epoch 114; Iter    12/   36] train: loss: 0.1774019
[Epoch 114] ogbg-molsider: 0.576698 val loss: 0.750771
[Epoch 114] ogbg-molsider: 0.586758 test loss: 0.780707
[Epoch 115; Iter     6/   36] train: loss: 0.1865346
[Epoch 115; Iter    36/   36] train: loss: 0.2116648
[Epoch 115] ogbg-molsider: 0.587540 val loss: 0.804365
[Epoch 115] ogbg-molsider: 0.577928 test loss: 0.832841
[Epoch 116; Iter    30/   36] train: loss: 0.1645034
[Epoch 116] ogbg-molsider: 0.575655 val loss: 0.850783
[Epoch 116] ogbg-molsider: 0.582756 test loss: 0.800492
[Epoch 117; Iter    24/   36] train: loss: 0.1559904
[Epoch 117] ogbg-molsider: 0.568994 val loss: 0.869683
[Epoch 117] ogbg-molsider: 0.582091 test loss: 0.853114
[Epoch 118; Iter    18/   36] train: loss: 0.1851441
[Epoch 118] ogbg-molsider: 0.579283 val loss: 0.780753
[Epoch 118] ogbg-molsider: 0.566761 test loss: 0.848405
[Epoch 119; Iter    12/   36] train: loss: 0.1714264
[Epoch 119] ogbg-molsider: 0.561756 val loss: 0.789662
[Epoch 119] ogbg-molsider: 0.576021 test loss: 0.787350
[Epoch 120; Iter     6/   36] train: loss: 0.2061800
[Epoch 120; Iter    36/   36] train: loss: 0.1510694
[Epoch 120] ogbg-molsider: 0.567806 val loss: 1.090675
[Epoch 120] ogbg-molsider: 0.576366 test loss: 0.841177
[Epoch 121; Iter    30/   36] train: loss: 0.2037340
[Epoch 121] ogbg-molsider: 0.564741 val loss: 0.771848
[Epoch 121] ogbg-molsider: 0.576249 test loss: 0.788472
[Epoch 122; Iter    24/   36] train: loss: 0.1830822
[Epoch 122] ogbg-molsider: 0.564635 val loss: 0.742337
[Epoch 122] ogbg-molsider: 0.584986 test loss: 0.755712
[Epoch 123; Iter    18/   36] train: loss: 0.1925727
[Epoch 123] ogbg-molsider: 0.557856 val loss: 1.212683
[Epoch 123] ogbg-molsider: 0.573266 test loss: 0.893328
[Epoch 124; Iter    12/   36] train: loss: 0.1672980
[Epoch 124] ogbg-molsider: 0.569949 val loss: 0.981039
[Epoch 124] ogbg-molsider: 0.581648 test loss: 0.822764
[Epoch 125; Iter     6/   36] train: loss: 0.1454089
[Epoch 125; Iter    36/   36] train: loss: 0.2005708
[Epoch 125] ogbg-molsider: 0.570199 val loss: 0.896899
[Epoch 125] ogbg-molsider: 0.577884 test loss: 0.836808
[Epoch 126; Iter    30/   36] train: loss: 0.1504071
[Epoch 126] ogbg-molsider: 0.564111 val loss: 1.161563
[Epoch 126] ogbg-molsider: 0.577766 test loss: 0.841705
[Epoch 127; Iter    24/   36] train: loss: 0.1607718
[Epoch 127] ogbg-molsider: 0.564447 val loss: 0.813893
[Epoch 127] ogbg-molsider: 0.575981 test loss: 0.818187
[Epoch 128; Iter    18/   36] train: loss: 0.1666322
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2309225
[Epoch 81] ogbg-molsider: 0.545747 val loss: 15.820648
[Epoch 81] ogbg-molsider: 0.565700 test loss: 21.674926
[Epoch 82; Iter    24/   36] train: loss: 0.2503537
[Epoch 82] ogbg-molsider: 0.556019 val loss: 13.224027
[Epoch 82] ogbg-molsider: 0.587474 test loss: 22.470126
[Epoch 83; Iter    18/   36] train: loss: 0.2359097
[Epoch 83] ogbg-molsider: 0.547686 val loss: 9.096373
[Epoch 83] ogbg-molsider: 0.582555 test loss: 9.400738
[Epoch 84; Iter    12/   36] train: loss: 0.2294519
[Epoch 84] ogbg-molsider: 0.552840 val loss: 9.509192
[Epoch 84] ogbg-molsider: 0.581296 test loss: 16.698795
[Epoch 85; Iter     6/   36] train: loss: 0.2446109
[Epoch 85; Iter    36/   36] train: loss: 0.2203067
[Epoch 85] ogbg-molsider: 0.559880 val loss: 5.560129
[Epoch 85] ogbg-molsider: 0.561604 test loss: 4.615091
[Epoch 86; Iter    30/   36] train: loss: 0.2361815
[Epoch 86] ogbg-molsider: 0.569224 val loss: 6.341437
[Epoch 86] ogbg-molsider: 0.564110 test loss: 5.622793
[Epoch 87; Iter    24/   36] train: loss: 0.2443332
[Epoch 87] ogbg-molsider: 0.549125 val loss: 1.439552
[Epoch 87] ogbg-molsider: 0.582723 test loss: 1.918435
[Epoch 88; Iter    18/   36] train: loss: 0.2363378
[Epoch 88] ogbg-molsider: 0.546427 val loss: 2.844222
[Epoch 88] ogbg-molsider: 0.574629 test loss: 2.189463
[Epoch 89; Iter    12/   36] train: loss: 0.2199740
[Epoch 89] ogbg-molsider: 0.547290 val loss: 1.426794
[Epoch 89] ogbg-molsider: 0.583172 test loss: 1.245878
[Epoch 90; Iter     6/   36] train: loss: 0.2211147
[Epoch 90; Iter    36/   36] train: loss: 0.2523057
[Epoch 90] ogbg-molsider: 0.549781 val loss: 7.079371
[Epoch 90] ogbg-molsider: 0.577412 test loss: 3.735493
[Epoch 91; Iter    30/   36] train: loss: 0.2465183
[Epoch 91] ogbg-molsider: 0.565424 val loss: 5.610072
[Epoch 91] ogbg-molsider: 0.610597 test loss: 3.632739
[Epoch 92; Iter    24/   36] train: loss: 0.2253096
[Epoch 92] ogbg-molsider: 0.552832 val loss: 1.512031
[Epoch 92] ogbg-molsider: 0.584922 test loss: 1.885155
[Epoch 93; Iter    18/   36] train: loss: 0.2488156
[Epoch 93] ogbg-molsider: 0.557126 val loss: 11.315212
[Epoch 93] ogbg-molsider: 0.585751 test loss: 11.877543
[Epoch 94; Iter    12/   36] train: loss: 0.1965938
[Epoch 94] ogbg-molsider: 0.547204 val loss: 10.499562
[Epoch 94] ogbg-molsider: 0.571349 test loss: 9.962835
[Epoch 95; Iter     6/   36] train: loss: 0.1863969
[Epoch 95; Iter    36/   36] train: loss: 0.2359547
[Epoch 95] ogbg-molsider: 0.550911 val loss: 2.586571
[Epoch 95] ogbg-molsider: 0.566407 test loss: 2.141750
[Epoch 96; Iter    30/   36] train: loss: 0.1931464
[Epoch 96] ogbg-molsider: 0.546130 val loss: 10.600722
[Epoch 96] ogbg-molsider: 0.576823 test loss: 14.274873
[Epoch 97; Iter    24/   36] train: loss: 0.1823596
[Epoch 97] ogbg-molsider: 0.557138 val loss: 3.910247
[Epoch 97] ogbg-molsider: 0.567848 test loss: 2.510139
[Epoch 98; Iter    18/   36] train: loss: 0.2129787
[Epoch 98] ogbg-molsider: 0.539347 val loss: 2.879489
[Epoch 98] ogbg-molsider: 0.581663 test loss: 3.817490
[Epoch 99; Iter    12/   36] train: loss: 0.1627769
[Epoch 99] ogbg-molsider: 0.551641 val loss: 1.656745
[Epoch 99] ogbg-molsider: 0.584894 test loss: 2.704962
[Epoch 100; Iter     6/   36] train: loss: 0.1653426
[Epoch 100; Iter    36/   36] train: loss: 0.1980926
[Epoch 100] ogbg-molsider: 0.555526 val loss: 15.614885
[Epoch 100] ogbg-molsider: 0.576201 test loss: 23.360247
[Epoch 101; Iter    30/   36] train: loss: 0.2375677
[Epoch 101] ogbg-molsider: 0.560373 val loss: 12.886353
[Epoch 101] ogbg-molsider: 0.580806 test loss: 15.114556
[Epoch 102; Iter    24/   36] train: loss: 0.1935265
[Epoch 102] ogbg-molsider: 0.545904 val loss: 3.542314
[Epoch 102] ogbg-molsider: 0.573436 test loss: 5.307007
[Epoch 103; Iter    18/   36] train: loss: 0.2015732
[Epoch 103] ogbg-molsider: 0.546314 val loss: 27.086513
[Epoch 103] ogbg-molsider: 0.556415 test loss: 40.979106
[Epoch 104; Iter    12/   36] train: loss: 0.2149662
[Epoch 104] ogbg-molsider: 0.532184 val loss: 12.541457
[Epoch 104] ogbg-molsider: 0.566387 test loss: 14.562730
[Epoch 105; Iter     6/   36] train: loss: 0.1987435
[Epoch 105; Iter    36/   36] train: loss: 0.2081304
[Epoch 105] ogbg-molsider: 0.541462 val loss: 2.269513
[Epoch 105] ogbg-molsider: 0.572713 test loss: 1.561711
[Epoch 106; Iter    30/   36] train: loss: 0.1876691
[Epoch 106] ogbg-molsider: 0.519981 val loss: 1.258374
[Epoch 106] ogbg-molsider: 0.568948 test loss: 1.419098
[Epoch 107; Iter    24/   36] train: loss: 0.1538794
[Epoch 107] ogbg-molsider: 0.532668 val loss: 1.213536
[Epoch 107] ogbg-molsider: 0.568171 test loss: 1.447457
[Epoch 108; Iter    18/   36] train: loss: 0.1611261
[Epoch 108] ogbg-molsider: 0.556277 val loss: 1.554018
[Epoch 108] ogbg-molsider: 0.584161 test loss: 1.562000
[Epoch 109; Iter    12/   36] train: loss: 0.1626563
[Epoch 109] ogbg-molsider: 0.545826 val loss: 4.901682
[Epoch 109] ogbg-molsider: 0.575738 test loss: 2.393599
[Epoch 110; Iter     6/   36] train: loss: 0.1581185
[Epoch 110; Iter    36/   36] train: loss: 0.1916485
[Epoch 110] ogbg-molsider: 0.546521 val loss: 3.212932
[Epoch 110] ogbg-molsider: 0.567962 test loss: 1.343518
[Epoch 111; Iter    30/   36] train: loss: 0.2053728
[Epoch 111] ogbg-molsider: 0.540728 val loss: 2.515414
[Epoch 111] ogbg-molsider: 0.553995 test loss: 2.945761
[Epoch 112; Iter    24/   36] train: loss: 0.1595880
[Epoch 112] ogbg-molsider: 0.548299 val loss: 8.148797
[Epoch 112] ogbg-molsider: 0.574766 test loss: 5.083506
[Epoch 113; Iter    18/   36] train: loss: 0.1675483
[Epoch 113] ogbg-molsider: 0.557288 val loss: 6.490772
[Epoch 113] ogbg-molsider: 0.575582 test loss: 5.007772
[Epoch 114; Iter    12/   36] train: loss: 0.1766747
[Epoch 114] ogbg-molsider: 0.565446 val loss: 2.099328
[Epoch 114] ogbg-molsider: 0.566828 test loss: 2.171046
[Epoch 115; Iter     6/   36] train: loss: 0.1243001
[Epoch 115; Iter    36/   36] train: loss: 0.1959272
[Epoch 115] ogbg-molsider: 0.529809 val loss: 5.197935
[Epoch 115] ogbg-molsider: 0.565372 test loss: 3.580400
[Epoch 116; Iter    30/   36] train: loss: 0.1502422
[Epoch 116] ogbg-molsider: 0.556433 val loss: 10.374785
[Epoch 116] ogbg-molsider: 0.555557 test loss: 9.584194
[Epoch 117; Iter    24/   36] train: loss: 0.1852338
[Epoch 117] ogbg-molsider: 0.550311 val loss: 10.161447
[Epoch 117] ogbg-molsider: 0.583187 test loss: 7.559143
[Epoch 118; Iter    18/   36] train: loss: 0.1332444
[Epoch 118] ogbg-molsider: 0.556682 val loss: 22.019551
[Epoch 118] ogbg-molsider: 0.569704 test loss: 35.846845
[Epoch 119; Iter    12/   36] train: loss: 0.1483134
[Epoch 119] ogbg-molsider: 0.557672 val loss: 27.773989
[Epoch 119] ogbg-molsider: 0.564321 test loss: 41.704160
[Epoch 120; Iter     6/   36] train: loss: 0.1229035
[Epoch 120; Iter    36/   36] train: loss: 0.1347750
[Epoch 120] ogbg-molsider: 0.556519 val loss: 4.151753
[Epoch 120] ogbg-molsider: 0.569860 test loss: 2.290798
[Epoch 121; Iter    30/   36] train: loss: 0.1066166
[Epoch 121] ogbg-molsider: 0.557041 val loss: 16.857761
[Epoch 121] ogbg-molsider: 0.555213 test loss: 23.101591
[Epoch 122; Iter    24/   36] train: loss: 0.1191737
[Epoch 122] ogbg-molsider: 0.553347 val loss: 22.600437
[Epoch 122] ogbg-molsider: 0.565846 test loss: 30.913931
[Epoch 123; Iter    18/   36] train: loss: 0.1030749
[Epoch 123] ogbg-molsider: 0.559393 val loss: 2.703407
[Epoch 123] ogbg-molsider: 0.563444 test loss: 1.973153
[Epoch 124; Iter    12/   36] train: loss: 0.1132615
[Epoch 124] ogbg-molsider: 0.560046 val loss: 10.511826
[Epoch 124] ogbg-molsider: 0.566139 test loss: 12.336540
[Epoch 125; Iter     6/   36] train: loss: 0.0993839
[Epoch 125; Iter    36/   36] train: loss: 0.1505359
[Epoch 125] ogbg-molsider: 0.538996 val loss: 6.028802
[Epoch 125] ogbg-molsider: 0.581268 test loss: 3.806450
Early stopping criterion based on -ogbg-molsider- that should be max reached after 125 epochs. Best model checkpoint was in epoch 65.
Statistics on  val_best_checkpoint
mean_pred: -2.053511381149292
std_pred: 223.78997802734375
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6332130598006123
rocauc: 0.583945209448109
ogbg-molsider: 0.583945209448109
OGBNanLabelBCEWithLogitsLoss: 25.931513500213622
Statistics on  test
mean_pred: -3.021357774734497
std_pred: 252.16065979003906
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6185121553962167
rocauc: 0.5780560877797958
ogbg-molsider: 0.5780560877797958
OGBNanLabelBCEWithLogitsLoss: 49.32796058654785
Statistics on  train
mean_pred: 0.6343206763267517
std_pred: 2.9601633548736572
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8612214903894432
rocauc: 0.9036017395141078
ogbg-molsider: 0.9036017395141078
OGBNanLabelBCEWithLogitsLoss: 0.31287312921550536
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.2.yml --seed 4 --device cuda:3
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.2.yml --seed 5 --device cuda:3
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.2.yml --seed 6 --device cuda:3
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.564994 val loss: 0.804958
[Epoch 128] ogbg-molsider: 0.583366 test loss: 0.833119
[Epoch 129; Iter    12/   36] train: loss: 0.1766447
[Epoch 129] ogbg-molsider: 0.552266 val loss: 1.097196
[Epoch 129] ogbg-molsider: 0.571397 test loss: 0.847156
[Epoch 130; Iter     6/   36] train: loss: 0.1451942
[Epoch 130; Iter    36/   36] train: loss: 0.1486002
[Epoch 130] ogbg-molsider: 0.559158 val loss: 0.911009
[Epoch 130] ogbg-molsider: 0.578847 test loss: 0.867748
[Epoch 131; Iter    30/   36] train: loss: 0.1681035
[Epoch 131] ogbg-molsider: 0.571300 val loss: 0.873261
[Epoch 131] ogbg-molsider: 0.569062 test loss: 0.890395
[Epoch 132; Iter    24/   36] train: loss: 0.1375028
[Epoch 132] ogbg-molsider: 0.558233 val loss: 1.029411
[Epoch 132] ogbg-molsider: 0.574671 test loss: 0.869366
Early stopping criterion based on -ogbg-molsider- that should be max reached after 132 epochs. Best model checkpoint was in epoch 72.
Statistics on  val_best_checkpoint
mean_pred: 0.2923831641674042
std_pred: 2.6678454875946045
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.664548983226242
rocauc: 0.6229989876535093
ogbg-molsider: 0.6229989876535093
OGBNanLabelBCEWithLogitsLoss: 0.5469677865505218
Statistics on  test
mean_pred: 0.4862338900566101
std_pred: 2.6890456676483154
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6393193313396052
rocauc: 0.6018029570749653
ogbg-molsider: 0.6018029570749653
OGBNanLabelBCEWithLogitsLoss: 0.5599494636058807
Statistics on  train
mean_pred: 1.0749200582504272
std_pred: 3.2524735927581787
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8567492071806007
rocauc: 0.905526044174027
ogbg-molsider: 0.905526044174027
OGBNanLabelBCEWithLogitsLoss: 0.31241244905524784
[Epoch 81; Iter    30/   36] train: loss: 0.2681740
[Epoch 81] ogbg-molsider: 0.582438 val loss: 0.599335
[Epoch 81] ogbg-molsider: 0.593083 test loss: 0.643462
[Epoch 82; Iter    24/   36] train: loss: 0.2845248
[Epoch 82] ogbg-molsider: 0.568634 val loss: 0.609915
[Epoch 82] ogbg-molsider: 0.581365 test loss: 0.674253
[Epoch 83; Iter    18/   36] train: loss: 0.2527920
[Epoch 83] ogbg-molsider: 0.603868 val loss: 0.596124
[Epoch 83] ogbg-molsider: 0.595543 test loss: 0.641740
[Epoch 84; Iter    12/   36] train: loss: 0.2697046
[Epoch 84] ogbg-molsider: 0.601822 val loss: 0.601691
[Epoch 84] ogbg-molsider: 0.610837 test loss: 0.656235
[Epoch 85; Iter     6/   36] train: loss: 0.2999191
[Epoch 85; Iter    36/   36] train: loss: 0.2839682
[Epoch 85] ogbg-molsider: 0.627950 val loss: 0.565788
[Epoch 85] ogbg-molsider: 0.618892 test loss: 0.609102
[Epoch 86; Iter    30/   36] train: loss: 0.2835045
[Epoch 86] ogbg-molsider: 0.609686 val loss: 0.582568
[Epoch 86] ogbg-molsider: 0.613623 test loss: 0.629093
[Epoch 87; Iter    24/   36] train: loss: 0.2692713
[Epoch 87] ogbg-molsider: 0.620595 val loss: 0.586973
[Epoch 87] ogbg-molsider: 0.608884 test loss: 0.637973
[Epoch 88; Iter    18/   36] train: loss: 0.2698475
[Epoch 88] ogbg-molsider: 0.619826 val loss: 0.627764
[Epoch 88] ogbg-molsider: 0.613103 test loss: 0.659607
[Epoch 89; Iter    12/   36] train: loss: 0.2459760
[Epoch 89] ogbg-molsider: 0.615745 val loss: 0.583847
[Epoch 89] ogbg-molsider: 0.610613 test loss: 0.640075
[Epoch 90; Iter     6/   36] train: loss: 0.2470336
[Epoch 90; Iter    36/   36] train: loss: 0.2807983
[Epoch 90] ogbg-molsider: 0.603054 val loss: 0.616934
[Epoch 90] ogbg-molsider: 0.604632 test loss: 0.663221
[Epoch 91; Iter    30/   36] train: loss: 0.2907161
[Epoch 91] ogbg-molsider: 0.630286 val loss: 0.582458
[Epoch 91] ogbg-molsider: 0.605243 test loss: 0.638020
[Epoch 92; Iter    24/   36] train: loss: 0.2903955
[Epoch 92] ogbg-molsider: 0.608181 val loss: 0.592095
[Epoch 92] ogbg-molsider: 0.608852 test loss: 0.607136
[Epoch 93; Iter    18/   36] train: loss: 0.2912965
[Epoch 93] ogbg-molsider: 0.610063 val loss: 0.599867
[Epoch 93] ogbg-molsider: 0.603454 test loss: 0.636204
[Epoch 94; Iter    12/   36] train: loss: 0.2133618
[Epoch 94] ogbg-molsider: 0.595595 val loss: 0.672190
[Epoch 94] ogbg-molsider: 0.608301 test loss: 0.730842
[Epoch 95; Iter     6/   36] train: loss: 0.2241071
[Epoch 95; Iter    36/   36] train: loss: 0.2700063
[Epoch 95] ogbg-molsider: 0.610996 val loss: 0.601461
[Epoch 95] ogbg-molsider: 0.587436 test loss: 0.647709
[Epoch 96; Iter    30/   36] train: loss: 0.2332705
[Epoch 96] ogbg-molsider: 0.611025 val loss: 0.599089
[Epoch 96] ogbg-molsider: 0.602402 test loss: 0.637485
[Epoch 97; Iter    24/   36] train: loss: 0.2289760
[Epoch 97] ogbg-molsider: 0.597349 val loss: 0.631557
[Epoch 97] ogbg-molsider: 0.594794 test loss: 0.688270
[Epoch 98; Iter    18/   36] train: loss: 0.2684317
[Epoch 98] ogbg-molsider: 0.609722 val loss: 0.611309
[Epoch 98] ogbg-molsider: 0.613573 test loss: 0.794986
[Epoch 99; Iter    12/   36] train: loss: 0.2205391
[Epoch 99] ogbg-molsider: 0.619020 val loss: 0.655278
[Epoch 99] ogbg-molsider: 0.594831 test loss: 0.722881
[Epoch 100; Iter     6/   36] train: loss: 0.2222681
[Epoch 100; Iter    36/   36] train: loss: 0.2407302
[Epoch 100] ogbg-molsider: 0.595847 val loss: 0.610596
[Epoch 100] ogbg-molsider: 0.593718 test loss: 0.640054
[Epoch 101; Iter    30/   36] train: loss: 0.2805267
[Epoch 101] ogbg-molsider: 0.608292 val loss: 0.629173
[Epoch 101] ogbg-molsider: 0.605501 test loss: 0.681674
[Epoch 102; Iter    24/   36] train: loss: 0.2788884
[Epoch 102] ogbg-molsider: 0.587703 val loss: 0.628132
[Epoch 102] ogbg-molsider: 0.593141 test loss: 0.638413
[Epoch 103; Iter    18/   36] train: loss: 0.2424698
[Epoch 103] ogbg-molsider: 0.617571 val loss: 0.634666
[Epoch 103] ogbg-molsider: 0.609473 test loss: 0.677598
[Epoch 104; Iter    12/   36] train: loss: 0.2461704
[Epoch 104] ogbg-molsider: 0.595759 val loss: 0.641498
[Epoch 104] ogbg-molsider: 0.605889 test loss: 0.687950
[Epoch 105; Iter     6/   36] train: loss: 0.2503577
[Epoch 105; Iter    36/   36] train: loss: 0.2600859
[Epoch 105] ogbg-molsider: 0.591256 val loss: 0.664639
[Epoch 105] ogbg-molsider: 0.595469 test loss: 0.706619
[Epoch 106; Iter    30/   36] train: loss: 0.2171425
[Epoch 106] ogbg-molsider: 0.592643 val loss: 0.652514
[Epoch 106] ogbg-molsider: 0.597189 test loss: 0.690173
[Epoch 107; Iter    24/   36] train: loss: 0.1978942
[Epoch 107] ogbg-molsider: 0.586285 val loss: 0.707892
[Epoch 107] ogbg-molsider: 0.596903 test loss: 0.772792
[Epoch 108; Iter    18/   36] train: loss: 0.2139973
[Epoch 108] ogbg-molsider: 0.610125 val loss: 0.634444
[Epoch 108] ogbg-molsider: 0.610102 test loss: 0.643939
[Epoch 109; Iter    12/   36] train: loss: 0.2237890
[Epoch 109] ogbg-molsider: 0.599071 val loss: 0.692612
[Epoch 109] ogbg-molsider: 0.602931 test loss: 0.774442
[Epoch 110; Iter     6/   36] train: loss: 0.2090257
[Epoch 110; Iter    36/   36] train: loss: 0.2426218
[Epoch 110] ogbg-molsider: 0.589633 val loss: 0.672257
[Epoch 110] ogbg-molsider: 0.603441 test loss: 0.693137
[Epoch 111; Iter    30/   36] train: loss: 0.2353274
[Epoch 111] ogbg-molsider: 0.593543 val loss: 0.711024
[Epoch 111] ogbg-molsider: 0.601778 test loss: 0.752382
[Epoch 112; Iter    24/   36] train: loss: 0.2001121
[Epoch 112] ogbg-molsider: 0.607302 val loss: 0.691216
[Epoch 112] ogbg-molsider: 0.614322 test loss: 0.743214
[Epoch 113; Iter    18/   36] train: loss: 0.2035128
[Epoch 113] ogbg-molsider: 0.601609 val loss: 0.724037
[Epoch 113] ogbg-molsider: 0.607793 test loss: 0.795244
[Epoch 114; Iter    12/   36] train: loss: 0.2082146
[Epoch 114] ogbg-molsider: 0.618411 val loss: 0.672775
[Epoch 114] ogbg-molsider: 0.612552 test loss: 0.709627
[Epoch 115; Iter     6/   36] train: loss: 0.1781679
[Epoch 115; Iter    36/   36] train: loss: 0.2440471
[Epoch 115] ogbg-molsider: 0.593569 val loss: 0.739199
[Epoch 115] ogbg-molsider: 0.610657 test loss: 0.783194
[Epoch 116; Iter    30/   36] train: loss: 0.1903477
[Epoch 116] ogbg-molsider: 0.599636 val loss: 0.701661
[Epoch 116] ogbg-molsider: 0.613602 test loss: 0.740105
[Epoch 117; Iter    24/   36] train: loss: 0.2143978
[Epoch 117] ogbg-molsider: 0.611715 val loss: 0.681674
[Epoch 117] ogbg-molsider: 0.610660 test loss: 0.722860
[Epoch 118; Iter    18/   36] train: loss: 0.1858331
[Epoch 118] ogbg-molsider: 0.606114 val loss: 0.703983
[Epoch 118] ogbg-molsider: 0.604404 test loss: 0.749318
[Epoch 119; Iter    12/   36] train: loss: 0.1745995
[Epoch 119] ogbg-molsider: 0.612853 val loss: 0.711930
[Epoch 119] ogbg-molsider: 0.609263 test loss: 0.785038
[Epoch 120; Iter     6/   36] train: loss: 0.1788514
[Epoch 120; Iter    36/   36] train: loss: 0.2210181
[Epoch 120] ogbg-molsider: 0.599305 val loss: 0.706864
[Epoch 120] ogbg-molsider: 0.605169 test loss: 0.740848
[Epoch 121; Iter    30/   36] train: loss: 0.1652803
[Epoch 121] ogbg-molsider: 0.599031 val loss: 0.711830
[Epoch 121] ogbg-molsider: 0.612886 test loss: 0.728534
[Epoch 122; Iter    24/   36] train: loss: 0.1895521
[Epoch 122] ogbg-molsider: 0.607214 val loss: 0.699346
[Epoch 122] ogbg-molsider: 0.605164 test loss: 0.749350
[Epoch 123; Iter    18/   36] train: loss: 0.1697242
[Epoch 123] ogbg-molsider: 0.619403 val loss: 0.761382
[Epoch 123] ogbg-molsider: 0.605115 test loss: 0.829067
[Epoch 124; Iter    12/   36] train: loss: 0.1762070
[Epoch 124] ogbg-molsider: 0.600363 val loss: 0.720617
[Epoch 124] ogbg-molsider: 0.614262 test loss: 0.758257
[Epoch 125; Iter     6/   36] train: loss: 0.1640166
[Epoch 125; Iter    36/   36] train: loss: 0.1810520
[Epoch 125] ogbg-molsider: 0.608713 val loss: 0.722874
[Epoch 125] ogbg-molsider: 0.609176 test loss: 0.792782
[Epoch 126; Iter    30/   36] train: loss: 0.1722403
[Epoch 126] ogbg-molsider: 0.600786 val loss: 0.733457
[Epoch 126] ogbg-molsider: 0.605917 test loss: 0.765623
[Epoch 127; Iter    24/   36] train: loss: 0.1897261
[Epoch 127] ogbg-molsider: 0.593890 val loss: 0.778487
[Epoch 127] ogbg-molsider: 0.609889 test loss: 0.831306
[Epoch 128; Iter    18/   36] train: loss: 0.1770094
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.1.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.1.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.1.yml --seed 6 --device cuda:2
All runs completed.
[Epoch 81; Iter    30/   36] train: loss: 0.2527913
[Epoch 81] ogbg-molsider: 0.609422 val loss: 0.607587
[Epoch 81] ogbg-molsider: 0.585651 test loss: 0.703925
[Epoch 82; Iter    24/   36] train: loss: 0.3017105
[Epoch 82] ogbg-molsider: 0.610264 val loss: 0.799570
[Epoch 82] ogbg-molsider: 0.584455 test loss: 0.973694
[Epoch 83; Iter    18/   36] train: loss: 0.2802898
[Epoch 83] ogbg-molsider: 0.622194 val loss: 0.704221
[Epoch 83] ogbg-molsider: 0.590377 test loss: 0.812899
[Epoch 84; Iter    12/   36] train: loss: 0.2887128
[Epoch 84] ogbg-molsider: 0.622601 val loss: 0.620026
[Epoch 84] ogbg-molsider: 0.586574 test loss: 0.722569
[Epoch 85; Iter     6/   36] train: loss: 0.2738921
[Epoch 85; Iter    36/   36] train: loss: 0.2948594
[Epoch 85] ogbg-molsider: 0.622822 val loss: 0.745852
[Epoch 85] ogbg-molsider: 0.592601 test loss: 0.867525
[Epoch 86; Iter    30/   36] train: loss: 0.3091012
[Epoch 86] ogbg-molsider: 0.627720 val loss: 0.726515
[Epoch 86] ogbg-molsider: 0.581721 test loss: 0.905328
[Epoch 87; Iter    24/   36] train: loss: 0.2854645
[Epoch 87] ogbg-molsider: 0.614136 val loss: 0.625633
[Epoch 87] ogbg-molsider: 0.598077 test loss: 0.689927
[Epoch 88; Iter    18/   36] train: loss: 0.2931281
[Epoch 88] ogbg-molsider: 0.617998 val loss: 0.654254
[Epoch 88] ogbg-molsider: 0.592233 test loss: 0.836456
[Epoch 89; Iter    12/   36] train: loss: 0.2738675
[Epoch 89] ogbg-molsider: 0.622194 val loss: 0.850918
[Epoch 89] ogbg-molsider: 0.590858 test loss: 1.105249
[Epoch 90; Iter     6/   36] train: loss: 0.2594916
[Epoch 90; Iter    36/   36] train: loss: 0.2930532
[Epoch 90] ogbg-molsider: 0.617560 val loss: 0.654953
[Epoch 90] ogbg-molsider: 0.586157 test loss: 0.773421
[Epoch 91; Iter    30/   36] train: loss: 0.2806486
[Epoch 91] ogbg-molsider: 0.609327 val loss: 0.864699
[Epoch 91] ogbg-molsider: 0.582774 test loss: 1.042525
[Epoch 92; Iter    24/   36] train: loss: 0.2841897
[Epoch 92] ogbg-molsider: 0.617454 val loss: 0.651531
[Epoch 92] ogbg-molsider: 0.585410 test loss: 0.787450
[Epoch 93; Iter    18/   36] train: loss: 0.2294201
[Epoch 93] ogbg-molsider: 0.613218 val loss: 0.723695
[Epoch 93] ogbg-molsider: 0.577292 test loss: 0.933899
[Epoch 94; Iter    12/   36] train: loss: 0.2357508
[Epoch 94] ogbg-molsider: 0.614946 val loss: 0.724144
[Epoch 94] ogbg-molsider: 0.576685 test loss: 0.872637
[Epoch 95; Iter     6/   36] train: loss: 0.2748059
[Epoch 95; Iter    36/   36] train: loss: 0.2755019
[Epoch 95] ogbg-molsider: 0.615641 val loss: 0.649900
[Epoch 95] ogbg-molsider: 0.592111 test loss: 0.731560
[Epoch 96; Iter    30/   36] train: loss: 0.2973190
[Epoch 96] ogbg-molsider: 0.624304 val loss: 0.654500
[Epoch 96] ogbg-molsider: 0.577363 test loss: 0.746675
[Epoch 97; Iter    24/   36] train: loss: 0.2846316
[Epoch 97] ogbg-molsider: 0.612023 val loss: 0.636071
[Epoch 97] ogbg-molsider: 0.587627 test loss: 0.759840
[Epoch 98; Iter    18/   36] train: loss: 0.2736328
[Epoch 98] ogbg-molsider: 0.613063 val loss: 0.652861
[Epoch 98] ogbg-molsider: 0.598929 test loss: 0.725572
[Epoch 99; Iter    12/   36] train: loss: 0.2929947
[Epoch 99] ogbg-molsider: 0.633097 val loss: 0.616717
[Epoch 99] ogbg-molsider: 0.590800 test loss: 0.716027
[Epoch 100; Iter     6/   36] train: loss: 0.2095913
[Epoch 100; Iter    36/   36] train: loss: 0.2721898
[Epoch 100] ogbg-molsider: 0.605563 val loss: 0.655654
[Epoch 100] ogbg-molsider: 0.595981 test loss: 0.829398
[Epoch 101; Iter    30/   36] train: loss: 0.2228714
[Epoch 101] ogbg-molsider: 0.603269 val loss: 0.761696
[Epoch 101] ogbg-molsider: 0.590908 test loss: 0.923102
[Epoch 102; Iter    24/   36] train: loss: 0.2661098
[Epoch 102] ogbg-molsider: 0.623946 val loss: 0.841814
[Epoch 102] ogbg-molsider: 0.601137 test loss: 1.128354
[Epoch 103; Iter    18/   36] train: loss: 0.2382731
[Epoch 103] ogbg-molsider: 0.617220 val loss: 0.866675
[Epoch 103] ogbg-molsider: 0.604061 test loss: 1.137321
[Epoch 104; Iter    12/   36] train: loss: 0.2145631
[Epoch 104] ogbg-molsider: 0.613571 val loss: 1.086033
[Epoch 104] ogbg-molsider: 0.590226 test loss: 1.499498
[Epoch 105; Iter     6/   36] train: loss: 0.2083829
[Epoch 105; Iter    36/   36] train: loss: 0.2605127
[Epoch 105] ogbg-molsider: 0.627364 val loss: 0.712104
[Epoch 105] ogbg-molsider: 0.595233 test loss: 0.929799
[Epoch 106; Iter    30/   36] train: loss: 0.2426872
[Epoch 106] ogbg-molsider: 0.608813 val loss: 0.954252
[Epoch 106] ogbg-molsider: 0.602413 test loss: 1.295465
[Epoch 107; Iter    24/   36] train: loss: 0.2218585
[Epoch 107] ogbg-molsider: 0.618568 val loss: 0.773636
[Epoch 107] ogbg-molsider: 0.595565 test loss: 0.937766
[Epoch 108; Iter    18/   36] train: loss: 0.2085264
[Epoch 108] ogbg-molsider: 0.621050 val loss: 0.712354
[Epoch 108] ogbg-molsider: 0.593504 test loss: 0.875676
[Epoch 109; Iter    12/   36] train: loss: 0.2280652
[Epoch 109] ogbg-molsider: 0.627484 val loss: 0.841383
[Epoch 109] ogbg-molsider: 0.594810 test loss: 1.188628
[Epoch 110; Iter     6/   36] train: loss: 0.2370641
[Epoch 110; Iter    36/   36] train: loss: 0.2015644
[Epoch 110] ogbg-molsider: 0.625828 val loss: 0.794408
[Epoch 110] ogbg-molsider: 0.599384 test loss: 1.036905
[Epoch 111; Iter    30/   36] train: loss: 0.1974041
[Epoch 111] ogbg-molsider: 0.619429 val loss: 0.734146
[Epoch 111] ogbg-molsider: 0.597961 test loss: 0.937364
[Epoch 112; Iter    24/   36] train: loss: 0.2279257
[Epoch 112] ogbg-molsider: 0.614089 val loss: 0.851266
[Epoch 112] ogbg-molsider: 0.600372 test loss: 1.303251
[Epoch 113; Iter    18/   36] train: loss: 0.1762855
[Epoch 113] ogbg-molsider: 0.617637 val loss: 0.793368
[Epoch 113] ogbg-molsider: 0.593269 test loss: 1.160996
[Epoch 114; Iter    12/   36] train: loss: 0.2171370
[Epoch 114] ogbg-molsider: 0.621590 val loss: 0.776616
[Epoch 114] ogbg-molsider: 0.602597 test loss: 1.029437
[Epoch 115; Iter     6/   36] train: loss: 0.1965890
[Epoch 115; Iter    36/   36] train: loss: 0.2404945
[Epoch 115] ogbg-molsider: 0.616460 val loss: 0.710378
[Epoch 115] ogbg-molsider: 0.597644 test loss: 0.926736
[Epoch 116; Iter    30/   36] train: loss: 0.1966268
[Epoch 116] ogbg-molsider: 0.624431 val loss: 0.775031
[Epoch 116] ogbg-molsider: 0.606695 test loss: 1.084490
[Epoch 117; Iter    24/   36] train: loss: 0.1822837
[Epoch 117] ogbg-molsider: 0.622377 val loss: 0.698922
[Epoch 117] ogbg-molsider: 0.597122 test loss: 0.915807
[Epoch 118; Iter    18/   36] train: loss: 0.2269160
[Epoch 118] ogbg-molsider: 0.621329 val loss: 0.876695
[Epoch 118] ogbg-molsider: 0.601979 test loss: 1.241151
[Epoch 119; Iter    12/   36] train: loss: 0.1846539
[Epoch 119] ogbg-molsider: 0.631312 val loss: 0.684785
[Epoch 119] ogbg-molsider: 0.603880 test loss: 0.865120
[Epoch 120; Iter     6/   36] train: loss: 0.2445326
[Epoch 120; Iter    36/   36] train: loss: 0.1703638
[Epoch 120] ogbg-molsider: 0.625073 val loss: 0.777157
[Epoch 120] ogbg-molsider: 0.598622 test loss: 1.003230
[Epoch 121; Iter    30/   36] train: loss: 0.2427697
[Epoch 121] ogbg-molsider: 0.617657 val loss: 0.731864
[Epoch 121] ogbg-molsider: 0.605690 test loss: 0.923430
[Epoch 122; Iter    24/   36] train: loss: 0.2173185
[Epoch 122] ogbg-molsider: 0.631637 val loss: 0.772174
[Epoch 122] ogbg-molsider: 0.602274 test loss: 1.091897
[Epoch 123; Iter    18/   36] train: loss: 0.2286535
[Epoch 123] ogbg-molsider: 0.625224 val loss: 0.746450
[Epoch 123] ogbg-molsider: 0.601194 test loss: 1.024327
[Epoch 124; Iter    12/   36] train: loss: 0.1978760
[Epoch 124] ogbg-molsider: 0.608437 val loss: 0.773003
[Epoch 124] ogbg-molsider: 0.594030 test loss: 0.974456
[Epoch 125; Iter     6/   36] train: loss: 0.1723992
[Epoch 125; Iter    36/   36] train: loss: 0.2188499
[Epoch 125] ogbg-molsider: 0.619567 val loss: 0.767591
[Epoch 125] ogbg-molsider: 0.594350 test loss: 1.040599
[Epoch 126; Iter    30/   36] train: loss: 0.1798108
[Epoch 126] ogbg-molsider: 0.617489 val loss: 0.774224
[Epoch 126] ogbg-molsider: 0.600334 test loss: 1.100788
[Epoch 127; Iter    24/   36] train: loss: 0.1948568
[Epoch 127] ogbg-molsider: 0.620431 val loss: 0.784714
[Epoch 127] ogbg-molsider: 0.603995 test loss: 1.071063
[Epoch 128; Iter    18/   36] train: loss: 0.1980188
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.616188 val loss: 0.735370
[Epoch 128] ogbg-molsider: 0.607681 test loss: 0.977755
[Epoch 129; Iter    12/   36] train: loss: 0.1933053
[Epoch 129] ogbg-molsider: 0.619548 val loss: 0.780280
[Epoch 129] ogbg-molsider: 0.606567 test loss: 1.076365
[Epoch 130; Iter     6/   36] train: loss: 0.1754471
[Epoch 130; Iter    36/   36] train: loss: 0.1613617
[Epoch 130] ogbg-molsider: 0.620523 val loss: 0.848669
[Epoch 130] ogbg-molsider: 0.605750 test loss: 1.136976
[Epoch 131; Iter    30/   36] train: loss: 0.1917341
[Epoch 131] ogbg-molsider: 0.620295 val loss: 0.810259
[Epoch 131] ogbg-molsider: 0.604200 test loss: 1.128509
[Epoch 132; Iter    24/   36] train: loss: 0.1527231
[Epoch 132] ogbg-molsider: 0.617264 val loss: 0.756632
[Epoch 132] ogbg-molsider: 0.602130 test loss: 1.017572
Early stopping criterion based on -ogbg-molsider- that should be max reached after 132 epochs. Best model checkpoint was in epoch 72.
Statistics on  val_best_checkpoint
mean_pred: 0.22743113338947296
std_pred: 12.315144538879395
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.673454851233132
rocauc: 0.6408995936895153
ogbg-molsider: 0.6408995936895153
OGBNanLabelBCEWithLogitsLoss: 0.6219113588333129
Statistics on  test
mean_pred: 0.36519351601600647
std_pred: 23.130495071411133
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6145886234533049
rocauc: 0.5651742318485936
ogbg-molsider: 0.5651742318485936
OGBNanLabelBCEWithLogitsLoss: 0.8173720598220825
Statistics on  train
mean_pred: 1.257539987564087
std_pred: 3.239013195037842
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.832932538263301
rocauc: 0.8845030319621926
ogbg-molsider: 0.8845030319621926
OGBNanLabelBCEWithLogitsLoss: 0.3420699718925688
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 128] ogbg-molsider: 0.588017 val loss: 1.677323
[Epoch 128] ogbg-molsider: 0.600559 test loss: 1.349483
[Epoch 129; Iter    12/   36] train: loss: 0.1511052
[Epoch 129] ogbg-molsider: 0.594795 val loss: 5.082938
[Epoch 129] ogbg-molsider: 0.601694 test loss: 2.952369
[Epoch 130; Iter     6/   36] train: loss: 0.1633039
[Epoch 130; Iter    36/   36] train: loss: 0.1700079
[Epoch 130] ogbg-molsider: 0.589924 val loss: 4.875709
[Epoch 130] ogbg-molsider: 0.585381 test loss: 3.102485
[Epoch 131; Iter    30/   36] train: loss: 0.1876066
[Epoch 131] ogbg-molsider: 0.591955 val loss: 2.081632
[Epoch 131] ogbg-molsider: 0.600410 test loss: 1.267266
[Epoch 132; Iter    24/   36] train: loss: 0.1355688
[Epoch 132] ogbg-molsider: 0.572655 val loss: 1.418606
[Epoch 132] ogbg-molsider: 0.590499 test loss: 0.908417
[Epoch 133; Iter    18/   36] train: loss: 0.1599736
[Epoch 133] ogbg-molsider: 0.589535 val loss: 0.981393
[Epoch 133] ogbg-molsider: 0.597566 test loss: 0.877244
[Epoch 134; Iter    12/   36] train: loss: 0.1223778
[Epoch 134] ogbg-molsider: 0.588083 val loss: 3.763005
[Epoch 134] ogbg-molsider: 0.604448 test loss: 1.980243
[Epoch 135; Iter     6/   36] train: loss: 0.1486179
[Epoch 135; Iter    36/   36] train: loss: 0.1984623
[Epoch 135] ogbg-molsider: 0.589226 val loss: 2.043576
[Epoch 135] ogbg-molsider: 0.600816 test loss: 1.275718
[Epoch 136; Iter    30/   36] train: loss: 0.1413957
[Epoch 136] ogbg-molsider: 0.585406 val loss: 0.804712
[Epoch 136] ogbg-molsider: 0.601493 test loss: 0.854150
[Epoch 137; Iter    24/   36] train: loss: 0.1357846
[Epoch 137] ogbg-molsider: 0.590464 val loss: 3.259347
[Epoch 137] ogbg-molsider: 0.599438 test loss: 1.783991
[Epoch 138; Iter    18/   36] train: loss: 0.1737804
[Epoch 138] ogbg-molsider: 0.588333 val loss: 2.261401
[Epoch 138] ogbg-molsider: 0.604498 test loss: 1.364189
[Epoch 139; Iter    12/   36] train: loss: 0.1152338
[Epoch 139] ogbg-molsider: 0.590963 val loss: 3.122586
[Epoch 139] ogbg-molsider: 0.598938 test loss: 1.597055
[Epoch 140; Iter     6/   36] train: loss: 0.1257430
[Epoch 140; Iter    36/   36] train: loss: 0.1315425
[Epoch 140] ogbg-molsider: 0.580638 val loss: 2.754928
[Epoch 140] ogbg-molsider: 0.603332 test loss: 1.613099
[Epoch 141; Iter    30/   36] train: loss: 0.1285403
[Epoch 141] ogbg-molsider: 0.587472 val loss: 5.296036
[Epoch 141] ogbg-molsider: 0.602021 test loss: 2.547448
[Epoch 142; Iter    24/   36] train: loss: 0.1361852
[Epoch 142] ogbg-molsider: 0.595690 val loss: 2.105001
[Epoch 142] ogbg-molsider: 0.594612 test loss: 1.563915
[Epoch 143; Iter    18/   36] train: loss: 0.1433420
[Epoch 143] ogbg-molsider: 0.595788 val loss: 2.610398
[Epoch 143] ogbg-molsider: 0.602068 test loss: 1.580825
Early stopping criterion based on -ogbg-molsider- that should be max reached after 143 epochs. Best model checkpoint was in epoch 83.
Statistics on  val_best_checkpoint
mean_pred: -6.2606096267700195
std_pred: 148.56219482421875
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6470890895995604
rocauc: 0.6071375838257848
ogbg-molsider: 0.6071375838257848
OGBNanLabelBCEWithLogitsLoss: 15.226007056236266
Statistics on  test
mean_pred: -5.639927864074707
std_pred: 106.0951919555664
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.623363347862738
rocauc: 0.5927826059732518
ogbg-molsider: 0.5927826059732518
OGBNanLabelBCEWithLogitsLoss: 16.044510614871978
Statistics on  train
mean_pred: 0.5996926426887512
std_pred: 3.457282304763794
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.9011546452829312
rocauc: 0.9418791517358241
ogbg-molsider: 0.9418791517358241
OGBNanLabelBCEWithLogitsLoss: 0.24827158451080322
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.603446 val loss: 0.705975
[Epoch 128] ogbg-molsider: 0.605762 test loss: 0.721186
[Epoch 129; Iter    12/   36] train: loss: 0.1725540
[Epoch 129] ogbg-molsider: 0.601996 val loss: 0.720814
[Epoch 129] ogbg-molsider: 0.614318 test loss: 0.737506
[Epoch 130; Iter     6/   36] train: loss: 0.1698931
[Epoch 130; Iter    36/   36] train: loss: 0.1329656
[Epoch 130] ogbg-molsider: 0.595642 val loss: 0.722621
[Epoch 130] ogbg-molsider: 0.599655 test loss: 0.762741
[Epoch 131; Iter    30/   36] train: loss: 0.1939183
[Epoch 131] ogbg-molsider: 0.586284 val loss: 0.737976
[Epoch 131] ogbg-molsider: 0.607155 test loss: 0.734967
[Epoch 132; Iter    24/   36] train: loss: 0.1676696
[Epoch 132] ogbg-molsider: 0.603737 val loss: 0.735460
[Epoch 132] ogbg-molsider: 0.607389 test loss: 0.789352
[Epoch 133; Iter    18/   36] train: loss: 0.1760178
[Epoch 133] ogbg-molsider: 0.593251 val loss: 0.745401
[Epoch 133] ogbg-molsider: 0.608240 test loss: 0.796020
[Epoch 134; Iter    12/   36] train: loss: 0.1666892
[Epoch 134] ogbg-molsider: 0.588374 val loss: 0.757249
[Epoch 134] ogbg-molsider: 0.606397 test loss: 0.802177
[Epoch 135; Iter     6/   36] train: loss: 0.1482283
[Epoch 135; Iter    36/   36] train: loss: 0.1588696
[Epoch 135] ogbg-molsider: 0.601639 val loss: 0.711325
[Epoch 135] ogbg-molsider: 0.604366 test loss: 0.749704
[Epoch 136; Iter    30/   36] train: loss: 0.1709153
[Epoch 136] ogbg-molsider: 0.585646 val loss: 0.750645
[Epoch 136] ogbg-molsider: 0.612564 test loss: 0.775446
[Epoch 137; Iter    24/   36] train: loss: 0.1641182
[Epoch 137] ogbg-molsider: 0.582750 val loss: 0.758559
[Epoch 137] ogbg-molsider: 0.612547 test loss: 0.801302
[Epoch 138; Iter    18/   36] train: loss: 0.1766535
[Epoch 138] ogbg-molsider: 0.591724 val loss: 0.755646
[Epoch 138] ogbg-molsider: 0.610465 test loss: 0.786733
[Epoch 139; Iter    12/   36] train: loss: 0.1382610
[Epoch 139] ogbg-molsider: 0.593507 val loss: 0.753123
[Epoch 139] ogbg-molsider: 0.609972 test loss: 0.776020
[Epoch 140; Iter     6/   36] train: loss: 0.1175830
[Epoch 140; Iter    36/   36] train: loss: 0.1514055
[Epoch 140] ogbg-molsider: 0.592235 val loss: 0.771970
[Epoch 140] ogbg-molsider: 0.608432 test loss: 0.818880
[Epoch 141; Iter    30/   36] train: loss: 0.1361083
[Epoch 141] ogbg-molsider: 0.592265 val loss: 0.747138
[Epoch 141] ogbg-molsider: 0.603131 test loss: 0.798024
[Epoch 142; Iter    24/   36] train: loss: 0.1513516
[Epoch 142] ogbg-molsider: 0.589944 val loss: 0.763031
[Epoch 142] ogbg-molsider: 0.608388 test loss: 0.809168
[Epoch 143; Iter    18/   36] train: loss: 0.1544012
[Epoch 143] ogbg-molsider: 0.590637 val loss: 0.778357
[Epoch 143] ogbg-molsider: 0.607537 test loss: 0.829750
[Epoch 144; Iter    12/   36] train: loss: 0.1285006
[Epoch 144] ogbg-molsider: 0.588727 val loss: 0.755111
[Epoch 144] ogbg-molsider: 0.608338 test loss: 0.774791
[Epoch 145; Iter     6/   36] train: loss: 0.1409276
[Epoch 145; Iter    36/   36] train: loss: 0.1652799
[Epoch 145] ogbg-molsider: 0.588751 val loss: 0.768874
[Epoch 145] ogbg-molsider: 0.609472 test loss: 0.824703
[Epoch 146; Iter    30/   36] train: loss: 0.1596009
[Epoch 146] ogbg-molsider: 0.583467 val loss: 0.752966
[Epoch 146] ogbg-molsider: 0.605632 test loss: 0.784336
[Epoch 147; Iter    24/   36] train: loss: 0.1622120
[Epoch 147] ogbg-molsider: 0.585945 val loss: 0.779170
[Epoch 147] ogbg-molsider: 0.599979 test loss: 0.827746
[Epoch 148; Iter    18/   36] train: loss: 0.1600093
[Epoch 148] ogbg-molsider: 0.588875 val loss: 0.801475
[Epoch 148] ogbg-molsider: 0.604415 test loss: 0.864878
[Epoch 149; Iter    12/   36] train: loss: 0.1404853
[Epoch 149] ogbg-molsider: 0.589951 val loss: 0.761099
[Epoch 149] ogbg-molsider: 0.609079 test loss: 0.792095
[Epoch 150; Iter     6/   36] train: loss: 0.1384760
[Epoch 150; Iter    36/   36] train: loss: 0.1435679
[Epoch 150] ogbg-molsider: 0.586930 val loss: 0.768667
[Epoch 150] ogbg-molsider: 0.602494 test loss: 0.795982
[Epoch 151; Iter    30/   36] train: loss: 0.1491871
[Epoch 151] ogbg-molsider: 0.587223 val loss: 0.780479
[Epoch 151] ogbg-molsider: 0.602247 test loss: 0.835932
Early stopping criterion based on -ogbg-molsider- that should be max reached after 151 epochs. Best model checkpoint was in epoch 91.
Statistics on  val_best_checkpoint
mean_pred: 1.108303189277649
std_pred: 3.462887763977051
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6679745224242649
rocauc: 0.6302856830546956
ogbg-molsider: 0.6302856830546956
OGBNanLabelBCEWithLogitsLoss: 0.5824579358100891
Statistics on  test
mean_pred: 1.1049600839614868
std_pred: 3.437323570251465
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6273030285789268
rocauc: 0.6052427995468198
ogbg-molsider: 0.6052427995468198
OGBNanLabelBCEWithLogitsLoss: 0.6380198240280152
Statistics on  train
mean_pred: 0.8730490803718567
std_pred: 3.722383975982666
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.9056642083666039
rocauc: 0.9440124071003332
ogbg-molsider: 0.9440124071003332
OGBNanLabelBCEWithLogitsLoss: 0.2478437643084261
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.05.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.05.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.05.yml --seed 6 --device cuda:1
All runs completed.
[Epoch 81; Iter    30/   36] train: loss: 0.4747778
[Epoch 81] ogbg-molsider: 0.622443 val loss: 0.513920
[Epoch 81] ogbg-molsider: 0.602355 test loss: 0.551783
[Epoch 82; Iter    24/   36] train: loss: 0.4082930
[Epoch 82] ogbg-molsider: 0.613552 val loss: 0.512709
[Epoch 82] ogbg-molsider: 0.587755 test loss: 0.563503
[Epoch 83; Iter    18/   36] train: loss: 0.4014495
[Epoch 83] ogbg-molsider: 0.638248 val loss: 0.515559
[Epoch 83] ogbg-molsider: 0.605293 test loss: 0.550886
[Epoch 84; Iter    12/   36] train: loss: 0.3603890
[Epoch 84] ogbg-molsider: 0.625590 val loss: 0.499195
[Epoch 84] ogbg-molsider: 0.575597 test loss: 0.560686
[Epoch 85; Iter     6/   36] train: loss: 0.3953222
[Epoch 85; Iter    36/   36] train: loss: 0.4255466
[Epoch 85] ogbg-molsider: 0.631987 val loss: 0.520154
[Epoch 85] ogbg-molsider: 0.581404 test loss: 0.596823
[Epoch 86; Iter    30/   36] train: loss: 0.3988284
[Epoch 86] ogbg-molsider: 0.613216 val loss: 0.519422
[Epoch 86] ogbg-molsider: 0.585649 test loss: 0.580524
[Epoch 87; Iter    24/   36] train: loss: 0.3879648
[Epoch 87] ogbg-molsider: 0.653931 val loss: 0.496341
[Epoch 87] ogbg-molsider: 0.591803 test loss: 0.569153
[Epoch 88; Iter    18/   36] train: loss: 0.3583630
[Epoch 88] ogbg-molsider: 0.621145 val loss: 0.532891
[Epoch 88] ogbg-molsider: 0.579746 test loss: 0.558889
[Epoch 89; Iter    12/   36] train: loss: 0.3981431
[Epoch 89] ogbg-molsider: 0.639593 val loss: 0.508156
[Epoch 89] ogbg-molsider: 0.608489 test loss: 0.581842
[Epoch 90; Iter     6/   36] train: loss: 0.3149589
[Epoch 90; Iter    36/   36] train: loss: 0.3992333
[Epoch 90] ogbg-molsider: 0.621103 val loss: 0.533322
[Epoch 90] ogbg-molsider: 0.581391 test loss: 0.580069
[Epoch 91; Iter    30/   36] train: loss: 0.3822412
[Epoch 91] ogbg-molsider: 0.629633 val loss: 0.536691
[Epoch 91] ogbg-molsider: 0.610357 test loss: 0.593928
[Epoch 92; Iter    24/   36] train: loss: 0.3210093
[Epoch 92] ogbg-molsider: 0.648704 val loss: 0.519742
[Epoch 92] ogbg-molsider: 0.597499 test loss: 0.581293
[Epoch 93; Iter    18/   36] train: loss: 0.2950756
[Epoch 93] ogbg-molsider: 0.638201 val loss: 0.508074
[Epoch 93] ogbg-molsider: 0.588719 test loss: 0.570015
[Epoch 94; Iter    12/   36] train: loss: 0.3272060
[Epoch 94] ogbg-molsider: 0.642583 val loss: 0.517245
[Epoch 94] ogbg-molsider: 0.588345 test loss: 0.594646
[Epoch 95; Iter     6/   36] train: loss: 0.3391411
[Epoch 95; Iter    36/   36] train: loss: 0.3741849
[Epoch 95] ogbg-molsider: 0.629677 val loss: 0.528189
[Epoch 95] ogbg-molsider: 0.596609 test loss: 0.604957
[Epoch 96; Iter    30/   36] train: loss: 0.3682983
[Epoch 96] ogbg-molsider: 0.639900 val loss: 0.549390
[Epoch 96] ogbg-molsider: 0.607729 test loss: 0.612862
[Epoch 97; Iter    24/   36] train: loss: 0.3206202
[Epoch 97] ogbg-molsider: 0.641596 val loss: 0.523116
[Epoch 97] ogbg-molsider: 0.588925 test loss: 0.584955
[Epoch 98; Iter    18/   36] train: loss: 0.3478138
[Epoch 98] ogbg-molsider: 0.641489 val loss: 0.528584
[Epoch 98] ogbg-molsider: 0.597157 test loss: 0.595990
[Epoch 99; Iter    12/   36] train: loss: 0.3701443
[Epoch 99] ogbg-molsider: 0.637774 val loss: 0.536719
[Epoch 99] ogbg-molsider: 0.617189 test loss: 0.588001
[Epoch 100; Iter     6/   36] train: loss: 0.2740227
[Epoch 100; Iter    36/   36] train: loss: 0.3451576
[Epoch 100] ogbg-molsider: 0.633060 val loss: 0.540297
[Epoch 100] ogbg-molsider: 0.597762 test loss: 0.599362
[Epoch 101; Iter    30/   36] train: loss: 0.3126568
[Epoch 101] ogbg-molsider: 0.638413 val loss: 0.551696
[Epoch 101] ogbg-molsider: 0.603977 test loss: 0.634188
[Epoch 102; Iter    24/   36] train: loss: 0.3338098
[Epoch 102] ogbg-molsider: 0.628914 val loss: 0.556931
[Epoch 102] ogbg-molsider: 0.601206 test loss: 0.638210
[Epoch 103; Iter    18/   36] train: loss: 0.2953105
[Epoch 103] ogbg-molsider: 0.629501 val loss: 0.546891
[Epoch 103] ogbg-molsider: 0.604352 test loss: 0.613525
[Epoch 104; Iter    12/   36] train: loss: 0.3109313
[Epoch 104] ogbg-molsider: 0.633411 val loss: 0.537671
[Epoch 104] ogbg-molsider: 0.616106 test loss: 0.584542
[Epoch 105; Iter     6/   36] train: loss: 0.2901194
[Epoch 105; Iter    36/   36] train: loss: 0.3688649
[Epoch 105] ogbg-molsider: 0.631549 val loss: 0.552644
[Epoch 105] ogbg-molsider: 0.605696 test loss: 0.598022
[Epoch 106; Iter    30/   36] train: loss: 0.2797591
[Epoch 106] ogbg-molsider: 0.613062 val loss: 0.559544
[Epoch 106] ogbg-molsider: 0.579852 test loss: 0.644003
[Epoch 107; Iter    24/   36] train: loss: 0.3143395
[Epoch 107] ogbg-molsider: 0.629977 val loss: 0.554712
[Epoch 107] ogbg-molsider: 0.578148 test loss: 0.631464
[Epoch 108; Iter    18/   36] train: loss: 0.3012438
[Epoch 108] ogbg-molsider: 0.638623 val loss: 0.556057
[Epoch 108] ogbg-molsider: 0.598579 test loss: 0.630007
[Epoch 109; Iter    12/   36] train: loss: 0.2827467
[Epoch 109] ogbg-molsider: 0.617302 val loss: 0.580070
[Epoch 109] ogbg-molsider: 0.598107 test loss: 0.633085
[Epoch 110; Iter     6/   36] train: loss: 0.3104692
[Epoch 110; Iter    36/   36] train: loss: 0.3087770
[Epoch 110] ogbg-molsider: 0.619624 val loss: 0.583689
[Epoch 110] ogbg-molsider: 0.602467 test loss: 0.641287
[Epoch 111; Iter    30/   36] train: loss: 0.3054058
[Epoch 111] ogbg-molsider: 0.639045 val loss: 0.542917
[Epoch 111] ogbg-molsider: 0.598679 test loss: 0.622518
[Epoch 112; Iter    24/   36] train: loss: 0.2811757
[Epoch 112] ogbg-molsider: 0.626119 val loss: 0.595431
[Epoch 112] ogbg-molsider: 0.610161 test loss: 0.675417
[Epoch 113; Iter    18/   36] train: loss: 0.2793537
[Epoch 113] ogbg-molsider: 0.630585 val loss: 0.561643
[Epoch 113] ogbg-molsider: 0.603337 test loss: 0.639767
[Epoch 114; Iter    12/   36] train: loss: 0.3144613
[Epoch 114] ogbg-molsider: 0.633110 val loss: 0.581110
[Epoch 114] ogbg-molsider: 0.603611 test loss: 0.659433
[Epoch 115; Iter     6/   36] train: loss: 0.2915382
[Epoch 115; Iter    36/   36] train: loss: 0.2988537
[Epoch 115] ogbg-molsider: 0.629211 val loss: 0.575884
[Epoch 115] ogbg-molsider: 0.608454 test loss: 0.639057
[Epoch 116; Iter    30/   36] train: loss: 0.2789412
[Epoch 116] ogbg-molsider: 0.613241 val loss: 0.599047
[Epoch 116] ogbg-molsider: 0.592533 test loss: 0.663325
[Epoch 117; Iter    24/   36] train: loss: 0.3245573
[Epoch 117] ogbg-molsider: 0.630565 val loss: 0.578903
[Epoch 117] ogbg-molsider: 0.613152 test loss: 0.619311
[Epoch 118; Iter    18/   36] train: loss: 0.2460718
[Epoch 118] ogbg-molsider: 0.631059 val loss: 0.572116
[Epoch 118] ogbg-molsider: 0.606784 test loss: 0.639527
[Epoch 119; Iter    12/   36] train: loss: 0.2770346
[Epoch 119] ogbg-molsider: 0.630238 val loss: 0.583715
[Epoch 119] ogbg-molsider: 0.605062 test loss: 0.635182
[Epoch 120; Iter     6/   36] train: loss: 0.3052607
[Epoch 120; Iter    36/   36] train: loss: 0.3880497
[Epoch 120] ogbg-molsider: 0.640298 val loss: 0.574275
[Epoch 120] ogbg-molsider: 0.607451 test loss: 0.645636
[Epoch 121; Iter    30/   36] train: loss: 0.3023614
[Epoch 121] ogbg-molsider: 0.626744 val loss: 0.575953
[Epoch 121] ogbg-molsider: 0.604250 test loss: 0.636553
[Epoch 122; Iter    24/   36] train: loss: 0.2887164
[Epoch 122] ogbg-molsider: 0.630889 val loss: 0.589121
[Epoch 122] ogbg-molsider: 0.607707 test loss: 0.641358
[Epoch 123; Iter    18/   36] train: loss: 0.3120926
[Epoch 123] ogbg-molsider: 0.624292 val loss: 0.593468
[Epoch 123] ogbg-molsider: 0.596594 test loss: 0.675937
[Epoch 124; Iter    12/   36] train: loss: 0.3167555
[Epoch 124] ogbg-molsider: 0.630510 val loss: 0.584072
[Epoch 124] ogbg-molsider: 0.610249 test loss: 0.645787
[Epoch 125; Iter     6/   36] train: loss: 0.2725092
[Epoch 125; Iter    36/   36] train: loss: 0.2568264
[Epoch 125] ogbg-molsider: 0.633855 val loss: 0.583043
[Epoch 125] ogbg-molsider: 0.599646 test loss: 0.650668
[Epoch 126; Iter    30/   36] train: loss: 0.2591902
[Epoch 126] ogbg-molsider: 0.627555 val loss: 0.596111
[Epoch 126] ogbg-molsider: 0.604452 test loss: 0.654162
[Epoch 127; Iter    24/   36] train: loss: 0.2837183
[Epoch 127] ogbg-molsider: 0.625018 val loss: 0.592732
[Epoch 127] ogbg-molsider: 0.607394 test loss: 0.650698
[Epoch 128; Iter    18/   36] train: loss: 0.2798349
[Epoch 81; Iter    30/   36] train: loss: 0.2820487
[Epoch 81] ogbg-molsider: 0.626540 val loss: 0.532790
[Epoch 81] ogbg-molsider: 0.599513 test loss: 0.586491
[Epoch 82; Iter    24/   36] train: loss: 0.3135066
[Epoch 82] ogbg-molsider: 0.604250 val loss: 0.589821
[Epoch 82] ogbg-molsider: 0.583871 test loss: 0.640050
[Epoch 83; Iter    18/   36] train: loss: 0.2930122
[Epoch 83] ogbg-molsider: 0.612853 val loss: 0.557034
[Epoch 83] ogbg-molsider: 0.573600 test loss: 0.595446
[Epoch 84; Iter    12/   36] train: loss: 0.2810468
[Epoch 84] ogbg-molsider: 0.614765 val loss: 0.567547
[Epoch 84] ogbg-molsider: 0.581524 test loss: 0.600706
[Epoch 85; Iter     6/   36] train: loss: 0.3244215
[Epoch 85; Iter    36/   36] train: loss: 0.3006735
[Epoch 85] ogbg-molsider: 0.620260 val loss: 0.562161
[Epoch 85] ogbg-molsider: 0.585582 test loss: 0.594598
[Epoch 86; Iter    30/   36] train: loss: 0.3259622
[Epoch 86] ogbg-molsider: 0.610838 val loss: 0.564659
[Epoch 86] ogbg-molsider: 0.593517 test loss: 0.593476
[Epoch 87; Iter    24/   36] train: loss: 0.3220463
[Epoch 87] ogbg-molsider: 0.602605 val loss: 0.569106
[Epoch 87] ogbg-molsider: 0.594909 test loss: 0.600718
[Epoch 88; Iter    18/   36] train: loss: 0.3079362
[Epoch 88] ogbg-molsider: 0.604008 val loss: 0.572517
[Epoch 88] ogbg-molsider: 0.579637 test loss: 0.607627
[Epoch 89; Iter    12/   36] train: loss: 0.2825350
[Epoch 89] ogbg-molsider: 0.622024 val loss: 0.574019
[Epoch 89] ogbg-molsider: 0.588030 test loss: 0.601385
[Epoch 90; Iter     6/   36] train: loss: 0.2873626
[Epoch 90; Iter    36/   36] train: loss: 0.3081556
[Epoch 90] ogbg-molsider: 0.614017 val loss: 0.582130
[Epoch 90] ogbg-molsider: 0.584959 test loss: 0.614350
[Epoch 91; Iter    30/   36] train: loss: 0.3430811
[Epoch 91] ogbg-molsider: 0.601702 val loss: 0.621965
[Epoch 91] ogbg-molsider: 0.578785 test loss: 0.661073
[Epoch 92; Iter    24/   36] train: loss: 0.3178396
[Epoch 92] ogbg-molsider: 0.606082 val loss: 0.582050
[Epoch 92] ogbg-molsider: 0.594540 test loss: 0.604796
[Epoch 93; Iter    18/   36] train: loss: 0.3077202
[Epoch 93] ogbg-molsider: 0.605786 val loss: 0.606502
[Epoch 93] ogbg-molsider: 0.586071 test loss: 0.638537
[Epoch 94; Iter    12/   36] train: loss: 0.2688508
[Epoch 94] ogbg-molsider: 0.605358 val loss: 0.610164
[Epoch 94] ogbg-molsider: 0.595836 test loss: 0.630013
[Epoch 95; Iter     6/   36] train: loss: 0.2756413
[Epoch 95; Iter    36/   36] train: loss: 0.2851505
[Epoch 95] ogbg-molsider: 0.602952 val loss: 0.609935
[Epoch 95] ogbg-molsider: 0.584631 test loss: 0.632133
[Epoch 96; Iter    30/   36] train: loss: 0.2738438
[Epoch 96] ogbg-molsider: 0.592732 val loss: 0.612443
[Epoch 96] ogbg-molsider: 0.579868 test loss: 0.633010
[Epoch 97; Iter    24/   36] train: loss: 0.2712475
[Epoch 97] ogbg-molsider: 0.622292 val loss: 0.599747
[Epoch 97] ogbg-molsider: 0.598720 test loss: 0.642409
[Epoch 98; Iter    18/   36] train: loss: 0.2814689
[Epoch 98] ogbg-molsider: 0.605814 val loss: 0.607471
[Epoch 98] ogbg-molsider: 0.583349 test loss: 0.638300
[Epoch 99; Iter    12/   36] train: loss: 0.2591732
[Epoch 99] ogbg-molsider: 0.602487 val loss: 0.636666
[Epoch 99] ogbg-molsider: 0.584264 test loss: 0.678230
[Epoch 100; Iter     6/   36] train: loss: 0.2615018
[Epoch 100; Iter    36/   36] train: loss: 0.2899708
[Epoch 100] ogbg-molsider: 0.603222 val loss: 0.611317
[Epoch 100] ogbg-molsider: 0.583149 test loss: 0.630272
[Epoch 101; Iter    30/   36] train: loss: 0.3088325
[Epoch 101] ogbg-molsider: 0.602040 val loss: 0.633035
[Epoch 101] ogbg-molsider: 0.594070 test loss: 0.635325
[Epoch 102; Iter    24/   36] train: loss: 0.3051458
[Epoch 102] ogbg-molsider: 0.605552 val loss: 0.642404
[Epoch 102] ogbg-molsider: 0.590088 test loss: 0.682390
[Epoch 103; Iter    18/   36] train: loss: 0.2935725
[Epoch 103] ogbg-molsider: 0.587363 val loss: 0.664924
[Epoch 103] ogbg-molsider: 0.578345 test loss: 0.715117
[Epoch 104; Iter    12/   36] train: loss: 0.2906132
[Epoch 104] ogbg-molsider: 0.610095 val loss: 0.621353
[Epoch 104] ogbg-molsider: 0.585499 test loss: 0.631978
[Epoch 105; Iter     6/   36] train: loss: 0.3042749
[Epoch 105; Iter    36/   36] train: loss: 0.2932114
[Epoch 105] ogbg-molsider: 0.598636 val loss: 0.622764
[Epoch 105] ogbg-molsider: 0.583270 test loss: 0.644013
[Epoch 106; Iter    30/   36] train: loss: 0.2514420
[Epoch 106] ogbg-molsider: 0.617575 val loss: 0.611617
[Epoch 106] ogbg-molsider: 0.585745 test loss: 0.640272
[Epoch 107; Iter    24/   36] train: loss: 0.2397031
[Epoch 107] ogbg-molsider: 0.613089 val loss: 0.616620
[Epoch 107] ogbg-molsider: 0.589592 test loss: 0.640895
[Epoch 108; Iter    18/   36] train: loss: 0.2668926
[Epoch 108] ogbg-molsider: 0.606814 val loss: 0.619572
[Epoch 108] ogbg-molsider: 0.584861 test loss: 0.634044
[Epoch 109; Iter    12/   36] train: loss: 0.2408449
[Epoch 109] ogbg-molsider: 0.613310 val loss: 0.620588
[Epoch 109] ogbg-molsider: 0.597629 test loss: 0.647238
[Epoch 110; Iter     6/   36] train: loss: 0.2480775
[Epoch 110; Iter    36/   36] train: loss: 0.2768344
[Epoch 110] ogbg-molsider: 0.604792 val loss: 0.644742
[Epoch 110] ogbg-molsider: 0.597176 test loss: 0.647482
[Epoch 111; Iter    30/   36] train: loss: 0.2788927
[Epoch 111] ogbg-molsider: 0.607255 val loss: 0.638893
[Epoch 111] ogbg-molsider: 0.597727 test loss: 0.649049
[Epoch 112; Iter    24/   36] train: loss: 0.2552307
[Epoch 112] ogbg-molsider: 0.604568 val loss: 0.650226
[Epoch 112] ogbg-molsider: 0.578754 test loss: 0.678327
[Epoch 113; Iter    18/   36] train: loss: 0.2625991
[Epoch 113] ogbg-molsider: 0.609456 val loss: 0.640016
[Epoch 113] ogbg-molsider: 0.597624 test loss: 0.654087
[Epoch 114; Iter    12/   36] train: loss: 0.2255092
[Epoch 114] ogbg-molsider: 0.608469 val loss: 0.649565
[Epoch 114] ogbg-molsider: 0.588858 test loss: 0.666258
[Epoch 115; Iter     6/   36] train: loss: 0.2398219
[Epoch 115; Iter    36/   36] train: loss: 0.2825623
[Epoch 115] ogbg-molsider: 0.608864 val loss: 0.646986
[Epoch 115] ogbg-molsider: 0.591197 test loss: 0.668609
[Epoch 116; Iter    30/   36] train: loss: 0.2524449
[Epoch 116] ogbg-molsider: 0.611088 val loss: 0.657009
[Epoch 116] ogbg-molsider: 0.591740 test loss: 0.681064
[Epoch 117; Iter    24/   36] train: loss: 0.2535421
[Epoch 117] ogbg-molsider: 0.607152 val loss: 0.652851
[Epoch 117] ogbg-molsider: 0.583643 test loss: 0.681490
[Epoch 118; Iter    18/   36] train: loss: 0.2483834
[Epoch 118] ogbg-molsider: 0.603140 val loss: 0.657170
[Epoch 118] ogbg-molsider: 0.596389 test loss: 0.672308
[Epoch 119; Iter    12/   36] train: loss: 0.2248347
[Epoch 119] ogbg-molsider: 0.604133 val loss: 0.677457
[Epoch 119] ogbg-molsider: 0.594866 test loss: 0.702348
[Epoch 120; Iter     6/   36] train: loss: 0.2286492
[Epoch 120; Iter    36/   36] train: loss: 0.2935643
[Epoch 120] ogbg-molsider: 0.602357 val loss: 0.676317
[Epoch 120] ogbg-molsider: 0.594747 test loss: 0.716511
[Epoch 121; Iter    30/   36] train: loss: 0.2278886
[Epoch 121] ogbg-molsider: 0.600894 val loss: 0.671945
[Epoch 121] ogbg-molsider: 0.596162 test loss: 0.670502
[Epoch 122; Iter    24/   36] train: loss: 0.2386089
[Epoch 122] ogbg-molsider: 0.608985 val loss: 0.657318
[Epoch 122] ogbg-molsider: 0.598149 test loss: 0.673529
[Epoch 123; Iter    18/   36] train: loss: 0.2140107
[Epoch 123] ogbg-molsider: 0.611232 val loss: 0.666707
[Epoch 123] ogbg-molsider: 0.592910 test loss: 0.674867
[Epoch 124; Iter    12/   36] train: loss: 0.2300157
[Epoch 124] ogbg-molsider: 0.612191 val loss: 0.655150
[Epoch 124] ogbg-molsider: 0.595466 test loss: 0.667364
[Epoch 125; Iter     6/   36] train: loss: 0.2247727
[Epoch 125; Iter    36/   36] train: loss: 0.2215186
[Epoch 125] ogbg-molsider: 0.611342 val loss: 0.670155
[Epoch 125] ogbg-molsider: 0.597499 test loss: 0.703127
[Epoch 126; Iter    30/   36] train: loss: 0.2313715
[Epoch 126] ogbg-molsider: 0.602799 val loss: 0.673808
[Epoch 126] ogbg-molsider: 0.589333 test loss: 0.695018
[Epoch 127; Iter    24/   36] train: loss: 0.2607642
[Epoch 127] ogbg-molsider: 0.601074 val loss: 0.725588
[Epoch 127] ogbg-molsider: 0.597267 test loss: 0.751938
[Epoch 128; Iter    18/   36] train: loss: 0.2411035
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.604020 val loss: 0.670119
[Epoch 128] ogbg-molsider: 0.593998 test loss: 0.693735
[Epoch 129; Iter    12/   36] train: loss: 0.2307645
[Epoch 129] ogbg-molsider: 0.616450 val loss: 0.681895
[Epoch 129] ogbg-molsider: 0.600354 test loss: 0.715927
Early stopping criterion based on -ogbg-molsider- that should be max reached after 129 epochs. Best model checkpoint was in epoch 69.
Statistics on  val_best_checkpoint
mean_pred: 0.574234664440155
std_pred: 2.690805673599243
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6741287452669502
rocauc: 0.641688248227934
ogbg-molsider: 0.641688248227934
OGBNanLabelBCEWithLogitsLoss: 0.49615824818611143
Statistics on  test
mean_pred: 0.47637927532196045
std_pred: 2.58229923248291
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6292765192040605
rocauc: 0.6140147950992225
ogbg-molsider: 0.6140147950992225
OGBNanLabelBCEWithLogitsLoss: 0.532723480463028
Statistics on  train
mean_pred: 0.15474160015583038
std_pred: 17.588672637939453
mean_targets: 0.5641575455665588
std_targets: 0.4958747327327728
prcauc: 0.8140396543960373
rocauc: 0.8678537447249568
ogbg-molsider: 0.8678537447249568
OGBNanLabelBCEWithLogitsLoss: 0.39190101209614014
[Epoch 81; Iter    30/   36] train: loss: 0.3263279
[Epoch 81] ogbg-molsider: 0.620210 val loss: 0.518218
[Epoch 81] ogbg-molsider: 0.591097 test loss: 0.574180
[Epoch 82; Iter    24/   36] train: loss: 0.3559794
[Epoch 82] ogbg-molsider: 0.621649 val loss: 0.521027
[Epoch 82] ogbg-molsider: 0.569852 test loss: 0.579825
[Epoch 83; Iter    18/   36] train: loss: 0.3455671
[Epoch 83] ogbg-molsider: 0.617584 val loss: 0.524675
[Epoch 83] ogbg-molsider: 0.566423 test loss: 0.586787
[Epoch 84; Iter    12/   36] train: loss: 0.3473454
[Epoch 84] ogbg-molsider: 0.615462 val loss: 0.537816
[Epoch 84] ogbg-molsider: 0.563531 test loss: 0.614801
[Epoch 85; Iter     6/   36] train: loss: 0.3173071
[Epoch 85; Iter    36/   36] train: loss: 0.3236748
[Epoch 85] ogbg-molsider: 0.628640 val loss: 0.528473
[Epoch 85] ogbg-molsider: 0.562979 test loss: 0.586560
[Epoch 86; Iter    30/   36] train: loss: 0.3432424
[Epoch 86] ogbg-molsider: 0.606934 val loss: 0.543260
[Epoch 86] ogbg-molsider: 0.564031 test loss: 0.615621
[Epoch 87; Iter    24/   36] train: loss: 0.3299787
[Epoch 87] ogbg-molsider: 0.626452 val loss: 0.526488
[Epoch 87] ogbg-molsider: 0.569754 test loss: 0.589257
[Epoch 88; Iter    18/   36] train: loss: 0.3143231
[Epoch 88] ogbg-molsider: 0.603677 val loss: 0.537432
[Epoch 88] ogbg-molsider: 0.565950 test loss: 0.611033
[Epoch 89; Iter    12/   36] train: loss: 0.2948739
[Epoch 89] ogbg-molsider: 0.625588 val loss: 0.587144
[Epoch 89] ogbg-molsider: 0.564112 test loss: 0.707992
[Epoch 90; Iter     6/   36] train: loss: 0.2966947
[Epoch 90; Iter    36/   36] train: loss: 0.3382585
[Epoch 90] ogbg-molsider: 0.630264 val loss: 0.536457
[Epoch 90] ogbg-molsider: 0.571844 test loss: 0.616279
[Epoch 91; Iter    30/   36] train: loss: 0.3344169
[Epoch 91] ogbg-molsider: 0.627937 val loss: 0.538512
[Epoch 91] ogbg-molsider: 0.588973 test loss: 0.594720
[Epoch 92; Iter    24/   36] train: loss: 0.3253705
[Epoch 92] ogbg-molsider: 0.621798 val loss: 0.561180
[Epoch 92] ogbg-molsider: 0.566930 test loss: 0.616964
[Epoch 93; Iter    18/   36] train: loss: 0.2890851
[Epoch 93] ogbg-molsider: 0.634896 val loss: 0.538178
[Epoch 93] ogbg-molsider: 0.567895 test loss: 0.625280
[Epoch 94; Iter    12/   36] train: loss: 0.2932825
[Epoch 94] ogbg-molsider: 0.603049 val loss: 0.576205
[Epoch 94] ogbg-molsider: 0.578673 test loss: 0.642237
[Epoch 95; Iter     6/   36] train: loss: 0.3544260
[Epoch 95; Iter    36/   36] train: loss: 0.3106286
[Epoch 95] ogbg-molsider: 0.619663 val loss: 0.546526
[Epoch 95] ogbg-molsider: 0.573598 test loss: 0.632853
[Epoch 96; Iter    30/   36] train: loss: 0.3314142
[Epoch 96] ogbg-molsider: 0.631227 val loss: 0.543897
[Epoch 96] ogbg-molsider: 0.577998 test loss: 0.635191
[Epoch 97; Iter    24/   36] train: loss: 0.3289265
[Epoch 97] ogbg-molsider: 0.622425 val loss: 0.565224
[Epoch 97] ogbg-molsider: 0.566443 test loss: 0.672541
[Epoch 98; Iter    18/   36] train: loss: 0.3371843
[Epoch 98] ogbg-molsider: 0.610133 val loss: 0.558815
[Epoch 98] ogbg-molsider: 0.573447 test loss: 0.642228
[Epoch 99; Iter    12/   36] train: loss: 0.3618392
[Epoch 99] ogbg-molsider: 0.612049 val loss: 0.563413
[Epoch 99] ogbg-molsider: 0.571921 test loss: 0.652055
[Epoch 100; Iter     6/   36] train: loss: 0.2600614
[Epoch 100; Iter    36/   36] train: loss: 0.3110866
[Epoch 100] ogbg-molsider: 0.651824 val loss: 0.543663
[Epoch 100] ogbg-molsider: 0.590386 test loss: 0.649291
[Epoch 101; Iter    30/   36] train: loss: 0.2976381
[Epoch 101] ogbg-molsider: 0.634143 val loss: 0.563485
[Epoch 101] ogbg-molsider: 0.584162 test loss: 0.657960
[Epoch 102; Iter    24/   36] train: loss: 0.3220647
[Epoch 102] ogbg-molsider: 0.624854 val loss: 0.565633
[Epoch 102] ogbg-molsider: 0.572144 test loss: 0.643397
[Epoch 103; Iter    18/   36] train: loss: 0.2843980
[Epoch 103] ogbg-molsider: 0.628523 val loss: 0.564026
[Epoch 103] ogbg-molsider: 0.582075 test loss: 0.647631
[Epoch 104; Iter    12/   36] train: loss: 0.2808409
[Epoch 104] ogbg-molsider: 0.608394 val loss: 0.587774
[Epoch 104] ogbg-molsider: 0.563605 test loss: 0.692682
[Epoch 105; Iter     6/   36] train: loss: 0.2548145
[Epoch 105; Iter    36/   36] train: loss: 0.3212283
[Epoch 105] ogbg-molsider: 0.618389 val loss: 0.581712
[Epoch 105] ogbg-molsider: 0.597814 test loss: 0.634942
[Epoch 106; Iter    30/   36] train: loss: 0.3204786
[Epoch 106] ogbg-molsider: 0.633735 val loss: 0.559056
[Epoch 106] ogbg-molsider: 0.583930 test loss: 0.627677
[Epoch 107; Iter    24/   36] train: loss: 0.2958412
[Epoch 107] ogbg-molsider: 0.631915 val loss: 0.577031
[Epoch 107] ogbg-molsider: 0.579849 test loss: 0.668948
[Epoch 108; Iter    18/   36] train: loss: 0.2826604
[Epoch 108] ogbg-molsider: 0.638493 val loss: 0.559515
[Epoch 108] ogbg-molsider: 0.592871 test loss: 0.635205
[Epoch 109; Iter    12/   36] train: loss: 0.2750845
[Epoch 109] ogbg-molsider: 0.638736 val loss: 0.570013
[Epoch 109] ogbg-molsider: 0.582854 test loss: 0.661871
[Epoch 110; Iter     6/   36] train: loss: 0.3116097
[Epoch 110; Iter    36/   36] train: loss: 0.2497399
[Epoch 110] ogbg-molsider: 0.639995 val loss: 0.562745
[Epoch 110] ogbg-molsider: 0.573563 test loss: 0.644949
[Epoch 111; Iter    30/   36] train: loss: 0.2617089
[Epoch 111] ogbg-molsider: 0.642706 val loss: 0.568040
[Epoch 111] ogbg-molsider: 0.588769 test loss: 0.656456
[Epoch 112; Iter    24/   36] train: loss: 0.2720540
[Epoch 112] ogbg-molsider: 0.634917 val loss: 0.572934
[Epoch 112] ogbg-molsider: 0.580386 test loss: 0.653781
[Epoch 113; Iter    18/   36] train: loss: 0.2373116
[Epoch 113] ogbg-molsider: 0.635648 val loss: 0.587748
[Epoch 113] ogbg-molsider: 0.576492 test loss: 0.684994
[Epoch 114; Iter    12/   36] train: loss: 0.2610817
[Epoch 114] ogbg-molsider: 0.645598 val loss: 0.576449
[Epoch 114] ogbg-molsider: 0.572919 test loss: 0.670553
[Epoch 115; Iter     6/   36] train: loss: 0.2652768
[Epoch 115; Iter    36/   36] train: loss: 0.2829675
[Epoch 115] ogbg-molsider: 0.636934 val loss: 0.574970
[Epoch 115] ogbg-molsider: 0.578819 test loss: 0.653853
[Epoch 116; Iter    30/   36] train: loss: 0.2617829
[Epoch 116] ogbg-molsider: 0.634409 val loss: 0.585061
[Epoch 116] ogbg-molsider: 0.581439 test loss: 0.683240
[Epoch 117; Iter    24/   36] train: loss: 0.2358841
[Epoch 117] ogbg-molsider: 0.635890 val loss: 0.587805
[Epoch 117] ogbg-molsider: 0.580714 test loss: 0.668789
[Epoch 118; Iter    18/   36] train: loss: 0.2987283
[Epoch 118] ogbg-molsider: 0.639596 val loss: 0.579788
[Epoch 118] ogbg-molsider: 0.585064 test loss: 0.668415
[Epoch 119; Iter    12/   36] train: loss: 0.2382403
[Epoch 119] ogbg-molsider: 0.641725 val loss: 0.590005
[Epoch 119] ogbg-molsider: 0.585367 test loss: 0.674740
[Epoch 120; Iter     6/   36] train: loss: 0.3016045
[Epoch 120; Iter    36/   36] train: loss: 0.2404716
[Epoch 120] ogbg-molsider: 0.645866 val loss: 0.573138
[Epoch 120] ogbg-molsider: 0.591391 test loss: 0.662296
[Epoch 121; Iter    30/   36] train: loss: 0.3076175
[Epoch 121] ogbg-molsider: 0.644050 val loss: 0.592849
[Epoch 121] ogbg-molsider: 0.592197 test loss: 0.669658
[Epoch 122; Iter    24/   36] train: loss: 0.2710577
[Epoch 122] ogbg-molsider: 0.642222 val loss: 0.593326
[Epoch 122] ogbg-molsider: 0.584512 test loss: 0.678609
[Epoch 123; Iter    18/   36] train: loss: 0.2817143
[Epoch 123] ogbg-molsider: 0.641286 val loss: 0.579735
[Epoch 123] ogbg-molsider: 0.578515 test loss: 0.676949
[Epoch 124; Iter    12/   36] train: loss: 0.2582863
[Epoch 124] ogbg-molsider: 0.637325 val loss: 0.596946
[Epoch 124] ogbg-molsider: 0.578827 test loss: 0.694412
[Epoch 125; Iter     6/   36] train: loss: 0.2229450
[Epoch 125; Iter    36/   36] train: loss: 0.2800639
[Epoch 125] ogbg-molsider: 0.639234 val loss: 0.594751
[Epoch 125] ogbg-molsider: 0.580912 test loss: 0.675795
[Epoch 126; Iter    30/   36] train: loss: 0.2529105
[Epoch 126] ogbg-molsider: 0.639593 val loss: 0.596830
[Epoch 126] ogbg-molsider: 0.584326 test loss: 0.685492
[Epoch 127; Iter    24/   36] train: loss: 0.2479568
[Epoch 127] ogbg-molsider: 0.636528 val loss: 0.593800
[Epoch 127] ogbg-molsider: 0.598141 test loss: 0.675890
[Epoch 128; Iter    18/   36] train: loss: 0.2557129
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.624292 val loss: 0.607446
[Epoch 128] ogbg-molsider: 0.596443 test loss: 0.691543
[Epoch 129; Iter    12/   36] train: loss: 0.2811753
[Epoch 129] ogbg-molsider: 0.628173 val loss: 0.598757
[Epoch 129] ogbg-molsider: 0.604605 test loss: 0.664883
[Epoch 130; Iter     6/   36] train: loss: 0.2757368
[Epoch 130; Iter    36/   36] train: loss: 0.2618162
[Epoch 130] ogbg-molsider: 0.623014 val loss: 0.604046
[Epoch 130] ogbg-molsider: 0.604918 test loss: 0.666395
[Epoch 131; Iter    30/   36] train: loss: 0.3022570
[Epoch 131] ogbg-molsider: 0.627715 val loss: 0.597351
[Epoch 131] ogbg-molsider: 0.606908 test loss: 0.666269
[Epoch 132; Iter    24/   36] train: loss: 0.2628186
[Epoch 132] ogbg-molsider: 0.629530 val loss: 0.603678
[Epoch 132] ogbg-molsider: 0.606097 test loss: 0.674533
[Epoch 133; Iter    18/   36] train: loss: 0.2680746
[Epoch 133] ogbg-molsider: 0.622792 val loss: 0.605536
[Epoch 133] ogbg-molsider: 0.602810 test loss: 0.675931
[Epoch 134; Iter    12/   36] train: loss: 0.2701451
[Epoch 134] ogbg-molsider: 0.628065 val loss: 0.599039
[Epoch 134] ogbg-molsider: 0.614010 test loss: 0.656529
[Epoch 135; Iter     6/   36] train: loss: 0.2748336
[Epoch 135; Iter    36/   36] train: loss: 0.3559205
[Epoch 135] ogbg-molsider: 0.635076 val loss: 0.604513
[Epoch 135] ogbg-molsider: 0.614524 test loss: 0.692853
[Epoch 136; Iter    30/   36] train: loss: 0.2564547
[Epoch 136] ogbg-molsider: 0.628084 val loss: 0.619670
[Epoch 136] ogbg-molsider: 0.617436 test loss: 0.681198
[Epoch 137; Iter    24/   36] train: loss: 0.2566170
[Epoch 137] ogbg-molsider: 0.618636 val loss: 0.639500
[Epoch 137] ogbg-molsider: 0.611188 test loss: 0.698481
[Epoch 138; Iter    18/   36] train: loss: 0.3214477
[Epoch 138] ogbg-molsider: 0.625173 val loss: 0.614788
[Epoch 138] ogbg-molsider: 0.606747 test loss: 0.687901
[Epoch 139; Iter    12/   36] train: loss: 0.2350202
[Epoch 139] ogbg-molsider: 0.627783 val loss: 0.607239
[Epoch 139] ogbg-molsider: 0.607188 test loss: 0.692876
[Epoch 140; Iter     6/   36] train: loss: 0.2581821
[Epoch 140; Iter    36/   36] train: loss: 0.2552208
[Epoch 140] ogbg-molsider: 0.624871 val loss: 0.629748
[Epoch 140] ogbg-molsider: 0.612449 test loss: 0.718441
[Epoch 141; Iter    30/   36] train: loss: 0.2731266
[Epoch 141] ogbg-molsider: 0.619624 val loss: 0.618615
[Epoch 141] ogbg-molsider: 0.612139 test loss: 0.690971
[Epoch 142; Iter    24/   36] train: loss: 0.2631077
[Epoch 142] ogbg-molsider: 0.627045 val loss: 0.623549
[Epoch 142] ogbg-molsider: 0.619488 test loss: 0.685566
[Epoch 143; Iter    18/   36] train: loss: 0.2806585
[Epoch 143] ogbg-molsider: 0.615349 val loss: 0.628547
[Epoch 143] ogbg-molsider: 0.616163 test loss: 0.685244
[Epoch 144; Iter    12/   36] train: loss: 0.2327329
[Epoch 144] ogbg-molsider: 0.625523 val loss: 0.623132
[Epoch 144] ogbg-molsider: 0.616645 test loss: 0.679782
[Epoch 145; Iter     6/   36] train: loss: 0.2811841
[Epoch 145; Iter    36/   36] train: loss: 0.2782956
[Epoch 145] ogbg-molsider: 0.630052 val loss: 0.617571
[Epoch 145] ogbg-molsider: 0.612205 test loss: 0.684199
[Epoch 146; Iter    30/   36] train: loss: 0.2595369
[Epoch 146] ogbg-molsider: 0.624296 val loss: 0.628472
[Epoch 146] ogbg-molsider: 0.612428 test loss: 0.694850
[Epoch 147; Iter    24/   36] train: loss: 0.2623318
[Epoch 147] ogbg-molsider: 0.620003 val loss: 0.624012
[Epoch 147] ogbg-molsider: 0.611677 test loss: 0.689686
Early stopping criterion based on -ogbg-molsider- that should be max reached after 147 epochs. Best model checkpoint was in epoch 87.
Statistics on  val_best_checkpoint
mean_pred: 0.6894398331642151
std_pred: 2.6583240032196045
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6801489955395226
rocauc: 0.6539313957800325
ogbg-molsider: 0.6539313957800325
OGBNanLabelBCEWithLogitsLoss: 0.49634101390838625
Statistics on  test
mean_pred: 0.7169032096862793
std_pred: 2.70559024810791
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6224850919612822
rocauc: 0.5918028027588232
ogbg-molsider: 0.5918028027588232
OGBNanLabelBCEWithLogitsLoss: 0.569152569770813
Statistics on  train
mean_pred: 0.36802899837493896
std_pred: 2.6585214138031006
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8083499856042679
rocauc: 0.8546582053537347
ogbg-molsider: 0.8546582053537347
OGBNanLabelBCEWithLogitsLoss: 0.36758090886804795
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.646637 val loss: 0.586239
[Epoch 128] ogbg-molsider: 0.577493 test loss: 0.690966
[Epoch 129; Iter    12/   36] train: loss: 0.2733988
[Epoch 129] ogbg-molsider: 0.640861 val loss: 0.598644
[Epoch 129] ogbg-molsider: 0.585491 test loss: 0.688135
[Epoch 130; Iter     6/   36] train: loss: 0.2166212
[Epoch 130; Iter    36/   36] train: loss: 0.2232078
[Epoch 130] ogbg-molsider: 0.637556 val loss: 0.606627
[Epoch 130] ogbg-molsider: 0.584575 test loss: 0.697425
[Epoch 131; Iter    30/   36] train: loss: 0.2729703
[Epoch 131] ogbg-molsider: 0.631945 val loss: 0.604528
[Epoch 131] ogbg-molsider: 0.582652 test loss: 0.690440
[Epoch 132; Iter    24/   36] train: loss: 0.2207426
[Epoch 132] ogbg-molsider: 0.627528 val loss: 0.615160
[Epoch 132] ogbg-molsider: 0.582215 test loss: 0.705114
[Epoch 133; Iter    18/   36] train: loss: 0.1962525
[Epoch 133] ogbg-molsider: 0.624717 val loss: 0.640686
[Epoch 133] ogbg-molsider: 0.575188 test loss: 0.756558
[Epoch 134; Iter    12/   36] train: loss: 0.2142273
[Epoch 134] ogbg-molsider: 0.646349 val loss: 0.607580
[Epoch 134] ogbg-molsider: 0.591474 test loss: 0.693897
[Epoch 135; Iter     6/   36] train: loss: 0.2458757
[Epoch 135; Iter    36/   36] train: loss: 0.2856344
[Epoch 135] ogbg-molsider: 0.639450 val loss: 0.606328
[Epoch 135] ogbg-molsider: 0.586820 test loss: 0.703932
[Epoch 136; Iter    30/   36] train: loss: 0.2054633
[Epoch 136] ogbg-molsider: 0.636378 val loss: 0.619582
[Epoch 136] ogbg-molsider: 0.584978 test loss: 0.716227
[Epoch 137; Iter    24/   36] train: loss: 0.2430008
[Epoch 137] ogbg-molsider: 0.636504 val loss: 0.621645
[Epoch 137] ogbg-molsider: 0.580965 test loss: 0.723021
[Epoch 138; Iter    18/   36] train: loss: 0.2196980
[Epoch 138] ogbg-molsider: 0.643971 val loss: 0.614912
[Epoch 138] ogbg-molsider: 0.585909 test loss: 0.710852
[Epoch 139; Iter    12/   36] train: loss: 0.2137108
[Epoch 139] ogbg-molsider: 0.640202 val loss: 0.621114
[Epoch 139] ogbg-molsider: 0.582420 test loss: 0.712697
[Epoch 140; Iter     6/   36] train: loss: 0.2209346
[Epoch 140; Iter    36/   36] train: loss: 0.2350484
[Epoch 140] ogbg-molsider: 0.642223 val loss: 0.620039
[Epoch 140] ogbg-molsider: 0.590602 test loss: 0.711155
[Epoch 141; Iter    30/   36] train: loss: 0.2298514
[Epoch 141] ogbg-molsider: 0.637819 val loss: 0.624405
[Epoch 141] ogbg-molsider: 0.580432 test loss: 0.720130
[Epoch 142; Iter    24/   36] train: loss: 0.2511122
[Epoch 142] ogbg-molsider: 0.633856 val loss: 0.633905
[Epoch 142] ogbg-molsider: 0.590500 test loss: 0.720512
[Epoch 143; Iter    18/   36] train: loss: 0.2196987
[Epoch 143] ogbg-molsider: 0.638445 val loss: 0.628277
[Epoch 143] ogbg-molsider: 0.583395 test loss: 0.719882
[Epoch 144; Iter    12/   36] train: loss: 0.2025890
[Epoch 144] ogbg-molsider: 0.635908 val loss: 0.641364
[Epoch 144] ogbg-molsider: 0.581671 test loss: 0.733132
[Epoch 145; Iter     6/   36] train: loss: 0.2431207
[Epoch 145; Iter    36/   36] train: loss: 0.2534490
[Epoch 145] ogbg-molsider: 0.635961 val loss: 0.639410
[Epoch 145] ogbg-molsider: 0.593379 test loss: 0.726663
[Epoch 146; Iter    30/   36] train: loss: 0.1964670
[Epoch 146] ogbg-molsider: 0.640222 val loss: 0.629418
[Epoch 146] ogbg-molsider: 0.592205 test loss: 0.709077
[Epoch 147; Iter    24/   36] train: loss: 0.2265364
[Epoch 147] ogbg-molsider: 0.635419 val loss: 0.632036
[Epoch 147] ogbg-molsider: 0.582978 test loss: 0.736242
[Epoch 148; Iter    18/   36] train: loss: 0.2046892
[Epoch 148] ogbg-molsider: 0.633890 val loss: 0.643289
[Epoch 148] ogbg-molsider: 0.584614 test loss: 0.741409
[Epoch 149; Iter    12/   36] train: loss: 0.2097597
[Epoch 149] ogbg-molsider: 0.645180 val loss: 0.632125
[Epoch 149] ogbg-molsider: 0.589485 test loss: 0.727509
[Epoch 150; Iter     6/   36] train: loss: 0.2559799
[Epoch 150; Iter    36/   36] train: loss: 0.2578498
[Epoch 150] ogbg-molsider: 0.638616 val loss: 0.637656
[Epoch 150] ogbg-molsider: 0.590671 test loss: 0.728032
[Epoch 151; Iter    30/   36] train: loss: 0.2088067
[Epoch 151] ogbg-molsider: 0.640823 val loss: 0.643365
[Epoch 151] ogbg-molsider: 0.594009 test loss: 0.737172
[Epoch 152; Iter    24/   36] train: loss: 0.2057638
[Epoch 152] ogbg-molsider: 0.637172 val loss: 0.645452
[Epoch 152] ogbg-molsider: 0.588135 test loss: 0.735685
[Epoch 153; Iter    18/   36] train: loss: 0.2056821
[Epoch 153] ogbg-molsider: 0.639907 val loss: 0.647206
[Epoch 153] ogbg-molsider: 0.588146 test loss: 0.735880
[Epoch 154; Iter    12/   36] train: loss: 0.1976015
[Epoch 154] ogbg-molsider: 0.647770 val loss: 0.641921
[Epoch 154] ogbg-molsider: 0.591781 test loss: 0.747291
[Epoch 155; Iter     6/   36] train: loss: 0.1843359
[Epoch 155; Iter    36/   36] train: loss: 0.2325758
[Epoch 155] ogbg-molsider: 0.638936 val loss: 0.638290
[Epoch 155] ogbg-molsider: 0.589781 test loss: 0.725487
[Epoch 156; Iter    30/   36] train: loss: 0.2420352
[Epoch 156] ogbg-molsider: 0.633372 val loss: 0.651083
[Epoch 156] ogbg-molsider: 0.592692 test loss: 0.736602
[Epoch 157; Iter    24/   36] train: loss: 0.2195570
[Epoch 157] ogbg-molsider: 0.635413 val loss: 0.643486
[Epoch 157] ogbg-molsider: 0.593998 test loss: 0.718902
[Epoch 158; Iter    18/   36] train: loss: 0.1972111
[Epoch 158] ogbg-molsider: 0.639033 val loss: 0.650037
[Epoch 158] ogbg-molsider: 0.595371 test loss: 0.727215
[Epoch 159; Iter    12/   36] train: loss: 0.1928777
[Epoch 159] ogbg-molsider: 0.642323 val loss: 0.646277
[Epoch 159] ogbg-molsider: 0.592972 test loss: 0.738059
[Epoch 160; Iter     6/   36] train: loss: 0.2412863
[Epoch 160; Iter    36/   36] train: loss: 0.2463937
[Epoch 160] ogbg-molsider: 0.644904 val loss: 0.642453
[Epoch 160] ogbg-molsider: 0.592813 test loss: 0.739383
Early stopping criterion based on -ogbg-molsider- that should be max reached after 160 epochs. Best model checkpoint was in epoch 100.
Statistics on  val_best_checkpoint
mean_pred: 0.9017665982246399
std_pred: 3.310075521469116
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6901372445952977
rocauc: 0.6518240744819352
ogbg-molsider: 0.6518240744819352
OGBNanLabelBCEWithLogitsLoss: 0.5436629951000214
Statistics on  test
mean_pred: 0.9692298769950867
std_pred: 3.3097774982452393
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6294389759588591
rocauc: 0.5903858846293931
ogbg-molsider: 0.5903858846293931
OGBNanLabelBCEWithLogitsLoss: 0.6492912173271179
Statistics on  train
mean_pred: 0.6947835683822632
std_pred: 3.305356025695801
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8698570454316195
rocauc: 0.9129198293055387
ogbg-molsider: 0.9129198293055387
OGBNanLabelBCEWithLogitsLoss: 0.28919317821661633
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.0.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.0.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/sider/noise=0.0.yml --seed 6 --device cuda:0
All runs completed.
