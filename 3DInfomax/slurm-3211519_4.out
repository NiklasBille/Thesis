>>> Starting run for dataset: muv
Running configs_static_noise_experiments/GraphCL/muv/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/GraphCL/muv/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/GraphCL/muv/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/GraphCL/muv/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/muv/noise=0.0/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.0_6_26-05_10-46-00
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.0
logdir: runs/static_noise/GraphCL/muv/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6952796
[Epoch 1; Iter    60/ 2483] train: loss: 0.6934785
[Epoch 1; Iter    90/ 2483] train: loss: 0.6947290
[Epoch 1; Iter   120/ 2483] train: loss: 0.6930490
[Epoch 1; Iter   150/ 2483] train: loss: 0.6931462
[Epoch 1; Iter   180/ 2483] train: loss: 0.6954095
[Epoch 1; Iter   210/ 2483] train: loss: 0.6934831
[Epoch 1; Iter   240/ 2483] train: loss: 0.6947033
[Epoch 1; Iter   270/ 2483] train: loss: 0.6932601
[Epoch 1; Iter   300/ 2483] train: loss: 0.6949934
[Epoch 1; Iter   330/ 2483] train: loss: 0.6914181
[Epoch 1; Iter   360/ 2483] train: loss: 0.6910745
[Epoch 1; Iter   390/ 2483] train: loss: 0.6928051
[Epoch 1; Iter   420/ 2483] train: loss: 0.6923913
[Epoch 1; Iter   450/ 2483] train: loss: 0.6928921
[Epoch 1; Iter   480/ 2483] train: loss: 0.6911702
[Epoch 1; Iter   510/ 2483] train: loss: 0.6901565
[Epoch 1; Iter   540/ 2483] train: loss: 0.6922672
[Epoch 1; Iter   570/ 2483] train: loss: 0.6909302
[Epoch 1; Iter   600/ 2483] train: loss: 0.6912378
[Epoch 1; Iter   630/ 2483] train: loss: 0.6912470
[Epoch 1; Iter   660/ 2483] train: loss: 0.6896852
[Epoch 1; Iter   690/ 2483] train: loss: 0.6908053
[Epoch 1; Iter   720/ 2483] train: loss: 0.6854127
[Epoch 1; Iter   750/ 2483] train: loss: 0.6770173
[Epoch 1; Iter   780/ 2483] train: loss: 0.6577449
[Epoch 1; Iter   810/ 2483] train: loss: 0.6281791
[Epoch 1; Iter   840/ 2483] train: loss: 0.5945736
[Epoch 1; Iter   870/ 2483] train: loss: 0.5432993
[Epoch 1; Iter   900/ 2483] train: loss: 0.5130760
[Epoch 1; Iter   930/ 2483] train: loss: 0.4367364
[Epoch 1; Iter   960/ 2483] train: loss: 0.3865984
[Epoch 1; Iter   990/ 2483] train: loss: 0.3353871
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2338542
[Epoch 1; Iter  1050/ 2483] train: loss: 0.2198204
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1589025
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1529935
[Epoch 1; Iter  1140/ 2483] train: loss: 0.1034282
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0874643
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0640629
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0575758
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0510115
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0339963
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0288052
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0258255
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0187389
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0192475
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0669842
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0186361
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0630478
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0159011
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0177846
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0152851
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0159861
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0120365
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0123145
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0109413
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0100424
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0099625
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0099327
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0559033
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0078934
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0061007
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0064149
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0057141
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0573415
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0561044
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0051975
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0052775
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0049580
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0044457
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0044454
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0656167
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0038017
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0038192
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0808676
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0033993
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0035030
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0037920
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0054065
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0031905
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0815728
[Epoch 1] ogbg-molmuv: 0.042456 val loss: 0.015236
[Epoch 1] ogbg-molmuv: 0.014515 test loss: 0.015543
[Epoch 2; Iter     7/ 2483] train: loss: 0.0031188
[Epoch 2; Iter    37/ 2483] train: loss: 0.0029900
[Epoch 2; Iter    67/ 2483] train: loss: 0.0024186
[Epoch 2; Iter    97/ 2483] train: loss: 0.0022710
[Epoch 2; Iter   127/ 2483] train: loss: 0.0803722
[Epoch 2; Iter   157/ 2483] train: loss: 0.0033275
[Epoch 2; Iter   187/ 2483] train: loss: 0.0048399
[Epoch 2; Iter   217/ 2483] train: loss: 0.0036190
[Epoch 2; Iter   247/ 2483] train: loss: 0.0024256
[Epoch 2; Iter   277/ 2483] train: loss: 0.0028673
[Epoch 2; Iter   307/ 2483] train: loss: 0.0022416
[Epoch 2; Iter   337/ 2483] train: loss: 0.0022865
[Epoch 2; Iter   367/ 2483] train: loss: 0.0021881
[Epoch 2; Iter   397/ 2483] train: loss: 0.0023046
[Epoch 2; Iter   427/ 2483] train: loss: 0.0022546
[Epoch 2; Iter   457/ 2483] train: loss: 0.0022778
[Epoch 2; Iter   487/ 2483] train: loss: 0.0020471
[Epoch 2; Iter   517/ 2483] train: loss: 0.0021986
[Epoch 2; Iter   547/ 2483] train: loss: 0.1022263
[Epoch 2; Iter   577/ 2483] train: loss: 0.1435495
[Epoch 2; Iter   607/ 2483] train: loss: 0.0024548
[Epoch 2; Iter   637/ 2483] train: loss: 0.0606496
[Epoch 2; Iter   667/ 2483] train: loss: 0.0028801
[Epoch 2; Iter   697/ 2483] train: loss: 0.0028497
[Epoch 2; Iter   727/ 2483] train: loss: 0.0022342
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/muv/noise=0.0/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.0_4_26-05_10-46-00
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.0
logdir: runs/static_noise/GraphCL/muv/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6930441
[Epoch 1; Iter    60/ 2483] train: loss: 0.6942822
[Epoch 1; Iter    90/ 2483] train: loss: 0.6912583
[Epoch 1; Iter   120/ 2483] train: loss: 0.6946166
[Epoch 1; Iter   150/ 2483] train: loss: 0.6924197
[Epoch 1; Iter   180/ 2483] train: loss: 0.6921885
[Epoch 1; Iter   210/ 2483] train: loss: 0.6917521
[Epoch 1; Iter   240/ 2483] train: loss: 0.6934283
[Epoch 1; Iter   270/ 2483] train: loss: 0.6925229
[Epoch 1; Iter   300/ 2483] train: loss: 0.6921265
[Epoch 1; Iter   330/ 2483] train: loss: 0.6926635
[Epoch 1; Iter   360/ 2483] train: loss: 0.6902333
[Epoch 1; Iter   390/ 2483] train: loss: 0.6917111
[Epoch 1; Iter   420/ 2483] train: loss: 0.6909686
[Epoch 1; Iter   450/ 2483] train: loss: 0.6906404
[Epoch 1; Iter   480/ 2483] train: loss: 0.6894994
[Epoch 1; Iter   510/ 2483] train: loss: 0.6891860
[Epoch 1; Iter   540/ 2483] train: loss: 0.6920524
[Epoch 1; Iter   570/ 2483] train: loss: 0.6901283
[Epoch 1; Iter   600/ 2483] train: loss: 0.6884238
[Epoch 1; Iter   630/ 2483] train: loss: 0.6893192
[Epoch 1; Iter   660/ 2483] train: loss: 0.6886856
[Epoch 1; Iter   690/ 2483] train: loss: 0.6873891
[Epoch 1; Iter   720/ 2483] train: loss: 0.6861106
[Epoch 1; Iter   750/ 2483] train: loss: 0.6730834
[Epoch 1; Iter   780/ 2483] train: loss: 0.6516213
[Epoch 1; Iter   810/ 2483] train: loss: 0.6322014
[Epoch 1; Iter   840/ 2483] train: loss: 0.5886401
[Epoch 1; Iter   870/ 2483] train: loss: 0.5530310
[Epoch 1; Iter   900/ 2483] train: loss: 0.4859580
[Epoch 1; Iter   930/ 2483] train: loss: 0.4324163
[Epoch 1; Iter   960/ 2483] train: loss: 0.3633503
[Epoch 1; Iter   990/ 2483] train: loss: 0.3221545
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2647384
[Epoch 1; Iter  1050/ 2483] train: loss: 0.1988406
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1577590
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1203746
[Epoch 1; Iter  1140/ 2483] train: loss: 0.1008427
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0790875
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0606207
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0503188
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0397263
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0378755
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0310860
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0214300
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0195871
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0571919
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0174412
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0171734
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0161949
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0158994
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0130239
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0154895
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0145058
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0602943
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0973307
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0111611
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0098243
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0092361
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0086593
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0067547
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0067080
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0062653
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0065417
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0051692
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0050144
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0057913
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0046550
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0622960
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0041706
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0038241
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0035255
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0032924
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0042396
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0034806
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0039677
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0032159
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0031322
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0027607
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0025328
[Epoch 1; Iter  2430/ 2483] train: loss: 0.1127605
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0036255
[Epoch 1] ogbg-molmuv: 0.010025 val loss: 0.016025
[Epoch 1] ogbg-molmuv: 0.010950 test loss: 0.016367
[Epoch 2; Iter     7/ 2483] train: loss: 0.0029431
[Epoch 2; Iter    37/ 2483] train: loss: 0.0030004
[Epoch 2; Iter    67/ 2483] train: loss: 0.0026066
[Epoch 2; Iter    97/ 2483] train: loss: 0.0027657
[Epoch 2; Iter   127/ 2483] train: loss: 0.0033024
[Epoch 2; Iter   157/ 2483] train: loss: 0.0025633
[Epoch 2; Iter   187/ 2483] train: loss: 0.0957732
[Epoch 2; Iter   217/ 2483] train: loss: 0.0793103
[Epoch 2; Iter   247/ 2483] train: loss: 0.0027083
[Epoch 2; Iter   277/ 2483] train: loss: 0.0025886
[Epoch 2; Iter   307/ 2483] train: loss: 0.0026604
[Epoch 2; Iter   337/ 2483] train: loss: 0.1810237
[Epoch 2; Iter   367/ 2483] train: loss: 0.0027062
[Epoch 2; Iter   397/ 2483] train: loss: 0.0027658
[Epoch 2; Iter   427/ 2483] train: loss: 0.0026690
[Epoch 2; Iter   457/ 2483] train: loss: 0.0023962
[Epoch 2; Iter   487/ 2483] train: loss: 0.0024000
[Epoch 2; Iter   517/ 2483] train: loss: 0.0029424
[Epoch 2; Iter   547/ 2483] train: loss: 0.0022674
[Epoch 2; Iter   577/ 2483] train: loss: 0.0022807
[Epoch 2; Iter   607/ 2483] train: loss: 0.0019201
[Epoch 2; Iter   637/ 2483] train: loss: 0.0021139
[Epoch 2; Iter   667/ 2483] train: loss: 0.0020004
[Epoch 2; Iter   697/ 2483] train: loss: 0.0018881
[Epoch 2; Iter   727/ 2483] train: loss: 0.1641312
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/muv/noise=0.0/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.0_5_26-05_10-46-00
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.0
logdir: runs/static_noise/GraphCL/muv/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6926621
[Epoch 1; Iter    60/ 2483] train: loss: 0.6936054
[Epoch 1; Iter    90/ 2483] train: loss: 0.6931292
[Epoch 1; Iter   120/ 2483] train: loss: 0.6927601
[Epoch 1; Iter   150/ 2483] train: loss: 0.6927921
[Epoch 1; Iter   180/ 2483] train: loss: 0.6926409
[Epoch 1; Iter   210/ 2483] train: loss: 0.6930503
[Epoch 1; Iter   240/ 2483] train: loss: 0.6925803
[Epoch 1; Iter   270/ 2483] train: loss: 0.6917484
[Epoch 1; Iter   300/ 2483] train: loss: 0.6922560
[Epoch 1; Iter   330/ 2483] train: loss: 0.6927465
[Epoch 1; Iter   360/ 2483] train: loss: 0.6916938
[Epoch 1; Iter   390/ 2483] train: loss: 0.6904137
[Epoch 1; Iter   420/ 2483] train: loss: 0.6939397
[Epoch 1; Iter   450/ 2483] train: loss: 0.6895716
[Epoch 1; Iter   480/ 2483] train: loss: 0.6911019
[Epoch 1; Iter   510/ 2483] train: loss: 0.6896297
[Epoch 1; Iter   540/ 2483] train: loss: 0.6906319
[Epoch 1; Iter   570/ 2483] train: loss: 0.6883613
[Epoch 1; Iter   600/ 2483] train: loss: 0.6882179
[Epoch 1; Iter   630/ 2483] train: loss: 0.6914595
[Epoch 1; Iter   660/ 2483] train: loss: 0.6875159
[Epoch 1; Iter   690/ 2483] train: loss: 0.6881213
[Epoch 1; Iter   720/ 2483] train: loss: 0.6854972
[Epoch 1; Iter   750/ 2483] train: loss: 0.6759569
[Epoch 1; Iter   780/ 2483] train: loss: 0.6556168
[Epoch 1; Iter   810/ 2483] train: loss: 0.6322904
[Epoch 1; Iter   840/ 2483] train: loss: 0.5782848
[Epoch 1; Iter   870/ 2483] train: loss: 0.5494775
[Epoch 1; Iter   900/ 2483] train: loss: 0.4997626
[Epoch 1; Iter   930/ 2483] train: loss: 0.4288304
[Epoch 1; Iter   960/ 2483] train: loss: 0.3741678
[Epoch 1; Iter   990/ 2483] train: loss: 0.2775380
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2170656
[Epoch 1; Iter  1050/ 2483] train: loss: 0.1746322
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1345513
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1796148
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0866967
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0666007
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0593857
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0477178
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0392777
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0301505
[Epoch 1; Iter  1320/ 2483] train: loss: 0.1010711
[Epoch 1; Iter  1350/ 2483] train: loss: 0.1403736
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0647712
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0728130
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0618650
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0156819
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0165219
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0796806
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0168365
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0143250
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0123497
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0818074
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0116457
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0105840
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0087262
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0076730
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0674575
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0071416
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0057987
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0057217
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0068100
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0056518
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0051177
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0050451
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0042038
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0048280
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0043031
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0041020
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0039370
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0576943
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0042150
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0826501
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0035409
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0031387
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0031501
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0032035
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0038449
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0035823
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0032893
[Epoch 1] ogbg-molmuv: 0.044229 val loss: 0.041423
[Epoch 1] ogbg-molmuv: 0.024693 test loss: 0.035067
[Epoch 2; Iter     7/ 2483] train: loss: 0.0038099
[Epoch 2; Iter    37/ 2483] train: loss: 0.0030274
[Epoch 2; Iter    67/ 2483] train: loss: 0.0044979
[Epoch 2; Iter    97/ 2483] train: loss: 0.0032099
[Epoch 2; Iter   127/ 2483] train: loss: 0.0027162
[Epoch 2; Iter   157/ 2483] train: loss: 0.0034898
[Epoch 2; Iter   187/ 2483] train: loss: 0.0028538
[Epoch 2; Iter   217/ 2483] train: loss: 0.0026260
[Epoch 2; Iter   247/ 2483] train: loss: 0.0024563
[Epoch 2; Iter   277/ 2483] train: loss: 0.0025425
[Epoch 2; Iter   307/ 2483] train: loss: 0.0026963
[Epoch 2; Iter   337/ 2483] train: loss: 0.0026306
[Epoch 2; Iter   367/ 2483] train: loss: 0.1009492
[Epoch 2; Iter   397/ 2483] train: loss: 0.0759636
[Epoch 2; Iter   427/ 2483] train: loss: 0.0026327
[Epoch 2; Iter   457/ 2483] train: loss: 0.0022694
[Epoch 2; Iter   487/ 2483] train: loss: 0.0811720
[Epoch 2; Iter   517/ 2483] train: loss: 0.0032392
[Epoch 2; Iter   547/ 2483] train: loss: 0.0026473
[Epoch 2; Iter   577/ 2483] train: loss: 0.0023009
[Epoch 2; Iter   607/ 2483] train: loss: 0.0022600
[Epoch 2; Iter   637/ 2483] train: loss: 0.0026608
[Epoch 2; Iter   667/ 2483] train: loss: 0.0019504
[Epoch 2; Iter   697/ 2483] train: loss: 0.0025661
[Epoch 2; Iter   727/ 2483] train: loss: 0.0020644
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[Epoch 2; Iter   757/ 2483] train: loss: 0.0020185
[Epoch 2; Iter   787/ 2483] train: loss: 0.0025384
[Epoch 2; Iter   817/ 2483] train: loss: 0.0023211
[Epoch 2; Iter   847/ 2483] train: loss: 0.0018127
[Epoch 2; Iter   877/ 2483] train: loss: 0.0760958
[Epoch 2; Iter   907/ 2483] train: loss: 0.0837581
[Epoch 2; Iter   937/ 2483] train: loss: 0.0023555
[Epoch 2; Iter   967/ 2483] train: loss: 0.0023814
[Epoch 2; Iter   997/ 2483] train: loss: 0.0024293
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0027140
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0022709
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0019469
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0029469
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0030270
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0015857
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0025662
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0019273
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0710316
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0019243
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0020240
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0016880
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0019102
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0015629
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0020207
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0030485
[Epoch 2; Iter  1507/ 2483] train: loss: 0.1735029
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0017580
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0019674
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0017942
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0018660
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0021732
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0022327
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0018741
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0018942
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0025635
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0019393
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0017970
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0014988
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0016158
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0017910
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0023331
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0019741
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0018067
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0026378
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0020647
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0019310
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0020084
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0020363
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0016922
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0020329
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0021202
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0024953
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0937220
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0016474
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0015157
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0014961
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0015814
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0020939
[Epoch 2] ogbg-molmuv: 0.016912 val loss: 0.015769
[Epoch 2] ogbg-molmuv: 0.048473 test loss: 0.016238
[Epoch 3; Iter    14/ 2483] train: loss: 0.0023494
[Epoch 3; Iter    44/ 2483] train: loss: 0.0027909
[Epoch 3; Iter    74/ 2483] train: loss: 0.0018249
[Epoch 3; Iter   104/ 2483] train: loss: 0.0860373
[Epoch 3; Iter   134/ 2483] train: loss: 0.0014666
[Epoch 3; Iter   164/ 2483] train: loss: 0.0023618
[Epoch 3; Iter   194/ 2483] train: loss: 0.0024456
[Epoch 3; Iter   224/ 2483] train: loss: 0.0017998
[Epoch 3; Iter   254/ 2483] train: loss: 0.0017574
[Epoch 3; Iter   284/ 2483] train: loss: 0.0015858
[Epoch 3; Iter   314/ 2483] train: loss: 0.0787870
[Epoch 3; Iter   344/ 2483] train: loss: 0.0015862
[Epoch 3; Iter   374/ 2483] train: loss: 0.0015279
[Epoch 3; Iter   404/ 2483] train: loss: 0.0684597
[Epoch 3; Iter   434/ 2483] train: loss: 0.0020733
[Epoch 3; Iter   464/ 2483] train: loss: 0.0014509
[Epoch 3; Iter   494/ 2483] train: loss: 0.3421586
[Epoch 3; Iter   524/ 2483] train: loss: 0.0020559
[Epoch 3; Iter   554/ 2483] train: loss: 0.0016838
[Epoch 3; Iter   584/ 2483] train: loss: 0.0018851
[Epoch 3; Iter   614/ 2483] train: loss: 0.0015073
[Epoch 3; Iter   644/ 2483] train: loss: 0.0018983
[Epoch 3; Iter   674/ 2483] train: loss: 0.0019166
[Epoch 3; Iter   704/ 2483] train: loss: 0.0021867
[Epoch 3; Iter   734/ 2483] train: loss: 0.0021848
[Epoch 3; Iter   764/ 2483] train: loss: 0.0024432
[Epoch 3; Iter   794/ 2483] train: loss: 0.0021338
[Epoch 3; Iter   824/ 2483] train: loss: 0.0014918
[Epoch 3; Iter   854/ 2483] train: loss: 0.0020625
[Epoch 3; Iter   884/ 2483] train: loss: 0.0021249
[Epoch 3; Iter   914/ 2483] train: loss: 0.0017740
[Epoch 3; Iter   944/ 2483] train: loss: 0.0018561
[Epoch 3; Iter   974/ 2483] train: loss: 0.0015385
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0017889
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0017001
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0017922
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0018396
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0016773
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0017312
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0018101
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0019549
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0020996
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0018195
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0017716
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0027647
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0020044
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0015401
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0014892
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0014752
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0015133
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0016150
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0016242
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0017293
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0017582
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0014380
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0014864
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0015376
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0017348
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0020198
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0760135
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0017849
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0023641
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0648268
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0020119
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0016703
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0019896
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0021949
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0016774
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0019370
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0015418
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0073783
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0737552
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0020233
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0023613
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0022172
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0023094
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0018584
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0019266
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0420099
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0023630
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0025185
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0023284
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0017709
[Epoch 3] ogbg-molmuv: 0.019263 val loss: 0.014848
[Epoch 3] ogbg-molmuv: 0.025726 test loss: 0.017480
[Epoch 4; Iter    21/ 2483] train: loss: 0.0017367
[Epoch 4; Iter    51/ 2483] train: loss: 0.0022123
[Epoch 4; Iter    81/ 2483] train: loss: 0.0015073
[Epoch 4; Iter   111/ 2483] train: loss: 0.0015061
[Epoch 4; Iter   141/ 2483] train: loss: 0.0414827
[Epoch 4; Iter   171/ 2483] train: loss: 0.0023400
[Epoch 4; Iter   201/ 2483] train: loss: 0.0025841
[Epoch 4; Iter   231/ 2483] train: loss: 0.0022555
[Epoch 4; Iter   261/ 2483] train: loss: 0.0018217
[Epoch 4; Iter   291/ 2483] train: loss: 0.0014811
[Epoch 4; Iter   321/ 2483] train: loss: 0.0026940
[Epoch 4; Iter   351/ 2483] train: loss: 0.0017488
[Epoch 4; Iter   381/ 2483] train: loss: 0.0020578
[Epoch 4; Iter   411/ 2483] train: loss: 0.0014702
[Epoch 4; Iter   441/ 2483] train: loss: 0.0014823
[Epoch 2; Iter   757/ 2483] train: loss: 0.0023710
[Epoch 2; Iter   787/ 2483] train: loss: 0.0016381
[Epoch 2; Iter   817/ 2483] train: loss: 0.0016759
[Epoch 2; Iter   847/ 2483] train: loss: 0.0022439
[Epoch 2; Iter   877/ 2483] train: loss: 0.0017697
[Epoch 2; Iter   907/ 2483] train: loss: 0.0016770
[Epoch 2; Iter   937/ 2483] train: loss: 0.0016987
[Epoch 2; Iter   967/ 2483] train: loss: 0.0752579
[Epoch 2; Iter   997/ 2483] train: loss: 0.0018080
[Epoch 2; Iter  1027/ 2483] train: loss: 0.1038340
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0016025
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0014515
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0016044
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0018704
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0020372
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0693068
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0020744
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0017086
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0017455
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0789200
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0017423
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0018149
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0016912
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0016889
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0015824
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0793872
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0527129
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0019765
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0018284
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0018318
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0019162
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0660412
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0678483
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0021919
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0016906
[Epoch 2; Iter  1807/ 2483] train: loss: 0.2079047
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0035778
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0015348
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0021134
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0022563
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0018496
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0016887
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0018415
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0021181
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0645244
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0025137
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0017803
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0021502
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0025795
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0019933
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0606865
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0024324
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0017792
[Epoch 2; Iter  2347/ 2483] train: loss: 0.1605427
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0019477
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0021537
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0022263
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0020580
[Epoch 2] ogbg-molmuv: 0.022570 val loss: 0.015398
[Epoch 2] ogbg-molmuv: 0.065351 test loss: 0.015733
[Epoch 3; Iter    14/ 2483] train: loss: 0.0621957
[Epoch 3; Iter    44/ 2483] train: loss: 0.0022413
[Epoch 3; Iter    74/ 2483] train: loss: 0.0021196
[Epoch 3; Iter   104/ 2483] train: loss: 0.0666371
[Epoch 3; Iter   134/ 2483] train: loss: 0.0020472
[Epoch 3; Iter   164/ 2483] train: loss: 0.0688260
[Epoch 3; Iter   194/ 2483] train: loss: 0.0015824
[Epoch 3; Iter   224/ 2483] train: loss: 0.0013501
[Epoch 3; Iter   254/ 2483] train: loss: 0.0013372
[Epoch 3; Iter   284/ 2483] train: loss: 0.0017313
[Epoch 3; Iter   314/ 2483] train: loss: 0.0017010
[Epoch 3; Iter   344/ 2483] train: loss: 0.0719669
[Epoch 3; Iter   374/ 2483] train: loss: 0.0017238
[Epoch 3; Iter   404/ 2483] train: loss: 0.0014967
[Epoch 3; Iter   434/ 2483] train: loss: 0.0015995
[Epoch 3; Iter   464/ 2483] train: loss: 0.0782487
[Epoch 3; Iter   494/ 2483] train: loss: 0.0020825
[Epoch 3; Iter   524/ 2483] train: loss: 0.0020891
[Epoch 3; Iter   554/ 2483] train: loss: 0.0024391
[Epoch 3; Iter   584/ 2483] train: loss: 0.0029078
[Epoch 3; Iter   614/ 2483] train: loss: 0.0025290
[Epoch 3; Iter   644/ 2483] train: loss: 0.0021376
[Epoch 3; Iter   674/ 2483] train: loss: 0.0017730
[Epoch 3; Iter   704/ 2483] train: loss: 0.0964771
[Epoch 3; Iter   734/ 2483] train: loss: 0.0018560
[Epoch 3; Iter   764/ 2483] train: loss: 0.0016397
[Epoch 3; Iter   794/ 2483] train: loss: 0.0016187
[Epoch 3; Iter   824/ 2483] train: loss: 0.0915746
[Epoch 3; Iter   854/ 2483] train: loss: 0.0830940
[Epoch 3; Iter   884/ 2483] train: loss: 0.0014845
[Epoch 3; Iter   914/ 2483] train: loss: 0.0022712
[Epoch 3; Iter   944/ 2483] train: loss: 0.0018895
[Epoch 3; Iter   974/ 2483] train: loss: 0.0015860
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0015042
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0016254
[Epoch 3; Iter  1064/ 2483] train: loss: 0.1046563
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0020772
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0575248
[Epoch 3; Iter  1154/ 2483] train: loss: 0.1049093
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0014711
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0014390
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0014984
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0012212
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0013114
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0016285
[Epoch 3; Iter  1364/ 2483] train: loss: 0.1426817
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0014988
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0016248
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0015837
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0017818
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0562875
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0015710
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0020308
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0019079
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0015432
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0019860
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0020668
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0028079
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0021406
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0023330
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0019426
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0020584
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0019928
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0670770
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0019592
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0021608
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0910390
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0027389
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0762699
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0780517
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0040215
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0034132
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0016887
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0025646
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0026706
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0026049
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0023175
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0020824
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0019235
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0016130
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0015446
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0018589
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0017481
[Epoch 3] ogbg-molmuv: 0.017317 val loss: 0.023846
[Epoch 3] ogbg-molmuv: 0.021171 test loss: 0.031665
[Epoch 4; Iter    21/ 2483] train: loss: 0.0014434
[Epoch 4; Iter    51/ 2483] train: loss: 0.0013278
[Epoch 4; Iter    81/ 2483] train: loss: 0.0016692
[Epoch 4; Iter   111/ 2483] train: loss: 0.0015066
[Epoch 4; Iter   141/ 2483] train: loss: 0.0017284
[Epoch 4; Iter   171/ 2483] train: loss: 0.0022730
[Epoch 4; Iter   201/ 2483] train: loss: 0.0766086
[Epoch 4; Iter   231/ 2483] train: loss: 0.0021062
[Epoch 4; Iter   261/ 2483] train: loss: 0.0839992
[Epoch 4; Iter   291/ 2483] train: loss: 0.0022102
[Epoch 4; Iter   321/ 2483] train: loss: 0.0021453
[Epoch 4; Iter   351/ 2483] train: loss: 0.0725327
[Epoch 4; Iter   381/ 2483] train: loss: 0.0019621
[Epoch 4; Iter   411/ 2483] train: loss: 0.0017517
[Epoch 4; Iter   441/ 2483] train: loss: 0.0016869
[Epoch 2; Iter   757/ 2483] train: loss: 0.0021527
[Epoch 2; Iter   787/ 2483] train: loss: 0.0020251
[Epoch 2; Iter   817/ 2483] train: loss: 0.0044919
[Epoch 2; Iter   847/ 2483] train: loss: 0.0019732
[Epoch 2; Iter   877/ 2483] train: loss: 0.0019081
[Epoch 2; Iter   907/ 2483] train: loss: 0.0026252
[Epoch 2; Iter   937/ 2483] train: loss: 0.0018032
[Epoch 2; Iter   967/ 2483] train: loss: 0.0028940
[Epoch 2; Iter   997/ 2483] train: loss: 0.0029178
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0045668
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0019256
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0019250
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0019374
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0680118
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0017742
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0015947
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0017503
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0018176
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0883828
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0018562
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0030713
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0020450
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0021177
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0016838
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0805208
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0028938
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0020171
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0015706
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0019652
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0019474
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0018376
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0022751
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0019694
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0023161
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0024348
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0018530
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0026783
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0020219
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0020161
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0019925
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0017849
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0020353
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0678733
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0024974
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0019159
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0026134
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0026752
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0016127
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0594888
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0020060
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0028937
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0032146
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0020953
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0017729
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0016710
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0015125
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0017509
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0014568
[Epoch 2] ogbg-molmuv: 0.006098 val loss: 0.016920
[Epoch 2] ogbg-molmuv: 0.013990 test loss: 0.015894
[Epoch 3; Iter    14/ 2483] train: loss: 0.0014081
[Epoch 3; Iter    44/ 2483] train: loss: 0.0013339
[Epoch 3; Iter    74/ 2483] train: loss: 0.0016995
[Epoch 3; Iter   104/ 2483] train: loss: 0.0017872
[Epoch 3; Iter   134/ 2483] train: loss: 0.0638245
[Epoch 3; Iter   164/ 2483] train: loss: 0.0020312
[Epoch 3; Iter   194/ 2483] train: loss: 0.0021208
[Epoch 3; Iter   224/ 2483] train: loss: 0.0017858
[Epoch 3; Iter   254/ 2483] train: loss: 0.0016429
[Epoch 3; Iter   284/ 2483] train: loss: 0.0018218
[Epoch 3; Iter   314/ 2483] train: loss: 0.0020588
[Epoch 3; Iter   344/ 2483] train: loss: 0.0020574
[Epoch 3; Iter   374/ 2483] train: loss: 0.0019469
[Epoch 3; Iter   404/ 2483] train: loss: 0.0020241
[Epoch 3; Iter   434/ 2483] train: loss: 0.0020896
[Epoch 3; Iter   464/ 2483] train: loss: 0.0017033
[Epoch 3; Iter   494/ 2483] train: loss: 0.0018934
[Epoch 3; Iter   524/ 2483] train: loss: 0.0014414
[Epoch 3; Iter   554/ 2483] train: loss: 0.0018731
[Epoch 3; Iter   584/ 2483] train: loss: 0.0019266
[Epoch 3; Iter   614/ 2483] train: loss: 0.0020339
[Epoch 3; Iter   644/ 2483] train: loss: 0.0021020
[Epoch 3; Iter   674/ 2483] train: loss: 0.0015392
[Epoch 3; Iter   704/ 2483] train: loss: 0.0019020
[Epoch 3; Iter   734/ 2483] train: loss: 0.0015210
[Epoch 3; Iter   764/ 2483] train: loss: 0.0021210
[Epoch 3; Iter   794/ 2483] train: loss: 0.0020954
[Epoch 3; Iter   824/ 2483] train: loss: 0.0022986
[Epoch 3; Iter   854/ 2483] train: loss: 0.0017823
[Epoch 3; Iter   884/ 2483] train: loss: 0.0027948
[Epoch 3; Iter   914/ 2483] train: loss: 0.0016596
[Epoch 3; Iter   944/ 2483] train: loss: 0.0015455
[Epoch 3; Iter   974/ 2483] train: loss: 0.0015732
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0016324
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0018849
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0015362
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0019129
[Epoch 3; Iter  1124/ 2483] train: loss: 0.1142317
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0012889
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0022987
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0818392
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0021018
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0020406
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0016133
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0716656
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0020897
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0027533
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0636886
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0017522
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0018298
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0694394
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0020920
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0017450
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0014843
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0023028
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0019335
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0017072
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0013502
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0011931
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0017991
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0020291
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0025491
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0015325
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0017157
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0712940
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0022552
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0034483
[Epoch 3; Iter  2024/ 2483] train: loss: 0.1047996
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0022379
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0019222
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0972223
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0023540
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0014546
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0726861
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0016315
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0021000
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0018297
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0020657
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0025498
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0027469
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0023791
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0018875
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0022774
[Epoch 3] ogbg-molmuv: 0.042157 val loss: 0.014797
[Epoch 3] ogbg-molmuv: 0.026861 test loss: 0.016154
[Epoch 4; Iter    21/ 2483] train: loss: 0.0031661
[Epoch 4; Iter    51/ 2483] train: loss: 0.0021631
[Epoch 4; Iter    81/ 2483] train: loss: 0.0021127
[Epoch 4; Iter   111/ 2483] train: loss: 0.0018993
[Epoch 4; Iter   141/ 2483] train: loss: 0.0026407
[Epoch 4; Iter   171/ 2483] train: loss: 0.0023771
[Epoch 4; Iter   201/ 2483] train: loss: 0.0019210
[Epoch 4; Iter   231/ 2483] train: loss: 0.0017411
[Epoch 4; Iter   261/ 2483] train: loss: 0.0022786
[Epoch 4; Iter   291/ 2483] train: loss: 0.1172038
[Epoch 4; Iter   321/ 2483] train: loss: 0.0018909
[Epoch 4; Iter   351/ 2483] train: loss: 0.0017143
[Epoch 4; Iter   381/ 2483] train: loss: 0.0013955
[Epoch 4; Iter   411/ 2483] train: loss: 0.0011857
[Epoch 4; Iter   441/ 2483] train: loss: 0.0012437
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/muv/noise=0.05/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.05_6_26-05_11-08-40
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.05
logdir: runs/static_noise/GraphCL/muv/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6955513
[Epoch 1; Iter    60/ 2483] train: loss: 0.6938544
[Epoch 1; Iter    90/ 2483] train: loss: 0.6946979
[Epoch 1; Iter   120/ 2483] train: loss: 0.6927850
[Epoch 1; Iter   150/ 2483] train: loss: 0.6936981
[Epoch 1; Iter   180/ 2483] train: loss: 0.6948186
[Epoch 1; Iter   210/ 2483] train: loss: 0.6940086
[Epoch 1; Iter   240/ 2483] train: loss: 0.6950694
[Epoch 1; Iter   270/ 2483] train: loss: 0.6936786
[Epoch 1; Iter   300/ 2483] train: loss: 0.6928649
[Epoch 1; Iter   330/ 2483] train: loss: 0.6920428
[Epoch 1; Iter   360/ 2483] train: loss: 0.6919100
[Epoch 1; Iter   390/ 2483] train: loss: 0.6947342
[Epoch 1; Iter   420/ 2483] train: loss: 0.6916596
[Epoch 1; Iter   450/ 2483] train: loss: 0.6928476
[Epoch 1; Iter   480/ 2483] train: loss: 0.6916566
[Epoch 1; Iter   510/ 2483] train: loss: 0.6910854
[Epoch 1; Iter   540/ 2483] train: loss: 0.6919581
[Epoch 1; Iter   570/ 2483] train: loss: 0.6910169
[Epoch 1; Iter   600/ 2483] train: loss: 0.6900046
[Epoch 1; Iter   630/ 2483] train: loss: 0.6923520
[Epoch 1; Iter   660/ 2483] train: loss: 0.6889282
[Epoch 1; Iter   690/ 2483] train: loss: 0.6915832
[Epoch 1; Iter   720/ 2483] train: loss: 0.6850206
[Epoch 1; Iter   750/ 2483] train: loss: 0.6768371
[Epoch 1; Iter   780/ 2483] train: loss: 0.6630361
[Epoch 1; Iter   810/ 2483] train: loss: 0.6298441
[Epoch 1; Iter   840/ 2483] train: loss: 0.5899009
[Epoch 1; Iter   870/ 2483] train: loss: 0.5622593
[Epoch 1; Iter   900/ 2483] train: loss: 0.5370216
[Epoch 1; Iter   930/ 2483] train: loss: 0.4541680
[Epoch 1; Iter   960/ 2483] train: loss: 0.4031084
[Epoch 1; Iter   990/ 2483] train: loss: 0.3534992
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2683840
[Epoch 1; Iter  1050/ 2483] train: loss: 0.2174717
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1622178
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1567072
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0999296
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0839436
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0619599
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0529731
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0429685
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0357254
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0283394
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0250958
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0202664
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0183985
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0607495
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0187178
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0622798
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0158385
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0171097
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0148719
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0161672
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0125508
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0117751
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0107292
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0142870
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0097106
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0084872
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0676801
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0077330
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0074077
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0063549
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0062735
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0611179
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0555994
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0051564
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0050392
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0046314
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0044915
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0044330
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0637700
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0041410
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0042956
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0712714
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0037649
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0036200
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0035039
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0030834
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0029341
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0820927
[Epoch 1] ogbg-molmuv: 0.009031 val loss: 0.143891
[Epoch 1] ogbg-molmuv: 0.008979 test loss: 0.143855
[Epoch 2; Iter     7/ 2483] train: loss: 0.0029635
[Epoch 2; Iter    37/ 2483] train: loss: 0.0025902
[Epoch 2; Iter    67/ 2483] train: loss: 0.0025359
[Epoch 2; Iter    97/ 2483] train: loss: 0.0024917
[Epoch 2; Iter   127/ 2483] train: loss: 0.0809208
[Epoch 2; Iter   157/ 2483] train: loss: 0.0028853
[Epoch 2; Iter   187/ 2483] train: loss: 0.0045054
[Epoch 2; Iter   217/ 2483] train: loss: 0.0030552
[Epoch 2; Iter   247/ 2483] train: loss: 0.0024247
[Epoch 2; Iter   277/ 2483] train: loss: 0.0025913
[Epoch 2; Iter   307/ 2483] train: loss: 0.0024488
[Epoch 2; Iter   337/ 2483] train: loss: 0.0029057
[Epoch 2; Iter   367/ 2483] train: loss: 0.0022742
[Epoch 2; Iter   397/ 2483] train: loss: 0.0020759
[Epoch 2; Iter   427/ 2483] train: loss: 0.0020868
[Epoch 2; Iter   457/ 2483] train: loss: 0.0020155
[Epoch 2; Iter   487/ 2483] train: loss: 0.0020349
[Epoch 2; Iter   517/ 2483] train: loss: 0.0021356
[Epoch 2; Iter   547/ 2483] train: loss: 0.0845760
[Epoch 2; Iter   577/ 2483] train: loss: 0.1296750
[Epoch 2; Iter   607/ 2483] train: loss: 0.0023694
[Epoch 2; Iter   637/ 2483] train: loss: 0.0650173
[Epoch 2; Iter   667/ 2483] train: loss: 0.0028547
[Epoch 2; Iter   697/ 2483] train: loss: 0.0023926
[Epoch 2; Iter   727/ 2483] train: loss: 0.0021246
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/muv/noise=0.05/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.05_5_26-05_11-08-38
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.05
logdir: runs/static_noise/GraphCL/muv/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6926401
[Epoch 1; Iter    60/ 2483] train: loss: 0.6933069
[Epoch 1; Iter    90/ 2483] train: loss: 0.6920623
[Epoch 1; Iter   120/ 2483] train: loss: 0.6928430
[Epoch 1; Iter   150/ 2483] train: loss: 0.6928905
[Epoch 1; Iter   180/ 2483] train: loss: 0.6924037
[Epoch 1; Iter   210/ 2483] train: loss: 0.6936342
[Epoch 1; Iter   240/ 2483] train: loss: 0.6918300
[Epoch 1; Iter   270/ 2483] train: loss: 0.6908567
[Epoch 1; Iter   300/ 2483] train: loss: 0.6926340
[Epoch 1; Iter   330/ 2483] train: loss: 0.6916103
[Epoch 1; Iter   360/ 2483] train: loss: 0.6909459
[Epoch 1; Iter   390/ 2483] train: loss: 0.6907343
[Epoch 1; Iter   420/ 2483] train: loss: 0.6944193
[Epoch 1; Iter   450/ 2483] train: loss: 0.6899902
[Epoch 1; Iter   480/ 2483] train: loss: 0.6920551
[Epoch 1; Iter   510/ 2483] train: loss: 0.6896158
[Epoch 1; Iter   540/ 2483] train: loss: 0.6907413
[Epoch 1; Iter   570/ 2483] train: loss: 0.6879824
[Epoch 1; Iter   600/ 2483] train: loss: 0.6883885
[Epoch 1; Iter   630/ 2483] train: loss: 0.6914372
[Epoch 1; Iter   660/ 2483] train: loss: 0.6873447
[Epoch 1; Iter   690/ 2483] train: loss: 0.6875966
[Epoch 1; Iter   720/ 2483] train: loss: 0.6868424
[Epoch 1; Iter   750/ 2483] train: loss: 0.6749230
[Epoch 1; Iter   780/ 2483] train: loss: 0.6600481
[Epoch 1; Iter   810/ 2483] train: loss: 0.6289191
[Epoch 1; Iter   840/ 2483] train: loss: 0.5972613
[Epoch 1; Iter   870/ 2483] train: loss: 0.5496003
[Epoch 1; Iter   900/ 2483] train: loss: 0.5090054
[Epoch 1; Iter   930/ 2483] train: loss: 0.4334716
[Epoch 1; Iter   960/ 2483] train: loss: 0.3767286
[Epoch 1; Iter   990/ 2483] train: loss: 0.2895444
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2387909
[Epoch 1; Iter  1050/ 2483] train: loss: 0.1812804
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1460347
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1880908
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0901139
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0690748
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0575029
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0443980
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0384769
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0323481
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0948572
[Epoch 1; Iter  1350/ 2483] train: loss: 0.1416886
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0677160
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0737046
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0625649
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0146481
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0160401
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0690073
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0148205
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0135975
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0120081
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0740972
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0118302
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0099274
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0093188
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0092303
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0724932
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0071792
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0060672
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0058841
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0060901
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0069835
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0058806
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0050787
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0048673
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0047445
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0040981
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0040869
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0041410
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0817538
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0037755
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0817998
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0034395
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0032508
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0033530
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0032917
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0037444
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0037570
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0033053
[Epoch 1] ogbg-molmuv: 0.010069 val loss: 0.023584
[Epoch 1] ogbg-molmuv: 0.018345 test loss: 0.022778
[Epoch 2; Iter     7/ 2483] train: loss: 0.0042415
[Epoch 2; Iter    37/ 2483] train: loss: 0.0030593
[Epoch 2; Iter    67/ 2483] train: loss: 0.0043728
[Epoch 2; Iter    97/ 2483] train: loss: 0.0030037
[Epoch 2; Iter   127/ 2483] train: loss: 0.0026127
[Epoch 2; Iter   157/ 2483] train: loss: 0.0034830
[Epoch 2; Iter   187/ 2483] train: loss: 0.0026252
[Epoch 2; Iter   217/ 2483] train: loss: 0.0024778
[Epoch 2; Iter   247/ 2483] train: loss: 0.0029907
[Epoch 2; Iter   277/ 2483] train: loss: 0.0025275
[Epoch 2; Iter   307/ 2483] train: loss: 0.0023908
[Epoch 2; Iter   337/ 2483] train: loss: 0.0023715
[Epoch 2; Iter   367/ 2483] train: loss: 0.0992755
[Epoch 2; Iter   397/ 2483] train: loss: 0.0585186
[Epoch 2; Iter   427/ 2483] train: loss: 0.0026448
[Epoch 2; Iter   457/ 2483] train: loss: 0.0022599
[Epoch 2; Iter   487/ 2483] train: loss: 0.0779839
[Epoch 2; Iter   517/ 2483] train: loss: 0.0028119
[Epoch 2; Iter   547/ 2483] train: loss: 0.0026205
[Epoch 2; Iter   577/ 2483] train: loss: 0.0021535
[Epoch 2; Iter   607/ 2483] train: loss: 0.0028120
[Epoch 2; Iter   637/ 2483] train: loss: 0.0021834
[Epoch 2; Iter   667/ 2483] train: loss: 0.0019542
[Epoch 2; Iter   697/ 2483] train: loss: 0.0020030
[Epoch 2; Iter   727/ 2483] train: loss: 0.0019057
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/muv/noise=0.05/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.05_4_26-05_11-08-48
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.05
logdir: runs/static_noise/GraphCL/muv/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6934643
[Epoch 1; Iter    60/ 2483] train: loss: 0.6943303
[Epoch 1; Iter    90/ 2483] train: loss: 0.6917229
[Epoch 1; Iter   120/ 2483] train: loss: 0.6936611
[Epoch 1; Iter   150/ 2483] train: loss: 0.6917008
[Epoch 1; Iter   180/ 2483] train: loss: 0.6920148
[Epoch 1; Iter   210/ 2483] train: loss: 0.6921563
[Epoch 1; Iter   240/ 2483] train: loss: 0.6939762
[Epoch 1; Iter   270/ 2483] train: loss: 0.6920784
[Epoch 1; Iter   300/ 2483] train: loss: 0.6921603
[Epoch 1; Iter   330/ 2483] train: loss: 0.6918196
[Epoch 1; Iter   360/ 2483] train: loss: 0.6910400
[Epoch 1; Iter   390/ 2483] train: loss: 0.6922075
[Epoch 1; Iter   420/ 2483] train: loss: 0.6916388
[Epoch 1; Iter   450/ 2483] train: loss: 0.6908015
[Epoch 1; Iter   480/ 2483] train: loss: 0.6893110
[Epoch 1; Iter   510/ 2483] train: loss: 0.6885342
[Epoch 1; Iter   540/ 2483] train: loss: 0.6912161
[Epoch 1; Iter   570/ 2483] train: loss: 0.6903803
[Epoch 1; Iter   600/ 2483] train: loss: 0.6887656
[Epoch 1; Iter   630/ 2483] train: loss: 0.6901579
[Epoch 1; Iter   660/ 2483] train: loss: 0.6884928
[Epoch 1; Iter   690/ 2483] train: loss: 0.6874973
[Epoch 1; Iter   720/ 2483] train: loss: 0.6865985
[Epoch 1; Iter   750/ 2483] train: loss: 0.6731586
[Epoch 1; Iter   780/ 2483] train: loss: 0.6530510
[Epoch 1; Iter   810/ 2483] train: loss: 0.6391103
[Epoch 1; Iter   840/ 2483] train: loss: 0.5961628
[Epoch 1; Iter   870/ 2483] train: loss: 0.5508440
[Epoch 1; Iter   900/ 2483] train: loss: 0.4958491
[Epoch 1; Iter   930/ 2483] train: loss: 0.4321370
[Epoch 1; Iter   960/ 2483] train: loss: 0.3826194
[Epoch 1; Iter   990/ 2483] train: loss: 0.3292310
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2738155
[Epoch 1; Iter  1050/ 2483] train: loss: 0.1999037
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1560545
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1237153
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0985567
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0795105
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0619792
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0497490
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0397227
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0371750
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0304101
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0240803
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0194504
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0560146
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0174422
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0183004
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0172957
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0164484
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0156451
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0156059
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0140684
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0609594
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0908247
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0109155
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0099341
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0101650
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0084081
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0075841
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0067640
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0060457
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0066206
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0064318
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0054357
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0055506
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0050261
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0675932
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0041589
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0039081
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0038678
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0034992
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0037174
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0033704
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0040205
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0032961
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0034108
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0029748
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0029969
[Epoch 1; Iter  2430/ 2483] train: loss: 0.1179349
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0030380
[Epoch 1] ogbg-molmuv: 0.030841 val loss: 0.015286
[Epoch 1] ogbg-molmuv: 0.026827 test loss: 0.015756
[Epoch 2; Iter     7/ 2483] train: loss: 0.0028386
[Epoch 2; Iter    37/ 2483] train: loss: 0.0028359
[Epoch 2; Iter    67/ 2483] train: loss: 0.0029348
[Epoch 2; Iter    97/ 2483] train: loss: 0.0030786
[Epoch 2; Iter   127/ 2483] train: loss: 0.0031509
[Epoch 2; Iter   157/ 2483] train: loss: 0.0027280
[Epoch 2; Iter   187/ 2483] train: loss: 0.0932601
[Epoch 2; Iter   217/ 2483] train: loss: 0.0778347
[Epoch 2; Iter   247/ 2483] train: loss: 0.0026784
[Epoch 2; Iter   277/ 2483] train: loss: 0.0025856
[Epoch 2; Iter   307/ 2483] train: loss: 0.0025605
[Epoch 2; Iter   337/ 2483] train: loss: 0.1749823
[Epoch 2; Iter   367/ 2483] train: loss: 0.0026693
[Epoch 2; Iter   397/ 2483] train: loss: 0.0025091
[Epoch 2; Iter   427/ 2483] train: loss: 0.0025359
[Epoch 2; Iter   457/ 2483] train: loss: 0.0023629
[Epoch 2; Iter   487/ 2483] train: loss: 0.0023690
[Epoch 2; Iter   517/ 2483] train: loss: 0.0026782
[Epoch 2; Iter   547/ 2483] train: loss: 0.0021557
[Epoch 2; Iter   577/ 2483] train: loss: 0.0021380
[Epoch 2; Iter   607/ 2483] train: loss: 0.0021581
[Epoch 2; Iter   637/ 2483] train: loss: 0.0019778
[Epoch 2; Iter   667/ 2483] train: loss: 0.0022984
[Epoch 2; Iter   697/ 2483] train: loss: 0.0022591
[Epoch 2; Iter   727/ 2483] train: loss: 0.1505813
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/muv/noise=0.1/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.1_6_26-05_11-11-46
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.1
logdir: runs/static_noise/GraphCL/muv/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6959531
[Epoch 1; Iter    60/ 2483] train: loss: 0.6941413
[Epoch 1; Iter    90/ 2483] train: loss: 0.6950362
[Epoch 1; Iter   120/ 2483] train: loss: 0.6938450
[Epoch 1; Iter   150/ 2483] train: loss: 0.6947048
[Epoch 1; Iter   180/ 2483] train: loss: 0.6943817
[Epoch 1; Iter   210/ 2483] train: loss: 0.6928966
[Epoch 1; Iter   240/ 2483] train: loss: 0.6959485
[Epoch 1; Iter   270/ 2483] train: loss: 0.6945944
[Epoch 1; Iter   300/ 2483] train: loss: 0.6918968
[Epoch 1; Iter   330/ 2483] train: loss: 0.6918812
[Epoch 1; Iter   360/ 2483] train: loss: 0.6926601
[Epoch 1; Iter   390/ 2483] train: loss: 0.6934856
[Epoch 1; Iter   420/ 2483] train: loss: 0.6931142
[Epoch 1; Iter   450/ 2483] train: loss: 0.6923723
[Epoch 1; Iter   480/ 2483] train: loss: 0.6910869
[Epoch 1; Iter   510/ 2483] train: loss: 0.6915148
[Epoch 1; Iter   540/ 2483] train: loss: 0.6915511
[Epoch 1; Iter   570/ 2483] train: loss: 0.6910412
[Epoch 1; Iter   600/ 2483] train: loss: 0.6910467
[Epoch 1; Iter   630/ 2483] train: loss: 0.6923355
[Epoch 1; Iter   660/ 2483] train: loss: 0.6903184
[Epoch 1; Iter   690/ 2483] train: loss: 0.6916819
[Epoch 1; Iter   720/ 2483] train: loss: 0.6858267
[Epoch 1; Iter   750/ 2483] train: loss: 0.6788016
[Epoch 1; Iter   780/ 2483] train: loss: 0.6649582
[Epoch 1; Iter   810/ 2483] train: loss: 0.6322767
[Epoch 1; Iter   840/ 2483] train: loss: 0.5885341
[Epoch 1; Iter   870/ 2483] train: loss: 0.5780309
[Epoch 1; Iter   900/ 2483] train: loss: 0.5338880
[Epoch 1; Iter   930/ 2483] train: loss: 0.4676331
[Epoch 1; Iter   960/ 2483] train: loss: 0.3958394
[Epoch 1; Iter   990/ 2483] train: loss: 0.3415069
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2647521
[Epoch 1; Iter  1050/ 2483] train: loss: 0.2165621
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1610305
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1538850
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0958836
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0824457
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0622265
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0519688
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0425014
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0340281
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0282774
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0248356
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0199194
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0183188
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0629258
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0180593
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0620738
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0158867
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0168964
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0151757
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0150617
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0120533
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0113608
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0111546
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0137423
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0090889
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0082158
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0672637
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0076006
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0068395
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0064545
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0060372
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0631978
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0548424
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0052197
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0051270
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0045819
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0045444
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0043031
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0655875
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0038620
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0039928
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0683479
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0039234
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0037081
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0038279
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0035279
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0031532
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0786778
[Epoch 1] ogbg-molmuv: 0.007028 val loss: 0.120994
[Epoch 1] ogbg-molmuv: 0.008496 test loss: 0.130470
[Epoch 2; Iter     7/ 2483] train: loss: 0.0031034
[Epoch 2; Iter    37/ 2483] train: loss: 0.0026852
[Epoch 2; Iter    67/ 2483] train: loss: 0.0025759
[Epoch 2; Iter    97/ 2483] train: loss: 0.0024435
[Epoch 2; Iter   127/ 2483] train: loss: 0.0786715
[Epoch 2; Iter   157/ 2483] train: loss: 0.0024967
[Epoch 2; Iter   187/ 2483] train: loss: 0.0038854
[Epoch 2; Iter   217/ 2483] train: loss: 0.0039143
[Epoch 2; Iter   247/ 2483] train: loss: 0.0024392
[Epoch 2; Iter   277/ 2483] train: loss: 0.0023633
[Epoch 2; Iter   307/ 2483] train: loss: 0.0021123
[Epoch 2; Iter   337/ 2483] train: loss: 0.0024115
[Epoch 2; Iter   367/ 2483] train: loss: 0.0022010
[Epoch 2; Iter   397/ 2483] train: loss: 0.0020270
[Epoch 2; Iter   427/ 2483] train: loss: 0.0020223
[Epoch 2; Iter   457/ 2483] train: loss: 0.0020430
[Epoch 2; Iter   487/ 2483] train: loss: 0.0020155
[Epoch 2; Iter   517/ 2483] train: loss: 0.0020263
[Epoch 2; Iter   547/ 2483] train: loss: 0.0947094
[Epoch 2; Iter   577/ 2483] train: loss: 0.1324789
[Epoch 2; Iter   607/ 2483] train: loss: 0.0022965
[Epoch 2; Iter   637/ 2483] train: loss: 0.0600164
[Epoch 2; Iter   667/ 2483] train: loss: 0.0022462
[Epoch 2; Iter   697/ 2483] train: loss: 0.0023510
[Epoch 2; Iter   727/ 2483] train: loss: 0.0021123
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/muv/noise=0.1/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.1_4_26-05_11-12-04
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.1
logdir: runs/static_noise/GraphCL/muv/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6945444
[Epoch 1; Iter    60/ 2483] train: loss: 0.6948417
[Epoch 1; Iter    90/ 2483] train: loss: 0.6918565
[Epoch 1; Iter   120/ 2483] train: loss: 0.6931904
[Epoch 1; Iter   150/ 2483] train: loss: 0.6916504
[Epoch 1; Iter   180/ 2483] train: loss: 0.6924169
[Epoch 1; Iter   210/ 2483] train: loss: 0.6928375
[Epoch 1; Iter   240/ 2483] train: loss: 0.6917815
[Epoch 1; Iter   270/ 2483] train: loss: 0.6918682
[Epoch 1; Iter   300/ 2483] train: loss: 0.6909297
[Epoch 1; Iter   330/ 2483] train: loss: 0.6924868
[Epoch 1; Iter   360/ 2483] train: loss: 0.6918098
[Epoch 1; Iter   390/ 2483] train: loss: 0.6922424
[Epoch 1; Iter   420/ 2483] train: loss: 0.6920766
[Epoch 1; Iter   450/ 2483] train: loss: 0.6914043
[Epoch 1; Iter   480/ 2483] train: loss: 0.6896390
[Epoch 1; Iter   510/ 2483] train: loss: 0.6887160
[Epoch 1; Iter   540/ 2483] train: loss: 0.6926648
[Epoch 1; Iter   570/ 2483] train: loss: 0.6905513
[Epoch 1; Iter   600/ 2483] train: loss: 0.6889892
[Epoch 1; Iter   630/ 2483] train: loss: 0.6907110
[Epoch 1; Iter   660/ 2483] train: loss: 0.6880381
[Epoch 1; Iter   690/ 2483] train: loss: 0.6872941
[Epoch 1; Iter   720/ 2483] train: loss: 0.6862894
[Epoch 1; Iter   750/ 2483] train: loss: 0.6730954
[Epoch 1; Iter   780/ 2483] train: loss: 0.6538287
[Epoch 1; Iter   810/ 2483] train: loss: 0.6382246
[Epoch 1; Iter   840/ 2483] train: loss: 0.5914388
[Epoch 1; Iter   870/ 2483] train: loss: 0.5597805
[Epoch 1; Iter   900/ 2483] train: loss: 0.5166185
[Epoch 1; Iter   930/ 2483] train: loss: 0.4551333
[Epoch 1; Iter   960/ 2483] train: loss: 0.3704910
[Epoch 1; Iter   990/ 2483] train: loss: 0.3236417
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2717388
[Epoch 1; Iter  1050/ 2483] train: loss: 0.2007933
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1580886
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1213318
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0951551
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0830289
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0603439
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0494180
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0408410
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0356667
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0284502
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0234326
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0199235
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0575558
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0177657
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0180842
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0168176
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0169142
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0147792
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0154396
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0136437
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0578818
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0931172
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0110148
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0098786
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0096882
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0082984
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0074382
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0068675
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0062513
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0061101
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0062560
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0056073
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0058167
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0050431
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0677225
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0041547
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0037756
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0034982
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0034380
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0034144
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0033680
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0034125
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0030727
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0030567
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0028765
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0027453
[Epoch 1; Iter  2430/ 2483] train: loss: 0.1064899
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0031000
[Epoch 1] ogbg-molmuv: 0.019838 val loss: 0.020076
[Epoch 1] ogbg-molmuv: 0.018503 test loss: 0.021831
[Epoch 2; Iter     7/ 2483] train: loss: 0.0028293
[Epoch 2; Iter    37/ 2483] train: loss: 0.0026661
[Epoch 2; Iter    67/ 2483] train: loss: 0.0024865
[Epoch 2; Iter    97/ 2483] train: loss: 0.0028028
[Epoch 2; Iter   127/ 2483] train: loss: 0.0029590
[Epoch 2; Iter   157/ 2483] train: loss: 0.0026600
[Epoch 2; Iter   187/ 2483] train: loss: 0.0961577
[Epoch 2; Iter   217/ 2483] train: loss: 0.0836637
[Epoch 2; Iter   247/ 2483] train: loss: 0.0024500
[Epoch 2; Iter   277/ 2483] train: loss: 0.0023447
[Epoch 2; Iter   307/ 2483] train: loss: 0.0024426
[Epoch 2; Iter   337/ 2483] train: loss: 0.1802966
[Epoch 2; Iter   367/ 2483] train: loss: 0.0029717
[Epoch 2; Iter   397/ 2483] train: loss: 0.0028296
[Epoch 2; Iter   427/ 2483] train: loss: 0.0028388
[Epoch 2; Iter   457/ 2483] train: loss: 0.0024170
[Epoch 2; Iter   487/ 2483] train: loss: 0.0023494
[Epoch 2; Iter   517/ 2483] train: loss: 0.0025393
[Epoch 2; Iter   547/ 2483] train: loss: 0.0022177
[Epoch 2; Iter   577/ 2483] train: loss: 0.0021111
[Epoch 2; Iter   607/ 2483] train: loss: 0.0020601
[Epoch 2; Iter   637/ 2483] train: loss: 0.0026537
[Epoch 2; Iter   667/ 2483] train: loss: 0.0023564
[Epoch 2; Iter   697/ 2483] train: loss: 0.0019645
[Epoch 2; Iter   727/ 2483] train: loss: 0.1598657
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/muv/noise=0.1/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.1_5_26-05_11-11-52
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.1
logdir: runs/static_noise/GraphCL/muv/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6935723
[Epoch 1; Iter    60/ 2483] train: loss: 0.6938506
[Epoch 1; Iter    90/ 2483] train: loss: 0.6918349
[Epoch 1; Iter   120/ 2483] train: loss: 0.6931596
[Epoch 1; Iter   150/ 2483] train: loss: 0.6917281
[Epoch 1; Iter   180/ 2483] train: loss: 0.6921146
[Epoch 1; Iter   210/ 2483] train: loss: 0.6937364
[Epoch 1; Iter   240/ 2483] train: loss: 0.6899489
[Epoch 1; Iter   270/ 2483] train: loss: 0.6919005
[Epoch 1; Iter   300/ 2483] train: loss: 0.6930003
[Epoch 1; Iter   330/ 2483] train: loss: 0.6916736
[Epoch 1; Iter   360/ 2483] train: loss: 0.6912485
[Epoch 1; Iter   390/ 2483] train: loss: 0.6917288
[Epoch 1; Iter   420/ 2483] train: loss: 0.6931094
[Epoch 1; Iter   450/ 2483] train: loss: 0.6898804
[Epoch 1; Iter   480/ 2483] train: loss: 0.6924772
[Epoch 1; Iter   510/ 2483] train: loss: 0.6891356
[Epoch 1; Iter   540/ 2483] train: loss: 0.6903224
[Epoch 1; Iter   570/ 2483] train: loss: 0.6882953
[Epoch 1; Iter   600/ 2483] train: loss: 0.6879671
[Epoch 1; Iter   630/ 2483] train: loss: 0.6912355
[Epoch 1; Iter   660/ 2483] train: loss: 0.6866144
[Epoch 1; Iter   690/ 2483] train: loss: 0.6871775
[Epoch 1; Iter   720/ 2483] train: loss: 0.6865968
[Epoch 1; Iter   750/ 2483] train: loss: 0.6755481
[Epoch 1; Iter   780/ 2483] train: loss: 0.6603243
[Epoch 1; Iter   810/ 2483] train: loss: 0.6231405
[Epoch 1; Iter   840/ 2483] train: loss: 0.5932713
[Epoch 1; Iter   870/ 2483] train: loss: 0.5554776
[Epoch 1; Iter   900/ 2483] train: loss: 0.4946664
[Epoch 1; Iter   930/ 2483] train: loss: 0.4316622
[Epoch 1; Iter   960/ 2483] train: loss: 0.3727815
[Epoch 1; Iter   990/ 2483] train: loss: 0.2950896
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2373963
[Epoch 1; Iter  1050/ 2483] train: loss: 0.1798921
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1414894
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1828390
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0869895
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0667738
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0560689
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0439342
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0381070
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0300012
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0967172
[Epoch 1; Iter  1350/ 2483] train: loss: 0.1459712
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0705984
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0724680
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0673386
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0151770
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0166285
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0735617
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0142927
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0143768
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0119006
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0803768
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0116139
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0101498
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0096125
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0079896
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0683549
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0071587
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0065232
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0059419
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0059118
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0057529
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0052802
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0050833
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0045080
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0043378
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0041036
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0038881
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0037398
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0777351
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0041653
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0789329
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0034571
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0039978
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0043985
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0034098
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0044064
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0036400
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0032370
[Epoch 1] ogbg-molmuv: 0.015610 val loss: 0.016572
[Epoch 1] ogbg-molmuv: 0.033278 test loss: 0.016970
[Epoch 2; Iter     7/ 2483] train: loss: 0.0041604
[Epoch 2; Iter    37/ 2483] train: loss: 0.0032341
[Epoch 2; Iter    67/ 2483] train: loss: 0.0031884
[Epoch 2; Iter    97/ 2483] train: loss: 0.0027097
[Epoch 2; Iter   127/ 2483] train: loss: 0.0026979
[Epoch 2; Iter   157/ 2483] train: loss: 0.0033779
[Epoch 2; Iter   187/ 2483] train: loss: 0.0027288
[Epoch 2; Iter   217/ 2483] train: loss: 0.0025925
[Epoch 2; Iter   247/ 2483] train: loss: 0.0024642
[Epoch 2; Iter   277/ 2483] train: loss: 0.0025554
[Epoch 2; Iter   307/ 2483] train: loss: 0.0023210
[Epoch 2; Iter   337/ 2483] train: loss: 0.0023651
[Epoch 2; Iter   367/ 2483] train: loss: 0.1018671
[Epoch 2; Iter   397/ 2483] train: loss: 0.0608222
[Epoch 2; Iter   427/ 2483] train: loss: 0.0031805
[Epoch 2; Iter   457/ 2483] train: loss: 0.0023254
[Epoch 2; Iter   487/ 2483] train: loss: 0.0783927
[Epoch 2; Iter   517/ 2483] train: loss: 0.0028675
[Epoch 2; Iter   547/ 2483] train: loss: 0.0024240
[Epoch 2; Iter   577/ 2483] train: loss: 0.0020390
[Epoch 2; Iter   607/ 2483] train: loss: 0.0021110
[Epoch 2; Iter   637/ 2483] train: loss: 0.0025598
[Epoch 2; Iter   667/ 2483] train: loss: 0.0021513
[Epoch 2; Iter   697/ 2483] train: loss: 0.0021692
[Epoch 2; Iter   727/ 2483] train: loss: 0.0019175
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/muv/noise=0.2/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.2_4_26-05_11-13-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.2
logdir: runs/static_noise/GraphCL/muv/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6924779
[Epoch 1; Iter    60/ 2483] train: loss: 0.6938676
[Epoch 1; Iter    90/ 2483] train: loss: 0.6917236
[Epoch 1; Iter   120/ 2483] train: loss: 0.6935514
[Epoch 1; Iter   150/ 2483] train: loss: 0.6919244
[Epoch 1; Iter   180/ 2483] train: loss: 0.6923503
[Epoch 1; Iter   210/ 2483] train: loss: 0.6936477
[Epoch 1; Iter   240/ 2483] train: loss: 0.6924978
[Epoch 1; Iter   270/ 2483] train: loss: 0.6926090
[Epoch 1; Iter   300/ 2483] train: loss: 0.6912807
[Epoch 1; Iter   330/ 2483] train: loss: 0.6924214
[Epoch 1; Iter   360/ 2483] train: loss: 0.6927327
[Epoch 1; Iter   390/ 2483] train: loss: 0.6908441
[Epoch 1; Iter   420/ 2483] train: loss: 0.6912583
[Epoch 1; Iter   450/ 2483] train: loss: 0.6911748
[Epoch 1; Iter   480/ 2483] train: loss: 0.6905031
[Epoch 1; Iter   510/ 2483] train: loss: 0.6909887
[Epoch 1; Iter   540/ 2483] train: loss: 0.6900362
[Epoch 1; Iter   570/ 2483] train: loss: 0.6922743
[Epoch 1; Iter   600/ 2483] train: loss: 0.6890166
[Epoch 1; Iter   630/ 2483] train: loss: 0.6890796
[Epoch 1; Iter   660/ 2483] train: loss: 0.6874225
[Epoch 1; Iter   690/ 2483] train: loss: 0.6877212
[Epoch 1; Iter   720/ 2483] train: loss: 0.6870623
[Epoch 1; Iter   750/ 2483] train: loss: 0.6763794
[Epoch 1; Iter   780/ 2483] train: loss: 0.6614846
[Epoch 1; Iter   810/ 2483] train: loss: 0.6398728
[Epoch 1; Iter   840/ 2483] train: loss: 0.5963420
[Epoch 1; Iter   870/ 2483] train: loss: 0.5468430
[Epoch 1; Iter   900/ 2483] train: loss: 0.5216342
[Epoch 1; Iter   930/ 2483] train: loss: 0.4448184
[Epoch 1; Iter   960/ 2483] train: loss: 0.3796132
[Epoch 1; Iter   990/ 2483] train: loss: 0.3192938
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2548954
[Epoch 1; Iter  1050/ 2483] train: loss: 0.1966067
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1538827
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1176068
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0924141
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0787224
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0586643
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0471598
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0385104
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0335116
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0264030
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0240949
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0191674
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0585133
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0174468
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0175131
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0169499
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0157956
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0141089
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0145717
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0139485
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0620835
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0918478
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0104322
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0094429
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0089776
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0079466
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0072490
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0068447
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0062084
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0058485
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0061776
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0054051
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0052601
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0048212
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0681310
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0040501
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0040227
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0035992
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0036156
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0036581
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0032295
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0032364
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0034603
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0031547
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0030524
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0026471
[Epoch 1; Iter  2430/ 2483] train: loss: 0.1089523
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0029867
[Epoch 1] ogbg-molmuv: 0.006870 val loss: 0.372956
[Epoch 1] ogbg-molmuv: 0.006445 test loss: 0.383995
[Epoch 2; Iter     7/ 2483] train: loss: 0.0030210
[Epoch 2; Iter    37/ 2483] train: loss: 0.0028280
[Epoch 2; Iter    67/ 2483] train: loss: 0.0027467
[Epoch 2; Iter    97/ 2483] train: loss: 0.0027078
[Epoch 2; Iter   127/ 2483] train: loss: 0.0029784
[Epoch 2; Iter   157/ 2483] train: loss: 0.0027744
[Epoch 2; Iter   187/ 2483] train: loss: 0.0925474
[Epoch 2; Iter   217/ 2483] train: loss: 0.0715135
[Epoch 2; Iter   247/ 2483] train: loss: 0.0026742
[Epoch 2; Iter   277/ 2483] train: loss: 0.0026589
[Epoch 2; Iter   307/ 2483] train: loss: 0.0024205
[Epoch 2; Iter   337/ 2483] train: loss: 0.1842489
[Epoch 2; Iter   367/ 2483] train: loss: 0.0027894
[Epoch 2; Iter   397/ 2483] train: loss: 0.0026116
[Epoch 2; Iter   427/ 2483] train: loss: 0.0028358
[Epoch 2; Iter   457/ 2483] train: loss: 0.0022120
[Epoch 2; Iter   487/ 2483] train: loss: 0.0023145
[Epoch 2; Iter   517/ 2483] train: loss: 0.0023224
[Epoch 2; Iter   547/ 2483] train: loss: 0.0022342
[Epoch 2; Iter   577/ 2483] train: loss: 0.0021024
[Epoch 2; Iter   607/ 2483] train: loss: 0.0021156
[Epoch 2; Iter   637/ 2483] train: loss: 0.0021897
[Epoch 2; Iter   667/ 2483] train: loss: 0.0028810
[Epoch 2; Iter   697/ 2483] train: loss: 0.0018226
[Epoch 2; Iter   727/ 2483] train: loss: 0.1488783
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/muv/noise=0.2/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.2_5_26-05_11-13-56
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.2
logdir: runs/static_noise/GraphCL/muv/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6926410
[Epoch 1; Iter    60/ 2483] train: loss: 0.6947161
[Epoch 1; Iter    90/ 2483] train: loss: 0.6908606
[Epoch 1; Iter   120/ 2483] train: loss: 0.6935812
[Epoch 1; Iter   150/ 2483] train: loss: 0.6934065
[Epoch 1; Iter   180/ 2483] train: loss: 0.6925998
[Epoch 1; Iter   210/ 2483] train: loss: 0.6941020
[Epoch 1; Iter   240/ 2483] train: loss: 0.6909792
[Epoch 1; Iter   270/ 2483] train: loss: 0.6933172
[Epoch 1; Iter   300/ 2483] train: loss: 0.6928948
[Epoch 1; Iter   330/ 2483] train: loss: 0.6906520
[Epoch 1; Iter   360/ 2483] train: loss: 0.6911869
[Epoch 1; Iter   390/ 2483] train: loss: 0.6907229
[Epoch 1; Iter   420/ 2483] train: loss: 0.6926704
[Epoch 1; Iter   450/ 2483] train: loss: 0.6906918
[Epoch 1; Iter   480/ 2483] train: loss: 0.6924614
[Epoch 1; Iter   510/ 2483] train: loss: 0.6897860
[Epoch 1; Iter   540/ 2483] train: loss: 0.6902241
[Epoch 1; Iter   570/ 2483] train: loss: 0.6896520
[Epoch 1; Iter   600/ 2483] train: loss: 0.6891179
[Epoch 1; Iter   630/ 2483] train: loss: 0.6897084
[Epoch 1; Iter   660/ 2483] train: loss: 0.6886482
[Epoch 1; Iter   690/ 2483] train: loss: 0.6875101
[Epoch 1; Iter   720/ 2483] train: loss: 0.6856861
[Epoch 1; Iter   750/ 2483] train: loss: 0.6784818
[Epoch 1; Iter   780/ 2483] train: loss: 0.6598438
[Epoch 1; Iter   810/ 2483] train: loss: 0.6303198
[Epoch 1; Iter   840/ 2483] train: loss: 0.5971630
[Epoch 1; Iter   870/ 2483] train: loss: 0.5532083
[Epoch 1; Iter   900/ 2483] train: loss: 0.4986300
[Epoch 1; Iter   930/ 2483] train: loss: 0.4400907
[Epoch 1; Iter   960/ 2483] train: loss: 0.3602238
[Epoch 1; Iter   990/ 2483] train: loss: 0.3011561
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2262841
[Epoch 1; Iter  1050/ 2483] train: loss: 0.1750499
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1388086
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1839184
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0855585
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0657082
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0542788
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0425957
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0373884
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0296434
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0916665
[Epoch 1; Iter  1350/ 2483] train: loss: 0.1419482
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0693519
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0702077
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0603018
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0160982
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0152976
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0788082
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0145572
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0138685
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0127293
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0908404
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0105163
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0104075
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0105762
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0080433
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0690496
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0076179
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0064272
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0070038
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0057352
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0064241
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0051933
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0046024
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0043591
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0042067
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0038429
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0037264
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0039532
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0752136
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0037532
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0787097
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0036020
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0032016
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0033183
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0030861
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0041236
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0030893
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0030683
[Epoch 1] ogbg-molmuv: 0.016707 val loss: 0.017008
[Epoch 1] ogbg-molmuv: 0.031128 test loss: 0.019662
[Epoch 2; Iter     7/ 2483] train: loss: 0.0034494
[Epoch 2; Iter    37/ 2483] train: loss: 0.0038948
[Epoch 2; Iter    67/ 2483] train: loss: 0.0034900
[Epoch 2; Iter    97/ 2483] train: loss: 0.0030059
[Epoch 2; Iter   127/ 2483] train: loss: 0.0028447
[Epoch 2; Iter   157/ 2483] train: loss: 0.0034871
[Epoch 2; Iter   187/ 2483] train: loss: 0.0026217
[Epoch 2; Iter   217/ 2483] train: loss: 0.0025647
[Epoch 2; Iter   247/ 2483] train: loss: 0.0022671
[Epoch 2; Iter   277/ 2483] train: loss: 0.0032126
[Epoch 2; Iter   307/ 2483] train: loss: 0.0024665
[Epoch 2; Iter   337/ 2483] train: loss: 0.0024451
[Epoch 2; Iter   367/ 2483] train: loss: 0.0968619
[Epoch 2; Iter   397/ 2483] train: loss: 0.0525279
[Epoch 2; Iter   427/ 2483] train: loss: 0.0030820
[Epoch 2; Iter   457/ 2483] train: loss: 0.0021372
[Epoch 2; Iter   487/ 2483] train: loss: 0.0834324
[Epoch 2; Iter   517/ 2483] train: loss: 0.0023436
[Epoch 2; Iter   547/ 2483] train: loss: 0.0024402
[Epoch 2; Iter   577/ 2483] train: loss: 0.0021988
[Epoch 2; Iter   607/ 2483] train: loss: 0.0033456
[Epoch 2; Iter   637/ 2483] train: loss: 0.0024699
[Epoch 2; Iter   667/ 2483] train: loss: 0.0021328
[Epoch 2; Iter   697/ 2483] train: loss: 0.0021456
[Epoch 2; Iter   727/ 2483] train: loss: 0.0020610
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/muv/noise=0.2/PNA_ogbg-molmuv_GraphCL_muv_static_noise=0.2_6_26-05_11-14-13
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/muv/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_muv_static_noise=0.2
logdir: runs/static_noise/GraphCL/muv/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molmuv
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molmuv
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 17
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/ 2483] train: loss: 0.6943272
[Epoch 1; Iter    60/ 2483] train: loss: 0.6950572
[Epoch 1; Iter    90/ 2483] train: loss: 0.6947491
[Epoch 1; Iter   120/ 2483] train: loss: 0.6934963
[Epoch 1; Iter   150/ 2483] train: loss: 0.6934180
[Epoch 1; Iter   180/ 2483] train: loss: 0.6933531
[Epoch 1; Iter   210/ 2483] train: loss: 0.6943108
[Epoch 1; Iter   240/ 2483] train: loss: 0.6946415
[Epoch 1; Iter   270/ 2483] train: loss: 0.6930599
[Epoch 1; Iter   300/ 2483] train: loss: 0.6931487
[Epoch 1; Iter   330/ 2483] train: loss: 0.6902870
[Epoch 1; Iter   360/ 2483] train: loss: 0.6926546
[Epoch 1; Iter   390/ 2483] train: loss: 0.6933464
[Epoch 1; Iter   420/ 2483] train: loss: 0.6931939
[Epoch 1; Iter   450/ 2483] train: loss: 0.6919980
[Epoch 1; Iter   480/ 2483] train: loss: 0.6897900
[Epoch 1; Iter   510/ 2483] train: loss: 0.6917667
[Epoch 1; Iter   540/ 2483] train: loss: 0.6906295
[Epoch 1; Iter   570/ 2483] train: loss: 0.6909996
[Epoch 1; Iter   600/ 2483] train: loss: 0.6893128
[Epoch 1; Iter   630/ 2483] train: loss: 0.6911654
[Epoch 1; Iter   660/ 2483] train: loss: 0.6881029
[Epoch 1; Iter   690/ 2483] train: loss: 0.6912094
[Epoch 1; Iter   720/ 2483] train: loss: 0.6876571
[Epoch 1; Iter   750/ 2483] train: loss: 0.6794496
[Epoch 1; Iter   780/ 2483] train: loss: 0.6680197
[Epoch 1; Iter   810/ 2483] train: loss: 0.6397232
[Epoch 1; Iter   840/ 2483] train: loss: 0.5837389
[Epoch 1; Iter   870/ 2483] train: loss: 0.5667612
[Epoch 1; Iter   900/ 2483] train: loss: 0.5411506
[Epoch 1; Iter   930/ 2483] train: loss: 0.4575905
[Epoch 1; Iter   960/ 2483] train: loss: 0.3863235
[Epoch 1; Iter   990/ 2483] train: loss: 0.3471256
[Epoch 1; Iter  1020/ 2483] train: loss: 0.2650437
[Epoch 1; Iter  1050/ 2483] train: loss: 0.2175422
[Epoch 1; Iter  1080/ 2483] train: loss: 0.1717634
[Epoch 1; Iter  1110/ 2483] train: loss: 0.1614917
[Epoch 1; Iter  1140/ 2483] train: loss: 0.0966702
[Epoch 1; Iter  1170/ 2483] train: loss: 0.0803042
[Epoch 1; Iter  1200/ 2483] train: loss: 0.0628650
[Epoch 1; Iter  1230/ 2483] train: loss: 0.0546708
[Epoch 1; Iter  1260/ 2483] train: loss: 0.0432225
[Epoch 1; Iter  1290/ 2483] train: loss: 0.0340289
[Epoch 1; Iter  1320/ 2483] train: loss: 0.0286157
[Epoch 1; Iter  1350/ 2483] train: loss: 0.0250160
[Epoch 1; Iter  1380/ 2483] train: loss: 0.0207159
[Epoch 1; Iter  1410/ 2483] train: loss: 0.0186356
[Epoch 1; Iter  1440/ 2483] train: loss: 0.0612170
[Epoch 1; Iter  1470/ 2483] train: loss: 0.0177980
[Epoch 1; Iter  1500/ 2483] train: loss: 0.0644436
[Epoch 1; Iter  1530/ 2483] train: loss: 0.0164195
[Epoch 1; Iter  1560/ 2483] train: loss: 0.0171796
[Epoch 1; Iter  1590/ 2483] train: loss: 0.0144416
[Epoch 1; Iter  1620/ 2483] train: loss: 0.0150923
[Epoch 1; Iter  1650/ 2483] train: loss: 0.0121700
[Epoch 1; Iter  1680/ 2483] train: loss: 0.0114502
[Epoch 1; Iter  1710/ 2483] train: loss: 0.0104563
[Epoch 1; Iter  1740/ 2483] train: loss: 0.0116617
[Epoch 1; Iter  1770/ 2483] train: loss: 0.0094007
[Epoch 1; Iter  1800/ 2483] train: loss: 0.0087440
[Epoch 1; Iter  1830/ 2483] train: loss: 0.0686773
[Epoch 1; Iter  1860/ 2483] train: loss: 0.0076443
[Epoch 1; Iter  1890/ 2483] train: loss: 0.0071675
[Epoch 1; Iter  1920/ 2483] train: loss: 0.0064029
[Epoch 1; Iter  1950/ 2483] train: loss: 0.0058287
[Epoch 1; Iter  1980/ 2483] train: loss: 0.0638781
[Epoch 1; Iter  2010/ 2483] train: loss: 0.0569812
[Epoch 1; Iter  2040/ 2483] train: loss: 0.0052044
[Epoch 1; Iter  2070/ 2483] train: loss: 0.0053499
[Epoch 1; Iter  2100/ 2483] train: loss: 0.0047223
[Epoch 1; Iter  2130/ 2483] train: loss: 0.0043060
[Epoch 1; Iter  2160/ 2483] train: loss: 0.0046869
[Epoch 1; Iter  2190/ 2483] train: loss: 0.0742321
[Epoch 1; Iter  2220/ 2483] train: loss: 0.0038472
[Epoch 1; Iter  2250/ 2483] train: loss: 0.0039350
[Epoch 1; Iter  2280/ 2483] train: loss: 0.0709335
[Epoch 1; Iter  2310/ 2483] train: loss: 0.0036083
[Epoch 1; Iter  2340/ 2483] train: loss: 0.0039183
[Epoch 1; Iter  2370/ 2483] train: loss: 0.0033527
[Epoch 1; Iter  2400/ 2483] train: loss: 0.0033354
[Epoch 1; Iter  2430/ 2483] train: loss: 0.0029685
[Epoch 1; Iter  2460/ 2483] train: loss: 0.0809766
[Epoch 1] ogbg-molmuv: 0.003657 val loss: 0.028912
[Epoch 1] ogbg-molmuv: 0.010172 test loss: 0.028474
[Epoch 2; Iter     7/ 2483] train: loss: 0.0030330
[Epoch 2; Iter    37/ 2483] train: loss: 0.0029088
[Epoch 2; Iter    67/ 2483] train: loss: 0.0025884
[Epoch 2; Iter    97/ 2483] train: loss: 0.0025152
[Epoch 2; Iter   127/ 2483] train: loss: 0.0741992
[Epoch 2; Iter   157/ 2483] train: loss: 0.0026866
[Epoch 2; Iter   187/ 2483] train: loss: 0.0032004
[Epoch 2; Iter   217/ 2483] train: loss: 0.0035657
[Epoch 2; Iter   247/ 2483] train: loss: 0.0026744
[Epoch 2; Iter   277/ 2483] train: loss: 0.0025256
[Epoch 2; Iter   307/ 2483] train: loss: 0.0023686
[Epoch 2; Iter   337/ 2483] train: loss: 0.0028508
[Epoch 2; Iter   367/ 2483] train: loss: 0.0022235
[Epoch 2; Iter   397/ 2483] train: loss: 0.0022965
[Epoch 2; Iter   427/ 2483] train: loss: 0.0022207
[Epoch 2; Iter   457/ 2483] train: loss: 0.0020227
[Epoch 2; Iter   487/ 2483] train: loss: 0.0020826
[Epoch 2; Iter   517/ 2483] train: loss: 0.0024860
[Epoch 2; Iter   547/ 2483] train: loss: 0.0845579
[Epoch 2; Iter   577/ 2483] train: loss: 0.1378383
[Epoch 2; Iter   607/ 2483] train: loss: 0.0026738
[Epoch 2; Iter   637/ 2483] train: loss: 0.0615410
[Epoch 2; Iter   667/ 2483] train: loss: 0.0023263
[Epoch 2; Iter   697/ 2483] train: loss: 0.0022705
[Epoch 2; Iter   727/ 2483] train: loss: 0.0020533
[Epoch 4; Iter   471/ 2483] train: loss: 0.0014684
[Epoch 4; Iter   501/ 2483] train: loss: 0.0014594
[Epoch 4; Iter   531/ 2483] train: loss: 0.0017386
[Epoch 4; Iter   561/ 2483] train: loss: 0.1252297
[Epoch 4; Iter   591/ 2483] train: loss: 0.0021035
[Epoch 4; Iter   621/ 2483] train: loss: 0.0018283
[Epoch 4; Iter   651/ 2483] train: loss: 0.0016460
[Epoch 4; Iter   681/ 2483] train: loss: 0.0823662
[Epoch 4; Iter   711/ 2483] train: loss: 0.0552814
[Epoch 4; Iter   741/ 2483] train: loss: 0.0020049
[Epoch 4; Iter   771/ 2483] train: loss: 0.0031949
[Epoch 4; Iter   801/ 2483] train: loss: 0.0019832
[Epoch 4; Iter   831/ 2483] train: loss: 0.0023657
[Epoch 4; Iter   861/ 2483] train: loss: 0.0016324
[Epoch 4; Iter   891/ 2483] train: loss: 0.0017076
[Epoch 4; Iter   921/ 2483] train: loss: 0.0907570
[Epoch 4; Iter   951/ 2483] train: loss: 0.0020636
[Epoch 4; Iter   981/ 2483] train: loss: 0.0738621
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0021478
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0017456
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0020204
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0017119
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0019954
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0020819
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0863967
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0014060
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0016247
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0013263
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0016780
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0013499
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0010926
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0693720
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0017257
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0019082
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0013767
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0017254
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0015190
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0014125
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0016100
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0015836
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0015461
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0736622
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0957097
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0018601
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0016714
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0027641
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0022813
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0025102
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0499461
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0022171
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0020567
[Epoch 4; Iter  2001/ 2483] train: loss: 0.1299245
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0016833
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0018181
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0021498
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0016202
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0781631
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0017238
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0020104
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0842134
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0653351
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0037319
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0023113
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0023331
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0652936
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0021186
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0020395
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0021413
[Epoch 4] ogbg-molmuv: 0.059566 val loss: 0.017575
[Epoch 4] ogbg-molmuv: 0.040896 test loss: 0.017663
[Epoch 5; Iter    28/ 2483] train: loss: 0.0013915
[Epoch 5; Iter    58/ 2483] train: loss: 0.0015418
[Epoch 5; Iter    88/ 2483] train: loss: 0.0017737
[Epoch 5; Iter   118/ 2483] train: loss: 0.0015594
[Epoch 5; Iter   148/ 2483] train: loss: 0.0019198
[Epoch 5; Iter   178/ 2483] train: loss: 0.0015445
[Epoch 5; Iter   208/ 2483] train: loss: 0.0017636
[Epoch 5; Iter   238/ 2483] train: loss: 0.0016800
[Epoch 5; Iter   268/ 2483] train: loss: 0.0018673
[Epoch 5; Iter   298/ 2483] train: loss: 0.0016365
[Epoch 5; Iter   328/ 2483] train: loss: 0.0014396
[Epoch 5; Iter   358/ 2483] train: loss: 0.0019290
[Epoch 5; Iter   388/ 2483] train: loss: 0.0017732
[Epoch 5; Iter   418/ 2483] train: loss: 0.0028068
[Epoch 5; Iter   448/ 2483] train: loss: 0.2207988
[Epoch 5; Iter   478/ 2483] train: loss: 0.0026898
[Epoch 5; Iter   508/ 2483] train: loss: 0.0022067
[Epoch 5; Iter   538/ 2483] train: loss: 0.0022546
[Epoch 5; Iter   568/ 2483] train: loss: 0.0019342
[Epoch 5; Iter   598/ 2483] train: loss: 0.0017349
[Epoch 5; Iter   628/ 2483] train: loss: 0.0017170
[Epoch 5; Iter   658/ 2483] train: loss: 0.0018490
[Epoch 5; Iter   688/ 2483] train: loss: 0.0018352
[Epoch 5; Iter   718/ 2483] train: loss: 0.0018056
[Epoch 5; Iter   748/ 2483] train: loss: 0.0021303
[Epoch 5; Iter   778/ 2483] train: loss: 0.0016108
[Epoch 5; Iter   808/ 2483] train: loss: 0.0015530
[Epoch 5; Iter   838/ 2483] train: loss: 0.0015977
[Epoch 5; Iter   868/ 2483] train: loss: 0.0022938
[Epoch 5; Iter   898/ 2483] train: loss: 0.0014951
[Epoch 5; Iter   928/ 2483] train: loss: 0.0907499
[Epoch 5; Iter   958/ 2483] train: loss: 0.0018563
[Epoch 5; Iter   988/ 2483] train: loss: 0.0016851
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0010084
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0015034
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0011307
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0011877
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0013534
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0708983
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0018841
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0015830
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0019239
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0019353
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0020142
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0020488
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0018918
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0019620
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0021312
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0017791
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0016483
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0015050
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0013361
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0017571
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0020368
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0015640
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0021053
[Epoch 5; Iter  1708/ 2483] train: loss: 0.2270027
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0025064
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0026429
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0017635
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0017268
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0018720
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0022477
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0026195
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0022497
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0024775
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0022324
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0016143
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0025913
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0020723
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0021274
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0020117
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0018235
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0667744
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0019407
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0026000
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0021162
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0018126
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0026951
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0019983
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0014639
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0018856
[Epoch 5] ogbg-molmuv: 0.010738 val loss: 0.015118
[Epoch 5] ogbg-molmuv: 0.048550 test loss: 0.015453
[Epoch 6; Iter     5/ 2483] train: loss: 0.0018835
[Epoch 6; Iter    35/ 2483] train: loss: 0.0023273
[Epoch 6; Iter    65/ 2483] train: loss: 0.0721382
[Epoch 6; Iter    95/ 2483] train: loss: 0.0020580
[Epoch 6; Iter   125/ 2483] train: loss: 0.0018260
[Epoch 6; Iter   155/ 2483] train: loss: 0.0014273
[Epoch 4; Iter   471/ 2483] train: loss: 0.0024988
[Epoch 4; Iter   501/ 2483] train: loss: 0.0013243
[Epoch 4; Iter   531/ 2483] train: loss: 0.0014152
[Epoch 4; Iter   561/ 2483] train: loss: 0.0015950
[Epoch 4; Iter   591/ 2483] train: loss: 0.0019134
[Epoch 4; Iter   621/ 2483] train: loss: 0.0014589
[Epoch 4; Iter   651/ 2483] train: loss: 0.0012857
[Epoch 4; Iter   681/ 2483] train: loss: 0.0015146
[Epoch 4; Iter   711/ 2483] train: loss: 0.0014587
[Epoch 4; Iter   741/ 2483] train: loss: 0.0030012
[Epoch 4; Iter   771/ 2483] train: loss: 0.0013766
[Epoch 4; Iter   801/ 2483] train: loss: 0.0015273
[Epoch 4; Iter   831/ 2483] train: loss: 0.0011991
[Epoch 4; Iter   861/ 2483] train: loss: 0.0709241
[Epoch 4; Iter   891/ 2483] train: loss: 0.0012073
[Epoch 4; Iter   921/ 2483] train: loss: 0.0014203
[Epoch 4; Iter   951/ 2483] train: loss: 0.0018201
[Epoch 4; Iter   981/ 2483] train: loss: 0.0026462
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0022823
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0015542
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0021167
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0020436
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0022679
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0024388
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0025799
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0016530
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0031295
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0023493
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0019807
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0021094
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0016644
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0012761
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0014123
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0013273
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0013655
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0018159
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0013517
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0016290
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0829916
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0019493
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0020954
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0016047
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0017472
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0014291
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0016462
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0543741
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0019909
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0016795
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0019313
[Epoch 4; Iter  1941/ 2483] train: loss: 0.1095326
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0018423
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0015528
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0015944
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0027353
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0834189
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0681647
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0022854
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0023359
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0022038
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0026674
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0019027
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0015944
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0015280
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0014887
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0013695
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0017810
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0018068
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0024523
[Epoch 4] ogbg-molmuv: 0.018783 val loss: 0.016844
[Epoch 4] ogbg-molmuv: 0.019529 test loss: 0.016949
[Epoch 5; Iter    28/ 2483] train: loss: 0.0020808
[Epoch 5; Iter    58/ 2483] train: loss: 0.0019844
[Epoch 5; Iter    88/ 2483] train: loss: 0.0759604
[Epoch 5; Iter   118/ 2483] train: loss: 0.0885968
[Epoch 5; Iter   148/ 2483] train: loss: 0.0027766
[Epoch 5; Iter   178/ 2483] train: loss: 0.0020341
[Epoch 5; Iter   208/ 2483] train: loss: 0.0014929
[Epoch 5; Iter   238/ 2483] train: loss: 0.0017391
[Epoch 5; Iter   268/ 2483] train: loss: 0.0019844
[Epoch 5; Iter   298/ 2483] train: loss: 0.0016971
[Epoch 5; Iter   328/ 2483] train: loss: 0.0025771
[Epoch 5; Iter   358/ 2483] train: loss: 0.0020032
[Epoch 5; Iter   388/ 2483] train: loss: 0.0922578
[Epoch 5; Iter   418/ 2483] train: loss: 0.0021641
[Epoch 5; Iter   448/ 2483] train: loss: 0.0024767
[Epoch 5; Iter   478/ 2483] train: loss: 0.0018058
[Epoch 5; Iter   508/ 2483] train: loss: 0.0018472
[Epoch 5; Iter   538/ 2483] train: loss: 0.0016285
[Epoch 5; Iter   568/ 2483] train: loss: 0.1000949
[Epoch 5; Iter   598/ 2483] train: loss: 0.0016957
[Epoch 5; Iter   628/ 2483] train: loss: 0.0779601
[Epoch 5; Iter   658/ 2483] train: loss: 0.0020211
[Epoch 5; Iter   688/ 2483] train: loss: 0.0025822
[Epoch 5; Iter   718/ 2483] train: loss: 0.0020017
[Epoch 5; Iter   748/ 2483] train: loss: 0.0018863
[Epoch 5; Iter   778/ 2483] train: loss: 0.0017231
[Epoch 5; Iter   808/ 2483] train: loss: 0.0014995
[Epoch 5; Iter   838/ 2483] train: loss: 0.0012752
[Epoch 5; Iter   868/ 2483] train: loss: 0.0017485
[Epoch 5; Iter   898/ 2483] train: loss: 0.0021424
[Epoch 5; Iter   928/ 2483] train: loss: 0.0359050
[Epoch 5; Iter   958/ 2483] train: loss: 0.0018755
[Epoch 5; Iter   988/ 2483] train: loss: 0.0019589
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0024158
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0015612
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0014978
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0013766
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0014400
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0016853
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0015463
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0016350
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0019180
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0013378
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0013049
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0012151
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0658108
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0014811
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0012875
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0013468
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0735430
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0013696
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0024339
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0014508
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0029360
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0015738
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0015688
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0011502
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0014567
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0014833
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0020962
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0044502
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0706114
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0019057
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0669115
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0017679
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0015117
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0024589
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0031583
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0040928
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0018325
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0790507
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0021987
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0022503
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0015388
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0014495
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0014734
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0017307
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0016423
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0019693
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0018215
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0017863
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0015335
[Epoch 5] ogbg-molmuv: 0.029247 val loss: 0.014884
[Epoch 5] ogbg-molmuv: 0.039822 test loss: 0.015874
[Epoch 6; Iter     5/ 2483] train: loss: 0.0018365
[Epoch 6; Iter    35/ 2483] train: loss: 0.0473127
[Epoch 6; Iter    65/ 2483] train: loss: 0.0026158
[Epoch 6; Iter    95/ 2483] train: loss: 0.0016694
[Epoch 6; Iter   125/ 2483] train: loss: 0.0020205
[Epoch 6; Iter   155/ 2483] train: loss: 0.0018579
[Epoch 4; Iter   471/ 2483] train: loss: 0.0014313
[Epoch 4; Iter   501/ 2483] train: loss: 0.0014078
[Epoch 4; Iter   531/ 2483] train: loss: 0.0014216
[Epoch 4; Iter   561/ 2483] train: loss: 0.0015581
[Epoch 4; Iter   591/ 2483] train: loss: 0.0012486
[Epoch 4; Iter   621/ 2483] train: loss: 0.0018654
[Epoch 4; Iter   651/ 2483] train: loss: 0.1881925
[Epoch 4; Iter   681/ 2483] train: loss: 0.0804549
[Epoch 4; Iter   711/ 2483] train: loss: 0.0022060
[Epoch 4; Iter   741/ 2483] train: loss: 0.0019727
[Epoch 4; Iter   771/ 2483] train: loss: 0.0017813
[Epoch 4; Iter   801/ 2483] train: loss: 0.0578848
[Epoch 4; Iter   831/ 2483] train: loss: 0.0017710
[Epoch 4; Iter   861/ 2483] train: loss: 0.0017579
[Epoch 4; Iter   891/ 2483] train: loss: 0.0022095
[Epoch 4; Iter   921/ 2483] train: loss: 0.0018237
[Epoch 4; Iter   951/ 2483] train: loss: 0.0019454
[Epoch 4; Iter   981/ 2483] train: loss: 0.0019896
[Epoch 4; Iter  1011/ 2483] train: loss: 0.1213021
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0023865
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0026054
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0019475
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0016371
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0015345
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0024047
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0019338
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0757505
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0021402
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0015491
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0018009
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0017354
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0762165
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0018229
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0019512
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0015554
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0019392
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0021341
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0023078
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0856617
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0016645
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0017047
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0023306
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0022733
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0698464
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0017833
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0020000
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0021355
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0018909
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0018039
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0019527
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0022342
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0023254
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0017587
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0017882
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0014313
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0025465
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0014693
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0019883
[Epoch 4; Iter  2211/ 2483] train: loss: 0.1519997
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0834249
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0019972
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0016506
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0875751
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0014304
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0024196
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0023106
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0019289
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0018178
[Epoch 4] ogbg-molmuv: 0.010506 val loss: 0.015573
[Epoch 4] ogbg-molmuv: 0.013737 test loss: 0.016647
[Epoch 5; Iter    28/ 2483] train: loss: 0.0014886
[Epoch 5; Iter    58/ 2483] train: loss: 0.0018033
[Epoch 5; Iter    88/ 2483] train: loss: 0.0019400
[Epoch 5; Iter   118/ 2483] train: loss: 0.0015046
[Epoch 5; Iter   148/ 2483] train: loss: 0.0016554
[Epoch 5; Iter   178/ 2483] train: loss: 0.0015541
[Epoch 5; Iter   208/ 2483] train: loss: 0.0014235
[Epoch 5; Iter   238/ 2483] train: loss: 0.0019887
[Epoch 5; Iter   268/ 2483] train: loss: 0.0015867
[Epoch 5; Iter   298/ 2483] train: loss: 0.0017820
[Epoch 5; Iter   328/ 2483] train: loss: 0.0019601
[Epoch 5; Iter   358/ 2483] train: loss: 0.0020111
[Epoch 5; Iter   388/ 2483] train: loss: 0.0021401
[Epoch 5; Iter   418/ 2483] train: loss: 0.0016239
[Epoch 5; Iter   448/ 2483] train: loss: 0.0020613
[Epoch 5; Iter   478/ 2483] train: loss: 0.0021646
[Epoch 5; Iter   508/ 2483] train: loss: 0.0022919
[Epoch 5; Iter   538/ 2483] train: loss: 0.0026103
[Epoch 5; Iter   568/ 2483] train: loss: 0.0799989
[Epoch 5; Iter   598/ 2483] train: loss: 0.0765671
[Epoch 5; Iter   628/ 2483] train: loss: 0.0033621
[Epoch 5; Iter   658/ 2483] train: loss: 0.0019046
[Epoch 5; Iter   688/ 2483] train: loss: 0.0015214
[Epoch 5; Iter   718/ 2483] train: loss: 0.0016674
[Epoch 5; Iter   748/ 2483] train: loss: 0.0015910
[Epoch 5; Iter   778/ 2483] train: loss: 0.0020542
[Epoch 5; Iter   808/ 2483] train: loss: 0.0020199
[Epoch 5; Iter   838/ 2483] train: loss: 0.0015558
[Epoch 5; Iter   868/ 2483] train: loss: 0.0013332
[Epoch 5; Iter   898/ 2483] train: loss: 0.0023190
[Epoch 5; Iter   928/ 2483] train: loss: 0.0015776
[Epoch 5; Iter   958/ 2483] train: loss: 0.0020238
[Epoch 5; Iter   988/ 2483] train: loss: 0.0018254
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0013716
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0012977
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0015297
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0013714
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0014086
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0014573
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0015398
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0011357
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0014633
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0017741
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0017872
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0016055
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0024077
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0027223
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0907293
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0023364
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0021439
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0025538
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0018817
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0018103
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0017321
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0751721
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0016709
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0026891
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0024965
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0587135
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0019469
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0017098
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0740592
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0016466
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0767334
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0019431
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0019844
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0014382
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0014813
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0618338
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0014716
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0014314
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0016263
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0019601
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0017473
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0019757
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0019043
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0017287
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0015760
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0016817
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0019604
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0017997
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0828675
[Epoch 5] ogbg-molmuv: 0.041607 val loss: 0.014933
[Epoch 5] ogbg-molmuv: 0.018266 test loss: 0.015800
[Epoch 6; Iter     5/ 2483] train: loss: 0.0025786
[Epoch 6; Iter    35/ 2483] train: loss: 0.0018352
[Epoch 6; Iter    65/ 2483] train: loss: 0.0022172
[Epoch 6; Iter    95/ 2483] train: loss: 0.0957852
[Epoch 6; Iter   125/ 2483] train: loss: 0.0018606
[Epoch 6; Iter   155/ 2483] train: loss: 0.0017419
[Epoch 2; Iter   757/ 2483] train: loss: 0.0023650
[Epoch 2; Iter   787/ 2483] train: loss: 0.0026480
[Epoch 2; Iter   817/ 2483] train: loss: 0.0020083
[Epoch 2; Iter   847/ 2483] train: loss: 0.0028637
[Epoch 2; Iter   877/ 2483] train: loss: 0.0839085
[Epoch 2; Iter   907/ 2483] train: loss: 0.0797682
[Epoch 2; Iter   937/ 2483] train: loss: 0.0027643
[Epoch 2; Iter   967/ 2483] train: loss: 0.0023660
[Epoch 2; Iter   997/ 2483] train: loss: 0.0027668
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0033374
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0021562
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0020257
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0026779
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0030221
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0020367
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0023798
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0020061
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0856684
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0016926
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0021037
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0017416
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0017046
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0017141
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0017803
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0046007
[Epoch 2; Iter  1507/ 2483] train: loss: 0.1574109
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0017939
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0022662
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0018459
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0016396
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0019269
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0024638
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0017371
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0019125
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0023184
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0016735
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0017763
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0014398
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0016691
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0015046
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0025901
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0019522
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0015477
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0018945
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0019368
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0014433
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0016075
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0018173
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0020320
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0019835
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0017787
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0021068
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0941481
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0018241
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0021471
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0018246
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0018589
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0022430
[Epoch 2] ogbg-molmuv: 0.013663 val loss: 0.015689
[Epoch 2] ogbg-molmuv: 0.049642 test loss: 0.015992
[Epoch 3; Iter    14/ 2483] train: loss: 0.0023933
[Epoch 3; Iter    44/ 2483] train: loss: 0.0018073
[Epoch 3; Iter    74/ 2483] train: loss: 0.0024228
[Epoch 3; Iter   104/ 2483] train: loss: 0.0943827
[Epoch 3; Iter   134/ 2483] train: loss: 0.0014202
[Epoch 3; Iter   164/ 2483] train: loss: 0.0020600
[Epoch 3; Iter   194/ 2483] train: loss: 0.0025061
[Epoch 3; Iter   224/ 2483] train: loss: 0.0017954
[Epoch 3; Iter   254/ 2483] train: loss: 0.0017980
[Epoch 3; Iter   284/ 2483] train: loss: 0.0016135
[Epoch 3; Iter   314/ 2483] train: loss: 0.0849274
[Epoch 3; Iter   344/ 2483] train: loss: 0.0017111
[Epoch 3; Iter   374/ 2483] train: loss: 0.0014696
[Epoch 3; Iter   404/ 2483] train: loss: 0.0603726
[Epoch 3; Iter   434/ 2483] train: loss: 0.0018871
[Epoch 3; Iter   464/ 2483] train: loss: 0.0014775
[Epoch 3; Iter   494/ 2483] train: loss: 0.3248156
[Epoch 3; Iter   524/ 2483] train: loss: 0.0019485
[Epoch 3; Iter   554/ 2483] train: loss: 0.0016889
[Epoch 3; Iter   584/ 2483] train: loss: 0.0018604
[Epoch 3; Iter   614/ 2483] train: loss: 0.0016049
[Epoch 3; Iter   644/ 2483] train: loss: 0.0019989
[Epoch 3; Iter   674/ 2483] train: loss: 0.0020473
[Epoch 3; Iter   704/ 2483] train: loss: 0.0021143
[Epoch 3; Iter   734/ 2483] train: loss: 0.0023240
[Epoch 3; Iter   764/ 2483] train: loss: 0.0025926
[Epoch 3; Iter   794/ 2483] train: loss: 0.0023339
[Epoch 3; Iter   824/ 2483] train: loss: 0.0017133
[Epoch 3; Iter   854/ 2483] train: loss: 0.0020843
[Epoch 3; Iter   884/ 2483] train: loss: 0.0024495
[Epoch 3; Iter   914/ 2483] train: loss: 0.0017610
[Epoch 3; Iter   944/ 2483] train: loss: 0.0024114
[Epoch 3; Iter   974/ 2483] train: loss: 0.0015241
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0019586
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0016244
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0017653
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0019755
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0017855
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0017156
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0015694
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0021941
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0025272
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0020714
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0020999
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0022712
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0022014
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0015573
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0014068
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0013940
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0014412
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0016632
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0016167
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0016084
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0018469
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0013405
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0016959
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0014559
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0018800
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0020775
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0679503
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0019020
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0020707
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0680553
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0020551
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0020216
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0020620
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0021318
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0018931
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0024396
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0015156
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0022591
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0762157
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0030479
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0024058
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0022662
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0030514
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0020562
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0023958
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0434422
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0023663
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0023348
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0027559
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0017921
[Epoch 3] ogbg-molmuv: 0.023589 val loss: 0.015241
[Epoch 3] ogbg-molmuv: 0.060254 test loss: 0.016085
[Epoch 4; Iter    21/ 2483] train: loss: 0.0018071
[Epoch 4; Iter    51/ 2483] train: loss: 0.0020510
[Epoch 4; Iter    81/ 2483] train: loss: 0.0013320
[Epoch 4; Iter   111/ 2483] train: loss: 0.0015598
[Epoch 4; Iter   141/ 2483] train: loss: 0.0343148
[Epoch 4; Iter   171/ 2483] train: loss: 0.0024807
[Epoch 4; Iter   201/ 2483] train: loss: 0.0035020
[Epoch 4; Iter   231/ 2483] train: loss: 0.0028825
[Epoch 4; Iter   261/ 2483] train: loss: 0.0023649
[Epoch 4; Iter   291/ 2483] train: loss: 0.0017815
[Epoch 4; Iter   321/ 2483] train: loss: 0.0017237
[Epoch 4; Iter   351/ 2483] train: loss: 0.0016537
[Epoch 4; Iter   381/ 2483] train: loss: 0.0021240
[Epoch 4; Iter   411/ 2483] train: loss: 0.0012935
[Epoch 4; Iter   441/ 2483] train: loss: 0.0016600
[Epoch 2; Iter   757/ 2483] train: loss: 0.0033376
[Epoch 2; Iter   787/ 2483] train: loss: 0.0017866
[Epoch 2; Iter   817/ 2483] train: loss: 0.0017984
[Epoch 2; Iter   847/ 2483] train: loss: 0.0023547
[Epoch 2; Iter   877/ 2483] train: loss: 0.0016831
[Epoch 2; Iter   907/ 2483] train: loss: 0.0016185
[Epoch 2; Iter   937/ 2483] train: loss: 0.0016242
[Epoch 2; Iter   967/ 2483] train: loss: 0.0778391
[Epoch 2; Iter   997/ 2483] train: loss: 0.0017167
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0987996
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0015586
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0015180
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0016810
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0017304
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0019039
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0682791
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0021219
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0016328
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0013546
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0829922
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0014836
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0013804
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0013932
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0015208
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0015831
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0729395
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0613092
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0018310
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0023207
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0016641
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0016785
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0723082
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0738447
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0019774
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0020563
[Epoch 2; Iter  1807/ 2483] train: loss: 0.2099778
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0023292
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0018630
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0021624
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0021332
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0022922
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0017665
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0021104
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0027967
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0649148
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0027319
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0018076
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0036191
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0027345
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0017900
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0738627
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0021531
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0021886
[Epoch 2; Iter  2347/ 2483] train: loss: 0.1796560
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0021213
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0023025
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0020976
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0022538
[Epoch 2] ogbg-molmuv: 0.022671 val loss: 0.015271
[Epoch 2] ogbg-molmuv: 0.016263 test loss: 0.015540
[Epoch 3; Iter    14/ 2483] train: loss: 0.0636419
[Epoch 3; Iter    44/ 2483] train: loss: 0.0025802
[Epoch 3; Iter    74/ 2483] train: loss: 0.0022246
[Epoch 3; Iter   104/ 2483] train: loss: 0.0729799
[Epoch 3; Iter   134/ 2483] train: loss: 0.0019925
[Epoch 3; Iter   164/ 2483] train: loss: 0.0797776
[Epoch 3; Iter   194/ 2483] train: loss: 0.0014022
[Epoch 3; Iter   224/ 2483] train: loss: 0.0012478
[Epoch 3; Iter   254/ 2483] train: loss: 0.0013057
[Epoch 3; Iter   284/ 2483] train: loss: 0.0016536
[Epoch 3; Iter   314/ 2483] train: loss: 0.0016649
[Epoch 3; Iter   344/ 2483] train: loss: 0.0711077
[Epoch 3; Iter   374/ 2483] train: loss: 0.0019232
[Epoch 3; Iter   404/ 2483] train: loss: 0.0014962
[Epoch 3; Iter   434/ 2483] train: loss: 0.0015616
[Epoch 3; Iter   464/ 2483] train: loss: 0.0773978
[Epoch 3; Iter   494/ 2483] train: loss: 0.0019106
[Epoch 3; Iter   524/ 2483] train: loss: 0.0021689
[Epoch 3; Iter   554/ 2483] train: loss: 0.0027133
[Epoch 3; Iter   584/ 2483] train: loss: 0.0024145
[Epoch 3; Iter   614/ 2483] train: loss: 0.0022329
[Epoch 3; Iter   644/ 2483] train: loss: 0.0022522
[Epoch 3; Iter   674/ 2483] train: loss: 0.0020386
[Epoch 3; Iter   704/ 2483] train: loss: 0.0947130
[Epoch 3; Iter   734/ 2483] train: loss: 0.0018666
[Epoch 3; Iter   764/ 2483] train: loss: 0.0015127
[Epoch 3; Iter   794/ 2483] train: loss: 0.0016312
[Epoch 3; Iter   824/ 2483] train: loss: 0.0835996
[Epoch 3; Iter   854/ 2483] train: loss: 0.0778958
[Epoch 3; Iter   884/ 2483] train: loss: 0.0016532
[Epoch 3; Iter   914/ 2483] train: loss: 0.0019929
[Epoch 3; Iter   944/ 2483] train: loss: 0.0018156
[Epoch 3; Iter   974/ 2483] train: loss: 0.0016864
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0015930
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0017724
[Epoch 3; Iter  1064/ 2483] train: loss: 0.1001589
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0020633
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0635583
[Epoch 3; Iter  1154/ 2483] train: loss: 0.1057658
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0013932
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0013433
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0016450
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0012172
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0012756
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0012426
[Epoch 3; Iter  1364/ 2483] train: loss: 0.1414603
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0014500
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0013672
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0013522
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0017516
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0677681
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0016532
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0019324
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0016610
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0016237
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0019598
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0024850
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0033472
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0025151
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0028961
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0020062
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0020728
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0019635
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0680616
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0019366
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0020461
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0921335
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0025082
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0812180
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0724782
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0040056
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0033748
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0022883
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0025722
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0027876
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0033577
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0025451
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0018467
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0019589
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0019527
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0016990
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0019385
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0017779
[Epoch 3] ogbg-molmuv: 0.026122 val loss: 0.015034
[Epoch 3] ogbg-molmuv: 0.027947 test loss: 0.015707
[Epoch 4; Iter    21/ 2483] train: loss: 0.0014773
[Epoch 4; Iter    51/ 2483] train: loss: 0.0014211
[Epoch 4; Iter    81/ 2483] train: loss: 0.0019199
[Epoch 4; Iter   111/ 2483] train: loss: 0.0014913
[Epoch 4; Iter   141/ 2483] train: loss: 0.0018334
[Epoch 4; Iter   171/ 2483] train: loss: 0.0020171
[Epoch 4; Iter   201/ 2483] train: loss: 0.0828827
[Epoch 4; Iter   231/ 2483] train: loss: 0.0022480
[Epoch 4; Iter   261/ 2483] train: loss: 0.0779714
[Epoch 4; Iter   291/ 2483] train: loss: 0.0022157
[Epoch 4; Iter   321/ 2483] train: loss: 0.0023256
[Epoch 4; Iter   351/ 2483] train: loss: 0.0712399
[Epoch 4; Iter   381/ 2483] train: loss: 0.0020579
[Epoch 4; Iter   411/ 2483] train: loss: 0.0020275
[Epoch 4; Iter   441/ 2483] train: loss: 0.0018119
[Epoch 2; Iter   757/ 2483] train: loss: 0.0018060
[Epoch 2; Iter   787/ 2483] train: loss: 0.0018932
[Epoch 2; Iter   817/ 2483] train: loss: 0.0027736
[Epoch 2; Iter   847/ 2483] train: loss: 0.0023419
[Epoch 2; Iter   877/ 2483] train: loss: 0.0022965
[Epoch 2; Iter   907/ 2483] train: loss: 0.0022039
[Epoch 2; Iter   937/ 2483] train: loss: 0.0018689
[Epoch 2; Iter   967/ 2483] train: loss: 0.0020881
[Epoch 2; Iter   997/ 2483] train: loss: 0.0020903
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0028426
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0019081
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0020078
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0017964
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0612201
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0017002
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0016066
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0019494
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0021281
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0918164
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0014402
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0022523
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0020182
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0022817
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0018615
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0706515
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0024019
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0019422
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0017404
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0019041
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0023955
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0019659
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0019271
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0019234
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0024465
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0029490
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0021284
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0018769
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0018979
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0017503
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0019876
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0015952
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0014374
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0663092
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0023084
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0059144
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0019899
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0022789
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0017898
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0613001
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0023124
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0025191
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0025650
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0018853
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0019709
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0017594
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0015502
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0019507
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0015563
[Epoch 2] ogbg-molmuv: 0.006127 val loss: 0.015943
[Epoch 2] ogbg-molmuv: 0.026596 test loss: 0.015808
[Epoch 3; Iter    14/ 2483] train: loss: 0.0014030
[Epoch 3; Iter    44/ 2483] train: loss: 0.0014343
[Epoch 3; Iter    74/ 2483] train: loss: 0.0018268
[Epoch 3; Iter   104/ 2483] train: loss: 0.0017085
[Epoch 3; Iter   134/ 2483] train: loss: 0.0520587
[Epoch 3; Iter   164/ 2483] train: loss: 0.0017245
[Epoch 3; Iter   194/ 2483] train: loss: 0.0026260
[Epoch 3; Iter   224/ 2483] train: loss: 0.0015691
[Epoch 3; Iter   254/ 2483] train: loss: 0.0016427
[Epoch 3; Iter   284/ 2483] train: loss: 0.0018034
[Epoch 3; Iter   314/ 2483] train: loss: 0.0020777
[Epoch 3; Iter   344/ 2483] train: loss: 0.0022954
[Epoch 3; Iter   374/ 2483] train: loss: 0.0015949
[Epoch 3; Iter   404/ 2483] train: loss: 0.0024106
[Epoch 3; Iter   434/ 2483] train: loss: 0.0019588
[Epoch 3; Iter   464/ 2483] train: loss: 0.0015737
[Epoch 3; Iter   494/ 2483] train: loss: 0.0016982
[Epoch 3; Iter   524/ 2483] train: loss: 0.0015337
[Epoch 3; Iter   554/ 2483] train: loss: 0.0016989
[Epoch 3; Iter   584/ 2483] train: loss: 0.0017829
[Epoch 3; Iter   614/ 2483] train: loss: 0.0016879
[Epoch 3; Iter   644/ 2483] train: loss: 0.0018832
[Epoch 3; Iter   674/ 2483] train: loss: 0.0017110
[Epoch 3; Iter   704/ 2483] train: loss: 0.0018810
[Epoch 3; Iter   734/ 2483] train: loss: 0.0015604
[Epoch 3; Iter   764/ 2483] train: loss: 0.0019205
[Epoch 3; Iter   794/ 2483] train: loss: 0.0018669
[Epoch 3; Iter   824/ 2483] train: loss: 0.0024173
[Epoch 3; Iter   854/ 2483] train: loss: 0.0018247
[Epoch 3; Iter   884/ 2483] train: loss: 0.0022222
[Epoch 3; Iter   914/ 2483] train: loss: 0.0017665
[Epoch 3; Iter   944/ 2483] train: loss: 0.0017836
[Epoch 3; Iter   974/ 2483] train: loss: 0.0016213
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0016274
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0015679
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0015624
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0017346
[Epoch 3; Iter  1124/ 2483] train: loss: 0.1019632
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0014272
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0019433
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0786999
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0021352
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0017465
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0015720
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0720879
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0017850
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0025883
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0636328
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0015946
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0020475
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0676824
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0019656
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0018110
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0016153
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0022557
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0018035
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0016994
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0014401
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0012913
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0015395
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0017406
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0019270
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0018088
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0018537
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0753685
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0020042
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0036980
[Epoch 3; Iter  2024/ 2483] train: loss: 0.1232907
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0018231
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0013548
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0964008
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0024241
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0015006
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0624038
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0015931
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0020363
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0016564
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0017906
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0022840
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0031011
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0030855
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0022705
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0020461
[Epoch 3] ogbg-molmuv: 0.015864 val loss: 0.015158
[Epoch 3] ogbg-molmuv: 0.039896 test loss: 0.015970
[Epoch 4; Iter    21/ 2483] train: loss: 0.0029315
[Epoch 4; Iter    51/ 2483] train: loss: 0.0025818
[Epoch 4; Iter    81/ 2483] train: loss: 0.0024492
[Epoch 4; Iter   111/ 2483] train: loss: 0.0022872
[Epoch 4; Iter   141/ 2483] train: loss: 0.0025952
[Epoch 4; Iter   171/ 2483] train: loss: 0.0025722
[Epoch 4; Iter   201/ 2483] train: loss: 0.0021014
[Epoch 4; Iter   231/ 2483] train: loss: 0.0017379
[Epoch 4; Iter   261/ 2483] train: loss: 0.0019445
[Epoch 4; Iter   291/ 2483] train: loss: 0.1013799
[Epoch 4; Iter   321/ 2483] train: loss: 0.0019067
[Epoch 4; Iter   351/ 2483] train: loss: 0.0016033
[Epoch 4; Iter   381/ 2483] train: loss: 0.0012956
[Epoch 4; Iter   411/ 2483] train: loss: 0.0016196
[Epoch 4; Iter   441/ 2483] train: loss: 0.0013634
[Epoch 2; Iter   757/ 2483] train: loss: 0.0022226
[Epoch 2; Iter   787/ 2483] train: loss: 0.0028062
[Epoch 2; Iter   817/ 2483] train: loss: 0.0021201
[Epoch 2; Iter   847/ 2483] train: loss: 0.0018939
[Epoch 2; Iter   877/ 2483] train: loss: 0.0721043
[Epoch 2; Iter   907/ 2483] train: loss: 0.0855358
[Epoch 2; Iter   937/ 2483] train: loss: 0.0021376
[Epoch 2; Iter   967/ 2483] train: loss: 0.0023821
[Epoch 2; Iter   997/ 2483] train: loss: 0.0022766
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0029352
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0023270
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0020647
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0025871
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0028406
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0018523
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0020847
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0020863
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0836805
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0020680
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0023841
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0020257
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0019238
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0017489
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0019510
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0022088
[Epoch 2; Iter  1507/ 2483] train: loss: 0.1718359
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0018071
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0019799
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0019713
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0020663
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0019777
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0021692
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0020291
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0018807
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0022687
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0017223
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0017151
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0015992
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0015673
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0015798
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0019736
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0018710
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0016901
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0019953
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0020227
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0016896
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0021801
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0018265
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0017297
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0015670
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0015567
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0018341
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0953241
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0017850
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0018694
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0016237
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0018286
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0022854
[Epoch 2] ogbg-molmuv: 0.020504 val loss: 0.025584
[Epoch 2] ogbg-molmuv: 0.037643 test loss: 0.017700
[Epoch 3; Iter    14/ 2483] train: loss: 0.0023412
[Epoch 3; Iter    44/ 2483] train: loss: 0.0021105
[Epoch 3; Iter    74/ 2483] train: loss: 0.0019944
[Epoch 3; Iter   104/ 2483] train: loss: 0.0919914
[Epoch 3; Iter   134/ 2483] train: loss: 0.0017243
[Epoch 3; Iter   164/ 2483] train: loss: 0.0020096
[Epoch 3; Iter   194/ 2483] train: loss: 0.0028560
[Epoch 3; Iter   224/ 2483] train: loss: 0.0025749
[Epoch 3; Iter   254/ 2483] train: loss: 0.0018743
[Epoch 3; Iter   284/ 2483] train: loss: 0.0016375
[Epoch 3; Iter   314/ 2483] train: loss: 0.0785324
[Epoch 3; Iter   344/ 2483] train: loss: 0.0015538
[Epoch 3; Iter   374/ 2483] train: loss: 0.0016810
[Epoch 3; Iter   404/ 2483] train: loss: 0.0651891
[Epoch 3; Iter   434/ 2483] train: loss: 0.0018451
[Epoch 3; Iter   464/ 2483] train: loss: 0.0014399
[Epoch 3; Iter   494/ 2483] train: loss: 0.3260463
[Epoch 3; Iter   524/ 2483] train: loss: 0.0019390
[Epoch 3; Iter   554/ 2483] train: loss: 0.0016461
[Epoch 3; Iter   584/ 2483] train: loss: 0.0019265
[Epoch 3; Iter   614/ 2483] train: loss: 0.0017936
[Epoch 3; Iter   644/ 2483] train: loss: 0.0019666
[Epoch 3; Iter   674/ 2483] train: loss: 0.0020044
[Epoch 3; Iter   704/ 2483] train: loss: 0.0020447
[Epoch 3; Iter   734/ 2483] train: loss: 0.0022174
[Epoch 3; Iter   764/ 2483] train: loss: 0.0025287
[Epoch 3; Iter   794/ 2483] train: loss: 0.0022340
[Epoch 3; Iter   824/ 2483] train: loss: 0.0017985
[Epoch 3; Iter   854/ 2483] train: loss: 0.0024910
[Epoch 3; Iter   884/ 2483] train: loss: 0.0023692
[Epoch 3; Iter   914/ 2483] train: loss: 0.0017445
[Epoch 3; Iter   944/ 2483] train: loss: 0.0019467
[Epoch 3; Iter   974/ 2483] train: loss: 0.0016331
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0018027
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0017875
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0017433
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0019528
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0018138
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0016918
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0015838
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0022480
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0020916
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0017645
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0019036
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0018524
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0020659
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0017963
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0014618
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0015201
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0014231
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0016811
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0016817
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0016674
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0020201
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0014326
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0014848
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0016491
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0016552
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0019390
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0725134
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0017635
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0021313
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0695865
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0019927
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0019533
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0020653
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0019373
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0018144
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0023147
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0015362
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0026121
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0739353
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0029640
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0024134
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0024171
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0024390
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0020293
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0020609
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0382594
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0024622
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0020095
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0027800
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0020862
[Epoch 3] ogbg-molmuv: 0.012634 val loss: 0.015196
[Epoch 3] ogbg-molmuv: 0.037816 test loss: 0.016194
[Epoch 4; Iter    21/ 2483] train: loss: 0.0017889
[Epoch 4; Iter    51/ 2483] train: loss: 0.0020112
[Epoch 4; Iter    81/ 2483] train: loss: 0.0017204
[Epoch 4; Iter   111/ 2483] train: loss: 0.0017455
[Epoch 4; Iter   141/ 2483] train: loss: 0.0456038
[Epoch 4; Iter   171/ 2483] train: loss: 0.0023282
[Epoch 4; Iter   201/ 2483] train: loss: 0.0025299
[Epoch 4; Iter   231/ 2483] train: loss: 0.0024085
[Epoch 4; Iter   261/ 2483] train: loss: 0.0019685
[Epoch 4; Iter   291/ 2483] train: loss: 0.0017732
[Epoch 4; Iter   321/ 2483] train: loss: 0.0019560
[Epoch 4; Iter   351/ 2483] train: loss: 0.0016861
[Epoch 4; Iter   381/ 2483] train: loss: 0.0019829
[Epoch 4; Iter   411/ 2483] train: loss: 0.0016861
[Epoch 4; Iter   441/ 2483] train: loss: 0.0014935
[Epoch 2; Iter   757/ 2483] train: loss: 0.0018110
[Epoch 2; Iter   787/ 2483] train: loss: 0.0020463
[Epoch 2; Iter   817/ 2483] train: loss: 0.0028469
[Epoch 2; Iter   847/ 2483] train: loss: 0.0024015
[Epoch 2; Iter   877/ 2483] train: loss: 0.0023863
[Epoch 2; Iter   907/ 2483] train: loss: 0.0022832
[Epoch 2; Iter   937/ 2483] train: loss: 0.0017530
[Epoch 2; Iter   967/ 2483] train: loss: 0.0022309
[Epoch 2; Iter   997/ 2483] train: loss: 0.0021534
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0023290
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0018176
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0020098
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0016835
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0686741
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0017163
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0016072
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0021388
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0019698
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0953556
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0014756
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0019925
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0019157
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0019772
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0016888
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0778101
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0027155
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0026612
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0018737
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0020021
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0020641
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0023568
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0020328
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0020417
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0023101
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0020655
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0020691
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0021269
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0019269
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0019173
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0018091
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0017677
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0016289
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0712144
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0018074
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0021418
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0022588
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0028142
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0018641
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0663419
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0021243
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0028343
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0026623
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0020642
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0021982
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0018606
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0017673
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0018423
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0015896
[Epoch 2] ogbg-molmuv: 0.005379 val loss: 0.015810
[Epoch 2] ogbg-molmuv: 0.012043 test loss: 0.015985
[Epoch 3; Iter    14/ 2483] train: loss: 0.0014646
[Epoch 3; Iter    44/ 2483] train: loss: 0.0013313
[Epoch 3; Iter    74/ 2483] train: loss: 0.0019271
[Epoch 3; Iter   104/ 2483] train: loss: 0.0015825
[Epoch 3; Iter   134/ 2483] train: loss: 0.0656623
[Epoch 3; Iter   164/ 2483] train: loss: 0.0019212
[Epoch 3; Iter   194/ 2483] train: loss: 0.0024006
[Epoch 3; Iter   224/ 2483] train: loss: 0.0019081
[Epoch 3; Iter   254/ 2483] train: loss: 0.0018315
[Epoch 3; Iter   284/ 2483] train: loss: 0.0018873
[Epoch 3; Iter   314/ 2483] train: loss: 0.0021303
[Epoch 3; Iter   344/ 2483] train: loss: 0.0024161
[Epoch 3; Iter   374/ 2483] train: loss: 0.0015257
[Epoch 3; Iter   404/ 2483] train: loss: 0.0020130
[Epoch 3; Iter   434/ 2483] train: loss: 0.0018030
[Epoch 3; Iter   464/ 2483] train: loss: 0.0015863
[Epoch 3; Iter   494/ 2483] train: loss: 0.0016022
[Epoch 3; Iter   524/ 2483] train: loss: 0.0015541
[Epoch 3; Iter   554/ 2483] train: loss: 0.0015755
[Epoch 3; Iter   584/ 2483] train: loss: 0.0017035
[Epoch 3; Iter   614/ 2483] train: loss: 0.0018375
[Epoch 3; Iter   644/ 2483] train: loss: 0.0018922
[Epoch 3; Iter   674/ 2483] train: loss: 0.0020887
[Epoch 3; Iter   704/ 2483] train: loss: 0.0019126
[Epoch 3; Iter   734/ 2483] train: loss: 0.0015331
[Epoch 3; Iter   764/ 2483] train: loss: 0.0019109
[Epoch 3; Iter   794/ 2483] train: loss: 0.0020022
[Epoch 3; Iter   824/ 2483] train: loss: 0.0027789
[Epoch 3; Iter   854/ 2483] train: loss: 0.0020766
[Epoch 3; Iter   884/ 2483] train: loss: 0.0019161
[Epoch 3; Iter   914/ 2483] train: loss: 0.0016907
[Epoch 3; Iter   944/ 2483] train: loss: 0.0021028
[Epoch 3; Iter   974/ 2483] train: loss: 0.0017768
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0016480
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0016780
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0015128
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0016529
[Epoch 3; Iter  1124/ 2483] train: loss: 0.1082328
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0015427
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0020485
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0727980
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0019057
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0016957
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0016195
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0696499
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0021194
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0026194
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0599277
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0017858
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0019457
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0842428
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0020060
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0018639
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0015019
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0018527
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0027248
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0017306
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0015260
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0014331
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0018337
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0018716
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0020415
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0016417
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0018046
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0727538
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0018279
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0024098
[Epoch 3; Iter  2024/ 2483] train: loss: 0.1068741
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0015249
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0013141
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0983173
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0025493
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0019940
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0760821
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0016495
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0018072
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0017386
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0015461
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0023285
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0025423
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0023692
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0023101
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0020299
[Epoch 3] ogbg-molmuv: 0.015746 val loss: 0.020944
[Epoch 3] ogbg-molmuv: 0.015375 test loss: 0.024480
[Epoch 4; Iter    21/ 2483] train: loss: 0.0029170
[Epoch 4; Iter    51/ 2483] train: loss: 0.0035090
[Epoch 4; Iter    81/ 2483] train: loss: 0.0026261
[Epoch 4; Iter   111/ 2483] train: loss: 0.0021993
[Epoch 4; Iter   141/ 2483] train: loss: 0.0028707
[Epoch 4; Iter   171/ 2483] train: loss: 0.0025778
[Epoch 4; Iter   201/ 2483] train: loss: 0.0021557
[Epoch 4; Iter   231/ 2483] train: loss: 0.0016884
[Epoch 4; Iter   261/ 2483] train: loss: 0.0019787
[Epoch 4; Iter   291/ 2483] train: loss: 0.1050573
[Epoch 4; Iter   321/ 2483] train: loss: 0.0016544
[Epoch 4; Iter   351/ 2483] train: loss: 0.0015485
[Epoch 4; Iter   381/ 2483] train: loss: 0.0011876
[Epoch 4; Iter   411/ 2483] train: loss: 0.0012450
[Epoch 4; Iter   441/ 2483] train: loss: 0.0012636
[Epoch 2; Iter   757/ 2483] train: loss: 0.0023691
[Epoch 2; Iter   787/ 2483] train: loss: 0.0016107
[Epoch 2; Iter   817/ 2483] train: loss: 0.0016426
[Epoch 2; Iter   847/ 2483] train: loss: 0.0022856
[Epoch 2; Iter   877/ 2483] train: loss: 0.0017002
[Epoch 2; Iter   907/ 2483] train: loss: 0.0017962
[Epoch 2; Iter   937/ 2483] train: loss: 0.0016119
[Epoch 2; Iter   967/ 2483] train: loss: 0.0778677
[Epoch 2; Iter   997/ 2483] train: loss: 0.0016518
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0964169
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0015848
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0014306
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0017560
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0018207
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0018141
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0702124
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0017639
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0016201
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0014318
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0877348
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0014920
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0015646
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0014345
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0016341
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0016705
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0810270
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0577064
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0017312
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0019279
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0016580
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0017475
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0748191
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0715335
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0020575
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0016477
[Epoch 2; Iter  1807/ 2483] train: loss: 0.1768176
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0023728
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0016057
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0020860
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0021916
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0021670
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0017413
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0021016
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0023921
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0624409
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0022819
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0018281
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0024333
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0025586
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0021257
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0671235
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0022595
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0021965
[Epoch 2; Iter  2347/ 2483] train: loss: 0.1739141
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0024959
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0023621
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0020452
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0022399
[Epoch 2] ogbg-molmuv: 0.035380 val loss: 0.015495
[Epoch 2] ogbg-molmuv: 0.026615 test loss: 0.015920
[Epoch 3; Iter    14/ 2483] train: loss: 0.0519124
[Epoch 3; Iter    44/ 2483] train: loss: 0.0025272
[Epoch 3; Iter    74/ 2483] train: loss: 0.0027105
[Epoch 3; Iter   104/ 2483] train: loss: 0.0816508
[Epoch 3; Iter   134/ 2483] train: loss: 0.0021158
[Epoch 3; Iter   164/ 2483] train: loss: 0.0867283
[Epoch 3; Iter   194/ 2483] train: loss: 0.0013847
[Epoch 3; Iter   224/ 2483] train: loss: 0.0012784
[Epoch 3; Iter   254/ 2483] train: loss: 0.0012838
[Epoch 3; Iter   284/ 2483] train: loss: 0.0017090
[Epoch 3; Iter   314/ 2483] train: loss: 0.0013737
[Epoch 3; Iter   344/ 2483] train: loss: 0.0653726
[Epoch 3; Iter   374/ 2483] train: loss: 0.0020609
[Epoch 3; Iter   404/ 2483] train: loss: 0.0014500
[Epoch 3; Iter   434/ 2483] train: loss: 0.0019362
[Epoch 3; Iter   464/ 2483] train: loss: 0.0794887
[Epoch 3; Iter   494/ 2483] train: loss: 0.0018571
[Epoch 3; Iter   524/ 2483] train: loss: 0.0020779
[Epoch 3; Iter   554/ 2483] train: loss: 0.0025051
[Epoch 3; Iter   584/ 2483] train: loss: 0.0024956
[Epoch 3; Iter   614/ 2483] train: loss: 0.0020657
[Epoch 3; Iter   644/ 2483] train: loss: 0.0020883
[Epoch 3; Iter   674/ 2483] train: loss: 0.0019593
[Epoch 3; Iter   704/ 2483] train: loss: 0.0985326
[Epoch 3; Iter   734/ 2483] train: loss: 0.0019525
[Epoch 3; Iter   764/ 2483] train: loss: 0.0014954
[Epoch 3; Iter   794/ 2483] train: loss: 0.0015593
[Epoch 3; Iter   824/ 2483] train: loss: 0.0805095
[Epoch 3; Iter   854/ 2483] train: loss: 0.0837755
[Epoch 3; Iter   884/ 2483] train: loss: 0.0018540
[Epoch 3; Iter   914/ 2483] train: loss: 0.0019329
[Epoch 3; Iter   944/ 2483] train: loss: 0.0018583
[Epoch 3; Iter   974/ 2483] train: loss: 0.0019110
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0017262
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0016577
[Epoch 3; Iter  1064/ 2483] train: loss: 0.1135759
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0022953
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0584633
[Epoch 3; Iter  1154/ 2483] train: loss: 0.1111385
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0014022
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0012850
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0015246
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0012020
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0012975
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0015756
[Epoch 3; Iter  1364/ 2483] train: loss: 0.1426332
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0015223
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0015130
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0014200
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0016216
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0631718
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0015711
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0021151
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0018459
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0016144
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0026510
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0024385
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0024156
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0022909
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0024513
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0019783
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0022420
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0019599
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0724701
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0020751
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0022655
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0871067
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0029660
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0841413
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0722897
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0032707
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0029256
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0019359
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0019296
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0019944
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0032030
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0024009
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0019095
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0020522
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0018471
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0017422
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0019449
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0020905
[Epoch 3] ogbg-molmuv: 0.025244 val loss: 0.115327
[Epoch 3] ogbg-molmuv: 0.029843 test loss: 0.108016
[Epoch 4; Iter    21/ 2483] train: loss: 0.0015681
[Epoch 4; Iter    51/ 2483] train: loss: 0.0014460
[Epoch 4; Iter    81/ 2483] train: loss: 0.0018581
[Epoch 4; Iter   111/ 2483] train: loss: 0.0016038
[Epoch 4; Iter   141/ 2483] train: loss: 0.0019685
[Epoch 4; Iter   171/ 2483] train: loss: 0.0023131
[Epoch 4; Iter   201/ 2483] train: loss: 0.0803696
[Epoch 4; Iter   231/ 2483] train: loss: 0.0024678
[Epoch 4; Iter   261/ 2483] train: loss: 0.0806939
[Epoch 4; Iter   291/ 2483] train: loss: 0.0022928
[Epoch 4; Iter   321/ 2483] train: loss: 0.0022314
[Epoch 4; Iter   351/ 2483] train: loss: 0.0758056
[Epoch 4; Iter   381/ 2483] train: loss: 0.0019808
[Epoch 4; Iter   411/ 2483] train: loss: 0.0020114
[Epoch 4; Iter   441/ 2483] train: loss: 0.0017869
[Epoch 2; Iter   757/ 2483] train: loss: 0.0020942
[Epoch 2; Iter   787/ 2483] train: loss: 0.0018865
[Epoch 2; Iter   817/ 2483] train: loss: 0.0018297
[Epoch 2; Iter   847/ 2483] train: loss: 0.0020121
[Epoch 2; Iter   877/ 2483] train: loss: 0.0017554
[Epoch 2; Iter   907/ 2483] train: loss: 0.0016208
[Epoch 2; Iter   937/ 2483] train: loss: 0.0016695
[Epoch 2; Iter   967/ 2483] train: loss: 0.0767272
[Epoch 2; Iter   997/ 2483] train: loss: 0.0016707
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0933384
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0015015
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0015082
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0017505
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0018000
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0018152
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0718466
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0017964
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0017827
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0014811
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0804419
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0016220
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0016650
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0016132
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0016501
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0018018
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0767670
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0686070
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0018194
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0017589
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0018712
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0019595
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0615594
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0683368
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0018021
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0025691
[Epoch 2; Iter  1807/ 2483] train: loss: 0.2060286
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0025818
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0017099
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0024881
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0024488
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0019954
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0015118
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0018786
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0024035
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0626069
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0019679
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0018133
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0025197
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0027458
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0020927
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0678273
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0023383
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0025296
[Epoch 2; Iter  2347/ 2483] train: loss: 0.1863585
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0023745
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0023103
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0020666
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0020119
[Epoch 2] ogbg-molmuv: 0.014275 val loss: 0.024223
[Epoch 2] ogbg-molmuv: 0.017149 test loss: 0.017682
[Epoch 3; Iter    14/ 2483] train: loss: 0.0536519
[Epoch 3; Iter    44/ 2483] train: loss: 0.0024441
[Epoch 3; Iter    74/ 2483] train: loss: 0.0023462
[Epoch 3; Iter   104/ 2483] train: loss: 0.0744471
[Epoch 3; Iter   134/ 2483] train: loss: 0.0021882
[Epoch 3; Iter   164/ 2483] train: loss: 0.0896006
[Epoch 3; Iter   194/ 2483] train: loss: 0.0015487
[Epoch 3; Iter   224/ 2483] train: loss: 0.0013031
[Epoch 3; Iter   254/ 2483] train: loss: 0.0012153
[Epoch 3; Iter   284/ 2483] train: loss: 0.0016856
[Epoch 3; Iter   314/ 2483] train: loss: 0.0017063
[Epoch 3; Iter   344/ 2483] train: loss: 0.0720429
[Epoch 3; Iter   374/ 2483] train: loss: 0.0015393
[Epoch 3; Iter   404/ 2483] train: loss: 0.0014895
[Epoch 3; Iter   434/ 2483] train: loss: 0.0015565
[Epoch 3; Iter   464/ 2483] train: loss: 0.0793568
[Epoch 3; Iter   494/ 2483] train: loss: 0.0022413
[Epoch 3; Iter   524/ 2483] train: loss: 0.0020944
[Epoch 3; Iter   554/ 2483] train: loss: 0.0022812
[Epoch 3; Iter   584/ 2483] train: loss: 0.0026322
[Epoch 3; Iter   614/ 2483] train: loss: 0.0019639
[Epoch 3; Iter   644/ 2483] train: loss: 0.0021692
[Epoch 3; Iter   674/ 2483] train: loss: 0.0023537
[Epoch 3; Iter   704/ 2483] train: loss: 0.0975073
[Epoch 3; Iter   734/ 2483] train: loss: 0.0020376
[Epoch 3; Iter   764/ 2483] train: loss: 0.0015508
[Epoch 3; Iter   794/ 2483] train: loss: 0.0016313
[Epoch 3; Iter   824/ 2483] train: loss: 0.0887344
[Epoch 3; Iter   854/ 2483] train: loss: 0.0793086
[Epoch 3; Iter   884/ 2483] train: loss: 0.0016815
[Epoch 3; Iter   914/ 2483] train: loss: 0.0020308
[Epoch 3; Iter   944/ 2483] train: loss: 0.0015571
[Epoch 3; Iter   974/ 2483] train: loss: 0.0014834
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0015237
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0022569
[Epoch 3; Iter  1064/ 2483] train: loss: 0.1052252
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0019893
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0619355
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0944459
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0014309
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0011982
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0016902
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0012181
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0012867
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0017225
[Epoch 3; Iter  1364/ 2483] train: loss: 0.1556466
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0019019
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0016306
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0018699
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0018959
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0652920
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0015123
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0018889
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0016747
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0017545
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0021095
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0021939
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0027891
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0023700
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0025237
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0020167
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0021239
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0019472
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0763544
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0021229
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0019408
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0898618
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0026346
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0826067
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0753902
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0028706
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0040476
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0027372
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0019611
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0022949
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0030497
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0024742
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0021796
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0020752
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0019563
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0018104
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0021360
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0018493
[Epoch 3] ogbg-molmuv: 0.022551 val loss: 0.015305
[Epoch 3] ogbg-molmuv: 0.011321 test loss: 0.016368
[Epoch 4; Iter    21/ 2483] train: loss: 0.0014100
[Epoch 4; Iter    51/ 2483] train: loss: 0.0013809
[Epoch 4; Iter    81/ 2483] train: loss: 0.0017798
[Epoch 4; Iter   111/ 2483] train: loss: 0.0015824
[Epoch 4; Iter   141/ 2483] train: loss: 0.0017683
[Epoch 4; Iter   171/ 2483] train: loss: 0.0020008
[Epoch 4; Iter   201/ 2483] train: loss: 0.0800873
[Epoch 4; Iter   231/ 2483] train: loss: 0.0021499
[Epoch 4; Iter   261/ 2483] train: loss: 0.0845370
[Epoch 4; Iter   291/ 2483] train: loss: 0.0022486
[Epoch 4; Iter   321/ 2483] train: loss: 0.0026554
[Epoch 4; Iter   351/ 2483] train: loss: 0.0739542
[Epoch 4; Iter   381/ 2483] train: loss: 0.0019721
[Epoch 4; Iter   411/ 2483] train: loss: 0.0019342
[Epoch 4; Iter   441/ 2483] train: loss: 0.0019013
[Epoch 2; Iter   757/ 2483] train: loss: 0.0023209
[Epoch 2; Iter   787/ 2483] train: loss: 0.0024462
[Epoch 2; Iter   817/ 2483] train: loss: 0.0022864
[Epoch 2; Iter   847/ 2483] train: loss: 0.0020055
[Epoch 2; Iter   877/ 2483] train: loss: 0.0664630
[Epoch 2; Iter   907/ 2483] train: loss: 0.0772626
[Epoch 2; Iter   937/ 2483] train: loss: 0.0022100
[Epoch 2; Iter   967/ 2483] train: loss: 0.0024903
[Epoch 2; Iter   997/ 2483] train: loss: 0.0023292
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0024411
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0020145
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0020344
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0021728
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0023628
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0019727
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0019628
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0022474
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0857379
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0020200
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0023477
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0021816
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0017332
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0018060
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0018366
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0022215
[Epoch 2; Iter  1507/ 2483] train: loss: 0.1641627
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0020846
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0017921
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0018388
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0021108
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0020062
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0021569
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0019372
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0017925
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0021850
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0017694
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0018745
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0015615
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0016024
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0015725
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0019307
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0018499
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0016723
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0021464
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0020709
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0019139
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0019010
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0016837
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0018175
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0016940
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0018672
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0019691
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0914759
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0019300
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0018302
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0016459
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0016207
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0024053
[Epoch 2] ogbg-molmuv: 0.018425 val loss: 0.017898
[Epoch 2] ogbg-molmuv: 0.053156 test loss: 0.019207
[Epoch 3; Iter    14/ 2483] train: loss: 0.0026276
[Epoch 3; Iter    44/ 2483] train: loss: 0.0018295
[Epoch 3; Iter    74/ 2483] train: loss: 0.0016190
[Epoch 3; Iter   104/ 2483] train: loss: 0.0919954
[Epoch 3; Iter   134/ 2483] train: loss: 0.0015220
[Epoch 3; Iter   164/ 2483] train: loss: 0.0017480
[Epoch 3; Iter   194/ 2483] train: loss: 0.0020161
[Epoch 3; Iter   224/ 2483] train: loss: 0.0017880
[Epoch 3; Iter   254/ 2483] train: loss: 0.0018235
[Epoch 3; Iter   284/ 2483] train: loss: 0.0018537
[Epoch 3; Iter   314/ 2483] train: loss: 0.0823496
[Epoch 3; Iter   344/ 2483] train: loss: 0.0018503
[Epoch 3; Iter   374/ 2483] train: loss: 0.0016869
[Epoch 3; Iter   404/ 2483] train: loss: 0.0641825
[Epoch 3; Iter   434/ 2483] train: loss: 0.0018292
[Epoch 3; Iter   464/ 2483] train: loss: 0.0019080
[Epoch 3; Iter   494/ 2483] train: loss: 0.3313509
[Epoch 3; Iter   524/ 2483] train: loss: 0.0019771
[Epoch 3; Iter   554/ 2483] train: loss: 0.0019056
[Epoch 3; Iter   584/ 2483] train: loss: 0.0020733
[Epoch 3; Iter   614/ 2483] train: loss: 0.0016260
[Epoch 3; Iter   644/ 2483] train: loss: 0.0019702
[Epoch 3; Iter   674/ 2483] train: loss: 0.0021263
[Epoch 3; Iter   704/ 2483] train: loss: 0.0020448
[Epoch 3; Iter   734/ 2483] train: loss: 0.0024523
[Epoch 3; Iter   764/ 2483] train: loss: 0.0019912
[Epoch 3; Iter   794/ 2483] train: loss: 0.0023331
[Epoch 3; Iter   824/ 2483] train: loss: 0.0015528
[Epoch 3; Iter   854/ 2483] train: loss: 0.0020442
[Epoch 3; Iter   884/ 2483] train: loss: 0.0021278
[Epoch 3; Iter   914/ 2483] train: loss: 0.0014829
[Epoch 3; Iter   944/ 2483] train: loss: 0.0018710
[Epoch 3; Iter   974/ 2483] train: loss: 0.0016384
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0019910
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0016110
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0016895
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0019047
[Epoch 3; Iter  1124/ 2483] train: loss: 0.0017649
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0016669
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0015932
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0021797
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0024281
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0018491
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0020408
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0019548
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0017625
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0015500
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0014050
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0015265
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0014991
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0017547
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0018025
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0017988
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0019416
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0015011
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0016808
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0014917
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0018742
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0019481
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0647297
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0021909
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0023999
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0645611
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0021562
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0018012
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0019048
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0019470
[Epoch 3; Iter  2024/ 2483] train: loss: 0.0018766
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0019603
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0015811
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0022825
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0737075
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0025621
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0026631
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0027043
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0027967
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0019477
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0025362
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0458458
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0029683
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0023393
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0027288
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0025237
[Epoch 3] ogbg-molmuv: 0.007790 val loss: 0.016621
[Epoch 3] ogbg-molmuv: 0.017371 test loss: 0.017223
[Epoch 4; Iter    21/ 2483] train: loss: 0.0019207
[Epoch 4; Iter    51/ 2483] train: loss: 0.0021184
[Epoch 4; Iter    81/ 2483] train: loss: 0.0015590
[Epoch 4; Iter   111/ 2483] train: loss: 0.0015944
[Epoch 4; Iter   141/ 2483] train: loss: 0.0618323
[Epoch 4; Iter   171/ 2483] train: loss: 0.0023760
[Epoch 4; Iter   201/ 2483] train: loss: 0.0023189
[Epoch 4; Iter   231/ 2483] train: loss: 0.0022428
[Epoch 4; Iter   261/ 2483] train: loss: 0.0021077
[Epoch 4; Iter   291/ 2483] train: loss: 0.0018771
[Epoch 4; Iter   321/ 2483] train: loss: 0.0019716
[Epoch 4; Iter   351/ 2483] train: loss: 0.0015643
[Epoch 4; Iter   381/ 2483] train: loss: 0.0022553
[Epoch 4; Iter   411/ 2483] train: loss: 0.0016229
[Epoch 4; Iter   441/ 2483] train: loss: 0.0015043
[Epoch 2; Iter   757/ 2483] train: loss: 0.0017589
[Epoch 2; Iter   787/ 2483] train: loss: 0.0022963
[Epoch 2; Iter   817/ 2483] train: loss: 0.0020348
[Epoch 2; Iter   847/ 2483] train: loss: 0.0017699
[Epoch 2; Iter   877/ 2483] train: loss: 0.0017435
[Epoch 2; Iter   907/ 2483] train: loss: 0.0018250
[Epoch 2; Iter   937/ 2483] train: loss: 0.0018502
[Epoch 2; Iter   967/ 2483] train: loss: 0.0018191
[Epoch 2; Iter   997/ 2483] train: loss: 0.0017602
[Epoch 2; Iter  1027/ 2483] train: loss: 0.0021809
[Epoch 2; Iter  1057/ 2483] train: loss: 0.0018744
[Epoch 2; Iter  1087/ 2483] train: loss: 0.0018892
[Epoch 2; Iter  1117/ 2483] train: loss: 0.0018152
[Epoch 2; Iter  1147/ 2483] train: loss: 0.0730129
[Epoch 2; Iter  1177/ 2483] train: loss: 0.0018340
[Epoch 2; Iter  1207/ 2483] train: loss: 0.0018500
[Epoch 2; Iter  1237/ 2483] train: loss: 0.0021381
[Epoch 2; Iter  1267/ 2483] train: loss: 0.0019375
[Epoch 2; Iter  1297/ 2483] train: loss: 0.0959394
[Epoch 2; Iter  1327/ 2483] train: loss: 0.0017091
[Epoch 2; Iter  1357/ 2483] train: loss: 0.0021557
[Epoch 2; Iter  1387/ 2483] train: loss: 0.0021534
[Epoch 2; Iter  1417/ 2483] train: loss: 0.0019111
[Epoch 2; Iter  1447/ 2483] train: loss: 0.0023368
[Epoch 2; Iter  1477/ 2483] train: loss: 0.0755764
[Epoch 2; Iter  1507/ 2483] train: loss: 0.0025757
[Epoch 2; Iter  1537/ 2483] train: loss: 0.0018215
[Epoch 2; Iter  1567/ 2483] train: loss: 0.0019817
[Epoch 2; Iter  1597/ 2483] train: loss: 0.0018285
[Epoch 2; Iter  1627/ 2483] train: loss: 0.0047371
[Epoch 2; Iter  1657/ 2483] train: loss: 0.0020938
[Epoch 2; Iter  1687/ 2483] train: loss: 0.0022945
[Epoch 2; Iter  1717/ 2483] train: loss: 0.0019324
[Epoch 2; Iter  1747/ 2483] train: loss: 0.0023951
[Epoch 2; Iter  1777/ 2483] train: loss: 0.0021584
[Epoch 2; Iter  1807/ 2483] train: loss: 0.0021092
[Epoch 2; Iter  1837/ 2483] train: loss: 0.0018297
[Epoch 2; Iter  1867/ 2483] train: loss: 0.0017953
[Epoch 2; Iter  1897/ 2483] train: loss: 0.0017199
[Epoch 2; Iter  1927/ 2483] train: loss: 0.0017473
[Epoch 2; Iter  1957/ 2483] train: loss: 0.0016998
[Epoch 2; Iter  1987/ 2483] train: loss: 0.0014460
[Epoch 2; Iter  2017/ 2483] train: loss: 0.0665425
[Epoch 2; Iter  2047/ 2483] train: loss: 0.0018002
[Epoch 2; Iter  2077/ 2483] train: loss: 0.0018691
[Epoch 2; Iter  2107/ 2483] train: loss: 0.0021574
[Epoch 2; Iter  2137/ 2483] train: loss: 0.0024479
[Epoch 2; Iter  2167/ 2483] train: loss: 0.0019304
[Epoch 2; Iter  2197/ 2483] train: loss: 0.0610456
[Epoch 2; Iter  2227/ 2483] train: loss: 0.0020477
[Epoch 2; Iter  2257/ 2483] train: loss: 0.0027368
[Epoch 2; Iter  2287/ 2483] train: loss: 0.0022580
[Epoch 2; Iter  2317/ 2483] train: loss: 0.0020357
[Epoch 2; Iter  2347/ 2483] train: loss: 0.0022032
[Epoch 2; Iter  2377/ 2483] train: loss: 0.0017912
[Epoch 2; Iter  2407/ 2483] train: loss: 0.0017286
[Epoch 2; Iter  2437/ 2483] train: loss: 0.0019650
[Epoch 2; Iter  2467/ 2483] train: loss: 0.0016690
[Epoch 2] ogbg-molmuv: 0.013496 val loss: 0.016050
[Epoch 2] ogbg-molmuv: 0.031573 test loss: 0.015824
[Epoch 3; Iter    14/ 2483] train: loss: 0.0014303
[Epoch 3; Iter    44/ 2483] train: loss: 0.0013975
[Epoch 3; Iter    74/ 2483] train: loss: 0.0015626
[Epoch 3; Iter   104/ 2483] train: loss: 0.0017225
[Epoch 3; Iter   134/ 2483] train: loss: 0.0700325
[Epoch 3; Iter   164/ 2483] train: loss: 0.0022987
[Epoch 3; Iter   194/ 2483] train: loss: 0.0021236
[Epoch 3; Iter   224/ 2483] train: loss: 0.0019833
[Epoch 3; Iter   254/ 2483] train: loss: 0.0018112
[Epoch 3; Iter   284/ 2483] train: loss: 0.0022349
[Epoch 3; Iter   314/ 2483] train: loss: 0.0018891
[Epoch 3; Iter   344/ 2483] train: loss: 0.0026840
[Epoch 3; Iter   374/ 2483] train: loss: 0.0017994
[Epoch 3; Iter   404/ 2483] train: loss: 0.0023838
[Epoch 3; Iter   434/ 2483] train: loss: 0.0020555
[Epoch 3; Iter   464/ 2483] train: loss: 0.0017962
[Epoch 3; Iter   494/ 2483] train: loss: 0.0016044
[Epoch 3; Iter   524/ 2483] train: loss: 0.0016406
[Epoch 3; Iter   554/ 2483] train: loss: 0.0016504
[Epoch 3; Iter   584/ 2483] train: loss: 0.0016889
[Epoch 3; Iter   614/ 2483] train: loss: 0.0021637
[Epoch 3; Iter   644/ 2483] train: loss: 0.0019149
[Epoch 3; Iter   674/ 2483] train: loss: 0.0017968
[Epoch 3; Iter   704/ 2483] train: loss: 0.0018755
[Epoch 3; Iter   734/ 2483] train: loss: 0.0017153
[Epoch 3; Iter   764/ 2483] train: loss: 0.0019393
[Epoch 3; Iter   794/ 2483] train: loss: 0.0020656
[Epoch 3; Iter   824/ 2483] train: loss: 0.0025379
[Epoch 3; Iter   854/ 2483] train: loss: 0.0019202
[Epoch 3; Iter   884/ 2483] train: loss: 0.0017761
[Epoch 3; Iter   914/ 2483] train: loss: 0.0018440
[Epoch 3; Iter   944/ 2483] train: loss: 0.0016664
[Epoch 3; Iter   974/ 2483] train: loss: 0.0016324
[Epoch 3; Iter  1004/ 2483] train: loss: 0.0017681
[Epoch 3; Iter  1034/ 2483] train: loss: 0.0017089
[Epoch 3; Iter  1064/ 2483] train: loss: 0.0015720
[Epoch 3; Iter  1094/ 2483] train: loss: 0.0015098
[Epoch 3; Iter  1124/ 2483] train: loss: 0.1067916
[Epoch 3; Iter  1154/ 2483] train: loss: 0.0015334
[Epoch 3; Iter  1184/ 2483] train: loss: 0.0017906
[Epoch 3; Iter  1214/ 2483] train: loss: 0.0746475
[Epoch 3; Iter  1244/ 2483] train: loss: 0.0023161
[Epoch 3; Iter  1274/ 2483] train: loss: 0.0018384
[Epoch 3; Iter  1304/ 2483] train: loss: 0.0015541
[Epoch 3; Iter  1334/ 2483] train: loss: 0.0720852
[Epoch 3; Iter  1364/ 2483] train: loss: 0.0016880
[Epoch 3; Iter  1394/ 2483] train: loss: 0.0026951
[Epoch 3; Iter  1424/ 2483] train: loss: 0.0614273
[Epoch 3; Iter  1454/ 2483] train: loss: 0.0021094
[Epoch 3; Iter  1484/ 2483] train: loss: 0.0020054
[Epoch 3; Iter  1514/ 2483] train: loss: 0.0651122
[Epoch 3; Iter  1544/ 2483] train: loss: 0.0021670
[Epoch 3; Iter  1574/ 2483] train: loss: 0.0019999
[Epoch 3; Iter  1604/ 2483] train: loss: 0.0015574
[Epoch 3; Iter  1634/ 2483] train: loss: 0.0034817
[Epoch 3; Iter  1664/ 2483] train: loss: 0.0017064
[Epoch 3; Iter  1694/ 2483] train: loss: 0.0016796
[Epoch 3; Iter  1724/ 2483] train: loss: 0.0012959
[Epoch 3; Iter  1754/ 2483] train: loss: 0.0013178
[Epoch 3; Iter  1784/ 2483] train: loss: 0.0019387
[Epoch 3; Iter  1814/ 2483] train: loss: 0.0020564
[Epoch 3; Iter  1844/ 2483] train: loss: 0.0019790
[Epoch 3; Iter  1874/ 2483] train: loss: 0.0017028
[Epoch 3; Iter  1904/ 2483] train: loss: 0.0013700
[Epoch 3; Iter  1934/ 2483] train: loss: 0.0679010
[Epoch 3; Iter  1964/ 2483] train: loss: 0.0027264
[Epoch 3; Iter  1994/ 2483] train: loss: 0.0025453
[Epoch 3; Iter  2024/ 2483] train: loss: 0.1186716
[Epoch 3; Iter  2054/ 2483] train: loss: 0.0014309
[Epoch 3; Iter  2084/ 2483] train: loss: 0.0015661
[Epoch 3; Iter  2114/ 2483] train: loss: 0.0997371
[Epoch 3; Iter  2144/ 2483] train: loss: 0.0014584
[Epoch 3; Iter  2174/ 2483] train: loss: 0.0018622
[Epoch 3; Iter  2204/ 2483] train: loss: 0.0770392
[Epoch 3; Iter  2234/ 2483] train: loss: 0.0015859
[Epoch 3; Iter  2264/ 2483] train: loss: 0.0017540
[Epoch 3; Iter  2294/ 2483] train: loss: 0.0020334
[Epoch 3; Iter  2324/ 2483] train: loss: 0.0018265
[Epoch 3; Iter  2354/ 2483] train: loss: 0.0020556
[Epoch 3; Iter  2384/ 2483] train: loss: 0.0027251
[Epoch 3; Iter  2414/ 2483] train: loss: 0.0026524
[Epoch 3; Iter  2444/ 2483] train: loss: 0.0022689
[Epoch 3; Iter  2474/ 2483] train: loss: 0.0020511
[Epoch 3] ogbg-molmuv: 0.018804 val loss: 0.018388
[Epoch 3] ogbg-molmuv: 0.013773 test loss: 0.018827
[Epoch 4; Iter    21/ 2483] train: loss: 0.0032607
[Epoch 4; Iter    51/ 2483] train: loss: 0.0025792
[Epoch 4; Iter    81/ 2483] train: loss: 0.0026413
[Epoch 4; Iter   111/ 2483] train: loss: 0.0021250
[Epoch 4; Iter   141/ 2483] train: loss: 0.0023885
[Epoch 4; Iter   171/ 2483] train: loss: 0.0025132
[Epoch 4; Iter   201/ 2483] train: loss: 0.0022802
[Epoch 4; Iter   231/ 2483] train: loss: 0.0017471
[Epoch 4; Iter   261/ 2483] train: loss: 0.0019772
[Epoch 4; Iter   291/ 2483] train: loss: 0.0946108
[Epoch 4; Iter   321/ 2483] train: loss: 0.0019646
[Epoch 4; Iter   351/ 2483] train: loss: 0.0015476
[Epoch 4; Iter   381/ 2483] train: loss: 0.0013630
[Epoch 4; Iter   411/ 2483] train: loss: 0.0013943
[Epoch 4; Iter   441/ 2483] train: loss: 0.0013288
[Epoch 4; Iter   471/ 2483] train: loss: 0.0014812
[Epoch 4; Iter   501/ 2483] train: loss: 0.0017023
[Epoch 4; Iter   531/ 2483] train: loss: 0.0013414
[Epoch 4; Iter   561/ 2483] train: loss: 0.0016160
[Epoch 4; Iter   591/ 2483] train: loss: 0.0013660
[Epoch 4; Iter   621/ 2483] train: loss: 0.0014193
[Epoch 4; Iter   651/ 2483] train: loss: 0.1784877
[Epoch 4; Iter   681/ 2483] train: loss: 0.0790553
[Epoch 4; Iter   711/ 2483] train: loss: 0.0023779
[Epoch 4; Iter   741/ 2483] train: loss: 0.0019317
[Epoch 4; Iter   771/ 2483] train: loss: 0.0018551
[Epoch 4; Iter   801/ 2483] train: loss: 0.0594478
[Epoch 4; Iter   831/ 2483] train: loss: 0.0017572
[Epoch 4; Iter   861/ 2483] train: loss: 0.0016629
[Epoch 4; Iter   891/ 2483] train: loss: 0.0021713
[Epoch 4; Iter   921/ 2483] train: loss: 0.0019869
[Epoch 4; Iter   951/ 2483] train: loss: 0.0019543
[Epoch 4; Iter   981/ 2483] train: loss: 0.0021438
[Epoch 4; Iter  1011/ 2483] train: loss: 0.1215776
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0019683
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0025852
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0017662
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0016526
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0016319
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0021340
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0017930
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0834619
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0024945
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0019226
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0016975
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0016117
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0770388
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0018289
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0023810
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0017704
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0017895
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0021755
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0027613
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0924790
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0019091
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0015586
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0029466
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0028818
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0583537
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0017125
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0018481
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0021061
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0021011
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0017222
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0015770
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0018949
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0016450
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0013929
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0020817
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0012150
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0019133
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0018588
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0022060
[Epoch 4; Iter  2211/ 2483] train: loss: 0.1526447
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0840891
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0019212
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0014843
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0751061
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0014815
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0026983
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0020072
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0016436
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0016828
[Epoch 4] ogbg-molmuv: 0.023495 val loss: 0.019134
[Epoch 4] ogbg-molmuv: 0.015123 test loss: 0.020289
[Epoch 5; Iter    28/ 2483] train: loss: 0.0015399
[Epoch 5; Iter    58/ 2483] train: loss: 0.0017743
[Epoch 5; Iter    88/ 2483] train: loss: 0.0018419
[Epoch 5; Iter   118/ 2483] train: loss: 0.0014745
[Epoch 5; Iter   148/ 2483] train: loss: 0.0016857
[Epoch 5; Iter   178/ 2483] train: loss: 0.0014829
[Epoch 5; Iter   208/ 2483] train: loss: 0.0013018
[Epoch 5; Iter   238/ 2483] train: loss: 0.0016307
[Epoch 5; Iter   268/ 2483] train: loss: 0.0016175
[Epoch 5; Iter   298/ 2483] train: loss: 0.0018458
[Epoch 5; Iter   328/ 2483] train: loss: 0.0017672
[Epoch 5; Iter   358/ 2483] train: loss: 0.0016690
[Epoch 5; Iter   388/ 2483] train: loss: 0.0018467
[Epoch 5; Iter   418/ 2483] train: loss: 0.0019729
[Epoch 5; Iter   448/ 2483] train: loss: 0.0021966
[Epoch 5; Iter   478/ 2483] train: loss: 0.0018930
[Epoch 5; Iter   508/ 2483] train: loss: 0.0023781
[Epoch 5; Iter   538/ 2483] train: loss: 0.0023182
[Epoch 5; Iter   568/ 2483] train: loss: 0.0925039
[Epoch 5; Iter   598/ 2483] train: loss: 0.0793116
[Epoch 5; Iter   628/ 2483] train: loss: 0.0030326
[Epoch 5; Iter   658/ 2483] train: loss: 0.0018583
[Epoch 5; Iter   688/ 2483] train: loss: 0.0016126
[Epoch 5; Iter   718/ 2483] train: loss: 0.0017097
[Epoch 5; Iter   748/ 2483] train: loss: 0.0018110
[Epoch 5; Iter   778/ 2483] train: loss: 0.0015730
[Epoch 5; Iter   808/ 2483] train: loss: 0.0018565
[Epoch 5; Iter   838/ 2483] train: loss: 0.0017957
[Epoch 5; Iter   868/ 2483] train: loss: 0.0016320
[Epoch 5; Iter   898/ 2483] train: loss: 0.0024967
[Epoch 5; Iter   928/ 2483] train: loss: 0.0019351
[Epoch 5; Iter   958/ 2483] train: loss: 0.0020295
[Epoch 5; Iter   988/ 2483] train: loss: 0.0017976
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0015309
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0014289
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0016041
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0013536
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0014877
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0016342
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0017657
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0015622
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0013145
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0017262
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0016406
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0018629
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0018310
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0024583
[Epoch 5; Iter  1438/ 2483] train: loss: 0.1005413
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0023515
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0021715
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0025253
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0017374
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0022299
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0017417
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0767729
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0025547
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0026992
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0017360
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0673198
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0020040
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0016885
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0987810
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0016798
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0650944
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0020135
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0022550
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0014924
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0014877
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0602579
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0018981
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0014414
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0014819
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0020762
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0017968
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0019110
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0018926
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0015860
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0015273
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0016694
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0021738
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0022994
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0823828
[Epoch 5] ogbg-molmuv: 0.009038 val loss: 0.015448
[Epoch 5] ogbg-molmuv: 0.027266 test loss: 0.015806
[Epoch 6; Iter     5/ 2483] train: loss: 0.0019490
[Epoch 6; Iter    35/ 2483] train: loss: 0.0022110
[Epoch 6; Iter    65/ 2483] train: loss: 0.0027352
[Epoch 6; Iter    95/ 2483] train: loss: 0.0908056
[Epoch 6; Iter   125/ 2483] train: loss: 0.0023328
[Epoch 6; Iter   155/ 2483] train: loss: 0.0020579
[Epoch 6; Iter   185/ 2483] train: loss: 0.0014700
[Epoch 6; Iter   215/ 2483] train: loss: 0.0016799
[Epoch 6; Iter   245/ 2483] train: loss: 0.0868329
[Epoch 6; Iter   275/ 2483] train: loss: 0.0015427
[Epoch 6; Iter   305/ 2483] train: loss: 0.0017881
[Epoch 6; Iter   335/ 2483] train: loss: 0.0020528
[Epoch 6; Iter   365/ 2483] train: loss: 0.0012384
[Epoch 6; Iter   395/ 2483] train: loss: 0.0020149
[Epoch 6; Iter   425/ 2483] train: loss: 0.0791283
[Epoch 6; Iter   455/ 2483] train: loss: 0.0641483
[Epoch 6; Iter   485/ 2483] train: loss: 0.0024127
[Epoch 6; Iter   515/ 2483] train: loss: 0.0022650
[Epoch 6; Iter   545/ 2483] train: loss: 0.0018781
[Epoch 6; Iter   575/ 2483] train: loss: 0.0018940
[Epoch 6; Iter   605/ 2483] train: loss: 0.0015769
[Epoch 6; Iter   635/ 2483] train: loss: 0.0019717
[Epoch 6; Iter   665/ 2483] train: loss: 0.0015971
[Epoch 6; Iter   695/ 2483] train: loss: 0.0025188
[Epoch 6; Iter   725/ 2483] train: loss: 0.0023873
[Epoch 6; Iter   755/ 2483] train: loss: 0.0020269
[Epoch 6; Iter   785/ 2483] train: loss: 0.0015454
[Epoch 6; Iter   815/ 2483] train: loss: 0.0016546
[Epoch 6; Iter   845/ 2483] train: loss: 0.0022528
[Epoch 6; Iter   875/ 2483] train: loss: 0.0022529
[Epoch 6; Iter   905/ 2483] train: loss: 0.0027184
[Epoch 6; Iter   935/ 2483] train: loss: 0.0018685
[Epoch 6; Iter   965/ 2483] train: loss: 0.0025501
[Epoch 6; Iter   995/ 2483] train: loss: 0.0020468
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0021413
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0034740
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0021898
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0020876
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0024812
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0927523
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0022380
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0020530
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0660841
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0016642
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0016660
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0019212
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0019652
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0990578
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0022910
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0016069
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0018608
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0018278
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0534905
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0012400
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0014808
[Epoch 6; Iter  1655/ 2483] train: loss: 0.1151016
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0018424
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0012112
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0015726
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0009565
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0014857
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0018783
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0013255
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0021816
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0016781
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0012756
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0014198
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0018602
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0014688
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0021851
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0769503
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0018711
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0635055
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0016003
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0013190
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0012959
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0013350
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0012097
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0009616
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0011175
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0009678
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0011654
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0019848
[Epoch 6] ogbg-molmuv: 0.027593 val loss: 0.015026
[Epoch 6] ogbg-molmuv: 0.055005 test loss: 0.018784
[Epoch 7; Iter    12/ 2483] train: loss: 0.0016518
[Epoch 7; Iter    42/ 2483] train: loss: 0.0014032
[Epoch 7; Iter    72/ 2483] train: loss: 0.0017468
[Epoch 7; Iter   102/ 2483] train: loss: 0.0038067
[Epoch 7; Iter   132/ 2483] train: loss: 0.0020745
[Epoch 7; Iter   162/ 2483] train: loss: 0.0021608
[Epoch 7; Iter   192/ 2483] train: loss: 0.0016545
[Epoch 7; Iter   222/ 2483] train: loss: 0.0017789
[Epoch 7; Iter   252/ 2483] train: loss: 0.0665480
[Epoch 7; Iter   282/ 2483] train: loss: 0.0018467
[Epoch 7; Iter   312/ 2483] train: loss: 0.0018709
[Epoch 7; Iter   342/ 2483] train: loss: 0.0014367
[Epoch 7; Iter   372/ 2483] train: loss: 0.0015783
[Epoch 7; Iter   402/ 2483] train: loss: 0.0012600
[Epoch 7; Iter   432/ 2483] train: loss: 0.0016935
[Epoch 7; Iter   462/ 2483] train: loss: 0.0014714
[Epoch 7; Iter   492/ 2483] train: loss: 0.0020918
[Epoch 7; Iter   522/ 2483] train: loss: 0.0022235
[Epoch 7; Iter   552/ 2483] train: loss: 0.0020649
[Epoch 7; Iter   582/ 2483] train: loss: 0.0851089
[Epoch 7; Iter   612/ 2483] train: loss: 0.0908318
[Epoch 7; Iter   642/ 2483] train: loss: 0.0016045
[Epoch 7; Iter   672/ 2483] train: loss: 0.0687396
[Epoch 7; Iter   702/ 2483] train: loss: 0.0016933
[Epoch 7; Iter   732/ 2483] train: loss: 0.0016829
[Epoch 7; Iter   762/ 2483] train: loss: 0.0020049
[Epoch 7; Iter   792/ 2483] train: loss: 0.0018663
[Epoch 7; Iter   822/ 2483] train: loss: 0.0599570
[Epoch 7; Iter   852/ 2483] train: loss: 0.0016943
[Epoch 7; Iter   882/ 2483] train: loss: 0.0015385
[Epoch 7; Iter   912/ 2483] train: loss: 0.0020007
[Epoch 7; Iter   942/ 2483] train: loss: 0.0024430
[Epoch 7; Iter   972/ 2483] train: loss: 0.0016792
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0688218
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0019744
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0018910
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0029722
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0020035
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0015278
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0013385
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0607703
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0022399
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0458361
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0014490
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0013822
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0017359
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0015742
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0016644
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0019353
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0015860
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0018665
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0011332
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0013664
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0022732
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0015865
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0017053
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0020516
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0041121
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0033153
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0014396
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0024618
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0015133
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0013373
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0019399
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0018914
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0014718
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0018904
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0015347
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0016925
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0022803
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0032969
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0012686
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0020083
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0643401
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0021711
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0016761
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0016808
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0021768
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0806378
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0501716
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0024236
[Epoch 6; Iter   185/ 2483] train: loss: 0.0017243
[Epoch 6; Iter   215/ 2483] train: loss: 0.0016770
[Epoch 6; Iter   245/ 2483] train: loss: 0.0715369
[Epoch 6; Iter   275/ 2483] train: loss: 0.0017713
[Epoch 6; Iter   305/ 2483] train: loss: 0.0020550
[Epoch 6; Iter   335/ 2483] train: loss: 0.0016874
[Epoch 6; Iter   365/ 2483] train: loss: 0.0020090
[Epoch 6; Iter   395/ 2483] train: loss: 0.0017030
[Epoch 6; Iter   425/ 2483] train: loss: 0.0036747
[Epoch 6; Iter   455/ 2483] train: loss: 0.0021920
[Epoch 6; Iter   485/ 2483] train: loss: 0.0019680
[Epoch 6; Iter   515/ 2483] train: loss: 0.0581018
[Epoch 6; Iter   545/ 2483] train: loss: 0.1712769
[Epoch 6; Iter   575/ 2483] train: loss: 0.0024391
[Epoch 6; Iter   605/ 2483] train: loss: 0.0024014
[Epoch 6; Iter   635/ 2483] train: loss: 0.0021406
[Epoch 6; Iter   665/ 2483] train: loss: 0.0022854
[Epoch 6; Iter   695/ 2483] train: loss: 0.0018063
[Epoch 6; Iter   725/ 2483] train: loss: 0.0860336
[Epoch 6; Iter   755/ 2483] train: loss: 0.0014734
[Epoch 6; Iter   785/ 2483] train: loss: 0.0017094
[Epoch 6; Iter   815/ 2483] train: loss: 0.0022018
[Epoch 6; Iter   845/ 2483] train: loss: 0.0014370
[Epoch 6; Iter   875/ 2483] train: loss: 0.0013676
[Epoch 6; Iter   905/ 2483] train: loss: 0.0790942
[Epoch 6; Iter   935/ 2483] train: loss: 0.0018240
[Epoch 6; Iter   965/ 2483] train: loss: 0.0023643
[Epoch 6; Iter   995/ 2483] train: loss: 0.0020557
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0034373
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0017830
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0021406
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0020097
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0019812
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0021410
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0014215
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0021070
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0726670
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0021562
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0017884
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0015080
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0526892
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0015579
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0019752
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0026268
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0020619
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0023904
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0636135
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0025439
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0015986
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0017840
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0014556
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0026896
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0015918
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0016190
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0012249
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0012846
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0019172
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0012218
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0017785
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0017240
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0014935
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0015378
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0020437
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0016266
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0017708
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0588890
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0022146
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0024898
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0020196
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0017002
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0023607
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0027354
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0028423
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0990455
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0687432
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0013088
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0015198
[Epoch 6] ogbg-molmuv: 0.035274 val loss: 0.015386
[Epoch 6] ogbg-molmuv: 0.020204 test loss: 0.024302
[Epoch 7; Iter    12/ 2483] train: loss: 0.0013891
[Epoch 7; Iter    42/ 2483] train: loss: 0.0014074
[Epoch 7; Iter    72/ 2483] train: loss: 0.0014131
[Epoch 7; Iter   102/ 2483] train: loss: 0.0015555
[Epoch 7; Iter   132/ 2483] train: loss: 0.0014100
[Epoch 7; Iter   162/ 2483] train: loss: 0.0624992
[Epoch 7; Iter   192/ 2483] train: loss: 0.0644309
[Epoch 7; Iter   222/ 2483] train: loss: 0.0018972
[Epoch 7; Iter   252/ 2483] train: loss: 0.0030890
[Epoch 7; Iter   282/ 2483] train: loss: 0.0018838
[Epoch 7; Iter   312/ 2483] train: loss: 0.0018287
[Epoch 7; Iter   342/ 2483] train: loss: 0.1051404
[Epoch 7; Iter   372/ 2483] train: loss: 0.0021584
[Epoch 7; Iter   402/ 2483] train: loss: 0.0020998
[Epoch 7; Iter   432/ 2483] train: loss: 0.0017444
[Epoch 7; Iter   462/ 2483] train: loss: 0.0014114
[Epoch 7; Iter   492/ 2483] train: loss: 0.0016224
[Epoch 7; Iter   522/ 2483] train: loss: 0.0016987
[Epoch 7; Iter   552/ 2483] train: loss: 0.0014889
[Epoch 7; Iter   582/ 2483] train: loss: 0.0011863
[Epoch 7; Iter   612/ 2483] train: loss: 0.0014953
[Epoch 7; Iter   642/ 2483] train: loss: 0.0010562
[Epoch 7; Iter   672/ 2483] train: loss: 0.0018231
[Epoch 7; Iter   702/ 2483] train: loss: 0.1029083
[Epoch 7; Iter   732/ 2483] train: loss: 0.0013780
[Epoch 7; Iter   762/ 2483] train: loss: 0.0923367
[Epoch 7; Iter   792/ 2483] train: loss: 0.0020917
[Epoch 7; Iter   822/ 2483] train: loss: 0.0015366
[Epoch 7; Iter   852/ 2483] train: loss: 0.0014796
[Epoch 7; Iter   882/ 2483] train: loss: 0.0016074
[Epoch 7; Iter   912/ 2483] train: loss: 0.0017172
[Epoch 7; Iter   942/ 2483] train: loss: 0.0022891
[Epoch 7; Iter   972/ 2483] train: loss: 0.0031622
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0019554
[Epoch 7; Iter  1032/ 2483] train: loss: 0.1790595
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0017600
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0670813
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0027896
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0020937
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0023353
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0020804
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0015462
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0449336
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0014282
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0014292
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0018031
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0012258
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0015438
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0017022
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0012817
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0015391
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0015112
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0017425
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0013427
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0013372
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0013278
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0015422
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0010360
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0011008
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0015758
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0015051
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0018598
[Epoch 7; Iter  1872/ 2483] train: loss: 0.1906171
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0018029
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0965183
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0019547
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0020354
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0016088
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0023765
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0019130
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0020017
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0028142
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0029745
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0025958
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0019911
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0716690
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0800111
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0024632
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0024919
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0018984
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0024113
[Epoch 4; Iter   471/ 2483] train: loss: 0.0015248
[Epoch 4; Iter   501/ 2483] train: loss: 0.0015576
[Epoch 4; Iter   531/ 2483] train: loss: 0.0018250
[Epoch 4; Iter   561/ 2483] train: loss: 0.1131193
[Epoch 4; Iter   591/ 2483] train: loss: 0.0017737
[Epoch 4; Iter   621/ 2483] train: loss: 0.0017682
[Epoch 4; Iter   651/ 2483] train: loss: 0.0017147
[Epoch 4; Iter   681/ 2483] train: loss: 0.0757077
[Epoch 4; Iter   711/ 2483] train: loss: 0.0594363
[Epoch 4; Iter   741/ 2483] train: loss: 0.0021131
[Epoch 4; Iter   771/ 2483] train: loss: 0.0029750
[Epoch 4; Iter   801/ 2483] train: loss: 0.0020090
[Epoch 4; Iter   831/ 2483] train: loss: 0.0019056
[Epoch 4; Iter   861/ 2483] train: loss: 0.0016409
[Epoch 4; Iter   891/ 2483] train: loss: 0.0019890
[Epoch 4; Iter   921/ 2483] train: loss: 0.0915547
[Epoch 4; Iter   951/ 2483] train: loss: 0.0021493
[Epoch 4; Iter   981/ 2483] train: loss: 0.0739257
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0021983
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0017885
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0021241
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0018535
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0019751
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0018609
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0924073
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0013890
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0014086
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0013061
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0016144
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0014252
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0013727
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0760918
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0017931
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0020020
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0016094
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0018196
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0015449
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0014996
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0018546
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0017149
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0015495
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0723609
[Epoch 4; Iter  1731/ 2483] train: loss: 0.1145840
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0018255
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0016067
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0024201
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0022243
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0020676
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0525794
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0028788
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0021140
[Epoch 4; Iter  2001/ 2483] train: loss: 0.1311155
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0016388
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0018938
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0017276
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0018108
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0805351
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0016231
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0016736
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0954990
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0729395
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0029933
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0022681
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0018990
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0663145
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0018909
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0022173
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0022282
[Epoch 4] ogbg-molmuv: 0.035785 val loss: 0.016187
[Epoch 4] ogbg-molmuv: 0.031105 test loss: 0.015806
[Epoch 5; Iter    28/ 2483] train: loss: 0.0013494
[Epoch 5; Iter    58/ 2483] train: loss: 0.0014458
[Epoch 5; Iter    88/ 2483] train: loss: 0.0015376
[Epoch 5; Iter   118/ 2483] train: loss: 0.0016977
[Epoch 5; Iter   148/ 2483] train: loss: 0.0017775
[Epoch 5; Iter   178/ 2483] train: loss: 0.0015157
[Epoch 5; Iter   208/ 2483] train: loss: 0.0018356
[Epoch 5; Iter   238/ 2483] train: loss: 0.0016419
[Epoch 5; Iter   268/ 2483] train: loss: 0.0018187
[Epoch 5; Iter   298/ 2483] train: loss: 0.0016337
[Epoch 5; Iter   328/ 2483] train: loss: 0.0014956
[Epoch 5; Iter   358/ 2483] train: loss: 0.0016694
[Epoch 5; Iter   388/ 2483] train: loss: 0.0017842
[Epoch 5; Iter   418/ 2483] train: loss: 0.0019987
[Epoch 5; Iter   448/ 2483] train: loss: 0.2086625
[Epoch 5; Iter   478/ 2483] train: loss: 0.0027678
[Epoch 5; Iter   508/ 2483] train: loss: 0.0022943
[Epoch 5; Iter   538/ 2483] train: loss: 0.0023166
[Epoch 5; Iter   568/ 2483] train: loss: 0.0019677
[Epoch 5; Iter   598/ 2483] train: loss: 0.0015527
[Epoch 5; Iter   628/ 2483] train: loss: 0.0014865
[Epoch 5; Iter   658/ 2483] train: loss: 0.0015582
[Epoch 5; Iter   688/ 2483] train: loss: 0.0015138
[Epoch 5; Iter   718/ 2483] train: loss: 0.0018031
[Epoch 5; Iter   748/ 2483] train: loss: 0.0020141
[Epoch 5; Iter   778/ 2483] train: loss: 0.0016497
[Epoch 5; Iter   808/ 2483] train: loss: 0.0015197
[Epoch 5; Iter   838/ 2483] train: loss: 0.0013896
[Epoch 5; Iter   868/ 2483] train: loss: 0.0014297
[Epoch 5; Iter   898/ 2483] train: loss: 0.0014849
[Epoch 5; Iter   928/ 2483] train: loss: 0.1001409
[Epoch 5; Iter   958/ 2483] train: loss: 0.0018765
[Epoch 5; Iter   988/ 2483] train: loss: 0.0018400
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0011950
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0014561
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0014010
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0012333
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0013586
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0731163
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0017241
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0015202
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0021812
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0015026
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0051351
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0017828
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0017879
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0020728
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0019537
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0015206
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0012838
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0017293
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0014004
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0019136
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0021457
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0014859
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0021112
[Epoch 5; Iter  1708/ 2483] train: loss: 0.2338778
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0027265
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0024727
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0018858
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0019236
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0018839
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0019852
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0023615
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0022448
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0021549
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0024070
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0020408
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0023142
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0020447
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0019342
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0019710
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0018974
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0696535
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0022042
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0028135
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0019675
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0017727
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0027359
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0022004
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0016600
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0017485
[Epoch 5] ogbg-molmuv: 0.013286 val loss: 0.015256
[Epoch 5] ogbg-molmuv: 0.019827 test loss: 0.015745
[Epoch 6; Iter     5/ 2483] train: loss: 0.0022279
[Epoch 6; Iter    35/ 2483] train: loss: 0.0022476
[Epoch 6; Iter    65/ 2483] train: loss: 0.0728080
[Epoch 6; Iter    95/ 2483] train: loss: 0.0021695
[Epoch 6; Iter   125/ 2483] train: loss: 0.0015983
[Epoch 6; Iter   155/ 2483] train: loss: 0.0016613
[Epoch 6; Iter   185/ 2483] train: loss: 0.0015203
[Epoch 6; Iter   215/ 2483] train: loss: 0.0017286
[Epoch 6; Iter   245/ 2483] train: loss: 0.0020091
[Epoch 6; Iter   275/ 2483] train: loss: 0.0802207
[Epoch 6; Iter   305/ 2483] train: loss: 0.0019109
[Epoch 6; Iter   335/ 2483] train: loss: 0.0680090
[Epoch 6; Iter   365/ 2483] train: loss: 0.0020340
[Epoch 6; Iter   395/ 2483] train: loss: 0.1136661
[Epoch 6; Iter   425/ 2483] train: loss: 0.0019599
[Epoch 6; Iter   455/ 2483] train: loss: 0.0022553
[Epoch 6; Iter   485/ 2483] train: loss: 0.0039715
[Epoch 6; Iter   515/ 2483] train: loss: 0.0024152
[Epoch 6; Iter   545/ 2483] train: loss: 0.0026224
[Epoch 6; Iter   575/ 2483] train: loss: 0.0019221
[Epoch 6; Iter   605/ 2483] train: loss: 0.0014792
[Epoch 6; Iter   635/ 2483] train: loss: 0.0014180
[Epoch 6; Iter   665/ 2483] train: loss: 0.0014577
[Epoch 6; Iter   695/ 2483] train: loss: 0.0017435
[Epoch 6; Iter   725/ 2483] train: loss: 0.0012789
[Epoch 6; Iter   755/ 2483] train: loss: 0.0014568
[Epoch 6; Iter   785/ 2483] train: loss: 0.0020136
[Epoch 6; Iter   815/ 2483] train: loss: 0.0014889
[Epoch 6; Iter   845/ 2483] train: loss: 0.0015961
[Epoch 6; Iter   875/ 2483] train: loss: 0.0014498
[Epoch 6; Iter   905/ 2483] train: loss: 0.0017576
[Epoch 6; Iter   935/ 2483] train: loss: 0.0015457
[Epoch 6; Iter   965/ 2483] train: loss: 0.0016016
[Epoch 6; Iter   995/ 2483] train: loss: 0.0017951
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0792605
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0023663
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0017101
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0019009
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0018449
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0020981
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0019201
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0018460
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0014375
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0024998
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0687145
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0018866
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0020373
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0023197
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0016756
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0023293
[Epoch 6; Iter  1505/ 2483] train: loss: 0.1588420
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0024438
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0021428
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0028281
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0707038
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0018311
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0016810
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0012732
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0015317
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0014839
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0020589
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0015293
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0018325
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0017912
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0015278
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0017658
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0646757
[Epoch 6; Iter  2015/ 2483] train: loss: 0.1221166
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0014057
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0016453
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0017866
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0022541
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0593630
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0022588
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0025417
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0014105
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0676487
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0023362
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0019647
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0016389
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0019540
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0013019
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0012189
[Epoch 6] ogbg-molmuv: 0.049798 val loss: 0.014683
[Epoch 6] ogbg-molmuv: 0.031332 test loss: 0.015848
[Epoch 7; Iter    12/ 2483] train: loss: 0.0013105
[Epoch 7; Iter    42/ 2483] train: loss: 0.0016280
[Epoch 7; Iter    72/ 2483] train: loss: 0.0014618
[Epoch 7; Iter   102/ 2483] train: loss: 0.0019939
[Epoch 7; Iter   132/ 2483] train: loss: 0.0349112
[Epoch 7; Iter   162/ 2483] train: loss: 0.0031283
[Epoch 7; Iter   192/ 2483] train: loss: 0.0021001
[Epoch 7; Iter   222/ 2483] train: loss: 0.0033075
[Epoch 7; Iter   252/ 2483] train: loss: 0.0024821
[Epoch 7; Iter   282/ 2483] train: loss: 0.0018586
[Epoch 7; Iter   312/ 2483] train: loss: 0.0013328
[Epoch 7; Iter   342/ 2483] train: loss: 0.0016928
[Epoch 7; Iter   372/ 2483] train: loss: 0.0014547
[Epoch 7; Iter   402/ 2483] train: loss: 0.0025386
[Epoch 7; Iter   432/ 2483] train: loss: 0.0015470
[Epoch 7; Iter   462/ 2483] train: loss: 0.0012659
[Epoch 7; Iter   492/ 2483] train: loss: 0.0013231
[Epoch 7; Iter   522/ 2483] train: loss: 0.0013141
[Epoch 7; Iter   552/ 2483] train: loss: 0.0017157
[Epoch 7; Iter   582/ 2483] train: loss: 0.0017070
[Epoch 7; Iter   612/ 2483] train: loss: 0.0018167
[Epoch 7; Iter   642/ 2483] train: loss: 0.0016131
[Epoch 7; Iter   672/ 2483] train: loss: 0.0012602
[Epoch 7; Iter   702/ 2483] train: loss: 0.0017366
[Epoch 7; Iter   732/ 2483] train: loss: 0.0011558
[Epoch 7; Iter   762/ 2483] train: loss: 0.0015584
[Epoch 7; Iter   792/ 2483] train: loss: 0.0013597
[Epoch 7; Iter   822/ 2483] train: loss: 0.0017622
[Epoch 7; Iter   852/ 2483] train: loss: 0.0558291
[Epoch 7; Iter   882/ 2483] train: loss: 0.0016224
[Epoch 7; Iter   912/ 2483] train: loss: 0.0014315
[Epoch 7; Iter   942/ 2483] train: loss: 0.0014498
[Epoch 7; Iter   972/ 2483] train: loss: 0.0017887
[Epoch 7; Iter  1002/ 2483] train: loss: 0.1095540
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0681831
[Epoch 7; Iter  1062/ 2483] train: loss: 0.1638414
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0021865
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0015388
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0864426
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0012764
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0016527
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0015995
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0018773
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0016570
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0018092
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0024958
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0019426
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0661062
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0976586
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0024765
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0020727
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0019151
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0719269
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0832267
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0013911
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0724501
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0022921
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0019303
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0016366
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0026862
[Epoch 7; Iter  1812/ 2483] train: loss: 0.1384180
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0020931
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0023074
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0023827
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0023981
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0023807
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0017861
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0019060
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0019788
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0017278
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0012800
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0014823
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0012930
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0013567
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0658408
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0011113
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0027905
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0037161
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0030481
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0019953
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0019470
[Epoch 4; Iter   471/ 2483] train: loss: 0.0023190
[Epoch 4; Iter   501/ 2483] train: loss: 0.0013856
[Epoch 4; Iter   531/ 2483] train: loss: 0.0014247
[Epoch 4; Iter   561/ 2483] train: loss: 0.0017084
[Epoch 4; Iter   591/ 2483] train: loss: 0.0018342
[Epoch 4; Iter   621/ 2483] train: loss: 0.0014496
[Epoch 4; Iter   651/ 2483] train: loss: 0.0014831
[Epoch 4; Iter   681/ 2483] train: loss: 0.0012442
[Epoch 4; Iter   711/ 2483] train: loss: 0.0017544
[Epoch 4; Iter   741/ 2483] train: loss: 0.0017687
[Epoch 4; Iter   771/ 2483] train: loss: 0.0012516
[Epoch 4; Iter   801/ 2483] train: loss: 0.0016625
[Epoch 4; Iter   831/ 2483] train: loss: 0.0015320
[Epoch 4; Iter   861/ 2483] train: loss: 0.0728814
[Epoch 4; Iter   891/ 2483] train: loss: 0.0014258
[Epoch 4; Iter   921/ 2483] train: loss: 0.0017937
[Epoch 4; Iter   951/ 2483] train: loss: 0.0017084
[Epoch 4; Iter   981/ 2483] train: loss: 0.0021671
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0025182
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0019974
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0021192
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0021409
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0026225
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0025032
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0024507
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0020481
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0032777
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0023950
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0024175
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0020383
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0018263
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0015138
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0015777
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0013780
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0013454
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0017988
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0014442
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0014132
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0764526
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0016640
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0020483
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0014480
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0016859
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0012662
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0015481
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0704506
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0018030
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0015742
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0018053
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0919806
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0021446
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0013715
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0014568
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0024338
[Epoch 4; Iter  2091/ 2483] train: loss: 0.1018675
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0647058
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0022937
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0021847
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0022452
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0022999
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0020675
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0016819
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0018604
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0016861
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0015417
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0017507
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0018353
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0022361
[Epoch 4] ogbg-molmuv: 0.014420 val loss: 0.015171
[Epoch 4] ogbg-molmuv: 0.039301 test loss: 0.015428
[Epoch 5; Iter    28/ 2483] train: loss: 0.0018871
[Epoch 5; Iter    58/ 2483] train: loss: 0.0020738
[Epoch 5; Iter    88/ 2483] train: loss: 0.0602889
[Epoch 5; Iter   118/ 2483] train: loss: 0.0808793
[Epoch 5; Iter   148/ 2483] train: loss: 0.0027908
[Epoch 5; Iter   178/ 2483] train: loss: 0.0020847
[Epoch 5; Iter   208/ 2483] train: loss: 0.0015105
[Epoch 5; Iter   238/ 2483] train: loss: 0.0017073
[Epoch 5; Iter   268/ 2483] train: loss: 0.0019767
[Epoch 5; Iter   298/ 2483] train: loss: 0.0015815
[Epoch 5; Iter   328/ 2483] train: loss: 0.0020299
[Epoch 5; Iter   358/ 2483] train: loss: 0.0015772
[Epoch 5; Iter   388/ 2483] train: loss: 0.0855250
[Epoch 5; Iter   418/ 2483] train: loss: 0.0021090
[Epoch 5; Iter   448/ 2483] train: loss: 0.0022545
[Epoch 5; Iter   478/ 2483] train: loss: 0.0020090
[Epoch 5; Iter   508/ 2483] train: loss: 0.0020018
[Epoch 5; Iter   538/ 2483] train: loss: 0.0017430
[Epoch 5; Iter   568/ 2483] train: loss: 0.0976240
[Epoch 5; Iter   598/ 2483] train: loss: 0.0016805
[Epoch 5; Iter   628/ 2483] train: loss: 0.0789082
[Epoch 5; Iter   658/ 2483] train: loss: 0.0019410
[Epoch 5; Iter   688/ 2483] train: loss: 0.0022087
[Epoch 5; Iter   718/ 2483] train: loss: 0.0018964
[Epoch 5; Iter   748/ 2483] train: loss: 0.0020960
[Epoch 5; Iter   778/ 2483] train: loss: 0.0018913
[Epoch 5; Iter   808/ 2483] train: loss: 0.0014778
[Epoch 5; Iter   838/ 2483] train: loss: 0.0015611
[Epoch 5; Iter   868/ 2483] train: loss: 0.0017843
[Epoch 5; Iter   898/ 2483] train: loss: 0.0020007
[Epoch 5; Iter   928/ 2483] train: loss: 0.0620924
[Epoch 5; Iter   958/ 2483] train: loss: 0.0021141
[Epoch 5; Iter   988/ 2483] train: loss: 0.0018754
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0025399
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0016894
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0017901
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0015946
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0016025
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0015727
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0014315
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0014883
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0020059
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0013128
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0012989
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0011923
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0694410
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0013761
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0013662
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0013508
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0858860
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0016107
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0019818
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0016529
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0020455
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0021641
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0020053
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0015911
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0013437
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0015778
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0022833
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0055519
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0724040
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0018958
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0684746
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0018594
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0014501
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0018837
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0028603
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0033643
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0021451
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0957014
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0022938
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0019927
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0015834
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0015006
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0015923
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0018408
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0017219
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0017434
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0018881
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0018676
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0014258
[Epoch 5] ogbg-molmuv: 0.019023 val loss: 0.015159
[Epoch 5] ogbg-molmuv: 0.046987 test loss: 0.015867
[Epoch 6; Iter     5/ 2483] train: loss: 0.0018724
[Epoch 6; Iter    35/ 2483] train: loss: 0.0567107
[Epoch 6; Iter    65/ 2483] train: loss: 0.0022661
[Epoch 6; Iter    95/ 2483] train: loss: 0.0017006
[Epoch 6; Iter   125/ 2483] train: loss: 0.0020750
[Epoch 6; Iter   155/ 2483] train: loss: 0.0021037
[Epoch 4; Iter   471/ 2483] train: loss: 0.0028268
[Epoch 4; Iter   501/ 2483] train: loss: 0.0014106
[Epoch 4; Iter   531/ 2483] train: loss: 0.0015778
[Epoch 4; Iter   561/ 2483] train: loss: 0.0016550
[Epoch 4; Iter   591/ 2483] train: loss: 0.0018065
[Epoch 4; Iter   621/ 2483] train: loss: 0.0015010
[Epoch 4; Iter   651/ 2483] train: loss: 0.0012884
[Epoch 4; Iter   681/ 2483] train: loss: 0.0012325
[Epoch 4; Iter   711/ 2483] train: loss: 0.0016115
[Epoch 4; Iter   741/ 2483] train: loss: 0.0019380
[Epoch 4; Iter   771/ 2483] train: loss: 0.0013330
[Epoch 4; Iter   801/ 2483] train: loss: 0.0016177
[Epoch 4; Iter   831/ 2483] train: loss: 0.0014278
[Epoch 4; Iter   861/ 2483] train: loss: 0.0782531
[Epoch 4; Iter   891/ 2483] train: loss: 0.0013323
[Epoch 4; Iter   921/ 2483] train: loss: 0.0015756
[Epoch 4; Iter   951/ 2483] train: loss: 0.0017597
[Epoch 4; Iter   981/ 2483] train: loss: 0.0023171
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0021242
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0020656
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0021859
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0021807
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0025659
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0024659
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0023123
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0018242
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0025907
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0022968
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0023427
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0020141
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0015378
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0015268
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0015287
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0014790
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0015797
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0019975
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0014030
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0016557
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0706033
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0021395
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0019001
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0015906
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0015753
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0012865
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0016021
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0770053
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0021022
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0015948
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0017494
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0954491
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0016048
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0014467
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0015272
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0031066
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0981637
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0690721
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0025208
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0021098
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0018992
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0016729
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0015517
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0018141
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0016913
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0016990
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0016207
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0017770
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0017781
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0019871
[Epoch 4] ogbg-molmuv: 0.031570 val loss: 0.015179
[Epoch 4] ogbg-molmuv: 0.051221 test loss: 0.015497
[Epoch 5; Iter    28/ 2483] train: loss: 0.0018282
[Epoch 5; Iter    58/ 2483] train: loss: 0.0019933
[Epoch 5; Iter    88/ 2483] train: loss: 0.0644690
[Epoch 5; Iter   118/ 2483] train: loss: 0.0833682
[Epoch 5; Iter   148/ 2483] train: loss: 0.0026885
[Epoch 5; Iter   178/ 2483] train: loss: 0.0019040
[Epoch 5; Iter   208/ 2483] train: loss: 0.0018337
[Epoch 5; Iter   238/ 2483] train: loss: 0.0019551
[Epoch 5; Iter   268/ 2483] train: loss: 0.0021562
[Epoch 5; Iter   298/ 2483] train: loss: 0.0017692
[Epoch 5; Iter   328/ 2483] train: loss: 0.0026560
[Epoch 5; Iter   358/ 2483] train: loss: 0.0018645
[Epoch 5; Iter   388/ 2483] train: loss: 0.0992648
[Epoch 5; Iter   418/ 2483] train: loss: 0.0018594
[Epoch 5; Iter   448/ 2483] train: loss: 0.0017749
[Epoch 5; Iter   478/ 2483] train: loss: 0.0020420
[Epoch 5; Iter   508/ 2483] train: loss: 0.0018988
[Epoch 5; Iter   538/ 2483] train: loss: 0.0016530
[Epoch 5; Iter   568/ 2483] train: loss: 0.1022418
[Epoch 5; Iter   598/ 2483] train: loss: 0.0015849
[Epoch 5; Iter   628/ 2483] train: loss: 0.0834771
[Epoch 5; Iter   658/ 2483] train: loss: 0.0019720
[Epoch 5; Iter   688/ 2483] train: loss: 0.0021012
[Epoch 5; Iter   718/ 2483] train: loss: 0.0020611
[Epoch 5; Iter   748/ 2483] train: loss: 0.0018428
[Epoch 5; Iter   778/ 2483] train: loss: 0.0017136
[Epoch 5; Iter   808/ 2483] train: loss: 0.0015202
[Epoch 5; Iter   838/ 2483] train: loss: 0.0015942
[Epoch 5; Iter   868/ 2483] train: loss: 0.0017865
[Epoch 5; Iter   898/ 2483] train: loss: 0.0019494
[Epoch 5; Iter   928/ 2483] train: loss: 0.0635002
[Epoch 5; Iter   958/ 2483] train: loss: 0.0020000
[Epoch 5; Iter   988/ 2483] train: loss: 0.0021005
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0028210
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0015935
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0018089
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0016105
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0016189
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0016371
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0016261
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0018189
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0022097
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0012452
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0013416
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0013894
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0676760
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0014481
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0013402
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0013189
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0742002
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0014657
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0018089
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0014682
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0017978
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0015054
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0025317
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0013647
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0013844
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0014569
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0020673
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0041409
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0867437
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0019918
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0611515
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0025447
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0018283
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0022723
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0020100
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0026731
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0021005
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0931678
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0019968
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0021002
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0016046
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0016031
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0015393
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0017176
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0015234
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0016762
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0017159
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0021648
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0014042
[Epoch 5] ogbg-molmuv: 0.037981 val loss: 0.015252
[Epoch 5] ogbg-molmuv: 0.059899 test loss: 0.016156
[Epoch 6; Iter     5/ 2483] train: loss: 0.0020132
[Epoch 6; Iter    35/ 2483] train: loss: 0.0660452
[Epoch 6; Iter    65/ 2483] train: loss: 0.0022917
[Epoch 6; Iter    95/ 2483] train: loss: 0.0019271
[Epoch 6; Iter   125/ 2483] train: loss: 0.0019992
[Epoch 6; Iter   155/ 2483] train: loss: 0.0020225
[Epoch 4; Iter   471/ 2483] train: loss: 0.0012866
[Epoch 4; Iter   501/ 2483] train: loss: 0.0017713
[Epoch 4; Iter   531/ 2483] train: loss: 0.0013603
[Epoch 4; Iter   561/ 2483] train: loss: 0.0015849
[Epoch 4; Iter   591/ 2483] train: loss: 0.0012839
[Epoch 4; Iter   621/ 2483] train: loss: 0.0013622
[Epoch 4; Iter   651/ 2483] train: loss: 0.1862692
[Epoch 4; Iter   681/ 2483] train: loss: 0.0857149
[Epoch 4; Iter   711/ 2483] train: loss: 0.0021597
[Epoch 4; Iter   741/ 2483] train: loss: 0.0021566
[Epoch 4; Iter   771/ 2483] train: loss: 0.0020693
[Epoch 4; Iter   801/ 2483] train: loss: 0.0603812
[Epoch 4; Iter   831/ 2483] train: loss: 0.0015516
[Epoch 4; Iter   861/ 2483] train: loss: 0.0016322
[Epoch 4; Iter   891/ 2483] train: loss: 0.0018980
[Epoch 4; Iter   921/ 2483] train: loss: 0.0019078
[Epoch 4; Iter   951/ 2483] train: loss: 0.0020877
[Epoch 4; Iter   981/ 2483] train: loss: 0.0020759
[Epoch 4; Iter  1011/ 2483] train: loss: 0.1262379
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0021283
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0020867
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0017202
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0015701
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0018078
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0019934
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0018731
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0870174
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0020734
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0016911
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0017698
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0017998
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0749723
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0017411
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0024060
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0014355
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0018435
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0020305
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0028164
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0790340
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0018865
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0016169
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0036009
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0034481
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0702213
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0016272
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0018895
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0024117
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0017709
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0018055
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0017510
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0020784
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0016990
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0015963
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0017415
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0013503
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0017336
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0016271
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0023404
[Epoch 4; Iter  2211/ 2483] train: loss: 0.1493987
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0874419
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0022376
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0017007
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0835042
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0015995
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0021741
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0021336
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0019121
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0017825
[Epoch 4] ogbg-molmuv: 0.012478 val loss: 0.041664
[Epoch 4] ogbg-molmuv: 0.017443 test loss: 0.040633
[Epoch 5; Iter    28/ 2483] train: loss: 0.0016993
[Epoch 5; Iter    58/ 2483] train: loss: 0.0018211
[Epoch 5; Iter    88/ 2483] train: loss: 0.0019210
[Epoch 5; Iter   118/ 2483] train: loss: 0.0015650
[Epoch 5; Iter   148/ 2483] train: loss: 0.0016630
[Epoch 5; Iter   178/ 2483] train: loss: 0.0013317
[Epoch 5; Iter   208/ 2483] train: loss: 0.0013279
[Epoch 5; Iter   238/ 2483] train: loss: 0.0016396
[Epoch 5; Iter   268/ 2483] train: loss: 0.0016838
[Epoch 5; Iter   298/ 2483] train: loss: 0.0020636
[Epoch 5; Iter   328/ 2483] train: loss: 0.0017794
[Epoch 5; Iter   358/ 2483] train: loss: 0.0016561
[Epoch 5; Iter   388/ 2483] train: loss: 0.0015480
[Epoch 5; Iter   418/ 2483] train: loss: 0.0017388
[Epoch 5; Iter   448/ 2483] train: loss: 0.0020602
[Epoch 5; Iter   478/ 2483] train: loss: 0.0019299
[Epoch 5; Iter   508/ 2483] train: loss: 0.0030444
[Epoch 5; Iter   538/ 2483] train: loss: 0.0023112
[Epoch 5; Iter   568/ 2483] train: loss: 0.0849872
[Epoch 5; Iter   598/ 2483] train: loss: 0.0698563
[Epoch 5; Iter   628/ 2483] train: loss: 0.0026609
[Epoch 5; Iter   658/ 2483] train: loss: 0.0019362
[Epoch 5; Iter   688/ 2483] train: loss: 0.0017672
[Epoch 5; Iter   718/ 2483] train: loss: 0.0019106
[Epoch 5; Iter   748/ 2483] train: loss: 0.0021568
[Epoch 5; Iter   778/ 2483] train: loss: 0.0018402
[Epoch 5; Iter   808/ 2483] train: loss: 0.0018494
[Epoch 5; Iter   838/ 2483] train: loss: 0.0017909
[Epoch 5; Iter   868/ 2483] train: loss: 0.0017532
[Epoch 5; Iter   898/ 2483] train: loss: 0.0023193
[Epoch 5; Iter   928/ 2483] train: loss: 0.0020431
[Epoch 5; Iter   958/ 2483] train: loss: 0.0018970
[Epoch 5; Iter   988/ 2483] train: loss: 0.0017539
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0017419
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0014443
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0014857
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0014341
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0014711
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0014526
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0014962
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0013899
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0017001
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0019034
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0018982
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0020049
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0023024
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0025448
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0923408
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0026170
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0019295
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0025107
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0017636
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0022362
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0018047
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0717475
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0021651
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0029124
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0017283
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0512068
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0019794
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0017146
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0780780
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0017950
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0785610
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0017494
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0019980
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0015464
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0013618
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0547480
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0021417
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0015473
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0017434
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0021676
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0018238
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0021370
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0017699
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0016216
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0018057
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0015283
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0022862
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0023386
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0910596
[Epoch 5] ogbg-molmuv: 0.014140 val loss: 0.015233
[Epoch 5] ogbg-molmuv: 0.015985 test loss: 0.015615
[Epoch 6; Iter     5/ 2483] train: loss: 0.0017368
[Epoch 6; Iter    35/ 2483] train: loss: 0.0020484
[Epoch 6; Iter    65/ 2483] train: loss: 0.0021066
[Epoch 6; Iter    95/ 2483] train: loss: 0.0910096
[Epoch 6; Iter   125/ 2483] train: loss: 0.0018844
[Epoch 6; Iter   155/ 2483] train: loss: 0.0017845
[Epoch 4; Iter   471/ 2483] train: loss: 0.0014280
[Epoch 4; Iter   501/ 2483] train: loss: 0.0016390
[Epoch 4; Iter   531/ 2483] train: loss: 0.0018081
[Epoch 4; Iter   561/ 2483] train: loss: 0.1168694
[Epoch 4; Iter   591/ 2483] train: loss: 0.0020087
[Epoch 4; Iter   621/ 2483] train: loss: 0.0019221
[Epoch 4; Iter   651/ 2483] train: loss: 0.0015067
[Epoch 4; Iter   681/ 2483] train: loss: 0.0892566
[Epoch 4; Iter   711/ 2483] train: loss: 0.0658379
[Epoch 4; Iter   741/ 2483] train: loss: 0.0017463
[Epoch 4; Iter   771/ 2483] train: loss: 0.0029105
[Epoch 4; Iter   801/ 2483] train: loss: 0.0024515
[Epoch 4; Iter   831/ 2483] train: loss: 0.0023690
[Epoch 4; Iter   861/ 2483] train: loss: 0.0018182
[Epoch 4; Iter   891/ 2483] train: loss: 0.0018440
[Epoch 4; Iter   921/ 2483] train: loss: 0.0888536
[Epoch 4; Iter   951/ 2483] train: loss: 0.0027131
[Epoch 4; Iter   981/ 2483] train: loss: 0.0693073
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0018651
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0013837
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0016355
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0016658
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0021465
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0023029
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0929746
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0013581
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0012393
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0011807
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0013588
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0012825
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0013214
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0683996
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0018278
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0020901
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0015636
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0019612
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0016068
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0015664
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0025700
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0019557
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0016258
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0775664
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0871773
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0018335
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0017208
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0028772
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0027205
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0023848
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0452742
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0028457
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0020987
[Epoch 4; Iter  2001/ 2483] train: loss: 0.1367253
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0015119
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0015943
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0018840
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0016810
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0897883
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0013972
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0017788
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0805374
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0720451
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0031249
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0024372
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0022620
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0746397
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0022948
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0019196
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0023488
[Epoch 4] ogbg-molmuv: 0.024458 val loss: 0.021525
[Epoch 4] ogbg-molmuv: 0.028708 test loss: 0.023658
[Epoch 5; Iter    28/ 2483] train: loss: 0.0013587
[Epoch 5; Iter    58/ 2483] train: loss: 0.0015529
[Epoch 5; Iter    88/ 2483] train: loss: 0.0016005
[Epoch 5; Iter   118/ 2483] train: loss: 0.0017191
[Epoch 5; Iter   148/ 2483] train: loss: 0.0017387
[Epoch 5; Iter   178/ 2483] train: loss: 0.0013769
[Epoch 5; Iter   208/ 2483] train: loss: 0.0019927
[Epoch 5; Iter   238/ 2483] train: loss: 0.0018631
[Epoch 5; Iter   268/ 2483] train: loss: 0.0018679
[Epoch 5; Iter   298/ 2483] train: loss: 0.0016399
[Epoch 5; Iter   328/ 2483] train: loss: 0.0014294
[Epoch 5; Iter   358/ 2483] train: loss: 0.0016783
[Epoch 5; Iter   388/ 2483] train: loss: 0.0018384
[Epoch 5; Iter   418/ 2483] train: loss: 0.0019435
[Epoch 5; Iter   448/ 2483] train: loss: 0.2192774
[Epoch 5; Iter   478/ 2483] train: loss: 0.0023492
[Epoch 5; Iter   508/ 2483] train: loss: 0.0026745
[Epoch 5; Iter   538/ 2483] train: loss: 0.0023991
[Epoch 5; Iter   568/ 2483] train: loss: 0.0019889
[Epoch 5; Iter   598/ 2483] train: loss: 0.0014688
[Epoch 5; Iter   628/ 2483] train: loss: 0.0014916
[Epoch 5; Iter   658/ 2483] train: loss: 0.0016674
[Epoch 5; Iter   688/ 2483] train: loss: 0.0015737
[Epoch 5; Iter   718/ 2483] train: loss: 0.0017201
[Epoch 5; Iter   748/ 2483] train: loss: 0.0020814
[Epoch 5; Iter   778/ 2483] train: loss: 0.0014488
[Epoch 5; Iter   808/ 2483] train: loss: 0.0015037
[Epoch 5; Iter   838/ 2483] train: loss: 0.0012866
[Epoch 5; Iter   868/ 2483] train: loss: 0.0014741
[Epoch 5; Iter   898/ 2483] train: loss: 0.0015662
[Epoch 5; Iter   928/ 2483] train: loss: 0.0965405
[Epoch 5; Iter   958/ 2483] train: loss: 0.0019764
[Epoch 5; Iter   988/ 2483] train: loss: 0.0014706
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0010469
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0012941
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0012699
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0011949
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0015343
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0709588
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0017706
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0014728
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0018429
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0014674
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0017917
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0018187
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0017507
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0024892
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0021933
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0015603
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0011742
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0013911
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0012822
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0017166
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0019595
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0017130
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0021807
[Epoch 5; Iter  1708/ 2483] train: loss: 0.2461203
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0036719
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0021414
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0017366
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0019555
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0018635
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0018618
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0024804
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0024556
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0022900
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0023231
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0020075
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0023979
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0021368
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0017716
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0026233
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0018903
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0755973
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0022387
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0024919
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0018842
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0018380
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0021818
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0019948
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0018808
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0018956
[Epoch 5] ogbg-molmuv: 0.009805 val loss: 0.016605
[Epoch 5] ogbg-molmuv: 0.014602 test loss: 0.027634
[Epoch 6; Iter     5/ 2483] train: loss: 0.0022777
[Epoch 6; Iter    35/ 2483] train: loss: 0.0020036
[Epoch 6; Iter    65/ 2483] train: loss: 0.0729847
[Epoch 6; Iter    95/ 2483] train: loss: 0.0022234
[Epoch 6; Iter   125/ 2483] train: loss: 0.0017330
[Epoch 6; Iter   155/ 2483] train: loss: 0.0021628
[Epoch 4; Iter   471/ 2483] train: loss: 0.0014436
[Epoch 4; Iter   501/ 2483] train: loss: 0.0014772
[Epoch 4; Iter   531/ 2483] train: loss: 0.0018874
[Epoch 4; Iter   561/ 2483] train: loss: 0.1183241
[Epoch 4; Iter   591/ 2483] train: loss: 0.0020668
[Epoch 4; Iter   621/ 2483] train: loss: 0.0022139
[Epoch 4; Iter   651/ 2483] train: loss: 0.0017290
[Epoch 4; Iter   681/ 2483] train: loss: 0.0984857
[Epoch 4; Iter   711/ 2483] train: loss: 0.0649587
[Epoch 4; Iter   741/ 2483] train: loss: 0.0019660
[Epoch 4; Iter   771/ 2483] train: loss: 0.0018285
[Epoch 4; Iter   801/ 2483] train: loss: 0.0022537
[Epoch 4; Iter   831/ 2483] train: loss: 0.0023585
[Epoch 4; Iter   861/ 2483] train: loss: 0.0020305
[Epoch 4; Iter   891/ 2483] train: loss: 0.0018196
[Epoch 4; Iter   921/ 2483] train: loss: 0.0911519
[Epoch 4; Iter   951/ 2483] train: loss: 0.0022322
[Epoch 4; Iter   981/ 2483] train: loss: 0.0754144
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0025825
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0018017
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0017302
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0016243
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0015323
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0016310
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0895022
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0014577
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0014642
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0014292
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0015320
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0013948
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0014377
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0743191
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0016076
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0022379
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0020252
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0020259
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0017989
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0017918
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0020957
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0017846
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0019647
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0853696
[Epoch 4; Iter  1731/ 2483] train: loss: 0.1067438
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0018253
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0016823
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0023822
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0021681
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0020996
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0604717
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0021933
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0019000
[Epoch 4; Iter  2001/ 2483] train: loss: 0.1400632
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0016275
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0018782
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0018580
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0020515
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0801082
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0015590
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0017858
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0906144
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0688198
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0024382
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0019865
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0017493
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0632071
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0017637
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0017798
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0020723
[Epoch 4] ogbg-molmuv: 0.006309 val loss: 0.059883
[Epoch 4] ogbg-molmuv: 0.005973 test loss: 0.052491
[Epoch 5; Iter    28/ 2483] train: loss: 0.0015644
[Epoch 5; Iter    58/ 2483] train: loss: 0.0015410
[Epoch 5; Iter    88/ 2483] train: loss: 0.0015118
[Epoch 5; Iter   118/ 2483] train: loss: 0.0018265
[Epoch 5; Iter   148/ 2483] train: loss: 0.0016590
[Epoch 5; Iter   178/ 2483] train: loss: 0.0014700
[Epoch 5; Iter   208/ 2483] train: loss: 0.0018675
[Epoch 5; Iter   238/ 2483] train: loss: 0.0021140
[Epoch 5; Iter   268/ 2483] train: loss: 0.0017717
[Epoch 5; Iter   298/ 2483] train: loss: 0.0016019
[Epoch 5; Iter   328/ 2483] train: loss: 0.0013701
[Epoch 5; Iter   358/ 2483] train: loss: 0.0016752
[Epoch 5; Iter   388/ 2483] train: loss: 0.0018316
[Epoch 5; Iter   418/ 2483] train: loss: 0.0025842
[Epoch 5; Iter   448/ 2483] train: loss: 0.2215012
[Epoch 5; Iter   478/ 2483] train: loss: 0.0046841
[Epoch 5; Iter   508/ 2483] train: loss: 0.0025306
[Epoch 5; Iter   538/ 2483] train: loss: 0.0023221
[Epoch 5; Iter   568/ 2483] train: loss: 0.0019532
[Epoch 5; Iter   598/ 2483] train: loss: 0.0014552
[Epoch 5; Iter   628/ 2483] train: loss: 0.0015904
[Epoch 5; Iter   658/ 2483] train: loss: 0.0021293
[Epoch 5; Iter   688/ 2483] train: loss: 0.0015639
[Epoch 5; Iter   718/ 2483] train: loss: 0.0019100
[Epoch 5; Iter   748/ 2483] train: loss: 0.0020302
[Epoch 5; Iter   778/ 2483] train: loss: 0.0016930
[Epoch 5; Iter   808/ 2483] train: loss: 0.0015974
[Epoch 5; Iter   838/ 2483] train: loss: 0.0013856
[Epoch 5; Iter   868/ 2483] train: loss: 0.0014594
[Epoch 5; Iter   898/ 2483] train: loss: 0.0017064
[Epoch 5; Iter   928/ 2483] train: loss: 0.0988368
[Epoch 5; Iter   958/ 2483] train: loss: 0.0019373
[Epoch 5; Iter   988/ 2483] train: loss: 0.0018192
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0011425
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0014076
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0012455
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0013004
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0015710
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0739876
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0025631
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0016027
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0018067
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0015955
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0016876
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0016646
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0016842
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0021066
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0021292
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0016961
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0013234
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0015067
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0013407
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0017623
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0023503
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0018326
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0025265
[Epoch 5; Iter  1708/ 2483] train: loss: 0.2294717
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0023466
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0019195
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0015598
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0017638
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0018175
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0017526
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0025783
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0025102
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0020456
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0022718
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0016889
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0024838
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0022151
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0021567
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0021297
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0019058
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0676278
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0024189
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0022532
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0019149
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0016130
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0022989
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0024832
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0021212
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0016795
[Epoch 5] ogbg-molmuv: 0.011791 val loss: 0.050633
[Epoch 5] ogbg-molmuv: 0.017034 test loss: 0.053839
[Epoch 6; Iter     5/ 2483] train: loss: 0.0024735
[Epoch 6; Iter    35/ 2483] train: loss: 0.0020639
[Epoch 6; Iter    65/ 2483] train: loss: 0.0857606
[Epoch 6; Iter    95/ 2483] train: loss: 0.0021624
[Epoch 6; Iter   125/ 2483] train: loss: 0.0016922
[Epoch 6; Iter   155/ 2483] train: loss: 0.0015152
[Epoch 4; Iter   471/ 2483] train: loss: 0.0031904
[Epoch 4; Iter   501/ 2483] train: loss: 0.0013709
[Epoch 4; Iter   531/ 2483] train: loss: 0.0015839
[Epoch 4; Iter   561/ 2483] train: loss: 0.0015850
[Epoch 4; Iter   591/ 2483] train: loss: 0.0019467
[Epoch 4; Iter   621/ 2483] train: loss: 0.0014616
[Epoch 4; Iter   651/ 2483] train: loss: 0.0013824
[Epoch 4; Iter   681/ 2483] train: loss: 0.0011168
[Epoch 4; Iter   711/ 2483] train: loss: 0.0017105
[Epoch 4; Iter   741/ 2483] train: loss: 0.0015849
[Epoch 4; Iter   771/ 2483] train: loss: 0.0013546
[Epoch 4; Iter   801/ 2483] train: loss: 0.0016936
[Epoch 4; Iter   831/ 2483] train: loss: 0.0015945
[Epoch 4; Iter   861/ 2483] train: loss: 0.0764557
[Epoch 4; Iter   891/ 2483] train: loss: 0.0014537
[Epoch 4; Iter   921/ 2483] train: loss: 0.0015105
[Epoch 4; Iter   951/ 2483] train: loss: 0.0018936
[Epoch 4; Iter   981/ 2483] train: loss: 0.0021164
[Epoch 4; Iter  1011/ 2483] train: loss: 0.0026017
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0019840
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0025086
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0020907
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0024702
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0022814
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0021070
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0018851
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0024397
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0022840
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0022738
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0020480
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0016765
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0015269
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0016137
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0014559
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0014097
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0018146
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0015707
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0015473
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0646663
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0017612
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0021700
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0017338
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0016320
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0014704
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0018004
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0817525
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0016575
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0018953
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0019312
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0906142
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0016593
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0013792
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0015639
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0030717
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0959198
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0718314
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0023794
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0019584
[Epoch 4; Iter  2211/ 2483] train: loss: 0.0017634
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0021714
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0024437
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0019140
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0020168
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0018297
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0016345
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0017671
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0018769
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0018521
[Epoch 4] ogbg-molmuv: 0.009339 val loss: 0.015537
[Epoch 4] ogbg-molmuv: 0.056114 test loss: 0.015774
[Epoch 5; Iter    28/ 2483] train: loss: 0.0019252
[Epoch 5; Iter    58/ 2483] train: loss: 0.0018546
[Epoch 5; Iter    88/ 2483] train: loss: 0.0682094
[Epoch 5; Iter   118/ 2483] train: loss: 0.0820532
[Epoch 5; Iter   148/ 2483] train: loss: 0.0024732
[Epoch 5; Iter   178/ 2483] train: loss: 0.0021323
[Epoch 5; Iter   208/ 2483] train: loss: 0.0015700
[Epoch 5; Iter   238/ 2483] train: loss: 0.0017544
[Epoch 5; Iter   268/ 2483] train: loss: 0.0019194
[Epoch 5; Iter   298/ 2483] train: loss: 0.0019095
[Epoch 5; Iter   328/ 2483] train: loss: 0.0020747
[Epoch 5; Iter   358/ 2483] train: loss: 0.0018361
[Epoch 5; Iter   388/ 2483] train: loss: 0.0918529
[Epoch 5; Iter   418/ 2483] train: loss: 0.0020996
[Epoch 5; Iter   448/ 2483] train: loss: 0.0020939
[Epoch 5; Iter   478/ 2483] train: loss: 0.0020480
[Epoch 5; Iter   508/ 2483] train: loss: 0.0020152
[Epoch 5; Iter   538/ 2483] train: loss: 0.0019661
[Epoch 5; Iter   568/ 2483] train: loss: 0.1029214
[Epoch 5; Iter   598/ 2483] train: loss: 0.0016771
[Epoch 5; Iter   628/ 2483] train: loss: 0.0779992
[Epoch 5; Iter   658/ 2483] train: loss: 0.0017610
[Epoch 5; Iter   688/ 2483] train: loss: 0.0023861
[Epoch 5; Iter   718/ 2483] train: loss: 0.0020048
[Epoch 5; Iter   748/ 2483] train: loss: 0.0021467
[Epoch 5; Iter   778/ 2483] train: loss: 0.0017885
[Epoch 5; Iter   808/ 2483] train: loss: 0.0015697
[Epoch 5; Iter   838/ 2483] train: loss: 0.0015392
[Epoch 5; Iter   868/ 2483] train: loss: 0.0015414
[Epoch 5; Iter   898/ 2483] train: loss: 0.0017467
[Epoch 5; Iter   928/ 2483] train: loss: 0.0787471
[Epoch 5; Iter   958/ 2483] train: loss: 0.0021994
[Epoch 5; Iter   988/ 2483] train: loss: 0.0019396
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0021433
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0016805
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0017743
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0016125
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0016384
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0016387
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0016193
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0015142
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0016136
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0011021
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0013113
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0010955
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0677347
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0013462
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0014311
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0014823
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0866463
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0014297
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0015926
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0016022
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0020982
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0019150
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0017620
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0013973
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0013422
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0014326
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0020166
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0030090
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0781246
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0020499
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0782045
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0022845
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0018173
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0019692
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0021863
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0024902
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0021647
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0936445
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0025670
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0021318
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0017951
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0014775
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0016908
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0016379
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0014871
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0015894
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0018215
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0017995
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0015028
[Epoch 5] ogbg-molmuv: 0.011016 val loss: 0.043218
[Epoch 5] ogbg-molmuv: 0.018899 test loss: 0.054737
[Epoch 6; Iter     5/ 2483] train: loss: 0.0018996
[Epoch 6; Iter    35/ 2483] train: loss: 0.0683640
[Epoch 6; Iter    65/ 2483] train: loss: 0.0021804
[Epoch 6; Iter    95/ 2483] train: loss: 0.0020198
[Epoch 6; Iter   125/ 2483] train: loss: 0.0020632
[Epoch 6; Iter   155/ 2483] train: loss: 0.0017987
[Epoch 4; Iter   471/ 2483] train: loss: 0.0013918
[Epoch 4; Iter   501/ 2483] train: loss: 0.0016395
[Epoch 4; Iter   531/ 2483] train: loss: 0.0016004
[Epoch 4; Iter   561/ 2483] train: loss: 0.0016304
[Epoch 4; Iter   591/ 2483] train: loss: 0.0013413
[Epoch 4; Iter   621/ 2483] train: loss: 0.0015550
[Epoch 4; Iter   651/ 2483] train: loss: 0.1762611
[Epoch 4; Iter   681/ 2483] train: loss: 0.0875176
[Epoch 4; Iter   711/ 2483] train: loss: 0.0022639
[Epoch 4; Iter   741/ 2483] train: loss: 0.0021931
[Epoch 4; Iter   771/ 2483] train: loss: 0.0018958
[Epoch 4; Iter   801/ 2483] train: loss: 0.0579998
[Epoch 4; Iter   831/ 2483] train: loss: 0.0014275
[Epoch 4; Iter   861/ 2483] train: loss: 0.0015757
[Epoch 4; Iter   891/ 2483] train: loss: 0.0018319
[Epoch 4; Iter   921/ 2483] train: loss: 0.0020103
[Epoch 4; Iter   951/ 2483] train: loss: 0.0018524
[Epoch 4; Iter   981/ 2483] train: loss: 0.0022551
[Epoch 4; Iter  1011/ 2483] train: loss: 0.1258904
[Epoch 4; Iter  1041/ 2483] train: loss: 0.0021433
[Epoch 4; Iter  1071/ 2483] train: loss: 0.0021500
[Epoch 4; Iter  1101/ 2483] train: loss: 0.0019112
[Epoch 4; Iter  1131/ 2483] train: loss: 0.0018588
[Epoch 4; Iter  1161/ 2483] train: loss: 0.0017099
[Epoch 4; Iter  1191/ 2483] train: loss: 0.0019663
[Epoch 4; Iter  1221/ 2483] train: loss: 0.0020725
[Epoch 4; Iter  1251/ 2483] train: loss: 0.0874741
[Epoch 4; Iter  1281/ 2483] train: loss: 0.0020613
[Epoch 4; Iter  1311/ 2483] train: loss: 0.0020652
[Epoch 4; Iter  1341/ 2483] train: loss: 0.0016736
[Epoch 4; Iter  1371/ 2483] train: loss: 0.0018900
[Epoch 4; Iter  1401/ 2483] train: loss: 0.0759762
[Epoch 4; Iter  1431/ 2483] train: loss: 0.0018749
[Epoch 4; Iter  1461/ 2483] train: loss: 0.0028189
[Epoch 4; Iter  1491/ 2483] train: loss: 0.0019011
[Epoch 4; Iter  1521/ 2483] train: loss: 0.0023780
[Epoch 4; Iter  1551/ 2483] train: loss: 0.0027552
[Epoch 4; Iter  1581/ 2483] train: loss: 0.0025340
[Epoch 4; Iter  1611/ 2483] train: loss: 0.0665660
[Epoch 4; Iter  1641/ 2483] train: loss: 0.0019166
[Epoch 4; Iter  1671/ 2483] train: loss: 0.0017434
[Epoch 4; Iter  1701/ 2483] train: loss: 0.0027712
[Epoch 4; Iter  1731/ 2483] train: loss: 0.0027367
[Epoch 4; Iter  1761/ 2483] train: loss: 0.0709394
[Epoch 4; Iter  1791/ 2483] train: loss: 0.0017412
[Epoch 4; Iter  1821/ 2483] train: loss: 0.0022084
[Epoch 4; Iter  1851/ 2483] train: loss: 0.0021069
[Epoch 4; Iter  1881/ 2483] train: loss: 0.0019435
[Epoch 4; Iter  1911/ 2483] train: loss: 0.0016499
[Epoch 4; Iter  1941/ 2483] train: loss: 0.0016680
[Epoch 4; Iter  1971/ 2483] train: loss: 0.0019507
[Epoch 4; Iter  2001/ 2483] train: loss: 0.0016146
[Epoch 4; Iter  2031/ 2483] train: loss: 0.0017987
[Epoch 4; Iter  2061/ 2483] train: loss: 0.0019174
[Epoch 4; Iter  2091/ 2483] train: loss: 0.0017048
[Epoch 4; Iter  2121/ 2483] train: loss: 0.0021762
[Epoch 4; Iter  2151/ 2483] train: loss: 0.0016779
[Epoch 4; Iter  2181/ 2483] train: loss: 0.0018539
[Epoch 4; Iter  2211/ 2483] train: loss: 0.1401818
[Epoch 4; Iter  2241/ 2483] train: loss: 0.0901751
[Epoch 4; Iter  2271/ 2483] train: loss: 0.0020869
[Epoch 4; Iter  2301/ 2483] train: loss: 0.0018273
[Epoch 4; Iter  2331/ 2483] train: loss: 0.0762330
[Epoch 4; Iter  2361/ 2483] train: loss: 0.0014425
[Epoch 4; Iter  2391/ 2483] train: loss: 0.0020569
[Epoch 4; Iter  2421/ 2483] train: loss: 0.0021721
[Epoch 4; Iter  2451/ 2483] train: loss: 0.0018840
[Epoch 4; Iter  2481/ 2483] train: loss: 0.0019542
[Epoch 4] ogbg-molmuv: 0.017205 val loss: 0.397883
[Epoch 4] ogbg-molmuv: 0.048073 test loss: 0.460248
[Epoch 5; Iter    28/ 2483] train: loss: 0.0015829
[Epoch 5; Iter    58/ 2483] train: loss: 0.0019089
[Epoch 5; Iter    88/ 2483] train: loss: 0.0018605
[Epoch 5; Iter   118/ 2483] train: loss: 0.0015794
[Epoch 5; Iter   148/ 2483] train: loss: 0.0016100
[Epoch 5; Iter   178/ 2483] train: loss: 0.0014773
[Epoch 5; Iter   208/ 2483] train: loss: 0.0013177
[Epoch 5; Iter   238/ 2483] train: loss: 0.0017362
[Epoch 5; Iter   268/ 2483] train: loss: 0.0015042
[Epoch 5; Iter   298/ 2483] train: loss: 0.0021720
[Epoch 5; Iter   328/ 2483] train: loss: 0.0020770
[Epoch 5; Iter   358/ 2483] train: loss: 0.0017651
[Epoch 5; Iter   388/ 2483] train: loss: 0.0016125
[Epoch 5; Iter   418/ 2483] train: loss: 0.0018803
[Epoch 5; Iter   448/ 2483] train: loss: 0.0021188
[Epoch 5; Iter   478/ 2483] train: loss: 0.0024897
[Epoch 5; Iter   508/ 2483] train: loss: 0.0024829
[Epoch 5; Iter   538/ 2483] train: loss: 0.0021176
[Epoch 5; Iter   568/ 2483] train: loss: 0.0812517
[Epoch 5; Iter   598/ 2483] train: loss: 0.0920588
[Epoch 5; Iter   628/ 2483] train: loss: 0.0029719
[Epoch 5; Iter   658/ 2483] train: loss: 0.0019446
[Epoch 5; Iter   688/ 2483] train: loss: 0.0018249
[Epoch 5; Iter   718/ 2483] train: loss: 0.0015834
[Epoch 5; Iter   748/ 2483] train: loss: 0.0019180
[Epoch 5; Iter   778/ 2483] train: loss: 0.0018315
[Epoch 5; Iter   808/ 2483] train: loss: 0.0020028
[Epoch 5; Iter   838/ 2483] train: loss: 0.0019528
[Epoch 5; Iter   868/ 2483] train: loss: 0.0017621
[Epoch 5; Iter   898/ 2483] train: loss: 0.0023793
[Epoch 5; Iter   928/ 2483] train: loss: 0.0021414
[Epoch 5; Iter   958/ 2483] train: loss: 0.0016398
[Epoch 5; Iter   988/ 2483] train: loss: 0.0015878
[Epoch 5; Iter  1018/ 2483] train: loss: 0.0016379
[Epoch 5; Iter  1048/ 2483] train: loss: 0.0013721
[Epoch 5; Iter  1078/ 2483] train: loss: 0.0015142
[Epoch 5; Iter  1108/ 2483] train: loss: 0.0015762
[Epoch 5; Iter  1138/ 2483] train: loss: 0.0016661
[Epoch 5; Iter  1168/ 2483] train: loss: 0.0015878
[Epoch 5; Iter  1198/ 2483] train: loss: 0.0015075
[Epoch 5; Iter  1228/ 2483] train: loss: 0.0013233
[Epoch 5; Iter  1258/ 2483] train: loss: 0.0016500
[Epoch 5; Iter  1288/ 2483] train: loss: 0.0020096
[Epoch 5; Iter  1318/ 2483] train: loss: 0.0021359
[Epoch 5; Iter  1348/ 2483] train: loss: 0.0018953
[Epoch 5; Iter  1378/ 2483] train: loss: 0.0019914
[Epoch 5; Iter  1408/ 2483] train: loss: 0.0022949
[Epoch 5; Iter  1438/ 2483] train: loss: 0.0978709
[Epoch 5; Iter  1468/ 2483] train: loss: 0.0022743
[Epoch 5; Iter  1498/ 2483] train: loss: 0.0022923
[Epoch 5; Iter  1528/ 2483] train: loss: 0.0024273
[Epoch 5; Iter  1558/ 2483] train: loss: 0.0019251
[Epoch 5; Iter  1588/ 2483] train: loss: 0.0018880
[Epoch 5; Iter  1618/ 2483] train: loss: 0.0021735
[Epoch 5; Iter  1648/ 2483] train: loss: 0.0746577
[Epoch 5; Iter  1678/ 2483] train: loss: 0.0019952
[Epoch 5; Iter  1708/ 2483] train: loss: 0.0022510
[Epoch 5; Iter  1738/ 2483] train: loss: 0.0018921
[Epoch 5; Iter  1768/ 2483] train: loss: 0.0611022
[Epoch 5; Iter  1798/ 2483] train: loss: 0.0026646
[Epoch 5; Iter  1828/ 2483] train: loss: 0.0019360
[Epoch 5; Iter  1858/ 2483] train: loss: 0.0936753
[Epoch 5; Iter  1888/ 2483] train: loss: 0.0019235
[Epoch 5; Iter  1918/ 2483] train: loss: 0.0722125
[Epoch 5; Iter  1948/ 2483] train: loss: 0.0019259
[Epoch 5; Iter  1978/ 2483] train: loss: 0.0019940
[Epoch 5; Iter  2008/ 2483] train: loss: 0.0015796
[Epoch 5; Iter  2038/ 2483] train: loss: 0.0015608
[Epoch 5; Iter  2068/ 2483] train: loss: 0.0596636
[Epoch 5; Iter  2098/ 2483] train: loss: 0.0015179
[Epoch 5; Iter  2128/ 2483] train: loss: 0.0013554
[Epoch 5; Iter  2158/ 2483] train: loss: 0.0016715
[Epoch 5; Iter  2188/ 2483] train: loss: 0.0025962
[Epoch 5; Iter  2218/ 2483] train: loss: 0.0016370
[Epoch 5; Iter  2248/ 2483] train: loss: 0.0015806
[Epoch 5; Iter  2278/ 2483] train: loss: 0.0020506
[Epoch 5; Iter  2308/ 2483] train: loss: 0.0022286
[Epoch 5; Iter  2338/ 2483] train: loss: 0.0018603
[Epoch 5; Iter  2368/ 2483] train: loss: 0.0018983
[Epoch 5; Iter  2398/ 2483] train: loss: 0.0026464
[Epoch 5; Iter  2428/ 2483] train: loss: 0.0015973
[Epoch 5; Iter  2458/ 2483] train: loss: 0.0796185
[Epoch 5] ogbg-molmuv: 0.015459 val loss: 0.015200
[Epoch 5] ogbg-molmuv: 0.029406 test loss: 0.015717
[Epoch 6; Iter     5/ 2483] train: loss: 0.0015832
[Epoch 6; Iter    35/ 2483] train: loss: 0.0019454
[Epoch 6; Iter    65/ 2483] train: loss: 0.0021155
[Epoch 6; Iter    95/ 2483] train: loss: 0.0854369
[Epoch 6; Iter   125/ 2483] train: loss: 0.0025555
[Epoch 6; Iter   155/ 2483] train: loss: 0.0020309
[Epoch 6; Iter   185/ 2483] train: loss: 0.0016517
[Epoch 6; Iter   215/ 2483] train: loss: 0.0019524
[Epoch 6; Iter   245/ 2483] train: loss: 0.0675996
[Epoch 6; Iter   275/ 2483] train: loss: 0.0016055
[Epoch 6; Iter   305/ 2483] train: loss: 0.0019271
[Epoch 6; Iter   335/ 2483] train: loss: 0.0016501
[Epoch 6; Iter   365/ 2483] train: loss: 0.0020712
[Epoch 6; Iter   395/ 2483] train: loss: 0.0017460
[Epoch 6; Iter   425/ 2483] train: loss: 0.0030057
[Epoch 6; Iter   455/ 2483] train: loss: 0.0023263
[Epoch 6; Iter   485/ 2483] train: loss: 0.0015230
[Epoch 6; Iter   515/ 2483] train: loss: 0.0507047
[Epoch 6; Iter   545/ 2483] train: loss: 0.1633687
[Epoch 6; Iter   575/ 2483] train: loss: 0.0030236
[Epoch 6; Iter   605/ 2483] train: loss: 0.0019767
[Epoch 6; Iter   635/ 2483] train: loss: 0.0018936
[Epoch 6; Iter   665/ 2483] train: loss: 0.0021725
[Epoch 6; Iter   695/ 2483] train: loss: 0.0014964
[Epoch 6; Iter   725/ 2483] train: loss: 0.0779752
[Epoch 6; Iter   755/ 2483] train: loss: 0.0013455
[Epoch 6; Iter   785/ 2483] train: loss: 0.0016546
[Epoch 6; Iter   815/ 2483] train: loss: 0.0018722
[Epoch 6; Iter   845/ 2483] train: loss: 0.0013195
[Epoch 6; Iter   875/ 2483] train: loss: 0.0013979
[Epoch 6; Iter   905/ 2483] train: loss: 0.0813324
[Epoch 6; Iter   935/ 2483] train: loss: 0.0015360
[Epoch 6; Iter   965/ 2483] train: loss: 0.0020771
[Epoch 6; Iter   995/ 2483] train: loss: 0.0021010
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0028829
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0016666
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0024272
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0020136
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0020329
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0026606
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0013125
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0020988
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0722511
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0015727
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0019586
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0014899
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0671720
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0015339
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0019407
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0021104
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0022518
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0023961
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0604013
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0026908
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0014515
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0016978
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0014140
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0018360
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0017324
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0015411
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0014653
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0012171
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0013888
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0012253
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0021763
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0017123
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0015255
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0019013
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0016185
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0020102
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0020455
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0521069
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0020557
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0025515
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0022801
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0016489
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0025307
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0024617
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0021939
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0813343
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0623234
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0014394
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0016137
[Epoch 6] ogbg-molmuv: 0.013760 val loss: 0.018597
[Epoch 6] ogbg-molmuv: 0.016173 test loss: 0.021115
[Epoch 7; Iter    12/ 2483] train: loss: 0.0013571
[Epoch 7; Iter    42/ 2483] train: loss: 0.0012117
[Epoch 7; Iter    72/ 2483] train: loss: 0.0015263
[Epoch 7; Iter   102/ 2483] train: loss: 0.0018011
[Epoch 7; Iter   132/ 2483] train: loss: 0.0014514
[Epoch 7; Iter   162/ 2483] train: loss: 0.0607659
[Epoch 7; Iter   192/ 2483] train: loss: 0.0661401
[Epoch 7; Iter   222/ 2483] train: loss: 0.0017748
[Epoch 7; Iter   252/ 2483] train: loss: 0.0033936
[Epoch 7; Iter   282/ 2483] train: loss: 0.0019720
[Epoch 7; Iter   312/ 2483] train: loss: 0.0017189
[Epoch 7; Iter   342/ 2483] train: loss: 0.0858876
[Epoch 7; Iter   372/ 2483] train: loss: 0.0021682
[Epoch 7; Iter   402/ 2483] train: loss: 0.0025648
[Epoch 7; Iter   432/ 2483] train: loss: 0.0015716
[Epoch 7; Iter   462/ 2483] train: loss: 0.0014049
[Epoch 7; Iter   492/ 2483] train: loss: 0.0017593
[Epoch 7; Iter   522/ 2483] train: loss: 0.0016828
[Epoch 7; Iter   552/ 2483] train: loss: 0.0015413
[Epoch 7; Iter   582/ 2483] train: loss: 0.0013203
[Epoch 7; Iter   612/ 2483] train: loss: 0.0014249
[Epoch 7; Iter   642/ 2483] train: loss: 0.0011203
[Epoch 7; Iter   672/ 2483] train: loss: 0.0020824
[Epoch 7; Iter   702/ 2483] train: loss: 0.1053569
[Epoch 7; Iter   732/ 2483] train: loss: 0.0013683
[Epoch 7; Iter   762/ 2483] train: loss: 0.0986776
[Epoch 7; Iter   792/ 2483] train: loss: 0.0021000
[Epoch 7; Iter   822/ 2483] train: loss: 0.0015423
[Epoch 7; Iter   852/ 2483] train: loss: 0.0013649
[Epoch 7; Iter   882/ 2483] train: loss: 0.0017462
[Epoch 7; Iter   912/ 2483] train: loss: 0.0016356
[Epoch 7; Iter   942/ 2483] train: loss: 0.0018975
[Epoch 7; Iter   972/ 2483] train: loss: 0.0025877
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0018266
[Epoch 7; Iter  1032/ 2483] train: loss: 0.1799563
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0014672
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0732255
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0028582
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0023483
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0018902
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0017610
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0017365
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0514433
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0014165
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0015019
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0018183
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0012364
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0015107
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0013123
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0015601
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0014344
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0015881
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0018759
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0013149
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0014260
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0013287
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0015244
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0010972
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0011681
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0019010
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0018171
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0017599
[Epoch 7; Iter  1872/ 2483] train: loss: 0.1775716
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0019912
[Epoch 7; Iter  1932/ 2483] train: loss: 0.1074447
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0019806
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0022714
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0015568
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0017256
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0019319
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0019641
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0032956
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0024879
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0027432
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0019066
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0608533
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0990330
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0019424
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0021759
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0019696
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0020447
[Epoch 6; Iter   185/ 2483] train: loss: 0.0018340
[Epoch 6; Iter   215/ 2483] train: loss: 0.0017642
[Epoch 6; Iter   245/ 2483] train: loss: 0.0886049
[Epoch 6; Iter   275/ 2483] train: loss: 0.0016615
[Epoch 6; Iter   305/ 2483] train: loss: 0.0020291
[Epoch 6; Iter   335/ 2483] train: loss: 0.0021269
[Epoch 6; Iter   365/ 2483] train: loss: 0.0014990
[Epoch 6; Iter   395/ 2483] train: loss: 0.0018393
[Epoch 6; Iter   425/ 2483] train: loss: 0.0732802
[Epoch 6; Iter   455/ 2483] train: loss: 0.0566196
[Epoch 6; Iter   485/ 2483] train: loss: 0.0022501
[Epoch 6; Iter   515/ 2483] train: loss: 0.0024857
[Epoch 6; Iter   545/ 2483] train: loss: 0.0022995
[Epoch 6; Iter   575/ 2483] train: loss: 0.0020315
[Epoch 6; Iter   605/ 2483] train: loss: 0.0027137
[Epoch 6; Iter   635/ 2483] train: loss: 0.0018862
[Epoch 6; Iter   665/ 2483] train: loss: 0.0021388
[Epoch 6; Iter   695/ 2483] train: loss: 0.0022524
[Epoch 6; Iter   725/ 2483] train: loss: 0.0032933
[Epoch 6; Iter   755/ 2483] train: loss: 0.0021451
[Epoch 6; Iter   785/ 2483] train: loss: 0.0022137
[Epoch 6; Iter   815/ 2483] train: loss: 0.0016815
[Epoch 6; Iter   845/ 2483] train: loss: 0.0020239
[Epoch 6; Iter   875/ 2483] train: loss: 0.0016289
[Epoch 6; Iter   905/ 2483] train: loss: 0.0024208
[Epoch 6; Iter   935/ 2483] train: loss: 0.0024542
[Epoch 6; Iter   965/ 2483] train: loss: 0.0039262
[Epoch 6; Iter   995/ 2483] train: loss: 0.0028662
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0023533
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0042899
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0032406
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0020278
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0024287
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0900689
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0022953
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0022118
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0604419
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0016939
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0022065
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0015228
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0024922
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0852512
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0023142
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0014268
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0019633
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0018168
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0612499
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0022875
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0017106
[Epoch 6; Iter  1655/ 2483] train: loss: 0.1119158
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0023545
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0016423
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0021149
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0013875
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0015708
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0016968
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0014424
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0021875
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0016378
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0014626
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0014364
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0021331
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0014256
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0020843
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0709014
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0022503
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0697024
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0022953
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0014954
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0011673
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0011995
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0014931
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0010742
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0011978
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0011290
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0014075
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0016296
[Epoch 6] ogbg-molmuv: 0.013046 val loss: 0.016083
[Epoch 6] ogbg-molmuv: 0.033873 test loss: 0.029185
[Epoch 7; Iter    12/ 2483] train: loss: 0.0014846
[Epoch 7; Iter    42/ 2483] train: loss: 0.0012968
[Epoch 7; Iter    72/ 2483] train: loss: 0.0017111
[Epoch 7; Iter   102/ 2483] train: loss: 0.0025078
[Epoch 7; Iter   132/ 2483] train: loss: 0.0023798
[Epoch 7; Iter   162/ 2483] train: loss: 0.0023836
[Epoch 7; Iter   192/ 2483] train: loss: 0.0016729
[Epoch 7; Iter   222/ 2483] train: loss: 0.0020856
[Epoch 7; Iter   252/ 2483] train: loss: 0.0700793
[Epoch 7; Iter   282/ 2483] train: loss: 0.0024011
[Epoch 7; Iter   312/ 2483] train: loss: 0.0020237
[Epoch 7; Iter   342/ 2483] train: loss: 0.0014273
[Epoch 7; Iter   372/ 2483] train: loss: 0.0014574
[Epoch 7; Iter   402/ 2483] train: loss: 0.0014846
[Epoch 7; Iter   432/ 2483] train: loss: 0.0015127
[Epoch 7; Iter   462/ 2483] train: loss: 0.0015172
[Epoch 7; Iter   492/ 2483] train: loss: 0.0018376
[Epoch 7; Iter   522/ 2483] train: loss: 0.0015663
[Epoch 7; Iter   552/ 2483] train: loss: 0.0017229
[Epoch 7; Iter   582/ 2483] train: loss: 0.0783393
[Epoch 7; Iter   612/ 2483] train: loss: 0.0852891
[Epoch 7; Iter   642/ 2483] train: loss: 0.0015152
[Epoch 7; Iter   672/ 2483] train: loss: 0.0740991
[Epoch 7; Iter   702/ 2483] train: loss: 0.0013839
[Epoch 7; Iter   732/ 2483] train: loss: 0.0017203
[Epoch 7; Iter   762/ 2483] train: loss: 0.0017887
[Epoch 7; Iter   792/ 2483] train: loss: 0.0028658
[Epoch 7; Iter   822/ 2483] train: loss: 0.0559169
[Epoch 7; Iter   852/ 2483] train: loss: 0.0017879
[Epoch 7; Iter   882/ 2483] train: loss: 0.0013071
[Epoch 7; Iter   912/ 2483] train: loss: 0.0017000
[Epoch 7; Iter   942/ 2483] train: loss: 0.0016414
[Epoch 7; Iter   972/ 2483] train: loss: 0.0017042
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0724523
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0020725
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0020590
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0021314
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0021657
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0013777
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0018450
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0665101
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0019120
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0529015
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0015259
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0015689
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0015648
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0015379
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0016628
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0016676
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0016455
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0020393
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0011587
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0015235
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0016789
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0015042
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0015285
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0025900
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0019004
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0038347
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0014509
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0025069
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0016242
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0017043
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0030079
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0020121
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0014924
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0017904
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0014677
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0020309
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0021794
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0018270
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0012826
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0017930
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0724744
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0018454
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0018248
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0021806
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0020941
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0875389
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0539712
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0023678
[Epoch 6; Iter   185/ 2483] train: loss: 0.0017376
[Epoch 6; Iter   215/ 2483] train: loss: 0.0017599
[Epoch 6; Iter   245/ 2483] train: loss: 0.0018820
[Epoch 6; Iter   275/ 2483] train: loss: 0.0878889
[Epoch 6; Iter   305/ 2483] train: loss: 0.0019143
[Epoch 6; Iter   335/ 2483] train: loss: 0.0660710
[Epoch 6; Iter   365/ 2483] train: loss: 0.0020649
[Epoch 6; Iter   395/ 2483] train: loss: 0.0919319
[Epoch 6; Iter   425/ 2483] train: loss: 0.0022987
[Epoch 6; Iter   455/ 2483] train: loss: 0.0023805
[Epoch 6; Iter   485/ 2483] train: loss: 0.0033425
[Epoch 6; Iter   515/ 2483] train: loss: 0.0023511
[Epoch 6; Iter   545/ 2483] train: loss: 0.0024117
[Epoch 6; Iter   575/ 2483] train: loss: 0.0023093
[Epoch 6; Iter   605/ 2483] train: loss: 0.0016661
[Epoch 6; Iter   635/ 2483] train: loss: 0.0014354
[Epoch 6; Iter   665/ 2483] train: loss: 0.0012670
[Epoch 6; Iter   695/ 2483] train: loss: 0.0018585
[Epoch 6; Iter   725/ 2483] train: loss: 0.0016361
[Epoch 6; Iter   755/ 2483] train: loss: 0.0015047
[Epoch 6; Iter   785/ 2483] train: loss: 0.0019011
[Epoch 6; Iter   815/ 2483] train: loss: 0.0016709
[Epoch 6; Iter   845/ 2483] train: loss: 0.0016778
[Epoch 6; Iter   875/ 2483] train: loss: 0.0014865
[Epoch 6; Iter   905/ 2483] train: loss: 0.0018831
[Epoch 6; Iter   935/ 2483] train: loss: 0.0013574
[Epoch 6; Iter   965/ 2483] train: loss: 0.0017917
[Epoch 6; Iter   995/ 2483] train: loss: 0.0018754
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0783557
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0024396
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0023997
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0027112
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0019403
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0019985
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0017906
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0030435
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0017309
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0021344
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0702683
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0021626
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0019322
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0021500
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0022558
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0020096
[Epoch 6; Iter  1505/ 2483] train: loss: 0.1379440
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0033993
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0027526
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0030311
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0544359
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0017935
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0022341
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0017458
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0021967
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0016363
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0018698
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0016382
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0018588
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0018249
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0014812
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0017108
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0734454
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0943674
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0016922
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0015450
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0018404
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0020968
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0723617
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0023396
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0021939
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0017276
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0638411
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0022707
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0020851
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0017760
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0019435
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0014459
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0012580
[Epoch 6] ogbg-molmuv: 0.028288 val loss: 0.014933
[Epoch 6] ogbg-molmuv: 0.042800 test loss: 0.015470
[Epoch 7; Iter    12/ 2483] train: loss: 0.0012818
[Epoch 7; Iter    42/ 2483] train: loss: 0.0016058
[Epoch 7; Iter    72/ 2483] train: loss: 0.0014792
[Epoch 7; Iter   102/ 2483] train: loss: 0.0023995
[Epoch 7; Iter   132/ 2483] train: loss: 0.0404485
[Epoch 7; Iter   162/ 2483] train: loss: 0.0025182
[Epoch 7; Iter   192/ 2483] train: loss: 0.0024455
[Epoch 7; Iter   222/ 2483] train: loss: 0.0032127
[Epoch 7; Iter   252/ 2483] train: loss: 0.0028242
[Epoch 7; Iter   282/ 2483] train: loss: 0.0017849
[Epoch 7; Iter   312/ 2483] train: loss: 0.0013730
[Epoch 7; Iter   342/ 2483] train: loss: 0.0018035
[Epoch 7; Iter   372/ 2483] train: loss: 0.0014057
[Epoch 7; Iter   402/ 2483] train: loss: 0.0019872
[Epoch 7; Iter   432/ 2483] train: loss: 0.0012472
[Epoch 7; Iter   462/ 2483] train: loss: 0.0015792
[Epoch 7; Iter   492/ 2483] train: loss: 0.0012626
[Epoch 7; Iter   522/ 2483] train: loss: 0.0013894
[Epoch 7; Iter   552/ 2483] train: loss: 0.0017563
[Epoch 7; Iter   582/ 2483] train: loss: 0.0014158
[Epoch 7; Iter   612/ 2483] train: loss: 0.0020454
[Epoch 7; Iter   642/ 2483] train: loss: 0.0018349
[Epoch 7; Iter   672/ 2483] train: loss: 0.0020007
[Epoch 7; Iter   702/ 2483] train: loss: 0.0018250
[Epoch 7; Iter   732/ 2483] train: loss: 0.0014725
[Epoch 7; Iter   762/ 2483] train: loss: 0.0013516
[Epoch 7; Iter   792/ 2483] train: loss: 0.0014673
[Epoch 7; Iter   822/ 2483] train: loss: 0.0017024
[Epoch 7; Iter   852/ 2483] train: loss: 0.0543563
[Epoch 7; Iter   882/ 2483] train: loss: 0.0017424
[Epoch 7; Iter   912/ 2483] train: loss: 0.0013501
[Epoch 7; Iter   942/ 2483] train: loss: 0.0012537
[Epoch 7; Iter   972/ 2483] train: loss: 0.0017807
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0938816
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0849848
[Epoch 7; Iter  1062/ 2483] train: loss: 0.1713109
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0021087
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0014780
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0987014
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0014457
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0018749
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0014753
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0019123
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0013621
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0017163
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0020271
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0019747
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0637851
[Epoch 7; Iter  1452/ 2483] train: loss: 0.1040295
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0020915
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0017490
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0019894
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0701474
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0934627
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0014452
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0721962
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0022485
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0020299
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0016589
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0025893
[Epoch 7; Iter  1812/ 2483] train: loss: 0.1534461
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0021480
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0024988
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0022719
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0020865
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0019451
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0018640
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0020995
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0021499
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0018140
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0013777
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0014602
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0015306
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0013467
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0793435
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0011114
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0028612
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0021765
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0037840
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0023649
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0022446
[Epoch 6; Iter   185/ 2483] train: loss: 0.0016767
[Epoch 6; Iter   215/ 2483] train: loss: 0.0017651
[Epoch 6; Iter   245/ 2483] train: loss: 0.0018292
[Epoch 6; Iter   275/ 2483] train: loss: 0.0811323
[Epoch 6; Iter   305/ 2483] train: loss: 0.0018899
[Epoch 6; Iter   335/ 2483] train: loss: 0.0615984
[Epoch 6; Iter   365/ 2483] train: loss: 0.0021299
[Epoch 6; Iter   395/ 2483] train: loss: 0.0936144
[Epoch 6; Iter   425/ 2483] train: loss: 0.0023288
[Epoch 6; Iter   455/ 2483] train: loss: 0.0025795
[Epoch 6; Iter   485/ 2483] train: loss: 0.0027186
[Epoch 6; Iter   515/ 2483] train: loss: 0.0021300
[Epoch 6; Iter   545/ 2483] train: loss: 0.0019936
[Epoch 6; Iter   575/ 2483] train: loss: 0.0021024
[Epoch 6; Iter   605/ 2483] train: loss: 0.0016347
[Epoch 6; Iter   635/ 2483] train: loss: 0.0015594
[Epoch 6; Iter   665/ 2483] train: loss: 0.0014602
[Epoch 6; Iter   695/ 2483] train: loss: 0.0017705
[Epoch 6; Iter   725/ 2483] train: loss: 0.0013615
[Epoch 6; Iter   755/ 2483] train: loss: 0.0015561
[Epoch 6; Iter   785/ 2483] train: loss: 0.0026507
[Epoch 6; Iter   815/ 2483] train: loss: 0.0019780
[Epoch 6; Iter   845/ 2483] train: loss: 0.0018931
[Epoch 6; Iter   875/ 2483] train: loss: 0.0016145
[Epoch 6; Iter   905/ 2483] train: loss: 0.0022144
[Epoch 6; Iter   935/ 2483] train: loss: 0.0014613
[Epoch 6; Iter   965/ 2483] train: loss: 0.0016457
[Epoch 6; Iter   995/ 2483] train: loss: 0.0038843
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0775786
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0019838
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0025325
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0023566
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0017604
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0023988
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0016021
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0034314
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0011119
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0019861
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0671302
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0019497
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0017326
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0025112
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0032294
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0021259
[Epoch 6; Iter  1505/ 2483] train: loss: 0.1385002
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0036801
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0028988
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0028586
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0761210
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0014651
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0016707
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0014732
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0015717
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0017788
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0018694
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0018635
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0018706
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0016613
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0015991
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0018017
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0680538
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0900362
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0017061
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0017080
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0018442
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0020551
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0691597
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0022852
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0021823
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0016838
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0692331
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0022097
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0023728
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0016756
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0019858
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0016497
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0014069
[Epoch 6] ogbg-molmuv: 0.026556 val loss: 0.014594
[Epoch 6] ogbg-molmuv: 0.045353 test loss: 0.016037
[Epoch 7; Iter    12/ 2483] train: loss: 0.0013767
[Epoch 7; Iter    42/ 2483] train: loss: 0.0014877
[Epoch 7; Iter    72/ 2483] train: loss: 0.0016194
[Epoch 7; Iter   102/ 2483] train: loss: 0.0039550
[Epoch 7; Iter   132/ 2483] train: loss: 0.0326590
[Epoch 7; Iter   162/ 2483] train: loss: 0.0020537
[Epoch 7; Iter   192/ 2483] train: loss: 0.0019471
[Epoch 7; Iter   222/ 2483] train: loss: 0.0035796
[Epoch 7; Iter   252/ 2483] train: loss: 0.0023725
[Epoch 7; Iter   282/ 2483] train: loss: 0.0015332
[Epoch 7; Iter   312/ 2483] train: loss: 0.0014009
[Epoch 7; Iter   342/ 2483] train: loss: 0.0016713
[Epoch 7; Iter   372/ 2483] train: loss: 0.0014900
[Epoch 7; Iter   402/ 2483] train: loss: 0.0018232
[Epoch 7; Iter   432/ 2483] train: loss: 0.0013126
[Epoch 7; Iter   462/ 2483] train: loss: 0.0013718
[Epoch 7; Iter   492/ 2483] train: loss: 0.0011947
[Epoch 7; Iter   522/ 2483] train: loss: 0.0014404
[Epoch 7; Iter   552/ 2483] train: loss: 0.0014297
[Epoch 7; Iter   582/ 2483] train: loss: 0.0012502
[Epoch 7; Iter   612/ 2483] train: loss: 0.0019605
[Epoch 7; Iter   642/ 2483] train: loss: 0.0018338
[Epoch 7; Iter   672/ 2483] train: loss: 0.0016795
[Epoch 7; Iter   702/ 2483] train: loss: 0.0026044
[Epoch 7; Iter   732/ 2483] train: loss: 0.0011889
[Epoch 7; Iter   762/ 2483] train: loss: 0.0013125
[Epoch 7; Iter   792/ 2483] train: loss: 0.0013404
[Epoch 7; Iter   822/ 2483] train: loss: 0.0017690
[Epoch 7; Iter   852/ 2483] train: loss: 0.0570590
[Epoch 7; Iter   882/ 2483] train: loss: 0.0022907
[Epoch 7; Iter   912/ 2483] train: loss: 0.0015889
[Epoch 7; Iter   942/ 2483] train: loss: 0.0013957
[Epoch 7; Iter   972/ 2483] train: loss: 0.0019242
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0993824
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0695605
[Epoch 7; Iter  1062/ 2483] train: loss: 0.1721963
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0021959
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0014706
[Epoch 7; Iter  1152/ 2483] train: loss: 0.1023270
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0014565
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0017533
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0015946
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0018630
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0013909
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0015273
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0019424
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0018091
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0666861
[Epoch 7; Iter  1452/ 2483] train: loss: 0.1144707
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0023065
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0018779
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0019464
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0744237
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0726707
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0014268
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0684057
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0022558
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0018950
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0018039
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0029356
[Epoch 7; Iter  1812/ 2483] train: loss: 0.1566266
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0022243
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0025848
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0023014
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0021402
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0019004
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0021412
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0019728
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0020438
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0020180
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0013417
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0012049
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0015137
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0014590
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0825832
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0012569
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0036026
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0029489
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0027164
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0021323
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0025479
[Epoch 6; Iter   185/ 2483] train: loss: 0.0016689
[Epoch 6; Iter   215/ 2483] train: loss: 0.0029791
[Epoch 6; Iter   245/ 2483] train: loss: 0.0747342
[Epoch 6; Iter   275/ 2483] train: loss: 0.0016708
[Epoch 6; Iter   305/ 2483] train: loss: 0.0021776
[Epoch 6; Iter   335/ 2483] train: loss: 0.0016616
[Epoch 6; Iter   365/ 2483] train: loss: 0.0019942
[Epoch 6; Iter   395/ 2483] train: loss: 0.0017572
[Epoch 6; Iter   425/ 2483] train: loss: 0.0020902
[Epoch 6; Iter   455/ 2483] train: loss: 0.0018329
[Epoch 6; Iter   485/ 2483] train: loss: 0.0014126
[Epoch 6; Iter   515/ 2483] train: loss: 0.0433654
[Epoch 6; Iter   545/ 2483] train: loss: 0.1434090
[Epoch 6; Iter   575/ 2483] train: loss: 0.0028646
[Epoch 6; Iter   605/ 2483] train: loss: 0.0020653
[Epoch 6; Iter   635/ 2483] train: loss: 0.0019116
[Epoch 6; Iter   665/ 2483] train: loss: 0.0020684
[Epoch 6; Iter   695/ 2483] train: loss: 0.0020937
[Epoch 6; Iter   725/ 2483] train: loss: 0.0794529
[Epoch 6; Iter   755/ 2483] train: loss: 0.0015020
[Epoch 6; Iter   785/ 2483] train: loss: 0.0014209
[Epoch 6; Iter   815/ 2483] train: loss: 0.0017246
[Epoch 6; Iter   845/ 2483] train: loss: 0.0014650
[Epoch 6; Iter   875/ 2483] train: loss: 0.0012688
[Epoch 6; Iter   905/ 2483] train: loss: 0.0809578
[Epoch 6; Iter   935/ 2483] train: loss: 0.0016463
[Epoch 6; Iter   965/ 2483] train: loss: 0.0019060
[Epoch 6; Iter   995/ 2483] train: loss: 0.0019048
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0030236
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0017425
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0024857
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0017923
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0019567
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0022669
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0017389
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0022894
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0762095
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0015955
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0016894
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0015698
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0758510
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0016718
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0020519
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0026973
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0033670
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0020782
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0705626
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0024539
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0014370
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0016593
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0020098
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0022249
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0015329
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0017356
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0013534
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0011320
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0015756
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0012175
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0018719
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0015452
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0016325
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0016987
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0016014
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0020919
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0018361
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0582952
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0019415
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0021184
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0023114
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0016212
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0020214
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0028086
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0023128
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0663300
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0614481
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0015663
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0016301
[Epoch 6] ogbg-molmuv: 0.023583 val loss: 0.014850
[Epoch 6] ogbg-molmuv: 0.034648 test loss: 0.015554
[Epoch 7; Iter    12/ 2483] train: loss: 0.0013036
[Epoch 7; Iter    42/ 2483] train: loss: 0.0014026
[Epoch 7; Iter    72/ 2483] train: loss: 0.0014237
[Epoch 7; Iter   102/ 2483] train: loss: 0.0015817
[Epoch 7; Iter   132/ 2483] train: loss: 0.0013882
[Epoch 7; Iter   162/ 2483] train: loss: 0.0583574
[Epoch 7; Iter   192/ 2483] train: loss: 0.0656112
[Epoch 7; Iter   222/ 2483] train: loss: 0.0016920
[Epoch 7; Iter   252/ 2483] train: loss: 0.0024997
[Epoch 7; Iter   282/ 2483] train: loss: 0.0020342
[Epoch 7; Iter   312/ 2483] train: loss: 0.0017775
[Epoch 7; Iter   342/ 2483] train: loss: 0.0821411
[Epoch 7; Iter   372/ 2483] train: loss: 0.0019112
[Epoch 7; Iter   402/ 2483] train: loss: 0.0022443
[Epoch 7; Iter   432/ 2483] train: loss: 0.0015100
[Epoch 7; Iter   462/ 2483] train: loss: 0.0015170
[Epoch 7; Iter   492/ 2483] train: loss: 0.0017975
[Epoch 7; Iter   522/ 2483] train: loss: 0.0020071
[Epoch 7; Iter   552/ 2483] train: loss: 0.0015503
[Epoch 7; Iter   582/ 2483] train: loss: 0.0012630
[Epoch 7; Iter   612/ 2483] train: loss: 0.0013597
[Epoch 7; Iter   642/ 2483] train: loss: 0.0011941
[Epoch 7; Iter   672/ 2483] train: loss: 0.0015558
[Epoch 7; Iter   702/ 2483] train: loss: 0.1071525
[Epoch 7; Iter   732/ 2483] train: loss: 0.0016059
[Epoch 7; Iter   762/ 2483] train: loss: 0.0832479
[Epoch 7; Iter   792/ 2483] train: loss: 0.0020383
[Epoch 7; Iter   822/ 2483] train: loss: 0.0018873
[Epoch 7; Iter   852/ 2483] train: loss: 0.0015255
[Epoch 7; Iter   882/ 2483] train: loss: 0.0015061
[Epoch 7; Iter   912/ 2483] train: loss: 0.0014720
[Epoch 7; Iter   942/ 2483] train: loss: 0.0021777
[Epoch 7; Iter   972/ 2483] train: loss: 0.0023061
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0015518
[Epoch 7; Iter  1032/ 2483] train: loss: 0.1960616
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0013641
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0612127
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0041422
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0021453
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0021694
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0018045
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0015574
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0536159
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0013108
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0013239
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0014461
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0013645
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0014159
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0013992
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0015043
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0013052
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0017740
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0020302
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0012560
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0015013
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0012437
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0015727
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0011833
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0011925
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0014785
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0017677
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0016492
[Epoch 7; Iter  1872/ 2483] train: loss: 0.1522337
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0018481
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0904332
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0023235
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0019297
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0016028
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0017406
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0019221
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0022916
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0027441
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0026400
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0024381
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0023337
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0695514
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0897227
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0020343
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0023706
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0023676
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0023928
[Epoch 6; Iter   185/ 2483] train: loss: 0.0017209
[Epoch 6; Iter   215/ 2483] train: loss: 0.0018766
[Epoch 6; Iter   245/ 2483] train: loss: 0.0925299
[Epoch 6; Iter   275/ 2483] train: loss: 0.0015934
[Epoch 6; Iter   305/ 2483] train: loss: 0.0019760
[Epoch 6; Iter   335/ 2483] train: loss: 0.0017730
[Epoch 6; Iter   365/ 2483] train: loss: 0.0013621
[Epoch 6; Iter   395/ 2483] train: loss: 0.0019020
[Epoch 6; Iter   425/ 2483] train: loss: 0.0750556
[Epoch 6; Iter   455/ 2483] train: loss: 0.0642819
[Epoch 6; Iter   485/ 2483] train: loss: 0.0021450
[Epoch 6; Iter   515/ 2483] train: loss: 0.0021431
[Epoch 6; Iter   545/ 2483] train: loss: 0.0022778
[Epoch 6; Iter   575/ 2483] train: loss: 0.0023643
[Epoch 6; Iter   605/ 2483] train: loss: 0.0015329
[Epoch 6; Iter   635/ 2483] train: loss: 0.0019369
[Epoch 6; Iter   665/ 2483] train: loss: 0.0019624
[Epoch 6; Iter   695/ 2483] train: loss: 0.0020380
[Epoch 6; Iter   725/ 2483] train: loss: 0.0028922
[Epoch 6; Iter   755/ 2483] train: loss: 0.0021939
[Epoch 6; Iter   785/ 2483] train: loss: 0.0018570
[Epoch 6; Iter   815/ 2483] train: loss: 0.0015792
[Epoch 6; Iter   845/ 2483] train: loss: 0.0022686
[Epoch 6; Iter   875/ 2483] train: loss: 0.0017332
[Epoch 6; Iter   905/ 2483] train: loss: 0.0034138
[Epoch 6; Iter   935/ 2483] train: loss: 0.0025743
[Epoch 6; Iter   965/ 2483] train: loss: 0.0040356
[Epoch 6; Iter   995/ 2483] train: loss: 0.0027216
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0023196
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0030975
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0042848
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0019423
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0023029
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0960013
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0029569
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0019853
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0611205
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0020771
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0019549
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0018210
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0019841
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0761804
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0019934
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0016238
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0018300
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0016779
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0632004
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0018935
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0014352
[Epoch 6; Iter  1655/ 2483] train: loss: 0.1077364
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0022464
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0018407
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0016301
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0012406
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0015353
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0015080
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0016780
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0022091
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0014018
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0014619
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0013604
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0018271
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0015180
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0022291
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0600781
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0022558
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0693752
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0017613
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0010516
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0010378
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0010809
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0011374
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0008433
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0010651
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0011461
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0014201
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0015905
[Epoch 6] ogbg-molmuv: 0.019362 val loss: 0.292540
[Epoch 6] ogbg-molmuv: 0.021796 test loss: 0.382459
[Epoch 7; Iter    12/ 2483] train: loss: 0.0013470
[Epoch 7; Iter    42/ 2483] train: loss: 0.0014943
[Epoch 7; Iter    72/ 2483] train: loss: 0.0017640
[Epoch 7; Iter   102/ 2483] train: loss: 0.0028222
[Epoch 7; Iter   132/ 2483] train: loss: 0.0029808
[Epoch 7; Iter   162/ 2483] train: loss: 0.0022114
[Epoch 7; Iter   192/ 2483] train: loss: 0.0016271
[Epoch 7; Iter   222/ 2483] train: loss: 0.0024312
[Epoch 7; Iter   252/ 2483] train: loss: 0.0716494
[Epoch 7; Iter   282/ 2483] train: loss: 0.0020208
[Epoch 7; Iter   312/ 2483] train: loss: 0.0018259
[Epoch 7; Iter   342/ 2483] train: loss: 0.0012745
[Epoch 7; Iter   372/ 2483] train: loss: 0.0012231
[Epoch 7; Iter   402/ 2483] train: loss: 0.0014705
[Epoch 7; Iter   432/ 2483] train: loss: 0.0014441
[Epoch 7; Iter   462/ 2483] train: loss: 0.0014918
[Epoch 7; Iter   492/ 2483] train: loss: 0.0021077
[Epoch 7; Iter   522/ 2483] train: loss: 0.0016140
[Epoch 7; Iter   552/ 2483] train: loss: 0.0016416
[Epoch 7; Iter   582/ 2483] train: loss: 0.0714816
[Epoch 7; Iter   612/ 2483] train: loss: 0.0795055
[Epoch 7; Iter   642/ 2483] train: loss: 0.0018250
[Epoch 7; Iter   672/ 2483] train: loss: 0.0661480
[Epoch 7; Iter   702/ 2483] train: loss: 0.0014955
[Epoch 7; Iter   732/ 2483] train: loss: 0.0016340
[Epoch 7; Iter   762/ 2483] train: loss: 0.0020037
[Epoch 7; Iter   792/ 2483] train: loss: 0.0020848
[Epoch 7; Iter   822/ 2483] train: loss: 0.0700538
[Epoch 7; Iter   852/ 2483] train: loss: 0.0016142
[Epoch 7; Iter   882/ 2483] train: loss: 0.0015559
[Epoch 7; Iter   912/ 2483] train: loss: 0.0015805
[Epoch 7; Iter   942/ 2483] train: loss: 0.0018442
[Epoch 7; Iter   972/ 2483] train: loss: 0.0021562
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0818803
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0019277
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0016753
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0018855
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0019606
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0014963
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0017843
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0756899
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0017609
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0645612
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0014332
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0013362
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0021597
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0015228
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0015117
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0015916
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0016387
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0019631
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0012458
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0012892
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0017133
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0016779
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0015606
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0027858
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0018029
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0025365
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0020679
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0017978
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0017681
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0016464
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0036713
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0017537
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0013019
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0017734
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0014325
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0016688
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0020110
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0022227
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0013372
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0016206
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0750482
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0015972
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0018672
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0022893
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0019874
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0905001
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0664691
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0024587
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0026512
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0017317
[Epoch 7] ogbg-molmuv: 0.070041 val loss: 0.014463
[Epoch 7] ogbg-molmuv: 0.075642 test loss: 0.015120
[Epoch 8; Iter    19/ 2483] train: loss: 0.0019555
[Epoch 8; Iter    49/ 2483] train: loss: 0.0018900
[Epoch 8; Iter    79/ 2483] train: loss: 0.0021846
[Epoch 8; Iter   109/ 2483] train: loss: 0.0030489
[Epoch 8; Iter   139/ 2483] train: loss: 0.0038569
[Epoch 8; Iter   169/ 2483] train: loss: 0.0533164
[Epoch 8; Iter   199/ 2483] train: loss: 0.0027107
[Epoch 8; Iter   229/ 2483] train: loss: 0.0038371
[Epoch 8; Iter   259/ 2483] train: loss: 0.0023463
[Epoch 8; Iter   289/ 2483] train: loss: 0.0025081
[Epoch 8; Iter   319/ 2483] train: loss: 0.0019986
[Epoch 8; Iter   349/ 2483] train: loss: 0.0020917
[Epoch 8; Iter   379/ 2483] train: loss: 0.0014256
[Epoch 8; Iter   409/ 2483] train: loss: 0.0018149
[Epoch 8; Iter   439/ 2483] train: loss: 0.0019857
[Epoch 8; Iter   469/ 2483] train: loss: 0.0016858
[Epoch 8; Iter   499/ 2483] train: loss: 0.0015799
[Epoch 8; Iter   529/ 2483] train: loss: 0.0029646
[Epoch 8; Iter   559/ 2483] train: loss: 0.0015846
[Epoch 8; Iter   589/ 2483] train: loss: 0.0015678
[Epoch 8; Iter   619/ 2483] train: loss: 0.0997948
[Epoch 8; Iter   649/ 2483] train: loss: 0.0015502
[Epoch 8; Iter   679/ 2483] train: loss: 0.0014362
[Epoch 8; Iter   709/ 2483] train: loss: 0.0019121
[Epoch 8; Iter   739/ 2483] train: loss: 0.0022683
[Epoch 8; Iter   769/ 2483] train: loss: 0.0017576
[Epoch 8; Iter   799/ 2483] train: loss: 0.0018393
[Epoch 8; Iter   829/ 2483] train: loss: 0.0014037
[Epoch 8; Iter   859/ 2483] train: loss: 0.0474783
[Epoch 8; Iter   889/ 2483] train: loss: 0.0019539
[Epoch 8; Iter   919/ 2483] train: loss: 0.0015728
[Epoch 8; Iter   949/ 2483] train: loss: 0.0018411
[Epoch 8; Iter   979/ 2483] train: loss: 0.0013275
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0012712
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0697087
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0018455
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0018116
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0012894
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0641589
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0016669
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0020363
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0017820
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0021846
[Epoch 8; Iter  1309/ 2483] train: loss: 0.1234299
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0015377
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0014513
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0010561
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0634287
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0016676
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0017080
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0016375
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0012997
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0018519
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0016312
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0024412
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0020229
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0021306
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0504320
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0018583
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0026359
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0019786
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0021564
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0026521
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0023997
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0041764
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0682228
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0017333
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0017698
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0801422
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0012771
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0012052
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0015404
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0016481
[Epoch 8; Iter  2209/ 2483] train: loss: 0.1795767
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0020833
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0983630
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0775774
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0021102
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0020443
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0020868
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0018651
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0020558
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0018710
[Epoch 8] ogbg-molmuv: 0.034081 val loss: 0.137352
[Epoch 8] ogbg-molmuv: 0.027527 test loss: 0.172123
[Epoch 9; Iter    26/ 2483] train: loss: 0.0017666
[Epoch 9; Iter    56/ 2483] train: loss: 0.0021119
[Epoch 9; Iter    86/ 2483] train: loss: 0.0742041
[Epoch 9; Iter   116/ 2483] train: loss: 0.0016287
[Epoch 9; Iter   146/ 2483] train: loss: 0.0018347
[Epoch 9; Iter   176/ 2483] train: loss: 0.0021013
[Epoch 9; Iter   206/ 2483] train: loss: 0.0012907
[Epoch 9; Iter   236/ 2483] train: loss: 0.0022070
[Epoch 9; Iter   266/ 2483] train: loss: 0.0021857
[Epoch 9; Iter   296/ 2483] train: loss: 0.0013920
[Epoch 9; Iter   326/ 2483] train: loss: 0.0011049
[Epoch 9; Iter   356/ 2483] train: loss: 0.0011951
[Epoch 9; Iter   386/ 2483] train: loss: 0.0017584
[Epoch 9; Iter   416/ 2483] train: loss: 0.0015300
[Epoch 9; Iter   446/ 2483] train: loss: 0.0013011
[Epoch 9; Iter   476/ 2483] train: loss: 0.0016670
[Epoch 9; Iter   506/ 2483] train: loss: 0.0022740
[Epoch 9; Iter   536/ 2483] train: loss: 0.0020782
[Epoch 9; Iter   566/ 2483] train: loss: 0.0019009
[Epoch 9; Iter   596/ 2483] train: loss: 0.0019574
[Epoch 9; Iter   626/ 2483] train: loss: 0.0016103
[Epoch 9; Iter   656/ 2483] train: loss: 0.0014146
[Epoch 9; Iter   686/ 2483] train: loss: 0.0014777
[Epoch 9; Iter   716/ 2483] train: loss: 0.0013160
[Epoch 9; Iter   746/ 2483] train: loss: 0.0013614
[Epoch 9; Iter   776/ 2483] train: loss: 0.0016345
[Epoch 9; Iter   806/ 2483] train: loss: 0.1031657
[Epoch 9; Iter   836/ 2483] train: loss: 0.0015272
[Epoch 9; Iter   866/ 2483] train: loss: 0.0019469
[Epoch 9; Iter   896/ 2483] train: loss: 0.0022930
[Epoch 9; Iter   926/ 2483] train: loss: 0.0016998
[Epoch 9; Iter   956/ 2483] train: loss: 0.0020807
[Epoch 9; Iter   986/ 2483] train: loss: 0.0020630
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0021327
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0012258
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0018293
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0018420
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0025431
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0538915
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0016997
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0016911
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0020382
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0022281
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0054884
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0019961
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0018373
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0649174
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0012958
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0010654
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0092698
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0014926
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0826462
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0957742
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0019797
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0021261
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0025435
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0020662
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0025215
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0030766
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0021067
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0017323
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0020549
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0021563
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0017943
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0019232
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0020109
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0025497
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0019138
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0015077
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0017931
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0011274
[Epoch 6; Iter   185/ 2483] train: loss: 0.0016255
[Epoch 6; Iter   215/ 2483] train: loss: 0.0018431
[Epoch 6; Iter   245/ 2483] train: loss: 0.0888262
[Epoch 6; Iter   275/ 2483] train: loss: 0.0018951
[Epoch 6; Iter   305/ 2483] train: loss: 0.0017351
[Epoch 6; Iter   335/ 2483] train: loss: 0.0015481
[Epoch 6; Iter   365/ 2483] train: loss: 0.0015888
[Epoch 6; Iter   395/ 2483] train: loss: 0.0017293
[Epoch 6; Iter   425/ 2483] train: loss: 0.0776770
[Epoch 6; Iter   455/ 2483] train: loss: 0.0767472
[Epoch 6; Iter   485/ 2483] train: loss: 0.0019957
[Epoch 6; Iter   515/ 2483] train: loss: 0.0021067
[Epoch 6; Iter   545/ 2483] train: loss: 0.0022935
[Epoch 6; Iter   575/ 2483] train: loss: 0.0026623
[Epoch 6; Iter   605/ 2483] train: loss: 0.0019263
[Epoch 6; Iter   635/ 2483] train: loss: 0.0025850
[Epoch 6; Iter   665/ 2483] train: loss: 0.0019190
[Epoch 6; Iter   695/ 2483] train: loss: 0.0022325
[Epoch 6; Iter   725/ 2483] train: loss: 0.0025994
[Epoch 6; Iter   755/ 2483] train: loss: 0.0022037
[Epoch 6; Iter   785/ 2483] train: loss: 0.0019577
[Epoch 6; Iter   815/ 2483] train: loss: 0.0014899
[Epoch 6; Iter   845/ 2483] train: loss: 0.0018925
[Epoch 6; Iter   875/ 2483] train: loss: 0.0018400
[Epoch 6; Iter   905/ 2483] train: loss: 0.0029786
[Epoch 6; Iter   935/ 2483] train: loss: 0.0027539
[Epoch 6; Iter   965/ 2483] train: loss: 0.0032938
[Epoch 6; Iter   995/ 2483] train: loss: 0.0026368
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0023261
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0025252
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0032066
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0021219
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0018754
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0796710
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0025250
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0018267
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0622948
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0018494
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0026060
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0014741
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0021863
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0785298
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0025580
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0017795
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0016332
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0020305
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0644775
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0017563
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0015469
[Epoch 6; Iter  1655/ 2483] train: loss: 0.1002826
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0018471
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0014313
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0016082
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0013721
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0028608
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0017147
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0015412
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0018066
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0012924
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0013604
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0017469
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0019313
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0016133
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0017702
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0720655
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0017370
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0637800
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0021424
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0012495
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0011718
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0010835
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0011013
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0010652
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0010537
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0010559
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0013217
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0015621
[Epoch 6] ogbg-molmuv: 0.019155 val loss: 0.403962
[Epoch 6] ogbg-molmuv: 0.018001 test loss: 0.395954
[Epoch 7; Iter    12/ 2483] train: loss: 0.0016600
[Epoch 7; Iter    42/ 2483] train: loss: 0.0016966
[Epoch 7; Iter    72/ 2483] train: loss: 0.0016612
[Epoch 7; Iter   102/ 2483] train: loss: 0.0020584
[Epoch 7; Iter   132/ 2483] train: loss: 0.0021119
[Epoch 7; Iter   162/ 2483] train: loss: 0.0023009
[Epoch 7; Iter   192/ 2483] train: loss: 0.0020223
[Epoch 7; Iter   222/ 2483] train: loss: 0.0020386
[Epoch 7; Iter   252/ 2483] train: loss: 0.0699624
[Epoch 7; Iter   282/ 2483] train: loss: 0.0021295
[Epoch 7; Iter   312/ 2483] train: loss: 0.0017165
[Epoch 7; Iter   342/ 2483] train: loss: 0.0011296
[Epoch 7; Iter   372/ 2483] train: loss: 0.0013550
[Epoch 7; Iter   402/ 2483] train: loss: 0.0012235
[Epoch 7; Iter   432/ 2483] train: loss: 0.0013903
[Epoch 7; Iter   462/ 2483] train: loss: 0.0012975
[Epoch 7; Iter   492/ 2483] train: loss: 0.0022733
[Epoch 7; Iter   522/ 2483] train: loss: 0.0016750
[Epoch 7; Iter   552/ 2483] train: loss: 0.0018338
[Epoch 7; Iter   582/ 2483] train: loss: 0.0855174
[Epoch 7; Iter   612/ 2483] train: loss: 0.0809997
[Epoch 7; Iter   642/ 2483] train: loss: 0.0012349
[Epoch 7; Iter   672/ 2483] train: loss: 0.0674640
[Epoch 7; Iter   702/ 2483] train: loss: 0.0016993
[Epoch 7; Iter   732/ 2483] train: loss: 0.0020877
[Epoch 7; Iter   762/ 2483] train: loss: 0.0021507
[Epoch 7; Iter   792/ 2483] train: loss: 0.0025166
[Epoch 7; Iter   822/ 2483] train: loss: 0.0704775
[Epoch 7; Iter   852/ 2483] train: loss: 0.0018084
[Epoch 7; Iter   882/ 2483] train: loss: 0.0015153
[Epoch 7; Iter   912/ 2483] train: loss: 0.0020274
[Epoch 7; Iter   942/ 2483] train: loss: 0.0018322
[Epoch 7; Iter   972/ 2483] train: loss: 0.0019474
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0693852
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0019907
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0022442
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0017340
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0018030
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0013579
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0016883
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0850067
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0017649
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0595245
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0015717
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0014084
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0017243
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0015214
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0015432
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0019958
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0019780
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0023292
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0014636
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0012212
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0016799
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0024008
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0014619
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0022868
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0019044
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0032023
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0014671
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0016102
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0015111
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0014093
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0020356
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0019387
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0012746
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0017861
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0014492
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0018007
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0021198
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0019634
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0013679
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0015689
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0738792
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0016519
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0016221
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0017764
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0019820
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0776403
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0719994
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0028503
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0017997
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0022854
[Epoch 7] ogbg-molmuv: 0.048468 val loss: 0.014705
[Epoch 7] ogbg-molmuv: 0.048495 test loss: 0.016181
[Epoch 8; Iter    19/ 2483] train: loss: 0.0019247
[Epoch 8; Iter    49/ 2483] train: loss: 0.0754684
[Epoch 8; Iter    79/ 2483] train: loss: 0.0767298
[Epoch 8; Iter   109/ 2483] train: loss: 0.0740069
[Epoch 8; Iter   139/ 2483] train: loss: 0.0019580
[Epoch 8; Iter   169/ 2483] train: loss: 0.0669451
[Epoch 8; Iter   199/ 2483] train: loss: 0.0478618
[Epoch 8; Iter   229/ 2483] train: loss: 0.0019660
[Epoch 8; Iter   259/ 2483] train: loss: 0.0021279
[Epoch 8; Iter   289/ 2483] train: loss: 0.0017669
[Epoch 8; Iter   319/ 2483] train: loss: 0.0020437
[Epoch 8; Iter   349/ 2483] train: loss: 0.0021266
[Epoch 8; Iter   379/ 2483] train: loss: 0.0018600
[Epoch 8; Iter   409/ 2483] train: loss: 0.0743614
[Epoch 8; Iter   439/ 2483] train: loss: 0.0998869
[Epoch 8; Iter   469/ 2483] train: loss: 0.0017748
[Epoch 8; Iter   499/ 2483] train: loss: 0.0042284
[Epoch 8; Iter   529/ 2483] train: loss: 0.0021979
[Epoch 8; Iter   559/ 2483] train: loss: 0.0649241
[Epoch 8; Iter   589/ 2483] train: loss: 0.0021966
[Epoch 8; Iter   619/ 2483] train: loss: 0.0022387
[Epoch 8; Iter   649/ 2483] train: loss: 0.0021947
[Epoch 8; Iter   679/ 2483] train: loss: 0.0015318
[Epoch 8; Iter   709/ 2483] train: loss: 0.0024396
[Epoch 8; Iter   739/ 2483] train: loss: 0.0018009
[Epoch 8; Iter   769/ 2483] train: loss: 0.0759481
[Epoch 8; Iter   799/ 2483] train: loss: 0.0016106
[Epoch 8; Iter   829/ 2483] train: loss: 0.0019601
[Epoch 8; Iter   859/ 2483] train: loss: 0.0021031
[Epoch 8; Iter   889/ 2483] train: loss: 0.0751892
[Epoch 8; Iter   919/ 2483] train: loss: 0.0015177
[Epoch 8; Iter   949/ 2483] train: loss: 0.0018171
[Epoch 8; Iter   979/ 2483] train: loss: 0.0017811
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0021954
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0031470
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0020986
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0890995
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0019964
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0024861
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0017481
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0014734
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0021630
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0017854
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0013747
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0569313
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0028061
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0019136
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0018623
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0021091
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0016991
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0886795
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0016426
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0016594
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0019639
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0492716
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0036536
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0012832
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0012191
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0010886
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0016212
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0019661
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0022490
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0022132
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0011841
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0012349
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0044503
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0053015
[Epoch 8; Iter  2029/ 2483] train: loss: 0.1107710
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0016601
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0014207
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0022371
[Epoch 8; Iter  2149/ 2483] train: loss: 0.1328707
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0018438
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0015914
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0022323
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0013659
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0013705
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0013316
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0017582
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0020453
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0016361
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0013643
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0777899
[Epoch 8] ogbg-molmuv: 0.121200 val loss: 0.014132
[Epoch 8] ogbg-molmuv: 0.082969 test loss: 0.015592
[Epoch 9; Iter    26/ 2483] train: loss: 0.0014035
[Epoch 9; Iter    56/ 2483] train: loss: 0.1002127
[Epoch 9; Iter    86/ 2483] train: loss: 0.1073027
[Epoch 9; Iter   116/ 2483] train: loss: 0.0016048
[Epoch 9; Iter   146/ 2483] train: loss: 0.0015226
[Epoch 9; Iter   176/ 2483] train: loss: 0.0015273
[Epoch 9; Iter   206/ 2483] train: loss: 0.0024474
[Epoch 9; Iter   236/ 2483] train: loss: 0.0853387
[Epoch 9; Iter   266/ 2483] train: loss: 0.0020816
[Epoch 9; Iter   296/ 2483] train: loss: 0.0028503
[Epoch 9; Iter   326/ 2483] train: loss: 0.0022876
[Epoch 9; Iter   356/ 2483] train: loss: 0.0016893
[Epoch 9; Iter   386/ 2483] train: loss: 0.0018187
[Epoch 9; Iter   416/ 2483] train: loss: 0.0011965
[Epoch 9; Iter   446/ 2483] train: loss: 0.0017944
[Epoch 9; Iter   476/ 2483] train: loss: 0.0013680
[Epoch 9; Iter   506/ 2483] train: loss: 0.0748321
[Epoch 9; Iter   536/ 2483] train: loss: 0.0030939
[Epoch 9; Iter   566/ 2483] train: loss: 0.0020328
[Epoch 9; Iter   596/ 2483] train: loss: 0.0015442
[Epoch 9; Iter   626/ 2483] train: loss: 0.0026418
[Epoch 9; Iter   656/ 2483] train: loss: 0.0016630
[Epoch 9; Iter   686/ 2483] train: loss: 0.0012327
[Epoch 9; Iter   716/ 2483] train: loss: 0.0011649
[Epoch 9; Iter   746/ 2483] train: loss: 0.0013884
[Epoch 9; Iter   776/ 2483] train: loss: 0.1914465
[Epoch 9; Iter   806/ 2483] train: loss: 0.0015508
[Epoch 9; Iter   836/ 2483] train: loss: 0.0015624
[Epoch 9; Iter   866/ 2483] train: loss: 0.0017665
[Epoch 9; Iter   896/ 2483] train: loss: 0.0027824
[Epoch 9; Iter   926/ 2483] train: loss: 0.0884140
[Epoch 9; Iter   956/ 2483] train: loss: 0.0424149
[Epoch 9; Iter   986/ 2483] train: loss: 0.0024805
[Epoch 9; Iter  1016/ 2483] train: loss: 0.1058130
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0014904
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0017467
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0024329
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0019573
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0763814
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0815605
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0024558
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0026353
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0025961
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0016672
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0014617
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0017473
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0020901
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0015449
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0024453
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0684220
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0019269
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0022578
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0788579
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0021764
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0018436
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0578946
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0013704
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0011958
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0016893
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0015525
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0017234
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0719074
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0015584
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0014625
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0014446
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0021905
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0020818
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0014513
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0011842
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0011503
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0023138
[Epoch 6; Iter   185/ 2483] train: loss: 0.0017376
[Epoch 6; Iter   215/ 2483] train: loss: 0.0016225
[Epoch 6; Iter   245/ 2483] train: loss: 0.0020386
[Epoch 6; Iter   275/ 2483] train: loss: 0.0826668
[Epoch 6; Iter   305/ 2483] train: loss: 0.0018177
[Epoch 6; Iter   335/ 2483] train: loss: 0.0646832
[Epoch 6; Iter   365/ 2483] train: loss: 0.0021795
[Epoch 6; Iter   395/ 2483] train: loss: 0.0906880
[Epoch 6; Iter   425/ 2483] train: loss: 0.0023545
[Epoch 6; Iter   455/ 2483] train: loss: 0.0024368
[Epoch 6; Iter   485/ 2483] train: loss: 0.0024717
[Epoch 6; Iter   515/ 2483] train: loss: 0.0022247
[Epoch 6; Iter   545/ 2483] train: loss: 0.0021828
[Epoch 6; Iter   575/ 2483] train: loss: 0.0024858
[Epoch 6; Iter   605/ 2483] train: loss: 0.0015811
[Epoch 6; Iter   635/ 2483] train: loss: 0.0014986
[Epoch 6; Iter   665/ 2483] train: loss: 0.0013549
[Epoch 6; Iter   695/ 2483] train: loss: 0.0018482
[Epoch 6; Iter   725/ 2483] train: loss: 0.0014451
[Epoch 6; Iter   755/ 2483] train: loss: 0.0017126
[Epoch 6; Iter   785/ 2483] train: loss: 0.0019532
[Epoch 6; Iter   815/ 2483] train: loss: 0.0022540
[Epoch 6; Iter   845/ 2483] train: loss: 0.0020530
[Epoch 6; Iter   875/ 2483] train: loss: 0.0014243
[Epoch 6; Iter   905/ 2483] train: loss: 0.0018105
[Epoch 6; Iter   935/ 2483] train: loss: 0.0015274
[Epoch 6; Iter   965/ 2483] train: loss: 0.0020084
[Epoch 6; Iter   995/ 2483] train: loss: 0.0017210
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0860314
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0021363
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0021241
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0019837
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0015540
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0018742
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0016281
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0024609
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0016989
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0021010
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0609449
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0020893
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0019668
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0022981
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0020736
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0021838
[Epoch 6; Iter  1505/ 2483] train: loss: 0.1412167
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0028004
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0026253
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0026688
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0685728
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0018958
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0017018
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0019492
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0022406
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0018742
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0020015
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0021080
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0020905
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0019718
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0017225
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0019137
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0788046
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0929958
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0018746
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0016403
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0018464
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0017998
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0688590
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0022074
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0020000
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0015818
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0646698
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0019688
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0020254
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0017024
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0017089
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0016110
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0013621
[Epoch 6] ogbg-molmuv: 0.016802 val loss: 0.015493
[Epoch 6] ogbg-molmuv: 0.046914 test loss: 0.016344
[Epoch 7; Iter    12/ 2483] train: loss: 0.0013207
[Epoch 7; Iter    42/ 2483] train: loss: 0.0014408
[Epoch 7; Iter    72/ 2483] train: loss: 0.0013013
[Epoch 7; Iter   102/ 2483] train: loss: 0.0032761
[Epoch 7; Iter   132/ 2483] train: loss: 0.0367299
[Epoch 7; Iter   162/ 2483] train: loss: 0.0022018
[Epoch 7; Iter   192/ 2483] train: loss: 0.0020871
[Epoch 7; Iter   222/ 2483] train: loss: 0.0028389
[Epoch 7; Iter   252/ 2483] train: loss: 0.0024228
[Epoch 7; Iter   282/ 2483] train: loss: 0.0018289
[Epoch 7; Iter   312/ 2483] train: loss: 0.0014357
[Epoch 7; Iter   342/ 2483] train: loss: 0.0017967
[Epoch 7; Iter   372/ 2483] train: loss: 0.0015364
[Epoch 7; Iter   402/ 2483] train: loss: 0.0017605
[Epoch 7; Iter   432/ 2483] train: loss: 0.0012681
[Epoch 7; Iter   462/ 2483] train: loss: 0.0014334
[Epoch 7; Iter   492/ 2483] train: loss: 0.0013162
[Epoch 7; Iter   522/ 2483] train: loss: 0.0012619
[Epoch 7; Iter   552/ 2483] train: loss: 0.0014281
[Epoch 7; Iter   582/ 2483] train: loss: 0.0014351
[Epoch 7; Iter   612/ 2483] train: loss: 0.0022108
[Epoch 7; Iter   642/ 2483] train: loss: 0.0016736
[Epoch 7; Iter   672/ 2483] train: loss: 0.0015904
[Epoch 7; Iter   702/ 2483] train: loss: 0.0021135
[Epoch 7; Iter   732/ 2483] train: loss: 0.0016639
[Epoch 7; Iter   762/ 2483] train: loss: 0.0011850
[Epoch 7; Iter   792/ 2483] train: loss: 0.0014846
[Epoch 7; Iter   822/ 2483] train: loss: 0.0022488
[Epoch 7; Iter   852/ 2483] train: loss: 0.0582345
[Epoch 7; Iter   882/ 2483] train: loss: 0.0013411
[Epoch 7; Iter   912/ 2483] train: loss: 0.0018839
[Epoch 7; Iter   942/ 2483] train: loss: 0.0013623
[Epoch 7; Iter   972/ 2483] train: loss: 0.0015380
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0907528
[Epoch 7; Iter  1032/ 2483] train: loss: 0.0882050
[Epoch 7; Iter  1062/ 2483] train: loss: 0.1596209
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0020338
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0016062
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0984092
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0014168
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0017655
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0018126
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0017763
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0016034
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0018583
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0019123
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0018217
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0649426
[Epoch 7; Iter  1452/ 2483] train: loss: 0.1117836
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0019257
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0018172
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0018875
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0664786
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0776839
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0015257
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0699958
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0018866
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0019885
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0016890
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0023450
[Epoch 7; Iter  1812/ 2483] train: loss: 0.1298940
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0026527
[Epoch 7; Iter  1872/ 2483] train: loss: 0.0024762
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0021692
[Epoch 7; Iter  1932/ 2483] train: loss: 0.0020662
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0022211
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0021611
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0020706
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0022507
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0016297
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0012361
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0013442
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0014671
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0012850
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0773147
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0011439
[Epoch 7; Iter  2292/ 2483] train: loss: 0.0016958
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0022012
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0028892
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0024981
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0023060
[Epoch 6; Iter   185/ 2483] train: loss: 0.0018947
[Epoch 6; Iter   215/ 2483] train: loss: 0.0019166
[Epoch 6; Iter   245/ 2483] train: loss: 0.0748985
[Epoch 6; Iter   275/ 2483] train: loss: 0.0016252
[Epoch 6; Iter   305/ 2483] train: loss: 0.0018622
[Epoch 6; Iter   335/ 2483] train: loss: 0.0017327
[Epoch 6; Iter   365/ 2483] train: loss: 0.0018297
[Epoch 6; Iter   395/ 2483] train: loss: 0.0016835
[Epoch 6; Iter   425/ 2483] train: loss: 0.0021496
[Epoch 6; Iter   455/ 2483] train: loss: 0.0021231
[Epoch 6; Iter   485/ 2483] train: loss: 0.0016320
[Epoch 6; Iter   515/ 2483] train: loss: 0.0577034
[Epoch 6; Iter   545/ 2483] train: loss: 0.1637787
[Epoch 6; Iter   575/ 2483] train: loss: 0.0025357
[Epoch 6; Iter   605/ 2483] train: loss: 0.0027303
[Epoch 6; Iter   635/ 2483] train: loss: 0.0020919
[Epoch 6; Iter   665/ 2483] train: loss: 0.0024134
[Epoch 6; Iter   695/ 2483] train: loss: 0.0017050
[Epoch 6; Iter   725/ 2483] train: loss: 0.0810305
[Epoch 6; Iter   755/ 2483] train: loss: 0.0016025
[Epoch 6; Iter   785/ 2483] train: loss: 0.0018271
[Epoch 6; Iter   815/ 2483] train: loss: 0.0019796
[Epoch 6; Iter   845/ 2483] train: loss: 0.0014335
[Epoch 6; Iter   875/ 2483] train: loss: 0.0013567
[Epoch 6; Iter   905/ 2483] train: loss: 0.0856570
[Epoch 6; Iter   935/ 2483] train: loss: 0.0017056
[Epoch 6; Iter   965/ 2483] train: loss: 0.0020833
[Epoch 6; Iter   995/ 2483] train: loss: 0.0021601
[Epoch 6; Iter  1025/ 2483] train: loss: 0.0023229
[Epoch 6; Iter  1055/ 2483] train: loss: 0.0019260
[Epoch 6; Iter  1085/ 2483] train: loss: 0.0025946
[Epoch 6; Iter  1115/ 2483] train: loss: 0.0016550
[Epoch 6; Iter  1145/ 2483] train: loss: 0.0020193
[Epoch 6; Iter  1175/ 2483] train: loss: 0.0022691
[Epoch 6; Iter  1205/ 2483] train: loss: 0.0017968
[Epoch 6; Iter  1235/ 2483] train: loss: 0.0022045
[Epoch 6; Iter  1265/ 2483] train: loss: 0.0756798
[Epoch 6; Iter  1295/ 2483] train: loss: 0.0018374
[Epoch 6; Iter  1325/ 2483] train: loss: 0.0018588
[Epoch 6; Iter  1355/ 2483] train: loss: 0.0015134
[Epoch 6; Iter  1385/ 2483] train: loss: 0.0733355
[Epoch 6; Iter  1415/ 2483] train: loss: 0.0015858
[Epoch 6; Iter  1445/ 2483] train: loss: 0.0022885
[Epoch 6; Iter  1475/ 2483] train: loss: 0.0027574
[Epoch 6; Iter  1505/ 2483] train: loss: 0.0021496
[Epoch 6; Iter  1535/ 2483] train: loss: 0.0019863
[Epoch 6; Iter  1565/ 2483] train: loss: 0.0673676
[Epoch 6; Iter  1595/ 2483] train: loss: 0.0030828
[Epoch 6; Iter  1625/ 2483] train: loss: 0.0015040
[Epoch 6; Iter  1655/ 2483] train: loss: 0.0014333
[Epoch 6; Iter  1685/ 2483] train: loss: 0.0012704
[Epoch 6; Iter  1715/ 2483] train: loss: 0.0020172
[Epoch 6; Iter  1745/ 2483] train: loss: 0.0013620
[Epoch 6; Iter  1775/ 2483] train: loss: 0.0015976
[Epoch 6; Iter  1805/ 2483] train: loss: 0.0013396
[Epoch 6; Iter  1835/ 2483] train: loss: 0.0015831
[Epoch 6; Iter  1865/ 2483] train: loss: 0.0015165
[Epoch 6; Iter  1895/ 2483] train: loss: 0.0015078
[Epoch 6; Iter  1925/ 2483] train: loss: 0.0019275
[Epoch 6; Iter  1955/ 2483] train: loss: 0.0017203
[Epoch 6; Iter  1985/ 2483] train: loss: 0.0014653
[Epoch 6; Iter  2015/ 2483] train: loss: 0.0017373
[Epoch 6; Iter  2045/ 2483] train: loss: 0.0018208
[Epoch 6; Iter  2075/ 2483] train: loss: 0.0016401
[Epoch 6; Iter  2105/ 2483] train: loss: 0.0020153
[Epoch 6; Iter  2135/ 2483] train: loss: 0.0600001
[Epoch 6; Iter  2165/ 2483] train: loss: 0.0021098
[Epoch 6; Iter  2195/ 2483] train: loss: 0.0023499
[Epoch 6; Iter  2225/ 2483] train: loss: 0.0020178
[Epoch 6; Iter  2255/ 2483] train: loss: 0.0020468
[Epoch 6; Iter  2285/ 2483] train: loss: 0.0029795
[Epoch 6; Iter  2315/ 2483] train: loss: 0.0024357
[Epoch 6; Iter  2345/ 2483] train: loss: 0.0023703
[Epoch 6; Iter  2375/ 2483] train: loss: 0.0992654
[Epoch 6; Iter  2405/ 2483] train: loss: 0.0680029
[Epoch 6; Iter  2435/ 2483] train: loss: 0.0016087
[Epoch 6; Iter  2465/ 2483] train: loss: 0.0015830
[Epoch 6] ogbg-molmuv: 0.012868 val loss: 0.015715
[Epoch 6] ogbg-molmuv: 0.014491 test loss: 0.015568
[Epoch 7; Iter    12/ 2483] train: loss: 0.0014418
[Epoch 7; Iter    42/ 2483] train: loss: 0.0014793
[Epoch 7; Iter    72/ 2483] train: loss: 0.0015751
[Epoch 7; Iter   102/ 2483] train: loss: 0.0017049
[Epoch 7; Iter   132/ 2483] train: loss: 0.0014201
[Epoch 7; Iter   162/ 2483] train: loss: 0.0550662
[Epoch 7; Iter   192/ 2483] train: loss: 0.0512106
[Epoch 7; Iter   222/ 2483] train: loss: 0.0014412
[Epoch 7; Iter   252/ 2483] train: loss: 0.0073275
[Epoch 7; Iter   282/ 2483] train: loss: 0.0020006
[Epoch 7; Iter   312/ 2483] train: loss: 0.0017734
[Epoch 7; Iter   342/ 2483] train: loss: 0.0875275
[Epoch 7; Iter   372/ 2483] train: loss: 0.0022137
[Epoch 7; Iter   402/ 2483] train: loss: 0.0021206
[Epoch 7; Iter   432/ 2483] train: loss: 0.0023790
[Epoch 7; Iter   462/ 2483] train: loss: 0.0017425
[Epoch 7; Iter   492/ 2483] train: loss: 0.0017700
[Epoch 7; Iter   522/ 2483] train: loss: 0.0015572
[Epoch 7; Iter   552/ 2483] train: loss: 0.0017661
[Epoch 7; Iter   582/ 2483] train: loss: 0.0015012
[Epoch 7; Iter   612/ 2483] train: loss: 0.0015915
[Epoch 7; Iter   642/ 2483] train: loss: 0.0013242
[Epoch 7; Iter   672/ 2483] train: loss: 0.0014019
[Epoch 7; Iter   702/ 2483] train: loss: 0.1136836
[Epoch 7; Iter   732/ 2483] train: loss: 0.0017632
[Epoch 7; Iter   762/ 2483] train: loss: 0.0828901
[Epoch 7; Iter   792/ 2483] train: loss: 0.0017111
[Epoch 7; Iter   822/ 2483] train: loss: 0.0015875
[Epoch 7; Iter   852/ 2483] train: loss: 0.0012769
[Epoch 7; Iter   882/ 2483] train: loss: 0.0014640
[Epoch 7; Iter   912/ 2483] train: loss: 0.0018412
[Epoch 7; Iter   942/ 2483] train: loss: 0.0024961
[Epoch 7; Iter   972/ 2483] train: loss: 0.0018318
[Epoch 7; Iter  1002/ 2483] train: loss: 0.0020332
[Epoch 7; Iter  1032/ 2483] train: loss: 0.2054977
[Epoch 7; Iter  1062/ 2483] train: loss: 0.0014597
[Epoch 7; Iter  1092/ 2483] train: loss: 0.0786071
[Epoch 7; Iter  1122/ 2483] train: loss: 0.0016825
[Epoch 7; Iter  1152/ 2483] train: loss: 0.0019912
[Epoch 7; Iter  1182/ 2483] train: loss: 0.0021050
[Epoch 7; Iter  1212/ 2483] train: loss: 0.0019027
[Epoch 7; Iter  1242/ 2483] train: loss: 0.0016727
[Epoch 7; Iter  1272/ 2483] train: loss: 0.0689684
[Epoch 7; Iter  1302/ 2483] train: loss: 0.0016081
[Epoch 7; Iter  1332/ 2483] train: loss: 0.0014847
[Epoch 7; Iter  1362/ 2483] train: loss: 0.0020817
[Epoch 7; Iter  1392/ 2483] train: loss: 0.0014070
[Epoch 7; Iter  1422/ 2483] train: loss: 0.0015320
[Epoch 7; Iter  1452/ 2483] train: loss: 0.0017793
[Epoch 7; Iter  1482/ 2483] train: loss: 0.0015307
[Epoch 7; Iter  1512/ 2483] train: loss: 0.0014460
[Epoch 7; Iter  1542/ 2483] train: loss: 0.0015701
[Epoch 7; Iter  1572/ 2483] train: loss: 0.0016356
[Epoch 7; Iter  1602/ 2483] train: loss: 0.0013077
[Epoch 7; Iter  1632/ 2483] train: loss: 0.0014144
[Epoch 7; Iter  1662/ 2483] train: loss: 0.0014713
[Epoch 7; Iter  1692/ 2483] train: loss: 0.0015315
[Epoch 7; Iter  1722/ 2483] train: loss: 0.0013661
[Epoch 7; Iter  1752/ 2483] train: loss: 0.0012062
[Epoch 7; Iter  1782/ 2483] train: loss: 0.0014841
[Epoch 7; Iter  1812/ 2483] train: loss: 0.0016660
[Epoch 7; Iter  1842/ 2483] train: loss: 0.0014915
[Epoch 7; Iter  1872/ 2483] train: loss: 0.1776772
[Epoch 7; Iter  1902/ 2483] train: loss: 0.0019423
[Epoch 7; Iter  1932/ 2483] train: loss: 0.1064684
[Epoch 7; Iter  1962/ 2483] train: loss: 0.0018862
[Epoch 7; Iter  1992/ 2483] train: loss: 0.0019121
[Epoch 7; Iter  2022/ 2483] train: loss: 0.0016289
[Epoch 7; Iter  2052/ 2483] train: loss: 0.0021017
[Epoch 7; Iter  2082/ 2483] train: loss: 0.0024242
[Epoch 7; Iter  2112/ 2483] train: loss: 0.0023517
[Epoch 7; Iter  2142/ 2483] train: loss: 0.0024314
[Epoch 7; Iter  2172/ 2483] train: loss: 0.0031197
[Epoch 7; Iter  2202/ 2483] train: loss: 0.0023869
[Epoch 7; Iter  2232/ 2483] train: loss: 0.0023115
[Epoch 7; Iter  2262/ 2483] train: loss: 0.0758704
[Epoch 7; Iter  2292/ 2483] train: loss: 0.1045387
[Epoch 7; Iter  2322/ 2483] train: loss: 0.0020050
[Epoch 7; Iter  2352/ 2483] train: loss: 0.0021665
[Epoch 7; Iter  2382/ 2483] train: loss: 0.0021364
[Epoch 7; Iter  2412/ 2483] train: loss: 0.0023373
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0018888
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0016102
[Epoch 7] ogbg-molmuv: 0.066419 val loss: 0.016803
[Epoch 7] ogbg-molmuv: 0.052229 test loss: 0.015543
[Epoch 8; Iter    19/ 2483] train: loss: 0.0023156
[Epoch 8; Iter    49/ 2483] train: loss: 0.0022063
[Epoch 8; Iter    79/ 2483] train: loss: 0.0017974
[Epoch 8; Iter   109/ 2483] train: loss: 0.0012298
[Epoch 8; Iter   139/ 2483] train: loss: 0.0012729
[Epoch 8; Iter   169/ 2483] train: loss: 0.0012996
[Epoch 8; Iter   199/ 2483] train: loss: 0.0034139
[Epoch 8; Iter   229/ 2483] train: loss: 0.0016860
[Epoch 8; Iter   259/ 2483] train: loss: 0.0830086
[Epoch 8; Iter   289/ 2483] train: loss: 0.0017904
[Epoch 8; Iter   319/ 2483] train: loss: 0.0022282
[Epoch 8; Iter   349/ 2483] train: loss: 0.0016982
[Epoch 8; Iter   379/ 2483] train: loss: 0.0018022
[Epoch 8; Iter   409/ 2483] train: loss: 0.0037816
[Epoch 8; Iter   439/ 2483] train: loss: 0.0020242
[Epoch 8; Iter   469/ 2483] train: loss: 0.0040076
[Epoch 8; Iter   499/ 2483] train: loss: 0.0029387
[Epoch 8; Iter   529/ 2483] train: loss: 0.0022627
[Epoch 8; Iter   559/ 2483] train: loss: 0.0021420
[Epoch 8; Iter   589/ 2483] train: loss: 0.0725781
[Epoch 8; Iter   619/ 2483] train: loss: 0.0017546
[Epoch 8; Iter   649/ 2483] train: loss: 0.0016420
[Epoch 8; Iter   679/ 2483] train: loss: 0.0930469
[Epoch 8; Iter   709/ 2483] train: loss: 0.0017607
[Epoch 8; Iter   739/ 2483] train: loss: 0.0012664
[Epoch 8; Iter   769/ 2483] train: loss: 0.0035145
[Epoch 8; Iter   799/ 2483] train: loss: 0.0015179
[Epoch 8; Iter   829/ 2483] train: loss: 0.0017426
[Epoch 8; Iter   859/ 2483] train: loss: 0.0020350
[Epoch 8; Iter   889/ 2483] train: loss: 0.0017460
[Epoch 8; Iter   919/ 2483] train: loss: 0.0017190
[Epoch 8; Iter   949/ 2483] train: loss: 0.0020191
[Epoch 8; Iter   979/ 2483] train: loss: 0.0021200
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0014974
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0018707
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0034357
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0015191
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0012940
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0014700
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0803577
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0018768
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0023692
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0020164
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0020749
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0024861
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0026112
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0033850
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0022396
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0021624
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0025246
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0020129
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0026180
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0015642
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0023211
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0020534
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0019749
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0016256
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0015281
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0015168
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0012610
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0025994
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0031434
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0012486
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0018655
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0022512
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0014177
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0013769
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0023648
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0014324
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0806747
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0014576
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0012707
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0010014
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0011843
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0009398
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0010663
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0011603
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0865666
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0011213
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0671125
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0013470
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0012338
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0010576
[Epoch 8] ogbg-molmuv: 0.059911 val loss: 0.014224
[Epoch 8] ogbg-molmuv: 0.032400 test loss: 0.015157
[Epoch 9; Iter    26/ 2483] train: loss: 0.0014198
[Epoch 9; Iter    56/ 2483] train: loss: 0.0013518
[Epoch 9; Iter    86/ 2483] train: loss: 0.0018016
[Epoch 9; Iter   116/ 2483] train: loss: 0.0021016
[Epoch 9; Iter   146/ 2483] train: loss: 0.0011164
[Epoch 9; Iter   176/ 2483] train: loss: 0.0011410
[Epoch 9; Iter   206/ 2483] train: loss: 0.0012833
[Epoch 9; Iter   236/ 2483] train: loss: 0.0013094
[Epoch 9; Iter   266/ 2483] train: loss: 0.0010946
[Epoch 9; Iter   296/ 2483] train: loss: 0.0019033
[Epoch 9; Iter   326/ 2483] train: loss: 0.0013373
[Epoch 9; Iter   356/ 2483] train: loss: 0.0017866
[Epoch 9; Iter   386/ 2483] train: loss: 0.0662147
[Epoch 9; Iter   416/ 2483] train: loss: 0.0024126
[Epoch 9; Iter   446/ 2483] train: loss: 0.0020868
[Epoch 9; Iter   476/ 2483] train: loss: 0.0021117
[Epoch 9; Iter   506/ 2483] train: loss: 0.0017152
[Epoch 9; Iter   536/ 2483] train: loss: 0.0014466
[Epoch 9; Iter   566/ 2483] train: loss: 0.0015056
[Epoch 9; Iter   596/ 2483] train: loss: 0.0017748
[Epoch 9; Iter   626/ 2483] train: loss: 0.0022041
[Epoch 9; Iter   656/ 2483] train: loss: 0.0038607
[Epoch 9; Iter   686/ 2483] train: loss: 0.0017267
[Epoch 9; Iter   716/ 2483] train: loss: 0.0867208
[Epoch 9; Iter   746/ 2483] train: loss: 0.0018320
[Epoch 9; Iter   776/ 2483] train: loss: 0.0013146
[Epoch 9; Iter   806/ 2483] train: loss: 0.0017207
[Epoch 9; Iter   836/ 2483] train: loss: 0.0012225
[Epoch 9; Iter   866/ 2483] train: loss: 0.0014398
[Epoch 9; Iter   896/ 2483] train: loss: 0.0021047
[Epoch 9; Iter   926/ 2483] train: loss: 0.0456595
[Epoch 9; Iter   956/ 2483] train: loss: 0.0027694
[Epoch 9; Iter   986/ 2483] train: loss: 0.0021441
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0018600
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0015526
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0018829
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0681274
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0024252
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0028267
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0016930
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0017121
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0014979
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0020738
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0021051
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0022700
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0018094
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0037440
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0027249
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0018419
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0033017
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0022963
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0010176
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0018538
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0017637
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0041332
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0015975
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0514657
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0010301
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0010638
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0009745
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0011080
[Epoch 9; Iter  1856/ 2483] train: loss: 0.1158561
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0017356
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0584788
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0014789
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0040342
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0689458
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0493079
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0021488
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0017059
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0879179
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0023367
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0019818
[Epoch 7] ogbg-molmuv: 0.026659 val loss: 0.014954
[Epoch 7] ogbg-molmuv: 0.034288 test loss: 0.016083
[Epoch 8; Iter    19/ 2483] train: loss: 0.0021451
[Epoch 8; Iter    49/ 2483] train: loss: 0.0020249
[Epoch 8; Iter    79/ 2483] train: loss: 0.0022505
[Epoch 8; Iter   109/ 2483] train: loss: 0.0027166
[Epoch 8; Iter   139/ 2483] train: loss: 0.0033053
[Epoch 8; Iter   169/ 2483] train: loss: 0.0538177
[Epoch 8; Iter   199/ 2483] train: loss: 0.0024033
[Epoch 8; Iter   229/ 2483] train: loss: 0.0031889
[Epoch 8; Iter   259/ 2483] train: loss: 0.0034870
[Epoch 8; Iter   289/ 2483] train: loss: 0.0031453
[Epoch 8; Iter   319/ 2483] train: loss: 0.0020027
[Epoch 8; Iter   349/ 2483] train: loss: 0.0021078
[Epoch 8; Iter   379/ 2483] train: loss: 0.0018944
[Epoch 8; Iter   409/ 2483] train: loss: 0.0019792
[Epoch 8; Iter   439/ 2483] train: loss: 0.0018537
[Epoch 8; Iter   469/ 2483] train: loss: 0.0020547
[Epoch 8; Iter   499/ 2483] train: loss: 0.0014792
[Epoch 8; Iter   529/ 2483] train: loss: 0.0020740
[Epoch 8; Iter   559/ 2483] train: loss: 0.0019319
[Epoch 8; Iter   589/ 2483] train: loss: 0.0019706
[Epoch 8; Iter   619/ 2483] train: loss: 0.1005781
[Epoch 8; Iter   649/ 2483] train: loss: 0.0021172
[Epoch 8; Iter   679/ 2483] train: loss: 0.0013409
[Epoch 8; Iter   709/ 2483] train: loss: 0.0025427
[Epoch 8; Iter   739/ 2483] train: loss: 0.0019887
[Epoch 8; Iter   769/ 2483] train: loss: 0.0022238
[Epoch 8; Iter   799/ 2483] train: loss: 0.0020707
[Epoch 8; Iter   829/ 2483] train: loss: 0.0016478
[Epoch 8; Iter   859/ 2483] train: loss: 0.0774029
[Epoch 8; Iter   889/ 2483] train: loss: 0.0016274
[Epoch 8; Iter   919/ 2483] train: loss: 0.0012090
[Epoch 8; Iter   949/ 2483] train: loss: 0.0014145
[Epoch 8; Iter   979/ 2483] train: loss: 0.0011192
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0012061
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0673428
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0020460
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0019270
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0013132
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0804630
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0017501
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0021019
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0016235
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0015246
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0868059
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0015669
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0014250
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0009790
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0701970
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0020096
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0022907
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0012656
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0015164
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0016953
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0015402
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0020774
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0022979
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0024163
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0476449
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0020069
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0026523
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0021425
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0028732
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0023903
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0022704
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0027688
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0688161
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0016913
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0013351
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0841983
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0013638
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0011213
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0014883
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0019554
[Epoch 8; Iter  2209/ 2483] train: loss: 0.1817493
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0023217
[Epoch 8; Iter  2269/ 2483] train: loss: 0.1030126
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0797217
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0016870
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0019327
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0016843
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0021565
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0019336
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0021214
[Epoch 8] ogbg-molmuv: 0.025716 val loss: 0.029485
[Epoch 8] ogbg-molmuv: 0.014923 test loss: 0.032192
[Epoch 9; Iter    26/ 2483] train: loss: 0.0017900
[Epoch 9; Iter    56/ 2483] train: loss: 0.0019023
[Epoch 9; Iter    86/ 2483] train: loss: 0.0539751
[Epoch 9; Iter   116/ 2483] train: loss: 0.0017630
[Epoch 9; Iter   146/ 2483] train: loss: 0.0016091
[Epoch 9; Iter   176/ 2483] train: loss: 0.0025765
[Epoch 9; Iter   206/ 2483] train: loss: 0.0019245
[Epoch 9; Iter   236/ 2483] train: loss: 0.0018184
[Epoch 9; Iter   266/ 2483] train: loss: 0.0026751
[Epoch 9; Iter   296/ 2483] train: loss: 0.0014332
[Epoch 9; Iter   326/ 2483] train: loss: 0.0009882
[Epoch 9; Iter   356/ 2483] train: loss: 0.0010715
[Epoch 9; Iter   386/ 2483] train: loss: 0.0017430
[Epoch 9; Iter   416/ 2483] train: loss: 0.0014837
[Epoch 9; Iter   446/ 2483] train: loss: 0.0010815
[Epoch 9; Iter   476/ 2483] train: loss: 0.0014108
[Epoch 9; Iter   506/ 2483] train: loss: 0.0022575
[Epoch 9; Iter   536/ 2483] train: loss: 0.0024335
[Epoch 9; Iter   566/ 2483] train: loss: 0.0016919
[Epoch 9; Iter   596/ 2483] train: loss: 0.0018127
[Epoch 9; Iter   626/ 2483] train: loss: 0.0013652
[Epoch 9; Iter   656/ 2483] train: loss: 0.0010797
[Epoch 9; Iter   686/ 2483] train: loss: 0.0012736
[Epoch 9; Iter   716/ 2483] train: loss: 0.0013429
[Epoch 9; Iter   746/ 2483] train: loss: 0.0014493
[Epoch 9; Iter   776/ 2483] train: loss: 0.0015727
[Epoch 9; Iter   806/ 2483] train: loss: 0.0954901
[Epoch 9; Iter   836/ 2483] train: loss: 0.0017864
[Epoch 9; Iter   866/ 2483] train: loss: 0.0017941
[Epoch 9; Iter   896/ 2483] train: loss: 0.0021387
[Epoch 9; Iter   926/ 2483] train: loss: 0.0014901
[Epoch 9; Iter   956/ 2483] train: loss: 0.0021428
[Epoch 9; Iter   986/ 2483] train: loss: 0.0022293
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0042189
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0017223
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0013182
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0019051
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0027594
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0570457
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0021031
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0014779
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0017287
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0016988
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0029371
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0023341
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0022141
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0655053
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0013072
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0012756
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0020213
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0016782
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0841859
[Epoch 9; Iter  1586/ 2483] train: loss: 0.1069649
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0023526
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0021963
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0022379
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0023198
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0031660
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0027918
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0019829
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0020816
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0017111
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0020706
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0017520
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0020610
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0018624
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0028961
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0019935
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0019690
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0021655
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0011036
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0020293
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0025060
[Epoch 7] ogbg-molmuv: 0.021693 val loss: 0.015088
[Epoch 7] ogbg-molmuv: 0.034725 test loss: 0.015833
[Epoch 8; Iter    19/ 2483] train: loss: 0.0020978
[Epoch 8; Iter    49/ 2483] train: loss: 0.0687293
[Epoch 8; Iter    79/ 2483] train: loss: 0.0851422
[Epoch 8; Iter   109/ 2483] train: loss: 0.0744528
[Epoch 8; Iter   139/ 2483] train: loss: 0.0018864
[Epoch 8; Iter   169/ 2483] train: loss: 0.0642224
[Epoch 8; Iter   199/ 2483] train: loss: 0.0625676
[Epoch 8; Iter   229/ 2483] train: loss: 0.0019892
[Epoch 8; Iter   259/ 2483] train: loss: 0.0024226
[Epoch 8; Iter   289/ 2483] train: loss: 0.0020337
[Epoch 8; Iter   319/ 2483] train: loss: 0.0021110
[Epoch 8; Iter   349/ 2483] train: loss: 0.0021443
[Epoch 8; Iter   379/ 2483] train: loss: 0.0020241
[Epoch 8; Iter   409/ 2483] train: loss: 0.0660250
[Epoch 8; Iter   439/ 2483] train: loss: 0.0889739
[Epoch 8; Iter   469/ 2483] train: loss: 0.0016295
[Epoch 8; Iter   499/ 2483] train: loss: 0.0044649
[Epoch 8; Iter   529/ 2483] train: loss: 0.0023906
[Epoch 8; Iter   559/ 2483] train: loss: 0.0678194
[Epoch 8; Iter   589/ 2483] train: loss: 0.0028377
[Epoch 8; Iter   619/ 2483] train: loss: 0.0018028
[Epoch 8; Iter   649/ 2483] train: loss: 0.0023933
[Epoch 8; Iter   679/ 2483] train: loss: 0.0017679
[Epoch 8; Iter   709/ 2483] train: loss: 0.0018080
[Epoch 8; Iter   739/ 2483] train: loss: 0.0019478
[Epoch 8; Iter   769/ 2483] train: loss: 0.1037012
[Epoch 8; Iter   799/ 2483] train: loss: 0.0014318
[Epoch 8; Iter   829/ 2483] train: loss: 0.0040375
[Epoch 8; Iter   859/ 2483] train: loss: 0.0023217
[Epoch 8; Iter   889/ 2483] train: loss: 0.0819917
[Epoch 8; Iter   919/ 2483] train: loss: 0.0015442
[Epoch 8; Iter   949/ 2483] train: loss: 0.0019032
[Epoch 8; Iter   979/ 2483] train: loss: 0.0015778
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0019989
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0029164
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0020093
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0742207
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0026091
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0018805
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0017374
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0014995
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0021203
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0016840
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0017006
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0528125
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0021121
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0021636
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0022908
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0016186
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0016784
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0832014
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0016660
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0022986
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0016031
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0880270
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0017535
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0017846
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0016127
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0012587
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0020503
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0017408
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0023046
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0019425
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0011530
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0014362
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0016929
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0013032
[Epoch 8; Iter  2029/ 2483] train: loss: 0.1065155
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0013543
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0012828
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0015817
[Epoch 8; Iter  2149/ 2483] train: loss: 0.1391524
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0019944
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0019157
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0018198
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0014934
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0014844
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0014518
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0017491
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0016776
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0016710
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0014886
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0820337
[Epoch 8] ogbg-molmuv: 0.025441 val loss: 0.015282
[Epoch 8] ogbg-molmuv: 0.050888 test loss: 0.015657
[Epoch 9; Iter    26/ 2483] train: loss: 0.0016363
[Epoch 9; Iter    56/ 2483] train: loss: 0.0852946
[Epoch 9; Iter    86/ 2483] train: loss: 0.1020233
[Epoch 9; Iter   116/ 2483] train: loss: 0.0027657
[Epoch 9; Iter   146/ 2483] train: loss: 0.0018120
[Epoch 9; Iter   176/ 2483] train: loss: 0.0018373
[Epoch 9; Iter   206/ 2483] train: loss: 0.0018137
[Epoch 9; Iter   236/ 2483] train: loss: 0.0621798
[Epoch 9; Iter   266/ 2483] train: loss: 0.0027751
[Epoch 9; Iter   296/ 2483] train: loss: 0.0019003
[Epoch 9; Iter   326/ 2483] train: loss: 0.0021753
[Epoch 9; Iter   356/ 2483] train: loss: 0.0017389
[Epoch 9; Iter   386/ 2483] train: loss: 0.0013790
[Epoch 9; Iter   416/ 2483] train: loss: 0.0016781
[Epoch 9; Iter   446/ 2483] train: loss: 0.0019706
[Epoch 9; Iter   476/ 2483] train: loss: 0.0016269
[Epoch 9; Iter   506/ 2483] train: loss: 0.0743474
[Epoch 9; Iter   536/ 2483] train: loss: 0.0021164
[Epoch 9; Iter   566/ 2483] train: loss: 0.0014604
[Epoch 9; Iter   596/ 2483] train: loss: 0.0023252
[Epoch 9; Iter   626/ 2483] train: loss: 0.0018321
[Epoch 9; Iter   656/ 2483] train: loss: 0.0016857
[Epoch 9; Iter   686/ 2483] train: loss: 0.0012933
[Epoch 9; Iter   716/ 2483] train: loss: 0.0014564
[Epoch 9; Iter   746/ 2483] train: loss: 0.0018708
[Epoch 9; Iter   776/ 2483] train: loss: 0.1498432
[Epoch 9; Iter   806/ 2483] train: loss: 0.0015848
[Epoch 9; Iter   836/ 2483] train: loss: 0.0016804
[Epoch 9; Iter   866/ 2483] train: loss: 0.0017434
[Epoch 9; Iter   896/ 2483] train: loss: 0.0021503
[Epoch 9; Iter   926/ 2483] train: loss: 0.0928682
[Epoch 9; Iter   956/ 2483] train: loss: 0.0426920
[Epoch 9; Iter   986/ 2483] train: loss: 0.0022734
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0975327
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0016705
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0023268
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0022205
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0028361
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0724275
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0790624
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0026280
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0023344
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0022386
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0018463
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0019903
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0021680
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0021769
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0020296
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0020211
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0670783
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0016306
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0019315
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0804237
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0021208
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0019674
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0604367
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0014488
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0013020
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0017802
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0017414
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0017094
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0906371
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0016349
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0012344
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0016302
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0026837
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0019907
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0015995
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0015387
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0017483
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0017113
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0027825
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0017699
[Epoch 7] ogbg-molmuv: 0.025323 val loss: 0.119055
[Epoch 7] ogbg-molmuv: 0.035079 test loss: 0.068692
[Epoch 8; Iter    19/ 2483] train: loss: 0.0021625
[Epoch 8; Iter    49/ 2483] train: loss: 0.0019451
[Epoch 8; Iter    79/ 2483] train: loss: 0.0013480
[Epoch 8; Iter   109/ 2483] train: loss: 0.0015788
[Epoch 8; Iter   139/ 2483] train: loss: 0.0013557
[Epoch 8; Iter   169/ 2483] train: loss: 0.0013079
[Epoch 8; Iter   199/ 2483] train: loss: 0.0020275
[Epoch 8; Iter   229/ 2483] train: loss: 0.0013599
[Epoch 8; Iter   259/ 2483] train: loss: 0.1041620
[Epoch 8; Iter   289/ 2483] train: loss: 0.0022148
[Epoch 8; Iter   319/ 2483] train: loss: 0.0028042
[Epoch 8; Iter   349/ 2483] train: loss: 0.0018136
[Epoch 8; Iter   379/ 2483] train: loss: 0.0016109
[Epoch 8; Iter   409/ 2483] train: loss: 0.0026846
[Epoch 8; Iter   439/ 2483] train: loss: 0.0035022
[Epoch 8; Iter   469/ 2483] train: loss: 0.0025490
[Epoch 8; Iter   499/ 2483] train: loss: 0.0027782
[Epoch 8; Iter   529/ 2483] train: loss: 0.0019879
[Epoch 8; Iter   559/ 2483] train: loss: 0.0026639
[Epoch 8; Iter   589/ 2483] train: loss: 0.0771476
[Epoch 8; Iter   619/ 2483] train: loss: 0.0019289
[Epoch 8; Iter   649/ 2483] train: loss: 0.0017503
[Epoch 8; Iter   679/ 2483] train: loss: 0.0807880
[Epoch 8; Iter   709/ 2483] train: loss: 0.0015763
[Epoch 8; Iter   739/ 2483] train: loss: 0.0015322
[Epoch 8; Iter   769/ 2483] train: loss: 0.0032560
[Epoch 8; Iter   799/ 2483] train: loss: 0.0017459
[Epoch 8; Iter   829/ 2483] train: loss: 0.0016214
[Epoch 8; Iter   859/ 2483] train: loss: 0.0025342
[Epoch 8; Iter   889/ 2483] train: loss: 0.0022537
[Epoch 8; Iter   919/ 2483] train: loss: 0.0017686
[Epoch 8; Iter   949/ 2483] train: loss: 0.0021127
[Epoch 8; Iter   979/ 2483] train: loss: 0.0020177
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0016519
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0020380
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0022649
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0023563
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0011359
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0014281
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0683518
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0021724
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0021908
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0017813
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0024081
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0026921
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0026336
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0027244
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0023724
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0025618
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0024200
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0025251
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0027572
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0013820
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0024767
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0018294
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0018852
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0022052
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0016052
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0016519
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0015609
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0024557
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0022062
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0013900
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0019298
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0020181
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0018898
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0013800
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0010473
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0011655
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0858138
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0013109
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0013761
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0011461
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0012426
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0010720
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0009413
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0011331
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0917309
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0013137
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0681179
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0017279
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0012483
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0012166
[Epoch 8] ogbg-molmuv: 0.021829 val loss: 0.015135
[Epoch 8] ogbg-molmuv: 0.020450 test loss: 0.016013
[Epoch 9; Iter    26/ 2483] train: loss: 0.0014460
[Epoch 9; Iter    56/ 2483] train: loss: 0.0011967
[Epoch 9; Iter    86/ 2483] train: loss: 0.0019029
[Epoch 9; Iter   116/ 2483] train: loss: 0.0018200
[Epoch 9; Iter   146/ 2483] train: loss: 0.0013011
[Epoch 9; Iter   176/ 2483] train: loss: 0.0012442
[Epoch 9; Iter   206/ 2483] train: loss: 0.0014719
[Epoch 9; Iter   236/ 2483] train: loss: 0.0014773
[Epoch 9; Iter   266/ 2483] train: loss: 0.0013734
[Epoch 9; Iter   296/ 2483] train: loss: 0.0025890
[Epoch 9; Iter   326/ 2483] train: loss: 0.0018411
[Epoch 9; Iter   356/ 2483] train: loss: 0.0019054
[Epoch 9; Iter   386/ 2483] train: loss: 0.0570671
[Epoch 9; Iter   416/ 2483] train: loss: 0.0018718
[Epoch 9; Iter   446/ 2483] train: loss: 0.0018050
[Epoch 9; Iter   476/ 2483] train: loss: 0.0021412
[Epoch 9; Iter   506/ 2483] train: loss: 0.0013312
[Epoch 9; Iter   536/ 2483] train: loss: 0.0016690
[Epoch 9; Iter   566/ 2483] train: loss: 0.0015805
[Epoch 9; Iter   596/ 2483] train: loss: 0.0018464
[Epoch 9; Iter   626/ 2483] train: loss: 0.0020039
[Epoch 9; Iter   656/ 2483] train: loss: 0.0055200
[Epoch 9; Iter   686/ 2483] train: loss: 0.0019284
[Epoch 9; Iter   716/ 2483] train: loss: 0.0876179
[Epoch 9; Iter   746/ 2483] train: loss: 0.0017040
[Epoch 9; Iter   776/ 2483] train: loss: 0.0019086
[Epoch 9; Iter   806/ 2483] train: loss: 0.0020482
[Epoch 9; Iter   836/ 2483] train: loss: 0.0018929
[Epoch 9; Iter   866/ 2483] train: loss: 0.0015792
[Epoch 9; Iter   896/ 2483] train: loss: 0.0017492
[Epoch 9; Iter   926/ 2483] train: loss: 0.0502310
[Epoch 9; Iter   956/ 2483] train: loss: 0.0042190
[Epoch 9; Iter   986/ 2483] train: loss: 0.0023080
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0026739
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0017291
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0018464
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0569987
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0018781
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0025343
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0016805
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0018609
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0020766
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0019902
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0022568
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0028979
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0022083
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0029423
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0020923
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0020321
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0031819
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0018197
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0018497
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0016882
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0016414
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0014961
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0018155
[Epoch 9; Iter  1706/ 2483] train: loss: 0.1052978
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0014846
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0013346
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0013241
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0012636
[Epoch 9; Iter  1856/ 2483] train: loss: 0.1123358
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0021529
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0671784
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0015026
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0017639
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0731480
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0751499
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0016136
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0019570
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0792571
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0025481
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0016256
[Epoch 7] ogbg-molmuv: 0.033637 val loss: 0.014586
[Epoch 7] ogbg-molmuv: 0.046182 test loss: 0.015526
[Epoch 8; Iter    19/ 2483] train: loss: 0.0019306
[Epoch 8; Iter    49/ 2483] train: loss: 0.0019685
[Epoch 8; Iter    79/ 2483] train: loss: 0.0017962
[Epoch 8; Iter   109/ 2483] train: loss: 0.0014510
[Epoch 8; Iter   139/ 2483] train: loss: 0.0015498
[Epoch 8; Iter   169/ 2483] train: loss: 0.0012363
[Epoch 8; Iter   199/ 2483] train: loss: 0.0017848
[Epoch 8; Iter   229/ 2483] train: loss: 0.0013844
[Epoch 8; Iter   259/ 2483] train: loss: 0.1164669
[Epoch 8; Iter   289/ 2483] train: loss: 0.0022733
[Epoch 8; Iter   319/ 2483] train: loss: 0.0029885
[Epoch 8; Iter   349/ 2483] train: loss: 0.0017864
[Epoch 8; Iter   379/ 2483] train: loss: 0.0020963
[Epoch 8; Iter   409/ 2483] train: loss: 0.0030696
[Epoch 8; Iter   439/ 2483] train: loss: 0.0027053
[Epoch 8; Iter   469/ 2483] train: loss: 0.0026734
[Epoch 8; Iter   499/ 2483] train: loss: 0.0029069
[Epoch 8; Iter   529/ 2483] train: loss: 0.0020610
[Epoch 8; Iter   559/ 2483] train: loss: 0.0024875
[Epoch 8; Iter   589/ 2483] train: loss: 0.0719731
[Epoch 8; Iter   619/ 2483] train: loss: 0.0024403
[Epoch 8; Iter   649/ 2483] train: loss: 0.0017743
[Epoch 8; Iter   679/ 2483] train: loss: 0.0812629
[Epoch 8; Iter   709/ 2483] train: loss: 0.0016943
[Epoch 8; Iter   739/ 2483] train: loss: 0.0013210
[Epoch 8; Iter   769/ 2483] train: loss: 0.0025817
[Epoch 8; Iter   799/ 2483] train: loss: 0.0018304
[Epoch 8; Iter   829/ 2483] train: loss: 0.0020358
[Epoch 8; Iter   859/ 2483] train: loss: 0.0024491
[Epoch 8; Iter   889/ 2483] train: loss: 0.0020748
[Epoch 8; Iter   919/ 2483] train: loss: 0.0020369
[Epoch 8; Iter   949/ 2483] train: loss: 0.0021039
[Epoch 8; Iter   979/ 2483] train: loss: 0.0018409
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0015810
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0019975
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0022077
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0022416
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0013915
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0016865
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0738683
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0023041
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0030749
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0021927
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0022676
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0029720
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0027831
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0028344
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0021240
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0029495
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0026200
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0021879
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0027322
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0013877
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0023083
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0018223
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0020965
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0022309
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0017689
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0016032
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0014243
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0020363
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0017963
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0013566
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0025399
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0019917
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0013285
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0014735
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0010321
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0013119
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0849445
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0012444
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0014160
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0011415
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0011111
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0010617
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0009813
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0012188
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0774547
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0010974
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0726203
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0016458
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0013179
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0012309
[Epoch 8] ogbg-molmuv: 0.028101 val loss: 0.014711
[Epoch 8] ogbg-molmuv: 0.045039 test loss: 0.015490
[Epoch 9; Iter    26/ 2483] train: loss: 0.0014457
[Epoch 9; Iter    56/ 2483] train: loss: 0.0013079
[Epoch 9; Iter    86/ 2483] train: loss: 0.0017574
[Epoch 9; Iter   116/ 2483] train: loss: 0.0021153
[Epoch 9; Iter   146/ 2483] train: loss: 0.0013161
[Epoch 9; Iter   176/ 2483] train: loss: 0.0011518
[Epoch 9; Iter   206/ 2483] train: loss: 0.0012875
[Epoch 9; Iter   236/ 2483] train: loss: 0.0012801
[Epoch 9; Iter   266/ 2483] train: loss: 0.0014058
[Epoch 9; Iter   296/ 2483] train: loss: 0.0020936
[Epoch 9; Iter   326/ 2483] train: loss: 0.0022907
[Epoch 9; Iter   356/ 2483] train: loss: 0.0016086
[Epoch 9; Iter   386/ 2483] train: loss: 0.0628688
[Epoch 9; Iter   416/ 2483] train: loss: 0.0018733
[Epoch 9; Iter   446/ 2483] train: loss: 0.0015911
[Epoch 9; Iter   476/ 2483] train: loss: 0.0018171
[Epoch 9; Iter   506/ 2483] train: loss: 0.0015932
[Epoch 9; Iter   536/ 2483] train: loss: 0.0016674
[Epoch 9; Iter   566/ 2483] train: loss: 0.0017840
[Epoch 9; Iter   596/ 2483] train: loss: 0.0019533
[Epoch 9; Iter   626/ 2483] train: loss: 0.0014330
[Epoch 9; Iter   656/ 2483] train: loss: 0.0055712
[Epoch 9; Iter   686/ 2483] train: loss: 0.0016715
[Epoch 9; Iter   716/ 2483] train: loss: 0.0905027
[Epoch 9; Iter   746/ 2483] train: loss: 0.0014641
[Epoch 9; Iter   776/ 2483] train: loss: 0.0016532
[Epoch 9; Iter   806/ 2483] train: loss: 0.0017624
[Epoch 9; Iter   836/ 2483] train: loss: 0.0013406
[Epoch 9; Iter   866/ 2483] train: loss: 0.0013696
[Epoch 9; Iter   896/ 2483] train: loss: 0.0017081
[Epoch 9; Iter   926/ 2483] train: loss: 0.0751904
[Epoch 9; Iter   956/ 2483] train: loss: 0.0041598
[Epoch 9; Iter   986/ 2483] train: loss: 0.0024117
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0022684
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0013643
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0015106
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0742993
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0021049
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0025714
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0016530
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0019670
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0015006
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0021257
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0023398
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0025767
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0020269
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0034227
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0019386
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0021164
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0023569
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0020467
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0016158
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0020918
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0019584
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0018259
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0018895
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0995444
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0013765
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0011343
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0011179
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0010364
[Epoch 9; Iter  1856/ 2483] train: loss: 0.1646446
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0015314
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0670858
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0016192
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0019365
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0716496
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0655222
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0021235
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0027424
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0818409
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0024125
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0024253
[Epoch 7] ogbg-molmuv: 0.016195 val loss: 0.015682
[Epoch 7] ogbg-molmuv: 0.026836 test loss: 0.016360
[Epoch 8; Iter    19/ 2483] train: loss: 0.0021496
[Epoch 8; Iter    49/ 2483] train: loss: 0.0632620
[Epoch 8; Iter    79/ 2483] train: loss: 0.0771530
[Epoch 8; Iter   109/ 2483] train: loss: 0.0657865
[Epoch 8; Iter   139/ 2483] train: loss: 0.0023331
[Epoch 8; Iter   169/ 2483] train: loss: 0.0659596
[Epoch 8; Iter   199/ 2483] train: loss: 0.0618495
[Epoch 8; Iter   229/ 2483] train: loss: 0.0022527
[Epoch 8; Iter   259/ 2483] train: loss: 0.0021957
[Epoch 8; Iter   289/ 2483] train: loss: 0.0021558
[Epoch 8; Iter   319/ 2483] train: loss: 0.0025057
[Epoch 8; Iter   349/ 2483] train: loss: 0.0021683
[Epoch 8; Iter   379/ 2483] train: loss: 0.0023557
[Epoch 8; Iter   409/ 2483] train: loss: 0.0676988
[Epoch 8; Iter   439/ 2483] train: loss: 0.0956391
[Epoch 8; Iter   469/ 2483] train: loss: 0.0016822
[Epoch 8; Iter   499/ 2483] train: loss: 0.0046059
[Epoch 8; Iter   529/ 2483] train: loss: 0.0019874
[Epoch 8; Iter   559/ 2483] train: loss: 0.0712624
[Epoch 8; Iter   589/ 2483] train: loss: 0.0033188
[Epoch 8; Iter   619/ 2483] train: loss: 0.0020256
[Epoch 8; Iter   649/ 2483] train: loss: 0.0021231
[Epoch 8; Iter   679/ 2483] train: loss: 0.0018081
[Epoch 8; Iter   709/ 2483] train: loss: 0.0018177
[Epoch 8; Iter   739/ 2483] train: loss: 0.0015629
[Epoch 8; Iter   769/ 2483] train: loss: 0.0987518
[Epoch 8; Iter   799/ 2483] train: loss: 0.0016123
[Epoch 8; Iter   829/ 2483] train: loss: 0.0024220
[Epoch 8; Iter   859/ 2483] train: loss: 0.0020578
[Epoch 8; Iter   889/ 2483] train: loss: 0.0828179
[Epoch 8; Iter   919/ 2483] train: loss: 0.0016094
[Epoch 8; Iter   949/ 2483] train: loss: 0.0021901
[Epoch 8; Iter   979/ 2483] train: loss: 0.0020334
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0020777
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0028441
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0025731
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0833993
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0020210
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0022133
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0017758
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0016358
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0018717
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0020657
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0016398
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0579507
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0022399
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0025360
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0028304
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0015737
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0014517
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0797534
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0019364
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0020203
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0014040
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0929728
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0016295
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0014640
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0013205
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0014611
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0020891
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0017915
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0017988
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0016635
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0013958
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0013323
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0015058
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0017004
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0999194
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0017145
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0015631
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0016151
[Epoch 8; Iter  2149/ 2483] train: loss: 0.1235849
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0021292
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0016358
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0017621
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0017313
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0018031
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0018154
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0020262
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0015687
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0015922
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0014631
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0754486
[Epoch 8] ogbg-molmuv: 0.029942 val loss: 0.015679
[Epoch 8] ogbg-molmuv: 0.039013 test loss: 0.016094
[Epoch 9; Iter    26/ 2483] train: loss: 0.0015365
[Epoch 9; Iter    56/ 2483] train: loss: 0.0850095
[Epoch 9; Iter    86/ 2483] train: loss: 0.0895750
[Epoch 9; Iter   116/ 2483] train: loss: 0.0022365
[Epoch 9; Iter   146/ 2483] train: loss: 0.0015843
[Epoch 9; Iter   176/ 2483] train: loss: 0.0020296
[Epoch 9; Iter   206/ 2483] train: loss: 0.0020305
[Epoch 9; Iter   236/ 2483] train: loss: 0.0756220
[Epoch 9; Iter   266/ 2483] train: loss: 0.0022990
[Epoch 9; Iter   296/ 2483] train: loss: 0.0025147
[Epoch 9; Iter   326/ 2483] train: loss: 0.0024706
[Epoch 9; Iter   356/ 2483] train: loss: 0.0017048
[Epoch 9; Iter   386/ 2483] train: loss: 0.0014713
[Epoch 9; Iter   416/ 2483] train: loss: 0.0016679
[Epoch 9; Iter   446/ 2483] train: loss: 0.0017900
[Epoch 9; Iter   476/ 2483] train: loss: 0.0016968
[Epoch 9; Iter   506/ 2483] train: loss: 0.0775773
[Epoch 9; Iter   536/ 2483] train: loss: 0.0021642
[Epoch 9; Iter   566/ 2483] train: loss: 0.0017483
[Epoch 9; Iter   596/ 2483] train: loss: 0.0021951
[Epoch 9; Iter   626/ 2483] train: loss: 0.0017736
[Epoch 9; Iter   656/ 2483] train: loss: 0.0019064
[Epoch 9; Iter   686/ 2483] train: loss: 0.0013336
[Epoch 9; Iter   716/ 2483] train: loss: 0.0015201
[Epoch 9; Iter   746/ 2483] train: loss: 0.0015851
[Epoch 9; Iter   776/ 2483] train: loss: 0.1533661
[Epoch 9; Iter   806/ 2483] train: loss: 0.0014446
[Epoch 9; Iter   836/ 2483] train: loss: 0.0017741
[Epoch 9; Iter   866/ 2483] train: loss: 0.0017456
[Epoch 9; Iter   896/ 2483] train: loss: 0.0020505
[Epoch 9; Iter   926/ 2483] train: loss: 0.0924188
[Epoch 9; Iter   956/ 2483] train: loss: 0.0423775
[Epoch 9; Iter   986/ 2483] train: loss: 0.0024509
[Epoch 9; Iter  1016/ 2483] train: loss: 0.1122418
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0015996
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0022959
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0026134
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0025068
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0736485
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0880216
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0025559
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0017687
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0018042
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0021259
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0018755
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0020781
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0017854
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0020202
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0018398
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0607752
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0024542
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0021051
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0874169
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0020014
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0018259
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0542843
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0014330
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0013985
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0019060
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0014759
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0018266
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0925599
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0016146
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0013116
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0016780
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0017663
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0023270
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0013741
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0013157
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0012785
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0022113
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0025349
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0020735
[Epoch 7] ogbg-molmuv: 0.028820 val loss: 0.022208
[Epoch 7] ogbg-molmuv: 0.014829 test loss: 0.062596
[Epoch 8; Iter    19/ 2483] train: loss: 0.0018491
[Epoch 8; Iter    49/ 2483] train: loss: 0.0023065
[Epoch 8; Iter    79/ 2483] train: loss: 0.0021484
[Epoch 8; Iter   109/ 2483] train: loss: 0.0038746
[Epoch 8; Iter   139/ 2483] train: loss: 0.0037532
[Epoch 8; Iter   169/ 2483] train: loss: 0.0690576
[Epoch 8; Iter   199/ 2483] train: loss: 0.0023376
[Epoch 8; Iter   229/ 2483] train: loss: 0.0040962
[Epoch 8; Iter   259/ 2483] train: loss: 0.0035871
[Epoch 8; Iter   289/ 2483] train: loss: 0.0032870
[Epoch 8; Iter   319/ 2483] train: loss: 0.0021181
[Epoch 8; Iter   349/ 2483] train: loss: 0.0022705
[Epoch 8; Iter   379/ 2483] train: loss: 0.0018528
[Epoch 8; Iter   409/ 2483] train: loss: 0.0023391
[Epoch 8; Iter   439/ 2483] train: loss: 0.0017790
[Epoch 8; Iter   469/ 2483] train: loss: 0.0020233
[Epoch 8; Iter   499/ 2483] train: loss: 0.0015259
[Epoch 8; Iter   529/ 2483] train: loss: 0.0020962
[Epoch 8; Iter   559/ 2483] train: loss: 0.0018782
[Epoch 8; Iter   589/ 2483] train: loss: 0.0017265
[Epoch 8; Iter   619/ 2483] train: loss: 0.1042886
[Epoch 8; Iter   649/ 2483] train: loss: 0.0019233
[Epoch 8; Iter   679/ 2483] train: loss: 0.0015531
[Epoch 8; Iter   709/ 2483] train: loss: 0.0022920
[Epoch 8; Iter   739/ 2483] train: loss: 0.0018456
[Epoch 8; Iter   769/ 2483] train: loss: 0.0020159
[Epoch 8; Iter   799/ 2483] train: loss: 0.0021955
[Epoch 8; Iter   829/ 2483] train: loss: 0.0015889
[Epoch 8; Iter   859/ 2483] train: loss: 0.0694509
[Epoch 8; Iter   889/ 2483] train: loss: 0.0014120
[Epoch 8; Iter   919/ 2483] train: loss: 0.0013252
[Epoch 8; Iter   949/ 2483] train: loss: 0.0016871
[Epoch 8; Iter   979/ 2483] train: loss: 0.0011508
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0012673
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0658063
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0020973
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0021372
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0012070
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0653327
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0018971
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0017310
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0017921
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0016259
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0853901
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0013179
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0013881
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0009919
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0622388
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0022982
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0020460
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0016681
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0012733
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0018932
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0015738
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0018351
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0023865
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0034762
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0603832
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0017909
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0020312
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0021774
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0022499
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0023132
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0029027
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0021942
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0694411
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0014877
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0013361
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0888704
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0013239
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0012762
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0015328
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0023544
[Epoch 8; Iter  2209/ 2483] train: loss: 0.1621120
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0021162
[Epoch 8; Iter  2269/ 2483] train: loss: 0.1069864
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0785510
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0017046
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0021851
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0016862
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0024613
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0017162
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0020873
[Epoch 8] ogbg-molmuv: 0.040747 val loss: 0.020893
[Epoch 8] ogbg-molmuv: 0.016676 test loss: 0.046092
[Epoch 9; Iter    26/ 2483] train: loss: 0.0018305
[Epoch 9; Iter    56/ 2483] train: loss: 0.0022754
[Epoch 9; Iter    86/ 2483] train: loss: 0.0609793
[Epoch 9; Iter   116/ 2483] train: loss: 0.0017082
[Epoch 9; Iter   146/ 2483] train: loss: 0.0016270
[Epoch 9; Iter   176/ 2483] train: loss: 0.0023070
[Epoch 9; Iter   206/ 2483] train: loss: 0.0021571
[Epoch 9; Iter   236/ 2483] train: loss: 0.0014298
[Epoch 9; Iter   266/ 2483] train: loss: 0.0022532
[Epoch 9; Iter   296/ 2483] train: loss: 0.0013722
[Epoch 9; Iter   326/ 2483] train: loss: 0.0013634
[Epoch 9; Iter   356/ 2483] train: loss: 0.0012037
[Epoch 9; Iter   386/ 2483] train: loss: 0.0015867
[Epoch 9; Iter   416/ 2483] train: loss: 0.0015924
[Epoch 9; Iter   446/ 2483] train: loss: 0.0012935
[Epoch 9; Iter   476/ 2483] train: loss: 0.0016302
[Epoch 9; Iter   506/ 2483] train: loss: 0.0019007
[Epoch 9; Iter   536/ 2483] train: loss: 0.0026376
[Epoch 9; Iter   566/ 2483] train: loss: 0.0018732
[Epoch 9; Iter   596/ 2483] train: loss: 0.0012970
[Epoch 9; Iter   626/ 2483] train: loss: 0.0012996
[Epoch 9; Iter   656/ 2483] train: loss: 0.0014360
[Epoch 9; Iter   686/ 2483] train: loss: 0.0011498
[Epoch 9; Iter   716/ 2483] train: loss: 0.0012534
[Epoch 9; Iter   746/ 2483] train: loss: 0.0015449
[Epoch 9; Iter   776/ 2483] train: loss: 0.0015283
[Epoch 9; Iter   806/ 2483] train: loss: 0.0941785
[Epoch 9; Iter   836/ 2483] train: loss: 0.0017888
[Epoch 9; Iter   866/ 2483] train: loss: 0.0018443
[Epoch 9; Iter   896/ 2483] train: loss: 0.0022893
[Epoch 9; Iter   926/ 2483] train: loss: 0.0013750
[Epoch 9; Iter   956/ 2483] train: loss: 0.0018797
[Epoch 9; Iter   986/ 2483] train: loss: 0.0020305
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0028729
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0014739
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0013120
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0020196
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0020855
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0688467
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0018625
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0017748
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0018664
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0019166
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0037380
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0022384
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0023203
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0619112
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0014284
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0012345
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0019378
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0019190
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0804756
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0891838
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0023213
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0020284
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0019587
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0025475
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0030976
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0027937
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0020667
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0022681
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0023825
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0024139
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0020670
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0032453
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0018479
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0023706
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0022771
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0016206
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0020224
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0010414
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0022127
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0022659
[Epoch 7] ogbg-molmuv: 0.051616 val loss: 0.031746
[Epoch 7] ogbg-molmuv: 0.016256 test loss: 0.061412
[Epoch 8; Iter    19/ 2483] train: loss: 0.0018590
[Epoch 8; Iter    49/ 2483] train: loss: 0.0024586
[Epoch 8; Iter    79/ 2483] train: loss: 0.0026034
[Epoch 8; Iter   109/ 2483] train: loss: 0.0032678
[Epoch 8; Iter   139/ 2483] train: loss: 0.0038251
[Epoch 8; Iter   169/ 2483] train: loss: 0.0756107
[Epoch 8; Iter   199/ 2483] train: loss: 0.0027098
[Epoch 8; Iter   229/ 2483] train: loss: 0.0032092
[Epoch 8; Iter   259/ 2483] train: loss: 0.0034097
[Epoch 8; Iter   289/ 2483] train: loss: 0.0032549
[Epoch 8; Iter   319/ 2483] train: loss: 0.0031738
[Epoch 8; Iter   349/ 2483] train: loss: 0.0024438
[Epoch 8; Iter   379/ 2483] train: loss: 0.0018217
[Epoch 8; Iter   409/ 2483] train: loss: 0.0019221
[Epoch 8; Iter   439/ 2483] train: loss: 0.0017555
[Epoch 8; Iter   469/ 2483] train: loss: 0.0019625
[Epoch 8; Iter   499/ 2483] train: loss: 0.0020684
[Epoch 8; Iter   529/ 2483] train: loss: 0.0030027
[Epoch 8; Iter   559/ 2483] train: loss: 0.0021584
[Epoch 8; Iter   589/ 2483] train: loss: 0.0022984
[Epoch 8; Iter   619/ 2483] train: loss: 0.0907542
[Epoch 8; Iter   649/ 2483] train: loss: 0.0014548
[Epoch 8; Iter   679/ 2483] train: loss: 0.0015963
[Epoch 8; Iter   709/ 2483] train: loss: 0.0018455
[Epoch 8; Iter   739/ 2483] train: loss: 0.0019745
[Epoch 8; Iter   769/ 2483] train: loss: 0.0021156
[Epoch 8; Iter   799/ 2483] train: loss: 0.0023438
[Epoch 8; Iter   829/ 2483] train: loss: 0.0016119
[Epoch 8; Iter   859/ 2483] train: loss: 0.0659842
[Epoch 8; Iter   889/ 2483] train: loss: 0.0013415
[Epoch 8; Iter   919/ 2483] train: loss: 0.0014670
[Epoch 8; Iter   949/ 2483] train: loss: 0.0015947
[Epoch 8; Iter   979/ 2483] train: loss: 0.0011866
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0010883
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0773740
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0017655
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0018985
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0013825
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0677183
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0020249
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0015883
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0017222
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0015512
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0925394
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0013805
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0015486
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0011380
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0796010
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0018150
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0026531
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0017456
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0015447
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0018301
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0018020
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0020365
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0024280
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0023183
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0703985
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0022463
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0020303
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0021829
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0021288
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0018744
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0022162
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0028360
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0694827
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0014463
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0013519
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0936015
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0013865
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0013465
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0015103
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0019820
[Epoch 8; Iter  2209/ 2483] train: loss: 0.1835249
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0017682
[Epoch 8; Iter  2269/ 2483] train: loss: 0.1032811
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0779296
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0021001
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0016983
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0019457
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0020581
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0018010
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0020427
[Epoch 8] ogbg-molmuv: 0.024171 val loss: 0.198324
[Epoch 8] ogbg-molmuv: 0.017441 test loss: 0.248335
[Epoch 9; Iter    26/ 2483] train: loss: 0.0019161
[Epoch 9; Iter    56/ 2483] train: loss: 0.0020858
[Epoch 9; Iter    86/ 2483] train: loss: 0.0784111
[Epoch 9; Iter   116/ 2483] train: loss: 0.0018053
[Epoch 9; Iter   146/ 2483] train: loss: 0.0017361
[Epoch 9; Iter   176/ 2483] train: loss: 0.0019537
[Epoch 9; Iter   206/ 2483] train: loss: 0.0016535
[Epoch 9; Iter   236/ 2483] train: loss: 0.0019499
[Epoch 9; Iter   266/ 2483] train: loss: 0.0026263
[Epoch 9; Iter   296/ 2483] train: loss: 0.0013331
[Epoch 9; Iter   326/ 2483] train: loss: 0.0010334
[Epoch 9; Iter   356/ 2483] train: loss: 0.0011533
[Epoch 9; Iter   386/ 2483] train: loss: 0.0013821
[Epoch 9; Iter   416/ 2483] train: loss: 0.0014933
[Epoch 9; Iter   446/ 2483] train: loss: 0.0012362
[Epoch 9; Iter   476/ 2483] train: loss: 0.0013281
[Epoch 9; Iter   506/ 2483] train: loss: 0.0018709
[Epoch 9; Iter   536/ 2483] train: loss: 0.0023579
[Epoch 9; Iter   566/ 2483] train: loss: 0.0018023
[Epoch 9; Iter   596/ 2483] train: loss: 0.0016122
[Epoch 9; Iter   626/ 2483] train: loss: 0.0014977
[Epoch 9; Iter   656/ 2483] train: loss: 0.0014369
[Epoch 9; Iter   686/ 2483] train: loss: 0.0013550
[Epoch 9; Iter   716/ 2483] train: loss: 0.0013284
[Epoch 9; Iter   746/ 2483] train: loss: 0.0014684
[Epoch 9; Iter   776/ 2483] train: loss: 0.0014917
[Epoch 9; Iter   806/ 2483] train: loss: 0.0992843
[Epoch 9; Iter   836/ 2483] train: loss: 0.0021743
[Epoch 9; Iter   866/ 2483] train: loss: 0.0020170
[Epoch 9; Iter   896/ 2483] train: loss: 0.0019082
[Epoch 9; Iter   926/ 2483] train: loss: 0.0016521
[Epoch 9; Iter   956/ 2483] train: loss: 0.0023163
[Epoch 9; Iter   986/ 2483] train: loss: 0.0023295
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0027916
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0010937
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0013030
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0016864
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0018254
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0861399
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0018210
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0017491
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0012826
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0025734
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0041883
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0023598
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0028296
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0550742
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0013527
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0013902
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0022830
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0018891
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0819420
[Epoch 9; Iter  1586/ 2483] train: loss: 0.1024381
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0022013
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0021462
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0023447
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0034049
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0029855
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0025924
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0022424
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0023972
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0024981
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0029265
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0018554
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0022622
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0022540
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0025622
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0021346
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0013487
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0018894
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0011703
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0021375
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0018097
[Epoch 7] ogbg-molmuv: 0.020243 val loss: 0.016052
[Epoch 7] ogbg-molmuv: 0.031326 test loss: 0.016436
[Epoch 8; Iter    19/ 2483] train: loss: 0.0020394
[Epoch 8; Iter    49/ 2483] train: loss: 0.0019721
[Epoch 8; Iter    79/ 2483] train: loss: 0.0014872
[Epoch 8; Iter   109/ 2483] train: loss: 0.0018520
[Epoch 8; Iter   139/ 2483] train: loss: 0.0016820
[Epoch 8; Iter   169/ 2483] train: loss: 0.0015340
[Epoch 8; Iter   199/ 2483] train: loss: 0.0022874
[Epoch 8; Iter   229/ 2483] train: loss: 0.0015854
[Epoch 8; Iter   259/ 2483] train: loss: 0.0933786
[Epoch 8; Iter   289/ 2483] train: loss: 0.0021424
[Epoch 8; Iter   319/ 2483] train: loss: 0.0027648
[Epoch 8; Iter   349/ 2483] train: loss: 0.0017498
[Epoch 8; Iter   379/ 2483] train: loss: 0.0022687
[Epoch 8; Iter   409/ 2483] train: loss: 0.0024481
[Epoch 8; Iter   439/ 2483] train: loss: 0.0021745
[Epoch 8; Iter   469/ 2483] train: loss: 0.0028394
[Epoch 8; Iter   499/ 2483] train: loss: 0.0043724
[Epoch 8; Iter   529/ 2483] train: loss: 0.0021375
[Epoch 8; Iter   559/ 2483] train: loss: 0.0024972
[Epoch 8; Iter   589/ 2483] train: loss: 0.0825765
[Epoch 8; Iter   619/ 2483] train: loss: 0.0021656
[Epoch 8; Iter   649/ 2483] train: loss: 0.0015355
[Epoch 8; Iter   679/ 2483] train: loss: 0.0747644
[Epoch 8; Iter   709/ 2483] train: loss: 0.0016607
[Epoch 8; Iter   739/ 2483] train: loss: 0.0017710
[Epoch 8; Iter   769/ 2483] train: loss: 0.0021352
[Epoch 8; Iter   799/ 2483] train: loss: 0.0019813
[Epoch 8; Iter   829/ 2483] train: loss: 0.0024162
[Epoch 8; Iter   859/ 2483] train: loss: 0.0025766
[Epoch 8; Iter   889/ 2483] train: loss: 0.0019384
[Epoch 8; Iter   919/ 2483] train: loss: 0.0017450
[Epoch 8; Iter   949/ 2483] train: loss: 0.0025346
[Epoch 8; Iter   979/ 2483] train: loss: 0.0020444
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0015559
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0020832
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0019893
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0019223
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0013590
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0014682
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0821649
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0022172
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0022490
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0020998
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0023424
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0023632
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0024460
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0026762
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0021945
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0025278
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0032643
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0022745
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0025035
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0016813
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0026422
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0019661
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0029627
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0022082
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0018565
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0017309
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0016195
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0021918
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0015835
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0013893
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0018521
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0016095
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0014977
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0014436
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0012311
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0013723
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0828529
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0013997
[Epoch 8; Iter  2149/ 2483] train: loss: 0.0013956
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0011826
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0011317
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0009440
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0011185
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0011027
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0844353
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0014208
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0746764
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0014868
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0012687
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0011660
[Epoch 8] ogbg-molmuv: 0.011857 val loss: 0.015138
[Epoch 8] ogbg-molmuv: 0.018781 test loss: 0.015465
[Epoch 9; Iter    26/ 2483] train: loss: 0.0013490
[Epoch 9; Iter    56/ 2483] train: loss: 0.0011794
[Epoch 9; Iter    86/ 2483] train: loss: 0.0016597
[Epoch 9; Iter   116/ 2483] train: loss: 0.0017368
[Epoch 9; Iter   146/ 2483] train: loss: 0.0015719
[Epoch 9; Iter   176/ 2483] train: loss: 0.0013122
[Epoch 9; Iter   206/ 2483] train: loss: 0.0014498
[Epoch 9; Iter   236/ 2483] train: loss: 0.0017649
[Epoch 9; Iter   266/ 2483] train: loss: 0.0013429
[Epoch 9; Iter   296/ 2483] train: loss: 0.0026977
[Epoch 9; Iter   326/ 2483] train: loss: 0.0018103
[Epoch 9; Iter   356/ 2483] train: loss: 0.0020429
[Epoch 9; Iter   386/ 2483] train: loss: 0.0784568
[Epoch 9; Iter   416/ 2483] train: loss: 0.0016969
[Epoch 9; Iter   446/ 2483] train: loss: 0.0016519
[Epoch 9; Iter   476/ 2483] train: loss: 0.0020495
[Epoch 9; Iter   506/ 2483] train: loss: 0.0015207
[Epoch 9; Iter   536/ 2483] train: loss: 0.0019142
[Epoch 9; Iter   566/ 2483] train: loss: 0.0016057
[Epoch 9; Iter   596/ 2483] train: loss: 0.0020151
[Epoch 9; Iter   626/ 2483] train: loss: 0.0015391
[Epoch 9; Iter   656/ 2483] train: loss: 0.0021817
[Epoch 9; Iter   686/ 2483] train: loss: 0.0018152
[Epoch 9; Iter   716/ 2483] train: loss: 0.0857078
[Epoch 9; Iter   746/ 2483] train: loss: 0.0022889
[Epoch 9; Iter   776/ 2483] train: loss: 0.0020307
[Epoch 9; Iter   806/ 2483] train: loss: 0.0019199
[Epoch 9; Iter   836/ 2483] train: loss: 0.0014540
[Epoch 9; Iter   866/ 2483] train: loss: 0.0013920
[Epoch 9; Iter   896/ 2483] train: loss: 0.0018790
[Epoch 9; Iter   926/ 2483] train: loss: 0.1007101
[Epoch 9; Iter   956/ 2483] train: loss: 0.0024269
[Epoch 9; Iter   986/ 2483] train: loss: 0.0020813
[Epoch 9; Iter  1016/ 2483] train: loss: 0.0028938
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0017521
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0019802
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0813329
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0023941
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0024663
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0017663
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0023196
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0019727
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0019072
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0021955
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0028653
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0029877
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0023362
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0021216
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0022380
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0021423
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0019114
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0017976
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0019820
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0017965
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0017512
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0018214
[Epoch 9; Iter  1706/ 2483] train: loss: 0.1020298
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0014965
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0010865
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0011218
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0011218
[Epoch 9; Iter  1856/ 2483] train: loss: 0.1790581
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0015097
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0736431
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0019576
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0023713
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0652756
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0789721
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0017619
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0020211
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0768722
[Epoch 7; Iter  2442/ 2483] train: loss: 0.0021829
[Epoch 7; Iter  2472/ 2483] train: loss: 0.0019101
[Epoch 7] ogbg-molmuv: 0.038813 val loss: 0.015078
[Epoch 7] ogbg-molmuv: 0.027297 test loss: 0.015459
[Epoch 8; Iter    19/ 2483] train: loss: 0.0020802
[Epoch 8; Iter    49/ 2483] train: loss: 0.0597106
[Epoch 8; Iter    79/ 2483] train: loss: 0.0903464
[Epoch 8; Iter   109/ 2483] train: loss: 0.0717294
[Epoch 8; Iter   139/ 2483] train: loss: 0.0023225
[Epoch 8; Iter   169/ 2483] train: loss: 0.0707094
[Epoch 8; Iter   199/ 2483] train: loss: 0.0494812
[Epoch 8; Iter   229/ 2483] train: loss: 0.0023353
[Epoch 8; Iter   259/ 2483] train: loss: 0.0024208
[Epoch 8; Iter   289/ 2483] train: loss: 0.0017618
[Epoch 8; Iter   319/ 2483] train: loss: 0.0022066
[Epoch 8; Iter   349/ 2483] train: loss: 0.0022838
[Epoch 8; Iter   379/ 2483] train: loss: 0.0018722
[Epoch 8; Iter   409/ 2483] train: loss: 0.0681302
[Epoch 8; Iter   439/ 2483] train: loss: 0.0886438
[Epoch 8; Iter   469/ 2483] train: loss: 0.0019417
[Epoch 8; Iter   499/ 2483] train: loss: 0.0025830
[Epoch 8; Iter   529/ 2483] train: loss: 0.0019893
[Epoch 8; Iter   559/ 2483] train: loss: 0.0740034
[Epoch 8; Iter   589/ 2483] train: loss: 0.0031068
[Epoch 8; Iter   619/ 2483] train: loss: 0.0021567
[Epoch 8; Iter   649/ 2483] train: loss: 0.0018640
[Epoch 8; Iter   679/ 2483] train: loss: 0.0018941
[Epoch 8; Iter   709/ 2483] train: loss: 0.0018975
[Epoch 8; Iter   739/ 2483] train: loss: 0.0016751
[Epoch 8; Iter   769/ 2483] train: loss: 0.0992752
[Epoch 8; Iter   799/ 2483] train: loss: 0.0019959
[Epoch 8; Iter   829/ 2483] train: loss: 0.0021856
[Epoch 8; Iter   859/ 2483] train: loss: 0.0022474
[Epoch 8; Iter   889/ 2483] train: loss: 0.0849251
[Epoch 8; Iter   919/ 2483] train: loss: 0.0016708
[Epoch 8; Iter   949/ 2483] train: loss: 0.0018433
[Epoch 8; Iter   979/ 2483] train: loss: 0.0018883
[Epoch 8; Iter  1009/ 2483] train: loss: 0.0021438
[Epoch 8; Iter  1039/ 2483] train: loss: 0.0032934
[Epoch 8; Iter  1069/ 2483] train: loss: 0.0020211
[Epoch 8; Iter  1099/ 2483] train: loss: 0.0780348
[Epoch 8; Iter  1129/ 2483] train: loss: 0.0017308
[Epoch 8; Iter  1159/ 2483] train: loss: 0.0026624
[Epoch 8; Iter  1189/ 2483] train: loss: 0.0020654
[Epoch 8; Iter  1219/ 2483] train: loss: 0.0013723
[Epoch 8; Iter  1249/ 2483] train: loss: 0.0018707
[Epoch 8; Iter  1279/ 2483] train: loss: 0.0019958
[Epoch 8; Iter  1309/ 2483] train: loss: 0.0016871
[Epoch 8; Iter  1339/ 2483] train: loss: 0.0535642
[Epoch 8; Iter  1369/ 2483] train: loss: 0.0024007
[Epoch 8; Iter  1399/ 2483] train: loss: 0.0024926
[Epoch 8; Iter  1429/ 2483] train: loss: 0.0030215
[Epoch 8; Iter  1459/ 2483] train: loss: 0.0021627
[Epoch 8; Iter  1489/ 2483] train: loss: 0.0014814
[Epoch 8; Iter  1519/ 2483] train: loss: 0.0801557
[Epoch 8; Iter  1549/ 2483] train: loss: 0.0022659
[Epoch 8; Iter  1579/ 2483] train: loss: 0.0022578
[Epoch 8; Iter  1609/ 2483] train: loss: 0.0014733
[Epoch 8; Iter  1639/ 2483] train: loss: 0.0940984
[Epoch 8; Iter  1669/ 2483] train: loss: 0.0027270
[Epoch 8; Iter  1699/ 2483] train: loss: 0.0018275
[Epoch 8; Iter  1729/ 2483] train: loss: 0.0015049
[Epoch 8; Iter  1759/ 2483] train: loss: 0.0015530
[Epoch 8; Iter  1789/ 2483] train: loss: 0.0020629
[Epoch 8; Iter  1819/ 2483] train: loss: 0.0013939
[Epoch 8; Iter  1849/ 2483] train: loss: 0.0015514
[Epoch 8; Iter  1879/ 2483] train: loss: 0.0015894
[Epoch 8; Iter  1909/ 2483] train: loss: 0.0011856
[Epoch 8; Iter  1939/ 2483] train: loss: 0.0012451
[Epoch 8; Iter  1969/ 2483] train: loss: 0.0018934
[Epoch 8; Iter  1999/ 2483] train: loss: 0.0014435
[Epoch 8; Iter  2029/ 2483] train: loss: 0.0996794
[Epoch 8; Iter  2059/ 2483] train: loss: 0.0014875
[Epoch 8; Iter  2089/ 2483] train: loss: 0.0014047
[Epoch 8; Iter  2119/ 2483] train: loss: 0.0020735
[Epoch 8; Iter  2149/ 2483] train: loss: 0.1325409
[Epoch 8; Iter  2179/ 2483] train: loss: 0.0023396
[Epoch 8; Iter  2209/ 2483] train: loss: 0.0016816
[Epoch 8; Iter  2239/ 2483] train: loss: 0.0016435
[Epoch 8; Iter  2269/ 2483] train: loss: 0.0012988
[Epoch 8; Iter  2299/ 2483] train: loss: 0.0014125
[Epoch 8; Iter  2329/ 2483] train: loss: 0.0014626
[Epoch 8; Iter  2359/ 2483] train: loss: 0.0016625
[Epoch 8; Iter  2389/ 2483] train: loss: 0.0015955
[Epoch 8; Iter  2419/ 2483] train: loss: 0.0015942
[Epoch 8; Iter  2449/ 2483] train: loss: 0.0012648
[Epoch 8; Iter  2479/ 2483] train: loss: 0.0827670
[Epoch 8] ogbg-molmuv: 0.024333 val loss: 0.015854
[Epoch 8] ogbg-molmuv: 0.037316 test loss: 0.015981
[Epoch 9; Iter    26/ 2483] train: loss: 0.0012042
[Epoch 9; Iter    56/ 2483] train: loss: 0.1005448
[Epoch 9; Iter    86/ 2483] train: loss: 0.0999159
[Epoch 9; Iter   116/ 2483] train: loss: 0.0025917
[Epoch 9; Iter   146/ 2483] train: loss: 0.0018284
[Epoch 9; Iter   176/ 2483] train: loss: 0.0019987
[Epoch 9; Iter   206/ 2483] train: loss: 0.0019333
[Epoch 9; Iter   236/ 2483] train: loss: 0.0464093
[Epoch 9; Iter   266/ 2483] train: loss: 0.0026762
[Epoch 9; Iter   296/ 2483] train: loss: 0.0025584
[Epoch 9; Iter   326/ 2483] train: loss: 0.0024364
[Epoch 9; Iter   356/ 2483] train: loss: 0.0020197
[Epoch 9; Iter   386/ 2483] train: loss: 0.0015589
[Epoch 9; Iter   416/ 2483] train: loss: 0.0016386
[Epoch 9; Iter   446/ 2483] train: loss: 0.0022482
[Epoch 9; Iter   476/ 2483] train: loss: 0.0015330
[Epoch 9; Iter   506/ 2483] train: loss: 0.0659764
[Epoch 9; Iter   536/ 2483] train: loss: 0.0023017
[Epoch 9; Iter   566/ 2483] train: loss: 0.0014312
[Epoch 9; Iter   596/ 2483] train: loss: 0.0021964
[Epoch 9; Iter   626/ 2483] train: loss: 0.0024967
[Epoch 9; Iter   656/ 2483] train: loss: 0.0018798
[Epoch 9; Iter   686/ 2483] train: loss: 0.0017507
[Epoch 9; Iter   716/ 2483] train: loss: 0.0017153
[Epoch 9; Iter   746/ 2483] train: loss: 0.0014150
[Epoch 9; Iter   776/ 2483] train: loss: 0.1661591
[Epoch 9; Iter   806/ 2483] train: loss: 0.0017946
[Epoch 9; Iter   836/ 2483] train: loss: 0.0018291
[Epoch 9; Iter   866/ 2483] train: loss: 0.0017616
[Epoch 9; Iter   896/ 2483] train: loss: 0.0021266
[Epoch 9; Iter   926/ 2483] train: loss: 0.0764449
[Epoch 9; Iter   956/ 2483] train: loss: 0.0345731
[Epoch 9; Iter   986/ 2483] train: loss: 0.0027173
[Epoch 9; Iter  1016/ 2483] train: loss: 0.1131993
[Epoch 9; Iter  1046/ 2483] train: loss: 0.0013294
[Epoch 9; Iter  1076/ 2483] train: loss: 0.0019576
[Epoch 9; Iter  1106/ 2483] train: loss: 0.0031486
[Epoch 9; Iter  1136/ 2483] train: loss: 0.0023435
[Epoch 9; Iter  1166/ 2483] train: loss: 0.0732972
[Epoch 9; Iter  1196/ 2483] train: loss: 0.0924645
[Epoch 9; Iter  1226/ 2483] train: loss: 0.0026798
[Epoch 9; Iter  1256/ 2483] train: loss: 0.0024211
[Epoch 9; Iter  1286/ 2483] train: loss: 0.0021856
[Epoch 9; Iter  1316/ 2483] train: loss: 0.0023122
[Epoch 9; Iter  1346/ 2483] train: loss: 0.0017954
[Epoch 9; Iter  1376/ 2483] train: loss: 0.0020212
[Epoch 9; Iter  1406/ 2483] train: loss: 0.0020987
[Epoch 9; Iter  1436/ 2483] train: loss: 0.0019758
[Epoch 9; Iter  1466/ 2483] train: loss: 0.0023573
[Epoch 9; Iter  1496/ 2483] train: loss: 0.0641480
[Epoch 9; Iter  1526/ 2483] train: loss: 0.0019058
[Epoch 9; Iter  1556/ 2483] train: loss: 0.0019005
[Epoch 9; Iter  1586/ 2483] train: loss: 0.0764052
[Epoch 9; Iter  1616/ 2483] train: loss: 0.0020271
[Epoch 9; Iter  1646/ 2483] train: loss: 0.0019024
[Epoch 9; Iter  1676/ 2483] train: loss: 0.0888529
[Epoch 9; Iter  1706/ 2483] train: loss: 0.0015587
[Epoch 9; Iter  1736/ 2483] train: loss: 0.0011614
[Epoch 9; Iter  1766/ 2483] train: loss: 0.0013739
[Epoch 9; Iter  1796/ 2483] train: loss: 0.0015778
[Epoch 9; Iter  1826/ 2483] train: loss: 0.0017892
[Epoch 9; Iter  1856/ 2483] train: loss: 0.0795237
[Epoch 9; Iter  1886/ 2483] train: loss: 0.0019446
[Epoch 9; Iter  1916/ 2483] train: loss: 0.0014215
[Epoch 9; Iter  1946/ 2483] train: loss: 0.0019851
[Epoch 9; Iter  1976/ 2483] train: loss: 0.0020284
[Epoch 9; Iter  2006/ 2483] train: loss: 0.0025710
[Epoch 9; Iter  2036/ 2483] train: loss: 0.0016822
[Epoch 9; Iter  2066/ 2483] train: loss: 0.0015635
[Epoch 9; Iter  2096/ 2483] train: loss: 0.0015718
[Epoch 9; Iter  2126/ 2483] train: loss: 0.0020600
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0016814
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0013090
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0016323
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0025209
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0015924
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0015533
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0602134
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0013566
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0020312
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0047090
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0023370
[Epoch 9] ogbg-molmuv: 0.064459 val loss: 0.050458
[Epoch 9] ogbg-molmuv: 0.052053 test loss: 0.119053
[Epoch 10; Iter     3/ 2483] train: loss: 0.0027958
[Epoch 10; Iter    33/ 2483] train: loss: 0.0018595
[Epoch 10; Iter    63/ 2483] train: loss: 0.0015379
[Epoch 10; Iter    93/ 2483] train: loss: 0.0015011
[Epoch 10; Iter   123/ 2483] train: loss: 0.0908360
[Epoch 10; Iter   153/ 2483] train: loss: 0.0022531
[Epoch 10; Iter   183/ 2483] train: loss: 0.0013235
[Epoch 10; Iter   213/ 2483] train: loss: 0.0016220
[Epoch 10; Iter   243/ 2483] train: loss: 0.0815545
[Epoch 10; Iter   273/ 2483] train: loss: 0.0013288
[Epoch 10; Iter   303/ 2483] train: loss: 0.0015668
[Epoch 10; Iter   333/ 2483] train: loss: 0.0015437
[Epoch 10; Iter   363/ 2483] train: loss: 0.0015758
[Epoch 10; Iter   393/ 2483] train: loss: 0.0011602
[Epoch 10; Iter   423/ 2483] train: loss: 0.0012967
[Epoch 10; Iter   453/ 2483] train: loss: 0.0012541
[Epoch 10; Iter   483/ 2483] train: loss: 0.0016388
[Epoch 10; Iter   513/ 2483] train: loss: 0.0015485
[Epoch 10; Iter   543/ 2483] train: loss: 0.0018558
[Epoch 10; Iter   573/ 2483] train: loss: 0.0020510
[Epoch 10; Iter   603/ 2483] train: loss: 0.0015013
[Epoch 10; Iter   633/ 2483] train: loss: 0.0018176
[Epoch 10; Iter   663/ 2483] train: loss: 0.0014184
[Epoch 10; Iter   693/ 2483] train: loss: 0.0013669
[Epoch 10; Iter   723/ 2483] train: loss: 0.0015474
[Epoch 10; Iter   753/ 2483] train: loss: 0.0015218
[Epoch 10; Iter   783/ 2483] train: loss: 0.0630136
[Epoch 10; Iter   813/ 2483] train: loss: 0.0015949
[Epoch 10; Iter   843/ 2483] train: loss: 0.0024246
[Epoch 10; Iter   873/ 2483] train: loss: 0.0018838
[Epoch 10; Iter   903/ 2483] train: loss: 0.0013961
[Epoch 10; Iter   933/ 2483] train: loss: 0.0020546
[Epoch 10; Iter   963/ 2483] train: loss: 0.0026702
[Epoch 10; Iter   993/ 2483] train: loss: 0.0029270
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0886742
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0043425
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0015660
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0027551
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0031765
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0014050
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0019433
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0469304
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0029421
[Epoch 10; Iter  1293/ 2483] train: loss: 0.1017328
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0035023
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0014789
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0017237
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0013146
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0016087
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0020394
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0016315
[Epoch 10; Iter  1533/ 2483] train: loss: 0.1630823
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0023700
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0018337
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0015129
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0017627
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0019489
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0028345
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0026358
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0021791
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0217562
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0030472
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0019868
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0018453
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0012635
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0740065
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0022448
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0016908
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0939528
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0016460
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0017039
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0020546
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0021951
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0018445
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0787699
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0017253
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0020491
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0018137
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0034725
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0021422
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0013283
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0012413
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0804846
[Epoch 10] ogbg-molmuv: 0.059003 val loss: 0.055076
[Epoch 10] ogbg-molmuv: 0.078108 test loss: 0.096970
[Epoch 11; Iter    10/ 2483] train: loss: 0.0014260
[Epoch 11; Iter    40/ 2483] train: loss: 0.0627666
[Epoch 11; Iter    70/ 2483] train: loss: 0.0612861
[Epoch 11; Iter   100/ 2483] train: loss: 0.0011712
[Epoch 11; Iter   130/ 2483] train: loss: 0.0012335
[Epoch 11; Iter   160/ 2483] train: loss: 0.0014287
[Epoch 11; Iter   190/ 2483] train: loss: 0.0013108
[Epoch 11; Iter   220/ 2483] train: loss: 0.0018804
[Epoch 11; Iter   250/ 2483] train: loss: 0.0028451
[Epoch 11; Iter   280/ 2483] train: loss: 0.0552180
[Epoch 11; Iter   310/ 2483] train: loss: 0.0847633
[Epoch 11; Iter   340/ 2483] train: loss: 0.0010715
[Epoch 11; Iter   370/ 2483] train: loss: 0.0015252
[Epoch 11; Iter   400/ 2483] train: loss: 0.0013800
[Epoch 11; Iter   430/ 2483] train: loss: 0.0014069
[Epoch 11; Iter   460/ 2483] train: loss: 0.0010957
[Epoch 11; Iter   490/ 2483] train: loss: 0.0012525
[Epoch 11; Iter   520/ 2483] train: loss: 0.0012171
[Epoch 11; Iter   550/ 2483] train: loss: 0.0011444
[Epoch 11; Iter   580/ 2483] train: loss: 0.0017187
[Epoch 11; Iter   610/ 2483] train: loss: 0.0011559
[Epoch 11; Iter   640/ 2483] train: loss: 0.0979225
[Epoch 11; Iter   670/ 2483] train: loss: 0.0029492
[Epoch 11; Iter   700/ 2483] train: loss: 0.0012448
[Epoch 11; Iter   730/ 2483] train: loss: 0.0017171
[Epoch 11; Iter   760/ 2483] train: loss: 0.0010299
[Epoch 11; Iter   790/ 2483] train: loss: 0.0014676
[Epoch 11; Iter   820/ 2483] train: loss: 0.0015238
[Epoch 11; Iter   850/ 2483] train: loss: 0.0016537
[Epoch 11; Iter   880/ 2483] train: loss: 0.0011556
[Epoch 11; Iter   910/ 2483] train: loss: 0.0013096
[Epoch 11; Iter   940/ 2483] train: loss: 0.0708518
[Epoch 11; Iter   970/ 2483] train: loss: 0.0926161
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0013660
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0765862
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0017807
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0019148
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0015230
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0030035
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0495385
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0023493
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0033370
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0027853
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0023637
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0030318
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0025948
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0665602
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0020014
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0020224
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0019642
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0015406
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0015025
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0014521
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0062143
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0020929
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0011381
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0013460
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0017702
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0015584
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0017469
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0733289
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0016631
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0015153
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0012527
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0862300
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0016209
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0015288
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0017519
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0011988
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0015043
[Epoch 9] ogbg-molmuv: 0.133118 val loss: 0.013785
[Epoch 9] ogbg-molmuv: 0.073601 test loss: 0.015835
[Epoch 10; Iter     3/ 2483] train: loss: 0.0670963
[Epoch 10; Iter    33/ 2483] train: loss: 0.0016936
[Epoch 10; Iter    63/ 2483] train: loss: 0.0013308
[Epoch 10; Iter    93/ 2483] train: loss: 0.0014118
[Epoch 10; Iter   123/ 2483] train: loss: 0.0022115
[Epoch 10; Iter   153/ 2483] train: loss: 0.0468872
[Epoch 10; Iter   183/ 2483] train: loss: 0.0016284
[Epoch 10; Iter   213/ 2483] train: loss: 0.0016846
[Epoch 10; Iter   243/ 2483] train: loss: 0.0343356
[Epoch 10; Iter   273/ 2483] train: loss: 0.0031707
[Epoch 10; Iter   303/ 2483] train: loss: 0.0011326
[Epoch 10; Iter   333/ 2483] train: loss: 0.0012474
[Epoch 10; Iter   363/ 2483] train: loss: 0.0011604
[Epoch 10; Iter   393/ 2483] train: loss: 0.1033441
[Epoch 10; Iter   423/ 2483] train: loss: 0.0533550
[Epoch 10; Iter   453/ 2483] train: loss: 0.0016355
[Epoch 10; Iter   483/ 2483] train: loss: 0.0012406
[Epoch 10; Iter   513/ 2483] train: loss: 0.0013338
[Epoch 10; Iter   543/ 2483] train: loss: 0.0015907
[Epoch 10; Iter   573/ 2483] train: loss: 0.0012911
[Epoch 10; Iter   603/ 2483] train: loss: 0.0759232
[Epoch 10; Iter   633/ 2483] train: loss: 0.0028061
[Epoch 10; Iter   663/ 2483] train: loss: 0.0020472
[Epoch 10; Iter   693/ 2483] train: loss: 0.0032884
[Epoch 10; Iter   723/ 2483] train: loss: 0.0023484
[Epoch 10; Iter   753/ 2483] train: loss: 0.0022732
[Epoch 10; Iter   783/ 2483] train: loss: 0.0014362
[Epoch 10; Iter   813/ 2483] train: loss: 0.0019656
[Epoch 10; Iter   843/ 2483] train: loss: 0.0016135
[Epoch 10; Iter   873/ 2483] train: loss: 0.0014242
[Epoch 10; Iter   903/ 2483] train: loss: 0.0015280
[Epoch 10; Iter   933/ 2483] train: loss: 0.0013264
[Epoch 10; Iter   963/ 2483] train: loss: 0.0819316
[Epoch 10; Iter   993/ 2483] train: loss: 0.0025914
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0027673
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0022302
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0009452
[Epoch 10; Iter  1113/ 2483] train: loss: 0.1024610
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0024774
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0026767
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0013567
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0438774
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0014953
[Epoch 10; Iter  1293/ 2483] train: loss: 0.0804863
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0017636
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0017695
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0023951
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0687017
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0014239
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0023955
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0016898
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0013384
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0017028
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0016031
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0018932
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0011178
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0023746
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0011326
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0015167
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0011567
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0031136
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0024446
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0017210
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0014616
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0017435
[Epoch 10; Iter  1953/ 2483] train: loss: 0.1546942
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0025175
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0022937
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0023110
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0013739
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0015481
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0016884
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0030116
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0019994
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0016472
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0015906
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0015127
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0022077
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0015292
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0019971
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0612836
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0020081
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0734234
[Epoch 10] ogbg-molmuv: 0.127093 val loss: 0.013717
[Epoch 10] ogbg-molmuv: 0.093003 test loss: 0.014742
[Epoch 11; Iter    10/ 2483] train: loss: 0.0812177
[Epoch 11; Iter    40/ 2483] train: loss: 0.0023674
[Epoch 11; Iter    70/ 2483] train: loss: 0.0743260
[Epoch 11; Iter   100/ 2483] train: loss: 0.0019978
[Epoch 11; Iter   130/ 2483] train: loss: 0.0012882
[Epoch 11; Iter   160/ 2483] train: loss: 0.0020987
[Epoch 11; Iter   190/ 2483] train: loss: 0.0021159
[Epoch 11; Iter   220/ 2483] train: loss: 0.0771678
[Epoch 11; Iter   250/ 2483] train: loss: 0.0033107
[Epoch 11; Iter   280/ 2483] train: loss: 0.0018999
[Epoch 11; Iter   310/ 2483] train: loss: 0.0016383
[Epoch 11; Iter   340/ 2483] train: loss: 0.0017853
[Epoch 11; Iter   370/ 2483] train: loss: 0.0011044
[Epoch 11; Iter   400/ 2483] train: loss: 0.0011328
[Epoch 11; Iter   430/ 2483] train: loss: 0.0013099
[Epoch 11; Iter   460/ 2483] train: loss: 0.0015083
[Epoch 11; Iter   490/ 2483] train: loss: 0.0020659
[Epoch 11; Iter   520/ 2483] train: loss: 0.0015301
[Epoch 11; Iter   550/ 2483] train: loss: 0.0017133
[Epoch 11; Iter   580/ 2483] train: loss: 0.0015972
[Epoch 11; Iter   610/ 2483] train: loss: 0.0009353
[Epoch 11; Iter   640/ 2483] train: loss: 0.0010686
[Epoch 11; Iter   670/ 2483] train: loss: 0.0026312
[Epoch 11; Iter   700/ 2483] train: loss: 0.0014351
[Epoch 11; Iter   730/ 2483] train: loss: 0.0017672
[Epoch 11; Iter   760/ 2483] train: loss: 0.0018432
[Epoch 11; Iter   790/ 2483] train: loss: 0.0028242
[Epoch 11; Iter   820/ 2483] train: loss: 0.0012412
[Epoch 11; Iter   850/ 2483] train: loss: 0.0025394
[Epoch 11; Iter   880/ 2483] train: loss: 0.0016975
[Epoch 11; Iter   910/ 2483] train: loss: 0.0018852
[Epoch 11; Iter   940/ 2483] train: loss: 0.0023311
[Epoch 11; Iter   970/ 2483] train: loss: 0.0016438
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0012203
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0012268
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0015021
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0011215
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0631359
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0841087
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0014098
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0009298
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0534500
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0049020
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0016803
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0025730
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0022600
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0020092
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0017932
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0711936
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0018277
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0018269
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0023852
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0021307
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0015764
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0010773
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0018047
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0013709
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0014603
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0016484
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0014458
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0015370
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0015371
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0020684
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0011524
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0022484
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0667453
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0813526
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0011654
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0016404
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0019567
[Epoch 9] ogbg-molmuv: 0.065030 val loss: 0.019334
[Epoch 9] ogbg-molmuv: 0.050648 test loss: 0.017740
[Epoch 10; Iter     3/ 2483] train: loss: 0.0019685
[Epoch 10; Iter    33/ 2483] train: loss: 0.0012640
[Epoch 10; Iter    63/ 2483] train: loss: 0.0011624
[Epoch 10; Iter    93/ 2483] train: loss: 0.0018590
[Epoch 10; Iter   123/ 2483] train: loss: 0.0024872
[Epoch 10; Iter   153/ 2483] train: loss: 0.0015404
[Epoch 10; Iter   183/ 2483] train: loss: 0.0017489
[Epoch 10; Iter   213/ 2483] train: loss: 0.0675563
[Epoch 10; Iter   243/ 2483] train: loss: 0.0023937
[Epoch 10; Iter   273/ 2483] train: loss: 0.0023106
[Epoch 10; Iter   303/ 2483] train: loss: 0.0015937
[Epoch 10; Iter   333/ 2483] train: loss: 0.0019314
[Epoch 10; Iter   363/ 2483] train: loss: 0.0025141
[Epoch 10; Iter   393/ 2483] train: loss: 0.0021474
[Epoch 10; Iter   423/ 2483] train: loss: 0.0016629
[Epoch 10; Iter   453/ 2483] train: loss: 0.0862669
[Epoch 10; Iter   483/ 2483] train: loss: 0.0017111
[Epoch 10; Iter   513/ 2483] train: loss: 0.0022065
[Epoch 10; Iter   543/ 2483] train: loss: 0.0080936
[Epoch 10; Iter   573/ 2483] train: loss: 0.0034052
[Epoch 10; Iter   603/ 2483] train: loss: 0.0012928
[Epoch 10; Iter   633/ 2483] train: loss: 0.0022830
[Epoch 10; Iter   663/ 2483] train: loss: 0.0026818
[Epoch 10; Iter   693/ 2483] train: loss: 0.0011983
[Epoch 10; Iter   723/ 2483] train: loss: 0.0014207
[Epoch 10; Iter   753/ 2483] train: loss: 0.0017708
[Epoch 10; Iter   783/ 2483] train: loss: 0.0020567
[Epoch 10; Iter   813/ 2483] train: loss: 0.0020450
[Epoch 10; Iter   843/ 2483] train: loss: 0.0019905
[Epoch 10; Iter   873/ 2483] train: loss: 0.0018646
[Epoch 10; Iter   903/ 2483] train: loss: 0.0014477
[Epoch 10; Iter   933/ 2483] train: loss: 0.0014297
[Epoch 10; Iter   963/ 2483] train: loss: 0.0015944
[Epoch 10; Iter   993/ 2483] train: loss: 0.0012401
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0489389
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0022622
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0021954
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0016143
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0021285
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0019288
[Epoch 10; Iter  1203/ 2483] train: loss: 0.1483561
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0015158
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0018436
[Epoch 10; Iter  1293/ 2483] train: loss: 0.0015090
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0010774
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0011432
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0010797
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0011512
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0794342
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0731010
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0014174
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0017840
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0017062
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0017157
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0753342
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0031983
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0026406
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0029207
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0027420
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0024649
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0016483
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0377654
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0027079
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0018760
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0019573
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0018708
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0016223
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0019721
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0025681
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0013160
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0015897
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0010494
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0716267
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0011505
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0012869
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0015017
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0619434
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0017215
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0021533
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0685079
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0014758
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0026312
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0023778
[Epoch 10] ogbg-molmuv: 0.088542 val loss: 0.014268
[Epoch 10] ogbg-molmuv: 0.074482 test loss: 0.015149
[Epoch 11; Iter    10/ 2483] train: loss: 0.0013824
[Epoch 11; Iter    40/ 2483] train: loss: 0.0013459
[Epoch 11; Iter    70/ 2483] train: loss: 0.0017489
[Epoch 11; Iter   100/ 2483] train: loss: 0.0785055
[Epoch 11; Iter   130/ 2483] train: loss: 0.0024892
[Epoch 11; Iter   160/ 2483] train: loss: 0.0069766
[Epoch 11; Iter   190/ 2483] train: loss: 0.0018871
[Epoch 11; Iter   220/ 2483] train: loss: 0.0341142
[Epoch 11; Iter   250/ 2483] train: loss: 0.0780003
[Epoch 11; Iter   280/ 2483] train: loss: 0.0020765
[Epoch 11; Iter   310/ 2483] train: loss: 0.0033792
[Epoch 11; Iter   340/ 2483] train: loss: 0.0014379
[Epoch 11; Iter   370/ 2483] train: loss: 0.0017156
[Epoch 11; Iter   400/ 2483] train: loss: 0.0024028
[Epoch 11; Iter   430/ 2483] train: loss: 0.0020032
[Epoch 11; Iter   460/ 2483] train: loss: 0.0628309
[Epoch 11; Iter   490/ 2483] train: loss: 0.0015752
[Epoch 11; Iter   520/ 2483] train: loss: 0.0014820
[Epoch 11; Iter   550/ 2483] train: loss: 0.0014281
[Epoch 11; Iter   580/ 2483] train: loss: 0.0009699
[Epoch 11; Iter   610/ 2483] train: loss: 0.0010421
[Epoch 11; Iter   640/ 2483] train: loss: 0.0012745
[Epoch 11; Iter   670/ 2483] train: loss: 0.0014062
[Epoch 11; Iter   700/ 2483] train: loss: 0.0013219
[Epoch 11; Iter   730/ 2483] train: loss: 0.0011409
[Epoch 11; Iter   760/ 2483] train: loss: 0.0009045
[Epoch 11; Iter   790/ 2483] train: loss: 0.0019944
[Epoch 11; Iter   820/ 2483] train: loss: 0.0565698
[Epoch 11; Iter   850/ 2483] train: loss: 0.0028340
[Epoch 11; Iter   880/ 2483] train: loss: 0.0022357
[Epoch 11; Iter   910/ 2483] train: loss: 0.0020948
[Epoch 11; Iter   940/ 2483] train: loss: 0.0019645
[Epoch 11; Iter   970/ 2483] train: loss: 0.0013861
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0019600
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0023072
[Epoch 11; Iter  1060/ 2483] train: loss: 0.1627711
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0776713
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0023575
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0018431
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0021285
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0019184
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0507270
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0020291
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0012603
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0022543
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0012616
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0010998
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0021487
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0019858
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0015610
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0021168
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0034974
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0018785
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0013316
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0011722
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0014961
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0010046
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0014432
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0019513
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0018692
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0017482
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0020327
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0031658
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0014217
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0016179
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0591482
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0013578
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0028915
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0027328
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0019346
[Epoch 9] ogbg-molmuv: 0.032904 val loss: 0.015048
[Epoch 9] ogbg-molmuv: 0.026877 test loss: 0.018976
[Epoch 10; Iter     3/ 2483] train: loss: 0.0030390
[Epoch 10; Iter    33/ 2483] train: loss: 0.0015872
[Epoch 10; Iter    63/ 2483] train: loss: 0.0012695
[Epoch 10; Iter    93/ 2483] train: loss: 0.0015545
[Epoch 10; Iter   123/ 2483] train: loss: 0.0898033
[Epoch 10; Iter   153/ 2483] train: loss: 0.0019824
[Epoch 10; Iter   183/ 2483] train: loss: 0.0017027
[Epoch 10; Iter   213/ 2483] train: loss: 0.0021767
[Epoch 10; Iter   243/ 2483] train: loss: 0.0772841
[Epoch 10; Iter   273/ 2483] train: loss: 0.0014553
[Epoch 10; Iter   303/ 2483] train: loss: 0.0015013
[Epoch 10; Iter   333/ 2483] train: loss: 0.0014894
[Epoch 10; Iter   363/ 2483] train: loss: 0.0014913
[Epoch 10; Iter   393/ 2483] train: loss: 0.0012676
[Epoch 10; Iter   423/ 2483] train: loss: 0.0012317
[Epoch 10; Iter   453/ 2483] train: loss: 0.0012336
[Epoch 10; Iter   483/ 2483] train: loss: 0.0016497
[Epoch 10; Iter   513/ 2483] train: loss: 0.0022490
[Epoch 10; Iter   543/ 2483] train: loss: 0.0013170
[Epoch 10; Iter   573/ 2483] train: loss: 0.0016804
[Epoch 10; Iter   603/ 2483] train: loss: 0.0018458
[Epoch 10; Iter   633/ 2483] train: loss: 0.0018843
[Epoch 10; Iter   663/ 2483] train: loss: 0.0017187
[Epoch 10; Iter   693/ 2483] train: loss: 0.0016378
[Epoch 10; Iter   723/ 2483] train: loss: 0.0016496
[Epoch 10; Iter   753/ 2483] train: loss: 0.0014393
[Epoch 10; Iter   783/ 2483] train: loss: 0.0727350
[Epoch 10; Iter   813/ 2483] train: loss: 0.0016248
[Epoch 10; Iter   843/ 2483] train: loss: 0.0018643
[Epoch 10; Iter   873/ 2483] train: loss: 0.0014717
[Epoch 10; Iter   903/ 2483] train: loss: 0.0013798
[Epoch 10; Iter   933/ 2483] train: loss: 0.0018411
[Epoch 10; Iter   963/ 2483] train: loss: 0.0022189
[Epoch 10; Iter   993/ 2483] train: loss: 0.0022357
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0836192
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0020063
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0016529
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0023556
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0030671
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0020298
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0019458
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0479707
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0019597
[Epoch 10; Iter  1293/ 2483] train: loss: 0.1087408
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0027231
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0025493
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0014341
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0013816
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0022372
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0016223
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0019518
[Epoch 10; Iter  1533/ 2483] train: loss: 0.1438834
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0020393
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0018733
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0014376
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0016865
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0018946
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0029113
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0029794
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0018530
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0461344
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0029211
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0024116
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0018227
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0014597
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0805106
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0016815
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0017771
[Epoch 10; Iter  2043/ 2483] train: loss: 0.1007487
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0029216
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0022937
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0019612
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0022064
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0022219
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0906516
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0027957
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0019327
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0021799
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0020004
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0019399
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0014281
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0012409
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0716768
[Epoch 10] ogbg-molmuv: 0.035669 val loss: 0.014573
[Epoch 10] ogbg-molmuv: 0.027456 test loss: 0.015318
[Epoch 11; Iter    10/ 2483] train: loss: 0.0014591
[Epoch 11; Iter    40/ 2483] train: loss: 0.0698980
[Epoch 11; Iter    70/ 2483] train: loss: 0.0768404
[Epoch 11; Iter   100/ 2483] train: loss: 0.0015625
[Epoch 11; Iter   130/ 2483] train: loss: 0.0010493
[Epoch 11; Iter   160/ 2483] train: loss: 0.0011674
[Epoch 11; Iter   190/ 2483] train: loss: 0.0013958
[Epoch 11; Iter   220/ 2483] train: loss: 0.0015230
[Epoch 11; Iter   250/ 2483] train: loss: 0.0023015
[Epoch 11; Iter   280/ 2483] train: loss: 0.1034317
[Epoch 11; Iter   310/ 2483] train: loss: 0.0789844
[Epoch 11; Iter   340/ 2483] train: loss: 0.0016186
[Epoch 11; Iter   370/ 2483] train: loss: 0.0013834
[Epoch 11; Iter   400/ 2483] train: loss: 0.0015356
[Epoch 11; Iter   430/ 2483] train: loss: 0.0015393
[Epoch 11; Iter   460/ 2483] train: loss: 0.0012809
[Epoch 11; Iter   490/ 2483] train: loss: 0.0012717
[Epoch 11; Iter   520/ 2483] train: loss: 0.0013880
[Epoch 11; Iter   550/ 2483] train: loss: 0.0012123
[Epoch 11; Iter   580/ 2483] train: loss: 0.0018185
[Epoch 11; Iter   610/ 2483] train: loss: 0.0015403
[Epoch 11; Iter   640/ 2483] train: loss: 0.1117362
[Epoch 11; Iter   670/ 2483] train: loss: 0.0037386
[Epoch 11; Iter   700/ 2483] train: loss: 0.0013972
[Epoch 11; Iter   730/ 2483] train: loss: 0.0022367
[Epoch 11; Iter   760/ 2483] train: loss: 0.0018990
[Epoch 11; Iter   790/ 2483] train: loss: 0.0024483
[Epoch 11; Iter   820/ 2483] train: loss: 0.0020930
[Epoch 11; Iter   850/ 2483] train: loss: 0.0015230
[Epoch 11; Iter   880/ 2483] train: loss: 0.0010496
[Epoch 11; Iter   910/ 2483] train: loss: 0.0013582
[Epoch 11; Iter   940/ 2483] train: loss: 0.0664481
[Epoch 11; Iter   970/ 2483] train: loss: 0.0987148
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0013834
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0688569
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0018359
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0015671
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0026163
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0024575
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0710763
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0016994
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0014502
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0015890
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0040197
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0029407
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0030875
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0670192
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0049341
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0016856
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0021500
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0017543
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0014623
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0017929
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0282772
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0024477
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0013868
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0013975
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0014750
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0017579
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0020993
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0761903
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0017888
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0017254
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0012146
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0821501
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0016748
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0017999
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0014094
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0013622
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0013287
[Epoch 9] ogbg-molmuv: 0.031101 val loss: 0.014773
[Epoch 9] ogbg-molmuv: 0.026741 test loss: 0.015544
[Epoch 10; Iter     3/ 2483] train: loss: 0.0802835
[Epoch 10; Iter    33/ 2483] train: loss: 0.0018364
[Epoch 10; Iter    63/ 2483] train: loss: 0.0017108
[Epoch 10; Iter    93/ 2483] train: loss: 0.0018851
[Epoch 10; Iter   123/ 2483] train: loss: 0.0016475
[Epoch 10; Iter   153/ 2483] train: loss: 0.0584863
[Epoch 10; Iter   183/ 2483] train: loss: 0.0013826
[Epoch 10; Iter   213/ 2483] train: loss: 0.0018011
[Epoch 10; Iter   243/ 2483] train: loss: 0.0334439
[Epoch 10; Iter   273/ 2483] train: loss: 0.0018840
[Epoch 10; Iter   303/ 2483] train: loss: 0.0015738
[Epoch 10; Iter   333/ 2483] train: loss: 0.0015049
[Epoch 10; Iter   363/ 2483] train: loss: 0.0011916
[Epoch 10; Iter   393/ 2483] train: loss: 0.0922803
[Epoch 10; Iter   423/ 2483] train: loss: 0.0646012
[Epoch 10; Iter   453/ 2483] train: loss: 0.0018169
[Epoch 10; Iter   483/ 2483] train: loss: 0.0016632
[Epoch 10; Iter   513/ 2483] train: loss: 0.0011939
[Epoch 10; Iter   543/ 2483] train: loss: 0.0023527
[Epoch 10; Iter   573/ 2483] train: loss: 0.0016591
[Epoch 10; Iter   603/ 2483] train: loss: 0.0580939
[Epoch 10; Iter   633/ 2483] train: loss: 0.0017220
[Epoch 10; Iter   663/ 2483] train: loss: 0.0019437
[Epoch 10; Iter   693/ 2483] train: loss: 0.0023850
[Epoch 10; Iter   723/ 2483] train: loss: 0.0037254
[Epoch 10; Iter   753/ 2483] train: loss: 0.0016319
[Epoch 10; Iter   783/ 2483] train: loss: 0.0019716
[Epoch 10; Iter   813/ 2483] train: loss: 0.0021512
[Epoch 10; Iter   843/ 2483] train: loss: 0.0016539
[Epoch 10; Iter   873/ 2483] train: loss: 0.0017089
[Epoch 10; Iter   903/ 2483] train: loss: 0.0024950
[Epoch 10; Iter   933/ 2483] train: loss: 0.0013768
[Epoch 10; Iter   963/ 2483] train: loss: 0.0883290
[Epoch 10; Iter   993/ 2483] train: loss: 0.0027786
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0033149
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0026193
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0011338
[Epoch 10; Iter  1113/ 2483] train: loss: 0.1006225
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0020303
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0013653
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0017241
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0886898
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0015040
[Epoch 10; Iter  1293/ 2483] train: loss: 0.0782925
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0016515
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0024741
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0019976
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0697259
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0017983
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0024783
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0019511
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0013834
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0018559
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0019342
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0018153
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0017489
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0016326
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0013724
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0016065
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0013061
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0016849
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0015336
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0013288
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0016603
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0021386
[Epoch 10; Iter  1953/ 2483] train: loss: 0.1346931
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0019288
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0027120
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0022117
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0016325
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0016044
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0017618
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0027029
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0024007
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0014704
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0013949
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0015648
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0020129
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0017774
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0022086
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0511936
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0023282
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0859297
[Epoch 10] ogbg-molmuv: 0.044293 val loss: 0.014779
[Epoch 10] ogbg-molmuv: 0.030378 test loss: 0.015015
[Epoch 11; Iter    10/ 2483] train: loss: 0.0913020
[Epoch 11; Iter    40/ 2483] train: loss: 0.0031174
[Epoch 11; Iter    70/ 2483] train: loss: 0.0569705
[Epoch 11; Iter   100/ 2483] train: loss: 0.0019951
[Epoch 11; Iter   130/ 2483] train: loss: 0.0015957
[Epoch 11; Iter   160/ 2483] train: loss: 0.0019720
[Epoch 11; Iter   190/ 2483] train: loss: 0.0023275
[Epoch 11; Iter   220/ 2483] train: loss: 0.0827533
[Epoch 11; Iter   250/ 2483] train: loss: 0.0016968
[Epoch 11; Iter   280/ 2483] train: loss: 0.0018468
[Epoch 11; Iter   310/ 2483] train: loss: 0.0015974
[Epoch 11; Iter   340/ 2483] train: loss: 0.0013727
[Epoch 11; Iter   370/ 2483] train: loss: 0.0012971
[Epoch 11; Iter   400/ 2483] train: loss: 0.0014742
[Epoch 11; Iter   430/ 2483] train: loss: 0.0015594
[Epoch 11; Iter   460/ 2483] train: loss: 0.0015762
[Epoch 11; Iter   490/ 2483] train: loss: 0.0020062
[Epoch 11; Iter   520/ 2483] train: loss: 0.0015664
[Epoch 11; Iter   550/ 2483] train: loss: 0.0022070
[Epoch 11; Iter   580/ 2483] train: loss: 0.0017244
[Epoch 11; Iter   610/ 2483] train: loss: 0.0013469
[Epoch 11; Iter   640/ 2483] train: loss: 0.0013175
[Epoch 11; Iter   670/ 2483] train: loss: 0.0022346
[Epoch 11; Iter   700/ 2483] train: loss: 0.0017616
[Epoch 11; Iter   730/ 2483] train: loss: 0.0017887
[Epoch 11; Iter   760/ 2483] train: loss: 0.0018575
[Epoch 11; Iter   790/ 2483] train: loss: 0.0021751
[Epoch 11; Iter   820/ 2483] train: loss: 0.0014343
[Epoch 11; Iter   850/ 2483] train: loss: 0.0028327
[Epoch 11; Iter   880/ 2483] train: loss: 0.0018787
[Epoch 11; Iter   910/ 2483] train: loss: 0.0025563
[Epoch 11; Iter   940/ 2483] train: loss: 0.0027695
[Epoch 11; Iter   970/ 2483] train: loss: 0.0020395
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0013108
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0015422
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0016772
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0011980
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0683926
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0770992
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0021670
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0010559
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0607025
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0019538
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0016224
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0019148
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0021972
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0024133
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0018796
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0746683
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0015440
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0015957
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0026185
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0032771
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0022566
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0016725
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0019163
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0015800
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0017045
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0015293
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0018190
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0018567
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0020204
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0013986
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0016219
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0020515
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0695365
[Epoch 9; Iter  2366/ 2483] train: loss: 0.1030616
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0016398
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0020046
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0020646
[Epoch 9] ogbg-molmuv: 0.017548 val loss: 0.045899
[Epoch 9] ogbg-molmuv: 0.020950 test loss: 0.067842
[Epoch 10; Iter     3/ 2483] train: loss: 0.0022909
[Epoch 10; Iter    33/ 2483] train: loss: 0.0016086
[Epoch 10; Iter    63/ 2483] train: loss: 0.0016115
[Epoch 10; Iter    93/ 2483] train: loss: 0.0018744
[Epoch 10; Iter   123/ 2483] train: loss: 0.0017640
[Epoch 10; Iter   153/ 2483] train: loss: 0.0015571
[Epoch 10; Iter   183/ 2483] train: loss: 0.0017074
[Epoch 10; Iter   213/ 2483] train: loss: 0.0698314
[Epoch 10; Iter   243/ 2483] train: loss: 0.0020989
[Epoch 10; Iter   273/ 2483] train: loss: 0.0021377
[Epoch 10; Iter   303/ 2483] train: loss: 0.0019824
[Epoch 10; Iter   333/ 2483] train: loss: 0.0020038
[Epoch 10; Iter   363/ 2483] train: loss: 0.0021477
[Epoch 10; Iter   393/ 2483] train: loss: 0.0023264
[Epoch 10; Iter   423/ 2483] train: loss: 0.0018908
[Epoch 10; Iter   453/ 2483] train: loss: 0.0875101
[Epoch 10; Iter   483/ 2483] train: loss: 0.0020767
[Epoch 10; Iter   513/ 2483] train: loss: 0.0022676
[Epoch 10; Iter   543/ 2483] train: loss: 0.0025031
[Epoch 10; Iter   573/ 2483] train: loss: 0.0022326
[Epoch 10; Iter   603/ 2483] train: loss: 0.0020850
[Epoch 10; Iter   633/ 2483] train: loss: 0.0026439
[Epoch 10; Iter   663/ 2483] train: loss: 0.0019002
[Epoch 10; Iter   693/ 2483] train: loss: 0.0017431
[Epoch 10; Iter   723/ 2483] train: loss: 0.0017897
[Epoch 10; Iter   753/ 2483] train: loss: 0.0021814
[Epoch 10; Iter   783/ 2483] train: loss: 0.0039310
[Epoch 10; Iter   813/ 2483] train: loss: 0.0024759
[Epoch 10; Iter   843/ 2483] train: loss: 0.0020050
[Epoch 10; Iter   873/ 2483] train: loss: 0.0022658
[Epoch 10; Iter   903/ 2483] train: loss: 0.0016618
[Epoch 10; Iter   933/ 2483] train: loss: 0.0018201
[Epoch 10; Iter   963/ 2483] train: loss: 0.0017957
[Epoch 10; Iter   993/ 2483] train: loss: 0.0017127
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0523285
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0020417
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0021269
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0016414
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0021697
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0019750
[Epoch 10; Iter  1203/ 2483] train: loss: 0.1774464
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0022857
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0016005
[Epoch 10; Iter  1293/ 2483] train: loss: 0.0023065
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0013998
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0011014
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0011465
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0010879
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0834156
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0674015
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0014225
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0014941
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0012415
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0016260
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0722756
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0021424
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0027892
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0039342
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0022005
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0027669
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0017169
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0629418
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0017936
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0021140
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0017109
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0017184
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0013276
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0019645
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0019490
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0014703
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0016244
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0010657
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0916330
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0013238
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0011324
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0013072
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0688337
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0013929
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0016312
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0724338
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0024137
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0028442
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0036327
[Epoch 10] ogbg-molmuv: 0.016719 val loss: 0.015557
[Epoch 10] ogbg-molmuv: 0.027115 test loss: 0.024071
[Epoch 11; Iter    10/ 2483] train: loss: 0.0016576
[Epoch 11; Iter    40/ 2483] train: loss: 0.0016127
[Epoch 11; Iter    70/ 2483] train: loss: 0.0017102
[Epoch 11; Iter   100/ 2483] train: loss: 0.0921884
[Epoch 11; Iter   130/ 2483] train: loss: 0.0017394
[Epoch 11; Iter   160/ 2483] train: loss: 0.0025005
[Epoch 11; Iter   190/ 2483] train: loss: 0.0021131
[Epoch 11; Iter   220/ 2483] train: loss: 0.0581650
[Epoch 11; Iter   250/ 2483] train: loss: 0.0691119
[Epoch 11; Iter   280/ 2483] train: loss: 0.0022399
[Epoch 11; Iter   310/ 2483] train: loss: 0.0024449
[Epoch 11; Iter   340/ 2483] train: loss: 0.0017115
[Epoch 11; Iter   370/ 2483] train: loss: 0.0015070
[Epoch 11; Iter   400/ 2483] train: loss: 0.0023415
[Epoch 11; Iter   430/ 2483] train: loss: 0.0018303
[Epoch 11; Iter   460/ 2483] train: loss: 0.0813680
[Epoch 11; Iter   490/ 2483] train: loss: 0.0026142
[Epoch 11; Iter   520/ 2483] train: loss: 0.0016314
[Epoch 11; Iter   550/ 2483] train: loss: 0.0016832
[Epoch 11; Iter   580/ 2483] train: loss: 0.0010521
[Epoch 11; Iter   610/ 2483] train: loss: 0.0012446
[Epoch 11; Iter   640/ 2483] train: loss: 0.0017761
[Epoch 11; Iter   670/ 2483] train: loss: 0.0016547
[Epoch 11; Iter   700/ 2483] train: loss: 0.0012477
[Epoch 11; Iter   730/ 2483] train: loss: 0.0013362
[Epoch 11; Iter   760/ 2483] train: loss: 0.0012233
[Epoch 11; Iter   790/ 2483] train: loss: 0.0017986
[Epoch 11; Iter   820/ 2483] train: loss: 0.0473345
[Epoch 11; Iter   850/ 2483] train: loss: 0.0030176
[Epoch 11; Iter   880/ 2483] train: loss: 0.0019722
[Epoch 11; Iter   910/ 2483] train: loss: 0.0026228
[Epoch 11; Iter   940/ 2483] train: loss: 0.0025384
[Epoch 11; Iter   970/ 2483] train: loss: 0.0014658
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0030617
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0031268
[Epoch 11; Iter  1060/ 2483] train: loss: 0.1473568
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0750572
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0023808
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0028467
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0023182
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0023296
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0772449
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0016843
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0019345
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0013663
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0013061
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0011145
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0025114
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0013852
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0015774
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0018846
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0020638
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0020320
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0017245
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0014329
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0018384
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0014394
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0016067
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0019522
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0015351
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0014218
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0018035
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0013879
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0011621
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0016590
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0718461
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0858323
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0014832
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0015495
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0021312
[Epoch 9] ogbg-molmuv: 0.012848 val loss: 0.323468
[Epoch 9] ogbg-molmuv: 0.014192 test loss: 0.364620
[Epoch 10; Iter     3/ 2483] train: loss: 0.0024851
[Epoch 10; Iter    33/ 2483] train: loss: 0.0015167
[Epoch 10; Iter    63/ 2483] train: loss: 0.0018574
[Epoch 10; Iter    93/ 2483] train: loss: 0.0019041
[Epoch 10; Iter   123/ 2483] train: loss: 0.0017651
[Epoch 10; Iter   153/ 2483] train: loss: 0.0016360
[Epoch 10; Iter   183/ 2483] train: loss: 0.0016219
[Epoch 10; Iter   213/ 2483] train: loss: 0.0805455
[Epoch 10; Iter   243/ 2483] train: loss: 0.0021299
[Epoch 10; Iter   273/ 2483] train: loss: 0.0016963
[Epoch 10; Iter   303/ 2483] train: loss: 0.0020124
[Epoch 10; Iter   333/ 2483] train: loss: 0.0019682
[Epoch 10; Iter   363/ 2483] train: loss: 0.0019865
[Epoch 10; Iter   393/ 2483] train: loss: 0.0029767
[Epoch 10; Iter   423/ 2483] train: loss: 0.0019043
[Epoch 10; Iter   453/ 2483] train: loss: 0.0905580
[Epoch 10; Iter   483/ 2483] train: loss: 0.0018457
[Epoch 10; Iter   513/ 2483] train: loss: 0.0027242
[Epoch 10; Iter   543/ 2483] train: loss: 0.0025534
[Epoch 10; Iter   573/ 2483] train: loss: 0.0019027
[Epoch 10; Iter   603/ 2483] train: loss: 0.0017026
[Epoch 10; Iter   633/ 2483] train: loss: 0.0032852
[Epoch 10; Iter   663/ 2483] train: loss: 0.0020005
[Epoch 10; Iter   693/ 2483] train: loss: 0.0015282
[Epoch 10; Iter   723/ 2483] train: loss: 0.0016726
[Epoch 10; Iter   753/ 2483] train: loss: 0.0019540
[Epoch 10; Iter   783/ 2483] train: loss: 0.0044848
[Epoch 10; Iter   813/ 2483] train: loss: 0.0023679
[Epoch 10; Iter   843/ 2483] train: loss: 0.0021494
[Epoch 10; Iter   873/ 2483] train: loss: 0.0023690
[Epoch 10; Iter   903/ 2483] train: loss: 0.0016984
[Epoch 10; Iter   933/ 2483] train: loss: 0.0021175
[Epoch 10; Iter   963/ 2483] train: loss: 0.0017255
[Epoch 10; Iter   993/ 2483] train: loss: 0.0013215
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0550113
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0019010
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0020023
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0015413
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0026087
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0019842
[Epoch 10; Iter  1203/ 2483] train: loss: 0.1751556
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0019585
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0017385
[Epoch 10; Iter  1293/ 2483] train: loss: 0.0018227
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0014730
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0011081
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0015012
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0012585
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0863227
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0793087
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0017036
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0018250
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0013525
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0014611
[Epoch 10; Iter  1623/ 2483] train: loss: 0.1132066
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0034959
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0028842
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0033186
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0023119
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0025695
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0016547
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0608536
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0017944
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0019204
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0019840
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0018494
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0011995
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0016390
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0017595
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0013911
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0016347
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0009514
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0827733
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0012535
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0010996
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0016789
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0628995
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0017391
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0020734
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0700943
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0028780
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0031534
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0018725
[Epoch 10] ogbg-molmuv: 0.015546 val loss: 0.501292
[Epoch 10] ogbg-molmuv: 0.020933 test loss: 1.187578
[Epoch 11; Iter    10/ 2483] train: loss: 0.0022420
[Epoch 11; Iter    40/ 2483] train: loss: 0.0014494
[Epoch 11; Iter    70/ 2483] train: loss: 0.0023483
[Epoch 11; Iter   100/ 2483] train: loss: 0.0732542
[Epoch 11; Iter   130/ 2483] train: loss: 0.0029443
[Epoch 11; Iter   160/ 2483] train: loss: 0.0031496
[Epoch 11; Iter   190/ 2483] train: loss: 0.0041977
[Epoch 11; Iter   220/ 2483] train: loss: 0.0478913
[Epoch 11; Iter   250/ 2483] train: loss: 0.0858411
[Epoch 11; Iter   280/ 2483] train: loss: 0.0026149
[Epoch 11; Iter   310/ 2483] train: loss: 0.0025350
[Epoch 11; Iter   340/ 2483] train: loss: 0.0016854
[Epoch 11; Iter   370/ 2483] train: loss: 0.0012721
[Epoch 11; Iter   400/ 2483] train: loss: 0.0027391
[Epoch 11; Iter   430/ 2483] train: loss: 0.0035134
[Epoch 11; Iter   460/ 2483] train: loss: 0.0531994
[Epoch 11; Iter   490/ 2483] train: loss: 0.0017298
[Epoch 11; Iter   520/ 2483] train: loss: 0.0012629
[Epoch 11; Iter   550/ 2483] train: loss: 0.0015613
[Epoch 11; Iter   580/ 2483] train: loss: 0.0009626
[Epoch 11; Iter   610/ 2483] train: loss: 0.0011449
[Epoch 11; Iter   640/ 2483] train: loss: 0.0016472
[Epoch 11; Iter   670/ 2483] train: loss: 0.0011293
[Epoch 11; Iter   700/ 2483] train: loss: 0.0012199
[Epoch 11; Iter   730/ 2483] train: loss: 0.0013348
[Epoch 11; Iter   760/ 2483] train: loss: 0.0010918
[Epoch 11; Iter   790/ 2483] train: loss: 0.0013896
[Epoch 11; Iter   820/ 2483] train: loss: 0.0474085
[Epoch 11; Iter   850/ 2483] train: loss: 0.0033779
[Epoch 11; Iter   880/ 2483] train: loss: 0.0019755
[Epoch 11; Iter   910/ 2483] train: loss: 0.0023892
[Epoch 11; Iter   940/ 2483] train: loss: 0.0021414
[Epoch 11; Iter   970/ 2483] train: loss: 0.0018562
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0021931
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0033246
[Epoch 11; Iter  1060/ 2483] train: loss: 0.1693499
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0751042
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0020593
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0023696
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0022938
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0023284
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0681059
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0018259
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0015745
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0013488
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0014365
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0012381
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0023025
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0015860
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0015935
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0021450
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0023694
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0020465
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0017415
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0017359
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0026128
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0013773
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0017399
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0017929
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0019303
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0794418
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0016198
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0016189
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0014122
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0798029
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0012788
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0015381
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0013137
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0012261
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0012365
[Epoch 9] ogbg-molmuv: 0.027772 val loss: 0.015610
[Epoch 9] ogbg-molmuv: 0.041922 test loss: 0.016306
[Epoch 10; Iter     3/ 2483] train: loss: 0.0993451
[Epoch 10; Iter    33/ 2483] train: loss: 0.0014424
[Epoch 10; Iter    63/ 2483] train: loss: 0.0020854
[Epoch 10; Iter    93/ 2483] train: loss: 0.0019538
[Epoch 10; Iter   123/ 2483] train: loss: 0.0017495
[Epoch 10; Iter   153/ 2483] train: loss: 0.0690872
[Epoch 10; Iter   183/ 2483] train: loss: 0.0016202
[Epoch 10; Iter   213/ 2483] train: loss: 0.0018682
[Epoch 10; Iter   243/ 2483] train: loss: 0.0278007
[Epoch 10; Iter   273/ 2483] train: loss: 0.0015987
[Epoch 10; Iter   303/ 2483] train: loss: 0.0018279
[Epoch 10; Iter   333/ 2483] train: loss: 0.0020046
[Epoch 10; Iter   363/ 2483] train: loss: 0.0011243
[Epoch 10; Iter   393/ 2483] train: loss: 0.0960578
[Epoch 10; Iter   423/ 2483] train: loss: 0.0715248
[Epoch 10; Iter   453/ 2483] train: loss: 0.0017035
[Epoch 10; Iter   483/ 2483] train: loss: 0.0014171
[Epoch 10; Iter   513/ 2483] train: loss: 0.0013949
[Epoch 10; Iter   543/ 2483] train: loss: 0.0021863
[Epoch 10; Iter   573/ 2483] train: loss: 0.0017585
[Epoch 10; Iter   603/ 2483] train: loss: 0.0721754
[Epoch 10; Iter   633/ 2483] train: loss: 0.0018448
[Epoch 10; Iter   663/ 2483] train: loss: 0.0017224
[Epoch 10; Iter   693/ 2483] train: loss: 0.0025718
[Epoch 10; Iter   723/ 2483] train: loss: 0.0028931
[Epoch 10; Iter   753/ 2483] train: loss: 0.0038337
[Epoch 10; Iter   783/ 2483] train: loss: 0.0017974
[Epoch 10; Iter   813/ 2483] train: loss: 0.0023685
[Epoch 10; Iter   843/ 2483] train: loss: 0.0019170
[Epoch 10; Iter   873/ 2483] train: loss: 0.0015429
[Epoch 10; Iter   903/ 2483] train: loss: 0.0021846
[Epoch 10; Iter   933/ 2483] train: loss: 0.0016744
[Epoch 10; Iter   963/ 2483] train: loss: 0.0851623
[Epoch 10; Iter   993/ 2483] train: loss: 0.0018887
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0019998
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0019803
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0014704
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0936514
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0021876
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0014648
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0017651
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0844312
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0016149
[Epoch 10; Iter  1293/ 2483] train: loss: 0.0796362
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0017410
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0018863
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0018231
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0642009
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0019469
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0019859
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0024027
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0015041
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0016871
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0018410
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0017302
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0020420
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0016336
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0014055
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0014775
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0013146
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0016406
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0013575
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0013520
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0013195
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0021578
[Epoch 10; Iter  1953/ 2483] train: loss: 0.1269823
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0017247
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0025558
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0023756
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0014834
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0016329
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0017734
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0025518
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0022046
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0019214
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0015467
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0016105
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0022558
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0020114
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0017351
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0785497
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0022269
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0801443
[Epoch 10] ogbg-molmuv: 0.021933 val loss: 0.015330
[Epoch 10] ogbg-molmuv: 0.037947 test loss: 0.015663
[Epoch 11; Iter    10/ 2483] train: loss: 0.0866716
[Epoch 11; Iter    40/ 2483] train: loss: 0.0037223
[Epoch 11; Iter    70/ 2483] train: loss: 0.0736213
[Epoch 11; Iter   100/ 2483] train: loss: 0.0022196
[Epoch 11; Iter   130/ 2483] train: loss: 0.0016465
[Epoch 11; Iter   160/ 2483] train: loss: 0.0017944
[Epoch 11; Iter   190/ 2483] train: loss: 0.0023722
[Epoch 11; Iter   220/ 2483] train: loss: 0.0904820
[Epoch 11; Iter   250/ 2483] train: loss: 0.0016756
[Epoch 11; Iter   280/ 2483] train: loss: 0.0024832
[Epoch 11; Iter   310/ 2483] train: loss: 0.0018156
[Epoch 11; Iter   340/ 2483] train: loss: 0.0013487
[Epoch 11; Iter   370/ 2483] train: loss: 0.0014137
[Epoch 11; Iter   400/ 2483] train: loss: 0.0015432
[Epoch 11; Iter   430/ 2483] train: loss: 0.0015663
[Epoch 11; Iter   460/ 2483] train: loss: 0.0018127
[Epoch 11; Iter   490/ 2483] train: loss: 0.0021092
[Epoch 11; Iter   520/ 2483] train: loss: 0.0013752
[Epoch 11; Iter   550/ 2483] train: loss: 0.0023365
[Epoch 11; Iter   580/ 2483] train: loss: 0.0019974
[Epoch 11; Iter   610/ 2483] train: loss: 0.0011270
[Epoch 11; Iter   640/ 2483] train: loss: 0.0013141
[Epoch 11; Iter   670/ 2483] train: loss: 0.0025435
[Epoch 11; Iter   700/ 2483] train: loss: 0.0015724
[Epoch 11; Iter   730/ 2483] train: loss: 0.0024363
[Epoch 11; Iter   760/ 2483] train: loss: 0.0019197
[Epoch 11; Iter   790/ 2483] train: loss: 0.0020223
[Epoch 11; Iter   820/ 2483] train: loss: 0.0017345
[Epoch 11; Iter   850/ 2483] train: loss: 0.0029234
[Epoch 11; Iter   880/ 2483] train: loss: 0.0016015
[Epoch 11; Iter   910/ 2483] train: loss: 0.0020130
[Epoch 11; Iter   940/ 2483] train: loss: 0.0021853
[Epoch 11; Iter   970/ 2483] train: loss: 0.0021546
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0012928
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0016996
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0017284
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0014329
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0504903
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0760854
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0022802
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0014113
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0821929
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0014734
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0016773
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0019652
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0022930
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0016705
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0025042
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0769883
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0016573
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0016810
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0032835
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0028195
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0019499
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0018493
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0017928
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0016122
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0020711
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0021452
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0020147
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0015822
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0038932
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0043212
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0015385
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0016530
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0581794
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0013236
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0030250
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0020766
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0027205
[Epoch 9] ogbg-molmuv: 0.035286 val loss: 0.040544
[Epoch 9] ogbg-molmuv: 0.026030 test loss: 0.074296
[Epoch 10; Iter     3/ 2483] train: loss: 0.0030358
[Epoch 10; Iter    33/ 2483] train: loss: 0.0018471
[Epoch 10; Iter    63/ 2483] train: loss: 0.0015545
[Epoch 10; Iter    93/ 2483] train: loss: 0.0014184
[Epoch 10; Iter   123/ 2483] train: loss: 0.0957219
[Epoch 10; Iter   153/ 2483] train: loss: 0.0019514
[Epoch 10; Iter   183/ 2483] train: loss: 0.0012290
[Epoch 10; Iter   213/ 2483] train: loss: 0.0020560
[Epoch 10; Iter   243/ 2483] train: loss: 0.0678219
[Epoch 10; Iter   273/ 2483] train: loss: 0.0019335
[Epoch 10; Iter   303/ 2483] train: loss: 0.0011992
[Epoch 10; Iter   333/ 2483] train: loss: 0.0016282
[Epoch 10; Iter   363/ 2483] train: loss: 0.0015807
[Epoch 10; Iter   393/ 2483] train: loss: 0.0016350
[Epoch 10; Iter   423/ 2483] train: loss: 0.0012577
[Epoch 10; Iter   453/ 2483] train: loss: 0.0013879
[Epoch 10; Iter   483/ 2483] train: loss: 0.0017168
[Epoch 10; Iter   513/ 2483] train: loss: 0.0019639
[Epoch 10; Iter   543/ 2483] train: loss: 0.0019565
[Epoch 10; Iter   573/ 2483] train: loss: 0.0024393
[Epoch 10; Iter   603/ 2483] train: loss: 0.0014857
[Epoch 10; Iter   633/ 2483] train: loss: 0.0021259
[Epoch 10; Iter   663/ 2483] train: loss: 0.0015848
[Epoch 10; Iter   693/ 2483] train: loss: 0.0013835
[Epoch 10; Iter   723/ 2483] train: loss: 0.0018679
[Epoch 10; Iter   753/ 2483] train: loss: 0.0014476
[Epoch 10; Iter   783/ 2483] train: loss: 0.0782937
[Epoch 10; Iter   813/ 2483] train: loss: 0.0016231
[Epoch 10; Iter   843/ 2483] train: loss: 0.0017988
[Epoch 10; Iter   873/ 2483] train: loss: 0.0016442
[Epoch 10; Iter   903/ 2483] train: loss: 0.0013102
[Epoch 10; Iter   933/ 2483] train: loss: 0.0023288
[Epoch 10; Iter   963/ 2483] train: loss: 0.0033012
[Epoch 10; Iter   993/ 2483] train: loss: 0.0018676
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0753351
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0019031
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0014711
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0025360
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0024032
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0027733
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0030698
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0414869
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0030548
[Epoch 10; Iter  1293/ 2483] train: loss: 0.1129569
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0031379
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0018928
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0020134
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0018001
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0018956
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0019142
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0020879
[Epoch 10; Iter  1533/ 2483] train: loss: 0.1333810
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0021344
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0022140
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0018216
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0021856
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0019243
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0029323
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0028679
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0020329
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0467406
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0031802
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0029262
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0015759
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0013184
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0734100
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0018298
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0021327
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0930579
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0023205
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0020927
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0018052
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0028883
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0020921
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0983944
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0020514
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0020455
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0022118
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0019493
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0027423
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0018952
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0013553
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0740999
[Epoch 10] ogbg-molmuv: 0.018977 val loss: 0.015376
[Epoch 10] ogbg-molmuv: 0.020398 test loss: 0.016046
[Epoch 11; Iter    10/ 2483] train: loss: 0.0016109
[Epoch 11; Iter    40/ 2483] train: loss: 0.0664669
[Epoch 11; Iter    70/ 2483] train: loss: 0.0612975
[Epoch 11; Iter   100/ 2483] train: loss: 0.0016817
[Epoch 11; Iter   130/ 2483] train: loss: 0.0011103
[Epoch 11; Iter   160/ 2483] train: loss: 0.0013666
[Epoch 11; Iter   190/ 2483] train: loss: 0.0014693
[Epoch 11; Iter   220/ 2483] train: loss: 0.0020924
[Epoch 11; Iter   250/ 2483] train: loss: 0.0022288
[Epoch 11; Iter   280/ 2483] train: loss: 0.1044758
[Epoch 11; Iter   310/ 2483] train: loss: 0.0799369
[Epoch 11; Iter   340/ 2483] train: loss: 0.0014892
[Epoch 11; Iter   370/ 2483] train: loss: 0.0013098
[Epoch 11; Iter   400/ 2483] train: loss: 0.0012731
[Epoch 11; Iter   430/ 2483] train: loss: 0.0015536
[Epoch 11; Iter   460/ 2483] train: loss: 0.0009912
[Epoch 11; Iter   490/ 2483] train: loss: 0.0012271
[Epoch 11; Iter   520/ 2483] train: loss: 0.0012120
[Epoch 11; Iter   550/ 2483] train: loss: 0.0011408
[Epoch 11; Iter   580/ 2483] train: loss: 0.0015281
[Epoch 11; Iter   610/ 2483] train: loss: 0.0017927
[Epoch 11; Iter   640/ 2483] train: loss: 0.1047225
[Epoch 11; Iter   670/ 2483] train: loss: 0.0022910
[Epoch 11; Iter   700/ 2483] train: loss: 0.0015338
[Epoch 11; Iter   730/ 2483] train: loss: 0.0017088
[Epoch 11; Iter   760/ 2483] train: loss: 0.0015208
[Epoch 11; Iter   790/ 2483] train: loss: 0.0018173
[Epoch 11; Iter   820/ 2483] train: loss: 0.0015853
[Epoch 11; Iter   850/ 2483] train: loss: 0.0013092
[Epoch 11; Iter   880/ 2483] train: loss: 0.0013806
[Epoch 11; Iter   910/ 2483] train: loss: 0.0011811
[Epoch 11; Iter   940/ 2483] train: loss: 0.0639613
[Epoch 11; Iter   970/ 2483] train: loss: 0.1038901
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0013144
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0733912
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0022011
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0022362
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0018534
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0023893
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0501090
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0019518
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0019401
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0024330
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0032131
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0029586
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0042119
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0594699
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0019477
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0018238
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0016946
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0018784
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0017457
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0019194
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0406801
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0026585
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0019411
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0015360
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0021971
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0019130
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0022554
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0022853
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0024810
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0027884
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0015119
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0013610
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0621123
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0012359
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0034353
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0018823
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0015266
[Epoch 9] ogbg-molmuv: 0.014588 val loss: 0.111459
[Epoch 9] ogbg-molmuv: 0.022992 test loss: 0.129601
[Epoch 10; Iter     3/ 2483] train: loss: 0.0023123
[Epoch 10; Iter    33/ 2483] train: loss: 0.0019396
[Epoch 10; Iter    63/ 2483] train: loss: 0.0013940
[Epoch 10; Iter    93/ 2483] train: loss: 0.0015276
[Epoch 10; Iter   123/ 2483] train: loss: 0.0812376
[Epoch 10; Iter   153/ 2483] train: loss: 0.0015472
[Epoch 10; Iter   183/ 2483] train: loss: 0.0013470
[Epoch 10; Iter   213/ 2483] train: loss: 0.0022011
[Epoch 10; Iter   243/ 2483] train: loss: 0.0720215
[Epoch 10; Iter   273/ 2483] train: loss: 0.0016765
[Epoch 10; Iter   303/ 2483] train: loss: 0.0014345
[Epoch 10; Iter   333/ 2483] train: loss: 0.0013536
[Epoch 10; Iter   363/ 2483] train: loss: 0.0014974
[Epoch 10; Iter   393/ 2483] train: loss: 0.0018406
[Epoch 10; Iter   423/ 2483] train: loss: 0.0016256
[Epoch 10; Iter   453/ 2483] train: loss: 0.0014217
[Epoch 10; Iter   483/ 2483] train: loss: 0.0016978
[Epoch 10; Iter   513/ 2483] train: loss: 0.0021024
[Epoch 10; Iter   543/ 2483] train: loss: 0.0017543
[Epoch 10; Iter   573/ 2483] train: loss: 0.0020964
[Epoch 10; Iter   603/ 2483] train: loss: 0.0016662
[Epoch 10; Iter   633/ 2483] train: loss: 0.0020968
[Epoch 10; Iter   663/ 2483] train: loss: 0.0016096
[Epoch 10; Iter   693/ 2483] train: loss: 0.0022838
[Epoch 10; Iter   723/ 2483] train: loss: 0.0016746
[Epoch 10; Iter   753/ 2483] train: loss: 0.0015229
[Epoch 10; Iter   783/ 2483] train: loss: 0.0588465
[Epoch 10; Iter   813/ 2483] train: loss: 0.0021011
[Epoch 10; Iter   843/ 2483] train: loss: 0.0020681
[Epoch 10; Iter   873/ 2483] train: loss: 0.0016561
[Epoch 10; Iter   903/ 2483] train: loss: 0.0017081
[Epoch 10; Iter   933/ 2483] train: loss: 0.0018056
[Epoch 10; Iter   963/ 2483] train: loss: 0.0023199
[Epoch 10; Iter   993/ 2483] train: loss: 0.0020865
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0893520
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0020508
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0015838
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0020121
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0021393
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0020585
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0032421
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0606322
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0023493
[Epoch 10; Iter  1293/ 2483] train: loss: 0.1251012
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0033054
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0017707
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0019698
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0015692
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0023826
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0020464
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0014068
[Epoch 10; Iter  1533/ 2483] train: loss: 0.1548949
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0024136
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0041309
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0015287
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0022045
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0026911
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0034005
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0031754
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0018958
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0645679
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0019794
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0019845
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0015072
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0015081
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0685669
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0017055
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0019195
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0968406
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0018451
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0022660
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0021583
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0022707
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0018966
[Epoch 10; Iter  2223/ 2483] train: loss: 0.1019558
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0032769
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0022362
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0026645
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0018389
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0028241
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0017593
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0017619
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0763326
[Epoch 10] ogbg-molmuv: 0.043945 val loss: 0.018672
[Epoch 10] ogbg-molmuv: 0.033068 test loss: 0.017407
[Epoch 11; Iter    10/ 2483] train: loss: 0.0015554
[Epoch 11; Iter    40/ 2483] train: loss: 0.0724576
[Epoch 11; Iter    70/ 2483] train: loss: 0.0643806
[Epoch 11; Iter   100/ 2483] train: loss: 0.0014640
[Epoch 11; Iter   130/ 2483] train: loss: 0.0012886
[Epoch 11; Iter   160/ 2483] train: loss: 0.0012285
[Epoch 11; Iter   190/ 2483] train: loss: 0.0018430
[Epoch 11; Iter   220/ 2483] train: loss: 0.0015506
[Epoch 11; Iter   250/ 2483] train: loss: 0.0018014
[Epoch 11; Iter   280/ 2483] train: loss: 0.1150895
[Epoch 11; Iter   310/ 2483] train: loss: 0.0784764
[Epoch 11; Iter   340/ 2483] train: loss: 0.0016684
[Epoch 11; Iter   370/ 2483] train: loss: 0.0013725
[Epoch 11; Iter   400/ 2483] train: loss: 0.0011962
[Epoch 11; Iter   430/ 2483] train: loss: 0.0014876
[Epoch 11; Iter   460/ 2483] train: loss: 0.0012302
[Epoch 11; Iter   490/ 2483] train: loss: 0.0013387
[Epoch 11; Iter   520/ 2483] train: loss: 0.0018339
[Epoch 11; Iter   550/ 2483] train: loss: 0.0013204
[Epoch 11; Iter   580/ 2483] train: loss: 0.0020524
[Epoch 11; Iter   610/ 2483] train: loss: 0.0014440
[Epoch 11; Iter   640/ 2483] train: loss: 0.0918378
[Epoch 11; Iter   670/ 2483] train: loss: 0.0026496
[Epoch 11; Iter   700/ 2483] train: loss: 0.0013671
[Epoch 11; Iter   730/ 2483] train: loss: 0.0021999
[Epoch 11; Iter   760/ 2483] train: loss: 0.0014064
[Epoch 11; Iter   790/ 2483] train: loss: 0.0022206
[Epoch 11; Iter   820/ 2483] train: loss: 0.0011314
[Epoch 11; Iter   850/ 2483] train: loss: 0.0013386
[Epoch 11; Iter   880/ 2483] train: loss: 0.0009515
[Epoch 11; Iter   910/ 2483] train: loss: 0.0013488
[Epoch 11; Iter   940/ 2483] train: loss: 0.0710594
[Epoch 11; Iter   970/ 2483] train: loss: 0.0887559
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0016241
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0763916
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0021277
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0019839
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0019984
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0018123
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0566757
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0026806
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0022046
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0019839
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0037471
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0024188
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0029412
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0788740
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0022237
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0020305
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0018554
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0020982
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0020336
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0019845
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0431369
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0026448
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0016910
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0019519
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0019228
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0014777
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0018646
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0016755
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0020217
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0013745
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0012340
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0017576
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0527825
[Epoch 9; Iter  2366/ 2483] train: loss: 0.1043281
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0019656
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0018543
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0019867
[Epoch 9] ogbg-molmuv: 0.046000 val loss: 0.015780
[Epoch 9] ogbg-molmuv: 0.034740 test loss: 0.016337
[Epoch 10; Iter     3/ 2483] train: loss: 0.0028871
[Epoch 10; Iter    33/ 2483] train: loss: 0.0014065
[Epoch 10; Iter    63/ 2483] train: loss: 0.0018083
[Epoch 10; Iter    93/ 2483] train: loss: 0.0020953
[Epoch 10; Iter   123/ 2483] train: loss: 0.0015366
[Epoch 10; Iter   153/ 2483] train: loss: 0.0018518
[Epoch 10; Iter   183/ 2483] train: loss: 0.0016826
[Epoch 10; Iter   213/ 2483] train: loss: 0.0650360
[Epoch 10; Iter   243/ 2483] train: loss: 0.0024733
[Epoch 10; Iter   273/ 2483] train: loss: 0.0024435
[Epoch 10; Iter   303/ 2483] train: loss: 0.0025539
[Epoch 10; Iter   333/ 2483] train: loss: 0.0024874
[Epoch 10; Iter   363/ 2483] train: loss: 0.0019194
[Epoch 10; Iter   393/ 2483] train: loss: 0.0018413
[Epoch 10; Iter   423/ 2483] train: loss: 0.0017516
[Epoch 10; Iter   453/ 2483] train: loss: 0.0848855
[Epoch 10; Iter   483/ 2483] train: loss: 0.0017862
[Epoch 10; Iter   513/ 2483] train: loss: 0.0020126
[Epoch 10; Iter   543/ 2483] train: loss: 0.0022390
[Epoch 10; Iter   573/ 2483] train: loss: 0.0021285
[Epoch 10; Iter   603/ 2483] train: loss: 0.0018606
[Epoch 10; Iter   633/ 2483] train: loss: 0.0025333
[Epoch 10; Iter   663/ 2483] train: loss: 0.0022764
[Epoch 10; Iter   693/ 2483] train: loss: 0.0018083
[Epoch 10; Iter   723/ 2483] train: loss: 0.0017259
[Epoch 10; Iter   753/ 2483] train: loss: 0.0019144
[Epoch 10; Iter   783/ 2483] train: loss: 0.0044955
[Epoch 10; Iter   813/ 2483] train: loss: 0.0033053
[Epoch 10; Iter   843/ 2483] train: loss: 0.0020411
[Epoch 10; Iter   873/ 2483] train: loss: 0.0021680
[Epoch 10; Iter   903/ 2483] train: loss: 0.0018054
[Epoch 10; Iter   933/ 2483] train: loss: 0.0019041
[Epoch 10; Iter   963/ 2483] train: loss: 0.0019054
[Epoch 10; Iter   993/ 2483] train: loss: 0.0015184
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0678310
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0015202
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0022657
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0016981
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0022693
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0020337
[Epoch 10; Iter  1203/ 2483] train: loss: 0.1650050
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0020953
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0019895
[Epoch 10; Iter  1293/ 2483] train: loss: 0.0015056
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0013914
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0009071
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0011270
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0015114
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0740438
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0698871
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0018907
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0018801
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0021580
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0015781
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0940120
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0020093
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0034478
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0023675
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0027380
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0024260
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0016134
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0661888
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0018545
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0018709
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0016286
[Epoch 10; Iter  1953/ 2483] train: loss: 0.0017723
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0014871
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0017982
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0017172
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0013643
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0016033
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0009888
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0827054
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0013030
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0012457
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0013974
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0620432
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0016493
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0014226
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0816608
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0023789
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0031798
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0026995
[Epoch 10] ogbg-molmuv: 0.045882 val loss: 0.015118
[Epoch 10] ogbg-molmuv: 0.023656 test loss: 0.015952
[Epoch 11; Iter    10/ 2483] train: loss: 0.0020331
[Epoch 11; Iter    40/ 2483] train: loss: 0.0018166
[Epoch 11; Iter    70/ 2483] train: loss: 0.0019424
[Epoch 11; Iter   100/ 2483] train: loss: 0.0797653
[Epoch 11; Iter   130/ 2483] train: loss: 0.0042310
[Epoch 11; Iter   160/ 2483] train: loss: 0.0029729
[Epoch 11; Iter   190/ 2483] train: loss: 0.0020255
[Epoch 11; Iter   220/ 2483] train: loss: 0.0544616
[Epoch 11; Iter   250/ 2483] train: loss: 0.0835751
[Epoch 11; Iter   280/ 2483] train: loss: 0.0022795
[Epoch 11; Iter   310/ 2483] train: loss: 0.0021295
[Epoch 11; Iter   340/ 2483] train: loss: 0.0017273
[Epoch 11; Iter   370/ 2483] train: loss: 0.0017386
[Epoch 11; Iter   400/ 2483] train: loss: 0.0022851
[Epoch 11; Iter   430/ 2483] train: loss: 0.0024045
[Epoch 11; Iter   460/ 2483] train: loss: 0.0598676
[Epoch 11; Iter   490/ 2483] train: loss: 0.0020002
[Epoch 11; Iter   520/ 2483] train: loss: 0.0023529
[Epoch 11; Iter   550/ 2483] train: loss: 0.0017331
[Epoch 11; Iter   580/ 2483] train: loss: 0.0012848
[Epoch 11; Iter   610/ 2483] train: loss: 0.0011737
[Epoch 11; Iter   640/ 2483] train: loss: 0.0014802
[Epoch 11; Iter   670/ 2483] train: loss: 0.0015807
[Epoch 11; Iter   700/ 2483] train: loss: 0.0013244
[Epoch 11; Iter   730/ 2483] train: loss: 0.0012605
[Epoch 11; Iter   760/ 2483] train: loss: 0.0012616
[Epoch 11; Iter   790/ 2483] train: loss: 0.0015283
[Epoch 11; Iter   820/ 2483] train: loss: 0.0427540
[Epoch 11; Iter   850/ 2483] train: loss: 0.0020520
[Epoch 11; Iter   880/ 2483] train: loss: 0.0020934
[Epoch 11; Iter   910/ 2483] train: loss: 0.0028640
[Epoch 11; Iter   940/ 2483] train: loss: 0.0021291
[Epoch 11; Iter   970/ 2483] train: loss: 0.0014973
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0037859
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0022938
[Epoch 11; Iter  1060/ 2483] train: loss: 0.1607334
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0959246
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0018637
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0019541
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0024025
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0019332
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0670097
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0023615
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0020555
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0015628
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0012334
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0012836
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0015856
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0015397
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0018409
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0018859
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0027491
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0021788
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0014852
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0019276
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0020357
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0013827
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0016631
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0021182
[Epoch 9; Iter  2156/ 2483] train: loss: 0.0017039
[Epoch 9; Iter  2186/ 2483] train: loss: 0.0756645
[Epoch 9; Iter  2216/ 2483] train: loss: 0.0016443
[Epoch 9; Iter  2246/ 2483] train: loss: 0.0015768
[Epoch 9; Iter  2276/ 2483] train: loss: 0.0014876
[Epoch 9; Iter  2306/ 2483] train: loss: 0.0817509
[Epoch 9; Iter  2336/ 2483] train: loss: 0.0014115
[Epoch 9; Iter  2366/ 2483] train: loss: 0.0014762
[Epoch 9; Iter  2396/ 2483] train: loss: 0.0016109
[Epoch 9; Iter  2426/ 2483] train: loss: 0.0015796
[Epoch 9; Iter  2456/ 2483] train: loss: 0.0014075
[Epoch 9] ogbg-molmuv: 0.020147 val loss: 0.123588
[Epoch 9] ogbg-molmuv: 0.026406 test loss: 0.112805
[Epoch 10; Iter     3/ 2483] train: loss: 0.0793038
[Epoch 10; Iter    33/ 2483] train: loss: 0.0015993
[Epoch 10; Iter    63/ 2483] train: loss: 0.0020409
[Epoch 10; Iter    93/ 2483] train: loss: 0.0023259
[Epoch 10; Iter   123/ 2483] train: loss: 0.0016556
[Epoch 10; Iter   153/ 2483] train: loss: 0.0575847
[Epoch 10; Iter   183/ 2483] train: loss: 0.0017556
[Epoch 10; Iter   213/ 2483] train: loss: 0.0020894
[Epoch 10; Iter   243/ 2483] train: loss: 0.0659713
[Epoch 10; Iter   273/ 2483] train: loss: 0.0015663
[Epoch 10; Iter   303/ 2483] train: loss: 0.0017178
[Epoch 10; Iter   333/ 2483] train: loss: 0.0015315
[Epoch 10; Iter   363/ 2483] train: loss: 0.0011879
[Epoch 10; Iter   393/ 2483] train: loss: 0.1228934
[Epoch 10; Iter   423/ 2483] train: loss: 0.0706470
[Epoch 10; Iter   453/ 2483] train: loss: 0.0013252
[Epoch 10; Iter   483/ 2483] train: loss: 0.0016846
[Epoch 10; Iter   513/ 2483] train: loss: 0.0016369
[Epoch 10; Iter   543/ 2483] train: loss: 0.0020790
[Epoch 10; Iter   573/ 2483] train: loss: 0.0030099
[Epoch 10; Iter   603/ 2483] train: loss: 0.0888662
[Epoch 10; Iter   633/ 2483] train: loss: 0.0019972
[Epoch 10; Iter   663/ 2483] train: loss: 0.0022119
[Epoch 10; Iter   693/ 2483] train: loss: 0.0022532
[Epoch 10; Iter   723/ 2483] train: loss: 0.0034523
[Epoch 10; Iter   753/ 2483] train: loss: 0.0019403
[Epoch 10; Iter   783/ 2483] train: loss: 0.0019465
[Epoch 10; Iter   813/ 2483] train: loss: 0.0021676
[Epoch 10; Iter   843/ 2483] train: loss: 0.0025663
[Epoch 10; Iter   873/ 2483] train: loss: 0.0015666
[Epoch 10; Iter   903/ 2483] train: loss: 0.0022189
[Epoch 10; Iter   933/ 2483] train: loss: 0.0017355
[Epoch 10; Iter   963/ 2483] train: loss: 0.0890125
[Epoch 10; Iter   993/ 2483] train: loss: 0.0026635
[Epoch 10; Iter  1023/ 2483] train: loss: 0.0022134
[Epoch 10; Iter  1053/ 2483] train: loss: 0.0018444
[Epoch 10; Iter  1083/ 2483] train: loss: 0.0015539
[Epoch 10; Iter  1113/ 2483] train: loss: 0.0915939
[Epoch 10; Iter  1143/ 2483] train: loss: 0.0023265
[Epoch 10; Iter  1173/ 2483] train: loss: 0.0013983
[Epoch 10; Iter  1203/ 2483] train: loss: 0.0022133
[Epoch 10; Iter  1233/ 2483] train: loss: 0.0739943
[Epoch 10; Iter  1263/ 2483] train: loss: 0.0014938
[Epoch 10; Iter  1293/ 2483] train: loss: 0.1002656
[Epoch 10; Iter  1323/ 2483] train: loss: 0.0012900
[Epoch 10; Iter  1353/ 2483] train: loss: 0.0019043
[Epoch 10; Iter  1383/ 2483] train: loss: 0.0017121
[Epoch 10; Iter  1413/ 2483] train: loss: 0.0725900
[Epoch 10; Iter  1443/ 2483] train: loss: 0.0018019
[Epoch 10; Iter  1473/ 2483] train: loss: 0.0017467
[Epoch 10; Iter  1503/ 2483] train: loss: 0.0021589
[Epoch 10; Iter  1533/ 2483] train: loss: 0.0021300
[Epoch 10; Iter  1563/ 2483] train: loss: 0.0016901
[Epoch 10; Iter  1593/ 2483] train: loss: 0.0018258
[Epoch 10; Iter  1623/ 2483] train: loss: 0.0017937
[Epoch 10; Iter  1653/ 2483] train: loss: 0.0015484
[Epoch 10; Iter  1683/ 2483] train: loss: 0.0015252
[Epoch 10; Iter  1713/ 2483] train: loss: 0.0013689
[Epoch 10; Iter  1743/ 2483] train: loss: 0.0015232
[Epoch 10; Iter  1773/ 2483] train: loss: 0.0013097
[Epoch 10; Iter  1803/ 2483] train: loss: 0.0012616
[Epoch 10; Iter  1833/ 2483] train: loss: 0.0013671
[Epoch 10; Iter  1863/ 2483] train: loss: 0.0015567
[Epoch 10; Iter  1893/ 2483] train: loss: 0.0016234
[Epoch 10; Iter  1923/ 2483] train: loss: 0.0015067
[Epoch 10; Iter  1953/ 2483] train: loss: 0.1501192
[Epoch 10; Iter  1983/ 2483] train: loss: 0.0020949
[Epoch 10; Iter  2013/ 2483] train: loss: 0.0038874
[Epoch 10; Iter  2043/ 2483] train: loss: 0.0024072
[Epoch 10; Iter  2073/ 2483] train: loss: 0.0017510
[Epoch 10; Iter  2103/ 2483] train: loss: 0.0016165
[Epoch 10; Iter  2133/ 2483] train: loss: 0.0018269
[Epoch 10; Iter  2163/ 2483] train: loss: 0.0022057
[Epoch 10; Iter  2193/ 2483] train: loss: 0.0024115
[Epoch 10; Iter  2223/ 2483] train: loss: 0.0023644
[Epoch 10; Iter  2253/ 2483] train: loss: 0.0017010
[Epoch 10; Iter  2283/ 2483] train: loss: 0.0016367
[Epoch 10; Iter  2313/ 2483] train: loss: 0.0020180
[Epoch 10; Iter  2343/ 2483] train: loss: 0.0018728
[Epoch 10; Iter  2373/ 2483] train: loss: 0.0022237
[Epoch 10; Iter  2403/ 2483] train: loss: 0.0736034
[Epoch 10; Iter  2433/ 2483] train: loss: 0.0023115
[Epoch 10; Iter  2463/ 2483] train: loss: 0.0801399
[Epoch 10] ogbg-molmuv: 0.027318 val loss: 0.015167
[Epoch 10] ogbg-molmuv: 0.021864 test loss: 0.015277
[Epoch 11; Iter    10/ 2483] train: loss: 0.1048077
[Epoch 11; Iter    40/ 2483] train: loss: 0.0026628
[Epoch 11; Iter    70/ 2483] train: loss: 0.0944360
[Epoch 11; Iter   100/ 2483] train: loss: 0.0019217
[Epoch 11; Iter   130/ 2483] train: loss: 0.0016799
[Epoch 11; Iter   160/ 2483] train: loss: 0.0016069
[Epoch 11; Iter   190/ 2483] train: loss: 0.0017255
[Epoch 11; Iter   220/ 2483] train: loss: 0.0804184
[Epoch 11; Iter   250/ 2483] train: loss: 0.0014189
[Epoch 11; Iter   280/ 2483] train: loss: 0.0024017
[Epoch 11; Iter   310/ 2483] train: loss: 0.0015912
[Epoch 11; Iter   340/ 2483] train: loss: 0.0013663
[Epoch 11; Iter   370/ 2483] train: loss: 0.0014751
[Epoch 11; Iter   400/ 2483] train: loss: 0.0015676
[Epoch 11; Iter   430/ 2483] train: loss: 0.0014886
[Epoch 11; Iter   460/ 2483] train: loss: 0.0014787
[Epoch 11; Iter   490/ 2483] train: loss: 0.0019605
[Epoch 11; Iter   520/ 2483] train: loss: 0.0015176
[Epoch 11; Iter   550/ 2483] train: loss: 0.0021724
[Epoch 11; Iter   580/ 2483] train: loss: 0.0018761
[Epoch 11; Iter   610/ 2483] train: loss: 0.0015053
[Epoch 11; Iter   640/ 2483] train: loss: 0.0012449
[Epoch 11; Iter   670/ 2483] train: loss: 0.0019330
[Epoch 11; Iter   700/ 2483] train: loss: 0.0019254
[Epoch 11; Iter   730/ 2483] train: loss: 0.0017824
[Epoch 11; Iter   760/ 2483] train: loss: 0.0016059
[Epoch 11; Iter   790/ 2483] train: loss: 0.0021325
[Epoch 11; Iter   820/ 2483] train: loss: 0.0018454
[Epoch 11; Iter   850/ 2483] train: loss: 0.0027244
[Epoch 11; Iter   880/ 2483] train: loss: 0.0018336
[Epoch 11; Iter   910/ 2483] train: loss: 0.0018611
[Epoch 11; Iter   940/ 2483] train: loss: 0.0023161
[Epoch 11; Iter   970/ 2483] train: loss: 0.0027136
[Epoch 11; Iter  1000/ 2483] train: loss: 0.0012405
[Epoch 11; Iter  1030/ 2483] train: loss: 0.0012540
[Epoch 11; Iter  1060/ 2483] train: loss: 0.0022097
[Epoch 11; Iter  1090/ 2483] train: loss: 0.0013064
[Epoch 11; Iter  1120/ 2483] train: loss: 0.0549024
[Epoch 11; Iter  1150/ 2483] train: loss: 0.0858851
[Epoch 11; Iter  1180/ 2483] train: loss: 0.0016933
[Epoch 11; Iter  1210/ 2483] train: loss: 0.0014630
[Epoch 11; Iter  1240/ 2483] train: loss: 0.0796025
[Epoch 11; Iter  1270/ 2483] train: loss: 0.0017500
[Epoch 11; Iter  1300/ 2483] train: loss: 0.0016349
[Epoch 11; Iter  1330/ 2483] train: loss: 0.0017352
[Epoch 11; Iter  1360/ 2483] train: loss: 0.0025853
[Epoch 11; Iter  1390/ 2483] train: loss: 0.0016165
[Epoch 11; Iter  1420/ 2483] train: loss: 0.0019037
[Epoch 11; Iter  1450/ 2483] train: loss: 0.0789803
[Epoch 11; Iter  1480/ 2483] train: loss: 0.0017713
[Epoch 11; Iter  1510/ 2483] train: loss: 0.0017016
[Epoch 11; Iter  1540/ 2483] train: loss: 0.0021508
[Epoch 11; Iter  1570/ 2483] train: loss: 0.0027060
[Epoch 11; Iter  1600/ 2483] train: loss: 0.0024125
[Epoch 11; Iter  1630/ 2483] train: loss: 0.0016328
[Epoch 11; Iter  1660/ 2483] train: loss: 0.0020293
[Epoch 11; Iter  1690/ 2483] train: loss: 0.0016059
[Epoch 11; Iter  1720/ 2483] train: loss: 0.0016191
[Epoch 11; Iter  1750/ 2483] train: loss: 0.0015385
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0019652
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0019604
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0695835
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0031447
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0021799
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0024331
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0021233
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0927779
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0021167
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0015792
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0022001
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0014112
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0012978
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0017000
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0016407
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0019450
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0015754
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0731055
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0016227
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0016264
[Epoch 11; Iter  2380/ 2483] train: loss: 0.1968713
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0017013
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0017399
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0021394
[Epoch 11] ogbg-molmuv: 0.098328 val loss: 0.014240
[Epoch 11] ogbg-molmuv: 0.063352 test loss: 0.014740
[Epoch 12; Iter    17/ 2483] train: loss: 0.0038127
[Epoch 12; Iter    47/ 2483] train: loss: 0.0027339
[Epoch 12; Iter    77/ 2483] train: loss: 0.0016122
[Epoch 12; Iter   107/ 2483] train: loss: 0.0015297
[Epoch 12; Iter   137/ 2483] train: loss: 0.0016618
[Epoch 12; Iter   167/ 2483] train: loss: 0.0020115
[Epoch 12; Iter   197/ 2483] train: loss: 0.0014802
[Epoch 12; Iter   227/ 2483] train: loss: 0.0022087
[Epoch 12; Iter   257/ 2483] train: loss: 0.0813036
[Epoch 12; Iter   287/ 2483] train: loss: 0.0012952
[Epoch 12; Iter   317/ 2483] train: loss: 0.0017572
[Epoch 12; Iter   347/ 2483] train: loss: 0.0018305
[Epoch 12; Iter   377/ 2483] train: loss: 0.0015578
[Epoch 12; Iter   407/ 2483] train: loss: 0.0017322
[Epoch 12; Iter   437/ 2483] train: loss: 0.0014762
[Epoch 12; Iter   467/ 2483] train: loss: 0.0017627
[Epoch 12; Iter   497/ 2483] train: loss: 0.0020075
[Epoch 12; Iter   527/ 2483] train: loss: 0.0018113
[Epoch 12; Iter   557/ 2483] train: loss: 0.0015918
[Epoch 12; Iter   587/ 2483] train: loss: 0.0018042
[Epoch 12; Iter   617/ 2483] train: loss: 0.0014961
[Epoch 12; Iter   647/ 2483] train: loss: 0.0016010
[Epoch 12; Iter   677/ 2483] train: loss: 0.0015799
[Epoch 12; Iter   707/ 2483] train: loss: 0.0020157
[Epoch 12; Iter   737/ 2483] train: loss: 0.0015344
[Epoch 12; Iter   767/ 2483] train: loss: 0.0012682
[Epoch 12; Iter   797/ 2483] train: loss: 0.0813286
[Epoch 12; Iter   827/ 2483] train: loss: 0.0018362
[Epoch 12; Iter   857/ 2483] train: loss: 0.0012815
[Epoch 12; Iter   887/ 2483] train: loss: 0.0013899
[Epoch 12; Iter   917/ 2483] train: loss: 0.0011572
[Epoch 12; Iter   947/ 2483] train: loss: 0.0946367
[Epoch 12; Iter   977/ 2483] train: loss: 0.0011544
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0015256
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0018067
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0015942
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0020831
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0031713
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0022390
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0625163
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0014122
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0018265
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0710382
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0030667
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0044684
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0014087
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0623498
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0040193
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0030277
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0024340
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0028820
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0703763
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0021212
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0516510
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0020731
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0037967
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0015904
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0033696
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0025729
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0628789
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0029363
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0016524
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0023897
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0015169
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0016893
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0013004
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0013841
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0010839
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0015744
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0019583
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0012712
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0029321
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0014323
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0034615
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0017761
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0016372
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0012674
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0013658
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0021840
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0012612
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0059922
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0019184
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0015361
[Epoch 12] ogbg-molmuv: 0.116083 val loss: 0.014971
[Epoch 12] ogbg-molmuv: 0.101103 test loss: 0.053742
[Epoch 13; Iter    24/ 2483] train: loss: 0.0842713
[Epoch 13; Iter    54/ 2483] train: loss: 0.0274405
[Epoch 13; Iter    84/ 2483] train: loss: 0.0030611
[Epoch 13; Iter   114/ 2483] train: loss: 0.0019888
[Epoch 13; Iter   144/ 2483] train: loss: 0.0017783
[Epoch 13; Iter   174/ 2483] train: loss: 0.0020381
[Epoch 13; Iter   204/ 2483] train: loss: 0.0017023
[Epoch 13; Iter   234/ 2483] train: loss: 0.0721175
[Epoch 13; Iter   264/ 2483] train: loss: 0.0729640
[Epoch 13; Iter   294/ 2483] train: loss: 0.0016406
[Epoch 13; Iter   324/ 2483] train: loss: 0.0015732
[Epoch 13; Iter   354/ 2483] train: loss: 0.0583429
[Epoch 13; Iter   384/ 2483] train: loss: 0.0025319
[Epoch 13; Iter   414/ 2483] train: loss: 0.0600076
[Epoch 13; Iter   444/ 2483] train: loss: 0.0022397
[Epoch 13; Iter   474/ 2483] train: loss: 0.0018316
[Epoch 13; Iter   504/ 2483] train: loss: 0.0011990
[Epoch 13; Iter   534/ 2483] train: loss: 0.0014331
[Epoch 13; Iter   564/ 2483] train: loss: 0.0020590
[Epoch 13; Iter   594/ 2483] train: loss: 0.0013770
[Epoch 13; Iter   624/ 2483] train: loss: 0.0018664
[Epoch 13; Iter   654/ 2483] train: loss: 0.0019886
[Epoch 13; Iter   684/ 2483] train: loss: 0.0023380
[Epoch 13; Iter   714/ 2483] train: loss: 0.0014612
[Epoch 13; Iter   744/ 2483] train: loss: 0.0011425
[Epoch 13; Iter   774/ 2483] train: loss: 0.0009356
[Epoch 13; Iter   804/ 2483] train: loss: 0.0010831
[Epoch 13; Iter   834/ 2483] train: loss: 0.0021321
[Epoch 13; Iter   864/ 2483] train: loss: 0.0017688
[Epoch 13; Iter   894/ 2483] train: loss: 0.0011794
[Epoch 13; Iter   924/ 2483] train: loss: 0.0014646
[Epoch 13; Iter   954/ 2483] train: loss: 0.0017424
[Epoch 13; Iter   984/ 2483] train: loss: 0.0750127
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0013171
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0019270
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0013284
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0011634
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0010511
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0548092
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0018413
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0012345
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0402765
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0019624
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0058118
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0018851
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0376343
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0022564
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0928880
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0017135
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0016256
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0021062
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0018782
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0023068
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0036787
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0024049
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0027515
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0017274
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0015911
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0537189
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0017365
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0019707
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0016962
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0041389
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0020723
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0022127
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0024546
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0014809
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0017323
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0018809
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0021562
[Epoch 11] ogbg-molmuv: 0.048643 val loss: 0.014478
[Epoch 11] ogbg-molmuv: 0.023353 test loss: 0.015469
[Epoch 12; Iter    17/ 2483] train: loss: 0.0015180
[Epoch 12; Iter    47/ 2483] train: loss: 0.0016066
[Epoch 12; Iter    77/ 2483] train: loss: 0.0012132
[Epoch 12; Iter   107/ 2483] train: loss: 0.0014406
[Epoch 12; Iter   137/ 2483] train: loss: 0.0014159
[Epoch 12; Iter   167/ 2483] train: loss: 0.0012284
[Epoch 12; Iter   197/ 2483] train: loss: 0.0014712
[Epoch 12; Iter   227/ 2483] train: loss: 0.0014559
[Epoch 12; Iter   257/ 2483] train: loss: 0.0014690
[Epoch 12; Iter   287/ 2483] train: loss: 0.0019652
[Epoch 12; Iter   317/ 2483] train: loss: 0.0015179
[Epoch 12; Iter   347/ 2483] train: loss: 0.0021298
[Epoch 12; Iter   377/ 2483] train: loss: 0.0015248
[Epoch 12; Iter   407/ 2483] train: loss: 0.0012262
[Epoch 12; Iter   437/ 2483] train: loss: 0.0016516
[Epoch 12; Iter   467/ 2483] train: loss: 0.0016250
[Epoch 12; Iter   497/ 2483] train: loss: 0.0011778
[Epoch 12; Iter   527/ 2483] train: loss: 0.0017001
[Epoch 12; Iter   557/ 2483] train: loss: 0.0022150
[Epoch 12; Iter   587/ 2483] train: loss: 0.0020105
[Epoch 12; Iter   617/ 2483] train: loss: 0.0021649
[Epoch 12; Iter   647/ 2483] train: loss: 0.0026722
[Epoch 12; Iter   677/ 2483] train: loss: 0.0019131
[Epoch 12; Iter   707/ 2483] train: loss: 0.0020445
[Epoch 12; Iter   737/ 2483] train: loss: 0.0012296
[Epoch 12; Iter   767/ 2483] train: loss: 0.0012937
[Epoch 12; Iter   797/ 2483] train: loss: 0.0030780
[Epoch 12; Iter   827/ 2483] train: loss: 0.0015316
[Epoch 12; Iter   857/ 2483] train: loss: 0.0018393
[Epoch 12; Iter   887/ 2483] train: loss: 0.0014057
[Epoch 12; Iter   917/ 2483] train: loss: 0.0017394
[Epoch 12; Iter   947/ 2483] train: loss: 0.0017697
[Epoch 12; Iter   977/ 2483] train: loss: 0.0030782
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0813354
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0022133
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0015400
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0040148
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0019489
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0013465
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0017290
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0025851
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0020225
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0027562
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0020341
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0012444
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0011450
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0015846
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0020725
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0023906
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0013890
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0015437
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0026525
[Epoch 12; Iter  1577/ 2483] train: loss: 0.1717978
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0020549
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0016287
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0014357
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0013204
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0013043
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0022529
[Epoch 12; Iter  1787/ 2483] train: loss: 0.2422574
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0024607
[Epoch 12; Iter  1847/ 2483] train: loss: 0.1239711
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0022930
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0681140
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0022272
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0024451
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0016847
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0708288
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0013683
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0017631
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0016392
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0881656
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0018651
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0024231
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0665108
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0023452
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0025239
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0022612
[Epoch 12; Iter  2357/ 2483] train: loss: 0.1148462
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0614912
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0023691
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0033077
[Epoch 12; Iter  2477/ 2483] train: loss: 0.1331127
[Epoch 12] ogbg-molmuv: 0.041779 val loss: 0.015802
[Epoch 12] ogbg-molmuv: 0.016093 test loss: 0.016822
[Epoch 13; Iter    24/ 2483] train: loss: 0.0649506
[Epoch 13; Iter    54/ 2483] train: loss: 0.0018446
[Epoch 13; Iter    84/ 2483] train: loss: 0.0021973
[Epoch 13; Iter   114/ 2483] train: loss: 0.1359091
[Epoch 13; Iter   144/ 2483] train: loss: 0.0029312
[Epoch 13; Iter   174/ 2483] train: loss: 0.0040792
[Epoch 13; Iter   204/ 2483] train: loss: 0.0064592
[Epoch 13; Iter   234/ 2483] train: loss: 0.0039859
[Epoch 13; Iter   264/ 2483] train: loss: 0.0038822
[Epoch 13; Iter   294/ 2483] train: loss: 0.0033303
[Epoch 13; Iter   324/ 2483] train: loss: 0.0028547
[Epoch 13; Iter   354/ 2483] train: loss: 0.0026117
[Epoch 13; Iter   384/ 2483] train: loss: 0.0033135
[Epoch 13; Iter   414/ 2483] train: loss: 0.0741934
[Epoch 13; Iter   444/ 2483] train: loss: 0.0017938
[Epoch 13; Iter   474/ 2483] train: loss: 0.0840206
[Epoch 13; Iter   504/ 2483] train: loss: 0.0027945
[Epoch 13; Iter   534/ 2483] train: loss: 0.0819837
[Epoch 13; Iter   564/ 2483] train: loss: 0.0026051
[Epoch 13; Iter   594/ 2483] train: loss: 0.0018238
[Epoch 13; Iter   624/ 2483] train: loss: 0.0035221
[Epoch 13; Iter   654/ 2483] train: loss: 0.0018968
[Epoch 13; Iter   684/ 2483] train: loss: 0.0021447
[Epoch 13; Iter   714/ 2483] train: loss: 0.0012865
[Epoch 13; Iter   744/ 2483] train: loss: 0.0014666
[Epoch 13; Iter   774/ 2483] train: loss: 0.0013419
[Epoch 13; Iter   804/ 2483] train: loss: 0.0016474
[Epoch 13; Iter   834/ 2483] train: loss: 0.0014392
[Epoch 13; Iter   864/ 2483] train: loss: 0.0014312
[Epoch 13; Iter   894/ 2483] train: loss: 0.0010421
[Epoch 13; Iter   924/ 2483] train: loss: 0.0014735
[Epoch 13; Iter   954/ 2483] train: loss: 0.0014133
[Epoch 13; Iter   984/ 2483] train: loss: 0.0010206
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0011675
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0014362
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0012192
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0012384
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0827550
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0018432
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0017287
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0024004
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0013992
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0017046
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0012342
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0012944
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0010094
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0019162
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0026900
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0017557
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0017022
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0030142
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0022696
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0588995
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0017573
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0017304
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0753131
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0014511
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0012753
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0014383
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0014534
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0015254
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0645176
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0018052
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0030875
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0015170
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0016224
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0017264
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0019077
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0930980
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0017201
[Epoch 11] ogbg-molmuv: 0.017024 val loss: 0.017798
[Epoch 11] ogbg-molmuv: 0.023784 test loss: 0.017407
[Epoch 12; Iter    17/ 2483] train: loss: 0.0939187
[Epoch 12; Iter    47/ 2483] train: loss: 0.0023371
[Epoch 12; Iter    77/ 2483] train: loss: 0.0026281
[Epoch 12; Iter   107/ 2483] train: loss: 0.0030719
[Epoch 12; Iter   137/ 2483] train: loss: 0.0979857
[Epoch 12; Iter   167/ 2483] train: loss: 0.0028919
[Epoch 12; Iter   197/ 2483] train: loss: 0.0019645
[Epoch 12; Iter   227/ 2483] train: loss: 0.0019595
[Epoch 12; Iter   257/ 2483] train: loss: 0.0022571
[Epoch 12; Iter   287/ 2483] train: loss: 0.0015805
[Epoch 12; Iter   317/ 2483] train: loss: 0.0016354
[Epoch 12; Iter   347/ 2483] train: loss: 0.0016083
[Epoch 12; Iter   377/ 2483] train: loss: 0.0023249
[Epoch 12; Iter   407/ 2483] train: loss: 0.0015721
[Epoch 12; Iter   437/ 2483] train: loss: 0.0014388
[Epoch 12; Iter   467/ 2483] train: loss: 0.0016307
[Epoch 12; Iter   497/ 2483] train: loss: 0.0012367
[Epoch 12; Iter   527/ 2483] train: loss: 0.0021264
[Epoch 12; Iter   557/ 2483] train: loss: 0.0014328
[Epoch 12; Iter   587/ 2483] train: loss: 0.0022379
[Epoch 12; Iter   617/ 2483] train: loss: 0.0574969
[Epoch 12; Iter   647/ 2483] train: loss: 0.0636316
[Epoch 12; Iter   677/ 2483] train: loss: 0.0029889
[Epoch 12; Iter   707/ 2483] train: loss: 0.0013770
[Epoch 12; Iter   737/ 2483] train: loss: 0.0014773
[Epoch 12; Iter   767/ 2483] train: loss: 0.0015429
[Epoch 12; Iter   797/ 2483] train: loss: 0.0821962
[Epoch 12; Iter   827/ 2483] train: loss: 0.0015413
[Epoch 12; Iter   857/ 2483] train: loss: 0.0012293
[Epoch 12; Iter   887/ 2483] train: loss: 0.0024782
[Epoch 12; Iter   917/ 2483] train: loss: 0.0020865
[Epoch 12; Iter   947/ 2483] train: loss: 0.0017935
[Epoch 12; Iter   977/ 2483] train: loss: 0.0566835
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0828242
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0015967
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0021744
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0017352
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0021601
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0017847
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0022913
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0020981
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0021818
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0019808
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0017787
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0027098
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0945149
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0013325
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0013486
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0017951
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0017619
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0013992
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0016425
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0019651
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0018306
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0013412
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0027665
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0022273
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0018181
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0023848
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0019775
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0012462
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0022937
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0035771
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0018104
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0015991
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0931042
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0016256
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0479236
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0017283
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0017103
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0015315
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0015721
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0019144
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0015537
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0714864
[Epoch 12; Iter  2267/ 2483] train: loss: 0.1006503
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0617369
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0018606
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0019652
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0017623
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0015190
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0023396
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0013977
[Epoch 12] ogbg-molmuv: 0.034691 val loss: 0.016537
[Epoch 12] ogbg-molmuv: 0.044751 test loss: 0.017748
[Epoch 13; Iter    24/ 2483] train: loss: 0.0018015
[Epoch 13; Iter    54/ 2483] train: loss: 0.0018168
[Epoch 13; Iter    84/ 2483] train: loss: 0.0018080
[Epoch 13; Iter   114/ 2483] train: loss: 0.0012786
[Epoch 13; Iter   144/ 2483] train: loss: 0.0020398
[Epoch 13; Iter   174/ 2483] train: loss: 0.0018489
[Epoch 13; Iter   204/ 2483] train: loss: 0.0017118
[Epoch 13; Iter   234/ 2483] train: loss: 0.0013717
[Epoch 13; Iter   264/ 2483] train: loss: 0.0012100
[Epoch 13; Iter   294/ 2483] train: loss: 0.0011030
[Epoch 13; Iter   324/ 2483] train: loss: 0.0016963
[Epoch 13; Iter   354/ 2483] train: loss: 0.0014850
[Epoch 13; Iter   384/ 2483] train: loss: 0.0866413
[Epoch 13; Iter   414/ 2483] train: loss: 0.0023534
[Epoch 13; Iter   444/ 2483] train: loss: 0.0016300
[Epoch 13; Iter   474/ 2483] train: loss: 0.0013491
[Epoch 13; Iter   504/ 2483] train: loss: 0.0012838
[Epoch 13; Iter   534/ 2483] train: loss: 0.0011709
[Epoch 13; Iter   564/ 2483] train: loss: 0.0909080
[Epoch 13; Iter   594/ 2483] train: loss: 0.0016831
[Epoch 13; Iter   624/ 2483] train: loss: 0.0015038
[Epoch 13; Iter   654/ 2483] train: loss: 0.0014838
[Epoch 13; Iter   684/ 2483] train: loss: 0.0014709
[Epoch 13; Iter   714/ 2483] train: loss: 0.0729755
[Epoch 13; Iter   744/ 2483] train: loss: 0.0017078
[Epoch 13; Iter   774/ 2483] train: loss: 0.0021202
[Epoch 13; Iter   804/ 2483] train: loss: 0.0021316
[Epoch 13; Iter   834/ 2483] train: loss: 0.0023012
[Epoch 13; Iter   864/ 2483] train: loss: 0.0013840
[Epoch 13; Iter   894/ 2483] train: loss: 0.0014863
[Epoch 13; Iter   924/ 2483] train: loss: 0.0015072
[Epoch 13; Iter   954/ 2483] train: loss: 0.0019622
[Epoch 13; Iter   984/ 2483] train: loss: 0.0023640
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0022011
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0013581
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0012439
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0687295
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0634627
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0026412
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0024750
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0015874
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0031291
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0013268
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0019995
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0018425
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0016281
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0023053
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0021218
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0820721
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0029703
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0018876
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0031423
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0027805
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0928019
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0022260
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0016005
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0018820
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0014043
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0020893
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0016466
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0026832
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0021087
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0018139
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0639937
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0020153
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0015816
[Epoch 11; Iter  2380/ 2483] train: loss: 0.1397359
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0017408
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0015759
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0018169
[Epoch 11] ogbg-molmuv: 0.048262 val loss: 0.014502
[Epoch 11] ogbg-molmuv: 0.026683 test loss: 0.015477
[Epoch 12; Iter    17/ 2483] train: loss: 0.0033728
[Epoch 12; Iter    47/ 2483] train: loss: 0.0027734
[Epoch 12; Iter    77/ 2483] train: loss: 0.0018034
[Epoch 12; Iter   107/ 2483] train: loss: 0.0014763
[Epoch 12; Iter   137/ 2483] train: loss: 0.0013144
[Epoch 12; Iter   167/ 2483] train: loss: 0.0019101
[Epoch 12; Iter   197/ 2483] train: loss: 0.0018661
[Epoch 12; Iter   227/ 2483] train: loss: 0.0018393
[Epoch 12; Iter   257/ 2483] train: loss: 0.0816386
[Epoch 12; Iter   287/ 2483] train: loss: 0.0014685
[Epoch 12; Iter   317/ 2483] train: loss: 0.0020218
[Epoch 12; Iter   347/ 2483] train: loss: 0.0015801
[Epoch 12; Iter   377/ 2483] train: loss: 0.0019965
[Epoch 12; Iter   407/ 2483] train: loss: 0.0017856
[Epoch 12; Iter   437/ 2483] train: loss: 0.0015875
[Epoch 12; Iter   467/ 2483] train: loss: 0.0018578
[Epoch 12; Iter   497/ 2483] train: loss: 0.0014634
[Epoch 12; Iter   527/ 2483] train: loss: 0.0022096
[Epoch 12; Iter   557/ 2483] train: loss: 0.0014624
[Epoch 12; Iter   587/ 2483] train: loss: 0.0017212
[Epoch 12; Iter   617/ 2483] train: loss: 0.0018865
[Epoch 12; Iter   647/ 2483] train: loss: 0.0019263
[Epoch 12; Iter   677/ 2483] train: loss: 0.0013213
[Epoch 12; Iter   707/ 2483] train: loss: 0.0028085
[Epoch 12; Iter   737/ 2483] train: loss: 0.0018510
[Epoch 12; Iter   767/ 2483] train: loss: 0.0019374
[Epoch 12; Iter   797/ 2483] train: loss: 0.0854606
[Epoch 12; Iter   827/ 2483] train: loss: 0.0020782
[Epoch 12; Iter   857/ 2483] train: loss: 0.0012348
[Epoch 12; Iter   887/ 2483] train: loss: 0.0013662
[Epoch 12; Iter   917/ 2483] train: loss: 0.0011015
[Epoch 12; Iter   947/ 2483] train: loss: 0.0882270
[Epoch 12; Iter   977/ 2483] train: loss: 0.0011237
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0012044
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0017019
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0017448
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0018437
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0031896
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0021049
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0478101
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0024877
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0021316
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0820603
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0022489
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0025159
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0021404
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0547591
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0024608
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0036177
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0030322
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0032930
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0637791
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0028344
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0464215
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0024742
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0043580
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0017782
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0030723
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0022986
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0560236
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0025637
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0021613
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0017818
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0019934
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0018833
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0011527
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0019823
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0012210
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0018517
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0025633
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0015019
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0035345
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0017077
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0021615
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0017208
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0012182
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0015107
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0014580
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0019119
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0019344
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0015811
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0055487
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0014985
[Epoch 12] ogbg-molmuv: 0.097435 val loss: 0.013980
[Epoch 12] ogbg-molmuv: 0.047068 test loss: 0.015543
[Epoch 13; Iter    24/ 2483] train: loss: 0.0845916
[Epoch 13; Iter    54/ 2483] train: loss: 0.0611654
[Epoch 13; Iter    84/ 2483] train: loss: 0.0030881
[Epoch 13; Iter   114/ 2483] train: loss: 0.0020085
[Epoch 13; Iter   144/ 2483] train: loss: 0.0017767
[Epoch 13; Iter   174/ 2483] train: loss: 0.0018030
[Epoch 13; Iter   204/ 2483] train: loss: 0.0022436
[Epoch 13; Iter   234/ 2483] train: loss: 0.0511318
[Epoch 13; Iter   264/ 2483] train: loss: 0.0693156
[Epoch 13; Iter   294/ 2483] train: loss: 0.0016825
[Epoch 13; Iter   324/ 2483] train: loss: 0.0017413
[Epoch 13; Iter   354/ 2483] train: loss: 0.0609596
[Epoch 13; Iter   384/ 2483] train: loss: 0.0022795
[Epoch 13; Iter   414/ 2483] train: loss: 0.0543982
[Epoch 13; Iter   444/ 2483] train: loss: 0.0018739
[Epoch 13; Iter   474/ 2483] train: loss: 0.0018140
[Epoch 13; Iter   504/ 2483] train: loss: 0.0015048
[Epoch 13; Iter   534/ 2483] train: loss: 0.0014349
[Epoch 13; Iter   564/ 2483] train: loss: 0.0022867
[Epoch 13; Iter   594/ 2483] train: loss: 0.0013734
[Epoch 13; Iter   624/ 2483] train: loss: 0.0015742
[Epoch 13; Iter   654/ 2483] train: loss: 0.0022790
[Epoch 13; Iter   684/ 2483] train: loss: 0.0016455
[Epoch 13; Iter   714/ 2483] train: loss: 0.0014874
[Epoch 13; Iter   744/ 2483] train: loss: 0.0009841
[Epoch 13; Iter   774/ 2483] train: loss: 0.0010623
[Epoch 13; Iter   804/ 2483] train: loss: 0.0010043
[Epoch 13; Iter   834/ 2483] train: loss: 0.0035888
[Epoch 13; Iter   864/ 2483] train: loss: 0.0018803
[Epoch 13; Iter   894/ 2483] train: loss: 0.0011046
[Epoch 13; Iter   924/ 2483] train: loss: 0.0015969
[Epoch 13; Iter   954/ 2483] train: loss: 0.0018448
[Epoch 13; Iter   984/ 2483] train: loss: 0.0624750
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0017988
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0013177
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0010656
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0013559
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0011363
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0585376
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0024456
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0013097
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0698433
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0019912
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0057436
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0023047
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0612326
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0013767
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0894841
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0015365
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0013528
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0019306
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0017312
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0036228
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0022567
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0031399
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0034656
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0011783
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0013844
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0630703
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0018940
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0016611
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0012534
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0042862
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0015189
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0016968
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0024143
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0013177
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0014751
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0026700
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0018681
[Epoch 11] ogbg-molmuv: 0.139211 val loss: 0.013276
[Epoch 11] ogbg-molmuv: 0.089151 test loss: 0.015448
[Epoch 12; Iter    17/ 2483] train: loss: 0.0017266
[Epoch 12; Iter    47/ 2483] train: loss: 0.0012919
[Epoch 12; Iter    77/ 2483] train: loss: 0.0012826
[Epoch 12; Iter   107/ 2483] train: loss: 0.0014165
[Epoch 12; Iter   137/ 2483] train: loss: 0.0011011
[Epoch 12; Iter   167/ 2483] train: loss: 0.0013587
[Epoch 12; Iter   197/ 2483] train: loss: 0.0014396
[Epoch 12; Iter   227/ 2483] train: loss: 0.0014701
[Epoch 12; Iter   257/ 2483] train: loss: 0.0017426
[Epoch 12; Iter   287/ 2483] train: loss: 0.0019388
[Epoch 12; Iter   317/ 2483] train: loss: 0.0014198
[Epoch 12; Iter   347/ 2483] train: loss: 0.0024693
[Epoch 12; Iter   377/ 2483] train: loss: 0.0010712
[Epoch 12; Iter   407/ 2483] train: loss: 0.0011986
[Epoch 12; Iter   437/ 2483] train: loss: 0.0013508
[Epoch 12; Iter   467/ 2483] train: loss: 0.0025458
[Epoch 12; Iter   497/ 2483] train: loss: 0.0014970
[Epoch 12; Iter   527/ 2483] train: loss: 0.0015340
[Epoch 12; Iter   557/ 2483] train: loss: 0.0020778
[Epoch 12; Iter   587/ 2483] train: loss: 0.0019354
[Epoch 12; Iter   617/ 2483] train: loss: 0.0016563
[Epoch 12; Iter   647/ 2483] train: loss: 0.0025628
[Epoch 12; Iter   677/ 2483] train: loss: 0.0015132
[Epoch 12; Iter   707/ 2483] train: loss: 0.0021982
[Epoch 12; Iter   737/ 2483] train: loss: 0.0013840
[Epoch 12; Iter   767/ 2483] train: loss: 0.0012956
[Epoch 12; Iter   797/ 2483] train: loss: 0.0022269
[Epoch 12; Iter   827/ 2483] train: loss: 0.0011676
[Epoch 12; Iter   857/ 2483] train: loss: 0.0012958
[Epoch 12; Iter   887/ 2483] train: loss: 0.0012403
[Epoch 12; Iter   917/ 2483] train: loss: 0.0016333
[Epoch 12; Iter   947/ 2483] train: loss: 0.0014570
[Epoch 12; Iter   977/ 2483] train: loss: 0.0010732
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0763936
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0017676
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0018010
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0014978
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0027985
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0014054
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0019394
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0020674
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0019195
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0029323
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0010464
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0011609
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0012625
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0012416
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0018446
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0015231
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0013497
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0019948
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0028093
[Epoch 12; Iter  1577/ 2483] train: loss: 0.1816726
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0031240
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0014946
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0013519
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0016757
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0010971
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0020187
[Epoch 12; Iter  1787/ 2483] train: loss: 0.1804276
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0015747
[Epoch 12; Iter  1847/ 2483] train: loss: 0.1214303
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0022146
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0318998
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0016656
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0014952
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0019483
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0741920
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0019113
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0021449
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0014332
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0931010
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0020078
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0027506
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0790517
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0022161
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0024879
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0014942
[Epoch 12; Iter  2357/ 2483] train: loss: 0.1116550
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0738035
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0022978
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0024375
[Epoch 12; Iter  2477/ 2483] train: loss: 0.1227453
[Epoch 12] ogbg-molmuv: 0.132230 val loss: 0.013916
[Epoch 12] ogbg-molmuv: 0.099147 test loss: 0.015480
[Epoch 13; Iter    24/ 2483] train: loss: 0.0573857
[Epoch 13; Iter    54/ 2483] train: loss: 0.0025773
[Epoch 13; Iter    84/ 2483] train: loss: 0.0022162
[Epoch 13; Iter   114/ 2483] train: loss: 0.0884213
[Epoch 13; Iter   144/ 2483] train: loss: 0.0030695
[Epoch 13; Iter   174/ 2483] train: loss: 0.0019988
[Epoch 13; Iter   204/ 2483] train: loss: 0.0182011
[Epoch 13; Iter   234/ 2483] train: loss: 0.0032130
[Epoch 13; Iter   264/ 2483] train: loss: 0.0038276
[Epoch 13; Iter   294/ 2483] train: loss: 0.0027606
[Epoch 13; Iter   324/ 2483] train: loss: 0.0033429
[Epoch 13; Iter   354/ 2483] train: loss: 0.0025392
[Epoch 13; Iter   384/ 2483] train: loss: 0.0020150
[Epoch 13; Iter   414/ 2483] train: loss: 0.0880563
[Epoch 13; Iter   444/ 2483] train: loss: 0.0021663
[Epoch 13; Iter   474/ 2483] train: loss: 0.0795285
[Epoch 13; Iter   504/ 2483] train: loss: 0.0016494
[Epoch 13; Iter   534/ 2483] train: loss: 0.0749758
[Epoch 13; Iter   564/ 2483] train: loss: 0.0022314
[Epoch 13; Iter   594/ 2483] train: loss: 0.0014251
[Epoch 13; Iter   624/ 2483] train: loss: 0.0015713
[Epoch 13; Iter   654/ 2483] train: loss: 0.0019917
[Epoch 13; Iter   684/ 2483] train: loss: 0.0019971
[Epoch 13; Iter   714/ 2483] train: loss: 0.0009596
[Epoch 13; Iter   744/ 2483] train: loss: 0.0016886
[Epoch 13; Iter   774/ 2483] train: loss: 0.0016406
[Epoch 13; Iter   804/ 2483] train: loss: 0.0024001
[Epoch 13; Iter   834/ 2483] train: loss: 0.0038139
[Epoch 13; Iter   864/ 2483] train: loss: 0.0011315
[Epoch 13; Iter   894/ 2483] train: loss: 0.0011451
[Epoch 13; Iter   924/ 2483] train: loss: 0.0010450
[Epoch 13; Iter   954/ 2483] train: loss: 0.0011109
[Epoch 13; Iter   984/ 2483] train: loss: 0.0007427
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0011030
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0014139
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0007850
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0013497
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0712855
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0023714
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0016982
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0022144
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0013532
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0014240
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0010655
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0010468
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0010576
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0020668
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0019278
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0014901
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0015451
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0022975
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0017958
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0417689
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0017940
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0015640
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0742435
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0010381
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0011461
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0016867
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0019747
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0013846
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0519054
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0032971
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0019464
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0012053
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0009070
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0014957
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0015482
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0988086
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0013298
[Epoch 11] ogbg-molmuv: 0.130276 val loss: 0.014105
[Epoch 11] ogbg-molmuv: 0.086021 test loss: 0.015102
[Epoch 12; Iter    17/ 2483] train: loss: 0.0934104
[Epoch 12; Iter    47/ 2483] train: loss: 0.0014723
[Epoch 12; Iter    77/ 2483] train: loss: 0.0017110
[Epoch 12; Iter   107/ 2483] train: loss: 0.0026184
[Epoch 12; Iter   137/ 2483] train: loss: 0.0884342
[Epoch 12; Iter   167/ 2483] train: loss: 0.0029120
[Epoch 12; Iter   197/ 2483] train: loss: 0.0023982
[Epoch 12; Iter   227/ 2483] train: loss: 0.0016700
[Epoch 12; Iter   257/ 2483] train: loss: 0.0016824
[Epoch 12; Iter   287/ 2483] train: loss: 0.0015321
[Epoch 12; Iter   317/ 2483] train: loss: 0.0017255
[Epoch 12; Iter   347/ 2483] train: loss: 0.0015316
[Epoch 12; Iter   377/ 2483] train: loss: 0.0018206
[Epoch 12; Iter   407/ 2483] train: loss: 0.0015418
[Epoch 12; Iter   437/ 2483] train: loss: 0.0014745
[Epoch 12; Iter   467/ 2483] train: loss: 0.0011960
[Epoch 12; Iter   497/ 2483] train: loss: 0.0014741
[Epoch 12; Iter   527/ 2483] train: loss: 0.0016417
[Epoch 12; Iter   557/ 2483] train: loss: 0.0014712
[Epoch 12; Iter   587/ 2483] train: loss: 0.0020198
[Epoch 12; Iter   617/ 2483] train: loss: 0.0645557
[Epoch 12; Iter   647/ 2483] train: loss: 0.0636380
[Epoch 12; Iter   677/ 2483] train: loss: 0.0020338
[Epoch 12; Iter   707/ 2483] train: loss: 0.0012373
[Epoch 12; Iter   737/ 2483] train: loss: 0.0017093
[Epoch 12; Iter   767/ 2483] train: loss: 0.0022484
[Epoch 12; Iter   797/ 2483] train: loss: 0.0719326
[Epoch 12; Iter   827/ 2483] train: loss: 0.0014086
[Epoch 12; Iter   857/ 2483] train: loss: 0.0012927
[Epoch 12; Iter   887/ 2483] train: loss: 0.0015180
[Epoch 12; Iter   917/ 2483] train: loss: 0.0016190
[Epoch 12; Iter   947/ 2483] train: loss: 0.0024283
[Epoch 12; Iter   977/ 2483] train: loss: 0.0442083
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0837359
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0015620
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0023142
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0012052
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0015125
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0035898
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0068278
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0014951
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0017094
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0015496
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0021216
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0015015
[Epoch 12; Iter  1367/ 2483] train: loss: 0.1064495
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0012861
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0010550
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0016542
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0015171
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0015896
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0011191
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0075358
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0015114
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0008313
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0016462
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0047532
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0017966
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0021994
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0013272
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0013656
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0018670
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0019478
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0033161
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0014597
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0771086
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0017453
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0573919
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0032406
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0011405
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0020110
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0012559
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0016576
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0013610
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0720273
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0840828
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0643539
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0016390
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0015459
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0013132
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0028174
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0019560
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0013040
[Epoch 12] ogbg-molmuv: 0.110764 val loss: 0.014237
[Epoch 12] ogbg-molmuv: 0.082536 test loss: 0.015034
[Epoch 13; Iter    24/ 2483] train: loss: 0.0017587
[Epoch 13; Iter    54/ 2483] train: loss: 0.0012677
[Epoch 13; Iter    84/ 2483] train: loss: 0.0009953
[Epoch 13; Iter   114/ 2483] train: loss: 0.0012462
[Epoch 13; Iter   144/ 2483] train: loss: 0.0017638
[Epoch 13; Iter   174/ 2483] train: loss: 0.0014044
[Epoch 13; Iter   204/ 2483] train: loss: 0.0013312
[Epoch 13; Iter   234/ 2483] train: loss: 0.0018297
[Epoch 13; Iter   264/ 2483] train: loss: 0.0010176
[Epoch 13; Iter   294/ 2483] train: loss: 0.0010499
[Epoch 13; Iter   324/ 2483] train: loss: 0.0020986
[Epoch 13; Iter   354/ 2483] train: loss: 0.0010981
[Epoch 13; Iter   384/ 2483] train: loss: 0.0671048
[Epoch 13; Iter   414/ 2483] train: loss: 0.0016889
[Epoch 13; Iter   444/ 2483] train: loss: 0.0018001
[Epoch 13; Iter   474/ 2483] train: loss: 0.0009647
[Epoch 13; Iter   504/ 2483] train: loss: 0.0011935
[Epoch 13; Iter   534/ 2483] train: loss: 0.0009910
[Epoch 13; Iter   564/ 2483] train: loss: 0.1000209
[Epoch 13; Iter   594/ 2483] train: loss: 0.0012512
[Epoch 13; Iter   624/ 2483] train: loss: 0.0013302
[Epoch 13; Iter   654/ 2483] train: loss: 0.0016506
[Epoch 13; Iter   684/ 2483] train: loss: 0.0015484
[Epoch 13; Iter   714/ 2483] train: loss: 0.0791711
[Epoch 13; Iter   744/ 2483] train: loss: 0.0009304
[Epoch 13; Iter   774/ 2483] train: loss: 0.0019677
[Epoch 13; Iter   804/ 2483] train: loss: 0.0024748
[Epoch 13; Iter   834/ 2483] train: loss: 0.0016990
[Epoch 13; Iter   864/ 2483] train: loss: 0.0011233
[Epoch 13; Iter   894/ 2483] train: loss: 0.0014024
[Epoch 13; Iter   924/ 2483] train: loss: 0.0014470
[Epoch 13; Iter   954/ 2483] train: loss: 0.0014378
[Epoch 13; Iter   984/ 2483] train: loss: 0.0020341
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0023230
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0014875
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0008884
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0828998
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0751851
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0029553
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0018566
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0039717
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0029291
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0011892
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0017287
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0015254
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0016090
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0023783
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0019844
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0019495
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0017804
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0024247
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0027147
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0505684
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0019251
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0018794
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0801108
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0013367
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0012983
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0012168
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0015197
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0016328
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0604774
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0021084
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0021287
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0017026
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0016684
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0017254
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0018383
[Epoch 11; Iter  2440/ 2483] train: loss: 0.1083951
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0022271
[Epoch 11] ogbg-molmuv: 0.022886 val loss: 0.015702
[Epoch 11] ogbg-molmuv: 0.041978 test loss: 0.021016
[Epoch 12; Iter    17/ 2483] train: loss: 0.0948904
[Epoch 12; Iter    47/ 2483] train: loss: 0.0021075
[Epoch 12; Iter    77/ 2483] train: loss: 0.0019789
[Epoch 12; Iter   107/ 2483] train: loss: 0.0024979
[Epoch 12; Iter   137/ 2483] train: loss: 0.0882879
[Epoch 12; Iter   167/ 2483] train: loss: 0.0032690
[Epoch 12; Iter   197/ 2483] train: loss: 0.0023055
[Epoch 12; Iter   227/ 2483] train: loss: 0.0017850
[Epoch 12; Iter   257/ 2483] train: loss: 0.0018405
[Epoch 12; Iter   287/ 2483] train: loss: 0.0017854
[Epoch 12; Iter   317/ 2483] train: loss: 0.0017886
[Epoch 12; Iter   347/ 2483] train: loss: 0.0020027
[Epoch 12; Iter   377/ 2483] train: loss: 0.0021568
[Epoch 12; Iter   407/ 2483] train: loss: 0.0017966
[Epoch 12; Iter   437/ 2483] train: loss: 0.0018434
[Epoch 12; Iter   467/ 2483] train: loss: 0.0017690
[Epoch 12; Iter   497/ 2483] train: loss: 0.0013254
[Epoch 12; Iter   527/ 2483] train: loss: 0.0022306
[Epoch 12; Iter   557/ 2483] train: loss: 0.0016347
[Epoch 12; Iter   587/ 2483] train: loss: 0.0025618
[Epoch 12; Iter   617/ 2483] train: loss: 0.0641667
[Epoch 12; Iter   647/ 2483] train: loss: 0.0586588
[Epoch 12; Iter   677/ 2483] train: loss: 0.0038205
[Epoch 12; Iter   707/ 2483] train: loss: 0.0013126
[Epoch 12; Iter   737/ 2483] train: loss: 0.0016888
[Epoch 12; Iter   767/ 2483] train: loss: 0.0020615
[Epoch 12; Iter   797/ 2483] train: loss: 0.0539033
[Epoch 12; Iter   827/ 2483] train: loss: 0.0015713
[Epoch 12; Iter   857/ 2483] train: loss: 0.0012817
[Epoch 12; Iter   887/ 2483] train: loss: 0.0021654
[Epoch 12; Iter   917/ 2483] train: loss: 0.0017280
[Epoch 12; Iter   947/ 2483] train: loss: 0.0019422
[Epoch 12; Iter   977/ 2483] train: loss: 0.0663218
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0971688
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0014452
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0020469
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0019356
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0017508
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0013550
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0020127
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0025741
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0021700
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0026952
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0018504
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0019517
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0975051
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0013164
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0015552
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0017072
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0017941
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0016300
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0012992
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0014563
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0014225
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0013247
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0017166
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0022369
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0021912
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0019946
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0020133
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0012925
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0020728
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0028972
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0015410
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0013063
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0862650
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0017221
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0604618
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0019267
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0020958
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0014923
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0014803
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0020729
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0017618
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0871031
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0915499
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0747351
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0017109
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0014453
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0014964
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0020448
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0022935
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0014896
[Epoch 12] ogbg-molmuv: 0.022233 val loss: 0.014787
[Epoch 12] ogbg-molmuv: 0.042817 test loss: 0.015581
[Epoch 13; Iter    24/ 2483] train: loss: 0.0018089
[Epoch 13; Iter    54/ 2483] train: loss: 0.0014066
[Epoch 13; Iter    84/ 2483] train: loss: 0.0013540
[Epoch 13; Iter   114/ 2483] train: loss: 0.0012882
[Epoch 13; Iter   144/ 2483] train: loss: 0.0019066
[Epoch 13; Iter   174/ 2483] train: loss: 0.0017245
[Epoch 13; Iter   204/ 2483] train: loss: 0.0016764
[Epoch 13; Iter   234/ 2483] train: loss: 0.0017241
[Epoch 13; Iter   264/ 2483] train: loss: 0.0012847
[Epoch 13; Iter   294/ 2483] train: loss: 0.0015955
[Epoch 13; Iter   324/ 2483] train: loss: 0.0015075
[Epoch 13; Iter   354/ 2483] train: loss: 0.0013455
[Epoch 13; Iter   384/ 2483] train: loss: 0.0853741
[Epoch 13; Iter   414/ 2483] train: loss: 0.0020748
[Epoch 13; Iter   444/ 2483] train: loss: 0.0018001
[Epoch 13; Iter   474/ 2483] train: loss: 0.0015216
[Epoch 13; Iter   504/ 2483] train: loss: 0.0012432
[Epoch 13; Iter   534/ 2483] train: loss: 0.0013631
[Epoch 13; Iter   564/ 2483] train: loss: 0.0790426
[Epoch 13; Iter   594/ 2483] train: loss: 0.0016042
[Epoch 13; Iter   624/ 2483] train: loss: 0.0012056
[Epoch 13; Iter   654/ 2483] train: loss: 0.0020533
[Epoch 13; Iter   684/ 2483] train: loss: 0.0010800
[Epoch 13; Iter   714/ 2483] train: loss: 0.0936089
[Epoch 13; Iter   744/ 2483] train: loss: 0.0013793
[Epoch 13; Iter   774/ 2483] train: loss: 0.0020890
[Epoch 13; Iter   804/ 2483] train: loss: 0.0020625
[Epoch 13; Iter   834/ 2483] train: loss: 0.0020952
[Epoch 13; Iter   864/ 2483] train: loss: 0.0011342
[Epoch 13; Iter   894/ 2483] train: loss: 0.0015301
[Epoch 13; Iter   924/ 2483] train: loss: 0.0015211
[Epoch 13; Iter   954/ 2483] train: loss: 0.0020180
[Epoch 13; Iter   984/ 2483] train: loss: 0.0024411
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0030120
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0015620
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0010750
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0710728
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0641384
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0032991
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0025527
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0015695
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0027941
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0016903
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0018560
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0016842
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0017699
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0017331
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0959762
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0013380
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0019934
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0020744
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0018350
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0025216
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0043469
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0030594
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0028656
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0016385
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0018716
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0509881
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0019706
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0020649
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0017867
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0029974
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0015907
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0025420
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0026825
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0013536
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0018819
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0020255
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0018477
[Epoch 11] ogbg-molmuv: 0.034530 val loss: 0.024273
[Epoch 11] ogbg-molmuv: 0.026131 test loss: 0.049433
[Epoch 12; Iter    17/ 2483] train: loss: 0.0017002
[Epoch 12; Iter    47/ 2483] train: loss: 0.0014278
[Epoch 12; Iter    77/ 2483] train: loss: 0.0011390
[Epoch 12; Iter   107/ 2483] train: loss: 0.0018016
[Epoch 12; Iter   137/ 2483] train: loss: 0.0013447
[Epoch 12; Iter   167/ 2483] train: loss: 0.0012302
[Epoch 12; Iter   197/ 2483] train: loss: 0.0017324
[Epoch 12; Iter   227/ 2483] train: loss: 0.0016065
[Epoch 12; Iter   257/ 2483] train: loss: 0.0013363
[Epoch 12; Iter   287/ 2483] train: loss: 0.0018686
[Epoch 12; Iter   317/ 2483] train: loss: 0.0020322
[Epoch 12; Iter   347/ 2483] train: loss: 0.0013653
[Epoch 12; Iter   377/ 2483] train: loss: 0.0016796
[Epoch 12; Iter   407/ 2483] train: loss: 0.0013154
[Epoch 12; Iter   437/ 2483] train: loss: 0.0014576
[Epoch 12; Iter   467/ 2483] train: loss: 0.0019524
[Epoch 12; Iter   497/ 2483] train: loss: 0.0011543
[Epoch 12; Iter   527/ 2483] train: loss: 0.0018637
[Epoch 12; Iter   557/ 2483] train: loss: 0.0019959
[Epoch 12; Iter   587/ 2483] train: loss: 0.0025954
[Epoch 12; Iter   617/ 2483] train: loss: 0.0016882
[Epoch 12; Iter   647/ 2483] train: loss: 0.0022966
[Epoch 12; Iter   677/ 2483] train: loss: 0.0018857
[Epoch 12; Iter   707/ 2483] train: loss: 0.0023119
[Epoch 12; Iter   737/ 2483] train: loss: 0.0015465
[Epoch 12; Iter   767/ 2483] train: loss: 0.0014122
[Epoch 12; Iter   797/ 2483] train: loss: 0.0017537
[Epoch 12; Iter   827/ 2483] train: loss: 0.0023343
[Epoch 12; Iter   857/ 2483] train: loss: 0.0022616
[Epoch 12; Iter   887/ 2483] train: loss: 0.0017906
[Epoch 12; Iter   917/ 2483] train: loss: 0.0023026
[Epoch 12; Iter   947/ 2483] train: loss: 0.0020600
[Epoch 12; Iter   977/ 2483] train: loss: 0.0019268
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0883847
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0017164
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0016030
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0027429
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0022402
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0021196
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0014466
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0015444
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0015901
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0024814
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0018236
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0016147
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0013990
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0014512
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0018201
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0018854
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0016217
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0020694
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0026685
[Epoch 12; Iter  1577/ 2483] train: loss: 0.1883746
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0040149
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0018295
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0013468
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0013772
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0013168
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0020981
[Epoch 12; Iter  1787/ 2483] train: loss: 0.3031745
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0018655
[Epoch 12; Iter  1847/ 2483] train: loss: 0.1406378
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0029477
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0590719
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0019697
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0024460
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0015744
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0710936
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0015798
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0016033
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0017418
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0898915
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0019530
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0021480
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0755646
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0021720
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0025868
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0017937
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0971459
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0615571
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0024236
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0029729
[Epoch 12; Iter  2477/ 2483] train: loss: 0.1314437
[Epoch 12] ogbg-molmuv: 0.031956 val loss: 0.018297
[Epoch 12] ogbg-molmuv: 0.024872 test loss: 0.048841
[Epoch 13; Iter    24/ 2483] train: loss: 0.0579927
[Epoch 13; Iter    54/ 2483] train: loss: 0.0020563
[Epoch 13; Iter    84/ 2483] train: loss: 0.0027399
[Epoch 13; Iter   114/ 2483] train: loss: 0.1229188
[Epoch 13; Iter   144/ 2483] train: loss: 0.0028973
[Epoch 13; Iter   174/ 2483] train: loss: 0.0028987
[Epoch 13; Iter   204/ 2483] train: loss: 0.0035583
[Epoch 13; Iter   234/ 2483] train: loss: 0.0041005
[Epoch 13; Iter   264/ 2483] train: loss: 0.0030084
[Epoch 13; Iter   294/ 2483] train: loss: 0.0029538
[Epoch 13; Iter   324/ 2483] train: loss: 0.0037866
[Epoch 13; Iter   354/ 2483] train: loss: 0.0024735
[Epoch 13; Iter   384/ 2483] train: loss: 0.0019190
[Epoch 13; Iter   414/ 2483] train: loss: 0.0797678
[Epoch 13; Iter   444/ 2483] train: loss: 0.0020021
[Epoch 13; Iter   474/ 2483] train: loss: 0.0865371
[Epoch 13; Iter   504/ 2483] train: loss: 0.0025176
[Epoch 13; Iter   534/ 2483] train: loss: 0.0706478
[Epoch 13; Iter   564/ 2483] train: loss: 0.0023816
[Epoch 13; Iter   594/ 2483] train: loss: 0.0019387
[Epoch 13; Iter   624/ 2483] train: loss: 0.0027997
[Epoch 13; Iter   654/ 2483] train: loss: 0.0029724
[Epoch 13; Iter   684/ 2483] train: loss: 0.0021598
[Epoch 13; Iter   714/ 2483] train: loss: 0.0014307
[Epoch 13; Iter   744/ 2483] train: loss: 0.0014185
[Epoch 13; Iter   774/ 2483] train: loss: 0.0015381
[Epoch 13; Iter   804/ 2483] train: loss: 0.0014629
[Epoch 13; Iter   834/ 2483] train: loss: 0.0024525
[Epoch 13; Iter   864/ 2483] train: loss: 0.0014052
[Epoch 13; Iter   894/ 2483] train: loss: 0.0011275
[Epoch 13; Iter   924/ 2483] train: loss: 0.0012800
[Epoch 13; Iter   954/ 2483] train: loss: 0.0012606
[Epoch 13; Iter   984/ 2483] train: loss: 0.0011905
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0012313
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0011856
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0015097
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0012736
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0749231
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0017824
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0019519
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0020292
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0013574
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0018022
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0016671
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0014050
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0009147
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0023738
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0020722
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0820007
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0027907
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0036704
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0043464
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0035734
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0803648
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0031167
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0015459
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0018971
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0014956
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0016861
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0015823
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0028486
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0017455
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0020379
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0723183
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0024957
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0015169
[Epoch 11; Iter  2380/ 2483] train: loss: 0.1539731
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0026274
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0020949
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0018648
[Epoch 11] ogbg-molmuv: 0.040147 val loss: 0.021898
[Epoch 11] ogbg-molmuv: 0.016995 test loss: 0.036141
[Epoch 12; Iter    17/ 2483] train: loss: 0.0021339
[Epoch 12; Iter    47/ 2483] train: loss: 0.0016681
[Epoch 12; Iter    77/ 2483] train: loss: 0.0018212
[Epoch 12; Iter   107/ 2483] train: loss: 0.0020132
[Epoch 12; Iter   137/ 2483] train: loss: 0.0013699
[Epoch 12; Iter   167/ 2483] train: loss: 0.0015446
[Epoch 12; Iter   197/ 2483] train: loss: 0.0021385
[Epoch 12; Iter   227/ 2483] train: loss: 0.0018311
[Epoch 12; Iter   257/ 2483] train: loss: 0.0845267
[Epoch 12; Iter   287/ 2483] train: loss: 0.0011745
[Epoch 12; Iter   317/ 2483] train: loss: 0.0016696
[Epoch 12; Iter   347/ 2483] train: loss: 0.0014096
[Epoch 12; Iter   377/ 2483] train: loss: 0.0020437
[Epoch 12; Iter   407/ 2483] train: loss: 0.0017388
[Epoch 12; Iter   437/ 2483] train: loss: 0.0018012
[Epoch 12; Iter   467/ 2483] train: loss: 0.0016679
[Epoch 12; Iter   497/ 2483] train: loss: 0.0016536
[Epoch 12; Iter   527/ 2483] train: loss: 0.0020301
[Epoch 12; Iter   557/ 2483] train: loss: 0.0013482
[Epoch 12; Iter   587/ 2483] train: loss: 0.0017609
[Epoch 12; Iter   617/ 2483] train: loss: 0.0014343
[Epoch 12; Iter   647/ 2483] train: loss: 0.0015795
[Epoch 12; Iter   677/ 2483] train: loss: 0.0022207
[Epoch 12; Iter   707/ 2483] train: loss: 0.0026076
[Epoch 12; Iter   737/ 2483] train: loss: 0.0014044
[Epoch 12; Iter   767/ 2483] train: loss: 0.0016245
[Epoch 12; Iter   797/ 2483] train: loss: 0.0881168
[Epoch 12; Iter   827/ 2483] train: loss: 0.0023898
[Epoch 12; Iter   857/ 2483] train: loss: 0.0013489
[Epoch 12; Iter   887/ 2483] train: loss: 0.0012287
[Epoch 12; Iter   917/ 2483] train: loss: 0.0012380
[Epoch 12; Iter   947/ 2483] train: loss: 0.0881486
[Epoch 12; Iter   977/ 2483] train: loss: 0.0012808
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0014392
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0015697
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0012914
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0015691
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0024421
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0023035
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0655408
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0015239
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0015891
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0751278
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0017040
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0025126
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0025520
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0715038
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0034277
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0027506
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0022762
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0031885
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0820345
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0022068
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0726338
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0026809
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0028951
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0019008
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0026872
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0026734
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0688680
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0023592
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0028797
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0020330
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0017722
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0016466
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0012435
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0010781
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0009657
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0016905
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0016120
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0016578
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0049215
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0011947
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0016905
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0013496
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0017280
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0016198
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0013826
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0016173
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0017116
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0017205
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0014961
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0015018
[Epoch 12] ogbg-molmuv: 0.043866 val loss: 0.019178
[Epoch 12] ogbg-molmuv: 0.015920 test loss: 0.016910
[Epoch 13; Iter    24/ 2483] train: loss: 0.0934393
[Epoch 13; Iter    54/ 2483] train: loss: 0.0684662
[Epoch 13; Iter    84/ 2483] train: loss: 0.0031552
[Epoch 13; Iter   114/ 2483] train: loss: 0.0020631
[Epoch 13; Iter   144/ 2483] train: loss: 0.0018823
[Epoch 13; Iter   174/ 2483] train: loss: 0.0021070
[Epoch 13; Iter   204/ 2483] train: loss: 0.0019555
[Epoch 13; Iter   234/ 2483] train: loss: 0.0709783
[Epoch 13; Iter   264/ 2483] train: loss: 0.0752335
[Epoch 13; Iter   294/ 2483] train: loss: 0.0017894
[Epoch 13; Iter   324/ 2483] train: loss: 0.0026966
[Epoch 13; Iter   354/ 2483] train: loss: 0.0413758
[Epoch 13; Iter   384/ 2483] train: loss: 0.0030752
[Epoch 13; Iter   414/ 2483] train: loss: 0.0572238
[Epoch 13; Iter   444/ 2483] train: loss: 0.0024013
[Epoch 13; Iter   474/ 2483] train: loss: 0.0019493
[Epoch 13; Iter   504/ 2483] train: loss: 0.0014589
[Epoch 13; Iter   534/ 2483] train: loss: 0.0010069
[Epoch 13; Iter   564/ 2483] train: loss: 0.0026284
[Epoch 13; Iter   594/ 2483] train: loss: 0.0012960
[Epoch 13; Iter   624/ 2483] train: loss: 0.0017975
[Epoch 13; Iter   654/ 2483] train: loss: 0.0020368
[Epoch 13; Iter   684/ 2483] train: loss: 0.0018303
[Epoch 13; Iter   714/ 2483] train: loss: 0.0015181
[Epoch 13; Iter   744/ 2483] train: loss: 0.0010535
[Epoch 13; Iter   774/ 2483] train: loss: 0.0015980
[Epoch 13; Iter   804/ 2483] train: loss: 0.0009754
[Epoch 13; Iter   834/ 2483] train: loss: 0.0011235
[Epoch 13; Iter   864/ 2483] train: loss: 0.0013185
[Epoch 13; Iter   894/ 2483] train: loss: 0.0009347
[Epoch 13; Iter   924/ 2483] train: loss: 0.0014052
[Epoch 13; Iter   954/ 2483] train: loss: 0.0012576
[Epoch 13; Iter   984/ 2483] train: loss: 0.0619003
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0027808
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0018074
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0009935
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0018611
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0011639
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0512092
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0014862
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0014710
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0973619
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0043390
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0068016
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0021742
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0642027
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0015705
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0022402
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0680812
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0033183
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0017915
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0028149
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0025407
[Epoch 11; Iter  1990/ 2483] train: loss: 0.1379721
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0024509
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0018083
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0017706
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0013504
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0018989
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0018460
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0018582
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0022034
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0022154
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0580751
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0017453
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0014187
[Epoch 11; Iter  2380/ 2483] train: loss: 0.1347064
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0021433
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0018764
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0016037
[Epoch 11] ogbg-molmuv: 0.033404 val loss: 0.014831
[Epoch 11] ogbg-molmuv: 0.017044 test loss: 0.015856
[Epoch 12; Iter    17/ 2483] train: loss: 0.0020900
[Epoch 12; Iter    47/ 2483] train: loss: 0.0031402
[Epoch 12; Iter    77/ 2483] train: loss: 0.0014504
[Epoch 12; Iter   107/ 2483] train: loss: 0.0013879
[Epoch 12; Iter   137/ 2483] train: loss: 0.0013300
[Epoch 12; Iter   167/ 2483] train: loss: 0.0024877
[Epoch 12; Iter   197/ 2483] train: loss: 0.0019397
[Epoch 12; Iter   227/ 2483] train: loss: 0.0016938
[Epoch 12; Iter   257/ 2483] train: loss: 0.0708715
[Epoch 12; Iter   287/ 2483] train: loss: 0.0014546
[Epoch 12; Iter   317/ 2483] train: loss: 0.0022640
[Epoch 12; Iter   347/ 2483] train: loss: 0.0020036
[Epoch 12; Iter   377/ 2483] train: loss: 0.0015649
[Epoch 12; Iter   407/ 2483] train: loss: 0.0019066
[Epoch 12; Iter   437/ 2483] train: loss: 0.0017466
[Epoch 12; Iter   467/ 2483] train: loss: 0.0019267
[Epoch 12; Iter   497/ 2483] train: loss: 0.0026437
[Epoch 12; Iter   527/ 2483] train: loss: 0.0016670
[Epoch 12; Iter   557/ 2483] train: loss: 0.0012763
[Epoch 12; Iter   587/ 2483] train: loss: 0.0019211
[Epoch 12; Iter   617/ 2483] train: loss: 0.0015056
[Epoch 12; Iter   647/ 2483] train: loss: 0.0018545
[Epoch 12; Iter   677/ 2483] train: loss: 0.0011585
[Epoch 12; Iter   707/ 2483] train: loss: 0.0018999
[Epoch 12; Iter   737/ 2483] train: loss: 0.0014137
[Epoch 12; Iter   767/ 2483] train: loss: 0.0018120
[Epoch 12; Iter   797/ 2483] train: loss: 0.0736618
[Epoch 12; Iter   827/ 2483] train: loss: 0.0019275
[Epoch 12; Iter   857/ 2483] train: loss: 0.0014734
[Epoch 12; Iter   887/ 2483] train: loss: 0.0014226
[Epoch 12; Iter   917/ 2483] train: loss: 0.0011223
[Epoch 12; Iter   947/ 2483] train: loss: 0.0834228
[Epoch 12; Iter   977/ 2483] train: loss: 0.0012360
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0011057
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0020974
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0017780
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0022098
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0026291
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0015980
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0572010
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0013832
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0021483
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0837191
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0020467
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0024605
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0013002
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0711662
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0022003
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0030098
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0020988
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0023180
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0703114
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0019384
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0698985
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0030329
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0032016
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0024258
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0027744
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0034368
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0651045
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0025851
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0024545
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0019394
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0012488
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0014434
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0017039
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0010096
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0013061
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0015492
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0021093
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0015287
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0019151
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0019564
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0017134
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0018846
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0014384
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0017129
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0015371
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0017318
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0020664
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0013927
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0016853
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0015283
[Epoch 12] ogbg-molmuv: 0.028323 val loss: 0.015333
[Epoch 12] ogbg-molmuv: 0.025215 test loss: 0.016589
[Epoch 13; Iter    24/ 2483] train: loss: 0.0918190
[Epoch 13; Iter    54/ 2483] train: loss: 0.0698734
[Epoch 13; Iter    84/ 2483] train: loss: 0.0056743
[Epoch 13; Iter   114/ 2483] train: loss: 0.0015829
[Epoch 13; Iter   144/ 2483] train: loss: 0.0023207
[Epoch 13; Iter   174/ 2483] train: loss: 0.0022351
[Epoch 13; Iter   204/ 2483] train: loss: 0.0026722
[Epoch 13; Iter   234/ 2483] train: loss: 0.0622310
[Epoch 13; Iter   264/ 2483] train: loss: 0.0646188
[Epoch 13; Iter   294/ 2483] train: loss: 0.0017167
[Epoch 13; Iter   324/ 2483] train: loss: 0.0017269
[Epoch 13; Iter   354/ 2483] train: loss: 0.0641798
[Epoch 13; Iter   384/ 2483] train: loss: 0.0025792
[Epoch 13; Iter   414/ 2483] train: loss: 0.0606958
[Epoch 13; Iter   444/ 2483] train: loss: 0.0018640
[Epoch 13; Iter   474/ 2483] train: loss: 0.0019358
[Epoch 13; Iter   504/ 2483] train: loss: 0.0015716
[Epoch 13; Iter   534/ 2483] train: loss: 0.0013472
[Epoch 13; Iter   564/ 2483] train: loss: 0.0020271
[Epoch 13; Iter   594/ 2483] train: loss: 0.0020462
[Epoch 13; Iter   624/ 2483] train: loss: 0.0030806
[Epoch 13; Iter   654/ 2483] train: loss: 0.0026482
[Epoch 13; Iter   684/ 2483] train: loss: 0.0021798
[Epoch 13; Iter   714/ 2483] train: loss: 0.0013895
[Epoch 13; Iter   744/ 2483] train: loss: 0.0012636
[Epoch 13; Iter   774/ 2483] train: loss: 0.0012724
[Epoch 13; Iter   804/ 2483] train: loss: 0.0011812
[Epoch 13; Iter   834/ 2483] train: loss: 0.0017621
[Epoch 13; Iter   864/ 2483] train: loss: 0.0033698
[Epoch 13; Iter   894/ 2483] train: loss: 0.0010805
[Epoch 13; Iter   924/ 2483] train: loss: 0.0014607
[Epoch 13; Iter   954/ 2483] train: loss: 0.0022250
[Epoch 13; Iter   984/ 2483] train: loss: 0.0447225
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0016661
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0011413
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0010922
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0012150
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0012031
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0636362
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0015215
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0014411
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0678859
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0020808
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0107409
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0019866
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0888566
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0022918
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0024462
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0018405
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0020033
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0022888
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0027439
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0695393
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0015618
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0019381
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0743736
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0015021
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0013315
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0017279
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0015383
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0016325
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0551062
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0027823
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0020256
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0014071
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0016500
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0015595
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0023007
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0866085
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0022031
[Epoch 11] ogbg-molmuv: 0.050063 val loss: 0.014929
[Epoch 11] ogbg-molmuv: 0.026359 test loss: 0.015991
[Epoch 12; Iter    17/ 2483] train: loss: 0.0992710
[Epoch 12; Iter    47/ 2483] train: loss: 0.0021396
[Epoch 12; Iter    77/ 2483] train: loss: 0.0023344
[Epoch 12; Iter   107/ 2483] train: loss: 0.0026529
[Epoch 12; Iter   137/ 2483] train: loss: 0.0920201
[Epoch 12; Iter   167/ 2483] train: loss: 0.0032355
[Epoch 12; Iter   197/ 2483] train: loss: 0.0019245
[Epoch 12; Iter   227/ 2483] train: loss: 0.0017648
[Epoch 12; Iter   257/ 2483] train: loss: 0.0018953
[Epoch 12; Iter   287/ 2483] train: loss: 0.0028371
[Epoch 12; Iter   317/ 2483] train: loss: 0.0015068
[Epoch 12; Iter   347/ 2483] train: loss: 0.0017346
[Epoch 12; Iter   377/ 2483] train: loss: 0.0024989
[Epoch 12; Iter   407/ 2483] train: loss: 0.0017717
[Epoch 12; Iter   437/ 2483] train: loss: 0.0017196
[Epoch 12; Iter   467/ 2483] train: loss: 0.0015398
[Epoch 12; Iter   497/ 2483] train: loss: 0.0011924
[Epoch 12; Iter   527/ 2483] train: loss: 0.0018216
[Epoch 12; Iter   557/ 2483] train: loss: 0.0015927
[Epoch 12; Iter   587/ 2483] train: loss: 0.0021621
[Epoch 12; Iter   617/ 2483] train: loss: 0.0713672
[Epoch 12; Iter   647/ 2483] train: loss: 0.0768584
[Epoch 12; Iter   677/ 2483] train: loss: 0.0057379
[Epoch 12; Iter   707/ 2483] train: loss: 0.0015777
[Epoch 12; Iter   737/ 2483] train: loss: 0.0013052
[Epoch 12; Iter   767/ 2483] train: loss: 0.0017865
[Epoch 12; Iter   797/ 2483] train: loss: 0.0582304
[Epoch 12; Iter   827/ 2483] train: loss: 0.0015692
[Epoch 12; Iter   857/ 2483] train: loss: 0.0012052
[Epoch 12; Iter   887/ 2483] train: loss: 0.0019853
[Epoch 12; Iter   917/ 2483] train: loss: 0.0030104
[Epoch 12; Iter   947/ 2483] train: loss: 0.0017722
[Epoch 12; Iter   977/ 2483] train: loss: 0.0789182
[Epoch 12; Iter  1007/ 2483] train: loss: 0.1008369
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0014111
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0015448
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0018230
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0015420
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0013678
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0022143
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0022131
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0026403
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0022519
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0015600
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0027655
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0989096
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0013397
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0014709
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0018349
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0016084
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0020038
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0014419
[Epoch 12; Iter  1577/ 2483] train: loss: 0.0015693
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0016618
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0014007
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0029498
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0021639
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0017429
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0029874
[Epoch 12; Iter  1787/ 2483] train: loss: 0.0019299
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0018643
[Epoch 12; Iter  1847/ 2483] train: loss: 0.0022830
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0029827
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0018657
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0015062
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0792436
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0017783
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0698238
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0012699
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0019214
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0017413
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0015084
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0023502
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0019129
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0770341
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0966449
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0752867
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0019941
[Epoch 12; Iter  2357/ 2483] train: loss: 0.0014999
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0017611
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0018429
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0022551
[Epoch 12; Iter  2477/ 2483] train: loss: 0.0017237
[Epoch 12] ogbg-molmuv: 0.039058 val loss: 0.015094
[Epoch 12] ogbg-molmuv: 0.025345 test loss: 0.016072
[Epoch 13; Iter    24/ 2483] train: loss: 0.0018689
[Epoch 13; Iter    54/ 2483] train: loss: 0.0019503
[Epoch 13; Iter    84/ 2483] train: loss: 0.0013506
[Epoch 13; Iter   114/ 2483] train: loss: 0.0013918
[Epoch 13; Iter   144/ 2483] train: loss: 0.0016771
[Epoch 13; Iter   174/ 2483] train: loss: 0.0015509
[Epoch 13; Iter   204/ 2483] train: loss: 0.0015867
[Epoch 13; Iter   234/ 2483] train: loss: 0.0015797
[Epoch 13; Iter   264/ 2483] train: loss: 0.0013474
[Epoch 13; Iter   294/ 2483] train: loss: 0.0014260
[Epoch 13; Iter   324/ 2483] train: loss: 0.0015370
[Epoch 13; Iter   354/ 2483] train: loss: 0.0016265
[Epoch 13; Iter   384/ 2483] train: loss: 0.0933681
[Epoch 13; Iter   414/ 2483] train: loss: 0.0024093
[Epoch 13; Iter   444/ 2483] train: loss: 0.0019000
[Epoch 13; Iter   474/ 2483] train: loss: 0.0014310
[Epoch 13; Iter   504/ 2483] train: loss: 0.0013919
[Epoch 13; Iter   534/ 2483] train: loss: 0.0015122
[Epoch 13; Iter   564/ 2483] train: loss: 0.0841191
[Epoch 13; Iter   594/ 2483] train: loss: 0.0021011
[Epoch 13; Iter   624/ 2483] train: loss: 0.0014306
[Epoch 13; Iter   654/ 2483] train: loss: 0.0018522
[Epoch 13; Iter   684/ 2483] train: loss: 0.0015242
[Epoch 13; Iter   714/ 2483] train: loss: 0.0967376
[Epoch 13; Iter   744/ 2483] train: loss: 0.0016344
[Epoch 13; Iter   774/ 2483] train: loss: 0.0027529
[Epoch 13; Iter   804/ 2483] train: loss: 0.0025741
[Epoch 13; Iter   834/ 2483] train: loss: 0.0022489
[Epoch 13; Iter   864/ 2483] train: loss: 0.0016201
[Epoch 13; Iter   894/ 2483] train: loss: 0.0016666
[Epoch 13; Iter   924/ 2483] train: loss: 0.0015538
[Epoch 13; Iter   954/ 2483] train: loss: 0.0019920
[Epoch 13; Iter   984/ 2483] train: loss: 0.0028763
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0017680
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0014242
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0014315
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0687455
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0387063
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0030675
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0025773
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0022260
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0024700
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0019396
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0020730
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0018127
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0010946
[Epoch 11; Iter  1780/ 2483] train: loss: 0.0012663
[Epoch 11; Iter  1810/ 2483] train: loss: 0.0959497
[Epoch 11; Iter  1840/ 2483] train: loss: 0.0015342
[Epoch 11; Iter  1870/ 2483] train: loss: 0.0019189
[Epoch 11; Iter  1900/ 2483] train: loss: 0.0020076
[Epoch 11; Iter  1930/ 2483] train: loss: 0.0016966
[Epoch 11; Iter  1960/ 2483] train: loss: 0.0030831
[Epoch 11; Iter  1990/ 2483] train: loss: 0.0050841
[Epoch 11; Iter  2020/ 2483] train: loss: 0.0039009
[Epoch 11; Iter  2050/ 2483] train: loss: 0.0023581
[Epoch 11; Iter  2080/ 2483] train: loss: 0.0018360
[Epoch 11; Iter  2110/ 2483] train: loss: 0.0017346
[Epoch 11; Iter  2140/ 2483] train: loss: 0.0597138
[Epoch 11; Iter  2170/ 2483] train: loss: 0.0017763
[Epoch 11; Iter  2200/ 2483] train: loss: 0.0021290
[Epoch 11; Iter  2230/ 2483] train: loss: 0.0016887
[Epoch 11; Iter  2260/ 2483] train: loss: 0.0023499
[Epoch 11; Iter  2290/ 2483] train: loss: 0.0020057
[Epoch 11; Iter  2320/ 2483] train: loss: 0.0020767
[Epoch 11; Iter  2350/ 2483] train: loss: 0.0023616
[Epoch 11; Iter  2380/ 2483] train: loss: 0.0016372
[Epoch 11; Iter  2410/ 2483] train: loss: 0.0021664
[Epoch 11; Iter  2440/ 2483] train: loss: 0.0017359
[Epoch 11; Iter  2470/ 2483] train: loss: 0.0016637
[Epoch 11] ogbg-molmuv: 0.016887 val loss: 0.031659
[Epoch 11] ogbg-molmuv: 0.037635 test loss: 0.042893
[Epoch 12; Iter    17/ 2483] train: loss: 0.0018659
[Epoch 12; Iter    47/ 2483] train: loss: 0.0020151
[Epoch 12; Iter    77/ 2483] train: loss: 0.0011860
[Epoch 12; Iter   107/ 2483] train: loss: 0.0016347
[Epoch 12; Iter   137/ 2483] train: loss: 0.0015606
[Epoch 12; Iter   167/ 2483] train: loss: 0.0011246
[Epoch 12; Iter   197/ 2483] train: loss: 0.0013409
[Epoch 12; Iter   227/ 2483] train: loss: 0.0016393
[Epoch 12; Iter   257/ 2483] train: loss: 0.0013063
[Epoch 12; Iter   287/ 2483] train: loss: 0.0019325
[Epoch 12; Iter   317/ 2483] train: loss: 0.0015524
[Epoch 12; Iter   347/ 2483] train: loss: 0.0017702
[Epoch 12; Iter   377/ 2483] train: loss: 0.0013657
[Epoch 12; Iter   407/ 2483] train: loss: 0.0015627
[Epoch 12; Iter   437/ 2483] train: loss: 0.0019940
[Epoch 12; Iter   467/ 2483] train: loss: 0.0016605
[Epoch 12; Iter   497/ 2483] train: loss: 0.0011518
[Epoch 12; Iter   527/ 2483] train: loss: 0.0016089
[Epoch 12; Iter   557/ 2483] train: loss: 0.0017679
[Epoch 12; Iter   587/ 2483] train: loss: 0.0028712
[Epoch 12; Iter   617/ 2483] train: loss: 0.0018624
[Epoch 12; Iter   647/ 2483] train: loss: 0.0030786
[Epoch 12; Iter   677/ 2483] train: loss: 0.0019333
[Epoch 12; Iter   707/ 2483] train: loss: 0.0014746
[Epoch 12; Iter   737/ 2483] train: loss: 0.0018511
[Epoch 12; Iter   767/ 2483] train: loss: 0.0016175
[Epoch 12; Iter   797/ 2483] train: loss: 0.0022695
[Epoch 12; Iter   827/ 2483] train: loss: 0.0015040
[Epoch 12; Iter   857/ 2483] train: loss: 0.0015334
[Epoch 12; Iter   887/ 2483] train: loss: 0.0015866
[Epoch 12; Iter   917/ 2483] train: loss: 0.0024109
[Epoch 12; Iter   947/ 2483] train: loss: 0.0019454
[Epoch 12; Iter   977/ 2483] train: loss: 0.0015507
[Epoch 12; Iter  1007/ 2483] train: loss: 0.0836952
[Epoch 12; Iter  1037/ 2483] train: loss: 0.0015175
[Epoch 12; Iter  1067/ 2483] train: loss: 0.0016295
[Epoch 12; Iter  1097/ 2483] train: loss: 0.0016642
[Epoch 12; Iter  1127/ 2483] train: loss: 0.0023002
[Epoch 12; Iter  1157/ 2483] train: loss: 0.0016527
[Epoch 12; Iter  1187/ 2483] train: loss: 0.0019307
[Epoch 12; Iter  1217/ 2483] train: loss: 0.0024867
[Epoch 12; Iter  1247/ 2483] train: loss: 0.0022874
[Epoch 12; Iter  1277/ 2483] train: loss: 0.0017407
[Epoch 12; Iter  1307/ 2483] train: loss: 0.0019617
[Epoch 12; Iter  1337/ 2483] train: loss: 0.0017902
[Epoch 12; Iter  1367/ 2483] train: loss: 0.0012054
[Epoch 12; Iter  1397/ 2483] train: loss: 0.0012685
[Epoch 12; Iter  1427/ 2483] train: loss: 0.0017690
[Epoch 12; Iter  1457/ 2483] train: loss: 0.0015833
[Epoch 12; Iter  1487/ 2483] train: loss: 0.0014540
[Epoch 12; Iter  1517/ 2483] train: loss: 0.0023620
[Epoch 12; Iter  1547/ 2483] train: loss: 0.0018900
[Epoch 12; Iter  1577/ 2483] train: loss: 0.1996170
[Epoch 12; Iter  1607/ 2483] train: loss: 0.0025158
[Epoch 12; Iter  1637/ 2483] train: loss: 0.0019465
[Epoch 12; Iter  1667/ 2483] train: loss: 0.0015199
[Epoch 12; Iter  1697/ 2483] train: loss: 0.0014409
[Epoch 12; Iter  1727/ 2483] train: loss: 0.0013003
[Epoch 12; Iter  1757/ 2483] train: loss: 0.0022885
[Epoch 12; Iter  1787/ 2483] train: loss: 0.2090663
[Epoch 12; Iter  1817/ 2483] train: loss: 0.0025177
[Epoch 12; Iter  1847/ 2483] train: loss: 0.1293714
[Epoch 12; Iter  1877/ 2483] train: loss: 0.0027693
[Epoch 12; Iter  1907/ 2483] train: loss: 0.0551453
[Epoch 12; Iter  1937/ 2483] train: loss: 0.0015980
[Epoch 12; Iter  1967/ 2483] train: loss: 0.0012321
[Epoch 12; Iter  1997/ 2483] train: loss: 0.0013868
[Epoch 12; Iter  2027/ 2483] train: loss: 0.0905626
[Epoch 12; Iter  2057/ 2483] train: loss: 0.0015090
[Epoch 12; Iter  2087/ 2483] train: loss: 0.0022666
[Epoch 12; Iter  2117/ 2483] train: loss: 0.0018351
[Epoch 12; Iter  2147/ 2483] train: loss: 0.0837752
[Epoch 12; Iter  2177/ 2483] train: loss: 0.0022037
[Epoch 12; Iter  2207/ 2483] train: loss: 0.0021559
[Epoch 12; Iter  2237/ 2483] train: loss: 0.0652687
[Epoch 12; Iter  2267/ 2483] train: loss: 0.0025826
[Epoch 12; Iter  2297/ 2483] train: loss: 0.0019828
[Epoch 12; Iter  2327/ 2483] train: loss: 0.0019797
[Epoch 12; Iter  2357/ 2483] train: loss: 0.1040451
[Epoch 12; Iter  2387/ 2483] train: loss: 0.0555555
[Epoch 12; Iter  2417/ 2483] train: loss: 0.0019283
[Epoch 12; Iter  2447/ 2483] train: loss: 0.0024920
[Epoch 12; Iter  2477/ 2483] train: loss: 0.1478422
[Epoch 12] ogbg-molmuv: 0.016900 val loss: 0.023508
[Epoch 12] ogbg-molmuv: 0.017445 test loss: 0.027614
[Epoch 13; Iter    24/ 2483] train: loss: 0.0624757
[Epoch 13; Iter    54/ 2483] train: loss: 0.0023569
[Epoch 13; Iter    84/ 2483] train: loss: 0.0020828
[Epoch 13; Iter   114/ 2483] train: loss: 0.1073386
[Epoch 13; Iter   144/ 2483] train: loss: 0.0042862
[Epoch 13; Iter   174/ 2483] train: loss: 0.0029335
[Epoch 13; Iter   204/ 2483] train: loss: 0.0026994
[Epoch 13; Iter   234/ 2483] train: loss: 0.0045986
[Epoch 13; Iter   264/ 2483] train: loss: 0.0030164
[Epoch 13; Iter   294/ 2483] train: loss: 0.0027857
[Epoch 13; Iter   324/ 2483] train: loss: 0.0021108
[Epoch 13; Iter   354/ 2483] train: loss: 0.0019457
[Epoch 13; Iter   384/ 2483] train: loss: 0.0017076
[Epoch 13; Iter   414/ 2483] train: loss: 0.0721849
[Epoch 13; Iter   444/ 2483] train: loss: 0.0027517
[Epoch 13; Iter   474/ 2483] train: loss: 0.0803519
[Epoch 13; Iter   504/ 2483] train: loss: 0.0020064
[Epoch 13; Iter   534/ 2483] train: loss: 0.0818385
[Epoch 13; Iter   564/ 2483] train: loss: 0.0023651
[Epoch 13; Iter   594/ 2483] train: loss: 0.0029786
[Epoch 13; Iter   624/ 2483] train: loss: 0.0020829
[Epoch 13; Iter   654/ 2483] train: loss: 0.0023523
[Epoch 13; Iter   684/ 2483] train: loss: 0.0021689
[Epoch 13; Iter   714/ 2483] train: loss: 0.0016269
[Epoch 13; Iter   744/ 2483] train: loss: 0.0016933
[Epoch 13; Iter   774/ 2483] train: loss: 0.0017924
[Epoch 13; Iter   804/ 2483] train: loss: 0.0015300
[Epoch 13; Iter   834/ 2483] train: loss: 0.0025595
[Epoch 13; Iter   864/ 2483] train: loss: 0.0016589
[Epoch 13; Iter   894/ 2483] train: loss: 0.0010426
[Epoch 13; Iter   924/ 2483] train: loss: 0.0015695
[Epoch 13; Iter   954/ 2483] train: loss: 0.0014903
[Epoch 13; Iter   984/ 2483] train: loss: 0.0010770
[Epoch 13; Iter  1014/ 2483] train: loss: 0.0011600
[Epoch 13; Iter  1044/ 2483] train: loss: 0.0019133
[Epoch 13; Iter  1074/ 2483] train: loss: 0.0012708
[Epoch 13; Iter  1104/ 2483] train: loss: 0.0015929
[Epoch 13; Iter  1134/ 2483] train: loss: 0.0855117
[Epoch 13; Iter  1164/ 2483] train: loss: 0.0026701
[Epoch 13; Iter  1194/ 2483] train: loss: 0.0027781
[Epoch 13; Iter  1224/ 2483] train: loss: 0.0022382
[Epoch 13; Iter  1254/ 2483] train: loss: 0.0016739
[Epoch 13; Iter  1284/ 2483] train: loss: 0.0016633
[Epoch 13; Iter  1314/ 2483] train: loss: 0.0013230
[Epoch 13; Iter  1344/ 2483] train: loss: 0.0023210
[Epoch 13; Iter  1374/ 2483] train: loss: 0.0009818
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0015857
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0010931
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0015178
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0011298
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0012237
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0012524
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0759663
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0011680
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0014903
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0021669
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0017099
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0014440
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0018523
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0019558
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0013974
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0017274
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0013603
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0014160
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0013616
[Epoch 13; Iter  1974/ 2483] train: loss: 0.1470250
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0565535
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0016609
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0020825
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0020988
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0021441
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0752992
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0019415
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0024794
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0016959
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0023089
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0020644
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0025611
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0017510
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0479620
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0017699
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0012592
[Epoch 13] ogbg-molmuv: 0.037334 val loss: 0.014982
[Epoch 13] ogbg-molmuv: 0.027938 test loss: 0.015543
[Epoch 14; Iter     1/ 2483] train: loss: 0.0022446
[Epoch 14; Iter    31/ 2483] train: loss: 0.0019180
[Epoch 14; Iter    61/ 2483] train: loss: 0.0022622
[Epoch 14; Iter    91/ 2483] train: loss: 0.0013795
[Epoch 14; Iter   121/ 2483] train: loss: 0.0015719
[Epoch 14; Iter   151/ 2483] train: loss: 0.0028090
[Epoch 14; Iter   181/ 2483] train: loss: 0.0012575
[Epoch 14; Iter   211/ 2483] train: loss: 0.0017910
[Epoch 14; Iter   241/ 2483] train: loss: 0.0015762
[Epoch 14; Iter   271/ 2483] train: loss: 0.1130459
[Epoch 14; Iter   301/ 2483] train: loss: 0.0026460
[Epoch 14; Iter   331/ 2483] train: loss: 0.0017719
[Epoch 14; Iter   361/ 2483] train: loss: 0.0012834
[Epoch 14; Iter   391/ 2483] train: loss: 0.0017377
[Epoch 14; Iter   421/ 2483] train: loss: 0.0015322
[Epoch 14; Iter   451/ 2483] train: loss: 0.0017686
[Epoch 14; Iter   481/ 2483] train: loss: 0.0010160
[Epoch 14; Iter   511/ 2483] train: loss: 0.0013326
[Epoch 14; Iter   541/ 2483] train: loss: 0.0010308
[Epoch 14; Iter   571/ 2483] train: loss: 0.0015129
[Epoch 14; Iter   601/ 2483] train: loss: 0.0013865
[Epoch 14; Iter   631/ 2483] train: loss: 0.0703052
[Epoch 14; Iter   661/ 2483] train: loss: 0.0010429
[Epoch 14; Iter   691/ 2483] train: loss: 0.0016362
[Epoch 14; Iter   721/ 2483] train: loss: 0.0032574
[Epoch 14; Iter   751/ 2483] train: loss: 0.0017525
[Epoch 14; Iter   781/ 2483] train: loss: 0.0015242
[Epoch 14; Iter   811/ 2483] train: loss: 0.0026272
[Epoch 14; Iter   841/ 2483] train: loss: 0.0026167
[Epoch 14; Iter   871/ 2483] train: loss: 0.0030419
[Epoch 14; Iter   901/ 2483] train: loss: 0.0018671
[Epoch 14; Iter   931/ 2483] train: loss: 0.0035974
[Epoch 14; Iter   961/ 2483] train: loss: 0.0017454
[Epoch 14; Iter   991/ 2483] train: loss: 0.0018365
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0023441
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0015656
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0020009
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0015129
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0011460
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0018655
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0015645
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0017175
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0018801
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0017058
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0015472
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0018756
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0019742
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0021082
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0024191
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0593330
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0015365
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0024477
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0021465
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0021509
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0014260
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0011233
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0012857
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0018319
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0015876
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0014153
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0013103
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0013702
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0013026
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0022955
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0037600
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0763649
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0567833
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0021627
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0223522
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0021488
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0018310
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0016120
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0014500
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0442348
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0024810
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0020232
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0025080
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0025770
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0021743
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0017456
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0020491
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0626805
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0711035
[Epoch 14] ogbg-molmuv: 0.061586 val loss: 0.014426
[Epoch 14] ogbg-molmuv: 0.024962 test loss: 0.015112
[Epoch 15; Iter     8/ 2483] train: loss: 0.0018651
[Epoch 15; Iter    38/ 2483] train: loss: 0.0034655
[Epoch 15; Iter    68/ 2483] train: loss: 0.0017316
[Epoch 15; Iter    98/ 2483] train: loss: 0.0023321
[Epoch 15; Iter   128/ 2483] train: loss: 0.0419722
[Epoch 15; Iter   158/ 2483] train: loss: 0.0034600
[Epoch 15; Iter   188/ 2483] train: loss: 0.0896169
[Epoch 15; Iter   218/ 2483] train: loss: 0.0021959
[Epoch 15; Iter   248/ 2483] train: loss: 0.0018841
[Epoch 15; Iter   278/ 2483] train: loss: 0.0025219
[Epoch 15; Iter   308/ 2483] train: loss: 0.0018188
[Epoch 15; Iter   338/ 2483] train: loss: 0.0017352
[Epoch 15; Iter   368/ 2483] train: loss: 0.0014969
[Epoch 15; Iter   398/ 2483] train: loss: 0.0016615
[Epoch 15; Iter   428/ 2483] train: loss: 0.0021065
[Epoch 15; Iter   458/ 2483] train: loss: 0.0674136
[Epoch 15; Iter   488/ 2483] train: loss: 0.0015104
[Epoch 15; Iter   518/ 2483] train: loss: 0.0017159
[Epoch 15; Iter   548/ 2483] train: loss: 0.0013846
[Epoch 15; Iter   578/ 2483] train: loss: 0.0014262
[Epoch 15; Iter   608/ 2483] train: loss: 0.0586187
[Epoch 15; Iter   638/ 2483] train: loss: 0.0895076
[Epoch 15; Iter   668/ 2483] train: loss: 0.0021412
[Epoch 15; Iter   698/ 2483] train: loss: 0.0015569
[Epoch 15; Iter   728/ 2483] train: loss: 0.0014210
[Epoch 15; Iter   758/ 2483] train: loss: 0.0017204
[Epoch 15; Iter   788/ 2483] train: loss: 0.0011407
[Epoch 15; Iter   818/ 2483] train: loss: 0.0015091
[Epoch 15; Iter   848/ 2483] train: loss: 0.0019305
[Epoch 15; Iter   878/ 2483] train: loss: 0.0019551
[Epoch 15; Iter   908/ 2483] train: loss: 0.0018849
[Epoch 15; Iter   938/ 2483] train: loss: 0.0010722
[Epoch 15; Iter   968/ 2483] train: loss: 0.0020264
[Epoch 15; Iter   998/ 2483] train: loss: 0.0014070
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0036631
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0027663
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0021291
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0021459
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0020648
[Epoch 13; Iter  1554/ 2483] train: loss: 0.1088564
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0020121
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0027446
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0016213
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0019813
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0017486
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0016412
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0014738
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0017416
[Epoch 13; Iter  1824/ 2483] train: loss: 0.1090824
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0016921
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0013106
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0016815
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0016465
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0015301
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0025585
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0022501
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0029123
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0019559
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0013173
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0015830
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0031442
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0679663
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0831931
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0019221
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0014990
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0012875
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0011019
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0015512
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0036644
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0020094
[Epoch 13] ogbg-molmuv: 0.072384 val loss: 0.014815
[Epoch 13] ogbg-molmuv: 0.050646 test loss: 0.016033
[Epoch 14; Iter     1/ 2483] train: loss: 0.0011491
[Epoch 14; Iter    31/ 2483] train: loss: 0.0010821
[Epoch 14; Iter    61/ 2483] train: loss: 0.0011857
[Epoch 14; Iter    91/ 2483] train: loss: 0.0016842
[Epoch 14; Iter   121/ 2483] train: loss: 0.1311921
[Epoch 14; Iter   151/ 2483] train: loss: 0.0016630
[Epoch 14; Iter   181/ 2483] train: loss: 0.0021651
[Epoch 14; Iter   211/ 2483] train: loss: 0.0022191
[Epoch 14; Iter   241/ 2483] train: loss: 0.0020909
[Epoch 14; Iter   271/ 2483] train: loss: 0.0013257
[Epoch 14; Iter   301/ 2483] train: loss: 0.0015171
[Epoch 14; Iter   331/ 2483] train: loss: 0.0016511
[Epoch 14; Iter   361/ 2483] train: loss: 0.0036122
[Epoch 14; Iter   391/ 2483] train: loss: 0.0012405
[Epoch 14; Iter   421/ 2483] train: loss: 0.0014989
[Epoch 14; Iter   451/ 2483] train: loss: 0.0577216
[Epoch 14; Iter   481/ 2483] train: loss: 0.0474754
[Epoch 14; Iter   511/ 2483] train: loss: 0.0728377
[Epoch 14; Iter   541/ 2483] train: loss: 0.0022179
[Epoch 14; Iter   571/ 2483] train: loss: 0.0014069
[Epoch 14; Iter   601/ 2483] train: loss: 0.0660274
[Epoch 14; Iter   631/ 2483] train: loss: 0.0021332
[Epoch 14; Iter   661/ 2483] train: loss: 0.0024616
[Epoch 14; Iter   691/ 2483] train: loss: 0.0018741
[Epoch 14; Iter   721/ 2483] train: loss: 0.0018291
[Epoch 14; Iter   751/ 2483] train: loss: 0.0018165
[Epoch 14; Iter   781/ 2483] train: loss: 0.0017320
[Epoch 14; Iter   811/ 2483] train: loss: 0.0017970
[Epoch 14; Iter   841/ 2483] train: loss: 0.0600153
[Epoch 14; Iter   871/ 2483] train: loss: 0.0823305
[Epoch 14; Iter   901/ 2483] train: loss: 0.0022803
[Epoch 14; Iter   931/ 2483] train: loss: 0.0091207
[Epoch 14; Iter   961/ 2483] train: loss: 0.0011994
[Epoch 14; Iter   991/ 2483] train: loss: 0.0014886
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0017622
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0016452
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0011324
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0015650
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0016835
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0041955
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0014082
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0018789
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0791979
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0013432
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0536695
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0019809
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0710506
[Epoch 14; Iter  1411/ 2483] train: loss: 0.1483260
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0019008
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0025581
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0017613
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0015783
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0025806
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0019595
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0033411
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0017497
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0013676
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0665078
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0016477
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0018624
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0014493
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0577894
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0016960
[Epoch 14; Iter  1891/ 2483] train: loss: 0.1202563
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0016290
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0020006
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0020894
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0014458
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0015545
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0016098
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0021196
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0023572
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0027404
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0704854
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0023676
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0883172
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0017908
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0014021
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0018065
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0017964
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0020830
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0012844
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0020595
[Epoch 14] ogbg-molmuv: 0.089437 val loss: 0.014381
[Epoch 14] ogbg-molmuv: 0.094232 test loss: 0.016476
[Epoch 15; Iter     8/ 2483] train: loss: 0.0011141
[Epoch 15; Iter    38/ 2483] train: loss: 0.0435724
[Epoch 15; Iter    68/ 2483] train: loss: 0.0020961
[Epoch 15; Iter    98/ 2483] train: loss: 0.0026090
[Epoch 15; Iter   128/ 2483] train: loss: 0.0024258
[Epoch 15; Iter   158/ 2483] train: loss: 0.0025896
[Epoch 15; Iter   188/ 2483] train: loss: 0.0015458
[Epoch 15; Iter   218/ 2483] train: loss: 0.0085062
[Epoch 15; Iter   248/ 2483] train: loss: 0.0015643
[Epoch 15; Iter   278/ 2483] train: loss: 0.0010933
[Epoch 15; Iter   308/ 2483] train: loss: 0.0017692
[Epoch 15; Iter   338/ 2483] train: loss: 0.0760599
[Epoch 15; Iter   368/ 2483] train: loss: 0.0015483
[Epoch 15; Iter   398/ 2483] train: loss: 0.0017060
[Epoch 15; Iter   428/ 2483] train: loss: 0.0015587
[Epoch 15; Iter   458/ 2483] train: loss: 0.0624539
[Epoch 15; Iter   488/ 2483] train: loss: 0.0019158
[Epoch 15; Iter   518/ 2483] train: loss: 0.1667097
[Epoch 15; Iter   548/ 2483] train: loss: 0.0023431
[Epoch 15; Iter   578/ 2483] train: loss: 0.0018876
[Epoch 15; Iter   608/ 2483] train: loss: 0.1023398
[Epoch 15; Iter   638/ 2483] train: loss: 0.0031773
[Epoch 15; Iter   668/ 2483] train: loss: 0.0732946
[Epoch 15; Iter   698/ 2483] train: loss: 0.0010241
[Epoch 15; Iter   728/ 2483] train: loss: 0.0015979
[Epoch 15; Iter   758/ 2483] train: loss: 0.0015162
[Epoch 15; Iter   788/ 2483] train: loss: 0.0016200
[Epoch 15; Iter   818/ 2483] train: loss: 0.0035015
[Epoch 15; Iter   848/ 2483] train: loss: 0.0009410
[Epoch 15; Iter   878/ 2483] train: loss: 0.0016375
[Epoch 15; Iter   908/ 2483] train: loss: 0.1047315
[Epoch 15; Iter   938/ 2483] train: loss: 0.0502497
[Epoch 15; Iter   968/ 2483] train: loss: 0.0015208
[Epoch 15; Iter   998/ 2483] train: loss: 0.0014530
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0012300
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0015224
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0011166
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0029602
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0016695
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0032755
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0030767
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0019198
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0018627
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0020089
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0039356
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0600865
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0022688
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0027902
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0013514
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0027928
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0016989
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0021512
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0717788
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0659180
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0016185
[Epoch 13; Iter  2034/ 2483] train: loss: 0.1061242
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0022767
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0015369
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0996862
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0020216
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0019267
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0021884
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0020937
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0014224
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0019999
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0016914
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0015163
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0015193
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0017711
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0017859
[Epoch 13] ogbg-molmuv: 0.038564 val loss: 0.034233
[Epoch 13] ogbg-molmuv: 0.030251 test loss: 0.049322
[Epoch 14; Iter     1/ 2483] train: loss: 0.0016022
[Epoch 14; Iter    31/ 2483] train: loss: 0.0015255
[Epoch 14; Iter    61/ 2483] train: loss: 0.0777484
[Epoch 14; Iter    91/ 2483] train: loss: 0.0021911
[Epoch 14; Iter   121/ 2483] train: loss: 0.0018447
[Epoch 14; Iter   151/ 2483] train: loss: 0.0013770
[Epoch 14; Iter   181/ 2483] train: loss: 0.0017231
[Epoch 14; Iter   211/ 2483] train: loss: 0.0016817
[Epoch 14; Iter   241/ 2483] train: loss: 0.0016997
[Epoch 14; Iter   271/ 2483] train: loss: 0.0028875
[Epoch 14; Iter   301/ 2483] train: loss: 0.0012886
[Epoch 14; Iter   331/ 2483] train: loss: 0.0028579
[Epoch 14; Iter   361/ 2483] train: loss: 0.0015655
[Epoch 14; Iter   391/ 2483] train: loss: 0.0019208
[Epoch 14; Iter   421/ 2483] train: loss: 0.0015706
[Epoch 14; Iter   451/ 2483] train: loss: 0.0638078
[Epoch 14; Iter   481/ 2483] train: loss: 0.0013800
[Epoch 14; Iter   511/ 2483] train: loss: 0.0019868
[Epoch 14; Iter   541/ 2483] train: loss: 0.0016836
[Epoch 14; Iter   571/ 2483] train: loss: 0.0397072
[Epoch 14; Iter   601/ 2483] train: loss: 0.0015813
[Epoch 14; Iter   631/ 2483] train: loss: 0.0010253
[Epoch 14; Iter   661/ 2483] train: loss: 0.0015095
[Epoch 14; Iter   691/ 2483] train: loss: 0.0012695
[Epoch 14; Iter   721/ 2483] train: loss: 0.1039241
[Epoch 14; Iter   751/ 2483] train: loss: 0.0867270
[Epoch 14; Iter   781/ 2483] train: loss: 0.0020386
[Epoch 14; Iter   811/ 2483] train: loss: 0.0027469
[Epoch 14; Iter   841/ 2483] train: loss: 0.0017618
[Epoch 14; Iter   871/ 2483] train: loss: 0.0013892
[Epoch 14; Iter   901/ 2483] train: loss: 0.0916047
[Epoch 14; Iter   931/ 2483] train: loss: 0.0017815
[Epoch 14; Iter   961/ 2483] train: loss: 0.0033978
[Epoch 14; Iter   991/ 2483] train: loss: 0.0021064
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0022354
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0016838
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0017116
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0017028
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0017211
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0013934
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0919776
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0016197
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0016627
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0018399
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0016628
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0473123
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0781777
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0021748
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0012927
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0015955
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0793456
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0073362
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0035437
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0034738
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0015674
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0020283
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0532312
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0013443
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0756938
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0016864
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0021315
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0013752
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0022170
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0033530
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0017068
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0029468
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0032061
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0011379
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0014230
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0032950
[Epoch 14; Iter  2101/ 2483] train: loss: 0.1132390
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0650730
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0015521
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0021449
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0019757
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0076943
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0014041
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0019202
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0016661
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0015285
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0014895
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0015881
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0013570
[Epoch 14] ogbg-molmuv: 0.077693 val loss: 0.014721
[Epoch 14] ogbg-molmuv: 0.026561 test loss: 0.035345
[Epoch 15; Iter     8/ 2483] train: loss: 0.0014412
[Epoch 15; Iter    38/ 2483] train: loss: 0.0015795
[Epoch 15; Iter    68/ 2483] train: loss: 0.0012497
[Epoch 15; Iter    98/ 2483] train: loss: 0.0011712
[Epoch 15; Iter   128/ 2483] train: loss: 0.0902984
[Epoch 15; Iter   158/ 2483] train: loss: 0.0010431
[Epoch 15; Iter   188/ 2483] train: loss: 0.0019163
[Epoch 15; Iter   218/ 2483] train: loss: 0.0023934
[Epoch 15; Iter   248/ 2483] train: loss: 0.0013557
[Epoch 15; Iter   278/ 2483] train: loss: 0.0013046
[Epoch 15; Iter   308/ 2483] train: loss: 0.0009526
[Epoch 15; Iter   338/ 2483] train: loss: 0.0015776
[Epoch 15; Iter   368/ 2483] train: loss: 0.0018806
[Epoch 15; Iter   398/ 2483] train: loss: 0.0026353
[Epoch 15; Iter   428/ 2483] train: loss: 0.0017777
[Epoch 15; Iter   458/ 2483] train: loss: 0.0015175
[Epoch 15; Iter   488/ 2483] train: loss: 0.0029792
[Epoch 15; Iter   518/ 2483] train: loss: 0.0021650
[Epoch 15; Iter   548/ 2483] train: loss: 0.0013327
[Epoch 15; Iter   578/ 2483] train: loss: 0.0017125
[Epoch 15; Iter   608/ 2483] train: loss: 0.0024208
[Epoch 15; Iter   638/ 2483] train: loss: 0.1048979
[Epoch 15; Iter   668/ 2483] train: loss: 0.0018434
[Epoch 15; Iter   698/ 2483] train: loss: 0.0014532
[Epoch 15; Iter   728/ 2483] train: loss: 0.0019672
[Epoch 15; Iter   758/ 2483] train: loss: 0.0018151
[Epoch 15; Iter   788/ 2483] train: loss: 0.0795756
[Epoch 15; Iter   818/ 2483] train: loss: 0.0024411
[Epoch 15; Iter   848/ 2483] train: loss: 0.0013453
[Epoch 15; Iter   878/ 2483] train: loss: 0.0010163
[Epoch 15; Iter   908/ 2483] train: loss: 0.0018737
[Epoch 15; Iter   938/ 2483] train: loss: 0.0011413
[Epoch 15; Iter   968/ 2483] train: loss: 0.0026131
[Epoch 15; Iter   998/ 2483] train: loss: 0.0023331
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0015211
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0016166
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0012966
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0014777
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0017358
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0018995
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0021297
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0017103
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0016082
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0023060
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0051547
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0589282
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0033426
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0026552
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0013277
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0029935
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0016131
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0020434
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0731558
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0588565
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0015022
[Epoch 13; Iter  2034/ 2483] train: loss: 0.1014216
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0020118
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0012741
[Epoch 13; Iter  2124/ 2483] train: loss: 0.1039824
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0030420
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0020033
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0021256
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0019153
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0012309
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0016134
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0021455
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0013326
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0013146
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0021572
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0018878
[Epoch 13] ogbg-molmuv: 0.021247 val loss: 0.015854
[Epoch 13] ogbg-molmuv: 0.033371 test loss: 0.015895
[Epoch 14; Iter     1/ 2483] train: loss: 0.0022745
[Epoch 14; Iter    31/ 2483] train: loss: 0.0018281
[Epoch 14; Iter    61/ 2483] train: loss: 0.0720123
[Epoch 14; Iter    91/ 2483] train: loss: 0.0023666
[Epoch 14; Iter   121/ 2483] train: loss: 0.0013694
[Epoch 14; Iter   151/ 2483] train: loss: 0.0012596
[Epoch 14; Iter   181/ 2483] train: loss: 0.0013601
[Epoch 14; Iter   211/ 2483] train: loss: 0.0016112
[Epoch 14; Iter   241/ 2483] train: loss: 0.0014494
[Epoch 14; Iter   271/ 2483] train: loss: 0.0011972
[Epoch 14; Iter   301/ 2483] train: loss: 0.0015882
[Epoch 14; Iter   331/ 2483] train: loss: 0.0015906
[Epoch 14; Iter   361/ 2483] train: loss: 0.0016539
[Epoch 14; Iter   391/ 2483] train: loss: 0.0019127
[Epoch 14; Iter   421/ 2483] train: loss: 0.0038496
[Epoch 14; Iter   451/ 2483] train: loss: 0.0715072
[Epoch 14; Iter   481/ 2483] train: loss: 0.0014113
[Epoch 14; Iter   511/ 2483] train: loss: 0.0017821
[Epoch 14; Iter   541/ 2483] train: loss: 0.0017143
[Epoch 14; Iter   571/ 2483] train: loss: 0.0499268
[Epoch 14; Iter   601/ 2483] train: loss: 0.0017386
[Epoch 14; Iter   631/ 2483] train: loss: 0.0013616
[Epoch 14; Iter   661/ 2483] train: loss: 0.0012952
[Epoch 14; Iter   691/ 2483] train: loss: 0.0009336
[Epoch 14; Iter   721/ 2483] train: loss: 0.1031303
[Epoch 14; Iter   751/ 2483] train: loss: 0.0670277
[Epoch 14; Iter   781/ 2483] train: loss: 0.0027295
[Epoch 14; Iter   811/ 2483] train: loss: 0.0019761
[Epoch 14; Iter   841/ 2483] train: loss: 0.0019037
[Epoch 14; Iter   871/ 2483] train: loss: 0.0014812
[Epoch 14; Iter   901/ 2483] train: loss: 0.0784005
[Epoch 14; Iter   931/ 2483] train: loss: 0.0021057
[Epoch 14; Iter   961/ 2483] train: loss: 0.0015867
[Epoch 14; Iter   991/ 2483] train: loss: 0.0022831
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0018850
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0021905
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0015623
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0016289
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0018678
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0014726
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0994487
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0012259
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0026502
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0018880
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0025612
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0475403
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0948784
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0022456
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0019662
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0018347
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0700515
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0034132
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0054916
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0027093
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0014143
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0017384
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0608669
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0013702
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0706234
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0014194
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0038354
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0016246
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0020191
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0039328
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0022380
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0037709
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0028844
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0012893
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0021311
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0035836
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0761163
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0636881
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0018220
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0019346
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0019861
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0034688
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0012804
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0022164
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0015748
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0016214
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0015304
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0016561
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0014260
[Epoch 14] ogbg-molmuv: 0.013378 val loss: 0.344184
[Epoch 14] ogbg-molmuv: 0.015391 test loss: 0.307272
[Epoch 15; Iter     8/ 2483] train: loss: 0.0017259
[Epoch 15; Iter    38/ 2483] train: loss: 0.0013961
[Epoch 15; Iter    68/ 2483] train: loss: 0.0013471
[Epoch 15; Iter    98/ 2483] train: loss: 0.0012553
[Epoch 15; Iter   128/ 2483] train: loss: 0.1081978
[Epoch 15; Iter   158/ 2483] train: loss: 0.0014641
[Epoch 15; Iter   188/ 2483] train: loss: 0.0019448
[Epoch 15; Iter   218/ 2483] train: loss: 0.0017877
[Epoch 15; Iter   248/ 2483] train: loss: 0.0017203
[Epoch 15; Iter   278/ 2483] train: loss: 0.0016386
[Epoch 15; Iter   308/ 2483] train: loss: 0.0010801
[Epoch 15; Iter   338/ 2483] train: loss: 0.0021013
[Epoch 15; Iter   368/ 2483] train: loss: 0.0020838
[Epoch 15; Iter   398/ 2483] train: loss: 0.0029302
[Epoch 15; Iter   428/ 2483] train: loss: 0.0017225
[Epoch 15; Iter   458/ 2483] train: loss: 0.0020500
[Epoch 15; Iter   488/ 2483] train: loss: 0.0022832
[Epoch 15; Iter   518/ 2483] train: loss: 0.0019202
[Epoch 15; Iter   548/ 2483] train: loss: 0.0013914
[Epoch 15; Iter   578/ 2483] train: loss: 0.0014777
[Epoch 15; Iter   608/ 2483] train: loss: 0.0024540
[Epoch 15; Iter   638/ 2483] train: loss: 0.0788123
[Epoch 15; Iter   668/ 2483] train: loss: 0.0014802
[Epoch 15; Iter   698/ 2483] train: loss: 0.0020288
[Epoch 15; Iter   728/ 2483] train: loss: 0.0014817
[Epoch 15; Iter   758/ 2483] train: loss: 0.0016785
[Epoch 15; Iter   788/ 2483] train: loss: 0.0763815
[Epoch 15; Iter   818/ 2483] train: loss: 0.0020086
[Epoch 15; Iter   848/ 2483] train: loss: 0.0023766
[Epoch 15; Iter   878/ 2483] train: loss: 0.0018486
[Epoch 15; Iter   908/ 2483] train: loss: 0.0021751
[Epoch 15; Iter   938/ 2483] train: loss: 0.0012725
[Epoch 15; Iter   968/ 2483] train: loss: 0.0025814
[Epoch 15; Iter   998/ 2483] train: loss: 0.0022033
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0013969
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0011925
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0013701
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0012000
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0020074
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0014119
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0801153
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0012239
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0013991
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0020880
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0020206
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0014617
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0017850
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0019315
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0012594
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0016112
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0017703
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0013305
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0013562
[Epoch 13; Iter  1974/ 2483] train: loss: 0.1453716
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0489889
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0013476
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0023157
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0017789
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0018765
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0800594
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0019620
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0023273
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0018218
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0021253
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0019206
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0020892
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0025731
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0381901
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0016683
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0014786
[Epoch 13] ogbg-molmuv: 0.027622 val loss: 0.032483
[Epoch 13] ogbg-molmuv: 0.043134 test loss: 0.016133
[Epoch 14; Iter     1/ 2483] train: loss: 0.0022439
[Epoch 14; Iter    31/ 2483] train: loss: 0.0022301
[Epoch 14; Iter    61/ 2483] train: loss: 0.0020114
[Epoch 14; Iter    91/ 2483] train: loss: 0.0016585
[Epoch 14; Iter   121/ 2483] train: loss: 0.0018740
[Epoch 14; Iter   151/ 2483] train: loss: 0.0026388
[Epoch 14; Iter   181/ 2483] train: loss: 0.0015697
[Epoch 14; Iter   211/ 2483] train: loss: 0.0016872
[Epoch 14; Iter   241/ 2483] train: loss: 0.0017659
[Epoch 14; Iter   271/ 2483] train: loss: 0.0941655
[Epoch 14; Iter   301/ 2483] train: loss: 0.0016438
[Epoch 14; Iter   331/ 2483] train: loss: 0.0018986
[Epoch 14; Iter   361/ 2483] train: loss: 0.0015814
[Epoch 14; Iter   391/ 2483] train: loss: 0.0024396
[Epoch 14; Iter   421/ 2483] train: loss: 0.0021903
[Epoch 14; Iter   451/ 2483] train: loss: 0.0017993
[Epoch 14; Iter   481/ 2483] train: loss: 0.0013002
[Epoch 14; Iter   511/ 2483] train: loss: 0.0012354
[Epoch 14; Iter   541/ 2483] train: loss: 0.0015302
[Epoch 14; Iter   571/ 2483] train: loss: 0.0014970
[Epoch 14; Iter   601/ 2483] train: loss: 0.0014100
[Epoch 14; Iter   631/ 2483] train: loss: 0.0622618
[Epoch 14; Iter   661/ 2483] train: loss: 0.0010837
[Epoch 14; Iter   691/ 2483] train: loss: 0.0017858
[Epoch 14; Iter   721/ 2483] train: loss: 0.0035745
[Epoch 14; Iter   751/ 2483] train: loss: 0.0019704
[Epoch 14; Iter   781/ 2483] train: loss: 0.0021855
[Epoch 14; Iter   811/ 2483] train: loss: 0.0014737
[Epoch 14; Iter   841/ 2483] train: loss: 0.0022381
[Epoch 14; Iter   871/ 2483] train: loss: 0.0021980
[Epoch 14; Iter   901/ 2483] train: loss: 0.0024146
[Epoch 14; Iter   931/ 2483] train: loss: 0.0046517
[Epoch 14; Iter   961/ 2483] train: loss: 0.0024005
[Epoch 14; Iter   991/ 2483] train: loss: 0.0015975
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0017526
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0016410
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0018931
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0016537
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0013475
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0015358
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0016629
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0037340
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0018400
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0014684
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0016081
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0018749
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0016398
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0015534
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0025619
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0691664
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0019445
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0016950
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0030103
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0020579
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0013764
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0011984
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0014619
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0016119
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0014842
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0016504
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0015829
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0022405
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0015325
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0021573
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0026174
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0729077
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0660310
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0017896
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0787417
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0027228
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0023053
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0020745
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0017149
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0572649
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0022089
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0021446
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0032118
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0018679
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0020618
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0023763
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0017465
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0666651
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0825464
[Epoch 14] ogbg-molmuv: 0.019314 val loss: 0.020298
[Epoch 14] ogbg-molmuv: 0.038827 test loss: 0.015888
[Epoch 15; Iter     8/ 2483] train: loss: 0.0017960
[Epoch 15; Iter    38/ 2483] train: loss: 0.0018821
[Epoch 15; Iter    68/ 2483] train: loss: 0.0015220
[Epoch 15; Iter    98/ 2483] train: loss: 0.0022818
[Epoch 15; Iter   128/ 2483] train: loss: 0.0519933
[Epoch 15; Iter   158/ 2483] train: loss: 0.0018534
[Epoch 15; Iter   188/ 2483] train: loss: 0.0898622
[Epoch 15; Iter   218/ 2483] train: loss: 0.0025902
[Epoch 15; Iter   248/ 2483] train: loss: 0.0023681
[Epoch 15; Iter   278/ 2483] train: loss: 0.0024658
[Epoch 15; Iter   308/ 2483] train: loss: 0.0015862
[Epoch 15; Iter   338/ 2483] train: loss: 0.0013879
[Epoch 15; Iter   368/ 2483] train: loss: 0.0014562
[Epoch 15; Iter   398/ 2483] train: loss: 0.0020000
[Epoch 15; Iter   428/ 2483] train: loss: 0.0019168
[Epoch 15; Iter   458/ 2483] train: loss: 0.0614584
[Epoch 15; Iter   488/ 2483] train: loss: 0.0020999
[Epoch 15; Iter   518/ 2483] train: loss: 0.0011504
[Epoch 15; Iter   548/ 2483] train: loss: 0.0013502
[Epoch 15; Iter   578/ 2483] train: loss: 0.0020129
[Epoch 15; Iter   608/ 2483] train: loss: 0.0544378
[Epoch 15; Iter   638/ 2483] train: loss: 0.0855774
[Epoch 15; Iter   668/ 2483] train: loss: 0.0010638
[Epoch 15; Iter   698/ 2483] train: loss: 0.0015411
[Epoch 15; Iter   728/ 2483] train: loss: 0.0016309
[Epoch 15; Iter   758/ 2483] train: loss: 0.0016926
[Epoch 15; Iter   788/ 2483] train: loss: 0.0012884
[Epoch 15; Iter   818/ 2483] train: loss: 0.0012415
[Epoch 15; Iter   848/ 2483] train: loss: 0.0021647
[Epoch 15; Iter   878/ 2483] train: loss: 0.0020733
[Epoch 15; Iter   908/ 2483] train: loss: 0.0015701
[Epoch 15; Iter   938/ 2483] train: loss: 0.0014438
[Epoch 15; Iter   968/ 2483] train: loss: 0.0019536
[Epoch 15; Iter   998/ 2483] train: loss: 0.0019815
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0039826
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0029077
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0015878
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0027279
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0018963
[Epoch 13; Iter  1554/ 2483] train: loss: 0.1199311
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0020939
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0042395
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0018372
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0013524
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0021657
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0018041
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0016075
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0015447
[Epoch 13; Iter  1824/ 2483] train: loss: 0.1200322
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0021463
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0015756
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0018113
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0017082
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0020156
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0016986
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0015423
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0020048
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0021211
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0020179
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0022681
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0028859
[Epoch 13; Iter  2214/ 2483] train: loss: 0.1100044
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0640405
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0022596
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0019023
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0017960
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0013461
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0014271
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0021432
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0016211
[Epoch 13] ogbg-molmuv: 0.023289 val loss: 0.109855
[Epoch 13] ogbg-molmuv: 0.024040 test loss: 0.039907
[Epoch 14; Iter     1/ 2483] train: loss: 0.0015794
[Epoch 14; Iter    31/ 2483] train: loss: 0.0012519
[Epoch 14; Iter    61/ 2483] train: loss: 0.0010474
[Epoch 14; Iter    91/ 2483] train: loss: 0.0016131
[Epoch 14; Iter   121/ 2483] train: loss: 0.1482356
[Epoch 14; Iter   151/ 2483] train: loss: 0.0019052
[Epoch 14; Iter   181/ 2483] train: loss: 0.0013480
[Epoch 14; Iter   211/ 2483] train: loss: 0.0020166
[Epoch 14; Iter   241/ 2483] train: loss: 0.0015262
[Epoch 14; Iter   271/ 2483] train: loss: 0.0015215
[Epoch 14; Iter   301/ 2483] train: loss: 0.0015926
[Epoch 14; Iter   331/ 2483] train: loss: 0.0015691
[Epoch 14; Iter   361/ 2483] train: loss: 0.0024241
[Epoch 14; Iter   391/ 2483] train: loss: 0.0014299
[Epoch 14; Iter   421/ 2483] train: loss: 0.0019082
[Epoch 14; Iter   451/ 2483] train: loss: 0.0389247
[Epoch 14; Iter   481/ 2483] train: loss: 0.0401141
[Epoch 14; Iter   511/ 2483] train: loss: 0.0657627
[Epoch 14; Iter   541/ 2483] train: loss: 0.0018289
[Epoch 14; Iter   571/ 2483] train: loss: 0.0013853
[Epoch 14; Iter   601/ 2483] train: loss: 0.0626781
[Epoch 14; Iter   631/ 2483] train: loss: 0.0026024
[Epoch 14; Iter   661/ 2483] train: loss: 0.0027891
[Epoch 14; Iter   691/ 2483] train: loss: 0.0079446
[Epoch 14; Iter   721/ 2483] train: loss: 0.0017075
[Epoch 14; Iter   751/ 2483] train: loss: 0.0019420
[Epoch 14; Iter   781/ 2483] train: loss: 0.0016604
[Epoch 14; Iter   811/ 2483] train: loss: 0.0018885
[Epoch 14; Iter   841/ 2483] train: loss: 0.0559780
[Epoch 14; Iter   871/ 2483] train: loss: 0.1197517
[Epoch 14; Iter   901/ 2483] train: loss: 0.0041078
[Epoch 14; Iter   931/ 2483] train: loss: 0.0026557
[Epoch 14; Iter   961/ 2483] train: loss: 0.0016517
[Epoch 14; Iter   991/ 2483] train: loss: 0.0013648
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0016559
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0013046
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0012248
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0015172
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0011203
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0016550
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0014686
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0021379
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0704237
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0022276
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0672582
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0025098
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0704900
[Epoch 14; Iter  1411/ 2483] train: loss: 0.1431860
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0020589
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0027742
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0021377
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0018841
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0018404
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0027783
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0032406
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0013512
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0017589
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0726490
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0011322
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0015242
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0018088
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0692960
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0020700
[Epoch 14; Iter  1891/ 2483] train: loss: 0.1235197
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0020715
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0015685
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0014130
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0011974
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0010856
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0013541
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0020525
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0018800
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0020398
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0594169
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0023379
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0805361
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0021800
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0025704
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0030384
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0023973
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0014063
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0039016
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0014646
[Epoch 14] ogbg-molmuv: 0.074448 val loss: 0.016435
[Epoch 14] ogbg-molmuv: 0.023018 test loss: 0.020520
[Epoch 15; Iter     8/ 2483] train: loss: 0.0014262
[Epoch 15; Iter    38/ 2483] train: loss: 0.0591076
[Epoch 15; Iter    68/ 2483] train: loss: 0.0019677
[Epoch 15; Iter    98/ 2483] train: loss: 0.0018882
[Epoch 15; Iter   128/ 2483] train: loss: 0.0021123
[Epoch 15; Iter   158/ 2483] train: loss: 0.0045091
[Epoch 15; Iter   188/ 2483] train: loss: 0.0022498
[Epoch 15; Iter   218/ 2483] train: loss: 0.0344014
[Epoch 15; Iter   248/ 2483] train: loss: 0.0025884
[Epoch 15; Iter   278/ 2483] train: loss: 0.0012356
[Epoch 15; Iter   308/ 2483] train: loss: 0.0017427
[Epoch 15; Iter   338/ 2483] train: loss: 0.0682682
[Epoch 15; Iter   368/ 2483] train: loss: 0.0017914
[Epoch 15; Iter   398/ 2483] train: loss: 0.0012484
[Epoch 15; Iter   428/ 2483] train: loss: 0.0032183
[Epoch 15; Iter   458/ 2483] train: loss: 0.0624047
[Epoch 15; Iter   488/ 2483] train: loss: 0.0016108
[Epoch 15; Iter   518/ 2483] train: loss: 0.1322075
[Epoch 15; Iter   548/ 2483] train: loss: 0.0040131
[Epoch 15; Iter   578/ 2483] train: loss: 0.0014249
[Epoch 15; Iter   608/ 2483] train: loss: 0.0941226
[Epoch 15; Iter   638/ 2483] train: loss: 0.0028921
[Epoch 15; Iter   668/ 2483] train: loss: 0.0693671
[Epoch 15; Iter   698/ 2483] train: loss: 0.0018375
[Epoch 15; Iter   728/ 2483] train: loss: 0.0017483
[Epoch 15; Iter   758/ 2483] train: loss: 0.0016890
[Epoch 15; Iter   788/ 2483] train: loss: 0.0014284
[Epoch 15; Iter   818/ 2483] train: loss: 0.0016053
[Epoch 15; Iter   848/ 2483] train: loss: 0.0016198
[Epoch 15; Iter   878/ 2483] train: loss: 0.0012230
[Epoch 15; Iter   908/ 2483] train: loss: 0.0984567
[Epoch 15; Iter   938/ 2483] train: loss: 0.0496790
[Epoch 15; Iter   968/ 2483] train: loss: 0.0018028
[Epoch 15; Iter   998/ 2483] train: loss: 0.0012988
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0011269
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0017883
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0010551
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0009507
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0011331
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0012125
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0753480
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0013725
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0018066
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0015374
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0015963
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0015392
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0017506
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0014938
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0012259
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0012094
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0018960
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0011048
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0012436
[Epoch 13; Iter  1974/ 2483] train: loss: 0.1500636
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0608382
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0012033
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0024612
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0021446
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0018978
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0805950
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0019751
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0023160
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0019448
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0017502
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0018344
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0019059
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0022849
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0374129
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0017794
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0010405
[Epoch 13] ogbg-molmuv: 0.108377 val loss: 0.044689
[Epoch 13] ogbg-molmuv: 0.085972 test loss: 0.134341
[Epoch 14; Iter     1/ 2483] train: loss: 0.0020165
[Epoch 14; Iter    31/ 2483] train: loss: 0.0014819
[Epoch 14; Iter    61/ 2483] train: loss: 0.0020001
[Epoch 14; Iter    91/ 2483] train: loss: 0.0013361
[Epoch 14; Iter   121/ 2483] train: loss: 0.0015853
[Epoch 14; Iter   151/ 2483] train: loss: 0.0027108
[Epoch 14; Iter   181/ 2483] train: loss: 0.0013335
[Epoch 14; Iter   211/ 2483] train: loss: 0.0048230
[Epoch 14; Iter   241/ 2483] train: loss: 0.0016536
[Epoch 14; Iter   271/ 2483] train: loss: 0.1185405
[Epoch 14; Iter   301/ 2483] train: loss: 0.0020484
[Epoch 14; Iter   331/ 2483] train: loss: 0.0017558
[Epoch 14; Iter   361/ 2483] train: loss: 0.0012114
[Epoch 14; Iter   391/ 2483] train: loss: 0.0020939
[Epoch 14; Iter   421/ 2483] train: loss: 0.0013834
[Epoch 14; Iter   451/ 2483] train: loss: 0.0020792
[Epoch 14; Iter   481/ 2483] train: loss: 0.0010831
[Epoch 14; Iter   511/ 2483] train: loss: 0.0013554
[Epoch 14; Iter   541/ 2483] train: loss: 0.0009616
[Epoch 14; Iter   571/ 2483] train: loss: 0.0012491
[Epoch 14; Iter   601/ 2483] train: loss: 0.0019012
[Epoch 14; Iter   631/ 2483] train: loss: 0.0553099
[Epoch 14; Iter   661/ 2483] train: loss: 0.0018040
[Epoch 14; Iter   691/ 2483] train: loss: 0.0017204
[Epoch 14; Iter   721/ 2483] train: loss: 0.0023357
[Epoch 14; Iter   751/ 2483] train: loss: 0.0016660
[Epoch 14; Iter   781/ 2483] train: loss: 0.0015295
[Epoch 14; Iter   811/ 2483] train: loss: 0.0013187
[Epoch 14; Iter   841/ 2483] train: loss: 0.0026818
[Epoch 14; Iter   871/ 2483] train: loss: 0.0023040
[Epoch 14; Iter   901/ 2483] train: loss: 0.0029421
[Epoch 14; Iter   931/ 2483] train: loss: 0.0029928
[Epoch 14; Iter   961/ 2483] train: loss: 0.0013804
[Epoch 14; Iter   991/ 2483] train: loss: 0.0018235
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0018079
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0020311
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0019185
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0015382
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0011851
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0016706
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0018290
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0015683
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0021179
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0011647
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0012552
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0016448
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0021318
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0010678
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0019438
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0733174
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0012936
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0017914
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0012173
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0015753
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0019101
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0011646
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0009254
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0016895
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0012506
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0011851
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0012460
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0016144
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0019522
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0087584
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0025807
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0726537
[Epoch 14; Iter  1981/ 2483] train: loss: 0.1006376
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0015762
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0055987
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0020989
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0029402
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0012628
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0024493
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0398982
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0026563
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0025377
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0024169
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0022059
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0024885
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0016274
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0030042
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0722973
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0645331
[Epoch 14] ogbg-molmuv: 0.117557 val loss: 0.056600
[Epoch 14] ogbg-molmuv: 0.122489 test loss: 0.014539
[Epoch 15; Iter     8/ 2483] train: loss: 0.0015630
[Epoch 15; Iter    38/ 2483] train: loss: 0.0012041
[Epoch 15; Iter    68/ 2483] train: loss: 0.0018156
[Epoch 15; Iter    98/ 2483] train: loss: 0.0016404
[Epoch 15; Iter   128/ 2483] train: loss: 0.0316255
[Epoch 15; Iter   158/ 2483] train: loss: 0.0038829
[Epoch 15; Iter   188/ 2483] train: loss: 0.0730241
[Epoch 15; Iter   218/ 2483] train: loss: 0.0018128
[Epoch 15; Iter   248/ 2483] train: loss: 0.0016509
[Epoch 15; Iter   278/ 2483] train: loss: 0.0030468
[Epoch 15; Iter   308/ 2483] train: loss: 0.0010796
[Epoch 15; Iter   338/ 2483] train: loss: 0.0014230
[Epoch 15; Iter   368/ 2483] train: loss: 0.0014222
[Epoch 15; Iter   398/ 2483] train: loss: 0.0017215
[Epoch 15; Iter   428/ 2483] train: loss: 0.0023090
[Epoch 15; Iter   458/ 2483] train: loss: 0.0652914
[Epoch 15; Iter   488/ 2483] train: loss: 0.0012487
[Epoch 15; Iter   518/ 2483] train: loss: 0.0010597
[Epoch 15; Iter   548/ 2483] train: loss: 0.0012870
[Epoch 15; Iter   578/ 2483] train: loss: 0.0024361
[Epoch 15; Iter   608/ 2483] train: loss: 0.0362650
[Epoch 15; Iter   638/ 2483] train: loss: 0.0905921
[Epoch 15; Iter   668/ 2483] train: loss: 0.0012710
[Epoch 15; Iter   698/ 2483] train: loss: 0.0019382
[Epoch 15; Iter   728/ 2483] train: loss: 0.0021079
[Epoch 15; Iter   758/ 2483] train: loss: 0.0016441
[Epoch 15; Iter   788/ 2483] train: loss: 0.0017621
[Epoch 15; Iter   818/ 2483] train: loss: 0.0014236
[Epoch 15; Iter   848/ 2483] train: loss: 0.0018944
[Epoch 15; Iter   878/ 2483] train: loss: 0.0014847
[Epoch 15; Iter   908/ 2483] train: loss: 0.0016811
[Epoch 15; Iter   938/ 2483] train: loss: 0.0010813
[Epoch 15; Iter   968/ 2483] train: loss: 0.0016196
[Epoch 15; Iter   998/ 2483] train: loss: 0.0011399
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0032899
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0019782
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0020063
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0033104
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0020391
[Epoch 13; Iter  1554/ 2483] train: loss: 0.1008447
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0016589
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0030502
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0026392
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0014137
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0019545
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0019975
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0019351
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0018810
[Epoch 13; Iter  1824/ 2483] train: loss: 0.1354181
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0016755
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0012368
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0016108
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0018630
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0016691
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0024866
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0018211
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0018916
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0033445
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0016607
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0021153
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0019397
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0792135
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0826251
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0023568
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0009860
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0015294
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0010565
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0017714
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0046256
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0009621
[Epoch 13] ogbg-molmuv: 0.111642 val loss: 0.018224
[Epoch 13] ogbg-molmuv: 0.094728 test loss: 0.037678
[Epoch 14; Iter     1/ 2483] train: loss: 0.0010462
[Epoch 14; Iter    31/ 2483] train: loss: 0.0007893
[Epoch 14; Iter    61/ 2483] train: loss: 0.0013242
[Epoch 14; Iter    91/ 2483] train: loss: 0.0015165
[Epoch 14; Iter   121/ 2483] train: loss: 0.0986343
[Epoch 14; Iter   151/ 2483] train: loss: 0.0016260
[Epoch 14; Iter   181/ 2483] train: loss: 0.0025223
[Epoch 14; Iter   211/ 2483] train: loss: 0.0011295
[Epoch 14; Iter   241/ 2483] train: loss: 0.0022736
[Epoch 14; Iter   271/ 2483] train: loss: 0.0013181
[Epoch 14; Iter   301/ 2483] train: loss: 0.0015590
[Epoch 14; Iter   331/ 2483] train: loss: 0.0015729
[Epoch 14; Iter   361/ 2483] train: loss: 0.0073642
[Epoch 14; Iter   391/ 2483] train: loss: 0.0016153
[Epoch 14; Iter   421/ 2483] train: loss: 0.0015542
[Epoch 14; Iter   451/ 2483] train: loss: 0.0598754
[Epoch 14; Iter   481/ 2483] train: loss: 0.0287004
[Epoch 14; Iter   511/ 2483] train: loss: 0.0783871
[Epoch 14; Iter   541/ 2483] train: loss: 0.0017394
[Epoch 14; Iter   571/ 2483] train: loss: 0.0013950
[Epoch 14; Iter   601/ 2483] train: loss: 0.0737004
[Epoch 14; Iter   631/ 2483] train: loss: 0.0023322
[Epoch 14; Iter   661/ 2483] train: loss: 0.0020974
[Epoch 14; Iter   691/ 2483] train: loss: 0.0018379
[Epoch 14; Iter   721/ 2483] train: loss: 0.0014629
[Epoch 14; Iter   751/ 2483] train: loss: 0.0016809
[Epoch 14; Iter   781/ 2483] train: loss: 0.0016802
[Epoch 14; Iter   811/ 2483] train: loss: 0.0016289
[Epoch 14; Iter   841/ 2483] train: loss: 0.0728823
[Epoch 14; Iter   871/ 2483] train: loss: 0.0439502
[Epoch 14; Iter   901/ 2483] train: loss: 0.0028324
[Epoch 14; Iter   931/ 2483] train: loss: 0.0057941
[Epoch 14; Iter   961/ 2483] train: loss: 0.0012932
[Epoch 14; Iter   991/ 2483] train: loss: 0.0018774
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0012214
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0012284
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0013509
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0017610
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0010755
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0011663
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0017342
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0020306
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0916912
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0013720
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0548323
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0022070
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0566331
[Epoch 14; Iter  1411/ 2483] train: loss: 0.1817410
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0019240
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0023698
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0020058
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0017761
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0025437
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0019584
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0026710
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0015022
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0014547
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0718114
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0010386
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0016493
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0013434
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0552192
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0013590
[Epoch 14; Iter  1891/ 2483] train: loss: 0.1166584
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0012077
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0016923
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0021122
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0012830
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0011941
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0011766
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0019253
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0023999
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0019856
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0713699
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0026461
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0886517
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0014712
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0022034
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0018986
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0015745
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0016175
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0022202
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0029480
[Epoch 14] ogbg-molmuv: 0.142007 val loss: 0.014048
[Epoch 14] ogbg-molmuv: 0.084637 test loss: 0.034307
[Epoch 15; Iter     8/ 2483] train: loss: 0.0015563
[Epoch 15; Iter    38/ 2483] train: loss: 0.0214511
[Epoch 15; Iter    68/ 2483] train: loss: 0.0016631
[Epoch 15; Iter    98/ 2483] train: loss: 0.0022917
[Epoch 15; Iter   128/ 2483] train: loss: 0.0019123
[Epoch 15; Iter   158/ 2483] train: loss: 0.0023382
[Epoch 15; Iter   188/ 2483] train: loss: 0.0013940
[Epoch 15; Iter   218/ 2483] train: loss: 0.0035233
[Epoch 15; Iter   248/ 2483] train: loss: 0.0018629
[Epoch 15; Iter   278/ 2483] train: loss: 0.0011969
[Epoch 15; Iter   308/ 2483] train: loss: 0.0011376
[Epoch 15; Iter   338/ 2483] train: loss: 0.0623679
[Epoch 15; Iter   368/ 2483] train: loss: 0.0017178
[Epoch 15; Iter   398/ 2483] train: loss: 0.0016782
[Epoch 15; Iter   428/ 2483] train: loss: 0.0015382
[Epoch 15; Iter   458/ 2483] train: loss: 0.0688483
[Epoch 15; Iter   488/ 2483] train: loss: 0.0014319
[Epoch 15; Iter   518/ 2483] train: loss: 0.1564896
[Epoch 15; Iter   548/ 2483] train: loss: 0.0020885
[Epoch 15; Iter   578/ 2483] train: loss: 0.0013969
[Epoch 15; Iter   608/ 2483] train: loss: 0.0928070
[Epoch 15; Iter   638/ 2483] train: loss: 0.0021207
[Epoch 15; Iter   668/ 2483] train: loss: 0.0750738
[Epoch 15; Iter   698/ 2483] train: loss: 0.0010765
[Epoch 15; Iter   728/ 2483] train: loss: 0.0018028
[Epoch 15; Iter   758/ 2483] train: loss: 0.0015961
[Epoch 15; Iter   788/ 2483] train: loss: 0.0014929
[Epoch 15; Iter   818/ 2483] train: loss: 0.0016057
[Epoch 15; Iter   848/ 2483] train: loss: 0.0010308
[Epoch 15; Iter   878/ 2483] train: loss: 0.0010560
[Epoch 15; Iter   908/ 2483] train: loss: 0.0961734
[Epoch 15; Iter   938/ 2483] train: loss: 0.0427063
[Epoch 15; Iter   968/ 2483] train: loss: 0.0018385
[Epoch 15; Iter   998/ 2483] train: loss: 0.0013809
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0041931
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0015935
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0018965
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0024471
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0019307
[Epoch 13; Iter  1554/ 2483] train: loss: 0.1402408
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0018400
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0027521
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0018769
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0018815
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0022095
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0022692
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0018218
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0018471
[Epoch 13; Iter  1824/ 2483] train: loss: 0.1096143
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0020626
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0014690
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0015524
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0017190
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0017256
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0023608
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0020894
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0017486
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0018334
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0016105
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0016284
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0017103
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0670288
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0862832
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0020666
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0012075
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0019197
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0013536
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0019621
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0029777
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0022413
[Epoch 13] ogbg-molmuv: 0.015473 val loss: 0.015384
[Epoch 13] ogbg-molmuv: 0.020260 test loss: 0.016538
[Epoch 14; Iter     1/ 2483] train: loss: 0.0015494
[Epoch 14; Iter    31/ 2483] train: loss: 0.0014407
[Epoch 14; Iter    61/ 2483] train: loss: 0.0010912
[Epoch 14; Iter    91/ 2483] train: loss: 0.0011803
[Epoch 14; Iter   121/ 2483] train: loss: 0.1326591
[Epoch 14; Iter   151/ 2483] train: loss: 0.0027632
[Epoch 14; Iter   181/ 2483] train: loss: 0.0016828
[Epoch 14; Iter   211/ 2483] train: loss: 0.0017937
[Epoch 14; Iter   241/ 2483] train: loss: 0.0021932
[Epoch 14; Iter   271/ 2483] train: loss: 0.0015705
[Epoch 14; Iter   301/ 2483] train: loss: 0.0018389
[Epoch 14; Iter   331/ 2483] train: loss: 0.0020222
[Epoch 14; Iter   361/ 2483] train: loss: 0.0023799
[Epoch 14; Iter   391/ 2483] train: loss: 0.0016237
[Epoch 14; Iter   421/ 2483] train: loss: 0.0015139
[Epoch 14; Iter   451/ 2483] train: loss: 0.0521696
[Epoch 14; Iter   481/ 2483] train: loss: 0.0641625
[Epoch 14; Iter   511/ 2483] train: loss: 0.0685191
[Epoch 14; Iter   541/ 2483] train: loss: 0.0058172
[Epoch 14; Iter   571/ 2483] train: loss: 0.0046820
[Epoch 14; Iter   601/ 2483] train: loss: 0.0563004
[Epoch 14; Iter   631/ 2483] train: loss: 0.0028673
[Epoch 14; Iter   661/ 2483] train: loss: 0.0021011
[Epoch 14; Iter   691/ 2483] train: loss: 0.0014757
[Epoch 14; Iter   721/ 2483] train: loss: 0.0020124
[Epoch 14; Iter   751/ 2483] train: loss: 0.0019076
[Epoch 14; Iter   781/ 2483] train: loss: 0.0018733
[Epoch 14; Iter   811/ 2483] train: loss: 0.0020815
[Epoch 14; Iter   841/ 2483] train: loss: 0.0683873
[Epoch 14; Iter   871/ 2483] train: loss: 0.1045342
[Epoch 14; Iter   901/ 2483] train: loss: 0.0030358
[Epoch 14; Iter   931/ 2483] train: loss: 0.0024781
[Epoch 14; Iter   961/ 2483] train: loss: 0.0017425
[Epoch 14; Iter   991/ 2483] train: loss: 0.0021920
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0014798
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0017573
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0014965
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0013645
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0016650
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0013495
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0014676
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0027101
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0997011
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0018359
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0636848
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0026355
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0715463
[Epoch 14; Iter  1411/ 2483] train: loss: 0.1533478
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0023063
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0013194
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0014939
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0015159
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0024552
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0020923
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0027145
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0015369
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0017505
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0542265
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0011900
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0019007
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0011827
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0698704
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0020110
[Epoch 14; Iter  1891/ 2483] train: loss: 0.1455370
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0017037
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0024459
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0022249
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0012897
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0018665
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0017115
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0032427
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0026457
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0017584
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0815327
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0022064
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0884366
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0018311
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0019912
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0026193
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0022576
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0017599
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0015158
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0016571
[Epoch 14] ogbg-molmuv: 0.026234 val loss: 0.015391
[Epoch 14] ogbg-molmuv: 0.029229 test loss: 0.016284
[Epoch 15; Iter     8/ 2483] train: loss: 0.0013066
[Epoch 15; Iter    38/ 2483] train: loss: 0.0575920
[Epoch 15; Iter    68/ 2483] train: loss: 0.0020680
[Epoch 15; Iter    98/ 2483] train: loss: 0.0018849
[Epoch 15; Iter   128/ 2483] train: loss: 0.0021498
[Epoch 15; Iter   158/ 2483] train: loss: 0.0026841
[Epoch 15; Iter   188/ 2483] train: loss: 0.0020951
[Epoch 15; Iter   218/ 2483] train: loss: 0.0419161
[Epoch 15; Iter   248/ 2483] train: loss: 0.0015820
[Epoch 15; Iter   278/ 2483] train: loss: 0.0016941
[Epoch 15; Iter   308/ 2483] train: loss: 0.0015285
[Epoch 15; Iter   338/ 2483] train: loss: 0.0651356
[Epoch 15; Iter   368/ 2483] train: loss: 0.0017177
[Epoch 15; Iter   398/ 2483] train: loss: 0.0011683
[Epoch 15; Iter   428/ 2483] train: loss: 0.0015104
[Epoch 15; Iter   458/ 2483] train: loss: 0.0721646
[Epoch 15; Iter   488/ 2483] train: loss: 0.0019878
[Epoch 15; Iter   518/ 2483] train: loss: 0.1282158
[Epoch 15; Iter   548/ 2483] train: loss: 0.0023156
[Epoch 15; Iter   578/ 2483] train: loss: 0.0019750
[Epoch 15; Iter   608/ 2483] train: loss: 0.1073135
[Epoch 15; Iter   638/ 2483] train: loss: 0.0020630
[Epoch 15; Iter   668/ 2483] train: loss: 0.0785380
[Epoch 15; Iter   698/ 2483] train: loss: 0.0018642
[Epoch 15; Iter   728/ 2483] train: loss: 0.0021306
[Epoch 15; Iter   758/ 2483] train: loss: 0.0018589
[Epoch 15; Iter   788/ 2483] train: loss: 0.0020350
[Epoch 15; Iter   818/ 2483] train: loss: 0.0017808
[Epoch 15; Iter   848/ 2483] train: loss: 0.0009358
[Epoch 15; Iter   878/ 2483] train: loss: 0.0015176
[Epoch 15; Iter   908/ 2483] train: loss: 0.0780669
[Epoch 15; Iter   938/ 2483] train: loss: 0.0816700
[Epoch 15; Iter   968/ 2483] train: loss: 0.0015166
[Epoch 15; Iter   998/ 2483] train: loss: 0.0011821
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0014125
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0013997
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0015535
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0018418
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0022194
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0016670
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0024835
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0019401
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0012624
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0024096
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0051779
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0555694
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0021764
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0020191
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0011846
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0018804
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0015889
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0021589
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0781254
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0693052
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0011591
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0994065
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0015833
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0016691
[Epoch 13; Iter  2124/ 2483] train: loss: 0.1116442
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0026932
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0019848
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0027529
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0024752
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0014833
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0017193
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0017095
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0017990
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0016417
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0018871
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0017050
[Epoch 13] ogbg-molmuv: 0.029420 val loss: 0.016057
[Epoch 13] ogbg-molmuv: 0.014708 test loss: 0.016929
[Epoch 14; Iter     1/ 2483] train: loss: 0.0021114
[Epoch 14; Iter    31/ 2483] train: loss: 0.0019604
[Epoch 14; Iter    61/ 2483] train: loss: 0.0786417
[Epoch 14; Iter    91/ 2483] train: loss: 0.0020339
[Epoch 14; Iter   121/ 2483] train: loss: 0.0017666
[Epoch 14; Iter   151/ 2483] train: loss: 0.0015565
[Epoch 14; Iter   181/ 2483] train: loss: 0.0013289
[Epoch 14; Iter   211/ 2483] train: loss: 0.0016445
[Epoch 14; Iter   241/ 2483] train: loss: 0.0022047
[Epoch 14; Iter   271/ 2483] train: loss: 0.0018412
[Epoch 14; Iter   301/ 2483] train: loss: 0.0017075
[Epoch 14; Iter   331/ 2483] train: loss: 0.0015636
[Epoch 14; Iter   361/ 2483] train: loss: 0.0016233
[Epoch 14; Iter   391/ 2483] train: loss: 0.0019577
[Epoch 14; Iter   421/ 2483] train: loss: 0.0016996
[Epoch 14; Iter   451/ 2483] train: loss: 0.0608796
[Epoch 14; Iter   481/ 2483] train: loss: 0.0012455
[Epoch 14; Iter   511/ 2483] train: loss: 0.0017099
[Epoch 14; Iter   541/ 2483] train: loss: 0.0017368
[Epoch 14; Iter   571/ 2483] train: loss: 0.0723775
[Epoch 14; Iter   601/ 2483] train: loss: 0.0017035
[Epoch 14; Iter   631/ 2483] train: loss: 0.0012453
[Epoch 14; Iter   661/ 2483] train: loss: 0.0011191
[Epoch 14; Iter   691/ 2483] train: loss: 0.0013354
[Epoch 14; Iter   721/ 2483] train: loss: 0.0997087
[Epoch 14; Iter   751/ 2483] train: loss: 0.0610126
[Epoch 14; Iter   781/ 2483] train: loss: 0.0028740
[Epoch 14; Iter   811/ 2483] train: loss: 0.0023245
[Epoch 14; Iter   841/ 2483] train: loss: 0.0016109
[Epoch 14; Iter   871/ 2483] train: loss: 0.0014799
[Epoch 14; Iter   901/ 2483] train: loss: 0.1019932
[Epoch 14; Iter   931/ 2483] train: loss: 0.0019836
[Epoch 14; Iter   961/ 2483] train: loss: 0.0019478
[Epoch 14; Iter   991/ 2483] train: loss: 0.0021969
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0017909
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0018608
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0013821
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0021016
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0015475
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0015505
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0896285
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0020338
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0016997
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0020687
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0017451
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0776300
[Epoch 14; Iter  1381/ 2483] train: loss: 0.1004803
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0016279
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0011310
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0019400
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0790601
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0024566
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0036239
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0035825
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0016527
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0018282
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0601055
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0015539
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0757617
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0016805
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0021192
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0016949
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0028587
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0032692
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0022722
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0031921
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0023843
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0017967
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0017674
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0031473
[Epoch 14; Iter  2101/ 2483] train: loss: 0.1085267
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0738297
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0022117
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0026110
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0017992
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0024364
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0018773
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0037322
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0017182
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0012984
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0012245
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0015010
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0011526
[Epoch 14] ogbg-molmuv: 0.025880 val loss: 0.016260
[Epoch 14] ogbg-molmuv: 0.024650 test loss: 0.017049
[Epoch 15; Iter     8/ 2483] train: loss: 0.0012721
[Epoch 15; Iter    38/ 2483] train: loss: 0.0016837
[Epoch 15; Iter    68/ 2483] train: loss: 0.0013797
[Epoch 15; Iter    98/ 2483] train: loss: 0.0017151
[Epoch 15; Iter   128/ 2483] train: loss: 0.0684994
[Epoch 15; Iter   158/ 2483] train: loss: 0.0012889
[Epoch 15; Iter   188/ 2483] train: loss: 0.0022682
[Epoch 15; Iter   218/ 2483] train: loss: 0.0020674
[Epoch 15; Iter   248/ 2483] train: loss: 0.0013749
[Epoch 15; Iter   278/ 2483] train: loss: 0.0016333
[Epoch 15; Iter   308/ 2483] train: loss: 0.0011212
[Epoch 15; Iter   338/ 2483] train: loss: 0.0015419
[Epoch 15; Iter   368/ 2483] train: loss: 0.0014354
[Epoch 15; Iter   398/ 2483] train: loss: 0.0014974
[Epoch 15; Iter   428/ 2483] train: loss: 0.0017260
[Epoch 15; Iter   458/ 2483] train: loss: 0.0014458
[Epoch 15; Iter   488/ 2483] train: loss: 0.0036866
[Epoch 15; Iter   518/ 2483] train: loss: 0.0016964
[Epoch 15; Iter   548/ 2483] train: loss: 0.0017600
[Epoch 15; Iter   578/ 2483] train: loss: 0.0015298
[Epoch 15; Iter   608/ 2483] train: loss: 0.0019718
[Epoch 15; Iter   638/ 2483] train: loss: 0.0901821
[Epoch 15; Iter   668/ 2483] train: loss: 0.0016492
[Epoch 15; Iter   698/ 2483] train: loss: 0.0015755
[Epoch 15; Iter   728/ 2483] train: loss: 0.0011840
[Epoch 15; Iter   758/ 2483] train: loss: 0.0039220
[Epoch 15; Iter   788/ 2483] train: loss: 0.0742907
[Epoch 15; Iter   818/ 2483] train: loss: 0.0023962
[Epoch 15; Iter   848/ 2483] train: loss: 0.0022984
[Epoch 15; Iter   878/ 2483] train: loss: 0.0015300
[Epoch 15; Iter   908/ 2483] train: loss: 0.0024741
[Epoch 15; Iter   938/ 2483] train: loss: 0.0012280
[Epoch 15; Iter   968/ 2483] train: loss: 0.0019983
[Epoch 15; Iter   998/ 2483] train: loss: 0.0036538
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0013856
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0017331
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0008918
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0018768
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0017484
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0038329
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0023612
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0018987
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0015051
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0018983
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0032960
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0560340
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0021560
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0029320
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0011878
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0016119
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0017280
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0020477
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0585113
[Epoch 13; Iter  1974/ 2483] train: loss: 0.0611447
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0013500
[Epoch 13; Iter  2034/ 2483] train: loss: 0.1021030
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0017096
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0014277
[Epoch 13; Iter  2124/ 2483] train: loss: 0.1084000
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0024154
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0019395
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0099721
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0033920
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0010303
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0017315
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0016790
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0015894
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0010817
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0018361
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0013866
[Epoch 13] ogbg-molmuv: 0.142798 val loss: 0.014855
[Epoch 13] ogbg-molmuv: 0.060105 test loss: 0.015211
[Epoch 14; Iter     1/ 2483] train: loss: 0.0010923
[Epoch 14; Iter    31/ 2483] train: loss: 0.0012181
[Epoch 14; Iter    61/ 2483] train: loss: 0.0795177
[Epoch 14; Iter    91/ 2483] train: loss: 0.0017648
[Epoch 14; Iter   121/ 2483] train: loss: 0.0015346
[Epoch 14; Iter   151/ 2483] train: loss: 0.0009579
[Epoch 14; Iter   181/ 2483] train: loss: 0.0015731
[Epoch 14; Iter   211/ 2483] train: loss: 0.0014953
[Epoch 14; Iter   241/ 2483] train: loss: 0.0022012
[Epoch 14; Iter   271/ 2483] train: loss: 0.0011233
[Epoch 14; Iter   301/ 2483] train: loss: 0.0011316
[Epoch 14; Iter   331/ 2483] train: loss: 0.0021110
[Epoch 14; Iter   361/ 2483] train: loss: 0.0013746
[Epoch 14; Iter   391/ 2483] train: loss: 0.0027794
[Epoch 14; Iter   421/ 2483] train: loss: 0.0012112
[Epoch 14; Iter   451/ 2483] train: loss: 0.0654383
[Epoch 14; Iter   481/ 2483] train: loss: 0.0012435
[Epoch 14; Iter   511/ 2483] train: loss: 0.0017751
[Epoch 14; Iter   541/ 2483] train: loss: 0.0021594
[Epoch 14; Iter   571/ 2483] train: loss: 0.0221486
[Epoch 14; Iter   601/ 2483] train: loss: 0.0026798
[Epoch 14; Iter   631/ 2483] train: loss: 0.0011036
[Epoch 14; Iter   661/ 2483] train: loss: 0.0008816
[Epoch 14; Iter   691/ 2483] train: loss: 0.0007911
[Epoch 14; Iter   721/ 2483] train: loss: 0.1055676
[Epoch 14; Iter   751/ 2483] train: loss: 0.0757666
[Epoch 14; Iter   781/ 2483] train: loss: 0.0015924
[Epoch 14; Iter   811/ 2483] train: loss: 0.0017270
[Epoch 14; Iter   841/ 2483] train: loss: 0.0014777
[Epoch 14; Iter   871/ 2483] train: loss: 0.0014746
[Epoch 14; Iter   901/ 2483] train: loss: 0.0835704
[Epoch 14; Iter   931/ 2483] train: loss: 0.0017643
[Epoch 14; Iter   961/ 2483] train: loss: 0.0015176
[Epoch 14; Iter   991/ 2483] train: loss: 0.0017355
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0024016
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0013634
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0015125
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0019949
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0035912
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0014956
[Epoch 14; Iter  1201/ 2483] train: loss: 0.1079805
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0016791
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0028529
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0035217
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0032894
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0480845
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0893542
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0014074
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0012196
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0019748
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0658690
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0207918
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0031665
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0039932
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0025926
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0020888
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0731157
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0014802
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0713201
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0014819
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0018694
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0014166
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0017124
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0030195
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0015814
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0023480
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0039390
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0009790
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0011574
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0046117
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0777338
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0497846
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0028636
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0021800
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0018428
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0036210
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0014882
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0016962
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0015787
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0012143
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0011452
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0018577
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0014112
[Epoch 14] ogbg-molmuv: 0.152326 val loss: 0.013832
[Epoch 14] ogbg-molmuv: 0.093088 test loss: 0.015135
[Epoch 15; Iter     8/ 2483] train: loss: 0.0015212
[Epoch 15; Iter    38/ 2483] train: loss: 0.0011138
[Epoch 15; Iter    68/ 2483] train: loss: 0.0013984
[Epoch 15; Iter    98/ 2483] train: loss: 0.0013648
[Epoch 15; Iter   128/ 2483] train: loss: 0.0233930
[Epoch 15; Iter   158/ 2483] train: loss: 0.0013701
[Epoch 15; Iter   188/ 2483] train: loss: 0.0013231
[Epoch 15; Iter   218/ 2483] train: loss: 0.0055857
[Epoch 15; Iter   248/ 2483] train: loss: 0.0017937
[Epoch 15; Iter   278/ 2483] train: loss: 0.0016480
[Epoch 15; Iter   308/ 2483] train: loss: 0.0007697
[Epoch 15; Iter   338/ 2483] train: loss: 0.0014122
[Epoch 15; Iter   368/ 2483] train: loss: 0.0016331
[Epoch 15; Iter   398/ 2483] train: loss: 0.0030237
[Epoch 15; Iter   428/ 2483] train: loss: 0.0015570
[Epoch 15; Iter   458/ 2483] train: loss: 0.0021470
[Epoch 15; Iter   488/ 2483] train: loss: 0.0041860
[Epoch 15; Iter   518/ 2483] train: loss: 0.0013268
[Epoch 15; Iter   548/ 2483] train: loss: 0.0015848
[Epoch 15; Iter   578/ 2483] train: loss: 0.0010558
[Epoch 15; Iter   608/ 2483] train: loss: 0.0016962
[Epoch 15; Iter   638/ 2483] train: loss: 0.0805535
[Epoch 15; Iter   668/ 2483] train: loss: 0.0018520
[Epoch 15; Iter   698/ 2483] train: loss: 0.0013145
[Epoch 15; Iter   728/ 2483] train: loss: 0.0013450
[Epoch 15; Iter   758/ 2483] train: loss: 0.0026472
[Epoch 15; Iter   788/ 2483] train: loss: 0.0655176
[Epoch 15; Iter   818/ 2483] train: loss: 0.0015426
[Epoch 15; Iter   848/ 2483] train: loss: 0.0017600
[Epoch 15; Iter   878/ 2483] train: loss: 0.0022306
[Epoch 15; Iter   908/ 2483] train: loss: 0.0010571
[Epoch 15; Iter   938/ 2483] train: loss: 0.0008976
[Epoch 15; Iter   968/ 2483] train: loss: 0.0019941
[Epoch 15; Iter   998/ 2483] train: loss: 0.0017884
[Epoch 13; Iter  1404/ 2483] train: loss: 0.0013790
[Epoch 13; Iter  1434/ 2483] train: loss: 0.0013442
[Epoch 13; Iter  1464/ 2483] train: loss: 0.0016912
[Epoch 13; Iter  1494/ 2483] train: loss: 0.0011471
[Epoch 13; Iter  1524/ 2483] train: loss: 0.0012094
[Epoch 13; Iter  1554/ 2483] train: loss: 0.0012426
[Epoch 13; Iter  1584/ 2483] train: loss: 0.0699213
[Epoch 13; Iter  1614/ 2483] train: loss: 0.0010839
[Epoch 13; Iter  1644/ 2483] train: loss: 0.0017107
[Epoch 13; Iter  1674/ 2483] train: loss: 0.0013769
[Epoch 13; Iter  1704/ 2483] train: loss: 0.0018999
[Epoch 13; Iter  1734/ 2483] train: loss: 0.0014395
[Epoch 13; Iter  1764/ 2483] train: loss: 0.0017884
[Epoch 13; Iter  1794/ 2483] train: loss: 0.0020605
[Epoch 13; Iter  1824/ 2483] train: loss: 0.0019087
[Epoch 13; Iter  1854/ 2483] train: loss: 0.0015411
[Epoch 13; Iter  1884/ 2483] train: loss: 0.0016031
[Epoch 13; Iter  1914/ 2483] train: loss: 0.0016632
[Epoch 13; Iter  1944/ 2483] train: loss: 0.0015867
[Epoch 13; Iter  1974/ 2483] train: loss: 0.1384039
[Epoch 13; Iter  2004/ 2483] train: loss: 0.0629905
[Epoch 13; Iter  2034/ 2483] train: loss: 0.0017236
[Epoch 13; Iter  2064/ 2483] train: loss: 0.0031738
[Epoch 13; Iter  2094/ 2483] train: loss: 0.0015734
[Epoch 13; Iter  2124/ 2483] train: loss: 0.0021119
[Epoch 13; Iter  2154/ 2483] train: loss: 0.0647704
[Epoch 13; Iter  2184/ 2483] train: loss: 0.0017353
[Epoch 13; Iter  2214/ 2483] train: loss: 0.0022936
[Epoch 13; Iter  2244/ 2483] train: loss: 0.0023291
[Epoch 13; Iter  2274/ 2483] train: loss: 0.0018050
[Epoch 13; Iter  2304/ 2483] train: loss: 0.0019426
[Epoch 13; Iter  2334/ 2483] train: loss: 0.0018021
[Epoch 13; Iter  2364/ 2483] train: loss: 0.0024422
[Epoch 13; Iter  2394/ 2483] train: loss: 0.0687880
[Epoch 13; Iter  2424/ 2483] train: loss: 0.0021142
[Epoch 13; Iter  2454/ 2483] train: loss: 0.0014497
[Epoch 13] ogbg-molmuv: 0.021020 val loss: 0.377291
[Epoch 13] ogbg-molmuv: 0.028660 test loss: 0.493365
[Epoch 14; Iter     1/ 2483] train: loss: 0.0021665
[Epoch 14; Iter    31/ 2483] train: loss: 0.0020397
[Epoch 14; Iter    61/ 2483] train: loss: 0.0027159
[Epoch 14; Iter    91/ 2483] train: loss: 0.0016532
[Epoch 14; Iter   121/ 2483] train: loss: 0.0019541
[Epoch 14; Iter   151/ 2483] train: loss: 0.0029503
[Epoch 14; Iter   181/ 2483] train: loss: 0.0017641
[Epoch 14; Iter   211/ 2483] train: loss: 0.0018933
[Epoch 14; Iter   241/ 2483] train: loss: 0.0020289
[Epoch 14; Iter   271/ 2483] train: loss: 0.1110846
[Epoch 14; Iter   301/ 2483] train: loss: 0.0020624
[Epoch 14; Iter   331/ 2483] train: loss: 0.0018144
[Epoch 14; Iter   361/ 2483] train: loss: 0.0014780
[Epoch 14; Iter   391/ 2483] train: loss: 0.0017295
[Epoch 14; Iter   421/ 2483] train: loss: 0.0014162
[Epoch 14; Iter   451/ 2483] train: loss: 0.0015721
[Epoch 14; Iter   481/ 2483] train: loss: 0.0012493
[Epoch 14; Iter   511/ 2483] train: loss: 0.0012186
[Epoch 14; Iter   541/ 2483] train: loss: 0.0012315
[Epoch 14; Iter   571/ 2483] train: loss: 0.0014346
[Epoch 14; Iter   601/ 2483] train: loss: 0.0015419
[Epoch 14; Iter   631/ 2483] train: loss: 0.0578699
[Epoch 14; Iter   661/ 2483] train: loss: 0.0012271
[Epoch 14; Iter   691/ 2483] train: loss: 0.0021380
[Epoch 14; Iter   721/ 2483] train: loss: 0.0023950
[Epoch 14; Iter   751/ 2483] train: loss: 0.0018371
[Epoch 14; Iter   781/ 2483] train: loss: 0.0018508
[Epoch 14; Iter   811/ 2483] train: loss: 0.0018205
[Epoch 14; Iter   841/ 2483] train: loss: 0.0046135
[Epoch 14; Iter   871/ 2483] train: loss: 0.0015829
[Epoch 14; Iter   901/ 2483] train: loss: 0.0017046
[Epoch 14; Iter   931/ 2483] train: loss: 0.0050863
[Epoch 14; Iter   961/ 2483] train: loss: 0.0020702
[Epoch 14; Iter   991/ 2483] train: loss: 0.0018792
[Epoch 14; Iter  1021/ 2483] train: loss: 0.0019612
[Epoch 14; Iter  1051/ 2483] train: loss: 0.0017102
[Epoch 14; Iter  1081/ 2483] train: loss: 0.0017591
[Epoch 14; Iter  1111/ 2483] train: loss: 0.0019025
[Epoch 14; Iter  1141/ 2483] train: loss: 0.0012307
[Epoch 14; Iter  1171/ 2483] train: loss: 0.0015748
[Epoch 14; Iter  1201/ 2483] train: loss: 0.0014785
[Epoch 14; Iter  1231/ 2483] train: loss: 0.0014937
[Epoch 14; Iter  1261/ 2483] train: loss: 0.0020327
[Epoch 14; Iter  1291/ 2483] train: loss: 0.0013454
[Epoch 14; Iter  1321/ 2483] train: loss: 0.0021833
[Epoch 14; Iter  1351/ 2483] train: loss: 0.0013870
[Epoch 14; Iter  1381/ 2483] train: loss: 0.0012228
[Epoch 14; Iter  1411/ 2483] train: loss: 0.0015368
[Epoch 14; Iter  1441/ 2483] train: loss: 0.0020739
[Epoch 14; Iter  1471/ 2483] train: loss: 0.0747649
[Epoch 14; Iter  1501/ 2483] train: loss: 0.0023525
[Epoch 14; Iter  1531/ 2483] train: loss: 0.0021288
[Epoch 14; Iter  1561/ 2483] train: loss: 0.0017654
[Epoch 14; Iter  1591/ 2483] train: loss: 0.0016401
[Epoch 14; Iter  1621/ 2483] train: loss: 0.0014638
[Epoch 14; Iter  1651/ 2483] train: loss: 0.0010912
[Epoch 14; Iter  1681/ 2483] train: loss: 0.0012631
[Epoch 14; Iter  1711/ 2483] train: loss: 0.0021149
[Epoch 14; Iter  1741/ 2483] train: loss: 0.0015038
[Epoch 14; Iter  1771/ 2483] train: loss: 0.0013671
[Epoch 14; Iter  1801/ 2483] train: loss: 0.0012991
[Epoch 14; Iter  1831/ 2483] train: loss: 0.0014982
[Epoch 14; Iter  1861/ 2483] train: loss: 0.0013689
[Epoch 14; Iter  1891/ 2483] train: loss: 0.0022687
[Epoch 14; Iter  1921/ 2483] train: loss: 0.0022690
[Epoch 14; Iter  1951/ 2483] train: loss: 0.0671652
[Epoch 14; Iter  1981/ 2483] train: loss: 0.0603517
[Epoch 14; Iter  2011/ 2483] train: loss: 0.0023093
[Epoch 14; Iter  2041/ 2483] train: loss: 0.0526666
[Epoch 14; Iter  2071/ 2483] train: loss: 0.0020253
[Epoch 14; Iter  2101/ 2483] train: loss: 0.0022027
[Epoch 14; Iter  2131/ 2483] train: loss: 0.0021826
[Epoch 14; Iter  2161/ 2483] train: loss: 0.0020601
[Epoch 14; Iter  2191/ 2483] train: loss: 0.0515387
[Epoch 14; Iter  2221/ 2483] train: loss: 0.0023299
[Epoch 14; Iter  2251/ 2483] train: loss: 0.0019354
[Epoch 14; Iter  2281/ 2483] train: loss: 0.0018850
[Epoch 14; Iter  2311/ 2483] train: loss: 0.0022321
[Epoch 14; Iter  2341/ 2483] train: loss: 0.0027541
[Epoch 14; Iter  2371/ 2483] train: loss: 0.0024159
[Epoch 14; Iter  2401/ 2483] train: loss: 0.0021141
[Epoch 14; Iter  2431/ 2483] train: loss: 0.0629651
[Epoch 14; Iter  2461/ 2483] train: loss: 0.0658819
[Epoch 14] ogbg-molmuv: 0.051350 val loss: 0.015301
[Epoch 14] ogbg-molmuv: 0.053835 test loss: 0.015072
[Epoch 15; Iter     8/ 2483] train: loss: 0.0018862
[Epoch 15; Iter    38/ 2483] train: loss: 0.0024905
[Epoch 15; Iter    68/ 2483] train: loss: 0.0017731
[Epoch 15; Iter    98/ 2483] train: loss: 0.0024877
[Epoch 15; Iter   128/ 2483] train: loss: 0.0377403
[Epoch 15; Iter   158/ 2483] train: loss: 0.0030687
[Epoch 15; Iter   188/ 2483] train: loss: 0.0769563
[Epoch 15; Iter   218/ 2483] train: loss: 0.0018382
[Epoch 15; Iter   248/ 2483] train: loss: 0.0013016
[Epoch 15; Iter   278/ 2483] train: loss: 0.0021189
[Epoch 15; Iter   308/ 2483] train: loss: 0.0019440
[Epoch 15; Iter   338/ 2483] train: loss: 0.0013086
[Epoch 15; Iter   368/ 2483] train: loss: 0.0014769
[Epoch 15; Iter   398/ 2483] train: loss: 0.0021212
[Epoch 15; Iter   428/ 2483] train: loss: 0.0020615
[Epoch 15; Iter   458/ 2483] train: loss: 0.0606484
[Epoch 15; Iter   488/ 2483] train: loss: 0.0011169
[Epoch 15; Iter   518/ 2483] train: loss: 0.0013595
[Epoch 15; Iter   548/ 2483] train: loss: 0.0012760
[Epoch 15; Iter   578/ 2483] train: loss: 0.0021252
[Epoch 15; Iter   608/ 2483] train: loss: 0.0392256
[Epoch 15; Iter   638/ 2483] train: loss: 0.0849594
[Epoch 15; Iter   668/ 2483] train: loss: 0.0015734
[Epoch 15; Iter   698/ 2483] train: loss: 0.0015625
[Epoch 15; Iter   728/ 2483] train: loss: 0.0014021
[Epoch 15; Iter   758/ 2483] train: loss: 0.0018156
[Epoch 15; Iter   788/ 2483] train: loss: 0.0014604
[Epoch 15; Iter   818/ 2483] train: loss: 0.0014118
[Epoch 15; Iter   848/ 2483] train: loss: 0.0028037
[Epoch 15; Iter   878/ 2483] train: loss: 0.0017986
[Epoch 15; Iter   908/ 2483] train: loss: 0.0018885
[Epoch 15; Iter   938/ 2483] train: loss: 0.0013494
[Epoch 15; Iter   968/ 2483] train: loss: 0.0015033
[Epoch 15; Iter   998/ 2483] train: loss: 0.0023290
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0013653
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0019902
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0010793
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0011311
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0843284
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0013646
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0017122
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0019406
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0014439
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0012824
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0017454
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0028060
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0027839
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0011878
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0010527
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0013857
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0019134
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0015811
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0755393
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0695766
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0014172
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0019923
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0021050
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0017977
[Epoch 15; Iter  1748/ 2483] train: loss: 0.1066006
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0778246
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0019550
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0012124
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0022783
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0023683
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0735752
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0820655
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0018829
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0019591
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0015160
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0021525
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0640059
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0301381
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0015928
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0022305
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0022578
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0013042
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0014843
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0591573
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0061792
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0012966
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0012183
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0019760
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0756683
[Epoch 15] ogbg-molmuv: 0.077782 val loss: 0.016017
[Epoch 15] ogbg-molmuv: 0.037021 test loss: 0.016775
[Epoch 16; Iter    15/ 2483] train: loss: 0.0022309
[Epoch 16; Iter    45/ 2483] train: loss: 0.0011584
[Epoch 16; Iter    75/ 2483] train: loss: 0.0013025
[Epoch 16; Iter   105/ 2483] train: loss: 0.0011562
[Epoch 16; Iter   135/ 2483] train: loss: 0.0877111
[Epoch 16; Iter   165/ 2483] train: loss: 0.0009581
[Epoch 16; Iter   195/ 2483] train: loss: 0.0013639
[Epoch 16; Iter   225/ 2483] train: loss: 0.0028985
[Epoch 16; Iter   255/ 2483] train: loss: 0.0025674
[Epoch 16; Iter   285/ 2483] train: loss: 0.0024282
[Epoch 16; Iter   315/ 2483] train: loss: 0.0016607
[Epoch 16; Iter   345/ 2483] train: loss: 0.0014461
[Epoch 16; Iter   375/ 2483] train: loss: 0.0019856
[Epoch 16; Iter   405/ 2483] train: loss: 0.0024428
[Epoch 16; Iter   435/ 2483] train: loss: 0.0012401
[Epoch 16; Iter   465/ 2483] train: loss: 0.0013670
[Epoch 16; Iter   495/ 2483] train: loss: 0.0014009
[Epoch 16; Iter   525/ 2483] train: loss: 0.0009945
[Epoch 16; Iter   555/ 2483] train: loss: 0.0007295
[Epoch 16; Iter   585/ 2483] train: loss: 0.0616241
[Epoch 16; Iter   615/ 2483] train: loss: 0.0692613
[Epoch 16; Iter   645/ 2483] train: loss: 0.0016770
[Epoch 16; Iter   675/ 2483] train: loss: 0.0014493
[Epoch 16; Iter   705/ 2483] train: loss: 0.0017775
[Epoch 16; Iter   735/ 2483] train: loss: 0.0018462
[Epoch 16; Iter   765/ 2483] train: loss: 0.0022899
[Epoch 16; Iter   795/ 2483] train: loss: 0.0658858
[Epoch 16; Iter   825/ 2483] train: loss: 0.0025866
[Epoch 16; Iter   855/ 2483] train: loss: 0.0023871
[Epoch 16; Iter   885/ 2483] train: loss: 0.0023026
[Epoch 16; Iter   915/ 2483] train: loss: 0.0017881
[Epoch 16; Iter   945/ 2483] train: loss: 0.0025604
[Epoch 16; Iter   975/ 2483] train: loss: 0.0022163
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0018207
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0024044
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0015048
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0020100
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0017600
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0018238
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0022338
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0884113
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0014061
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0010632
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0035792
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0019948
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0019227
[Epoch 16; Iter  1395/ 2483] train: loss: 0.1870806
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0018055
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0019442
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0012101
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0014766
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0031434
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0025215
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0017681
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0059683
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0010072
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0927970
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0016831
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0016574
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0015361
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0017464
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0013376
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0026347
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0028538
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0018453
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0016989
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0019311
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0020719
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0025304
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0016325
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0015448
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0017021
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0030854
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0020280
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0016798
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0016023
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0012439
[Epoch 16; Iter  2325/ 2483] train: loss: 0.0011082
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0011629
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0030756
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0017412
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0014244
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0023481
[Epoch 16] ogbg-molmuv: 0.079425 val loss: 0.015159
[Epoch 16] ogbg-molmuv: 0.052109 test loss: 0.015399
[Epoch 17; Iter    22/ 2483] train: loss: 0.0508874
[Epoch 17; Iter    52/ 2483] train: loss: 0.0017237
[Epoch 17; Iter    82/ 2483] train: loss: 0.0920030
[Epoch 17; Iter   112/ 2483] train: loss: 0.0014512
[Epoch 17; Iter   142/ 2483] train: loss: 0.0019238
[Epoch 17; Iter   172/ 2483] train: loss: 0.0019295
[Epoch 17; Iter   202/ 2483] train: loss: 0.0831748
[Epoch 17; Iter   232/ 2483] train: loss: 0.0572006
[Epoch 17; Iter   262/ 2483] train: loss: 0.0027193
[Epoch 17; Iter   292/ 2483] train: loss: 0.0024362
[Epoch 17; Iter   322/ 2483] train: loss: 0.0014831
[Epoch 17; Iter   352/ 2483] train: loss: 0.0022834
[Epoch 17; Iter   382/ 2483] train: loss: 0.0018720
[Epoch 17; Iter   412/ 2483] train: loss: 0.0036907
[Epoch 17; Iter   442/ 2483] train: loss: 0.0016995
[Epoch 17; Iter   472/ 2483] train: loss: 0.0021894
[Epoch 17; Iter   502/ 2483] train: loss: 0.0022918
[Epoch 17; Iter   532/ 2483] train: loss: 0.0015339
[Epoch 17; Iter   562/ 2483] train: loss: 0.0433985
[Epoch 17; Iter   592/ 2483] train: loss: 0.0034437
[Epoch 17; Iter   622/ 2483] train: loss: 0.0017575
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0014068
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0016961
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0015640
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0016076
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0017773
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0017727
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0011711
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0017683
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0019985
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0021703
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0018374
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0028372
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0023286
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0013475
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0012129
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0049451
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0013488
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0014673
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0021981
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0023227
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0791274
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0023842
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0026339
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0014961
[Epoch 15; Iter  1748/ 2483] train: loss: 0.0019266
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0014937
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0026213
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0014429
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0652062
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0017159
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0018084
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0016617
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0016485
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0012052
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0014431
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0013927
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0013871
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0011248
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0019922
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0017050
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0749942
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0017867
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0054584
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0602348
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0663323
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0015385
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0011558
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0018766
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0012826
[Epoch 15] ogbg-molmuv: 0.098791 val loss: 0.065855
[Epoch 15] ogbg-molmuv: 0.078784 test loss: 0.015558
[Epoch 16; Iter    15/ 2483] train: loss: 0.0874358
[Epoch 16; Iter    45/ 2483] train: loss: 0.0020255
[Epoch 16; Iter    75/ 2483] train: loss: 0.0019836
[Epoch 16; Iter   105/ 2483] train: loss: 0.0014536
[Epoch 16; Iter   135/ 2483] train: loss: 0.0615571
[Epoch 16; Iter   165/ 2483] train: loss: 0.0030685
[Epoch 16; Iter   195/ 2483] train: loss: 0.0026590
[Epoch 16; Iter   225/ 2483] train: loss: 0.0015745
[Epoch 16; Iter   255/ 2483] train: loss: 0.0018035
[Epoch 16; Iter   285/ 2483] train: loss: 0.0018453
[Epoch 16; Iter   315/ 2483] train: loss: 0.0014295
[Epoch 16; Iter   345/ 2483] train: loss: 0.0024064
[Epoch 16; Iter   375/ 2483] train: loss: 0.0016446
[Epoch 16; Iter   405/ 2483] train: loss: 0.0021273
[Epoch 16; Iter   435/ 2483] train: loss: 0.0057124
[Epoch 16; Iter   465/ 2483] train: loss: 0.0017141
[Epoch 16; Iter   495/ 2483] train: loss: 0.0015717
[Epoch 16; Iter   525/ 2483] train: loss: 0.0019722
[Epoch 16; Iter   555/ 2483] train: loss: 0.0012579
[Epoch 16; Iter   585/ 2483] train: loss: 0.0012587
[Epoch 16; Iter   615/ 2483] train: loss: 0.0016230
[Epoch 16; Iter   645/ 2483] train: loss: 0.0922159
[Epoch 16; Iter   675/ 2483] train: loss: 0.0030591
[Epoch 16; Iter   705/ 2483] train: loss: 0.0015460
[Epoch 16; Iter   735/ 2483] train: loss: 0.0716583
[Epoch 16; Iter   765/ 2483] train: loss: 0.0012410
[Epoch 16; Iter   795/ 2483] train: loss: 0.0016624
[Epoch 16; Iter   825/ 2483] train: loss: 0.0008921
[Epoch 16; Iter   855/ 2483] train: loss: 0.0024318
[Epoch 16; Iter   885/ 2483] train: loss: 0.0016344
[Epoch 16; Iter   915/ 2483] train: loss: 0.0015123
[Epoch 16; Iter   945/ 2483] train: loss: 0.0019648
[Epoch 16; Iter   975/ 2483] train: loss: 0.0016737
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0010461
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0015680
[Epoch 16; Iter  1065/ 2483] train: loss: 0.1074748
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0107923
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0012608
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0023328
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0015610
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0028420
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0017072
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0018188
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0732078
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0021505
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0570576
[Epoch 16; Iter  1395/ 2483] train: loss: 0.0588128
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0010119
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0015518
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0018650
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0012732
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0017004
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0014415
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0014433
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0010153
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0673747
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0019287
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0021006
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0053226
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0015188
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0018950
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0633508
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0016477
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0016984
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0013645
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0014742
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0809601
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0017508
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0011085
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0011632
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0014068
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0920973
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0016192
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0015502
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0596760
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0014671
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0012907
[Epoch 16; Iter  2325/ 2483] train: loss: 0.0010163
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0563872
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0010545
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0013100
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0027108
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0023207
[Epoch 16] ogbg-molmuv: 0.105361 val loss: 0.079371
[Epoch 16] ogbg-molmuv: 0.073960 test loss: 0.017342
[Epoch 17; Iter    22/ 2483] train: loss: 0.0019423
[Epoch 17; Iter    52/ 2483] train: loss: 0.0022855
[Epoch 17; Iter    82/ 2483] train: loss: 0.0032346
[Epoch 17; Iter   112/ 2483] train: loss: 0.0028786
[Epoch 17; Iter   142/ 2483] train: loss: 0.0020347
[Epoch 17; Iter   172/ 2483] train: loss: 0.0007907
[Epoch 17; Iter   202/ 2483] train: loss: 0.0013939
[Epoch 17; Iter   232/ 2483] train: loss: 0.0013478
[Epoch 17; Iter   262/ 2483] train: loss: 0.0014472
[Epoch 17; Iter   292/ 2483] train: loss: 0.0012936
[Epoch 17; Iter   322/ 2483] train: loss: 0.0015031
[Epoch 17; Iter   352/ 2483] train: loss: 0.0012193
[Epoch 17; Iter   382/ 2483] train: loss: 0.0011508
[Epoch 17; Iter   412/ 2483] train: loss: 0.0012554
[Epoch 17; Iter   442/ 2483] train: loss: 0.0016650
[Epoch 17; Iter   472/ 2483] train: loss: 0.0021164
[Epoch 17; Iter   502/ 2483] train: loss: 0.0019294
[Epoch 17; Iter   532/ 2483] train: loss: 0.0012316
[Epoch 17; Iter   562/ 2483] train: loss: 0.0015607
[Epoch 17; Iter   592/ 2483] train: loss: 0.0018150
[Epoch 17; Iter   622/ 2483] train: loss: 0.0014152
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0021334
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0015381
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0011307
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0947601
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0016566
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0012999
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0013720
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0943779
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0012647
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0041342
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0026700
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0014715
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0023736
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0026381
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0022961
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0011217
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0011470
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0012026
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0013671
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0017862
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0014041
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0014217
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0012296
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0015522
[Epoch 15; Iter  1748/ 2483] train: loss: 0.0011385
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0022432
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0012689
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0027440
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0042510
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0015763
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0014158
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0014464
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0737200
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0019972
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0020925
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0023984
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0022365
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0012320
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0016875
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0019721
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0021732
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0015788
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0023077
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0017962
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0020713
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0015734
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0018044
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0016436
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0025467
[Epoch 15] ogbg-molmuv: 0.056039 val loss: 0.014953
[Epoch 15] ogbg-molmuv: 0.030057 test loss: 0.025721
[Epoch 16; Iter    15/ 2483] train: loss: 0.0017822
[Epoch 16; Iter    45/ 2483] train: loss: 0.0013999
[Epoch 16; Iter    75/ 2483] train: loss: 0.0012807
[Epoch 16; Iter   105/ 2483] train: loss: 0.0039973
[Epoch 16; Iter   135/ 2483] train: loss: 0.0670009
[Epoch 16; Iter   165/ 2483] train: loss: 0.0014345
[Epoch 16; Iter   195/ 2483] train: loss: 0.0039020
[Epoch 16; Iter   225/ 2483] train: loss: 0.0020640
[Epoch 16; Iter   255/ 2483] train: loss: 0.0028534
[Epoch 16; Iter   285/ 2483] train: loss: 0.0028536
[Epoch 16; Iter   315/ 2483] train: loss: 0.0026629
[Epoch 16; Iter   345/ 2483] train: loss: 0.0015202
[Epoch 16; Iter   375/ 2483] train: loss: 0.0032945
[Epoch 16; Iter   405/ 2483] train: loss: 0.0020412
[Epoch 16; Iter   435/ 2483] train: loss: 0.0015897
[Epoch 16; Iter   465/ 2483] train: loss: 0.0016485
[Epoch 16; Iter   495/ 2483] train: loss: 0.0018319
[Epoch 16; Iter   525/ 2483] train: loss: 0.0014388
[Epoch 16; Iter   555/ 2483] train: loss: 0.0016480
[Epoch 16; Iter   585/ 2483] train: loss: 0.0015518
[Epoch 16; Iter   615/ 2483] train: loss: 0.0013515
[Epoch 16; Iter   645/ 2483] train: loss: 0.0010327
[Epoch 16; Iter   675/ 2483] train: loss: 0.0011210
[Epoch 16; Iter   705/ 2483] train: loss: 0.0011704
[Epoch 16; Iter   735/ 2483] train: loss: 0.0011208
[Epoch 16; Iter   765/ 2483] train: loss: 0.0015042
[Epoch 16; Iter   795/ 2483] train: loss: 0.0010998
[Epoch 16; Iter   825/ 2483] train: loss: 0.0015288
[Epoch 16; Iter   855/ 2483] train: loss: 0.0017876
[Epoch 16; Iter   885/ 2483] train: loss: 0.0021607
[Epoch 16; Iter   915/ 2483] train: loss: 0.0016286
[Epoch 16; Iter   945/ 2483] train: loss: 0.0029436
[Epoch 16; Iter   975/ 2483] train: loss: 0.0028595
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0736178
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0023328
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0543269
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0021741
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0019975
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0020417
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0051038
[Epoch 16; Iter  1215/ 2483] train: loss: 0.1051116
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0013948
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0017117
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0014900
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0633506
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0988832
[Epoch 16; Iter  1395/ 2483] train: loss: 0.0603594
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0020467
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0013326
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0019258
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0802925
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0018215
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0021130
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0014684
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0700250
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0018722
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0018720
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0022311
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0014671
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0027176
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0014726
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0016742
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0530813
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0016913
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0017773
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0018451
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0015242
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0591514
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0013641
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0011568
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0010335
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0013107
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0019078
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0015967
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0018561
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0012240
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0032133
[Epoch 16; Iter  2325/ 2483] train: loss: 0.1815006
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0015768
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0016073
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0027365
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0012191
[Epoch 16; Iter  2475/ 2483] train: loss: 0.1055118
[Epoch 16] ogbg-molmuv: 0.051036 val loss: 0.150948
[Epoch 16] ogbg-molmuv: 0.029425 test loss: 0.180878
[Epoch 17; Iter    22/ 2483] train: loss: 0.0019678
[Epoch 17; Iter    52/ 2483] train: loss: 0.0012528
[Epoch 17; Iter    82/ 2483] train: loss: 0.0012135
[Epoch 17; Iter   112/ 2483] train: loss: 0.0014980
[Epoch 17; Iter   142/ 2483] train: loss: 0.0014150
[Epoch 17; Iter   172/ 2483] train: loss: 0.0019377
[Epoch 17; Iter   202/ 2483] train: loss: 0.0012958
[Epoch 17; Iter   232/ 2483] train: loss: 0.0013130
[Epoch 17; Iter   262/ 2483] train: loss: 0.0022440
[Epoch 17; Iter   292/ 2483] train: loss: 0.0036084
[Epoch 17; Iter   322/ 2483] train: loss: 0.0013169
[Epoch 17; Iter   352/ 2483] train: loss: 0.0010678
[Epoch 17; Iter   382/ 2483] train: loss: 0.0013216
[Epoch 17; Iter   412/ 2483] train: loss: 0.1379316
[Epoch 17; Iter   442/ 2483] train: loss: 0.0021401
[Epoch 17; Iter   472/ 2483] train: loss: 0.0014261
[Epoch 17; Iter   502/ 2483] train: loss: 0.0014946
[Epoch 17; Iter   532/ 2483] train: loss: 0.0015722
[Epoch 17; Iter   562/ 2483] train: loss: 0.0023496
[Epoch 17; Iter   592/ 2483] train: loss: 0.0018488
[Epoch 17; Iter   622/ 2483] train: loss: 0.0020965
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0023814
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0016012
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0013364
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0826390
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0022204
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0015375
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0022924
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0812519
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0016505
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0125719
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0026272
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0015862
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0024864
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0053777
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0017333
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0012212
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0011821
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0013052
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0012821
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0020435
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0015818
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0015867
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0010210
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0015693
[Epoch 15; Iter  1748/ 2483] train: loss: 0.0012666
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0020453
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0017682
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0032735
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0042460
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0021918
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0013287
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0018181
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0769539
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0017242
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0019225
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0019246
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0020413
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0014076
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0021280
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0025177
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0054386
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0014763
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0022057
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0017348
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0018311
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0016241
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0014334
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0014428
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0026109
[Epoch 15] ogbg-molmuv: 0.018079 val loss: 0.046597
[Epoch 15] ogbg-molmuv: 0.027939 test loss: 0.068657
[Epoch 16; Iter    15/ 2483] train: loss: 0.0014973
[Epoch 16; Iter    45/ 2483] train: loss: 0.0014940
[Epoch 16; Iter    75/ 2483] train: loss: 0.0015030
[Epoch 16; Iter   105/ 2483] train: loss: 0.0014255
[Epoch 16; Iter   135/ 2483] train: loss: 0.0649399
[Epoch 16; Iter   165/ 2483] train: loss: 0.0032999
[Epoch 16; Iter   195/ 2483] train: loss: 0.0036009
[Epoch 16; Iter   225/ 2483] train: loss: 0.0016776
[Epoch 16; Iter   255/ 2483] train: loss: 0.0049960
[Epoch 16; Iter   285/ 2483] train: loss: 0.0026137
[Epoch 16; Iter   315/ 2483] train: loss: 0.0018263
[Epoch 16; Iter   345/ 2483] train: loss: 0.0018461
[Epoch 16; Iter   375/ 2483] train: loss: 0.0031319
[Epoch 16; Iter   405/ 2483] train: loss: 0.0017477
[Epoch 16; Iter   435/ 2483] train: loss: 0.0018516
[Epoch 16; Iter   465/ 2483] train: loss: 0.0014854
[Epoch 16; Iter   495/ 2483] train: loss: 0.0040128
[Epoch 16; Iter   525/ 2483] train: loss: 0.0011996
[Epoch 16; Iter   555/ 2483] train: loss: 0.0011564
[Epoch 16; Iter   585/ 2483] train: loss: 0.0019155
[Epoch 16; Iter   615/ 2483] train: loss: 0.0015369
[Epoch 16; Iter   645/ 2483] train: loss: 0.0009026
[Epoch 16; Iter   675/ 2483] train: loss: 0.0036537
[Epoch 16; Iter   705/ 2483] train: loss: 0.0006535
[Epoch 16; Iter   735/ 2483] train: loss: 0.0011500
[Epoch 16; Iter   765/ 2483] train: loss: 0.0014453
[Epoch 16; Iter   795/ 2483] train: loss: 0.0012888
[Epoch 16; Iter   825/ 2483] train: loss: 0.0012540
[Epoch 16; Iter   855/ 2483] train: loss: 0.0018392
[Epoch 16; Iter   885/ 2483] train: loss: 0.0025035
[Epoch 16; Iter   915/ 2483] train: loss: 0.0015329
[Epoch 16; Iter   945/ 2483] train: loss: 0.0019319
[Epoch 16; Iter   975/ 2483] train: loss: 0.0023860
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0501102
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0038291
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0605880
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0027135
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0024724
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0017261
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0072041
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0976793
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0015514
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0014958
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0017585
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0841293
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0904805
[Epoch 16; Iter  1395/ 2483] train: loss: 0.0899334
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0019477
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0014430
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0017116
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0644860
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0021161
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0024652
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0019899
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0663072
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0022183
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0023765
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0026273
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0017478
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0022212
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0015714
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0008388
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0729794
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0016285
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0024332
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0029665
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0017229
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0462411
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0014730
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0011095
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0009997
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0011631
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0031104
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0016557
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0013041
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0008762
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0021810
[Epoch 16; Iter  2325/ 2483] train: loss: 0.2319254
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0024661
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0015036
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0017825
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0020663
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0997022
[Epoch 16] ogbg-molmuv: 0.045976 val loss: 0.015123
[Epoch 16] ogbg-molmuv: 0.053122 test loss: 0.016303
[Epoch 17; Iter    22/ 2483] train: loss: 0.0023101
[Epoch 17; Iter    52/ 2483] train: loss: 0.0013772
[Epoch 17; Iter    82/ 2483] train: loss: 0.0012374
[Epoch 17; Iter   112/ 2483] train: loss: 0.0011994
[Epoch 17; Iter   142/ 2483] train: loss: 0.0011307
[Epoch 17; Iter   172/ 2483] train: loss: 0.0016086
[Epoch 17; Iter   202/ 2483] train: loss: 0.0013223
[Epoch 17; Iter   232/ 2483] train: loss: 0.0012333
[Epoch 17; Iter   262/ 2483] train: loss: 0.0032173
[Epoch 17; Iter   292/ 2483] train: loss: 0.0019260
[Epoch 17; Iter   322/ 2483] train: loss: 0.0015407
[Epoch 17; Iter   352/ 2483] train: loss: 0.0011852
[Epoch 17; Iter   382/ 2483] train: loss: 0.0016455
[Epoch 17; Iter   412/ 2483] train: loss: 0.1520563
[Epoch 17; Iter   442/ 2483] train: loss: 0.0021795
[Epoch 17; Iter   472/ 2483] train: loss: 0.0020428
[Epoch 17; Iter   502/ 2483] train: loss: 0.0012109
[Epoch 17; Iter   532/ 2483] train: loss: 0.0015465
[Epoch 17; Iter   562/ 2483] train: loss: 0.0025811
[Epoch 17; Iter   592/ 2483] train: loss: 0.0016572
[Epoch 17; Iter   622/ 2483] train: loss: 0.0016789
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0016772
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0015793
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0014552
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0012466
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0757104
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0016883
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0016852
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0024355
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0016097
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0017941
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0018103
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0037536
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0021614
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0013286
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0013863
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0011806
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0019173
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0015927
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0684227
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0539012
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0014848
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0017522
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0019409
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0012167
[Epoch 15; Iter  1748/ 2483] train: loss: 0.1224009
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0909163
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0020294
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0012482
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0035224
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0018399
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0811712
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0761805
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0029068
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0019168
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0022724
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0015701
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0707708
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0880233
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0019018
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0019494
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0022141
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0018562
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0014853
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0525256
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0012746
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0016917
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0013458
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0023402
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0704535
[Epoch 15] ogbg-molmuv: 0.030547 val loss: 0.017306
[Epoch 15] ogbg-molmuv: 0.040220 test loss: 0.017254
[Epoch 16; Iter    15/ 2483] train: loss: 0.0023451
[Epoch 16; Iter    45/ 2483] train: loss: 0.0013012
[Epoch 16; Iter    75/ 2483] train: loss: 0.0013576
[Epoch 16; Iter   105/ 2483] train: loss: 0.0012582
[Epoch 16; Iter   135/ 2483] train: loss: 0.0836887
[Epoch 16; Iter   165/ 2483] train: loss: 0.0010986
[Epoch 16; Iter   195/ 2483] train: loss: 0.0011311
[Epoch 16; Iter   225/ 2483] train: loss: 0.0018060
[Epoch 16; Iter   255/ 2483] train: loss: 0.0027917
[Epoch 16; Iter   285/ 2483] train: loss: 0.0022208
[Epoch 16; Iter   315/ 2483] train: loss: 0.0018141
[Epoch 16; Iter   345/ 2483] train: loss: 0.0022531
[Epoch 16; Iter   375/ 2483] train: loss: 0.0015863
[Epoch 16; Iter   405/ 2483] train: loss: 0.0021621
[Epoch 16; Iter   435/ 2483] train: loss: 0.0009967
[Epoch 16; Iter   465/ 2483] train: loss: 0.0014906
[Epoch 16; Iter   495/ 2483] train: loss: 0.0013845
[Epoch 16; Iter   525/ 2483] train: loss: 0.0008903
[Epoch 16; Iter   555/ 2483] train: loss: 0.0008031
[Epoch 16; Iter   585/ 2483] train: loss: 0.0662356
[Epoch 16; Iter   615/ 2483] train: loss: 0.0707775
[Epoch 16; Iter   645/ 2483] train: loss: 0.0013317
[Epoch 16; Iter   675/ 2483] train: loss: 0.0013444
[Epoch 16; Iter   705/ 2483] train: loss: 0.0017223
[Epoch 16; Iter   735/ 2483] train: loss: 0.0018208
[Epoch 16; Iter   765/ 2483] train: loss: 0.0022720
[Epoch 16; Iter   795/ 2483] train: loss: 0.0628608
[Epoch 16; Iter   825/ 2483] train: loss: 0.0023516
[Epoch 16; Iter   855/ 2483] train: loss: 0.0024725
[Epoch 16; Iter   885/ 2483] train: loss: 0.0028898
[Epoch 16; Iter   915/ 2483] train: loss: 0.0018339
[Epoch 16; Iter   945/ 2483] train: loss: 0.0024555
[Epoch 16; Iter   975/ 2483] train: loss: 0.0018510
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0018547
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0027516
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0017428
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0026468
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0016328
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0026133
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0023350
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0748659
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0026870
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0015078
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0034021
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0026450
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0020555
[Epoch 16; Iter  1395/ 2483] train: loss: 0.1729654
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0017065
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0018534
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0014689
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0018990
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0015276
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0023368
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0026537
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0022987
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0020555
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0682664
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0016165
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0017557
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0013537
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0020928
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0014978
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0020290
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0022326
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0018920
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0024447
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0019946
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0040399
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0020090
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0015953
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0034684
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0016633
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0016403
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0023382
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0014364
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0016457
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0013031
[Epoch 16; Iter  2325/ 2483] train: loss: 0.0013683
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0012059
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0019957
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0013705
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0017670
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0032629
[Epoch 16] ogbg-molmuv: 0.054869 val loss: 0.015415
[Epoch 16] ogbg-molmuv: 0.067713 test loss: 0.016244
[Epoch 17; Iter    22/ 2483] train: loss: 0.0215619
[Epoch 17; Iter    52/ 2483] train: loss: 0.0015112
[Epoch 17; Iter    82/ 2483] train: loss: 0.0953158
[Epoch 17; Iter   112/ 2483] train: loss: 0.0016106
[Epoch 17; Iter   142/ 2483] train: loss: 0.0016835
[Epoch 17; Iter   172/ 2483] train: loss: 0.0017027
[Epoch 17; Iter   202/ 2483] train: loss: 0.0928207
[Epoch 17; Iter   232/ 2483] train: loss: 0.0771898
[Epoch 17; Iter   262/ 2483] train: loss: 0.0028391
[Epoch 17; Iter   292/ 2483] train: loss: 0.0029648
[Epoch 17; Iter   322/ 2483] train: loss: 0.0016070
[Epoch 17; Iter   352/ 2483] train: loss: 0.0029467
[Epoch 17; Iter   382/ 2483] train: loss: 0.0021170
[Epoch 17; Iter   412/ 2483] train: loss: 0.0029026
[Epoch 17; Iter   442/ 2483] train: loss: 0.0019698
[Epoch 17; Iter   472/ 2483] train: loss: 0.0035489
[Epoch 17; Iter   502/ 2483] train: loss: 0.0046647
[Epoch 17; Iter   532/ 2483] train: loss: 0.0017848
[Epoch 17; Iter   562/ 2483] train: loss: 0.0571805
[Epoch 17; Iter   592/ 2483] train: loss: 0.0020239
[Epoch 17; Iter   622/ 2483] train: loss: 0.0017121
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0016394
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0014273
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0021805
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0024312
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0042915
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0032825
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0013950
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0012943
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0023330
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0023602
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0035306
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0019777
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0015675
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0012176
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0012326
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0022827
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0014304
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0015036
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0020825
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0013785
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0893348
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0025411
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0022696
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0017455
[Epoch 15; Iter  1748/ 2483] train: loss: 0.0025971
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0014595
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0023264
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0020220
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0715390
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0015508
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0019821
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0020703
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0015912
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0013625
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0014225
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0016566
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0012846
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0013235
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0017181
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0018827
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0662119
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0021809
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0014394
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0509727
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0959603
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0015763
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0011957
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0017382
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0015023
[Epoch 15] ogbg-molmuv: 0.028323 val loss: 0.228838
[Epoch 15] ogbg-molmuv: 0.015428 test loss: 0.153735
[Epoch 16; Iter    15/ 2483] train: loss: 0.0883205
[Epoch 16; Iter    45/ 2483] train: loss: 0.0023402
[Epoch 16; Iter    75/ 2483] train: loss: 0.0037639
[Epoch 16; Iter   105/ 2483] train: loss: 0.0013285
[Epoch 16; Iter   135/ 2483] train: loss: 0.0516684
[Epoch 16; Iter   165/ 2483] train: loss: 0.0018836
[Epoch 16; Iter   195/ 2483] train: loss: 0.0016565
[Epoch 16; Iter   225/ 2483] train: loss: 0.0027808
[Epoch 16; Iter   255/ 2483] train: loss: 0.0027494
[Epoch 16; Iter   285/ 2483] train: loss: 0.0022996
[Epoch 16; Iter   315/ 2483] train: loss: 0.0014175
[Epoch 16; Iter   345/ 2483] train: loss: 0.0029561
[Epoch 16; Iter   375/ 2483] train: loss: 0.0019985
[Epoch 16; Iter   405/ 2483] train: loss: 0.0017898
[Epoch 16; Iter   435/ 2483] train: loss: 0.0031057
[Epoch 16; Iter   465/ 2483] train: loss: 0.0023412
[Epoch 16; Iter   495/ 2483] train: loss: 0.0013931
[Epoch 16; Iter   525/ 2483] train: loss: 0.0018280
[Epoch 16; Iter   555/ 2483] train: loss: 0.0017815
[Epoch 16; Iter   585/ 2483] train: loss: 0.0019264
[Epoch 16; Iter   615/ 2483] train: loss: 0.0012183
[Epoch 16; Iter   645/ 2483] train: loss: 0.0658554
[Epoch 16; Iter   675/ 2483] train: loss: 0.0024383
[Epoch 16; Iter   705/ 2483] train: loss: 0.0045640
[Epoch 16; Iter   735/ 2483] train: loss: 0.0472860
[Epoch 16; Iter   765/ 2483] train: loss: 0.0013194
[Epoch 16; Iter   795/ 2483] train: loss: 0.0014054
[Epoch 16; Iter   825/ 2483] train: loss: 0.0022289
[Epoch 16; Iter   855/ 2483] train: loss: 0.0022861
[Epoch 16; Iter   885/ 2483] train: loss: 0.0020784
[Epoch 16; Iter   915/ 2483] train: loss: 0.0032277
[Epoch 16; Iter   945/ 2483] train: loss: 0.0013755
[Epoch 16; Iter   975/ 2483] train: loss: 0.0015656
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0013537
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0011992
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0973411
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0024251
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0022428
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0021434
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0019311
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0014661
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0019604
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0015687
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0681468
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0018585
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0605464
[Epoch 16; Iter  1395/ 2483] train: loss: 0.0662206
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0009209
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0016104
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0015623
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0017093
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0014908
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0013757
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0016442
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0018174
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0584401
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0015739
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0030424
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0024133
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0023497
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0018609
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0512987
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0032398
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0022473
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0018896
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0016103
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0618161
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0014175
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0014110
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0011642
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0013024
[Epoch 16; Iter  2145/ 2483] train: loss: 0.1030901
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0018273
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0020891
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0887924
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0018182
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0013076
[Epoch 16; Iter  2325/ 2483] train: loss: 0.0009977
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0522423
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0016597
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0016377
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0035838
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0016301
[Epoch 16] ogbg-molmuv: 0.041990 val loss: 0.016698
[Epoch 16] ogbg-molmuv: 0.042038 test loss: 0.017476
[Epoch 17; Iter    22/ 2483] train: loss: 0.0018590
[Epoch 17; Iter    52/ 2483] train: loss: 0.0015550
[Epoch 17; Iter    82/ 2483] train: loss: 0.0016102
[Epoch 17; Iter   112/ 2483] train: loss: 0.0021618
[Epoch 17; Iter   142/ 2483] train: loss: 0.0017461
[Epoch 17; Iter   172/ 2483] train: loss: 0.0012579
[Epoch 17; Iter   202/ 2483] train: loss: 0.0012795
[Epoch 17; Iter   232/ 2483] train: loss: 0.0019419
[Epoch 17; Iter   262/ 2483] train: loss: 0.0016146
[Epoch 17; Iter   292/ 2483] train: loss: 0.0011159
[Epoch 17; Iter   322/ 2483] train: loss: 0.0013823
[Epoch 17; Iter   352/ 2483] train: loss: 0.0009786
[Epoch 17; Iter   382/ 2483] train: loss: 0.0013898
[Epoch 17; Iter   412/ 2483] train: loss: 0.0019307
[Epoch 17; Iter   442/ 2483] train: loss: 0.0014770
[Epoch 17; Iter   472/ 2483] train: loss: 0.0020667
[Epoch 17; Iter   502/ 2483] train: loss: 0.0015128
[Epoch 17; Iter   532/ 2483] train: loss: 0.0011851
[Epoch 17; Iter   562/ 2483] train: loss: 0.0024169
[Epoch 17; Iter   592/ 2483] train: loss: 0.0013583
[Epoch 17; Iter   622/ 2483] train: loss: 0.0014662
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0012664
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0015327
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0014819
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0019010
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0018451
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0029937
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0013512
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0015419
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0022171
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0046186
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0018570
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0028917
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0020407
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0012126
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0021469
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0026109
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0011814
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0013678
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0012832
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0014327
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0944047
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0031567
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0021836
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0027871
[Epoch 15; Iter  1748/ 2483] train: loss: 0.0015279
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0015473
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0017513
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0018980
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0835353
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0020496
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0015754
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0015172
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0016707
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0015446
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0012711
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0016703
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0012516
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0015624
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0027994
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0019187
[Epoch 15; Iter  2228/ 2483] train: loss: 0.1148259
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0015679
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0016222
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0587044
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0618290
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0017241
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0015607
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0015487
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0014593
[Epoch 15] ogbg-molmuv: 0.048018 val loss: 0.015912
[Epoch 15] ogbg-molmuv: 0.034331 test loss: 0.016889
[Epoch 16; Iter    15/ 2483] train: loss: 0.0637900
[Epoch 16; Iter    45/ 2483] train: loss: 0.0022702
[Epoch 16; Iter    75/ 2483] train: loss: 0.0016090
[Epoch 16; Iter   105/ 2483] train: loss: 0.0019081
[Epoch 16; Iter   135/ 2483] train: loss: 0.0875627
[Epoch 16; Iter   165/ 2483] train: loss: 0.0028906
[Epoch 16; Iter   195/ 2483] train: loss: 0.0020578
[Epoch 16; Iter   225/ 2483] train: loss: 0.0020100
[Epoch 16; Iter   255/ 2483] train: loss: 0.0026962
[Epoch 16; Iter   285/ 2483] train: loss: 0.0026798
[Epoch 16; Iter   315/ 2483] train: loss: 0.0032579
[Epoch 16; Iter   345/ 2483] train: loss: 0.0025520
[Epoch 16; Iter   375/ 2483] train: loss: 0.0022761
[Epoch 16; Iter   405/ 2483] train: loss: 0.0019414
[Epoch 16; Iter   435/ 2483] train: loss: 0.0016023
[Epoch 16; Iter   465/ 2483] train: loss: 0.0021399
[Epoch 16; Iter   495/ 2483] train: loss: 0.0019961
[Epoch 16; Iter   525/ 2483] train: loss: 0.0028084
[Epoch 16; Iter   555/ 2483] train: loss: 0.0016784
[Epoch 16; Iter   585/ 2483] train: loss: 0.0018177
[Epoch 16; Iter   615/ 2483] train: loss: 0.0019302
[Epoch 16; Iter   645/ 2483] train: loss: 0.0553204
[Epoch 16; Iter   675/ 2483] train: loss: 0.0029903
[Epoch 16; Iter   705/ 2483] train: loss: 0.0020719
[Epoch 16; Iter   735/ 2483] train: loss: 0.0679373
[Epoch 16; Iter   765/ 2483] train: loss: 0.0027640
[Epoch 16; Iter   795/ 2483] train: loss: 0.0013725
[Epoch 16; Iter   825/ 2483] train: loss: 0.0011368
[Epoch 16; Iter   855/ 2483] train: loss: 0.0034330
[Epoch 16; Iter   885/ 2483] train: loss: 0.0021482
[Epoch 16; Iter   915/ 2483] train: loss: 0.0017415
[Epoch 16; Iter   945/ 2483] train: loss: 0.0011653
[Epoch 16; Iter   975/ 2483] train: loss: 0.0015166
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0010661
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0013953
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0755759
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0025764
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0017909
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0030702
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0022166
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0025227
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0015560
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0020546
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0692461
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0026467
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0563850
[Epoch 16; Iter  1395/ 2483] train: loss: 0.0553627
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0011757
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0038472
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0015712
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0018504
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0013171
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0019713
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0019990
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0010845
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0617774
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0032154
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0018933
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0022779
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0018464
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0021520
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0615164
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0020544
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0018130
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0016207
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0015586
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0989512
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0014137
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0010625
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0018331
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0015849
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0776254
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0020399
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0016146
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0611690
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0011425
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0011458
[Epoch 16; Iter  2325/ 2483] train: loss: 0.0013135
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0565531
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0014882
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0017287
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0012584
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0019913
[Epoch 16] ogbg-molmuv: 0.019510 val loss: 0.089327
[Epoch 16] ogbg-molmuv: 0.015167 test loss: 0.129737
[Epoch 17; Iter    22/ 2483] train: loss: 0.0016679
[Epoch 17; Iter    52/ 2483] train: loss: 0.0015792
[Epoch 17; Iter    82/ 2483] train: loss: 0.0016333
[Epoch 17; Iter   112/ 2483] train: loss: 0.0019446
[Epoch 17; Iter   142/ 2483] train: loss: 0.0012293
[Epoch 17; Iter   172/ 2483] train: loss: 0.0013571
[Epoch 17; Iter   202/ 2483] train: loss: 0.0012761
[Epoch 17; Iter   232/ 2483] train: loss: 0.0010512
[Epoch 17; Iter   262/ 2483] train: loss: 0.0018262
[Epoch 17; Iter   292/ 2483] train: loss: 0.0009509
[Epoch 17; Iter   322/ 2483] train: loss: 0.0014071
[Epoch 17; Iter   352/ 2483] train: loss: 0.0013963
[Epoch 17; Iter   382/ 2483] train: loss: 0.0010338
[Epoch 17; Iter   412/ 2483] train: loss: 0.0018599
[Epoch 17; Iter   442/ 2483] train: loss: 0.0018232
[Epoch 17; Iter   472/ 2483] train: loss: 0.0025151
[Epoch 17; Iter   502/ 2483] train: loss: 0.0018849
[Epoch 17; Iter   532/ 2483] train: loss: 0.0011096
[Epoch 17; Iter   562/ 2483] train: loss: 0.0010878
[Epoch 17; Iter   592/ 2483] train: loss: 0.0019649
[Epoch 17; Iter   622/ 2483] train: loss: 0.0011852
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0016496
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0015781
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0017910
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0796037
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0021016
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0015087
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0023188
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0668856
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0015610
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0030051
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0029832
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0018774
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0021377
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0035906
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0026118
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0015345
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0015898
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0009857
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0014180
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0016455
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0014233
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0013409
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0011305
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0013612
[Epoch 15; Iter  1748/ 2483] train: loss: 0.0014886
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0026647
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0020631
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0019302
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0049885
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0016868
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0016871
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0020928
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0759169
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0019837
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0016734
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0020226
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0032257
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0014047
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0014949
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0014406
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0022243
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0013828
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0022063
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0018850
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0012562
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0016012
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0024484
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0023782
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0021304
[Epoch 15] ogbg-molmuv: 0.012430 val loss: 2.673978
[Epoch 15] ogbg-molmuv: 0.019257 test loss: 1.319439
[Epoch 16; Iter    15/ 2483] train: loss: 0.0019098
[Epoch 16; Iter    45/ 2483] train: loss: 0.0019837
[Epoch 16; Iter    75/ 2483] train: loss: 0.0014252
[Epoch 16; Iter   105/ 2483] train: loss: 0.0026558
[Epoch 16; Iter   135/ 2483] train: loss: 0.0700027
[Epoch 16; Iter   165/ 2483] train: loss: 0.0052995
[Epoch 16; Iter   195/ 2483] train: loss: 0.0029967
[Epoch 16; Iter   225/ 2483] train: loss: 0.0015699
[Epoch 16; Iter   255/ 2483] train: loss: 0.0028719
[Epoch 16; Iter   285/ 2483] train: loss: 0.0037326
[Epoch 16; Iter   315/ 2483] train: loss: 0.0026975
[Epoch 16; Iter   345/ 2483] train: loss: 0.0016656
[Epoch 16; Iter   375/ 2483] train: loss: 0.0034566
[Epoch 16; Iter   405/ 2483] train: loss: 0.0016464
[Epoch 16; Iter   435/ 2483] train: loss: 0.0017345
[Epoch 16; Iter   465/ 2483] train: loss: 0.0013565
[Epoch 16; Iter   495/ 2483] train: loss: 0.0014874
[Epoch 16; Iter   525/ 2483] train: loss: 0.0014139
[Epoch 16; Iter   555/ 2483] train: loss: 0.0011468
[Epoch 16; Iter   585/ 2483] train: loss: 0.0014562
[Epoch 16; Iter   615/ 2483] train: loss: 0.0011080
[Epoch 16; Iter   645/ 2483] train: loss: 0.0009930
[Epoch 16; Iter   675/ 2483] train: loss: 0.0009719
[Epoch 16; Iter   705/ 2483] train: loss: 0.0011699
[Epoch 16; Iter   735/ 2483] train: loss: 0.0009705
[Epoch 16; Iter   765/ 2483] train: loss: 0.0013214
[Epoch 16; Iter   795/ 2483] train: loss: 0.0010273
[Epoch 16; Iter   825/ 2483] train: loss: 0.0031436
[Epoch 16; Iter   855/ 2483] train: loss: 0.0020334
[Epoch 16; Iter   885/ 2483] train: loss: 0.0017002
[Epoch 16; Iter   915/ 2483] train: loss: 0.0035726
[Epoch 16; Iter   945/ 2483] train: loss: 0.0019756
[Epoch 16; Iter   975/ 2483] train: loss: 0.0020919
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0515004
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0030425
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0587233
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0024487
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0016813
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0022131
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0038791
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0707888
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0018018
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0019177
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0019021
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0852134
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0601132
[Epoch 16; Iter  1395/ 2483] train: loss: 0.1348555
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0015507
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0075313
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0013069
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0891429
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0016642
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0029477
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0016693
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0630718
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0018242
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0017241
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0018703
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0013398
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0020505
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0023948
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0009255
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0663863
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0019891
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0015665
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0015896
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0013552
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0577300
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0013132
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0009362
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0010967
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0012540
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0026134
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0020523
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0014312
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0015942
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0026904
[Epoch 16; Iter  2325/ 2483] train: loss: 0.1779208
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0016415
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0041628
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0013308
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0013251
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0918931
[Epoch 16] ogbg-molmuv: 0.014868 val loss: 1.812842
[Epoch 16] ogbg-molmuv: 0.017541 test loss: 0.734148
[Epoch 17; Iter    22/ 2483] train: loss: 0.0014288
[Epoch 17; Iter    52/ 2483] train: loss: 0.0016830
[Epoch 17; Iter    82/ 2483] train: loss: 0.0013081
[Epoch 17; Iter   112/ 2483] train: loss: 0.0013279
[Epoch 17; Iter   142/ 2483] train: loss: 0.0009135
[Epoch 17; Iter   172/ 2483] train: loss: 0.0016057
[Epoch 17; Iter   202/ 2483] train: loss: 0.0013379
[Epoch 17; Iter   232/ 2483] train: loss: 0.0031631
[Epoch 17; Iter   262/ 2483] train: loss: 0.0043642
[Epoch 17; Iter   292/ 2483] train: loss: 0.0059900
[Epoch 17; Iter   322/ 2483] train: loss: 0.0015556
[Epoch 17; Iter   352/ 2483] train: loss: 0.0022340
[Epoch 17; Iter   382/ 2483] train: loss: 0.0025243
[Epoch 17; Iter   412/ 2483] train: loss: 0.1593035
[Epoch 17; Iter   442/ 2483] train: loss: 0.0013619
[Epoch 17; Iter   472/ 2483] train: loss: 0.0016487
[Epoch 17; Iter   502/ 2483] train: loss: 0.0014709
[Epoch 17; Iter   532/ 2483] train: loss: 0.0016268
[Epoch 17; Iter   562/ 2483] train: loss: 0.0012647
[Epoch 17; Iter   592/ 2483] train: loss: 0.0021397
[Epoch 17; Iter   622/ 2483] train: loss: 0.0027382
[Epoch 15; Iter  1028/ 2483] train: loss: 0.0013876
[Epoch 15; Iter  1058/ 2483] train: loss: 0.0013631
[Epoch 15; Iter  1088/ 2483] train: loss: 0.0010415
[Epoch 15; Iter  1118/ 2483] train: loss: 0.0014588
[Epoch 15; Iter  1148/ 2483] train: loss: 0.0803146
[Epoch 15; Iter  1178/ 2483] train: loss: 0.0016138
[Epoch 15; Iter  1208/ 2483] train: loss: 0.0015324
[Epoch 15; Iter  1238/ 2483] train: loss: 0.0015592
[Epoch 15; Iter  1268/ 2483] train: loss: 0.0018309
[Epoch 15; Iter  1298/ 2483] train: loss: 0.0019385
[Epoch 15; Iter  1328/ 2483] train: loss: 0.0013076
[Epoch 15; Iter  1358/ 2483] train: loss: 0.0017117
[Epoch 15; Iter  1388/ 2483] train: loss: 0.0026886
[Epoch 15; Iter  1418/ 2483] train: loss: 0.0012912
[Epoch 15; Iter  1448/ 2483] train: loss: 0.0017373
[Epoch 15; Iter  1478/ 2483] train: loss: 0.0010298
[Epoch 15; Iter  1508/ 2483] train: loss: 0.0019069
[Epoch 15; Iter  1538/ 2483] train: loss: 0.0031101
[Epoch 15; Iter  1568/ 2483] train: loss: 0.0568257
[Epoch 15; Iter  1598/ 2483] train: loss: 0.0754350
[Epoch 15; Iter  1628/ 2483] train: loss: 0.0015989
[Epoch 15; Iter  1658/ 2483] train: loss: 0.0023950
[Epoch 15; Iter  1688/ 2483] train: loss: 0.0020936
[Epoch 15; Iter  1718/ 2483] train: loss: 0.0013896
[Epoch 15; Iter  1748/ 2483] train: loss: 0.1145073
[Epoch 15; Iter  1778/ 2483] train: loss: 0.0960509
[Epoch 15; Iter  1808/ 2483] train: loss: 0.0024953
[Epoch 15; Iter  1838/ 2483] train: loss: 0.0014314
[Epoch 15; Iter  1868/ 2483] train: loss: 0.0023721
[Epoch 15; Iter  1898/ 2483] train: loss: 0.0023940
[Epoch 15; Iter  1928/ 2483] train: loss: 0.0911033
[Epoch 15; Iter  1958/ 2483] train: loss: 0.0798614
[Epoch 15; Iter  1988/ 2483] train: loss: 0.0033699
[Epoch 15; Iter  2018/ 2483] train: loss: 0.0030423
[Epoch 15; Iter  2048/ 2483] train: loss: 0.0015997
[Epoch 15; Iter  2078/ 2483] train: loss: 0.0012438
[Epoch 15; Iter  2108/ 2483] train: loss: 0.0769575
[Epoch 15; Iter  2138/ 2483] train: loss: 0.0654620
[Epoch 15; Iter  2168/ 2483] train: loss: 0.0014664
[Epoch 15; Iter  2198/ 2483] train: loss: 0.0021242
[Epoch 15; Iter  2228/ 2483] train: loss: 0.0023042
[Epoch 15; Iter  2258/ 2483] train: loss: 0.0016703
[Epoch 15; Iter  2288/ 2483] train: loss: 0.0012389
[Epoch 15; Iter  2318/ 2483] train: loss: 0.0791672
[Epoch 15; Iter  2348/ 2483] train: loss: 0.0012001
[Epoch 15; Iter  2378/ 2483] train: loss: 0.0016142
[Epoch 15; Iter  2408/ 2483] train: loss: 0.0013459
[Epoch 15; Iter  2438/ 2483] train: loss: 0.0020567
[Epoch 15; Iter  2468/ 2483] train: loss: 0.0886441
[Epoch 15] ogbg-molmuv: 0.029355 val loss: 0.016029
[Epoch 15] ogbg-molmuv: 0.037732 test loss: 0.016464
[Epoch 16; Iter    15/ 2483] train: loss: 0.0016841
[Epoch 16; Iter    45/ 2483] train: loss: 0.0020632
[Epoch 16; Iter    75/ 2483] train: loss: 0.0017645
[Epoch 16; Iter   105/ 2483] train: loss: 0.0015166
[Epoch 16; Iter   135/ 2483] train: loss: 0.0799837
[Epoch 16; Iter   165/ 2483] train: loss: 0.0015986
[Epoch 16; Iter   195/ 2483] train: loss: 0.0011552
[Epoch 16; Iter   225/ 2483] train: loss: 0.0028804
[Epoch 16; Iter   255/ 2483] train: loss: 0.0030353
[Epoch 16; Iter   285/ 2483] train: loss: 0.0017737
[Epoch 16; Iter   315/ 2483] train: loss: 0.0020093
[Epoch 16; Iter   345/ 2483] train: loss: 0.0013778
[Epoch 16; Iter   375/ 2483] train: loss: 0.0019440
[Epoch 16; Iter   405/ 2483] train: loss: 0.0020000
[Epoch 16; Iter   435/ 2483] train: loss: 0.0012899
[Epoch 16; Iter   465/ 2483] train: loss: 0.0012202
[Epoch 16; Iter   495/ 2483] train: loss: 0.0013799
[Epoch 16; Iter   525/ 2483] train: loss: 0.0008594
[Epoch 16; Iter   555/ 2483] train: loss: 0.0006656
[Epoch 16; Iter   585/ 2483] train: loss: 0.0618042
[Epoch 16; Iter   615/ 2483] train: loss: 0.0668783
[Epoch 16; Iter   645/ 2483] train: loss: 0.0025138
[Epoch 16; Iter   675/ 2483] train: loss: 0.0015989
[Epoch 16; Iter   705/ 2483] train: loss: 0.0013085
[Epoch 16; Iter   735/ 2483] train: loss: 0.0012376
[Epoch 16; Iter   765/ 2483] train: loss: 0.0014996
[Epoch 16; Iter   795/ 2483] train: loss: 0.0634011
[Epoch 16; Iter   825/ 2483] train: loss: 0.0025756
[Epoch 16; Iter   855/ 2483] train: loss: 0.0022247
[Epoch 16; Iter   885/ 2483] train: loss: 0.0024353
[Epoch 16; Iter   915/ 2483] train: loss: 0.0025420
[Epoch 16; Iter   945/ 2483] train: loss: 0.0023492
[Epoch 16; Iter   975/ 2483] train: loss: 0.0017927
[Epoch 16; Iter  1005/ 2483] train: loss: 0.0015922
[Epoch 16; Iter  1035/ 2483] train: loss: 0.0023724
[Epoch 16; Iter  1065/ 2483] train: loss: 0.0021165
[Epoch 16; Iter  1095/ 2483] train: loss: 0.0016495
[Epoch 16; Iter  1125/ 2483] train: loss: 0.0016135
[Epoch 16; Iter  1155/ 2483] train: loss: 0.0018216
[Epoch 16; Iter  1185/ 2483] train: loss: 0.0015339
[Epoch 16; Iter  1215/ 2483] train: loss: 0.0714339
[Epoch 16; Iter  1245/ 2483] train: loss: 0.0035641
[Epoch 16; Iter  1275/ 2483] train: loss: 0.0024873
[Epoch 16; Iter  1305/ 2483] train: loss: 0.0018743
[Epoch 16; Iter  1335/ 2483] train: loss: 0.0020709
[Epoch 16; Iter  1365/ 2483] train: loss: 0.0021130
[Epoch 16; Iter  1395/ 2483] train: loss: 0.1658138
[Epoch 16; Iter  1425/ 2483] train: loss: 0.0023836
[Epoch 16; Iter  1455/ 2483] train: loss: 0.0019290
[Epoch 16; Iter  1485/ 2483] train: loss: 0.0016586
[Epoch 16; Iter  1515/ 2483] train: loss: 0.0016303
[Epoch 16; Iter  1545/ 2483] train: loss: 0.0016739
[Epoch 16; Iter  1575/ 2483] train: loss: 0.0018681
[Epoch 16; Iter  1605/ 2483] train: loss: 0.0018212
[Epoch 16; Iter  1635/ 2483] train: loss: 0.0027188
[Epoch 16; Iter  1665/ 2483] train: loss: 0.0030009
[Epoch 16; Iter  1695/ 2483] train: loss: 0.0844650
[Epoch 16; Iter  1725/ 2483] train: loss: 0.0020961
[Epoch 16; Iter  1755/ 2483] train: loss: 0.0020173
[Epoch 16; Iter  1785/ 2483] train: loss: 0.0013537
[Epoch 16; Iter  1815/ 2483] train: loss: 0.0017436
[Epoch 16; Iter  1845/ 2483] train: loss: 0.0011047
[Epoch 16; Iter  1875/ 2483] train: loss: 0.0031123
[Epoch 16; Iter  1905/ 2483] train: loss: 0.0017770
[Epoch 16; Iter  1935/ 2483] train: loss: 0.0019357
[Epoch 16; Iter  1965/ 2483] train: loss: 0.0032937
[Epoch 16; Iter  1995/ 2483] train: loss: 0.0022959
[Epoch 16; Iter  2025/ 2483] train: loss: 0.0022395
[Epoch 16; Iter  2055/ 2483] train: loss: 0.0024306
[Epoch 16; Iter  2085/ 2483] train: loss: 0.0023634
[Epoch 16; Iter  2115/ 2483] train: loss: 0.0014038
[Epoch 16; Iter  2145/ 2483] train: loss: 0.0017056
[Epoch 16; Iter  2175/ 2483] train: loss: 0.0019674
[Epoch 16; Iter  2205/ 2483] train: loss: 0.0023434
[Epoch 16; Iter  2235/ 2483] train: loss: 0.0020370
[Epoch 16; Iter  2265/ 2483] train: loss: 0.0016322
[Epoch 16; Iter  2295/ 2483] train: loss: 0.0016283
[Epoch 16; Iter  2325/ 2483] train: loss: 0.0014235
[Epoch 16; Iter  2355/ 2483] train: loss: 0.0014942
[Epoch 16; Iter  2385/ 2483] train: loss: 0.0024500
[Epoch 16; Iter  2415/ 2483] train: loss: 0.0017951
[Epoch 16; Iter  2445/ 2483] train: loss: 0.0016901
[Epoch 16; Iter  2475/ 2483] train: loss: 0.0028403
[Epoch 16] ogbg-molmuv: 0.015227 val loss: 0.015898
[Epoch 16] ogbg-molmuv: 0.022265 test loss: 0.015842
[Epoch 17; Iter    22/ 2483] train: loss: 0.0549013
[Epoch 17; Iter    52/ 2483] train: loss: 0.0023103
[Epoch 17; Iter    82/ 2483] train: loss: 0.0864856
[Epoch 17; Iter   112/ 2483] train: loss: 0.0013207
[Epoch 17; Iter   142/ 2483] train: loss: 0.0025016
[Epoch 17; Iter   172/ 2483] train: loss: 0.0017773
[Epoch 17; Iter   202/ 2483] train: loss: 0.0712737
[Epoch 17; Iter   232/ 2483] train: loss: 0.0635278
[Epoch 17; Iter   262/ 2483] train: loss: 0.0034035
[Epoch 17; Iter   292/ 2483] train: loss: 0.0024803
[Epoch 17; Iter   322/ 2483] train: loss: 0.0024290
[Epoch 17; Iter   352/ 2483] train: loss: 0.0048295
[Epoch 17; Iter   382/ 2483] train: loss: 0.0023880
[Epoch 17; Iter   412/ 2483] train: loss: 0.0014252
[Epoch 17; Iter   442/ 2483] train: loss: 0.0012881
[Epoch 17; Iter   472/ 2483] train: loss: 0.0031111
[Epoch 17; Iter   502/ 2483] train: loss: 0.0024584
[Epoch 17; Iter   532/ 2483] train: loss: 0.0013845
[Epoch 17; Iter   562/ 2483] train: loss: 0.0521951
[Epoch 17; Iter   592/ 2483] train: loss: 0.0019402
[Epoch 17; Iter   622/ 2483] train: loss: 0.0021267
