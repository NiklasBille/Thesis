>>> Starting run for dataset: bace
Running RANDOM configs_split_experiments/GraphCL/bace/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/bace/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/bace/random/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.7.yml --seed 6 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bace/random/train_prop=0.8.yml --seed 6 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.8/PNA_ogbg-molbace_GraphCL_bace_random=0.8_4_26-05_09-37-59
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.8
logdir: runs/split/GraphCL/bace/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6939850
[Epoch 1] ogbg-molbace: 0.508615 val loss: 0.693044
[Epoch 1] ogbg-molbace: 0.517998 test loss: 0.693303
[Epoch 2; Iter    19/   41] train: loss: 0.6973513
[Epoch 2] ogbg-molbace: 0.458158 val loss: 0.693362
[Epoch 2] ogbg-molbace: 0.455487 test loss: 0.694278
[Epoch 3; Iter     8/   41] train: loss: 0.6908641
[Epoch 3; Iter    38/   41] train: loss: 0.6928161
[Epoch 3] ogbg-molbace: 0.456048 val loss: 0.694007
[Epoch 3] ogbg-molbace: 0.441264 test loss: 0.694533
[Epoch 4; Iter    27/   41] train: loss: 0.6947510
[Epoch 4] ogbg-molbace: 0.458333 val loss: 0.694109
[Epoch 4] ogbg-molbace: 0.442318 test loss: 0.694584
[Epoch 5; Iter    16/   41] train: loss: 0.6955258
[Epoch 5] ogbg-molbace: 0.459564 val loss: 0.694064
[Epoch 5] ogbg-molbace: 0.450922 test loss: 0.693991
[Epoch 6; Iter     5/   41] train: loss: 0.6939446
[Epoch 6; Iter    35/   41] train: loss: 0.6895037
[Epoch 6] ogbg-molbace: 0.462729 val loss: 0.694077
[Epoch 6] ogbg-molbace: 0.453205 test loss: 0.694185
[Epoch 7; Iter    24/   41] train: loss: 0.6932044
[Epoch 7] ogbg-molbace: 0.463080 val loss: 0.694035
[Epoch 7] ogbg-molbace: 0.458472 test loss: 0.693845
[Epoch 8; Iter    13/   41] train: loss: 0.6989012
[Epoch 8] ogbg-molbace: 0.460267 val loss: 0.694626
[Epoch 8] ogbg-molbace: 0.445654 test loss: 0.693880
[Epoch 9; Iter     2/   41] train: loss: 0.6894386
[Epoch 9; Iter    32/   41] train: loss: 0.6963452
[Epoch 9] ogbg-molbace: 0.463080 val loss: 0.694276
[Epoch 9] ogbg-molbace: 0.458472 test loss: 0.693387
[Epoch 10; Iter    21/   41] train: loss: 0.6954477
[Epoch 10] ogbg-molbace: 0.463608 val loss: 0.694735
[Epoch 10] ogbg-molbace: 0.452678 test loss: 0.693070
[Epoch 11; Iter    10/   41] train: loss: 0.6938121
[Epoch 11; Iter    40/   41] train: loss: 0.6947930
[Epoch 11] ogbg-molbace: 0.464487 val loss: 0.694744
[Epoch 11] ogbg-molbace: 0.460053 test loss: 0.692843
[Epoch 12; Iter    29/   41] train: loss: 0.6894475
[Epoch 12] ogbg-molbace: 0.471519 val loss: 0.694857
[Epoch 12] ogbg-molbace: 0.469886 test loss: 0.692484
[Epoch 13; Iter    18/   41] train: loss: 0.6984476
[Epoch 13] ogbg-molbace: 0.474508 val loss: 0.695254
[Epoch 13] ogbg-molbace: 0.464267 test loss: 0.691928
[Epoch 14; Iter     7/   41] train: loss: 0.6951801
[Epoch 14; Iter    37/   41] train: loss: 0.6923062
[Epoch 14] ogbg-molbace: 0.489100 val loss: 0.694938
[Epoch 14] ogbg-molbace: 0.487972 test loss: 0.691612
[Epoch 15; Iter    26/   41] train: loss: 0.6955081
[Epoch 15] ogbg-molbace: 0.494726 val loss: 0.695280
[Epoch 15] ogbg-molbace: 0.487270 test loss: 0.690960
[Epoch 16; Iter    15/   41] train: loss: 0.6999709
[Epoch 16] ogbg-molbace: 0.492440 val loss: 0.695911
[Epoch 16] ogbg-molbace: 0.487972 test loss: 0.690440
[Epoch 17; Iter     4/   41] train: loss: 0.6985945
[Epoch 17; Iter    34/   41] train: loss: 0.6873424
[Epoch 17] ogbg-molbace: 0.718003 val loss: 0.683946
[Epoch 17] ogbg-molbace: 0.752941 test loss: 0.692263
[Epoch 18; Iter    23/   41] train: loss: 0.6712857
[Epoch 18] ogbg-molbace: 0.778833 val loss: 0.655725
[Epoch 18] ogbg-molbace: 0.802458 test loss: 0.657090
[Epoch 19; Iter    12/   41] train: loss: 0.6393878
[Epoch 19] ogbg-molbace: 0.804852 val loss: 0.616311
[Epoch 19] ogbg-molbace: 0.801580 test loss: 0.611768
[Epoch 20; Iter     1/   41] train: loss: 0.5886563
[Epoch 20; Iter    31/   41] train: loss: 0.6232097
[Epoch 20] ogbg-molbace: 0.816983 val loss: 0.567271
[Epoch 20] ogbg-molbace: 0.823354 test loss: 0.556487
[Epoch 21; Iter    20/   41] train: loss: 0.6505519
[Epoch 21] ogbg-molbace: 0.832454 val loss: 0.555312
[Epoch 21] ogbg-molbace: 0.822651 test loss: 0.504378
[Epoch 22; Iter     9/   41] train: loss: 0.5627306
[Epoch 22; Iter    39/   41] train: loss: 0.4749919
[Epoch 22] ogbg-molbace: 0.808368 val loss: 0.535730
[Epoch 22] ogbg-molbace: 0.805092 test loss: 0.565775
[Epoch 23; Iter    28/   41] train: loss: 0.4807750
[Epoch 23] ogbg-molbace: 0.852145 val loss: 0.480756
[Epoch 23] ogbg-molbace: 0.837752 test loss: 0.466792
[Epoch 24; Iter    17/   41] train: loss: 0.6570336
[Epoch 24] ogbg-molbace: 0.876231 val loss: 0.464180
[Epoch 24] ogbg-molbace: 0.839333 test loss: 0.529788
[Epoch 25; Iter     6/   41] train: loss: 0.4719972
[Epoch 25; Iter    36/   41] train: loss: 0.4888102
[Epoch 25] ogbg-molbace: 0.877989 val loss: 0.517809
[Epoch 25] ogbg-molbace: 0.845303 test loss: 0.446358
[Epoch 26; Iter    25/   41] train: loss: 0.5465015
[Epoch 26] ogbg-molbace: 0.889768 val loss: 0.450791
[Epoch 26] ogbg-molbace: 0.856892 test loss: 0.425328
[Epoch 27; Iter    14/   41] train: loss: 0.5405861
[Epoch 27] ogbg-molbace: 0.871132 val loss: 0.605507
[Epoch 27] ogbg-molbace: 0.847410 test loss: 0.463371
[Epoch 28; Iter     3/   41] train: loss: 0.3255938
[Epoch 28; Iter    33/   41] train: loss: 0.4423048
[Epoch 28] ogbg-molbace: 0.806610 val loss: 0.504770
[Epoch 28] ogbg-molbace: 0.813169 test loss: 0.467850
[Epoch 29; Iter    22/   41] train: loss: 0.3906777
[Epoch 29] ogbg-molbace: 0.874648 val loss: 0.461629
[Epoch 29] ogbg-molbace: 0.858472 test loss: 0.416708
[Epoch 30; Iter    11/   41] train: loss: 0.4404808
[Epoch 30; Iter    41/   41] train: loss: 0.3645957
[Epoch 30] ogbg-molbace: 0.851793 val loss: 0.472592
[Epoch 30] ogbg-molbace: 0.800702 test loss: 0.545535
[Epoch 31; Iter    30/   41] train: loss: 0.5695224
[Epoch 31] ogbg-molbace: 0.831751 val loss: 0.492342
[Epoch 31] ogbg-molbace: 0.768569 test loss: 0.559182
[Epoch 32; Iter    19/   41] train: loss: 0.3986666
[Epoch 32] ogbg-molbace: 0.887131 val loss: 0.436269
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.8/PNA_ogbg-molbace_GraphCL_bace_random=0.8_5_26-05_09-38-00
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.8
logdir: runs/split/GraphCL/bace/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6934652
[Epoch 1] ogbg-molbace: 0.537096 val loss: 0.693317
[Epoch 1] ogbg-molbace: 0.495698 test loss: 0.693298
[Epoch 2; Iter    19/   41] train: loss: 0.6949320
[Epoch 2] ogbg-molbace: 0.512658 val loss: 0.694553
[Epoch 2] ogbg-molbace: 0.445830 test loss: 0.694235
[Epoch 3; Iter     8/   41] train: loss: 0.6922138
[Epoch 3; Iter    38/   41] train: loss: 0.6955892
[Epoch 3] ogbg-molbace: 0.501231 val loss: 0.694653
[Epoch 3] ogbg-molbace: 0.415452 test loss: 0.694956
[Epoch 4; Iter    27/   41] train: loss: 0.6921354
[Epoch 4] ogbg-molbace: 0.514592 val loss: 0.694676
[Epoch 4] ogbg-molbace: 0.423529 test loss: 0.694496
[Epoch 5; Iter    16/   41] train: loss: 0.6933140
[Epoch 5] ogbg-molbace: 0.516350 val loss: 0.694538
[Epoch 5] ogbg-molbace: 0.421422 test loss: 0.694606
[Epoch 6; Iter     5/   41] train: loss: 0.6968065
[Epoch 6; Iter    35/   41] train: loss: 0.6955060
[Epoch 6] ogbg-molbace: 0.514065 val loss: 0.694838
[Epoch 6] ogbg-molbace: 0.428095 test loss: 0.694270
[Epoch 7; Iter    24/   41] train: loss: 0.6923663
[Epoch 7] ogbg-molbace: 0.517053 val loss: 0.694974
[Epoch 7] ogbg-molbace: 0.428797 test loss: 0.694122
[Epoch 8; Iter    13/   41] train: loss: 0.6937447
[Epoch 8] ogbg-molbace: 0.511955 val loss: 0.695286
[Epoch 8] ogbg-molbace: 0.428270 test loss: 0.694007
[Epoch 9; Iter     2/   41] train: loss: 0.6944379
[Epoch 9; Iter    32/   41] train: loss: 0.6939526
[Epoch 9] ogbg-molbace: 0.519515 val loss: 0.695057
[Epoch 9] ogbg-molbace: 0.420369 test loss: 0.693781
[Epoch 10; Iter    21/   41] train: loss: 0.6922515
[Epoch 10] ogbg-molbace: 0.529008 val loss: 0.695163
[Epoch 10] ogbg-molbace: 0.436172 test loss: 0.693503
[Epoch 11; Iter    10/   41] train: loss: 0.6947718
[Epoch 11; Iter    40/   41] train: loss: 0.6918707
[Epoch 11] ogbg-molbace: 0.524437 val loss: 0.695441
[Epoch 11] ogbg-molbace: 0.446708 test loss: 0.693138
[Epoch 12; Iter    29/   41] train: loss: 0.6929824
[Epoch 12] ogbg-molbace: 0.534810 val loss: 0.695466
[Epoch 12] ogbg-molbace: 0.453907 test loss: 0.692665
[Epoch 13; Iter    18/   41] train: loss: 0.6936276
[Epoch 13] ogbg-molbace: 0.551688 val loss: 0.695436
[Epoch 13] ogbg-molbace: 0.468832 test loss: 0.692118
[Epoch 14; Iter     7/   41] train: loss: 0.6926047
[Epoch 14; Iter    37/   41] train: loss: 0.6925942
[Epoch 14] ogbg-molbace: 0.557841 val loss: 0.695752
[Epoch 14] ogbg-molbace: 0.484987 test loss: 0.691698
[Epoch 15; Iter    26/   41] train: loss: 0.6887385
[Epoch 15] ogbg-molbace: 0.564873 val loss: 0.695883
[Epoch 15] ogbg-molbace: 0.484284 test loss: 0.691354
[Epoch 16; Iter    15/   41] train: loss: 0.6962363
[Epoch 16] ogbg-molbace: 0.586674 val loss: 0.696018
[Epoch 16] ogbg-molbace: 0.508341 test loss: 0.690824
[Epoch 17; Iter     4/   41] train: loss: 0.6893116
[Epoch 17; Iter    34/   41] train: loss: 0.6952446
[Epoch 17] ogbg-molbace: 0.647504 val loss: 0.688516
[Epoch 17] ogbg-molbace: 0.634416 test loss: 0.690082
[Epoch 18; Iter    23/   41] train: loss: 0.6659845
[Epoch 18] ogbg-molbace: 0.777426 val loss: 0.663539
[Epoch 18] ogbg-molbace: 0.802458 test loss: 0.663882
[Epoch 19; Iter    12/   41] train: loss: 0.6442345
[Epoch 19] ogbg-molbace: 0.831224 val loss: 0.604686
[Epoch 19] ogbg-molbace: 0.823529 test loss: 0.627369
[Epoch 20; Iter     1/   41] train: loss: 0.6399525
[Epoch 20; Iter    31/   41] train: loss: 0.6721491
[Epoch 20] ogbg-molbace: 0.779887 val loss: 0.575708
[Epoch 20] ogbg-molbace: 0.791220 test loss: 0.638029
[Epoch 21; Iter    20/   41] train: loss: 0.5353307
[Epoch 21] ogbg-molbace: 0.864276 val loss: 0.516769
[Epoch 21] ogbg-molbace: 0.833187 test loss: 0.546999
[Epoch 22; Iter     9/   41] train: loss: 0.5841660
[Epoch 22; Iter    39/   41] train: loss: 0.5811592
[Epoch 22] ogbg-molbace: 0.793249 val loss: 0.530060
[Epoch 22] ogbg-molbace: 0.799298 test loss: 0.558231
[Epoch 23; Iter    28/   41] train: loss: 0.5425628
[Epoch 23] ogbg-molbace: 0.774613 val loss: 0.584002
[Epoch 23] ogbg-molbace: 0.754697 test loss: 0.505520
[Epoch 24; Iter    17/   41] train: loss: 0.4050494
[Epoch 24] ogbg-molbace: 0.827180 val loss: 0.512060
[Epoch 24] ogbg-molbace: 0.817208 test loss: 0.475421
[Epoch 25; Iter     6/   41] train: loss: 0.5207783
[Epoch 25; Iter    36/   41] train: loss: 0.4870463
[Epoch 25] ogbg-molbace: 0.876582 val loss: 0.472322
[Epoch 25] ogbg-molbace: 0.851273 test loss: 0.426319
[Epoch 26; Iter    25/   41] train: loss: 0.4024875
[Epoch 26] ogbg-molbace: 0.821554 val loss: 0.548111
[Epoch 26] ogbg-molbace: 0.811765 test loss: 0.463362
[Epoch 27; Iter    14/   41] train: loss: 0.6641818
[Epoch 27] ogbg-molbace: 0.828235 val loss: 0.634286
[Epoch 27] ogbg-molbace: 0.820018 test loss: 0.492524
[Epoch 28; Iter     3/   41] train: loss: 0.3816174
[Epoch 28; Iter    33/   41] train: loss: 0.5113152
[Epoch 28] ogbg-molbace: 0.892053 val loss: 0.517446
[Epoch 28] ogbg-molbace: 0.858648 test loss: 0.429269
[Epoch 29; Iter    22/   41] train: loss: 0.4864452
[Epoch 29] ogbg-molbace: 0.879219 val loss: 0.511942
[Epoch 29] ogbg-molbace: 0.843547 test loss: 0.434222
[Epoch 30; Iter    11/   41] train: loss: 0.4497038
[Epoch 30; Iter    41/   41] train: loss: 0.5188735
[Epoch 30] ogbg-molbace: 0.875527 val loss: 0.416177
[Epoch 30] ogbg-molbace: 0.841440 test loss: 0.431967
[Epoch 31; Iter    30/   41] train: loss: 0.5424415
[Epoch 31] ogbg-molbace: 0.881329 val loss: 0.438821
[Epoch 31] ogbg-molbace: 0.853205 test loss: 0.415661
[Epoch 32; Iter    19/   41] train: loss: 0.4561994
[Epoch 32] ogbg-molbace: 0.825774 val loss: 0.570056
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.7/PNA_ogbg-molbace_GraphCL_bace_random=0.7_5_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.7
logdir: runs/split/GraphCL/bace/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6946946
[Epoch 1] ogbg-molbace: 0.453283 val loss: 0.693346
[Epoch 1] ogbg-molbace: 0.528145 test loss: 0.693100
[Epoch 2; Iter    24/   36] train: loss: 0.6951447
[Epoch 2] ogbg-molbace: 0.450994 val loss: 0.693774
[Epoch 2] ogbg-molbace: 0.514306 test loss: 0.693245
[Epoch 3; Iter    18/   36] train: loss: 0.6918933
[Epoch 3] ogbg-molbace: 0.431897 val loss: 0.694044
[Epoch 3] ogbg-molbace: 0.462137 test loss: 0.693989
[Epoch 4; Iter    12/   36] train: loss: 0.6923340
[Epoch 4] ogbg-molbace: 0.439631 val loss: 0.694023
[Epoch 4] ogbg-molbace: 0.463614 test loss: 0.693989
[Epoch 5; Iter     6/   36] train: loss: 0.6902485
[Epoch 5; Iter    36/   36] train: loss: 0.6934843
[Epoch 5] ogbg-molbace: 0.441604 val loss: 0.693887
[Epoch 5] ogbg-molbace: 0.459493 test loss: 0.693895
[Epoch 6; Iter    30/   36] train: loss: 0.6935322
[Epoch 6] ogbg-molbace: 0.450047 val loss: 0.693758
[Epoch 6] ogbg-molbace: 0.462759 test loss: 0.693918
[Epoch 7; Iter    24/   36] train: loss: 0.6929871
[Epoch 7] ogbg-molbace: 0.434659 val loss: 0.693732
[Epoch 7] ogbg-molbace: 0.459493 test loss: 0.693944
[Epoch 8; Iter    18/   36] train: loss: 0.6963760
[Epoch 8] ogbg-molbace: 0.447838 val loss: 0.693574
[Epoch 8] ogbg-molbace: 0.466724 test loss: 0.693789
[Epoch 9; Iter    12/   36] train: loss: 0.6944759
[Epoch 9] ogbg-molbace: 0.447680 val loss: 0.693605
[Epoch 9] ogbg-molbace: 0.469600 test loss: 0.693769
[Epoch 10; Iter     6/   36] train: loss: 0.6933731
[Epoch 10; Iter    36/   36] train: loss: 0.6940706
[Epoch 10] ogbg-molbace: 0.451547 val loss: 0.693357
[Epoch 10] ogbg-molbace: 0.469523 test loss: 0.693608
[Epoch 11; Iter    30/   36] train: loss: 0.6961582
[Epoch 11] ogbg-molbace: 0.466067 val loss: 0.693116
[Epoch 11] ogbg-molbace: 0.477997 test loss: 0.693457
[Epoch 12; Iter    24/   36] train: loss: 0.6913539
[Epoch 12] ogbg-molbace: 0.463857 val loss: 0.693046
[Epoch 12] ogbg-molbace: 0.477842 test loss: 0.693430
[Epoch 13; Iter    18/   36] train: loss: 0.6906497
[Epoch 13] ogbg-molbace: 0.479482 val loss: 0.692758
[Epoch 13] ogbg-molbace: 0.503188 test loss: 0.693094
[Epoch 14; Iter    12/   36] train: loss: 0.6899396
[Epoch 14] ogbg-molbace: 0.478535 val loss: 0.692667
[Epoch 14] ogbg-molbace: 0.491137 test loss: 0.693217
[Epoch 15; Iter     6/   36] train: loss: 0.6936792
[Epoch 15; Iter    36/   36] train: loss: 0.6994576
[Epoch 15] ogbg-molbace: 0.500000 val loss: 0.692309
[Epoch 15] ogbg-molbace: 0.514150 test loss: 0.692833
[Epoch 16; Iter    30/   36] train: loss: 0.6897352
[Epoch 16] ogbg-molbace: 0.498422 val loss: 0.692207
[Epoch 16] ogbg-molbace: 0.521381 test loss: 0.692723
[Epoch 17; Iter    24/   36] train: loss: 0.6920407
[Epoch 17] ogbg-molbace: 0.509943 val loss: 0.692008
[Epoch 17] ogbg-molbace: 0.514228 test loss: 0.692798
[Epoch 18; Iter    18/   36] train: loss: 0.6898361
[Epoch 18] ogbg-molbace: 0.513415 val loss: 0.691933
[Epoch 18] ogbg-molbace: 0.522858 test loss: 0.692609
[Epoch 19; Iter    12/   36] train: loss: 0.6892207
[Epoch 19] ogbg-molbace: 0.530934 val loss: 0.691673
[Epoch 19] ogbg-molbace: 0.536619 test loss: 0.692503
[Epoch 20; Iter     6/   36] train: loss: 0.6891590
[Epoch 20; Iter    36/   36] train: loss: 0.6684816
[Epoch 20] ogbg-molbace: 0.735480 val loss: 0.678842
[Epoch 20] ogbg-molbace: 0.739076 test loss: 0.678138
[Epoch 21; Iter    30/   36] train: loss: 0.6685768
[Epoch 21] ogbg-molbace: 0.788352 val loss: 0.649633
[Epoch 21] ogbg-molbace: 0.795288 test loss: 0.645949
[Epoch 22; Iter    24/   36] train: loss: 0.6302701
[Epoch 22] ogbg-molbace: 0.801215 val loss: 0.628352
[Epoch 22] ogbg-molbace: 0.827865 test loss: 0.615163
[Epoch 23; Iter    18/   36] train: loss: 0.6147684
[Epoch 23] ogbg-molbace: 0.827967 val loss: 0.597731
[Epoch 23] ogbg-molbace: 0.836029 test loss: 0.585201
[Epoch 24; Iter    12/   36] train: loss: 0.5819024
[Epoch 24] ogbg-molbace: 0.824337 val loss: 0.581766
[Epoch 24] ogbg-molbace: 0.847924 test loss: 0.574002
[Epoch 25; Iter     6/   36] train: loss: 0.5372205
[Epoch 25; Iter    36/   36] train: loss: 0.6806965
[Epoch 25] ogbg-molbace: 0.835938 val loss: 0.586891
[Epoch 25] ogbg-molbace: 0.849790 test loss: 0.550267
[Epoch 26; Iter    30/   36] train: loss: 0.5811128
[Epoch 26] ogbg-molbace: 0.824021 val loss: 0.543393
[Epoch 26] ogbg-molbace: 0.817524 test loss: 0.526017
[Epoch 27; Iter    24/   36] train: loss: 0.4854569
[Epoch 27] ogbg-molbace: 0.839804 val loss: 0.629273
[Epoch 27] ogbg-molbace: 0.843959 test loss: 0.593070
[Epoch 28; Iter    18/   36] train: loss: 0.3547254
[Epoch 28] ogbg-molbace: 0.835780 val loss: 0.537193
[Epoch 28] ogbg-molbace: 0.834085 test loss: 0.521951
[Epoch 29; Iter    12/   36] train: loss: 0.4208980
[Epoch 29] ogbg-molbace: 0.847064 val loss: 0.527727
[Epoch 29] ogbg-molbace: 0.848002 test loss: 0.512276
[Epoch 30; Iter     6/   36] train: loss: 0.5020025
[Epoch 30; Iter    36/   36] train: loss: 0.3582993
[Epoch 30] ogbg-molbace: 0.842882 val loss: 0.524220
[Epoch 30] ogbg-molbace: 0.856399 test loss: 0.484931
[Epoch 31; Iter    30/   36] train: loss: 0.6729367
[Epoch 31] ogbg-molbace: 0.846828 val loss: 0.593227
[Epoch 31] ogbg-molbace: 0.843492 test loss: 0.567932
[Epoch 32; Iter    24/   36] train: loss: 0.5453001
[Epoch 32] ogbg-molbace: 0.856061 val loss: 0.502476
[Epoch 32] ogbg-molbace: 0.856399 test loss: 0.483273
[Epoch 33; Iter    18/   36] train: loss: 0.4434503
[Epoch 33] ogbg-molbace: 0.851326 val loss: 0.573128
[Epoch 33] ogbg-molbace: 0.849401 test loss: 0.541802
[Epoch 34; Iter    12/   36] train: loss: 0.5601455
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.7/PNA_ogbg-molbace_GraphCL_bace_random=0.7_6_26-05_09-38-00
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.7
logdir: runs/split/GraphCL/bace/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6958816
[Epoch 1] ogbg-molbace: 0.326389 val loss: 0.693825
[Epoch 1] ogbg-molbace: 0.320868 test loss: 0.693804
[Epoch 2; Iter    24/   36] train: loss: 0.6973562
[Epoch 2] ogbg-molbace: 0.329624 val loss: 0.696026
[Epoch 2] ogbg-molbace: 0.325921 test loss: 0.695979
[Epoch 3; Iter    18/   36] train: loss: 0.6925369
[Epoch 3] ogbg-molbace: 0.332465 val loss: 0.697549
[Epoch 3] ogbg-molbace: 0.335329 test loss: 0.696960
[Epoch 4; Iter    12/   36] train: loss: 0.6912962
[Epoch 4] ogbg-molbace: 0.332939 val loss: 0.697342
[Epoch 4] ogbg-molbace: 0.339761 test loss: 0.696810
[Epoch 5; Iter     6/   36] train: loss: 0.6941913
[Epoch 5; Iter    36/   36] train: loss: 0.6918948
[Epoch 5] ogbg-molbace: 0.333018 val loss: 0.697256
[Epoch 5] ogbg-molbace: 0.336417 test loss: 0.696659
[Epoch 6; Iter    30/   36] train: loss: 0.6969132
[Epoch 6] ogbg-molbace: 0.334754 val loss: 0.697093
[Epoch 6] ogbg-molbace: 0.336106 test loss: 0.696632
[Epoch 7; Iter    24/   36] train: loss: 0.6937486
[Epoch 7] ogbg-molbace: 0.334754 val loss: 0.697229
[Epoch 7] ogbg-molbace: 0.341238 test loss: 0.696683
[Epoch 8; Iter    18/   36] train: loss: 0.6931396
[Epoch 8] ogbg-molbace: 0.331992 val loss: 0.697042
[Epoch 8] ogbg-molbace: 0.342248 test loss: 0.696444
[Epoch 9; Iter    12/   36] train: loss: 0.6928968
[Epoch 9] ogbg-molbace: 0.331992 val loss: 0.696876
[Epoch 9] ogbg-molbace: 0.338828 test loss: 0.696402
[Epoch 10; Iter     6/   36] train: loss: 0.6891822
[Epoch 10; Iter    36/   36] train: loss: 0.6979713
[Epoch 10] ogbg-molbace: 0.333807 val loss: 0.696596
[Epoch 10] ogbg-molbace: 0.339216 test loss: 0.696252
[Epoch 11; Iter    30/   36] train: loss: 0.6910614
[Epoch 11] ogbg-molbace: 0.330019 val loss: 0.696714
[Epoch 11] ogbg-molbace: 0.341782 test loss: 0.696254
[Epoch 12; Iter    24/   36] train: loss: 0.6930175
[Epoch 12] ogbg-molbace: 0.332150 val loss: 0.696317
[Epoch 12] ogbg-molbace: 0.340538 test loss: 0.696061
[Epoch 13; Iter    18/   36] train: loss: 0.6968260
[Epoch 13] ogbg-molbace: 0.332307 val loss: 0.696161
[Epoch 13] ogbg-molbace: 0.336651 test loss: 0.695899
[Epoch 14; Iter    12/   36] train: loss: 0.6971684
[Epoch 14] ogbg-molbace: 0.331203 val loss: 0.695980
[Epoch 14] ogbg-molbace: 0.337039 test loss: 0.695870
[Epoch 15; Iter     6/   36] train: loss: 0.6950061
[Epoch 15; Iter    36/   36] train: loss: 0.6923870
[Epoch 15] ogbg-molbace: 0.334280 val loss: 0.695730
[Epoch 15] ogbg-molbace: 0.341549 test loss: 0.695637
[Epoch 16; Iter    30/   36] train: loss: 0.6970193
[Epoch 16] ogbg-molbace: 0.335227 val loss: 0.695603
[Epoch 16] ogbg-molbace: 0.344503 test loss: 0.695553
[Epoch 17; Iter    24/   36] train: loss: 0.6952190
[Epoch 17] ogbg-molbace: 0.337358 val loss: 0.695158
[Epoch 17] ogbg-molbace: 0.349246 test loss: 0.695068
[Epoch 18; Iter    18/   36] train: loss: 0.6920815
[Epoch 18] ogbg-molbace: 0.342093 val loss: 0.695055
[Epoch 18] ogbg-molbace: 0.354610 test loss: 0.695093
[Epoch 19; Iter    12/   36] train: loss: 0.6938270
[Epoch 19] ogbg-molbace: 0.341067 val loss: 0.694745
[Epoch 19] ogbg-molbace: 0.356476 test loss: 0.694833
[Epoch 20; Iter     6/   36] train: loss: 0.6903539
[Epoch 20; Iter    36/   36] train: loss: 0.6650698
[Epoch 20] ogbg-molbace: 0.738005 val loss: 0.679411
[Epoch 20] ogbg-molbace: 0.730446 test loss: 0.679384
[Epoch 21; Iter    30/   36] train: loss: 0.6592738
[Epoch 21] ogbg-molbace: 0.802478 val loss: 0.646181
[Epoch 21] ogbg-molbace: 0.806251 test loss: 0.640267
[Epoch 22; Iter    24/   36] train: loss: 0.6372765
[Epoch 22] ogbg-molbace: 0.803977 val loss: 0.625596
[Epoch 22] ogbg-molbace: 0.809439 test loss: 0.621214
[Epoch 23; Iter    18/   36] train: loss: 0.6276747
[Epoch 23] ogbg-molbace: 0.750710 val loss: 0.618885
[Epoch 23] ogbg-molbace: 0.741642 test loss: 0.620541
[Epoch 24; Iter    12/   36] train: loss: 0.6200123
[Epoch 24] ogbg-molbace: 0.826231 val loss: 0.555058
[Epoch 24] ogbg-molbace: 0.849246 test loss: 0.533966
[Epoch 25; Iter     6/   36] train: loss: 0.5651430
[Epoch 25; Iter    36/   36] train: loss: 0.6097288
[Epoch 25] ogbg-molbace: 0.836963 val loss: 0.548084
[Epoch 25] ogbg-molbace: 0.824911 test loss: 0.545689
[Epoch 26; Iter    30/   36] train: loss: 0.4722401
[Epoch 26] ogbg-molbace: 0.835148 val loss: 0.530635
[Epoch 26] ogbg-molbace: 0.828720 test loss: 0.518715
[Epoch 27; Iter    24/   36] train: loss: 0.5463022
[Epoch 27] ogbg-molbace: 0.745423 val loss: 0.622059
[Epoch 27] ogbg-molbace: 0.734256 test loss: 0.620557
[Epoch 28; Iter    18/   36] train: loss: 0.5368805
[Epoch 28] ogbg-molbace: 0.805713 val loss: 0.554960
[Epoch 28] ogbg-molbace: 0.799720 test loss: 0.545682
[Epoch 29; Iter    12/   36] train: loss: 0.4011363
[Epoch 29] ogbg-molbace: 0.844145 val loss: 0.527779
[Epoch 29] ogbg-molbace: 0.838128 test loss: 0.514478
[Epoch 30; Iter     6/   36] train: loss: 0.6025798
[Epoch 30; Iter    36/   36] train: loss: 0.5045491
[Epoch 30] ogbg-molbace: 0.847301 val loss: 0.537303
[Epoch 30] ogbg-molbace: 0.847846 test loss: 0.534374
[Epoch 31; Iter    30/   36] train: loss: 0.5186197
[Epoch 31] ogbg-molbace: 0.844776 val loss: 0.523964
[Epoch 31] ogbg-molbace: 0.839294 test loss: 0.512027
[Epoch 32; Iter    24/   36] train: loss: 0.7708670
[Epoch 32] ogbg-molbace: 0.825442 val loss: 0.557672
[Epoch 32] ogbg-molbace: 0.805707 test loss: 0.553981
[Epoch 33; Iter    18/   36] train: loss: 0.5027319
[Epoch 33] ogbg-molbace: 0.846591 val loss: 0.530248
[Epoch 33] ogbg-molbace: 0.841471 test loss: 0.525138
[Epoch 34; Iter    12/   36] train: loss: 0.4284683
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.8/PNA_ogbg-molbace_GraphCL_bace_random=0.8_6_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.8
logdir: runs/split/GraphCL/bace/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6969405
[Epoch 1] ogbg-molbace: 0.349684 val loss: 0.693862
[Epoch 1] ogbg-molbace: 0.309921 test loss: 0.693816
[Epoch 2; Iter    19/   41] train: loss: 0.6975721
[Epoch 2] ogbg-molbace: 0.363045 val loss: 0.695959
[Epoch 2] ogbg-molbace: 0.313608 test loss: 0.695519
[Epoch 3; Iter     8/   41] train: loss: 0.6953120
[Epoch 3; Iter    38/   41] train: loss: 0.6927821
[Epoch 3] ogbg-molbace: 0.377989 val loss: 0.696515
[Epoch 3] ogbg-molbace: 0.322037 test loss: 0.695629
[Epoch 4; Iter    27/   41] train: loss: 0.6980303
[Epoch 4] ogbg-molbace: 0.382208 val loss: 0.696691
[Epoch 4] ogbg-molbace: 0.324846 test loss: 0.695004
[Epoch 5; Iter    16/   41] train: loss: 0.6931832
[Epoch 5] ogbg-molbace: 0.382736 val loss: 0.696742
[Epoch 5] ogbg-molbace: 0.332046 test loss: 0.694953
[Epoch 6; Iter     5/   41] train: loss: 0.7005410
[Epoch 6; Iter    35/   41] train: loss: 0.6944423
[Epoch 6] ogbg-molbace: 0.382560 val loss: 0.696764
[Epoch 6] ogbg-molbace: 0.322564 test loss: 0.695051
[Epoch 7; Iter    24/   41] train: loss: 0.6958288
[Epoch 7] ogbg-molbace: 0.376934 val loss: 0.696810
[Epoch 7] ogbg-molbace: 0.320983 test loss: 0.694838
[Epoch 8; Iter    13/   41] train: loss: 0.6972665
[Epoch 8] ogbg-molbace: 0.373769 val loss: 0.697023
[Epoch 8] ogbg-molbace: 0.323090 test loss: 0.694527
[Epoch 9; Iter     2/   41] train: loss: 0.6960711
[Epoch 9; Iter    32/   41] train: loss: 0.6947266
[Epoch 9] ogbg-molbace: 0.375879 val loss: 0.697112
[Epoch 9] ogbg-molbace: 0.328183 test loss: 0.694285
[Epoch 10; Iter    21/   41] train: loss: 0.6949697
[Epoch 10] ogbg-molbace: 0.382560 val loss: 0.697385
[Epoch 10] ogbg-molbace: 0.330817 test loss: 0.693531
[Epoch 11; Iter    10/   41] train: loss: 0.6936648
[Epoch 11; Iter    40/   41] train: loss: 0.6980581
[Epoch 11] ogbg-molbace: 0.379571 val loss: 0.697276
[Epoch 11] ogbg-molbace: 0.323793 test loss: 0.693673
[Epoch 12; Iter    29/   41] train: loss: 0.6980103
[Epoch 12] ogbg-molbace: 0.379044 val loss: 0.697581
[Epoch 12] ogbg-molbace: 0.322037 test loss: 0.693176
[Epoch 13; Iter    18/   41] train: loss: 0.6931178
[Epoch 13] ogbg-molbace: 0.376582 val loss: 0.697922
[Epoch 13] ogbg-molbace: 0.331870 test loss: 0.692641
[Epoch 14; Iter     7/   41] train: loss: 0.6912960
[Epoch 14; Iter    37/   41] train: loss: 0.6896162
[Epoch 14] ogbg-molbace: 0.376582 val loss: 0.697781
[Epoch 14] ogbg-molbace: 0.329939 test loss: 0.692441
[Epoch 15; Iter    26/   41] train: loss: 0.6934042
[Epoch 15] ogbg-molbace: 0.379747 val loss: 0.698053
[Epoch 15] ogbg-molbace: 0.336260 test loss: 0.691685
[Epoch 16; Iter    15/   41] train: loss: 0.6937108
[Epoch 16] ogbg-molbace: 0.382032 val loss: 0.698212
[Epoch 16] ogbg-molbace: 0.341001 test loss: 0.691492
[Epoch 17; Iter     4/   41] train: loss: 0.6969360
[Epoch 17; Iter    34/   41] train: loss: 0.6919531
[Epoch 17] ogbg-molbace: 0.608298 val loss: 0.687568
[Epoch 17] ogbg-molbace: 0.611940 test loss: 0.690784
[Epoch 18; Iter    23/   41] train: loss: 0.6706480
[Epoch 18] ogbg-molbace: 0.735759 val loss: 0.664937
[Epoch 18] ogbg-molbace: 0.757507 test loss: 0.654983
[Epoch 19; Iter    12/   41] train: loss: 0.6640599
[Epoch 19] ogbg-molbace: 0.766878 val loss: 0.626962
[Epoch 19] ogbg-molbace: 0.765057 test loss: 0.616678
[Epoch 20; Iter     1/   41] train: loss: 0.6165665
[Epoch 20; Iter    31/   41] train: loss: 0.5834126
[Epoch 20] ogbg-molbace: 0.819444 val loss: 0.557213
[Epoch 20] ogbg-molbace: 0.827744 test loss: 0.566356
[Epoch 21; Iter    20/   41] train: loss: 0.5522494
[Epoch 21] ogbg-molbace: 0.828938 val loss: 0.550143
[Epoch 21] ogbg-molbace: 0.803687 test loss: 0.567165
[Epoch 22; Iter     9/   41] train: loss: 0.5355231
[Epoch 22; Iter    39/   41] train: loss: 0.5014116
[Epoch 22] ogbg-molbace: 0.842124 val loss: 0.512218
[Epoch 22] ogbg-molbace: 0.822125 test loss: 0.563308
[Epoch 23; Iter    28/   41] train: loss: 0.4937758
[Epoch 23] ogbg-molbace: 0.856188 val loss: 0.477186
[Epoch 23] ogbg-molbace: 0.845478 test loss: 0.446533
[Epoch 24; Iter    17/   41] train: loss: 0.4449240
[Epoch 24] ogbg-molbace: 0.766350 val loss: 0.560008
[Epoch 24] ogbg-molbace: 0.775066 test loss: 0.538327
[Epoch 25; Iter     6/   41] train: loss: 0.5062234
[Epoch 25; Iter    36/   41] train: loss: 0.5014051
[Epoch 25] ogbg-molbace: 0.883439 val loss: 0.459162
[Epoch 25] ogbg-molbace: 0.851624 test loss: 0.460004
[Epoch 26; Iter    25/   41] train: loss: 0.3969993
[Epoch 26] ogbg-molbace: 0.857068 val loss: 0.477331
[Epoch 26] ogbg-molbace: 0.841967 test loss: 0.546271
[Epoch 27; Iter    14/   41] train: loss: 0.4746106
[Epoch 27] ogbg-molbace: 0.841948 val loss: 0.503268
[Epoch 27] ogbg-molbace: 0.824407 test loss: 0.445047
[Epoch 28; Iter     3/   41] train: loss: 0.4915861
[Epoch 28; Iter    33/   41] train: loss: 0.5534521
[Epoch 28] ogbg-molbace: 0.882208 val loss: 0.442866
[Epoch 28] ogbg-molbace: 0.850219 test loss: 0.476739
[Epoch 29; Iter    22/   41] train: loss: 0.3482172
[Epoch 29] ogbg-molbace: 0.848277 val loss: 0.460834
[Epoch 29] ogbg-molbace: 0.812643 test loss: 0.462735
[Epoch 30; Iter    11/   41] train: loss: 0.6179512
[Epoch 30; Iter    41/   41] train: loss: 0.4634510
[Epoch 30] ogbg-molbace: 0.875527 val loss: 0.491582
[Epoch 30] ogbg-molbace: 0.854609 test loss: 0.416588
[Epoch 31; Iter    30/   41] train: loss: 0.5120866
[Epoch 31] ogbg-molbace: 0.866034 val loss: 0.512157
[Epoch 31] ogbg-molbace: 0.827568 test loss: 0.449177
[Epoch 32; Iter    19/   41] train: loss: 0.4862331
[Epoch 32] ogbg-molbace: 0.862518 val loss: 0.447088
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.6/PNA_ogbg-molbace_GraphCL_bace_random=0.6_6_26-05_09-38-00
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.6
logdir: runs/split/GraphCL/bace/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6961467
[Epoch 1] ogbg-molbace: 0.324410 val loss: 0.693691
[Epoch 1] ogbg-molbace: 0.333566 test loss: 0.693545
[Epoch 2; Iter    29/   31] train: loss: 0.6922985
[Epoch 2] ogbg-molbace: 0.323469 val loss: 0.695600
[Epoch 2] ogbg-molbace: 0.335922 test loss: 0.694896
[Epoch 3; Iter    28/   31] train: loss: 0.6963475
[Epoch 3] ogbg-molbace: 0.330643 val loss: 0.697371
[Epoch 3] ogbg-molbace: 0.344560 test loss: 0.695961
[Epoch 4; Iter    27/   31] train: loss: 0.6956800
[Epoch 4] ogbg-molbace: 0.330239 val loss: 0.697725
[Epoch 4] ogbg-molbace: 0.351976 test loss: 0.696061
[Epoch 5; Iter    26/   31] train: loss: 0.6967772
[Epoch 5] ogbg-molbace: 0.329118 val loss: 0.697402
[Epoch 5] ogbg-molbace: 0.349577 test loss: 0.695916
[Epoch 6; Iter    25/   31] train: loss: 0.6958101
[Epoch 6] ogbg-molbace: 0.328446 val loss: 0.697221
[Epoch 6] ogbg-molbace: 0.347047 test loss: 0.695903
[Epoch 7; Iter    24/   31] train: loss: 0.6958124
[Epoch 7] ogbg-molbace: 0.328760 val loss: 0.697514
[Epoch 7] ogbg-molbace: 0.350624 test loss: 0.696000
[Epoch 8; Iter    23/   31] train: loss: 0.6952715
[Epoch 8] ogbg-molbace: 0.331585 val loss: 0.697314
[Epoch 8] ogbg-molbace: 0.351060 test loss: 0.695867
[Epoch 9; Iter    22/   31] train: loss: 0.6958161
[Epoch 9] ogbg-molbace: 0.337145 val loss: 0.697077
[Epoch 9] ogbg-molbace: 0.352020 test loss: 0.695804
[Epoch 10; Iter    21/   31] train: loss: 0.6950154
[Epoch 10] ogbg-molbace: 0.330284 val loss: 0.697325
[Epoch 10] ogbg-molbace: 0.348530 test loss: 0.695973
[Epoch 11; Iter    20/   31] train: loss: 0.6967009
[Epoch 11] ogbg-molbace: 0.332168 val loss: 0.697021
[Epoch 11] ogbg-molbace: 0.351715 test loss: 0.695634
[Epoch 12; Iter    19/   31] train: loss: 0.6930246
[Epoch 12] ogbg-molbace: 0.334230 val loss: 0.696816
[Epoch 12] ogbg-molbace: 0.353329 test loss: 0.695464
[Epoch 13; Iter    18/   31] train: loss: 0.6969811
[Epoch 13] ogbg-molbace: 0.339925 val loss: 0.696564
[Epoch 13] ogbg-molbace: 0.352849 test loss: 0.695265
[Epoch 14; Iter    17/   31] train: loss: 0.6966115
[Epoch 14] ogbg-molbace: 0.337189 val loss: 0.696329
[Epoch 14] ogbg-molbace: 0.346741 test loss: 0.695212
[Epoch 15; Iter    16/   31] train: loss: 0.6905547
[Epoch 15] ogbg-molbace: 0.336158 val loss: 0.696225
[Epoch 15] ogbg-molbace: 0.348792 test loss: 0.695046
[Epoch 16; Iter    15/   31] train: loss: 0.6933481
[Epoch 16] ogbg-molbace: 0.335575 val loss: 0.696210
[Epoch 16] ogbg-molbace: 0.351845 test loss: 0.694976
[Epoch 17; Iter    14/   31] train: loss: 0.6922728
[Epoch 17] ogbg-molbace: 0.340373 val loss: 0.695789
[Epoch 17] ogbg-molbace: 0.349184 test loss: 0.694787
[Epoch 18; Iter    13/   31] train: loss: 0.6963187
[Epoch 18] ogbg-molbace: 0.344947 val loss: 0.695593
[Epoch 18] ogbg-molbace: 0.351016 test loss: 0.694615
[Epoch 19; Iter    12/   31] train: loss: 0.6969208
[Epoch 19] ogbg-molbace: 0.353421 val loss: 0.695225
[Epoch 19] ogbg-molbace: 0.352020 test loss: 0.694398
[Epoch 20; Iter    11/   31] train: loss: 0.6966270
[Epoch 20] ogbg-molbace: 0.355887 val loss: 0.695198
[Epoch 20] ogbg-molbace: 0.355684 test loss: 0.694307
[Epoch 21; Iter    10/   31] train: loss: 0.6922480
[Epoch 21] ogbg-molbace: 0.346964 val loss: 0.694937
[Epoch 21] ogbg-molbace: 0.350798 test loss: 0.694174
[Epoch 22; Iter     9/   31] train: loss: 0.6958871
[Epoch 22] ogbg-molbace: 0.369698 val loss: 0.694581
[Epoch 22] ogbg-molbace: 0.356077 test loss: 0.693844
[Epoch 23; Iter     8/   31] train: loss: 0.6889281
[Epoch 23] ogbg-molbace: 0.727827 val loss: 0.677969
[Epoch 23] ogbg-molbace: 0.671364 test loss: 0.679058
[Epoch 24; Iter     7/   31] train: loss: 0.6778407
[Epoch 24] ogbg-molbace: 0.793247 val loss: 0.657476
[Epoch 24] ogbg-molbace: 0.783134 test loss: 0.659262
[Epoch 25; Iter     6/   31] train: loss: 0.6469482
[Epoch 25] ogbg-molbace: 0.770424 val loss: 0.630467
[Epoch 25] ogbg-molbace: 0.742954 test loss: 0.634838
[Epoch 26; Iter     5/   31] train: loss: 0.7163380
[Epoch 26] ogbg-molbace: 0.805399 val loss: 0.596277
[Epoch 26] ogbg-molbace: 0.785054 test loss: 0.596917
[Epoch 27; Iter     4/   31] train: loss: 0.6014004
[Epoch 27] ogbg-molbace: 0.834858 val loss: 0.553642
[Epoch 27] ogbg-molbace: 0.836838 test loss: 0.548293
[Epoch 28; Iter     3/   31] train: loss: 0.5575615
[Epoch 28] ogbg-molbace: 0.795534 val loss: 0.561192
[Epoch 28] ogbg-molbace: 0.765640 test loss: 0.569815
[Epoch 29; Iter     2/   31] train: loss: 0.5916219
[Epoch 29] ogbg-molbace: 0.823693 val loss: 0.535428
[Epoch 29] ogbg-molbace: 0.801457 test loss: 0.541244
[Epoch 30; Iter     1/   31] train: loss: 0.5180918
[Epoch 30; Iter    31/   31] train: loss: 0.5085572
[Epoch 30] ogbg-molbace: 0.852793 val loss: 0.518304
[Epoch 30] ogbg-molbace: 0.847483 test loss: 0.506068
[Epoch 31; Iter    30/   31] train: loss: 0.6418256
[Epoch 31] ogbg-molbace: 0.840821 val loss: 0.496713
[Epoch 31] ogbg-molbace: 0.843164 test loss: 0.491079
[Epoch 32; Iter    29/   31] train: loss: 0.4531857
[Epoch 32] ogbg-molbace: 0.845978 val loss: 0.527845
[Epoch 32] ogbg-molbace: 0.831908 test loss: 0.536111
[Epoch 33; Iter    28/   31] train: loss: 0.4118346
[Epoch 33] ogbg-molbace: 0.857233 val loss: 0.483668
[Epoch 33] ogbg-molbace: 0.858782 test loss: 0.470823
[Epoch 34; Iter    27/   31] train: loss: 0.5186300
[Epoch 34] ogbg-molbace: 0.834320 val loss: 0.580095
[Epoch 34] ogbg-molbace: 0.819562 test loss: 0.590276
[Epoch 35; Iter    26/   31] train: loss: 0.4770042
[Epoch 35] ogbg-molbace: 0.864093 val loss: 0.476874
[Epoch 35] ogbg-molbace: 0.866111 test loss: 0.472592
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.6/PNA_ogbg-molbace_GraphCL_bace_random=0.6_5_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.6
logdir: runs/split/GraphCL/bace/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6911591
[Epoch 1] ogbg-molbace: 0.464443 val loss: 0.693312
[Epoch 1] ogbg-molbace: 0.518323 test loss: 0.693249
[Epoch 2; Iter    29/   31] train: loss: 0.6951883
[Epoch 2] ogbg-molbace: 0.471572 val loss: 0.693500
[Epoch 2] ogbg-molbace: 0.520461 test loss: 0.693566
[Epoch 3; Iter    28/   31] train: loss: 0.6926768
[Epoch 3] ogbg-molbace: 0.433190 val loss: 0.694084
[Epoch 3] ogbg-molbace: 0.473519 test loss: 0.694252
[Epoch 4; Iter    27/   31] train: loss: 0.6916895
[Epoch 4] ogbg-molbace: 0.422922 val loss: 0.694155
[Epoch 4] ogbg-molbace: 0.458424 test loss: 0.694331
[Epoch 5; Iter    26/   31] train: loss: 0.6967655
[Epoch 5] ogbg-molbace: 0.414088 val loss: 0.694286
[Epoch 5] ogbg-molbace: 0.454454 test loss: 0.694465
[Epoch 6; Iter    25/   31] train: loss: 0.6942620
[Epoch 6] ogbg-molbace: 0.417765 val loss: 0.694111
[Epoch 6] ogbg-molbace: 0.455370 test loss: 0.694300
[Epoch 7; Iter    24/   31] train: loss: 0.6937192
[Epoch 7] ogbg-molbace: 0.421352 val loss: 0.694137
[Epoch 7] ogbg-molbace: 0.460998 test loss: 0.694264
[Epoch 8; Iter    23/   31] train: loss: 0.6949116
[Epoch 8] ogbg-molbace: 0.419245 val loss: 0.694177
[Epoch 8] ogbg-molbace: 0.455327 test loss: 0.694359
[Epoch 9; Iter    22/   31] train: loss: 0.6924235
[Epoch 9] ogbg-molbace: 0.430275 val loss: 0.693829
[Epoch 9] ogbg-molbace: 0.465841 test loss: 0.694048
[Epoch 10; Iter    21/   31] train: loss: 0.6966973
[Epoch 10] ogbg-molbace: 0.442247 val loss: 0.693678
[Epoch 10] ogbg-molbace: 0.477533 test loss: 0.693975
[Epoch 11; Iter    20/   31] train: loss: 0.6953730
[Epoch 11] ogbg-molbace: 0.446417 val loss: 0.693493
[Epoch 11] ogbg-molbace: 0.477707 test loss: 0.693833
[Epoch 12; Iter    19/   31] train: loss: 0.6896248
[Epoch 12] ogbg-molbace: 0.441575 val loss: 0.693535
[Epoch 12] ogbg-molbace: 0.474217 test loss: 0.693871
[Epoch 13; Iter    18/   31] train: loss: 0.6927100
[Epoch 13] ogbg-molbace: 0.447449 val loss: 0.693391
[Epoch 13] ogbg-molbace: 0.475046 test loss: 0.693746
[Epoch 14; Iter    17/   31] train: loss: 0.6916088
[Epoch 14] ogbg-molbace: 0.464891 val loss: 0.692990
[Epoch 14] ogbg-molbace: 0.489442 test loss: 0.693394
[Epoch 15; Iter    16/   31] train: loss: 0.6920906
[Epoch 15] ogbg-molbace: 0.445834 val loss: 0.693140
[Epoch 15] ogbg-molbace: 0.480106 test loss: 0.693497
[Epoch 16; Iter    15/   31] train: loss: 0.6946241
[Epoch 16] ogbg-molbace: 0.459331 val loss: 0.692903
[Epoch 16] ogbg-molbace: 0.485123 test loss: 0.693317
[Epoch 17; Iter    14/   31] train: loss: 0.6934069
[Epoch 17] ogbg-molbace: 0.483365 val loss: 0.692668
[Epoch 17] ogbg-molbace: 0.499869 test loss: 0.693115
[Epoch 18; Iter    13/   31] train: loss: 0.6926003
[Epoch 18] ogbg-molbace: 0.485113 val loss: 0.692593
[Epoch 18] ogbg-molbace: 0.511910 test loss: 0.692836
[Epoch 19; Iter    12/   31] train: loss: 0.6901650
[Epoch 19] ogbg-molbace: 0.493409 val loss: 0.692332
[Epoch 19] ogbg-molbace: 0.509467 test loss: 0.692759
[Epoch 20; Iter    11/   31] train: loss: 0.6885812
[Epoch 20] ogbg-molbace: 0.507398 val loss: 0.692083
[Epoch 20] ogbg-molbace: 0.514004 test loss: 0.692572
[Epoch 21; Iter    10/   31] train: loss: 0.6906742
[Epoch 21] ogbg-molbace: 0.519729 val loss: 0.691815
[Epoch 21] ogbg-molbace: 0.528357 test loss: 0.692325
[Epoch 22; Iter     9/   31] train: loss: 0.6920657
[Epoch 22] ogbg-molbace: 0.559546 val loss: 0.691539
[Epoch 22] ogbg-molbace: 0.555274 test loss: 0.692084
[Epoch 23; Iter     8/   31] train: loss: 0.6948113
[Epoch 23] ogbg-molbace: 0.783383 val loss: 0.682963
[Epoch 23] ogbg-molbace: 0.750807 test loss: 0.683178
[Epoch 24; Iter     7/   31] train: loss: 0.6832748
[Epoch 24] ogbg-molbace: 0.816743 val loss: 0.660264
[Epoch 24] ogbg-molbace: 0.813280 test loss: 0.657164
[Epoch 25; Iter     6/   31] train: loss: 0.6394994
[Epoch 25] ogbg-molbace: 0.789257 val loss: 0.634065
[Epoch 25] ogbg-molbace: 0.750807 test loss: 0.633940
[Epoch 26; Iter     5/   31] train: loss: 0.6319115
[Epoch 26] ogbg-molbace: 0.828760 val loss: 0.628048
[Epoch 26] ogbg-molbace: 0.819606 test loss: 0.622476
[Epoch 27; Iter     4/   31] train: loss: 0.6419203
[Epoch 27] ogbg-molbace: 0.811766 val loss: 0.588875
[Epoch 27] ogbg-molbace: 0.779120 test loss: 0.584908
[Epoch 28; Iter     3/   31] train: loss: 0.6263818
[Epoch 28] ogbg-molbace: 0.827549 val loss: 0.578628
[Epoch 28] ogbg-molbace: 0.824841 test loss: 0.575196
[Epoch 29; Iter     2/   31] train: loss: 0.6491237
[Epoch 29] ogbg-molbace: 0.806699 val loss: 0.617840
[Epoch 29] ogbg-molbace: 0.829422 test loss: 0.604468
[Epoch 30; Iter     1/   31] train: loss: 0.5149706
[Epoch 30; Iter    31/   31] train: loss: 0.7136217
[Epoch 30] ogbg-molbace: 0.815487 val loss: 0.545632
[Epoch 30] ogbg-molbace: 0.801675 test loss: 0.533033
[Epoch 31; Iter    30/   31] train: loss: 0.5824591
[Epoch 31] ogbg-molbace: 0.846740 val loss: 0.509701
[Epoch 31] ogbg-molbace: 0.834962 test loss: 0.499134
[Epoch 32; Iter    29/   31] train: loss: 0.5565257
[Epoch 32] ogbg-molbace: 0.834858 val loss: 0.529476
[Epoch 32] ogbg-molbace: 0.820958 test loss: 0.526444
[Epoch 33; Iter    28/   31] train: loss: 0.5517619
[Epoch 33] ogbg-molbace: 0.858847 val loss: 0.495015
[Epoch 33] ogbg-molbace: 0.864759 test loss: 0.475885
[Epoch 34; Iter    27/   31] train: loss: 0.6385260
[Epoch 34] ogbg-molbace: 0.815936 val loss: 0.526942
[Epoch 34] ogbg-molbace: 0.820696 test loss: 0.511563
[Epoch 35; Iter    26/   31] train: loss: 0.4067952
[Epoch 35] ogbg-molbace: 0.835844 val loss: 0.518532
[Epoch 35] ogbg-molbace: 0.813193 test loss: 0.510312
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.7/PNA_ogbg-molbace_GraphCL_bace_random=0.7_4_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.7
logdir: runs/split/GraphCL/bace/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6982856
[Epoch 1] ogbg-molbace: 0.425900 val loss: 0.693556
[Epoch 1] ogbg-molbace: 0.526590 test loss: 0.693075
[Epoch 2; Iter    24/   36] train: loss: 0.6994110
[Epoch 2] ogbg-molbace: 0.403172 val loss: 0.694978
[Epoch 2] ogbg-molbace: 0.468512 test loss: 0.693832
[Epoch 3; Iter    18/   36] train: loss: 0.6999974
[Epoch 3] ogbg-molbace: 0.397491 val loss: 0.695506
[Epoch 3] ogbg-molbace: 0.453895 test loss: 0.694403
[Epoch 4; Iter    12/   36] train: loss: 0.6958095
[Epoch 4] ogbg-molbace: 0.396228 val loss: 0.695531
[Epoch 4] ogbg-molbace: 0.450785 test loss: 0.694517
[Epoch 5; Iter     6/   36] train: loss: 0.6960931
[Epoch 5; Iter    36/   36] train: loss: 0.6905063
[Epoch 5] ogbg-molbace: 0.401594 val loss: 0.695363
[Epoch 5] ogbg-molbace: 0.457627 test loss: 0.694296
[Epoch 6; Iter    30/   36] train: loss: 0.6914454
[Epoch 6] ogbg-molbace: 0.402699 val loss: 0.695031
[Epoch 6] ogbg-molbace: 0.459338 test loss: 0.694028
[Epoch 7; Iter    24/   36] train: loss: 0.6932794
[Epoch 7] ogbg-molbace: 0.399779 val loss: 0.695259
[Epoch 7] ogbg-molbace: 0.458327 test loss: 0.694260
[Epoch 8; Iter    18/   36] train: loss: 0.6951345
[Epoch 8] ogbg-molbace: 0.402620 val loss: 0.694952
[Epoch 8] ogbg-molbace: 0.463536 test loss: 0.694071
[Epoch 9; Iter    12/   36] train: loss: 0.6927793
[Epoch 9] ogbg-molbace: 0.402383 val loss: 0.694976
[Epoch 9] ogbg-molbace: 0.459960 test loss: 0.694092
[Epoch 10; Iter     6/   36] train: loss: 0.6949911
[Epoch 10; Iter    36/   36] train: loss: 0.7068154
[Epoch 10] ogbg-molbace: 0.399384 val loss: 0.694900
[Epoch 10] ogbg-molbace: 0.456072 test loss: 0.694082
[Epoch 11; Iter    30/   36] train: loss: 0.6934005
[Epoch 11] ogbg-molbace: 0.406881 val loss: 0.694530
[Epoch 11] ogbg-molbace: 0.467968 test loss: 0.693762
[Epoch 12; Iter    24/   36] train: loss: 0.6934407
[Epoch 12] ogbg-molbace: 0.412721 val loss: 0.694320
[Epoch 12] ogbg-molbace: 0.475043 test loss: 0.693561
[Epoch 13; Iter    18/   36] train: loss: 0.6946934
[Epoch 13] ogbg-molbace: 0.410590 val loss: 0.694260
[Epoch 13] ogbg-molbace: 0.471544 test loss: 0.693550
[Epoch 14; Iter    12/   36] train: loss: 0.6972840
[Epoch 14] ogbg-molbace: 0.408617 val loss: 0.694124
[Epoch 14] ogbg-molbace: 0.471777 test loss: 0.693509
[Epoch 15; Iter     6/   36] train: loss: 0.6996387
[Epoch 15; Iter    36/   36] train: loss: 0.6851021
[Epoch 15] ogbg-molbace: 0.414694 val loss: 0.693760
[Epoch 15] ogbg-molbace: 0.476442 test loss: 0.693319
[Epoch 16; Iter    30/   36] train: loss: 0.6982439
[Epoch 16] ogbg-molbace: 0.420297 val loss: 0.693536
[Epoch 16] ogbg-molbace: 0.483284 test loss: 0.693097
[Epoch 17; Iter    24/   36] train: loss: 0.6978250
[Epoch 17] ogbg-molbace: 0.419429 val loss: 0.693410
[Epoch 17] ogbg-molbace: 0.487560 test loss: 0.693011
[Epoch 18; Iter    18/   36] train: loss: 0.6919854
[Epoch 18] ogbg-molbace: 0.419586 val loss: 0.693345
[Epoch 18] ogbg-molbace: 0.491137 test loss: 0.692983
[Epoch 19; Iter    12/   36] train: loss: 0.6866037
[Epoch 19] ogbg-molbace: 0.436395 val loss: 0.692934
[Epoch 19] ogbg-molbace: 0.510807 test loss: 0.692606
[Epoch 20; Iter     6/   36] train: loss: 0.6927525
[Epoch 20; Iter    36/   36] train: loss: 0.7054072
[Epoch 20] ogbg-molbace: 0.774384 val loss: 0.674785
[Epoch 20] ogbg-molbace: 0.766988 test loss: 0.672886
[Epoch 21; Iter    30/   36] train: loss: 0.6685525
[Epoch 21] ogbg-molbace: 0.774542 val loss: 0.646542
[Epoch 21] ogbg-molbace: 0.777640 test loss: 0.644004
[Epoch 22; Iter    24/   36] train: loss: 0.6489674
[Epoch 22] ogbg-molbace: 0.795849 val loss: 0.610545
[Epoch 22] ogbg-molbace: 0.784170 test loss: 0.611430
[Epoch 23; Iter    18/   36] train: loss: 0.5906125
[Epoch 23] ogbg-molbace: 0.730587 val loss: 0.613472
[Epoch 23] ogbg-molbace: 0.709532 test loss: 0.617531
[Epoch 24; Iter    12/   36] train: loss: 0.4778146
[Epoch 24] ogbg-molbace: 0.833254 val loss: 0.577379
[Epoch 24] ogbg-molbace: 0.845903 test loss: 0.555433
[Epoch 25; Iter     6/   36] train: loss: 0.4812051
[Epoch 25; Iter    36/   36] train: loss: 0.4933258
[Epoch 25] ogbg-molbace: 0.826626 val loss: 0.546607
[Epoch 25] ogbg-molbace: 0.824522 test loss: 0.529546
[Epoch 26; Iter    30/   36] train: loss: 0.5844143
[Epoch 26] ogbg-molbace: 0.850300 val loss: 0.521951
[Epoch 26] ogbg-molbace: 0.846680 test loss: 0.508966
[Epoch 27; Iter    24/   36] train: loss: 0.5353135
[Epoch 27] ogbg-molbace: 0.854246 val loss: 0.517748
[Epoch 27] ogbg-molbace: 0.863318 test loss: 0.492110
[Epoch 28; Iter    18/   36] train: loss: 0.6683553
[Epoch 28] ogbg-molbace: 0.782592 val loss: 0.594862
[Epoch 28] ogbg-molbace: 0.762712 test loss: 0.595490
[Epoch 29; Iter    12/   36] train: loss: 0.4478799
[Epoch 29] ogbg-molbace: 0.834833 val loss: 0.538555
[Epoch 29] ogbg-molbace: 0.826388 test loss: 0.530744
[Epoch 30; Iter     6/   36] train: loss: 0.3736239
[Epoch 30; Iter    36/   36] train: loss: 0.4702291
[Epoch 30] ogbg-molbace: 0.832781 val loss: 0.520983
[Epoch 30] ogbg-molbace: 0.849090 test loss: 0.496668
[Epoch 31; Iter    30/   36] train: loss: 0.4684857
[Epoch 31] ogbg-molbace: 0.832623 val loss: 0.535456
[Epoch 31] ogbg-molbace: 0.826543 test loss: 0.527351
[Epoch 32; Iter    24/   36] train: loss: 0.4018400
[Epoch 32] ogbg-molbace: 0.770597 val loss: 0.624192
[Epoch 32] ogbg-molbace: 0.760846 test loss: 0.619292
[Epoch 33; Iter    18/   36] train: loss: 0.5606089
[Epoch 33] ogbg-molbace: 0.862137 val loss: 0.502941
[Epoch 33] ogbg-molbace: 0.856088 test loss: 0.497397
[Epoch 34; Iter    12/   36] train: loss: 0.3776035
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bace/random/train_prop=0.6/PNA_ogbg-molbace_GraphCL_bace_random=0.6_4_26-05_09-38-00
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_random=0.6
logdir: runs/split/GraphCL/bace/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6904262
[Epoch 1] ogbg-molbace: 0.414851 val loss: 0.693531
[Epoch 1] ogbg-molbace: 0.526176 test loss: 0.693316
[Epoch 2; Iter    29/   31] train: loss: 0.6918616
[Epoch 2] ogbg-molbace: 0.386064 val loss: 0.694910
[Epoch 2] ogbg-molbace: 0.474042 test loss: 0.694012
[Epoch 3; Iter    28/   31] train: loss: 0.6963159
[Epoch 3] ogbg-molbace: 0.371177 val loss: 0.696005
[Epoch 3] ogbg-molbace: 0.447387 test loss: 0.694760
[Epoch 4; Iter    27/   31] train: loss: 0.6951498
[Epoch 4] ogbg-molbace: 0.377051 val loss: 0.696134
[Epoch 4] ogbg-molbace: 0.454541 test loss: 0.694832
[Epoch 5; Iter    26/   31] train: loss: 0.6933915
[Epoch 5] ogbg-molbace: 0.374092 val loss: 0.696268
[Epoch 5] ogbg-molbace: 0.451095 test loss: 0.694923
[Epoch 6; Iter    25/   31] train: loss: 0.6954743
[Epoch 6] ogbg-molbace: 0.373913 val loss: 0.696271
[Epoch 6] ogbg-molbace: 0.451357 test loss: 0.694940
[Epoch 7; Iter    24/   31] train: loss: 0.6937476
[Epoch 7] ogbg-molbace: 0.371760 val loss: 0.696209
[Epoch 7] ogbg-molbace: 0.447692 test loss: 0.694890
[Epoch 8; Iter    23/   31] train: loss: 0.6987782
[Epoch 8] ogbg-molbace: 0.376199 val loss: 0.696095
[Epoch 8] ogbg-molbace: 0.453494 test loss: 0.694801
[Epoch 9; Iter    22/   31] train: loss: 0.6994824
[Epoch 9] ogbg-molbace: 0.374989 val loss: 0.695913
[Epoch 9] ogbg-molbace: 0.452884 test loss: 0.694658
[Epoch 10; Iter    21/   31] train: loss: 0.6947502
[Epoch 10] ogbg-molbace: 0.377276 val loss: 0.695626
[Epoch 10] ogbg-molbace: 0.453276 test loss: 0.694486
[Epoch 11; Iter    20/   31] train: loss: 0.6964346
[Epoch 11] ogbg-molbace: 0.370146 val loss: 0.695807
[Epoch 11] ogbg-molbace: 0.447561 test loss: 0.694555
[Epoch 12; Iter    19/   31] train: loss: 0.6984704
[Epoch 12] ogbg-molbace: 0.379562 val loss: 0.695490
[Epoch 12] ogbg-molbace: 0.457464 test loss: 0.694308
[Epoch 13; Iter    18/   31] train: loss: 0.6991088
[Epoch 13] ogbg-molbace: 0.388127 val loss: 0.694972
[Epoch 13] ogbg-molbace: 0.465841 test loss: 0.693921
[Epoch 14; Iter    17/   31] train: loss: 0.6951146
[Epoch 14] ogbg-molbace: 0.374137 val loss: 0.695333
[Epoch 14] ogbg-molbace: 0.453538 test loss: 0.694137
[Epoch 15; Iter    16/   31] train: loss: 0.6938861
[Epoch 15] ogbg-molbace: 0.374989 val loss: 0.695237
[Epoch 15] ogbg-molbace: 0.450572 test loss: 0.694091
[Epoch 16; Iter    15/   31] train: loss: 0.6938435
[Epoch 16] ogbg-molbace: 0.382701 val loss: 0.695157
[Epoch 16] ogbg-molbace: 0.462787 test loss: 0.693986
[Epoch 17; Iter    14/   31] train: loss: 0.6975351
[Epoch 17] ogbg-molbace: 0.388351 val loss: 0.694812
[Epoch 17] ogbg-molbace: 0.468022 test loss: 0.693674
[Epoch 18; Iter    13/   31] train: loss: 0.6959062
[Epoch 18] ogbg-molbace: 0.390055 val loss: 0.694674
[Epoch 18] ogbg-molbace: 0.469767 test loss: 0.693568
[Epoch 19; Iter    12/   31] train: loss: 0.6952350
[Epoch 19] ogbg-molbace: 0.390727 val loss: 0.694294
[Epoch 19] ogbg-molbace: 0.472472 test loss: 0.693282
[Epoch 20; Iter    11/   31] train: loss: 0.6947089
[Epoch 20] ogbg-molbace: 0.398933 val loss: 0.694196
[Epoch 20] ogbg-molbace: 0.480106 test loss: 0.693117
[Epoch 21; Iter    10/   31] train: loss: 0.6919559
[Epoch 21] ogbg-molbace: 0.400323 val loss: 0.693703
[Epoch 21] ogbg-molbace: 0.479714 test loss: 0.692837
[Epoch 22; Iter     9/   31] train: loss: 0.6949915
[Epoch 22] ogbg-molbace: 0.416017 val loss: 0.693366
[Epoch 22] ogbg-molbace: 0.496205 test loss: 0.692514
[Epoch 23; Iter     8/   31] train: loss: 0.6902569
[Epoch 23] ogbg-molbace: 0.774325 val loss: 0.678745
[Epoch 23] ogbg-molbace: 0.750894 test loss: 0.678150
[Epoch 24; Iter     7/   31] train: loss: 0.6733987
[Epoch 24] ogbg-molbace: 0.804592 val loss: 0.658559
[Epoch 24] ogbg-molbace: 0.789416 test loss: 0.655879
[Epoch 25; Iter     6/   31] train: loss: 0.6360345
[Epoch 25] ogbg-molbace: 0.821272 val loss: 0.619131
[Epoch 25] ogbg-molbace: 0.802417 test loss: 0.617099
[Epoch 26; Iter     5/   31] train: loss: 0.6751019
[Epoch 26] ogbg-molbace: 0.797014 val loss: 0.593774
[Epoch 26] ogbg-molbace: 0.768301 test loss: 0.592882
[Epoch 27; Iter     4/   31] train: loss: 0.5939144
[Epoch 27] ogbg-molbace: 0.798314 val loss: 0.564022
[Epoch 27] ogbg-molbace: 0.767821 test loss: 0.563356
[Epoch 28; Iter     3/   31] train: loss: 0.6441180
[Epoch 28] ogbg-molbace: 0.837369 val loss: 0.545658
[Epoch 28] ogbg-molbace: 0.821787 test loss: 0.536378
[Epoch 29; Iter     2/   31] train: loss: 0.6578100
[Epoch 29] ogbg-molbace: 0.858936 val loss: 0.531444
[Epoch 29] ogbg-molbace: 0.863319 test loss: 0.532637
[Epoch 30; Iter     1/   31] train: loss: 0.4475743
[Epoch 30; Iter    31/   31] train: loss: 0.5643980
[Epoch 30] ogbg-molbace: 0.809165 val loss: 0.569135
[Epoch 30] ogbg-molbace: 0.776416 test loss: 0.557168
[Epoch 31; Iter    30/   31] train: loss: 0.5656238
[Epoch 31] ogbg-molbace: 0.822976 val loss: 0.522347
[Epoch 31] ogbg-molbace: 0.852238 test loss: 0.489504
[Epoch 32; Iter    29/   31] train: loss: 0.5158323
[Epoch 32] ogbg-molbace: 0.840373 val loss: 0.510619
[Epoch 32] ogbg-molbace: 0.830163 test loss: 0.503315
[Epoch 33; Iter    28/   31] train: loss: 0.4333503
[Epoch 33] ogbg-molbace: 0.846920 val loss: 0.507875
[Epoch 33] ogbg-molbace: 0.858171 test loss: 0.498483
[Epoch 34; Iter    27/   31] train: loss: 0.5887588
[Epoch 34] ogbg-molbace: 0.846561 val loss: 0.515674
[Epoch 34] ogbg-molbace: 0.837449 test loss: 0.502310
[Epoch 35; Iter    26/   31] train: loss: 0.3825844
[Epoch 35] ogbg-molbace: 0.820644 val loss: 0.544438
[Epoch 35] ogbg-molbace: 0.792165 test loss: 0.539766
[Epoch 32] ogbg-molbace: 0.838279 test loss: 0.498402
[Epoch 33; Iter     8/   41] train: loss: 0.5360037
[Epoch 33; Iter    38/   41] train: loss: 0.5595029
[Epoch 33] ogbg-molbace: 0.874648 val loss: 0.452203
[Epoch 33] ogbg-molbace: 0.851800 test loss: 0.419167
[Epoch 34; Iter    27/   41] train: loss: 0.4712071
[Epoch 34] ogbg-molbace: 0.884142 val loss: 0.414237
[Epoch 34] ogbg-molbace: 0.841791 test loss: 0.416425
[Epoch 35; Iter    16/   41] train: loss: 0.4849941
[Epoch 35] ogbg-molbace: 0.899613 val loss: 0.432662
[Epoch 35] ogbg-molbace: 0.840386 test loss: 0.618618
[Epoch 36; Iter     5/   41] train: loss: 0.4174590
[Epoch 36; Iter    35/   41] train: loss: 0.6812435
[Epoch 36] ogbg-molbace: 0.855485 val loss: 0.481694
[Epoch 36] ogbg-molbace: 0.840211 test loss: 0.482418
[Epoch 37; Iter    24/   41] train: loss: 0.6631315
[Epoch 37] ogbg-molbace: 0.895745 val loss: 0.372594
[Epoch 37] ogbg-molbace: 0.844776 test loss: 0.464721
[Epoch 38; Iter    13/   41] train: loss: 0.3673407
[Epoch 38] ogbg-molbace: 0.891350 val loss: 0.375613
[Epoch 38] ogbg-molbace: 0.865145 test loss: 0.387166
[Epoch 39; Iter     2/   41] train: loss: 0.3973894
[Epoch 39; Iter    32/   41] train: loss: 0.5421180
[Epoch 39] ogbg-molbace: 0.858650 val loss: 0.479745
[Epoch 39] ogbg-molbace: 0.864091 test loss: 0.403747
[Epoch 40; Iter    21/   41] train: loss: 0.4212014
[Epoch 40] ogbg-molbace: 0.847574 val loss: 0.500298
[Epoch 40] ogbg-molbace: 0.866023 test loss: 0.505828
[Epoch 41; Iter    10/   41] train: loss: 0.4501575
[Epoch 41; Iter    40/   41] train: loss: 0.5042496
[Epoch 41] ogbg-molbace: 0.905063 val loss: 0.452365
[Epoch 41] ogbg-molbace: 0.863916 test loss: 0.399040
[Epoch 42; Iter    29/   41] train: loss: 0.6679395
[Epoch 42] ogbg-molbace: 0.896449 val loss: 0.370253
[Epoch 42] ogbg-molbace: 0.856365 test loss: 0.434607
[Epoch 43; Iter    18/   41] train: loss: 0.2626464
[Epoch 43] ogbg-molbace: 0.931962 val loss: 0.336573
[Epoch 43] ogbg-molbace: 0.863389 test loss: 0.396411
[Epoch 44; Iter     7/   41] train: loss: 0.5885718
[Epoch 44; Iter    37/   41] train: loss: 0.4029652
[Epoch 44] ogbg-molbace: 0.897328 val loss: 0.392629
[Epoch 44] ogbg-molbace: 0.870588 test loss: 0.378708
[Epoch 45; Iter    26/   41] train: loss: 0.4299440
[Epoch 45] ogbg-molbace: 0.901371 val loss: 0.533858
[Epoch 45] ogbg-molbace: 0.879543 test loss: 0.415653
[Epoch 46; Iter    15/   41] train: loss: 0.5190901
[Epoch 46] ogbg-molbace: 0.885021 val loss: 0.420141
[Epoch 46] ogbg-molbace: 0.876383 test loss: 0.375635
[Epoch 47; Iter     4/   41] train: loss: 0.3941800
[Epoch 47; Iter    34/   41] train: loss: 0.2573588
[Epoch 47] ogbg-molbace: 0.876231 val loss: 0.446394
[Epoch 47] ogbg-molbace: 0.847234 test loss: 0.406157
[Epoch 48; Iter    23/   41] train: loss: 0.6218093
[Epoch 48] ogbg-molbace: 0.883263 val loss: 0.383225
[Epoch 48] ogbg-molbace: 0.888674 test loss: 0.395747
[Epoch 49; Iter    12/   41] train: loss: 0.4076578
[Epoch 49] ogbg-molbace: 0.884669 val loss: 0.421810
[Epoch 49] ogbg-molbace: 0.870939 test loss: 0.395350
[Epoch 50; Iter     1/   41] train: loss: 0.4410007
[Epoch 50; Iter    31/   41] train: loss: 0.5591813
[Epoch 50] ogbg-molbace: 0.858298 val loss: 0.591564
[Epoch 50] ogbg-molbace: 0.862511 test loss: 0.403535
[Epoch 51; Iter    20/   41] train: loss: 0.3689071
[Epoch 51] ogbg-molbace: 0.875879 val loss: 0.468642
[Epoch 51] ogbg-molbace: 0.871115 test loss: 0.388744
[Epoch 52; Iter     9/   41] train: loss: 0.5009655
[Epoch 52; Iter    39/   41] train: loss: 0.3128127
[Epoch 52] ogbg-molbace: 0.894163 val loss: 0.372466
[Epoch 52] ogbg-molbace: 0.881475 test loss: 0.382135
[Epoch 53; Iter    28/   41] train: loss: 0.5274734
[Epoch 53] ogbg-molbace: 0.885549 val loss: 0.384697
[Epoch 53] ogbg-molbace: 0.870588 test loss: 0.398578
[Epoch 54; Iter    17/   41] train: loss: 0.1961203
[Epoch 54] ogbg-molbace: 0.824367 val loss: 0.634635
[Epoch 54] ogbg-molbace: 0.836348 test loss: 0.504439
[Epoch 55; Iter     6/   41] train: loss: 0.3355454
[Epoch 55; Iter    36/   41] train: loss: 0.3885085
[Epoch 55] ogbg-molbace: 0.895042 val loss: 0.420297
[Epoch 55] ogbg-molbace: 0.880597 test loss: 0.386064
[Epoch 56; Iter    25/   41] train: loss: 0.3592837
[Epoch 56] ogbg-molbace: 0.888713 val loss: 0.396518
[Epoch 56] ogbg-molbace: 0.868306 test loss: 0.418830
[Epoch 57; Iter    14/   41] train: loss: 0.4946006
[Epoch 57] ogbg-molbace: 0.862342 val loss: 0.428009
[Epoch 57] ogbg-molbace: 0.846005 test loss: 0.501060
[Epoch 58; Iter     3/   41] train: loss: 0.2687638
[Epoch 58; Iter    33/   41] train: loss: 0.2565396
[Epoch 58] ogbg-molbace: 0.876406 val loss: 0.447679
[Epoch 58] ogbg-molbace: 0.854785 test loss: 0.441679
[Epoch 59; Iter    22/   41] train: loss: 0.2407042
[Epoch 59] ogbg-molbace: 0.895921 val loss: 0.362768
[Epoch 59] ogbg-molbace: 0.870764 test loss: 0.411150
[Epoch 60; Iter    11/   41] train: loss: 0.1459643
[Epoch 60; Iter    41/   41] train: loss: 0.6112666
[Epoch 60] ogbg-molbace: 0.914205 val loss: 0.374464
[Epoch 60] ogbg-molbace: 0.869535 test loss: 0.508384
[Epoch 61; Iter    30/   41] train: loss: 0.2247666
[Epoch 61] ogbg-molbace: 0.890823 val loss: 0.394212
[Epoch 61] ogbg-molbace: 0.869886 test loss: 0.398123
[Epoch 62; Iter    19/   41] train: loss: 0.2862523
[Epoch 62] ogbg-molbace: 0.916315 val loss: 0.327256
[Epoch 62] ogbg-molbace: 0.881299 test loss: 0.417264
[Epoch 63; Iter     8/   41] train: loss: 0.1524409
[Epoch 63; Iter    38/   41] train: loss: 0.3418657
[Epoch 63] ogbg-molbace: 0.891174 val loss: 0.485778
[Epoch 63] ogbg-molbace: 0.856190 test loss: 0.451849
[Epoch 64; Iter    27/   41] train: loss: 0.6541196
[Epoch 64] ogbg-molbace: 0.890647 val loss: 0.391635
[Epoch 64] ogbg-molbace: 0.859350 test loss: 0.470702
[Epoch 65; Iter    16/   41] train: loss: 0.4125873
[Epoch 65] ogbg-molbace: 0.904008 val loss: 0.384655
[Epoch 65] ogbg-molbace: 0.862160 test loss: 0.459895
[Epoch 66; Iter     5/   41] train: loss: 0.1266707
[Epoch 66; Iter    35/   41] train: loss: 0.3593102
[Epoch 66] ogbg-molbace: 0.908755 val loss: 0.342325
[Epoch 66] ogbg-molbace: 0.877085 test loss: 0.453900
[Epoch 67; Iter    24/   41] train: loss: 0.5002993
[Epoch 67] ogbg-molbace: 0.915612 val loss: 0.394281
[Epoch 67] ogbg-molbace: 0.867954 test loss: 0.513128
[Epoch 68; Iter    13/   41] train: loss: 0.3866160
[Epoch 68] ogbg-molbace: 0.886076 val loss: 0.417332
[Epoch 68] ogbg-molbace: 0.863916 test loss: 0.510773
[Epoch 69; Iter     2/   41] train: loss: 0.1917182
[Epoch 69; Iter    32/   41] train: loss: 0.3379995
[Epoch 69] ogbg-molbace: 0.911041 val loss: 0.366734
[Epoch 69] ogbg-molbace: 0.870588 test loss: 0.413699
[Epoch 70; Iter    21/   41] train: loss: 0.2948565
[Epoch 70] ogbg-molbace: 0.922996 val loss: 0.353509
[Epoch 70] ogbg-molbace: 0.861282 test loss: 0.514056
[Epoch 71; Iter    10/   41] train: loss: 0.2811166
[Epoch 71; Iter    40/   41] train: loss: 0.2871486
[Epoch 71] ogbg-molbace: 0.898383 val loss: 0.553889
[Epoch 71] ogbg-molbace: 0.884811 test loss: 0.525852
[Epoch 72; Iter    29/   41] train: loss: 0.5707609
[Epoch 72] ogbg-molbace: 0.872363 val loss: 0.538013
[Epoch 72] ogbg-molbace: 0.856365 test loss: 0.471698
[Epoch 73; Iter    18/   41] train: loss: 0.1334507
[Epoch 73] ogbg-molbace: 0.896273 val loss: 0.369322
[Epoch 73] ogbg-molbace: 0.861809 test loss: 0.440431
[Epoch 74; Iter     7/   41] train: loss: 0.2348607
[Epoch 74; Iter    37/   41] train: loss: 0.2032211
[Epoch 74] ogbg-molbace: 0.887482 val loss: 0.461515
[Epoch 74] ogbg-molbace: 0.869008 test loss: 0.451386
[Epoch 75; Iter    26/   41] train: loss: 0.2290230
[Epoch 75] ogbg-molbace: 0.896273 val loss: 0.396291
[Epoch 75] ogbg-molbace: 0.849342 test loss: 0.489214
[Epoch 76; Iter    15/   41] train: loss: 0.1645793
[Epoch 76] ogbg-molbace: 0.900316 val loss: 0.430599
[Epoch 76] ogbg-molbace: 0.846532 test loss: 0.584480
[Epoch 77; Iter     4/   41] train: loss: 0.2488445
[Epoch 77; Iter    34/   41] train: loss: 0.2653722
[Epoch 77] ogbg-molbace: 0.923699 val loss: 0.328362
[Epoch 77] ogbg-molbace: 0.851624 test loss: 0.495018
[Epoch 78; Iter    23/   41] train: loss: 0.3700812
[Epoch 34] ogbg-molbace: 0.804766 val loss: 0.577475
[Epoch 34] ogbg-molbace: 0.797310 test loss: 0.577345
[Epoch 35; Iter     6/   36] train: loss: 0.5364201
[Epoch 35; Iter    36/   36] train: loss: 0.5182033
[Epoch 35] ogbg-molbace: 0.841619 val loss: 0.530991
[Epoch 35] ogbg-molbace: 0.858109 test loss: 0.523681
[Epoch 36; Iter    30/   36] train: loss: 0.4229414
[Epoch 36] ogbg-molbace: 0.847301 val loss: 0.515686
[Epoch 36] ogbg-molbace: 0.856710 test loss: 0.506982
[Epoch 37; Iter    24/   36] train: loss: 0.4147818
[Epoch 37] ogbg-molbace: 0.826073 val loss: 0.558185
[Epoch 37] ogbg-molbace: 0.818302 test loss: 0.544547
[Epoch 38; Iter    18/   36] train: loss: 0.5282857
[Epoch 38] ogbg-molbace: 0.841304 val loss: 0.520450
[Epoch 38] ogbg-molbace: 0.853522 test loss: 0.496986
[Epoch 39; Iter    12/   36] train: loss: 0.4294391
[Epoch 39] ogbg-molbace: 0.842882 val loss: 0.505855
[Epoch 39] ogbg-molbace: 0.839683 test loss: 0.488402
[Epoch 40; Iter     6/   36] train: loss: 0.4082462
[Epoch 40; Iter    36/   36] train: loss: 0.3683062
[Epoch 40] ogbg-molbace: 0.853062 val loss: 0.525407
[Epoch 40] ogbg-molbace: 0.869383 test loss: 0.481312
[Epoch 41; Iter    30/   36] train: loss: 0.2394741
[Epoch 41] ogbg-molbace: 0.853772 val loss: 0.483503
[Epoch 41] ogbg-molbace: 0.870238 test loss: 0.461623
[Epoch 42; Iter    24/   36] train: loss: 0.4122250
[Epoch 42] ogbg-molbace: 0.846117 val loss: 0.494272
[Epoch 42] ogbg-molbace: 0.872026 test loss: 0.460115
[Epoch 43; Iter    18/   36] train: loss: 0.4266316
[Epoch 43] ogbg-molbace: 0.808160 val loss: 0.608991
[Epoch 43] ogbg-molbace: 0.806173 test loss: 0.559786
[Epoch 44; Iter    12/   36] train: loss: 0.3992325
[Epoch 44] ogbg-molbace: 0.827178 val loss: 0.593625
[Epoch 44] ogbg-molbace: 0.859120 test loss: 0.521605
[Epoch 45; Iter     6/   36] train: loss: 0.3810642
[Epoch 45; Iter    36/   36] train: loss: 0.2727272
[Epoch 45] ogbg-molbace: 0.864583 val loss: 0.493551
[Epoch 45] ogbg-molbace: 0.871871 test loss: 0.478575
[Epoch 46; Iter    30/   36] train: loss: 0.3160014
[Epoch 46] ogbg-molbace: 0.885417 val loss: 0.501130
[Epoch 46] ogbg-molbace: 0.876458 test loss: 0.505846
[Epoch 47; Iter    24/   36] train: loss: 0.6616019
[Epoch 47] ogbg-molbace: 0.819918 val loss: 0.609462
[Epoch 47] ogbg-molbace: 0.822500 test loss: 0.562914
[Epoch 48; Iter    18/   36] train: loss: 0.4408240
[Epoch 48] ogbg-molbace: 0.880919 val loss: 0.474480
[Epoch 48] ogbg-molbace: 0.863163 test loss: 0.461577
[Epoch 49; Iter    12/   36] train: loss: 0.5012975
[Epoch 49] ogbg-molbace: 0.862847 val loss: 0.476806
[Epoch 49] ogbg-molbace: 0.873037 test loss: 0.451366
[Epoch 50; Iter     6/   36] train: loss: 0.3479255
[Epoch 50; Iter    36/   36] train: loss: 0.4304375
[Epoch 50] ogbg-molbace: 0.865846 val loss: 0.534191
[Epoch 50] ogbg-molbace: 0.874670 test loss: 0.531872
[Epoch 51; Iter    30/   36] train: loss: 0.5977086
[Epoch 51] ogbg-molbace: 0.866951 val loss: 0.472214
[Epoch 51] ogbg-molbace: 0.878246 test loss: 0.441438
[Epoch 52; Iter    24/   36] train: loss: 0.3970820
[Epoch 52] ogbg-molbace: 0.849669 val loss: 0.501350
[Epoch 52] ogbg-molbace: 0.865495 test loss: 0.481169
[Epoch 53; Iter    18/   36] train: loss: 0.3546431
[Epoch 53] ogbg-molbace: 0.875631 val loss: 0.453076
[Epoch 53] ogbg-molbace: 0.872259 test loss: 0.465420
[Epoch 54; Iter    12/   36] train: loss: 0.1868497
[Epoch 54] ogbg-molbace: 0.874921 val loss: 0.486831
[Epoch 54] ogbg-molbace: 0.870082 test loss: 0.474798
[Epoch 55; Iter     6/   36] train: loss: 0.2880984
[Epoch 55; Iter    36/   36] train: loss: 0.4962015
[Epoch 55] ogbg-molbace: 0.853693 val loss: 0.492203
[Epoch 55] ogbg-molbace: 0.864251 test loss: 0.479697
[Epoch 56; Iter    30/   36] train: loss: 0.4271232
[Epoch 56] ogbg-molbace: 0.866241 val loss: 0.487186
[Epoch 56] ogbg-molbace: 0.860053 test loss: 0.505441
[Epoch 57; Iter    24/   36] train: loss: 0.4189401
[Epoch 57] ogbg-molbace: 0.865846 val loss: 0.511179
[Epoch 57] ogbg-molbace: 0.864485 test loss: 0.511181
[Epoch 58; Iter    18/   36] train: loss: 0.2415117
[Epoch 58] ogbg-molbace: 0.866872 val loss: 0.497922
[Epoch 58] ogbg-molbace: 0.879879 test loss: 0.464846
[Epoch 59; Iter    12/   36] train: loss: 0.3222210
[Epoch 59] ogbg-molbace: 0.887232 val loss: 0.443908
[Epoch 59] ogbg-molbace: 0.864796 test loss: 0.533086
[Epoch 60; Iter     6/   36] train: loss: 0.2834204
[Epoch 60; Iter    36/   36] train: loss: 0.4888166
[Epoch 60] ogbg-molbace: 0.879735 val loss: 0.513576
[Epoch 60] ogbg-molbace: 0.853911 test loss: 0.594428
[Epoch 61; Iter    30/   36] train: loss: 0.4863844
[Epoch 61] ogbg-molbace: 0.896307 val loss: 0.416392
[Epoch 61] ogbg-molbace: 0.879179 test loss: 0.449265
[Epoch 62; Iter    24/   36] train: loss: 0.5021608
[Epoch 62] ogbg-molbace: 0.888968 val loss: 0.453471
[Epoch 62] ogbg-molbace: 0.879956 test loss: 0.463697
[Epoch 63; Iter    18/   36] train: loss: 0.4006565
[Epoch 63] ogbg-molbace: 0.877131 val loss: 0.469434
[Epoch 63] ogbg-molbace: 0.882367 test loss: 0.493968
[Epoch 64; Iter    12/   36] train: loss: 0.4129543
[Epoch 64] ogbg-molbace: 0.858507 val loss: 0.574697
[Epoch 64] ogbg-molbace: 0.849946 test loss: 0.647928
[Epoch 65; Iter     6/   36] train: loss: 0.4102038
[Epoch 65; Iter    36/   36] train: loss: 0.2130274
[Epoch 65] ogbg-molbace: 0.897648 val loss: 0.574935
[Epoch 65] ogbg-molbace: 0.881745 test loss: 0.615052
[Epoch 66; Iter    30/   36] train: loss: 0.4603734
[Epoch 66] ogbg-molbace: 0.876578 val loss: 0.471434
[Epoch 66] ogbg-molbace: 0.867206 test loss: 0.530915
[Epoch 67; Iter    24/   36] train: loss: 0.3276050
[Epoch 67] ogbg-molbace: 0.901436 val loss: 0.429733
[Epoch 67] ogbg-molbace: 0.885321 test loss: 0.452837
[Epoch 68; Iter    18/   36] train: loss: 0.2216689
[Epoch 68] ogbg-molbace: 0.845565 val loss: 0.671786
[Epoch 68] ogbg-molbace: 0.850334 test loss: 0.650756
[Epoch 69; Iter    12/   36] train: loss: 0.4545681
[Epoch 69] ogbg-molbace: 0.874132 val loss: 0.507989
[Epoch 69] ogbg-molbace: 0.863007 test loss: 0.545241
[Epoch 70; Iter     6/   36] train: loss: 0.1485896
[Epoch 70; Iter    36/   36] train: loss: 0.9332135
[Epoch 70] ogbg-molbace: 0.844697 val loss: 0.636065
[Epoch 70] ogbg-molbace: 0.866273 test loss: 0.562021
[Epoch 71; Iter    30/   36] train: loss: 0.2597952
[Epoch 71] ogbg-molbace: 0.875395 val loss: 0.492128
[Epoch 71] ogbg-molbace: 0.851190 test loss: 0.588894
[Epoch 72; Iter    24/   36] train: loss: 0.2630705
[Epoch 72] ogbg-molbace: 0.881550 val loss: 0.491247
[Epoch 72] ogbg-molbace: 0.874592 test loss: 0.533609
[Epoch 73; Iter    18/   36] train: loss: 0.2029291
[Epoch 73] ogbg-molbace: 0.885653 val loss: 0.494602
[Epoch 73] ogbg-molbace: 0.874981 test loss: 0.553287
[Epoch 74; Iter    12/   36] train: loss: 0.2875445
[Epoch 74] ogbg-molbace: 0.883838 val loss: 0.521624
[Epoch 74] ogbg-molbace: 0.880889 test loss: 0.514232
[Epoch 75; Iter     6/   36] train: loss: 0.1348370
[Epoch 75; Iter    36/   36] train: loss: 0.4283584
[Epoch 75] ogbg-molbace: 0.877210 val loss: 0.501298
[Epoch 75] ogbg-molbace: 0.867206 test loss: 0.536280
[Epoch 76; Iter    30/   36] train: loss: 0.3085492
[Epoch 76] ogbg-molbace: 0.871765 val loss: 0.569290
[Epoch 76] ogbg-molbace: 0.874203 test loss: 0.580587
[Epoch 77; Iter    24/   36] train: loss: 0.1879550
[Epoch 77] ogbg-molbace: 0.851168 val loss: 0.632646
[Epoch 77] ogbg-molbace: 0.863007 test loss: 0.619742
[Epoch 78; Iter    18/   36] train: loss: 0.1369549
[Epoch 78] ogbg-molbace: 0.867503 val loss: 0.604129
[Epoch 78] ogbg-molbace: 0.865262 test loss: 0.598947
[Epoch 79; Iter    12/   36] train: loss: 0.1329884
[Epoch 79] ogbg-molbace: 0.867188 val loss: 0.711524
[Epoch 79] ogbg-molbace: 0.869460 test loss: 0.715029
[Epoch 80; Iter     6/   36] train: loss: 0.1922189
[Epoch 80; Iter    36/   36] train: loss: 0.5997113
[Epoch 80] ogbg-molbace: 0.875316 val loss: 0.575918
[Epoch 80] ogbg-molbace: 0.864873 test loss: 0.643573
[Epoch 81; Iter    30/   36] train: loss: 0.0961872
[Epoch 81] ogbg-molbace: 0.837121 val loss: 0.691502
[Epoch 81] ogbg-molbace: 0.869072 test loss: 0.622925
[Epoch 82; Iter    24/   36] train: loss: 0.1835725
[Epoch 32] ogbg-molbace: 0.832660 test loss: 0.453793
[Epoch 33; Iter     8/   41] train: loss: 0.4397881
[Epoch 33; Iter    38/   41] train: loss: 0.5399031
[Epoch 33] ogbg-molbace: 0.804325 val loss: 0.535788
[Epoch 33] ogbg-molbace: 0.790869 test loss: 0.478940
[Epoch 34; Iter    27/   41] train: loss: 0.4910509
[Epoch 34] ogbg-molbace: 0.856540 val loss: 0.464855
[Epoch 34] ogbg-molbace: 0.800878 test loss: 0.461601
[Epoch 35; Iter    16/   41] train: loss: 0.4714708
[Epoch 35] ogbg-molbace: 0.887834 val loss: 0.456008
[Epoch 35] ogbg-molbace: 0.867603 test loss: 0.580148
[Epoch 36; Iter     5/   41] train: loss: 0.4771900
[Epoch 36; Iter    35/   41] train: loss: 0.4408236
[Epoch 36] ogbg-molbace: 0.872011 val loss: 0.519681
[Epoch 36] ogbg-molbace: 0.817910 test loss: 0.484773
[Epoch 37; Iter    24/   41] train: loss: 0.4310172
[Epoch 37] ogbg-molbace: 0.915436 val loss: 0.367262
[Epoch 37] ogbg-molbace: 0.876032 test loss: 0.396125
[Epoch 38; Iter    13/   41] train: loss: 0.5047756
[Epoch 38] ogbg-molbace: 0.886252 val loss: 0.402062
[Epoch 38] ogbg-molbace: 0.878490 test loss: 0.426078
[Epoch 39; Iter     2/   41] train: loss: 0.5017897
[Epoch 39; Iter    32/   41] train: loss: 0.4928516
[Epoch 39] ogbg-molbace: 0.906470 val loss: 0.361411
[Epoch 39] ogbg-molbace: 0.871993 test loss: 0.469226
[Epoch 40; Iter    21/   41] train: loss: 0.3905760
[Epoch 40] ogbg-molbace: 0.819444 val loss: 0.551112
[Epoch 40] ogbg-molbace: 0.825637 test loss: 0.450604
[Epoch 41; Iter    10/   41] train: loss: 0.4999177
[Epoch 41; Iter    40/   41] train: loss: 0.3173040
[Epoch 41] ogbg-molbace: 0.911568 val loss: 0.335166
[Epoch 41] ogbg-molbace: 0.863038 test loss: 0.421956
[Epoch 42; Iter    29/   41] train: loss: 0.5247568
[Epoch 42] ogbg-molbace: 0.890823 val loss: 0.421394
[Epoch 42] ogbg-molbace: 0.859701 test loss: 0.402436
[Epoch 43; Iter    18/   41] train: loss: 0.3716899
[Epoch 43] ogbg-molbace: 0.901547 val loss: 0.363986
[Epoch 43] ogbg-molbace: 0.873047 test loss: 0.406250
[Epoch 44; Iter     7/   41] train: loss: 0.4507209
[Epoch 44; Iter    37/   41] train: loss: 0.3704632
[Epoch 44] ogbg-molbace: 0.914030 val loss: 0.349232
[Epoch 44] ogbg-molbace: 0.848815 test loss: 0.418331
[Epoch 45; Iter    26/   41] train: loss: 0.3351822
[Epoch 45] ogbg-molbace: 0.897855 val loss: 0.365004
[Epoch 45] ogbg-molbace: 0.858297 test loss: 0.415960
[Epoch 46; Iter    15/   41] train: loss: 0.5367036
[Epoch 46] ogbg-molbace: 0.920183 val loss: 0.379917
[Epoch 46] ogbg-molbace: 0.885514 test loss: 0.375399
[Epoch 47; Iter     4/   41] train: loss: 0.3754846
[Epoch 47; Iter    34/   41] train: loss: 0.3542111
[Epoch 47] ogbg-molbace: 0.883087 val loss: 0.430927
[Epoch 47] ogbg-molbace: 0.863565 test loss: 0.396496
[Epoch 48; Iter    23/   41] train: loss: 0.3071904
[Epoch 48] ogbg-molbace: 0.898207 val loss: 0.398255
[Epoch 48] ogbg-molbace: 0.883933 test loss: 0.370744
[Epoch 49; Iter    12/   41] train: loss: 0.2453061
[Epoch 49] ogbg-molbace: 0.917194 val loss: 0.391418
[Epoch 49] ogbg-molbace: 0.871115 test loss: 0.403847
[Epoch 50; Iter     1/   41] train: loss: 0.3635753
[Epoch 50; Iter    31/   41] train: loss: 0.5187404
[Epoch 50] ogbg-molbace: 0.910865 val loss: 0.457279
[Epoch 50] ogbg-molbace: 0.861984 test loss: 0.413155
[Epoch 51; Iter    20/   41] train: loss: 0.3657371
[Epoch 51] ogbg-molbace: 0.901723 val loss: 0.351169
[Epoch 51] ogbg-molbace: 0.854434 test loss: 0.451785
[Epoch 52; Iter     9/   41] train: loss: 0.1997347
[Epoch 52; Iter    39/   41] train: loss: 0.3344640
[Epoch 52] ogbg-molbace: 0.907525 val loss: 0.329519
[Epoch 52] ogbg-molbace: 0.847586 test loss: 0.447945
[Epoch 53; Iter    28/   41] train: loss: 0.2644784
[Epoch 53] ogbg-molbace: 0.913678 val loss: 0.339828
[Epoch 53] ogbg-molbace: 0.868657 test loss: 0.419786
[Epoch 54; Iter    17/   41] train: loss: 0.2906375
[Epoch 54] ogbg-molbace: 0.910338 val loss: 0.365220
[Epoch 54] ogbg-molbace: 0.867603 test loss: 0.453459
[Epoch 55; Iter     6/   41] train: loss: 0.1549381
[Epoch 55; Iter    36/   41] train: loss: 0.3390162
[Epoch 55] ogbg-molbace: 0.894339 val loss: 0.377183
[Epoch 55] ogbg-molbace: 0.875680 test loss: 0.394198
[Epoch 56; Iter    25/   41] train: loss: 0.3882908
[Epoch 56] ogbg-molbace: 0.887482 val loss: 0.545827
[Epoch 56] ogbg-molbace: 0.851273 test loss: 0.493412
[Epoch 57; Iter    14/   41] train: loss: 0.3104201
[Epoch 57] ogbg-molbace: 0.912623 val loss: 0.342329
[Epoch 57] ogbg-molbace: 0.869710 test loss: 0.472607
[Epoch 58; Iter     3/   41] train: loss: 0.3415857
[Epoch 58; Iter    33/   41] train: loss: 0.2380500
[Epoch 58] ogbg-molbace: 0.897679 val loss: 0.413837
[Epoch 58] ogbg-molbace: 0.864794 test loss: 0.414619
[Epoch 59; Iter    22/   41] train: loss: 0.3997098
[Epoch 59] ogbg-molbace: 0.900141 val loss: 0.351394
[Epoch 59] ogbg-molbace: 0.879543 test loss: 0.414031
[Epoch 60; Iter    11/   41] train: loss: 0.2488901
[Epoch 60; Iter    41/   41] train: loss: 0.2517852
[Epoch 60] ogbg-molbace: 0.916491 val loss: 0.318905
[Epoch 60] ogbg-molbace: 0.876558 test loss: 0.398854
[Epoch 61; Iter    30/   41] train: loss: 0.3668568
[Epoch 61] ogbg-molbace: 0.927215 val loss: 0.413295
[Epoch 61] ogbg-molbace: 0.875680 test loss: 0.650627
[Epoch 62; Iter    19/   41] train: loss: 0.2345985
[Epoch 62] ogbg-molbace: 0.927215 val loss: 0.315591
[Epoch 62] ogbg-molbace: 0.875856 test loss: 0.419076
[Epoch 63; Iter     8/   41] train: loss: 0.3302767
[Epoch 63; Iter    38/   41] train: loss: 0.2299454
[Epoch 63] ogbg-molbace: 0.886076 val loss: 0.457646
[Epoch 63] ogbg-molbace: 0.859526 test loss: 0.455042
[Epoch 64; Iter    27/   41] train: loss: 0.2328351
[Epoch 64] ogbg-molbace: 0.911920 val loss: 0.374258
[Epoch 64] ogbg-molbace: 0.867076 test loss: 0.523911
[Epoch 65; Iter    16/   41] train: loss: 0.1618867
[Epoch 65] ogbg-molbace: 0.899613 val loss: 0.819430
[Epoch 65] ogbg-molbace: 0.872520 test loss: 0.459332
[Epoch 66; Iter     5/   41] train: loss: 0.3327342
[Epoch 66; Iter    35/   41] train: loss: 0.2329719
[Epoch 66] ogbg-molbace: 0.906470 val loss: 0.353123
[Epoch 66] ogbg-molbace: 0.888850 test loss: 0.361576
[Epoch 67; Iter    24/   41] train: loss: 0.3742699
[Epoch 67] ogbg-molbace: 0.878868 val loss: 0.575577
[Epoch 67] ogbg-molbace: 0.851975 test loss: 0.504021
[Epoch 68; Iter    13/   41] train: loss: 0.6602530
[Epoch 68] ogbg-molbace: 0.908228 val loss: 0.339695
[Epoch 68] ogbg-molbace: 0.874978 test loss: 0.420950
[Epoch 69; Iter     2/   41] train: loss: 0.1714995
[Epoch 69; Iter    32/   41] train: loss: 0.1846202
[Epoch 69] ogbg-molbace: 0.895570 val loss: 0.752157
[Epoch 69] ogbg-molbace: 0.876032 test loss: 0.561518
[Epoch 70; Iter    21/   41] train: loss: 0.3664823
[Epoch 70] ogbg-molbace: 0.871660 val loss: 0.500278
[Epoch 70] ogbg-molbace: 0.866374 test loss: 0.492040
[Epoch 71; Iter    10/   41] train: loss: 0.2318079
[Epoch 71; Iter    40/   41] train: loss: 0.2068363
[Epoch 71] ogbg-molbace: 0.904184 val loss: 0.367888
[Epoch 71] ogbg-molbace: 0.875154 test loss: 0.424108
[Epoch 72; Iter    29/   41] train: loss: 0.2454209
[Epoch 72] ogbg-molbace: 0.891526 val loss: 0.422968
[Epoch 72] ogbg-molbace: 0.892186 test loss: 0.390009
[Epoch 73; Iter    18/   41] train: loss: 0.1793838
[Epoch 73] ogbg-molbace: 0.901899 val loss: 0.607348
[Epoch 73] ogbg-molbace: 0.870939 test loss: 0.588308
[Epoch 74; Iter     7/   41] train: loss: 0.2302467
[Epoch 74; Iter    37/   41] train: loss: 0.2138822
[Epoch 74] ogbg-molbace: 0.884318 val loss: 0.497631
[Epoch 74] ogbg-molbace: 0.849342 test loss: 0.637513
[Epoch 75; Iter    26/   41] train: loss: 0.1602152
[Epoch 75] ogbg-molbace: 0.918952 val loss: 0.561614
[Epoch 75] ogbg-molbace: 0.871466 test loss: 0.517726
[Epoch 76; Iter    15/   41] train: loss: 0.0880664
[Epoch 76] ogbg-molbace: 0.890647 val loss: 0.402860
[Epoch 76] ogbg-molbace: 0.884636 test loss: 0.417057
[Epoch 77; Iter     4/   41] train: loss: 0.1914316
[Epoch 77; Iter    34/   41] train: loss: 0.2182748
[Epoch 77] ogbg-molbace: 0.900141 val loss: 0.382948
[Epoch 77] ogbg-molbace: 0.841967 test loss: 0.497551
[Epoch 78; Iter    23/   41] train: loss: 0.2480415
[Epoch 36; Iter    25/   31] train: loss: 0.5228832
[Epoch 36] ogbg-molbace: 0.818761 val loss: 0.514858
[Epoch 36] ogbg-molbace: 0.821220 test loss: 0.502824
[Epoch 37; Iter    24/   31] train: loss: 0.4435068
[Epoch 37] ogbg-molbace: 0.841135 val loss: 0.503577
[Epoch 37] ogbg-molbace: 0.831123 test loss: 0.489509
[Epoch 38; Iter    23/   31] train: loss: 0.5131642
[Epoch 38] ogbg-molbace: 0.829567 val loss: 0.540504
[Epoch 38] ogbg-molbace: 0.812407 test loss: 0.509068
[Epoch 39; Iter    22/   31] train: loss: 0.4147644
[Epoch 39] ogbg-molbace: 0.850865 val loss: 0.479190
[Epoch 39] ogbg-molbace: 0.835180 test loss: 0.472764
[Epoch 40; Iter    21/   31] train: loss: 0.4824933
[Epoch 40] ogbg-molbace: 0.843691 val loss: 0.533582
[Epoch 40] ogbg-molbace: 0.840459 test loss: 0.502848
[Epoch 41; Iter    20/   31] train: loss: 0.4248846
[Epoch 41] ogbg-molbace: 0.837145 val loss: 0.523225
[Epoch 41] ogbg-molbace: 0.801588 test loss: 0.528293
[Epoch 42; Iter    19/   31] train: loss: 0.5516487
[Epoch 42] ogbg-molbace: 0.860282 val loss: 0.468410
[Epoch 42] ogbg-molbace: 0.856208 test loss: 0.457434
[Epoch 43; Iter    18/   31] train: loss: 0.4323820
[Epoch 43] ogbg-molbace: 0.849027 val loss: 0.540976
[Epoch 43] ogbg-molbace: 0.868162 test loss: 0.485410
[Epoch 44; Iter    17/   31] train: loss: 0.4808206
[Epoch 44] ogbg-molbace: 0.862524 val loss: 0.479878
[Epoch 44] ogbg-molbace: 0.857691 test loss: 0.455357
[Epoch 45; Iter    16/   31] train: loss: 0.5256560
[Epoch 45] ogbg-molbace: 0.886468 val loss: 0.430768
[Epoch 45] ogbg-molbace: 0.882951 test loss: 0.416862
[Epoch 46; Iter    15/   31] train: loss: 0.3720476
[Epoch 46] ogbg-molbace: 0.861358 val loss: 0.466915
[Epoch 46] ogbg-molbace: 0.866111 test loss: 0.429814
[Epoch 47; Iter    14/   31] train: loss: 0.4420735
[Epoch 47] ogbg-molbace: 0.855932 val loss: 0.484439
[Epoch 47] ogbg-molbace: 0.838670 test loss: 0.465251
[Epoch 48; Iter    13/   31] train: loss: 0.4682368
[Epoch 48] ogbg-molbace: 0.873240 val loss: 0.453425
[Epoch 48] ogbg-molbace: 0.864410 test loss: 0.426058
[Epoch 49; Iter    12/   31] train: loss: 0.4060284
[Epoch 49] ogbg-molbace: 0.876827 val loss: 0.465248
[Epoch 49] ogbg-molbace: 0.882471 test loss: 0.433045
[Epoch 50; Iter    11/   31] train: loss: 0.3161586
[Epoch 50] ogbg-molbace: 0.857905 val loss: 0.501589
[Epoch 50] ogbg-molbace: 0.850144 test loss: 0.475705
[Epoch 51; Iter    10/   31] train: loss: 0.5147926
[Epoch 51] ogbg-molbace: 0.884360 val loss: 0.414796
[Epoch 51] ogbg-molbace: 0.870910 test loss: 0.420024
[Epoch 52; Iter     9/   31] train: loss: 0.3040465
[Epoch 52] ogbg-molbace: 0.868936 val loss: 0.535882
[Epoch 52] ogbg-molbace: 0.872306 test loss: 0.488523
[Epoch 53; Iter     8/   31] train: loss: 0.4541548
[Epoch 53] ogbg-molbace: 0.864765 val loss: 0.448418
[Epoch 53] ogbg-molbace: 0.874924 test loss: 0.411023
[Epoch 54; Iter     7/   31] train: loss: 0.3122467
[Epoch 54] ogbg-molbace: 0.849834 val loss: 0.489177
[Epoch 54] ogbg-molbace: 0.830556 test loss: 0.479811
[Epoch 55; Iter     6/   31] train: loss: 0.5837155
[Epoch 55] ogbg-molbace: 0.887275 val loss: 0.408987
[Epoch 55] ogbg-molbace: 0.881380 test loss: 0.406476
[Epoch 56; Iter     5/   31] train: loss: 0.4079577
[Epoch 56] ogbg-molbace: 0.854542 val loss: 0.535787
[Epoch 56] ogbg-molbace: 0.818035 test loss: 0.602347
[Epoch 57; Iter     4/   31] train: loss: 0.4343041
[Epoch 57] ogbg-molbace: 0.871716 val loss: 0.427694
[Epoch 57] ogbg-molbace: 0.843077 test loss: 0.465308
[Epoch 58; Iter     3/   31] train: loss: 0.4229133
[Epoch 58] ogbg-molbace: 0.850731 val loss: 0.498088
[Epoch 58] ogbg-molbace: 0.851060 test loss: 0.526876
[Epoch 59; Iter     2/   31] train: loss: 0.3273914
[Epoch 59] ogbg-molbace: 0.878666 val loss: 0.414027
[Epoch 59] ogbg-molbace: 0.866591 test loss: 0.424675
[Epoch 60; Iter     1/   31] train: loss: 0.2816002
[Epoch 60; Iter    31/   31] train: loss: 0.6238405
[Epoch 60] ogbg-molbace: 0.884674 val loss: 0.422849
[Epoch 60] ogbg-molbace: 0.866417 test loss: 0.434486
[Epoch 61; Iter    30/   31] train: loss: 0.2851984
[Epoch 61] ogbg-molbace: 0.876379 val loss: 0.416265
[Epoch 61] ogbg-molbace: 0.880813 test loss: 0.402687
[Epoch 62; Iter    29/   31] train: loss: 0.3470237
[Epoch 62] ogbg-molbace: 0.878441 val loss: 0.419562
[Epoch 62] ogbg-molbace: 0.862534 test loss: 0.458098
[Epoch 63; Iter    28/   31] train: loss: 0.3740060
[Epoch 63] ogbg-molbace: 0.889786 val loss: 0.471404
[Epoch 63] ogbg-molbace: 0.882427 test loss: 0.456191
[Epoch 64; Iter    27/   31] train: loss: 0.4561800
[Epoch 64] ogbg-molbace: 0.887947 val loss: 0.410644
[Epoch 64] ogbg-molbace: 0.881642 test loss: 0.415901
[Epoch 65; Iter    26/   31] train: loss: 0.2257322
[Epoch 65] ogbg-molbace: 0.883867 val loss: 0.406130
[Epoch 65] ogbg-molbace: 0.869427 test loss: 0.434294
[Epoch 66; Iter    25/   31] train: loss: 0.4139629
[Epoch 66] ogbg-molbace: 0.898440 val loss: 0.389692
[Epoch 66] ogbg-molbace: 0.891982 test loss: 0.397163
[Epoch 67; Iter    24/   31] train: loss: 0.1733906
[Epoch 67] ogbg-molbace: 0.885436 val loss: 0.470276
[Epoch 67] ogbg-molbace: 0.861792 test loss: 0.527951
[Epoch 68; Iter    23/   31] train: loss: 0.2167283
[Epoch 68] ogbg-molbace: 0.865169 val loss: 0.456060
[Epoch 68] ogbg-molbace: 0.850057 test loss: 0.483873
[Epoch 69; Iter    22/   31] train: loss: 0.2754956
[Epoch 69] ogbg-molbace: 0.888082 val loss: 0.415393
[Epoch 69] ogbg-molbace: 0.867900 test loss: 0.447497
[Epoch 70; Iter    21/   31] train: loss: 0.1806848
[Epoch 70] ogbg-molbace: 0.884360 val loss: 0.436653
[Epoch 70] ogbg-molbace: 0.848879 test loss: 0.503596
[Epoch 71; Iter    20/   31] train: loss: 0.4491388
[Epoch 71] ogbg-molbace: 0.890727 val loss: 0.411666
[Epoch 71] ogbg-molbace: 0.869470 test loss: 0.462318
[Epoch 72; Iter    19/   31] train: loss: 0.3174044
[Epoch 72] ogbg-molbace: 0.875437 val loss: 0.447564
[Epoch 72] ogbg-molbace: 0.863319 test loss: 0.499383
[Epoch 73; Iter    18/   31] train: loss: 0.3131884
[Epoch 73] ogbg-molbace: 0.870819 val loss: 0.445663
[Epoch 73] ogbg-molbace: 0.875796 test loss: 0.448794
[Epoch 74; Iter    17/   31] train: loss: 0.1586470
[Epoch 74] ogbg-molbace: 0.868218 val loss: 0.464002
[Epoch 74] ogbg-molbace: 0.857779 test loss: 0.487797
[Epoch 75; Iter    16/   31] train: loss: 0.3513902
[Epoch 75] ogbg-molbace: 0.886826 val loss: 0.457802
[Epoch 75] ogbg-molbace: 0.885568 test loss: 0.441317
[Epoch 76; Iter    15/   31] train: loss: 0.1800163
[Epoch 76] ogbg-molbace: 0.895435 val loss: 0.406382
[Epoch 76] ogbg-molbace: 0.868380 test loss: 0.492812
[Epoch 77; Iter    14/   31] train: loss: 0.2368127
[Epoch 77] ogbg-molbace: 0.880459 val loss: 0.463194
[Epoch 77] ogbg-molbace: 0.866417 test loss: 0.500108
[Epoch 78; Iter    13/   31] train: loss: 0.2756198
[Epoch 78] ogbg-molbace: 0.872478 val loss: 0.520555
[Epoch 78] ogbg-molbace: 0.853198 test loss: 0.534593
[Epoch 79; Iter    12/   31] train: loss: 0.1871260
[Epoch 79] ogbg-molbace: 0.887902 val loss: 0.424045
[Epoch 79] ogbg-molbace: 0.868031 test loss: 0.505150
[Epoch 80; Iter    11/   31] train: loss: 0.3816236
[Epoch 80] ogbg-molbace: 0.879248 val loss: 0.510415
[Epoch 80] ogbg-molbace: 0.863755 test loss: 0.523311
[Epoch 81; Iter    10/   31] train: loss: 0.2413249
[Epoch 81] ogbg-molbace: 0.880818 val loss: 0.446172
[Epoch 81] ogbg-molbace: 0.867943 test loss: 0.483995
[Epoch 82; Iter     9/   31] train: loss: 0.2098025
[Epoch 82] ogbg-molbace: 0.861268 val loss: 0.536643
[Epoch 82] ogbg-molbace: 0.864890 test loss: 0.552363
[Epoch 83; Iter     8/   31] train: loss: 0.1084493
[Epoch 83] ogbg-molbace: 0.871671 val loss: 0.472889
[Epoch 83] ogbg-molbace: 0.855205 test loss: 0.572538
[Epoch 84; Iter     7/   31] train: loss: 0.4372788
[Epoch 84] ogbg-molbace: 0.872971 val loss: 0.472251
[Epoch 84] ogbg-molbace: 0.861749 test loss: 0.529042
[Epoch 85; Iter     6/   31] train: loss: 0.0914912
[Epoch 85] ogbg-molbace: 0.890727 val loss: 0.436627
[Epoch 85] ogbg-molbace: 0.869558 test loss: 0.515134
[Epoch 86; Iter     5/   31] train: loss: 0.0874193
[Epoch 86] ogbg-molbace: 0.890458 val loss: 0.462317
[Epoch 86] ogbg-molbace: 0.868467 test loss: 0.525432
[Epoch 34] ogbg-molbace: 0.850221 val loss: 0.511799
[Epoch 34] ogbg-molbace: 0.864951 test loss: 0.483935
[Epoch 35; Iter     6/   36] train: loss: 0.5724247
[Epoch 35; Iter    36/   36] train: loss: 0.9016430
[Epoch 35] ogbg-molbace: 0.839094 val loss: 0.527261
[Epoch 35] ogbg-molbace: 0.841937 test loss: 0.503716
[Epoch 36; Iter    30/   36] train: loss: 0.4179447
[Epoch 36] ogbg-molbace: 0.823311 val loss: 0.551305
[Epoch 36] ogbg-molbace: 0.814803 test loss: 0.555497
[Epoch 37; Iter    24/   36] train: loss: 0.5737154
[Epoch 37] ogbg-molbace: 0.864899 val loss: 0.530994
[Epoch 37] ogbg-molbace: 0.860286 test loss: 0.518766
[Epoch 38; Iter    18/   36] train: loss: 0.4076567
[Epoch 38] ogbg-molbace: 0.811790 val loss: 0.585529
[Epoch 38] ogbg-molbace: 0.796377 test loss: 0.599338
[Epoch 39; Iter    12/   36] train: loss: 0.4801521
[Epoch 39] ogbg-molbace: 0.860164 val loss: 0.514067
[Epoch 39] ogbg-molbace: 0.861686 test loss: 0.493885
[Epoch 40; Iter     6/   36] train: loss: 0.4783396
[Epoch 40; Iter    36/   36] train: loss: 0.3343846
[Epoch 40] ogbg-molbace: 0.817866 val loss: 0.553189
[Epoch 40] ogbg-molbace: 0.827399 test loss: 0.544537
[Epoch 41; Iter    30/   36] train: loss: 0.5488211
[Epoch 41] ogbg-molbace: 0.866635 val loss: 0.501542
[Epoch 41] ogbg-molbace: 0.868761 test loss: 0.498140
[Epoch 42; Iter    24/   36] train: loss: 0.3597183
[Epoch 42] ogbg-molbace: 0.843040 val loss: 0.607830
[Epoch 42] ogbg-molbace: 0.855310 test loss: 0.604119
[Epoch 43; Iter    18/   36] train: loss: 0.5256582
[Epoch 43] ogbg-molbace: 0.862295 val loss: 0.557314
[Epoch 43] ogbg-molbace: 0.873659 test loss: 0.516732
[Epoch 44; Iter    12/   36] train: loss: 0.5841578
[Epoch 44] ogbg-molbace: 0.865688 val loss: 0.465496
[Epoch 44] ogbg-molbace: 0.863863 test loss: 0.480415
[Epoch 45; Iter     6/   36] train: loss: 0.4159937
[Epoch 45; Iter    36/   36] train: loss: 0.2337935
[Epoch 45] ogbg-molbace: 0.863005 val loss: 0.481572
[Epoch 45] ogbg-molbace: 0.879879 test loss: 0.446670
[Epoch 46; Iter    30/   36] train: loss: 0.3127936
[Epoch 46] ogbg-molbace: 0.854561 val loss: 0.527197
[Epoch 46] ogbg-molbace: 0.854299 test loss: 0.534914
[Epoch 47; Iter    24/   36] train: loss: 0.3575418
[Epoch 47] ogbg-molbace: 0.889047 val loss: 0.416598
[Epoch 47] ogbg-molbace: 0.897528 test loss: 0.413644
[Epoch 48; Iter    18/   36] train: loss: 0.4551287
[Epoch 48] ogbg-molbace: 0.857797 val loss: 0.555565
[Epoch 48] ogbg-molbace: 0.860442 test loss: 0.545968
[Epoch 49; Iter    12/   36] train: loss: 0.3279887
[Epoch 49] ogbg-molbace: 0.847696 val loss: 0.504902
[Epoch 49] ogbg-molbace: 0.859664 test loss: 0.483629
[Epoch 50; Iter     6/   36] train: loss: 0.3674780
[Epoch 50; Iter    36/   36] train: loss: 0.4998571
[Epoch 50] ogbg-molbace: 0.873264 val loss: 0.462628
[Epoch 50] ogbg-molbace: 0.877857 test loss: 0.447766
[Epoch 51; Iter    30/   36] train: loss: 0.3505858
[Epoch 51] ogbg-molbace: 0.859533 val loss: 0.604379
[Epoch 51] ogbg-molbace: 0.879101 test loss: 0.559642
[Epoch 52; Iter    24/   36] train: loss: 0.3666976
[Epoch 52] ogbg-molbace: 0.868134 val loss: 0.456492
[Epoch 52] ogbg-molbace: 0.881356 test loss: 0.439199
[Epoch 53; Iter    18/   36] train: loss: 0.3136308
[Epoch 53] ogbg-molbace: 0.816998 val loss: 0.601826
[Epoch 53] ogbg-molbace: 0.860286 test loss: 0.522250
[Epoch 54; Iter    12/   36] train: loss: 0.3538369
[Epoch 54] ogbg-molbace: 0.843434 val loss: 0.513745
[Epoch 54] ogbg-molbace: 0.850257 test loss: 0.516668
[Epoch 55; Iter     6/   36] train: loss: 0.4443596
[Epoch 55; Iter    36/   36] train: loss: 0.4974909
[Epoch 55] ogbg-molbace: 0.885417 val loss: 0.431423
[Epoch 55] ogbg-molbace: 0.885088 test loss: 0.435859
[Epoch 56; Iter    30/   36] train: loss: 0.1763591
[Epoch 56] ogbg-molbace: 0.872633 val loss: 0.467249
[Epoch 56] ogbg-molbace: 0.875369 test loss: 0.471328
[Epoch 57; Iter    24/   36] train: loss: 0.2430885
[Epoch 57] ogbg-molbace: 0.883365 val loss: 0.487481
[Epoch 57] ogbg-molbace: 0.887576 test loss: 0.483157
[Epoch 58; Iter    18/   36] train: loss: 0.5450279
[Epoch 58] ogbg-molbace: 0.883759 val loss: 0.443667
[Epoch 58] ogbg-molbace: 0.875369 test loss: 0.458378
[Epoch 59; Iter    12/   36] train: loss: 0.3204500
[Epoch 59] ogbg-molbace: 0.857086 val loss: 0.513558
[Epoch 59] ogbg-molbace: 0.881978 test loss: 0.481502
[Epoch 60; Iter     6/   36] train: loss: 0.1776081
[Epoch 60; Iter    36/   36] train: loss: 0.3888210
[Epoch 60] ogbg-molbace: 0.848011 val loss: 0.554965
[Epoch 60] ogbg-molbace: 0.837117 test loss: 0.567664
[Epoch 61; Iter    30/   36] train: loss: 0.3467714
[Epoch 61] ogbg-molbace: 0.868687 val loss: 0.705606
[Epoch 61] ogbg-molbace: 0.878090 test loss: 0.652993
[Epoch 62; Iter    24/   36] train: loss: 0.2324503
[Epoch 62] ogbg-molbace: 0.871212 val loss: 0.500249
[Epoch 62] ogbg-molbace: 0.877313 test loss: 0.473658
[Epoch 63; Iter    18/   36] train: loss: 0.2213481
[Epoch 63] ogbg-molbace: 0.850300 val loss: 0.666499
[Epoch 63] ogbg-molbace: 0.842715 test loss: 0.661491
[Epoch 64; Iter    12/   36] train: loss: 0.2169072
[Epoch 64] ogbg-molbace: 0.875395 val loss: 0.461758
[Epoch 64] ogbg-molbace: 0.871404 test loss: 0.472138
[Epoch 65; Iter     6/   36] train: loss: 0.2160165
[Epoch 65; Iter    36/   36] train: loss: 0.1265876
[Epoch 65] ogbg-molbace: 0.874842 val loss: 0.473305
[Epoch 65] ogbg-molbace: 0.873659 test loss: 0.484639
[Epoch 66; Iter    30/   36] train: loss: 0.3229225
[Epoch 66] ogbg-molbace: 0.872711 val loss: 0.478922
[Epoch 66] ogbg-molbace: 0.873737 test loss: 0.487110
[Epoch 67; Iter    24/   36] train: loss: 0.2716553
[Epoch 67] ogbg-molbace: 0.874290 val loss: 0.511094
[Epoch 67] ogbg-molbace: 0.877857 test loss: 0.474761
[Epoch 68; Iter    18/   36] train: loss: 0.1562981
[Epoch 68] ogbg-molbace: 0.849274 val loss: 0.585537
[Epoch 68] ogbg-molbace: 0.858265 test loss: 0.596448
[Epoch 69; Iter    12/   36] train: loss: 0.2519778
[Epoch 69] ogbg-molbace: 0.860243 val loss: 0.539804
[Epoch 69] ogbg-molbace: 0.882133 test loss: 0.455245
[Epoch 70; Iter     6/   36] train: loss: 0.1841672
[Epoch 70; Iter    36/   36] train: loss: 0.6072252
[Epoch 70] ogbg-molbace: 0.870896 val loss: 0.479768
[Epoch 70] ogbg-molbace: 0.874436 test loss: 0.480084
[Epoch 71; Iter    30/   36] train: loss: 0.2286425
[Epoch 71] ogbg-molbace: 0.871291 val loss: 0.553218
[Epoch 71] ogbg-molbace: 0.884855 test loss: 0.494418
[Epoch 72; Iter    24/   36] train: loss: 0.3157952
[Epoch 72] ogbg-molbace: 0.853378 val loss: 0.621487
[Epoch 72] ogbg-molbace: 0.850334 test loss: 0.713542
[Epoch 73; Iter    18/   36] train: loss: 0.2886713
[Epoch 73] ogbg-molbace: 0.864662 val loss: 0.568252
[Epoch 73] ogbg-molbace: 0.865962 test loss: 0.524863
[Epoch 74; Iter    12/   36] train: loss: 0.1356302
[Epoch 74] ogbg-molbace: 0.863715 val loss: 0.515881
[Epoch 74] ogbg-molbace: 0.852667 test loss: 0.550992
[Epoch 75; Iter     6/   36] train: loss: 0.3318408
[Epoch 75; Iter    36/   36] train: loss: 0.8153553
[Epoch 75] ogbg-molbace: 0.885496 val loss: 0.464305
[Epoch 75] ogbg-molbace: 0.875836 test loss: 0.498138
[Epoch 76; Iter    30/   36] train: loss: 0.2744367
[Epoch 76] ogbg-molbace: 0.869160 val loss: 0.505592
[Epoch 76] ogbg-molbace: 0.883377 test loss: 0.487613
[Epoch 77; Iter    24/   36] train: loss: 0.2045719
[Epoch 77] ogbg-molbace: 0.876499 val loss: 0.521583
[Epoch 77] ogbg-molbace: 0.850879 test loss: 0.614260
[Epoch 78; Iter    18/   36] train: loss: 0.1586995
[Epoch 78] ogbg-molbace: 0.865136 val loss: 0.625859
[Epoch 78] ogbg-molbace: 0.885710 test loss: 0.577395
[Epoch 79; Iter    12/   36] train: loss: 0.1636924
[Epoch 79] ogbg-molbace: 0.876657 val loss: 0.562411
[Epoch 79] ogbg-molbace: 0.867594 test loss: 0.606065
[Epoch 80; Iter     6/   36] train: loss: 0.3140472
[Epoch 80; Iter    36/   36] train: loss: 0.3449176
[Epoch 80] ogbg-molbace: 0.882418 val loss: 0.520587
[Epoch 80] ogbg-molbace: 0.869616 test loss: 0.566550
[Epoch 81; Iter    30/   36] train: loss: 0.2004232
[Epoch 81] ogbg-molbace: 0.887705 val loss: 0.519410
[Epoch 81] ogbg-molbace: 0.885010 test loss: 0.516265
[Epoch 82; Iter    24/   36] train: loss: 0.0746120
[Epoch 34] ogbg-molbace: 0.803977 val loss: 0.556541
[Epoch 34] ogbg-molbace: 0.806251 test loss: 0.556511
[Epoch 35; Iter     6/   36] train: loss: 0.5833918
[Epoch 35; Iter    36/   36] train: loss: 0.4334668
[Epoch 35] ogbg-molbace: 0.854719 val loss: 0.499664
[Epoch 35] ogbg-molbace: 0.852511 test loss: 0.484031
[Epoch 36; Iter    30/   36] train: loss: 0.5301704
[Epoch 36] ogbg-molbace: 0.861348 val loss: 0.559402
[Epoch 36] ogbg-molbace: 0.857254 test loss: 0.561614
[Epoch 37; Iter    24/   36] train: loss: 0.5609531
[Epoch 37] ogbg-molbace: 0.851010 val loss: 0.508603
[Epoch 37] ogbg-molbace: 0.836806 test loss: 0.501556
[Epoch 38; Iter    18/   36] train: loss: 0.3799784
[Epoch 38] ogbg-molbace: 0.840041 val loss: 0.511628
[Epoch 38] ogbg-molbace: 0.848157 test loss: 0.494477
[Epoch 39; Iter    12/   36] train: loss: 0.5435833
[Epoch 39] ogbg-molbace: 0.875000 val loss: 0.457522
[Epoch 39] ogbg-molbace: 0.851034 test loss: 0.484237
[Epoch 40; Iter     6/   36] train: loss: 0.3688946
[Epoch 40; Iter    36/   36] train: loss: 0.3000216
[Epoch 40] ogbg-molbace: 0.863715 val loss: 0.488531
[Epoch 40] ogbg-molbace: 0.877080 test loss: 0.468742
[Epoch 41; Iter    30/   36] train: loss: 0.5615565
[Epoch 41] ogbg-molbace: 0.820944 val loss: 0.549808
[Epoch 41] ogbg-molbace: 0.836806 test loss: 0.532101
[Epoch 42; Iter    24/   36] train: loss: 0.6532286
[Epoch 42] ogbg-molbace: 0.877210 val loss: 0.457384
[Epoch 42] ogbg-molbace: 0.885088 test loss: 0.458527
[Epoch 43; Iter    18/   36] train: loss: 0.3706828
[Epoch 43] ogbg-molbace: 0.874448 val loss: 0.433721
[Epoch 43] ogbg-molbace: 0.859353 test loss: 0.475927
[Epoch 44; Iter    12/   36] train: loss: 0.4974596
[Epoch 44] ogbg-molbace: 0.854956 val loss: 0.517927
[Epoch 44] ogbg-molbace: 0.880578 test loss: 0.492377
[Epoch 45; Iter     6/   36] train: loss: 0.2921497
[Epoch 45; Iter    36/   36] train: loss: 0.1993653
[Epoch 45] ogbg-molbace: 0.853851 val loss: 0.486221
[Epoch 45] ogbg-molbace: 0.843337 test loss: 0.498763
[Epoch 46; Iter    30/   36] train: loss: 0.6236023
[Epoch 46] ogbg-molbace: 0.872396 val loss: 0.454575
[Epoch 46] ogbg-molbace: 0.884155 test loss: 0.434684
[Epoch 47; Iter    24/   36] train: loss: 0.5071218
[Epoch 47] ogbg-molbace: 0.857323 val loss: 0.749881
[Epoch 47] ogbg-molbace: 0.885088 test loss: 0.667276
[Epoch 48; Iter    18/   36] train: loss: 0.2660822
[Epoch 48] ogbg-molbace: 0.857876 val loss: 0.466705
[Epoch 48] ogbg-molbace: 0.859198 test loss: 0.482470
[Epoch 49; Iter    12/   36] train: loss: 0.5498441
[Epoch 49] ogbg-molbace: 0.870660 val loss: 0.453452
[Epoch 49] ogbg-molbace: 0.878712 test loss: 0.441845
[Epoch 50; Iter     6/   36] train: loss: 0.4898159
[Epoch 50; Iter    36/   36] train: loss: 0.4232230
[Epoch 50] ogbg-molbace: 0.862610 val loss: 0.477259
[Epoch 50] ogbg-molbace: 0.869305 test loss: 0.465594
[Epoch 51; Iter    30/   36] train: loss: 0.4823981
[Epoch 51] ogbg-molbace: 0.822601 val loss: 0.574479
[Epoch 51] ogbg-molbace: 0.838672 test loss: 0.546578
[Epoch 52; Iter    24/   36] train: loss: 0.5851303
[Epoch 52] ogbg-molbace: 0.863163 val loss: 0.472120
[Epoch 52] ogbg-molbace: 0.873503 test loss: 0.461114
[Epoch 53; Iter    18/   36] train: loss: 0.2917467
[Epoch 53] ogbg-molbace: 0.877052 val loss: 0.440458
[Epoch 53] ogbg-molbace: 0.891541 test loss: 0.440761
[Epoch 54; Iter    12/   36] train: loss: 0.4285511
[Epoch 54] ogbg-molbace: 0.882813 val loss: 0.441292
[Epoch 54] ogbg-molbace: 0.865962 test loss: 0.463627
[Epoch 55; Iter     6/   36] train: loss: 0.2649767
[Epoch 55; Iter    36/   36] train: loss: 0.2636838
[Epoch 55] ogbg-molbace: 0.892045 val loss: 0.431331
[Epoch 55] ogbg-molbace: 0.880656 test loss: 0.454250
[Epoch 56; Iter    30/   36] train: loss: 0.5190636
[Epoch 56] ogbg-molbace: 0.870186 val loss: 0.488844
[Epoch 56] ogbg-molbace: 0.866428 test loss: 0.500404
[Epoch 57; Iter    24/   36] train: loss: 0.4866681
[Epoch 57] ogbg-molbace: 0.885022 val loss: 0.444969
[Epoch 57] ogbg-molbace: 0.863785 test loss: 0.484939
[Epoch 58; Iter    18/   36] train: loss: 0.1738196
[Epoch 58] ogbg-molbace: 0.890388 val loss: 0.425423
[Epoch 58] ogbg-molbace: 0.888587 test loss: 0.435736
[Epoch 59; Iter    12/   36] train: loss: 0.3433137
[Epoch 59] ogbg-molbace: 0.877604 val loss: 0.477394
[Epoch 59] ogbg-molbace: 0.893251 test loss: 0.440904
[Epoch 60; Iter     6/   36] train: loss: 0.1534392
[Epoch 60; Iter    36/   36] train: loss: 0.2968093
[Epoch 60] ogbg-molbace: 0.846354 val loss: 0.579238
[Epoch 60] ogbg-molbace: 0.852978 test loss: 0.558004
[Epoch 61; Iter    30/   36] train: loss: 0.2690203
[Epoch 61] ogbg-molbace: 0.880287 val loss: 0.465313
[Epoch 61] ogbg-molbace: 0.863007 test loss: 0.542901
[Epoch 62; Iter    24/   36] train: loss: 0.1964884
[Epoch 62] ogbg-molbace: 0.879182 val loss: 0.481250
[Epoch 62] ogbg-molbace: 0.872182 test loss: 0.502287
[Epoch 63; Iter    18/   36] train: loss: 0.2828382
[Epoch 63] ogbg-molbace: 0.881550 val loss: 0.626422
[Epoch 63] ogbg-molbace: 0.877702 test loss: 0.656971
[Epoch 64; Iter    12/   36] train: loss: 0.2872228
[Epoch 64] ogbg-molbace: 0.863952 val loss: 0.497672
[Epoch 64] ogbg-molbace: 0.871793 test loss: 0.498203
[Epoch 65; Iter     6/   36] train: loss: 0.2162243
[Epoch 65; Iter    36/   36] train: loss: 0.2938886
[Epoch 65] ogbg-molbace: 0.881629 val loss: 0.478154
[Epoch 65] ogbg-molbace: 0.862463 test loss: 0.515722
[Epoch 66; Iter    30/   36] train: loss: 0.4746748
[Epoch 66] ogbg-molbace: 0.885417 val loss: 0.455399
[Epoch 66] ogbg-molbace: 0.866506 test loss: 0.532826
[Epoch 67; Iter    24/   36] train: loss: 0.2680494
[Epoch 67] ogbg-molbace: 0.892519 val loss: 0.457102
[Epoch 67] ogbg-molbace: 0.876302 test loss: 0.518637
[Epoch 68; Iter    18/   36] train: loss: 0.2198172
[Epoch 68] ogbg-molbace: 0.894650 val loss: 0.450150
[Epoch 68] ogbg-molbace: 0.872337 test loss: 0.532848
[Epoch 69; Iter    12/   36] train: loss: 0.3584358
[Epoch 69] ogbg-molbace: 0.884943 val loss: 0.502198
[Epoch 69] ogbg-molbace: 0.852589 test loss: 0.571818
[Epoch 70; Iter     6/   36] train: loss: 0.2234332
[Epoch 70; Iter    36/   36] train: loss: 0.4873530
[Epoch 70] ogbg-molbace: 0.881629 val loss: 0.479323
[Epoch 70] ogbg-molbace: 0.881356 test loss: 0.485938
[Epoch 71; Iter    30/   36] train: loss: 0.2589907
[Epoch 71] ogbg-molbace: 0.855587 val loss: 0.576358
[Epoch 71] ogbg-molbace: 0.862308 test loss: 0.548422
[Epoch 72; Iter    24/   36] train: loss: 0.3446362
[Epoch 72] ogbg-molbace: 0.865609 val loss: 0.513237
[Epoch 72] ogbg-molbace: 0.878712 test loss: 0.529449
[Epoch 73; Iter    18/   36] train: loss: 0.2791278
[Epoch 73] ogbg-molbace: 0.879182 val loss: 0.528288
[Epoch 73] ogbg-molbace: 0.877080 test loss: 0.555288
[Epoch 74; Iter    12/   36] train: loss: 0.2253689
[Epoch 74] ogbg-molbace: 0.871922 val loss: 0.574729
[Epoch 74] ogbg-molbace: 0.859431 test loss: 0.646540
[Epoch 75; Iter     6/   36] train: loss: 0.2229546
[Epoch 75; Iter    36/   36] train: loss: 0.1113111
[Epoch 75] ogbg-molbace: 0.878551 val loss: 0.543527
[Epoch 75] ogbg-molbace: 0.877235 test loss: 0.540735
[Epoch 76; Iter    30/   36] train: loss: 0.2127010
[Epoch 76] ogbg-molbace: 0.855114 val loss: 0.557101
[Epoch 76] ogbg-molbace: 0.869927 test loss: 0.548851
[Epoch 77; Iter    24/   36] train: loss: 0.1358200
[Epoch 77] ogbg-molbace: 0.867266 val loss: 0.598637
[Epoch 77] ogbg-molbace: 0.847458 test loss: 0.657997
[Epoch 78; Iter    18/   36] train: loss: 0.1181411
[Epoch 78] ogbg-molbace: 0.883917 val loss: 0.534317
[Epoch 78] ogbg-molbace: 0.881045 test loss: 0.564274
[Epoch 79; Iter    12/   36] train: loss: 0.0572380
[Epoch 79] ogbg-molbace: 0.876578 val loss: 0.648096
[Epoch 79] ogbg-molbace: 0.865495 test loss: 0.678993
[Epoch 80; Iter     6/   36] train: loss: 0.1033695
[Epoch 80; Iter    36/   36] train: loss: 0.3719946
[Epoch 80] ogbg-molbace: 0.865451 val loss: 0.589112
[Epoch 80] ogbg-molbace: 0.859042 test loss: 0.657548
[Epoch 81; Iter    30/   36] train: loss: 0.1670182
[Epoch 81] ogbg-molbace: 0.888021 val loss: 0.494309
[Epoch 81] ogbg-molbace: 0.879801 test loss: 0.549770
[Epoch 82; Iter    24/   36] train: loss: 0.1421113
[Epoch 32] ogbg-molbace: 0.849517 test loss: 0.411285
[Epoch 33; Iter     8/   41] train: loss: 0.4954144
[Epoch 33; Iter    38/   41] train: loss: 0.3472923
[Epoch 33] ogbg-molbace: 0.886252 val loss: 0.412437
[Epoch 33] ogbg-molbace: 0.844776 test loss: 0.446589
[Epoch 34; Iter    27/   41] train: loss: 0.4363592
[Epoch 34] ogbg-molbace: 0.860232 val loss: 0.510426
[Epoch 34] ogbg-molbace: 0.845830 test loss: 0.415172
[Epoch 35; Iter    16/   41] train: loss: 0.4288693
[Epoch 35] ogbg-molbace: 0.905239 val loss: 0.364921
[Epoch 35] ogbg-molbace: 0.881651 test loss: 0.447220
[Epoch 36; Iter     5/   41] train: loss: 0.4982873
[Epoch 36; Iter    35/   41] train: loss: 0.4568651
[Epoch 36] ogbg-molbace: 0.931962 val loss: 0.407176
[Epoch 36] ogbg-molbace: 0.855487 test loss: 0.408611
[Epoch 37; Iter    24/   41] train: loss: 0.3946474
[Epoch 37] ogbg-molbace: 0.861463 val loss: 0.423274
[Epoch 37] ogbg-molbace: 0.843020 test loss: 0.436593
[Epoch 38; Iter    13/   41] train: loss: 0.3926855
[Epoch 38] ogbg-molbace: 0.900844 val loss: 0.399379
[Epoch 38] ogbg-molbace: 0.842142 test loss: 0.426590
[Epoch 39; Iter     2/   41] train: loss: 0.3429592
[Epoch 39; Iter    32/   41] train: loss: 0.5831904
[Epoch 39] ogbg-molbace: 0.904184 val loss: 0.449742
[Epoch 39] ogbg-molbace: 0.863565 test loss: 0.398424
[Epoch 40; Iter    21/   41] train: loss: 0.3551339
[Epoch 40] ogbg-molbace: 0.908052 val loss: 0.358132
[Epoch 40] ogbg-molbace: 0.893942 test loss: 0.361222
[Epoch 41; Iter    10/   41] train: loss: 0.3559825
[Epoch 41; Iter    40/   41] train: loss: 0.3951375
[Epoch 41] ogbg-molbace: 0.881857 val loss: 0.428416
[Epoch 41] ogbg-molbace: 0.855312 test loss: 0.401095
[Epoch 42; Iter    29/   41] train: loss: 0.3715883
[Epoch 42] ogbg-molbace: 0.908755 val loss: 0.400572
[Epoch 42] ogbg-molbace: 0.873924 test loss: 0.376219
[Epoch 43; Iter    18/   41] train: loss: 0.3398471
[Epoch 43] ogbg-molbace: 0.891702 val loss: 0.362048
[Epoch 43] ogbg-molbace: 0.873222 test loss: 0.382997
[Epoch 44; Iter     7/   41] train: loss: 0.5342038
[Epoch 44; Iter    37/   41] train: loss: 0.3376428
[Epoch 44] ogbg-molbace: 0.922644 val loss: 0.373173
[Epoch 44] ogbg-molbace: 0.875505 test loss: 0.384850
[Epoch 45; Iter    26/   41] train: loss: 0.3818308
[Epoch 45] ogbg-molbace: 0.909459 val loss: 0.395273
[Epoch 45] ogbg-molbace: 0.871642 test loss: 0.378157
[Epoch 46; Iter    15/   41] train: loss: 0.4009373
[Epoch 46] ogbg-molbace: 0.883790 val loss: 0.441647
[Epoch 46] ogbg-molbace: 0.880948 test loss: 0.468135
[Epoch 47; Iter     4/   41] train: loss: 0.3646739
[Epoch 47; Iter    34/   41] train: loss: 0.8055211
[Epoch 47] ogbg-molbace: 0.896976 val loss: 0.459821
[Epoch 47] ogbg-molbace: 0.854083 test loss: 0.453929
[Epoch 48; Iter    23/   41] train: loss: 0.3617528
[Epoch 48] ogbg-molbace: 0.921414 val loss: 0.366626
[Epoch 48] ogbg-molbace: 0.881475 test loss: 0.370088
[Epoch 49; Iter    12/   41] train: loss: 0.3228056
[Epoch 49] ogbg-molbace: 0.900844 val loss: 0.347171
[Epoch 49] ogbg-molbace: 0.865672 test loss: 0.381178
[Epoch 50; Iter     1/   41] train: loss: 0.3926480
[Epoch 50; Iter    31/   41] train: loss: 0.2466129
[Epoch 50] ogbg-molbace: 0.909986 val loss: 0.346304
[Epoch 50] ogbg-molbace: 0.878841 test loss: 0.395826
[Epoch 51; Iter    20/   41] train: loss: 0.3244168
[Epoch 51] ogbg-molbace: 0.873594 val loss: 0.448162
[Epoch 51] ogbg-molbace: 0.861633 test loss: 0.396121
[Epoch 52; Iter     9/   41] train: loss: 0.2074134
[Epoch 52; Iter    39/   41] train: loss: 0.2213375
[Epoch 52] ogbg-molbace: 0.893987 val loss: 0.384961
[Epoch 52] ogbg-molbace: 0.883933 test loss: 0.434564
[Epoch 53; Iter    28/   41] train: loss: 0.3258756
[Epoch 53] ogbg-molbace: 0.878868 val loss: 0.475239
[Epoch 53] ogbg-molbace: 0.877963 test loss: 0.380499
[Epoch 54; Iter    17/   41] train: loss: 0.2963912
[Epoch 54] ogbg-molbace: 0.893812 val loss: 0.429563
[Epoch 54] ogbg-molbace: 0.871642 test loss: 0.384135
[Epoch 55; Iter     6/   41] train: loss: 0.2213426
[Epoch 55; Iter    36/   41] train: loss: 0.4959440
[Epoch 55] ogbg-molbace: 0.877286 val loss: 0.398693
[Epoch 55] ogbg-molbace: 0.857946 test loss: 0.445691
[Epoch 56; Iter    25/   41] train: loss: 0.4215773
[Epoch 56] ogbg-molbace: 0.903657 val loss: 0.439415
[Epoch 56] ogbg-molbace: 0.885689 test loss: 0.388639
[Epoch 57; Iter    14/   41] train: loss: 0.2922710
[Epoch 57] ogbg-molbace: 0.922820 val loss: 0.459169
[Epoch 57] ogbg-molbace: 0.886743 test loss: 0.608907
[Epoch 58; Iter     3/   41] train: loss: 0.3681034
[Epoch 58; Iter    33/   41] train: loss: 0.2193627
[Epoch 58] ogbg-molbace: 0.903129 val loss: 0.580825
[Epoch 58] ogbg-molbace: 0.869710 test loss: 0.472740
[Epoch 59; Iter    22/   41] train: loss: 0.4145446
[Epoch 59] ogbg-molbace: 0.906294 val loss: 0.426607
[Epoch 59] ogbg-molbace: 0.892186 test loss: 0.455481
[Epoch 60; Iter    11/   41] train: loss: 0.1203309
[Epoch 60; Iter    41/   41] train: loss: 0.2741894
[Epoch 60] ogbg-molbace: 0.873945 val loss: 0.503789
[Epoch 60] ogbg-molbace: 0.864442 test loss: 0.425389
[Epoch 61; Iter    30/   41] train: loss: 0.5630289
[Epoch 61] ogbg-molbace: 0.899789 val loss: 0.421023
[Epoch 61] ogbg-molbace: 0.881124 test loss: 0.373151
[Epoch 62; Iter    19/   41] train: loss: 0.1849573
[Epoch 62] ogbg-molbace: 0.892757 val loss: 0.378967
[Epoch 62] ogbg-molbace: 0.894469 test loss: 0.370730
[Epoch 63; Iter     8/   41] train: loss: 0.2184853
[Epoch 63; Iter    38/   41] train: loss: 0.1737686
[Epoch 63] ogbg-molbace: 0.870429 val loss: 0.834177
[Epoch 63] ogbg-molbace: 0.868306 test loss: 0.469677
[Epoch 64; Iter    27/   41] train: loss: 0.2568607
[Epoch 64] ogbg-molbace: 0.922293 val loss: 0.321476
[Epoch 64] ogbg-molbace: 0.887445 test loss: 0.397892
[Epoch 65; Iter    16/   41] train: loss: 0.2014273
[Epoch 65] ogbg-molbace: 0.880802 val loss: 0.441762
[Epoch 65] ogbg-molbace: 0.876558 test loss: 0.409561
[Epoch 66; Iter     5/   41] train: loss: 0.1911178
[Epoch 66; Iter    35/   41] train: loss: 0.1243820
[Epoch 66] ogbg-molbace: 0.888713 val loss: 0.375580
[Epoch 66] ogbg-molbace: 0.894118 test loss: 0.379422
[Epoch 67; Iter    24/   41] train: loss: 0.3219092
[Epoch 67] ogbg-molbace: 0.892932 val loss: 0.487829
[Epoch 67] ogbg-molbace: 0.882002 test loss: 0.561874
[Epoch 68; Iter    13/   41] train: loss: 0.4529874
[Epoch 68] ogbg-molbace: 0.890120 val loss: 0.381526
[Epoch 68] ogbg-molbace: 0.896225 test loss: 0.384938
[Epoch 69; Iter     2/   41] train: loss: 0.2825845
[Epoch 69; Iter    32/   41] train: loss: 0.2428869
[Epoch 69] ogbg-molbace: 0.921941 val loss: 0.321895
[Epoch 69] ogbg-molbace: 0.869710 test loss: 0.420230
[Epoch 70; Iter    21/   41] train: loss: 0.3249549
[Epoch 70] ogbg-molbace: 0.920183 val loss: 0.364343
[Epoch 70] ogbg-molbace: 0.878139 test loss: 0.488880
[Epoch 71; Iter    10/   41] train: loss: 0.2066722
[Epoch 71; Iter    40/   41] train: loss: 0.3207277
[Epoch 71] ogbg-molbace: 0.877637 val loss: 0.487434
[Epoch 71] ogbg-molbace: 0.883231 test loss: 0.421357
[Epoch 72; Iter    29/   41] train: loss: 0.5061735
[Epoch 72] ogbg-molbace: 0.910865 val loss: 0.497217
[Epoch 72] ogbg-molbace: 0.900790 test loss: 0.582516
[Epoch 73; Iter    18/   41] train: loss: 0.3287831
[Epoch 73] ogbg-molbace: 0.866034 val loss: 0.611731
[Epoch 73] ogbg-molbace: 0.857594 test loss: 0.450893
[Epoch 74; Iter     7/   41] train: loss: 0.1952740
[Epoch 74; Iter    37/   41] train: loss: 0.1974891
[Epoch 74] ogbg-molbace: 0.860584 val loss: 0.591935
[Epoch 74] ogbg-molbace: 0.885865 test loss: 0.415797
[Epoch 75; Iter    26/   41] train: loss: 0.4368377
[Epoch 75] ogbg-molbace: 0.878868 val loss: 0.445950
[Epoch 75] ogbg-molbace: 0.890255 test loss: 0.395287
[Epoch 76; Iter    15/   41] train: loss: 0.0869608
[Epoch 76] ogbg-molbace: 0.902075 val loss: 0.512420
[Epoch 76] ogbg-molbace: 0.875154 test loss: 0.441696
[Epoch 77; Iter     4/   41] train: loss: 0.3597826
[Epoch 77; Iter    34/   41] train: loss: 0.6376742
[Epoch 77] ogbg-molbace: 0.913326 val loss: 0.343871
[Epoch 77] ogbg-molbace: 0.877788 test loss: 0.441835
[Epoch 78; Iter    23/   41] train: loss: 0.1394327
[Epoch 36; Iter    25/   31] train: loss: 0.4145534
[Epoch 36] ogbg-molbace: 0.854094 val loss: 0.539064
[Epoch 36] ogbg-molbace: 0.830992 test loss: 0.537803
[Epoch 37; Iter    24/   31] train: loss: 0.4815057
[Epoch 37] ogbg-molbace: 0.856784 val loss: 0.474761
[Epoch 37] ogbg-molbace: 0.857517 test loss: 0.463048
[Epoch 38; Iter    23/   31] train: loss: 0.3860695
[Epoch 38] ogbg-molbace: 0.842256 val loss: 0.511705
[Epoch 38] ogbg-molbace: 0.820522 test loss: 0.503985
[Epoch 39; Iter    22/   31] train: loss: 0.7934862
[Epoch 39] ogbg-molbace: 0.836517 val loss: 0.505306
[Epoch 39] ogbg-molbace: 0.826150 test loss: 0.486664
[Epoch 40; Iter    21/   31] train: loss: 0.4382450
[Epoch 40] ogbg-molbace: 0.861941 val loss: 0.469312
[Epoch 40] ogbg-molbace: 0.860222 test loss: 0.458882
[Epoch 41; Iter    20/   31] train: loss: 0.3530443
[Epoch 41] ogbg-molbace: 0.858802 val loss: 0.491880
[Epoch 41] ogbg-molbace: 0.862228 test loss: 0.471945
[Epoch 42; Iter    19/   31] train: loss: 0.4238676
[Epoch 42] ogbg-molbace: 0.855798 val loss: 0.467543
[Epoch 42] ogbg-molbace: 0.841201 test loss: 0.470877
[Epoch 43; Iter    18/   31] train: loss: 0.5252363
[Epoch 43] ogbg-molbace: 0.841942 val loss: 0.524570
[Epoch 43] ogbg-molbace: 0.830992 test loss: 0.527532
[Epoch 44; Iter    17/   31] train: loss: 0.5617747
[Epoch 44] ogbg-molbace: 0.849117 val loss: 0.554971
[Epoch 44] ogbg-molbace: 0.832650 test loss: 0.536284
[Epoch 45; Iter    16/   31] train: loss: 0.5402948
[Epoch 45] ogbg-molbace: 0.871671 val loss: 0.436061
[Epoch 45] ogbg-molbace: 0.864628 test loss: 0.466192
[Epoch 46; Iter    15/   31] train: loss: 0.5260327
[Epoch 46] ogbg-molbace: 0.840059 val loss: 0.479521
[Epoch 46] ogbg-molbace: 0.832824 test loss: 0.483410
[Epoch 47; Iter    14/   31] train: loss: 0.5615647
[Epoch 47] ogbg-molbace: 0.863824 val loss: 0.534490
[Epoch 47] ogbg-molbace: 0.865893 test loss: 0.485747
[Epoch 48; Iter    13/   31] train: loss: 0.3338951
[Epoch 48] ogbg-molbace: 0.865169 val loss: 0.458220
[Epoch 48] ogbg-molbace: 0.850406 test loss: 0.449600
[Epoch 49; Iter    12/   31] train: loss: 0.3622491
[Epoch 49] ogbg-molbace: 0.802439 val loss: 0.624333
[Epoch 49] ogbg-molbace: 0.771181 test loss: 0.625191
[Epoch 50; Iter    11/   31] train: loss: 0.2791169
[Epoch 50] ogbg-molbace: 0.874316 val loss: 0.491595
[Epoch 50] ogbg-molbace: 0.863668 test loss: 0.488103
[Epoch 51; Iter    10/   31] train: loss: 0.3276664
[Epoch 51] ogbg-molbace: 0.861492 val loss: 0.539729
[Epoch 51] ogbg-molbace: 0.851366 test loss: 0.516419
[Epoch 52; Iter     9/   31] train: loss: 0.5049623
[Epoch 52] ogbg-molbace: 0.803471 val loss: 0.671091
[Epoch 52] ogbg-molbace: 0.779906 test loss: 0.672127
[Epoch 53; Iter     8/   31] train: loss: 0.4561867
[Epoch 53] ogbg-molbace: 0.896422 val loss: 0.410608
[Epoch 53] ogbg-molbace: 0.884740 test loss: 0.409498
[Epoch 54; Iter     7/   31] train: loss: 0.4949625
[Epoch 54] ogbg-molbace: 0.878397 val loss: 0.439855
[Epoch 54] ogbg-molbace: 0.865326 test loss: 0.447278
[Epoch 55; Iter     6/   31] train: loss: 0.3376545
[Epoch 55] ogbg-molbace: 0.831091 val loss: 0.545919
[Epoch 55] ogbg-molbace: 0.801152 test loss: 0.593401
[Epoch 56; Iter     5/   31] train: loss: 0.4846642
[Epoch 56] ogbg-molbace: 0.885033 val loss: 0.476874
[Epoch 56] ogbg-molbace: 0.873920 test loss: 0.470930
[Epoch 57; Iter     4/   31] train: loss: 0.3397942
[Epoch 57] ogbg-molbace: 0.881939 val loss: 0.410607
[Epoch 57] ogbg-molbace: 0.879199 test loss: 0.417419
[Epoch 58; Iter     3/   31] train: loss: 0.2534812
[Epoch 58] ogbg-molbace: 0.864138 val loss: 0.444387
[Epoch 58] ogbg-molbace: 0.862621 test loss: 0.431390
[Epoch 59; Iter     2/   31] train: loss: 0.3683676
[Epoch 59] ogbg-molbace: 0.869205 val loss: 0.439977
[Epoch 59] ogbg-molbace: 0.864017 test loss: 0.441434
[Epoch 60; Iter     1/   31] train: loss: 0.4105222
[Epoch 60; Iter    31/   31] train: loss: 0.1943636
[Epoch 60] ogbg-molbace: 0.878845 val loss: 0.448782
[Epoch 60] ogbg-molbace: 0.863188 test loss: 0.469354
[Epoch 61; Iter    30/   31] train: loss: 0.3344146
[Epoch 61] ogbg-molbace: 0.862165 val loss: 0.476439
[Epoch 61] ogbg-molbace: 0.852543 test loss: 0.489961
[Epoch 62; Iter    29/   31] train: loss: 0.3196470
[Epoch 62] ogbg-molbace: 0.878352 val loss: 0.419822
[Epoch 62] ogbg-molbace: 0.877061 test loss: 0.428639
[Epoch 63; Iter    28/   31] train: loss: 0.1949719
[Epoch 63] ogbg-molbace: 0.873375 val loss: 0.431493
[Epoch 63] ogbg-molbace: 0.861007 test loss: 0.443960
[Epoch 64; Iter    27/   31] train: loss: 0.3791983
[Epoch 64] ogbg-molbace: 0.877948 val loss: 0.479039
[Epoch 64] ogbg-molbace: 0.871041 test loss: 0.465631
[Epoch 65; Iter    26/   31] train: loss: 0.2822945
[Epoch 65] ogbg-molbace: 0.875078 val loss: 0.460240
[Epoch 65] ogbg-molbace: 0.864715 test loss: 0.472189
[Epoch 66; Iter    25/   31] train: loss: 0.3277623
[Epoch 66] ogbg-molbace: 0.868308 val loss: 0.432276
[Epoch 66] ogbg-molbace: 0.846218 test loss: 0.473090
[Epoch 67; Iter    24/   31] train: loss: 0.3131151
[Epoch 67] ogbg-molbace: 0.879607 val loss: 0.422345
[Epoch 67] ogbg-molbace: 0.871564 test loss: 0.430660
[Epoch 68; Iter    23/   31] train: loss: 0.3288667
[Epoch 68] ogbg-molbace: 0.885930 val loss: 0.398936
[Epoch 68] ogbg-molbace: 0.885438 test loss: 0.404391
[Epoch 69; Iter    22/   31] train: loss: 0.2875379
[Epoch 69] ogbg-molbace: 0.885347 val loss: 0.531415
[Epoch 69] ogbg-molbace: 0.881293 test loss: 0.526050
[Epoch 70; Iter    21/   31] train: loss: 0.3173828
[Epoch 70] ogbg-molbace: 0.886199 val loss: 0.478502
[Epoch 70] ogbg-molbace: 0.861618 test loss: 0.524603
[Epoch 71; Iter    20/   31] train: loss: 0.3781219
[Epoch 71] ogbg-molbace: 0.880280 val loss: 0.447787
[Epoch 71] ogbg-molbace: 0.875447 test loss: 0.463934
[Epoch 72; Iter    19/   31] train: loss: 0.2275002
[Epoch 72] ogbg-molbace: 0.885033 val loss: 0.425619
[Epoch 72] ogbg-molbace: 0.861749 test loss: 0.466684
[Epoch 73; Iter    18/   31] train: loss: 0.5273536
[Epoch 73] ogbg-molbace: 0.876603 val loss: 0.447288
[Epoch 73] ogbg-molbace: 0.870997 test loss: 0.450867
[Epoch 74; Iter    17/   31] train: loss: 0.2395832
[Epoch 74] ogbg-molbace: 0.875617 val loss: 0.457370
[Epoch 74] ogbg-molbace: 0.866286 test loss: 0.509158
[Epoch 75; Iter    16/   31] train: loss: 0.3076842
[Epoch 75] ogbg-molbace: 0.884853 val loss: 0.446021
[Epoch 75] ogbg-molbace: 0.861574 test loss: 0.481696
[Epoch 76; Iter    15/   31] train: loss: 0.2470652
[Epoch 76] ogbg-molbace: 0.889606 val loss: 0.439830
[Epoch 76] ogbg-molbace: 0.871608 test loss: 0.471224
[Epoch 77; Iter    14/   31] train: loss: 0.3433942
[Epoch 77] ogbg-molbace: 0.883374 val loss: 0.434838
[Epoch 77] ogbg-molbace: 0.871434 test loss: 0.451667
[Epoch 78; Iter    13/   31] train: loss: 0.2104874
[Epoch 78] ogbg-molbace: 0.859744 val loss: 0.495712
[Epoch 78] ogbg-molbace: 0.859218 test loss: 0.528125
[Epoch 79; Iter    12/   31] train: loss: 0.1729553
[Epoch 79] ogbg-molbace: 0.888396 val loss: 0.464060
[Epoch 79] ogbg-molbace: 0.879417 test loss: 0.479789
[Epoch 80; Iter    11/   31] train: loss: 0.1389534
[Epoch 80] ogbg-molbace: 0.889292 val loss: 0.438768
[Epoch 80] ogbg-molbace: 0.868554 test loss: 0.481516
[Epoch 81; Iter    10/   31] train: loss: 0.2186187
[Epoch 81] ogbg-molbace: 0.886154 val loss: 0.546578
[Epoch 81] ogbg-molbace: 0.869907 test loss: 0.544282
[Epoch 82; Iter     9/   31] train: loss: 0.3283360
[Epoch 82] ogbg-molbace: 0.895256 val loss: 0.406483
[Epoch 82] ogbg-molbace: 0.882558 test loss: 0.449432
[Epoch 83; Iter     8/   31] train: loss: 0.1920215
[Epoch 83] ogbg-molbace: 0.899830 val loss: 0.396786
[Epoch 83] ogbg-molbace: 0.882995 test loss: 0.460867
[Epoch 84; Iter     7/   31] train: loss: 0.1121902
[Epoch 84] ogbg-molbace: 0.900502 val loss: 0.445779
[Epoch 84] ogbg-molbace: 0.866504 test loss: 0.548282
[Epoch 85; Iter     6/   31] train: loss: 0.1078679
[Epoch 85] ogbg-molbace: 0.883105 val loss: 0.473007
[Epoch 85] ogbg-molbace: 0.871215 test loss: 0.529808
[Epoch 86; Iter     5/   31] train: loss: 0.1726747
[Epoch 86] ogbg-molbace: 0.900413 val loss: 0.450413
[Epoch 86] ogbg-molbace: 0.873789 test loss: 0.554107
[Epoch 36; Iter    25/   31] train: loss: 0.4737695
[Epoch 36] ogbg-molbace: 0.869294 val loss: 0.465135
[Epoch 36] ogbg-molbace: 0.868118 test loss: 0.451867
[Epoch 37; Iter    24/   31] train: loss: 0.4005468
[Epoch 37] ogbg-molbace: 0.836651 val loss: 0.579282
[Epoch 37] ogbg-molbace: 0.811927 test loss: 0.573785
[Epoch 38; Iter    23/   31] train: loss: 0.5224829
[Epoch 38] ogbg-molbace: 0.840552 val loss: 0.528127
[Epoch 38] ogbg-molbace: 0.861530 test loss: 0.489024
[Epoch 39; Iter    22/   31] train: loss: 0.4651889
[Epoch 39] ogbg-molbace: 0.823917 val loss: 0.518568
[Epoch 39] ogbg-molbace: 0.828811 test loss: 0.503558
[Epoch 40; Iter    21/   31] train: loss: 0.6291508
[Epoch 40] ogbg-molbace: 0.859071 val loss: 0.554038
[Epoch 40] ogbg-molbace: 0.843513 test loss: 0.541807
[Epoch 41; Iter    20/   31] train: loss: 0.4524561
[Epoch 41] ogbg-molbace: 0.855753 val loss: 0.485451
[Epoch 41] ogbg-molbace: 0.842029 test loss: 0.471412
[Epoch 42; Iter    19/   31] train: loss: 0.5295126
[Epoch 42] ogbg-molbace: 0.864003 val loss: 0.497401
[Epoch 42] ogbg-molbace: 0.852936 test loss: 0.481193
[Epoch 43; Iter    18/   31] train: loss: 0.4596465
[Epoch 43] ogbg-molbace: 0.852838 val loss: 0.508716
[Epoch 43] ogbg-molbace: 0.861356 test loss: 0.479129
[Epoch 44; Iter    17/   31] train: loss: 0.5867717
[Epoch 44] ogbg-molbace: 0.867949 val loss: 0.463295
[Epoch 44] ogbg-molbace: 0.873353 test loss: 0.456456
[Epoch 45; Iter    16/   31] train: loss: 0.4601690
[Epoch 45] ogbg-molbace: 0.870819 val loss: 0.435105
[Epoch 45] ogbg-molbace: 0.857081 test loss: 0.438612
[Epoch 46; Iter    15/   31] train: loss: 0.4960678
[Epoch 46] ogbg-molbace: 0.842794 val loss: 0.513338
[Epoch 46] ogbg-molbace: 0.856426 test loss: 0.447108
[Epoch 47; Iter    14/   31] train: loss: 0.4813096
[Epoch 47] ogbg-molbace: 0.877679 val loss: 0.425591
[Epoch 47] ogbg-molbace: 0.849664 test loss: 0.450828
[Epoch 48; Iter    13/   31] train: loss: 0.5690162
[Epoch 48] ogbg-molbace: 0.876020 val loss: 0.441163
[Epoch 48] ogbg-molbace: 0.876276 test loss: 0.435950
[Epoch 49; Iter    12/   31] train: loss: 0.3383420
[Epoch 49] ogbg-molbace: 0.889472 val loss: 0.414454
[Epoch 49] ogbg-molbace: 0.869689 test loss: 0.421975
[Epoch 50; Iter    11/   31] train: loss: 0.3817265
[Epoch 50] ogbg-molbace: 0.880190 val loss: 0.476968
[Epoch 50] ogbg-molbace: 0.873222 test loss: 0.453188
[Epoch 51; Iter    10/   31] train: loss: 0.2634915
[Epoch 51] ogbg-molbace: 0.844364 val loss: 0.482758
[Epoch 51] ogbg-molbace: 0.850144 test loss: 0.447900
[Epoch 52; Iter     9/   31] train: loss: 0.4595796
[Epoch 52] ogbg-molbace: 0.869563 val loss: 0.472770
[Epoch 52] ogbg-molbace: 0.856383 test loss: 0.474062
[Epoch 53; Iter     8/   31] train: loss: 0.3401125
[Epoch 53] ogbg-molbace: 0.876289 val loss: 0.431493
[Epoch 53] ogbg-molbace: 0.872088 test loss: 0.414924
[Epoch 54; Iter     7/   31] train: loss: 0.4828833
[Epoch 54] ogbg-molbace: 0.861134 val loss: 0.568108
[Epoch 54] ogbg-molbace: 0.827327 test loss: 0.483986
[Epoch 55; Iter     6/   31] train: loss: 0.3325458
[Epoch 55] ogbg-molbace: 0.863600 val loss: 0.478965
[Epoch 55] ogbg-molbace: 0.846130 test loss: 0.455574
[Epoch 56; Iter     5/   31] train: loss: 0.2793774
[Epoch 56] ogbg-molbace: 0.868622 val loss: 0.467260
[Epoch 56] ogbg-molbace: 0.865326 test loss: 0.437281
[Epoch 57; Iter     4/   31] train: loss: 0.3399084
[Epoch 57] ogbg-molbace: 0.872029 val loss: 0.446835
[Epoch 57] ogbg-molbace: 0.867594 test loss: 0.437458
[Epoch 58; Iter     3/   31] train: loss: 0.4111921
[Epoch 58] ogbg-molbace: 0.873150 val loss: 0.614406
[Epoch 58] ogbg-molbace: 0.859175 test loss: 0.591871
[Epoch 59; Iter     2/   31] train: loss: 0.2635417
[Epoch 59] ogbg-molbace: 0.884405 val loss: 0.412836
[Epoch 59] ogbg-molbace: 0.874618 test loss: 0.430537
[Epoch 60; Iter     1/   31] train: loss: 0.3203833
[Epoch 60; Iter    31/   31] train: loss: 0.5049211
[Epoch 60] ogbg-molbace: 0.886512 val loss: 0.452607
[Epoch 60] ogbg-molbace: 0.855423 test loss: 0.483888
[Epoch 61; Iter    30/   31] train: loss: 0.5350487
[Epoch 61] ogbg-molbace: 0.859116 val loss: 0.478721
[Epoch 61] ogbg-molbace: 0.863450 test loss: 0.447983
[Epoch 62; Iter    29/   31] train: loss: 0.4595318
[Epoch 62] ogbg-molbace: 0.880549 val loss: 0.449280
[Epoch 62] ogbg-molbace: 0.861749 test loss: 0.482961
[Epoch 63; Iter    28/   31] train: loss: 0.4497669
[Epoch 63] ogbg-molbace: 0.879831 val loss: 0.427116
[Epoch 63] ogbg-molbace: 0.872175 test loss: 0.440531
[Epoch 64; Iter    27/   31] train: loss: 0.3546914
[Epoch 64] ogbg-molbace: 0.876693 val loss: 0.433028
[Epoch 64] ogbg-molbace: 0.856426 test loss: 0.480955
[Epoch 65; Iter    26/   31] train: loss: 0.2124818
[Epoch 65] ogbg-molbace: 0.872926 val loss: 0.439941
[Epoch 65] ogbg-molbace: 0.865937 test loss: 0.442023
[Epoch 66; Iter    25/   31] train: loss: 0.3369936
[Epoch 66] ogbg-molbace: 0.899426 val loss: 0.389807
[Epoch 66] ogbg-molbace: 0.864759 test loss: 0.438062
[Epoch 67; Iter    24/   31] train: loss: 0.3383769
[Epoch 67] ogbg-molbace: 0.871312 val loss: 0.449746
[Epoch 67] ogbg-molbace: 0.867027 test loss: 0.467561
[Epoch 68; Iter    23/   31] train: loss: 0.6853045
[Epoch 68] ogbg-molbace: 0.898260 val loss: 0.406672
[Epoch 68] ogbg-molbace: 0.871826 test loss: 0.467316
[Epoch 69; Iter    22/   31] train: loss: 0.3091372
[Epoch 69] ogbg-molbace: 0.902027 val loss: 0.377196
[Epoch 69] ogbg-molbace: 0.872175 test loss: 0.438068
[Epoch 70; Iter    21/   31] train: loss: 0.5641322
[Epoch 70] ogbg-molbace: 0.866156 val loss: 0.444593
[Epoch 70] ogbg-molbace: 0.860963 test loss: 0.449182
[Epoch 71; Iter    20/   31] train: loss: 0.3238482
[Epoch 71] ogbg-molbace: 0.879024 val loss: 0.443460
[Epoch 71] ogbg-molbace: 0.852849 test loss: 0.485396
[Epoch 72; Iter    19/   31] train: loss: 0.2865177
[Epoch 72] ogbg-molbace: 0.861851 val loss: 0.540541
[Epoch 72] ogbg-molbace: 0.832126 test loss: 0.588210
[Epoch 73; Iter    18/   31] train: loss: 0.1530685
[Epoch 73] ogbg-molbace: 0.864855 val loss: 0.459686
[Epoch 73] ogbg-molbace: 0.860003 test loss: 0.469386
[Epoch 74; Iter    17/   31] train: loss: 0.3539308
[Epoch 74] ogbg-molbace: 0.880235 val loss: 0.418616
[Epoch 74] ogbg-molbace: 0.872742 test loss: 0.441539
[Epoch 75; Iter    16/   31] train: loss: 0.1956347
[Epoch 75] ogbg-molbace: 0.889786 val loss: 0.415426
[Epoch 75] ogbg-molbace: 0.863494 test loss: 0.457276
[Epoch 76; Iter    15/   31] train: loss: 0.4036565
[Epoch 76] ogbg-molbace: 0.896467 val loss: 0.407819
[Epoch 76] ogbg-molbace: 0.876712 test loss: 0.473035
[Epoch 77; Iter    14/   31] train: loss: 0.5630455
[Epoch 77] ogbg-molbace: 0.881715 val loss: 0.520221
[Epoch 77] ogbg-molbace: 0.865980 test loss: 0.554141
[Epoch 78; Iter    13/   31] train: loss: 0.2278485
[Epoch 78] ogbg-molbace: 0.878352 val loss: 0.549167
[Epoch 78] ogbg-molbace: 0.870474 test loss: 0.564584
[Epoch 79; Iter    12/   31] train: loss: 0.2005640
[Epoch 79] ogbg-molbace: 0.889382 val loss: 0.466658
[Epoch 79] ogbg-molbace: 0.864846 test loss: 0.519790
[Epoch 80; Iter    11/   31] train: loss: 0.2980585
[Epoch 80] ogbg-molbace: 0.888351 val loss: 0.463955
[Epoch 80] ogbg-molbace: 0.847745 test loss: 0.553416
[Epoch 81; Iter    10/   31] train: loss: 0.0903600
[Epoch 81] ogbg-molbace: 0.853914 val loss: 0.609637
[Epoch 81] ogbg-molbace: 0.834002 test loss: 0.643948
[Epoch 82; Iter     9/   31] train: loss: 0.3997668
[Epoch 82] ogbg-molbace: 0.899247 val loss: 0.601391
[Epoch 82] ogbg-molbace: 0.869776 test loss: 0.606267
[Epoch 83; Iter     8/   31] train: loss: 0.3986436
[Epoch 83] ogbg-molbace: 0.888530 val loss: 0.457510
[Epoch 83] ogbg-molbace: 0.855030 test loss: 0.524313
[Epoch 84; Iter     7/   31] train: loss: 0.3037365
[Epoch 84] ogbg-molbace: 0.902206 val loss: 0.461381
[Epoch 84] ogbg-molbace: 0.865849 test loss: 0.524491
[Epoch 85; Iter     6/   31] train: loss: 0.1415682
[Epoch 85] ogbg-molbace: 0.867994 val loss: 0.496126
[Epoch 85] ogbg-molbace: 0.844647 test loss: 0.546996
[Epoch 86; Iter     5/   31] train: loss: 0.5244604
[Epoch 86] ogbg-molbace: 0.890593 val loss: 0.452833
[Epoch 86] ogbg-molbace: 0.864453 test loss: 0.510897
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 82] ogbg-molbace: 0.881708 val loss: 0.571429
[Epoch 82] ogbg-molbace: 0.884077 test loss: 0.611331
[Epoch 83; Iter    18/   36] train: loss: 0.0773428
[Epoch 83] ogbg-molbace: 0.880840 val loss: 0.577289
[Epoch 83] ogbg-molbace: 0.874670 test loss: 0.562153
[Epoch 84; Iter    12/   36] train: loss: 0.0417529
[Epoch 84] ogbg-molbace: 0.878157 val loss: 0.689345
[Epoch 84] ogbg-molbace: 0.874281 test loss: 0.658646
[Epoch 85; Iter     6/   36] train: loss: 0.2924886
[Epoch 85; Iter    36/   36] train: loss: 0.1268405
[Epoch 85] ogbg-molbace: 0.886600 val loss: 0.560766
[Epoch 85] ogbg-molbace: 0.880967 test loss: 0.589889
[Epoch 86; Iter    30/   36] train: loss: 0.1499281
[Epoch 86] ogbg-molbace: 0.870896 val loss: 0.635490
[Epoch 86] ogbg-molbace: 0.863785 test loss: 0.689573
[Epoch 87; Iter    24/   36] train: loss: 0.0549443
[Epoch 87] ogbg-molbace: 0.875631 val loss: 0.628763
[Epoch 87] ogbg-molbace: 0.876147 test loss: 0.625838
[Epoch 88; Iter    18/   36] train: loss: 0.0806109
[Epoch 88] ogbg-molbace: 0.874605 val loss: 0.650971
[Epoch 88] ogbg-molbace: 0.878557 test loss: 0.628823
[Epoch 89; Iter    12/   36] train: loss: 0.1658387
[Epoch 89] ogbg-molbace: 0.876184 val loss: 0.690758
[Epoch 89] ogbg-molbace: 0.859586 test loss: 0.769250
[Epoch 90; Iter     6/   36] train: loss: 0.0665850
[Epoch 90; Iter    36/   36] train: loss: 0.0355687
[Epoch 90] ogbg-molbace: 0.881471 val loss: 0.711033
[Epoch 90] ogbg-molbace: 0.881434 test loss: 0.705583
[Epoch 91; Iter    30/   36] train: loss: 0.1690749
[Epoch 91] ogbg-molbace: 0.868845 val loss: 0.690332
[Epoch 91] ogbg-molbace: 0.857409 test loss: 0.732978
[Epoch 92; Iter    24/   36] train: loss: 0.0161865
[Epoch 92] ogbg-molbace: 0.863479 val loss: 0.841258
[Epoch 92] ogbg-molbace: 0.867050 test loss: 0.856352
[Epoch 93; Iter    18/   36] train: loss: 0.1647177
[Epoch 93] ogbg-molbace: 0.867898 val loss: 1.087791
[Epoch 93] ogbg-molbace: 0.860286 test loss: 1.093058
[Epoch 94; Iter    12/   36] train: loss: 0.1097866
[Epoch 94] ogbg-molbace: 0.859691 val loss: 0.934301
[Epoch 94] ogbg-molbace: 0.835096 test loss: 1.156419
[Epoch 95; Iter     6/   36] train: loss: 0.2034022
[Epoch 95; Iter    36/   36] train: loss: 0.0469457
[Epoch 95] ogbg-molbace: 0.875395 val loss: 0.591384
[Epoch 95] ogbg-molbace: 0.881745 test loss: 0.588822
[Epoch 96; Iter    30/   36] train: loss: 0.0880914
[Epoch 96] ogbg-molbace: 0.874921 val loss: 0.599098
[Epoch 96] ogbg-molbace: 0.854222 test loss: 0.733634
[Epoch 97; Iter    24/   36] train: loss: 0.0441288
[Epoch 97] ogbg-molbace: 0.882891 val loss: 0.586782
[Epoch 97] ogbg-molbace: 0.870160 test loss: 0.692580
[Epoch 98; Iter    18/   36] train: loss: 0.0695730
[Epoch 98] ogbg-molbace: 0.880208 val loss: 0.619815
[Epoch 98] ogbg-molbace: 0.867128 test loss: 0.684211
[Epoch 99; Iter    12/   36] train: loss: 0.0362158
[Epoch 99] ogbg-molbace: 0.883523 val loss: 0.604883
[Epoch 99] ogbg-molbace: 0.866739 test loss: 0.698689
[Epoch 100; Iter     6/   36] train: loss: 0.0978457
[Epoch 100; Iter    36/   36] train: loss: 0.6204898
[Epoch 100] ogbg-molbace: 0.879419 val loss: 0.737063
[Epoch 100] ogbg-molbace: 0.865729 test loss: 0.791006
[Epoch 101; Iter    30/   36] train: loss: 0.1557138
[Epoch 101] ogbg-molbace: 0.871133 val loss: 0.704466
[Epoch 101] ogbg-molbace: 0.855077 test loss: 0.766346
[Epoch 102; Iter    24/   36] train: loss: 0.0825603
[Epoch 102] ogbg-molbace: 0.874448 val loss: 0.685422
[Epoch 102] ogbg-molbace: 0.867517 test loss: 0.724389
[Epoch 103; Iter    18/   36] train: loss: 0.0652853
[Epoch 103] ogbg-molbace: 0.871449 val loss: 0.694257
[Epoch 103] ogbg-molbace: 0.867672 test loss: 0.731342
[Epoch 104; Iter    12/   36] train: loss: 0.0411495
[Epoch 104] ogbg-molbace: 0.878867 val loss: 0.770675
[Epoch 104] ogbg-molbace: 0.872259 test loss: 0.786260
[Epoch 105; Iter     6/   36] train: loss: 0.0733800
[Epoch 105; Iter    36/   36] train: loss: 0.1525272
[Epoch 105] ogbg-molbace: 0.883444 val loss: 0.668463
[Epoch 105] ogbg-molbace: 0.880967 test loss: 0.679181
[Epoch 106; Iter    30/   36] train: loss: 0.0243932
[Epoch 106] ogbg-molbace: 0.881155 val loss: 0.678653
[Epoch 106] ogbg-molbace: 0.873503 test loss: 0.731848
[Epoch 107; Iter    24/   36] train: loss: 0.0177683
[Epoch 107] ogbg-molbace: 0.883444 val loss: 0.728817
[Epoch 107] ogbg-molbace: 0.874747 test loss: 0.791340
[Epoch 108; Iter    18/   36] train: loss: 0.0166438
[Epoch 108] ogbg-molbace: 0.880997 val loss: 0.718130
[Epoch 108] ogbg-molbace: 0.864096 test loss: 0.819849
[Epoch 109; Iter    12/   36] train: loss: 0.0160537
[Epoch 109] ogbg-molbace: 0.878472 val loss: 0.776925
[Epoch 109] ogbg-molbace: 0.863552 test loss: 0.843626
[Epoch 110; Iter     6/   36] train: loss: 0.0376820
[Epoch 110; Iter    36/   36] train: loss: 0.1704312
[Epoch 110] ogbg-molbace: 0.880208 val loss: 0.766906
[Epoch 110] ogbg-molbace: 0.872182 test loss: 0.828552
[Epoch 111; Iter    30/   36] train: loss: 0.0128565
[Epoch 111] ogbg-molbace: 0.878235 val loss: 0.725425
[Epoch 111] ogbg-molbace: 0.869383 test loss: 0.852435
[Epoch 112; Iter    24/   36] train: loss: 0.0261244
[Epoch 112] ogbg-molbace: 0.878393 val loss: 0.784362
[Epoch 112] ogbg-molbace: 0.877391 test loss: 0.810244
[Epoch 113; Iter    18/   36] train: loss: 0.0447688
[Epoch 113] ogbg-molbace: 0.873501 val loss: 0.780590
[Epoch 113] ogbg-molbace: 0.872104 test loss: 0.821488
[Epoch 114; Iter    12/   36] train: loss: 0.0569947
[Epoch 114] ogbg-molbace: 0.875552 val loss: 0.618395
[Epoch 114] ogbg-molbace: 0.875214 test loss: 0.633523
[Epoch 115; Iter     6/   36] train: loss: 0.0508758
[Epoch 115; Iter    36/   36] train: loss: 0.0700308
[Epoch 115] ogbg-molbace: 0.870660 val loss: 0.623934
[Epoch 115] ogbg-molbace: 0.854222 test loss: 0.717726
[Epoch 116; Iter    30/   36] train: loss: 0.0327786
[Epoch 116] ogbg-molbace: 0.869871 val loss: 0.714308
[Epoch 116] ogbg-molbace: 0.865806 test loss: 0.739630
[Epoch 117; Iter    24/   36] train: loss: 0.0145094
[Epoch 117] ogbg-molbace: 0.875710 val loss: 0.672110
[Epoch 117] ogbg-molbace: 0.857876 test loss: 0.736624
[Epoch 118; Iter    18/   36] train: loss: 0.0342982
[Epoch 118] ogbg-molbace: 0.873185 val loss: 0.661327
[Epoch 118] ogbg-molbace: 0.853677 test loss: 0.762533
[Epoch 119; Iter    12/   36] train: loss: 0.0328082
[Epoch 119] ogbg-molbace: 0.881471 val loss: 0.682541
[Epoch 119] ogbg-molbace: 0.860364 test loss: 0.811638
[Epoch 120; Iter     6/   36] train: loss: 0.0185046
[Epoch 120; Iter    36/   36] train: loss: 0.2892850
[Epoch 120] ogbg-molbace: 0.879735 val loss: 0.675867
[Epoch 120] ogbg-molbace: 0.867361 test loss: 0.759501
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 47.
Statistics on  val_best_checkpoint
mean_pred: -0.46730750799179077
std_pred: 2.135706901550293
mean_targets: 0.4361233413219452
std_targets: 0.49699893593788147
prcauc: 0.8286015071500465
rocauc: 0.8890467171717172
ogbg-molbace: 0.8890467171717172
BCEWithLogitsLoss: 0.41659795865416527
Statistics on  test
mean_pred: -0.2718800902366638
std_pred: 2.0939884185791016
mean_targets: 0.48017618060112
std_targets: 0.5007109642028809
prcauc: 0.8982413962567714
rocauc: 0.897527600684186
ogbg-molbace: 0.897527600684186
BCEWithLogitsLoss: 0.41364404931664467
Statistics on  train
mean_pred: -0.39070627093315125
std_pred: 2.170255422592163
mean_targets: 0.45609065890312195
std_targets: 0.4983035624027252
prcauc: 0.9018655491815991
rocauc: 0.9271695997239474
ogbg-molbace: 0.9271695997239474
BCEWithLogitsLoss: 0.3513406279186408
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 82] ogbg-molbace: 0.876736 val loss: 0.517419
[Epoch 82] ogbg-molbace: 0.870782 test loss: 0.565889
[Epoch 83; Iter    18/   36] train: loss: 0.2413047
[Epoch 83] ogbg-molbace: 0.858902 val loss: 0.611867
[Epoch 83] ogbg-molbace: 0.869149 test loss: 0.586416
[Epoch 84; Iter    12/   36] train: loss: 0.1491232
[Epoch 84] ogbg-molbace: 0.862926 val loss: 0.613241
[Epoch 84] ogbg-molbace: 0.875214 test loss: 0.604153
[Epoch 85; Iter     6/   36] train: loss: 0.0788660
[Epoch 85; Iter    36/   36] train: loss: 0.4246688
[Epoch 85] ogbg-molbace: 0.872159 val loss: 0.572542
[Epoch 85] ogbg-molbace: 0.864251 test loss: 0.623287
[Epoch 86; Iter    30/   36] train: loss: 0.2453596
[Epoch 86] ogbg-molbace: 0.881944 val loss: 0.520135
[Epoch 86] ogbg-molbace: 0.883766 test loss: 0.545010
[Epoch 87; Iter    24/   36] train: loss: 0.1098312
[Epoch 87] ogbg-molbace: 0.873580 val loss: 0.545601
[Epoch 87] ogbg-molbace: 0.878246 test loss: 0.554938
[Epoch 88; Iter    18/   36] train: loss: 0.0347946
[Epoch 88] ogbg-molbace: 0.869949 val loss: 0.671286
[Epoch 88] ogbg-molbace: 0.853600 test loss: 0.676357
[Epoch 89; Iter    12/   36] train: loss: 0.0692896
[Epoch 89] ogbg-molbace: 0.857323 val loss: 0.646883
[Epoch 89] ogbg-molbace: 0.866350 test loss: 0.660737
[Epoch 90; Iter     6/   36] train: loss: 0.0748043
[Epoch 90; Iter    36/   36] train: loss: 0.0385122
[Epoch 90] ogbg-molbace: 0.884233 val loss: 0.619662
[Epoch 90] ogbg-molbace: 0.873426 test loss: 0.635903
[Epoch 91; Iter    30/   36] train: loss: 0.0574790
[Epoch 91] ogbg-molbace: 0.876105 val loss: 0.655366
[Epoch 91] ogbg-molbace: 0.873503 test loss: 0.678344
[Epoch 92; Iter    24/   36] train: loss: 0.0783674
[Epoch 92] ogbg-molbace: 0.876420 val loss: 0.750795
[Epoch 92] ogbg-molbace: 0.857176 test loss: 0.854846
[Epoch 93; Iter    18/   36] train: loss: 0.1471992
[Epoch 93] ogbg-molbace: 0.874132 val loss: 0.696960
[Epoch 93] ogbg-molbace: 0.872337 test loss: 0.704104
[Epoch 94; Iter    12/   36] train: loss: 0.1193880
[Epoch 94] ogbg-molbace: 0.878867 val loss: 0.617405
[Epoch 94] ogbg-molbace: 0.873115 test loss: 0.614318
[Epoch 95; Iter     6/   36] train: loss: 0.1374076
[Epoch 95; Iter    36/   36] train: loss: 0.2554587
[Epoch 95] ogbg-molbace: 0.863636 val loss: 0.762108
[Epoch 95] ogbg-molbace: 0.870238 test loss: 0.764944
[Epoch 96; Iter    30/   36] train: loss: 0.1846110
[Epoch 96] ogbg-molbace: 0.856455 val loss: 0.704595
[Epoch 96] ogbg-molbace: 0.839605 test loss: 0.774299
[Epoch 97; Iter    24/   36] train: loss: 0.1655493
[Epoch 97] ogbg-molbace: 0.892835 val loss: 0.535007
[Epoch 97] ogbg-molbace: 0.859198 test loss: 0.678474
[Epoch 98; Iter    18/   36] train: loss: 0.0697066
[Epoch 98] ogbg-molbace: 0.865372 val loss: 0.671426
[Epoch 98] ogbg-molbace: 0.851190 test loss: 0.778313
[Epoch 99; Iter    12/   36] train: loss: 0.0834562
[Epoch 99] ogbg-molbace: 0.862926 val loss: 0.710001
[Epoch 99] ogbg-molbace: 0.873503 test loss: 0.662230
[Epoch 100; Iter     6/   36] train: loss: 0.0587446
[Epoch 100; Iter    36/   36] train: loss: 0.1838032
[Epoch 100] ogbg-molbace: 0.868450 val loss: 0.724113
[Epoch 100] ogbg-molbace: 0.861763 test loss: 0.712695
[Epoch 101; Iter    30/   36] train: loss: 0.1340444
[Epoch 101] ogbg-molbace: 0.855508 val loss: 0.806857
[Epoch 101] ogbg-molbace: 0.860131 test loss: 0.797371
[Epoch 102; Iter    24/   36] train: loss: 0.0864548
[Epoch 102] ogbg-molbace: 0.881392 val loss: 0.644861
[Epoch 102] ogbg-molbace: 0.877469 test loss: 0.635902
[Epoch 103; Iter    18/   36] train: loss: 0.0322472
[Epoch 103] ogbg-molbace: 0.868766 val loss: 0.726155
[Epoch 103] ogbg-molbace: 0.860830 test loss: 0.741670
[Epoch 104; Iter    12/   36] train: loss: 0.0714775
[Epoch 104] ogbg-molbace: 0.854009 val loss: 0.779114
[Epoch 104] ogbg-molbace: 0.848313 test loss: 0.908346
[Epoch 105; Iter     6/   36] train: loss: 0.0743020
[Epoch 105; Iter    36/   36] train: loss: 0.0872764
[Epoch 105] ogbg-molbace: 0.862847 val loss: 0.728159
[Epoch 105] ogbg-molbace: 0.858576 test loss: 0.797126
[Epoch 106; Iter    30/   36] train: loss: 0.2432036
[Epoch 106] ogbg-molbace: 0.871449 val loss: 0.740704
[Epoch 106] ogbg-molbace: 0.881745 test loss: 0.714500
[Epoch 107; Iter    24/   36] train: loss: 0.0901702
[Epoch 107] ogbg-molbace: 0.860006 val loss: 0.779897
[Epoch 107] ogbg-molbace: 0.855155 test loss: 0.893065
[Epoch 108; Iter    18/   36] train: loss: 0.0604028
[Epoch 108] ogbg-molbace: 0.879656 val loss: 0.693058
[Epoch 108] ogbg-molbace: 0.855777 test loss: 0.818660
[Epoch 109; Iter    12/   36] train: loss: 0.0223901
[Epoch 109] ogbg-molbace: 0.868450 val loss: 0.799958
[Epoch 109] ogbg-molbace: 0.862385 test loss: 0.879899
[Epoch 110; Iter     6/   36] train: loss: 0.0196874
[Epoch 110; Iter    36/   36] train: loss: 0.0831696
[Epoch 110] ogbg-molbace: 0.871765 val loss: 0.726895
[Epoch 110] ogbg-molbace: 0.865107 test loss: 0.798046
[Epoch 111; Iter    30/   36] train: loss: 0.0232574
[Epoch 111] ogbg-molbace: 0.870975 val loss: 0.820790
[Epoch 111] ogbg-molbace: 0.861141 test loss: 0.909812
[Epoch 112; Iter    24/   36] train: loss: 0.0081458
[Epoch 112] ogbg-molbace: 0.870107 val loss: 0.797518
[Epoch 112] ogbg-molbace: 0.862230 test loss: 0.858704
[Epoch 113; Iter    18/   36] train: loss: 0.0329622
[Epoch 113] ogbg-molbace: 0.879577 val loss: 0.773839
[Epoch 113] ogbg-molbace: 0.868216 test loss: 0.813443
[Epoch 114; Iter    12/   36] train: loss: 0.0521522
[Epoch 114] ogbg-molbace: 0.873974 val loss: 0.798561
[Epoch 114] ogbg-molbace: 0.862930 test loss: 0.861173
[Epoch 115; Iter     6/   36] train: loss: 0.0350231
[Epoch 115; Iter    36/   36] train: loss: 0.2431039
[Epoch 115] ogbg-molbace: 0.873422 val loss: 0.765005
[Epoch 115] ogbg-molbace: 0.870860 test loss: 0.796445
[Epoch 116; Iter    30/   36] train: loss: 0.0510796
[Epoch 116] ogbg-molbace: 0.871922 val loss: 0.920976
[Epoch 116] ogbg-molbace: 0.869227 test loss: 0.972821
[Epoch 117; Iter    24/   36] train: loss: 0.0230179
[Epoch 117] ogbg-molbace: 0.865530 val loss: 0.837378
[Epoch 117] ogbg-molbace: 0.868527 test loss: 0.878188
[Epoch 118; Iter    18/   36] train: loss: 0.0293317
[Epoch 118] ogbg-molbace: 0.866872 val loss: 0.858510
[Epoch 118] ogbg-molbace: 0.864251 test loss: 0.911276
[Epoch 119; Iter    12/   36] train: loss: 0.0554469
[Epoch 119] ogbg-molbace: 0.881076 val loss: 0.875845
[Epoch 119] ogbg-molbace: 0.886176 test loss: 0.835182
[Epoch 120; Iter     6/   36] train: loss: 0.0218231
[Epoch 120; Iter    36/   36] train: loss: 0.0324728
[Epoch 120] ogbg-molbace: 0.874290 val loss: 0.807035
[Epoch 120] ogbg-molbace: 0.868838 test loss: 0.847568
[Epoch 121; Iter    30/   36] train: loss: 0.0217759
[Epoch 121] ogbg-molbace: 0.863479 val loss: 0.986030
[Epoch 121] ogbg-molbace: 0.874981 test loss: 0.915579
[Epoch 122; Iter    24/   36] train: loss: 0.0418103
[Epoch 122] ogbg-molbace: 0.881076 val loss: 0.885203
[Epoch 122] ogbg-molbace: 0.868294 test loss: 0.993579
[Epoch 123; Iter    18/   36] train: loss: 0.0120846
[Epoch 123] ogbg-molbace: 0.873816 val loss: 0.899752
[Epoch 123] ogbg-molbace: 0.861452 test loss: 0.969451
[Epoch 124; Iter    12/   36] train: loss: 0.0106620
[Epoch 124] ogbg-molbace: 0.868687 val loss: 0.860255
[Epoch 124] ogbg-molbace: 0.854299 test loss: 1.023390
[Epoch 125; Iter     6/   36] train: loss: 0.1930412
[Epoch 125; Iter    36/   36] train: loss: 0.3131446
[Epoch 125] ogbg-molbace: 0.860243 val loss: 1.015939
[Epoch 125] ogbg-molbace: 0.868139 test loss: 1.057074
[Epoch 126; Iter    30/   36] train: loss: 0.0867835
[Epoch 126] ogbg-molbace: 0.879972 val loss: 0.781446
[Epoch 126] ogbg-molbace: 0.864562 test loss: 0.930485
[Epoch 127; Iter    24/   36] train: loss: 0.0432929
[Epoch 127] ogbg-molbace: 0.875710 val loss: 0.752953
[Epoch 127] ogbg-molbace: 0.860597 test loss: 0.888423
Early stopping criterion based on -ogbg-molbace- that should be max reached after 127 epochs. Best model checkpoint was in epoch 67.
Statistics on  val_best_checkpoint
mean_pred: -0.5819642543792725
std_pred: 3.3640782833099365
mean_targets: 0.4361233413219452
std_targets: 0.49699893593788147
prcauc: 0.8338470562829869
rocauc: 0.9014362373737373
[Epoch 78] ogbg-molbace: 0.911920 val loss: 0.360660
[Epoch 78] ogbg-molbace: 0.874276 test loss: 0.474755
[Epoch 79; Iter    12/   41] train: loss: 0.1535350
[Epoch 79] ogbg-molbace: 0.922117 val loss: 0.340162
[Epoch 79] ogbg-molbace: 0.856541 test loss: 0.544727
[Epoch 80; Iter     1/   41] train: loss: 0.0830415
[Epoch 80; Iter    31/   41] train: loss: 0.3564832
[Epoch 80] ogbg-molbace: 0.916139 val loss: 0.391601
[Epoch 80] ogbg-molbace: 0.869183 test loss: 0.552139
[Epoch 81; Iter    20/   41] train: loss: 0.2203584
[Epoch 81] ogbg-molbace: 0.902602 val loss: 0.435225
[Epoch 81] ogbg-molbace: 0.880773 test loss: 0.446758
[Epoch 82; Iter     9/   41] train: loss: 0.1079578
[Epoch 82; Iter    39/   41] train: loss: 0.1803158
[Epoch 82] ogbg-molbace: 0.907876 val loss: 0.775383
[Epoch 82] ogbg-molbace: 0.865496 test loss: 0.536405
[Epoch 83; Iter    28/   41] train: loss: 0.3135531
[Epoch 83] ogbg-molbace: 0.912799 val loss: 0.367850
[Epoch 83] ogbg-molbace: 0.863916 test loss: 0.578225
[Epoch 84; Iter    17/   41] train: loss: 0.1582456
[Epoch 84] ogbg-molbace: 0.908755 val loss: 0.400166
[Epoch 84] ogbg-molbace: 0.861633 test loss: 0.510727
[Epoch 85; Iter     6/   41] train: loss: 0.1493372
[Epoch 85; Iter    36/   41] train: loss: 0.1549514
[Epoch 85] ogbg-molbace: 0.905063 val loss: 0.416357
[Epoch 85] ogbg-molbace: 0.873222 test loss: 0.467276
[Epoch 86; Iter    25/   41] train: loss: 0.1306040
[Epoch 86] ogbg-molbace: 0.900141 val loss: 0.467069
[Epoch 86] ogbg-molbace: 0.868657 test loss: 0.540801
[Epoch 87; Iter    14/   41] train: loss: 0.1973543
[Epoch 87] ogbg-molbace: 0.903129 val loss: 0.423409
[Epoch 87] ogbg-molbace: 0.878314 test loss: 0.430425
[Epoch 88; Iter     3/   41] train: loss: 0.1961210
[Epoch 88; Iter    33/   41] train: loss: 0.0616759
[Epoch 88] ogbg-molbace: 0.903833 val loss: 0.403854
[Epoch 88] ogbg-molbace: 0.863565 test loss: 0.488566
[Epoch 89; Iter    22/   41] train: loss: 0.3712013
[Epoch 89] ogbg-molbace: 0.916667 val loss: 0.532589
[Epoch 89] ogbg-molbace: 0.884811 test loss: 0.479158
[Epoch 90; Iter    11/   41] train: loss: 0.1231965
[Epoch 90; Iter    41/   41] train: loss: 0.7292971
[Epoch 90] ogbg-molbace: 0.869198 val loss: 0.529801
[Epoch 90] ogbg-molbace: 0.871642 test loss: 0.483893
[Epoch 91; Iter    30/   41] train: loss: 0.6470535
[Epoch 91] ogbg-molbace: 0.896097 val loss: 0.438001
[Epoch 91] ogbg-molbace: 0.869183 test loss: 0.477004
[Epoch 92; Iter    19/   41] train: loss: 0.1472350
[Epoch 92] ogbg-molbace: 0.905239 val loss: 0.439087
[Epoch 92] ogbg-molbace: 0.866198 test loss: 0.498646
[Epoch 93; Iter     8/   41] train: loss: 0.2477926
[Epoch 93; Iter    38/   41] train: loss: 0.0678197
[Epoch 93] ogbg-molbace: 0.923347 val loss: 0.350545
[Epoch 93] ogbg-molbace: 0.872695 test loss: 0.479911
[Epoch 94; Iter    27/   41] train: loss: 0.1343796
[Epoch 94] ogbg-molbace: 0.927039 val loss: 0.342440
[Epoch 94] ogbg-molbace: 0.884460 test loss: 0.475988
[Epoch 95; Iter    16/   41] train: loss: 0.1489867
[Epoch 95] ogbg-molbace: 0.917897 val loss: 0.382953
[Epoch 95] ogbg-molbace: 0.879017 test loss: 0.541244
[Epoch 96; Iter     5/   41] train: loss: 0.1206387
[Epoch 96; Iter    35/   41] train: loss: 0.0614912
[Epoch 96] ogbg-molbace: 0.899437 val loss: 0.467783
[Epoch 96] ogbg-molbace: 0.865320 test loss: 0.642893
[Epoch 97; Iter    24/   41] train: loss: 0.1043816
[Epoch 97] ogbg-molbace: 0.897328 val loss: 0.443719
[Epoch 97] ogbg-molbace: 0.874276 test loss: 0.503339
[Epoch 98; Iter    13/   41] train: loss: 0.0828096
[Epoch 98] ogbg-molbace: 0.913678 val loss: 0.406117
[Epoch 98] ogbg-molbace: 0.892011 test loss: 0.476182
[Epoch 99; Iter     2/   41] train: loss: 0.0459576
[Epoch 99; Iter    32/   41] train: loss: 0.1051025
[Epoch 99] ogbg-molbace: 0.903129 val loss: 0.447678
[Epoch 99] ogbg-molbace: 0.866374 test loss: 0.524802
[Epoch 100; Iter    21/   41] train: loss: 0.0752041
[Epoch 100] ogbg-molbace: 0.916667 val loss: 0.452598
[Epoch 100] ogbg-molbace: 0.893064 test loss: 0.578540
[Epoch 101; Iter    10/   41] train: loss: 0.1253594
[Epoch 101; Iter    40/   41] train: loss: 0.0669205
[Epoch 101] ogbg-molbace: 0.922117 val loss: 0.398989
[Epoch 101] ogbg-molbace: 0.886567 test loss: 0.515618
[Epoch 102; Iter    29/   41] train: loss: 0.1825127
[Epoch 102] ogbg-molbace: 0.916667 val loss: 0.400179
[Epoch 102] ogbg-molbace: 0.865847 test loss: 0.608997
[Epoch 103; Iter    18/   41] train: loss: 0.3587342
[Epoch 103] ogbg-molbace: 0.922117 val loss: 0.472103
[Epoch 103] ogbg-molbace: 0.886040 test loss: 0.485006
[Epoch 104; Iter     7/   41] train: loss: 0.1005831
[Epoch 104; Iter    37/   41] train: loss: 0.0647691
[Epoch 104] ogbg-molbace: 0.906294 val loss: 0.442456
[Epoch 104] ogbg-molbace: 0.889025 test loss: 0.531224
[Epoch 105; Iter    26/   41] train: loss: 0.1222972
[Epoch 105] ogbg-molbace: 0.899086 val loss: 0.476016
[Epoch 105] ogbg-molbace: 0.878841 test loss: 0.590541
[Epoch 106; Iter    15/   41] train: loss: 0.0394226
[Epoch 106] ogbg-molbace: 0.887131 val loss: 0.722076
[Epoch 106] ogbg-molbace: 0.871993 test loss: 0.564376
[Epoch 107; Iter     4/   41] train: loss: 0.2852113
[Epoch 107; Iter    34/   41] train: loss: 0.1709818
[Epoch 107] ogbg-molbace: 0.910513 val loss: 0.399852
[Epoch 107] ogbg-molbace: 0.890781 test loss: 0.497076
[Epoch 108; Iter    23/   41] train: loss: 0.0912227
[Epoch 108] ogbg-molbace: 0.924051 val loss: 0.361909
[Epoch 108] ogbg-molbace: 0.896576 test loss: 0.457183
[Epoch 109; Iter    12/   41] train: loss: 0.1771978
[Epoch 109] ogbg-molbace: 0.900316 val loss: 0.446890
[Epoch 109] ogbg-molbace: 0.884109 test loss: 0.535281
[Epoch 110; Iter     1/   41] train: loss: 0.2748471
[Epoch 110; Iter    31/   41] train: loss: 0.2637883
[Epoch 110] ogbg-molbace: 0.926336 val loss: 0.386996
[Epoch 110] ogbg-molbace: 0.887796 test loss: 0.538209
[Epoch 111; Iter    20/   41] train: loss: 0.2367252
[Epoch 111] ogbg-molbace: 0.928622 val loss: 0.553853
[Epoch 111] ogbg-molbace: 0.898156 test loss: 0.450316
[Epoch 112; Iter     9/   41] train: loss: 0.1310533
[Epoch 112; Iter    39/   41] train: loss: 0.0737314
[Epoch 112] ogbg-molbace: 0.903833 val loss: 0.445036
[Epoch 112] ogbg-molbace: 0.892537 test loss: 0.425631
[Epoch 113; Iter    28/   41] train: loss: 0.0936838
[Epoch 113] ogbg-molbace: 0.904887 val loss: 0.560191
[Epoch 113] ogbg-molbace: 0.880421 test loss: 0.479097
[Epoch 114; Iter    17/   41] train: loss: 0.0955864
[Epoch 114] ogbg-molbace: 0.894515 val loss: 0.545156
[Epoch 114] ogbg-molbace: 0.887972 test loss: 0.473256
[Epoch 115; Iter     6/   41] train: loss: 0.0659475
[Epoch 115; Iter    36/   41] train: loss: 0.1438874
[Epoch 115] ogbg-molbace: 0.901723 val loss: 0.489099
[Epoch 115] ogbg-molbace: 0.883758 test loss: 0.486858
[Epoch 116; Iter    25/   41] train: loss: 0.0513083
[Epoch 116] ogbg-molbace: 0.925457 val loss: 0.442676
[Epoch 116] ogbg-molbace: 0.882529 test loss: 0.698312
[Epoch 117; Iter    14/   41] train: loss: 0.0289265
[Epoch 117] ogbg-molbace: 0.894866 val loss: 0.586016
[Epoch 117] ogbg-molbace: 0.848464 test loss: 0.791464
[Epoch 118; Iter     3/   41] train: loss: 0.0233195
[Epoch 118; Iter    33/   41] train: loss: 0.0468131
[Epoch 118] ogbg-molbace: 0.894163 val loss: 0.707307
[Epoch 118] ogbg-molbace: 0.870939 test loss: 0.671139
[Epoch 119; Iter    22/   41] train: loss: 0.0372117
[Epoch 119] ogbg-molbace: 0.917722 val loss: 0.455860
[Epoch 119] ogbg-molbace: 0.886392 test loss: 0.564191
[Epoch 120; Iter    11/   41] train: loss: 0.0253006
[Epoch 120; Iter    41/   41] train: loss: 0.0783620
[Epoch 120] ogbg-molbace: 0.914381 val loss: 0.462519
[Epoch 120] ogbg-molbace: 0.873573 test loss: 0.619883
[Epoch 121; Iter    30/   41] train: loss: 0.1589269
[Epoch 121] ogbg-molbace: 0.907525 val loss: 0.505384
[Epoch 121] ogbg-molbace: 0.883758 test loss: 0.585878
[Epoch 122; Iter    19/   41] train: loss: 0.1049001
[Epoch 122] ogbg-molbace: 0.888361 val loss: 0.593176
[Epoch 122] ogbg-molbace: 0.876383 test loss: 0.656409
[Epoch 123; Iter     8/   41] train: loss: 0.0520920
[Epoch 123; Iter    38/   41] train: loss: 0.2024504
[Epoch 123] ogbg-molbace: 0.904184 val loss: 0.645206
ogbg-molbace: 0.9014362373737373
BCEWithLogitsLoss: 0.4297334775328636
Statistics on  test
mean_pred: -0.35433733463287354
std_pred: 3.2939388751983643
mean_targets: 0.48017618060112
std_targets: 0.5007109642028809
prcauc: 0.8648343927295445
rocauc: 0.8853211009174312
ogbg-molbace: 0.8853211009174312
BCEWithLogitsLoss: 0.4528374709188938
Statistics on  train
mean_pred: -0.5766923427581787
std_pred: 3.38950252532959
mean_targets: 0.45609065890312195
std_targets: 0.49830353260040283
prcauc: 0.972312900967203
rocauc: 0.9746987865194386
ogbg-molbace: 0.9746987865194386
BCEWithLogitsLoss: 0.21083255546788374
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.890471 val loss: 0.445370
[Epoch 78] ogbg-molbace: 0.861457 test loss: 0.521375
[Epoch 79; Iter    12/   41] train: loss: 0.2006816
[Epoch 79] ogbg-molbace: 0.905415 val loss: 0.382105
[Epoch 79] ogbg-molbace: 0.833363 test loss: 0.570421
[Epoch 80; Iter     1/   41] train: loss: 0.3713284
[Epoch 80; Iter    31/   41] train: loss: 0.1485075
[Epoch 80] ogbg-molbace: 0.870956 val loss: 0.531677
[Epoch 80] ogbg-molbace: 0.865145 test loss: 0.569428
[Epoch 81; Iter    20/   41] train: loss: 0.0356918
[Epoch 81] ogbg-molbace: 0.905415 val loss: 0.393037
[Epoch 81] ogbg-molbace: 0.867779 test loss: 0.496851
[Epoch 82; Iter     9/   41] train: loss: 0.1137604
[Epoch 82; Iter    39/   41] train: loss: 0.1174163
[Epoch 82] ogbg-molbace: 0.917546 val loss: 0.359896
[Epoch 82] ogbg-molbace: 0.874802 test loss: 0.457061
[Epoch 83; Iter    28/   41] train: loss: 0.0914620
[Epoch 83] ogbg-molbace: 0.909459 val loss: 0.406793
[Epoch 83] ogbg-molbace: 0.864794 test loss: 0.529495
[Epoch 84; Iter    17/   41] train: loss: 0.1378295
[Epoch 84] ogbg-molbace: 0.919480 val loss: 0.371131
[Epoch 84] ogbg-molbace: 0.870413 test loss: 0.536608
[Epoch 85; Iter     6/   41] train: loss: 0.0654960
[Epoch 85; Iter    36/   41] train: loss: 0.1286118
[Epoch 85] ogbg-molbace: 0.913150 val loss: 0.445721
[Epoch 85] ogbg-molbace: 0.870061 test loss: 0.512341
[Epoch 86; Iter    25/   41] train: loss: 0.0992897
[Epoch 86] ogbg-molbace: 0.899086 val loss: 0.451334
[Epoch 86] ogbg-molbace: 0.869359 test loss: 0.554836
[Epoch 87; Iter    14/   41] train: loss: 0.1133549
[Epoch 87] ogbg-molbace: 0.903129 val loss: 0.414859
[Epoch 87] ogbg-molbace: 0.856541 test loss: 0.582619
[Epoch 88; Iter     3/   41] train: loss: 0.0758171
[Epoch 88; Iter    33/   41] train: loss: 0.0590170
[Epoch 88] ogbg-molbace: 0.908755 val loss: 0.620844
[Epoch 88] ogbg-molbace: 0.852678 test loss: 0.594618
[Epoch 89; Iter    22/   41] train: loss: 0.0986414
[Epoch 89] ogbg-molbace: 0.889944 val loss: 0.734803
[Epoch 89] ogbg-molbace: 0.869008 test loss: 0.576001
[Epoch 90; Iter    11/   41] train: loss: 0.1159594
[Epoch 90; Iter    41/   41] train: loss: 0.1096994
[Epoch 90] ogbg-molbace: 0.899262 val loss: 0.509911
[Epoch 90] ogbg-molbace: 0.863038 test loss: 0.568623
[Epoch 91; Iter    30/   41] train: loss: 0.0932118
[Epoch 91] ogbg-molbace: 0.911920 val loss: 0.459223
[Epoch 91] ogbg-molbace: 0.866550 test loss: 0.584117
[Epoch 92; Iter    19/   41] train: loss: 0.0793684
[Epoch 92] ogbg-molbace: 0.894866 val loss: 0.588863
[Epoch 92] ogbg-molbace: 0.867779 test loss: 0.595527
[Epoch 93; Iter     8/   41] train: loss: 0.0129380
[Epoch 93; Iter    38/   41] train: loss: 0.1228416
[Epoch 93] ogbg-molbace: 0.922117 val loss: 0.412400
[Epoch 93] ogbg-molbace: 0.876734 test loss: 0.546287
[Epoch 94; Iter    27/   41] train: loss: 0.0717320
[Epoch 94] ogbg-molbace: 0.898207 val loss: 0.501998
[Epoch 94] ogbg-molbace: 0.852853 test loss: 0.626154
[Epoch 95; Iter    16/   41] train: loss: 0.0711823
[Epoch 95] ogbg-molbace: 0.905415 val loss: 0.445858
[Epoch 95] ogbg-molbace: 0.875154 test loss: 0.582669
[Epoch 96; Iter     5/   41] train: loss: 0.3326994
[Epoch 96; Iter    35/   41] train: loss: 0.2399403
[Epoch 96] ogbg-molbace: 0.895394 val loss: 0.603168
[Epoch 96] ogbg-molbace: 0.882880 test loss: 0.666704
[Epoch 97; Iter    24/   41] train: loss: 0.3906271
[Epoch 97] ogbg-molbace: 0.917546 val loss: 0.430170
[Epoch 97] ogbg-molbace: 0.865145 test loss: 0.633645
[Epoch 98; Iter    13/   41] train: loss: 0.0241044
[Epoch 98] ogbg-molbace: 0.890999 val loss: 0.493133
[Epoch 98] ogbg-molbace: 0.856892 test loss: 0.643364
[Epoch 99; Iter     2/   41] train: loss: 0.0757906
[Epoch 99; Iter    32/   41] train: loss: 0.0603351
[Epoch 99] ogbg-molbace: 0.873594 val loss: 0.789005
[Epoch 99] ogbg-molbace: 0.864091 test loss: 0.557246
[Epoch 100; Iter    21/   41] train: loss: 0.0789656
[Epoch 100] ogbg-molbace: 0.885900 val loss: 0.553085
[Epoch 100] ogbg-molbace: 0.865496 test loss: 0.574962
[Epoch 101; Iter    10/   41] train: loss: 0.0600798
[Epoch 101; Iter    40/   41] train: loss: 0.0639809
[Epoch 101] ogbg-molbace: 0.866913 val loss: 0.638642
[Epoch 101] ogbg-molbace: 0.877261 test loss: 0.564933
[Epoch 102; Iter    29/   41] train: loss: 0.0714550
[Epoch 102] ogbg-molbace: 0.889944 val loss: 0.622223
[Epoch 102] ogbg-molbace: 0.880070 test loss: 0.551041
[Epoch 103; Iter    18/   41] train: loss: 0.1064470
[Epoch 103] ogbg-molbace: 0.909986 val loss: 0.528049
[Epoch 103] ogbg-molbace: 0.874802 test loss: 0.608994
[Epoch 104; Iter     7/   41] train: loss: 0.0908782
[Epoch 104; Iter    37/   41] train: loss: 0.0718761
[Epoch 104] ogbg-molbace: 0.886252 val loss: 0.571816
[Epoch 104] ogbg-molbace: 0.861809 test loss: 0.638431
[Epoch 105; Iter    26/   41] train: loss: 0.0489003
[Epoch 105] ogbg-molbace: 0.906646 val loss: 0.558929
[Epoch 105] ogbg-molbace: 0.866901 test loss: 0.606627
[Epoch 106; Iter    15/   41] train: loss: 0.0618305
[Epoch 106] ogbg-molbace: 0.889592 val loss: 0.520452
[Epoch 106] ogbg-molbace: 0.887621 test loss: 0.512087
[Epoch 107; Iter     4/   41] train: loss: 0.0328469
[Epoch 107; Iter    34/   41] train: loss: 0.0528357
[Epoch 107] ogbg-molbace: 0.896449 val loss: 0.533044
[Epoch 107] ogbg-molbace: 0.887972 test loss: 0.546708
[Epoch 108; Iter    23/   41] train: loss: 0.0796855
[Epoch 108] ogbg-molbace: 0.899965 val loss: 0.525872
[Epoch 108] ogbg-molbace: 0.883406 test loss: 0.570928
[Epoch 109; Iter    12/   41] train: loss: 0.0217817
[Epoch 109] ogbg-molbace: 0.897855 val loss: 0.527123
[Epoch 109] ogbg-molbace: 0.882177 test loss: 0.636603
[Epoch 110; Iter     1/   41] train: loss: 0.0346829
[Epoch 110; Iter    31/   41] train: loss: 0.0970434
[Epoch 110] ogbg-molbace: 0.897855 val loss: 0.608044
[Epoch 110] ogbg-molbace: 0.876558 test loss: 0.608927
[Epoch 111; Iter    20/   41] train: loss: 0.0970570
[Epoch 111] ogbg-molbace: 0.873594 val loss: 0.789613
[Epoch 111] ogbg-molbace: 0.885514 test loss: 0.587051
[Epoch 112; Iter     9/   41] train: loss: 0.0176160
[Epoch 112; Iter    39/   41] train: loss: 0.0331669
[Epoch 112] ogbg-molbace: 0.889768 val loss: 0.846905
[Epoch 112] ogbg-molbace: 0.877612 test loss: 0.605559
[Epoch 113; Iter    28/   41] train: loss: 0.0309162
[Epoch 113] ogbg-molbace: 0.885724 val loss: 0.598312
[Epoch 113] ogbg-molbace: 0.872871 test loss: 0.628020
[Epoch 114; Iter    17/   41] train: loss: 0.0116132
[Epoch 114] ogbg-molbace: 0.885021 val loss: 0.650229
[Epoch 114] ogbg-molbace: 0.875329 test loss: 0.659423
[Epoch 115; Iter     6/   41] train: loss: 0.1276246
[Epoch 115; Iter    36/   41] train: loss: 0.0723436
[Epoch 115] ogbg-molbace: 0.882384 val loss: 0.623485
[Epoch 115] ogbg-molbace: 0.884284 test loss: 0.610745
[Epoch 116; Iter    25/   41] train: loss: 0.0169586
[Epoch 116] ogbg-molbace: 0.893812 val loss: 0.709222
[Epoch 116] ogbg-molbace: 0.876558 test loss: 0.622422
[Epoch 117; Iter    14/   41] train: loss: 0.0220300
[Epoch 117] ogbg-molbace: 0.902250 val loss: 0.633980
[Epoch 117] ogbg-molbace: 0.881651 test loss: 0.626020
[Epoch 118; Iter     3/   41] train: loss: 0.0108541
[Epoch 118; Iter    33/   41] train: loss: 0.0083341
[Epoch 118] ogbg-molbace: 0.897152 val loss: 0.739503
[Epoch 118] ogbg-molbace: 0.867428 test loss: 0.716841
[Epoch 119; Iter    22/   41] train: loss: 0.0194657
[Epoch 119] ogbg-molbace: 0.885021 val loss: 0.712148
[Epoch 119] ogbg-molbace: 0.885162 test loss: 0.594114
[Epoch 120; Iter    11/   41] train: loss: 0.0173538
[Epoch 120; Iter    41/   41] train: loss: 0.1207293
[Epoch 120] ogbg-molbace: 0.888713 val loss: 0.693157
[Epoch 120] ogbg-molbace: 0.882704 test loss: 0.654012
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 43.
Statistics on  val_best_checkpoint
mean_pred: 0.20856048166751862
std_pred: 2.0451390743255615
mean_targets: 0.5231788158416748
std_targets: 0.5011245608329773
prcauc: 0.9128545910554108
rocauc: 0.9319620253164557
ogbg-molbace: 0.9319620253164557
BCEWithLogitsLoss: 0.3365725303689639
Statistics on  test
mean_pred: -0.08694521337747574
std_pred: 2.139755964279175
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 82] ogbg-molbace: 0.877367 val loss: 0.574491
[Epoch 82] ogbg-molbace: 0.860908 test loss: 0.655557
[Epoch 83; Iter    18/   36] train: loss: 0.2163442
[Epoch 83] ogbg-molbace: 0.880682 val loss: 0.650487
[Epoch 83] ogbg-molbace: 0.859742 test loss: 0.729913
[Epoch 84; Iter    12/   36] train: loss: 0.0720368
[Epoch 84] ogbg-molbace: 0.861506 val loss: 0.671540
[Epoch 84] ogbg-molbace: 0.866195 test loss: 0.664102
[Epoch 85; Iter     6/   36] train: loss: 0.0968765
[Epoch 85; Iter    36/   36] train: loss: 0.1704502
[Epoch 85] ogbg-molbace: 0.876105 val loss: 0.719687
[Epoch 85] ogbg-molbace: 0.860364 test loss: 0.785915
[Epoch 86; Iter    30/   36] train: loss: 0.1075376
[Epoch 86] ogbg-molbace: 0.876263 val loss: 0.653991
[Epoch 86] ogbg-molbace: 0.863085 test loss: 0.714604
[Epoch 87; Iter    24/   36] train: loss: 0.0949559
[Epoch 87] ogbg-molbace: 0.877920 val loss: 0.617323
[Epoch 87] ogbg-molbace: 0.863241 test loss: 0.706696
[Epoch 88; Iter    18/   36] train: loss: 0.0431220
[Epoch 88] ogbg-molbace: 0.877999 val loss: 0.635869
[Epoch 88] ogbg-molbace: 0.872648 test loss: 0.698416
[Epoch 89; Iter    12/   36] train: loss: 0.0659191
[Epoch 89] ogbg-molbace: 0.854009 val loss: 0.662350
[Epoch 89] ogbg-molbace: 0.860131 test loss: 0.675533
[Epoch 90; Iter     6/   36] train: loss: 0.0335441
[Epoch 90; Iter    36/   36] train: loss: 0.1562918
[Epoch 90] ogbg-molbace: 0.886443 val loss: 0.582459
[Epoch 90] ogbg-molbace: 0.865340 test loss: 0.675122
[Epoch 91; Iter    30/   36] train: loss: 0.2181154
[Epoch 91] ogbg-molbace: 0.842566 val loss: 0.737375
[Epoch 91] ogbg-molbace: 0.859198 test loss: 0.748585
[Epoch 92; Iter    24/   36] train: loss: 0.1185885
[Epoch 92] ogbg-molbace: 0.865925 val loss: 0.672946
[Epoch 92] ogbg-molbace: 0.869383 test loss: 0.715432
[Epoch 93; Iter    18/   36] train: loss: 0.0321440
[Epoch 93] ogbg-molbace: 0.877683 val loss: 0.714317
[Epoch 93] ogbg-molbace: 0.873737 test loss: 0.778450
[Epoch 94; Iter    12/   36] train: loss: 0.1809587
[Epoch 94] ogbg-molbace: 0.874527 val loss: 0.606783
[Epoch 94] ogbg-molbace: 0.857254 test loss: 0.724248
[Epoch 95; Iter     6/   36] train: loss: 0.0732135
[Epoch 95; Iter    36/   36] train: loss: 0.1148036
[Epoch 95] ogbg-molbace: 0.872869 val loss: 0.621274
[Epoch 95] ogbg-molbace: 0.862308 test loss: 0.681333
[Epoch 96; Iter    30/   36] train: loss: 0.1676615
[Epoch 96] ogbg-molbace: 0.860795 val loss: 0.837275
[Epoch 96] ogbg-molbace: 0.854766 test loss: 0.867205
[Epoch 97; Iter    24/   36] train: loss: 0.1136694
[Epoch 97] ogbg-molbace: 0.863242 val loss: 0.782071
[Epoch 97] ogbg-molbace: 0.865262 test loss: 0.827230
[Epoch 98; Iter    18/   36] train: loss: 0.1411993
[Epoch 98] ogbg-molbace: 0.861585 val loss: 0.705942
[Epoch 98] ogbg-molbace: 0.864018 test loss: 0.703403
[Epoch 99; Iter    12/   36] train: loss: 0.1685391
[Epoch 99] ogbg-molbace: 0.866083 val loss: 0.690196
[Epoch 99] ogbg-molbace: 0.865262 test loss: 0.722051
[Epoch 100; Iter     6/   36] train: loss: 0.1394597
[Epoch 100; Iter    36/   36] train: loss: 0.1663824
[Epoch 100] ogbg-molbace: 0.864741 val loss: 0.711810
[Epoch 100] ogbg-molbace: 0.858964 test loss: 0.769609
[Epoch 101; Iter    30/   36] train: loss: 0.1080012
[Epoch 101] ogbg-molbace: 0.873816 val loss: 0.744713
[Epoch 101] ogbg-molbace: 0.869694 test loss: 0.743343
[Epoch 102; Iter    24/   36] train: loss: 0.0717459
[Epoch 102] ogbg-molbace: 0.874527 val loss: 0.678702
[Epoch 102] ogbg-molbace: 0.873970 test loss: 0.703918
[Epoch 103; Iter    18/   36] train: loss: 0.0806990
[Epoch 103] ogbg-molbace: 0.868213 val loss: 0.698913
[Epoch 103] ogbg-molbace: 0.861686 test loss: 0.797108
[Epoch 104; Iter    12/   36] train: loss: 0.1214145
[Epoch 104] ogbg-molbace: 0.867740 val loss: 0.683938
[Epoch 104] ogbg-molbace: 0.861375 test loss: 0.774129
[Epoch 105; Iter     6/   36] train: loss: 0.0187228
[Epoch 105; Iter    36/   36] train: loss: 0.0935595
[Epoch 105] ogbg-molbace: 0.882260 val loss: 0.655802
[Epoch 105] ogbg-molbace: 0.871326 test loss: 0.725564
[Epoch 106; Iter    30/   36] train: loss: 0.0445295
[Epoch 106] ogbg-molbace: 0.866793 val loss: 0.740225
[Epoch 106] ogbg-molbace: 0.860753 test loss: 0.833815
[Epoch 107; Iter    24/   36] train: loss: 0.0328245
[Epoch 107] ogbg-molbace: 0.872869 val loss: 0.706530
[Epoch 107] ogbg-molbace: 0.867672 test loss: 0.731156
[Epoch 108; Iter    18/   36] train: loss: 0.0504901
[Epoch 108] ogbg-molbace: 0.866714 val loss: 0.758653
[Epoch 108] ogbg-molbace: 0.861452 test loss: 0.805940
[Epoch 109; Iter    12/   36] train: loss: 0.0156101
[Epoch 109] ogbg-molbace: 0.877210 val loss: 0.723048
[Epoch 109] ogbg-molbace: 0.867517 test loss: 0.786791
[Epoch 110; Iter     6/   36] train: loss: 0.0421629
[Epoch 110; Iter    36/   36] train: loss: 0.0184979
[Epoch 110] ogbg-molbace: 0.867819 val loss: 0.723689
[Epoch 110] ogbg-molbace: 0.862463 test loss: 0.804396
[Epoch 111; Iter    30/   36] train: loss: 0.0188899
[Epoch 111] ogbg-molbace: 0.873737 val loss: 0.740858
[Epoch 111] ogbg-molbace: 0.865806 test loss: 0.807481
[Epoch 112; Iter    24/   36] train: loss: 0.0350884
[Epoch 112] ogbg-molbace: 0.871291 val loss: 0.772978
[Epoch 112] ogbg-molbace: 0.865184 test loss: 0.847022
[Epoch 113; Iter    18/   36] train: loss: 0.0178704
[Epoch 113] ogbg-molbace: 0.874290 val loss: 0.841030
[Epoch 113] ogbg-molbace: 0.865651 test loss: 0.897244
[Epoch 114; Iter    12/   36] train: loss: 0.0140850
[Epoch 114] ogbg-molbace: 0.874842 val loss: 0.736489
[Epoch 114] ogbg-molbace: 0.866506 test loss: 0.803854
[Epoch 115; Iter     6/   36] train: loss: 0.0100246
[Epoch 115; Iter    36/   36] train: loss: 0.0497949
[Epoch 115] ogbg-molbace: 0.875000 val loss: 0.768309
[Epoch 115] ogbg-molbace: 0.856710 test loss: 0.924511
[Epoch 116; Iter    30/   36] train: loss: 0.0120560
[Epoch 116] ogbg-molbace: 0.874921 val loss: 0.770446
[Epoch 116] ogbg-molbace: 0.863863 test loss: 0.863789
[Epoch 117; Iter    24/   36] train: loss: 0.0163988
[Epoch 117] ogbg-molbace: 0.866083 val loss: 0.910701
[Epoch 117] ogbg-molbace: 0.857643 test loss: 1.021496
[Epoch 118; Iter    18/   36] train: loss: 0.0378468
[Epoch 118] ogbg-molbace: 0.865372 val loss: 0.794141
[Epoch 118] ogbg-molbace: 0.856243 test loss: 0.923512
[Epoch 119; Iter    12/   36] train: loss: 0.0135343
[Epoch 119] ogbg-molbace: 0.860480 val loss: 0.831771
[Epoch 119] ogbg-molbace: 0.857876 test loss: 0.920717
[Epoch 120; Iter     6/   36] train: loss: 0.0044686
[Epoch 120; Iter    36/   36] train: loss: 0.0041775
[Epoch 120] ogbg-molbace: 0.866951 val loss: 0.826451
[Epoch 120] ogbg-molbace: 0.868061 test loss: 0.871536
[Epoch 121; Iter    30/   36] train: loss: 0.0637853
[Epoch 121] ogbg-molbace: 0.870660 val loss: 0.848959
[Epoch 121] ogbg-molbace: 0.852356 test loss: 0.979881
[Epoch 122; Iter    24/   36] train: loss: 0.0147466
[Epoch 122] ogbg-molbace: 0.863005 val loss: 0.838349
[Epoch 122] ogbg-molbace: 0.852589 test loss: 0.968392
[Epoch 123; Iter    18/   36] train: loss: 0.0235817
[Epoch 123] ogbg-molbace: 0.880129 val loss: 0.817691
[Epoch 123] ogbg-molbace: 0.873970 test loss: 0.867119
[Epoch 124; Iter    12/   36] train: loss: 0.0393755
[Epoch 124] ogbg-molbace: 0.869476 val loss: 0.921043
[Epoch 124] ogbg-molbace: 0.857332 test loss: 1.015846
[Epoch 125; Iter     6/   36] train: loss: 0.1169332
[Epoch 125; Iter    36/   36] train: loss: 0.0829914
[Epoch 125] ogbg-molbace: 0.872238 val loss: 0.921818
[Epoch 125] ogbg-molbace: 0.860830 test loss: 1.034127
[Epoch 126; Iter    30/   36] train: loss: 0.0425275
[Epoch 126] ogbg-molbace: 0.854798 val loss: 0.974703
[Epoch 126] ogbg-molbace: 0.858731 test loss: 1.040639
[Epoch 127; Iter    24/   36] train: loss: 0.0436511
[Epoch 127] ogbg-molbace: 0.879340 val loss: 0.928236
[Epoch 127] ogbg-molbace: 0.863163 test loss: 1.012557
[Epoch 128; Iter    18/   36] train: loss: 0.0292434
[Epoch 128] ogbg-molbace: 0.866398 val loss: 0.909716
[Epoch 128] ogbg-molbace: 0.858420 test loss: 1.082345
Early stopping criterion based on -ogbg-molbace- that should be max reached after 128 epochs. Best model checkpoint was in epoch 68.
Statistics on  val_best_checkpoint
mean_pred: -0.10574258118867874
mean_targets: 0.44078949093818665
std_targets: 0.4981229901313782
prcauc: 0.8047127108154256
rocauc: 0.8633889376646181
ogbg-molbace: 0.8633889376646181
BCEWithLogitsLoss: 0.39641093214352924
Statistics on  train
mean_pred: -0.10021280497312546
std_pred: 2.0485217571258545
mean_targets: 0.45041322708129883
std_targets: 0.497740775346756
prcauc: 0.8943793516048251
rocauc: 0.9232061805890874
ogbg-molbace: 0.9232061805890874
BCEWithLogitsLoss: 0.3713504021487585
std_pred: 3.293933391571045
mean_targets: 0.4361233413219452
std_targets: 0.49699893593788147
prcauc: 0.8418460259185201
rocauc: 0.8946496212121211
ogbg-molbace: 0.8946496212121211
BCEWithLogitsLoss: 0.45014961063861847
Statistics on  test
mean_pred: -0.012556335888803005
std_pred: 6.057407379150391
mean_targets: 0.48017618060112
std_targets: 0.5007109642028809
prcauc: 0.841886264902499
rocauc: 0.8723371170890998
ogbg-molbace: 0.8723371170890998
BCEWithLogitsLoss: 0.532848060131073
Statistics on  train
mean_pred: 0.04303859546780586
std_pred: 3.496298313140869
mean_targets: 0.45609065890312195
std_targets: 0.49830350279808044
prcauc: 0.9749519276214473
rocauc: 0.9792673107890499
ogbg-molbace: 0.9792673107890499
BCEWithLogitsLoss: 0.22047295876675183
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.881329 val loss: 0.548467
[Epoch 78] ogbg-molbace: 0.876207 test loss: 0.483129
[Epoch 79; Iter    12/   41] train: loss: 0.5170906
[Epoch 79] ogbg-molbace: 0.915612 val loss: 0.389948
[Epoch 79] ogbg-molbace: 0.876558 test loss: 0.441928
[Epoch 80; Iter     1/   41] train: loss: 0.2180878
[Epoch 80; Iter    31/   41] train: loss: 0.2036269
[Epoch 80] ogbg-molbace: 0.893636 val loss: 0.476478
[Epoch 80] ogbg-molbace: 0.899210 test loss: 0.447077
[Epoch 81; Iter    20/   41] train: loss: 0.1912424
[Epoch 81] ogbg-molbace: 0.916667 val loss: 0.359938
[Epoch 81] ogbg-molbace: 0.894644 test loss: 0.390349
[Epoch 82; Iter     9/   41] train: loss: 0.1801848
[Epoch 82; Iter    39/   41] train: loss: 0.2374454
[Epoch 82] ogbg-molbace: 0.889065 val loss: 0.430157
[Epoch 82] ogbg-molbace: 0.857946 test loss: 0.538982
[Epoch 83; Iter    28/   41] train: loss: 0.1676797
[Epoch 83] ogbg-molbace: 0.879044 val loss: 0.516485
[Epoch 83] ogbg-molbace: 0.892186 test loss: 0.523990
[Epoch 84; Iter    17/   41] train: loss: 0.3854537
[Epoch 84] ogbg-molbace: 0.901195 val loss: 0.442075
[Epoch 84] ogbg-molbace: 0.898332 test loss: 0.480135
[Epoch 85; Iter     6/   41] train: loss: 0.2037703
[Epoch 85; Iter    36/   41] train: loss: 0.2278875
[Epoch 85] ogbg-molbace: 0.915084 val loss: 0.462218
[Epoch 85] ogbg-molbace: 0.887270 test loss: 0.477383
[Epoch 86; Iter    25/   41] train: loss: 0.2828785
[Epoch 86] ogbg-molbace: 0.908931 val loss: 0.461307
[Epoch 86] ogbg-molbace: 0.900790 test loss: 0.520740
[Epoch 87; Iter    14/   41] train: loss: 0.1783055
[Epoch 87] ogbg-molbace: 0.906997 val loss: 0.391205
[Epoch 87] ogbg-molbace: 0.890606 test loss: 0.432443
[Epoch 88; Iter     3/   41] train: loss: 0.1773458
[Epoch 88; Iter    33/   41] train: loss: 0.1981021
[Epoch 88] ogbg-molbace: 0.895745 val loss: 0.423643
[Epoch 88] ogbg-molbace: 0.886567 test loss: 0.483630
[Epoch 89; Iter    22/   41] train: loss: 0.3119044
[Epoch 89] ogbg-molbace: 0.905942 val loss: 0.447382
[Epoch 89] ogbg-molbace: 0.891133 test loss: 0.509899
[Epoch 90; Iter    11/   41] train: loss: 0.1690461
[Epoch 90; Iter    41/   41] train: loss: 0.1071140
[Epoch 90] ogbg-molbace: 0.888186 val loss: 0.427046
[Epoch 90] ogbg-molbace: 0.895522 test loss: 0.469002
[Epoch 91; Iter    30/   41] train: loss: 0.1832462
[Epoch 91] ogbg-molbace: 0.901020 val loss: 0.456729
[Epoch 91] ogbg-molbace: 0.892186 test loss: 0.416442
[Epoch 92; Iter    19/   41] train: loss: 0.1680201
[Epoch 92] ogbg-molbace: 0.909634 val loss: 0.386477
[Epoch 92] ogbg-molbace: 0.887445 test loss: 0.416015
[Epoch 93; Iter     8/   41] train: loss: 0.1170421
[Epoch 93; Iter    38/   41] train: loss: 0.1368443
[Epoch 93] ogbg-molbace: 0.893636 val loss: 0.460425
[Epoch 93] ogbg-molbace: 0.899912 test loss: 0.444079
[Epoch 94; Iter    27/   41] train: loss: 0.1788035
[Epoch 94] ogbg-molbace: 0.906294 val loss: 0.388126
[Epoch 94] ogbg-molbace: 0.890430 test loss: 0.473356
[Epoch 95; Iter    16/   41] train: loss: 0.0889947
[Epoch 95] ogbg-molbace: 0.894691 val loss: 0.676435
[Epoch 95] ogbg-molbace: 0.884460 test loss: 0.511033
[Epoch 96; Iter     5/   41] train: loss: 0.1329754
[Epoch 96; Iter    35/   41] train: loss: 0.0557774
[Epoch 96] ogbg-molbace: 0.912096 val loss: 0.420721
[Epoch 96] ogbg-molbace: 0.898156 test loss: 0.431429
[Epoch 97; Iter    24/   41] train: loss: 0.1860298
[Epoch 97] ogbg-molbace: 0.913678 val loss: 0.391822
[Epoch 97] ogbg-molbace: 0.890255 test loss: 0.459136
[Epoch 98; Iter    13/   41] train: loss: 0.0576758
[Epoch 98] ogbg-molbace: 0.925457 val loss: 0.394945
[Epoch 98] ogbg-molbace: 0.908165 test loss: 0.521014
[Epoch 99; Iter     2/   41] train: loss: 0.0457790
[Epoch 99; Iter    32/   41] train: loss: 0.0614347
[Epoch 99] ogbg-molbace: 0.880450 val loss: 0.587863
[Epoch 99] ogbg-molbace: 0.867779 test loss: 0.585505
[Epoch 100; Iter    21/   41] train: loss: 0.3047951
[Epoch 100] ogbg-molbace: 0.898910 val loss: 0.475700
[Epoch 100] ogbg-molbace: 0.890079 test loss: 0.518796
[Epoch 101; Iter    10/   41] train: loss: 0.1923112
[Epoch 101; Iter    40/   41] train: loss: 0.1072600
[Epoch 101] ogbg-molbace: 0.908052 val loss: 0.454636
[Epoch 101] ogbg-molbace: 0.890781 test loss: 0.485095
[Epoch 102; Iter    29/   41] train: loss: 0.1743881
[Epoch 102] ogbg-molbace: 0.913326 val loss: 0.392822
[Epoch 102] ogbg-molbace: 0.892713 test loss: 0.443930
[Epoch 103; Iter    18/   41] train: loss: 0.0709919
[Epoch 103] ogbg-molbace: 0.914030 val loss: 0.407964
[Epoch 103] ogbg-molbace: 0.901317 test loss: 0.451246
[Epoch 104; Iter     7/   41] train: loss: 0.0589967
[Epoch 104; Iter    37/   41] train: loss: 0.0583270
[Epoch 104] ogbg-molbace: 0.897679 val loss: 0.607424
[Epoch 104] ogbg-molbace: 0.899737 test loss: 0.560465
[Epoch 105; Iter    26/   41] train: loss: 0.1299866
[Epoch 105] ogbg-molbace: 0.901547 val loss: 0.485196
[Epoch 105] ogbg-molbace: 0.895522 test loss: 0.487141
[Epoch 106; Iter    15/   41] train: loss: 0.1472625
[Epoch 106] ogbg-molbace: 0.901020 val loss: 0.650024
[Epoch 106] ogbg-molbace: 0.880773 test loss: 0.612590
[Epoch 107; Iter     4/   41] train: loss: 0.0484714
[Epoch 107; Iter    34/   41] train: loss: 0.0129071
[Epoch 107] ogbg-molbace: 0.902954 val loss: 0.503819
[Epoch 107] ogbg-molbace: 0.890430 test loss: 0.505791
[Epoch 108; Iter    23/   41] train: loss: 0.0725719
[Epoch 108] ogbg-molbace: 0.889065 val loss: 0.537644
[Epoch 108] ogbg-molbace: 0.887445 test loss: 0.503027
[Epoch 109; Iter    12/   41] train: loss: 0.0674356
[Epoch 109] ogbg-molbace: 0.913150 val loss: 0.438372
[Epoch 109] ogbg-molbace: 0.891308 test loss: 0.488571
[Epoch 110; Iter     1/   41] train: loss: 0.1194540
[Epoch 110; Iter    31/   41] train: loss: 0.0392471
[Epoch 110] ogbg-molbace: 0.892581 val loss: 0.508257
[Epoch 110] ogbg-molbace: 0.884460 test loss: 0.515345
[Epoch 111; Iter    20/   41] train: loss: 0.1095801
[Epoch 111] ogbg-molbace: 0.902250 val loss: 0.547392
[Epoch 111] ogbg-molbace: 0.889201 test loss: 0.554897
[Epoch 112; Iter     9/   41] train: loss: 0.0721386
[Epoch 112; Iter    39/   41] train: loss: 0.0638720
[Epoch 112] ogbg-molbace: 0.899965 val loss: 0.538513
[Epoch 112] ogbg-molbace: 0.896225 test loss: 0.544375
[Epoch 113; Iter    28/   41] train: loss: 0.0347275
[Epoch 113] ogbg-molbace: 0.906118 val loss: 0.498631
[Epoch 113] ogbg-molbace: 0.883758 test loss: 0.604655
[Epoch 114; Iter    17/   41] train: loss: 0.0642024
[Epoch 114] ogbg-molbace: 0.883615 val loss: 0.577797
[Epoch 114] ogbg-molbace: 0.889903 test loss: 0.521655
[Epoch 115; Iter     6/   41] train: loss: 0.0688967
[Epoch 115; Iter    36/   41] train: loss: 0.0348315
[Epoch 115] ogbg-molbace: 0.890999 val loss: 0.552777
[Epoch 115] ogbg-molbace: 0.890255 test loss: 0.559538
[Epoch 116; Iter    25/   41] train: loss: 0.0218695
[Epoch 116] ogbg-molbace: 0.893636 val loss: 0.678161
[Epoch 116] ogbg-molbace: 0.880773 test loss: 0.647498
[Epoch 117; Iter    14/   41] train: loss: 0.0117021
[Epoch 117] ogbg-molbace: 0.891526 val loss: 0.615436
[Epoch 117] ogbg-molbace: 0.878139 test loss: 0.602756
[Epoch 118; Iter     3/   41] train: loss: 0.0180785
[Epoch 118; Iter    33/   41] train: loss: 0.0472244
[Epoch 118] ogbg-molbace: 0.890823 val loss: 0.584871
[Epoch 118] ogbg-molbace: 0.882177 test loss: 0.595536
[Epoch 119; Iter    22/   41] train: loss: 0.0249765
[Epoch 119] ogbg-molbace: 0.887307 val loss: 0.599158
[Epoch 119] ogbg-molbace: 0.866550 test loss: 0.662297
[Epoch 120; Iter    11/   41] train: loss: 0.0692634
[Epoch 120; Iter    41/   41] train: loss: 0.0051007
[Epoch 120] ogbg-molbace: 0.907876 val loss: 0.478207
[Epoch 120] ogbg-molbace: 0.885865 test loss: 0.554658
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 36.
Statistics on  val_best_checkpoint
mean_pred: -0.7955453395843506
std_pred: 1.63766610622406
mean_targets: 0.5231788158416748
std_targets: 0.5011245608329773
prcauc: 0.9396679440825035
rocauc: 0.9319620253164557
ogbg-molbace: 0.9319620253164557
BCEWithLogitsLoss: 0.4071761568387349
Statistics on  test
mean_pred: -0.9295800924301147
std_pred: 1.8339307308197021
mean_targets: 0.44078949093818665
std_targets: 0.4981229901313782
prcauc: 0.8166797265783573
rocauc: 0.8554872695346795
ogbg-molbace: 0.8554872695346795
BCEWithLogitsLoss: 0.40861066182454425
Statistics on  train
mean_pred: -0.8785933256149292
std_pred: 1.69094979763031
mean_targets: 0.45041322708129883
std_targets: 0.497740775346756
prcauc: 0.8633460543253141
rocauc: 0.9068883217217356
ogbg-molbace: 0.9068883217217356
BCEWithLogitsLoss: 0.42520108164810555
[Epoch 87; Iter     4/   31] train: loss: 0.1433327
[Epoch 87] ogbg-molbace: 0.888396 val loss: 0.503692
[Epoch 87] ogbg-molbace: 0.871128 test loss: 0.533312
[Epoch 88; Iter     3/   31] train: loss: 0.0403688
[Epoch 88] ogbg-molbace: 0.882746 val loss: 0.500514
[Epoch 88] ogbg-molbace: 0.866286 test loss: 0.560282
[Epoch 89; Iter     2/   31] train: loss: 0.1066394
[Epoch 89] ogbg-molbace: 0.887902 val loss: 0.437233
[Epoch 89] ogbg-molbace: 0.869514 test loss: 0.503523
[Epoch 90; Iter     1/   31] train: loss: 0.0661299
[Epoch 90; Iter    31/   31] train: loss: 0.5483914
[Epoch 90] ogbg-molbace: 0.870550 val loss: 0.540863
[Epoch 90] ogbg-molbace: 0.851453 test loss: 0.589675
[Epoch 91; Iter    30/   31] train: loss: 0.0824960
[Epoch 91] ogbg-molbace: 0.893507 val loss: 0.495441
[Epoch 91] ogbg-molbace: 0.868467 test loss: 0.587794
[Epoch 92; Iter    29/   31] train: loss: 0.1195150
[Epoch 92] ogbg-molbace: 0.893328 val loss: 0.465224
[Epoch 92] ogbg-molbace: 0.864715 test loss: 0.560327
[Epoch 93; Iter    28/   31] train: loss: 0.0641168
[Epoch 93] ogbg-molbace: 0.884988 val loss: 0.507070
[Epoch 93] ogbg-molbace: 0.866853 test loss: 0.571314
[Epoch 94; Iter    27/   31] train: loss: 0.1099704
[Epoch 94] ogbg-molbace: 0.881759 val loss: 0.551991
[Epoch 94] ogbg-molbace: 0.873179 test loss: 0.581251
[Epoch 95; Iter    26/   31] train: loss: 0.0644286
[Epoch 95] ogbg-molbace: 0.883149 val loss: 0.607366
[Epoch 95] ogbg-molbace: 0.866635 test loss: 0.647776
[Epoch 96; Iter    25/   31] train: loss: 0.1012305
[Epoch 96] ogbg-molbace: 0.885885 val loss: 0.519617
[Epoch 96] ogbg-molbace: 0.870081 test loss: 0.579913
[Epoch 97; Iter    24/   31] train: loss: 0.1746594
[Epoch 97] ogbg-molbace: 0.893911 val loss: 0.533308
[Epoch 97] ogbg-molbace: 0.874793 test loss: 0.622260
[Epoch 98; Iter    23/   31] train: loss: 0.0791460
[Epoch 98] ogbg-molbace: 0.895480 val loss: 0.521324
[Epoch 98] ogbg-molbace: 0.860745 test loss: 0.691781
[Epoch 99; Iter    22/   31] train: loss: 0.0650292
[Epoch 99] ogbg-molbace: 0.885974 val loss: 0.506716
[Epoch 99] ogbg-molbace: 0.855248 test loss: 0.632638
[Epoch 100; Iter    21/   31] train: loss: 0.0552508
[Epoch 100] ogbg-molbace: 0.895973 val loss: 0.591493
[Epoch 100] ogbg-molbace: 0.880726 test loss: 0.576419
[Epoch 101; Iter    20/   31] train: loss: 0.0970071
[Epoch 101] ogbg-molbace: 0.890503 val loss: 0.652617
[Epoch 101] ogbg-molbace: 0.869514 test loss: 0.651605
[Epoch 102; Iter    19/   31] train: loss: 0.0773612
[Epoch 102] ogbg-molbace: 0.892028 val loss: 0.502127
[Epoch 102] ogbg-molbace: 0.870910 test loss: 0.574525
[Epoch 103; Iter    18/   31] train: loss: 0.0541973
[Epoch 103] ogbg-molbace: 0.883912 val loss: 0.558648
[Epoch 103] ogbg-molbace: 0.854986 test loss: 0.672065
[Epoch 104; Iter    17/   31] train: loss: 0.0344016
[Epoch 104] ogbg-molbace: 0.899785 val loss: 0.470886
[Epoch 104] ogbg-molbace: 0.866853 test loss: 0.587569
[Epoch 105; Iter    16/   31] train: loss: 0.0444151
[Epoch 105] ogbg-molbace: 0.891803 val loss: 0.518487
[Epoch 105] ogbg-molbace: 0.850580 test loss: 0.700560
[Epoch 106; Iter    15/   31] train: loss: 0.0360050
[Epoch 106] ogbg-molbace: 0.892835 val loss: 0.538489
[Epoch 106] ogbg-molbace: 0.876014 test loss: 0.574404
[Epoch 107; Iter    14/   31] train: loss: 0.0396512
[Epoch 107] ogbg-molbace: 0.890100 val loss: 0.500547
[Epoch 107] ogbg-molbace: 0.850711 test loss: 0.669905
[Epoch 108; Iter    13/   31] train: loss: 0.0410072
[Epoch 108] ogbg-molbace: 0.889158 val loss: 0.542038
[Epoch 108] ogbg-molbace: 0.855205 test loss: 0.673537
[Epoch 109; Iter    12/   31] train: loss: 0.0292670
[Epoch 109] ogbg-molbace: 0.887858 val loss: 0.563806
[Epoch 109] ogbg-molbace: 0.854507 test loss: 0.697743
[Epoch 110; Iter    11/   31] train: loss: 0.0264863
[Epoch 110] ogbg-molbace: 0.886109 val loss: 0.550539
[Epoch 110] ogbg-molbace: 0.850144 test loss: 0.714465
[Epoch 111; Iter    10/   31] train: loss: 0.0723370
[Epoch 111] ogbg-molbace: 0.893642 val loss: 0.583822
[Epoch 111] ogbg-molbace: 0.867813 test loss: 0.672405
[Epoch 112; Iter     9/   31] train: loss: 0.0326843
[Epoch 112] ogbg-molbace: 0.883374 val loss: 0.587898
[Epoch 112] ogbg-molbace: 0.856601 test loss: 0.703262
[Epoch 113; Iter     8/   31] train: loss: 0.0114197
[Epoch 113] ogbg-molbace: 0.890010 val loss: 0.564041
[Epoch 113] ogbg-molbace: 0.852631 test loss: 0.745181
[Epoch 114; Iter     7/   31] train: loss: 0.0982884
[Epoch 114] ogbg-molbace: 0.893866 val loss: 0.630372
[Epoch 114] ogbg-molbace: 0.871215 test loss: 0.694194
[Epoch 115; Iter     6/   31] train: loss: 0.1252936
[Epoch 115] ogbg-molbace: 0.893418 val loss: 0.564260
[Epoch 115] ogbg-molbace: 0.854812 test loss: 0.723226
[Epoch 116; Iter     5/   31] train: loss: 0.0186423
[Epoch 116] ogbg-molbace: 0.892521 val loss: 0.586752
[Epoch 116] ogbg-molbace: 0.855074 test loss: 0.731432
[Epoch 117; Iter     4/   31] train: loss: 0.0098698
[Epoch 117] ogbg-molbace: 0.891759 val loss: 0.616015
[Epoch 117] ogbg-molbace: 0.857953 test loss: 0.730951
[Epoch 118; Iter     3/   31] train: loss: 0.0189775
[Epoch 118] ogbg-molbace: 0.886647 val loss: 0.602061
[Epoch 118] ogbg-molbace: 0.854507 test loss: 0.744008
[Epoch 119; Iter     2/   31] train: loss: 0.0355841
[Epoch 119] ogbg-molbace: 0.884674 val loss: 0.640294
[Epoch 119] ogbg-molbace: 0.853198 test loss: 0.762213
[Epoch 120; Iter     1/   31] train: loss: 0.0213634
[Epoch 120; Iter    31/   31] train: loss: 0.0480760
[Epoch 120] ogbg-molbace: 0.887006 val loss: 0.636930
[Epoch 120] ogbg-molbace: 0.852456 test loss: 0.780714
[Epoch 121; Iter    30/   31] train: loss: 0.0048509
[Epoch 121] ogbg-molbace: 0.883419 val loss: 0.645542
[Epoch 121] ogbg-molbace: 0.847090 test loss: 0.795709
[Epoch 122; Iter    29/   31] train: loss: 0.0117425
[Epoch 122] ogbg-molbace: 0.888620 val loss: 0.693201
[Epoch 122] ogbg-molbace: 0.853503 test loss: 0.808015
[Epoch 123; Iter    28/   31] train: loss: 0.0187590
[Epoch 123] ogbg-molbace: 0.891086 val loss: 0.638096
[Epoch 123] ogbg-molbace: 0.854594 test loss: 0.778876
[Epoch 124; Iter    27/   31] train: loss: 0.0388050
[Epoch 124] ogbg-molbace: 0.887499 val loss: 0.739587
[Epoch 124] ogbg-molbace: 0.849620 test loss: 0.864401
[Epoch 125; Iter    26/   31] train: loss: 0.0079208
[Epoch 125] ogbg-molbace: 0.886109 val loss: 0.634723
[Epoch 125] ogbg-molbace: 0.840284 test loss: 0.860787
[Epoch 126; Iter    25/   31] train: loss: 0.0420326
[Epoch 126] ogbg-molbace: 0.886468 val loss: 0.714349
[Epoch 126] ogbg-molbace: 0.857299 test loss: 0.825330
[Epoch 127; Iter    24/   31] train: loss: 0.0048653
[Epoch 127] ogbg-molbace: 0.880594 val loss: 0.699879
[Epoch 127] ogbg-molbace: 0.843426 test loss: 0.864671
[Epoch 128; Iter    23/   31] train: loss: 0.0556900
[Epoch 128] ogbg-molbace: 0.884405 val loss: 0.718264
[Epoch 128] ogbg-molbace: 0.849228 test loss: 0.865774
[Epoch 129; Iter    22/   31] train: loss: 0.0109003
[Epoch 129] ogbg-molbace: 0.881759 val loss: 0.688523
[Epoch 129] ogbg-molbace: 0.850362 test loss: 0.827124
[Epoch 130; Iter    21/   31] train: loss: 0.0062698
[Epoch 130] ogbg-molbace: 0.883777 val loss: 0.753501
[Epoch 130] ogbg-molbace: 0.841637 test loss: 0.939951
[Epoch 131; Iter    20/   31] train: loss: 0.0388122
[Epoch 131] ogbg-molbace: 0.884136 val loss: 0.741062
[Epoch 131] ogbg-molbace: 0.850711 test loss: 0.896099
[Epoch 132; Iter    19/   31] train: loss: 0.0174486
[Epoch 132] ogbg-molbace: 0.891579 val loss: 0.620431
[Epoch 132] ogbg-molbace: 0.843905 test loss: 0.884695
[Epoch 133; Iter    18/   31] train: loss: 0.0424074
[Epoch 133] ogbg-molbace: 0.875168 val loss: 0.750478
[Epoch 133] ogbg-molbace: 0.827415 test loss: 0.995516
[Epoch 134; Iter    17/   31] train: loss: 0.0460316
[Epoch 134] ogbg-molbace: 0.886064 val loss: 0.777010
[Epoch 134] ogbg-molbace: 0.829640 test loss: 1.033692
[Epoch 135; Iter    16/   31] train: loss: 0.0179525
[Epoch 135] ogbg-molbace: 0.885974 val loss: 0.629519
[Epoch 135] ogbg-molbace: 0.838758 test loss: 0.884712
[Epoch 136; Iter    15/   31] train: loss: 0.0307729
[Epoch 136] ogbg-molbace: 0.888351 val loss: 0.655969
[Epoch 136] ogbg-molbace: 0.845912 test loss: 0.831195
[Epoch 87; Iter     4/   31] train: loss: 0.2624926
[Epoch 87] ogbg-molbace: 0.900009 val loss: 0.424730
[Epoch 87] ogbg-molbace: 0.870125 test loss: 0.546312
[Epoch 88; Iter     3/   31] train: loss: 0.2422364
[Epoch 88] ogbg-molbace: 0.891759 val loss: 0.516438
[Epoch 88] ogbg-molbace: 0.875011 test loss: 0.542413
[Epoch 89; Iter     2/   31] train: loss: 0.1473397
[Epoch 89] ogbg-molbace: 0.904493 val loss: 0.441999
[Epoch 89] ogbg-molbace: 0.886266 test loss: 0.531406
[Epoch 90; Iter     1/   31] train: loss: 0.1305997
[Epoch 90; Iter    31/   31] train: loss: 0.6456984
[Epoch 90] ogbg-molbace: 0.889337 val loss: 0.482532
[Epoch 90] ogbg-molbace: 0.873571 test loss: 0.551491
[Epoch 91; Iter    30/   31] train: loss: 0.1397858
[Epoch 91] ogbg-molbace: 0.888530 val loss: 0.488884
[Epoch 91] ogbg-molbace: 0.857822 test loss: 0.651642
[Epoch 92; Iter    29/   31] train: loss: 0.3331954
[Epoch 92] ogbg-molbace: 0.877231 val loss: 0.500781
[Epoch 92] ogbg-molbace: 0.852107 test loss: 0.609808
[Epoch 93; Iter    28/   31] train: loss: 0.3844880
[Epoch 93] ogbg-molbace: 0.897229 val loss: 0.472180
[Epoch 93] ogbg-molbace: 0.870299 test loss: 0.579593
[Epoch 94; Iter    27/   31] train: loss: 0.0562697
[Epoch 94] ogbg-molbace: 0.890413 val loss: 0.472851
[Epoch 94] ogbg-molbace: 0.873833 test loss: 0.526617
[Epoch 95; Iter    26/   31] train: loss: 0.1049160
[Epoch 95] ogbg-molbace: 0.897094 val loss: 0.451389
[Epoch 95] ogbg-molbace: 0.873964 test loss: 0.565372
[Epoch 96; Iter    25/   31] train: loss: 0.0536527
[Epoch 96] ogbg-molbace: 0.898260 val loss: 0.465636
[Epoch 96] ogbg-molbace: 0.873746 test loss: 0.564711
[Epoch 97; Iter    24/   31] train: loss: 0.0601136
[Epoch 97] ogbg-molbace: 0.899874 val loss: 0.453311
[Epoch 97] ogbg-molbace: 0.874836 test loss: 0.576067
[Epoch 98; Iter    23/   31] train: loss: 0.0660983
[Epoch 98] ogbg-molbace: 0.888754 val loss: 0.504518
[Epoch 98] ogbg-molbace: 0.868380 test loss: 0.617060
[Epoch 99; Iter    22/   31] train: loss: 0.1799716
[Epoch 99] ogbg-molbace: 0.887050 val loss: 0.521834
[Epoch 99] ogbg-molbace: 0.865980 test loss: 0.654296
[Epoch 100; Iter    21/   31] train: loss: 0.1977739
[Epoch 100] ogbg-molbace: 0.892790 val loss: 0.655717
[Epoch 100] ogbg-molbace: 0.881947 test loss: 0.668182
[Epoch 101; Iter    20/   31] train: loss: 0.0682227
[Epoch 101] ogbg-molbace: 0.872254 val loss: 0.607202
[Epoch 101] ogbg-molbace: 0.869558 test loss: 0.609142
[Epoch 102; Iter    19/   31] train: loss: 0.0218807
[Epoch 102] ogbg-molbace: 0.882432 val loss: 0.549019
[Epoch 102] ogbg-molbace: 0.865195 test loss: 0.631657
[Epoch 103; Iter    18/   31] train: loss: 0.0299058
[Epoch 103] ogbg-molbace: 0.882970 val loss: 0.601082
[Epoch 103] ogbg-molbace: 0.872262 test loss: 0.663495
[Epoch 104; Iter    17/   31] train: loss: 0.1361535
[Epoch 104] ogbg-molbace: 0.884584 val loss: 0.570975
[Epoch 104] ogbg-molbace: 0.867245 test loss: 0.659952
[Epoch 105; Iter    16/   31] train: loss: 0.1758128
[Epoch 105] ogbg-molbace: 0.891041 val loss: 0.546026
[Epoch 105] ogbg-molbace: 0.862098 test loss: 0.711722
[Epoch 106; Iter    15/   31] train: loss: 0.2070340
[Epoch 106] ogbg-molbace: 0.880818 val loss: 0.565491
[Epoch 106] ogbg-molbace: 0.859698 test loss: 0.674904
[Epoch 107; Iter    14/   31] train: loss: 0.0745082
[Epoch 107] ogbg-molbace: 0.883777 val loss: 0.543716
[Epoch 107] ogbg-molbace: 0.863799 test loss: 0.677096
[Epoch 108; Iter    13/   31] train: loss: 0.0837550
[Epoch 108] ogbg-molbace: 0.889382 val loss: 0.581401
[Epoch 108] ogbg-molbace: 0.877498 test loss: 0.632998
[Epoch 109; Iter    12/   31] train: loss: 0.0389061
[Epoch 109] ogbg-molbace: 0.888306 val loss: 0.551392
[Epoch 109] ogbg-molbace: 0.875055 test loss: 0.633825
[Epoch 110; Iter    11/   31] train: loss: 0.0140001
[Epoch 110] ogbg-molbace: 0.882791 val loss: 0.623687
[Epoch 110] ogbg-molbace: 0.866417 test loss: 0.688431
[Epoch 111; Iter    10/   31] train: loss: 0.0247414
[Epoch 111] ogbg-molbace: 0.892700 val loss: 0.582946
[Epoch 111] ogbg-molbace: 0.866329 test loss: 0.708833
[Epoch 112; Iter     9/   31] train: loss: 0.0572987
[Epoch 112] ogbg-molbace: 0.888575 val loss: 0.552326
[Epoch 112] ogbg-molbace: 0.874575 test loss: 0.661691
[Epoch 113; Iter     8/   31] train: loss: 0.0138487
[Epoch 113] ogbg-molbace: 0.884943 val loss: 0.586838
[Epoch 113] ogbg-molbace: 0.863363 test loss: 0.713632
[Epoch 114; Iter     7/   31] train: loss: 0.0113271
[Epoch 114] ogbg-molbace: 0.889068 val loss: 0.581754
[Epoch 114] ogbg-molbace: 0.869514 test loss: 0.674879
[Epoch 115; Iter     6/   31] train: loss: 0.0140392
[Epoch 115] ogbg-molbace: 0.893597 val loss: 0.575101
[Epoch 115] ogbg-molbace: 0.879286 test loss: 0.640605
[Epoch 116; Iter     5/   31] train: loss: 0.0516461
[Epoch 116] ogbg-molbace: 0.887185 val loss: 0.606814
[Epoch 116] ogbg-molbace: 0.857909 test loss: 0.755613
[Epoch 117; Iter     4/   31] train: loss: 0.0253727
[Epoch 117] ogbg-molbace: 0.893866 val loss: 0.578253
[Epoch 117] ogbg-molbace: 0.865348 test loss: 0.752867
[Epoch 118; Iter     3/   31] train: loss: 0.0173945
[Epoch 118] ogbg-molbace: 0.888037 val loss: 0.609180
[Epoch 118] ogbg-molbace: 0.861312 test loss: 0.753289
[Epoch 119; Iter     2/   31] train: loss: 0.0249472
[Epoch 119] ogbg-molbace: 0.890682 val loss: 0.614567
[Epoch 119] ogbg-molbace: 0.859087 test loss: 0.774910
[Epoch 120; Iter     1/   31] train: loss: 0.0891111
[Epoch 120; Iter    31/   31] train: loss: 0.0127874
[Epoch 120] ogbg-molbace: 0.893732 val loss: 0.576605
[Epoch 120] ogbg-molbace: 0.862272 test loss: 0.730206
[Epoch 121; Iter    30/   31] train: loss: 0.0399553
[Epoch 121] ogbg-molbace: 0.896198 val loss: 0.597270
[Epoch 121] ogbg-molbace: 0.865457 test loss: 0.750047
[Epoch 122; Iter    29/   31] train: loss: 0.0211516
[Epoch 122] ogbg-molbace: 0.891714 val loss: 0.607331
[Epoch 122] ogbg-molbace: 0.861138 test loss: 0.780605
[Epoch 123; Iter    28/   31] train: loss: 0.0272541
[Epoch 123] ogbg-molbace: 0.891265 val loss: 0.605099
[Epoch 123] ogbg-molbace: 0.860658 test loss: 0.777911
[Epoch 124; Iter    27/   31] train: loss: 0.0079766
[Epoch 124] ogbg-molbace: 0.888799 val loss: 0.613834
[Epoch 124] ogbg-molbace: 0.856775 test loss: 0.765840
[Epoch 125; Iter    26/   31] train: loss: 0.0382341
[Epoch 125] ogbg-molbace: 0.889920 val loss: 0.774376
[Epoch 125] ogbg-molbace: 0.861792 test loss: 0.864833
[Epoch 126; Iter    25/   31] train: loss: 0.0972927
[Epoch 126] ogbg-molbace: 0.896691 val loss: 0.582719
[Epoch 126] ogbg-molbace: 0.873833 test loss: 0.733627
[Epoch 127; Iter    24/   31] train: loss: 0.0201053
[Epoch 127] ogbg-molbace: 0.885974 val loss: 0.680528
[Epoch 127] ogbg-molbace: 0.870823 test loss: 0.773901
[Epoch 128; Iter    23/   31] train: loss: 0.0550109
[Epoch 128] ogbg-molbace: 0.888261 val loss: 0.646688
[Epoch 128] ogbg-molbace: 0.864715 test loss: 0.781445
[Epoch 129; Iter    22/   31] train: loss: 0.0666470
[Epoch 129] ogbg-molbace: 0.890458 val loss: 0.685327
[Epoch 129] ogbg-molbace: 0.866678 test loss: 0.809509
[Epoch 130; Iter    21/   31] train: loss: 0.0083911
[Epoch 130] ogbg-molbace: 0.893732 val loss: 0.620563
[Epoch 130] ogbg-molbace: 0.865021 test loss: 0.770942
[Epoch 131; Iter    20/   31] train: loss: 0.1882909
[Epoch 131] ogbg-molbace: 0.882836 val loss: 0.675695
[Epoch 131] ogbg-molbace: 0.871215 test loss: 0.739314
[Epoch 132; Iter    19/   31] train: loss: 0.1441308
[Epoch 132] ogbg-molbace: 0.893104 val loss: 0.624753
[Epoch 132] ogbg-molbace: 0.861443 test loss: 0.785895
[Epoch 133; Iter    18/   31] train: loss: 0.0370421
[Epoch 133] ogbg-molbace: 0.884226 val loss: 0.654877
[Epoch 133] ogbg-molbace: 0.861967 test loss: 0.803656
[Epoch 134; Iter    17/   31] train: loss: 0.0284234
[Epoch 134] ogbg-molbace: 0.881177 val loss: 0.678380
[Epoch 134] ogbg-molbace: 0.862054 test loss: 0.798170
[Epoch 135; Iter    16/   31] train: loss: 0.0056796
[Epoch 135] ogbg-molbace: 0.883867 val loss: 0.675962
[Epoch 135] ogbg-molbace: 0.861007 test loss: 0.795513
[Epoch 136; Iter    15/   31] train: loss: 0.0165374
[Epoch 136] ogbg-molbace: 0.888082 val loss: 0.660067
[Epoch 136] ogbg-molbace: 0.864322 test loss: 0.789338
[Epoch 87; Iter     4/   31] train: loss: 0.2838130
[Epoch 87] ogbg-molbace: 0.892028 val loss: 0.443037
[Epoch 87] ogbg-molbace: 0.881642 test loss: 0.468795
[Epoch 88; Iter     3/   31] train: loss: 0.2395457
[Epoch 88] ogbg-molbace: 0.900054 val loss: 0.417862
[Epoch 88] ogbg-molbace: 0.856732 test loss: 0.513565
[Epoch 89; Iter     2/   31] train: loss: 0.1094968
[Epoch 89] ogbg-molbace: 0.889337 val loss: 0.444987
[Epoch 89] ogbg-molbace: 0.872481 test loss: 0.481658
[Epoch 90; Iter     1/   31] train: loss: 0.3265544
[Epoch 90; Iter    31/   31] train: loss: 1.4090525
[Epoch 90] ogbg-molbace: 0.870415 val loss: 0.529585
[Epoch 90] ogbg-molbace: 0.846087 test loss: 0.595731
[Epoch 91; Iter    30/   31] train: loss: 0.1991036
[Epoch 91] ogbg-molbace: 0.887633 val loss: 0.567596
[Epoch 91] ogbg-molbace: 0.860222 test loss: 0.615821
[Epoch 92; Iter    29/   31] train: loss: 0.2782619
[Epoch 92] ogbg-molbace: 0.891310 val loss: 0.422879
[Epoch 92] ogbg-molbace: 0.864192 test loss: 0.484606
[Epoch 93; Iter    28/   31] train: loss: 0.2828423
[Epoch 93] ogbg-molbace: 0.881849 val loss: 0.469133
[Epoch 93] ogbg-molbace: 0.854376 test loss: 0.526158
[Epoch 94; Iter    27/   31] train: loss: 0.2005253
[Epoch 94] ogbg-molbace: 0.895704 val loss: 0.472856
[Epoch 94] ogbg-molbace: 0.880290 test loss: 0.501410
[Epoch 95; Iter    26/   31] train: loss: 0.1202991
[Epoch 95] ogbg-molbace: 0.875078 val loss: 0.613158
[Epoch 95] ogbg-molbace: 0.845389 test loss: 0.696768
[Epoch 96; Iter    25/   31] train: loss: 0.2073763
[Epoch 96] ogbg-molbace: 0.889203 val loss: 0.439021
[Epoch 96] ogbg-molbace: 0.868641 test loss: 0.501241
[Epoch 97; Iter    24/   31] train: loss: 0.1528545
[Epoch 97] ogbg-molbace: 0.885347 val loss: 0.483105
[Epoch 97] ogbg-molbace: 0.864671 test loss: 0.550786
[Epoch 98; Iter    23/   31] train: loss: 0.2259310
[Epoch 98] ogbg-molbace: 0.882029 val loss: 0.485486
[Epoch 98] ogbg-molbace: 0.872306 test loss: 0.515169
[Epoch 99; Iter    22/   31] train: loss: 0.2905050
[Epoch 99] ogbg-molbace: 0.894852 val loss: 0.506772
[Epoch 99] ogbg-molbace: 0.876189 test loss: 0.565898
[Epoch 100; Iter    21/   31] train: loss: 0.0921797
[Epoch 100] ogbg-molbace: 0.889427 val loss: 0.557288
[Epoch 100] ogbg-molbace: 0.864148 test loss: 0.613593
[Epoch 101; Iter    20/   31] train: loss: 0.1641925
[Epoch 101] ogbg-molbace: 0.889292 val loss: 0.502400
[Epoch 101] ogbg-molbace: 0.850537 test loss: 0.652569
[Epoch 102; Iter    19/   31] train: loss: 0.1513008
[Epoch 102] ogbg-molbace: 0.870056 val loss: 0.591131
[Epoch 102] ogbg-molbace: 0.847570 test loss: 0.665291
[Epoch 103; Iter    18/   31] train: loss: 0.1714268
[Epoch 103] ogbg-molbace: 0.881221 val loss: 0.533477
[Epoch 103] ogbg-molbace: 0.861007 test loss: 0.585387
[Epoch 104; Iter    17/   31] train: loss: 0.2107046
[Epoch 104] ogbg-molbace: 0.885033 val loss: 0.499758
[Epoch 104] ogbg-molbace: 0.865151 test loss: 0.558157
[Epoch 105; Iter    16/   31] train: loss: 0.1905488
[Epoch 105] ogbg-molbace: 0.871312 val loss: 0.534124
[Epoch 105] ogbg-molbace: 0.864715 test loss: 0.566933
[Epoch 106; Iter    15/   31] train: loss: 0.2057330
[Epoch 106] ogbg-molbace: 0.878217 val loss: 0.520190
[Epoch 106] ogbg-molbace: 0.867333 test loss: 0.600000
[Epoch 107; Iter    14/   31] train: loss: 0.3388427
[Epoch 107] ogbg-molbace: 0.886333 val loss: 0.541288
[Epoch 107] ogbg-molbace: 0.855379 test loss: 0.630400
[Epoch 108; Iter    13/   31] train: loss: 0.0725880
[Epoch 108] ogbg-molbace: 0.892655 val loss: 0.465378
[Epoch 108] ogbg-molbace: 0.862228 test loss: 0.579291
[Epoch 109; Iter    12/   31] train: loss: 0.1675630
[Epoch 109] ogbg-molbace: 0.888979 val loss: 0.511491
[Epoch 109] ogbg-molbace: 0.867333 test loss: 0.586208
[Epoch 110; Iter    11/   31] train: loss: 0.0657356
[Epoch 110] ogbg-molbace: 0.892700 val loss: 0.511921
[Epoch 110] ogbg-molbace: 0.880202 test loss: 0.565841
[Epoch 111; Iter    10/   31] train: loss: 0.2514035
[Epoch 111] ogbg-molbace: 0.890951 val loss: 0.508754
[Epoch 111] ogbg-molbace: 0.878501 test loss: 0.551208
[Epoch 112; Iter     9/   31] train: loss: 0.0356566
[Epoch 112] ogbg-molbace: 0.891803 val loss: 0.498111
[Epoch 112] ogbg-molbace: 0.863188 test loss: 0.612123
[Epoch 113; Iter     8/   31] train: loss: 0.0530150
[Epoch 113] ogbg-molbace: 0.892297 val loss: 0.532385
[Epoch 113] ogbg-molbace: 0.862970 test loss: 0.625340
[Epoch 114; Iter     7/   31] train: loss: 0.0356338
[Epoch 114] ogbg-molbace: 0.891534 val loss: 0.523096
[Epoch 114] ogbg-molbace: 0.870256 test loss: 0.621519
[Epoch 115; Iter     6/   31] train: loss: 0.0428539
[Epoch 115] ogbg-molbace: 0.891400 val loss: 0.573857
[Epoch 115] ogbg-molbace: 0.872044 test loss: 0.636785
[Epoch 116; Iter     5/   31] train: loss: 0.1049617
[Epoch 116] ogbg-molbace: 0.889875 val loss: 0.548924
[Epoch 116] ogbg-molbace: 0.868031 test loss: 0.634769
[Epoch 117; Iter     4/   31] train: loss: 0.0418517
[Epoch 117] ogbg-molbace: 0.887185 val loss: 0.543072
[Epoch 117] ogbg-molbace: 0.856034 test loss: 0.671081
[Epoch 118; Iter     3/   31] train: loss: 0.0500409
[Epoch 118] ogbg-molbace: 0.899247 val loss: 0.569909
[Epoch 118] ogbg-molbace: 0.866242 test loss: 0.697063
[Epoch 119; Iter     2/   31] train: loss: 0.0735615
[Epoch 119] ogbg-molbace: 0.894539 val loss: 0.544596
[Epoch 119] ogbg-molbace: 0.870081 test loss: 0.658555
[Epoch 120; Iter     1/   31] train: loss: 0.0359767
[Epoch 120; Iter    31/   31] train: loss: 0.0293868
[Epoch 120] ogbg-molbace: 0.894001 val loss: 0.545513
[Epoch 120] ogbg-molbace: 0.863232 test loss: 0.680750
[Epoch 121; Iter    30/   31] train: loss: 0.0207731
[Epoch 121] ogbg-molbace: 0.892655 val loss: 0.554509
[Epoch 121] ogbg-molbace: 0.860702 test loss: 0.681123
[Epoch 122; Iter    29/   31] train: loss: 0.0428759
[Epoch 122] ogbg-molbace: 0.893462 val loss: 0.551976
[Epoch 122] ogbg-molbace: 0.874793 test loss: 0.642775
[Epoch 123; Iter    28/   31] train: loss: 0.1134412
[Epoch 123] ogbg-molbace: 0.890010 val loss: 0.599988
[Epoch 123] ogbg-molbace: 0.871128 test loss: 0.675656
[Epoch 124; Iter    27/   31] train: loss: 0.1929130
[Epoch 124] ogbg-molbace: 0.882432 val loss: 0.771848
[Epoch 124] ogbg-molbace: 0.865762 test loss: 0.836322
[Epoch 125; Iter    26/   31] train: loss: 0.0183003
[Epoch 125] ogbg-molbace: 0.889561 val loss: 0.573694
[Epoch 125] ogbg-molbace: 0.861967 test loss: 0.686200
[Epoch 126; Iter    25/   31] train: loss: 0.0119480
[Epoch 126] ogbg-molbace: 0.893687 val loss: 0.672546
[Epoch 126] ogbg-molbace: 0.868729 test loss: 0.780010
[Epoch 127; Iter    24/   31] train: loss: 0.0757851
[Epoch 127] ogbg-molbace: 0.881759 val loss: 0.619281
[Epoch 127] ogbg-molbace: 0.868031 test loss: 0.648838
[Epoch 128; Iter    23/   31] train: loss: 0.0867150
[Epoch 128] ogbg-molbace: 0.882880 val loss: 0.664770
[Epoch 128] ogbg-molbace: 0.860527 test loss: 0.793313
[Epoch 129; Iter    22/   31] train: loss: 0.1565395
[Epoch 129] ogbg-molbace: 0.894135 val loss: 0.571103
[Epoch 129] ogbg-molbace: 0.855946 test loss: 0.741392
[Epoch 130; Iter    21/   31] train: loss: 0.0716072
[Epoch 130] ogbg-molbace: 0.874944 val loss: 0.695235
[Epoch 130] ogbg-molbace: 0.864410 test loss: 0.727111
[Epoch 131; Iter    20/   31] train: loss: 0.0529543
[Epoch 131] ogbg-molbace: 0.876872 val loss: 0.689823
[Epoch 131] ogbg-molbace: 0.839456 test loss: 0.915150
[Epoch 132; Iter    19/   31] train: loss: 0.0371456
[Epoch 132] ogbg-molbace: 0.900547 val loss: 0.941166
[Epoch 132] ogbg-molbace: 0.876625 test loss: 1.009418
[Epoch 133; Iter    18/   31] train: loss: 0.1477318
[Epoch 133] ogbg-molbace: 0.891803 val loss: 0.582653
[Epoch 133] ogbg-molbace: 0.870692 test loss: 0.679829
[Epoch 134; Iter    17/   31] train: loss: 0.0427693
[Epoch 134] ogbg-molbace: 0.890503 val loss: 0.628179
[Epoch 134] ogbg-molbace: 0.861967 test loss: 0.768293
[Epoch 135; Iter    16/   31] train: loss: 0.0166593
[Epoch 135] ogbg-molbace: 0.890324 val loss: 0.612043
[Epoch 135] ogbg-molbace: 0.867158 test loss: 0.725227
[Epoch 136; Iter    15/   31] train: loss: 0.0774195
[Epoch 136] ogbg-molbace: 0.893821 val loss: 0.604879
[Epoch 136] ogbg-molbace: 0.870038 test loss: 0.715945
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.887445 test loss: 0.634184
[Epoch 124; Iter    27/   41] train: loss: 0.1422279
[Epoch 124] ogbg-molbace: 0.903657 val loss: 0.526598
[Epoch 124] ogbg-molbace: 0.872344 test loss: 0.575921
[Epoch 125; Iter    16/   41] train: loss: 0.0250025
[Epoch 125] ogbg-molbace: 0.904887 val loss: 0.512481
[Epoch 125] ogbg-molbace: 0.886918 test loss: 0.578059
[Epoch 126; Iter     5/   41] train: loss: 0.0557264
[Epoch 126; Iter    35/   41] train: loss: 0.0125079
[Epoch 126] ogbg-molbace: 0.905063 val loss: 0.539744
[Epoch 126] ogbg-molbace: 0.890957 test loss: 0.558777
[Epoch 127; Iter    24/   41] train: loss: 0.0223407
[Epoch 127] ogbg-molbace: 0.888713 val loss: 0.621355
[Epoch 127] ogbg-molbace: 0.880246 test loss: 0.673519
[Epoch 128; Iter    13/   41] train: loss: 0.0047442
[Epoch 128] ogbg-molbace: 0.894339 val loss: 0.635270
[Epoch 128] ogbg-molbace: 0.877436 test loss: 0.713695
[Epoch 129; Iter     2/   41] train: loss: 0.0462203
[Epoch 129; Iter    32/   41] train: loss: 0.1122371
[Epoch 129] ogbg-molbace: 0.908052 val loss: 0.581881
[Epoch 129] ogbg-molbace: 0.888147 test loss: 0.618693
[Epoch 130; Iter    21/   41] train: loss: 0.0055800
[Epoch 130] ogbg-molbace: 0.901899 val loss: 0.540177
[Epoch 130] ogbg-molbace: 0.876558 test loss: 0.667784
[Epoch 131; Iter    10/   41] train: loss: 0.0140734
[Epoch 131; Iter    40/   41] train: loss: 0.4599227
[Epoch 131] ogbg-molbace: 0.894866 val loss: 0.639778
[Epoch 131] ogbg-molbace: 0.871466 test loss: 0.655107
[Epoch 132; Iter    29/   41] train: loss: 0.0514796
[Epoch 132] ogbg-molbace: 0.909107 val loss: 0.537574
[Epoch 132] ogbg-molbace: 0.877261 test loss: 0.688393
[Epoch 133; Iter    18/   41] train: loss: 0.0099771
[Epoch 133] ogbg-molbace: 0.915436 val loss: 0.497956
[Epoch 133] ogbg-molbace: 0.888147 test loss: 0.682393
[Epoch 134; Iter     7/   41] train: loss: 0.0183119
[Epoch 134; Iter    37/   41] train: loss: 0.0300075
[Epoch 134] ogbg-molbace: 0.899086 val loss: 0.547429
[Epoch 134] ogbg-molbace: 0.881475 test loss: 0.634329
[Epoch 135; Iter    26/   41] train: loss: 0.1687523
[Epoch 135] ogbg-molbace: 0.906646 val loss: 0.588899
[Epoch 135] ogbg-molbace: 0.886918 test loss: 0.651271
[Epoch 136; Iter    15/   41] train: loss: 0.1287318
[Epoch 136] ogbg-molbace: 0.904008 val loss: 0.614645
[Epoch 136] ogbg-molbace: 0.871993 test loss: 0.762653
[Epoch 137; Iter     4/   41] train: loss: 0.0198134
[Epoch 137; Iter    34/   41] train: loss: 0.0240499
[Epoch 137] ogbg-molbace: 0.920886 val loss: 0.505529
[Epoch 137] ogbg-molbace: 0.872871 test loss: 0.736225
[Epoch 138; Iter    23/   41] train: loss: 0.0330238
[Epoch 138] ogbg-molbace: 0.898558 val loss: 0.611357
[Epoch 138] ogbg-molbace: 0.873924 test loss: 0.706600
[Epoch 139; Iter    12/   41] train: loss: 0.1174117
[Epoch 139] ogbg-molbace: 0.909459 val loss: 0.633303
[Epoch 139] ogbg-molbace: 0.878314 test loss: 0.757875
[Epoch 140; Iter     1/   41] train: loss: 0.0380325
[Epoch 140; Iter    31/   41] train: loss: 0.0988751
[Epoch 140] ogbg-molbace: 0.892581 val loss: 0.714260
[Epoch 140] ogbg-molbace: 0.863038 test loss: 0.735854
[Epoch 141; Iter    20/   41] train: loss: 0.0321071
[Epoch 141] ogbg-molbace: 0.900844 val loss: 0.647649
[Epoch 141] ogbg-molbace: 0.887445 test loss: 0.738109
[Epoch 142; Iter     9/   41] train: loss: 0.1201433
[Epoch 142; Iter    39/   41] train: loss: 0.0376930
[Epoch 142] ogbg-molbace: 0.900492 val loss: 0.580136
[Epoch 142] ogbg-molbace: 0.886392 test loss: 0.682086
[Epoch 143; Iter    28/   41] train: loss: 0.0185033
[Epoch 143] ogbg-molbace: 0.897328 val loss: 0.621277
[Epoch 143] ogbg-molbace: 0.876207 test loss: 0.781742
[Epoch 144; Iter    17/   41] train: loss: 0.1177715
[Epoch 144] ogbg-molbace: 0.905591 val loss: 0.615480
[Epoch 144] ogbg-molbace: 0.878490 test loss: 0.757343
[Epoch 145; Iter     6/   41] train: loss: 0.0024617
[Epoch 145; Iter    36/   41] train: loss: 0.0033244
[Epoch 145] ogbg-molbace: 0.910338 val loss: 0.619617
[Epoch 145] ogbg-molbace: 0.886040 test loss: 0.763171
[Epoch 146; Iter    25/   41] train: loss: 0.0235570
[Epoch 146] ogbg-molbace: 0.901723 val loss: 0.707455
[Epoch 146] ogbg-molbace: 0.878139 test loss: 0.793099
[Epoch 147; Iter    14/   41] train: loss: 0.0050958
[Epoch 147] ogbg-molbace: 0.908931 val loss: 0.623086
[Epoch 147] ogbg-molbace: 0.888674 test loss: 0.698052
[Epoch 148; Iter     3/   41] train: loss: 0.0039085
[Epoch 148; Iter    33/   41] train: loss: 0.0384358
[Epoch 148] ogbg-molbace: 0.908579 val loss: 0.607276
[Epoch 148] ogbg-molbace: 0.881651 test loss: 0.785718
[Epoch 149; Iter    22/   41] train: loss: 0.0071442
[Epoch 149] ogbg-molbace: 0.903305 val loss: 0.657005
[Epoch 149] ogbg-molbace: 0.877436 test loss: 0.819496
[Epoch 150; Iter    11/   41] train: loss: 0.0034196
[Epoch 150; Iter    41/   41] train: loss: 0.0094867
[Epoch 150] ogbg-molbace: 0.909986 val loss: 0.611519
[Epoch 150] ogbg-molbace: 0.881826 test loss: 0.721210
[Epoch 151; Iter    30/   41] train: loss: 0.0035023
[Epoch 151] ogbg-molbace: 0.909986 val loss: 0.631834
[Epoch 151] ogbg-molbace: 0.876734 test loss: 0.823598
[Epoch 152; Iter    19/   41] train: loss: 0.0018614
[Epoch 152] ogbg-molbace: 0.921238 val loss: 0.571334
[Epoch 152] ogbg-molbace: 0.888323 test loss: 0.727568
[Epoch 153; Iter     8/   41] train: loss: 0.0093472
[Epoch 153; Iter    38/   41] train: loss: 0.0049584
[Epoch 153] ogbg-molbace: 0.916491 val loss: 0.573910
[Epoch 153] ogbg-molbace: 0.879543 test loss: 0.777632
[Epoch 154; Iter    27/   41] train: loss: 0.0117057
[Epoch 154] ogbg-molbace: 0.914733 val loss: 0.624449
[Epoch 154] ogbg-molbace: 0.869710 test loss: 0.811980
[Epoch 155; Iter    16/   41] train: loss: 0.0028918
[Epoch 155] ogbg-molbace: 0.910338 val loss: 0.654324
[Epoch 155] ogbg-molbace: 0.874276 test loss: 0.876665
[Epoch 156; Iter     5/   41] train: loss: 0.0161586
[Epoch 156; Iter    35/   41] train: loss: 0.0012898
[Epoch 156] ogbg-molbace: 0.912447 val loss: 0.642698
[Epoch 156] ogbg-molbace: 0.876207 test loss: 0.844565
[Epoch 157; Iter    24/   41] train: loss: 0.0059821
[Epoch 157] ogbg-molbace: 0.913678 val loss: 0.649831
[Epoch 157] ogbg-molbace: 0.880597 test loss: 0.810228
[Epoch 158; Iter    13/   41] train: loss: 0.0130622
[Epoch 158] ogbg-molbace: 0.915612 val loss: 0.638380
[Epoch 158] ogbg-molbace: 0.882002 test loss: 0.779534
[Epoch 159; Iter     2/   41] train: loss: 0.0017080
[Epoch 159; Iter    32/   41] train: loss: 0.0662451
[Epoch 159] ogbg-molbace: 0.909283 val loss: 0.655906
[Epoch 159] ogbg-molbace: 0.880948 test loss: 0.875833
[Epoch 160; Iter    21/   41] train: loss: 0.0020373
[Epoch 160] ogbg-molbace: 0.904360 val loss: 0.719782
[Epoch 160] ogbg-molbace: 0.875329 test loss: 0.808698
[Epoch 161; Iter    10/   41] train: loss: 0.0273985
[Epoch 161; Iter    40/   41] train: loss: 0.0025097
[Epoch 161] ogbg-molbace: 0.906118 val loss: 0.706699
[Epoch 161] ogbg-molbace: 0.874802 test loss: 0.825729
[Epoch 162; Iter    29/   41] train: loss: 0.0013754
[Epoch 162] ogbg-molbace: 0.908228 val loss: 0.666730
[Epoch 162] ogbg-molbace: 0.880773 test loss: 0.812700
[Epoch 163; Iter    18/   41] train: loss: 0.0007480
[Epoch 163] ogbg-molbace: 0.908755 val loss: 0.652621
[Epoch 163] ogbg-molbace: 0.882353 test loss: 0.836861
[Epoch 164; Iter     7/   41] train: loss: 0.0030553
[Epoch 164; Iter    37/   41] train: loss: 0.0143904
[Epoch 164] ogbg-molbace: 0.906118 val loss: 0.670585
[Epoch 164] ogbg-molbace: 0.878139 test loss: 0.890978
[Epoch 165; Iter    26/   41] train: loss: 0.0406584
[Epoch 165] ogbg-molbace: 0.909810 val loss: 0.601250
[Epoch 165] ogbg-molbace: 0.878665 test loss: 0.811079
[Epoch 166; Iter    15/   41] train: loss: 0.0425609
[Epoch 166] ogbg-molbace: 0.904712 val loss: 0.608187
[Epoch 166] ogbg-molbace: 0.874100 test loss: 0.836550
[Epoch 167; Iter     4/   41] train: loss: 0.0273815
[Epoch 167; Iter    34/   41] train: loss: 0.0077615
[Epoch 167] ogbg-molbace: 0.902954 val loss: 0.615181
[Epoch 167] ogbg-molbace: 0.875680 test loss: 0.790960
[Epoch 168; Iter    23/   41] train: loss: 0.0111296
[Epoch 168] ogbg-molbace: 0.908579 val loss: 0.612959
[Epoch 168] ogbg-molbace: 0.884284 test loss: 0.789950[Epoch 137; Iter    14/   31] train: loss: 0.0042787
[Epoch 137] ogbg-molbace: 0.888844 val loss: 0.653877
[Epoch 137] ogbg-molbace: 0.863755 test loss: 0.772890
[Epoch 138; Iter    13/   31] train: loss: 0.0074096
[Epoch 138] ogbg-molbace: 0.884226 val loss: 0.692274
[Epoch 138] ogbg-molbace: 0.860178 test loss: 0.818658
[Epoch 139; Iter    12/   31] train: loss: 0.0232498
[Epoch 139] ogbg-molbace: 0.888844 val loss: 0.678132
[Epoch 139] ogbg-molbace: 0.861661 test loss: 0.815729
[Epoch 140; Iter    11/   31] train: loss: 0.0083683
[Epoch 140] ogbg-molbace: 0.888799 val loss: 0.685768
[Epoch 140] ogbg-molbace: 0.863930 test loss: 0.823118
[Epoch 141; Iter    10/   31] train: loss: 0.0038135
[Epoch 141] ogbg-molbace: 0.885930 val loss: 0.702225
[Epoch 141] ogbg-molbace: 0.858607 test loss: 0.835526
[Epoch 142; Iter     9/   31] train: loss: 0.0070918
[Epoch 142] ogbg-molbace: 0.887364 val loss: 0.702552
[Epoch 142] ogbg-molbace: 0.857211 test loss: 0.862841
[Epoch 143; Iter     8/   31] train: loss: 0.0031463
[Epoch 143] ogbg-molbace: 0.887858 val loss: 0.691973
[Epoch 143] ogbg-molbace: 0.855074 test loss: 0.861533
[Epoch 144; Iter     7/   31] train: loss: 0.0047267
[Epoch 144] ogbg-molbace: 0.888171 val loss: 0.701930
[Epoch 144] ogbg-molbace: 0.854419 test loss: 0.895222
[Epoch 145; Iter     6/   31] train: loss: 0.0053167
[Epoch 145] ogbg-molbace: 0.886647 val loss: 0.701778
[Epoch 145] ogbg-molbace: 0.856252 test loss: 0.866718
[Epoch 146; Iter     5/   31] train: loss: 0.0076053
[Epoch 146] ogbg-molbace: 0.886333 val loss: 0.715194
[Epoch 146] ogbg-molbace: 0.862708 test loss: 0.847364
[Epoch 147; Iter     4/   31] train: loss: 0.0025892
[Epoch 147] ogbg-molbace: 0.889472 val loss: 0.690886
[Epoch 147] ogbg-molbace: 0.861051 test loss: 0.851200
[Epoch 148; Iter     3/   31] train: loss: 0.0026957
[Epoch 148] ogbg-molbace: 0.884405 val loss: 0.755396
[Epoch 148] ogbg-molbace: 0.860483 test loss: 0.883316
[Epoch 149; Iter     2/   31] train: loss: 0.0175166
[Epoch 149] ogbg-molbace: 0.882880 val loss: 0.737924
[Epoch 149] ogbg-molbace: 0.856993 test loss: 0.867483
Early stopping criterion based on -ogbg-molbace- that should be max reached after 149 epochs. Best model checkpoint was in epoch 89.
Statistics on  val_best_checkpoint
mean_pred: -1.020689845085144
std_pred: 4.663344383239746
mean_targets: 0.41584160923957825
std_targets: 0.49368178844451904
prcauc: 0.8183502486294496
rocauc: 0.9044928705945656
ogbg-molbace: 0.9044928705945656
BCEWithLogitsLoss: 0.4419990718703378
Statistics on  test
mean_pred: -0.5639050006866455
std_pred: 4.871901035308838
mean_targets: 0.48184821009635925
std_targets: 0.5004969835281372
prcauc: 0.879533858067396
rocauc: 0.8862664688945118
ogbg-molbace: 0.8862664688945118
BCEWithLogitsLoss: 0.5314055826853622
Statistics on  train
mean_pred: -0.7602851390838623
std_pred: 4.878440856933594
mean_targets: 0.4619625210762024
std_targets: 0.49882611632347107
prcauc: 0.9981101424793017
rocauc: 0.9982736022536093
ogbg-molbace: 0.9982736022536093
BCEWithLogitsLoss: 0.07270337250684539
[Epoch 137; Iter    14/   31] train: loss: 0.0260955
[Epoch 137] ogbg-molbace: 0.896332 val loss: 0.597958
[Epoch 137] ogbg-molbace: 0.869645 test loss: 0.725955
[Epoch 138; Iter    13/   31] train: loss: 0.0357138
[Epoch 138] ogbg-molbace: 0.887813 val loss: 0.660219
[Epoch 138] ogbg-molbace: 0.867725 test loss: 0.757886
[Epoch 139; Iter    12/   31] train: loss: 0.0057195
[Epoch 139] ogbg-molbace: 0.894314 val loss: 0.637954
[Epoch 139] ogbg-molbace: 0.868249 test loss: 0.764287
[Epoch 140; Iter    11/   31] train: loss: 0.0161159
[Epoch 140] ogbg-molbace: 0.898305 val loss: 0.603336
[Epoch 140] ogbg-molbace: 0.872437 test loss: 0.735916
[Epoch 141; Iter    10/   31] train: loss: 0.0720826
[Epoch 141] ogbg-molbace: 0.892431 val loss: 0.625536
[Epoch 141] ogbg-molbace: 0.875665 test loss: 0.692720
[Epoch 142; Iter     9/   31] train: loss: 0.0317241
[Epoch 142] ogbg-molbace: 0.898215 val loss: 0.623362
[Epoch 142] ogbg-molbace: 0.871739 test loss: 0.730681
[Epoch 143; Iter     8/   31] train: loss: 0.0080656
[Epoch 143] ogbg-molbace: 0.889203 val loss: 0.658092
[Epoch 143] ogbg-molbace: 0.875753 test loss: 0.721532
[Epoch 144; Iter     7/   31] train: loss: 0.0085933
[Epoch 144] ogbg-molbace: 0.888127 val loss: 0.684718
[Epoch 144] ogbg-molbace: 0.868292 test loss: 0.765244
Early stopping criterion based on -ogbg-molbace- that should be max reached after 144 epochs. Best model checkpoint was in epoch 84.
Statistics on  val_best_checkpoint
mean_pred: 0.42078736424446106
std_pred: 3.3135221004486084
mean_targets: 0.41584160923957825
std_targets: 0.49368178844451904
prcauc: 0.8243435729529062
rocauc: 0.9022060801721818
ogbg-molbace: 0.9022060801721818
BCEWithLogitsLoss: 0.461380579254844
Statistics on  test
mean_pred: 0.9369718432426453
std_pred: 3.3342907428741455
mean_targets: 0.48184821009635925
std_targets: 0.5004969835281372
prcauc: 0.8458089927492928
rocauc: 0.8658494023209143
ogbg-molbace: 0.8658494023209143
BCEWithLogitsLoss: 0.5244908243079077
Statistics on  train
mean_pred: 0.6891823410987854
std_pred: 3.332838773727417
mean_targets: 0.4619625210762024
std_targets: 0.49882611632347107
prcauc: 0.9691077836385438
rocauc: 0.9745979889667045
ogbg-molbace: 0.9745979889667045
BCEWithLogitsLoss: 0.2765651482247537
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 169; Iter    12/   41] train: loss: 0.0218880
[Epoch 169] ogbg-molbace: 0.909986 val loss: 0.635597
[Epoch 169] ogbg-molbace: 0.880948 test loss: 0.800724
[Epoch 170; Iter     1/   41] train: loss: 0.0021905
[Epoch 170; Iter    31/   41] train: loss: 0.0023851
[Epoch 170] ogbg-molbace: 0.912096 val loss: 0.599428
[Epoch 170] ogbg-molbace: 0.882704 test loss: 0.803659
[Epoch 171; Iter    20/   41] train: loss: 0.0037882
[Epoch 171] ogbg-molbace: 0.913678 val loss: 0.615440
[Epoch 171] ogbg-molbace: 0.883055 test loss: 0.802311
Early stopping criterion based on -ogbg-molbace- that should be max reached after 171 epochs. Best model checkpoint was in epoch 111.
Statistics on  val_best_checkpoint
mean_pred: -0.3135790228843689
std_pred: 16.84177017211914
mean_targets: 0.5231788158416748
std_targets: 0.5011245608329773
prcauc: 0.9262216012215694
rocauc: 0.9286216596343179
ogbg-molbace: 0.9286216596343179
BCEWithLogitsLoss: 0.5538531392812729
Statistics on  test
mean_pred: -2.128170967102051
std_pred: 4.365926265716553
mean_targets: 0.44078949093818665
std_targets: 0.4981229901313782
prcauc: 0.8715092883775942
rocauc: 0.8981562774363476
ogbg-molbace: 0.8981562774363476
BCEWithLogitsLoss: 0.45031586786111194
Statistics on  train
mean_pred: -1.746570110321045
std_pred: 14.779891967773438
mean_targets: 0.45041322708129883
std_targets: 0.49774080514907837
prcauc: 0.996055017997509
rocauc: 0.9963468303786991
ogbg-molbace: 0.9963468303786991
BCEWithLogitsLoss: 0.14577911707867935
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 137; Iter    14/   31] train: loss: 0.0085998
[Epoch 137] ogbg-molbace: 0.889965 val loss: 0.637544
[Epoch 137] ogbg-molbace: 0.841419 test loss: 0.868767
[Epoch 138; Iter    13/   31] train: loss: 0.0075383
[Epoch 138] ogbg-molbace: 0.881939 val loss: 0.677358
[Epoch 138] ogbg-molbace: 0.843513 test loss: 0.863153
[Epoch 139; Iter    12/   31] train: loss: 0.0114543
[Epoch 139] ogbg-molbace: 0.884764 val loss: 0.666129
[Epoch 139] ogbg-molbace: 0.839325 test loss: 0.879051
[Epoch 140; Iter    11/   31] train: loss: 0.0208512
[Epoch 140] ogbg-molbace: 0.886064 val loss: 0.681314
[Epoch 140] ogbg-molbace: 0.841637 test loss: 0.891462
[Epoch 141; Iter    10/   31] train: loss: 0.0130025
[Epoch 141] ogbg-molbace: 0.885930 val loss: 0.654631
[Epoch 141] ogbg-molbace: 0.845869 test loss: 0.858827
[Epoch 142; Iter     9/   31] train: loss: 0.0132209
[Epoch 142] ogbg-molbace: 0.889248 val loss: 0.665604
[Epoch 142] ogbg-molbace: 0.840590 test loss: 0.893698
[Epoch 143; Iter     8/   31] train: loss: 0.0094487
[Epoch 143] ogbg-molbace: 0.887006 val loss: 0.680899
[Epoch 143] ogbg-molbace: 0.842204 test loss: 0.892475
[Epoch 144; Iter     7/   31] train: loss: 0.0191701
[Epoch 144] ogbg-molbace: 0.886154 val loss: 0.692303
[Epoch 144] ogbg-molbace: 0.834090 test loss: 0.942350
[Epoch 145; Iter     6/   31] train: loss: 0.0085281
[Epoch 145] ogbg-molbace: 0.884988 val loss: 0.710396
[Epoch 145] ogbg-molbace: 0.838147 test loss: 0.923960
[Epoch 146; Iter     5/   31] train: loss: 0.0535570
[Epoch 146] ogbg-molbace: 0.886423 val loss: 0.747290
[Epoch 146] ogbg-molbace: 0.844429 test loss: 0.929132
[Epoch 147; Iter     4/   31] train: loss: 0.0071130
[Epoch 147] ogbg-molbace: 0.886692 val loss: 0.708077
[Epoch 147] ogbg-molbace: 0.845432 test loss: 0.878445
[Epoch 148; Iter     3/   31] train: loss: 0.0131932
[Epoch 148] ogbg-molbace: 0.887858 val loss: 0.720611
[Epoch 148] ogbg-molbace: 0.842597 test loss: 0.916119
[Epoch 149; Iter     2/   31] train: loss: 0.0040883
[Epoch 149] ogbg-molbace: 0.887544 val loss: 0.738773
[Epoch 149] ogbg-molbace: 0.843469 test loss: 0.917227
[Epoch 150; Iter     1/   31] train: loss: 0.0040208
[Epoch 150; Iter    31/   31] train: loss: 0.0188018
[Epoch 150] ogbg-molbace: 0.885930 val loss: 0.728114
[Epoch 150] ogbg-molbace: 0.839848 test loss: 0.936290
[Epoch 151; Iter    30/   31] train: loss: 0.0186857
[Epoch 151] ogbg-molbace: 0.888306 val loss: 0.738805
[Epoch 151] ogbg-molbace: 0.843426 test loss: 0.922722
[Epoch 152; Iter    29/   31] train: loss: 0.0146948
[Epoch 152] ogbg-molbace: 0.887633 val loss: 0.716836
[Epoch 152] ogbg-molbace: 0.842858 test loss: 0.906963
[Epoch 153; Iter    28/   31] train: loss: 0.0072615
[Epoch 153] ogbg-molbace: 0.886378 val loss: 0.764803
[Epoch 153] ogbg-molbace: 0.843295 test loss: 0.924777
[Epoch 154; Iter    27/   31] train: loss: 0.0075874
[Epoch 154] ogbg-molbace: 0.881715 val loss: 0.803216
[Epoch 154] ogbg-molbace: 0.841768 test loss: 0.968104
[Epoch 155; Iter    26/   31] train: loss: 0.0024971
[Epoch 155] ogbg-molbace: 0.880818 val loss: 0.821776
[Epoch 155] ogbg-molbace: 0.849141 test loss: 0.921268
[Epoch 156; Iter    25/   31] train: loss: 0.0049344
[Epoch 156] ogbg-molbace: 0.887499 val loss: 0.792227
[Epoch 156] ogbg-molbace: 0.853241 test loss: 0.922175
[Epoch 157; Iter    24/   31] train: loss: 0.0208467
[Epoch 157] ogbg-molbace: 0.879024 val loss: 0.753910
[Epoch 157] ogbg-molbace: 0.843644 test loss: 0.909478
[Epoch 158; Iter    23/   31] train: loss: 0.0219671
[Epoch 158] ogbg-molbace: 0.883419 val loss: 0.859627
[Epoch 158] ogbg-molbace: 0.855946 test loss: 0.945600
[Epoch 159; Iter    22/   31] train: loss: 0.0088909
[Epoch 159] ogbg-molbace: 0.885257 val loss: 0.801048
[Epoch 159] ogbg-molbace: 0.848486 test loss: 0.947148
[Epoch 160; Iter    21/   31] train: loss: 0.0026528
[Epoch 160] ogbg-molbace: 0.883463 val loss: 0.872599
[Epoch 160] ogbg-molbace: 0.840895 test loss: 1.024413
[Epoch 161; Iter    20/   31] train: loss: 0.0042575
[Epoch 161] ogbg-molbace: 0.880952 val loss: 0.779331
[Epoch 161] ogbg-molbace: 0.848094 test loss: 0.909972
[Epoch 162; Iter    19/   31] train: loss: 0.0030077
[Epoch 162] ogbg-molbace: 0.881939 val loss: 0.761008
[Epoch 162] ogbg-molbace: 0.845083 test loss: 0.917776
[Epoch 163; Iter    18/   31] train: loss: 0.0083083
[Epoch 163] ogbg-molbace: 0.883822 val loss: 0.794907
[Epoch 163] ogbg-molbace: 0.847614 test loss: 0.919549
[Epoch 164; Iter    17/   31] train: loss: 0.0034549
[Epoch 164] ogbg-molbace: 0.882701 val loss: 0.771940
[Epoch 164] ogbg-molbace: 0.847047 test loss: 0.915278
Early stopping criterion based on -ogbg-molbace- that should be max reached after 164 epochs. Best model checkpoint was in epoch 104.
Statistics on  val_best_checkpoint
mean_pred: -0.8423956632614136
std_pred: 4.825851917266846
mean_targets: 0.41584160923957825
std_targets: 0.49368178844451904
prcauc: 0.8154876520512764
rocauc: 0.8997847726661286
ogbg-molbace: 0.8997847726661286
BCEWithLogitsLoss: 0.4708857329731638
Statistics on  test
mean_pred: -0.5111314654350281
std_pred: 4.869548797607422
mean_targets: 0.48184821009635925
std_targets: 0.5004969835281372
prcauc: 0.8618110027440196
rocauc: 0.8668528051653434
ogbg-molbace: 0.8668528051653434
BCEWithLogitsLoss: 0.5875685811042786
Statistics on  train
mean_pred: -0.679704487323761
std_pred: 4.967930793762207
mean_targets: 0.4619625210762024
std_targets: 0.49882611632347107
prcauc: 0.9999492317510302
rocauc: 0.9999559841934348
ogbg-molbace: 0.9999559841934348
BCEWithLogitsLoss: 0.04506260481092238
All runs completed.
