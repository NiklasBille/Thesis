>>> Starting run for dataset: bace
Running configs_static_noise_experiments/3DInfomax/bace/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/3DInfomax/bace/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/3DInfomax/bace/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/3DInfomax/bace/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.05/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.05_4_26-05_09-18-50
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.05
logdir: runs/static_noise/3DInfomax/bace/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6942971
[Epoch 1] ogbg-molbace: 0.419780 val loss: 0.693613
[Epoch 1] ogbg-molbace: 0.312294 test loss: 0.693557
[Epoch 2; Iter    19/   41] train: loss: 0.6902157
[Epoch 2] ogbg-molbace: 0.420879 val loss: 0.695781
[Epoch 2] ogbg-molbace: 0.366023 test loss: 0.694922
[Epoch 3; Iter     8/   41] train: loss: 0.6957601
[Epoch 3; Iter    38/   41] train: loss: 0.6938777
[Epoch 3] ogbg-molbace: 0.420513 val loss: 0.696753
[Epoch 3] ogbg-molbace: 0.383585 test loss: 0.695375
[Epoch 4; Iter    27/   41] train: loss: 0.6927166
[Epoch 4] ogbg-molbace: 0.421978 val loss: 0.696889
[Epoch 4] ogbg-molbace: 0.396279 test loss: 0.695374
[Epoch 5; Iter    16/   41] train: loss: 0.6944122
[Epoch 5] ogbg-molbace: 0.421612 val loss: 0.696974
[Epoch 5] ogbg-molbace: 0.377847 test loss: 0.695824
[Epoch 6; Iter     5/   41] train: loss: 0.6946285
[Epoch 6; Iter    35/   41] train: loss: 0.6925685
[Epoch 6] ogbg-molbace: 0.413919 val loss: 0.697579
[Epoch 6] ogbg-molbace: 0.382368 test loss: 0.696072
[Epoch 7; Iter    24/   41] train: loss: 0.6967630
[Epoch 7] ogbg-molbace: 0.426374 val loss: 0.697595
[Epoch 7] ogbg-molbace: 0.403756 test loss: 0.695714
[Epoch 8; Iter    13/   41] train: loss: 0.6934068
[Epoch 8] ogbg-molbace: 0.432967 val loss: 0.698735
[Epoch 8] ogbg-molbace: 0.417493 test loss: 0.696166
[Epoch 9; Iter     2/   41] train: loss: 0.6933051
[Epoch 9; Iter    32/   41] train: loss: 0.6907860
[Epoch 9] ogbg-molbace: 0.419048 val loss: 0.699604
[Epoch 9] ogbg-molbace: 0.390019 test loss: 0.696776
[Epoch 10; Iter    21/   41] train: loss: 0.6903396
[Epoch 10] ogbg-molbace: 0.432601 val loss: 0.699670
[Epoch 10] ogbg-molbace: 0.397322 test loss: 0.696998
[Epoch 11; Iter    10/   41] train: loss: 0.6961371
[Epoch 11; Iter    40/   41] train: loss: 0.6913064
[Epoch 11] ogbg-molbace: 0.428938 val loss: 0.700311
[Epoch 11] ogbg-molbace: 0.412972 test loss: 0.697109
[Epoch 12; Iter    29/   41] train: loss: 0.6939765
[Epoch 12] ogbg-molbace: 0.426374 val loss: 0.701379
[Epoch 12] ogbg-molbace: 0.418710 test loss: 0.697734
[Epoch 13; Iter    18/   41] train: loss: 0.6944993
[Epoch 13] ogbg-molbace: 0.427473 val loss: 0.702000
[Epoch 13] ogbg-molbace: 0.416275 test loss: 0.698221
[Epoch 14; Iter     7/   41] train: loss: 0.6925159
[Epoch 14; Iter    37/   41] train: loss: 0.6880135
[Epoch 14] ogbg-molbace: 0.427473 val loss: 0.702968
[Epoch 14] ogbg-molbace: 0.425839 test loss: 0.698590
[Epoch 15; Iter    26/   41] train: loss: 0.6906257
[Epoch 15] ogbg-molbace: 0.430037 val loss: 0.703932
[Epoch 15] ogbg-molbace: 0.419405 test loss: 0.699101
[Epoch 16; Iter    15/   41] train: loss: 0.6922034
[Epoch 16] ogbg-molbace: 0.421612 val loss: 0.705309
[Epoch 16] ogbg-molbace: 0.424100 test loss: 0.699935
[Epoch 17; Iter     4/   41] train: loss: 0.6927453
[Epoch 17; Iter    34/   41] train: loss: 0.6909020
[Epoch 17] ogbg-molbace: 0.532234 val loss: 0.696856
[Epoch 17] ogbg-molbace: 0.537820 test loss: 0.697745
[Epoch 18; Iter    23/   41] train: loss: 0.6507540
[Epoch 18] ogbg-molbace: 0.641392 val loss: 0.683009
[Epoch 18] ogbg-molbace: 0.657277 test loss: 0.706432
[Epoch 19; Iter    12/   41] train: loss: 0.6441242
[Epoch 19] ogbg-molbace: 0.703663 val loss: 0.655054
[Epoch 19] ogbg-molbace: 0.749609 test loss: 0.687316
[Epoch 20; Iter     1/   41] train: loss: 0.6226562
[Epoch 20; Iter    31/   41] train: loss: 0.5664334
[Epoch 20] ogbg-molbace: 0.695238 val loss: 0.647611
[Epoch 20] ogbg-molbace: 0.770822 test loss: 0.664918
[Epoch 21; Iter    20/   41] train: loss: 0.5430153
[Epoch 21] ogbg-molbace: 0.646886 val loss: 0.741543
[Epoch 21] ogbg-molbace: 0.733090 test loss: 0.810413
[Epoch 22; Iter     9/   41] train: loss: 0.6304811
[Epoch 22; Iter    39/   41] train: loss: 0.4860988
[Epoch 22] ogbg-molbace: 0.697802 val loss: 0.824305
[Epoch 22] ogbg-molbace: 0.728569 test loss: 1.007100
[Epoch 23; Iter    28/   41] train: loss: 0.5901548
[Epoch 23] ogbg-molbace: 0.702930 val loss: 0.580095
[Epoch 23] ogbg-molbace: 0.777430 test loss: 0.746076
[Epoch 24; Iter    17/   41] train: loss: 0.6103329
[Epoch 24] ogbg-molbace: 0.749817 val loss: 0.841853
[Epoch 24] ogbg-molbace: 0.774126 test loss: 0.953476
[Epoch 25; Iter     6/   41] train: loss: 0.5047868
[Epoch 25; Iter    36/   41] train: loss: 0.4669243
[Epoch 25] ogbg-molbace: 0.705861 val loss: 1.062372
[Epoch 25] ogbg-molbace: 0.721788 test loss: 1.478423
[Epoch 26; Iter    25/   41] train: loss: 0.5767283
[Epoch 26] ogbg-molbace: 0.702198 val loss: 0.726734
[Epoch 26] ogbg-molbace: 0.757781 test loss: 0.963792
[Epoch 27; Iter    14/   41] train: loss: 0.4315653
[Epoch 27] ogbg-molbace: 0.711722 val loss: 0.834656
[Epoch 27] ogbg-molbace: 0.787515 test loss: 1.015534
[Epoch 28; Iter     3/   41] train: loss: 0.5555770
[Epoch 28; Iter    33/   41] train: loss: 0.5110912
[Epoch 28] ogbg-molbace: 0.683516 val loss: 0.881765
[Epoch 28] ogbg-molbace: 0.750652 test loss: 1.151300
[Epoch 29; Iter    22/   41] train: loss: 0.6360298
[Epoch 29] ogbg-molbace: 0.709890 val loss: 0.860061
[Epoch 29] ogbg-molbace: 0.754825 test loss: 1.079986
[Epoch 30; Iter    11/   41] train: loss: 0.5003906
[Epoch 30; Iter    41/   41] train: loss: 0.4600663
[Epoch 30] ogbg-molbace: 0.695971 val loss: 0.965031
[Epoch 30] ogbg-molbace: 0.774126 test loss: 1.167578
[Epoch 31; Iter    30/   41] train: loss: 0.6544697
[Epoch 31] ogbg-molbace: 0.643590 val loss: 0.740585
[Epoch 31] ogbg-molbace: 0.739524 test loss: 0.935568
[Epoch 32; Iter    19/   41] train: loss: 0.5435117
[Epoch 32] ogbg-molbace: 0.726007 val loss: 0.816060
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.1/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.1_5_26-05_09-18-53
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.1
logdir: runs/static_noise/3DInfomax/bace/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6946919
[Epoch 1] ogbg-molbace: 0.493407 val loss: 0.692852
[Epoch 1] ogbg-molbace: 0.478873 test loss: 0.692326
[Epoch 2; Iter    19/   41] train: loss: 0.6942068
[Epoch 2] ogbg-molbace: 0.512088 val loss: 0.691169
[Epoch 2] ogbg-molbace: 0.521475 test loss: 0.688669
[Epoch 3; Iter     8/   41] train: loss: 0.6950639
[Epoch 3; Iter    38/   41] train: loss: 0.6968139
[Epoch 3] ogbg-molbace: 0.504029 val loss: 0.692014
[Epoch 3] ogbg-molbace: 0.508259 test loss: 0.689376
[Epoch 4; Iter    27/   41] train: loss: 0.6903830
[Epoch 4] ogbg-molbace: 0.518681 val loss: 0.691194
[Epoch 4] ogbg-molbace: 0.530516 test loss: 0.688426
[Epoch 5; Iter    16/   41] train: loss: 0.6963301
[Epoch 5] ogbg-molbace: 0.498901 val loss: 0.692039
[Epoch 5] ogbg-molbace: 0.513824 test loss: 0.689912
[Epoch 6; Iter     5/   41] train: loss: 0.6937301
[Epoch 6; Iter    35/   41] train: loss: 0.6981205
[Epoch 6] ogbg-molbace: 0.515018 val loss: 0.692098
[Epoch 6] ogbg-molbace: 0.530516 test loss: 0.689211
[Epoch 7; Iter    24/   41] train: loss: 0.6948491
[Epoch 7] ogbg-molbace: 0.498901 val loss: 0.693504
[Epoch 7] ogbg-molbace: 0.512607 test loss: 0.690537
[Epoch 8; Iter    13/   41] train: loss: 0.6934399
[Epoch 8] ogbg-molbace: 0.503297 val loss: 0.693685
[Epoch 8] ogbg-molbace: 0.516258 test loss: 0.690255
[Epoch 9; Iter     2/   41] train: loss: 0.6913994
[Epoch 9; Iter    32/   41] train: loss: 0.6911469
[Epoch 9] ogbg-molbace: 0.493407 val loss: 0.694408
[Epoch 9] ogbg-molbace: 0.522344 test loss: 0.690474
[Epoch 10; Iter    21/   41] train: loss: 0.6915015
[Epoch 10] ogbg-molbace: 0.496337 val loss: 0.694297
[Epoch 10] ogbg-molbace: 0.532951 test loss: 0.690337
[Epoch 11; Iter    10/   41] train: loss: 0.6941409
[Epoch 11; Iter    40/   41] train: loss: 0.6909758
[Epoch 11] ogbg-molbace: 0.502198 val loss: 0.695513
[Epoch 11] ogbg-molbace: 0.529473 test loss: 0.691204
[Epoch 12; Iter    29/   41] train: loss: 0.6906767
[Epoch 12] ogbg-molbace: 0.493407 val loss: 0.696853
[Epoch 12] ogbg-molbace: 0.523909 test loss: 0.692005
[Epoch 13; Iter    18/   41] train: loss: 0.6904427
[Epoch 13] ogbg-molbace: 0.502564 val loss: 0.696814
[Epoch 13] ogbg-molbace: 0.543905 test loss: 0.691188
[Epoch 14; Iter     7/   41] train: loss: 0.6914837
[Epoch 14; Iter    37/   41] train: loss: 0.6903429
[Epoch 14] ogbg-molbace: 0.497802 val loss: 0.698516
[Epoch 14] ogbg-molbace: 0.532951 test loss: 0.692787
[Epoch 15; Iter    26/   41] train: loss: 0.6974533
[Epoch 15] ogbg-molbace: 0.500366 val loss: 0.698709
[Epoch 15] ogbg-molbace: 0.550861 test loss: 0.692707
[Epoch 16; Iter    15/   41] train: loss: 0.6858814
[Epoch 16] ogbg-molbace: 0.499634 val loss: 0.699507
[Epoch 16] ogbg-molbace: 0.559903 test loss: 0.692618
[Epoch 17; Iter     4/   41] train: loss: 0.6931339
[Epoch 17; Iter    34/   41] train: loss: 0.6914412
[Epoch 17] ogbg-molbace: 0.506593 val loss: 0.700777
[Epoch 17] ogbg-molbace: 0.616241 test loss: 0.695335
[Epoch 18; Iter    23/   41] train: loss: 0.6800822
[Epoch 18] ogbg-molbace: 0.649817 val loss: 0.672529
[Epoch 18] ogbg-molbace: 0.675535 test loss: 0.693374
[Epoch 19; Iter    12/   41] train: loss: 0.6636989
[Epoch 19] ogbg-molbace: 0.657143 val loss: 0.658750
[Epoch 19] ogbg-molbace: 0.752391 test loss: 0.686398
[Epoch 20; Iter     1/   41] train: loss: 0.6081278
[Epoch 20; Iter    31/   41] train: loss: 0.6099914
[Epoch 20] ogbg-molbace: 0.552747 val loss: 0.728329
[Epoch 20] ogbg-molbace: 0.625630 test loss: 0.808762
[Epoch 21; Iter    20/   41] train: loss: 0.6018044
[Epoch 21] ogbg-molbace: 0.638462 val loss: 0.653715
[Epoch 21] ogbg-molbace: 0.730134 test loss: 0.741701
[Epoch 22; Iter     9/   41] train: loss: 0.5002581
[Epoch 22; Iter    39/   41] train: loss: 0.5770323
[Epoch 22] ogbg-molbace: 0.616117 val loss: 0.761554
[Epoch 22] ogbg-molbace: 0.704921 test loss: 0.831081
[Epoch 23; Iter    28/   41] train: loss: 0.6180428
[Epoch 23] ogbg-molbace: 0.678755 val loss: 0.648797
[Epoch 23] ogbg-molbace: 0.727178 test loss: 0.759716
[Epoch 24; Iter    17/   41] train: loss: 0.5197551
[Epoch 24] ogbg-molbace: 0.729670 val loss: 0.819336
[Epoch 24] ogbg-molbace: 0.730656 test loss: 0.891146
[Epoch 25; Iter     6/   41] train: loss: 0.6031528
[Epoch 25; Iter    36/   41] train: loss: 0.5826613
[Epoch 25] ogbg-molbace: 0.698168 val loss: 0.814030
[Epoch 25] ogbg-molbace: 0.698487 test loss: 0.909982
[Epoch 26; Iter    25/   41] train: loss: 0.5554897
[Epoch 26] ogbg-molbace: 0.621612 val loss: 0.685532
[Epoch 26] ogbg-molbace: 0.743523 test loss: 0.684493
[Epoch 27; Iter    14/   41] train: loss: 0.5682744
[Epoch 27] ogbg-molbace: 0.716117 val loss: 0.724620
[Epoch 27] ogbg-molbace: 0.735350 test loss: 0.796870
[Epoch 28; Iter     3/   41] train: loss: 0.4950890
[Epoch 28; Iter    33/   41] train: loss: 0.5500802
[Epoch 28] ogbg-molbace: 0.625275 val loss: 1.004275
[Epoch 28] ogbg-molbace: 0.683012 test loss: 1.130709
[Epoch 29; Iter    22/   41] train: loss: 0.5079493
[Epoch 29] ogbg-molbace: 0.709524 val loss: 0.946679
[Epoch 29] ogbg-molbace: 0.722657 test loss: 1.069215
[Epoch 30; Iter    11/   41] train: loss: 0.6680822
[Epoch 30; Iter    41/   41] train: loss: 0.5490251
[Epoch 30] ogbg-molbace: 0.726740 val loss: 0.820328
[Epoch 30] ogbg-molbace: 0.749609 test loss: 0.879862
[Epoch 31; Iter    30/   41] train: loss: 0.6262180
[Epoch 31] ogbg-molbace: 0.684615 val loss: 0.737145
[Epoch 31] ogbg-molbace: 0.707529 test loss: 0.821993
[Epoch 32; Iter    19/   41] train: loss: 0.7872985
[Epoch 32] ogbg-molbace: 0.669597 val loss: 0.853728
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.05/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.05_5_26-05_09-18-51
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.05
logdir: runs/static_noise/3DInfomax/bace/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6958757
[Epoch 1] ogbg-molbace: 0.465568 val loss: 0.693307
[Epoch 1] ogbg-molbace: 0.439923 test loss: 0.692699
[Epoch 2; Iter    19/   41] train: loss: 0.6929083
[Epoch 2] ogbg-molbace: 0.486081 val loss: 0.693630
[Epoch 2] ogbg-molbace: 0.476439 test loss: 0.690514
[Epoch 3; Iter     8/   41] train: loss: 0.6967147
[Epoch 3; Iter    38/   41] train: loss: 0.6958907
[Epoch 3] ogbg-molbace: 0.484615 val loss: 0.694063
[Epoch 3] ogbg-molbace: 0.472787 test loss: 0.690603
[Epoch 4; Iter    27/   41] train: loss: 0.6929680
[Epoch 4] ogbg-molbace: 0.482784 val loss: 0.694557
[Epoch 4] ogbg-molbace: 0.471918 test loss: 0.690573
[Epoch 5; Iter    16/   41] train: loss: 0.6949419
[Epoch 5] ogbg-molbace: 0.482051 val loss: 0.694595
[Epoch 5] ogbg-molbace: 0.470875 test loss: 0.691228
[Epoch 6; Iter     5/   41] train: loss: 0.6929018
[Epoch 6; Iter    35/   41] train: loss: 0.6973126
[Epoch 6] ogbg-molbace: 0.491575 val loss: 0.695169
[Epoch 6] ogbg-molbace: 0.476613 test loss: 0.691082
[Epoch 7; Iter    24/   41] train: loss: 0.6966354
[Epoch 7] ogbg-molbace: 0.487912 val loss: 0.696031
[Epoch 7] ogbg-molbace: 0.466354 test loss: 0.692128
[Epoch 8; Iter    13/   41] train: loss: 0.6942202
[Epoch 8] ogbg-molbace: 0.481319 val loss: 0.696315
[Epoch 8] ogbg-molbace: 0.467223 test loss: 0.692063
[Epoch 9; Iter     2/   41] train: loss: 0.6920190
[Epoch 9; Iter    32/   41] train: loss: 0.6934794
[Epoch 9] ogbg-molbace: 0.464469 val loss: 0.696927
[Epoch 9] ogbg-molbace: 0.465658 test loss: 0.692345
[Epoch 10; Iter    21/   41] train: loss: 0.6923874
[Epoch 10] ogbg-molbace: 0.474725 val loss: 0.697294
[Epoch 10] ogbg-molbace: 0.470527 test loss: 0.692483
[Epoch 11; Iter    10/   41] train: loss: 0.6910861
[Epoch 11; Iter    40/   41] train: loss: 0.6925934
[Epoch 11] ogbg-molbace: 0.494139 val loss: 0.697709
[Epoch 11] ogbg-molbace: 0.481655 test loss: 0.692663
[Epoch 12; Iter    29/   41] train: loss: 0.6908816
[Epoch 12] ogbg-molbace: 0.471062 val loss: 0.699100
[Epoch 12] ogbg-molbace: 0.475569 test loss: 0.693462
[Epoch 13; Iter    18/   41] train: loss: 0.6935399
[Epoch 13] ogbg-molbace: 0.475824 val loss: 0.699764
[Epoch 13] ogbg-molbace: 0.484437 test loss: 0.693570
[Epoch 14; Iter     7/   41] train: loss: 0.6917704
[Epoch 14; Iter    37/   41] train: loss: 0.6892874
[Epoch 14] ogbg-molbace: 0.475092 val loss: 0.700586
[Epoch 14] ogbg-molbace: 0.486698 test loss: 0.693816
[Epoch 15; Iter    26/   41] train: loss: 0.6945617
[Epoch 15] ogbg-molbace: 0.483883 val loss: 0.701540
[Epoch 15] ogbg-molbace: 0.489480 test loss: 0.694895
[Epoch 16; Iter    15/   41] train: loss: 0.6831544
[Epoch 16] ogbg-molbace: 0.484249 val loss: 0.702234
[Epoch 16] ogbg-molbace: 0.500261 test loss: 0.694760
[Epoch 17; Iter     4/   41] train: loss: 0.6931726
[Epoch 17; Iter    34/   41] train: loss: 0.6914924
[Epoch 17] ogbg-molbace: 0.518681 val loss: 0.699356
[Epoch 17] ogbg-molbace: 0.544601 test loss: 0.698848
[Epoch 18; Iter    23/   41] train: loss: 0.6736538
[Epoch 18] ogbg-molbace: 0.582051 val loss: 0.689917
[Epoch 18] ogbg-molbace: 0.639019 test loss: 0.709887
[Epoch 19; Iter    12/   41] train: loss: 0.6341566
[Epoch 19] ogbg-molbace: 0.609890 val loss: 0.701242
[Epoch 19] ogbg-molbace: 0.698487 test loss: 0.731079
[Epoch 20; Iter     1/   41] train: loss: 0.6040026
[Epoch 20; Iter    31/   41] train: loss: 0.5624804
[Epoch 20] ogbg-molbace: 0.656410 val loss: 0.703518
[Epoch 20] ogbg-molbace: 0.722483 test loss: 0.780893
[Epoch 21; Iter    20/   41] train: loss: 0.6135421
[Epoch 21] ogbg-molbace: 0.679487 val loss: 0.640309
[Epoch 21] ogbg-molbace: 0.736046 test loss: 0.775021
[Epoch 22; Iter     9/   41] train: loss: 0.4794092
[Epoch 22; Iter    39/   41] train: loss: 0.5639310
[Epoch 22] ogbg-molbace: 0.691209 val loss: 0.701862
[Epoch 22] ogbg-molbace: 0.749957 test loss: 0.827578
[Epoch 23; Iter    28/   41] train: loss: 0.5996041
[Epoch 23] ogbg-molbace: 0.652381 val loss: 0.780043
[Epoch 23] ogbg-molbace: 0.682316 test loss: 1.047179
[Epoch 24; Iter    17/   41] train: loss: 0.5128339
[Epoch 24] ogbg-molbace: 0.756777 val loss: 0.675229
[Epoch 24] ogbg-molbace: 0.746827 test loss: 0.808077
[Epoch 25; Iter     6/   41] train: loss: 0.5197359
[Epoch 25; Iter    36/   41] train: loss: 0.5561329
[Epoch 25] ogbg-molbace: 0.736996 val loss: 0.762866
[Epoch 25] ogbg-molbace: 0.764389 test loss: 0.899233
[Epoch 26; Iter    25/   41] train: loss: 0.4912147
[Epoch 26] ogbg-molbace: 0.721612 val loss: 0.642531
[Epoch 26] ogbg-molbace: 0.765432 test loss: 0.862782
[Epoch 27; Iter    14/   41] train: loss: 0.5094663
[Epoch 27] ogbg-molbace: 0.740293 val loss: 0.750972
[Epoch 27] ogbg-molbace: 0.752217 test loss: 0.897589
[Epoch 28; Iter     3/   41] train: loss: 0.4603495
[Epoch 28; Iter    33/   41] train: loss: 0.4949163
[Epoch 28] ogbg-molbace: 0.649084 val loss: 0.886331
[Epoch 28] ogbg-molbace: 0.712572 test loss: 1.065468
[Epoch 29; Iter    22/   41] train: loss: 0.5547916
[Epoch 29] ogbg-molbace: 0.675458 val loss: 0.922256
[Epoch 29] ogbg-molbace: 0.717440 test loss: 1.178133
[Epoch 30; Iter    11/   41] train: loss: 0.6075205
[Epoch 30; Iter    41/   41] train: loss: 0.4123231
[Epoch 30] ogbg-molbace: 0.697802 val loss: 0.769264
[Epoch 30] ogbg-molbace: 0.768214 test loss: 0.834754
[Epoch 31; Iter    30/   41] train: loss: 0.6126691
[Epoch 31] ogbg-molbace: 0.707326 val loss: 0.831291
[Epoch 31] ogbg-molbace: 0.769084 test loss: 0.979617
[Epoch 32; Iter    19/   41] train: loss: 0.6902813
[Epoch 32] ogbg-molbace: 0.714652 val loss: 0.720056
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.2/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.2_4_26-05_09-18-54
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.2
logdir: runs/static_noise/3DInfomax/bace/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6938620
[Epoch 1] ogbg-molbace: 0.444322 val loss: 0.694329
[Epoch 1] ogbg-molbace: 0.239611 test loss: 0.694319
[Epoch 2; Iter    19/   41] train: loss: 0.6934450
[Epoch 2] ogbg-molbace: 0.439927 val loss: 0.697832
[Epoch 2] ogbg-molbace: 0.258216 test loss: 0.697389
[Epoch 3; Iter     8/   41] train: loss: 0.6957133
[Epoch 3; Iter    38/   41] train: loss: 0.6935302
[Epoch 3] ogbg-molbace: 0.441026 val loss: 0.699246
[Epoch 3] ogbg-molbace: 0.294731 test loss: 0.697564
[Epoch 4; Iter    27/   41] train: loss: 0.6963245
[Epoch 4] ogbg-molbace: 0.438828 val loss: 0.699735
[Epoch 4] ogbg-molbace: 0.301339 test loss: 0.697633
[Epoch 5; Iter    16/   41] train: loss: 0.6942317
[Epoch 5] ogbg-molbace: 0.437363 val loss: 0.699941
[Epoch 5] ogbg-molbace: 0.292819 test loss: 0.698249
[Epoch 6; Iter     5/   41] train: loss: 0.6951477
[Epoch 6; Iter    35/   41] train: loss: 0.6891316
[Epoch 6] ogbg-molbace: 0.449817 val loss: 0.700767
[Epoch 6] ogbg-molbace: 0.298209 test loss: 0.698064
[Epoch 7; Iter    24/   41] train: loss: 0.6949964
[Epoch 7] ogbg-molbace: 0.433333 val loss: 0.700916
[Epoch 7] ogbg-molbace: 0.286733 test loss: 0.698612
[Epoch 8; Iter    13/   41] train: loss: 0.6933969
[Epoch 8] ogbg-molbace: 0.455311 val loss: 0.701626
[Epoch 8] ogbg-molbace: 0.306729 test loss: 0.698322
[Epoch 9; Iter     2/   41] train: loss: 0.6948081
[Epoch 9; Iter    32/   41] train: loss: 0.6941395
[Epoch 9] ogbg-molbace: 0.427473 val loss: 0.702356
[Epoch 9] ogbg-molbace: 0.309164 test loss: 0.699233
[Epoch 10; Iter    21/   41] train: loss: 0.6926607
[Epoch 10] ogbg-molbace: 0.431868 val loss: 0.702341
[Epoch 10] ogbg-molbace: 0.307077 test loss: 0.698998
[Epoch 11; Iter    10/   41] train: loss: 0.6945636
[Epoch 11; Iter    40/   41] train: loss: 0.6906066
[Epoch 11] ogbg-molbace: 0.446520 val loss: 0.703045
[Epoch 11] ogbg-molbace: 0.324291 test loss: 0.699276
[Epoch 12; Iter    29/   41] train: loss: 0.6962608
[Epoch 12] ogbg-molbace: 0.439194 val loss: 0.704627
[Epoch 12] ogbg-molbace: 0.309685 test loss: 0.700529
[Epoch 13; Iter    18/   41] train: loss: 0.6907890
[Epoch 13] ogbg-molbace: 0.445055 val loss: 0.704417
[Epoch 13] ogbg-molbace: 0.326378 test loss: 0.700087
[Epoch 14; Iter     7/   41] train: loss: 0.6921332
[Epoch 14; Iter    37/   41] train: loss: 0.6888885
[Epoch 14] ogbg-molbace: 0.436264 val loss: 0.706215
[Epoch 14] ogbg-molbace: 0.319075 test loss: 0.701414
[Epoch 15; Iter    26/   41] train: loss: 0.6894853
[Epoch 15] ogbg-molbace: 0.440293 val loss: 0.707813
[Epoch 15] ogbg-molbace: 0.318727 test loss: 0.702268
[Epoch 16; Iter    15/   41] train: loss: 0.6941522
[Epoch 16] ogbg-molbace: 0.433333 val loss: 0.708262
[Epoch 16] ogbg-molbace: 0.335420 test loss: 0.702247
[Epoch 17; Iter     4/   41] train: loss: 0.6941150
[Epoch 17; Iter    34/   41] train: loss: 0.6872538
[Epoch 17] ogbg-molbace: 0.463004 val loss: 0.712570
[Epoch 17] ogbg-molbace: 0.396105 test loss: 0.707760
[Epoch 18; Iter    23/   41] train: loss: 0.6604989
[Epoch 18] ogbg-molbace: 0.599267 val loss: 0.723616
[Epoch 18] ogbg-molbace: 0.601461 test loss: 0.728303
[Epoch 19; Iter    12/   41] train: loss: 0.6341969
[Epoch 19] ogbg-molbace: 0.623810 val loss: 0.814689
[Epoch 19] ogbg-molbace: 0.624761 test loss: 0.794550
[Epoch 20; Iter     1/   41] train: loss: 0.6367910
[Epoch 20; Iter    31/   41] train: loss: 0.5519987
[Epoch 20] ogbg-molbace: 0.685714 val loss: 0.633003
[Epoch 20] ogbg-molbace: 0.759694 test loss: 0.672210
[Epoch 21; Iter    20/   41] train: loss: 0.5677983
[Epoch 21] ogbg-molbace: 0.667033 val loss: 0.731893
[Epoch 21] ogbg-molbace: 0.723179 test loss: 0.785014
[Epoch 22; Iter     9/   41] train: loss: 0.6648726
[Epoch 22; Iter    39/   41] train: loss: 0.4865453
[Epoch 22] ogbg-molbace: 0.505128 val loss: 1.789243
[Epoch 22] ogbg-molbace: 0.553121 test loss: 1.641034
[Epoch 23; Iter    28/   41] train: loss: 0.6116455
[Epoch 23] ogbg-molbace: 0.615751 val loss: 1.002797
[Epoch 23] ogbg-molbace: 0.678317 test loss: 1.026662
[Epoch 24; Iter    17/   41] train: loss: 0.6830478
[Epoch 24] ogbg-molbace: 0.593407 val loss: 1.309151
[Epoch 24] ogbg-molbace: 0.650148 test loss: 1.256014
[Epoch 25; Iter     6/   41] train: loss: 0.5422379
[Epoch 25; Iter    36/   41] train: loss: 0.5611628
[Epoch 25] ogbg-molbace: 0.718315 val loss: 0.830347
[Epoch 25] ogbg-molbace: 0.772387 test loss: 0.842943
[Epoch 26; Iter    25/   41] train: loss: 0.5571685
[Epoch 26] ogbg-molbace: 0.708059 val loss: 0.934108
[Epoch 26] ogbg-molbace: 0.748565 test loss: 0.978515
[Epoch 27; Iter    14/   41] train: loss: 0.5654231
[Epoch 27] ogbg-molbace: 0.708425 val loss: 0.922321
[Epoch 27] ogbg-molbace: 0.751000 test loss: 0.972471
[Epoch 28; Iter     3/   41] train: loss: 0.5569899
[Epoch 28; Iter    33/   41] train: loss: 0.5392384
[Epoch 28] ogbg-molbace: 0.682051 val loss: 1.264831
[Epoch 28] ogbg-molbace: 0.702660 test loss: 1.280307
[Epoch 29; Iter    22/   41] train: loss: 0.6239591
[Epoch 29] ogbg-molbace: 0.686813 val loss: 1.335036
[Epoch 29] ogbg-molbace: 0.720049 test loss: 1.300470
[Epoch 30; Iter    11/   41] train: loss: 0.6181350
[Epoch 30; Iter    41/   41] train: loss: 0.7548315
[Epoch 30] ogbg-molbace: 0.586447 val loss: 1.584078
[Epoch 30] ogbg-molbace: 0.613632 test loss: 1.940036
[Epoch 31; Iter    30/   41] train: loss: 0.5951394
[Epoch 31] ogbg-molbace: 0.578755 val loss: 2.700850
[Epoch 31] ogbg-molbace: 0.566336 test loss: 3.007498
[Epoch 32; Iter    19/   41] train: loss: 0.5517262
[Epoch 32] ogbg-molbace: 0.597070 val loss: 4.102848
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.1/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.1_6_26-05_09-18-54
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.1
logdir: runs/static_noise/3DInfomax/bace/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6918550
[Epoch 1] ogbg-molbace: 0.495238 val loss: 0.692982
[Epoch 1] ogbg-molbace: 0.294905 test loss: 0.693749
[Epoch 2; Iter    19/   41] train: loss: 0.6952927
[Epoch 2] ogbg-molbace: 0.453846 val loss: 0.695384
[Epoch 2] ogbg-molbace: 0.307773 test loss: 0.698245
[Epoch 3; Iter     8/   41] train: loss: 0.6931069
[Epoch 3; Iter    38/   41] train: loss: 0.6969199
[Epoch 3] ogbg-molbace: 0.436996 val loss: 0.697784
[Epoch 3] ogbg-molbace: 0.338376 test loss: 0.701801
[Epoch 4; Iter    27/   41] train: loss: 0.6936818
[Epoch 4] ogbg-molbace: 0.461172 val loss: 0.697644
[Epoch 4] ogbg-molbace: 0.349331 test loss: 0.701235
[Epoch 5; Iter    16/   41] train: loss: 0.6924117
[Epoch 5] ogbg-molbace: 0.452747 val loss: 0.699066
[Epoch 5] ogbg-molbace: 0.350200 test loss: 0.701903
[Epoch 6; Iter     5/   41] train: loss: 0.6926355
[Epoch 6; Iter    35/   41] train: loss: 0.6919128
[Epoch 6] ogbg-molbace: 0.447619 val loss: 0.699564
[Epoch 6] ogbg-molbace: 0.352460 test loss: 0.702824
[Epoch 7; Iter    24/   41] train: loss: 0.6970641
[Epoch 7] ogbg-molbace: 0.461905 val loss: 0.700010
[Epoch 7] ogbg-molbace: 0.350722 test loss: 0.702829
[Epoch 8; Iter    13/   41] train: loss: 0.6939722
[Epoch 8] ogbg-molbace: 0.458974 val loss: 0.700748
[Epoch 8] ogbg-molbace: 0.352113 test loss: 0.702945
[Epoch 9; Iter     2/   41] train: loss: 0.6861966
[Epoch 9; Iter    32/   41] train: loss: 0.6942218
[Epoch 9] ogbg-molbace: 0.455311 val loss: 0.701349
[Epoch 9] ogbg-molbace: 0.352982 test loss: 0.703895
[Epoch 10; Iter    21/   41] train: loss: 0.6973913
[Epoch 10] ogbg-molbace: 0.453480 val loss: 0.701322
[Epoch 10] ogbg-molbace: 0.355243 test loss: 0.703737
[Epoch 11; Iter    10/   41] train: loss: 0.6913907
[Epoch 11; Iter    40/   41] train: loss: 0.6886533
[Epoch 11] ogbg-molbace: 0.461172 val loss: 0.701960
[Epoch 11] ogbg-molbace: 0.366023 test loss: 0.703591
[Epoch 12; Iter    29/   41] train: loss: 0.6957923
[Epoch 12] ogbg-molbace: 0.468132 val loss: 0.702997
[Epoch 12] ogbg-molbace: 0.369501 test loss: 0.703736
[Epoch 13; Iter    18/   41] train: loss: 0.6954639
[Epoch 13] ogbg-molbace: 0.468132 val loss: 0.703980
[Epoch 13] ogbg-molbace: 0.368805 test loss: 0.704519
[Epoch 14; Iter     7/   41] train: loss: 0.6846132
[Epoch 14; Iter    37/   41] train: loss: 0.6925811
[Epoch 14] ogbg-molbace: 0.462637 val loss: 0.706596
[Epoch 14] ogbg-molbace: 0.366197 test loss: 0.706971
[Epoch 15; Iter    26/   41] train: loss: 0.6859823
[Epoch 15] ogbg-molbace: 0.470330 val loss: 0.705636
[Epoch 15] ogbg-molbace: 0.375065 test loss: 0.705273
[Epoch 16; Iter    15/   41] train: loss: 0.6898430
[Epoch 16] ogbg-molbace: 0.450916 val loss: 0.709569
[Epoch 16] ogbg-molbace: 0.366197 test loss: 0.709465
[Epoch 17; Iter     4/   41] train: loss: 0.6854180
[Epoch 17; Iter    34/   41] train: loss: 0.6945599
[Epoch 17] ogbg-molbace: 0.494139 val loss: 0.710047
[Epoch 17] ogbg-molbace: 0.471396 test loss: 0.711644
[Epoch 18; Iter    23/   41] train: loss: 0.6906834
[Epoch 18] ogbg-molbace: 0.668498 val loss: 0.699207
[Epoch 18] ogbg-molbace: 0.658668 test loss: 0.707528
[Epoch 19; Iter    12/   41] train: loss: 0.6620380
[Epoch 19] ogbg-molbace: 0.628205 val loss: 0.656899
[Epoch 19] ogbg-molbace: 0.705964 test loss: 0.693456
[Epoch 20; Iter     1/   41] train: loss: 0.6189426
[Epoch 20; Iter    31/   41] train: loss: 0.5856682
[Epoch 20] ogbg-molbace: 0.601832 val loss: 0.734544
[Epoch 20] ogbg-molbace: 0.715702 test loss: 0.763740
[Epoch 21; Iter    20/   41] train: loss: 0.6199796
[Epoch 21] ogbg-molbace: 0.682051 val loss: 0.689348
[Epoch 21] ogbg-molbace: 0.709094 test loss: 0.789657
[Epoch 22; Iter     9/   41] train: loss: 0.6011208
[Epoch 22; Iter    39/   41] train: loss: 0.5326680
[Epoch 22] ogbg-molbace: 0.702198 val loss: 0.640245
[Epoch 22] ogbg-molbace: 0.769084 test loss: 0.698815
[Epoch 23; Iter    28/   41] train: loss: 0.5196033
[Epoch 23] ogbg-molbace: 0.616850 val loss: 0.852919
[Epoch 23] ogbg-molbace: 0.690489 test loss: 1.005182
[Epoch 24; Iter    17/   41] train: loss: 0.5374926
[Epoch 24] ogbg-molbace: 0.704396 val loss: 0.666234
[Epoch 24] ogbg-molbace: 0.780560 test loss: 0.767139
[Epoch 25; Iter     6/   41] train: loss: 0.5170788
[Epoch 25; Iter    36/   41] train: loss: 0.5106129
[Epoch 25] ogbg-molbace: 0.623810 val loss: 0.773264
[Epoch 25] ogbg-molbace: 0.686663 test loss: 1.007480
[Epoch 26; Iter    25/   41] train: loss: 0.4808146
[Epoch 26] ogbg-molbace: 0.569597 val loss: 1.025015
[Epoch 26] ogbg-molbace: 0.628586 test loss: 1.310146
[Epoch 27; Iter    14/   41] train: loss: 0.6236633
[Epoch 27] ogbg-molbace: 0.663004 val loss: 0.969169
[Epoch 27] ogbg-molbace: 0.698661 test loss: 1.207321
[Epoch 28; Iter     3/   41] train: loss: 0.5979859
[Epoch 28; Iter    33/   41] train: loss: 0.5479910
[Epoch 28] ogbg-molbace: 0.668498 val loss: 0.869517
[Epoch 28] ogbg-molbace: 0.712398 test loss: 1.128765
[Epoch 29; Iter    22/   41] train: loss: 0.5609654
[Epoch 29] ogbg-molbace: 0.562271 val loss: 1.026080
[Epoch 29] ogbg-molbace: 0.664754 test loss: 1.260283
[Epoch 30; Iter    11/   41] train: loss: 0.4838842
[Epoch 30; Iter    41/   41] train: loss: 0.5732743
[Epoch 30] ogbg-molbace: 0.586447 val loss: 1.058572
[Epoch 30] ogbg-molbace: 0.683186 test loss: 1.357124
[Epoch 31; Iter    30/   41] train: loss: 0.5117902
[Epoch 31] ogbg-molbace: 0.578022 val loss: 1.791389
[Epoch 31] ogbg-molbace: 0.653625 test loss: 2.446295
[Epoch 32; Iter    19/   41] train: loss: 0.5403324
[Epoch 32] ogbg-molbace: 0.611355 val loss: 1.936075
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.2/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.2_5_26-05_09-18-54
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.2
logdir: runs/static_noise/3DInfomax/bace/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6927994
[Epoch 1] ogbg-molbace: 0.515751 val loss: 0.692124
[Epoch 1] ogbg-molbace: 0.547557 test loss: 0.691646
[Epoch 2; Iter    19/   41] train: loss: 0.6924359
[Epoch 2] ogbg-molbace: 0.526374 val loss: 0.688031
[Epoch 2] ogbg-molbace: 0.590680 test loss: 0.686254
[Epoch 3; Iter     8/   41] train: loss: 0.6954741
[Epoch 3; Iter    38/   41] train: loss: 0.6944817
[Epoch 3] ogbg-molbace: 0.505495 val loss: 0.689276
[Epoch 3] ogbg-molbace: 0.587550 test loss: 0.685910
[Epoch 4; Iter    27/   41] train: loss: 0.6911352
[Epoch 4] ogbg-molbace: 0.509524 val loss: 0.688532
[Epoch 4] ogbg-molbace: 0.597287 test loss: 0.685213
[Epoch 5; Iter    16/   41] train: loss: 0.6966022
[Epoch 5] ogbg-molbace: 0.504029 val loss: 0.688879
[Epoch 5] ogbg-molbace: 0.593288 test loss: 0.685869
[Epoch 6; Iter     5/   41] train: loss: 0.6967114
[Epoch 6; Iter    35/   41] train: loss: 0.6969762
[Epoch 6] ogbg-molbace: 0.509890 val loss: 0.689392
[Epoch 6] ogbg-molbace: 0.592593 test loss: 0.685938
[Epoch 7; Iter    24/   41] train: loss: 0.6920085
[Epoch 7] ogbg-molbace: 0.502198 val loss: 0.690968
[Epoch 7] ogbg-molbace: 0.594158 test loss: 0.686778
[Epoch 8; Iter    13/   41] train: loss: 0.6905971
[Epoch 8] ogbg-molbace: 0.499267 val loss: 0.690530
[Epoch 8] ogbg-molbace: 0.590506 test loss: 0.686561
[Epoch 9; Iter     2/   41] train: loss: 0.6903621
[Epoch 9; Iter    32/   41] train: loss: 0.6895509
[Epoch 9] ogbg-molbace: 0.505861 val loss: 0.691252
[Epoch 9] ogbg-molbace: 0.597635 test loss: 0.686602
[Epoch 10; Iter    21/   41] train: loss: 0.6913184
[Epoch 10] ogbg-molbace: 0.506960 val loss: 0.691955
[Epoch 10] ogbg-molbace: 0.596418 test loss: 0.687624
[Epoch 11; Iter    10/   41] train: loss: 0.6953127
[Epoch 11; Iter    40/   41] train: loss: 0.6938125
[Epoch 11] ogbg-molbace: 0.503663 val loss: 0.692704
[Epoch 11] ogbg-molbace: 0.593288 test loss: 0.687711
[Epoch 12; Iter    29/   41] train: loss: 0.6911474
[Epoch 12] ogbg-molbace: 0.503297 val loss: 0.693556
[Epoch 12] ogbg-molbace: 0.606155 test loss: 0.687561
[Epoch 13; Iter    18/   41] train: loss: 0.6886070
[Epoch 13] ogbg-molbace: 0.502564 val loss: 0.693909
[Epoch 13] ogbg-molbace: 0.607894 test loss: 0.687780
[Epoch 14; Iter     7/   41] train: loss: 0.6908689
[Epoch 14; Iter    37/   41] train: loss: 0.6855724
[Epoch 14] ogbg-molbace: 0.509524 val loss: 0.695027
[Epoch 14] ogbg-molbace: 0.610329 test loss: 0.688297
[Epoch 15; Iter    26/   41] train: loss: 0.6972757
[Epoch 15] ogbg-molbace: 0.498535 val loss: 0.695144
[Epoch 15] ogbg-molbace: 0.613632 test loss: 0.688736
[Epoch 16; Iter    15/   41] train: loss: 0.6865249
[Epoch 16] ogbg-molbace: 0.502930 val loss: 0.696932
[Epoch 16] ogbg-molbace: 0.616588 test loss: 0.689377
[Epoch 17; Iter     4/   41] train: loss: 0.6908358
[Epoch 17; Iter    34/   41] train: loss: 0.6923148
[Epoch 17] ogbg-molbace: 0.477656 val loss: 0.701386
[Epoch 17] ogbg-molbace: 0.641280 test loss: 0.693408
[Epoch 18; Iter    23/   41] train: loss: 0.6889511
[Epoch 18] ogbg-molbace: 0.575092 val loss: 0.692231
[Epoch 18] ogbg-molbace: 0.637280 test loss: 0.704496
[Epoch 19; Iter    12/   41] train: loss: 0.6575874
[Epoch 19] ogbg-molbace: 0.661172 val loss: 0.686368
[Epoch 19] ogbg-molbace: 0.727700 test loss: 0.702354
[Epoch 20; Iter     1/   41] train: loss: 0.6174247
[Epoch 20; Iter    31/   41] train: loss: 0.6188711
[Epoch 20] ogbg-molbace: 0.485348 val loss: 0.902943
[Epoch 20] ogbg-molbace: 0.530343 test loss: 0.991790
[Epoch 21; Iter    20/   41] train: loss: 0.6215865
[Epoch 21] ogbg-molbace: 0.568498 val loss: 0.602226
[Epoch 21] ogbg-molbace: 0.677100 test loss: 0.717066
[Epoch 22; Iter     9/   41] train: loss: 0.5132484
[Epoch 22; Iter    39/   41] train: loss: 0.5345802
[Epoch 22] ogbg-molbace: 0.633700 val loss: 0.843979
[Epoch 22] ogbg-molbace: 0.677274 test loss: 0.894812
[Epoch 23; Iter    28/   41] train: loss: 0.6545701
[Epoch 23] ogbg-molbace: 0.606593 val loss: 0.704995
[Epoch 23] ogbg-molbace: 0.672926 test loss: 0.847005
[Epoch 24; Iter    17/   41] train: loss: 0.5454371
[Epoch 24] ogbg-molbace: 0.701832 val loss: 0.605523
[Epoch 24] ogbg-molbace: 0.781951 test loss: 0.713750
[Epoch 25; Iter     6/   41] train: loss: 0.5690188
[Epoch 25; Iter    36/   41] train: loss: 0.6930559
[Epoch 25] ogbg-molbace: 0.705128 val loss: 0.594346
[Epoch 25] ogbg-molbace: 0.774474 test loss: 0.719719
[Epoch 26; Iter    25/   41] train: loss: 0.5660383
[Epoch 26] ogbg-molbace: 0.657143 val loss: 0.591067
[Epoch 26] ogbg-molbace: 0.772735 test loss: 0.722214
[Epoch 27; Iter    14/   41] train: loss: 0.5195285
[Epoch 27] ogbg-molbace: 0.705128 val loss: 0.594665
[Epoch 27] ogbg-molbace: 0.768910 test loss: 0.739253
[Epoch 28; Iter     3/   41] train: loss: 0.5363895
[Epoch 28; Iter    33/   41] train: loss: 0.5376956
[Epoch 28] ogbg-molbace: 0.629304 val loss: 0.723013
[Epoch 28] ogbg-molbace: 0.766475 test loss: 0.826608
[Epoch 29; Iter    22/   41] train: loss: 0.4765702
[Epoch 29] ogbg-molbace: 0.714652 val loss: 0.796484
[Epoch 29] ogbg-molbace: 0.771170 test loss: 0.917639
[Epoch 30; Iter    11/   41] train: loss: 0.6137378
[Epoch 30; Iter    41/   41] train: loss: 0.6119080
[Epoch 30] ogbg-molbace: 0.730403 val loss: 0.781355
[Epoch 30] ogbg-molbace: 0.754304 test loss: 0.848816
[Epoch 31; Iter    30/   41] train: loss: 0.5999778
[Epoch 31] ogbg-molbace: 0.625275 val loss: 0.764302
[Epoch 31] ogbg-molbace: 0.722831 test loss: 0.826036
[Epoch 32; Iter    19/   41] train: loss: 0.7464328
[Epoch 32] ogbg-molbace: 0.654945 val loss: 0.874251
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.05/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.05_6_26-05_09-18-50
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.05
logdir: runs/static_noise/3DInfomax/bace/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6918816
[Epoch 1] ogbg-molbace: 0.557143 val loss: 0.692684
[Epoch 1] ogbg-molbace: 0.337680 test loss: 0.693317
[Epoch 2; Iter    19/   41] train: loss: 0.6933197
[Epoch 2] ogbg-molbace: 0.510989 val loss: 0.692522
[Epoch 2] ogbg-molbace: 0.356808 test loss: 0.695056
[Epoch 3; Iter     8/   41] train: loss: 0.6932598
[Epoch 3; Iter    38/   41] train: loss: 0.6978973
[Epoch 3] ogbg-molbace: 0.481685 val loss: 0.693449
[Epoch 3] ogbg-molbace: 0.390193 test loss: 0.696420
[Epoch 4; Iter    27/   41] train: loss: 0.6934811
[Epoch 4] ogbg-molbace: 0.485348 val loss: 0.693630
[Epoch 4] ogbg-molbace: 0.388628 test loss: 0.696410
[Epoch 5; Iter    16/   41] train: loss: 0.6933836
[Epoch 5] ogbg-molbace: 0.494505 val loss: 0.693564
[Epoch 5] ogbg-molbace: 0.392801 test loss: 0.696435
[Epoch 6; Iter     5/   41] train: loss: 0.6908093
[Epoch 6; Iter    35/   41] train: loss: 0.6952474
[Epoch 6] ogbg-molbace: 0.471795 val loss: 0.695197
[Epoch 6] ogbg-molbace: 0.395583 test loss: 0.697710
[Epoch 7; Iter    24/   41] train: loss: 0.6947963
[Epoch 7] ogbg-molbace: 0.470330 val loss: 0.695557
[Epoch 7] ogbg-molbace: 0.404973 test loss: 0.698231
[Epoch 8; Iter    13/   41] train: loss: 0.6922058
[Epoch 8] ogbg-molbace: 0.477289 val loss: 0.695875
[Epoch 8] ogbg-molbace: 0.404799 test loss: 0.698044
[Epoch 9; Iter     2/   41] train: loss: 0.6893490
[Epoch 9; Iter    32/   41] train: loss: 0.6924032
[Epoch 9] ogbg-molbace: 0.485348 val loss: 0.696188
[Epoch 9] ogbg-molbace: 0.402191 test loss: 0.697935
[Epoch 10; Iter    21/   41] train: loss: 0.6952558
[Epoch 10] ogbg-molbace: 0.499634 val loss: 0.696923
[Epoch 10] ogbg-molbace: 0.413667 test loss: 0.697602
[Epoch 11; Iter    10/   41] train: loss: 0.6906865
[Epoch 11; Iter    40/   41] train: loss: 0.6879596
[Epoch 11] ogbg-molbace: 0.473626 val loss: 0.698370
[Epoch 11] ogbg-molbace: 0.411233 test loss: 0.699375
[Epoch 12; Iter    29/   41] train: loss: 0.6947145
[Epoch 12] ogbg-molbace: 0.489744 val loss: 0.698442
[Epoch 12] ogbg-molbace: 0.416971 test loss: 0.698902
[Epoch 13; Iter    18/   41] train: loss: 0.6943187
[Epoch 13] ogbg-molbace: 0.489011 val loss: 0.698840
[Epoch 13] ogbg-molbace: 0.425143 test loss: 0.699172
[Epoch 14; Iter     7/   41] train: loss: 0.6872231
[Epoch 14; Iter    37/   41] train: loss: 0.6916634
[Epoch 14] ogbg-molbace: 0.474725 val loss: 0.701069
[Epoch 14] ogbg-molbace: 0.413493 test loss: 0.700930
[Epoch 15; Iter    26/   41] train: loss: 0.6865038
[Epoch 15] ogbg-molbace: 0.464103 val loss: 0.702256
[Epoch 15] ogbg-molbace: 0.422014 test loss: 0.701763
[Epoch 16; Iter    15/   41] train: loss: 0.6881078
[Epoch 16] ogbg-molbace: 0.478755 val loss: 0.703044
[Epoch 16] ogbg-molbace: 0.425491 test loss: 0.701370
[Epoch 17; Iter     4/   41] train: loss: 0.6854874
[Epoch 17; Iter    34/   41] train: loss: 0.6957090
[Epoch 17] ogbg-molbace: 0.540293 val loss: 0.695686
[Epoch 17] ogbg-molbace: 0.553991 test loss: 0.699389
[Epoch 18; Iter    23/   41] train: loss: 0.6919396
[Epoch 18] ogbg-molbace: 0.689011 val loss: 0.696217
[Epoch 18] ogbg-molbace: 0.675535 test loss: 0.702223
[Epoch 19; Iter    12/   41] train: loss: 0.6696731
[Epoch 19] ogbg-molbace: 0.701465 val loss: 0.643515
[Epoch 19] ogbg-molbace: 0.772040 test loss: 0.667137
[Epoch 20; Iter     1/   41] train: loss: 0.6031414
[Epoch 20; Iter    31/   41] train: loss: 0.6017470
[Epoch 20] ogbg-molbace: 0.584982 val loss: 0.691268
[Epoch 20] ogbg-molbace: 0.722483 test loss: 0.740193
[Epoch 21; Iter    20/   41] train: loss: 0.6209354
[Epoch 21] ogbg-molbace: 0.615018 val loss: 0.794059
[Epoch 21] ogbg-molbace: 0.657625 test loss: 1.013856
[Epoch 22; Iter     9/   41] train: loss: 0.6337685
[Epoch 22; Iter    39/   41] train: loss: 0.4724051
[Epoch 22] ogbg-molbace: 0.728938 val loss: 0.761188
[Epoch 22] ogbg-molbace: 0.758303 test loss: 0.867644
[Epoch 23; Iter    28/   41] train: loss: 0.4993643
[Epoch 23] ogbg-molbace: 0.692308 val loss: 0.660584
[Epoch 23] ogbg-molbace: 0.749087 test loss: 0.845680
[Epoch 24; Iter    17/   41] train: loss: 0.4688699
[Epoch 24] ogbg-molbace: 0.702198 val loss: 0.778223
[Epoch 24] ogbg-molbace: 0.770822 test loss: 0.914243
[Epoch 25; Iter     6/   41] train: loss: 0.5659749
[Epoch 25; Iter    36/   41] train: loss: 0.4861179
[Epoch 25] ogbg-molbace: 0.709158 val loss: 0.660977
[Epoch 25] ogbg-molbace: 0.779343 test loss: 0.803750
[Epoch 26; Iter    25/   41] train: loss: 0.5766839
[Epoch 26] ogbg-molbace: 0.607692 val loss: 0.990745
[Epoch 26] ogbg-molbace: 0.703530 test loss: 1.222587
[Epoch 27; Iter    14/   41] train: loss: 0.6430066
[Epoch 27] ogbg-molbace: 0.649817 val loss: 0.810649
[Epoch 27] ogbg-molbace: 0.717962 test loss: 1.083444
[Epoch 28; Iter     3/   41] train: loss: 0.6674257
[Epoch 28; Iter    33/   41] train: loss: 0.4545488
[Epoch 28] ogbg-molbace: 0.613553 val loss: 0.970193
[Epoch 28] ogbg-molbace: 0.713267 test loss: 1.219298
[Epoch 29; Iter    22/   41] train: loss: 0.5484260
[Epoch 29] ogbg-molbace: 0.660440 val loss: 1.012301
[Epoch 29] ogbg-molbace: 0.738132 test loss: 1.201818
[Epoch 30; Iter    11/   41] train: loss: 0.4257763
[Epoch 30; Iter    41/   41] train: loss: 0.6631662
[Epoch 30] ogbg-molbace: 0.623810 val loss: 0.885894
[Epoch 30] ogbg-molbace: 0.732916 test loss: 1.101366
[Epoch 31; Iter    30/   41] train: loss: 0.5053039
[Epoch 31] ogbg-molbace: 0.598168 val loss: 1.110688
[Epoch 31] ogbg-molbace: 0.686663 test loss: 1.471211
[Epoch 32; Iter    19/   41] train: loss: 0.5670193
[Epoch 32] ogbg-molbace: 0.718681 val loss: 0.927330
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.1/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.1_4_26-05_09-18-52
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.1
logdir: runs/static_noise/3DInfomax/bace/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6926201
[Epoch 1] ogbg-molbace: 0.443956 val loss: 0.693934
[Epoch 1] ogbg-molbace: 0.245523 test loss: 0.693904
[Epoch 2; Iter    19/   41] train: loss: 0.6925948
[Epoch 2] ogbg-molbace: 0.426007 val loss: 0.696503
[Epoch 2] ogbg-molbace: 0.283951 test loss: 0.695987
[Epoch 3; Iter     8/   41] train: loss: 0.6939352
[Epoch 3; Iter    38/   41] train: loss: 0.6918532
[Epoch 3] ogbg-molbace: 0.419048 val loss: 0.698083
[Epoch 3] ogbg-molbace: 0.302382 test loss: 0.697077
[Epoch 4; Iter    27/   41] train: loss: 0.6937572
[Epoch 4] ogbg-molbace: 0.427106 val loss: 0.697306
[Epoch 4] ogbg-molbace: 0.311424 test loss: 0.696607
[Epoch 5; Iter    16/   41] train: loss: 0.6969847
[Epoch 5] ogbg-molbace: 0.416484 val loss: 0.698467
[Epoch 5] ogbg-molbace: 0.307251 test loss: 0.697237
[Epoch 6; Iter     5/   41] train: loss: 0.6930256
[Epoch 6; Iter    35/   41] train: loss: 0.6911175
[Epoch 6] ogbg-molbace: 0.420147 val loss: 0.698874
[Epoch 6] ogbg-molbace: 0.316119 test loss: 0.697193
[Epoch 7; Iter    24/   41] train: loss: 0.6954271
[Epoch 7] ogbg-molbace: 0.421245 val loss: 0.698443
[Epoch 7] ogbg-molbace: 0.322379 test loss: 0.697171
[Epoch 8; Iter    13/   41] train: loss: 0.6944601
[Epoch 8] ogbg-molbace: 0.426007 val loss: 0.700210
[Epoch 8] ogbg-molbace: 0.334898 test loss: 0.697394
[Epoch 9; Iter     2/   41] train: loss: 0.6933302
[Epoch 9; Iter    32/   41] train: loss: 0.6918128
[Epoch 9] ogbg-molbace: 0.415018 val loss: 0.700869
[Epoch 9] ogbg-molbace: 0.318206 test loss: 0.698386
[Epoch 10; Iter    21/   41] train: loss: 0.6926949
[Epoch 10] ogbg-molbace: 0.412821 val loss: 0.701213
[Epoch 10] ogbg-molbace: 0.311250 test loss: 0.698864
[Epoch 11; Iter    10/   41] train: loss: 0.6962882
[Epoch 11; Iter    40/   41] train: loss: 0.6893538
[Epoch 11] ogbg-molbace: 0.423077 val loss: 0.701390
[Epoch 11] ogbg-molbace: 0.336289 test loss: 0.698670
[Epoch 12; Iter    29/   41] train: loss: 0.6951832
[Epoch 12] ogbg-molbace: 0.415018 val loss: 0.702971
[Epoch 12] ogbg-molbace: 0.328986 test loss: 0.699474
[Epoch 13; Iter    18/   41] train: loss: 0.6922536
[Epoch 13] ogbg-molbace: 0.418315 val loss: 0.703205
[Epoch 13] ogbg-molbace: 0.340463 test loss: 0.699641
[Epoch 14; Iter     7/   41] train: loss: 0.6893931
[Epoch 14; Iter    37/   41] train: loss: 0.6885071
[Epoch 14] ogbg-molbace: 0.416117 val loss: 0.704490
[Epoch 14] ogbg-molbace: 0.342201 test loss: 0.700297
[Epoch 15; Iter    26/   41] train: loss: 0.6912410
[Epoch 15] ogbg-molbace: 0.415018 val loss: 0.705479
[Epoch 15] ogbg-molbace: 0.343592 test loss: 0.700632
[Epoch 16; Iter    15/   41] train: loss: 0.6934870
[Epoch 16] ogbg-molbace: 0.411722 val loss: 0.706293
[Epoch 16] ogbg-molbace: 0.343419 test loss: 0.701277
[Epoch 17; Iter     4/   41] train: loss: 0.6926618
[Epoch 17; Iter    34/   41] train: loss: 0.6858419
[Epoch 17] ogbg-molbace: 0.498535 val loss: 0.706566
[Epoch 17] ogbg-molbace: 0.434011 test loss: 0.702598
[Epoch 18; Iter    23/   41] train: loss: 0.6584201
[Epoch 18] ogbg-molbace: 0.589011 val loss: 0.701877
[Epoch 18] ogbg-molbace: 0.630325 test loss: 0.714286
[Epoch 19; Iter    12/   41] train: loss: 0.6423052
[Epoch 19] ogbg-molbace: 0.669963 val loss: 0.656567
[Epoch 19] ogbg-molbace: 0.731351 test loss: 0.682529
[Epoch 20; Iter     1/   41] train: loss: 0.6608683
[Epoch 20; Iter    31/   41] train: loss: 0.5732773
[Epoch 20] ogbg-molbace: 0.614286 val loss: 0.734071
[Epoch 20] ogbg-molbace: 0.728047 test loss: 0.788379
[Epoch 21; Iter    20/   41] train: loss: 0.5304688
[Epoch 21] ogbg-molbace: 0.587912 val loss: 0.766857
[Epoch 21] ogbg-molbace: 0.692923 test loss: 0.843677
[Epoch 22; Iter     9/   41] train: loss: 0.6430277
[Epoch 22; Iter    39/   41] train: loss: 0.5497378
[Epoch 22] ogbg-molbace: 0.642125 val loss: 0.859684
[Epoch 22] ogbg-molbace: 0.706312 test loss: 1.002354
[Epoch 23; Iter    28/   41] train: loss: 0.6334154
[Epoch 23] ogbg-molbace: 0.614286 val loss: 0.848266
[Epoch 23] ogbg-molbace: 0.690141 test loss: 1.110477
[Epoch 24; Iter    17/   41] train: loss: 0.6487370
[Epoch 24] ogbg-molbace: 0.656410 val loss: 0.937294
[Epoch 24] ogbg-molbace: 0.671014 test loss: 1.227379
[Epoch 25; Iter     6/   41] train: loss: 0.5199523
[Epoch 25; Iter    36/   41] train: loss: 0.5063754
[Epoch 25] ogbg-molbace: 0.660440 val loss: 1.132749
[Epoch 25] ogbg-molbace: 0.666319 test loss: 1.455835
[Epoch 26; Iter    25/   41] train: loss: 0.5819688
[Epoch 26] ogbg-molbace: 0.723443 val loss: 0.774329
[Epoch 26] ogbg-molbace: 0.739002 test loss: 1.055530
[Epoch 27; Iter    14/   41] train: loss: 0.4789889
[Epoch 27] ogbg-molbace: 0.718315 val loss: 0.905814
[Epoch 27] ogbg-molbace: 0.755521 test loss: 1.114124
[Epoch 28; Iter     3/   41] train: loss: 0.5401672
[Epoch 28; Iter    33/   41] train: loss: 0.4786721
[Epoch 28] ogbg-molbace: 0.712821 val loss: 0.920962
[Epoch 28] ogbg-molbace: 0.752217 test loss: 1.212922
[Epoch 29; Iter    22/   41] train: loss: 0.5892670
[Epoch 29] ogbg-molbace: 0.669963 val loss: 1.178509
[Epoch 29] ogbg-molbace: 0.732047 test loss: 1.397867
[Epoch 30; Iter    11/   41] train: loss: 0.4940367
[Epoch 30; Iter    41/   41] train: loss: 0.3997353
[Epoch 30] ogbg-molbace: 0.714286 val loss: 0.904384
[Epoch 30] ogbg-molbace: 0.756738 test loss: 1.140484
[Epoch 31; Iter    30/   41] train: loss: 0.5604197
[Epoch 31] ogbg-molbace: 0.649451 val loss: 0.978863
[Epoch 31] ogbg-molbace: 0.720744 test loss: 1.313020
[Epoch 32; Iter    19/   41] train: loss: 0.5626181
[Epoch 32] ogbg-molbace: 0.687179 val loss: 0.900716
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.2/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.2_6_26-05_09-18-55
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.2
logdir: runs/static_noise/3DInfomax/bace/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6924932
[Epoch 1] ogbg-molbace: 0.449084 val loss: 0.693996
[Epoch 1] ogbg-molbace: 0.279951 test loss: 0.694544
[Epoch 2; Iter    19/   41] train: loss: 0.6909296
[Epoch 2] ogbg-molbace: 0.429304 val loss: 0.701059
[Epoch 2] ogbg-molbace: 0.294210 test loss: 0.702071
[Epoch 3; Iter     8/   41] train: loss: 0.6935604
[Epoch 3; Iter    38/   41] train: loss: 0.6951777
[Epoch 3] ogbg-molbace: 0.428205 val loss: 0.708948
[Epoch 3] ogbg-molbace: 0.331768 test loss: 0.708645
[Epoch 4; Iter    27/   41] train: loss: 0.6927882
[Epoch 4] ogbg-molbace: 0.441392 val loss: 0.708218
[Epoch 4] ogbg-molbace: 0.347070 test loss: 0.707714
[Epoch 5; Iter    16/   41] train: loss: 0.6943920
[Epoch 5] ogbg-molbace: 0.439194 val loss: 0.708688
[Epoch 5] ogbg-molbace: 0.342549 test loss: 0.708335
[Epoch 6; Iter     5/   41] train: loss: 0.6951807
[Epoch 6; Iter    35/   41] train: loss: 0.6936529
[Epoch 6] ogbg-molbace: 0.439194 val loss: 0.711535
[Epoch 6] ogbg-molbace: 0.339071 test loss: 0.710299
[Epoch 7; Iter    24/   41] train: loss: 0.6957189
[Epoch 7] ogbg-molbace: 0.434066 val loss: 0.712004
[Epoch 7] ogbg-molbace: 0.331421 test loss: 0.711248
[Epoch 8; Iter    13/   41] train: loss: 0.6932461
[Epoch 8] ogbg-molbace: 0.435531 val loss: 0.712258
[Epoch 8] ogbg-molbace: 0.335246 test loss: 0.710595
[Epoch 9; Iter     2/   41] train: loss: 0.6946560
[Epoch 9; Iter    32/   41] train: loss: 0.6916238
[Epoch 9] ogbg-molbace: 0.447619 val loss: 0.711639
[Epoch 9] ogbg-molbace: 0.357155 test loss: 0.708925
[Epoch 10; Iter    21/   41] train: loss: 0.6965015
[Epoch 10] ogbg-molbace: 0.439927 val loss: 0.714128
[Epoch 10] ogbg-molbace: 0.347939 test loss: 0.711836
[Epoch 11; Iter    10/   41] train: loss: 0.6900505
[Epoch 11; Iter    40/   41] train: loss: 0.6902049
[Epoch 11] ogbg-molbace: 0.439560 val loss: 0.715264
[Epoch 11] ogbg-molbace: 0.341506 test loss: 0.713333
[Epoch 12; Iter    29/   41] train: loss: 0.6936740
[Epoch 12] ogbg-molbace: 0.435531 val loss: 0.716445
[Epoch 12] ogbg-molbace: 0.346375 test loss: 0.712969
[Epoch 13; Iter    18/   41] train: loss: 0.6952983
[Epoch 13] ogbg-molbace: 0.436996 val loss: 0.717605
[Epoch 13] ogbg-molbace: 0.348461 test loss: 0.713362
[Epoch 14; Iter     7/   41] train: loss: 0.6842778
[Epoch 14; Iter    37/   41] train: loss: 0.6952270
[Epoch 14] ogbg-molbace: 0.448718 val loss: 0.716680
[Epoch 14] ogbg-molbace: 0.358720 test loss: 0.712118
[Epoch 15; Iter    26/   41] train: loss: 0.6885884
[Epoch 15] ogbg-molbace: 0.447253 val loss: 0.718942
[Epoch 15] ogbg-molbace: 0.363763 test loss: 0.713568
[Epoch 16; Iter    15/   41] train: loss: 0.6859077
[Epoch 16] ogbg-molbace: 0.442857 val loss: 0.719315
[Epoch 16] ogbg-molbace: 0.357329 test loss: 0.714191
[Epoch 17; Iter     4/   41] train: loss: 0.6828599
[Epoch 17; Iter    34/   41] train: loss: 0.6949514
[Epoch 17] ogbg-molbace: 0.470330 val loss: 0.723483
[Epoch 17] ogbg-molbace: 0.416275 test loss: 0.719429
[Epoch 18; Iter    23/   41] train: loss: 0.6951517
[Epoch 18] ogbg-molbace: 0.609158 val loss: 0.713089
[Epoch 18] ogbg-molbace: 0.633977 test loss: 0.710004
[Epoch 19; Iter    12/   41] train: loss: 0.6681958
[Epoch 19] ogbg-molbace: 0.595238 val loss: 0.769122
[Epoch 19] ogbg-molbace: 0.641975 test loss: 0.779228
[Epoch 20; Iter     1/   41] train: loss: 0.6231444
[Epoch 20; Iter    31/   41] train: loss: 0.6367555
[Epoch 20] ogbg-molbace: 0.567766 val loss: 0.827700
[Epoch 20] ogbg-molbace: 0.679012 test loss: 0.796849
[Epoch 21; Iter    20/   41] train: loss: 0.6057951
[Epoch 21] ogbg-molbace: 0.554579 val loss: 1.265938
[Epoch 21] ogbg-molbace: 0.580073 test loss: 1.229620
[Epoch 22; Iter     9/   41] train: loss: 0.6212901
[Epoch 22; Iter    39/   41] train: loss: 0.5167555
[Epoch 22] ogbg-molbace: 0.566667 val loss: 1.018209
[Epoch 22] ogbg-molbace: 0.643019 test loss: 1.054966
[Epoch 23; Iter    28/   41] train: loss: 0.5636474
[Epoch 23] ogbg-molbace: 0.572161 val loss: 1.019392
[Epoch 23] ogbg-molbace: 0.638845 test loss: 1.096177
[Epoch 24; Iter    17/   41] train: loss: 0.5018905
[Epoch 24] ogbg-molbace: 0.704762 val loss: 0.728440
[Epoch 24] ogbg-molbace: 0.715702 test loss: 0.857309
[Epoch 25; Iter     6/   41] train: loss: 0.6604155
[Epoch 25; Iter    36/   41] train: loss: 0.5175768
[Epoch 25] ogbg-molbace: 0.687546 val loss: 0.946521
[Epoch 25] ogbg-molbace: 0.687011 test loss: 1.167577
[Epoch 26; Iter    25/   41] train: loss: 0.6407039
[Epoch 26] ogbg-molbace: 0.579487 val loss: 2.163276
[Epoch 26] ogbg-molbace: 0.642323 test loss: 2.796690
[Epoch 27; Iter    14/   41] train: loss: 0.6406788
[Epoch 27] ogbg-molbace: 0.635897 val loss: 1.834103
[Epoch 27] ogbg-molbace: 0.654669 test loss: 2.294898
[Epoch 28; Iter     3/   41] train: loss: 0.6483115
[Epoch 28; Iter    33/   41] train: loss: 0.5057578
[Epoch 28] ogbg-molbace: 0.625275 val loss: 1.868981
[Epoch 28] ogbg-molbace: 0.644584 test loss: 2.331040
[Epoch 29; Iter    22/   41] train: loss: 0.5652475
[Epoch 29] ogbg-molbace: 0.568132 val loss: 1.903308
[Epoch 29] ogbg-molbace: 0.615197 test loss: 2.346776
[Epoch 30; Iter    11/   41] train: loss: 0.4676875
[Epoch 30; Iter    41/   41] train: loss: 0.6380660
[Epoch 30] ogbg-molbace: 0.557875 val loss: 2.146798
[Epoch 30] ogbg-molbace: 0.618501 test loss: 2.597461
[Epoch 31; Iter    30/   41] train: loss: 0.5514922
[Epoch 31] ogbg-molbace: 0.621612 val loss: 2.055659
[Epoch 31] ogbg-molbace: 0.651191 test loss: 2.569494
[Epoch 32; Iter    19/   41] train: loss: 0.6032385
[Epoch 32] ogbg-molbace: 0.683883 val loss: 1.225003
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.0/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.0_5_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.0
logdir: runs/static_noise/3DInfomax/bace/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6950303
[Epoch 1] ogbg-molbace: 0.406593 val loss: 0.693259
[Epoch 1] ogbg-molbace: 0.372457 test loss: 0.692916
[Epoch 2; Iter    19/   41] train: loss: 0.6950377
[Epoch 2] ogbg-molbace: 0.447253 val loss: 0.694580
[Epoch 2] ogbg-molbace: 0.417666 test loss: 0.691552
[Epoch 3; Iter     8/   41] train: loss: 0.6960334
[Epoch 3; Iter    38/   41] train: loss: 0.6972963
[Epoch 3] ogbg-molbace: 0.431502 val loss: 0.695221
[Epoch 3] ogbg-molbace: 0.412972 test loss: 0.691748
[Epoch 4; Iter    27/   41] train: loss: 0.6938558
[Epoch 4] ogbg-molbace: 0.442857 val loss: 0.695608
[Epoch 4] ogbg-molbace: 0.424274 test loss: 0.691425
[Epoch 5; Iter    16/   41] train: loss: 0.6949973
[Epoch 5] ogbg-molbace: 0.441392 val loss: 0.695963
[Epoch 5] ogbg-molbace: 0.417145 test loss: 0.691923
[Epoch 6; Iter     5/   41] train: loss: 0.6961963
[Epoch 6; Iter    35/   41] train: loss: 0.6999989
[Epoch 6] ogbg-molbace: 0.446520 val loss: 0.696210
[Epoch 6] ogbg-molbace: 0.427056 test loss: 0.691761
[Epoch 7; Iter    24/   41] train: loss: 0.6939733
[Epoch 7] ogbg-molbace: 0.442125 val loss: 0.696963
[Epoch 7] ogbg-molbace: 0.417840 test loss: 0.692341
[Epoch 8; Iter    13/   41] train: loss: 0.6931360
[Epoch 8] ogbg-molbace: 0.420879 val loss: 0.697438
[Epoch 8] ogbg-molbace: 0.409320 test loss: 0.692982
[Epoch 9; Iter     2/   41] train: loss: 0.6941325
[Epoch 9; Iter    32/   41] train: loss: 0.6933050
[Epoch 9] ogbg-molbace: 0.428205 val loss: 0.697758
[Epoch 9] ogbg-molbace: 0.416102 test loss: 0.692804
[Epoch 10; Iter    21/   41] train: loss: 0.6952260
[Epoch 10] ogbg-molbace: 0.419414 val loss: 0.698516
[Epoch 10] ogbg-molbace: 0.410885 test loss: 0.693385
[Epoch 11; Iter    10/   41] train: loss: 0.6918857
[Epoch 11; Iter    40/   41] train: loss: 0.6938196
[Epoch 11] ogbg-molbace: 0.448718 val loss: 0.698331
[Epoch 11] ogbg-molbace: 0.441836 test loss: 0.693098
[Epoch 12; Iter    29/   41] train: loss: 0.6913506
[Epoch 12] ogbg-molbace: 0.427839 val loss: 0.699827
[Epoch 12] ogbg-molbace: 0.424622 test loss: 0.693830
[Epoch 13; Iter    18/   41] train: loss: 0.6924941
[Epoch 13] ogbg-molbace: 0.420513 val loss: 0.700463
[Epoch 13] ogbg-molbace: 0.430534 test loss: 0.694192
[Epoch 14; Iter     7/   41] train: loss: 0.6954867
[Epoch 14; Iter    37/   41] train: loss: 0.6920549
[Epoch 14] ogbg-molbace: 0.434066 val loss: 0.701485
[Epoch 14] ogbg-molbace: 0.440271 test loss: 0.694409
[Epoch 15; Iter    26/   41] train: loss: 0.6927629
[Epoch 15] ogbg-molbace: 0.428205 val loss: 0.702042
[Epoch 15] ogbg-molbace: 0.441315 test loss: 0.695451
[Epoch 16; Iter    15/   41] train: loss: 0.6844546
[Epoch 16] ogbg-molbace: 0.426374 val loss: 0.703127
[Epoch 16] ogbg-molbace: 0.446183 test loss: 0.695532
[Epoch 17; Iter     4/   41] train: loss: 0.6933329
[Epoch 17; Iter    34/   41] train: loss: 0.6949021
[Epoch 17] ogbg-molbace: 0.545788 val loss: 0.696921
[Epoch 17] ogbg-molbace: 0.602678 test loss: 0.695003
[Epoch 18; Iter    23/   41] train: loss: 0.6704170
[Epoch 18] ogbg-molbace: 0.624908 val loss: 0.686728
[Epoch 18] ogbg-molbace: 0.710311 test loss: 0.690051
[Epoch 19; Iter    12/   41] train: loss: 0.6262575
[Epoch 19] ogbg-molbace: 0.668864 val loss: 0.689252
[Epoch 19] ogbg-molbace: 0.748739 test loss: 0.695514
[Epoch 20; Iter     1/   41] train: loss: 0.5714597
[Epoch 20; Iter    31/   41] train: loss: 0.5175056
[Epoch 20] ogbg-molbace: 0.743223 val loss: 0.645782
[Epoch 20] ogbg-molbace: 0.754130 test loss: 0.683948
[Epoch 21; Iter    20/   41] train: loss: 0.4823948
[Epoch 21] ogbg-molbace: 0.756777 val loss: 0.707641
[Epoch 21] ogbg-molbace: 0.783690 test loss: 0.759231
[Epoch 22; Iter     9/   41] train: loss: 0.4359258
[Epoch 22; Iter    39/   41] train: loss: 0.5209023
[Epoch 22] ogbg-molbace: 0.682418 val loss: 0.750304
[Epoch 22] ogbg-molbace: 0.795340 test loss: 0.656327
[Epoch 23; Iter    28/   41] train: loss: 0.5123437
[Epoch 23] ogbg-molbace: 0.717582 val loss: 0.717612
[Epoch 23] ogbg-molbace: 0.790123 test loss: 0.808389
[Epoch 24; Iter    17/   41] train: loss: 0.4704634
[Epoch 24] ogbg-molbace: 0.753480 val loss: 0.814639
[Epoch 24] ogbg-molbace: 0.788211 test loss: 0.828489
[Epoch 25; Iter     6/   41] train: loss: 0.4005370
[Epoch 25; Iter    36/   41] train: loss: 0.4865997
[Epoch 25] ogbg-molbace: 0.705128 val loss: 0.822906
[Epoch 25] ogbg-molbace: 0.818466 test loss: 0.733153
[Epoch 26; Iter    25/   41] train: loss: 0.3554456
[Epoch 26] ogbg-molbace: 0.707692 val loss: 0.731633
[Epoch 26] ogbg-molbace: 0.819336 test loss: 0.773209
[Epoch 27; Iter    14/   41] train: loss: 0.4871371
[Epoch 27] ogbg-molbace: 0.715018 val loss: 0.668508
[Epoch 27] ogbg-molbace: 0.802295 test loss: 0.700438
[Epoch 28; Iter     3/   41] train: loss: 0.3050030
[Epoch 28; Iter    33/   41] train: loss: 0.5473686
[Epoch 28] ogbg-molbace: 0.712088 val loss: 1.004067
[Epoch 28] ogbg-molbace: 0.793949 test loss: 0.827717
[Epoch 29; Iter    22/   41] train: loss: 0.3282694
[Epoch 29] ogbg-molbace: 0.726374 val loss: 0.781274
[Epoch 29] ogbg-molbace: 0.777778 test loss: 0.944987
[Epoch 30; Iter    11/   41] train: loss: 0.5215871
[Epoch 30; Iter    41/   41] train: loss: 0.6341766
[Epoch 30] ogbg-molbace: 0.736630 val loss: 0.822337
[Epoch 30] ogbg-molbace: 0.808207 test loss: 0.647581
[Epoch 31; Iter    30/   41] train: loss: 0.4596748
[Epoch 31] ogbg-molbace: 0.735897 val loss: 1.050927
[Epoch 31] ogbg-molbace: 0.794471 test loss: 1.204253
[Epoch 32; Iter    19/   41] train: loss: 0.5901037
[Epoch 32] ogbg-molbace: 0.725641 val loss: 0.796150
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.0/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.0_4_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.0
logdir: runs/static_noise/3DInfomax/bace/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6927290
[Epoch 1] ogbg-molbace: 0.430769 val loss: 0.693569
[Epoch 1] ogbg-molbace: 0.397148 test loss: 0.693230
[Epoch 2; Iter    19/   41] train: loss: 0.6952031
[Epoch 2] ogbg-molbace: 0.429670 val loss: 0.694659
[Epoch 2] ogbg-molbace: 0.439402 test loss: 0.693252
[Epoch 3; Iter     8/   41] train: loss: 0.6980300
[Epoch 3; Iter    38/   41] train: loss: 0.6943874
[Epoch 3] ogbg-molbace: 0.427473 val loss: 0.695573
[Epoch 3] ogbg-molbace: 0.449139 test loss: 0.693333
[Epoch 4; Iter    27/   41] train: loss: 0.6930844
[Epoch 4] ogbg-molbace: 0.435897 val loss: 0.695909
[Epoch 4] ogbg-molbace: 0.453312 test loss: 0.693467
[Epoch 5; Iter    16/   41] train: loss: 0.6954031
[Epoch 5] ogbg-molbace: 0.431136 val loss: 0.696324
[Epoch 5] ogbg-molbace: 0.449139 test loss: 0.693693
[Epoch 6; Iter     5/   41] train: loss: 0.6925275
[Epoch 6; Iter    35/   41] train: loss: 0.6933805
[Epoch 6] ogbg-molbace: 0.429670 val loss: 0.696342
[Epoch 6] ogbg-molbace: 0.461137 test loss: 0.693728
[Epoch 7; Iter    24/   41] train: loss: 0.6966487
[Epoch 7] ogbg-molbace: 0.436630 val loss: 0.696925
[Epoch 7] ogbg-molbace: 0.449313 test loss: 0.693999
[Epoch 8; Iter    13/   41] train: loss: 0.6946664
[Epoch 8] ogbg-molbace: 0.443223 val loss: 0.697179
[Epoch 8] ogbg-molbace: 0.475222 test loss: 0.693782
[Epoch 9; Iter     2/   41] train: loss: 0.6927829
[Epoch 9; Iter    32/   41] train: loss: 0.6897456
[Epoch 9] ogbg-molbace: 0.432234 val loss: 0.697993
[Epoch 9] ogbg-molbace: 0.456442 test loss: 0.694640
[Epoch 10; Iter    21/   41] train: loss: 0.6913434
[Epoch 10] ogbg-molbace: 0.435531 val loss: 0.698198
[Epoch 10] ogbg-molbace: 0.465310 test loss: 0.694824
[Epoch 11; Iter    10/   41] train: loss: 0.6940972
[Epoch 11; Iter    40/   41] train: loss: 0.6894860
[Epoch 11] ogbg-molbace: 0.439560 val loss: 0.698878
[Epoch 11] ogbg-molbace: 0.476091 test loss: 0.694951
[Epoch 12; Iter    29/   41] train: loss: 0.6934273
[Epoch 12] ogbg-molbace: 0.438462 val loss: 0.699721
[Epoch 12] ogbg-molbace: 0.478352 test loss: 0.695537
[Epoch 13; Iter    18/   41] train: loss: 0.6940166
[Epoch 13] ogbg-molbace: 0.440293 val loss: 0.700601
[Epoch 13] ogbg-molbace: 0.481829 test loss: 0.695951
[Epoch 14; Iter     7/   41] train: loss: 0.6940421
[Epoch 14; Iter    37/   41] train: loss: 0.6867384
[Epoch 14] ogbg-molbace: 0.428571 val loss: 0.701695
[Epoch 14] ogbg-molbace: 0.479743 test loss: 0.696597
[Epoch 15; Iter    26/   41] train: loss: 0.6881240
[Epoch 15] ogbg-molbace: 0.427473 val loss: 0.702663
[Epoch 15] ogbg-molbace: 0.484785 test loss: 0.696974
[Epoch 16; Iter    15/   41] train: loss: 0.6942689
[Epoch 16] ogbg-molbace: 0.432601 val loss: 0.703207
[Epoch 16] ogbg-molbace: 0.495740 test loss: 0.697538
[Epoch 17; Iter     4/   41] train: loss: 0.6927571
[Epoch 17; Iter    34/   41] train: loss: 0.6883259
[Epoch 17] ogbg-molbace: 0.528205 val loss: 0.696972
[Epoch 17] ogbg-molbace: 0.611720 test loss: 0.696209
[Epoch 18; Iter    23/   41] train: loss: 0.6474306
[Epoch 18] ogbg-molbace: 0.680586 val loss: 0.679861
[Epoch 18] ogbg-molbace: 0.743001 test loss: 0.680993
[Epoch 19; Iter    12/   41] train: loss: 0.6175016
[Epoch 19] ogbg-molbace: 0.710623 val loss: 0.657612
[Epoch 19] ogbg-molbace: 0.749609 test loss: 0.685847
[Epoch 20; Iter     1/   41] train: loss: 0.5724236
[Epoch 20; Iter    31/   41] train: loss: 0.5149022
[Epoch 20] ogbg-molbace: 0.706593 val loss: 0.714616
[Epoch 20] ogbg-molbace: 0.810642 test loss: 0.665564
[Epoch 21; Iter    20/   41] train: loss: 0.4378804
[Epoch 21] ogbg-molbace: 0.594505 val loss: 0.681031
[Epoch 21] ogbg-molbace: 0.725613 test loss: 0.689480
[Epoch 22; Iter     9/   41] train: loss: 0.5697750
[Epoch 22; Iter    39/   41] train: loss: 0.3513356
[Epoch 22] ogbg-molbace: 0.709158 val loss: 0.625033
[Epoch 22] ogbg-molbace: 0.736568 test loss: 0.837734
[Epoch 23; Iter    28/   41] train: loss: 0.4636605
[Epoch 23] ogbg-molbace: 0.720513 val loss: 0.675856
[Epoch 23] ogbg-molbace: 0.806642 test loss: 0.577301
[Epoch 24; Iter    17/   41] train: loss: 0.5607677
[Epoch 24] ogbg-molbace: 0.744689 val loss: 0.908949
[Epoch 24] ogbg-molbace: 0.785776 test loss: 0.846017
[Epoch 25; Iter     6/   41] train: loss: 0.4998991
[Epoch 25; Iter    36/   41] train: loss: 0.4350850
[Epoch 25] ogbg-molbace: 0.710256 val loss: 0.839390
[Epoch 25] ogbg-molbace: 0.759520 test loss: 1.008497
[Epoch 26; Iter    25/   41] train: loss: 0.3897315
[Epoch 26] ogbg-molbace: 0.700733 val loss: 0.929364
[Epoch 26] ogbg-molbace: 0.800035 test loss: 0.858343
[Epoch 27; Iter    14/   41] train: loss: 0.3644746
[Epoch 27] ogbg-molbace: 0.643223 val loss: 0.938710
[Epoch 27] ogbg-molbace: 0.778821 test loss: 0.784683
[Epoch 28; Iter     3/   41] train: loss: 0.4022822
[Epoch 28; Iter    33/   41] train: loss: 0.4915125
[Epoch 28] ogbg-molbace: 0.737729 val loss: 0.679616
[Epoch 28] ogbg-molbace: 0.777952 test loss: 0.848452
[Epoch 29; Iter    22/   41] train: loss: 0.6041244
[Epoch 29] ogbg-molbace: 0.673993 val loss: 0.902325
[Epoch 29] ogbg-molbace: 0.780908 test loss: 0.990926
[Epoch 30; Iter    11/   41] train: loss: 0.3676251
[Epoch 30; Iter    41/   41] train: loss: 0.2871022
[Epoch 30] ogbg-molbace: 0.663370 val loss: 0.860227
[Epoch 30] ogbg-molbace: 0.780212 test loss: 0.856353
[Epoch 31; Iter    30/   41] train: loss: 0.3972546
[Epoch 31] ogbg-molbace: 0.665201 val loss: 0.890847
[Epoch 31] ogbg-molbace: 0.773605 test loss: 0.887777
[Epoch 32; Iter    19/   41] train: loss: 0.3453067
[Epoch 32] ogbg-molbace: 0.729304 val loss: 0.879522
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/bace/noise=0.0/PNA_ogbg-molbace_3DInfomax_bace_static_noise=0.0_6_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_static_noise=0.0
logdir: runs/static_noise/3DInfomax/bace/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6937714
[Epoch 1] ogbg-molbace: 0.524542 val loss: 0.692766
[Epoch 1] ogbg-molbace: 0.444966 test loss: 0.693076
[Epoch 2; Iter    19/   41] train: loss: 0.6916298
[Epoch 2] ogbg-molbace: 0.527473 val loss: 0.691610
[Epoch 2] ogbg-molbace: 0.449661 test loss: 0.693327
[Epoch 3; Iter     8/   41] train: loss: 0.6901196
[Epoch 3; Iter    38/   41] train: loss: 0.6973189
[Epoch 3] ogbg-molbace: 0.525275 val loss: 0.691448
[Epoch 3] ogbg-molbace: 0.435750 test loss: 0.694144
[Epoch 4; Iter    27/   41] train: loss: 0.6946090
[Epoch 4] ogbg-molbace: 0.526007 val loss: 0.691496
[Epoch 4] ogbg-molbace: 0.435229 test loss: 0.694053
[Epoch 5; Iter    16/   41] train: loss: 0.6932670
[Epoch 5] ogbg-molbace: 0.521978 val loss: 0.691895
[Epoch 5] ogbg-molbace: 0.432273 test loss: 0.694522
[Epoch 6; Iter     5/   41] train: loss: 0.6914856
[Epoch 6; Iter    35/   41] train: loss: 0.6918142
[Epoch 6] ogbg-molbace: 0.524176 val loss: 0.692430
[Epoch 6] ogbg-molbace: 0.440097 test loss: 0.694714
[Epoch 7; Iter    24/   41] train: loss: 0.6926837
[Epoch 7] ogbg-molbace: 0.523810 val loss: 0.692847
[Epoch 7] ogbg-molbace: 0.447748 test loss: 0.694696
[Epoch 8; Iter    13/   41] train: loss: 0.6906585
[Epoch 8] ogbg-molbace: 0.523077 val loss: 0.693199
[Epoch 8] ogbg-molbace: 0.443749 test loss: 0.694918
[Epoch 9; Iter     2/   41] train: loss: 0.6888597
[Epoch 9; Iter    32/   41] train: loss: 0.6915891
[Epoch 9] ogbg-molbace: 0.523443 val loss: 0.693622
[Epoch 9] ogbg-molbace: 0.440097 test loss: 0.695501
[Epoch 10; Iter    21/   41] train: loss: 0.6943637
[Epoch 10] ogbg-molbace: 0.520879 val loss: 0.694820
[Epoch 10] ogbg-molbace: 0.456268 test loss: 0.695610
[Epoch 11; Iter    10/   41] train: loss: 0.6918271
[Epoch 11; Iter    40/   41] train: loss: 0.6911523
[Epoch 11] ogbg-molbace: 0.517949 val loss: 0.695369
[Epoch 11] ogbg-molbace: 0.453486 test loss: 0.696310
[Epoch 12; Iter    29/   41] train: loss: 0.6941935
[Epoch 12] ogbg-molbace: 0.525641 val loss: 0.696124
[Epoch 12] ogbg-molbace: 0.461485 test loss: 0.696371
[Epoch 13; Iter    18/   41] train: loss: 0.6933154
[Epoch 13] ogbg-molbace: 0.526007 val loss: 0.696821
[Epoch 13] ogbg-molbace: 0.466701 test loss: 0.697060
[Epoch 14; Iter     7/   41] train: loss: 0.6868996
[Epoch 14; Iter    37/   41] train: loss: 0.6904666
[Epoch 14] ogbg-molbace: 0.529304 val loss: 0.697591
[Epoch 14] ogbg-molbace: 0.466006 test loss: 0.697458
[Epoch 15; Iter    26/   41] train: loss: 0.6860691
[Epoch 15] ogbg-molbace: 0.531868 val loss: 0.698654
[Epoch 15] ogbg-molbace: 0.458355 test loss: 0.698198
[Epoch 16; Iter    15/   41] train: loss: 0.6902760
[Epoch 16] ogbg-molbace: 0.528938 val loss: 0.699825
[Epoch 16] ogbg-molbace: 0.479569 test loss: 0.698691
[Epoch 17; Iter     4/   41] train: loss: 0.6872615
[Epoch 17; Iter    34/   41] train: loss: 0.6916454
[Epoch 17] ogbg-molbace: 0.617949 val loss: 0.693574
[Epoch 17] ogbg-molbace: 0.636063 test loss: 0.697725
[Epoch 18; Iter    23/   41] train: loss: 0.6777231
[Epoch 18] ogbg-molbace: 0.742125 val loss: 0.699685
[Epoch 18] ogbg-molbace: 0.732394 test loss: 0.698114
[Epoch 19; Iter    12/   41] train: loss: 0.6326268
[Epoch 19] ogbg-molbace: 0.681685 val loss: 0.671019
[Epoch 19] ogbg-molbace: 0.788906 test loss: 0.638630
[Epoch 20; Iter     1/   41] train: loss: 0.5702552
[Epoch 20; Iter    31/   41] train: loss: 0.5351404
[Epoch 20] ogbg-molbace: 0.621612 val loss: 0.882496
[Epoch 20] ogbg-molbace: 0.699704 test loss: 0.739904
[Epoch 21; Iter    20/   41] train: loss: 0.5623946
[Epoch 21] ogbg-molbace: 0.617216 val loss: 0.876717
[Epoch 21] ogbg-molbace: 0.707703 test loss: 0.928901
[Epoch 22; Iter     9/   41] train: loss: 0.5133814
[Epoch 22; Iter    39/   41] train: loss: 0.4075597
[Epoch 22] ogbg-molbace: 0.721612 val loss: 0.833885
[Epoch 22] ogbg-molbace: 0.792036 test loss: 0.695066
[Epoch 23; Iter    28/   41] train: loss: 0.4052437
[Epoch 23] ogbg-molbace: 0.730769 val loss: 0.731228
[Epoch 23] ogbg-molbace: 0.800904 test loss: 0.732020
[Epoch 24; Iter    17/   41] train: loss: 0.3993594
[Epoch 24] ogbg-molbace: 0.695604 val loss: 0.741853
[Epoch 24] ogbg-molbace: 0.754651 test loss: 0.869904
[Epoch 25; Iter     6/   41] train: loss: 0.4191405
[Epoch 25; Iter    36/   41] train: loss: 0.3267623
[Epoch 25] ogbg-molbace: 0.723077 val loss: 0.647056
[Epoch 25] ogbg-molbace: 0.817945 test loss: 0.655226
[Epoch 26; Iter    25/   41] train: loss: 0.4697115
[Epoch 26] ogbg-molbace: 0.713187 val loss: 0.642928
[Epoch 26] ogbg-molbace: 0.750652 test loss: 0.733972
[Epoch 27; Iter    14/   41] train: loss: 0.6136090
[Epoch 27] ogbg-molbace: 0.677289 val loss: 0.804564
[Epoch 27] ogbg-molbace: 0.764041 test loss: 0.824575
[Epoch 28; Iter     3/   41] train: loss: 0.4757843
[Epoch 28; Iter    33/   41] train: loss: 0.4417647
[Epoch 28] ogbg-molbace: 0.661905 val loss: 0.913401
[Epoch 28] ogbg-molbace: 0.809598 test loss: 0.880841
[Epoch 29; Iter    22/   41] train: loss: 0.4167109
[Epoch 29] ogbg-molbace: 0.742857 val loss: 0.795708
[Epoch 29] ogbg-molbace: 0.793775 test loss: 0.782244
[Epoch 30; Iter    11/   41] train: loss: 0.3964409
[Epoch 30; Iter    41/   41] train: loss: 0.5273265
[Epoch 30] ogbg-molbace: 0.730403 val loss: 0.715148
[Epoch 30] ogbg-molbace: 0.798470 test loss: 0.822592
[Epoch 31; Iter    30/   41] train: loss: 0.4081997
[Epoch 31] ogbg-molbace: 0.636264 val loss: 1.030670
[Epoch 31] ogbg-molbace: 0.745957 test loss: 1.247607
[Epoch 32; Iter    19/   41] train: loss: 0.4636970
[Epoch 32] ogbg-molbace: 0.728205 val loss: 0.950283
[Epoch 32] ogbg-molbace: 0.736394 test loss: 0.984757
[Epoch 33; Iter     8/   41] train: loss: 0.6377862
[Epoch 33; Iter    38/   41] train: loss: 0.6667191
[Epoch 33] ogbg-molbace: 0.702198 val loss: 1.007184
[Epoch 33] ogbg-molbace: 0.727526 test loss: 1.279731
[Epoch 34; Iter    27/   41] train: loss: 0.4779050
[Epoch 34] ogbg-molbace: 0.706227 val loss: 0.869250
[Epoch 34] ogbg-molbace: 0.747870 test loss: 1.089985
[Epoch 35; Iter    16/   41] train: loss: 0.4634216
[Epoch 35] ogbg-molbace: 0.676923 val loss: 0.934658
[Epoch 35] ogbg-molbace: 0.771692 test loss: 1.094678
[Epoch 36; Iter     5/   41] train: loss: 0.3938408
[Epoch 36; Iter    35/   41] train: loss: 0.4450138
[Epoch 36] ogbg-molbace: 0.600733 val loss: 0.983337
[Epoch 36] ogbg-molbace: 0.705269 test loss: 1.234452
[Epoch 37; Iter    24/   41] train: loss: 0.4186606
[Epoch 37] ogbg-molbace: 0.619414 val loss: 1.221944
[Epoch 37] ogbg-molbace: 0.699009 test loss: 1.687000
[Epoch 38; Iter    13/   41] train: loss: 0.4106132
[Epoch 38] ogbg-molbace: 0.681685 val loss: 0.961468
[Epoch 38] ogbg-molbace: 0.678317 test loss: 1.334975
[Epoch 39; Iter     2/   41] train: loss: 0.4249996
[Epoch 39; Iter    32/   41] train: loss: 0.4969897
[Epoch 39] ogbg-molbace: 0.757509 val loss: 0.818369
[Epoch 39] ogbg-molbace: 0.742306 test loss: 0.945383
[Epoch 40; Iter    21/   41] train: loss: 0.2062355
[Epoch 40] ogbg-molbace: 0.743590 val loss: 0.973007
[Epoch 40] ogbg-molbace: 0.764737 test loss: 1.195853
[Epoch 41; Iter    10/   41] train: loss: 0.5030767
[Epoch 41; Iter    40/   41] train: loss: 0.5264314
[Epoch 41] ogbg-molbace: 0.652381 val loss: 1.252910
[Epoch 41] ogbg-molbace: 0.737437 test loss: 1.114631
[Epoch 42; Iter    29/   41] train: loss: 0.4417063
[Epoch 42] ogbg-molbace: 0.697802 val loss: 0.947894
[Epoch 42] ogbg-molbace: 0.801426 test loss: 0.697159
[Epoch 43; Iter    18/   41] train: loss: 0.2102912
[Epoch 43] ogbg-molbace: 0.767399 val loss: 0.717036
[Epoch 43] ogbg-molbace: 0.826465 test loss: 0.832360
[Epoch 44; Iter     7/   41] train: loss: 0.1494273
[Epoch 44; Iter    37/   41] train: loss: 0.2808975
[Epoch 44] ogbg-molbace: 0.695238 val loss: 0.949710
[Epoch 44] ogbg-molbace: 0.721788 test loss: 1.583002
[Epoch 45; Iter    26/   41] train: loss: 0.3243626
[Epoch 45] ogbg-molbace: 0.728205 val loss: 1.035564
[Epoch 45] ogbg-molbace: 0.760563 test loss: 1.194204
[Epoch 46; Iter    15/   41] train: loss: 0.1167987
[Epoch 46] ogbg-molbace: 0.720513 val loss: 1.090074
[Epoch 46] ogbg-molbace: 0.742480 test loss: 1.571508
[Epoch 47; Iter     4/   41] train: loss: 0.1608632
[Epoch 47; Iter    34/   41] train: loss: 0.1603876
[Epoch 47] ogbg-molbace: 0.749451 val loss: 0.995800
[Epoch 47] ogbg-molbace: 0.790471 test loss: 1.207621
[Epoch 48; Iter    23/   41] train: loss: 0.3241743
[Epoch 48] ogbg-molbace: 0.683883 val loss: 1.547346
[Epoch 48] ogbg-molbace: 0.767519 test loss: 1.301388
[Epoch 49; Iter    12/   41] train: loss: 0.1868747
[Epoch 49] ogbg-molbace: 0.762637 val loss: 0.751504
[Epoch 49] ogbg-molbace: 0.776734 test loss: 1.311228
[Epoch 50; Iter     1/   41] train: loss: 0.0801531
[Epoch 50; Iter    31/   41] train: loss: 0.0857275
[Epoch 50] ogbg-molbace: 0.715751 val loss: 1.469995
[Epoch 50] ogbg-molbace: 0.813250 test loss: 0.973857
[Epoch 51; Iter    20/   41] train: loss: 0.2506418
[Epoch 51] ogbg-molbace: 0.693407 val loss: 1.312081
[Epoch 51] ogbg-molbace: 0.792384 test loss: 0.877383
[Epoch 52; Iter     9/   41] train: loss: 0.2843908
[Epoch 52; Iter    39/   41] train: loss: 0.0887198
[Epoch 52] ogbg-molbace: 0.736996 val loss: 1.155965
[Epoch 52] ogbg-molbace: 0.780038 test loss: 1.398095
[Epoch 53; Iter    28/   41] train: loss: 0.0215190
[Epoch 53] ogbg-molbace: 0.796703 val loss: 1.175716
[Epoch 53] ogbg-molbace: 0.767345 test loss: 1.456708
[Epoch 54; Iter    17/   41] train: loss: 0.0086583
[Epoch 54] ogbg-molbace: 0.746886 val loss: 1.030642
[Epoch 54] ogbg-molbace: 0.768910 test loss: 1.432907
[Epoch 55; Iter     6/   41] train: loss: 0.0092844
[Epoch 55; Iter    36/   41] train: loss: 0.0223441
[Epoch 55] ogbg-molbace: 0.713553 val loss: 1.349597
[Epoch 55] ogbg-molbace: 0.707877 test loss: 1.603868
[Epoch 56; Iter    25/   41] train: loss: 0.0098103
[Epoch 56] ogbg-molbace: 0.719414 val loss: 1.003706
[Epoch 56] ogbg-molbace: 0.792210 test loss: 0.893366
[Epoch 57; Iter    14/   41] train: loss: 0.0661351
[Epoch 57] ogbg-molbace: 0.706227 val loss: 1.324694
[Epoch 57] ogbg-molbace: 0.784907 test loss: 1.131070
[Epoch 58; Iter     3/   41] train: loss: 0.0615402
[Epoch 58; Iter    33/   41] train: loss: 0.0274426
[Epoch 58] ogbg-molbace: 0.720879 val loss: 1.382912
[Epoch 58] ogbg-molbace: 0.789428 test loss: 1.536829
[Epoch 59; Iter    22/   41] train: loss: 0.0360865
[Epoch 59] ogbg-molbace: 0.675092 val loss: 2.163522
[Epoch 59] ogbg-molbace: 0.704573 test loss: 2.670315
[Epoch 60; Iter    11/   41] train: loss: 0.0257747
[Epoch 60; Iter    41/   41] train: loss: 0.0457217
[Epoch 60] ogbg-molbace: 0.755678 val loss: 1.719293
[Epoch 60] ogbg-molbace: 0.731699 test loss: 2.501241
[Epoch 61; Iter    30/   41] train: loss: 0.0676735
[Epoch 61] ogbg-molbace: 0.667399 val loss: 2.351110
[Epoch 61] ogbg-molbace: 0.663189 test loss: 2.462579
[Epoch 62; Iter    19/   41] train: loss: 0.1568036
[Epoch 62] ogbg-molbace: 0.761172 val loss: 1.265163
[Epoch 62] ogbg-molbace: 0.815858 test loss: 1.602606
[Epoch 63; Iter     8/   41] train: loss: 0.1062663
[Epoch 63; Iter    38/   41] train: loss: 0.2418175
[Epoch 63] ogbg-molbace: 0.701099 val loss: 2.183600
[Epoch 63] ogbg-molbace: 0.774300 test loss: 1.603594
[Epoch 64; Iter    27/   41] train: loss: 0.1089338
[Epoch 64] ogbg-molbace: 0.712821 val loss: 1.383833
[Epoch 64] ogbg-molbace: 0.780038 test loss: 2.128834
[Epoch 65; Iter    16/   41] train: loss: 0.1625230
[Epoch 65] ogbg-molbace: 0.760073 val loss: 0.947747
[Epoch 65] ogbg-molbace: 0.790471 test loss: 1.413448
[Epoch 66; Iter     5/   41] train: loss: 0.1076292
[Epoch 66; Iter    35/   41] train: loss: 0.0680667
[Epoch 66] ogbg-molbace: 0.687912 val loss: 1.120612
[Epoch 66] ogbg-molbace: 0.778126 test loss: 1.529096
[Epoch 67; Iter    24/   41] train: loss: 0.0261346
[Epoch 67] ogbg-molbace: 0.690476 val loss: 1.046883
[Epoch 67] ogbg-molbace: 0.798470 test loss: 1.670402
[Epoch 68; Iter    13/   41] train: loss: 0.0101213
[Epoch 68] ogbg-molbace: 0.690842 val loss: 1.104639
[Epoch 68] ogbg-molbace: 0.789254 test loss: 1.573290
[Epoch 69; Iter     2/   41] train: loss: 0.0063692
[Epoch 69; Iter    32/   41] train: loss: 0.0034392
[Epoch 69] ogbg-molbace: 0.720147 val loss: 1.047221
[Epoch 69] ogbg-molbace: 0.789428 test loss: 1.740262
[Epoch 70; Iter    21/   41] train: loss: 0.0057694
[Epoch 70] ogbg-molbace: 0.689744 val loss: 1.231891
[Epoch 70] ogbg-molbace: 0.777952 test loss: 1.791063
[Epoch 71; Iter    10/   41] train: loss: 0.0030777
[Epoch 71; Iter    40/   41] train: loss: 0.0069934
[Epoch 71] ogbg-molbace: 0.698168 val loss: 1.128455
[Epoch 71] ogbg-molbace: 0.789602 test loss: 1.509884
[Epoch 72; Iter    29/   41] train: loss: 0.0031913
[Epoch 72] ogbg-molbace: 0.684249 val loss: 1.453989
[Epoch 72] ogbg-molbace: 0.778126 test loss: 1.605763
[Epoch 73; Iter    18/   41] train: loss: 0.0020456
[Epoch 73] ogbg-molbace: 0.703297 val loss: 1.392219
[Epoch 73] ogbg-molbace: 0.775691 test loss: 1.765356
[Epoch 74; Iter     7/   41] train: loss: 0.0065741
[Epoch 74; Iter    37/   41] train: loss: 0.0045290
[Epoch 74] ogbg-molbace: 0.721978 val loss: 1.185131
[Epoch 74] ogbg-molbace: 0.769258 test loss: 1.931063
[Epoch 75; Iter    26/   41] train: loss: 0.0213903
[Epoch 75] ogbg-molbace: 0.671429 val loss: 1.189665
[Epoch 75] ogbg-molbace: 0.721266 test loss: 2.240550
[Epoch 76; Iter    15/   41] train: loss: 0.0017219
[Epoch 76] ogbg-molbace: 0.712821 val loss: 1.408517
[Epoch 76] ogbg-molbace: 0.760216 test loss: 2.351500
[Epoch 77; Iter     4/   41] train: loss: 0.0404770
[Epoch 77; Iter    34/   41] train: loss: 0.0088457
[Epoch 77] ogbg-molbace: 0.684982 val loss: 1.297231
[Epoch 77] ogbg-molbace: 0.751521 test loss: 2.092793
[Epoch 78; Iter    23/   41] train: loss: 0.0071223
[Epoch 32] ogbg-molbace: 0.689271 test loss: 1.036541
[Epoch 33; Iter     8/   41] train: loss: 0.5778430
[Epoch 33; Iter    38/   41] train: loss: 0.6498004
[Epoch 33] ogbg-molbace: 0.727106 val loss: 0.945391
[Epoch 33] ogbg-molbace: 0.743871 test loss: 1.018420
[Epoch 34; Iter    27/   41] train: loss: 0.5281929
[Epoch 34] ogbg-molbace: 0.653846 val loss: 0.807399
[Epoch 34] ogbg-molbace: 0.744392 test loss: 0.894297
[Epoch 35; Iter    16/   41] train: loss: 0.4921924
[Epoch 35] ogbg-molbace: 0.630037 val loss: 0.549299
[Epoch 35] ogbg-molbace: 0.691358 test loss: 0.858664
[Epoch 36; Iter     5/   41] train: loss: 0.3773125
[Epoch 36; Iter    35/   41] train: loss: 0.4151640
[Epoch 36] ogbg-molbace: 0.672161 val loss: 0.599773
[Epoch 36] ogbg-molbace: 0.786646 test loss: 0.766032
[Epoch 37; Iter    24/   41] train: loss: 0.4616272
[Epoch 37] ogbg-molbace: 0.630037 val loss: 0.826909
[Epoch 37] ogbg-molbace: 0.724917 test loss: 1.016773
[Epoch 38; Iter    13/   41] train: loss: 0.5107586
[Epoch 38] ogbg-molbace: 0.637363 val loss: 1.285290
[Epoch 38] ogbg-molbace: 0.787341 test loss: 0.979922
[Epoch 39; Iter     2/   41] train: loss: 0.4166029
[Epoch 39; Iter    32/   41] train: loss: 0.3217009
[Epoch 39] ogbg-molbace: 0.706960 val loss: 1.206064
[Epoch 39] ogbg-molbace: 0.764041 test loss: 1.448284
[Epoch 40; Iter    21/   41] train: loss: 0.2829663
[Epoch 40] ogbg-molbace: 0.644689 val loss: 1.277653
[Epoch 40] ogbg-molbace: 0.754825 test loss: 1.047035
[Epoch 41; Iter    10/   41] train: loss: 0.4178667
[Epoch 41; Iter    40/   41] train: loss: 0.6196948
[Epoch 41] ogbg-molbace: 0.678388 val loss: 0.712863
[Epoch 41] ogbg-molbace: 0.764563 test loss: 0.894099
[Epoch 42; Iter    29/   41] train: loss: 0.2392623
[Epoch 42] ogbg-molbace: 0.685348 val loss: 0.814662
[Epoch 42] ogbg-molbace: 0.737263 test loss: 1.290514
[Epoch 43; Iter    18/   41] train: loss: 0.1625859
[Epoch 43] ogbg-molbace: 0.686813 val loss: 1.705008
[Epoch 43] ogbg-molbace: 0.658494 test loss: 12.929924
[Epoch 44; Iter     7/   41] train: loss: 0.2967535
[Epoch 44; Iter    37/   41] train: loss: 0.4927967
[Epoch 44] ogbg-molbace: 0.664103 val loss: 1.402477
[Epoch 44] ogbg-molbace: 0.733959 test loss: 1.447921
[Epoch 45; Iter    26/   41] train: loss: 0.1695716
[Epoch 45] ogbg-molbace: 0.638462 val loss: 1.746416
[Epoch 45] ogbg-molbace: 0.755347 test loss: 1.607728
[Epoch 46; Iter    15/   41] train: loss: 0.1855714
[Epoch 46] ogbg-molbace: 0.686447 val loss: 0.845138
[Epoch 46] ogbg-molbace: 0.753608 test loss: 1.376642
[Epoch 47; Iter     4/   41] train: loss: 0.1271756
[Epoch 47; Iter    34/   41] train: loss: 0.0258847
[Epoch 47] ogbg-molbace: 0.677289 val loss: 1.308530
[Epoch 47] ogbg-molbace: 0.751869 test loss: 1.651751
[Epoch 48; Iter    23/   41] train: loss: 0.0329215
[Epoch 48] ogbg-molbace: 0.650916 val loss: 1.261395
[Epoch 48] ogbg-molbace: 0.742132 test loss: 1.519081
[Epoch 49; Iter    12/   41] train: loss: 0.1174055
[Epoch 49] ogbg-molbace: 0.671429 val loss: 1.547821
[Epoch 49] ogbg-molbace: 0.737263 test loss: 3.556913
[Epoch 50; Iter     1/   41] train: loss: 0.0604299
[Epoch 50; Iter    31/   41] train: loss: 0.0877237
[Epoch 50] ogbg-molbace: 0.633700 val loss: 1.342717
[Epoch 50] ogbg-molbace: 0.728047 test loss: 1.823812
[Epoch 51; Iter    20/   41] train: loss: 0.1013882
[Epoch 51] ogbg-molbace: 0.645421 val loss: 1.479374
[Epoch 51] ogbg-molbace: 0.738132 test loss: 2.031616
[Epoch 52; Iter     9/   41] train: loss: 0.2296826
[Epoch 52; Iter    39/   41] train: loss: 0.0478617
[Epoch 52] ogbg-molbace: 0.663370 val loss: 1.504055
[Epoch 52] ogbg-molbace: 0.761954 test loss: 1.644133
[Epoch 53; Iter    28/   41] train: loss: 0.0290174
[Epoch 53] ogbg-molbace: 0.631136 val loss: 1.925569
[Epoch 53] ogbg-molbace: 0.724570 test loss: 2.225476
[Epoch 54; Iter    17/   41] train: loss: 0.0364178
[Epoch 54] ogbg-molbace: 0.642491 val loss: 1.902075
[Epoch 54] ogbg-molbace: 0.737785 test loss: 2.416431
[Epoch 55; Iter     6/   41] train: loss: 0.0129093
[Epoch 55; Iter    36/   41] train: loss: 0.0377141
[Epoch 55] ogbg-molbace: 0.635897 val loss: 1.553737
[Epoch 55] ogbg-molbace: 0.713615 test loss: 1.512049
[Epoch 56; Iter    25/   41] train: loss: 0.0123172
[Epoch 56] ogbg-molbace: 0.634799 val loss: 1.973546
[Epoch 56] ogbg-molbace: 0.776908 test loss: 2.336882
[Epoch 57; Iter    14/   41] train: loss: 0.0665226
[Epoch 57] ogbg-molbace: 0.660440 val loss: 1.535127
[Epoch 57] ogbg-molbace: 0.763519 test loss: 1.806481
[Epoch 58; Iter     3/   41] train: loss: 0.0203654
[Epoch 58; Iter    33/   41] train: loss: 0.1908556
[Epoch 58] ogbg-molbace: 0.687546 val loss: 1.842509
[Epoch 58] ogbg-molbace: 0.698313 test loss: 2.115919
[Epoch 59; Iter    22/   41] train: loss: 0.1014649
[Epoch 59] ogbg-molbace: 0.694505 val loss: 0.993850
[Epoch 59] ogbg-molbace: 0.722831 test loss: 1.927602
[Epoch 60; Iter    11/   41] train: loss: 0.0360889
[Epoch 60; Iter    41/   41] train: loss: 0.1321592
[Epoch 60] ogbg-molbace: 0.613187 val loss: 1.537256
[Epoch 60] ogbg-molbace: 0.705616 test loss: 2.250670
[Epoch 61; Iter    30/   41] train: loss: 0.0436743
[Epoch 61] ogbg-molbace: 0.633333 val loss: 2.131604
[Epoch 61] ogbg-molbace: 0.739350 test loss: 4.234465
[Epoch 62; Iter    19/   41] train: loss: 0.0254626
[Epoch 62] ogbg-molbace: 0.634799 val loss: 2.061392
[Epoch 62] ogbg-molbace: 0.760563 test loss: 2.502778
[Epoch 63; Iter     8/   41] train: loss: 0.0119103
[Epoch 63; Iter    38/   41] train: loss: 0.0269091
[Epoch 63] ogbg-molbace: 0.648352 val loss: 2.028996
[Epoch 63] ogbg-molbace: 0.747174 test loss: 2.083399
[Epoch 64; Iter    27/   41] train: loss: 0.0146267
[Epoch 64] ogbg-molbace: 0.654579 val loss: 1.828166
[Epoch 64] ogbg-molbace: 0.719875 test loss: 2.736528
[Epoch 65; Iter    16/   41] train: loss: 0.0109586
[Epoch 65] ogbg-molbace: 0.628938 val loss: 2.132954
[Epoch 65] ogbg-molbace: 0.707181 test loss: 2.771125
[Epoch 66; Iter     5/   41] train: loss: 0.0016771
[Epoch 66; Iter    35/   41] train: loss: 0.0060763
[Epoch 66] ogbg-molbace: 0.662271 val loss: 2.017084
[Epoch 66] ogbg-molbace: 0.746827 test loss: 2.438631
[Epoch 67; Iter    24/   41] train: loss: 0.0106187
[Epoch 67] ogbg-molbace: 0.658974 val loss: 1.901191
[Epoch 67] ogbg-molbace: 0.741089 test loss: 2.260942
[Epoch 68; Iter    13/   41] train: loss: 0.0050951
[Epoch 68] ogbg-molbace: 0.598535 val loss: 1.859353
[Epoch 68] ogbg-molbace: 0.686837 test loss: 2.774783
[Epoch 69; Iter     2/   41] train: loss: 0.0110634
[Epoch 69; Iter    32/   41] train: loss: 0.0043944
[Epoch 69] ogbg-molbace: 0.667399 val loss: 2.142615
[Epoch 69] ogbg-molbace: 0.758825 test loss: 2.511026
[Epoch 70; Iter    21/   41] train: loss: 0.0289170
[Epoch 70] ogbg-molbace: 0.665934 val loss: 2.275097
[Epoch 70] ogbg-molbace: 0.676404 test loss: 3.222281
[Epoch 71; Iter    10/   41] train: loss: 0.0047390
[Epoch 71; Iter    40/   41] train: loss: 0.0830538
[Epoch 71] ogbg-molbace: 0.601465 val loss: 2.275632
[Epoch 71] ogbg-molbace: 0.745957 test loss: 2.453524
[Epoch 72; Iter    29/   41] train: loss: 0.0039668
[Epoch 72] ogbg-molbace: 0.615385 val loss: 2.742157
[Epoch 72] ogbg-molbace: 0.728221 test loss: 2.763201
[Epoch 73; Iter    18/   41] train: loss: 0.0017273
[Epoch 73] ogbg-molbace: 0.589744 val loss: 3.389950
[Epoch 73] ogbg-molbace: 0.688576 test loss: 3.919878
[Epoch 74; Iter     7/   41] train: loss: 0.0259657
[Epoch 74; Iter    37/   41] train: loss: 0.0197037
[Epoch 74] ogbg-molbace: 0.637363 val loss: 2.565728
[Epoch 74] ogbg-molbace: 0.717788 test loss: 1.780675
[Epoch 75; Iter    26/   41] train: loss: 0.1539021
[Epoch 75] ogbg-molbace: 0.705128 val loss: 2.105974
[Epoch 75] ogbg-molbace: 0.693097 test loss: 2.634384
[Epoch 76; Iter    15/   41] train: loss: 0.0952009
[Epoch 76] ogbg-molbace: 0.659341 val loss: 2.963654
[Epoch 76] ogbg-molbace: 0.764215 test loss: 2.468617
[Epoch 77; Iter     4/   41] train: loss: 0.1151779
[Epoch 77; Iter    34/   41] train: loss: 0.0071997
[Epoch 77] ogbg-molbace: 0.635897 val loss: 4.131019
[Epoch 77] ogbg-molbace: 0.680056 test loss: 5.555339
[Epoch 78; Iter    23/   41] train: loss: 0.0692009
[Epoch 32] ogbg-molbace: 0.754999 test loss: 1.041566
[Epoch 33; Iter     8/   41] train: loss: 0.3694091
[Epoch 33; Iter    38/   41] train: loss: 0.4697551
[Epoch 33] ogbg-molbace: 0.712454 val loss: 0.816209
[Epoch 33] ogbg-molbace: 0.784733 test loss: 1.103228
[Epoch 34; Iter    27/   41] train: loss: 0.5765987
[Epoch 34] ogbg-molbace: 0.649817 val loss: 0.892443
[Epoch 34] ogbg-molbace: 0.697618 test loss: 1.289272
[Epoch 35; Iter    16/   41] train: loss: 0.3437955
[Epoch 35] ogbg-molbace: 0.585348 val loss: 0.737829
[Epoch 35] ogbg-molbace: 0.625109 test loss: 0.887437
[Epoch 36; Iter     5/   41] train: loss: 0.4113585
[Epoch 36; Iter    35/   41] train: loss: 0.4413822
[Epoch 36] ogbg-molbace: 0.732234 val loss: 0.742866
[Epoch 36] ogbg-molbace: 0.787515 test loss: 0.763283
[Epoch 37; Iter    24/   41] train: loss: 0.2573081
[Epoch 37] ogbg-molbace: 0.691209 val loss: 0.820015
[Epoch 37] ogbg-molbace: 0.774126 test loss: 0.885468
[Epoch 38; Iter    13/   41] train: loss: 0.2712724
[Epoch 38] ogbg-molbace: 0.692674 val loss: 1.429476
[Epoch 38] ogbg-molbace: 0.698313 test loss: 1.839297
[Epoch 39; Iter     2/   41] train: loss: 0.4499092
[Epoch 39; Iter    32/   41] train: loss: 0.2393580
[Epoch 39] ogbg-molbace: 0.700000 val loss: 0.735710
[Epoch 39] ogbg-molbace: 0.824378 test loss: 0.780308
[Epoch 40; Iter    21/   41] train: loss: 0.2024228
[Epoch 40] ogbg-molbace: 0.694139 val loss: 1.345061
[Epoch 40] ogbg-molbace: 0.762476 test loss: 1.676453
[Epoch 41; Iter    10/   41] train: loss: 0.3264076
[Epoch 41; Iter    40/   41] train: loss: 0.2571221
[Epoch 41] ogbg-molbace: 0.697802 val loss: 1.072674
[Epoch 41] ogbg-molbace: 0.734481 test loss: 0.740344
[Epoch 42; Iter    29/   41] train: loss: 0.3831776
[Epoch 42] ogbg-molbace: 0.652747 val loss: 1.482758
[Epoch 42] ogbg-molbace: 0.758651 test loss: 1.047232
[Epoch 43; Iter    18/   41] train: loss: 0.1446771
[Epoch 43] ogbg-molbace: 0.689377 val loss: 0.764618
[Epoch 43] ogbg-molbace: 0.816728 test loss: 1.063555
[Epoch 44; Iter     7/   41] train: loss: 0.2951640
[Epoch 44; Iter    37/   41] train: loss: 0.3029805
[Epoch 44] ogbg-molbace: 0.650549 val loss: 1.561962
[Epoch 44] ogbg-molbace: 0.790819 test loss: 1.836343
[Epoch 45; Iter    26/   41] train: loss: 0.2471765
[Epoch 45] ogbg-molbace: 0.750549 val loss: 1.098719
[Epoch 45] ogbg-molbace: 0.813076 test loss: 1.380288
[Epoch 46; Iter    15/   41] train: loss: 0.2535213
[Epoch 46] ogbg-molbace: 0.662637 val loss: 0.968428
[Epoch 46] ogbg-molbace: 0.789080 test loss: 0.739478
[Epoch 47; Iter     4/   41] train: loss: 0.1138952
[Epoch 47; Iter    34/   41] train: loss: 0.1065956
[Epoch 47] ogbg-molbace: 0.697070 val loss: 1.322595
[Epoch 47] ogbg-molbace: 0.791688 test loss: 1.321564
[Epoch 48; Iter    23/   41] train: loss: 0.1630214
[Epoch 48] ogbg-molbace: 0.616850 val loss: 1.357174
[Epoch 48] ogbg-molbace: 0.681099 test loss: 1.697495
[Epoch 49; Iter    12/   41] train: loss: 0.0750005
[Epoch 49] ogbg-molbace: 0.654212 val loss: 1.751590
[Epoch 49] ogbg-molbace: 0.794297 test loss: 0.915296
[Epoch 50; Iter     1/   41] train: loss: 0.0884711
[Epoch 50; Iter    31/   41] train: loss: 0.0747022
[Epoch 50] ogbg-molbace: 0.746520 val loss: 1.491262
[Epoch 50] ogbg-molbace: 0.815684 test loss: 0.873126
[Epoch 51; Iter    20/   41] train: loss: 0.0820493
[Epoch 51] ogbg-molbace: 0.683883 val loss: 1.602695
[Epoch 51] ogbg-molbace: 0.778473 test loss: 2.257059
[Epoch 52; Iter     9/   41] train: loss: 0.1607114
[Epoch 52; Iter    39/   41] train: loss: 0.0470878
[Epoch 52] ogbg-molbace: 0.637363 val loss: 1.701970
[Epoch 52] ogbg-molbace: 0.794644 test loss: 1.818527
[Epoch 53; Iter    28/   41] train: loss: 0.1895875
[Epoch 53] ogbg-molbace: 0.651648 val loss: 1.995493
[Epoch 53] ogbg-molbace: 0.706138 test loss: 2.861953
[Epoch 54; Iter    17/   41] train: loss: 0.0732260
[Epoch 54] ogbg-molbace: 0.696337 val loss: 1.535243
[Epoch 54] ogbg-molbace: 0.786646 test loss: 1.428816
[Epoch 55; Iter     6/   41] train: loss: 0.3417995
[Epoch 55; Iter    36/   41] train: loss: 0.0563483
[Epoch 55] ogbg-molbace: 0.691941 val loss: 1.771955
[Epoch 55] ogbg-molbace: 0.782125 test loss: 1.647008
[Epoch 56; Iter    25/   41] train: loss: 0.0514830
[Epoch 56] ogbg-molbace: 0.720147 val loss: 1.496803
[Epoch 56] ogbg-molbace: 0.768388 test loss: 2.296353
[Epoch 57; Iter    14/   41] train: loss: 0.0982258
[Epoch 57] ogbg-molbace: 0.707692 val loss: 1.637650
[Epoch 57] ogbg-molbace: 0.783864 test loss: 1.441448
[Epoch 58; Iter     3/   41] train: loss: 0.0081834
[Epoch 58; Iter    33/   41] train: loss: 0.0645365
[Epoch 58] ogbg-molbace: 0.672894 val loss: 1.749236
[Epoch 58] ogbg-molbace: 0.767519 test loss: 1.930993
[Epoch 59; Iter    22/   41] train: loss: 0.0278449
[Epoch 59] ogbg-molbace: 0.706227 val loss: 2.068833
[Epoch 59] ogbg-molbace: 0.768736 test loss: 1.299043
[Epoch 60; Iter    11/   41] train: loss: 0.0646552
[Epoch 60; Iter    41/   41] train: loss: 0.1020899
[Epoch 60] ogbg-molbace: 0.663370 val loss: 1.510241
[Epoch 60] ogbg-molbace: 0.752391 test loss: 1.575006
[Epoch 61; Iter    30/   41] train: loss: 0.0262385
[Epoch 61] ogbg-molbace: 0.641392 val loss: 2.353075
[Epoch 61] ogbg-molbace: 0.761954 test loss: 2.750657
[Epoch 62; Iter    19/   41] train: loss: 0.0131991
[Epoch 62] ogbg-molbace: 0.709890 val loss: 1.738160
[Epoch 62] ogbg-molbace: 0.813250 test loss: 1.703880
[Epoch 63; Iter     8/   41] train: loss: 0.0345977
[Epoch 63; Iter    38/   41] train: loss: 0.0200362
[Epoch 63] ogbg-molbace: 0.692674 val loss: 2.039723
[Epoch 63] ogbg-molbace: 0.771866 test loss: 2.237528
[Epoch 64; Iter    27/   41] train: loss: 0.0241719
[Epoch 64] ogbg-molbace: 0.701465 val loss: 1.559371
[Epoch 64] ogbg-molbace: 0.738654 test loss: 2.101224
[Epoch 65; Iter    16/   41] train: loss: 0.0289595
[Epoch 65] ogbg-molbace: 0.713187 val loss: 1.312812
[Epoch 65] ogbg-molbace: 0.789254 test loss: 1.272253
[Epoch 66; Iter     5/   41] train: loss: 0.0328211
[Epoch 66; Iter    35/   41] train: loss: 0.0667362
[Epoch 66] ogbg-molbace: 0.700733 val loss: 1.540446
[Epoch 66] ogbg-molbace: 0.702487 test loss: 1.419927
[Epoch 67; Iter    24/   41] train: loss: 0.0679178
[Epoch 67] ogbg-molbace: 0.708059 val loss: 1.965794
[Epoch 67] ogbg-molbace: 0.724570 test loss: 2.360951
[Epoch 68; Iter    13/   41] train: loss: 0.0269493
[Epoch 68] ogbg-molbace: 0.712454 val loss: 1.910463
[Epoch 68] ogbg-molbace: 0.780734 test loss: 2.377685
[Epoch 69; Iter     2/   41] train: loss: 0.0337364
[Epoch 69; Iter    32/   41] train: loss: 0.0426067
[Epoch 69] ogbg-molbace: 0.701099 val loss: 2.137958
[Epoch 69] ogbg-molbace: 0.760911 test loss: 2.206813
[Epoch 70; Iter    21/   41] train: loss: 0.0112812
[Epoch 70] ogbg-molbace: 0.673626 val loss: 2.053079
[Epoch 70] ogbg-molbace: 0.748044 test loss: 2.728246
[Epoch 71; Iter    10/   41] train: loss: 0.0077805
[Epoch 71; Iter    40/   41] train: loss: 0.0220943
[Epoch 71] ogbg-molbace: 0.718315 val loss: 1.401036
[Epoch 71] ogbg-molbace: 0.780212 test loss: 2.364324
[Epoch 72; Iter    29/   41] train: loss: 0.0136572
[Epoch 72] ogbg-molbace: 0.663004 val loss: 2.145830
[Epoch 72] ogbg-molbace: 0.713093 test loss: 2.674160
[Epoch 73; Iter    18/   41] train: loss: 0.0178932
[Epoch 73] ogbg-molbace: 0.700366 val loss: 2.190158
[Epoch 73] ogbg-molbace: 0.780734 test loss: 2.058702
[Epoch 74; Iter     7/   41] train: loss: 0.0050733
[Epoch 74; Iter    37/   41] train: loss: 0.0109386
[Epoch 74] ogbg-molbace: 0.703297 val loss: 2.349530
[Epoch 74] ogbg-molbace: 0.784559 test loss: 1.712751
[Epoch 75; Iter    26/   41] train: loss: 0.0051556
[Epoch 75] ogbg-molbace: 0.706227 val loss: 2.110034
[Epoch 75] ogbg-molbace: 0.764737 test loss: 2.031518
[Epoch 76; Iter    15/   41] train: loss: 0.0049546
[Epoch 76] ogbg-molbace: 0.701465 val loss: 2.100204
[Epoch 76] ogbg-molbace: 0.775865 test loss: 2.132340
[Epoch 77; Iter     4/   41] train: loss: 0.0035105
[Epoch 77; Iter    34/   41] train: loss: 0.0007222
[Epoch 77] ogbg-molbace: 0.719414 val loss: 2.353485
[Epoch 77] ogbg-molbace: 0.798991 test loss: 1.912587
[Epoch 78; Iter    23/   41] train: loss: 0.0442196
[Epoch 32] ogbg-molbace: 0.751521 test loss: 1.136384
[Epoch 33; Iter     8/   41] train: loss: 0.4476563
[Epoch 33; Iter    38/   41] train: loss: 0.5267550
[Epoch 33] ogbg-molbace: 0.721978 val loss: 0.935585
[Epoch 33] ogbg-molbace: 0.759694 test loss: 1.114586
[Epoch 34; Iter    27/   41] train: loss: 0.4657051
[Epoch 34] ogbg-molbace: 0.693407 val loss: 1.276572
[Epoch 34] ogbg-molbace: 0.733090 test loss: 1.650501
[Epoch 35; Iter    16/   41] train: loss: 0.3713573
[Epoch 35] ogbg-molbace: 0.729670 val loss: 1.028598
[Epoch 35] ogbg-molbace: 0.738480 test loss: 1.220807
[Epoch 36; Iter     5/   41] train: loss: 0.3463524
[Epoch 36; Iter    35/   41] train: loss: 0.4526715
[Epoch 36] ogbg-molbace: 0.664103 val loss: 1.707016
[Epoch 36] ogbg-molbace: 0.738828 test loss: 1.822292
[Epoch 37; Iter    24/   41] train: loss: 0.3853911
[Epoch 37] ogbg-molbace: 0.675824 val loss: 0.855307
[Epoch 37] ogbg-molbace: 0.768562 test loss: 0.887383
[Epoch 38; Iter    13/   41] train: loss: 0.3820569
[Epoch 38] ogbg-molbace: 0.598535 val loss: 1.967630
[Epoch 38] ogbg-molbace: 0.709268 test loss: 2.292611
[Epoch 39; Iter     2/   41] train: loss: 0.3620362
[Epoch 39; Iter    32/   41] train: loss: 0.2218207
[Epoch 39] ogbg-molbace: 0.623443 val loss: 1.127589
[Epoch 39] ogbg-molbace: 0.819336 test loss: 0.663341
[Epoch 40; Iter    21/   41] train: loss: 0.1996465
[Epoch 40] ogbg-molbace: 0.675824 val loss: 1.191524
[Epoch 40] ogbg-molbace: 0.814467 test loss: 0.777697
[Epoch 41; Iter    10/   41] train: loss: 0.3220551
[Epoch 41; Iter    40/   41] train: loss: 0.4529750
[Epoch 41] ogbg-molbace: 0.717949 val loss: 0.780033
[Epoch 41] ogbg-molbace: 0.809598 test loss: 0.882727
[Epoch 42; Iter    29/   41] train: loss: 0.2668090
[Epoch 42] ogbg-molbace: 0.674725 val loss: 1.076102
[Epoch 42] ogbg-molbace: 0.810294 test loss: 0.782595
[Epoch 43; Iter    18/   41] train: loss: 0.3171779
[Epoch 43] ogbg-molbace: 0.638462 val loss: 0.880826
[Epoch 43] ogbg-molbace: 0.757607 test loss: 0.752187
[Epoch 44; Iter     7/   41] train: loss: 0.3418372
[Epoch 44; Iter    37/   41] train: loss: 0.2047717
[Epoch 44] ogbg-molbace: 0.698535 val loss: 0.859251
[Epoch 44] ogbg-molbace: 0.838463 test loss: 0.908025
[Epoch 45; Iter    26/   41] train: loss: 0.1669797
[Epoch 45] ogbg-molbace: 0.683516 val loss: 1.362233
[Epoch 45] ogbg-molbace: 0.816032 test loss: 0.955804
[Epoch 46; Iter    15/   41] train: loss: 0.1063380
[Epoch 46] ogbg-molbace: 0.708791 val loss: 1.421008
[Epoch 46] ogbg-molbace: 0.830464 test loss: 1.952114
[Epoch 47; Iter     4/   41] train: loss: 0.1283895
[Epoch 47; Iter    34/   41] train: loss: 0.2160741
[Epoch 47] ogbg-molbace: 0.674725 val loss: 1.091248
[Epoch 47] ogbg-molbace: 0.792732 test loss: 0.879410
[Epoch 48; Iter    23/   41] train: loss: 0.1262358
[Epoch 48] ogbg-molbace: 0.678022 val loss: 1.323331
[Epoch 48] ogbg-molbace: 0.793775 test loss: 1.015506
[Epoch 49; Iter    12/   41] train: loss: 0.2227705
[Epoch 49] ogbg-molbace: 0.678388 val loss: 1.579056
[Epoch 49] ogbg-molbace: 0.785776 test loss: 0.979750
[Epoch 50; Iter     1/   41] train: loss: 0.1134495
[Epoch 50; Iter    31/   41] train: loss: 0.0867344
[Epoch 50] ogbg-molbace: 0.643590 val loss: 1.684892
[Epoch 50] ogbg-molbace: 0.839158 test loss: 1.083143
[Epoch 51; Iter    20/   41] train: loss: 0.1701613
[Epoch 51] ogbg-molbace: 0.664103 val loss: 1.694032
[Epoch 51] ogbg-molbace: 0.858807 test loss: 1.637146
[Epoch 52; Iter     9/   41] train: loss: 0.0613074
[Epoch 52; Iter    39/   41] train: loss: 0.0533140
[Epoch 52] ogbg-molbace: 0.694872 val loss: 1.654370
[Epoch 52] ogbg-molbace: 0.804034 test loss: 1.510213
[Epoch 53; Iter    28/   41] train: loss: 0.0246057
[Epoch 53] ogbg-molbace: 0.643590 val loss: 2.170323
[Epoch 53] ogbg-molbace: 0.816554 test loss: 1.742872
[Epoch 54; Iter    17/   41] train: loss: 0.0138406
[Epoch 54] ogbg-molbace: 0.672161 val loss: 2.235611
[Epoch 54] ogbg-molbace: 0.836898 test loss: 1.240095
[Epoch 55; Iter     6/   41] train: loss: 0.0239445
[Epoch 55; Iter    36/   41] train: loss: 0.0060489
[Epoch 55] ogbg-molbace: 0.667033 val loss: 2.402209
[Epoch 55] ogbg-molbace: 0.793253 test loss: 1.744365
[Epoch 56; Iter    25/   41] train: loss: 0.0763669
[Epoch 56] ogbg-molbace: 0.681319 val loss: 2.600261
[Epoch 56] ogbg-molbace: 0.805077 test loss: 1.441985
[Epoch 57; Iter    14/   41] train: loss: 0.0069541
[Epoch 57] ogbg-molbace: 0.671062 val loss: 2.418314
[Epoch 57] ogbg-molbace: 0.806816 test loss: 1.840466
[Epoch 58; Iter     3/   41] train: loss: 0.0027094
[Epoch 58; Iter    33/   41] train: loss: 0.0347920
[Epoch 58] ogbg-molbace: 0.679121 val loss: 2.308094
[Epoch 58] ogbg-molbace: 0.785429 test loss: 2.158745
[Epoch 59; Iter    22/   41] train: loss: 0.0048737
[Epoch 59] ogbg-molbace: 0.684249 val loss: 2.263420
[Epoch 59] ogbg-molbace: 0.830464 test loss: 1.687473
[Epoch 60; Iter    11/   41] train: loss: 0.0044726
[Epoch 60; Iter    41/   41] train: loss: 0.0275902
[Epoch 60] ogbg-molbace: 0.697802 val loss: 1.951224
[Epoch 60] ogbg-molbace: 0.769953 test loss: 1.682645
[Epoch 61; Iter    30/   41] train: loss: 0.1704550
[Epoch 61] ogbg-molbace: 0.695971 val loss: 1.972653
[Epoch 61] ogbg-molbace: 0.807338 test loss: 2.048212
[Epoch 62; Iter    19/   41] train: loss: 0.0350956
[Epoch 62] ogbg-molbace: 0.684615 val loss: 1.801765
[Epoch 62] ogbg-molbace: 0.720918 test loss: 2.080658
[Epoch 63; Iter     8/   41] train: loss: 0.1623823
[Epoch 63; Iter    38/   41] train: loss: 0.3886057
[Epoch 63] ogbg-molbace: 0.658608 val loss: 1.780190
[Epoch 63] ogbg-molbace: 0.797253 test loss: 1.848230
[Epoch 64; Iter    27/   41] train: loss: 0.3214495
[Epoch 64] ogbg-molbace: 0.676923 val loss: 1.757205
[Epoch 64] ogbg-molbace: 0.805599 test loss: 1.698530
[Epoch 65; Iter    16/   41] train: loss: 0.0397115
[Epoch 65] ogbg-molbace: 0.685714 val loss: 1.777491
[Epoch 65] ogbg-molbace: 0.831160 test loss: 1.071887
[Epoch 66; Iter     5/   41] train: loss: 0.1319864
[Epoch 66; Iter    35/   41] train: loss: 0.0237434
[Epoch 66] ogbg-molbace: 0.614652 val loss: 2.195898
[Epoch 66] ogbg-molbace: 0.805251 test loss: 2.317181
[Epoch 67; Iter    24/   41] train: loss: 0.0256248
[Epoch 67] ogbg-molbace: 0.673626 val loss: 1.659102
[Epoch 67] ogbg-molbace: 0.738306 test loss: 2.181049
[Epoch 68; Iter    13/   41] train: loss: 0.0789545
[Epoch 68] ogbg-molbace: 0.665934 val loss: 2.000343
[Epoch 68] ogbg-molbace: 0.848722 test loss: 1.250678
[Epoch 69; Iter     2/   41] train: loss: 0.0169445
[Epoch 69; Iter    32/   41] train: loss: 0.0304678
[Epoch 69] ogbg-molbace: 0.659341 val loss: 2.793215
[Epoch 69] ogbg-molbace: 0.797600 test loss: 2.176173
[Epoch 70; Iter    21/   41] train: loss: 0.0235308
[Epoch 70] ogbg-molbace: 0.664103 val loss: 2.330658
[Epoch 70] ogbg-molbace: 0.822466 test loss: 1.975413
[Epoch 71; Iter    10/   41] train: loss: 0.0028425
[Epoch 71; Iter    40/   41] train: loss: 0.0132587
[Epoch 71] ogbg-molbace: 0.659707 val loss: 2.595489
[Epoch 71] ogbg-molbace: 0.825596 test loss: 1.734717
[Epoch 72; Iter    29/   41] train: loss: 0.0848690
[Epoch 72] ogbg-molbace: 0.664469 val loss: 2.423829
[Epoch 72] ogbg-molbace: 0.789254 test loss: 1.803935
[Epoch 73; Iter    18/   41] train: loss: 0.0090204
[Epoch 73] ogbg-molbace: 0.683150 val loss: 2.273955
[Epoch 73] ogbg-molbace: 0.811685 test loss: 2.560714
[Epoch 74; Iter     7/   41] train: loss: 0.0391864
[Epoch 74; Iter    37/   41] train: loss: 0.0662223
[Epoch 74] ogbg-molbace: 0.601465 val loss: 3.395678
[Epoch 74] ogbg-molbace: 0.822987 test loss: 2.690455
[Epoch 75; Iter    26/   41] train: loss: 0.0608685
[Epoch 75] ogbg-molbace: 0.652381 val loss: 2.010476
[Epoch 75] ogbg-molbace: 0.772909 test loss: 2.159341
[Epoch 76; Iter    15/   41] train: loss: 0.0194002
[Epoch 76] ogbg-molbace: 0.663370 val loss: 2.911080
[Epoch 76] ogbg-molbace: 0.829421 test loss: 2.370305
[Epoch 77; Iter     4/   41] train: loss: 0.0539802
[Epoch 77; Iter    34/   41] train: loss: 0.0352591
[Epoch 77] ogbg-molbace: 0.669963 val loss: 1.986926
[Epoch 77] ogbg-molbace: 0.762476 test loss: 2.508177
[Epoch 78; Iter    23/   41] train: loss: 0.2174886
[Epoch 32] ogbg-molbace: 0.552078 test loss: 4.771087
[Epoch 33; Iter     8/   41] train: loss: 0.5368689
[Epoch 33; Iter    38/   41] train: loss: 0.4246548
[Epoch 33] ogbg-molbace: 0.618315 val loss: 1.818689
[Epoch 33] ogbg-molbace: 0.631890 test loss: 2.030828
[Epoch 34; Iter    27/   41] train: loss: 0.5716558
[Epoch 34] ogbg-molbace: 0.552747 val loss: 2.385777
[Epoch 34] ogbg-molbace: 0.571553 test loss: 2.817047
[Epoch 35; Iter    16/   41] train: loss: 0.5143136
[Epoch 35] ogbg-molbace: 0.524908 val loss: 2.601544
[Epoch 35] ogbg-molbace: 0.555729 test loss: 2.623678
[Epoch 36; Iter     5/   41] train: loss: 0.4288383
[Epoch 36; Iter    35/   41] train: loss: 0.4317038
[Epoch 36] ogbg-molbace: 0.599634 val loss: 5.939711
[Epoch 36] ogbg-molbace: 0.657799 test loss: 5.635341
[Epoch 37; Iter    24/   41] train: loss: 0.3101084
[Epoch 37] ogbg-molbace: 0.588645 val loss: 13.634349
[Epoch 37] ogbg-molbace: 0.609111 test loss: 16.088823
[Epoch 38; Iter    13/   41] train: loss: 0.2880936
[Epoch 38] ogbg-molbace: 0.680586 val loss: 2.155710
[Epoch 38] ogbg-molbace: 0.758651 test loss: 1.961471
[Epoch 39; Iter     2/   41] train: loss: 0.2353562
[Epoch 39; Iter    32/   41] train: loss: 0.2669406
[Epoch 39] ogbg-molbace: 0.645788 val loss: 1.536387
[Epoch 39] ogbg-molbace: 0.712572 test loss: 1.462275
[Epoch 40; Iter    21/   41] train: loss: 0.4550705
[Epoch 40] ogbg-molbace: 0.633700 val loss: 2.135909
[Epoch 40] ogbg-molbace: 0.725439 test loss: 1.811718
[Epoch 41; Iter    10/   41] train: loss: 0.3422059
[Epoch 41; Iter    40/   41] train: loss: 0.2758932
[Epoch 41] ogbg-molbace: 0.676923 val loss: 1.707876
[Epoch 41] ogbg-molbace: 0.770127 test loss: 1.225613
[Epoch 42; Iter    29/   41] train: loss: 0.2354063
[Epoch 42] ogbg-molbace: 0.678755 val loss: 1.718494
[Epoch 42] ogbg-molbace: 0.780908 test loss: 1.927404
[Epoch 43; Iter    18/   41] train: loss: 0.1180004
[Epoch 43] ogbg-molbace: 0.680952 val loss: 1.487224
[Epoch 43] ogbg-molbace: 0.781429 test loss: 1.672864
[Epoch 44; Iter     7/   41] train: loss: 0.2356777
[Epoch 44; Iter    37/   41] train: loss: 0.1888517
[Epoch 44] ogbg-molbace: 0.650549 val loss: 2.089179
[Epoch 44] ogbg-molbace: 0.735524 test loss: 2.086877
[Epoch 45; Iter    26/   41] train: loss: 0.1817701
[Epoch 45] ogbg-molbace: 0.599634 val loss: 3.065571
[Epoch 45] ogbg-molbace: 0.715354 test loss: 2.608450
[Epoch 46; Iter    15/   41] train: loss: 0.1775854
[Epoch 46] ogbg-molbace: 0.617582 val loss: 3.890268
[Epoch 46] ogbg-molbace: 0.695531 test loss: 3.902096
[Epoch 47; Iter     4/   41] train: loss: 0.0789038
[Epoch 47; Iter    34/   41] train: loss: 0.0847036
[Epoch 47] ogbg-molbace: 0.620147 val loss: 6.586325
[Epoch 47] ogbg-molbace: 0.699704 test loss: 4.786386
[Epoch 48; Iter    23/   41] train: loss: 0.0429608
[Epoch 48] ogbg-molbace: 0.591941 val loss: 4.743110
[Epoch 48] ogbg-molbace: 0.629456 test loss: 4.912183
[Epoch 49; Iter    12/   41] train: loss: 0.0820583
[Epoch 49] ogbg-molbace: 0.632967 val loss: 3.906296
[Epoch 49] ogbg-molbace: 0.723700 test loss: 3.581509
[Epoch 50; Iter     1/   41] train: loss: 0.1165496
[Epoch 50; Iter    31/   41] train: loss: 0.0187144
[Epoch 50] ogbg-molbace: 0.628205 val loss: 3.252881
[Epoch 50] ogbg-molbace: 0.744566 test loss: 3.499191
[Epoch 51; Iter    20/   41] train: loss: 0.0207232
[Epoch 51] ogbg-molbace: 0.653480 val loss: 2.728106
[Epoch 51] ogbg-molbace: 0.769431 test loss: 3.219585
[Epoch 52; Iter     9/   41] train: loss: 0.0389733
[Epoch 52; Iter    39/   41] train: loss: 0.0305705
[Epoch 52] ogbg-molbace: 0.658608 val loss: 4.184661
[Epoch 52] ogbg-molbace: 0.769431 test loss: 3.779493
[Epoch 53; Iter    28/   41] train: loss: 0.0100127
[Epoch 53] ogbg-molbace: 0.648352 val loss: 2.271017
[Epoch 53] ogbg-molbace: 0.705790 test loss: 2.709636
[Epoch 54; Iter    17/   41] train: loss: 0.0492448
[Epoch 54] ogbg-molbace: 0.625641 val loss: 2.543223
[Epoch 54] ogbg-molbace: 0.713441 test loss: 2.852535
[Epoch 55; Iter     6/   41] train: loss: 0.1310475
[Epoch 55; Iter    36/   41] train: loss: 0.0076344
[Epoch 55] ogbg-molbace: 0.626374 val loss: 2.731542
[Epoch 55] ogbg-molbace: 0.718658 test loss: 2.473873
[Epoch 56; Iter    25/   41] train: loss: 0.0268664
[Epoch 56] ogbg-molbace: 0.629304 val loss: 2.465657
[Epoch 56] ogbg-molbace: 0.716571 test loss: 2.511231
[Epoch 57; Iter    14/   41] train: loss: 0.1063546
[Epoch 57] ogbg-molbace: 0.628571 val loss: 4.754275
[Epoch 57] ogbg-molbace: 0.748565 test loss: 4.426690
[Epoch 58; Iter     3/   41] train: loss: 0.0988775
[Epoch 58; Iter    33/   41] train: loss: 0.0301642
[Epoch 58] ogbg-molbace: 0.569963 val loss: 12.284888
[Epoch 58] ogbg-molbace: 0.605634 test loss: 9.521023
[Epoch 59; Iter    22/   41] train: loss: 0.0977543
[Epoch 59] ogbg-molbace: 0.639560 val loss: 3.200601
[Epoch 59] ogbg-molbace: 0.755521 test loss: 3.883105
[Epoch 60; Iter    11/   41] train: loss: 0.0812005
[Epoch 60; Iter    41/   41] train: loss: 0.0791786
[Epoch 60] ogbg-molbace: 0.682418 val loss: 3.425721
[Epoch 60] ogbg-molbace: 0.743697 test loss: 3.827842
[Epoch 61; Iter    30/   41] train: loss: 0.0139530
[Epoch 61] ogbg-molbace: 0.630037 val loss: 3.524067
[Epoch 61] ogbg-molbace: 0.648757 test loss: 4.042038
[Epoch 62; Iter    19/   41] train: loss: 0.0168965
[Epoch 62] ogbg-molbace: 0.611722 val loss: 5.589509
[Epoch 62] ogbg-molbace: 0.692227 test loss: 5.378997
[Epoch 63; Iter     8/   41] train: loss: 0.0198076
[Epoch 63; Iter    38/   41] train: loss: 0.0338567
[Epoch 63] ogbg-molbace: 0.627473 val loss: 3.193633
[Epoch 63] ogbg-molbace: 0.755347 test loss: 3.553735
[Epoch 64; Iter    27/   41] train: loss: 0.0070191
[Epoch 64] ogbg-molbace: 0.600366 val loss: 7.735870
[Epoch 64] ogbg-molbace: 0.702313 test loss: 6.773706
[Epoch 65; Iter    16/   41] train: loss: 0.0063992
[Epoch 65] ogbg-molbace: 0.630037 val loss: 5.875703
[Epoch 65] ogbg-molbace: 0.730656 test loss: 5.382213
[Epoch 66; Iter     5/   41] train: loss: 0.0030858
[Epoch 66; Iter    35/   41] train: loss: 0.0369224
[Epoch 66] ogbg-molbace: 0.615751 val loss: 5.579263
[Epoch 66] ogbg-molbace: 0.727352 test loss: 5.359549
[Epoch 67; Iter    24/   41] train: loss: 0.0186807
[Epoch 67] ogbg-molbace: 0.618681 val loss: 5.438849
[Epoch 67] ogbg-molbace: 0.718658 test loss: 5.326405
[Epoch 68; Iter    13/   41] train: loss: 0.0303423
[Epoch 68] ogbg-molbace: 0.639560 val loss: 5.569407
[Epoch 68] ogbg-molbace: 0.731177 test loss: 4.962578
[Epoch 69; Iter     2/   41] train: loss: 0.0096869
[Epoch 69; Iter    32/   41] train: loss: 0.0054586
[Epoch 69] ogbg-molbace: 0.643223 val loss: 5.303236
[Epoch 69] ogbg-molbace: 0.714658 test loss: 5.193031
[Epoch 70; Iter    21/   41] train: loss: 0.0023653
[Epoch 70] ogbg-molbace: 0.625275 val loss: 4.918073
[Epoch 70] ogbg-molbace: 0.721961 test loss: 4.869029
[Epoch 71; Iter    10/   41] train: loss: 0.0046254
[Epoch 71; Iter    40/   41] train: loss: 0.0051085
[Epoch 71] ogbg-molbace: 0.604029 val loss: 5.202280
[Epoch 71] ogbg-molbace: 0.709094 test loss: 5.350856
[Epoch 72; Iter    29/   41] train: loss: 0.0041449
[Epoch 72] ogbg-molbace: 0.623443 val loss: 4.457754
[Epoch 72] ogbg-molbace: 0.735437 test loss: 4.811528
[Epoch 73; Iter    18/   41] train: loss: 0.0008258
[Epoch 73] ogbg-molbace: 0.613553 val loss: 4.945395
[Epoch 73] ogbg-molbace: 0.717093 test loss: 5.349451
[Epoch 74; Iter     7/   41] train: loss: 0.0035287
[Epoch 74; Iter    37/   41] train: loss: 0.0018446
[Epoch 74] ogbg-molbace: 0.624176 val loss: 6.258753
[Epoch 74] ogbg-molbace: 0.725787 test loss: 6.327418
[Epoch 75; Iter    26/   41] train: loss: 0.0007217
[Epoch 75] ogbg-molbace: 0.641026 val loss: 4.167066
[Epoch 75] ogbg-molbace: 0.748913 test loss: 4.495756
[Epoch 76; Iter    15/   41] train: loss: 0.0017679
[Epoch 76] ogbg-molbace: 0.637363 val loss: 3.238394
[Epoch 76] ogbg-molbace: 0.747522 test loss: 3.888507
[Epoch 77; Iter     4/   41] train: loss: 0.0079195
[Epoch 77; Iter    34/   41] train: loss: 0.0004596
[Epoch 77] ogbg-molbace: 0.641758 val loss: 3.887605
[Epoch 77] ogbg-molbace: 0.748739 test loss: 4.399862
[Epoch 78; Iter    23/   41] train: loss: 0.0138723
[Epoch 32] ogbg-molbace: 0.683533 test loss: 3.254534
[Epoch 33; Iter     8/   41] train: loss: 0.5070421
[Epoch 33; Iter    38/   41] train: loss: 0.5484022
[Epoch 33] ogbg-molbace: 0.688645 val loss: 2.093927
[Epoch 33] ogbg-molbace: 0.692054 test loss: 3.442268
[Epoch 34; Iter    27/   41] train: loss: 0.4408712
[Epoch 34] ogbg-molbace: 0.635165 val loss: 2.013752
[Epoch 34] ogbg-molbace: 0.685794 test loss: 3.364307
[Epoch 35; Iter    16/   41] train: loss: 0.4662001
[Epoch 35] ogbg-molbace: 0.685714 val loss: 0.822850
[Epoch 35] ogbg-molbace: 0.701791 test loss: 1.315197
[Epoch 36; Iter     5/   41] train: loss: 0.3309059
[Epoch 36; Iter    35/   41] train: loss: 0.4954317
[Epoch 36] ogbg-molbace: 0.660073 val loss: 0.921533
[Epoch 36] ogbg-molbace: 0.748218 test loss: 0.849147
[Epoch 37; Iter    24/   41] train: loss: 0.4519632
[Epoch 37] ogbg-molbace: 0.584249 val loss: 1.489150
[Epoch 37] ogbg-molbace: 0.623196 test loss: 1.588544
[Epoch 38; Iter    13/   41] train: loss: 0.3552299
[Epoch 38] ogbg-molbace: 0.561172 val loss: 2.618986
[Epoch 38] ogbg-molbace: 0.611372 test loss: 2.948782
[Epoch 39; Iter     2/   41] train: loss: 0.2897494
[Epoch 39; Iter    32/   41] train: loss: 0.2625087
[Epoch 39] ogbg-molbace: 0.687179 val loss: 1.318990
[Epoch 39] ogbg-molbace: 0.748565 test loss: 0.703544
[Epoch 40; Iter    21/   41] train: loss: 0.3288558
[Epoch 40] ogbg-molbace: 0.580220 val loss: 2.450178
[Epoch 40] ogbg-molbace: 0.711702 test loss: 3.032547
[Epoch 41; Iter    10/   41] train: loss: 0.2880700
[Epoch 41; Iter    40/   41] train: loss: 0.5005981
[Epoch 41] ogbg-molbace: 0.721245 val loss: 1.279265
[Epoch 41] ogbg-molbace: 0.729264 test loss: 1.476034
[Epoch 42; Iter    29/   41] train: loss: 0.2473860
[Epoch 42] ogbg-molbace: 0.675092 val loss: 0.948116
[Epoch 42] ogbg-molbace: 0.744392 test loss: 1.344229
[Epoch 43; Iter    18/   41] train: loss: 0.2337231
[Epoch 43] ogbg-molbace: 0.662271 val loss: 1.368965
[Epoch 43] ogbg-molbace: 0.687359 test loss: 2.063637
[Epoch 44; Iter     7/   41] train: loss: 0.1845369
[Epoch 44; Iter    37/   41] train: loss: 0.1265312
[Epoch 44] ogbg-molbace: 0.668132 val loss: 0.654378
[Epoch 44] ogbg-molbace: 0.789428 test loss: 0.862788
[Epoch 45; Iter    26/   41] train: loss: 0.0856558
[Epoch 45] ogbg-molbace: 0.714286 val loss: 1.112535
[Epoch 45] ogbg-molbace: 0.779517 test loss: 1.189330
[Epoch 46; Iter    15/   41] train: loss: 0.1223588
[Epoch 46] ogbg-molbace: 0.681319 val loss: 1.158983
[Epoch 46] ogbg-molbace: 0.766302 test loss: 1.711213
[Epoch 47; Iter     4/   41] train: loss: 0.0577114
[Epoch 47; Iter    34/   41] train: loss: 0.1414057
[Epoch 47] ogbg-molbace: 0.683150 val loss: 1.319145
[Epoch 47] ogbg-molbace: 0.807512 test loss: 0.974602
[Epoch 48; Iter    23/   41] train: loss: 0.4973771
[Epoch 48] ogbg-molbace: 0.717582 val loss: 1.216351
[Epoch 48] ogbg-molbace: 0.783516 test loss: 1.045712
[Epoch 49; Iter    12/   41] train: loss: 0.0836333
[Epoch 49] ogbg-molbace: 0.652381 val loss: 2.143859
[Epoch 49] ogbg-molbace: 0.707007 test loss: 2.361378
[Epoch 50; Iter     1/   41] train: loss: 0.0572641
[Epoch 50; Iter    31/   41] train: loss: 0.0854828
[Epoch 50] ogbg-molbace: 0.593773 val loss: 1.401594
[Epoch 50] ogbg-molbace: 0.667884 test loss: 1.542107
[Epoch 51; Iter    20/   41] train: loss: 0.0906391
[Epoch 51] ogbg-molbace: 0.695238 val loss: 1.712276
[Epoch 51] ogbg-molbace: 0.809077 test loss: 2.011516
[Epoch 52; Iter     9/   41] train: loss: 0.1032302
[Epoch 52; Iter    39/   41] train: loss: 0.0240433
[Epoch 52] ogbg-molbace: 0.667766 val loss: 1.043436
[Epoch 52] ogbg-molbace: 0.772909 test loss: 1.312021
[Epoch 53; Iter    28/   41] train: loss: 0.0183499
[Epoch 53] ogbg-molbace: 0.673260 val loss: 1.423967
[Epoch 53] ogbg-molbace: 0.783168 test loss: 1.810944
[Epoch 54; Iter    17/   41] train: loss: 0.0147689
[Epoch 54] ogbg-molbace: 0.658242 val loss: 1.425330
[Epoch 54] ogbg-molbace: 0.759346 test loss: 1.572242
[Epoch 55; Iter     6/   41] train: loss: 0.0118720
[Epoch 55; Iter    36/   41] train: loss: 0.0055065
[Epoch 55] ogbg-molbace: 0.675824 val loss: 1.614097
[Epoch 55] ogbg-molbace: 0.753086 test loss: 1.854841
[Epoch 56; Iter    25/   41] train: loss: 0.0112331
[Epoch 56] ogbg-molbace: 0.671429 val loss: 1.647934
[Epoch 56] ogbg-molbace: 0.766302 test loss: 1.670690
[Epoch 57; Iter    14/   41] train: loss: 0.0041085
[Epoch 57] ogbg-molbace: 0.683150 val loss: 1.494704
[Epoch 57] ogbg-molbace: 0.758477 test loss: 1.742627
[Epoch 58; Iter     3/   41] train: loss: 0.0017498
[Epoch 58; Iter    33/   41] train: loss: 0.0063833
[Epoch 58] ogbg-molbace: 0.665568 val loss: 1.693337
[Epoch 58] ogbg-molbace: 0.731351 test loss: 1.991708
[Epoch 59; Iter    22/   41] train: loss: 0.0021983
[Epoch 59] ogbg-molbace: 0.671429 val loss: 1.594456
[Epoch 59] ogbg-molbace: 0.745262 test loss: 1.900078
[Epoch 60; Iter    11/   41] train: loss: 0.0023500
[Epoch 60; Iter    41/   41] train: loss: 0.0009671
[Epoch 60] ogbg-molbace: 0.687546 val loss: 1.796456
[Epoch 60] ogbg-molbace: 0.812554 test loss: 1.375199
[Epoch 61; Iter    30/   41] train: loss: 0.0121623
[Epoch 61] ogbg-molbace: 0.672527 val loss: 1.535701
[Epoch 61] ogbg-molbace: 0.740915 test loss: 1.783632
[Epoch 62; Iter    19/   41] train: loss: 0.0018811
[Epoch 62] ogbg-molbace: 0.683883 val loss: 1.755964
[Epoch 62] ogbg-molbace: 0.807860 test loss: 1.216854
[Epoch 63; Iter     8/   41] train: loss: 0.0391840
[Epoch 63; Iter    38/   41] train: loss: 0.0086933
[Epoch 63] ogbg-molbace: 0.653114 val loss: 1.787199
[Epoch 63] ogbg-molbace: 0.744740 test loss: 1.424337
[Epoch 64; Iter    27/   41] train: loss: 0.1394912
[Epoch 64] ogbg-molbace: 0.663370 val loss: 2.956355
[Epoch 64] ogbg-molbace: 0.740567 test loss: 1.609162
[Epoch 65; Iter    16/   41] train: loss: 0.4127203
[Epoch 65] ogbg-molbace: 0.602564 val loss: 1.985830
[Epoch 65] ogbg-molbace: 0.709442 test loss: 2.317610
[Epoch 66; Iter     5/   41] train: loss: 0.4603234
[Epoch 66; Iter    35/   41] train: loss: 0.2328221
[Epoch 66] ogbg-molbace: 0.657509 val loss: 1.722279
[Epoch 66] ogbg-molbace: 0.739697 test loss: 2.322359
[Epoch 67; Iter    24/   41] train: loss: 0.1071196
[Epoch 67] ogbg-molbace: 0.701832 val loss: 0.937285
[Epoch 67] ogbg-molbace: 0.774996 test loss: 1.493465
[Epoch 68; Iter    13/   41] train: loss: 0.1288977
[Epoch 68] ogbg-molbace: 0.697802 val loss: 1.257954
[Epoch 68] ogbg-molbace: 0.754477 test loss: 2.100102
[Epoch 69; Iter     2/   41] train: loss: 0.0115933
[Epoch 69; Iter    32/   41] train: loss: 0.0485456
[Epoch 69] ogbg-molbace: 0.658608 val loss: 1.601765
[Epoch 69] ogbg-molbace: 0.715875 test loss: 2.292657
[Epoch 70; Iter    21/   41] train: loss: 0.0342621
[Epoch 70] ogbg-molbace: 0.697802 val loss: 1.325884
[Epoch 70] ogbg-molbace: 0.780908 test loss: 2.127026
[Epoch 71; Iter    10/   41] train: loss: 0.0130134
[Epoch 71; Iter    40/   41] train: loss: 0.0127479
[Epoch 71] ogbg-molbace: 0.698535 val loss: 1.376973
[Epoch 71] ogbg-molbace: 0.749435 test loss: 2.161562
[Epoch 72; Iter    29/   41] train: loss: 0.0289898
[Epoch 72] ogbg-molbace: 0.726740 val loss: 1.546003
[Epoch 72] ogbg-molbace: 0.779343 test loss: 2.389816
[Epoch 73; Iter    18/   41] train: loss: 0.0462627
[Epoch 73] ogbg-molbace: 0.708791 val loss: 2.100157
[Epoch 73] ogbg-molbace: 0.732220 test loss: 3.009752
[Epoch 74; Iter     7/   41] train: loss: 0.0572183
[Epoch 74; Iter    37/   41] train: loss: 0.0477449
[Epoch 74] ogbg-molbace: 0.703297 val loss: 1.391429
[Epoch 74] ogbg-molbace: 0.791688 test loss: 2.145426
[Epoch 75; Iter    26/   41] train: loss: 0.1367586
[Epoch 75] ogbg-molbace: 0.711722 val loss: 1.836247
[Epoch 75] ogbg-molbace: 0.747522 test loss: 1.497492
[Epoch 76; Iter    15/   41] train: loss: 0.1411042
[Epoch 76] ogbg-molbace: 0.686447 val loss: 1.313652
[Epoch 76] ogbg-molbace: 0.757086 test loss: 2.329185
[Epoch 77; Iter     4/   41] train: loss: 0.0307089
[Epoch 77; Iter    34/   41] train: loss: 0.0355292
[Epoch 77] ogbg-molbace: 0.703663 val loss: 1.208160
[Epoch 77] ogbg-molbace: 0.789776 test loss: 2.149484
[Epoch 78; Iter    23/   41] train: loss: 0.0114263
[Epoch 32] ogbg-molbace: 0.727700 test loss: 1.273775
[Epoch 33; Iter     8/   41] train: loss: 0.6032867
[Epoch 33; Iter    38/   41] train: loss: 0.6792613
[Epoch 33] ogbg-molbace: 0.711355 val loss: 0.712381
[Epoch 33] ogbg-molbace: 0.759520 test loss: 1.254331
[Epoch 34; Iter    27/   41] train: loss: 0.4836238
[Epoch 34] ogbg-molbace: 0.627473 val loss: 3.172802
[Epoch 34] ogbg-molbace: 0.750478 test loss: 2.490604
[Epoch 35; Iter    16/   41] train: loss: 0.5021820
[Epoch 35] ogbg-molbace: 0.578022 val loss: 0.939760
[Epoch 35] ogbg-molbace: 0.638324 test loss: 1.347217
[Epoch 36; Iter     5/   41] train: loss: 0.4241949
[Epoch 36; Iter    35/   41] train: loss: 0.4969621
[Epoch 36] ogbg-molbace: 0.588645 val loss: 0.808616
[Epoch 36] ogbg-molbace: 0.567553 test loss: 2.194066
[Epoch 37; Iter    24/   41] train: loss: 0.5175707
[Epoch 37] ogbg-molbace: 0.568864 val loss: 7.908727
[Epoch 37] ogbg-molbace: 0.605634 test loss: 9.751983
[Epoch 38; Iter    13/   41] train: loss: 0.3504427
[Epoch 38] ogbg-molbace: 0.651648 val loss: 0.985304
[Epoch 38] ogbg-molbace: 0.632064 test loss: 1.516536
[Epoch 39; Iter     2/   41] train: loss: 0.3512526
[Epoch 39; Iter    32/   41] train: loss: 0.3721435
[Epoch 39] ogbg-molbace: 0.630769 val loss: 3.113948
[Epoch 39] ogbg-molbace: 0.586507 test loss: 9.304237
[Epoch 40; Iter    21/   41] train: loss: 0.3380463
[Epoch 40] ogbg-molbace: 0.676190 val loss: 1.047070
[Epoch 40] ogbg-molbace: 0.773257 test loss: 0.944076
[Epoch 41; Iter    10/   41] train: loss: 0.4237588
[Epoch 41; Iter    40/   41] train: loss: 0.5455016
[Epoch 41] ogbg-molbace: 0.657875 val loss: 1.003947
[Epoch 41] ogbg-molbace: 0.762998 test loss: 1.152748
[Epoch 42; Iter    29/   41] train: loss: 0.3254932
[Epoch 42] ogbg-molbace: 0.586813 val loss: 3.663344
[Epoch 42] ogbg-molbace: 0.616936 test loss: 4.099100
[Epoch 43; Iter    18/   41] train: loss: 0.1914814
[Epoch 43] ogbg-molbace: 0.587912 val loss: 1.152785
[Epoch 43] ogbg-molbace: 0.677447 test loss: 1.476891
[Epoch 44; Iter     7/   41] train: loss: 0.2100804
[Epoch 44; Iter    37/   41] train: loss: 0.2348866
[Epoch 44] ogbg-molbace: 0.573626 val loss: 20.099006
[Epoch 44] ogbg-molbace: 0.526691 test loss: 22.154898
[Epoch 45; Iter    26/   41] train: loss: 0.2977112
[Epoch 45] ogbg-molbace: 0.638095 val loss: 1.474674
[Epoch 45] ogbg-molbace: 0.662493 test loss: 3.047742
[Epoch 46; Iter    15/   41] train: loss: 0.1248398
[Epoch 46] ogbg-molbace: 0.645788 val loss: 1.127531
[Epoch 46] ogbg-molbace: 0.684924 test loss: 1.750146
[Epoch 47; Iter     4/   41] train: loss: 0.1042858
[Epoch 47; Iter    34/   41] train: loss: 0.0521852
[Epoch 47] ogbg-molbace: 0.647253 val loss: 1.270773
[Epoch 47] ogbg-molbace: 0.692749 test loss: 1.692467
[Epoch 48; Iter    23/   41] train: loss: 0.0226644
[Epoch 48] ogbg-molbace: 0.582051 val loss: 1.359283
[Epoch 48] ogbg-molbace: 0.617110 test loss: 1.975160
[Epoch 49; Iter    12/   41] train: loss: 0.1538375
[Epoch 49] ogbg-molbace: 0.667766 val loss: 1.211590
[Epoch 49] ogbg-molbace: 0.748565 test loss: 1.652783
[Epoch 50; Iter     1/   41] train: loss: 0.0329612
[Epoch 50; Iter    31/   41] train: loss: 0.0161133
[Epoch 50] ogbg-molbace: 0.652015 val loss: 1.378433
[Epoch 50] ogbg-molbace: 0.721092 test loss: 1.590782
[Epoch 51; Iter    20/   41] train: loss: 0.0621334
[Epoch 51] ogbg-molbace: 0.708425 val loss: 2.309531
[Epoch 51] ogbg-molbace: 0.773778 test loss: 2.422730
[Epoch 52; Iter     9/   41] train: loss: 0.1964140
[Epoch 52; Iter    39/   41] train: loss: 0.1977642
[Epoch 52] ogbg-molbace: 0.691941 val loss: 1.135320
[Epoch 52] ogbg-molbace: 0.625630 test loss: 2.176867
[Epoch 53; Iter    28/   41] train: loss: 0.2463892
[Epoch 53] ogbg-molbace: 0.613553 val loss: 1.804400
[Epoch 53] ogbg-molbace: 0.605634 test loss: 2.564401
[Epoch 54; Iter    17/   41] train: loss: 0.1071448
[Epoch 54] ogbg-molbace: 0.686813 val loss: 1.270518
[Epoch 54] ogbg-molbace: 0.699009 test loss: 1.819861
[Epoch 55; Iter     6/   41] train: loss: 0.0325043
[Epoch 55; Iter    36/   41] train: loss: 0.1294747
[Epoch 55] ogbg-molbace: 0.639194 val loss: 1.467402
[Epoch 55] ogbg-molbace: 0.695531 test loss: 1.366953
[Epoch 56; Iter    25/   41] train: loss: 0.0205506
[Epoch 56] ogbg-molbace: 0.650916 val loss: 1.593610
[Epoch 56] ogbg-molbace: 0.678838 test loss: 1.767420
[Epoch 57; Iter    14/   41] train: loss: 0.0192132
[Epoch 57] ogbg-molbace: 0.651648 val loss: 1.549301
[Epoch 57] ogbg-molbace: 0.714484 test loss: 1.394089
[Epoch 58; Iter     3/   41] train: loss: 0.0135547
[Epoch 58; Iter    33/   41] train: loss: 0.0581790
[Epoch 58] ogbg-molbace: 0.628571 val loss: 1.449312
[Epoch 58] ogbg-molbace: 0.627543 test loss: 2.271517
[Epoch 59; Iter    22/   41] train: loss: 0.0322084
[Epoch 59] ogbg-molbace: 0.658974 val loss: 1.431119
[Epoch 59] ogbg-molbace: 0.643192 test loss: 2.083959
[Epoch 60; Iter    11/   41] train: loss: 0.0032168
[Epoch 60; Iter    41/   41] train: loss: 0.0042654
[Epoch 60] ogbg-molbace: 0.653846 val loss: 1.462094
[Epoch 60] ogbg-molbace: 0.684403 test loss: 1.915865
[Epoch 61; Iter    30/   41] train: loss: 0.0049809
[Epoch 61] ogbg-molbace: 0.635897 val loss: 1.495799
[Epoch 61] ogbg-molbace: 0.663363 test loss: 2.075523
[Epoch 62; Iter    19/   41] train: loss: 0.0082224
[Epoch 62] ogbg-molbace: 0.632234 val loss: 1.621086
[Epoch 62] ogbg-molbace: 0.621805 test loss: 2.234142
[Epoch 63; Iter     8/   41] train: loss: 0.0114977
[Epoch 63; Iter    38/   41] train: loss: 0.1367967
[Epoch 63] ogbg-molbace: 0.706593 val loss: 3.154503
[Epoch 63] ogbg-molbace: 0.670840 test loss: 3.375551
[Epoch 64; Iter    27/   41] train: loss: 0.1496474
[Epoch 64] ogbg-molbace: 0.710623 val loss: 2.079189
[Epoch 64] ogbg-molbace: 0.723700 test loss: 2.152152
[Epoch 65; Iter    16/   41] train: loss: 0.1702383
[Epoch 65] ogbg-molbace: 0.636630 val loss: 2.128163
[Epoch 65] ogbg-molbace: 0.663189 test loss: 1.751521
[Epoch 66; Iter     5/   41] train: loss: 0.0590138
[Epoch 66; Iter    35/   41] train: loss: 0.0560048
[Epoch 66] ogbg-molbace: 0.614652 val loss: 2.305157
[Epoch 66] ogbg-molbace: 0.686142 test loss: 2.048147
[Epoch 67; Iter    24/   41] train: loss: 0.0736219
[Epoch 67] ogbg-molbace: 0.665201 val loss: 1.731451
[Epoch 67] ogbg-molbace: 0.668405 test loss: 2.103201
[Epoch 68; Iter    13/   41] train: loss: 0.0241504
[Epoch 68] ogbg-molbace: 0.632234 val loss: 2.051219
[Epoch 68] ogbg-molbace: 0.689445 test loss: 1.778254
[Epoch 69; Iter     2/   41] train: loss: 0.0125422
[Epoch 69; Iter    32/   41] train: loss: 0.0183797
[Epoch 69] ogbg-molbace: 0.681685 val loss: 2.154261
[Epoch 69] ogbg-molbace: 0.753608 test loss: 1.747054
[Epoch 70; Iter    21/   41] train: loss: 0.0332001
[Epoch 70] ogbg-molbace: 0.661538 val loss: 2.292492
[Epoch 70] ogbg-molbace: 0.741436 test loss: 1.825506
[Epoch 71; Iter    10/   41] train: loss: 0.0045857
[Epoch 71; Iter    40/   41] train: loss: 0.0118081
[Epoch 71] ogbg-molbace: 0.661538 val loss: 2.160889
[Epoch 71] ogbg-molbace: 0.745957 test loss: 1.881871
[Epoch 72; Iter    29/   41] train: loss: 0.0041056
[Epoch 72] ogbg-molbace: 0.646154 val loss: 2.187224
[Epoch 72] ogbg-molbace: 0.732916 test loss: 1.991242
[Epoch 73; Iter    18/   41] train: loss: 0.0022578
[Epoch 73] ogbg-molbace: 0.642491 val loss: 2.181270
[Epoch 73] ogbg-molbace: 0.736220 test loss: 2.187795
[Epoch 74; Iter     7/   41] train: loss: 0.0041487
[Epoch 74; Iter    37/   41] train: loss: 0.0020205
[Epoch 74] ogbg-molbace: 0.629304 val loss: 2.196914
[Epoch 74] ogbg-molbace: 0.737263 test loss: 2.290517
[Epoch 75; Iter    26/   41] train: loss: 0.0431286
[Epoch 75] ogbg-molbace: 0.634066 val loss: 2.264528
[Epoch 75] ogbg-molbace: 0.741784 test loss: 2.208881
[Epoch 76; Iter    15/   41] train: loss: 0.0011181
[Epoch 76] ogbg-molbace: 0.645055 val loss: 2.499255
[Epoch 76] ogbg-molbace: 0.752391 test loss: 2.107999
[Epoch 77; Iter     4/   41] train: loss: 0.0011843
[Epoch 77; Iter    34/   41] train: loss: 0.0019076
[Epoch 77] ogbg-molbace: 0.646520 val loss: 2.542227
[Epoch 77] ogbg-molbace: 0.744392 test loss: 2.146670
[Epoch 78; Iter    23/   41] train: loss: 0.0014113
[Epoch 32] ogbg-molbace: 0.692575 test loss: 1.769693
[Epoch 33; Iter     8/   41] train: loss: 0.5710019
[Epoch 33; Iter    38/   41] train: loss: 0.5873796
[Epoch 33] ogbg-molbace: 0.656410 val loss: 1.566675
[Epoch 33] ogbg-molbace: 0.677100 test loss: 2.110331
[Epoch 34; Iter    27/   41] train: loss: 0.3658081
[Epoch 34] ogbg-molbace: 0.639560 val loss: 2.051142
[Epoch 34] ogbg-molbace: 0.660755 test loss: 2.714773
[Epoch 35; Iter    16/   41] train: loss: 0.3800384
[Epoch 35] ogbg-molbace: 0.641392 val loss: 2.976888
[Epoch 35] ogbg-molbace: 0.717614 test loss: 3.060820
[Epoch 36; Iter     5/   41] train: loss: 0.4375450
[Epoch 36; Iter    35/   41] train: loss: 0.5001560
[Epoch 36] ogbg-molbace: 0.629670 val loss: 1.346680
[Epoch 36] ogbg-molbace: 0.732394 test loss: 2.329822
[Epoch 37; Iter    24/   41] train: loss: 0.4013836
[Epoch 37] ogbg-molbace: 0.570330 val loss: 0.826882
[Epoch 37] ogbg-molbace: 0.647366 test loss: 0.895570
[Epoch 38; Iter    13/   41] train: loss: 0.4546452
[Epoch 38] ogbg-molbace: 0.659341 val loss: 2.353131
[Epoch 38] ogbg-molbace: 0.656581 test loss: 2.396762
[Epoch 39; Iter     2/   41] train: loss: 0.4087700
[Epoch 39; Iter    32/   41] train: loss: 0.5487722
[Epoch 39] ogbg-molbace: 0.680220 val loss: 1.132065
[Epoch 39] ogbg-molbace: 0.784907 test loss: 1.233236
[Epoch 40; Iter    21/   41] train: loss: 0.1854026
[Epoch 40] ogbg-molbace: 0.570330 val loss: 3.401405
[Epoch 40] ogbg-molbace: 0.686837 test loss: 3.304730
[Epoch 41; Iter    10/   41] train: loss: 0.3343603
[Epoch 41; Iter    40/   41] train: loss: 0.2476545
[Epoch 41] ogbg-molbace: 0.613919 val loss: 0.844382
[Epoch 41] ogbg-molbace: 0.718484 test loss: 1.164938
[Epoch 42; Iter    29/   41] train: loss: 0.4367460
[Epoch 42] ogbg-molbace: 0.690110 val loss: 1.835503
[Epoch 42] ogbg-molbace: 0.763693 test loss: 1.314998
[Epoch 43; Iter    18/   41] train: loss: 0.2143477
[Epoch 43] ogbg-molbace: 0.629304 val loss: 0.630319
[Epoch 43] ogbg-molbace: 0.667710 test loss: 0.911560
[Epoch 44; Iter     7/   41] train: loss: 0.2310022
[Epoch 44; Iter    37/   41] train: loss: 0.3178541
[Epoch 44] ogbg-molbace: 0.627473 val loss: 0.834629
[Epoch 44] ogbg-molbace: 0.707007 test loss: 1.194463
[Epoch 45; Iter    26/   41] train: loss: 0.1006083
[Epoch 45] ogbg-molbace: 0.729304 val loss: 1.534873
[Epoch 45] ogbg-molbace: 0.810989 test loss: 0.992775
[Epoch 46; Iter    15/   41] train: loss: 0.1506949
[Epoch 46] ogbg-molbace: 0.643956 val loss: 1.151323
[Epoch 46] ogbg-molbace: 0.772040 test loss: 1.119993
[Epoch 47; Iter     4/   41] train: loss: 0.1149346
[Epoch 47; Iter    34/   41] train: loss: 0.0828341
[Epoch 47] ogbg-molbace: 0.674725 val loss: 1.415357
[Epoch 47] ogbg-molbace: 0.767693 test loss: 0.919059
[Epoch 48; Iter    23/   41] train: loss: 0.0844069
[Epoch 48] ogbg-molbace: 0.687179 val loss: 1.052636
[Epoch 48] ogbg-molbace: 0.817249 test loss: 1.188485
[Epoch 49; Iter    12/   41] train: loss: 0.0257929
[Epoch 49] ogbg-molbace: 0.668498 val loss: 1.433865
[Epoch 49] ogbg-molbace: 0.812207 test loss: 0.956514
[Epoch 50; Iter     1/   41] train: loss: 0.0251232
[Epoch 50; Iter    31/   41] train: loss: 0.0399870
[Epoch 50] ogbg-molbace: 0.662637 val loss: 1.933659
[Epoch 50] ogbg-molbace: 0.810642 test loss: 1.254650
[Epoch 51; Iter    20/   41] train: loss: 0.1123535
[Epoch 51] ogbg-molbace: 0.682051 val loss: 1.716111
[Epoch 51] ogbg-molbace: 0.826291 test loss: 0.950378
[Epoch 52; Iter     9/   41] train: loss: 0.0776031
[Epoch 52; Iter    39/   41] train: loss: 0.0261690
[Epoch 52] ogbg-molbace: 0.653114 val loss: 1.505699
[Epoch 52] ogbg-molbace: 0.784038 test loss: 1.194742
[Epoch 53; Iter    28/   41] train: loss: 0.0128050
[Epoch 53] ogbg-molbace: 0.673626 val loss: 1.469347
[Epoch 53] ogbg-molbace: 0.798122 test loss: 1.040110
[Epoch 54; Iter    17/   41] train: loss: 0.0209994
[Epoch 54] ogbg-molbace: 0.667766 val loss: 1.803152
[Epoch 54] ogbg-molbace: 0.786298 test loss: 1.222382
[Epoch 55; Iter     6/   41] train: loss: 0.0155515
[Epoch 55; Iter    36/   41] train: loss: 0.0040804
[Epoch 55] ogbg-molbace: 0.692308 val loss: 1.551682
[Epoch 55] ogbg-molbace: 0.813945 test loss: 1.089471
[Epoch 56; Iter    25/   41] train: loss: 0.0178404
[Epoch 56] ogbg-molbace: 0.678755 val loss: 1.716928
[Epoch 56] ogbg-molbace: 0.802295 test loss: 1.158616
[Epoch 57; Iter    14/   41] train: loss: 0.0055941
[Epoch 57] ogbg-molbace: 0.699634 val loss: 1.796240
[Epoch 57] ogbg-molbace: 0.817771 test loss: 1.225362
[Epoch 58; Iter     3/   41] train: loss: 0.0015382
[Epoch 58; Iter    33/   41] train: loss: 0.0054381
[Epoch 58] ogbg-molbace: 0.702198 val loss: 1.847220
[Epoch 58] ogbg-molbace: 0.810120 test loss: 1.230759
[Epoch 59; Iter    22/   41] train: loss: 0.0015812
[Epoch 59] ogbg-molbace: 0.703297 val loss: 1.827689
[Epoch 59] ogbg-molbace: 0.811511 test loss: 1.281503
[Epoch 60; Iter    11/   41] train: loss: 0.0029642
[Epoch 60; Iter    41/   41] train: loss: 0.0019844
[Epoch 60] ogbg-molbace: 0.704762 val loss: 2.198306
[Epoch 60] ogbg-molbace: 0.818119 test loss: 1.374658
[Epoch 61; Iter    30/   41] train: loss: 0.0125564
[Epoch 61] ogbg-molbace: 0.698535 val loss: 1.890562
[Epoch 61] ogbg-molbace: 0.808033 test loss: 1.279763
[Epoch 62; Iter    19/   41] train: loss: 0.0016838
[Epoch 62] ogbg-molbace: 0.700366 val loss: 2.199053
[Epoch 62] ogbg-molbace: 0.825596 test loss: 1.229145
[Epoch 63; Iter     8/   41] train: loss: 0.0528043
[Epoch 63; Iter    38/   41] train: loss: 0.1140477
[Epoch 63] ogbg-molbace: 0.705861 val loss: 1.621975
[Epoch 63] ogbg-molbace: 0.743523 test loss: 2.797386
[Epoch 64; Iter    27/   41] train: loss: 0.3977371
[Epoch 64] ogbg-molbace: 0.627106 val loss: 1.267509
[Epoch 64] ogbg-molbace: 0.660233 test loss: 1.866514
[Epoch 65; Iter    16/   41] train: loss: 0.2194787
[Epoch 65] ogbg-molbace: 0.671062 val loss: 1.193703
[Epoch 65] ogbg-molbace: 0.805077 test loss: 0.935893
[Epoch 66; Iter     5/   41] train: loss: 0.3169892
[Epoch 66; Iter    35/   41] train: loss: 0.1194135
[Epoch 66] ogbg-molbace: 0.713187 val loss: 1.483628
[Epoch 66] ogbg-molbace: 0.810816 test loss: 1.219171
[Epoch 67; Iter    24/   41] train: loss: 0.2052075
[Epoch 67] ogbg-molbace: 0.696703 val loss: 1.470172
[Epoch 67] ogbg-molbace: 0.800904 test loss: 1.254209
[Epoch 68; Iter    13/   41] train: loss: 0.0810452
[Epoch 68] ogbg-molbace: 0.697070 val loss: 1.621532
[Epoch 68] ogbg-molbace: 0.794297 test loss: 1.048151
[Epoch 69; Iter     2/   41] train: loss: 0.0117579
[Epoch 69; Iter    32/   41] train: loss: 0.0915194
[Epoch 69] ogbg-molbace: 0.711355 val loss: 1.786250
[Epoch 69] ogbg-molbace: 0.806468 test loss: 1.118277
[Epoch 70; Iter    21/   41] train: loss: 0.0199571
[Epoch 70] ogbg-molbace: 0.723443 val loss: 1.963541
[Epoch 70] ogbg-molbace: 0.816554 test loss: 1.253238
[Epoch 71; Iter    10/   41] train: loss: 0.0041681
[Epoch 71; Iter    40/   41] train: loss: 0.0061628
[Epoch 71] ogbg-molbace: 0.696337 val loss: 2.184317
[Epoch 71] ogbg-molbace: 0.806468 test loss: 1.420001
[Epoch 72; Iter    29/   41] train: loss: 0.0150171
[Epoch 72] ogbg-molbace: 0.690842 val loss: 2.006805
[Epoch 72] ogbg-molbace: 0.818988 test loss: 1.325179
[Epoch 73; Iter    18/   41] train: loss: 0.0043363
[Epoch 73] ogbg-molbace: 0.698168 val loss: 2.178548
[Epoch 73] ogbg-molbace: 0.808555 test loss: 1.309673
[Epoch 74; Iter     7/   41] train: loss: 0.0149998
[Epoch 74; Iter    37/   41] train: loss: 0.0066204
[Epoch 74] ogbg-molbace: 0.694872 val loss: 2.389459
[Epoch 74] ogbg-molbace: 0.814119 test loss: 1.359222
[Epoch 75; Iter    26/   41] train: loss: 0.0012511
[Epoch 75] ogbg-molbace: 0.694139 val loss: 2.532052
[Epoch 75] ogbg-molbace: 0.819336 test loss: 1.387635
[Epoch 76; Iter    15/   41] train: loss: 0.0026909
[Epoch 76] ogbg-molbace: 0.691209 val loss: 2.553600
[Epoch 76] ogbg-molbace: 0.827856 test loss: 1.530183
[Epoch 77; Iter     4/   41] train: loss: 0.0013288
[Epoch 77; Iter    34/   41] train: loss: 0.0029078
[Epoch 77] ogbg-molbace: 0.690842 val loss: 2.695464
[Epoch 77] ogbg-molbace: 0.829769 test loss: 1.543953
[Epoch 78; Iter    23/   41] train: loss: 0.0009396
[Epoch 32] ogbg-molbace: 0.732742 test loss: 1.425044
[Epoch 33; Iter     8/   41] train: loss: 0.4192634
[Epoch 33; Iter    38/   41] train: loss: 0.4660360
[Epoch 33] ogbg-molbace: 0.712088 val loss: 0.898172
[Epoch 33] ogbg-molbace: 0.754130 test loss: 1.232472
[Epoch 34; Iter    27/   41] train: loss: 0.6166006
[Epoch 34] ogbg-molbace: 0.655678 val loss: 0.921311
[Epoch 34] ogbg-molbace: 0.693271 test loss: 1.776989
[Epoch 35; Iter    16/   41] train: loss: 0.4407134
[Epoch 35] ogbg-molbace: 0.587546 val loss: 1.014808
[Epoch 35] ogbg-molbace: 0.632933 test loss: 1.797827
[Epoch 36; Iter     5/   41] train: loss: 0.3709222
[Epoch 36; Iter    35/   41] train: loss: 0.4866998
[Epoch 36] ogbg-molbace: 0.698168 val loss: 1.225522
[Epoch 36] ogbg-molbace: 0.741958 test loss: 1.685929
[Epoch 37; Iter    24/   41] train: loss: 0.3298768
[Epoch 37] ogbg-molbace: 0.615751 val loss: 1.678347
[Epoch 37] ogbg-molbace: 0.731525 test loss: 2.404850
[Epoch 38; Iter    13/   41] train: loss: 0.2646199
[Epoch 38] ogbg-molbace: 0.672894 val loss: 1.044169
[Epoch 38] ogbg-molbace: 0.674318 test loss: 2.063115
[Epoch 39; Iter     2/   41] train: loss: 0.2408276
[Epoch 39; Iter    32/   41] train: loss: 0.2256209
[Epoch 39] ogbg-molbace: 0.694872 val loss: 1.209222
[Epoch 39] ogbg-molbace: 0.752217 test loss: 1.834685
[Epoch 40; Iter    21/   41] train: loss: 0.2346114
[Epoch 40] ogbg-molbace: 0.650549 val loss: 2.183944
[Epoch 40] ogbg-molbace: 0.767171 test loss: 2.269953
[Epoch 41; Iter    10/   41] train: loss: 0.1871436
[Epoch 41; Iter    40/   41] train: loss: 0.3381014
[Epoch 41] ogbg-molbace: 0.686813 val loss: 1.726698
[Epoch 41] ogbg-molbace: 0.767693 test loss: 1.151868
[Epoch 42; Iter    29/   41] train: loss: 0.3823093
[Epoch 42] ogbg-molbace: 0.646154 val loss: 2.184307
[Epoch 42] ogbg-molbace: 0.747174 test loss: 2.453719
[Epoch 43; Iter    18/   41] train: loss: 0.3227891
[Epoch 43] ogbg-molbace: 0.686813 val loss: 1.440284
[Epoch 43] ogbg-molbace: 0.806468 test loss: 0.984754
[Epoch 44; Iter     7/   41] train: loss: 0.2658844
[Epoch 44; Iter    37/   41] train: loss: 0.3323859
[Epoch 44] ogbg-molbace: 0.666667 val loss: 1.559767
[Epoch 44] ogbg-molbace: 0.779690 test loss: 0.910118
[Epoch 45; Iter    26/   41] train: loss: 0.1155362
[Epoch 45] ogbg-molbace: 0.684615 val loss: 1.316743
[Epoch 45] ogbg-molbace: 0.810816 test loss: 1.050950
[Epoch 46; Iter    15/   41] train: loss: 0.2432901
[Epoch 46] ogbg-molbace: 0.601099 val loss: 2.112391
[Epoch 46] ogbg-molbace: 0.719005 test loss: 2.243254
[Epoch 47; Iter     4/   41] train: loss: 0.1794995
[Epoch 47; Iter    34/   41] train: loss: 0.0561218
[Epoch 47] ogbg-molbace: 0.655311 val loss: 1.859235
[Epoch 47] ogbg-molbace: 0.731699 test loss: 2.466233
[Epoch 48; Iter    23/   41] train: loss: 0.0414625
[Epoch 48] ogbg-molbace: 0.664835 val loss: 1.495949
[Epoch 48] ogbg-molbace: 0.757781 test loss: 1.589209
[Epoch 49; Iter    12/   41] train: loss: 0.0752672
[Epoch 49] ogbg-molbace: 0.678755 val loss: 2.193458
[Epoch 49] ogbg-molbace: 0.809077 test loss: 1.951183
[Epoch 50; Iter     1/   41] train: loss: 0.0405629
[Epoch 50; Iter    31/   41] train: loss: 0.0205442
[Epoch 50] ogbg-molbace: 0.660440 val loss: 2.032173
[Epoch 50] ogbg-molbace: 0.806295 test loss: 1.553778
[Epoch 51; Iter    20/   41] train: loss: 0.0304747
[Epoch 51] ogbg-molbace: 0.643956 val loss: 2.673758
[Epoch 51] ogbg-molbace: 0.757260 test loss: 2.956831
[Epoch 52; Iter     9/   41] train: loss: 0.0282010
[Epoch 52; Iter    39/   41] train: loss: 0.2856216
[Epoch 52] ogbg-molbace: 0.658608 val loss: 1.823864
[Epoch 52] ogbg-molbace: 0.788037 test loss: 1.934297
[Epoch 53; Iter    28/   41] train: loss: 0.0812554
[Epoch 53] ogbg-molbace: 0.658608 val loss: 1.907337
[Epoch 53] ogbg-molbace: 0.771170 test loss: 1.961151
[Epoch 54; Iter    17/   41] train: loss: 0.2808474
[Epoch 54] ogbg-molbace: 0.624908 val loss: 3.410217
[Epoch 54] ogbg-molbace: 0.751521 test loss: 3.036361
[Epoch 55; Iter     6/   41] train: loss: 0.2127390
[Epoch 55; Iter    36/   41] train: loss: 0.0389356
[Epoch 55] ogbg-molbace: 0.652381 val loss: 2.079452
[Epoch 55] ogbg-molbace: 0.807338 test loss: 1.328083
[Epoch 56; Iter    25/   41] train: loss: 0.0710307
[Epoch 56] ogbg-molbace: 0.646154 val loss: 1.769773
[Epoch 56] ogbg-molbace: 0.788732 test loss: 1.285047
[Epoch 57; Iter    14/   41] train: loss: 0.0350383
[Epoch 57] ogbg-molbace: 0.643590 val loss: 1.960025
[Epoch 57] ogbg-molbace: 0.778995 test loss: 1.581193
[Epoch 58; Iter     3/   41] train: loss: 0.0069102
[Epoch 58; Iter    33/   41] train: loss: 0.0392017
[Epoch 58] ogbg-molbace: 0.577656 val loss: 1.980391
[Epoch 58] ogbg-molbace: 0.653452 test loss: 2.540508
[Epoch 59; Iter    22/   41] train: loss: 0.1820448
[Epoch 59] ogbg-molbace: 0.643223 val loss: 2.196182
[Epoch 59] ogbg-molbace: 0.742653 test loss: 2.218107
[Epoch 60; Iter    11/   41] train: loss: 0.0537369
[Epoch 60; Iter    41/   41] train: loss: 0.0066089
[Epoch 60] ogbg-molbace: 0.632234 val loss: 2.054724
[Epoch 60] ogbg-molbace: 0.786646 test loss: 1.937474
[Epoch 61; Iter    30/   41] train: loss: 0.0267536
[Epoch 61] ogbg-molbace: 0.637363 val loss: 2.728214
[Epoch 61] ogbg-molbace: 0.759346 test loss: 2.019920
[Epoch 62; Iter    19/   41] train: loss: 0.0067421
[Epoch 62] ogbg-molbace: 0.647985 val loss: 1.885908
[Epoch 62] ogbg-molbace: 0.777082 test loss: 2.008111
[Epoch 63; Iter     8/   41] train: loss: 0.0250529
[Epoch 63; Iter    38/   41] train: loss: 0.0101469
[Epoch 63] ogbg-molbace: 0.625275 val loss: 1.596424
[Epoch 63] ogbg-molbace: 0.774996 test loss: 1.049457
[Epoch 64; Iter    27/   41] train: loss: 0.0245698
[Epoch 64] ogbg-molbace: 0.647619 val loss: 2.303798
[Epoch 64] ogbg-molbace: 0.819510 test loss: 2.500077
[Epoch 65; Iter    16/   41] train: loss: 0.0291963
[Epoch 65] ogbg-molbace: 0.613187 val loss: 2.055455
[Epoch 65] ogbg-molbace: 0.759868 test loss: 1.429094
[Epoch 66; Iter     5/   41] train: loss: 0.0145142
[Epoch 66; Iter    35/   41] train: loss: 0.1986803
[Epoch 66] ogbg-molbace: 0.653846 val loss: 2.043367
[Epoch 66] ogbg-molbace: 0.808903 test loss: 2.125628
[Epoch 67; Iter    24/   41] train: loss: 0.0491083
[Epoch 67] ogbg-molbace: 0.660073 val loss: 2.031385
[Epoch 67] ogbg-molbace: 0.797600 test loss: 1.363679
[Epoch 68; Iter    13/   41] train: loss: 0.0563580
[Epoch 68] ogbg-molbace: 0.636996 val loss: 1.997313
[Epoch 68] ogbg-molbace: 0.786994 test loss: 1.564538
[Epoch 69; Iter     2/   41] train: loss: 0.0723283
[Epoch 69; Iter    32/   41] train: loss: 0.0175304
[Epoch 69] ogbg-molbace: 0.641758 val loss: 2.038431
[Epoch 69] ogbg-molbace: 0.768040 test loss: 1.704344
[Epoch 70; Iter    21/   41] train: loss: 0.0108829
[Epoch 70] ogbg-molbace: 0.631136 val loss: 2.449132
[Epoch 70] ogbg-molbace: 0.745609 test loss: 1.927136
[Epoch 71; Iter    10/   41] train: loss: 0.0123866
[Epoch 71; Iter    40/   41] train: loss: 0.0146008
[Epoch 71] ogbg-molbace: 0.640659 val loss: 2.197206
[Epoch 71] ogbg-molbace: 0.801774 test loss: 1.517433
[Epoch 72; Iter    29/   41] train: loss: 0.0120370
[Epoch 72] ogbg-molbace: 0.607326 val loss: 2.433864
[Epoch 72] ogbg-molbace: 0.749261 test loss: 1.917480
[Epoch 73; Iter    18/   41] train: loss: 0.0030482
[Epoch 73] ogbg-molbace: 0.609524 val loss: 2.407630
[Epoch 73] ogbg-molbace: 0.757433 test loss: 1.664599
[Epoch 74; Iter     7/   41] train: loss: 0.0075325
[Epoch 74; Iter    37/   41] train: loss: 0.0042682
[Epoch 74] ogbg-molbace: 0.587546 val loss: 2.901705
[Epoch 74] ogbg-molbace: 0.709442 test loss: 2.560623
[Epoch 75; Iter    26/   41] train: loss: 0.0045301
[Epoch 75] ogbg-molbace: 0.637363 val loss: 2.419393
[Epoch 75] ogbg-molbace: 0.798122 test loss: 1.862936
[Epoch 76; Iter    15/   41] train: loss: 0.0018441
[Epoch 76] ogbg-molbace: 0.624176 val loss: 2.740654
[Epoch 76] ogbg-molbace: 0.753782 test loss: 2.219349
[Epoch 77; Iter     4/   41] train: loss: 0.0017077
[Epoch 77; Iter    34/   41] train: loss: 0.0018162
[Epoch 77] ogbg-molbace: 0.613553 val loss: 2.829409
[Epoch 77] ogbg-molbace: 0.750478 test loss: 2.275139
[Epoch 78; Iter    23/   41] train: loss: 0.0251972
[Epoch 32] ogbg-molbace: 0.775170 test loss: 0.811893
[Epoch 33; Iter     8/   41] train: loss: 0.3382284
[Epoch 33; Iter    38/   41] train: loss: 0.3682047
[Epoch 33] ogbg-molbace: 0.746154 val loss: 0.847032
[Epoch 33] ogbg-molbace: 0.814467 test loss: 0.757076
[Epoch 34; Iter    27/   41] train: loss: 0.3620946
[Epoch 34] ogbg-molbace: 0.709890 val loss: 1.114558
[Epoch 34] ogbg-molbace: 0.749609 test loss: 1.118184
[Epoch 35; Iter    16/   41] train: loss: 0.3798922
[Epoch 35] ogbg-molbace: 0.704396 val loss: 1.038317
[Epoch 35] ogbg-molbace: 0.741089 test loss: 0.693891
[Epoch 36; Iter     5/   41] train: loss: 0.3567378
[Epoch 36; Iter    35/   41] train: loss: 0.5003220
[Epoch 36] ogbg-molbace: 0.678022 val loss: 1.065687
[Epoch 36] ogbg-molbace: 0.768562 test loss: 1.116155
[Epoch 37; Iter    24/   41] train: loss: 0.3313085
[Epoch 37] ogbg-molbace: 0.684982 val loss: 0.929916
[Epoch 37] ogbg-molbace: 0.764389 test loss: 0.588042
[Epoch 38; Iter    13/   41] train: loss: 0.3775475
[Epoch 38] ogbg-molbace: 0.634799 val loss: 0.818262
[Epoch 38] ogbg-molbace: 0.745262 test loss: 1.069723
[Epoch 39; Iter     2/   41] train: loss: 0.3149311
[Epoch 39; Iter    32/   41] train: loss: 0.2517487
[Epoch 39] ogbg-molbace: 0.708425 val loss: 0.931368
[Epoch 39] ogbg-molbace: 0.828378 test loss: 0.697325
[Epoch 40; Iter    21/   41] train: loss: 0.2404693
[Epoch 40] ogbg-molbace: 0.595971 val loss: 1.086934
[Epoch 40] ogbg-molbace: 0.793775 test loss: 0.708940
[Epoch 41; Iter    10/   41] train: loss: 0.2414226
[Epoch 41; Iter    40/   41] train: loss: 0.4066496
[Epoch 41] ogbg-molbace: 0.623443 val loss: 1.948307
[Epoch 41] ogbg-molbace: 0.756564 test loss: 1.939168
[Epoch 42; Iter    29/   41] train: loss: 0.3747616
[Epoch 42] ogbg-molbace: 0.643223 val loss: 0.990289
[Epoch 42] ogbg-molbace: 0.793601 test loss: 0.947288
[Epoch 43; Iter    18/   41] train: loss: 0.2984340
[Epoch 43] ogbg-molbace: 0.705128 val loss: 0.788756
[Epoch 43] ogbg-molbace: 0.782299 test loss: 0.841320
[Epoch 44; Iter     7/   41] train: loss: 0.3220544
[Epoch 44; Iter    37/   41] train: loss: 0.3385560
[Epoch 44] ogbg-molbace: 0.696337 val loss: 0.897714
[Epoch 44] ogbg-molbace: 0.823683 test loss: 1.180371
[Epoch 45; Iter    26/   41] train: loss: 0.2793334
[Epoch 45] ogbg-molbace: 0.671062 val loss: 1.709869
[Epoch 45] ogbg-molbace: 0.805773 test loss: 1.129922
[Epoch 46; Iter    15/   41] train: loss: 0.2139599
[Epoch 46] ogbg-molbace: 0.730769 val loss: 0.993065
[Epoch 46] ogbg-molbace: 0.801426 test loss: 1.231348
[Epoch 47; Iter     4/   41] train: loss: 0.3321202
[Epoch 47; Iter    34/   41] train: loss: 0.4637654
[Epoch 47] ogbg-molbace: 0.698168 val loss: 1.054158
[Epoch 47] ogbg-molbace: 0.825769 test loss: 1.234707
[Epoch 48; Iter    23/   41] train: loss: 0.2408220
[Epoch 48] ogbg-molbace: 0.746886 val loss: 1.000617
[Epoch 48] ogbg-molbace: 0.784733 test loss: 0.968914
[Epoch 49; Iter    12/   41] train: loss: 0.2651985
[Epoch 49] ogbg-molbace: 0.724176 val loss: 1.213507
[Epoch 49] ogbg-molbace: 0.819857 test loss: 0.959285
[Epoch 50; Iter     1/   41] train: loss: 0.3327173
[Epoch 50; Iter    31/   41] train: loss: 0.2256432
[Epoch 50] ogbg-molbace: 0.692674 val loss: 1.113583
[Epoch 50] ogbg-molbace: 0.826813 test loss: 1.362158
[Epoch 51; Iter    20/   41] train: loss: 0.3948951
[Epoch 51] ogbg-molbace: 0.684982 val loss: 1.157405
[Epoch 51] ogbg-molbace: 0.789602 test loss: 1.234604
[Epoch 52; Iter     9/   41] train: loss: 0.3461699
[Epoch 52; Iter    39/   41] train: loss: 0.3585025
[Epoch 52] ogbg-molbace: 0.695238 val loss: 0.895303
[Epoch 52] ogbg-molbace: 0.755347 test loss: 0.768231
[Epoch 53; Iter    28/   41] train: loss: 0.1695030
[Epoch 53] ogbg-molbace: 0.698535 val loss: 0.976339
[Epoch 53] ogbg-molbace: 0.791862 test loss: 1.318024
[Epoch 54; Iter    17/   41] train: loss: 0.1961433
[Epoch 54] ogbg-molbace: 0.717582 val loss: 1.378854
[Epoch 54] ogbg-molbace: 0.821075 test loss: 1.445208
[Epoch 55; Iter     6/   41] train: loss: 0.3447553
[Epoch 55; Iter    36/   41] train: loss: 0.2659015
[Epoch 55] ogbg-molbace: 0.670330 val loss: 1.369810
[Epoch 55] ogbg-molbace: 0.809251 test loss: 0.999307
[Epoch 56; Iter    25/   41] train: loss: 0.2853934
[Epoch 56] ogbg-molbace: 0.739560 val loss: 1.269954
[Epoch 56] ogbg-molbace: 0.802817 test loss: 1.288673
[Epoch 57; Iter    14/   41] train: loss: 0.3674282
[Epoch 57] ogbg-molbace: 0.716850 val loss: 1.375456
[Epoch 57] ogbg-molbace: 0.845418 test loss: 1.443662
[Epoch 58; Iter     3/   41] train: loss: 0.2655936
[Epoch 58; Iter    33/   41] train: loss: 0.2238102
[Epoch 58] ogbg-molbace: 0.712088 val loss: 1.089698
[Epoch 58] ogbg-molbace: 0.779343 test loss: 1.610223
[Epoch 59; Iter    22/   41] train: loss: 0.3767146
[Epoch 59] ogbg-molbace: 0.727839 val loss: 1.144921
[Epoch 59] ogbg-molbace: 0.790819 test loss: 1.247043
[Epoch 60; Iter    11/   41] train: loss: 0.4260295
[Epoch 60; Iter    41/   41] train: loss: 0.1620778
[Epoch 60] ogbg-molbace: 0.689011 val loss: 1.176654
[Epoch 60] ogbg-molbace: 0.759346 test loss: 1.577692
[Epoch 61; Iter    30/   41] train: loss: 0.2547365
[Epoch 61] ogbg-molbace: 0.666667 val loss: 0.918498
[Epoch 61] ogbg-molbace: 0.793427 test loss: 1.132614
[Epoch 62; Iter    19/   41] train: loss: 0.1968218
[Epoch 62] ogbg-molbace: 0.670696 val loss: 1.437571
[Epoch 62] ogbg-molbace: 0.752565 test loss: 1.645421
[Epoch 63; Iter     8/   41] train: loss: 0.2657888
[Epoch 63; Iter    38/   41] train: loss: 0.2240151
[Epoch 63] ogbg-molbace: 0.713187 val loss: 0.973923
[Epoch 63] ogbg-molbace: 0.848200 test loss: 0.718532
[Epoch 64; Iter    27/   41] train: loss: 0.2496355
[Epoch 64] ogbg-molbace: 0.758608 val loss: 1.054816
[Epoch 64] ogbg-molbace: 0.808381 test loss: 1.301527
[Epoch 65; Iter    16/   41] train: loss: 0.1306040
[Epoch 65] ogbg-molbace: 0.647619 val loss: 1.363889
[Epoch 65] ogbg-molbace: 0.802121 test loss: 1.561015
[Epoch 66; Iter     5/   41] train: loss: 0.1926614
[Epoch 66; Iter    35/   41] train: loss: 0.2850151
[Epoch 66] ogbg-molbace: 0.718681 val loss: 1.139613
[Epoch 66] ogbg-molbace: 0.819162 test loss: 1.380144
[Epoch 67; Iter    24/   41] train: loss: 0.1393800
[Epoch 67] ogbg-molbace: 0.724542 val loss: 1.336532
[Epoch 67] ogbg-molbace: 0.818466 test loss: 1.327104
[Epoch 68; Iter    13/   41] train: loss: 0.3294511
[Epoch 68] ogbg-molbace: 0.731502 val loss: 0.916428
[Epoch 68] ogbg-molbace: 0.769605 test loss: 1.608873
[Epoch 69; Iter     2/   41] train: loss: 0.1844806
[Epoch 69; Iter    32/   41] train: loss: 0.1373059
[Epoch 69] ogbg-molbace: 0.706593 val loss: 1.159662
[Epoch 69] ogbg-molbace: 0.817249 test loss: 1.323934
[Epoch 70; Iter    21/   41] train: loss: 0.1450637
[Epoch 70] ogbg-molbace: 0.700000 val loss: 1.302660
[Epoch 70] ogbg-molbace: 0.788037 test loss: 1.528361
[Epoch 71; Iter    10/   41] train: loss: 0.1370911
[Epoch 71; Iter    40/   41] train: loss: 0.1600663
[Epoch 71] ogbg-molbace: 0.712454 val loss: 1.177203
[Epoch 71] ogbg-molbace: 0.795340 test loss: 1.520476
[Epoch 72; Iter    29/   41] train: loss: 0.0720832
[Epoch 72] ogbg-molbace: 0.737729 val loss: 1.250711
[Epoch 72] ogbg-molbace: 0.771170 test loss: 1.626826
[Epoch 73; Iter    18/   41] train: loss: 0.2008254
[Epoch 73] ogbg-molbace: 0.730037 val loss: 1.105832
[Epoch 73] ogbg-molbace: 0.800730 test loss: 1.429825
[Epoch 74; Iter     7/   41] train: loss: 0.2006007
[Epoch 74; Iter    37/   41] train: loss: 0.1343438
[Epoch 74] ogbg-molbace: 0.711722 val loss: 1.552546
[Epoch 74] ogbg-molbace: 0.802817 test loss: 1.623629
[Epoch 75; Iter    26/   41] train: loss: 0.1455090
[Epoch 75] ogbg-molbace: 0.730037 val loss: 1.333399
[Epoch 75] ogbg-molbace: 0.797774 test loss: 1.973093
[Epoch 76; Iter    15/   41] train: loss: 0.0610520
[Epoch 76] ogbg-molbace: 0.726007 val loss: 1.285588
[Epoch 76] ogbg-molbace: 0.790297 test loss: 1.668063
[Epoch 77; Iter     4/   41] train: loss: 0.0933332
[Epoch 77; Iter    34/   41] train: loss: 0.0978055
[Epoch 77] ogbg-molbace: 0.713919 val loss: 1.183339
[Epoch 77] ogbg-molbace: 0.771866 test loss: 1.931118
[Epoch 78; Iter    23/   41] train: loss: 0.0535809
[Epoch 32] ogbg-molbace: 0.793079 test loss: 0.972748
[Epoch 33; Iter     8/   41] train: loss: 0.4664656
[Epoch 33; Iter    38/   41] train: loss: 0.4749136
[Epoch 33] ogbg-molbace: 0.690476 val loss: 0.893515
[Epoch 33] ogbg-molbace: 0.810989 test loss: 0.928181
[Epoch 34; Iter    27/   41] train: loss: 0.5195071
[Epoch 34] ogbg-molbace: 0.718315 val loss: 0.942915
[Epoch 34] ogbg-molbace: 0.811163 test loss: 0.855380
[Epoch 35; Iter    16/   41] train: loss: 0.4794209
[Epoch 35] ogbg-molbace: 0.690842 val loss: 1.212945
[Epoch 35] ogbg-molbace: 0.743523 test loss: 1.268775
[Epoch 36; Iter     5/   41] train: loss: 0.4227619
[Epoch 36; Iter    35/   41] train: loss: 0.4029105
[Epoch 36] ogbg-molbace: 0.732967 val loss: 0.836615
[Epoch 36] ogbg-molbace: 0.838289 test loss: 0.608944
[Epoch 37; Iter    24/   41] train: loss: 0.4754611
[Epoch 37] ogbg-molbace: 0.660806 val loss: 1.269424
[Epoch 37] ogbg-molbace: 0.782994 test loss: 0.849853
[Epoch 38; Iter    13/   41] train: loss: 0.3841422
[Epoch 38] ogbg-molbace: 0.678755 val loss: 0.835122
[Epoch 38] ogbg-molbace: 0.764563 test loss: 0.589416
[Epoch 39; Iter     2/   41] train: loss: 0.4671015
[Epoch 39; Iter    32/   41] train: loss: 0.3165196
[Epoch 39] ogbg-molbace: 0.730037 val loss: 0.894795
[Epoch 39] ogbg-molbace: 0.742480 test loss: 1.141526
[Epoch 40; Iter    21/   41] train: loss: 0.2437165
[Epoch 40] ogbg-molbace: 0.620147 val loss: 1.248324
[Epoch 40] ogbg-molbace: 0.772387 test loss: 0.853663
[Epoch 41; Iter    10/   41] train: loss: 0.6441117
[Epoch 41; Iter    40/   41] train: loss: 0.6029141
[Epoch 41] ogbg-molbace: 0.579487 val loss: 1.831505
[Epoch 41] ogbg-molbace: 0.669449 test loss: 2.044939
[Epoch 42; Iter    29/   41] train: loss: 0.4219172
[Epoch 42] ogbg-molbace: 0.707692 val loss: 0.930048
[Epoch 42] ogbg-molbace: 0.798122 test loss: 0.876903
[Epoch 43; Iter    18/   41] train: loss: 0.2873951
[Epoch 43] ogbg-molbace: 0.650549 val loss: 0.734027
[Epoch 43] ogbg-molbace: 0.747174 test loss: 0.908205
[Epoch 44; Iter     7/   41] train: loss: 0.2336060
[Epoch 44; Iter    37/   41] train: loss: 0.4077275
[Epoch 44] ogbg-molbace: 0.719414 val loss: 1.056019
[Epoch 44] ogbg-molbace: 0.686663 test loss: 1.297658
[Epoch 45; Iter    26/   41] train: loss: 0.2742249
[Epoch 45] ogbg-molbace: 0.636630 val loss: 1.524149
[Epoch 45] ogbg-molbace: 0.799339 test loss: 1.352214
[Epoch 46; Iter    15/   41] train: loss: 0.2617928
[Epoch 46] ogbg-molbace: 0.733700 val loss: 0.939507
[Epoch 46] ogbg-molbace: 0.769258 test loss: 1.002710
[Epoch 47; Iter     4/   41] train: loss: 0.4140804
[Epoch 47; Iter    34/   41] train: loss: 0.2462161
[Epoch 47] ogbg-molbace: 0.737729 val loss: 0.875470
[Epoch 47] ogbg-molbace: 0.808555 test loss: 0.765373
[Epoch 48; Iter    23/   41] train: loss: 0.2429998
[Epoch 48] ogbg-molbace: 0.690842 val loss: 1.410110
[Epoch 48] ogbg-molbace: 0.829943 test loss: 1.190477
[Epoch 49; Iter    12/   41] train: loss: 0.3144184
[Epoch 49] ogbg-molbace: 0.751282 val loss: 0.756688
[Epoch 49] ogbg-molbace: 0.769431 test loss: 1.844611
[Epoch 50; Iter     1/   41] train: loss: 0.3157371
[Epoch 50; Iter    31/   41] train: loss: 0.2412002
[Epoch 50] ogbg-molbace: 0.722711 val loss: 1.035440
[Epoch 50] ogbg-molbace: 0.813076 test loss: 1.700341
[Epoch 51; Iter    20/   41] train: loss: 0.3969741
[Epoch 51] ogbg-molbace: 0.650549 val loss: 1.163348
[Epoch 51] ogbg-molbace: 0.793949 test loss: 1.144015
[Epoch 52; Iter     9/   41] train: loss: 0.3099523
[Epoch 52; Iter    39/   41] train: loss: 0.2027982
[Epoch 52] ogbg-molbace: 0.730403 val loss: 1.101282
[Epoch 52] ogbg-molbace: 0.806295 test loss: 1.255247
[Epoch 53; Iter    28/   41] train: loss: 0.2261544
[Epoch 53] ogbg-molbace: 0.756044 val loss: 1.131873
[Epoch 53] ogbg-molbace: 0.801774 test loss: 1.244147
[Epoch 54; Iter    17/   41] train: loss: 0.2532418
[Epoch 54] ogbg-molbace: 0.749817 val loss: 0.987090
[Epoch 54] ogbg-molbace: 0.771866 test loss: 1.444569
[Epoch 55; Iter     6/   41] train: loss: 0.0922495
[Epoch 55; Iter    36/   41] train: loss: 0.2313154
[Epoch 55] ogbg-molbace: 0.682051 val loss: 1.071085
[Epoch 55] ogbg-molbace: 0.822466 test loss: 0.939378
[Epoch 56; Iter    25/   41] train: loss: 0.2165308
[Epoch 56] ogbg-molbace: 0.685714 val loss: 1.219018
[Epoch 56] ogbg-molbace: 0.795862 test loss: 1.322766
[Epoch 57; Iter    14/   41] train: loss: 0.3517072
[Epoch 57] ogbg-molbace: 0.724908 val loss: 1.206080
[Epoch 57] ogbg-molbace: 0.756216 test loss: 1.485279
[Epoch 58; Iter     3/   41] train: loss: 0.2949850
[Epoch 58; Iter    33/   41] train: loss: 0.2351948
[Epoch 58] ogbg-molbace: 0.695604 val loss: 1.303465
[Epoch 58] ogbg-molbace: 0.810468 test loss: 0.881548
[Epoch 59; Iter    22/   41] train: loss: 0.2352206
[Epoch 59] ogbg-molbace: 0.653480 val loss: 1.384245
[Epoch 59] ogbg-molbace: 0.783864 test loss: 1.502041
[Epoch 60; Iter    11/   41] train: loss: 0.5318975
[Epoch 60; Iter    41/   41] train: loss: 0.4693882
[Epoch 60] ogbg-molbace: 0.673260 val loss: 1.410474
[Epoch 60] ogbg-molbace: 0.791341 test loss: 1.703307
[Epoch 61; Iter    30/   41] train: loss: 0.4407798
[Epoch 61] ogbg-molbace: 0.698535 val loss: 1.304696
[Epoch 61] ogbg-molbace: 0.765954 test loss: 1.795703
[Epoch 62; Iter    19/   41] train: loss: 0.4237574
[Epoch 62] ogbg-molbace: 0.745788 val loss: 0.927362
[Epoch 62] ogbg-molbace: 0.814293 test loss: 1.258181
[Epoch 63; Iter     8/   41] train: loss: 0.2686975
[Epoch 63; Iter    38/   41] train: loss: 0.2084450
[Epoch 63] ogbg-molbace: 0.721978 val loss: 1.397296
[Epoch 63] ogbg-molbace: 0.786820 test loss: 1.830700
[Epoch 64; Iter    27/   41] train: loss: 0.1207911
[Epoch 64] ogbg-molbace: 0.748718 val loss: 1.125784
[Epoch 64] ogbg-molbace: 0.782473 test loss: 1.367174
[Epoch 65; Iter    16/   41] train: loss: 0.4020572
[Epoch 65] ogbg-molbace: 0.781685 val loss: 0.754225
[Epoch 65] ogbg-molbace: 0.778995 test loss: 1.508768
[Epoch 66; Iter     5/   41] train: loss: 0.1154000
[Epoch 66; Iter    35/   41] train: loss: 0.4919141
[Epoch 66] ogbg-molbace: 0.684615 val loss: 1.266428
[Epoch 66] ogbg-molbace: 0.694662 test loss: 1.644458
[Epoch 67; Iter    24/   41] train: loss: 0.3068199
[Epoch 67] ogbg-molbace: 0.728205 val loss: 1.056377
[Epoch 67] ogbg-molbace: 0.764737 test loss: 1.795126
[Epoch 68; Iter    13/   41] train: loss: 0.1980861
[Epoch 68] ogbg-molbace: 0.746886 val loss: 1.327911
[Epoch 68] ogbg-molbace: 0.761433 test loss: 2.185068
[Epoch 69; Iter     2/   41] train: loss: 0.1768414
[Epoch 69; Iter    32/   41] train: loss: 0.1910209
[Epoch 69] ogbg-molbace: 0.749084 val loss: 1.491775
[Epoch 69] ogbg-molbace: 0.757781 test loss: 1.901340
[Epoch 70; Iter    21/   41] train: loss: 0.2761658
[Epoch 70] ogbg-molbace: 0.729670 val loss: 1.208059
[Epoch 70] ogbg-molbace: 0.727352 test loss: 1.962787
[Epoch 71; Iter    10/   41] train: loss: 0.1913993
[Epoch 71; Iter    40/   41] train: loss: 0.2396839
[Epoch 71] ogbg-molbace: 0.672161 val loss: 1.439497
[Epoch 71] ogbg-molbace: 0.783168 test loss: 1.378604
[Epoch 72; Iter    29/   41] train: loss: 0.1326229
[Epoch 72] ogbg-molbace: 0.759341 val loss: 1.070875
[Epoch 72] ogbg-molbace: 0.798122 test loss: 1.661045
[Epoch 73; Iter    18/   41] train: loss: 0.1185548
[Epoch 73] ogbg-molbace: 0.772527 val loss: 1.202331
[Epoch 73] ogbg-molbace: 0.784385 test loss: 1.738998
[Epoch 74; Iter     7/   41] train: loss: 0.0818733
[Epoch 74; Iter    37/   41] train: loss: 0.6287865
[Epoch 74] ogbg-molbace: 0.747985 val loss: 1.155335
[Epoch 74] ogbg-molbace: 0.787689 test loss: 1.344679
[Epoch 75; Iter    26/   41] train: loss: 0.1770524
[Epoch 75] ogbg-molbace: 0.787546 val loss: 0.914178
[Epoch 75] ogbg-molbace: 0.779517 test loss: 1.780957
[Epoch 76; Iter    15/   41] train: loss: 0.2217532
[Epoch 76] ogbg-molbace: 0.794505 val loss: 1.044714
[Epoch 76] ogbg-molbace: 0.774300 test loss: 1.737509
[Epoch 77; Iter     4/   41] train: loss: 0.0797331
[Epoch 77; Iter    34/   41] train: loss: 0.2045431
[Epoch 77] ogbg-molbace: 0.763736 val loss: 1.337667
[Epoch 77] ogbg-molbace: 0.790819 test loss: 2.454137
[Epoch 78; Iter    23/   41] train: loss: 0.0674460
[Epoch 32] ogbg-molbace: 0.788732 test loss: 0.726849
[Epoch 33; Iter     8/   41] train: loss: 0.3832654
[Epoch 33; Iter    38/   41] train: loss: 0.4842205
[Epoch 33] ogbg-molbace: 0.705128 val loss: 0.933190
[Epoch 33] ogbg-molbace: 0.801774 test loss: 0.943386
[Epoch 34; Iter    27/   41] train: loss: 0.3651215
[Epoch 34] ogbg-molbace: 0.685348 val loss: 0.979617
[Epoch 34] ogbg-molbace: 0.763693 test loss: 0.827178
[Epoch 35; Iter    16/   41] train: loss: 0.3905387
[Epoch 35] ogbg-molbace: 0.671429 val loss: 0.975925
[Epoch 35] ogbg-molbace: 0.718658 test loss: 1.088127
[Epoch 36; Iter     5/   41] train: loss: 0.4010687
[Epoch 36; Iter    35/   41] train: loss: 0.3446886
[Epoch 36] ogbg-molbace: 0.743956 val loss: 0.908092
[Epoch 36] ogbg-molbace: 0.751695 test loss: 0.930742
[Epoch 37; Iter    24/   41] train: loss: 0.2595874
[Epoch 37] ogbg-molbace: 0.684249 val loss: 0.750459
[Epoch 37] ogbg-molbace: 0.798991 test loss: 0.597572
[Epoch 38; Iter    13/   41] train: loss: 0.3473659
[Epoch 38] ogbg-molbace: 0.708425 val loss: 1.395978
[Epoch 38] ogbg-molbace: 0.724396 test loss: 1.507499
[Epoch 39; Iter     2/   41] train: loss: 0.5945112
[Epoch 39; Iter    32/   41] train: loss: 0.4152804
[Epoch 39] ogbg-molbace: 0.692308 val loss: 1.415171
[Epoch 39] ogbg-molbace: 0.814293 test loss: 1.483020
[Epoch 40; Iter    21/   41] train: loss: 0.3300152
[Epoch 40] ogbg-molbace: 0.678388 val loss: 0.928880
[Epoch 40] ogbg-molbace: 0.787515 test loss: 0.774711
[Epoch 41; Iter    10/   41] train: loss: 0.3741730
[Epoch 41; Iter    40/   41] train: loss: 0.2375068
[Epoch 41] ogbg-molbace: 0.691941 val loss: 1.107865
[Epoch 41] ogbg-molbace: 0.785776 test loss: 1.008120
[Epoch 42; Iter    29/   41] train: loss: 0.2957832
[Epoch 42] ogbg-molbace: 0.687912 val loss: 1.126412
[Epoch 42] ogbg-molbace: 0.797948 test loss: 1.133571
[Epoch 43; Iter    18/   41] train: loss: 0.2433480
[Epoch 43] ogbg-molbace: 0.709158 val loss: 1.085100
[Epoch 43] ogbg-molbace: 0.795166 test loss: 1.085499
[Epoch 44; Iter     7/   41] train: loss: 0.3656789
[Epoch 44; Iter    37/   41] train: loss: 0.3513231
[Epoch 44] ogbg-molbace: 0.723077 val loss: 0.984375
[Epoch 44] ogbg-molbace: 0.764041 test loss: 0.803306
[Epoch 45; Iter    26/   41] train: loss: 0.2155602
[Epoch 45] ogbg-molbace: 0.717216 val loss: 0.853199
[Epoch 45] ogbg-molbace: 0.799165 test loss: 0.629417
[Epoch 46; Iter    15/   41] train: loss: 0.2536557
[Epoch 46] ogbg-molbace: 0.666300 val loss: 1.152447
[Epoch 46] ogbg-molbace: 0.809424 test loss: 0.748674
[Epoch 47; Iter     4/   41] train: loss: 0.2039963
[Epoch 47; Iter    34/   41] train: loss: 0.2875747
[Epoch 47] ogbg-molbace: 0.742125 val loss: 0.998678
[Epoch 47] ogbg-molbace: 0.813598 test loss: 0.984375
[Epoch 48; Iter    23/   41] train: loss: 0.2232529
[Epoch 48] ogbg-molbace: 0.660073 val loss: 0.821822
[Epoch 48] ogbg-molbace: 0.753608 test loss: 1.153119
[Epoch 49; Iter    12/   41] train: loss: 0.3359037
[Epoch 49] ogbg-molbace: 0.672894 val loss: 1.461018
[Epoch 49] ogbg-molbace: 0.758129 test loss: 1.327626
[Epoch 50; Iter     1/   41] train: loss: 0.4244674
[Epoch 50; Iter    31/   41] train: loss: 0.2925141
[Epoch 50] ogbg-molbace: 0.734799 val loss: 1.033086
[Epoch 50] ogbg-molbace: 0.804208 test loss: 0.774300
[Epoch 51; Iter    20/   41] train: loss: 0.4316849
[Epoch 51] ogbg-molbace: 0.747619 val loss: 1.017611
[Epoch 51] ogbg-molbace: 0.812207 test loss: 1.102093
[Epoch 52; Iter     9/   41] train: loss: 0.3199908
[Epoch 52; Iter    39/   41] train: loss: 0.2368694
[Epoch 52] ogbg-molbace: 0.746520 val loss: 1.256066
[Epoch 52] ogbg-molbace: 0.812902 test loss: 1.034200
[Epoch 53; Iter    28/   41] train: loss: 0.1413382
[Epoch 53] ogbg-molbace: 0.708059 val loss: 1.302282
[Epoch 53] ogbg-molbace: 0.821770 test loss: 1.133239
[Epoch 54; Iter    17/   41] train: loss: 0.4001735
[Epoch 54] ogbg-molbace: 0.745788 val loss: 1.713097
[Epoch 54] ogbg-molbace: 0.772561 test loss: 1.687428
[Epoch 55; Iter     6/   41] train: loss: 0.2688181
[Epoch 55; Iter    36/   41] train: loss: 0.1427635
[Epoch 55] ogbg-molbace: 0.699634 val loss: 1.082155
[Epoch 55] ogbg-molbace: 0.818640 test loss: 0.960397
[Epoch 56; Iter    25/   41] train: loss: 0.2339723
[Epoch 56] ogbg-molbace: 0.685714 val loss: 1.498466
[Epoch 56] ogbg-molbace: 0.797600 test loss: 1.943075
[Epoch 57; Iter    14/   41] train: loss: 0.2064912
[Epoch 57] ogbg-molbace: 0.687546 val loss: 1.529169
[Epoch 57] ogbg-molbace: 0.792036 test loss: 1.549162
[Epoch 58; Iter     3/   41] train: loss: 0.2175649
[Epoch 58; Iter    33/   41] train: loss: 0.2069963
[Epoch 58] ogbg-molbace: 0.733333 val loss: 1.064345
[Epoch 58] ogbg-molbace: 0.815336 test loss: 1.143422
[Epoch 59; Iter    22/   41] train: loss: 0.4354151
[Epoch 59] ogbg-molbace: 0.665934 val loss: 1.379384
[Epoch 59] ogbg-molbace: 0.810642 test loss: 1.026939
[Epoch 60; Iter    11/   41] train: loss: 0.1479279
[Epoch 60; Iter    41/   41] train: loss: 0.0747455
[Epoch 60] ogbg-molbace: 0.672894 val loss: 1.506148
[Epoch 60] ogbg-molbace: 0.834464 test loss: 0.920140
[Epoch 61; Iter    30/   41] train: loss: 0.2201206
[Epoch 61] ogbg-molbace: 0.701099 val loss: 1.244033
[Epoch 61] ogbg-molbace: 0.801947 test loss: 1.181083
[Epoch 62; Iter    19/   41] train: loss: 0.1109815
[Epoch 62] ogbg-molbace: 0.758974 val loss: 1.038216
[Epoch 62] ogbg-molbace: 0.822292 test loss: 1.115837
[Epoch 63; Iter     8/   41] train: loss: 0.2119472
[Epoch 63; Iter    38/   41] train: loss: 0.1581065
[Epoch 63] ogbg-molbace: 0.781319 val loss: 0.925885
[Epoch 63] ogbg-molbace: 0.797253 test loss: 1.297360
[Epoch 64; Iter    27/   41] train: loss: 0.4927990
[Epoch 64] ogbg-molbace: 0.711722 val loss: 1.502382
[Epoch 64] ogbg-molbace: 0.838289 test loss: 1.678401
[Epoch 65; Iter    16/   41] train: loss: 0.1220332
[Epoch 65] ogbg-molbace: 0.754945 val loss: 0.793598
[Epoch 65] ogbg-molbace: 0.815163 test loss: 1.085405
[Epoch 66; Iter     5/   41] train: loss: 0.2080414
[Epoch 66; Iter    35/   41] train: loss: 0.3496262
[Epoch 66] ogbg-molbace: 0.795238 val loss: 1.205306
[Epoch 66] ogbg-molbace: 0.765954 test loss: 1.790112
[Epoch 67; Iter    24/   41] train: loss: 0.2191359
[Epoch 67] ogbg-molbace: 0.737729 val loss: 1.653788
[Epoch 67] ogbg-molbace: 0.825074 test loss: 1.082036
[Epoch 68; Iter    13/   41] train: loss: 0.1576910
[Epoch 68] ogbg-molbace: 0.700366 val loss: 1.396720
[Epoch 68] ogbg-molbace: 0.826987 test loss: 1.424028
[Epoch 69; Iter     2/   41] train: loss: 0.2142936
[Epoch 69; Iter    32/   41] train: loss: 0.3693712
[Epoch 69] ogbg-molbace: 0.783150 val loss: 1.154899
[Epoch 69] ogbg-molbace: 0.818988 test loss: 1.367091
[Epoch 70; Iter    21/   41] train: loss: 0.1982600
[Epoch 70] ogbg-molbace: 0.683516 val loss: 1.299209
[Epoch 70] ogbg-molbace: 0.834464 test loss: 1.587213
[Epoch 71; Iter    10/   41] train: loss: 0.2568849
[Epoch 71; Iter    40/   41] train: loss: 0.1592791
[Epoch 71] ogbg-molbace: 0.744322 val loss: 1.105303
[Epoch 71] ogbg-molbace: 0.816206 test loss: 1.536372
[Epoch 72; Iter    29/   41] train: loss: 0.3001247
[Epoch 72] ogbg-molbace: 0.722711 val loss: 1.393548
[Epoch 72] ogbg-molbace: 0.803686 test loss: 1.634611
[Epoch 73; Iter    18/   41] train: loss: 0.0725358
[Epoch 73] ogbg-molbace: 0.698168 val loss: 1.008573
[Epoch 73] ogbg-molbace: 0.773778 test loss: 1.532981
[Epoch 74; Iter     7/   41] train: loss: 0.2077112
[Epoch 74; Iter    37/   41] train: loss: 0.1953825
[Epoch 74] ogbg-molbace: 0.743590 val loss: 1.150337
[Epoch 74] ogbg-molbace: 0.818292 test loss: 1.235358
[Epoch 75; Iter    26/   41] train: loss: 0.1174216
[Epoch 75] ogbg-molbace: 0.757509 val loss: 0.874221
[Epoch 75] ogbg-molbace: 0.793427 test loss: 1.345725
[Epoch 76; Iter    15/   41] train: loss: 0.0443959
[Epoch 76] ogbg-molbace: 0.720513 val loss: 1.547908
[Epoch 76] ogbg-molbace: 0.819857 test loss: 1.552231
[Epoch 77; Iter     4/   41] train: loss: 0.0350523
[Epoch 77; Iter    34/   41] train: loss: 0.1308917
[Epoch 77] ogbg-molbace: 0.728571 val loss: 1.218285
[Epoch 77] ogbg-molbace: 0.837246 test loss: 1.901834
[Epoch 78; Iter    23/   41] train: loss: 0.3504587
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.663004 val loss: 3.133079
[Epoch 78] ogbg-molbace: 0.673796 test loss: 5.024745
[Epoch 79; Iter    12/   41] train: loss: 0.1728088
[Epoch 79] ogbg-molbace: 0.683883 val loss: 1.796333
[Epoch 79] ogbg-molbace: 0.718832 test loss: 3.264423
[Epoch 80; Iter     1/   41] train: loss: 0.0900281
[Epoch 80; Iter    31/   41] train: loss: 0.1330000
[Epoch 80] ogbg-molbace: 0.682051 val loss: 1.232976
[Epoch 80] ogbg-molbace: 0.724570 test loss: 2.075299
[Epoch 81; Iter    20/   41] train: loss: 0.1612451
[Epoch 81] ogbg-molbace: 0.719048 val loss: 1.583743
[Epoch 81] ogbg-molbace: 0.755173 test loss: 1.721920
[Epoch 82; Iter     9/   41] train: loss: 0.0162773
[Epoch 82; Iter    39/   41] train: loss: 0.0338317
[Epoch 82] ogbg-molbace: 0.726740 val loss: 1.151989
[Epoch 82] ogbg-molbace: 0.777952 test loss: 1.680900
[Epoch 83; Iter    28/   41] train: loss: 0.0519787
[Epoch 83] ogbg-molbace: 0.716117 val loss: 1.207401
[Epoch 83] ogbg-molbace: 0.785776 test loss: 2.793934
[Epoch 84; Iter    17/   41] train: loss: 0.0115163
[Epoch 84] ogbg-molbace: 0.736264 val loss: 1.526254
[Epoch 84] ogbg-molbace: 0.750304 test loss: 2.440108
[Epoch 85; Iter     6/   41] train: loss: 0.0228712
[Epoch 85; Iter    36/   41] train: loss: 0.0061487
[Epoch 85] ogbg-molbace: 0.727839 val loss: 1.478137
[Epoch 85] ogbg-molbace: 0.783690 test loss: 2.051947
[Epoch 86; Iter    25/   41] train: loss: 0.0107811
[Epoch 86] ogbg-molbace: 0.667399 val loss: 1.763959
[Epoch 86] ogbg-molbace: 0.790123 test loss: 1.792189
[Epoch 87; Iter    14/   41] train: loss: 0.0012496
[Epoch 87] ogbg-molbace: 0.677289 val loss: 2.313827
[Epoch 87] ogbg-molbace: 0.734829 test loss: 3.332423
[Epoch 88; Iter     3/   41] train: loss: 0.0019207
[Epoch 88; Iter    33/   41] train: loss: 0.0038495
[Epoch 88] ogbg-molbace: 0.725275 val loss: 1.436177
[Epoch 88] ogbg-molbace: 0.787341 test loss: 1.713244
[Epoch 89; Iter    22/   41] train: loss: 0.0097908
[Epoch 89] ogbg-molbace: 0.635531 val loss: 2.006883
[Epoch 89] ogbg-molbace: 0.705790 test loss: 3.434762
[Epoch 90; Iter    11/   41] train: loss: 0.0194360
[Epoch 90; Iter    41/   41] train: loss: 0.0460890
[Epoch 90] ogbg-molbace: 0.653114 val loss: 2.733218
[Epoch 90] ogbg-molbace: 0.627369 test loss: 4.137598
[Epoch 91; Iter    30/   41] train: loss: 0.0018292
[Epoch 91] ogbg-molbace: 0.695604 val loss: 1.483077
[Epoch 91] ogbg-molbace: 0.772040 test loss: 2.782472
[Epoch 92; Iter    19/   41] train: loss: 0.0126300
[Epoch 92] ogbg-molbace: 0.703663 val loss: 1.949975
[Epoch 92] ogbg-molbace: 0.760216 test loss: 2.601100
[Epoch 93; Iter     8/   41] train: loss: 0.0039929
[Epoch 93; Iter    38/   41] train: loss: 0.0149366
[Epoch 93] ogbg-molbace: 0.678755 val loss: 1.587329
[Epoch 93] ogbg-molbace: 0.759520 test loss: 3.394961
[Epoch 94; Iter    27/   41] train: loss: 0.0412482
[Epoch 94] ogbg-molbace: 0.677656 val loss: 1.608493
[Epoch 94] ogbg-molbace: 0.746479 test loss: 2.229110
[Epoch 95; Iter    16/   41] train: loss: 0.0498491
[Epoch 95] ogbg-molbace: 0.752381 val loss: 1.401267
[Epoch 95] ogbg-molbace: 0.779517 test loss: 1.714531
[Epoch 96; Iter     5/   41] train: loss: 0.0185656
[Epoch 96; Iter    35/   41] train: loss: 0.0260625
[Epoch 96] ogbg-molbace: 0.638095 val loss: 1.939343
[Epoch 96] ogbg-molbace: 0.724744 test loss: 3.284033
[Epoch 97; Iter    24/   41] train: loss: 0.0051770
[Epoch 97] ogbg-molbace: 0.687912 val loss: 3.014147
[Epoch 97] ogbg-molbace: 0.700052 test loss: 5.085924
[Epoch 98; Iter    13/   41] train: loss: 0.0941823
[Epoch 98] ogbg-molbace: 0.712088 val loss: 1.289146
[Epoch 98] ogbg-molbace: 0.752217 test loss: 1.733529
[Epoch 99; Iter     2/   41] train: loss: 0.0037691
[Epoch 99; Iter    32/   41] train: loss: 0.0234865
[Epoch 99] ogbg-molbace: 0.716117 val loss: 1.880471
[Epoch 99] ogbg-molbace: 0.773778 test loss: 2.251252
[Epoch 100; Iter    21/   41] train: loss: 0.0300060
[Epoch 100] ogbg-molbace: 0.736264 val loss: 1.055985
[Epoch 100] ogbg-molbace: 0.773778 test loss: 1.841754
[Epoch 101; Iter    10/   41] train: loss: 0.0054593
[Epoch 101; Iter    40/   41] train: loss: 0.0070451
[Epoch 101] ogbg-molbace: 0.706960 val loss: 1.940237
[Epoch 101] ogbg-molbace: 0.736394 test loss: 3.032924
[Epoch 102; Iter    29/   41] train: loss: 0.0053861
[Epoch 102] ogbg-molbace: 0.742125 val loss: 1.417362
[Epoch 102] ogbg-molbace: 0.746305 test loss: 2.873144
[Epoch 103; Iter    18/   41] train: loss: 0.0016549
[Epoch 103] ogbg-molbace: 0.741392 val loss: 1.290658
[Epoch 103] ogbg-molbace: 0.767519 test loss: 2.485294
[Epoch 104; Iter     7/   41] train: loss: 0.0005641
[Epoch 104; Iter    37/   41] train: loss: 0.0008487
[Epoch 104] ogbg-molbace: 0.727839 val loss: 1.700139
[Epoch 104] ogbg-molbace: 0.731003 test loss: 2.702506
[Epoch 105; Iter    26/   41] train: loss: 0.0039104
[Epoch 105] ogbg-molbace: 0.708059 val loss: 1.752163
[Epoch 105] ogbg-molbace: 0.756912 test loss: 2.422880
[Epoch 106; Iter    15/   41] train: loss: 0.0032810
[Epoch 106] ogbg-molbace: 0.756777 val loss: 1.362364
[Epoch 106] ogbg-molbace: 0.779343 test loss: 2.346479
[Epoch 107; Iter     4/   41] train: loss: 0.0020542
[Epoch 107; Iter    34/   41] train: loss: 0.0015175
[Epoch 107] ogbg-molbace: 0.728571 val loss: 1.910751
[Epoch 107] ogbg-molbace: 0.768214 test loss: 2.370033
[Epoch 108; Iter    23/   41] train: loss: 0.0043179
[Epoch 108] ogbg-molbace: 0.733700 val loss: 1.869825
[Epoch 108] ogbg-molbace: 0.779517 test loss: 2.754275
[Epoch 109; Iter    12/   41] train: loss: 0.0014293
[Epoch 109] ogbg-molbace: 0.726374 val loss: 1.835644
[Epoch 109] ogbg-molbace: 0.804208 test loss: 2.727766
[Epoch 110; Iter     1/   41] train: loss: 0.0017100
[Epoch 110; Iter    31/   41] train: loss: 0.0268473
[Epoch 110] ogbg-molbace: 0.756044 val loss: 1.825718
[Epoch 110] ogbg-molbace: 0.780038 test loss: 3.151184
[Epoch 111; Iter    20/   41] train: loss: 0.0147472
[Epoch 111] ogbg-molbace: 0.697436 val loss: 1.922430
[Epoch 111] ogbg-molbace: 0.687706 test loss: 4.385145
[Epoch 112; Iter     9/   41] train: loss: 0.0684165
[Epoch 112; Iter    39/   41] train: loss: 0.0110491
[Epoch 112] ogbg-molbace: 0.615751 val loss: 2.155380
[Epoch 112] ogbg-molbace: 0.680577 test loss: 3.314211
[Epoch 113; Iter    28/   41] train: loss: 0.1166186
[Epoch 113] ogbg-molbace: 0.694139 val loss: 1.658938
[Epoch 113] ogbg-molbace: 0.714311 test loss: 2.317775
[Epoch 114; Iter    17/   41] train: loss: 0.1004612
[Epoch 114] ogbg-molbace: 0.688278 val loss: 1.756814
[Epoch 114] ogbg-molbace: 0.797948 test loss: 2.388593
[Epoch 115; Iter     6/   41] train: loss: 0.0662321
[Epoch 115; Iter    36/   41] train: loss: 0.0844823
[Epoch 115] ogbg-molbace: 0.736264 val loss: 0.992189
[Epoch 115] ogbg-molbace: 0.757086 test loss: 2.154884
[Epoch 116; Iter    25/   41] train: loss: 0.0111501
[Epoch 116] ogbg-molbace: 0.735897 val loss: 1.413731
[Epoch 116] ogbg-molbace: 0.732394 test loss: 2.152884
[Epoch 117; Iter    14/   41] train: loss: 0.0067231
[Epoch 117] ogbg-molbace: 0.666300 val loss: 1.552717
[Epoch 117] ogbg-molbace: 0.750304 test loss: 1.861651
[Epoch 118; Iter     3/   41] train: loss: 0.0057543
[Epoch 118; Iter    33/   41] train: loss: 0.0099911
[Epoch 118] ogbg-molbace: 0.686081 val loss: 1.618843
[Epoch 118] ogbg-molbace: 0.713615 test loss: 2.689209
[Epoch 119; Iter    22/   41] train: loss: 0.0026643
[Epoch 119] ogbg-molbace: 0.684615 val loss: 1.523464
[Epoch 119] ogbg-molbace: 0.728917 test loss: 2.703867
[Epoch 120; Iter    11/   41] train: loss: 0.0070711
[Epoch 120; Iter    41/   41] train: loss: 0.0065501
[Epoch 120] ogbg-molbace: 0.719048 val loss: 1.528255
[Epoch 120] ogbg-molbace: 0.755173 test loss: 2.792245
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 53.
Statistics on  val_best_checkpoint
mean_pred: 0.17882297933101654
std_pred: 4.558644771575928
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9642409082282855
rocauc: 0.7967032967032966
ogbg-molbace: 0.7967032967032966
BCEWithLogitsLoss: 1.175715943177541
Statistics on  test
mean_pred: -2.516977071762085
std_pred: 3.766737461090088
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.793779704456714
rocauc: 0.7673448095983307
ogbg-molbace: 0.7673448095983307
BCEWithLogitsLoss: 1.4567076712846756
Statistics on  train
mean_pred: -2.2528436183929443
std_pred: 5.392603397369385
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9999612690586469
rocauc: 0.9999743150684931
ogbg-molbace: 0.9999743150684931
BCEWithLogitsLoss: 0.03331876421210969
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.697802 val loss: 2.305741
[Epoch 78] ogbg-molbace: 0.786124 test loss: 2.146048
[Epoch 79; Iter    12/   41] train: loss: 0.0009744
[Epoch 79] ogbg-molbace: 0.689011 val loss: 2.305671
[Epoch 79] ogbg-molbace: 0.779343 test loss: 1.785943
[Epoch 80; Iter     1/   41] train: loss: 0.0015082
[Epoch 80; Iter    31/   41] train: loss: 0.0301637
[Epoch 80] ogbg-molbace: 0.694139 val loss: 2.267507
[Epoch 80] ogbg-molbace: 0.783690 test loss: 2.075353
[Epoch 81; Iter    20/   41] train: loss: 0.0005453
[Epoch 81] ogbg-molbace: 0.693773 val loss: 2.328752
[Epoch 81] ogbg-molbace: 0.784211 test loss: 2.069506
[Epoch 82; Iter     9/   41] train: loss: 0.0007207
[Epoch 82; Iter    39/   41] train: loss: 0.0014392
[Epoch 82] ogbg-molbace: 0.683150 val loss: 2.204170
[Epoch 82] ogbg-molbace: 0.755347 test loss: 1.886836
[Epoch 83; Iter    28/   41] train: loss: 0.0068794
[Epoch 83] ogbg-molbace: 0.692308 val loss: 2.410075
[Epoch 83] ogbg-molbace: 0.764737 test loss: 2.151682
[Epoch 84; Iter    17/   41] train: loss: 0.0004881
[Epoch 84] ogbg-molbace: 0.692308 val loss: 2.260678
[Epoch 84] ogbg-molbace: 0.772909 test loss: 2.120418
[Epoch 85; Iter     6/   41] train: loss: 0.0017867
[Epoch 85; Iter    36/   41] train: loss: 0.0039486
[Epoch 85] ogbg-molbace: 0.697070 val loss: 2.436161
[Epoch 85] ogbg-molbace: 0.777256 test loss: 2.167908
[Epoch 86; Iter    25/   41] train: loss: 0.0042640
[Epoch 86] ogbg-molbace: 0.681319 val loss: 2.207877
[Epoch 86] ogbg-molbace: 0.798644 test loss: 2.094818
[Epoch 87; Iter    14/   41] train: loss: 0.1054338
[Epoch 87] ogbg-molbace: 0.669231 val loss: 2.439231
[Epoch 87] ogbg-molbace: 0.801774 test loss: 1.905351
[Epoch 88; Iter     3/   41] train: loss: 0.0238621
[Epoch 88; Iter    33/   41] train: loss: 0.0127181
[Epoch 88] ogbg-molbace: 0.647253 val loss: 2.464800
[Epoch 88] ogbg-molbace: 0.766649 test loss: 2.463017
[Epoch 89; Iter    22/   41] train: loss: 0.0015858
[Epoch 89] ogbg-molbace: 0.667399 val loss: 2.586164
[Epoch 89] ogbg-molbace: 0.781777 test loss: 2.189129
[Epoch 90; Iter    11/   41] train: loss: 0.0005707
[Epoch 90; Iter    41/   41] train: loss: 0.1167413
[Epoch 90] ogbg-molbace: 0.683883 val loss: 2.898774
[Epoch 90] ogbg-molbace: 0.806295 test loss: 2.374518
[Epoch 91; Iter    30/   41] train: loss: 0.0118280
[Epoch 91] ogbg-molbace: 0.673993 val loss: 2.444644
[Epoch 91] ogbg-molbace: 0.747348 test loss: 2.725663
[Epoch 92; Iter    19/   41] train: loss: 0.0381097
[Epoch 92] ogbg-molbace: 0.679121 val loss: 2.597834
[Epoch 92] ogbg-molbace: 0.806642 test loss: 2.160816
[Epoch 93; Iter     8/   41] train: loss: 0.0077546
[Epoch 93; Iter    38/   41] train: loss: 0.0007692
[Epoch 93] ogbg-molbace: 0.680952 val loss: 2.454098
[Epoch 93] ogbg-molbace: 0.808729 test loss: 2.110635
[Epoch 94; Iter    27/   41] train: loss: 0.0067580
[Epoch 94] ogbg-molbace: 0.682051 val loss: 2.309471
[Epoch 94] ogbg-molbace: 0.672579 test loss: 2.896712
[Epoch 95; Iter    16/   41] train: loss: 0.0016926
[Epoch 95] ogbg-molbace: 0.671795 val loss: 2.876572
[Epoch 95] ogbg-molbace: 0.735176 test loss: 2.430390
[Epoch 96; Iter     5/   41] train: loss: 0.0346027
[Epoch 96; Iter    35/   41] train: loss: 0.0134958
[Epoch 96] ogbg-molbace: 0.718681 val loss: 2.303527
[Epoch 96] ogbg-molbace: 0.777430 test loss: 2.567321
[Epoch 97; Iter    24/   41] train: loss: 0.0099507
[Epoch 97] ogbg-molbace: 0.717582 val loss: 2.792211
[Epoch 97] ogbg-molbace: 0.784038 test loss: 2.220826
[Epoch 98; Iter    13/   41] train: loss: 0.0025359
[Epoch 98] ogbg-molbace: 0.697802 val loss: 2.392276
[Epoch 98] ogbg-molbace: 0.778821 test loss: 2.212259
[Epoch 99; Iter     2/   41] train: loss: 0.2208122
[Epoch 99; Iter    32/   41] train: loss: 0.0098161
[Epoch 99] ogbg-molbace: 0.693773 val loss: 2.596873
[Epoch 99] ogbg-molbace: 0.713963 test loss: 3.023748
[Epoch 100; Iter    21/   41] train: loss: 0.0012017
[Epoch 100] ogbg-molbace: 0.717949 val loss: 2.739503
[Epoch 100] ogbg-molbace: 0.793253 test loss: 1.955873
[Epoch 101; Iter    10/   41] train: loss: 0.0019014
[Epoch 101; Iter    40/   41] train: loss: 0.0038357
[Epoch 101] ogbg-molbace: 0.708791 val loss: 2.665967
[Epoch 101] ogbg-molbace: 0.768388 test loss: 2.247649
[Epoch 102; Iter    29/   41] train: loss: 0.0004209
[Epoch 102] ogbg-molbace: 0.713553 val loss: 2.554509
[Epoch 102] ogbg-molbace: 0.780212 test loss: 2.374497
[Epoch 103; Iter    18/   41] train: loss: 0.0008062
[Epoch 103] ogbg-molbace: 0.717216 val loss: 2.621452
[Epoch 103] ogbg-molbace: 0.802121 test loss: 2.463745
[Epoch 104; Iter     7/   41] train: loss: 0.0098993
[Epoch 104; Iter    37/   41] train: loss: 0.0020468
[Epoch 104] ogbg-molbace: 0.701832 val loss: 2.874513
[Epoch 104] ogbg-molbace: 0.774822 test loss: 2.471811
[Epoch 105; Iter    26/   41] train: loss: 0.0014149
[Epoch 105] ogbg-molbace: 0.686813 val loss: 2.612615
[Epoch 105] ogbg-molbace: 0.766128 test loss: 2.621694
[Epoch 106; Iter    15/   41] train: loss: 0.0003798
[Epoch 106] ogbg-molbace: 0.697802 val loss: 2.613552
[Epoch 106] ogbg-molbace: 0.782473 test loss: 2.398230
[Epoch 107; Iter     4/   41] train: loss: 0.0010455
[Epoch 107; Iter    34/   41] train: loss: 0.0003351
[Epoch 107] ogbg-molbace: 0.702930 val loss: 2.601640
[Epoch 107] ogbg-molbace: 0.762824 test loss: 2.875408
[Epoch 108; Iter    23/   41] train: loss: 0.0018506
[Epoch 108] ogbg-molbace: 0.708059 val loss: 2.624276
[Epoch 108] ogbg-molbace: 0.768736 test loss: 2.775227
[Epoch 109; Iter    12/   41] train: loss: 0.0009244
[Epoch 109] ogbg-molbace: 0.710256 val loss: 2.660610
[Epoch 109] ogbg-molbace: 0.772909 test loss: 2.839522
[Epoch 110; Iter     1/   41] train: loss: 0.0087190
[Epoch 110; Iter    31/   41] train: loss: 0.0014308
[Epoch 110] ogbg-molbace: 0.711722 val loss: 2.522727
[Epoch 110] ogbg-molbace: 0.777082 test loss: 2.585010
[Epoch 111; Iter    20/   41] train: loss: 0.0019733
[Epoch 111] ogbg-molbace: 0.710256 val loss: 2.623042
[Epoch 111] ogbg-molbace: 0.775691 test loss: 2.639849
[Epoch 112; Iter     9/   41] train: loss: 0.0005595
[Epoch 112; Iter    39/   41] train: loss: 0.0019196
[Epoch 112] ogbg-molbace: 0.707692 val loss: 2.699913
[Epoch 112] ogbg-molbace: 0.776561 test loss: 2.456058
[Epoch 113; Iter    28/   41] train: loss: 0.0003481
[Epoch 113] ogbg-molbace: 0.705495 val loss: 2.654751
[Epoch 113] ogbg-molbace: 0.775517 test loss: 2.737059
[Epoch 114; Iter    17/   41] train: loss: 0.0002541
[Epoch 114] ogbg-molbace: 0.704762 val loss: 2.658799
[Epoch 114] ogbg-molbace: 0.774822 test loss: 2.553395
[Epoch 115; Iter     6/   41] train: loss: 0.0007438
[Epoch 115; Iter    36/   41] train: loss: 0.0003503
[Epoch 115] ogbg-molbace: 0.705128 val loss: 2.667938
[Epoch 115] ogbg-molbace: 0.773778 test loss: 2.714344
[Epoch 116; Iter    25/   41] train: loss: 0.0001655
[Epoch 116] ogbg-molbace: 0.710623 val loss: 2.687137
[Epoch 116] ogbg-molbace: 0.784385 test loss: 2.464414
[Epoch 117; Iter    14/   41] train: loss: 0.0008474
[Epoch 117] ogbg-molbace: 0.702930 val loss: 2.712831
[Epoch 117] ogbg-molbace: 0.779864 test loss: 2.695158
[Epoch 118; Iter     3/   41] train: loss: 0.0001200
[Epoch 118; Iter    33/   41] train: loss: 0.0006550
[Epoch 118] ogbg-molbace: 0.706960 val loss: 2.707011
[Epoch 118] ogbg-molbace: 0.782299 test loss: 2.468842
[Epoch 119; Iter    22/   41] train: loss: 0.0003843
[Epoch 119] ogbg-molbace: 0.708059 val loss: 2.686125
[Epoch 119] ogbg-molbace: 0.779690 test loss: 2.745411
[Epoch 120; Iter    11/   41] train: loss: 0.0002555
[Epoch 120; Iter    41/   41] train: loss: 0.0006904
[Epoch 120] ogbg-molbace: 0.711722 val loss: 2.651073
[Epoch 120] ogbg-molbace: 0.790123 test loss: 2.624541
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 45.
Statistics on  val_best_checkpoint
mean_pred: 1.3393770456314087
std_pred: 3.4916136264801025
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9506765524448193
rocauc: 0.7505494505494505
ogbg-molbace: 0.7505494505494505
BCEWithLogitsLoss: 1.098718931277593
Statistics on  test
mean_pred: -1.5897043943405151
std_pred: 3.925769090652466
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7976447041948245
rocauc: 0.813075986784907
ogbg-molbace: 0.813075986784907
BCEWithLogitsLoss: 1.38028827868402
Statistics on  train
mean_pred: -1.3151838779449463
std_pred: 3.9499526023864746
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.9586803662221188
rocauc: 0.9718864155251142
ogbg-molbace: 0.9718864155251142
BCEWithLogitsLoss: 0.21197801028809896
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.640659 val loss: 2.340683
[Epoch 78] ogbg-molbace: 0.738828 test loss: 2.069817
[Epoch 79; Iter    12/   41] train: loss: 0.0024708
[Epoch 79] ogbg-molbace: 0.646886 val loss: 2.441301
[Epoch 79] ogbg-molbace: 0.752739 test loss: 2.177028
[Epoch 80; Iter     1/   41] train: loss: 0.0161928
[Epoch 80; Iter    31/   41] train: loss: 0.0133608
[Epoch 80] ogbg-molbace: 0.662637 val loss: 2.255288
[Epoch 80] ogbg-molbace: 0.739002 test loss: 2.873106
[Epoch 81; Iter    20/   41] train: loss: 0.0026345
[Epoch 81] ogbg-molbace: 0.620879 val loss: 2.282964
[Epoch 81] ogbg-molbace: 0.728917 test loss: 2.638988
[Epoch 82; Iter     9/   41] train: loss: 0.0015377
[Epoch 82; Iter    39/   41] train: loss: 0.0049103
[Epoch 82] ogbg-molbace: 0.636264 val loss: 2.357894
[Epoch 82] ogbg-molbace: 0.744566 test loss: 2.472705
[Epoch 83; Iter    28/   41] train: loss: 0.0013443
[Epoch 83] ogbg-molbace: 0.624908 val loss: 2.407825
[Epoch 83] ogbg-molbace: 0.736568 test loss: 2.640668
[Epoch 84; Iter    17/   41] train: loss: 0.0013617
[Epoch 84] ogbg-molbace: 0.642125 val loss: 2.392469
[Epoch 84] ogbg-molbace: 0.744740 test loss: 2.295760
[Epoch 85; Iter     6/   41] train: loss: 0.0183794
[Epoch 85; Iter    36/   41] train: loss: 0.0020607
[Epoch 85] ogbg-molbace: 0.642491 val loss: 2.465916
[Epoch 85] ogbg-molbace: 0.741610 test loss: 2.223023
[Epoch 86; Iter    25/   41] train: loss: 0.0052570
[Epoch 86] ogbg-molbace: 0.643590 val loss: 2.564896
[Epoch 86] ogbg-molbace: 0.748739 test loss: 2.194153
[Epoch 87; Iter    14/   41] train: loss: 0.0005677
[Epoch 87] ogbg-molbace: 0.646520 val loss: 2.543097
[Epoch 87] ogbg-molbace: 0.754999 test loss: 2.211846
[Epoch 88; Iter     3/   41] train: loss: 0.0005063
[Epoch 88; Iter    33/   41] train: loss: 0.0012965
[Epoch 88] ogbg-molbace: 0.645421 val loss: 2.694844
[Epoch 88] ogbg-molbace: 0.751174 test loss: 2.178778
[Epoch 89; Iter    22/   41] train: loss: 0.0010564
[Epoch 89] ogbg-molbace: 0.636996 val loss: 2.586729
[Epoch 89] ogbg-molbace: 0.737263 test loss: 2.196701
[Epoch 90; Iter    11/   41] train: loss: 0.0005240
[Epoch 90; Iter    41/   41] train: loss: 0.0454644
[Epoch 90] ogbg-molbace: 0.635165 val loss: 2.617341
[Epoch 90] ogbg-molbace: 0.735698 test loss: 2.223444
[Epoch 91; Iter    30/   41] train: loss: 0.0002566
[Epoch 91] ogbg-molbace: 0.645055 val loss: 2.581414
[Epoch 91] ogbg-molbace: 0.753956 test loss: 2.086633
[Epoch 92; Iter    19/   41] train: loss: 0.0008184
[Epoch 92] ogbg-molbace: 0.644322 val loss: 2.434431
[Epoch 92] ogbg-molbace: 0.751000 test loss: 2.164858
[Epoch 93; Iter     8/   41] train: loss: 0.0008003
[Epoch 93; Iter    38/   41] train: loss: 0.0005706
[Epoch 93] ogbg-molbace: 0.643223 val loss: 2.447970
[Epoch 93] ogbg-molbace: 0.745957 test loss: 2.085280
[Epoch 94; Iter    27/   41] train: loss: 0.0003877
[Epoch 94] ogbg-molbace: 0.637729 val loss: 2.430919
[Epoch 94] ogbg-molbace: 0.745088 test loss: 2.173588
[Epoch 95; Iter    16/   41] train: loss: 0.0004000
[Epoch 95] ogbg-molbace: 0.642857 val loss: 2.525484
[Epoch 95] ogbg-molbace: 0.744218 test loss: 2.182017
[Epoch 96; Iter     5/   41] train: loss: 0.0003436
[Epoch 96; Iter    35/   41] train: loss: 0.0003370
[Epoch 96] ogbg-molbace: 0.649451 val loss: 2.534926
[Epoch 96] ogbg-molbace: 0.752391 test loss: 2.167538
[Epoch 97; Iter    24/   41] train: loss: 0.0002110
[Epoch 97] ogbg-molbace: 0.642491 val loss: 2.484451
[Epoch 97] ogbg-molbace: 0.741784 test loss: 2.224714
[Epoch 98; Iter    13/   41] train: loss: 0.0021934
[Epoch 98] ogbg-molbace: 0.643956 val loss: 2.509999
[Epoch 98] ogbg-molbace: 0.743523 test loss: 2.138894
[Epoch 99; Iter     2/   41] train: loss: 0.0005283
[Epoch 99; Iter    32/   41] train: loss: 0.0004802
[Epoch 99] ogbg-molbace: 0.643956 val loss: 2.479989
[Epoch 99] ogbg-molbace: 0.741262 test loss: 2.217601
[Epoch 100; Iter    21/   41] train: loss: 0.0006362
[Epoch 100] ogbg-molbace: 0.645788 val loss: 2.557368
[Epoch 100] ogbg-molbace: 0.740393 test loss: 2.173685
[Epoch 101; Iter    10/   41] train: loss: 0.0005412
[Epoch 101; Iter    40/   41] train: loss: 0.0016186
[Epoch 101] ogbg-molbace: 0.644322 val loss: 2.564917
[Epoch 101] ogbg-molbace: 0.740219 test loss: 2.238444
[Epoch 102; Iter    29/   41] train: loss: 0.0002546
[Epoch 102] ogbg-molbace: 0.645788 val loss: 2.629049
[Epoch 102] ogbg-molbace: 0.743349 test loss: 2.214583
[Epoch 103; Iter    18/   41] train: loss: 0.0001585
[Epoch 103] ogbg-molbace: 0.647253 val loss: 2.731222
[Epoch 103] ogbg-molbace: 0.739871 test loss: 2.246276
[Epoch 104; Iter     7/   41] train: loss: 0.0001439
[Epoch 104; Iter    37/   41] train: loss: 0.0004299
[Epoch 104] ogbg-molbace: 0.646886 val loss: 2.590920
[Epoch 104] ogbg-molbace: 0.737785 test loss: 2.234148
[Epoch 105; Iter    26/   41] train: loss: 0.0007003
[Epoch 105] ogbg-molbace: 0.644322 val loss: 2.699992
[Epoch 105] ogbg-molbace: 0.736915 test loss: 2.246840
[Epoch 106; Iter    15/   41] train: loss: 0.0021515
[Epoch 106] ogbg-molbace: 0.644322 val loss: 2.593091
[Epoch 106] ogbg-molbace: 0.737437 test loss: 2.211817
[Epoch 107; Iter     4/   41] train: loss: 0.0003383
[Epoch 107; Iter    34/   41] train: loss: 0.0001907
[Epoch 107] ogbg-molbace: 0.645788 val loss: 2.780216
[Epoch 107] ogbg-molbace: 0.740915 test loss: 2.190885
[Epoch 108; Iter    23/   41] train: loss: 0.0027216
[Epoch 108] ogbg-molbace: 0.643956 val loss: 2.707745
[Epoch 108] ogbg-molbace: 0.738306 test loss: 2.282351
[Epoch 109; Iter    12/   41] train: loss: 0.0005366
[Epoch 109] ogbg-molbace: 0.642125 val loss: 2.605755
[Epoch 109] ogbg-molbace: 0.738480 test loss: 2.193084
[Epoch 110; Iter     1/   41] train: loss: 0.0003552
[Epoch 110; Iter    31/   41] train: loss: 0.0001317
[Epoch 110] ogbg-molbace: 0.632601 val loss: 2.561030
[Epoch 110] ogbg-molbace: 0.731525 test loss: 2.283253
[Epoch 111; Iter    20/   41] train: loss: 0.0001410
[Epoch 111] ogbg-molbace: 0.634799 val loss: 2.632830
[Epoch 111] ogbg-molbace: 0.735698 test loss: 2.270793
[Epoch 112; Iter     9/   41] train: loss: 0.0004743
[Epoch 112; Iter    39/   41] train: loss: 0.0002107
[Epoch 112] ogbg-molbace: 0.637729 val loss: 2.714703
[Epoch 112] ogbg-molbace: 0.732047 test loss: 2.339534
[Epoch 113; Iter    28/   41] train: loss: 0.0001963
[Epoch 113] ogbg-molbace: 0.649084 val loss: 2.751241
[Epoch 113] ogbg-molbace: 0.727178 test loss: 2.150834
[Epoch 114; Iter    17/   41] train: loss: 0.0002680
[Epoch 114] ogbg-molbace: 0.649817 val loss: 2.681223
[Epoch 114] ogbg-molbace: 0.735524 test loss: 2.117952
[Epoch 115; Iter     6/   41] train: loss: 0.0001879
[Epoch 115; Iter    36/   41] train: loss: 0.0025887
[Epoch 115] ogbg-molbace: 0.646886 val loss: 2.754954
[Epoch 115] ogbg-molbace: 0.740741 test loss: 2.180498
[Epoch 116; Iter    25/   41] train: loss: 0.0008475
[Epoch 116] ogbg-molbace: 0.649084 val loss: 2.752937
[Epoch 116] ogbg-molbace: 0.740219 test loss: 2.210805
[Epoch 117; Iter    14/   41] train: loss: 0.0007906
[Epoch 117] ogbg-molbace: 0.642125 val loss: 2.722933
[Epoch 117] ogbg-molbace: 0.732742 test loss: 2.257408
[Epoch 118; Iter     3/   41] train: loss: 0.0001886
[Epoch 118; Iter    33/   41] train: loss: 0.0013891
[Epoch 118] ogbg-molbace: 0.638462 val loss: 2.705036
[Epoch 118] ogbg-molbace: 0.732047 test loss: 2.280680
[Epoch 119; Iter    22/   41] train: loss: 0.0001686
[Epoch 119] ogbg-molbace: 0.642857 val loss: 2.802995
[Epoch 119] ogbg-molbace: 0.735350 test loss: 2.267196
[Epoch 120; Iter    11/   41] train: loss: 0.0002535
[Epoch 120; Iter    41/   41] train: loss: 0.0023489
[Epoch 120] ogbg-molbace: 0.653846 val loss: 2.786531
[Epoch 120] ogbg-molbace: 0.745262 test loss: 2.220379
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 30.
Statistics on  val_best_checkpoint
mean_pred: -0.2913006544113159
std_pred: 0.7089188098907471
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.951379877763668
rocauc: 0.7304029304029305
ogbg-molbace: 0.7304029304029305
BCEWithLogitsLoss: 0.7813546061515808
Statistics on  test
mean_pred: -0.4054082930088043
std_pred: 1.4495948553085327
[Epoch 78] ogbg-molbace: 0.674359 val loss: 2.351858
[Epoch 78] ogbg-molbace: 0.742132 test loss: 1.687756
[Epoch 79; Iter    12/   41] train: loss: 0.0632732
[Epoch 79] ogbg-molbace: 0.694505 val loss: 1.925048
[Epoch 79] ogbg-molbace: 0.758129 test loss: 1.955211
[Epoch 80; Iter     1/   41] train: loss: 0.0477997
[Epoch 80; Iter    31/   41] train: loss: 0.2290206
[Epoch 80] ogbg-molbace: 0.668132 val loss: 3.307451
[Epoch 80] ogbg-molbace: 0.730482 test loss: 4.465313
[Epoch 81; Iter    20/   41] train: loss: 0.0112753
[Epoch 81] ogbg-molbace: 0.673260 val loss: 2.355011
[Epoch 81] ogbg-molbace: 0.744218 test loss: 2.835876
[Epoch 82; Iter     9/   41] train: loss: 0.0476380
[Epoch 82; Iter    39/   41] train: loss: 0.0102427
[Epoch 82] ogbg-molbace: 0.672527 val loss: 2.270603
[Epoch 82] ogbg-molbace: 0.722483 test loss: 2.987707
[Epoch 83; Iter    28/   41] train: loss: 0.0025486
[Epoch 83] ogbg-molbace: 0.682418 val loss: 2.374192
[Epoch 83] ogbg-molbace: 0.761954 test loss: 2.406606
[Epoch 84; Iter    17/   41] train: loss: 0.0148009
[Epoch 84] ogbg-molbace: 0.679487 val loss: 2.352565
[Epoch 84] ogbg-molbace: 0.740219 test loss: 3.143523
[Epoch 85; Iter     6/   41] train: loss: 0.0469435
[Epoch 85; Iter    36/   41] train: loss: 0.0119234
[Epoch 85] ogbg-molbace: 0.688278 val loss: 2.277733
[Epoch 85] ogbg-molbace: 0.730482 test loss: 2.655531
[Epoch 86; Iter    25/   41] train: loss: 0.0117279
[Epoch 86] ogbg-molbace: 0.689011 val loss: 2.703149
[Epoch 86] ogbg-molbace: 0.741262 test loss: 2.475203
[Epoch 87; Iter    14/   41] train: loss: 0.0021768
[Epoch 87] ogbg-molbace: 0.680220 val loss: 2.332740
[Epoch 87] ogbg-molbace: 0.733090 test loss: 3.075380
[Epoch 88; Iter     3/   41] train: loss: 0.0009783
[Epoch 88; Iter    33/   41] train: loss: 0.0004150
[Epoch 88] ogbg-molbace: 0.672894 val loss: 2.510351
[Epoch 88] ogbg-molbace: 0.733264 test loss: 3.212240
[Epoch 89; Iter    22/   41] train: loss: 0.0041690
[Epoch 89] ogbg-molbace: 0.681319 val loss: 2.440187
[Epoch 89] ogbg-molbace: 0.728221 test loss: 2.557399
[Epoch 90; Iter    11/   41] train: loss: 0.0096652
[Epoch 90; Iter    41/   41] train: loss: 0.0170770
[Epoch 90] ogbg-molbace: 0.672527 val loss: 2.825661
[Epoch 90] ogbg-molbace: 0.704747 test loss: 2.680775
[Epoch 91; Iter    30/   41] train: loss: 0.0004521
[Epoch 91] ogbg-molbace: 0.631868 val loss: 3.010602
[Epoch 91] ogbg-molbace: 0.700226 test loss: 3.244780
[Epoch 92; Iter    19/   41] train: loss: 0.0095568
[Epoch 92] ogbg-molbace: 0.627106 val loss: 2.637240
[Epoch 92] ogbg-molbace: 0.677447 test loss: 3.077822
[Epoch 93; Iter     8/   41] train: loss: 0.0022276
[Epoch 93; Iter    38/   41] train: loss: 0.1106740
[Epoch 93] ogbg-molbace: 0.651648 val loss: 2.711438
[Epoch 93] ogbg-molbace: 0.680751 test loss: 2.544010
[Epoch 94; Iter    27/   41] train: loss: 0.1909685
[Epoch 94] ogbg-molbace: 0.627473 val loss: 3.675744
[Epoch 94] ogbg-molbace: 0.704747 test loss: 1.878760
[Epoch 95; Iter    16/   41] train: loss: 0.0723810
[Epoch 95] ogbg-molbace: 0.665201 val loss: 2.014081
[Epoch 95] ogbg-molbace: 0.723700 test loss: 3.973242
[Epoch 96; Iter     5/   41] train: loss: 0.2435509
[Epoch 96; Iter    35/   41] train: loss: 0.2509039
[Epoch 96] ogbg-molbace: 0.596703 val loss: 2.785509
[Epoch 96] ogbg-molbace: 0.711355 test loss: 3.186334
[Epoch 97; Iter    24/   41] train: loss: 0.1052456
[Epoch 97] ogbg-molbace: 0.669231 val loss: 2.028322
[Epoch 97] ogbg-molbace: 0.743871 test loss: 1.641982
[Epoch 98; Iter    13/   41] train: loss: 0.0841087
[Epoch 98] ogbg-molbace: 0.680220 val loss: 1.983258
[Epoch 98] ogbg-molbace: 0.725961 test loss: 2.056061
[Epoch 99; Iter     2/   41] train: loss: 0.0023212
[Epoch 99; Iter    32/   41] train: loss: 0.0090688
[Epoch 99] ogbg-molbace: 0.653480 val loss: 1.698082
[Epoch 99] ogbg-molbace: 0.728221 test loss: 1.845128
[Epoch 100; Iter    21/   41] train: loss: 0.0188128
[Epoch 100] ogbg-molbace: 0.682418 val loss: 1.994656
[Epoch 100] ogbg-molbace: 0.759172 test loss: 1.737136
[Epoch 101; Iter    10/   41] train: loss: 0.0078746
[Epoch 101; Iter    40/   41] train: loss: 0.0110310
[Epoch 101] ogbg-molbace: 0.667033 val loss: 1.953816
[Epoch 101] ogbg-molbace: 0.747348 test loss: 1.884054
[Epoch 102; Iter    29/   41] train: loss: 0.0043953
[Epoch 102] ogbg-molbace: 0.667399 val loss: 2.114988
[Epoch 102] ogbg-molbace: 0.756564 test loss: 1.814061
[Epoch 103; Iter    18/   41] train: loss: 0.0012394
[Epoch 103] ogbg-molbace: 0.675458 val loss: 2.118865
[Epoch 103] ogbg-molbace: 0.764910 test loss: 1.694026
[Epoch 104; Iter     7/   41] train: loss: 0.0012582
[Epoch 104; Iter    37/   41] train: loss: 0.0028883
[Epoch 104] ogbg-molbace: 0.675824 val loss: 2.089456
[Epoch 104] ogbg-molbace: 0.762302 test loss: 1.734128
[Epoch 105; Iter    26/   41] train: loss: 0.0010871
[Epoch 105] ogbg-molbace: 0.659341 val loss: 2.224500
[Epoch 105] ogbg-molbace: 0.748044 test loss: 1.946150
[Epoch 106; Iter    15/   41] train: loss: 0.0057663
[Epoch 106] ogbg-molbace: 0.653114 val loss: 2.267125
[Epoch 106] ogbg-molbace: 0.738306 test loss: 2.312857
[Epoch 107; Iter     4/   41] train: loss: 0.0010152
[Epoch 107; Iter    34/   41] train: loss: 0.0005814
[Epoch 107] ogbg-molbace: 0.661172 val loss: 2.326667
[Epoch 107] ogbg-molbace: 0.750304 test loss: 1.969201
[Epoch 108; Iter    23/   41] train: loss: 0.0044561
[Epoch 108] ogbg-molbace: 0.663370 val loss: 2.246439
[Epoch 108] ogbg-molbace: 0.752739 test loss: 2.055242
[Epoch 109; Iter    12/   41] train: loss: 0.0030127
[Epoch 109] ogbg-molbace: 0.682051 val loss: 2.222491
[Epoch 109] ogbg-molbace: 0.774474 test loss: 1.835316
[Epoch 110; Iter     1/   41] train: loss: 0.0027115
[Epoch 110; Iter    31/   41] train: loss: 0.0007337
[Epoch 110] ogbg-molbace: 0.661538 val loss: 2.270559
[Epoch 110] ogbg-molbace: 0.752217 test loss: 2.290981
[Epoch 111; Iter    20/   41] train: loss: 0.0006346
[Epoch 111] ogbg-molbace: 0.667399 val loss: 2.336780
[Epoch 111] ogbg-molbace: 0.754825 test loss: 2.045979
[Epoch 112; Iter     9/   41] train: loss: 0.0006331
[Epoch 112; Iter    39/   41] train: loss: 0.0008723
[Epoch 112] ogbg-molbace: 0.662637 val loss: 2.558199
[Epoch 112] ogbg-molbace: 0.758129 test loss: 2.290695
[Epoch 113; Iter    28/   41] train: loss: 0.0031248
[Epoch 113] ogbg-molbace: 0.652747 val loss: 2.612850
[Epoch 113] ogbg-molbace: 0.750478 test loss: 2.303654
[Epoch 114; Iter    17/   41] train: loss: 0.0058293
[Epoch 114] ogbg-molbace: 0.648352 val loss: 2.740352
[Epoch 114] ogbg-molbace: 0.750478 test loss: 2.308251
[Epoch 115; Iter     6/   41] train: loss: 0.0009266
[Epoch 115; Iter    36/   41] train: loss: 0.0036912
[Epoch 115] ogbg-molbace: 0.649817 val loss: 2.772310
[Epoch 115] ogbg-molbace: 0.756912 test loss: 2.457523
[Epoch 116; Iter    25/   41] train: loss: 0.0025492
[Epoch 116] ogbg-molbace: 0.656044 val loss: 2.718300
[Epoch 116] ogbg-molbace: 0.759346 test loss: 2.375599
[Epoch 117; Iter    14/   41] train: loss: 0.0035375
[Epoch 117] ogbg-molbace: 0.653480 val loss: 2.690765
[Epoch 117] ogbg-molbace: 0.756042 test loss: 2.367323
[Epoch 118; Iter     3/   41] train: loss: 0.0007464
[Epoch 118; Iter    33/   41] train: loss: 0.0016768
[Epoch 118] ogbg-molbace: 0.657875 val loss: 2.667030
[Epoch 118] ogbg-molbace: 0.757260 test loss: 2.446310
[Epoch 119; Iter    22/   41] train: loss: 0.0003902
[Epoch 119] ogbg-molbace: 0.661538 val loss: 2.662132
[Epoch 119] ogbg-molbace: 0.760563 test loss: 2.293115
[Epoch 120; Iter    11/   41] train: loss: 0.0003063
[Epoch 120; Iter    41/   41] train: loss: 0.0004538
[Epoch 120] ogbg-molbace: 0.677656 val loss: 2.542419
[Epoch 120] ogbg-molbace: 0.777952 test loss: 2.190447
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 24.
Statistics on  val_best_checkpoint
mean_pred: -0.4893122911453247
std_pred: 0.5727605223655701
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9493760917896914
rocauc: 0.7296703296703296
ogbg-molbace: 0.7296703296703296
BCEWithLogitsLoss: 0.8193362752596537
Statistics on  test
mean_pred: -0.8123742938041687
std_pred: 0.7195928692817688
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7092402750117187
rocauc: 0.7543035993740219
ogbg-molbace: 0.7543035993740219
BCEWithLogitsLoss: 0.8488158682982127
Statistics on  train
mean_pred: -0.5294712781906128
std_pred: 0.9618998765945435
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.6287533766573767
rocauc: 0.7370662100456621
ogbg-molbace: 0.7370662100456621
BCEWithLogitsLoss: 0.5812190632994582
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.6973761740108916
rocauc: 0.7306555381672754
ogbg-molbace: 0.7306555381672754
BCEWithLogitsLoss: 0.8911463295420011
Statistics on  train
mean_pred: -0.38008683919906616
std_pred: 0.8033416271209717
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.6457886113372288
rocauc: 0.7612614155251142
ogbg-molbace: 0.7612614155251142
BCEWithLogitsLoss: 0.5746299980617151
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.616484 val loss: 3.007752
[Epoch 78] ogbg-molbace: 0.764910 test loss: 2.767780
[Epoch 79; Iter    12/   41] train: loss: 0.0018784
[Epoch 79] ogbg-molbace: 0.634066 val loss: 3.097322
[Epoch 79] ogbg-molbace: 0.792558 test loss: 2.082233
[Epoch 80; Iter     1/   41] train: loss: 0.0118494
[Epoch 80; Iter    31/   41] train: loss: 0.0653433
[Epoch 80] ogbg-molbace: 0.597802 val loss: 2.802589
[Epoch 80] ogbg-molbace: 0.754477 test loss: 2.413972
[Epoch 81; Iter    20/   41] train: loss: 0.0209138
[Epoch 81] ogbg-molbace: 0.627839 val loss: 2.701655
[Epoch 81] ogbg-molbace: 0.780908 test loss: 2.540715
[Epoch 82; Iter     9/   41] train: loss: 0.0218299
[Epoch 82; Iter    39/   41] train: loss: 0.1813369
[Epoch 82] ogbg-molbace: 0.649451 val loss: 2.183069
[Epoch 82] ogbg-molbace: 0.799687 test loss: 1.764978
[Epoch 83; Iter    28/   41] train: loss: 0.1594228
[Epoch 83] ogbg-molbace: 0.649084 val loss: 3.009578
[Epoch 83] ogbg-molbace: 0.809424 test loss: 2.247202
[Epoch 84; Iter    17/   41] train: loss: 0.0672284
[Epoch 84] ogbg-molbace: 0.611355 val loss: 2.381342
[Epoch 84] ogbg-molbace: 0.779517 test loss: 1.773187
[Epoch 85; Iter     6/   41] train: loss: 0.0246089
[Epoch 85; Iter    36/   41] train: loss: 0.0393995
[Epoch 85] ogbg-molbace: 0.661905 val loss: 2.244673
[Epoch 85] ogbg-molbace: 0.788559 test loss: 1.852786
[Epoch 86; Iter    25/   41] train: loss: 0.0081717
[Epoch 86] ogbg-molbace: 0.626374 val loss: 2.520792
[Epoch 86] ogbg-molbace: 0.748739 test loss: 2.469641
[Epoch 87; Iter    14/   41] train: loss: 0.1124050
[Epoch 87] ogbg-molbace: 0.650916 val loss: 2.752344
[Epoch 87] ogbg-molbace: 0.803512 test loss: 2.067760
[Epoch 88; Iter     3/   41] train: loss: 0.0929038
[Epoch 88; Iter    33/   41] train: loss: 0.0116828
[Epoch 88] ogbg-molbace: 0.638828 val loss: 2.517760
[Epoch 88] ogbg-molbace: 0.801078 test loss: 1.769309
[Epoch 89; Iter    22/   41] train: loss: 0.0016090
[Epoch 89] ogbg-molbace: 0.640293 val loss: 2.682654
[Epoch 89] ogbg-molbace: 0.801078 test loss: 1.930156
[Epoch 90; Iter    11/   41] train: loss: 0.0012843
[Epoch 90; Iter    41/   41] train: loss: 0.1870145
[Epoch 90] ogbg-molbace: 0.644689 val loss: 2.702364
[Epoch 90] ogbg-molbace: 0.782820 test loss: 1.963648
[Epoch 91; Iter    30/   41] train: loss: 0.0144124
[Epoch 91] ogbg-molbace: 0.639560 val loss: 2.726679
[Epoch 91] ogbg-molbace: 0.773778 test loss: 2.463198
[Epoch 92; Iter    19/   41] train: loss: 0.0193836
[Epoch 92] ogbg-molbace: 0.631502 val loss: 2.972489
[Epoch 92] ogbg-molbace: 0.774822 test loss: 2.588645
[Epoch 93; Iter     8/   41] train: loss: 0.0102465
[Epoch 93; Iter    38/   41] train: loss: 0.0010620
[Epoch 93] ogbg-molbace: 0.642857 val loss: 2.589406
[Epoch 93] ogbg-molbace: 0.781777 test loss: 2.194521
[Epoch 94; Iter    27/   41] train: loss: 0.0066780
[Epoch 94] ogbg-molbace: 0.637729 val loss: 3.018818
[Epoch 94] ogbg-molbace: 0.758303 test loss: 2.901188
[Epoch 95; Iter    16/   41] train: loss: 0.0019200
[Epoch 95] ogbg-molbace: 0.623443 val loss: 3.512014
[Epoch 95] ogbg-molbace: 0.792732 test loss: 2.198532
[Epoch 96; Iter     5/   41] train: loss: 0.0023117
[Epoch 96; Iter    35/   41] train: loss: 0.0012318
[Epoch 96] ogbg-molbace: 0.636996 val loss: 3.212334
[Epoch 96] ogbg-molbace: 0.814119 test loss: 1.979150
[Epoch 97; Iter    24/   41] train: loss: 0.0010275
[Epoch 97] ogbg-molbace: 0.639560 val loss: 3.116855
[Epoch 97] ogbg-molbace: 0.807860 test loss: 2.138958
[Epoch 98; Iter    13/   41] train: loss: 0.0012798
[Epoch 98] ogbg-molbace: 0.639194 val loss: 3.207780
[Epoch 98] ogbg-molbace: 0.800730 test loss: 2.090371
[Epoch 99; Iter     2/   41] train: loss: 0.0067167
[Epoch 99; Iter    32/   41] train: loss: 0.0012671
[Epoch 99] ogbg-molbace: 0.638095 val loss: 3.160697
[Epoch 99] ogbg-molbace: 0.792558 test loss: 2.187161
[Epoch 100; Iter    21/   41] train: loss: 0.0104586
[Epoch 100] ogbg-molbace: 0.654212 val loss: 2.932084
[Epoch 100] ogbg-molbace: 0.804556 test loss: 2.155265
[Epoch 101; Iter    10/   41] train: loss: 0.0030179
[Epoch 101; Iter    40/   41] train: loss: 0.0032486
[Epoch 101] ogbg-molbace: 0.634432 val loss: 3.047936
[Epoch 101] ogbg-molbace: 0.810816 test loss: 2.117744
[Epoch 102; Iter    29/   41] train: loss: 0.0007778
[Epoch 102] ogbg-molbace: 0.635897 val loss: 3.088958
[Epoch 102] ogbg-molbace: 0.796731 test loss: 2.103897
[Epoch 103; Iter    18/   41] train: loss: 0.0007867
[Epoch 103] ogbg-molbace: 0.635165 val loss: 2.979900
[Epoch 103] ogbg-molbace: 0.789254 test loss: 1.867939
[Epoch 104; Iter     7/   41] train: loss: 0.1340473
[Epoch 104; Iter    37/   41] train: loss: 0.0015331
[Epoch 104] ogbg-molbace: 0.632967 val loss: 2.995995
[Epoch 104] ogbg-molbace: 0.772735 test loss: 2.766117
[Epoch 105; Iter    26/   41] train: loss: 0.0036884
[Epoch 105] ogbg-molbace: 0.645055 val loss: 2.244575
[Epoch 105] ogbg-molbace: 0.771692 test loss: 2.075484
[Epoch 106; Iter    15/   41] train: loss: 0.0011305
[Epoch 106] ogbg-molbace: 0.654945 val loss: 2.501793
[Epoch 106] ogbg-molbace: 0.789254 test loss: 1.921741
[Epoch 107; Iter     4/   41] train: loss: 0.0017047
[Epoch 107; Iter    34/   41] train: loss: 0.0058068
[Epoch 107] ogbg-molbace: 0.632601 val loss: 3.293845
[Epoch 107] ogbg-molbace: 0.734655 test loss: 3.330097
[Epoch 108; Iter    23/   41] train: loss: 0.0037581
[Epoch 108] ogbg-molbace: 0.655678 val loss: 3.288250
[Epoch 108] ogbg-molbace: 0.779517 test loss: 2.189912
[Epoch 109; Iter    12/   41] train: loss: 0.0007900
[Epoch 109] ogbg-molbace: 0.662271 val loss: 2.899445
[Epoch 109] ogbg-molbace: 0.761433 test loss: 2.424703
[Epoch 110; Iter     1/   41] train: loss: 0.0112565
[Epoch 110; Iter    31/   41] train: loss: 0.0006897
[Epoch 110] ogbg-molbace: 0.663004 val loss: 2.915485
[Epoch 110] ogbg-molbace: 0.762650 test loss: 2.283440
[Epoch 111; Iter    20/   41] train: loss: 0.0037040
[Epoch 111] ogbg-molbace: 0.658242 val loss: 2.975184
[Epoch 111] ogbg-molbace: 0.771518 test loss: 2.469283
[Epoch 112; Iter     9/   41] train: loss: 0.0005981
[Epoch 112; Iter    39/   41] train: loss: 0.0007595
[Epoch 112] ogbg-molbace: 0.653846 val loss: 2.874610
[Epoch 112] ogbg-molbace: 0.774300 test loss: 2.298976
[Epoch 113; Iter    28/   41] train: loss: 0.0004323
[Epoch 113] ogbg-molbace: 0.659707 val loss: 3.080567
[Epoch 113] ogbg-molbace: 0.769605 test loss: 2.726657
[Epoch 114; Iter    17/   41] train: loss: 0.0003208
[Epoch 114] ogbg-molbace: 0.650916 val loss: 3.306979
[Epoch 114] ogbg-molbace: 0.758651 test loss: 2.594827
[Epoch 115; Iter     6/   41] train: loss: 0.0071254
[Epoch 115; Iter    36/   41] train: loss: 0.0010591
[Epoch 115] ogbg-molbace: 0.652015 val loss: 3.400294
[Epoch 115] ogbg-molbace: 0.758303 test loss: 2.859386
[Epoch 116; Iter    25/   41] train: loss: 0.0007855
[Epoch 116] ogbg-molbace: 0.654212 val loss: 3.103141
[Epoch 116] ogbg-molbace: 0.766823 test loss: 2.397542
[Epoch 117; Iter    14/   41] train: loss: 0.0006904
[Epoch 117] ogbg-molbace: 0.652747 val loss: 3.262433
[Epoch 117] ogbg-molbace: 0.768910 test loss: 2.623735
[Epoch 118; Iter     3/   41] train: loss: 0.0003341
[Epoch 118; Iter    33/   41] train: loss: 0.0004094
[Epoch 118] ogbg-molbace: 0.655311 val loss: 2.979818
[Epoch 118] ogbg-molbace: 0.778299 test loss: 2.167583
[Epoch 119; Iter    22/   41] train: loss: 0.0007102
[Epoch 119] ogbg-molbace: 0.646886 val loss: 3.141743
[Epoch 119] ogbg-molbace: 0.767866 test loss: 2.598385
[Epoch 120; Iter    11/   41] train: loss: 0.0002592
[Epoch 120; Iter    41/   41] train: loss: 0.0024143
[Epoch 120] ogbg-molbace: 0.653846 val loss: 3.134461
[Epoch 120] ogbg-molbace: 0.785603 test loss: 2.277907
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 26.
Statistics on  val_best_checkpoint
mean_pred: -0.19462740421295166
std_pred: 1.1553184986114502
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9508412426759414
rocauc: 0.7234432234432234
ogbg-molbace: 0.7234432234432234
BCEWithLogitsLoss: 0.7743292649586996
Statistics on  test
mean_pred: -0.9724294543266296
std_pred: 1.954775333404541
[Epoch 78] ogbg-molbace: 0.754212 val loss: 1.546098
[Epoch 78] ogbg-molbace: 0.778126 test loss: 1.295807
[Epoch 79; Iter    12/   41] train: loss: 0.4964886
[Epoch 79] ogbg-molbace: 0.669231 val loss: 1.118548
[Epoch 79] ogbg-molbace: 0.784559 test loss: 1.184168
[Epoch 80; Iter     1/   41] train: loss: 0.1874693
[Epoch 80; Iter    31/   41] train: loss: 0.2647140
[Epoch 80] ogbg-molbace: 0.672894 val loss: 1.275070
[Epoch 80] ogbg-molbace: 0.794644 test loss: 1.578812
[Epoch 81; Iter    20/   41] train: loss: 0.1977711
[Epoch 81] ogbg-molbace: 0.712088 val loss: 1.251875
[Epoch 81] ogbg-molbace: 0.808381 test loss: 1.225007
[Epoch 82; Iter     9/   41] train: loss: 0.0616561
[Epoch 82; Iter    39/   41] train: loss: 0.1641207
[Epoch 82] ogbg-molbace: 0.672527 val loss: 1.598157
[Epoch 82] ogbg-molbace: 0.810816 test loss: 1.468571
[Epoch 83; Iter    28/   41] train: loss: 0.0547058
[Epoch 83] ogbg-molbace: 0.686081 val loss: 1.421954
[Epoch 83] ogbg-molbace: 0.787689 test loss: 1.278789
[Epoch 84; Iter    17/   41] train: loss: 0.0786751
[Epoch 84] ogbg-molbace: 0.679121 val loss: 1.294102
[Epoch 84] ogbg-molbace: 0.769779 test loss: 1.484831
[Epoch 85; Iter     6/   41] train: loss: 0.0780894
[Epoch 85; Iter    36/   41] train: loss: 0.0463869
[Epoch 85] ogbg-molbace: 0.646520 val loss: 2.040548
[Epoch 85] ogbg-molbace: 0.768562 test loss: 1.786415
[Epoch 86; Iter    25/   41] train: loss: 0.1322640
[Epoch 86] ogbg-molbace: 0.687912 val loss: 2.143890
[Epoch 86] ogbg-molbace: 0.828378 test loss: 1.169782
[Epoch 87; Iter    14/   41] train: loss: 0.1770028
[Epoch 87] ogbg-molbace: 0.706227 val loss: 1.489076
[Epoch 87] ogbg-molbace: 0.817597 test loss: 2.047400
[Epoch 88; Iter     3/   41] train: loss: 0.0990934
[Epoch 88; Iter    33/   41] train: loss: 0.0447118
[Epoch 88] ogbg-molbace: 0.705495 val loss: 1.704606
[Epoch 88] ogbg-molbace: 0.816554 test loss: 1.261873
[Epoch 89; Iter    22/   41] train: loss: 0.0830157
[Epoch 89] ogbg-molbace: 0.677289 val loss: 2.303398
[Epoch 89] ogbg-molbace: 0.806816 test loss: 1.871948
[Epoch 90; Iter    11/   41] train: loss: 0.0274068
[Epoch 90; Iter    41/   41] train: loss: 0.2689407
[Epoch 90] ogbg-molbace: 0.691209 val loss: 2.723814
[Epoch 90] ogbg-molbace: 0.801252 test loss: 1.263053
[Epoch 91; Iter    30/   41] train: loss: 0.0421928
[Epoch 91] ogbg-molbace: 0.650183 val loss: 2.222142
[Epoch 91] ogbg-molbace: 0.787863 test loss: 1.766873
[Epoch 92; Iter    19/   41] train: loss: 0.0075940
[Epoch 92] ogbg-molbace: 0.688278 val loss: 2.396961
[Epoch 92] ogbg-molbace: 0.796905 test loss: 1.935851
[Epoch 93; Iter     8/   41] train: loss: 0.0031720
[Epoch 93; Iter    38/   41] train: loss: 0.2880206
[Epoch 93] ogbg-molbace: 0.674725 val loss: 2.315965
[Epoch 93] ogbg-molbace: 0.813772 test loss: 1.644086
[Epoch 94; Iter    27/   41] train: loss: 0.0059859
[Epoch 94] ogbg-molbace: 0.662637 val loss: 2.709497
[Epoch 94] ogbg-molbace: 0.783864 test loss: 2.809750
[Epoch 95; Iter    16/   41] train: loss: 0.1220796
[Epoch 95] ogbg-molbace: 0.660073 val loss: 2.293433
[Epoch 95] ogbg-molbace: 0.829769 test loss: 2.103215
[Epoch 96; Iter     5/   41] train: loss: 0.0101408
[Epoch 96; Iter    35/   41] train: loss: 0.0965159
[Epoch 96] ogbg-molbace: 0.663004 val loss: 2.579408
[Epoch 96] ogbg-molbace: 0.822987 test loss: 1.532341
[Epoch 97; Iter    24/   41] train: loss: 0.2219187
[Epoch 97] ogbg-molbace: 0.632967 val loss: 2.870686
[Epoch 97] ogbg-molbace: 0.743871 test loss: 2.581548
[Epoch 98; Iter    13/   41] train: loss: 0.0828789
[Epoch 98] ogbg-molbace: 0.661905 val loss: 2.794996
[Epoch 98] ogbg-molbace: 0.809077 test loss: 1.645582
[Epoch 99; Iter     2/   41] train: loss: 0.0302624
[Epoch 99; Iter    32/   41] train: loss: 0.0494527
[Epoch 99] ogbg-molbace: 0.665568 val loss: 2.646813
[Epoch 99] ogbg-molbace: 0.755347 test loss: 2.381264
[Epoch 100; Iter    21/   41] train: loss: 0.0424550
[Epoch 100] ogbg-molbace: 0.630037 val loss: 2.742977
[Epoch 100] ogbg-molbace: 0.817249 test loss: 2.033773
[Epoch 101; Iter    10/   41] train: loss: 0.0082113
[Epoch 101; Iter    40/   41] train: loss: 0.0036858
[Epoch 101] ogbg-molbace: 0.658242 val loss: 2.883782
[Epoch 101] ogbg-molbace: 0.809077 test loss: 1.609482
[Epoch 102; Iter    29/   41] train: loss: 0.0130413
[Epoch 102] ogbg-molbace: 0.658608 val loss: 3.034568
[Epoch 102] ogbg-molbace: 0.809424 test loss: 2.005849
[Epoch 103; Iter    18/   41] train: loss: 0.0092392
[Epoch 103] ogbg-molbace: 0.656044 val loss: 3.011941
[Epoch 103] ogbg-molbace: 0.810989 test loss: 2.002539
[Epoch 104; Iter     7/   41] train: loss: 0.0020033
[Epoch 104; Iter    37/   41] train: loss: 0.0043877
[Epoch 104] ogbg-molbace: 0.667399 val loss: 2.831936
[Epoch 104] ogbg-molbace: 0.807338 test loss: 1.845506
[Epoch 105; Iter    26/   41] train: loss: 0.0009881
[Epoch 105] ogbg-molbace: 0.669963 val loss: 2.961086
[Epoch 105] ogbg-molbace: 0.810989 test loss: 2.069905
[Epoch 106; Iter    15/   41] train: loss: 0.0031501
[Epoch 106] ogbg-molbace: 0.667033 val loss: 2.909062
[Epoch 106] ogbg-molbace: 0.803165 test loss: 1.892718
[Epoch 107; Iter     4/   41] train: loss: 0.0005150
[Epoch 107; Iter    34/   41] train: loss: 0.0038994
[Epoch 107] ogbg-molbace: 0.669231 val loss: 3.006303
[Epoch 107] ogbg-molbace: 0.800209 test loss: 2.254709
[Epoch 108; Iter    23/   41] train: loss: 0.0030823
[Epoch 108] ogbg-molbace: 0.683516 val loss: 2.838821
[Epoch 108] ogbg-molbace: 0.804034 test loss: 1.855529
[Epoch 109; Iter    12/   41] train: loss: 0.0393431
[Epoch 109] ogbg-molbace: 0.657509 val loss: 3.023149
[Epoch 109] ogbg-molbace: 0.793253 test loss: 2.439344
[Epoch 110; Iter     1/   41] train: loss: 0.0054283
[Epoch 110; Iter    31/   41] train: loss: 0.0034741
[Epoch 110] ogbg-molbace: 0.666667 val loss: 2.804485
[Epoch 110] ogbg-molbace: 0.806990 test loss: 1.750514
[Epoch 111; Iter    20/   41] train: loss: 0.0064097
[Epoch 111] ogbg-molbace: 0.711722 val loss: 2.665526
[Epoch 111] ogbg-molbace: 0.762998 test loss: 2.079078
[Epoch 112; Iter     9/   41] train: loss: 0.0300374
[Epoch 112; Iter    39/   41] train: loss: 0.0017086
[Epoch 112] ogbg-molbace: 0.661905 val loss: 3.135664
[Epoch 112] ogbg-molbace: 0.767693 test loss: 2.026369
[Epoch 113; Iter    28/   41] train: loss: 0.0008183
[Epoch 113] ogbg-molbace: 0.678022 val loss: 3.374165
[Epoch 113] ogbg-molbace: 0.793601 test loss: 2.041208
[Epoch 114; Iter    17/   41] train: loss: 0.0239494
[Epoch 114] ogbg-molbace: 0.678022 val loss: 3.210807
[Epoch 114] ogbg-molbace: 0.816728 test loss: 1.945951
[Epoch 115; Iter     6/   41] train: loss: 0.0029101
[Epoch 115; Iter    36/   41] train: loss: 0.0042005
[Epoch 115] ogbg-molbace: 0.664103 val loss: 3.060318
[Epoch 115] ogbg-molbace: 0.791515 test loss: 2.058514
[Epoch 116; Iter    25/   41] train: loss: 0.0004819
[Epoch 116] ogbg-molbace: 0.673260 val loss: 3.149573
[Epoch 116] ogbg-molbace: 0.813598 test loss: 1.867155
[Epoch 117; Iter    14/   41] train: loss: 0.0042788
[Epoch 117] ogbg-molbace: 0.666667 val loss: 3.176586
[Epoch 117] ogbg-molbace: 0.801252 test loss: 1.837871
[Epoch 118; Iter     3/   41] train: loss: 0.0006597
[Epoch 118; Iter    33/   41] train: loss: 0.0006052
[Epoch 118] ogbg-molbace: 0.667033 val loss: 3.233280
[Epoch 118] ogbg-molbace: 0.808729 test loss: 1.780378
[Epoch 119; Iter    22/   41] train: loss: 0.0023850
[Epoch 119] ogbg-molbace: 0.672894 val loss: 3.286838
[Epoch 119] ogbg-molbace: 0.801774 test loss: 2.141570
[Epoch 120; Iter    11/   41] train: loss: 0.0008382
[Epoch 120; Iter    41/   41] train: loss: 0.0004738
[Epoch 120] ogbg-molbace: 0.674725 val loss: 3.194912
[Epoch 120] ogbg-molbace: 0.804382 test loss: 1.892964
[Epoch 121; Iter    30/   41] train: loss: 0.0005798
[Epoch 121] ogbg-molbace: 0.672894 val loss: 3.174202
[Epoch 121] ogbg-molbace: 0.804556 test loss: 1.778185
[Epoch 122; Iter    19/   41] train: loss: 0.0017008
[Epoch 122] ogbg-molbace: 0.676557 val loss: 3.367793
[Epoch 122] ogbg-molbace: 0.808555 test loss: 1.907243
[Epoch 123; Iter     8/   41] train: loss: 0.0006277
[Epoch 123; Iter    38/   41] train: loss: 0.0006505
[Epoch 123] ogbg-molbace: 0.664469 val loss: 3.324269
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7074699531274703
rocauc: 0.7390019127108328
ogbg-molbace: 0.7390019127108328
BCEWithLogitsLoss: 1.0555295993884404
Statistics on  train
mean_pred: -0.4457167983055115
std_pred: 1.1920838356018066
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.6786268048298842
rocauc: 0.7861415525114155
ogbg-molbace: 0.7861415525114155
BCEWithLogitsLoss: 0.5486142897024388
[Epoch 78] ogbg-molbace: 0.631868 val loss: 4.953605
[Epoch 78] ogbg-molbace: 0.727700 test loss: 5.692286
[Epoch 79; Iter    12/   41] train: loss: 0.0009920
[Epoch 79] ogbg-molbace: 0.620879 val loss: 5.208542
[Epoch 79] ogbg-molbace: 0.717962 test loss: 5.523882
[Epoch 80; Iter     1/   41] train: loss: 0.0012233
[Epoch 80; Iter    31/   41] train: loss: 0.0156272
[Epoch 80] ogbg-molbace: 0.622711 val loss: 6.084724
[Epoch 80] ogbg-molbace: 0.719179 test loss: 6.176444
[Epoch 81; Iter    20/   41] train: loss: 0.0009620
[Epoch 81] ogbg-molbace: 0.623810 val loss: 5.887034
[Epoch 81] ogbg-molbace: 0.721440 test loss: 5.858097
[Epoch 82; Iter     9/   41] train: loss: 0.0001956
[Epoch 82; Iter    39/   41] train: loss: 0.0021026
[Epoch 82] ogbg-molbace: 0.572527 val loss: 4.979958
[Epoch 82] ogbg-molbace: 0.660581 test loss: 5.471080
[Epoch 83; Iter    28/   41] train: loss: 0.0312754
[Epoch 83] ogbg-molbace: 0.634066 val loss: 5.066813
[Epoch 83] ogbg-molbace: 0.697792 test loss: 5.464250
[Epoch 84; Iter    17/   41] train: loss: 0.0011561
[Epoch 84] ogbg-molbace: 0.551282 val loss: 6.142465
[Epoch 84] ogbg-molbace: 0.730134 test loss: 5.757515
[Epoch 85; Iter     6/   41] train: loss: 0.1150778
[Epoch 85; Iter    36/   41] train: loss: 0.3661789
[Epoch 85] ogbg-molbace: 0.560073 val loss: 1.651723
[Epoch 85] ogbg-molbace: 0.757781 test loss: 2.902033
[Epoch 86; Iter    25/   41] train: loss: 0.2369074
[Epoch 86] ogbg-molbace: 0.636996 val loss: 1.450385
[Epoch 86] ogbg-molbace: 0.717267 test loss: 1.812633
[Epoch 87; Iter    14/   41] train: loss: 0.2688331
[Epoch 87] ogbg-molbace: 0.639194 val loss: 1.889270
[Epoch 87] ogbg-molbace: 0.701269 test loss: 1.783355
[Epoch 88; Iter     3/   41] train: loss: 0.1821254
[Epoch 88; Iter    33/   41] train: loss: 0.1452648
[Epoch 88] ogbg-molbace: 0.630769 val loss: 1.634231
[Epoch 88] ogbg-molbace: 0.727700 test loss: 2.114064
[Epoch 89; Iter    22/   41] train: loss: 0.0535020
[Epoch 89] ogbg-molbace: 0.614286 val loss: 3.030147
[Epoch 89] ogbg-molbace: 0.720744 test loss: 3.254495
[Epoch 90; Iter    11/   41] train: loss: 0.0130963
[Epoch 90; Iter    41/   41] train: loss: 0.2340489
[Epoch 90] ogbg-molbace: 0.630037 val loss: 3.016568
[Epoch 90] ogbg-molbace: 0.719179 test loss: 3.281723
[Epoch 91; Iter    30/   41] train: loss: 0.0398155
[Epoch 91] ogbg-molbace: 0.619780 val loss: 2.714849
[Epoch 91] ogbg-molbace: 0.740567 test loss: 3.122669
[Epoch 92; Iter    19/   41] train: loss: 0.0175642
[Epoch 92] ogbg-molbace: 0.619414 val loss: 3.360250
[Epoch 92] ogbg-molbace: 0.720918 test loss: 3.591416
[Epoch 93; Iter     8/   41] train: loss: 0.0190606
[Epoch 93; Iter    38/   41] train: loss: 0.0041076
[Epoch 93] ogbg-molbace: 0.623077 val loss: 3.781440
[Epoch 93] ogbg-molbace: 0.722135 test loss: 4.415453
[Epoch 94; Iter    27/   41] train: loss: 0.0023085
[Epoch 94] ogbg-molbace: 0.649451 val loss: 3.428174
[Epoch 94] ogbg-molbace: 0.731177 test loss: 3.709186
[Epoch 95; Iter    16/   41] train: loss: 0.0046820
[Epoch 95] ogbg-molbace: 0.600000 val loss: 3.376928
[Epoch 95] ogbg-molbace: 0.733959 test loss: 3.088151
[Epoch 96; Iter     5/   41] train: loss: 0.0094445
[Epoch 96; Iter    35/   41] train: loss: 0.0200265
[Epoch 96] ogbg-molbace: 0.632234 val loss: 2.893720
[Epoch 96] ogbg-molbace: 0.710833 test loss: 2.994421
[Epoch 97; Iter    24/   41] train: loss: 0.0043043
[Epoch 97] ogbg-molbace: 0.611722 val loss: 4.028348
[Epoch 97] ogbg-molbace: 0.747001 test loss: 4.019520
[Epoch 98; Iter    13/   41] train: loss: 0.0109391
[Epoch 98] ogbg-molbace: 0.626740 val loss: 4.368735
[Epoch 98] ogbg-molbace: 0.738132 test loss: 3.874616
[Epoch 99; Iter     2/   41] train: loss: 0.1154893
[Epoch 99; Iter    32/   41] train: loss: 0.0012892
[Epoch 99] ogbg-molbace: 0.631502 val loss: 5.129722
[Epoch 99] ogbg-molbace: 0.700574 test loss: 5.385877
[Epoch 100; Iter    21/   41] train: loss: 0.0028917
[Epoch 100] ogbg-molbace: 0.596337 val loss: 9.031119
[Epoch 100] ogbg-molbace: 0.694662 test loss: 8.106673
[Epoch 101; Iter    10/   41] train: loss: 0.0051895
[Epoch 101; Iter    40/   41] train: loss: 0.0143931
[Epoch 101] ogbg-molbace: 0.644689 val loss: 4.202802
[Epoch 101] ogbg-molbace: 0.761607 test loss: 5.804095
[Epoch 102; Iter    29/   41] train: loss: 0.0595601
[Epoch 102] ogbg-molbace: 0.598901 val loss: 4.070666
[Epoch 102] ogbg-molbace: 0.737437 test loss: 4.176877
[Epoch 103; Iter    18/   41] train: loss: 0.0355547
[Epoch 103] ogbg-molbace: 0.604762 val loss: 10.610912
[Epoch 103] ogbg-molbace: 0.671188 test loss: 8.974307
[Epoch 104; Iter     7/   41] train: loss: 0.2019099
[Epoch 104; Iter    37/   41] train: loss: 0.0292778
[Epoch 104] ogbg-molbace: 0.578755 val loss: 4.756870
[Epoch 104] ogbg-molbace: 0.624935 test loss: 5.051922
[Epoch 105; Iter    26/   41] train: loss: 0.2147290
[Epoch 105] ogbg-molbace: 0.592308 val loss: 5.122606
[Epoch 105] ogbg-molbace: 0.669623 test loss: 5.251862
[Epoch 106; Iter    15/   41] train: loss: 0.1464370
[Epoch 106] ogbg-molbace: 0.598168 val loss: 4.825828
[Epoch 106] ogbg-molbace: 0.708746 test loss: 5.011613
[Epoch 107; Iter     4/   41] train: loss: 0.0168285
[Epoch 107; Iter    34/   41] train: loss: 0.0141064
[Epoch 107] ogbg-molbace: 0.617582 val loss: 3.060045
[Epoch 107] ogbg-molbace: 0.726135 test loss: 2.890645
[Epoch 108; Iter    23/   41] train: loss: 0.0061142
[Epoch 108] ogbg-molbace: 0.614286 val loss: 3.077746
[Epoch 108] ogbg-molbace: 0.732742 test loss: 3.404739
[Epoch 109; Iter    12/   41] train: loss: 0.0320699
[Epoch 109] ogbg-molbace: 0.630037 val loss: 3.430391
[Epoch 109] ogbg-molbace: 0.746653 test loss: 3.706738
[Epoch 110; Iter     1/   41] train: loss: 0.0135840
[Epoch 110; Iter    31/   41] train: loss: 0.0114091
[Epoch 110] ogbg-molbace: 0.613187 val loss: 4.457892
[Epoch 110] ogbg-molbace: 0.744914 test loss: 4.486303
[Epoch 111; Iter    20/   41] train: loss: 0.0078075
[Epoch 111] ogbg-molbace: 0.617216 val loss: 5.209941
[Epoch 111] ogbg-molbace: 0.739350 test loss: 5.156142
[Epoch 112; Iter     9/   41] train: loss: 0.0014981
[Epoch 112; Iter    39/   41] train: loss: 0.0025117
[Epoch 112] ogbg-molbace: 0.614652 val loss: 5.044392
[Epoch 112] ogbg-molbace: 0.741262 test loss: 4.979940
[Epoch 113; Iter    28/   41] train: loss: 0.0014688
[Epoch 113] ogbg-molbace: 0.606960 val loss: 5.387824
[Epoch 113] ogbg-molbace: 0.730829 test loss: 5.293103
[Epoch 114; Iter    17/   41] train: loss: 0.0011015
[Epoch 114] ogbg-molbace: 0.606593 val loss: 5.003675
[Epoch 114] ogbg-molbace: 0.736568 test loss: 4.909210
[Epoch 115; Iter     6/   41] train: loss: 0.0026398
[Epoch 115; Iter    36/   41] train: loss: 0.0009700
[Epoch 115] ogbg-molbace: 0.608059 val loss: 5.334496
[Epoch 115] ogbg-molbace: 0.736568 test loss: 5.144495
[Epoch 116; Iter    25/   41] train: loss: 0.0007202
[Epoch 116] ogbg-molbace: 0.614286 val loss: 4.911529
[Epoch 116] ogbg-molbace: 0.740219 test loss: 4.933545
[Epoch 117; Iter    14/   41] train: loss: 0.0048947
[Epoch 117] ogbg-molbace: 0.605495 val loss: 5.376461
[Epoch 117] ogbg-molbace: 0.735003 test loss: 5.291365
[Epoch 118; Iter     3/   41] train: loss: 0.0006362
[Epoch 118; Iter    33/   41] train: loss: 0.0009411
[Epoch 118] ogbg-molbace: 0.611722 val loss: 4.987321
[Epoch 118] ogbg-molbace: 0.741436 test loss: 4.844525
[Epoch 119; Iter    22/   41] train: loss: 0.0005858
[Epoch 119] ogbg-molbace: 0.608425 val loss: 5.820081
[Epoch 119] ogbg-molbace: 0.734655 test loss: 5.579386
[Epoch 120; Iter    11/   41] train: loss: 0.0012264
[Epoch 120; Iter    41/   41] train: loss: 0.0031034
[Epoch 120] ogbg-molbace: 0.607326 val loss: 5.573213
[Epoch 120] ogbg-molbace: 0.735698 test loss: 5.514097
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 25.
Statistics on  val_best_checkpoint
mean_pred: -0.5417965650558472
std_pred: 0.596430242061615
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.948364246634455
rocauc: 0.7183150183150183
ogbg-molbace: 0.7183150183150183
BCEWithLogitsLoss: 0.8303474485874176
Statistics on  test
mean_pred: -0.6665000915527344
std_pred: 1.1258270740509033
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.748100642593366
rocauc: 0.7723874108850635
ogbg-molbace: 0.7723874108850635
BCEWithLogitsLoss: 0.8429428959886233
Statistics on  train
mean_pred: -0.30081111192703247
std_pred: 0.7974799871444702
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.6396234601427702
rocauc: 0.7835159817351598
ogbg-molbace: 0.7835159817351598
BCEWithLogitsLoss: 0.5710839287536901
[Epoch 78] ogbg-molbace: 0.682051 val loss: 1.332951
[Epoch 78] ogbg-molbace: 0.760389 test loss: 2.149058
[Epoch 79; Iter    12/   41] train: loss: 0.0060524
[Epoch 79] ogbg-molbace: 0.699634 val loss: 1.359137
[Epoch 79] ogbg-molbace: 0.787167 test loss: 2.065895
[Epoch 80; Iter     1/   41] train: loss: 0.0138113
[Epoch 80; Iter    31/   41] train: loss: 0.0028369
[Epoch 80] ogbg-molbace: 0.702198 val loss: 1.305564
[Epoch 80] ogbg-molbace: 0.785950 test loss: 2.065151
[Epoch 81; Iter    20/   41] train: loss: 0.0039116
[Epoch 81] ogbg-molbace: 0.691941 val loss: 1.525252
[Epoch 81] ogbg-molbace: 0.781951 test loss: 2.459829
[Epoch 82; Iter     9/   41] train: loss: 0.0026406
[Epoch 82; Iter    39/   41] train: loss: 0.0025317
[Epoch 82] ogbg-molbace: 0.703663 val loss: 1.362371
[Epoch 82] ogbg-molbace: 0.791862 test loss: 2.185860
[Epoch 83; Iter    28/   41] train: loss: 0.0014410
[Epoch 83] ogbg-molbace: 0.688278 val loss: 1.394044
[Epoch 83] ogbg-molbace: 0.782994 test loss: 2.165934
[Epoch 84; Iter    17/   41] train: loss: 0.0033791
[Epoch 84] ogbg-molbace: 0.693040 val loss: 1.473266
[Epoch 84] ogbg-molbace: 0.781951 test loss: 2.337252
[Epoch 85; Iter     6/   41] train: loss: 0.0026458
[Epoch 85; Iter    36/   41] train: loss: 0.0012730
[Epoch 85] ogbg-molbace: 0.697070 val loss: 1.425682
[Epoch 85] ogbg-molbace: 0.784038 test loss: 2.324408
[Epoch 86; Iter    25/   41] train: loss: 0.0084024
[Epoch 86] ogbg-molbace: 0.690842 val loss: 1.479254
[Epoch 86] ogbg-molbace: 0.782646 test loss: 2.272384
[Epoch 87; Iter    14/   41] train: loss: 0.0124443
[Epoch 87] ogbg-molbace: 0.695971 val loss: 1.383713
[Epoch 87] ogbg-molbace: 0.790123 test loss: 2.031037
[Epoch 88; Iter     3/   41] train: loss: 0.0019917
[Epoch 88; Iter    33/   41] train: loss: 0.0019002
[Epoch 88] ogbg-molbace: 0.696703 val loss: 1.407977
[Epoch 88] ogbg-molbace: 0.787341 test loss: 2.218235
[Epoch 89; Iter    22/   41] train: loss: 0.0118693
[Epoch 89] ogbg-molbace: 0.699634 val loss: 1.614453
[Epoch 89] ogbg-molbace: 0.782473 test loss: 2.580806
[Epoch 90; Iter    11/   41] train: loss: 0.0064996
[Epoch 90; Iter    41/   41] train: loss: 0.0233279
[Epoch 90] ogbg-molbace: 0.695238 val loss: 1.425936
[Epoch 90] ogbg-molbace: 0.760042 test loss: 2.670592
[Epoch 91; Iter    30/   41] train: loss: 0.0010376
[Epoch 91] ogbg-molbace: 0.691575 val loss: 1.484573
[Epoch 91] ogbg-molbace: 0.756390 test loss: 2.624884
[Epoch 92; Iter    19/   41] train: loss: 0.0014213
[Epoch 92] ogbg-molbace: 0.699267 val loss: 1.446613
[Epoch 92] ogbg-molbace: 0.766128 test loss: 2.501032
[Epoch 93; Iter     8/   41] train: loss: 0.0006138
[Epoch 93; Iter    38/   41] train: loss: 0.0079483
[Epoch 93] ogbg-molbace: 0.699267 val loss: 1.470717
[Epoch 93] ogbg-molbace: 0.767345 test loss: 2.535669
[Epoch 94; Iter    27/   41] train: loss: 0.0018072
[Epoch 94] ogbg-molbace: 0.694139 val loss: 1.566635
[Epoch 94] ogbg-molbace: 0.768388 test loss: 2.877641
[Epoch 95; Iter    16/   41] train: loss: 0.0279751
[Epoch 95] ogbg-molbace: 0.698901 val loss: 1.542649
[Epoch 95] ogbg-molbace: 0.780386 test loss: 2.619449
[Epoch 96; Iter     5/   41] train: loss: 0.0010989
[Epoch 96; Iter    35/   41] train: loss: 0.0142227
[Epoch 96] ogbg-molbace: 0.692674 val loss: 1.573018
[Epoch 96] ogbg-molbace: 0.775170 test loss: 2.701978
[Epoch 97; Iter    24/   41] train: loss: 0.0066126
[Epoch 97] ogbg-molbace: 0.693773 val loss: 1.593375
[Epoch 97] ogbg-molbace: 0.778995 test loss: 2.597996
[Epoch 98; Iter    13/   41] train: loss: 0.0014743
[Epoch 98] ogbg-molbace: 0.695238 val loss: 1.632950
[Epoch 98] ogbg-molbace: 0.786124 test loss: 2.541991
[Epoch 99; Iter     2/   41] train: loss: 0.0004367
[Epoch 99; Iter    32/   41] train: loss: 0.0042237
[Epoch 99] ogbg-molbace: 0.679853 val loss: 1.619528
[Epoch 99] ogbg-molbace: 0.738132 test loss: 3.117840
[Epoch 100; Iter    21/   41] train: loss: 0.0022542
[Epoch 100] ogbg-molbace: 0.705495 val loss: 1.602565
[Epoch 100] ogbg-molbace: 0.764563 test loss: 2.673983
[Epoch 101; Iter    10/   41] train: loss: 0.0005969
[Epoch 101; Iter    40/   41] train: loss: 0.0013955
[Epoch 101] ogbg-molbace: 0.683516 val loss: 1.655295
[Epoch 101] ogbg-molbace: 0.736568 test loss: 2.926196
[Epoch 102; Iter    29/   41] train: loss: 0.0152492
[Epoch 102] ogbg-molbace: 0.710989 val loss: 1.431180
[Epoch 102] ogbg-molbace: 0.756390 test loss: 2.561071
[Epoch 103; Iter    18/   41] train: loss: 0.0016160
[Epoch 103] ogbg-molbace: 0.717949 val loss: 1.503759
[Epoch 103] ogbg-molbace: 0.768040 test loss: 2.656804
[Epoch 104; Iter     7/   41] train: loss: 0.0003861
[Epoch 104; Iter    37/   41] train: loss: 0.0004517
[Epoch 104] ogbg-molbace: 0.713553 val loss: 1.461777
[Epoch 104] ogbg-molbace: 0.769431 test loss: 2.449464
[Epoch 105; Iter    26/   41] train: loss: 0.0002679
[Epoch 105] ogbg-molbace: 0.714286 val loss: 1.497085
[Epoch 105] ogbg-molbace: 0.763519 test loss: 2.744546
[Epoch 106; Iter    15/   41] train: loss: 0.0009071
[Epoch 106] ogbg-molbace: 0.709890 val loss: 1.468912
[Epoch 106] ogbg-molbace: 0.769258 test loss: 2.528453
[Epoch 107; Iter     4/   41] train: loss: 0.0005398
[Epoch 107; Iter    34/   41] train: loss: 0.0026267
[Epoch 107] ogbg-molbace: 0.705495 val loss: 1.564561
[Epoch 107] ogbg-molbace: 0.764563 test loss: 2.828623
[Epoch 108; Iter    23/   41] train: loss: 0.0016655
[Epoch 108] ogbg-molbace: 0.708059 val loss: 1.472918
[Epoch 108] ogbg-molbace: 0.769779 test loss: 2.447146
[Epoch 109; Iter    12/   41] train: loss: 0.0008984
[Epoch 109] ogbg-molbace: 0.691209 val loss: 1.516806
[Epoch 109] ogbg-molbace: 0.747696 test loss: 2.344231
[Epoch 110; Iter     1/   41] train: loss: 0.0010605
[Epoch 110; Iter    31/   41] train: loss: 0.0009747
[Epoch 110] ogbg-molbace: 0.697436 val loss: 1.511274
[Epoch 110] ogbg-molbace: 0.762128 test loss: 2.372061
[Epoch 111; Iter    20/   41] train: loss: 0.0004041
[Epoch 111] ogbg-molbace: 0.692674 val loss: 1.802712
[Epoch 111] ogbg-molbace: 0.776561 test loss: 2.620818
[Epoch 112; Iter     9/   41] train: loss: 0.0010429
[Epoch 112; Iter    39/   41] train: loss: 0.0002398
[Epoch 112] ogbg-molbace: 0.709158 val loss: 1.665725
[Epoch 112] ogbg-molbace: 0.775343 test loss: 2.486098
[Epoch 113; Iter    28/   41] train: loss: 0.0003626
[Epoch 113] ogbg-molbace: 0.705861 val loss: 1.677127
[Epoch 113] ogbg-molbace: 0.782299 test loss: 2.635109
[Epoch 114; Iter    17/   41] train: loss: 0.0008053
[Epoch 114] ogbg-molbace: 0.690842 val loss: 1.829143
[Epoch 114] ogbg-molbace: 0.768214 test loss: 2.926452
[Epoch 115; Iter     6/   41] train: loss: 0.0005674
[Epoch 115; Iter    36/   41] train: loss: 0.0077395
[Epoch 115] ogbg-molbace: 0.692674 val loss: 1.763982
[Epoch 115] ogbg-molbace: 0.774300 test loss: 2.632963
[Epoch 116; Iter    25/   41] train: loss: 0.0003505
[Epoch 116] ogbg-molbace: 0.690476 val loss: 1.812594
[Epoch 116] ogbg-molbace: 0.770649 test loss: 2.775906
[Epoch 117; Iter    14/   41] train: loss: 0.0004256
[Epoch 117] ogbg-molbace: 0.695971 val loss: 1.731622
[Epoch 117] ogbg-molbace: 0.777082 test loss: 2.637642
[Epoch 118; Iter     3/   41] train: loss: 0.0006202
[Epoch 118; Iter    33/   41] train: loss: 0.0002823
[Epoch 118] ogbg-molbace: 0.708059 val loss: 1.603981
[Epoch 118] ogbg-molbace: 0.789080 test loss: 2.398521
[Epoch 119; Iter    22/   41] train: loss: 0.0022704
[Epoch 119] ogbg-molbace: 0.695971 val loss: 1.715000
[Epoch 119] ogbg-molbace: 0.760911 test loss: 2.808896
[Epoch 120; Iter    11/   41] train: loss: 0.0004721
[Epoch 120; Iter    41/   41] train: loss: 0.0019119
[Epoch 120] ogbg-molbace: 0.701099 val loss: 1.657049
[Epoch 120] ogbg-molbace: 0.767693 test loss: 2.703868
[Epoch 121; Iter    30/   41] train: loss: 0.0004824
[Epoch 121] ogbg-molbace: 0.704396 val loss: 1.567194
[Epoch 121] ogbg-molbace: 0.779690 test loss: 2.457169
[Epoch 122; Iter    19/   41] train: loss: 0.0003178
[Epoch 122] ogbg-molbace: 0.711355 val loss: 1.587846
[Epoch 122] ogbg-molbace: 0.779690 test loss: 2.537701
[Epoch 123; Iter     8/   41] train: loss: 0.0004601
[Epoch 123; Iter    38/   41] train: loss: 0.0008345
[Epoch 123] ogbg-molbace: 0.704029 val loss: 1.661601
[Epoch 78] ogbg-molbace: 0.692308 val loss: 2.732479
[Epoch 78] ogbg-molbace: 0.817597 test loss: 1.630344
[Epoch 79; Iter    12/   41] train: loss: 0.0014497
[Epoch 79] ogbg-molbace: 0.682784 val loss: 2.362686
[Epoch 79] ogbg-molbace: 0.817249 test loss: 1.426878
[Epoch 80; Iter     1/   41] train: loss: 0.0029442
[Epoch 80; Iter    31/   41] train: loss: 0.0009146
[Epoch 80] ogbg-molbace: 0.689377 val loss: 2.550223
[Epoch 80] ogbg-molbace: 0.817945 test loss: 1.497276
[Epoch 81; Iter    20/   41] train: loss: 0.0022590
[Epoch 81] ogbg-molbace: 0.691575 val loss: 2.392141
[Epoch 81] ogbg-molbace: 0.825248 test loss: 1.404782
[Epoch 82; Iter     9/   41] train: loss: 0.0006224
[Epoch 82; Iter    39/   41] train: loss: 0.0008783
[Epoch 82] ogbg-molbace: 0.691941 val loss: 2.337790
[Epoch 82] ogbg-molbace: 0.820553 test loss: 1.371353
[Epoch 83; Iter    28/   41] train: loss: 0.0003912
[Epoch 83] ogbg-molbace: 0.695238 val loss: 2.567435
[Epoch 83] ogbg-molbace: 0.822466 test loss: 1.494887
[Epoch 84; Iter    17/   41] train: loss: 0.0009781
[Epoch 84] ogbg-molbace: 0.693407 val loss: 2.366467
[Epoch 84] ogbg-molbace: 0.824204 test loss: 1.392209
[Epoch 85; Iter     6/   41] train: loss: 0.0005305
[Epoch 85; Iter    36/   41] train: loss: 0.0003512
[Epoch 85] ogbg-molbace: 0.698168 val loss: 2.436650
[Epoch 85] ogbg-molbace: 0.826117 test loss: 1.432782
[Epoch 86; Iter    25/   41] train: loss: 0.0011372
[Epoch 86] ogbg-molbace: 0.696337 val loss: 2.593048
[Epoch 86] ogbg-molbace: 0.825248 test loss: 1.525110
[Epoch 87; Iter    14/   41] train: loss: 0.3343800
[Epoch 87] ogbg-molbace: 0.741026 val loss: 2.439343
[Epoch 87] ogbg-molbace: 0.801078 test loss: 2.472906
[Epoch 88; Iter     3/   41] train: loss: 0.1409457
[Epoch 88; Iter    33/   41] train: loss: 0.4551850
[Epoch 88] ogbg-molbace: 0.691575 val loss: 1.930625
[Epoch 88] ogbg-molbace: 0.810120 test loss: 1.584069
[Epoch 89; Iter    22/   41] train: loss: 0.6653790
[Epoch 89] ogbg-molbace: 0.706960 val loss: 2.310769
[Epoch 89] ogbg-molbace: 0.787863 test loss: 1.635733
[Epoch 90; Iter    11/   41] train: loss: 0.1521131
[Epoch 90; Iter    41/   41] train: loss: 0.2163287
[Epoch 90] ogbg-molbace: 0.703297 val loss: 1.456702
[Epoch 90] ogbg-molbace: 0.779169 test loss: 1.052852
[Epoch 91; Iter    30/   41] train: loss: 0.0114894
[Epoch 91] ogbg-molbace: 0.703297 val loss: 1.620224
[Epoch 91] ogbg-molbace: 0.785429 test loss: 1.058763
[Epoch 92; Iter    19/   41] train: loss: 0.0132155
[Epoch 92] ogbg-molbace: 0.713553 val loss: 1.853790
[Epoch 92] ogbg-molbace: 0.790471 test loss: 1.172991
[Epoch 93; Iter     8/   41] train: loss: 0.0087663
[Epoch 93; Iter    38/   41] train: loss: 0.0508148
[Epoch 93] ogbg-molbace: 0.706960 val loss: 1.697284
[Epoch 93] ogbg-molbace: 0.793427 test loss: 1.109333
[Epoch 94; Iter    27/   41] train: loss: 0.0086044
[Epoch 94] ogbg-molbace: 0.703663 val loss: 1.795145
[Epoch 94] ogbg-molbace: 0.785776 test loss: 1.156521
[Epoch 95; Iter    16/   41] train: loss: 0.0320000
[Epoch 95] ogbg-molbace: 0.704762 val loss: 1.908082
[Epoch 95] ogbg-molbace: 0.784385 test loss: 1.210662
[Epoch 96; Iter     5/   41] train: loss: 0.0050214
[Epoch 96; Iter    35/   41] train: loss: 0.0275438
[Epoch 96] ogbg-molbace: 0.709890 val loss: 2.063977
[Epoch 96] ogbg-molbace: 0.790993 test loss: 1.269511
[Epoch 97; Iter    24/   41] train: loss: 0.0238147
[Epoch 97] ogbg-molbace: 0.710623 val loss: 2.208484
[Epoch 97] ogbg-molbace: 0.790819 test loss: 1.319620
[Epoch 98; Iter    13/   41] train: loss: 0.0053495
[Epoch 98] ogbg-molbace: 0.706593 val loss: 2.204367
[Epoch 98] ogbg-molbace: 0.787341 test loss: 1.332691
[Epoch 99; Iter     2/   41] train: loss: 0.0024730
[Epoch 99; Iter    32/   41] train: loss: 0.0071989
[Epoch 99] ogbg-molbace: 0.707692 val loss: 2.322043
[Epoch 99] ogbg-molbace: 0.780908 test loss: 1.499705
[Epoch 100; Iter    21/   41] train: loss: 0.0044819
[Epoch 100] ogbg-molbace: 0.712454 val loss: 2.273686
[Epoch 100] ogbg-molbace: 0.786298 test loss: 1.457799
[Epoch 101; Iter    10/   41] train: loss: 0.0016944
[Epoch 101; Iter    40/   41] train: loss: 0.0010813
[Epoch 101] ogbg-molbace: 0.720879 val loss: 2.395032
[Epoch 101] ogbg-molbace: 0.791515 test loss: 1.513431
[Epoch 102; Iter    29/   41] train: loss: 0.0097496
[Epoch 102] ogbg-molbace: 0.712821 val loss: 2.196571
[Epoch 102] ogbg-molbace: 0.786124 test loss: 1.395136
[Epoch 103; Iter    18/   41] train: loss: 0.0024934
[Epoch 103] ogbg-molbace: 0.711355 val loss: 2.244099
[Epoch 103] ogbg-molbace: 0.784733 test loss: 1.429391
[Epoch 104; Iter     7/   41] train: loss: 0.0015361
[Epoch 104; Iter    37/   41] train: loss: 0.0012319
[Epoch 104] ogbg-molbace: 0.712454 val loss: 2.349045
[Epoch 104] ogbg-molbace: 0.786298 test loss: 1.497473
[Epoch 105; Iter    26/   41] train: loss: 0.0008886
[Epoch 105] ogbg-molbace: 0.711722 val loss: 2.214817
[Epoch 105] ogbg-molbace: 0.785603 test loss: 1.436073
[Epoch 106; Iter    15/   41] train: loss: 0.0022717
[Epoch 106] ogbg-molbace: 0.719780 val loss: 2.499341
[Epoch 106] ogbg-molbace: 0.788385 test loss: 1.594661
[Epoch 107; Iter     4/   41] train: loss: 0.0008362
[Epoch 107; Iter    34/   41] train: loss: 0.0037313
[Epoch 107] ogbg-molbace: 0.713187 val loss: 2.299482
[Epoch 107] ogbg-molbace: 0.784385 test loss: 1.491729
[Epoch 108; Iter    23/   41] train: loss: 0.0026008
[Epoch 108] ogbg-molbace: 0.711722 val loss: 2.699737
[Epoch 108] ogbg-molbace: 0.789602 test loss: 1.711339
[Epoch 109; Iter    12/   41] train: loss: 0.0041630
[Epoch 109] ogbg-molbace: 0.706960 val loss: 2.347677
[Epoch 109] ogbg-molbace: 0.773257 test loss: 1.652417
[Epoch 110; Iter     1/   41] train: loss: 0.0040154
[Epoch 110; Iter    31/   41] train: loss: 0.0013214
[Epoch 110] ogbg-molbace: 0.697436 val loss: 2.652971
[Epoch 110] ogbg-molbace: 0.785603 test loss: 1.665135
[Epoch 111; Iter    20/   41] train: loss: 0.0027829
[Epoch 111] ogbg-molbace: 0.698535 val loss: 2.750847
[Epoch 111] ogbg-molbace: 0.783168 test loss: 1.531200
[Epoch 112; Iter     9/   41] train: loss: 0.0015267
[Epoch 112; Iter    39/   41] train: loss: 0.0073019
[Epoch 112] ogbg-molbace: 0.699267 val loss: 3.013354
[Epoch 112] ogbg-molbace: 0.793601 test loss: 1.777814
[Epoch 113; Iter    28/   41] train: loss: 0.0020858
[Epoch 113] ogbg-molbace: 0.696337 val loss: 2.962473
[Epoch 113] ogbg-molbace: 0.792036 test loss: 1.746506
[Epoch 114; Iter    17/   41] train: loss: 0.0031412
[Epoch 114] ogbg-molbace: 0.704396 val loss: 2.762113
[Epoch 114] ogbg-molbace: 0.796209 test loss: 1.669854
[Epoch 115; Iter     6/   41] train: loss: 0.0028632
[Epoch 115; Iter    36/   41] train: loss: 0.0051474
[Epoch 115] ogbg-molbace: 0.702930 val loss: 2.758062
[Epoch 115] ogbg-molbace: 0.798991 test loss: 1.626632
[Epoch 116; Iter    25/   41] train: loss: 0.0003933
[Epoch 116] ogbg-molbace: 0.700366 val loss: 2.733039
[Epoch 116] ogbg-molbace: 0.800904 test loss: 1.637164
[Epoch 117; Iter    14/   41] train: loss: 0.0006226
[Epoch 117] ogbg-molbace: 0.702930 val loss: 2.783101
[Epoch 117] ogbg-molbace: 0.805077 test loss: 1.660469
[Epoch 118; Iter     3/   41] train: loss: 0.0006775
[Epoch 118; Iter    33/   41] train: loss: 0.0006569
[Epoch 118] ogbg-molbace: 0.702930 val loss: 2.850299
[Epoch 118] ogbg-molbace: 0.802991 test loss: 1.685366
[Epoch 119; Iter    22/   41] train: loss: 0.0021945
[Epoch 119] ogbg-molbace: 0.701099 val loss: 2.652847
[Epoch 119] ogbg-molbace: 0.801252 test loss: 1.615070
[Epoch 120; Iter    11/   41] train: loss: 0.0014011
[Epoch 120; Iter    41/   41] train: loss: 0.0010064
[Epoch 120] ogbg-molbace: 0.701465 val loss: 2.864225
[Epoch 120] ogbg-molbace: 0.801600 test loss: 1.717297
[Epoch 121; Iter    30/   41] train: loss: 0.0003628
[Epoch 121] ogbg-molbace: 0.703663 val loss: 3.039491
[Epoch 121] ogbg-molbace: 0.805251 test loss: 1.839718
[Epoch 122; Iter    19/   41] train: loss: 0.0008879
[Epoch 122] ogbg-molbace: 0.700733 val loss: 2.837568
[Epoch 122] ogbg-molbace: 0.803339 test loss: 1.717171
[Epoch 123; Iter     8/   41] train: loss: 0.0003872
[Epoch 123; Iter    38/   41] train: loss: 0.0009362
[Epoch 123] ogbg-molbace: 0.701099 val loss: 2.929155
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.812380 test loss: 2.091282
[Epoch 124; Iter    27/   41] train: loss: 0.0005537
[Epoch 124] ogbg-molbace: 0.664835 val loss: 3.318077
[Epoch 124] ogbg-molbace: 0.812554 test loss: 1.818939
[Epoch 125; Iter    16/   41] train: loss: 0.0008823
[Epoch 125] ogbg-molbace: 0.673626 val loss: 3.324483
[Epoch 125] ogbg-molbace: 0.812902 test loss: 1.837135
[Epoch 126; Iter     5/   41] train: loss: 0.0006757
[Epoch 126; Iter    35/   41] train: loss: 0.0003814
[Epoch 126] ogbg-molbace: 0.668498 val loss: 3.553267
[Epoch 126] ogbg-molbace: 0.802817 test loss: 1.812825
[Epoch 127; Iter    24/   41] train: loss: 0.1368855
[Epoch 127] ogbg-molbace: 0.651282 val loss: 3.708592
[Epoch 127] ogbg-molbace: 0.809077 test loss: 1.615310
[Epoch 128; Iter    13/   41] train: loss: 0.0003022
[Epoch 128] ogbg-molbace: 0.665568 val loss: 3.385453
[Epoch 128] ogbg-molbace: 0.812380 test loss: 1.396391
[Epoch 129; Iter     2/   41] train: loss: 0.0003451
[Epoch 129; Iter    32/   41] train: loss: 0.0019863
[Epoch 129] ogbg-molbace: 0.655311 val loss: 3.440579
[Epoch 129] ogbg-molbace: 0.805773 test loss: 1.790257
[Epoch 130; Iter    21/   41] train: loss: 0.0007784
[Epoch 130] ogbg-molbace: 0.662271 val loss: 3.398181
[Epoch 130] ogbg-molbace: 0.810468 test loss: 1.519040
[Epoch 131; Iter    10/   41] train: loss: 0.0005099
[Epoch 131; Iter    40/   41] train: loss: 0.0006816
[Epoch 131] ogbg-molbace: 0.641758 val loss: 3.279228
[Epoch 131] ogbg-molbace: 0.799513 test loss: 1.586854
[Epoch 132; Iter    29/   41] train: loss: 0.0022185
[Epoch 132] ogbg-molbace: 0.661172 val loss: 3.327301
[Epoch 132] ogbg-molbace: 0.808729 test loss: 1.603038
[Epoch 133; Iter    18/   41] train: loss: 0.0041172
[Epoch 133] ogbg-molbace: 0.661172 val loss: 3.316062
[Epoch 133] ogbg-molbace: 0.809772 test loss: 1.773811
[Epoch 134; Iter     7/   41] train: loss: 0.0006203
[Epoch 134; Iter    37/   41] train: loss: 0.0000991
[Epoch 134] ogbg-molbace: 0.665201 val loss: 3.293246
[Epoch 134] ogbg-molbace: 0.809772 test loss: 1.654191
[Epoch 135; Iter    26/   41] train: loss: 0.0001928
[Epoch 135] ogbg-molbace: 0.667399 val loss: 3.319968
[Epoch 135] ogbg-molbace: 0.812033 test loss: 1.521277
[Epoch 136; Iter    15/   41] train: loss: 0.0001449
[Epoch 136] ogbg-molbace: 0.664103 val loss: 3.234454
[Epoch 136] ogbg-molbace: 0.814641 test loss: 1.528944
[Epoch 137; Iter     4/   41] train: loss: 0.0002965
[Epoch 137; Iter    34/   41] train: loss: 0.0005537
[Epoch 137] ogbg-molbace: 0.658608 val loss: 3.401888
[Epoch 137] ogbg-molbace: 0.811859 test loss: 1.711255
[Epoch 138; Iter    23/   41] train: loss: 0.0002302
[Epoch 138] ogbg-molbace: 0.660440 val loss: 3.436944
[Epoch 138] ogbg-molbace: 0.813076 test loss: 1.722492
Early stopping criterion based on -ogbg-molbace- that should be max reached after 138 epochs. Best model checkpoint was in epoch 78.
Statistics on  val_best_checkpoint
mean_pred: 6.765253067016602
std_pred: 5.360386371612549
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9566053857637302
rocauc: 0.7542124542124542
ogbg-molbace: 0.7542124542124542
BCEWithLogitsLoss: 1.546098466962576
Statistics on  test
mean_pred: 2.272359609603882
std_pred: 8.756043434143066
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7742332243458031
rocauc: 0.7781255433837593
ogbg-molbace: 0.7781255433837593
BCEWithLogitsLoss: 1.2958074448009331
Statistics on  train
mean_pred: 5.9239821434021
std_pred: 8.303825378417969
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.809295337360206
rocauc: 0.8795861872146119
ogbg-molbace: 0.8795861872146119
BCEWithLogitsLoss: 2.35324107437599
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.05.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.05.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.05.yml --seed 6 --device cuda:1
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.798991 test loss: 1.741688
[Epoch 124; Iter    27/   41] train: loss: 0.0004824
[Epoch 124] ogbg-molbace: 0.701832 val loss: 3.041819
[Epoch 124] ogbg-molbace: 0.797774 test loss: 1.801762
[Epoch 125; Iter    16/   41] train: loss: 0.0009079
[Epoch 125] ogbg-molbace: 0.701465 val loss: 2.960654
[Epoch 125] ogbg-molbace: 0.797948 test loss: 1.749479
[Epoch 126; Iter     5/   41] train: loss: 0.0006259
[Epoch 126; Iter    35/   41] train: loss: 0.0006214
[Epoch 126] ogbg-molbace: 0.695238 val loss: 2.899280
[Epoch 126] ogbg-molbace: 0.795340 test loss: 1.686100
[Epoch 127; Iter    24/   41] train: loss: 0.0131280
[Epoch 127] ogbg-molbace: 0.685714 val loss: 2.864426
[Epoch 127] ogbg-molbace: 0.781082 test loss: 1.529756
[Epoch 128; Iter    13/   41] train: loss: 0.0007089
[Epoch 128] ogbg-molbace: 0.704029 val loss: 3.210408
[Epoch 128] ogbg-molbace: 0.795862 test loss: 1.737173
[Epoch 129; Iter     2/   41] train: loss: 0.0019352
[Epoch 129; Iter    32/   41] train: loss: 0.0014226
[Epoch 129] ogbg-molbace: 0.706227 val loss: 2.984819
[Epoch 129] ogbg-molbace: 0.798122 test loss: 1.700430
[Epoch 130; Iter    21/   41] train: loss: 0.0002544
[Epoch 130] ogbg-molbace: 0.712821 val loss: 3.197503
[Epoch 130] ogbg-molbace: 0.804730 test loss: 1.806840
[Epoch 131; Iter    10/   41] train: loss: 0.0005281
[Epoch 131; Iter    40/   41] train: loss: 0.0006159
[Epoch 131] ogbg-molbace: 0.705861 val loss: 3.294973
[Epoch 131] ogbg-molbace: 0.805251 test loss: 1.851662
[Epoch 132; Iter    29/   41] train: loss: 0.0023593
[Epoch 132] ogbg-molbace: 0.697802 val loss: 3.106056
[Epoch 132] ogbg-molbace: 0.803686 test loss: 1.764277
[Epoch 133; Iter    18/   41] train: loss: 0.0050390
[Epoch 133] ogbg-molbace: 0.700000 val loss: 3.076371
[Epoch 133] ogbg-molbace: 0.807164 test loss: 1.767344
[Epoch 134; Iter     7/   41] train: loss: 0.0013891
[Epoch 134; Iter    37/   41] train: loss: 0.0002722
[Epoch 134] ogbg-molbace: 0.698901 val loss: 3.160925
[Epoch 134] ogbg-molbace: 0.801947 test loss: 1.815498
[Epoch 135; Iter    26/   41] train: loss: 0.0004932
[Epoch 135] ogbg-molbace: 0.697070 val loss: 3.312255
[Epoch 135] ogbg-molbace: 0.802817 test loss: 1.895573
[Epoch 136; Iter    15/   41] train: loss: 0.0002798
[Epoch 136] ogbg-molbace: 0.695238 val loss: 3.105658
[Epoch 136] ogbg-molbace: 0.800209 test loss: 1.789960
[Epoch 137; Iter     4/   41] train: loss: 0.0009142
[Epoch 137; Iter    34/   41] train: loss: 0.0005923
[Epoch 137] ogbg-molbace: 0.691941 val loss: 2.971278
[Epoch 137] ogbg-molbace: 0.797600 test loss: 1.724101
[Epoch 138; Iter    23/   41] train: loss: 0.0004230
[Epoch 138] ogbg-molbace: 0.692308 val loss: 3.016776
[Epoch 138] ogbg-molbace: 0.796035 test loss: 1.743926
[Epoch 139; Iter    12/   41] train: loss: 0.0003541
[Epoch 139] ogbg-molbace: 0.691575 val loss: 3.046729
[Epoch 139] ogbg-molbace: 0.798122 test loss: 1.759078
[Epoch 140; Iter     1/   41] train: loss: 0.0002598
[Epoch 140; Iter    31/   41] train: loss: 0.0005238
[Epoch 140] ogbg-molbace: 0.694505 val loss: 2.986385
[Epoch 140] ogbg-molbace: 0.799339 test loss: 1.736273
[Epoch 141; Iter    20/   41] train: loss: 0.0007799
[Epoch 141] ogbg-molbace: 0.693407 val loss: 3.229857
[Epoch 141] ogbg-molbace: 0.796383 test loss: 1.844382
[Epoch 142; Iter     9/   41] train: loss: 0.0004131
[Epoch 142; Iter    39/   41] train: loss: 0.0006958
[Epoch 142] ogbg-molbace: 0.693407 val loss: 3.295376
[Epoch 142] ogbg-molbace: 0.800035 test loss: 1.873934
[Epoch 143; Iter    28/   41] train: loss: 0.0038717
[Epoch 143] ogbg-molbace: 0.693040 val loss: 3.212998
[Epoch 143] ogbg-molbace: 0.801078 test loss: 1.799615
[Epoch 144; Iter    17/   41] train: loss: 0.0002705
[Epoch 144] ogbg-molbace: 0.695238 val loss: 3.405936
[Epoch 144] ogbg-molbace: 0.800904 test loss: 1.920475
[Epoch 145; Iter     6/   41] train: loss: 0.0003760
[Epoch 145; Iter    36/   41] train: loss: 0.0001355
[Epoch 145] ogbg-molbace: 0.697436 val loss: 3.197279
[Epoch 145] ogbg-molbace: 0.801774 test loss: 1.783136
[Epoch 146; Iter    25/   41] train: loss: 0.0001714
[Epoch 146] ogbg-molbace: 0.693773 val loss: 3.057040
[Epoch 146] ogbg-molbace: 0.801078 test loss: 1.701744
[Epoch 147; Iter    14/   41] train: loss: 0.0001507
[Epoch 147] ogbg-molbace: 0.690842 val loss: 3.089460
[Epoch 147] ogbg-molbace: 0.804382 test loss: 1.738047
Early stopping criterion based on -ogbg-molbace- that should be max reached after 147 epochs. Best model checkpoint was in epoch 87.
Statistics on  val_best_checkpoint
mean_pred: 10.213196754455566
std_pred: 7.097670078277588
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9524876675547413
rocauc: 0.7410256410256411
ogbg-molbace: 0.7410256410256411
BCEWithLogitsLoss: 2.439342888382574
Statistics on  test
mean_pred: 4.726161479949951
std_pred: 12.161917686462402
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7725999850316693
rocauc: 0.8010780733785429
ogbg-molbace: 0.8010780733785429
BCEWithLogitsLoss: 2.4729062964518866
Statistics on  train
mean_pred: 0.55718994140625
std_pred: 15.292810440063477
mean_targets: 0.39669421315193176
std_targets: 0.4894138276576996
prcauc: 0.9933536831828421
rocauc: 0.9945605022831051
ogbg-molbace: 0.9945605022831051
BCEWithLogitsLoss: 0.21421009629237941
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.2.yml --seed 4 --device cuda:3
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.2.yml --seed 5 --device cuda:3
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.2.yml --seed 6 --device cuda:3
All runs completed.
[Epoch 123] ogbg-molbace: 0.770822 test loss: 2.794105
[Epoch 124; Iter    27/   41] train: loss: 0.0006866
[Epoch 124] ogbg-molbace: 0.705495 val loss: 1.599644
[Epoch 124] ogbg-molbace: 0.771344 test loss: 2.635906
[Epoch 125; Iter    16/   41] train: loss: 0.0006189
[Epoch 125] ogbg-molbace: 0.709524 val loss: 1.630655
[Epoch 125] ogbg-molbace: 0.777430 test loss: 2.755073
[Epoch 126; Iter     5/   41] train: loss: 0.0003964
[Epoch 126; Iter    35/   41] train: loss: 0.0002584
[Epoch 126] ogbg-molbace: 0.705495 val loss: 1.685366
[Epoch 126] ogbg-molbace: 0.769953 test loss: 2.860839
[Epoch 127; Iter    24/   41] train: loss: 0.0171771
[Epoch 127] ogbg-molbace: 0.717582 val loss: 1.953443
[Epoch 127] ogbg-molbace: 0.785603 test loss: 2.987844
[Epoch 128; Iter    13/   41] train: loss: 0.0013680
[Epoch 128] ogbg-molbace: 0.733700 val loss: 1.587735
[Epoch 128] ogbg-molbace: 0.811511 test loss: 2.304386
[Epoch 129; Iter     2/   41] train: loss: 0.0076738
[Epoch 129; Iter    32/   41] train: loss: 0.0057378
[Epoch 129] ogbg-molbace: 0.701465 val loss: 1.486764
[Epoch 129] ogbg-molbace: 0.775170 test loss: 2.085994
[Epoch 130; Iter    21/   41] train: loss: 0.0009640
[Epoch 130] ogbg-molbace: 0.712454 val loss: 1.759145
[Epoch 130] ogbg-molbace: 0.793601 test loss: 2.098647
[Epoch 131; Iter    10/   41] train: loss: 0.0007299
[Epoch 131; Iter    40/   41] train: loss: 0.0051409
[Epoch 131] ogbg-molbace: 0.714286 val loss: 1.781049
[Epoch 131] ogbg-molbace: 0.801426 test loss: 2.225818
[Epoch 132; Iter    29/   41] train: loss: 0.0032589
[Epoch 132] ogbg-molbace: 0.715751 val loss: 1.745849
[Epoch 132] ogbg-molbace: 0.803860 test loss: 2.255669
[Epoch 133; Iter    18/   41] train: loss: 0.0017309
[Epoch 133] ogbg-molbace: 0.717582 val loss: 1.759373
[Epoch 133] ogbg-molbace: 0.804208 test loss: 2.375772
[Epoch 134; Iter     7/   41] train: loss: 0.0019600
[Epoch 134; Iter    37/   41] train: loss: 0.0004199
[Epoch 134] ogbg-molbace: 0.718315 val loss: 1.880478
[Epoch 134] ogbg-molbace: 0.813945 test loss: 2.407315
[Epoch 135; Iter    26/   41] train: loss: 0.0004464
[Epoch 135] ogbg-molbace: 0.713187 val loss: 1.895666
[Epoch 135] ogbg-molbace: 0.809598 test loss: 2.336833
[Epoch 136; Iter    15/   41] train: loss: 0.0002584
[Epoch 136] ogbg-molbace: 0.708791 val loss: 1.815606
[Epoch 136] ogbg-molbace: 0.795166 test loss: 2.383354
[Epoch 137; Iter     4/   41] train: loss: 0.0011114
[Epoch 137; Iter    34/   41] train: loss: 0.0005010
[Epoch 137] ogbg-molbace: 0.712454 val loss: 1.837237
[Epoch 137] ogbg-molbace: 0.803860 test loss: 2.446464
[Epoch 138; Iter    23/   41] train: loss: 0.0002118
[Epoch 138] ogbg-molbace: 0.704762 val loss: 1.844500
[Epoch 138] ogbg-molbace: 0.798644 test loss: 2.593168
[Epoch 139; Iter    12/   41] train: loss: 0.0012793
[Epoch 139] ogbg-molbace: 0.709524 val loss: 1.788569
[Epoch 139] ogbg-molbace: 0.798122 test loss: 2.580723
[Epoch 140; Iter     1/   41] train: loss: 0.0001937
[Epoch 140; Iter    31/   41] train: loss: 0.0004988
[Epoch 140] ogbg-molbace: 0.707692 val loss: 1.852134
[Epoch 140] ogbg-molbace: 0.803512 test loss: 2.500326
[Epoch 141; Iter    20/   41] train: loss: 0.0036972
[Epoch 141] ogbg-molbace: 0.724176 val loss: 1.875356
[Epoch 141] ogbg-molbace: 0.811859 test loss: 2.294516
[Epoch 142; Iter     9/   41] train: loss: 0.0004771
[Epoch 142; Iter    39/   41] train: loss: 0.0008816
[Epoch 142] ogbg-molbace: 0.713553 val loss: 1.898351
[Epoch 142] ogbg-molbace: 0.801426 test loss: 2.354379
[Epoch 143; Iter    28/   41] train: loss: 0.0674744
[Epoch 143] ogbg-molbace: 0.720879 val loss: 1.653729
[Epoch 143] ogbg-molbace: 0.776039 test loss: 1.994865
[Epoch 144; Iter    17/   41] train: loss: 0.0002756
[Epoch 144] ogbg-molbace: 0.701465 val loss: 1.585719
[Epoch 144] ogbg-molbace: 0.758825 test loss: 2.397160
[Epoch 145; Iter     6/   41] train: loss: 0.0008193
[Epoch 145; Iter    36/   41] train: loss: 0.0006471
[Epoch 145] ogbg-molbace: 0.707326 val loss: 1.563768
[Epoch 145] ogbg-molbace: 0.769605 test loss: 2.520090
[Epoch 146; Iter    25/   41] train: loss: 0.0003050
[Epoch 146] ogbg-molbace: 0.712821 val loss: 1.684108
[Epoch 146] ogbg-molbace: 0.772387 test loss: 2.702963
[Epoch 147; Iter    14/   41] train: loss: 0.0001783
[Epoch 147] ogbg-molbace: 0.699267 val loss: 1.585529
[Epoch 147] ogbg-molbace: 0.722135 test loss: 3.129347
[Epoch 148; Iter     3/   41] train: loss: 0.0027450
[Epoch 148; Iter    33/   41] train: loss: 0.0008359
[Epoch 148] ogbg-molbace: 0.735897 val loss: 1.445474
[Epoch 148] ogbg-molbace: 0.761259 test loss: 2.645100
[Epoch 149; Iter    22/   41] train: loss: 0.0018495
[Epoch 149] ogbg-molbace: 0.726740 val loss: 1.617104
[Epoch 149] ogbg-molbace: 0.775865 test loss: 2.529418
[Epoch 150; Iter    11/   41] train: loss: 0.0010961
[Epoch 150; Iter    41/   41] train: loss: 0.0002913
[Epoch 150] ogbg-molbace: 0.727839 val loss: 1.672767
[Epoch 150] ogbg-molbace: 0.778821 test loss: 2.437413
[Epoch 151; Iter    30/   41] train: loss: 0.0005489
[Epoch 151] ogbg-molbace: 0.722344 val loss: 1.784877
[Epoch 151] ogbg-molbace: 0.783342 test loss: 2.426001
[Epoch 152; Iter    19/   41] train: loss: 0.0005545
[Epoch 152] ogbg-molbace: 0.728571 val loss: 1.786918
[Epoch 152] ogbg-molbace: 0.785776 test loss: 2.345612
[Epoch 153; Iter     8/   41] train: loss: 0.0009082
[Epoch 153; Iter    38/   41] train: loss: 0.0007698
[Epoch 153] ogbg-molbace: 0.720879 val loss: 1.816318
[Epoch 153] ogbg-molbace: 0.775865 test loss: 2.603000
[Epoch 154; Iter    27/   41] train: loss: 0.0003938
[Epoch 154] ogbg-molbace: 0.718681 val loss: 1.868946
[Epoch 154] ogbg-molbace: 0.778821 test loss: 2.455606
[Epoch 155; Iter    16/   41] train: loss: 0.0023886
[Epoch 155] ogbg-molbace: 0.717949 val loss: 1.752472
[Epoch 155] ogbg-molbace: 0.777082 test loss: 2.465273
[Epoch 156; Iter     5/   41] train: loss: 0.0001323
[Epoch 156; Iter    35/   41] train: loss: 0.0001394
[Epoch 156] ogbg-molbace: 0.719780 val loss: 1.835483
[Epoch 156] ogbg-molbace: 0.782473 test loss: 2.394605
[Epoch 157; Iter    24/   41] train: loss: 0.0003622
[Epoch 157] ogbg-molbace: 0.721612 val loss: 1.782575
[Epoch 157] ogbg-molbace: 0.776908 test loss: 2.407092
[Epoch 158; Iter    13/   41] train: loss: 0.0011882
[Epoch 158] ogbg-molbace: 0.721245 val loss: 1.827163
[Epoch 158] ogbg-molbace: 0.776213 test loss: 2.440478
[Epoch 159; Iter     2/   41] train: loss: 0.0002609
[Epoch 159; Iter    32/   41] train: loss: 0.0004749
[Epoch 159] ogbg-molbace: 0.730403 val loss: 1.954366
[Epoch 159] ogbg-molbace: 0.821075 test loss: 2.319869
[Epoch 160; Iter    21/   41] train: loss: 0.0006285
[Epoch 160] ogbg-molbace: 0.723077 val loss: 1.876811
[Epoch 160] ogbg-molbace: 0.815510 test loss: 2.300009
[Epoch 161; Iter    10/   41] train: loss: 0.0034044
[Epoch 161; Iter    40/   41] train: loss: 0.0003997
[Epoch 161] ogbg-molbace: 0.717216 val loss: 1.873626
[Epoch 161] ogbg-molbace: 0.809772 test loss: 2.591608
[Epoch 162; Iter    29/   41] train: loss: 0.0003725
[Epoch 162] ogbg-molbace: 0.721978 val loss: 1.922802
[Epoch 162] ogbg-molbace: 0.814119 test loss: 2.260855
[Epoch 163; Iter    18/   41] train: loss: 0.0003699
[Epoch 163] ogbg-molbace: 0.721978 val loss: 1.884647
[Epoch 163] ogbg-molbace: 0.811511 test loss: 2.452256
[Epoch 164; Iter     7/   41] train: loss: 0.0002260
[Epoch 164; Iter    37/   41] train: loss: 0.0001964
[Epoch 164] ogbg-molbace: 0.720513 val loss: 1.869574
[Epoch 164] ogbg-molbace: 0.803512 test loss: 2.619374
[Epoch 165; Iter    26/   41] train: loss: 0.0003338
[Epoch 165] ogbg-molbace: 0.718315 val loss: 1.860766
[Epoch 165] ogbg-molbace: 0.798991 test loss: 2.501737
[Epoch 166; Iter    15/   41] train: loss: 0.0001979
[Epoch 166] ogbg-molbace: 0.719780 val loss: 1.871539
[Epoch 166] ogbg-molbace: 0.794123 test loss: 2.519842
[Epoch 167; Iter     4/   41] train: loss: 0.0004558
[Epoch 167; Iter    34/   41] train: loss: 0.0001915
[Epoch 167] ogbg-molbace: 0.715751 val loss: 1.737473
[Epoch 167] ogbg-molbace: 0.787341 test loss: 2.822918
[Epoch 168; Iter    23/   41] train: loss: 0.0021909
[Epoch 168] ogbg-molbace: 0.718681 val loss: 1.814502
[Epoch 168] ogbg-molbace: 0.792384 test loss: 2.977556[Epoch 78] ogbg-molbace: 0.735165 val loss: 1.207148
[Epoch 78] ogbg-molbace: 0.777430 test loss: 1.707127
[Epoch 79; Iter    12/   41] train: loss: 0.2068093
[Epoch 79] ogbg-molbace: 0.755311 val loss: 1.352410
[Epoch 79] ogbg-molbace: 0.768562 test loss: 2.186914
[Epoch 80; Iter     1/   41] train: loss: 0.2397466
[Epoch 80; Iter    31/   41] train: loss: 0.0746728
[Epoch 80] ogbg-molbace: 0.753114 val loss: 1.346020
[Epoch 80] ogbg-molbace: 0.787515 test loss: 2.078289
[Epoch 81; Iter    20/   41] train: loss: 0.1809258
[Epoch 81] ogbg-molbace: 0.769963 val loss: 0.990719
[Epoch 81] ogbg-molbace: 0.771518 test loss: 1.604469
[Epoch 82; Iter     9/   41] train: loss: 0.1195644
[Epoch 82; Iter    39/   41] train: loss: 0.0980470
[Epoch 82] ogbg-molbace: 0.750549 val loss: 1.502370
[Epoch 82] ogbg-molbace: 0.781429 test loss: 1.963752
[Epoch 83; Iter    28/   41] train: loss: 0.2152847
[Epoch 83] ogbg-molbace: 0.779487 val loss: 1.101910
[Epoch 83] ogbg-molbace: 0.776039 test loss: 1.805282
[Epoch 84; Iter    17/   41] train: loss: 0.1014175
[Epoch 84] ogbg-molbace: 0.781685 val loss: 1.183921
[Epoch 84] ogbg-molbace: 0.762998 test loss: 2.108263
[Epoch 85; Iter     6/   41] train: loss: 0.2283323
[Epoch 85; Iter    36/   41] train: loss: 0.0879141
[Epoch 85] ogbg-molbace: 0.761172 val loss: 1.455877
[Epoch 85] ogbg-molbace: 0.779169 test loss: 1.951017
[Epoch 86; Iter    25/   41] train: loss: 0.0747625
[Epoch 86] ogbg-molbace: 0.777289 val loss: 1.180633
[Epoch 86] ogbg-molbace: 0.758477 test loss: 2.068766
[Epoch 87; Iter    14/   41] train: loss: 0.1134266
[Epoch 87] ogbg-molbace: 0.754579 val loss: 1.226615
[Epoch 87] ogbg-molbace: 0.768040 test loss: 2.060894
[Epoch 88; Iter     3/   41] train: loss: 0.1248626
[Epoch 88; Iter    33/   41] train: loss: 0.0422650
[Epoch 88] ogbg-molbace: 0.731502 val loss: 1.327035
[Epoch 88] ogbg-molbace: 0.777256 test loss: 1.886159
[Epoch 89; Iter    22/   41] train: loss: 0.1262465
[Epoch 89] ogbg-molbace: 0.724542 val loss: 1.632533
[Epoch 89] ogbg-molbace: 0.760563 test loss: 2.220993
[Epoch 90; Iter    11/   41] train: loss: 0.1193992
[Epoch 90; Iter    41/   41] train: loss: 0.3989879
[Epoch 90] ogbg-molbace: 0.770696 val loss: 1.460895
[Epoch 90] ogbg-molbace: 0.751521 test loss: 1.958239
[Epoch 91; Iter    30/   41] train: loss: 0.0376044
[Epoch 91] ogbg-molbace: 0.751648 val loss: 1.503594
[Epoch 91] ogbg-molbace: 0.771170 test loss: 1.970531
[Epoch 92; Iter    19/   41] train: loss: 0.1855094
[Epoch 92] ogbg-molbace: 0.752747 val loss: 1.185826
[Epoch 92] ogbg-molbace: 0.778473 test loss: 3.543078
[Epoch 93; Iter     8/   41] train: loss: 0.2779580
[Epoch 93; Iter    38/   41] train: loss: 0.0396965
[Epoch 93] ogbg-molbace: 0.794139 val loss: 1.358944
[Epoch 93] ogbg-molbace: 0.758129 test loss: 2.766575
[Epoch 94; Iter    27/   41] train: loss: 0.0858242
[Epoch 94] ogbg-molbace: 0.720513 val loss: 1.449851
[Epoch 94] ogbg-molbace: 0.760737 test loss: 2.538781
[Epoch 95; Iter    16/   41] train: loss: 0.0549335
[Epoch 95] ogbg-molbace: 0.803663 val loss: 0.919271
[Epoch 95] ogbg-molbace: 0.749783 test loss: 2.556553
[Epoch 96; Iter     5/   41] train: loss: 0.1906151
[Epoch 96; Iter    35/   41] train: loss: 0.0427793
[Epoch 96] ogbg-molbace: 0.750916 val loss: 1.215996
[Epoch 96] ogbg-molbace: 0.737611 test loss: 2.274691
[Epoch 97; Iter    24/   41] train: loss: 0.1887926
[Epoch 97] ogbg-molbace: 0.772527 val loss: 1.482666
[Epoch 97] ogbg-molbace: 0.782646 test loss: 2.021354
[Epoch 98; Iter    13/   41] train: loss: 0.1412309
[Epoch 98] ogbg-molbace: 0.791575 val loss: 1.167652
[Epoch 98] ogbg-molbace: 0.749087 test loss: 2.055255
[Epoch 99; Iter     2/   41] train: loss: 0.0052456
[Epoch 99; Iter    32/   41] train: loss: 0.0104183
[Epoch 99] ogbg-molbace: 0.760073 val loss: 1.266703
[Epoch 99] ogbg-molbace: 0.742653 test loss: 2.086151
[Epoch 100; Iter    21/   41] train: loss: 0.0250869
[Epoch 100] ogbg-molbace: 0.772527 val loss: 1.268471
[Epoch 100] ogbg-molbace: 0.748565 test loss: 2.044450
[Epoch 101; Iter    10/   41] train: loss: 0.0472155
[Epoch 101; Iter    40/   41] train: loss: 0.0488848
[Epoch 101] ogbg-molbace: 0.780952 val loss: 1.177986
[Epoch 101] ogbg-molbace: 0.745609 test loss: 2.119604
[Epoch 102; Iter    29/   41] train: loss: 0.0514317
[Epoch 102] ogbg-molbace: 0.775458 val loss: 1.453074
[Epoch 102] ogbg-molbace: 0.744914 test loss: 2.178519
[Epoch 103; Iter    18/   41] train: loss: 0.0317619
[Epoch 103] ogbg-molbace: 0.745788 val loss: 1.555403
[Epoch 103] ogbg-molbace: 0.744566 test loss: 2.277908
[Epoch 104; Iter     7/   41] train: loss: 0.0587431
[Epoch 104; Iter    37/   41] train: loss: 0.0133506
[Epoch 104] ogbg-molbace: 0.741392 val loss: 1.548875
[Epoch 104] ogbg-molbace: 0.777604 test loss: 1.903127
[Epoch 105; Iter    26/   41] train: loss: 0.0107078
[Epoch 105] ogbg-molbace: 0.764469 val loss: 1.204441
[Epoch 105] ogbg-molbace: 0.769258 test loss: 2.206781
[Epoch 106; Iter    15/   41] train: loss: 0.2212640
[Epoch 106] ogbg-molbace: 0.752747 val loss: 1.684845
[Epoch 106] ogbg-molbace: 0.779864 test loss: 2.055868
[Epoch 107; Iter     4/   41] train: loss: 0.1307790
[Epoch 107; Iter    34/   41] train: loss: 0.0125318
[Epoch 107] ogbg-molbace: 0.778388 val loss: 1.309186
[Epoch 107] ogbg-molbace: 0.754999 test loss: 2.423000
[Epoch 108; Iter    23/   41] train: loss: 0.0612731
[Epoch 108] ogbg-molbace: 0.777656 val loss: 1.467552
[Epoch 108] ogbg-molbace: 0.752391 test loss: 2.325303
[Epoch 109; Iter    12/   41] train: loss: 0.0390208
[Epoch 109] ogbg-molbace: 0.720513 val loss: 1.933592
[Epoch 109] ogbg-molbace: 0.768388 test loss: 2.210302
[Epoch 110; Iter     1/   41] train: loss: 0.0118370
[Epoch 110; Iter    31/   41] train: loss: 0.0169033
[Epoch 110] ogbg-molbace: 0.709524 val loss: 1.793901
[Epoch 110] ogbg-molbace: 0.759172 test loss: 2.202164
[Epoch 111; Iter    20/   41] train: loss: 0.0158931
[Epoch 111] ogbg-molbace: 0.727106 val loss: 1.771515
[Epoch 111] ogbg-molbace: 0.744392 test loss: 2.805608
[Epoch 112; Iter     9/   41] train: loss: 0.0163072
[Epoch 112; Iter    39/   41] train: loss: 0.0279484
[Epoch 112] ogbg-molbace: 0.736630 val loss: 1.920745
[Epoch 112] ogbg-molbace: 0.749957 test loss: 2.989846
[Epoch 113; Iter    28/   41] train: loss: 0.0119417
[Epoch 113] ogbg-molbace: 0.721978 val loss: 1.949830
[Epoch 113] ogbg-molbace: 0.752391 test loss: 2.195562
[Epoch 114; Iter    17/   41] train: loss: 0.3031251
[Epoch 114] ogbg-molbace: 0.685348 val loss: 2.404147
[Epoch 114] ogbg-molbace: 0.770127 test loss: 2.325180
[Epoch 115; Iter     6/   41] train: loss: 0.0297623
[Epoch 115; Iter    36/   41] train: loss: 0.0329870
[Epoch 115] ogbg-molbace: 0.730037 val loss: 2.226441
[Epoch 115] ogbg-molbace: 0.754477 test loss: 2.378669
[Epoch 116; Iter    25/   41] train: loss: 0.0538764
[Epoch 116] ogbg-molbace: 0.727106 val loss: 2.214718
[Epoch 116] ogbg-molbace: 0.743697 test loss: 2.461533
[Epoch 117; Iter    14/   41] train: loss: 0.0520061
[Epoch 117] ogbg-molbace: 0.746154 val loss: 2.015744
[Epoch 117] ogbg-molbace: 0.787689 test loss: 1.936254
[Epoch 118; Iter     3/   41] train: loss: 0.0137132
[Epoch 118; Iter    33/   41] train: loss: 0.0111363
[Epoch 118] ogbg-molbace: 0.753846 val loss: 1.977597
[Epoch 118] ogbg-molbace: 0.775343 test loss: 2.271036
[Epoch 119; Iter    22/   41] train: loss: 0.0211753
[Epoch 119] ogbg-molbace: 0.726007 val loss: 2.148863
[Epoch 119] ogbg-molbace: 0.767171 test loss: 2.372535
[Epoch 120; Iter    11/   41] train: loss: 0.0980842
[Epoch 120; Iter    41/   41] train: loss: 0.0140478
[Epoch 120] ogbg-molbace: 0.728938 val loss: 2.304904
[Epoch 120] ogbg-molbace: 0.772214 test loss: 2.169802
[Epoch 121; Iter    30/   41] train: loss: 0.0556540
[Epoch 121] ogbg-molbace: 0.739927 val loss: 2.187187
[Epoch 121] ogbg-molbace: 0.761954 test loss: 2.137937
[Epoch 122; Iter    19/   41] train: loss: 0.0128134
[Epoch 122] ogbg-molbace: 0.728205 val loss: 2.097241
[Epoch 122] ogbg-molbace: 0.778299 test loss: 2.063677
[Epoch 123; Iter     8/   41] train: loss: 0.0343878
[Epoch 123; Iter    38/   41] train: loss: 0.0465363
[Epoch 123] ogbg-molbace: 0.765568 val loss: 2.121521
[Epoch 78] ogbg-molbace: 0.750549 val loss: 1.116670
[Epoch 78] ogbg-molbace: 0.828030 test loss: 1.500146
[Epoch 79; Iter    12/   41] train: loss: 0.0697508
[Epoch 79] ogbg-molbace: 0.761172 val loss: 1.314155
[Epoch 79] ogbg-molbace: 0.793775 test loss: 1.598021
[Epoch 80; Iter     1/   41] train: loss: 0.1540671
[Epoch 80; Iter    31/   41] train: loss: 0.2811192
[Epoch 80] ogbg-molbace: 0.759341 val loss: 1.280759
[Epoch 80] ogbg-molbace: 0.807338 test loss: 1.644545
[Epoch 81; Iter    20/   41] train: loss: 0.0652270
[Epoch 81] ogbg-molbace: 0.734799 val loss: 1.561049
[Epoch 81] ogbg-molbace: 0.820553 test loss: 1.632231
[Epoch 82; Iter     9/   41] train: loss: 0.0588484
[Epoch 82; Iter    39/   41] train: loss: 0.1103839
[Epoch 82] ogbg-molbace: 0.727106 val loss: 1.581286
[Epoch 82] ogbg-molbace: 0.820553 test loss: 1.680199
[Epoch 83; Iter    28/   41] train: loss: 0.0770795
[Epoch 83] ogbg-molbace: 0.697070 val loss: 1.490943
[Epoch 83] ogbg-molbace: 0.814989 test loss: 1.531446
[Epoch 84; Iter    17/   41] train: loss: 0.0745055
[Epoch 84] ogbg-molbace: 0.721245 val loss: 1.655937
[Epoch 84] ogbg-molbace: 0.811859 test loss: 1.846313
[Epoch 85; Iter     6/   41] train: loss: 0.1161236
[Epoch 85; Iter    36/   41] train: loss: 0.2743900
[Epoch 85] ogbg-molbace: 0.680952 val loss: 1.683662
[Epoch 85] ogbg-molbace: 0.796383 test loss: 1.575288
[Epoch 86; Iter    25/   41] train: loss: 0.2174560
[Epoch 86] ogbg-molbace: 0.695971 val loss: 1.807097
[Epoch 86] ogbg-molbace: 0.818466 test loss: 2.052148
[Epoch 87; Iter    14/   41] train: loss: 0.0844972
[Epoch 87] ogbg-molbace: 0.731136 val loss: 1.709370
[Epoch 87] ogbg-molbace: 0.820727 test loss: 1.814471
[Epoch 88; Iter     3/   41] train: loss: 0.2339833
[Epoch 88; Iter    33/   41] train: loss: 0.2181617
[Epoch 88] ogbg-molbace: 0.738828 val loss: 1.408837
[Epoch 88] ogbg-molbace: 0.816901 test loss: 1.405147
[Epoch 89; Iter    22/   41] train: loss: 0.0198563
[Epoch 89] ogbg-molbace: 0.741758 val loss: 1.406590
[Epoch 89] ogbg-molbace: 0.813424 test loss: 1.311729
[Epoch 90; Iter    11/   41] train: loss: 0.0939092
[Epoch 90; Iter    41/   41] train: loss: 0.3890872
[Epoch 90] ogbg-molbace: 0.709524 val loss: 1.437650
[Epoch 90] ogbg-molbace: 0.816728 test loss: 2.091545
[Epoch 91; Iter    30/   41] train: loss: 0.0304860
[Epoch 91] ogbg-molbace: 0.720513 val loss: 1.542247
[Epoch 91] ogbg-molbace: 0.813945 test loss: 1.994434
[Epoch 92; Iter    19/   41] train: loss: 0.2217895
[Epoch 92] ogbg-molbace: 0.705495 val loss: 1.692182
[Epoch 92] ogbg-molbace: 0.825943 test loss: 1.797953
[Epoch 93; Iter     8/   41] train: loss: 0.2161878
[Epoch 93; Iter    38/   41] train: loss: 0.0930150
[Epoch 93] ogbg-molbace: 0.733700 val loss: 1.605883
[Epoch 93] ogbg-molbace: 0.816728 test loss: 2.484844
[Epoch 94; Iter    27/   41] train: loss: 0.0580491
[Epoch 94] ogbg-molbace: 0.670696 val loss: 2.131609
[Epoch 94] ogbg-molbace: 0.785081 test loss: 1.997033
[Epoch 95; Iter    16/   41] train: loss: 0.0918394
[Epoch 95] ogbg-molbace: 0.722344 val loss: 1.542280
[Epoch 95] ogbg-molbace: 0.822640 test loss: 1.258349
[Epoch 96; Iter     5/   41] train: loss: 0.0221102
[Epoch 96; Iter    35/   41] train: loss: 0.0357808
[Epoch 96] ogbg-molbace: 0.706227 val loss: 1.771715
[Epoch 96] ogbg-molbace: 0.810816 test loss: 1.984369
[Epoch 97; Iter    24/   41] train: loss: 0.0706429
[Epoch 97] ogbg-molbace: 0.682784 val loss: 1.722819
[Epoch 97] ogbg-molbace: 0.801252 test loss: 1.871623
[Epoch 98; Iter    13/   41] train: loss: 0.0674636
[Epoch 98] ogbg-molbace: 0.706227 val loss: 1.599998
[Epoch 98] ogbg-molbace: 0.814641 test loss: 1.800051
[Epoch 99; Iter     2/   41] train: loss: 0.1258664
[Epoch 99; Iter    32/   41] train: loss: 0.0771862
[Epoch 99] ogbg-molbace: 0.743223 val loss: 1.569527
[Epoch 99] ogbg-molbace: 0.811511 test loss: 1.763838
[Epoch 100; Iter    21/   41] train: loss: 0.0608724
[Epoch 100] ogbg-molbace: 0.727106 val loss: 1.336245
[Epoch 100] ogbg-molbace: 0.842984 test loss: 1.726025
[Epoch 101; Iter    10/   41] train: loss: 0.0776836
[Epoch 101; Iter    40/   41] train: loss: 0.0305430
[Epoch 101] ogbg-molbace: 0.711355 val loss: 1.790416
[Epoch 101] ogbg-molbace: 0.831855 test loss: 1.780603
[Epoch 102; Iter    29/   41] train: loss: 0.0392129
[Epoch 102] ogbg-molbace: 0.710623 val loss: 1.644066
[Epoch 102] ogbg-molbace: 0.822987 test loss: 2.101578
[Epoch 103; Iter    18/   41] train: loss: 0.0292737
[Epoch 103] ogbg-molbace: 0.726374 val loss: 1.846844
[Epoch 103] ogbg-molbace: 0.823509 test loss: 1.750810
[Epoch 104; Iter     7/   41] train: loss: 0.1310227
[Epoch 104; Iter    37/   41] train: loss: 0.0222941
[Epoch 104] ogbg-molbace: 0.694139 val loss: 1.869806
[Epoch 104] ogbg-molbace: 0.808903 test loss: 2.166918
[Epoch 105; Iter    26/   41] train: loss: 0.0057337
[Epoch 105] ogbg-molbace: 0.714652 val loss: 2.000022
[Epoch 105] ogbg-molbace: 0.816380 test loss: 1.891572
[Epoch 106; Iter    15/   41] train: loss: 0.0165718
[Epoch 106] ogbg-molbace: 0.705861 val loss: 2.139969
[Epoch 106] ogbg-molbace: 0.798818 test loss: 1.881956
[Epoch 107; Iter     4/   41] train: loss: 0.0100031
[Epoch 107; Iter    34/   41] train: loss: 0.0624056
[Epoch 107] ogbg-molbace: 0.712088 val loss: 2.107236
[Epoch 107] ogbg-molbace: 0.812033 test loss: 2.204718
[Epoch 108; Iter    23/   41] train: loss: 0.0760492
[Epoch 108] ogbg-molbace: 0.701099 val loss: 2.002397
[Epoch 108] ogbg-molbace: 0.819857 test loss: 2.129819
[Epoch 109; Iter    12/   41] train: loss: 0.0319141
[Epoch 109] ogbg-molbace: 0.694139 val loss: 2.241325
[Epoch 109] ogbg-molbace: 0.802991 test loss: 2.000243
[Epoch 110; Iter     1/   41] train: loss: 0.0401714
[Epoch 110; Iter    31/   41] train: loss: 0.0232142
[Epoch 110] ogbg-molbace: 0.721245 val loss: 2.205867
[Epoch 110] ogbg-molbace: 0.809424 test loss: 2.127808
[Epoch 111; Iter    20/   41] train: loss: 0.0509653
[Epoch 111] ogbg-molbace: 0.727106 val loss: 2.038534
[Epoch 111] ogbg-molbace: 0.815684 test loss: 2.171841
[Epoch 112; Iter     9/   41] train: loss: 0.0180652
[Epoch 112; Iter    39/   41] train: loss: 0.0088104
[Epoch 112] ogbg-molbace: 0.717582 val loss: 1.999549
[Epoch 112] ogbg-molbace: 0.806990 test loss: 2.136051
[Epoch 113; Iter    28/   41] train: loss: 0.0102229
[Epoch 113] ogbg-molbace: 0.694872 val loss: 2.194416
[Epoch 113] ogbg-molbace: 0.813424 test loss: 2.567345
[Epoch 114; Iter    17/   41] train: loss: 0.0105631
[Epoch 114] ogbg-molbace: 0.716117 val loss: 2.333522
[Epoch 114] ogbg-molbace: 0.809424 test loss: 2.170496
[Epoch 115; Iter     6/   41] train: loss: 0.0552171
[Epoch 115; Iter    36/   41] train: loss: 0.0049633
[Epoch 115] ogbg-molbace: 0.697802 val loss: 2.265074
[Epoch 115] ogbg-molbace: 0.812554 test loss: 2.254161
[Epoch 116; Iter    25/   41] train: loss: 0.0028806
[Epoch 116] ogbg-molbace: 0.712088 val loss: 2.209955
[Epoch 116] ogbg-molbace: 0.811337 test loss: 2.175353
[Epoch 117; Iter    14/   41] train: loss: 0.0419776
[Epoch 117] ogbg-molbace: 0.727473 val loss: 2.226444
[Epoch 117] ogbg-molbace: 0.806468 test loss: 2.610638
[Epoch 118; Iter     3/   41] train: loss: 0.0591916
[Epoch 118; Iter    33/   41] train: loss: 0.0098062
[Epoch 118] ogbg-molbace: 0.712454 val loss: 1.994942
[Epoch 118] ogbg-molbace: 0.808381 test loss: 2.118553
[Epoch 119; Iter    22/   41] train: loss: 0.0151686
[Epoch 119] ogbg-molbace: 0.683516 val loss: 2.045926
[Epoch 119] ogbg-molbace: 0.824378 test loss: 2.523524
[Epoch 120; Iter    11/   41] train: loss: 0.0220207
[Epoch 120; Iter    41/   41] train: loss: 0.0374794
[Epoch 120] ogbg-molbace: 0.716117 val loss: 2.303727
[Epoch 120] ogbg-molbace: 0.800556 test loss: 2.074766
[Epoch 121; Iter    30/   41] train: loss: 0.0635186
[Epoch 121] ogbg-molbace: 0.694139 val loss: 2.108404
[Epoch 121] ogbg-molbace: 0.813076 test loss: 2.487613
[Epoch 122; Iter    19/   41] train: loss: 0.0424319
[Epoch 122] ogbg-molbace: 0.676190 val loss: 2.361171
[Epoch 122] ogbg-molbace: 0.799165 test loss: 2.590871
[Epoch 123; Iter     8/   41] train: loss: 0.2023407
[Epoch 123; Iter    38/   41] train: loss: 0.0430502
[Epoch 123] ogbg-molbace: 0.716484 val loss: 2.028875
[Epoch 78] ogbg-molbace: 0.731502 val loss: 1.429552
[Epoch 78] ogbg-molbace: 0.786472 test loss: 1.658244
[Epoch 79; Iter    12/   41] train: loss: 0.1830130
[Epoch 79] ogbg-molbace: 0.694139 val loss: 1.475532
[Epoch 79] ogbg-molbace: 0.774648 test loss: 1.769901
[Epoch 80; Iter     1/   41] train: loss: 0.0389852
[Epoch 80; Iter    31/   41] train: loss: 0.2244993
[Epoch 80] ogbg-molbace: 0.721612 val loss: 1.398065
[Epoch 80] ogbg-molbace: 0.782125 test loss: 1.898337
[Epoch 81; Iter    20/   41] train: loss: 0.0971431
[Epoch 81] ogbg-molbace: 0.742857 val loss: 1.167267
[Epoch 81] ogbg-molbace: 0.772040 test loss: 1.935171
[Epoch 82; Iter     9/   41] train: loss: 0.0671507
[Epoch 82; Iter    39/   41] train: loss: 0.2535642
[Epoch 82] ogbg-molbace: 0.689377 val loss: 1.524123
[Epoch 82] ogbg-molbace: 0.757955 test loss: 2.054006
[Epoch 83; Iter    28/   41] train: loss: 0.0585011
[Epoch 83] ogbg-molbace: 0.748352 val loss: 1.241461
[Epoch 83] ogbg-molbace: 0.776213 test loss: 1.739823
[Epoch 84; Iter    17/   41] train: loss: 0.1643087
[Epoch 84] ogbg-molbace: 0.739194 val loss: 1.000220
[Epoch 84] ogbg-molbace: 0.754651 test loss: 1.937763
[Epoch 85; Iter     6/   41] train: loss: 0.0698991
[Epoch 85; Iter    36/   41] train: loss: 0.0391953
[Epoch 85] ogbg-molbace: 0.746154 val loss: 1.096128
[Epoch 85] ogbg-molbace: 0.788732 test loss: 1.826246
[Epoch 86; Iter    25/   41] train: loss: 0.2677195
[Epoch 86] ogbg-molbace: 0.770696 val loss: 1.672435
[Epoch 86] ogbg-molbace: 0.783516 test loss: 1.826570
[Epoch 87; Iter    14/   41] train: loss: 0.2195479
[Epoch 87] ogbg-molbace: 0.742857 val loss: 1.582279
[Epoch 87] ogbg-molbace: 0.785603 test loss: 1.711861
[Epoch 88; Iter     3/   41] train: loss: 0.1154391
[Epoch 88; Iter    33/   41] train: loss: 0.1106900
[Epoch 88] ogbg-molbace: 0.753480 val loss: 1.328844
[Epoch 88] ogbg-molbace: 0.749609 test loss: 2.178531
[Epoch 89; Iter    22/   41] train: loss: 0.2095989
[Epoch 89] ogbg-molbace: 0.734799 val loss: 1.658977
[Epoch 89] ogbg-molbace: 0.737959 test loss: 2.226197
[Epoch 90; Iter    11/   41] train: loss: 0.1494424
[Epoch 90; Iter    41/   41] train: loss: 0.4128526
[Epoch 90] ogbg-molbace: 0.682418 val loss: 1.907395
[Epoch 90] ogbg-molbace: 0.734655 test loss: 1.527421
[Epoch 91; Iter    30/   41] train: loss: 0.0544381
[Epoch 91] ogbg-molbace: 0.719780 val loss: 1.767383
[Epoch 91] ogbg-molbace: 0.740393 test loss: 1.952071
[Epoch 92; Iter    19/   41] train: loss: 0.0300667
[Epoch 92] ogbg-molbace: 0.732967 val loss: 1.979286
[Epoch 92] ogbg-molbace: 0.758998 test loss: 2.232539
[Epoch 93; Iter     8/   41] train: loss: 0.0898798
[Epoch 93; Iter    38/   41] train: loss: 0.1149811
[Epoch 93] ogbg-molbace: 0.747619 val loss: 1.704569
[Epoch 93] ogbg-molbace: 0.772735 test loss: 2.475060
[Epoch 94; Iter    27/   41] train: loss: 0.0312988
[Epoch 94] ogbg-molbace: 0.719780 val loss: 1.149458
[Epoch 94] ogbg-molbace: 0.763346 test loss: 2.168551
[Epoch 95; Iter    16/   41] train: loss: 0.1934593
[Epoch 95] ogbg-molbace: 0.746520 val loss: 1.630281
[Epoch 95] ogbg-molbace: 0.772214 test loss: 2.487044
[Epoch 96; Iter     5/   41] train: loss: 0.0737395
[Epoch 96; Iter    35/   41] train: loss: 0.0505819
[Epoch 96] ogbg-molbace: 0.736264 val loss: 1.811260
[Epoch 96] ogbg-molbace: 0.765084 test loss: 2.096929
[Epoch 97; Iter    24/   41] train: loss: 0.1396489
[Epoch 97] ogbg-molbace: 0.746154 val loss: 1.842717
[Epoch 97] ogbg-molbace: 0.750304 test loss: 2.176351
[Epoch 98; Iter    13/   41] train: loss: 0.0268919
[Epoch 98] ogbg-molbace: 0.756410 val loss: 1.588356
[Epoch 98] ogbg-molbace: 0.773431 test loss: 2.243954
[Epoch 99; Iter     2/   41] train: loss: 0.0068413
[Epoch 99; Iter    32/   41] train: loss: 0.0649880
[Epoch 99] ogbg-molbace: 0.754579 val loss: 1.645121
[Epoch 99] ogbg-molbace: 0.775170 test loss: 2.094282
[Epoch 100; Iter    21/   41] train: loss: 0.0462148
[Epoch 100] ogbg-molbace: 0.714286 val loss: 1.905123
[Epoch 100] ogbg-molbace: 0.749609 test loss: 2.168293
[Epoch 101; Iter    10/   41] train: loss: 0.0412367
[Epoch 101; Iter    40/   41] train: loss: 0.0394901
[Epoch 101] ogbg-molbace: 0.718681 val loss: 1.921205
[Epoch 101] ogbg-molbace: 0.744914 test loss: 2.171257
[Epoch 102; Iter    29/   41] train: loss: 0.0513636
[Epoch 102] ogbg-molbace: 0.727106 val loss: 2.005685
[Epoch 102] ogbg-molbace: 0.782473 test loss: 2.657849
[Epoch 103; Iter    18/   41] train: loss: 0.0487760
[Epoch 103] ogbg-molbace: 0.722344 val loss: 1.777058
[Epoch 103] ogbg-molbace: 0.765954 test loss: 2.542787
[Epoch 104; Iter     7/   41] train: loss: 0.0491805
[Epoch 104; Iter    37/   41] train: loss: 0.0209190
[Epoch 104] ogbg-molbace: 0.729304 val loss: 1.916930
[Epoch 104] ogbg-molbace: 0.757433 test loss: 2.567298
[Epoch 105; Iter    26/   41] train: loss: 0.0722149
[Epoch 105] ogbg-molbace: 0.717582 val loss: 2.166067
[Epoch 105] ogbg-molbace: 0.758477 test loss: 2.767624
[Epoch 106; Iter    15/   41] train: loss: 0.0059694
[Epoch 106] ogbg-molbace: 0.716484 val loss: 2.238788
[Epoch 106] ogbg-molbace: 0.748565 test loss: 2.929849
[Epoch 107; Iter     4/   41] train: loss: 0.0264460
[Epoch 107; Iter    34/   41] train: loss: 0.1374699
[Epoch 107] ogbg-molbace: 0.713187 val loss: 2.126680
[Epoch 107] ogbg-molbace: 0.767519 test loss: 2.152300
[Epoch 108; Iter    23/   41] train: loss: 0.1777692
[Epoch 108] ogbg-molbace: 0.751282 val loss: 1.891908
[Epoch 108] ogbg-molbace: 0.772735 test loss: 2.694352
[Epoch 109; Iter    12/   41] train: loss: 0.0194430
[Epoch 109] ogbg-molbace: 0.713187 val loss: 1.731290
[Epoch 109] ogbg-molbace: 0.754130 test loss: 2.520911
[Epoch 110; Iter     1/   41] train: loss: 0.0336000
[Epoch 110; Iter    31/   41] train: loss: 0.0169530
[Epoch 110] ogbg-molbace: 0.742857 val loss: 1.618627
[Epoch 110] ogbg-molbace: 0.762302 test loss: 2.474825
[Epoch 111; Iter    20/   41] train: loss: 0.0252511
[Epoch 111] ogbg-molbace: 0.732601 val loss: 1.530716
[Epoch 111] ogbg-molbace: 0.763693 test loss: 2.355056
[Epoch 112; Iter     9/   41] train: loss: 0.0775013
[Epoch 112; Iter    39/   41] train: loss: 0.0228487
[Epoch 112] ogbg-molbace: 0.749817 val loss: 1.625902
[Epoch 112] ogbg-molbace: 0.738132 test loss: 1.932675
[Epoch 113; Iter    28/   41] train: loss: 0.0191566
[Epoch 113] ogbg-molbace: 0.751648 val loss: 1.581692
[Epoch 113] ogbg-molbace: 0.762476 test loss: 2.514799
[Epoch 114; Iter    17/   41] train: loss: 0.0617027
[Epoch 114] ogbg-molbace: 0.772894 val loss: 1.648990
[Epoch 114] ogbg-molbace: 0.758303 test loss: 2.443950
[Epoch 115; Iter     6/   41] train: loss: 0.0126434
[Epoch 115; Iter    36/   41] train: loss: 0.0375049
[Epoch 115] ogbg-molbace: 0.761905 val loss: 1.340664
[Epoch 115] ogbg-molbace: 0.766302 test loss: 2.371385
[Epoch 116; Iter    25/   41] train: loss: 0.0287468
[Epoch 116] ogbg-molbace: 0.768132 val loss: 1.704495
[Epoch 116] ogbg-molbace: 0.756216 test loss: 2.707655
[Epoch 117; Iter    14/   41] train: loss: 0.0060621
[Epoch 117] ogbg-molbace: 0.765201 val loss: 1.520038
[Epoch 117] ogbg-molbace: 0.748913 test loss: 2.504149
[Epoch 118; Iter     3/   41] train: loss: 0.0030800
[Epoch 118; Iter    33/   41] train: loss: 0.0111946
[Epoch 118] ogbg-molbace: 0.771795 val loss: 1.474600
[Epoch 118] ogbg-molbace: 0.740741 test loss: 2.387336
[Epoch 119; Iter    22/   41] train: loss: 0.0174592
[Epoch 119] ogbg-molbace: 0.745055 val loss: 1.606840
[Epoch 119] ogbg-molbace: 0.754651 test loss: 2.705658
[Epoch 120; Iter    11/   41] train: loss: 0.0040428
[Epoch 120; Iter    41/   41] train: loss: 0.0053590
[Epoch 120] ogbg-molbace: 0.754945 val loss: 1.504728
[Epoch 120] ogbg-molbace: 0.758651 test loss: 2.468145
[Epoch 121; Iter    30/   41] train: loss: 0.0053734
[Epoch 121] ogbg-molbace: 0.753114 val loss: 1.720763
[Epoch 121] ogbg-molbace: 0.749957 test loss: 2.421961
[Epoch 122; Iter    19/   41] train: loss: 0.0045560
[Epoch 122] ogbg-molbace: 0.742857 val loss: 1.691254
[Epoch 122] ogbg-molbace: 0.747348 test loss: 2.835908
[Epoch 123; Iter     8/   41] train: loss: 0.0025031
[Epoch 123; Iter    38/   41] train: loss: 0.0431164
[Epoch 123] ogbg-molbace: 0.735531 val loss: 1.788411
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 169; Iter    12/   41] train: loss: 0.0002652
[Epoch 169] ogbg-molbace: 0.719048 val loss: 1.840277
[Epoch 169] ogbg-molbace: 0.797427 test loss: 2.922780
[Epoch 170; Iter     1/   41] train: loss: 0.0058178
[Epoch 170; Iter    31/   41] train: loss: 0.0003556
[Epoch 170] ogbg-molbace: 0.724176 val loss: 1.848186
[Epoch 170] ogbg-molbace: 0.801426 test loss: 2.699145
[Epoch 171; Iter    20/   41] train: loss: 0.0006178
[Epoch 171] ogbg-molbace: 0.725641 val loss: 1.907461
[Epoch 171] ogbg-molbace: 0.804208 test loss: 2.573457
[Epoch 172; Iter     9/   41] train: loss: 0.0011065
[Epoch 172; Iter    39/   41] train: loss: 0.0001295
[Epoch 172] ogbg-molbace: 0.717949 val loss: 1.869174
[Epoch 172] ogbg-molbace: 0.795514 test loss: 2.778125
[Epoch 173; Iter    28/   41] train: loss: 0.0002524
[Epoch 173] ogbg-molbace: 0.717216 val loss: 1.909975
[Epoch 173] ogbg-molbace: 0.793949 test loss: 2.748118
[Epoch 174; Iter    17/   41] train: loss: 0.0002030
[Epoch 174] ogbg-molbace: 0.715385 val loss: 1.924530
[Epoch 174] ogbg-molbace: 0.792558 test loss: 2.789197
[Epoch 175; Iter     6/   41] train: loss: 0.0005887
[Epoch 175; Iter    36/   41] train: loss: 0.0004170
[Epoch 175] ogbg-molbace: 0.715385 val loss: 1.928289
[Epoch 175] ogbg-molbace: 0.790819 test loss: 2.674221
[Epoch 176; Iter    25/   41] train: loss: 0.0002298
[Epoch 176] ogbg-molbace: 0.715751 val loss: 1.987023
[Epoch 176] ogbg-molbace: 0.795514 test loss: 2.697199
[Epoch 177; Iter    14/   41] train: loss: 0.0001046
[Epoch 177] ogbg-molbace: 0.716117 val loss: 1.976772
[Epoch 177] ogbg-molbace: 0.793253 test loss: 2.740708
[Epoch 178; Iter     3/   41] train: loss: 0.0002160
[Epoch 178; Iter    33/   41] train: loss: 0.0007155
[Epoch 178] ogbg-molbace: 0.716484 val loss: 1.949339
[Epoch 178] ogbg-molbace: 0.792906 test loss: 2.681476
[Epoch 179; Iter    22/   41] train: loss: 0.0002080
[Epoch 179] ogbg-molbace: 0.716117 val loss: 1.954355
[Epoch 179] ogbg-molbace: 0.793427 test loss: 2.696034
[Epoch 180; Iter    11/   41] train: loss: 0.0001373
[Epoch 180; Iter    41/   41] train: loss: 0.0000632
[Epoch 180] ogbg-molbace: 0.714652 val loss: 1.958713
[Epoch 180] ogbg-molbace: 0.787863 test loss: 2.741850
[Epoch 181; Iter    30/   41] train: loss: 0.0005710
[Epoch 181] ogbg-molbace: 0.715385 val loss: 1.966152
[Epoch 181] ogbg-molbace: 0.792732 test loss: 2.748424
[Epoch 182; Iter    19/   41] train: loss: 0.0001315
[Epoch 182] ogbg-molbace: 0.716850 val loss: 1.925744
[Epoch 182] ogbg-molbace: 0.790993 test loss: 2.709394
[Epoch 183; Iter     8/   41] train: loss: 0.0002041
[Epoch 183; Iter    38/   41] train: loss: 0.0001032
[Epoch 183] ogbg-molbace: 0.716850 val loss: 1.976928
[Epoch 183] ogbg-molbace: 0.792036 test loss: 2.796695
[Epoch 184; Iter    27/   41] train: loss: 0.0005374
[Epoch 184] ogbg-molbace: 0.713553 val loss: 1.962146
[Epoch 184] ogbg-molbace: 0.795166 test loss: 2.727971
[Epoch 185; Iter    16/   41] train: loss: 0.0001303
[Epoch 185] ogbg-molbace: 0.715385 val loss: 1.918897
[Epoch 185] ogbg-molbace: 0.792210 test loss: 2.776960
[Epoch 186; Iter     5/   41] train: loss: 0.0001162
[Epoch 186; Iter    35/   41] train: loss: 0.0002202
[Epoch 186] ogbg-molbace: 0.700366 val loss: 1.940498
[Epoch 186] ogbg-molbace: 0.800556 test loss: 2.405491
[Epoch 187; Iter    24/   41] train: loss: 0.0010084
[Epoch 187] ogbg-molbace: 0.714652 val loss: 1.889614
[Epoch 187] ogbg-molbace: 0.801252 test loss: 2.500725
[Epoch 188; Iter    13/   41] train: loss: 0.0018652
[Epoch 188] ogbg-molbace: 0.711722 val loss: 1.908857
[Epoch 188] ogbg-molbace: 0.801947 test loss: 2.559580
[Epoch 189; Iter     2/   41] train: loss: 0.0001545
[Epoch 189; Iter    32/   41] train: loss: 0.0003710
[Epoch 189] ogbg-molbace: 0.714286 val loss: 1.962523
[Epoch 189] ogbg-molbace: 0.801078 test loss: 2.582070
[Epoch 190; Iter    21/   41] train: loss: 0.0000818
[Epoch 190] ogbg-molbace: 0.712821 val loss: 1.941406
[Epoch 190] ogbg-molbace: 0.801078 test loss: 2.589208
[Epoch 191; Iter    10/   41] train: loss: 0.0001376
[Epoch 191; Iter    40/   41] train: loss: 0.0000999
[Epoch 191] ogbg-molbace: 0.712821 val loss: 1.939464
[Epoch 191] ogbg-molbace: 0.800904 test loss: 2.622188
[Epoch 192; Iter    29/   41] train: loss: 0.0001700
[Epoch 192] ogbg-molbace: 0.713187 val loss: 1.898369
[Epoch 192] ogbg-molbace: 0.800730 test loss: 2.553941
[Epoch 193; Iter    18/   41] train: loss: 0.0007876
[Epoch 193] ogbg-molbace: 0.712088 val loss: 1.927821
[Epoch 193] ogbg-molbace: 0.801426 test loss: 2.554522
[Epoch 194; Iter     7/   41] train: loss: 0.0001207
[Epoch 194; Iter    37/   41] train: loss: 0.0006879
[Epoch 194] ogbg-molbace: 0.712821 val loss: 1.922584
[Epoch 194] ogbg-molbace: 0.794818 test loss: 2.711448
[Epoch 195; Iter    26/   41] train: loss: 0.0001569
[Epoch 195] ogbg-molbace: 0.707692 val loss: 1.949730
[Epoch 195] ogbg-molbace: 0.795688 test loss: 2.632307
[Epoch 196; Iter    15/   41] train: loss: 0.0000996
[Epoch 196] ogbg-molbace: 0.714286 val loss: 1.901674
[Epoch 196] ogbg-molbace: 0.794471 test loss: 2.670692
[Epoch 197; Iter     4/   41] train: loss: 0.0005306
[Epoch 197; Iter    34/   41] train: loss: 0.0002491
[Epoch 197] ogbg-molbace: 0.709524 val loss: 1.916744
[Epoch 197] ogbg-molbace: 0.789602 test loss: 2.929036
[Epoch 198; Iter    23/   41] train: loss: 0.0002013
[Epoch 198] ogbg-molbace: 0.711355 val loss: 1.918668
[Epoch 198] ogbg-molbace: 0.791688 test loss: 2.730416
[Epoch 199; Iter    12/   41] train: loss: 0.0008370
[Epoch 199] ogbg-molbace: 0.713553 val loss: 1.952597
[Epoch 199] ogbg-molbace: 0.795166 test loss: 2.694451
[Epoch 200; Iter     1/   41] train: loss: 0.0001718
[Epoch 200; Iter    31/   41] train: loss: 0.0004063
[Epoch 200] ogbg-molbace: 0.711355 val loss: 1.951314
[Epoch 200] ogbg-molbace: 0.793601 test loss: 2.825578
[Epoch 201; Iter    20/   41] train: loss: 0.0004054
[Epoch 201] ogbg-molbace: 0.710256 val loss: 1.916233
[Epoch 201] ogbg-molbace: 0.794818 test loss: 2.716100
[Epoch 202; Iter     9/   41] train: loss: 0.0004492
[Epoch 202; Iter    39/   41] train: loss: 0.0004000
[Epoch 202] ogbg-molbace: 0.711722 val loss: 1.944482
[Epoch 202] ogbg-molbace: 0.793601 test loss: 2.730274
[Epoch 203; Iter    28/   41] train: loss: 0.0001449
[Epoch 203] ogbg-molbace: 0.715751 val loss: 1.923525
[Epoch 203] ogbg-molbace: 0.792384 test loss: 2.930859
[Epoch 204; Iter    17/   41] train: loss: 0.0000724
[Epoch 204] ogbg-molbace: 0.717216 val loss: 1.857922
[Epoch 204] ogbg-molbace: 0.793775 test loss: 2.675050
[Epoch 205; Iter     6/   41] train: loss: 0.0003168
[Epoch 205; Iter    36/   41] train: loss: 0.0001802
[Epoch 205] ogbg-molbace: 0.714652 val loss: 1.964682
[Epoch 205] ogbg-molbace: 0.794471 test loss: 2.962315
[Epoch 206; Iter    25/   41] train: loss: 0.0000731
[Epoch 206] ogbg-molbace: 0.715751 val loss: 1.952773
[Epoch 206] ogbg-molbace: 0.796905 test loss: 2.703585
[Epoch 207; Iter    14/   41] train: loss: 0.0003018
[Epoch 207] ogbg-molbace: 0.713187 val loss: 1.904332
[Epoch 207] ogbg-molbace: 0.791515 test loss: 2.832339
[Epoch 208; Iter     3/   41] train: loss: 0.0002135
[Epoch 208; Iter    33/   41] train: loss: 0.0001253
[Epoch 208] ogbg-molbace: 0.715018 val loss: 1.954802
[Epoch 208] ogbg-molbace: 0.792384 test loss: 2.931615
Early stopping criterion based on -ogbg-molbace- that should be max reached after 208 epochs. Best model checkpoint was in epoch 148.
Statistics on  val_best_checkpoint
mean_pred: 2.7618601322174072
std_pred: 6.467889308929443
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9476304130786042
rocauc: 0.735897435897436
ogbg-molbace: 0.735897435897436
BCEWithLogitsLoss: 1.4454739292462666
Statistics on  test
mean_pred: -4.185256004333496
std_pred: 7.120090007781982
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7956546970382001
rocauc: 0.7612589114936532
ogbg-molbace: 0.7612589114936532
BCEWithLogitsLoss: 2.6451003259668746
Statistics on  train
mean_pred: -2.435986280441284
std_pred: 10.0225248336792
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 1.0
rocauc: 1.0
ogbg-molbace: 1.0
BCEWithLogitsLoss: 0.0003326920011271041
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.1.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.1.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.1.yml --seed 6 --device cuda:2
All runs completed.
[Epoch 123] ogbg-molbace: 0.797253 test loss: 2.631139
[Epoch 124; Iter    27/   41] train: loss: 0.0284454
[Epoch 124] ogbg-molbace: 0.694139 val loss: 2.323521
[Epoch 124] ogbg-molbace: 0.805077 test loss: 2.800040
[Epoch 125; Iter    16/   41] train: loss: 0.0059876
[Epoch 125] ogbg-molbace: 0.725641 val loss: 1.847531
[Epoch 125] ogbg-molbace: 0.803165 test loss: 2.477764
[Epoch 126; Iter     5/   41] train: loss: 0.0055484
[Epoch 126; Iter    35/   41] train: loss: 0.0051914
[Epoch 126] ogbg-molbace: 0.732601 val loss: 2.069236
[Epoch 126] ogbg-molbace: 0.800383 test loss: 2.410184
Early stopping criterion based on -ogbg-molbace- that should be max reached after 126 epochs. Best model checkpoint was in epoch 66.
Statistics on  val_best_checkpoint
mean_pred: -0.1149471327662468
std_pred: 3.104233741760254
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9624874525960736
rocauc: 0.7952380952380953
ogbg-molbace: 0.7952380952380953
BCEWithLogitsLoss: 1.2053057948748271
Statistics on  test
mean_pred: -2.8017802238464355
std_pred: 3.3758912086486816
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7890214819646812
rocauc: 0.7659537471744045
ogbg-molbace: 0.7659537471744045
BCEWithLogitsLoss: 1.790112003373603
Statistics on  train
mean_pred: -2.2709875106811523
std_pred: 4.198091983795166
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9700355158375189
rocauc: 0.9807334474885845
ogbg-molbace: 0.9807334474885845
BCEWithLogitsLoss: 0.19718480737107555
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.767693 test loss: 2.096427
[Epoch 124; Iter    27/   41] train: loss: 0.0382754
[Epoch 124] ogbg-molbace: 0.752747 val loss: 2.181535
[Epoch 124] ogbg-molbace: 0.768910 test loss: 2.413056
[Epoch 125; Iter    16/   41] train: loss: 0.0058751
[Epoch 125] ogbg-molbace: 0.754579 val loss: 2.370986
[Epoch 125] ogbg-molbace: 0.763172 test loss: 2.403040
[Epoch 126; Iter     5/   41] train: loss: 0.0061130
[Epoch 126; Iter    35/   41] train: loss: 0.0045950
[Epoch 126] ogbg-molbace: 0.746520 val loss: 2.388715
[Epoch 126] ogbg-molbace: 0.766475 test loss: 2.389144
[Epoch 127; Iter    24/   41] train: loss: 0.0089404
[Epoch 127] ogbg-molbace: 0.754212 val loss: 2.546460
[Epoch 127] ogbg-molbace: 0.769605 test loss: 2.293691
[Epoch 128; Iter    13/   41] train: loss: 0.0196998
[Epoch 128] ogbg-molbace: 0.753846 val loss: 2.443475
[Epoch 128] ogbg-molbace: 0.777778 test loss: 2.349332
[Epoch 129; Iter     2/   41] train: loss: 0.0375029
[Epoch 129; Iter    32/   41] train: loss: 0.0020615
[Epoch 129] ogbg-molbace: 0.745788 val loss: 2.510041
[Epoch 129] ogbg-molbace: 0.771866 test loss: 2.470140
[Epoch 130; Iter    21/   41] train: loss: 0.0119905
[Epoch 130] ogbg-molbace: 0.737729 val loss: 2.350452
[Epoch 130] ogbg-molbace: 0.749435 test loss: 2.512126
[Epoch 131; Iter    10/   41] train: loss: 0.0360148
[Epoch 131; Iter    40/   41] train: loss: 0.0028627
[Epoch 131] ogbg-molbace: 0.736996 val loss: 2.450870
[Epoch 131] ogbg-molbace: 0.764563 test loss: 2.358920
[Epoch 132; Iter    29/   41] train: loss: 0.0048108
[Epoch 132] ogbg-molbace: 0.757509 val loss: 2.184934
[Epoch 132] ogbg-molbace: 0.759520 test loss: 2.286186
[Epoch 133; Iter    18/   41] train: loss: 0.0062774
[Epoch 133] ogbg-molbace: 0.757875 val loss: 2.294137
[Epoch 133] ogbg-molbace: 0.764389 test loss: 2.344980
[Epoch 134; Iter     7/   41] train: loss: 0.0065112
[Epoch 134; Iter    37/   41] train: loss: 0.0022065
[Epoch 134] ogbg-molbace: 0.753480 val loss: 2.450708
[Epoch 134] ogbg-molbace: 0.763172 test loss: 2.501675
[Epoch 135; Iter    26/   41] train: loss: 0.0089511
[Epoch 135] ogbg-molbace: 0.741758 val loss: 2.380137
[Epoch 135] ogbg-molbace: 0.765084 test loss: 2.306212
[Epoch 136; Iter    15/   41] train: loss: 0.0064184
[Epoch 136] ogbg-molbace: 0.741392 val loss: 2.290562
[Epoch 136] ogbg-molbace: 0.786646 test loss: 2.229520
[Epoch 137; Iter     4/   41] train: loss: 0.0113573
[Epoch 137; Iter    34/   41] train: loss: 0.0621149
[Epoch 137] ogbg-molbace: 0.750183 val loss: 2.374631
[Epoch 137] ogbg-molbace: 0.757433 test loss: 2.442887
[Epoch 138; Iter    23/   41] train: loss: 0.0090506
[Epoch 138] ogbg-molbace: 0.743590 val loss: 2.202404
[Epoch 138] ogbg-molbace: 0.748044 test loss: 2.538624
[Epoch 139; Iter    12/   41] train: loss: 0.0073292
[Epoch 139] ogbg-molbace: 0.752381 val loss: 1.899663
[Epoch 139] ogbg-molbace: 0.758129 test loss: 2.586624
[Epoch 140; Iter     1/   41] train: loss: 0.0063606
[Epoch 140; Iter    31/   41] train: loss: 0.0047521
[Epoch 140] ogbg-molbace: 0.754579 val loss: 2.003512
[Epoch 140] ogbg-molbace: 0.760389 test loss: 2.547196
[Epoch 141; Iter    20/   41] train: loss: 0.0072948
[Epoch 141] ogbg-molbace: 0.742491 val loss: 1.962060
[Epoch 141] ogbg-molbace: 0.755173 test loss: 2.506462
[Epoch 142; Iter     9/   41] train: loss: 0.0025447
[Epoch 142; Iter    39/   41] train: loss: 0.0041492
[Epoch 142] ogbg-molbace: 0.738828 val loss: 2.162159
[Epoch 142] ogbg-molbace: 0.750304 test loss: 2.635837
[Epoch 143; Iter    28/   41] train: loss: 0.0176597
[Epoch 143] ogbg-molbace: 0.750183 val loss: 1.983094
[Epoch 143] ogbg-molbace: 0.767693 test loss: 2.560020
[Epoch 144; Iter    17/   41] train: loss: 0.0029634
[Epoch 144] ogbg-molbace: 0.758242 val loss: 1.932862
[Epoch 144] ogbg-molbace: 0.750478 test loss: 2.521985
[Epoch 145; Iter     6/   41] train: loss: 0.0037866
[Epoch 145; Iter    36/   41] train: loss: 0.0076194
[Epoch 145] ogbg-molbace: 0.750183 val loss: 1.860263
[Epoch 145] ogbg-molbace: 0.746479 test loss: 2.589622
[Epoch 146; Iter    25/   41] train: loss: 0.0020749
[Epoch 146] ogbg-molbace: 0.727839 val loss: 2.263849
[Epoch 146] ogbg-molbace: 0.757433 test loss: 2.643047
[Epoch 147; Iter    14/   41] train: loss: 0.0028547
[Epoch 147] ogbg-molbace: 0.729304 val loss: 2.478485
[Epoch 147] ogbg-molbace: 0.772387 test loss: 2.366312
[Epoch 148; Iter     3/   41] train: loss: 0.0024645
[Epoch 148; Iter    33/   41] train: loss: 0.0043175
[Epoch 148] ogbg-molbace: 0.731136 val loss: 2.396279
[Epoch 148] ogbg-molbace: 0.769605 test loss: 2.447580
[Epoch 149; Iter    22/   41] train: loss: 0.0063163
[Epoch 149] ogbg-molbace: 0.716117 val loss: 2.393767
[Epoch 149] ogbg-molbace: 0.774648 test loss: 2.559894
[Epoch 150; Iter    11/   41] train: loss: 0.0025786
[Epoch 150; Iter    41/   41] train: loss: 0.0024892
[Epoch 150] ogbg-molbace: 0.717582 val loss: 2.409694
[Epoch 150] ogbg-molbace: 0.767693 test loss: 2.631207
[Epoch 151; Iter    30/   41] train: loss: 0.0030520
[Epoch 151] ogbg-molbace: 0.726740 val loss: 2.213907
[Epoch 151] ogbg-molbace: 0.771170 test loss: 2.471654
[Epoch 152; Iter    19/   41] train: loss: 0.0038994
[Epoch 152] ogbg-molbace: 0.730403 val loss: 2.297850
[Epoch 152] ogbg-molbace: 0.773952 test loss: 2.637613
[Epoch 153; Iter     8/   41] train: loss: 0.0048328
[Epoch 153; Iter    38/   41] train: loss: 0.0017318
[Epoch 153] ogbg-molbace: 0.734799 val loss: 2.427582
[Epoch 153] ogbg-molbace: 0.778821 test loss: 2.553575
[Epoch 154; Iter    27/   41] train: loss: 0.0015837
[Epoch 154] ogbg-molbace: 0.732967 val loss: 2.406561
[Epoch 154] ogbg-molbace: 0.776561 test loss: 2.432970
[Epoch 155; Iter    16/   41] train: loss: 0.0052394
[Epoch 155] ogbg-molbace: 0.730769 val loss: 2.541011
[Epoch 155] ogbg-molbace: 0.776387 test loss: 2.539167
Early stopping criterion based on -ogbg-molbace- that should be max reached after 155 epochs. Best model checkpoint was in epoch 95.
Statistics on  val_best_checkpoint
mean_pred: 1.3885442018508911
std_pred: 3.6810295581817627
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9466946831304849
rocauc: 0.8036630036630037
ogbg-molbace: 0.8036630036630037
BCEWithLogitsLoss: 0.9192714740832647
Statistics on  test
mean_pred: -1.0048456192016602
std_pred: 10.016844749450684
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7182553362892924
rocauc: 0.7497826464962614
ogbg-molbace: 0.7497826464962614
BCEWithLogitsLoss: 2.556553433338801
Statistics on  train
mean_pred: -1.4393705129623413
std_pred: 5.603540420532227
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9934556668593943
rocauc: 0.9959275114155252
ogbg-molbace: 0.9959275114155252
BCEWithLogitsLoss: 0.0965034241629083
[Epoch 123] ogbg-molbace: 0.752739 test loss: 2.745278
[Epoch 124; Iter    27/   41] train: loss: 0.0087642
[Epoch 124] ogbg-molbace: 0.741758 val loss: 1.896732
[Epoch 124] ogbg-molbace: 0.741089 test loss: 2.780308
[Epoch 125; Iter    16/   41] train: loss: 0.0039664
[Epoch 125] ogbg-molbace: 0.743590 val loss: 1.812450
[Epoch 125] ogbg-molbace: 0.740219 test loss: 2.770805
[Epoch 126; Iter     5/   41] train: loss: 0.0034678
[Epoch 126; Iter    35/   41] train: loss: 0.0074250
[Epoch 126] ogbg-molbace: 0.753114 val loss: 1.946717
[Epoch 126] ogbg-molbace: 0.734481 test loss: 2.690021
[Epoch 127; Iter    24/   41] train: loss: 0.0381599
[Epoch 127] ogbg-molbace: 0.739560 val loss: 2.143742
[Epoch 127] ogbg-molbace: 0.743871 test loss: 2.777088
[Epoch 128; Iter    13/   41] train: loss: 0.0027209
[Epoch 128] ogbg-molbace: 0.734432 val loss: 1.931644
[Epoch 128] ogbg-molbace: 0.732568 test loss: 2.725710
[Epoch 129; Iter     2/   41] train: loss: 0.0054575
[Epoch 129; Iter    32/   41] train: loss: 0.0260390
[Epoch 129] ogbg-molbace: 0.758974 val loss: 1.813243
[Epoch 129] ogbg-molbace: 0.743871 test loss: 2.793526
[Epoch 130; Iter    21/   41] train: loss: 0.0013728
[Epoch 130] ogbg-molbace: 0.739560 val loss: 1.909255
[Epoch 130] ogbg-molbace: 0.727526 test loss: 2.864743
[Epoch 131; Iter    10/   41] train: loss: 0.0032633
[Epoch 131; Iter    40/   41] train: loss: 0.0060081
[Epoch 131] ogbg-molbace: 0.738462 val loss: 1.694984
[Epoch 131] ogbg-molbace: 0.737611 test loss: 2.818419
[Epoch 132; Iter    29/   41] train: loss: 0.0089144
[Epoch 132] ogbg-molbace: 0.746886 val loss: 1.999377
[Epoch 132] ogbg-molbace: 0.755521 test loss: 2.735573
[Epoch 133; Iter    18/   41] train: loss: 0.0047923
[Epoch 133] ogbg-molbace: 0.742491 val loss: 2.203835
[Epoch 133] ogbg-molbace: 0.742306 test loss: 2.885458
[Epoch 134; Iter     7/   41] train: loss: 0.0099150
[Epoch 134; Iter    37/   41] train: loss: 0.0011451
[Epoch 134] ogbg-molbace: 0.731502 val loss: 2.110834
[Epoch 134] ogbg-molbace: 0.733959 test loss: 2.707063
[Epoch 135; Iter    26/   41] train: loss: 0.0029337
[Epoch 135] ogbg-molbace: 0.756044 val loss: 1.923333
[Epoch 135] ogbg-molbace: 0.729438 test loss: 2.758980
[Epoch 136; Iter    15/   41] train: loss: 0.0052028
[Epoch 136] ogbg-molbace: 0.739560 val loss: 1.763219
[Epoch 136] ogbg-molbace: 0.734829 test loss: 2.859595
[Epoch 137; Iter     4/   41] train: loss: 0.0152221
[Epoch 137; Iter    34/   41] train: loss: 0.0022850
[Epoch 137] ogbg-molbace: 0.750549 val loss: 1.950663
[Epoch 137] ogbg-molbace: 0.747174 test loss: 2.780469
[Epoch 138; Iter    23/   41] train: loss: 0.0025228
[Epoch 138] ogbg-molbace: 0.746886 val loss: 2.138529
[Epoch 138] ogbg-molbace: 0.737263 test loss: 2.931249
[Epoch 139; Iter    12/   41] train: loss: 0.0045642
[Epoch 139] ogbg-molbace: 0.750183 val loss: 2.349175
[Epoch 139] ogbg-molbace: 0.726656 test loss: 2.752704
[Epoch 140; Iter     1/   41] train: loss: 0.1236845
[Epoch 140; Iter    31/   41] train: loss: 0.0132467
[Epoch 140] ogbg-molbace: 0.748352 val loss: 1.904520
[Epoch 140] ogbg-molbace: 0.752565 test loss: 2.761554
[Epoch 141; Iter    20/   41] train: loss: 0.0217185
[Epoch 141] ogbg-molbace: 0.730403 val loss: 2.349089
[Epoch 141] ogbg-molbace: 0.745609 test loss: 2.603298
[Epoch 142; Iter     9/   41] train: loss: 0.0107254
[Epoch 142; Iter    39/   41] train: loss: 0.0024778
[Epoch 142] ogbg-molbace: 0.742491 val loss: 2.356722
[Epoch 142] ogbg-molbace: 0.741958 test loss: 2.682031
[Epoch 143; Iter    28/   41] train: loss: 0.2021331
[Epoch 143] ogbg-molbace: 0.747619 val loss: 2.282452
[Epoch 143] ogbg-molbace: 0.772387 test loss: 2.445617
[Epoch 144; Iter    17/   41] train: loss: 0.0116067
[Epoch 144] ogbg-molbace: 0.745788 val loss: 2.008462
[Epoch 144] ogbg-molbace: 0.742827 test loss: 2.688481
[Epoch 145; Iter     6/   41] train: loss: 0.0036539
[Epoch 145; Iter    36/   41] train: loss: 0.0018763
[Epoch 145] ogbg-molbace: 0.756777 val loss: 1.938991
[Epoch 145] ogbg-molbace: 0.745262 test loss: 2.607969
[Epoch 146; Iter    25/   41] train: loss: 0.0038126
[Epoch 146] ogbg-molbace: 0.763370 val loss: 1.863813
[Epoch 146] ogbg-molbace: 0.744045 test loss: 2.685245
[Epoch 147; Iter    14/   41] train: loss: 0.0042358
[Epoch 147] ogbg-molbace: 0.772161 val loss: 1.827684
[Epoch 147] ogbg-molbace: 0.745436 test loss: 2.651509
[Epoch 148; Iter     3/   41] train: loss: 0.0202028
[Epoch 148; Iter    33/   41] train: loss: 0.0027731
[Epoch 148] ogbg-molbace: 0.765934 val loss: 1.850151
[Epoch 148] ogbg-molbace: 0.746479 test loss: 2.830046
[Epoch 149; Iter    22/   41] train: loss: 0.0069532
[Epoch 149] ogbg-molbace: 0.762637 val loss: 1.772921
[Epoch 149] ogbg-molbace: 0.749957 test loss: 3.040573
[Epoch 150; Iter    11/   41] train: loss: 0.0038344
[Epoch 150; Iter    41/   41] train: loss: 0.0107001
[Epoch 150] ogbg-molbace: 0.769597 val loss: 1.805329
[Epoch 150] ogbg-molbace: 0.746131 test loss: 2.800804
[Epoch 151; Iter    30/   41] train: loss: 0.0094456
[Epoch 151] ogbg-molbace: 0.769597 val loss: 1.825710
[Epoch 151] ogbg-molbace: 0.747001 test loss: 2.852463
[Epoch 152; Iter    19/   41] train: loss: 0.0017917
[Epoch 152] ogbg-molbace: 0.765201 val loss: 1.778650
[Epoch 152] ogbg-molbace: 0.742827 test loss: 2.880304
[Epoch 153; Iter     8/   41] train: loss: 0.0018524
[Epoch 153; Iter    38/   41] train: loss: 0.0026291
[Epoch 153] ogbg-molbace: 0.762637 val loss: 1.856989
[Epoch 153] ogbg-molbace: 0.743871 test loss: 2.984333
[Epoch 154; Iter    27/   41] train: loss: 0.0060515
[Epoch 154] ogbg-molbace: 0.769231 val loss: 1.885097
[Epoch 154] ogbg-molbace: 0.743871 test loss: 2.858127
[Epoch 155; Iter    16/   41] train: loss: 0.0114802
[Epoch 155] ogbg-molbace: 0.769231 val loss: 1.809210
[Epoch 155] ogbg-molbace: 0.743349 test loss: 2.832814
[Epoch 156; Iter     5/   41] train: loss: 0.0016126
[Epoch 156; Iter    35/   41] train: loss: 0.0010776
[Epoch 156] ogbg-molbace: 0.763370 val loss: 1.783045
[Epoch 156] ogbg-molbace: 0.743523 test loss: 2.855179
[Epoch 157; Iter    24/   41] train: loss: 0.0014941
[Epoch 157] ogbg-molbace: 0.767399 val loss: 1.815085
[Epoch 157] ogbg-molbace: 0.737437 test loss: 2.936030
[Epoch 158; Iter    13/   41] train: loss: 0.0015901
[Epoch 158] ogbg-molbace: 0.770696 val loss: 1.744556
[Epoch 158] ogbg-molbace: 0.742306 test loss: 2.856384
[Epoch 159; Iter     2/   41] train: loss: 0.0011197
[Epoch 159; Iter    32/   41] train: loss: 0.0047951
[Epoch 159] ogbg-molbace: 0.765201 val loss: 1.633074
[Epoch 159] ogbg-molbace: 0.741784 test loss: 2.972369
[Epoch 160; Iter    21/   41] train: loss: 0.0023343
[Epoch 160] ogbg-molbace: 0.766667 val loss: 1.767947
[Epoch 160] ogbg-molbace: 0.746653 test loss: 2.982078
[Epoch 161; Iter    10/   41] train: loss: 0.0089095
[Epoch 161; Iter    40/   41] train: loss: 0.0058328
[Epoch 161] ogbg-molbace: 0.767033 val loss: 1.797610
[Epoch 161] ogbg-molbace: 0.734655 test loss: 3.144847
[Epoch 162; Iter    29/   41] train: loss: 0.0030501
[Epoch 162] ogbg-molbace: 0.766300 val loss: 1.747688
[Epoch 162] ogbg-molbace: 0.735176 test loss: 2.856787
[Epoch 163; Iter    18/   41] train: loss: 0.0024232
[Epoch 163] ogbg-molbace: 0.761905 val loss: 1.801127
[Epoch 163] ogbg-molbace: 0.737263 test loss: 3.016198
[Epoch 164; Iter     7/   41] train: loss: 0.0049060
[Epoch 164; Iter    37/   41] train: loss: 0.0009442
[Epoch 164] ogbg-molbace: 0.767766 val loss: 1.909881
[Epoch 164] ogbg-molbace: 0.737089 test loss: 3.066891
[Epoch 165; Iter    26/   41] train: loss: 0.0010842
[Epoch 165] ogbg-molbace: 0.758242 val loss: 1.926187
[Epoch 165] ogbg-molbace: 0.731525 test loss: 2.981493
[Epoch 166; Iter    15/   41] train: loss: 0.0010120
[Epoch 166] ogbg-molbace: 0.756410 val loss: 1.880503
[Epoch 166] ogbg-molbace: 0.735350 test loss: 2.861794
[Epoch 167; Iter     4/   41] train: loss: 0.0031951
[Epoch 167; Iter    34/   41] train: loss: 0.0015156
[Epoch 167] ogbg-molbace: 0.763004 val loss: 1.707483
[Epoch 167] ogbg-molbace: 0.732394 test loss: 3.096508
[Epoch 168; Iter    23/   41] train: loss: 0.0102870
[Epoch 168] ogbg-molbace: 0.756410 val loss: 1.836435
[Epoch 168] ogbg-molbace: 0.744914 test loss: 2.972341/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 169; Iter    12/   41] train: loss: 0.0006491
[Epoch 169] ogbg-molbace: 0.756044 val loss: 1.809787
[Epoch 169] ogbg-molbace: 0.744218 test loss: 3.001464
[Epoch 170; Iter     1/   41] train: loss: 0.0058419
[Epoch 170; Iter    31/   41] train: loss: 0.0027005
[Epoch 170] ogbg-molbace: 0.757143 val loss: 1.836539
[Epoch 170] ogbg-molbace: 0.740741 test loss: 2.864932
[Epoch 171; Iter    20/   41] train: loss: 0.0011630
[Epoch 171] ogbg-molbace: 0.762637 val loss: 1.765657
[Epoch 171] ogbg-molbace: 0.745609 test loss: 2.854869
[Epoch 172; Iter     9/   41] train: loss: 0.0076561
[Epoch 172; Iter    39/   41] train: loss: 0.0008332
[Epoch 172] ogbg-molbace: 0.765934 val loss: 1.841231
[Epoch 172] ogbg-molbace: 0.743871 test loss: 2.992015
[Epoch 173; Iter    28/   41] train: loss: 0.0006440
[Epoch 173] ogbg-molbace: 0.769231 val loss: 1.784308
[Epoch 173] ogbg-molbace: 0.740045 test loss: 2.970112
[Epoch 174; Iter    17/   41] train: loss: 0.0006331
[Epoch 174] ogbg-molbace: 0.768864 val loss: 1.735680
[Epoch 174] ogbg-molbace: 0.742306 test loss: 2.898477
Early stopping criterion based on -ogbg-molbace- that should be max reached after 174 epochs. Best model checkpoint was in epoch 114.
Statistics on  val_best_checkpoint
mean_pred: 1.762508511543274
std_pred: 5.247666358947754
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9594586152376523
rocauc: 0.772893772893773
ogbg-molbace: 0.772893772893773
BCEWithLogitsLoss: 1.6489904920260112
Statistics on  test
mean_pred: -2.6899917125701904
std_pred: 6.901976585388184
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7582204229105342
rocauc: 0.75830290384281
ogbg-molbace: 0.75830290384281
BCEWithLogitsLoss: 2.443949987490972
Statistics on  train
mean_pred: -2.020627975463867
std_pred: 7.041752338409424
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9999913554633472
rocauc: 0.999994292237443
ogbg-molbace: 0.999994292237443
BCEWithLogitsLoss: 0.015425818390212953
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.0.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.0.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/bace/noise=0.0.yml --seed 6 --device cuda:0
All runs completed.
