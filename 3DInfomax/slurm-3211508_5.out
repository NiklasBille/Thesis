>>> Starting run for dataset: sider
Running configs_static_noise_experiments/3DInfomax/sider/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/3DInfomax/sider/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/3DInfomax/sider/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/3DInfomax/sider/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.1/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.1_4_26-05_09-18-53
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.1
logdir: runs/static_noise/3DInfomax/sider/noise=0.1
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6931958
[Epoch 1] ogbg-molsider: 0.499951 val loss: 0.693545
[Epoch 1] ogbg-molsider: 0.519613 test loss: 0.693784
[Epoch 2; Iter    24/   36] train: loss: 0.6936668
[Epoch 2] ogbg-molsider: 0.498060 val loss: 0.694633
[Epoch 2] ogbg-molsider: 0.519694 test loss: 0.695555
[Epoch 3; Iter    18/   36] train: loss: 0.6933194
[Epoch 3] ogbg-molsider: 0.496188 val loss: 0.695390
[Epoch 3] ogbg-molsider: 0.508876 test loss: 0.696784
[Epoch 4; Iter    12/   36] train: loss: 0.6936719
[Epoch 4] ogbg-molsider: 0.496773 val loss: 0.695533
[Epoch 4] ogbg-molsider: 0.510436 test loss: 0.696966
[Epoch 5; Iter     6/   36] train: loss: 0.6933106
[Epoch 5; Iter    36/   36] train: loss: 0.6939585
[Epoch 5] ogbg-molsider: 0.497487 val loss: 0.695337
[Epoch 5] ogbg-molsider: 0.510016 test loss: 0.696676
[Epoch 6; Iter    30/   36] train: loss: 0.6929649
[Epoch 6] ogbg-molsider: 0.496263 val loss: 0.694960
[Epoch 6] ogbg-molsider: 0.509656 test loss: 0.696224
[Epoch 7; Iter    24/   36] train: loss: 0.6930523
[Epoch 7] ogbg-molsider: 0.497148 val loss: 0.694920
[Epoch 7] ogbg-molsider: 0.509815 test loss: 0.696186
[Epoch 8; Iter    18/   36] train: loss: 0.6929170
[Epoch 8] ogbg-molsider: 0.496949 val loss: 0.694935
[Epoch 8] ogbg-molsider: 0.512763 test loss: 0.696206
[Epoch 9; Iter    12/   36] train: loss: 0.6930408
[Epoch 9] ogbg-molsider: 0.495285 val loss: 0.695025
[Epoch 9] ogbg-molsider: 0.512347 test loss: 0.696487
[Epoch 10; Iter     6/   36] train: loss: 0.6931500
[Epoch 10; Iter    36/   36] train: loss: 0.6923190
[Epoch 10] ogbg-molsider: 0.494195 val loss: 0.695025
[Epoch 10] ogbg-molsider: 0.512640 test loss: 0.696527
[Epoch 11; Iter    30/   36] train: loss: 0.6920672
[Epoch 11] ogbg-molsider: 0.496206 val loss: 0.694518
[Epoch 11] ogbg-molsider: 0.512882 test loss: 0.695813
[Epoch 12; Iter    24/   36] train: loss: 0.6922216
[Epoch 12] ogbg-molsider: 0.496194 val loss: 0.694460
[Epoch 12] ogbg-molsider: 0.512465 test loss: 0.695854
[Epoch 13; Iter    18/   36] train: loss: 0.6922300
[Epoch 13] ogbg-molsider: 0.496694 val loss: 0.694339
[Epoch 13] ogbg-molsider: 0.512944 test loss: 0.695740
[Epoch 14; Iter    12/   36] train: loss: 0.6926633
[Epoch 14] ogbg-molsider: 0.494974 val loss: 0.694024
[Epoch 14] ogbg-molsider: 0.512995 test loss: 0.695265
[Epoch 15; Iter     6/   36] train: loss: 0.6922911
[Epoch 15; Iter    36/   36] train: loss: 0.6916762
[Epoch 15] ogbg-molsider: 0.494808 val loss: 0.693856
[Epoch 15] ogbg-molsider: 0.518300 test loss: 0.695224
[Epoch 16; Iter    30/   36] train: loss: 0.6915765
[Epoch 16] ogbg-molsider: 0.495603 val loss: 0.693995
[Epoch 16] ogbg-molsider: 0.512585 test loss: 0.695511
[Epoch 17; Iter    24/   36] train: loss: 0.6919746
[Epoch 17] ogbg-molsider: 0.495607 val loss: 0.693423
[Epoch 17] ogbg-molsider: 0.518109 test loss: 0.694750
[Epoch 18; Iter    18/   36] train: loss: 0.6912898
[Epoch 18] ogbg-molsider: 0.496497 val loss: 0.693175
[Epoch 18] ogbg-molsider: 0.518342 test loss: 0.694400
[Epoch 19; Iter    12/   36] train: loss: 0.6917434
[Epoch 19] ogbg-molsider: 0.495418 val loss: 0.692733
[Epoch 19] ogbg-molsider: 0.520538 test loss: 0.693877
[Epoch 20; Iter     6/   36] train: loss: 0.6907219
[Epoch 20; Iter    36/   36] train: loss: 0.6865233
[Epoch 20] ogbg-molsider: 0.527700 val loss: 0.689446
[Epoch 20] ogbg-molsider: 0.521764 test loss: 0.692291
[Epoch 21; Iter    30/   36] train: loss: 0.6762188
[Epoch 21] ogbg-molsider: 0.548236 val loss: 0.673455
[Epoch 21] ogbg-molsider: 0.553823 test loss: 0.679604
[Epoch 22; Iter    24/   36] train: loss: 0.6612039
[Epoch 22] ogbg-molsider: 0.515503 val loss: 0.617940
[Epoch 22] ogbg-molsider: 0.590969 test loss: 0.612529
[Epoch 23; Iter    18/   36] train: loss: 0.6365914
[Epoch 23] ogbg-molsider: 0.543463 val loss: 0.595989
[Epoch 23] ogbg-molsider: 0.581460 test loss: 0.595845
[Epoch 24; Iter    12/   36] train: loss: 0.6137563
[Epoch 24] ogbg-molsider: 0.536640 val loss: 0.573517
[Epoch 24] ogbg-molsider: 0.570573 test loss: 0.570079
[Epoch 25; Iter     6/   36] train: loss: 0.5858722
[Epoch 25; Iter    36/   36] train: loss: 0.5710998
[Epoch 25] ogbg-molsider: 0.541748 val loss: 0.623533
[Epoch 25] ogbg-molsider: 0.518558 test loss: 0.642227
[Epoch 26; Iter    30/   36] train: loss: 0.5490394
[Epoch 26] ogbg-molsider: 0.511290 val loss: 0.503435
[Epoch 26] ogbg-molsider: 0.587303 test loss: 0.501345
[Epoch 27; Iter    24/   36] train: loss: 0.5358189
[Epoch 27] ogbg-molsider: 0.538099 val loss: 0.506986
[Epoch 27] ogbg-molsider: 0.547697 test loss: 0.513308
[Epoch 28; Iter    18/   36] train: loss: 0.5023950
[Epoch 28] ogbg-molsider: 0.524612 val loss: 0.490645
[Epoch 28] ogbg-molsider: 0.585130 test loss: 0.496294
[Epoch 29; Iter    12/   36] train: loss: 0.5335624
[Epoch 29] ogbg-molsider: 0.518185 val loss: 0.490966
[Epoch 29] ogbg-molsider: 0.575799 test loss: 0.498112
[Epoch 30; Iter     6/   36] train: loss: 0.5197466
[Epoch 30; Iter    36/   36] train: loss: 0.5140020
[Epoch 30] ogbg-molsider: 0.547107 val loss: 0.490819
[Epoch 30] ogbg-molsider: 0.580782 test loss: 0.499969
[Epoch 31; Iter    30/   36] train: loss: 0.4683060
[Epoch 31] ogbg-molsider: 0.522987 val loss: 0.516398
[Epoch 31] ogbg-molsider: 0.574753 test loss: 0.527405
[Epoch 32; Iter    24/   36] train: loss: 0.5616025
[Epoch 32] ogbg-molsider: 0.507491 val loss: 0.500634
[Epoch 32] ogbg-molsider: 0.577932 test loss: 0.504334
[Epoch 33; Iter    18/   36] train: loss: 0.5010665
[Epoch 33] ogbg-molsider: 0.527144 val loss: 0.501335
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.05/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.05_5_26-05_09-18-49
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.05
logdir: runs/static_noise/3DInfomax/sider/noise=0.05
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932471
[Epoch 1] ogbg-molsider: 0.484065 val loss: 0.693208
[Epoch 1] ogbg-molsider: 0.507362 test loss: 0.693270
[Epoch 2; Iter    24/   36] train: loss: 0.6935974
[Epoch 2] ogbg-molsider: 0.504656 val loss: 0.693480
[Epoch 2] ogbg-molsider: 0.497536 test loss: 0.693958
[Epoch 3; Iter    18/   36] train: loss: 0.6934569
[Epoch 3] ogbg-molsider: 0.510486 val loss: 0.693628
[Epoch 3] ogbg-molsider: 0.502809 test loss: 0.694271
[Epoch 4; Iter    12/   36] train: loss: 0.6932372
[Epoch 4] ogbg-molsider: 0.507022 val loss: 0.693578
[Epoch 4] ogbg-molsider: 0.501080 test loss: 0.694166
[Epoch 5; Iter     6/   36] train: loss: 0.6932781
[Epoch 5; Iter    36/   36] train: loss: 0.6925182
[Epoch 5] ogbg-molsider: 0.506740 val loss: 0.693616
[Epoch 5] ogbg-molsider: 0.500450 test loss: 0.694192
[Epoch 6; Iter    30/   36] train: loss: 0.6932060
[Epoch 6] ogbg-molsider: 0.506146 val loss: 0.693410
[Epoch 6] ogbg-molsider: 0.508131 test loss: 0.693888
[Epoch 7; Iter    24/   36] train: loss: 0.6931799
[Epoch 7] ogbg-molsider: 0.505890 val loss: 0.693232
[Epoch 7] ogbg-molsider: 0.505143 test loss: 0.693751
[Epoch 8; Iter    18/   36] train: loss: 0.6924157
[Epoch 8] ogbg-molsider: 0.507945 val loss: 0.693325
[Epoch 8] ogbg-molsider: 0.503415 test loss: 0.693936
[Epoch 9; Iter    12/   36] train: loss: 0.6926916
[Epoch 9] ogbg-molsider: 0.504543 val loss: 0.693202
[Epoch 9] ogbg-molsider: 0.506532 test loss: 0.693769
[Epoch 10; Iter     6/   36] train: loss: 0.6926089
[Epoch 10; Iter    36/   36] train: loss: 0.6929296
[Epoch 10] ogbg-molsider: 0.503827 val loss: 0.692905
[Epoch 10] ogbg-molsider: 0.503662 test loss: 0.693401
[Epoch 11; Iter    30/   36] train: loss: 0.6926568
[Epoch 11] ogbg-molsider: 0.504721 val loss: 0.692858
[Epoch 11] ogbg-molsider: 0.505901 test loss: 0.693431
[Epoch 12; Iter    24/   36] train: loss: 0.6925343
[Epoch 12] ogbg-molsider: 0.507036 val loss: 0.692634
[Epoch 12] ogbg-molsider: 0.507543 test loss: 0.693227
[Epoch 13; Iter    18/   36] train: loss: 0.6924977
[Epoch 13] ogbg-molsider: 0.505942 val loss: 0.692601
[Epoch 13] ogbg-molsider: 0.507470 test loss: 0.693117
[Epoch 14; Iter    12/   36] train: loss: 0.6924999
[Epoch 14] ogbg-molsider: 0.505927 val loss: 0.692350
[Epoch 14] ogbg-molsider: 0.505518 test loss: 0.692956
[Epoch 15; Iter     6/   36] train: loss: 0.6924521
[Epoch 15; Iter    36/   36] train: loss: 0.6915331
[Epoch 15] ogbg-molsider: 0.507077 val loss: 0.692143
[Epoch 15] ogbg-molsider: 0.505400 test loss: 0.692585
[Epoch 16; Iter    30/   36] train: loss: 0.6920916
[Epoch 16] ogbg-molsider: 0.505042 val loss: 0.691960
[Epoch 16] ogbg-molsider: 0.505255 test loss: 0.692576
[Epoch 17; Iter    24/   36] train: loss: 0.6918229
[Epoch 17] ogbg-molsider: 0.506630 val loss: 0.691792
[Epoch 17] ogbg-molsider: 0.504903 test loss: 0.692398
[Epoch 18; Iter    18/   36] train: loss: 0.6915272
[Epoch 18] ogbg-molsider: 0.504093 val loss: 0.691470
[Epoch 18] ogbg-molsider: 0.510842 test loss: 0.691939
[Epoch 19; Iter    12/   36] train: loss: 0.6914192
[Epoch 19] ogbg-molsider: 0.506711 val loss: 0.691251
[Epoch 19] ogbg-molsider: 0.512275 test loss: 0.691787
[Epoch 20; Iter     6/   36] train: loss: 0.6908914
[Epoch 20; Iter    36/   36] train: loss: 0.6867025
[Epoch 20] ogbg-molsider: 0.531859 val loss: 0.682811
[Epoch 20] ogbg-molsider: 0.535241 test loss: 0.683542
[Epoch 21; Iter    30/   36] train: loss: 0.6790293
[Epoch 21] ogbg-molsider: 0.554620 val loss: 0.660530
[Epoch 21] ogbg-molsider: 0.580255 test loss: 0.661306
[Epoch 22; Iter    24/   36] train: loss: 0.6612476
[Epoch 22] ogbg-molsider: 0.535363 val loss: 0.639389
[Epoch 22] ogbg-molsider: 0.587280 test loss: 0.635687
[Epoch 23; Iter    18/   36] train: loss: 0.6358716
[Epoch 23] ogbg-molsider: 0.536948 val loss: 0.614685
[Epoch 23] ogbg-molsider: 0.601574 test loss: 0.615235
[Epoch 24; Iter    12/   36] train: loss: 0.6285875
[Epoch 24] ogbg-molsider: 0.506033 val loss: 0.589768
[Epoch 24] ogbg-molsider: 0.593905 test loss: 0.589181
[Epoch 25; Iter     6/   36] train: loss: 0.5914759
[Epoch 25; Iter    36/   36] train: loss: 0.5661013
[Epoch 25] ogbg-molsider: 0.546415 val loss: 0.546157
[Epoch 25] ogbg-molsider: 0.584805 test loss: 0.555774
[Epoch 26; Iter    30/   36] train: loss: 0.5682410
[Epoch 26] ogbg-molsider: 0.520328 val loss: 0.515080
[Epoch 26] ogbg-molsider: 0.592227 test loss: 0.516637
[Epoch 27; Iter    24/   36] train: loss: 0.5397522
[Epoch 27] ogbg-molsider: 0.554656 val loss: 0.505804
[Epoch 27] ogbg-molsider: 0.571803 test loss: 0.516836
[Epoch 28; Iter    18/   36] train: loss: 0.5300285
[Epoch 28] ogbg-molsider: 0.507574 val loss: 0.504706
[Epoch 28] ogbg-molsider: 0.594065 test loss: 0.503092
[Epoch 29; Iter    12/   36] train: loss: 0.5070019
[Epoch 29] ogbg-molsider: 0.552119 val loss: 0.486576
[Epoch 29] ogbg-molsider: 0.573015 test loss: 0.492880
[Epoch 30; Iter     6/   36] train: loss: 0.5040677
[Epoch 30; Iter    36/   36] train: loss: 0.4530239
[Epoch 30] ogbg-molsider: 0.513539 val loss: 0.492032
[Epoch 30] ogbg-molsider: 0.587446 test loss: 0.488745
[Epoch 31; Iter    30/   36] train: loss: 0.4936300
[Epoch 31] ogbg-molsider: 0.509275 val loss: 0.493646
[Epoch 31] ogbg-molsider: 0.616421 test loss: 0.494335
[Epoch 32; Iter    24/   36] train: loss: 0.4856440
[Epoch 32] ogbg-molsider: 0.566222 val loss: 0.485235
[Epoch 32] ogbg-molsider: 0.599410 test loss: 0.493691
[Epoch 33; Iter    18/   36] train: loss: 0.4921982
[Epoch 33] ogbg-molsider: 0.539702 val loss: 0.485708
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.1/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.1_5_26-05_09-18-53
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.1
logdir: runs/static_noise/3DInfomax/sider/noise=0.1
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6933529
[Epoch 1] ogbg-molsider: 0.479963 val loss: 0.693266
[Epoch 1] ogbg-molsider: 0.513207 test loss: 0.693350
[Epoch 2; Iter    24/   36] train: loss: 0.6936654
[Epoch 2] ogbg-molsider: 0.490149 val loss: 0.693769
[Epoch 2] ogbg-molsider: 0.507800 test loss: 0.694423
[Epoch 3; Iter    18/   36] train: loss: 0.6933156
[Epoch 3] ogbg-molsider: 0.505228 val loss: 0.694322
[Epoch 3] ogbg-molsider: 0.508552 test loss: 0.695234
[Epoch 4; Iter    12/   36] train: loss: 0.6932674
[Epoch 4] ogbg-molsider: 0.505577 val loss: 0.694292
[Epoch 4] ogbg-molsider: 0.510805 test loss: 0.695200
[Epoch 5; Iter     6/   36] train: loss: 0.6932017
[Epoch 5; Iter    36/   36] train: loss: 0.6928007
[Epoch 5] ogbg-molsider: 0.502139 val loss: 0.694269
[Epoch 5] ogbg-molsider: 0.511498 test loss: 0.695094
[Epoch 6; Iter    30/   36] train: loss: 0.6932606
[Epoch 6] ogbg-molsider: 0.506432 val loss: 0.694052
[Epoch 6] ogbg-molsider: 0.509629 test loss: 0.694878
[Epoch 7; Iter    24/   36] train: loss: 0.6930065
[Epoch 7] ogbg-molsider: 0.505340 val loss: 0.693767
[Epoch 7] ogbg-molsider: 0.507872 test loss: 0.694523
[Epoch 8; Iter    18/   36] train: loss: 0.6924718
[Epoch 8] ogbg-molsider: 0.502850 val loss: 0.693964
[Epoch 8] ogbg-molsider: 0.512339 test loss: 0.694851
[Epoch 9; Iter    12/   36] train: loss: 0.6931103
[Epoch 9] ogbg-molsider: 0.504857 val loss: 0.693904
[Epoch 9] ogbg-molsider: 0.514262 test loss: 0.694800
[Epoch 10; Iter     6/   36] train: loss: 0.6926367
[Epoch 10; Iter    36/   36] train: loss: 0.6926364
[Epoch 10] ogbg-molsider: 0.503282 val loss: 0.693569
[Epoch 10] ogbg-molsider: 0.516214 test loss: 0.694282
[Epoch 11; Iter    30/   36] train: loss: 0.6922975
[Epoch 11] ogbg-molsider: 0.503452 val loss: 0.693467
[Epoch 11] ogbg-molsider: 0.509493 test loss: 0.694424
[Epoch 12; Iter    24/   36] train: loss: 0.6925663
[Epoch 12] ogbg-molsider: 0.505908 val loss: 0.693327
[Epoch 12] ogbg-molsider: 0.510950 test loss: 0.694225
[Epoch 13; Iter    18/   36] train: loss: 0.6923668
[Epoch 13] ogbg-molsider: 0.504180 val loss: 0.693097
[Epoch 13] ogbg-molsider: 0.512805 test loss: 0.693921
[Epoch 14; Iter    12/   36] train: loss: 0.6922820
[Epoch 14] ogbg-molsider: 0.505497 val loss: 0.692998
[Epoch 14] ogbg-molsider: 0.511904 test loss: 0.693967
[Epoch 15; Iter     6/   36] train: loss: 0.6923007
[Epoch 15; Iter    36/   36] train: loss: 0.6917249
[Epoch 15] ogbg-molsider: 0.505740 val loss: 0.692666
[Epoch 15] ogbg-molsider: 0.511596 test loss: 0.693353
[Epoch 16; Iter    30/   36] train: loss: 0.6922755
[Epoch 16] ogbg-molsider: 0.505200 val loss: 0.692623
[Epoch 16] ogbg-molsider: 0.515215 test loss: 0.693542
[Epoch 17; Iter    24/   36] train: loss: 0.6917304
[Epoch 17] ogbg-molsider: 0.505640 val loss: 0.692381
[Epoch 17] ogbg-molsider: 0.514007 test loss: 0.693259
[Epoch 18; Iter    18/   36] train: loss: 0.6917049
[Epoch 18] ogbg-molsider: 0.503124 val loss: 0.691927
[Epoch 18] ogbg-molsider: 0.514808 test loss: 0.692735
[Epoch 19; Iter    12/   36] train: loss: 0.6916894
[Epoch 19] ogbg-molsider: 0.504034 val loss: 0.692059
[Epoch 19] ogbg-molsider: 0.513004 test loss: 0.692973
[Epoch 20; Iter     6/   36] train: loss: 0.6910676
[Epoch 20; Iter    36/   36] train: loss: 0.6868402
[Epoch 20] ogbg-molsider: 0.532310 val loss: 0.681693
[Epoch 20] ogbg-molsider: 0.532578 test loss: 0.681758
[Epoch 21; Iter    30/   36] train: loss: 0.6798835
[Epoch 21] ogbg-molsider: 0.513113 val loss: 0.657264
[Epoch 21] ogbg-molsider: 0.575674 test loss: 0.657777
[Epoch 22; Iter    24/   36] train: loss: 0.6597882
[Epoch 22] ogbg-molsider: 0.561564 val loss: 0.670085
[Epoch 22] ogbg-molsider: 0.537209 test loss: 0.678769
[Epoch 23; Iter    18/   36] train: loss: 0.6388239
[Epoch 23] ogbg-molsider: 0.539569 val loss: 0.632519
[Epoch 23] ogbg-molsider: 0.590540 test loss: 0.634378
[Epoch 24; Iter    12/   36] train: loss: 0.6346703
[Epoch 24] ogbg-molsider: 0.519512 val loss: 0.592873
[Epoch 24] ogbg-molsider: 0.573597 test loss: 0.589394
[Epoch 25; Iter     6/   36] train: loss: 0.5839156
[Epoch 25; Iter    36/   36] train: loss: 0.5658149
[Epoch 25] ogbg-molsider: 0.543680 val loss: 0.573523
[Epoch 25] ogbg-molsider: 0.561228 test loss: 0.583457
[Epoch 26; Iter    30/   36] train: loss: 0.5702956
[Epoch 26] ogbg-molsider: 0.521306 val loss: 0.510803
[Epoch 26] ogbg-molsider: 0.603891 test loss: 0.512032
[Epoch 27; Iter    24/   36] train: loss: 0.5277659
[Epoch 27] ogbg-molsider: 0.524773 val loss: 0.502545
[Epoch 27] ogbg-molsider: 0.593097 test loss: 0.506911
[Epoch 28; Iter    18/   36] train: loss: 0.5346986
[Epoch 28] ogbg-molsider: 0.525462 val loss: 0.512155
[Epoch 28] ogbg-molsider: 0.581294 test loss: 0.516841
[Epoch 29; Iter    12/   36] train: loss: 0.5029257
[Epoch 29] ogbg-molsider: 0.547881 val loss: 0.488869
[Epoch 29] ogbg-molsider: 0.583362 test loss: 0.497034
[Epoch 30; Iter     6/   36] train: loss: 0.5031230
[Epoch 30; Iter    36/   36] train: loss: 0.4600969
[Epoch 30] ogbg-molsider: 0.541259 val loss: 0.498891
[Epoch 30] ogbg-molsider: 0.571242 test loss: 0.509819
[Epoch 31; Iter    30/   36] train: loss: 0.5064549
[Epoch 31] ogbg-molsider: 0.523782 val loss: 0.505107
[Epoch 31] ogbg-molsider: 0.586870 test loss: 0.511245
[Epoch 32; Iter    24/   36] train: loss: 0.4908496
[Epoch 32] ogbg-molsider: 0.557869 val loss: 0.497998
[Epoch 32] ogbg-molsider: 0.579201 test loss: 0.510121
[Epoch 33; Iter    18/   36] train: loss: 0.4850664
[Epoch 33] ogbg-molsider: 0.537890 val loss: 0.523762
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.05/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.05_4_26-05_09-18-49
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.05
logdir: runs/static_noise/3DInfomax/sider/noise=0.05
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6931197
[Epoch 1] ogbg-molsider: 0.499844 val loss: 0.693376
[Epoch 1] ogbg-molsider: 0.522618 test loss: 0.693515
[Epoch 2; Iter    24/   36] train: loss: 0.6936249
[Epoch 2] ogbg-molsider: 0.500000 val loss: 0.693842
[Epoch 2] ogbg-molsider: 0.520080 test loss: 0.694270
[Epoch 3; Iter    18/   36] train: loss: 0.6934837
[Epoch 3] ogbg-molsider: 0.501222 val loss: 0.694393
[Epoch 3] ogbg-molsider: 0.516858 test loss: 0.695119
[Epoch 4; Iter    12/   36] train: loss: 0.6933581
[Epoch 4] ogbg-molsider: 0.501195 val loss: 0.694329
[Epoch 4] ogbg-molsider: 0.515274 test loss: 0.695025
[Epoch 5; Iter     6/   36] train: loss: 0.6931241
[Epoch 5; Iter    36/   36] train: loss: 0.6934881
[Epoch 5] ogbg-molsider: 0.502121 val loss: 0.694229
[Epoch 5] ogbg-molsider: 0.514594 test loss: 0.694880
[Epoch 6; Iter    30/   36] train: loss: 0.6928555
[Epoch 6] ogbg-molsider: 0.504541 val loss: 0.693969
[Epoch 6] ogbg-molsider: 0.516163 test loss: 0.694571
[Epoch 7; Iter    24/   36] train: loss: 0.6931003
[Epoch 7] ogbg-molsider: 0.503035 val loss: 0.693916
[Epoch 7] ogbg-molsider: 0.515502 test loss: 0.694507
[Epoch 8; Iter    18/   36] train: loss: 0.6927082
[Epoch 8] ogbg-molsider: 0.503015 val loss: 0.693860
[Epoch 8] ogbg-molsider: 0.515396 test loss: 0.694491
[Epoch 9; Iter    12/   36] train: loss: 0.6932265
[Epoch 9] ogbg-molsider: 0.502271 val loss: 0.693860
[Epoch 9] ogbg-molsider: 0.515978 test loss: 0.694521
[Epoch 10; Iter     6/   36] train: loss: 0.6928641
[Epoch 10; Iter    36/   36] train: loss: 0.6920303
[Epoch 10] ogbg-molsider: 0.499863 val loss: 0.693873
[Epoch 10] ogbg-molsider: 0.515623 test loss: 0.694617
[Epoch 11; Iter    30/   36] train: loss: 0.6917105
[Epoch 11] ogbg-molsider: 0.502700 val loss: 0.693470
[Epoch 11] ogbg-molsider: 0.518811 test loss: 0.694116
[Epoch 12; Iter    24/   36] train: loss: 0.6924231
[Epoch 12] ogbg-molsider: 0.501557 val loss: 0.693347
[Epoch 12] ogbg-molsider: 0.516071 test loss: 0.693958
[Epoch 13; Iter    18/   36] train: loss: 0.6919701
[Epoch 13] ogbg-molsider: 0.503470 val loss: 0.693196
[Epoch 13] ogbg-molsider: 0.515778 test loss: 0.693843
[Epoch 14; Iter    12/   36] train: loss: 0.6925086
[Epoch 14] ogbg-molsider: 0.502573 val loss: 0.693007
[Epoch 14] ogbg-molsider: 0.517363 test loss: 0.693617
[Epoch 15; Iter     6/   36] train: loss: 0.6923924
[Epoch 15; Iter    36/   36] train: loss: 0.6917588
[Epoch 15] ogbg-molsider: 0.501564 val loss: 0.692760
[Epoch 15] ogbg-molsider: 0.518041 test loss: 0.693390
[Epoch 16; Iter    30/   36] train: loss: 0.6914353
[Epoch 16] ogbg-molsider: 0.501540 val loss: 0.692797
[Epoch 16] ogbg-molsider: 0.516441 test loss: 0.693521
[Epoch 17; Iter    24/   36] train: loss: 0.6917815
[Epoch 17] ogbg-molsider: 0.501578 val loss: 0.692352
[Epoch 17] ogbg-molsider: 0.519614 test loss: 0.692970
[Epoch 18; Iter    18/   36] train: loss: 0.6910061
[Epoch 18] ogbg-molsider: 0.502033 val loss: 0.692094
[Epoch 18] ogbg-molsider: 0.518093 test loss: 0.692680
[Epoch 19; Iter    12/   36] train: loss: 0.6913977
[Epoch 19] ogbg-molsider: 0.502375 val loss: 0.691818
[Epoch 19] ogbg-molsider: 0.520946 test loss: 0.692369
[Epoch 20; Iter     6/   36] train: loss: 0.6905225
[Epoch 20; Iter    36/   36] train: loss: 0.6864884
[Epoch 20] ogbg-molsider: 0.533941 val loss: 0.687143
[Epoch 20] ogbg-molsider: 0.539765 test loss: 0.690128
[Epoch 21; Iter    30/   36] train: loss: 0.6772965
[Epoch 21] ogbg-molsider: 0.519533 val loss: 0.661536
[Epoch 21] ogbg-molsider: 0.576499 test loss: 0.660974
[Epoch 22; Iter    24/   36] train: loss: 0.6580224
[Epoch 22] ogbg-molsider: 0.505657 val loss: 0.613560
[Epoch 22] ogbg-molsider: 0.594083 test loss: 0.610681
[Epoch 23; Iter    18/   36] train: loss: 0.6347947
[Epoch 23] ogbg-molsider: 0.544431 val loss: 0.613057
[Epoch 23] ogbg-molsider: 0.587946 test loss: 0.616570
[Epoch 24; Iter    12/   36] train: loss: 0.6161705
[Epoch 24] ogbg-molsider: 0.519137 val loss: 0.584974
[Epoch 24] ogbg-molsider: 0.588335 test loss: 0.585840
[Epoch 25; Iter     6/   36] train: loss: 0.5887627
[Epoch 25; Iter    36/   36] train: loss: 0.5659292
[Epoch 25] ogbg-molsider: 0.560035 val loss: 0.572111
[Epoch 25] ogbg-molsider: 0.561834 test loss: 0.582341
[Epoch 26; Iter    30/   36] train: loss: 0.5545681
[Epoch 26] ogbg-molsider: 0.515384 val loss: 0.636164
[Epoch 26] ogbg-molsider: 0.568976 test loss: 0.535556
[Epoch 27; Iter    24/   36] train: loss: 0.5359101
[Epoch 27] ogbg-molsider: 0.538513 val loss: 0.505419
[Epoch 27] ogbg-molsider: 0.579197 test loss: 0.506058
[Epoch 28; Iter    18/   36] train: loss: 0.5061097
[Epoch 28] ogbg-molsider: 0.539222 val loss: 0.526247
[Epoch 28] ogbg-molsider: 0.561354 test loss: 0.524664
[Epoch 29; Iter    12/   36] train: loss: 0.5321787
[Epoch 29] ogbg-molsider: 0.530153 val loss: 0.484770
[Epoch 29] ogbg-molsider: 0.601861 test loss: 0.486836
[Epoch 30; Iter     6/   36] train: loss: 0.5222529
[Epoch 30; Iter    36/   36] train: loss: 0.5044625
[Epoch 30] ogbg-molsider: 0.553927 val loss: 0.483685
[Epoch 30] ogbg-molsider: 0.594992 test loss: 0.488485
[Epoch 31; Iter    30/   36] train: loss: 0.4743545
[Epoch 31] ogbg-molsider: 0.536872 val loss: 0.486157
[Epoch 31] ogbg-molsider: 0.592421 test loss: 0.489001
[Epoch 32; Iter    24/   36] train: loss: 0.5602058
[Epoch 32] ogbg-molsider: 0.516745 val loss: 0.496791
[Epoch 32] ogbg-molsider: 0.594260 test loss: 0.503521
[Epoch 33; Iter    18/   36] train: loss: 0.4930272
[Epoch 33] ogbg-molsider: 0.531369 val loss: 0.490261
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.2/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.2_5_26-05_09-18-58
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.2
logdir: runs/static_noise/3DInfomax/sider/noise=0.2
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6928248
[Epoch 1] ogbg-molsider: 0.459257 val loss: 0.693456
[Epoch 1] ogbg-molsider: 0.499568 test loss: 0.693632
[Epoch 2; Iter    24/   36] train: loss: 0.6935164
[Epoch 2] ogbg-molsider: 0.483476 val loss: 0.694857
[Epoch 2] ogbg-molsider: 0.496767 test loss: 0.696245
[Epoch 3; Iter    18/   36] train: loss: 0.6934440
[Epoch 3] ogbg-molsider: 0.495909 val loss: 0.696535
[Epoch 3] ogbg-molsider: 0.504464 test loss: 0.699099
[Epoch 4; Iter    12/   36] train: loss: 0.6934288
[Epoch 4] ogbg-molsider: 0.502680 val loss: 0.696768
[Epoch 4] ogbg-molsider: 0.502138 test loss: 0.699500
[Epoch 5; Iter     6/   36] train: loss: 0.6935386
[Epoch 5; Iter    36/   36] train: loss: 0.6927370
[Epoch 5] ogbg-molsider: 0.498125 val loss: 0.696617
[Epoch 5] ogbg-molsider: 0.505019 test loss: 0.699209
[Epoch 6; Iter    30/   36] train: loss: 0.6929440
[Epoch 6] ogbg-molsider: 0.501116 val loss: 0.696613
[Epoch 6] ogbg-molsider: 0.502329 test loss: 0.699399
[Epoch 7; Iter    24/   36] train: loss: 0.6930429
[Epoch 7] ogbg-molsider: 0.499434 val loss: 0.695902
[Epoch 7] ogbg-molsider: 0.502053 test loss: 0.698207
[Epoch 8; Iter    18/   36] train: loss: 0.6922445
[Epoch 8] ogbg-molsider: 0.500419 val loss: 0.696637
[Epoch 8] ogbg-molsider: 0.504454 test loss: 0.699529
[Epoch 9; Iter    12/   36] train: loss: 0.6933782
[Epoch 9] ogbg-molsider: 0.503179 val loss: 0.696495
[Epoch 9] ogbg-molsider: 0.506367 test loss: 0.699314
[Epoch 10; Iter     6/   36] train: loss: 0.6929272
[Epoch 10; Iter    36/   36] train: loss: 0.6929682
[Epoch 10] ogbg-molsider: 0.499270 val loss: 0.695931
[Epoch 10] ogbg-molsider: 0.503874 test loss: 0.698526
[Epoch 11; Iter    30/   36] train: loss: 0.6926226
[Epoch 11] ogbg-molsider: 0.497733 val loss: 0.695633
[Epoch 11] ogbg-molsider: 0.502171 test loss: 0.698177
[Epoch 12; Iter    24/   36] train: loss: 0.6925412
[Epoch 12] ogbg-molsider: 0.496577 val loss: 0.695232
[Epoch 12] ogbg-molsider: 0.502629 test loss: 0.697528
[Epoch 13; Iter    18/   36] train: loss: 0.6925061
[Epoch 13] ogbg-molsider: 0.501445 val loss: 0.695154
[Epoch 13] ogbg-molsider: 0.503839 test loss: 0.697496
[Epoch 14; Iter    12/   36] train: loss: 0.6923873
[Epoch 14] ogbg-molsider: 0.502870 val loss: 0.694956
[Epoch 14] ogbg-molsider: 0.501490 test loss: 0.697439
[Epoch 15; Iter     6/   36] train: loss: 0.6924819
[Epoch 15; Iter    36/   36] train: loss: 0.6915780
[Epoch 15] ogbg-molsider: 0.500682 val loss: 0.694497
[Epoch 15] ogbg-molsider: 0.503203 test loss: 0.696638
[Epoch 16; Iter    30/   36] train: loss: 0.6919468
[Epoch 16] ogbg-molsider: 0.502896 val loss: 0.695144
[Epoch 16] ogbg-molsider: 0.505385 test loss: 0.698003
[Epoch 17; Iter    24/   36] train: loss: 0.6915303
[Epoch 17] ogbg-molsider: 0.497352 val loss: 0.694410
[Epoch 17] ogbg-molsider: 0.503498 test loss: 0.696804
[Epoch 18; Iter    18/   36] train: loss: 0.6916140
[Epoch 18] ogbg-molsider: 0.499131 val loss: 0.694241
[Epoch 18] ogbg-molsider: 0.505042 test loss: 0.696677
[Epoch 19; Iter    12/   36] train: loss: 0.6914133
[Epoch 19] ogbg-molsider: 0.500003 val loss: 0.694506
[Epoch 19] ogbg-molsider: 0.504371 test loss: 0.697415
[Epoch 20; Iter     6/   36] train: loss: 0.6907696
[Epoch 20; Iter    36/   36] train: loss: 0.6863785
[Epoch 20] ogbg-molsider: 0.529023 val loss: 0.689772
[Epoch 20] ogbg-molsider: 0.514136 test loss: 0.694665
[Epoch 21; Iter    30/   36] train: loss: 0.6780154
[Epoch 21] ogbg-molsider: 0.525501 val loss: 0.648191
[Epoch 21] ogbg-molsider: 0.559667 test loss: 0.643925
[Epoch 22; Iter    24/   36] train: loss: 0.6614767
[Epoch 22] ogbg-molsider: 0.516726 val loss: 0.651891
[Epoch 22] ogbg-molsider: 0.576374 test loss: 0.653303
[Epoch 23; Iter    18/   36] train: loss: 0.6371563
[Epoch 23] ogbg-molsider: 0.542738 val loss: 0.626725
[Epoch 23] ogbg-molsider: 0.568071 test loss: 0.650143
[Epoch 24; Iter    12/   36] train: loss: 0.6305379
[Epoch 24] ogbg-molsider: 0.532768 val loss: 1.232321
[Epoch 24] ogbg-molsider: 0.496132 test loss: 1.697119
[Epoch 25; Iter     6/   36] train: loss: 0.5840739
[Epoch 25; Iter    36/   36] train: loss: 0.5700357
[Epoch 25] ogbg-molsider: 0.546043 val loss: 1.013954
[Epoch 25] ogbg-molsider: 0.508610 test loss: 1.436904
[Epoch 26; Iter    30/   36] train: loss: 0.5737430
[Epoch 26] ogbg-molsider: 0.545711 val loss: 1.277202
[Epoch 26] ogbg-molsider: 0.520946 test loss: 2.031139
[Epoch 27; Iter    24/   36] train: loss: 0.5588164
[Epoch 27] ogbg-molsider: 0.542883 val loss: 0.765323
[Epoch 27] ogbg-molsider: 0.525214 test loss: 0.855636
[Epoch 28; Iter    18/   36] train: loss: 0.5355807
[Epoch 28] ogbg-molsider: 0.515642 val loss: 1.109176
[Epoch 28] ogbg-molsider: 0.558023 test loss: 1.527980
[Epoch 29; Iter    12/   36] train: loss: 0.5044332
[Epoch 29] ogbg-molsider: 0.537821 val loss: 1.344620
[Epoch 29] ogbg-molsider: 0.551758 test loss: 1.863066
[Epoch 30; Iter     6/   36] train: loss: 0.5008673
[Epoch 30; Iter    36/   36] train: loss: 0.4473494
[Epoch 30] ogbg-molsider: 0.508244 val loss: 1.400643
[Epoch 30] ogbg-molsider: 0.510072 test loss: 1.853627
[Epoch 31; Iter    30/   36] train: loss: 0.4942184
[Epoch 31] ogbg-molsider: 0.500650 val loss: 1.411025
[Epoch 31] ogbg-molsider: 0.544797 test loss: 1.880799
[Epoch 32; Iter    24/   36] train: loss: 0.4969082
[Epoch 32] ogbg-molsider: 0.510488 val loss: 1.826355
[Epoch 32] ogbg-molsider: 0.543259 test loss: 1.966891
[Epoch 33; Iter    18/   36] train: loss: 0.4881223
[Epoch 33] ogbg-molsider: 0.507448 val loss: 1.511839
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.2/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.2_4_26-05_09-18-59
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.2
logdir: runs/static_noise/3DInfomax/sider/noise=0.2
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6931133
[Epoch 1] ogbg-molsider: 0.511721 val loss: 0.693848
[Epoch 1] ogbg-molsider: 0.522788 test loss: 0.694243
[Epoch 2; Iter    24/   36] train: loss: 0.6938033
[Epoch 2] ogbg-molsider: 0.502248 val loss: 0.696264
[Epoch 2] ogbg-molsider: 0.521288 test loss: 0.698068
[Epoch 3; Iter    18/   36] train: loss: 0.6932374
[Epoch 3] ogbg-molsider: 0.499395 val loss: 0.697558
[Epoch 3] ogbg-molsider: 0.505518 test loss: 0.700069
[Epoch 4; Iter    12/   36] train: loss: 0.6932516
[Epoch 4] ogbg-molsider: 0.497403 val loss: 0.697915
[Epoch 4] ogbg-molsider: 0.509050 test loss: 0.700684
[Epoch 5; Iter     6/   36] train: loss: 0.6934434
[Epoch 5; Iter    36/   36] train: loss: 0.6937398
[Epoch 5] ogbg-molsider: 0.490758 val loss: 0.698275
[Epoch 5] ogbg-molsider: 0.507772 test loss: 0.701186
[Epoch 6; Iter    30/   36] train: loss: 0.6927674
[Epoch 6] ogbg-molsider: 0.492670 val loss: 0.697299
[Epoch 6] ogbg-molsider: 0.507751 test loss: 0.699868
[Epoch 7; Iter    24/   36] train: loss: 0.6929076
[Epoch 7] ogbg-molsider: 0.495699 val loss: 0.697269
[Epoch 7] ogbg-molsider: 0.508611 test loss: 0.699865
[Epoch 8; Iter    18/   36] train: loss: 0.6929229
[Epoch 8] ogbg-molsider: 0.496544 val loss: 0.697469
[Epoch 8] ogbg-molsider: 0.510863 test loss: 0.700182
[Epoch 9; Iter    12/   36] train: loss: 0.6929188
[Epoch 9] ogbg-molsider: 0.487361 val loss: 0.697474
[Epoch 9] ogbg-molsider: 0.506845 test loss: 0.700186
[Epoch 10; Iter     6/   36] train: loss: 0.6929091
[Epoch 10; Iter    36/   36] train: loss: 0.6922820
[Epoch 10] ogbg-molsider: 0.485735 val loss: 0.697472
[Epoch 10] ogbg-molsider: 0.509801 test loss: 0.700278
[Epoch 11; Iter    30/   36] train: loss: 0.6924179
[Epoch 11] ogbg-molsider: 0.489650 val loss: 0.696836
[Epoch 11] ogbg-molsider: 0.508455 test loss: 0.699408
[Epoch 12; Iter    24/   36] train: loss: 0.6923572
[Epoch 12] ogbg-molsider: 0.496304 val loss: 0.696864
[Epoch 12] ogbg-molsider: 0.510046 test loss: 0.699578
[Epoch 13; Iter    18/   36] train: loss: 0.6919404
[Epoch 13] ogbg-molsider: 0.497624 val loss: 0.696390
[Epoch 13] ogbg-molsider: 0.511577 test loss: 0.698867
[Epoch 14; Iter    12/   36] train: loss: 0.6925089
[Epoch 14] ogbg-molsider: 0.496220 val loss: 0.696319
[Epoch 14] ogbg-molsider: 0.510753 test loss: 0.698898
[Epoch 15; Iter     6/   36] train: loss: 0.6922907
[Epoch 15; Iter    36/   36] train: loss: 0.6912733
[Epoch 15] ogbg-molsider: 0.495096 val loss: 0.696207
[Epoch 15] ogbg-molsider: 0.511720 test loss: 0.698778
[Epoch 16; Iter    30/   36] train: loss: 0.6908805
[Epoch 16] ogbg-molsider: 0.495964 val loss: 0.696651
[Epoch 16] ogbg-molsider: 0.512444 test loss: 0.699678
[Epoch 17; Iter    24/   36] train: loss: 0.6923257
[Epoch 17] ogbg-molsider: 0.497297 val loss: 0.695720
[Epoch 17] ogbg-molsider: 0.511631 test loss: 0.698334
[Epoch 18; Iter    18/   36] train: loss: 0.6910228
[Epoch 18] ogbg-molsider: 0.484579 val loss: 0.695399
[Epoch 18] ogbg-molsider: 0.511277 test loss: 0.697813
[Epoch 19; Iter    12/   36] train: loss: 0.6914759
[Epoch 19] ogbg-molsider: 0.495343 val loss: 0.695002
[Epoch 19] ogbg-molsider: 0.512872 test loss: 0.697420
[Epoch 20; Iter     6/   36] train: loss: 0.6906121
[Epoch 20; Iter    36/   36] train: loss: 0.6864509
[Epoch 20] ogbg-molsider: 0.525670 val loss: 0.716476
[Epoch 20] ogbg-molsider: 0.502148 test loss: 0.731416
[Epoch 21; Iter    30/   36] train: loss: 0.6778731
[Epoch 21] ogbg-molsider: 0.527056 val loss: 0.688124
[Epoch 21] ogbg-molsider: 0.528210 test loss: 0.698832
[Epoch 22; Iter    24/   36] train: loss: 0.6610533
[Epoch 22] ogbg-molsider: 0.561103 val loss: 0.658703
[Epoch 22] ogbg-molsider: 0.529731 test loss: 0.679629
[Epoch 23; Iter    18/   36] train: loss: 0.6378154
[Epoch 23] ogbg-molsider: 0.510072 val loss: 0.609427
[Epoch 23] ogbg-molsider: 0.590026 test loss: 0.617445
[Epoch 24; Iter    12/   36] train: loss: 0.6128870
[Epoch 24] ogbg-molsider: 0.491796 val loss: 0.565387
[Epoch 24] ogbg-molsider: 0.560822 test loss: 0.567698
[Epoch 25; Iter     6/   36] train: loss: 0.5923198
[Epoch 25; Iter    36/   36] train: loss: 0.5764219
[Epoch 25] ogbg-molsider: 0.500540 val loss: 0.653018
[Epoch 25] ogbg-molsider: 0.530644 test loss: 0.669834
[Epoch 26; Iter    30/   36] train: loss: 0.5475617
[Epoch 26] ogbg-molsider: 0.490394 val loss: 0.866860
[Epoch 26] ogbg-molsider: 0.550845 test loss: 0.895911
[Epoch 27; Iter    24/   36] train: loss: 0.5390154
[Epoch 27] ogbg-molsider: 0.492376 val loss: 0.985559
[Epoch 27] ogbg-molsider: 0.544530 test loss: 1.044889
[Epoch 28; Iter    18/   36] train: loss: 0.5119146
[Epoch 28] ogbg-molsider: 0.486475 val loss: 1.099111
[Epoch 28] ogbg-molsider: 0.550318 test loss: 1.179365
[Epoch 29; Iter    12/   36] train: loss: 0.5399823
[Epoch 29] ogbg-molsider: 0.472101 val loss: 0.662249
[Epoch 29] ogbg-molsider: 0.569648 test loss: 0.651920
[Epoch 30; Iter     6/   36] train: loss: 0.5212972
[Epoch 30; Iter    36/   36] train: loss: 0.5206855
[Epoch 30] ogbg-molsider: 0.489860 val loss: 0.663578
[Epoch 30] ogbg-molsider: 0.558327 test loss: 0.572376
[Epoch 31; Iter    30/   36] train: loss: 0.4763523
[Epoch 31] ogbg-molsider: 0.492185 val loss: 0.597091
[Epoch 31] ogbg-molsider: 0.563561 test loss: 0.562061
[Epoch 32; Iter    24/   36] train: loss: 0.5503168
[Epoch 32] ogbg-molsider: 0.519436 val loss: 0.585047
[Epoch 32] ogbg-molsider: 0.568279 test loss: 0.549990
[Epoch 33; Iter    18/   36] train: loss: 0.4992010
[Epoch 33] ogbg-molsider: 0.498370 val loss: 0.579612
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.05/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.05_6_26-05_09-18-49
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.05
logdir: runs/static_noise/3DInfomax/sider/noise=0.05
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6928153
[Epoch 1] ogbg-molsider: 0.507830 val loss: 0.692848
[Epoch 1] ogbg-molsider: 0.503949 test loss: 0.692613
[Epoch 2; Iter    24/   36] train: loss: 0.6935326
[Epoch 2] ogbg-molsider: 0.503551 val loss: 0.692478
[Epoch 2] ogbg-molsider: 0.507607 test loss: 0.691922
[Epoch 3; Iter    18/   36] train: loss: 0.6938927
[Epoch 3] ogbg-molsider: 0.504629 val loss: 0.692294
[Epoch 3] ogbg-molsider: 0.502262 test loss: 0.691677
[Epoch 4; Iter    12/   36] train: loss: 0.6930543
[Epoch 4] ogbg-molsider: 0.503249 val loss: 0.692272
[Epoch 4] ogbg-molsider: 0.503950 test loss: 0.691660
[Epoch 5; Iter     6/   36] train: loss: 0.6931473
[Epoch 5; Iter    36/   36] train: loss: 0.6935722
[Epoch 5] ogbg-molsider: 0.505002 val loss: 0.692269
[Epoch 5] ogbg-molsider: 0.502065 test loss: 0.691698
[Epoch 6; Iter    30/   36] train: loss: 0.6931900
[Epoch 6] ogbg-molsider: 0.503649 val loss: 0.692108
[Epoch 6] ogbg-molsider: 0.503983 test loss: 0.691519
[Epoch 7; Iter    24/   36] train: loss: 0.6930015
[Epoch 7] ogbg-molsider: 0.503918 val loss: 0.692075
[Epoch 7] ogbg-molsider: 0.502374 test loss: 0.691532
[Epoch 8; Iter    18/   36] train: loss: 0.6928385
[Epoch 8] ogbg-molsider: 0.502720 val loss: 0.691936
[Epoch 8] ogbg-molsider: 0.503023 test loss: 0.691329
[Epoch 9; Iter    12/   36] train: loss: 0.6927401
[Epoch 9] ogbg-molsider: 0.504687 val loss: 0.691832
[Epoch 9] ogbg-molsider: 0.504545 test loss: 0.691193
[Epoch 10; Iter     6/   36] train: loss: 0.6922946
[Epoch 10; Iter    36/   36] train: loss: 0.6930475
[Epoch 10] ogbg-molsider: 0.504992 val loss: 0.691773
[Epoch 10] ogbg-molsider: 0.501402 test loss: 0.691230
[Epoch 11; Iter    30/   36] train: loss: 0.6932164
[Epoch 11] ogbg-molsider: 0.505223 val loss: 0.691669
[Epoch 11] ogbg-molsider: 0.505219 test loss: 0.691103
[Epoch 12; Iter    24/   36] train: loss: 0.6924754
[Epoch 12] ogbg-molsider: 0.503703 val loss: 0.691596
[Epoch 12] ogbg-molsider: 0.503397 test loss: 0.691053
[Epoch 13; Iter    18/   36] train: loss: 0.6920908
[Epoch 13] ogbg-molsider: 0.506495 val loss: 0.691482
[Epoch 13] ogbg-molsider: 0.504642 test loss: 0.690926
[Epoch 14; Iter    12/   36] train: loss: 0.6926080
[Epoch 14] ogbg-molsider: 0.505079 val loss: 0.691208
[Epoch 14] ogbg-molsider: 0.505592 test loss: 0.690600
[Epoch 15; Iter     6/   36] train: loss: 0.6922798
[Epoch 15; Iter    36/   36] train: loss: 0.6922729
[Epoch 15] ogbg-molsider: 0.504834 val loss: 0.690982
[Epoch 15] ogbg-molsider: 0.501503 test loss: 0.690445
[Epoch 16; Iter    30/   36] train: loss: 0.6917716
[Epoch 16] ogbg-molsider: 0.506058 val loss: 0.690958
[Epoch 16] ogbg-molsider: 0.505804 test loss: 0.690396
[Epoch 17; Iter    24/   36] train: loss: 0.6918719
[Epoch 17] ogbg-molsider: 0.507529 val loss: 0.690654
[Epoch 17] ogbg-molsider: 0.503812 test loss: 0.690145
[Epoch 18; Iter    18/   36] train: loss: 0.6914080
[Epoch 18] ogbg-molsider: 0.504515 val loss: 0.690490
[Epoch 18] ogbg-molsider: 0.503388 test loss: 0.689958
[Epoch 19; Iter    12/   36] train: loss: 0.6912277
[Epoch 19] ogbg-molsider: 0.508781 val loss: 0.690239
[Epoch 19] ogbg-molsider: 0.501442 test loss: 0.689689
[Epoch 20; Iter     6/   36] train: loss: 0.6908963
[Epoch 20; Iter    36/   36] train: loss: 0.6875030
[Epoch 20] ogbg-molsider: 0.542569 val loss: 0.682132
[Epoch 20] ogbg-molsider: 0.557131 test loss: 0.681551
[Epoch 21; Iter    30/   36] train: loss: 0.6773440
[Epoch 21] ogbg-molsider: 0.567346 val loss: 0.661759
[Epoch 21] ogbg-molsider: 0.547974 test loss: 0.665734
[Epoch 22; Iter    24/   36] train: loss: 0.6668355
[Epoch 22] ogbg-molsider: 0.542948 val loss: 0.613325
[Epoch 22] ogbg-molsider: 0.573766 test loss: 0.620200
[Epoch 23; Iter    18/   36] train: loss: 0.6367980
[Epoch 23] ogbg-molsider: 0.533909 val loss: 0.634679
[Epoch 23] ogbg-molsider: 0.569069 test loss: 0.671396
[Epoch 24; Iter    12/   36] train: loss: 0.6174774
[Epoch 24] ogbg-molsider: 0.538260 val loss: 0.555730
[Epoch 24] ogbg-molsider: 0.600368 test loss: 0.555045
[Epoch 25; Iter     6/   36] train: loss: 0.5837026
[Epoch 25; Iter    36/   36] train: loss: 0.5832165
[Epoch 25] ogbg-molsider: 0.511757 val loss: 0.530531
[Epoch 25] ogbg-molsider: 0.576574 test loss: 0.533000
[Epoch 26; Iter    30/   36] train: loss: 0.5662754
[Epoch 26] ogbg-molsider: 0.560658 val loss: 0.538237
[Epoch 26] ogbg-molsider: 0.580332 test loss: 0.544252
[Epoch 27; Iter    24/   36] train: loss: 0.5212529
[Epoch 27] ogbg-molsider: 0.486102 val loss: 0.514763
[Epoch 27] ogbg-molsider: 0.589741 test loss: 0.516811
[Epoch 28; Iter    18/   36] train: loss: 0.4928878
[Epoch 28] ogbg-molsider: 0.568567 val loss: 0.483982
[Epoch 28] ogbg-molsider: 0.592283 test loss: 0.496022
[Epoch 29; Iter    12/   36] train: loss: 0.5366706
[Epoch 29] ogbg-molsider: 0.516517 val loss: 0.491771
[Epoch 29] ogbg-molsider: 0.585629 test loss: 0.501570
[Epoch 30; Iter     6/   36] train: loss: 0.5095690
[Epoch 30; Iter    36/   36] train: loss: 0.4854873
[Epoch 30] ogbg-molsider: 0.530326 val loss: 0.498450
[Epoch 30] ogbg-molsider: 0.581474 test loss: 0.511436
[Epoch 31; Iter    30/   36] train: loss: 0.5053617
[Epoch 31] ogbg-molsider: 0.516143 val loss: 0.489906
[Epoch 31] ogbg-molsider: 0.600674 test loss: 0.491423
[Epoch 32; Iter    24/   36] train: loss: 0.4939020
[Epoch 32] ogbg-molsider: 0.558603 val loss: 0.492485
[Epoch 32] ogbg-molsider: 0.571429 test loss: 0.499793
[Epoch 33; Iter    18/   36] train: loss: 0.5208877
[Epoch 33] ogbg-molsider: 0.526136 val loss: 0.490899
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.2/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.2_6_26-05_09-19-01
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.2
logdir: runs/static_noise/3DInfomax/sider/noise=0.2
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932980
[Epoch 1] ogbg-molsider: 0.515209 val loss: 0.692553
[Epoch 1] ogbg-molsider: 0.491625 test loss: 0.692236
[Epoch 2; Iter    24/   36] train: loss: 0.6932109
[Epoch 2] ogbg-molsider: 0.517881 val loss: 0.691566
[Epoch 2] ogbg-molsider: 0.492084 test loss: 0.690812
[Epoch 3; Iter    18/   36] train: loss: 0.6937991
[Epoch 3] ogbg-molsider: 0.519823 val loss: 0.691111
[Epoch 3] ogbg-molsider: 0.497948 test loss: 0.690190
[Epoch 4; Iter    12/   36] train: loss: 0.6932384
[Epoch 4] ogbg-molsider: 0.521054 val loss: 0.691135
[Epoch 4] ogbg-molsider: 0.499211 test loss: 0.690228
[Epoch 5; Iter     6/   36] train: loss: 0.6930052
[Epoch 5; Iter    36/   36] train: loss: 0.6933145
[Epoch 5] ogbg-molsider: 0.521534 val loss: 0.691170
[Epoch 5] ogbg-molsider: 0.500821 test loss: 0.690207
[Epoch 6; Iter    30/   36] train: loss: 0.6932765
[Epoch 6] ogbg-molsider: 0.520659 val loss: 0.691118
[Epoch 6] ogbg-molsider: 0.501393 test loss: 0.690183
[Epoch 7; Iter    24/   36] train: loss: 0.6927797
[Epoch 7] ogbg-molsider: 0.517859 val loss: 0.690806
[Epoch 7] ogbg-molsider: 0.500199 test loss: 0.689871
[Epoch 8; Iter    18/   36] train: loss: 0.6931182
[Epoch 8] ogbg-molsider: 0.518155 val loss: 0.690674
[Epoch 8] ogbg-molsider: 0.498508 test loss: 0.689693
[Epoch 9; Iter    12/   36] train: loss: 0.6926914
[Epoch 9] ogbg-molsider: 0.519970 val loss: 0.690816
[Epoch 9] ogbg-molsider: 0.497967 test loss: 0.689986
[Epoch 10; Iter     6/   36] train: loss: 0.6922390
[Epoch 10; Iter    36/   36] train: loss: 0.6928812
[Epoch 10] ogbg-molsider: 0.522258 val loss: 0.690952
[Epoch 10] ogbg-molsider: 0.501790 test loss: 0.690070
[Epoch 11; Iter    30/   36] train: loss: 0.6930474
[Epoch 11] ogbg-molsider: 0.521111 val loss: 0.690653
[Epoch 11] ogbg-molsider: 0.500198 test loss: 0.689807
[Epoch 12; Iter    24/   36] train: loss: 0.6925179
[Epoch 12] ogbg-molsider: 0.520828 val loss: 0.690612
[Epoch 12] ogbg-molsider: 0.499586 test loss: 0.689786
[Epoch 13; Iter    18/   36] train: loss: 0.6921324
[Epoch 13] ogbg-molsider: 0.521870 val loss: 0.690294
[Epoch 13] ogbg-molsider: 0.500179 test loss: 0.689441
[Epoch 14; Iter    12/   36] train: loss: 0.6923622
[Epoch 14] ogbg-molsider: 0.520358 val loss: 0.689960
[Epoch 14] ogbg-molsider: 0.501722 test loss: 0.689097
[Epoch 15; Iter     6/   36] train: loss: 0.6918231
[Epoch 15; Iter    36/   36] train: loss: 0.6928574
[Epoch 15] ogbg-molsider: 0.521472 val loss: 0.689598
[Epoch 15] ogbg-molsider: 0.502673 test loss: 0.688696
[Epoch 16; Iter    30/   36] train: loss: 0.6918401
[Epoch 16] ogbg-molsider: 0.521825 val loss: 0.690006
[Epoch 16] ogbg-molsider: 0.502932 test loss: 0.689166
[Epoch 17; Iter    24/   36] train: loss: 0.6918734
[Epoch 17] ogbg-molsider: 0.519228 val loss: 0.689668
[Epoch 17] ogbg-molsider: 0.498732 test loss: 0.688761
[Epoch 18; Iter    18/   36] train: loss: 0.6915415
[Epoch 18] ogbg-molsider: 0.520800 val loss: 0.689401
[Epoch 18] ogbg-molsider: 0.498997 test loss: 0.688625
[Epoch 19; Iter    12/   36] train: loss: 0.6916618
[Epoch 19] ogbg-molsider: 0.521373 val loss: 0.689285
[Epoch 19] ogbg-molsider: 0.499118 test loss: 0.688326
[Epoch 20; Iter     6/   36] train: loss: 0.6910757
[Epoch 20; Iter    36/   36] train: loss: 0.6879514
[Epoch 20] ogbg-molsider: 0.545479 val loss: 0.680298
[Epoch 20] ogbg-molsider: 0.510881 test loss: 0.679910
[Epoch 21; Iter    30/   36] train: loss: 0.6768051
[Epoch 21] ogbg-molsider: 0.525934 val loss: 0.654677
[Epoch 21] ogbg-molsider: 0.567931 test loss: 0.653393
[Epoch 22; Iter    24/   36] train: loss: 0.6659249
[Epoch 22] ogbg-molsider: 0.512404 val loss: 0.628449
[Epoch 22] ogbg-molsider: 0.571534 test loss: 0.629627
[Epoch 23; Iter    18/   36] train: loss: 0.6422477
[Epoch 23] ogbg-molsider: 0.507168 val loss: 0.627019
[Epoch 23] ogbg-molsider: 0.547911 test loss: 0.627846
[Epoch 24; Iter    12/   36] train: loss: 0.6186118
[Epoch 24] ogbg-molsider: 0.535028 val loss: 0.660330
[Epoch 24] ogbg-molsider: 0.494578 test loss: 0.725163
[Epoch 25; Iter     6/   36] train: loss: 0.5901265
[Epoch 25; Iter    36/   36] train: loss: 0.5845266
[Epoch 25] ogbg-molsider: 0.536229 val loss: 0.616341
[Epoch 25] ogbg-molsider: 0.503826 test loss: 0.690211
[Epoch 26; Iter    30/   36] train: loss: 0.5603593
[Epoch 26] ogbg-molsider: 0.509492 val loss: 0.693062
[Epoch 26] ogbg-molsider: 0.494621 test loss: 1.004732
[Epoch 27; Iter    24/   36] train: loss: 0.5213052
[Epoch 27] ogbg-molsider: 0.484016 val loss: 0.649428
[Epoch 27] ogbg-molsider: 0.526997 test loss: 1.054472
[Epoch 28; Iter    18/   36] train: loss: 0.4891178
[Epoch 28] ogbg-molsider: 0.538314 val loss: 0.844870
[Epoch 28] ogbg-molsider: 0.521547 test loss: 1.664581
[Epoch 29; Iter    12/   36] train: loss: 0.5475665
[Epoch 29] ogbg-molsider: 0.525276 val loss: 0.662732
[Epoch 29] ogbg-molsider: 0.524141 test loss: 1.262306
[Epoch 30; Iter     6/   36] train: loss: 0.5042314
[Epoch 30; Iter    36/   36] train: loss: 0.4763353
[Epoch 30] ogbg-molsider: 0.507412 val loss: 0.634444
[Epoch 30] ogbg-molsider: 0.545138 test loss: 1.335917
[Epoch 31; Iter    30/   36] train: loss: 0.5002421
[Epoch 31] ogbg-molsider: 0.527431 val loss: 1.112861
[Epoch 31] ogbg-molsider: 0.513585 test loss: 2.269232
[Epoch 32; Iter    24/   36] train: loss: 0.5120279
[Epoch 32] ogbg-molsider: 0.513053 val loss: 1.304817
[Epoch 32] ogbg-molsider: 0.515346 test loss: 2.875493
[Epoch 33; Iter    18/   36] train: loss: 0.5039674
[Epoch 33] ogbg-molsider: 0.522412 val loss: 1.192342
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.1/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.1_6_26-05_09-18-55
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.1
logdir: runs/static_noise/3DInfomax/sider/noise=0.1
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6930931
[Epoch 1] ogbg-molsider: 0.512275 val loss: 0.692719
[Epoch 1] ogbg-molsider: 0.494761 test loss: 0.692475
[Epoch 2; Iter    24/   36] train: loss: 0.6937984
[Epoch 2] ogbg-molsider: 0.509327 val loss: 0.692105
[Epoch 2] ogbg-molsider: 0.500722 test loss: 0.691560
[Epoch 3; Iter    18/   36] train: loss: 0.6939601
[Epoch 3] ogbg-molsider: 0.509889 val loss: 0.691571
[Epoch 3] ogbg-molsider: 0.495705 test loss: 0.690984
[Epoch 4; Iter    12/   36] train: loss: 0.6929019
[Epoch 4] ogbg-molsider: 0.507839 val loss: 0.691670
[Epoch 4] ogbg-molsider: 0.497887 test loss: 0.691185
[Epoch 5; Iter     6/   36] train: loss: 0.6931937
[Epoch 5; Iter    36/   36] train: loss: 0.6931362
[Epoch 5] ogbg-molsider: 0.510651 val loss: 0.691759
[Epoch 5] ogbg-molsider: 0.496710 test loss: 0.691267
[Epoch 6; Iter    30/   36] train: loss: 0.6932941
[Epoch 6] ogbg-molsider: 0.509136 val loss: 0.691639
[Epoch 6] ogbg-molsider: 0.497974 test loss: 0.691134
[Epoch 7; Iter    24/   36] train: loss: 0.6925941
[Epoch 7] ogbg-molsider: 0.509937 val loss: 0.691524
[Epoch 7] ogbg-molsider: 0.500725 test loss: 0.691036
[Epoch 8; Iter    18/   36] train: loss: 0.6926714
[Epoch 8] ogbg-molsider: 0.510750 val loss: 0.691435
[Epoch 8] ogbg-molsider: 0.496007 test loss: 0.690914
[Epoch 9; Iter    12/   36] train: loss: 0.6928417
[Epoch 9] ogbg-molsider: 0.508986 val loss: 0.691280
[Epoch 9] ogbg-molsider: 0.495733 test loss: 0.690736
[Epoch 10; Iter     6/   36] train: loss: 0.6922811
[Epoch 10; Iter    36/   36] train: loss: 0.6935972
[Epoch 10] ogbg-molsider: 0.510552 val loss: 0.691222
[Epoch 10] ogbg-molsider: 0.498272 test loss: 0.690750
[Epoch 11; Iter    30/   36] train: loss: 0.6932088
[Epoch 11] ogbg-molsider: 0.512340 val loss: 0.691220
[Epoch 11] ogbg-molsider: 0.497605 test loss: 0.690740
[Epoch 12; Iter    24/   36] train: loss: 0.6924059
[Epoch 12] ogbg-molsider: 0.511140 val loss: 0.691015
[Epoch 12] ogbg-molsider: 0.496694 test loss: 0.690538
[Epoch 13; Iter    18/   36] train: loss: 0.6923255
[Epoch 13] ogbg-molsider: 0.513144 val loss: 0.690906
[Epoch 13] ogbg-molsider: 0.495325 test loss: 0.690371
[Epoch 14; Iter    12/   36] train: loss: 0.6926441
[Epoch 14] ogbg-molsider: 0.508357 val loss: 0.690540
[Epoch 14] ogbg-molsider: 0.497297 test loss: 0.689918
[Epoch 15; Iter     6/   36] train: loss: 0.6919777
[Epoch 15; Iter    36/   36] train: loss: 0.6926227
[Epoch 15] ogbg-molsider: 0.510317 val loss: 0.690401
[Epoch 15] ogbg-molsider: 0.494587 test loss: 0.689882
[Epoch 16; Iter    30/   36] train: loss: 0.6915761
[Epoch 16] ogbg-molsider: 0.512494 val loss: 0.690366
[Epoch 16] ogbg-molsider: 0.497295 test loss: 0.689885
[Epoch 17; Iter    24/   36] train: loss: 0.6916611
[Epoch 17] ogbg-molsider: 0.515650 val loss: 0.690081
[Epoch 17] ogbg-molsider: 0.496018 test loss: 0.689654
[Epoch 18; Iter    18/   36] train: loss: 0.6918912
[Epoch 18] ogbg-molsider: 0.513344 val loss: 0.689880
[Epoch 18] ogbg-molsider: 0.495113 test loss: 0.689428
[Epoch 19; Iter    12/   36] train: loss: 0.6914663
[Epoch 19] ogbg-molsider: 0.517827 val loss: 0.689814
[Epoch 19] ogbg-molsider: 0.493932 test loss: 0.689330
[Epoch 20; Iter     6/   36] train: loss: 0.6908199
[Epoch 20; Iter    36/   36] train: loss: 0.6877923
[Epoch 20] ogbg-molsider: 0.558123 val loss: 0.683004
[Epoch 20] ogbg-molsider: 0.557297 test loss: 0.683165
[Epoch 21; Iter    30/   36] train: loss: 0.6765110
[Epoch 21] ogbg-molsider: 0.547412 val loss: 0.666646
[Epoch 21] ogbg-molsider: 0.557588 test loss: 0.669825
[Epoch 22; Iter    24/   36] train: loss: 0.6661185
[Epoch 22] ogbg-molsider: 0.530166 val loss: 0.624379
[Epoch 22] ogbg-molsider: 0.586715 test loss: 0.625837
[Epoch 23; Iter    18/   36] train: loss: 0.6368623
[Epoch 23] ogbg-molsider: 0.524235 val loss: 0.592267
[Epoch 23] ogbg-molsider: 0.566806 test loss: 0.584200
[Epoch 24; Iter    12/   36] train: loss: 0.6216599
[Epoch 24] ogbg-molsider: 0.527237 val loss: 0.557457
[Epoch 24] ogbg-molsider: 0.572281 test loss: 0.553335
[Epoch 25; Iter     6/   36] train: loss: 0.5864872
[Epoch 25; Iter    36/   36] train: loss: 0.5887610
[Epoch 25] ogbg-molsider: 0.529902 val loss: 0.548255
[Epoch 25] ogbg-molsider: 0.573201 test loss: 0.548545
[Epoch 26; Iter    30/   36] train: loss: 0.5653443
[Epoch 26] ogbg-molsider: 0.546873 val loss: 0.547629
[Epoch 26] ogbg-molsider: 0.545753 test loss: 0.562459
[Epoch 27; Iter    24/   36] train: loss: 0.5222275
[Epoch 27] ogbg-molsider: 0.537682 val loss: 0.559134
[Epoch 27] ogbg-molsider: 0.569401 test loss: 0.627144
[Epoch 28; Iter    18/   36] train: loss: 0.4883961
[Epoch 28] ogbg-molsider: 0.557681 val loss: 0.490576
[Epoch 28] ogbg-molsider: 0.560866 test loss: 0.516178
[Epoch 29; Iter    12/   36] train: loss: 0.5409387
[Epoch 29] ogbg-molsider: 0.528028 val loss: 0.490717
[Epoch 29] ogbg-molsider: 0.580935 test loss: 0.497254
[Epoch 30; Iter     6/   36] train: loss: 0.5115056
[Epoch 30; Iter    36/   36] train: loss: 0.4798725
[Epoch 30] ogbg-molsider: 0.523090 val loss: 0.484823
[Epoch 30] ogbg-molsider: 0.584199 test loss: 0.495377
[Epoch 31; Iter    30/   36] train: loss: 0.4977625
[Epoch 31] ogbg-molsider: 0.549884 val loss: 0.494769
[Epoch 31] ogbg-molsider: 0.562168 test loss: 0.518377
[Epoch 32; Iter    24/   36] train: loss: 0.5063612
[Epoch 32] ogbg-molsider: 0.555464 val loss: 0.528726
[Epoch 32] ogbg-molsider: 0.527480 test loss: 0.593464
[Epoch 33; Iter    18/   36] train: loss: 0.5177014
[Epoch 33] ogbg-molsider: 0.522616 val loss: 0.487813
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.0/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.0_5_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.0
logdir: runs/static_noise/3DInfomax/sider/noise=0.0
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6936331
[Epoch 1] ogbg-molsider: 0.483690 val loss: 0.693155
[Epoch 1] ogbg-molsider: 0.503371 test loss: 0.693228
[Epoch 2; Iter    24/   36] train: loss: 0.6937493
[Epoch 2] ogbg-molsider: 0.499742 val loss: 0.693115
[Epoch 2] ogbg-molsider: 0.498544 test loss: 0.693428
[Epoch 3; Iter    18/   36] train: loss: 0.6935792
[Epoch 3] ogbg-molsider: 0.503120 val loss: 0.693016
[Epoch 3] ogbg-molsider: 0.498856 test loss: 0.693424
[Epoch 4; Iter    12/   36] train: loss: 0.6933743
[Epoch 4] ogbg-molsider: 0.506100 val loss: 0.693076
[Epoch 4] ogbg-molsider: 0.498171 test loss: 0.693434
[Epoch 5; Iter     6/   36] train: loss: 0.6931689
[Epoch 5; Iter    36/   36] train: loss: 0.6929625
[Epoch 5] ogbg-molsider: 0.500667 val loss: 0.693055
[Epoch 5] ogbg-molsider: 0.497338 test loss: 0.693482
[Epoch 6; Iter    30/   36] train: loss: 0.6932262
[Epoch 6] ogbg-molsider: 0.503284 val loss: 0.692926
[Epoch 6] ogbg-molsider: 0.502133 test loss: 0.693212
[Epoch 7; Iter    24/   36] train: loss: 0.6932282
[Epoch 7] ogbg-molsider: 0.507675 val loss: 0.692746
[Epoch 7] ogbg-molsider: 0.500213 test loss: 0.693114
[Epoch 8; Iter    18/   36] train: loss: 0.6924952
[Epoch 8] ogbg-molsider: 0.500417 val loss: 0.692816
[Epoch 8] ogbg-molsider: 0.502040 test loss: 0.693236
[Epoch 9; Iter    12/   36] train: loss: 0.6931766
[Epoch 9] ogbg-molsider: 0.500951 val loss: 0.692719
[Epoch 9] ogbg-molsider: 0.500490 test loss: 0.693116
[Epoch 10; Iter     6/   36] train: loss: 0.6926487
[Epoch 10; Iter    36/   36] train: loss: 0.6925964
[Epoch 10] ogbg-molsider: 0.503446 val loss: 0.692483
[Epoch 10] ogbg-molsider: 0.500008 test loss: 0.692806
[Epoch 11; Iter    30/   36] train: loss: 0.6926727
[Epoch 11] ogbg-molsider: 0.508322 val loss: 0.692318
[Epoch 11] ogbg-molsider: 0.500958 test loss: 0.692698
[Epoch 12; Iter    24/   36] train: loss: 0.6924053
[Epoch 12] ogbg-molsider: 0.505031 val loss: 0.692115
[Epoch 12] ogbg-molsider: 0.501400 test loss: 0.692495
[Epoch 13; Iter    18/   36] train: loss: 0.6925083
[Epoch 13] ogbg-molsider: 0.506162 val loss: 0.692111
[Epoch 13] ogbg-molsider: 0.500341 test loss: 0.692417
[Epoch 14; Iter    12/   36] train: loss: 0.6922555
[Epoch 14] ogbg-molsider: 0.507015 val loss: 0.691930
[Epoch 14] ogbg-molsider: 0.503859 test loss: 0.692312
[Epoch 15; Iter     6/   36] train: loss: 0.6926001
[Epoch 15; Iter    36/   36] train: loss: 0.6916689
[Epoch 15] ogbg-molsider: 0.510460 val loss: 0.691762
[Epoch 15] ogbg-molsider: 0.501479 test loss: 0.692044
[Epoch 16; Iter    30/   36] train: loss: 0.6923040
[Epoch 16] ogbg-molsider: 0.505530 val loss: 0.691555
[Epoch 16] ogbg-molsider: 0.502183 test loss: 0.691920
[Epoch 17; Iter    24/   36] train: loss: 0.6917722
[Epoch 17] ogbg-molsider: 0.509580 val loss: 0.691361
[Epoch 17] ogbg-molsider: 0.500816 test loss: 0.691727
[Epoch 18; Iter    18/   36] train: loss: 0.6916637
[Epoch 18] ogbg-molsider: 0.504801 val loss: 0.691030
[Epoch 18] ogbg-molsider: 0.504960 test loss: 0.691314
[Epoch 19; Iter    12/   36] train: loss: 0.6917031
[Epoch 19] ogbg-molsider: 0.507032 val loss: 0.690880
[Epoch 19] ogbg-molsider: 0.505516 test loss: 0.691205
[Epoch 20; Iter     6/   36] train: loss: 0.6909327
[Epoch 20; Iter    36/   36] train: loss: 0.6863186
[Epoch 20] ogbg-molsider: 0.540658 val loss: 0.682393
[Epoch 20] ogbg-molsider: 0.563531 test loss: 0.682920
[Epoch 21; Iter    30/   36] train: loss: 0.6790473
[Epoch 21] ogbg-molsider: 0.532132 val loss: 0.657990
[Epoch 21] ogbg-molsider: 0.598657 test loss: 0.656422
[Epoch 22; Iter    24/   36] train: loss: 0.6603025
[Epoch 22] ogbg-molsider: 0.524145 val loss: 0.635209
[Epoch 22] ogbg-molsider: 0.588564 test loss: 0.631409
[Epoch 23; Iter    18/   36] train: loss: 0.6343783
[Epoch 23] ogbg-molsider: 0.524446 val loss: 0.624035
[Epoch 23] ogbg-molsider: 0.592573 test loss: 0.621016
[Epoch 24; Iter    12/   36] train: loss: 0.6304240
[Epoch 24] ogbg-molsider: 0.518792 val loss: 0.584454
[Epoch 24] ogbg-molsider: 0.605914 test loss: 0.579633
[Epoch 25; Iter     6/   36] train: loss: 0.5884835
[Epoch 25; Iter    36/   36] train: loss: 0.5748572
[Epoch 25] ogbg-molsider: 0.546613 val loss: 0.556347
[Epoch 25] ogbg-molsider: 0.576834 test loss: 0.557699
[Epoch 26; Iter    30/   36] train: loss: 0.5646052
[Epoch 26] ogbg-molsider: 0.541455 val loss: 0.516047
[Epoch 26] ogbg-molsider: 0.616528 test loss: 0.513307
[Epoch 27; Iter    24/   36] train: loss: 0.5343263
[Epoch 27] ogbg-molsider: 0.564166 val loss: 0.500281
[Epoch 27] ogbg-molsider: 0.584414 test loss: 0.506693
[Epoch 28; Iter    18/   36] train: loss: 0.5266917
[Epoch 28] ogbg-molsider: 0.556186 val loss: 0.506421
[Epoch 28] ogbg-molsider: 0.589240 test loss: 0.511249
[Epoch 29; Iter    12/   36] train: loss: 0.4922637
[Epoch 29] ogbg-molsider: 0.577750 val loss: 0.483618
[Epoch 29] ogbg-molsider: 0.586111 test loss: 0.496318
[Epoch 30; Iter     6/   36] train: loss: 0.4977187
[Epoch 30; Iter    36/   36] train: loss: 0.4504667
[Epoch 30] ogbg-molsider: 0.572794 val loss: 0.481923
[Epoch 30] ogbg-molsider: 0.591383 test loss: 0.489832
[Epoch 31; Iter    30/   36] train: loss: 0.4858133
[Epoch 31] ogbg-molsider: 0.542017 val loss: 0.496543
[Epoch 31] ogbg-molsider: 0.603267 test loss: 0.500724
[Epoch 32; Iter    24/   36] train: loss: 0.4832959
[Epoch 32] ogbg-molsider: 0.552524 val loss: 0.489654
[Epoch 32] ogbg-molsider: 0.601478 test loss: 0.502903
[Epoch 33; Iter    18/   36] train: loss: 0.4870285
[Epoch 33] ogbg-molsider: 0.544017 val loss: 0.491450
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.0/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.0_6_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.0
logdir: runs/static_noise/3DInfomax/sider/noise=0.0
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6931961
[Epoch 1] ogbg-molsider: 0.490884 val loss: 0.693054
[Epoch 1] ogbg-molsider: 0.509410 test loss: 0.692827
[Epoch 2; Iter    24/   36] train: loss: 0.6936196
[Epoch 2] ogbg-molsider: 0.489924 val loss: 0.693133
[Epoch 2] ogbg-molsider: 0.516967 test loss: 0.692477
[Epoch 3; Iter    18/   36] train: loss: 0.6939726
[Epoch 3] ogbg-molsider: 0.491759 val loss: 0.693116
[Epoch 3] ogbg-molsider: 0.511385 test loss: 0.692419
[Epoch 4; Iter    12/   36] train: loss: 0.6926282
[Epoch 4] ogbg-molsider: 0.491560 val loss: 0.693117
[Epoch 4] ogbg-molsider: 0.513804 test loss: 0.692416
[Epoch 5; Iter     6/   36] train: loss: 0.6931393
[Epoch 5; Iter    36/   36] train: loss: 0.6932732
[Epoch 5] ogbg-molsider: 0.489966 val loss: 0.693072
[Epoch 5] ogbg-molsider: 0.509569 test loss: 0.692477
[Epoch 6; Iter    30/   36] train: loss: 0.6932129
[Epoch 6] ogbg-molsider: 0.492337 val loss: 0.692958
[Epoch 6] ogbg-molsider: 0.511323 test loss: 0.692295
[Epoch 7; Iter    24/   36] train: loss: 0.6926952
[Epoch 7] ogbg-molsider: 0.491181 val loss: 0.692911
[Epoch 7] ogbg-molsider: 0.510374 test loss: 0.692301
[Epoch 8; Iter    18/   36] train: loss: 0.6929107
[Epoch 8] ogbg-molsider: 0.488013 val loss: 0.692732
[Epoch 8] ogbg-molsider: 0.510594 test loss: 0.692068
[Epoch 9; Iter    12/   36] train: loss: 0.6928544
[Epoch 9] ogbg-molsider: 0.493199 val loss: 0.692690
[Epoch 9] ogbg-molsider: 0.511060 test loss: 0.692027
[Epoch 10; Iter     6/   36] train: loss: 0.6922631
[Epoch 10; Iter    36/   36] train: loss: 0.6929717
[Epoch 10] ogbg-molsider: 0.492938 val loss: 0.692558
[Epoch 10] ogbg-molsider: 0.513193 test loss: 0.691881
[Epoch 11; Iter    30/   36] train: loss: 0.6932401
[Epoch 11] ogbg-molsider: 0.490834 val loss: 0.692513
[Epoch 11] ogbg-molsider: 0.511071 test loss: 0.691871
[Epoch 12; Iter    24/   36] train: loss: 0.6923122
[Epoch 12] ogbg-molsider: 0.491703 val loss: 0.692338
[Epoch 12] ogbg-molsider: 0.513121 test loss: 0.691658
[Epoch 13; Iter    18/   36] train: loss: 0.6922776
[Epoch 13] ogbg-molsider: 0.491281 val loss: 0.692257
[Epoch 13] ogbg-molsider: 0.513698 test loss: 0.691583
[Epoch 14; Iter    12/   36] train: loss: 0.6925902
[Epoch 14] ogbg-molsider: 0.494063 val loss: 0.691995
[Epoch 14] ogbg-molsider: 0.513760 test loss: 0.691305
[Epoch 15; Iter     6/   36] train: loss: 0.6922340
[Epoch 15; Iter    36/   36] train: loss: 0.6927293
[Epoch 15] ogbg-molsider: 0.490850 val loss: 0.691932
[Epoch 15] ogbg-molsider: 0.511715 test loss: 0.691305
[Epoch 16; Iter    30/   36] train: loss: 0.6916839
[Epoch 16] ogbg-molsider: 0.492200 val loss: 0.691784
[Epoch 16] ogbg-molsider: 0.513469 test loss: 0.691110
[Epoch 17; Iter    24/   36] train: loss: 0.6917000
[Epoch 17] ogbg-molsider: 0.492984 val loss: 0.691490
[Epoch 17] ogbg-molsider: 0.510831 test loss: 0.690845
[Epoch 18; Iter    18/   36] train: loss: 0.6913743
[Epoch 18] ogbg-molsider: 0.492640 val loss: 0.691267
[Epoch 18] ogbg-molsider: 0.510537 test loss: 0.690630
[Epoch 19; Iter    12/   36] train: loss: 0.6915489
[Epoch 19] ogbg-molsider: 0.491967 val loss: 0.691043
[Epoch 19] ogbg-molsider: 0.509310 test loss: 0.690393
[Epoch 20; Iter     6/   36] train: loss: 0.6906611
[Epoch 20; Iter    36/   36] train: loss: 0.6875112
[Epoch 20] ogbg-molsider: 0.545491 val loss: 0.682009
[Epoch 20] ogbg-molsider: 0.565039 test loss: 0.681641
[Epoch 21; Iter    30/   36] train: loss: 0.6754796
[Epoch 21] ogbg-molsider: 0.548758 val loss: 0.654561
[Epoch 21] ogbg-molsider: 0.589446 test loss: 0.654506
[Epoch 22; Iter    24/   36] train: loss: 0.6661252
[Epoch 22] ogbg-molsider: 0.511655 val loss: 0.625296
[Epoch 22] ogbg-molsider: 0.589097 test loss: 0.624218
[Epoch 23; Iter    18/   36] train: loss: 0.6336802
[Epoch 23] ogbg-molsider: 0.517355 val loss: 0.596698
[Epoch 23] ogbg-molsider: 0.585489 test loss: 0.591106
[Epoch 24; Iter    12/   36] train: loss: 0.6128667
[Epoch 24] ogbg-molsider: 0.537016 val loss: 0.559121
[Epoch 24] ogbg-molsider: 0.600061 test loss: 0.552700
[Epoch 25; Iter     6/   36] train: loss: 0.5809156
[Epoch 25; Iter    36/   36] train: loss: 0.5864478
[Epoch 25] ogbg-molsider: 0.520711 val loss: 0.539366
[Epoch 25] ogbg-molsider: 0.604724 test loss: 0.535821
[Epoch 26; Iter    30/   36] train: loss: 0.5590328
[Epoch 26] ogbg-molsider: 0.539743 val loss: 0.537725
[Epoch 26] ogbg-molsider: 0.594552 test loss: 0.537141
[Epoch 27; Iter    24/   36] train: loss: 0.5181759
[Epoch 27] ogbg-molsider: 0.532578 val loss: 0.515637
[Epoch 27] ogbg-molsider: 0.574374 test loss: 0.523513
[Epoch 28; Iter    18/   36] train: loss: 0.4803813
[Epoch 28] ogbg-molsider: 0.579646 val loss: 0.487071
[Epoch 28] ogbg-molsider: 0.588851 test loss: 0.502601
[Epoch 29; Iter    12/   36] train: loss: 0.5443570
[Epoch 29] ogbg-molsider: 0.574885 val loss: 0.488329
[Epoch 29] ogbg-molsider: 0.597064 test loss: 0.499656
[Epoch 30; Iter     6/   36] train: loss: 0.5037438
[Epoch 30; Iter    36/   36] train: loss: 0.4780190
[Epoch 30] ogbg-molsider: 0.582283 val loss: 0.486423
[Epoch 30] ogbg-molsider: 0.585864 test loss: 0.505241
[Epoch 31; Iter    30/   36] train: loss: 0.5053059
[Epoch 31] ogbg-molsider: 0.526004 val loss: 0.489297
[Epoch 31] ogbg-molsider: 0.629304 test loss: 0.485332
[Epoch 32; Iter    24/   36] train: loss: 0.4848566
[Epoch 32] ogbg-molsider: 0.601409 val loss: 0.474621
[Epoch 32] ogbg-molsider: 0.583086 test loss: 0.496300
[Epoch 33; Iter    18/   36] train: loss: 0.5130961
[Epoch 33] ogbg-molsider: 0.547576 val loss: 0.486108
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/3DInfomax/sider/noise=0.0/PNA_ogbg-molsider_3DInfomax_sider_static_noise=0.0_4_26-05_09-18-24
config: <_io.TextIOWrapper name='configs_static_noise_experiments/3DInfomax/sider/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_static_noise=0.0
logdir: runs/static_noise/3DInfomax/sider/noise=0.0
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932428
[Epoch 1] ogbg-molsider: 0.508463 val loss: 0.693147
[Epoch 1] ogbg-molsider: 0.525730 test loss: 0.693261
[Epoch 2; Iter    24/   36] train: loss: 0.6936606
[Epoch 2] ogbg-molsider: 0.513912 val loss: 0.692988
[Epoch 2] ogbg-molsider: 0.522213 test loss: 0.693174
[Epoch 3; Iter    18/   36] train: loss: 0.6932107
[Epoch 3] ogbg-molsider: 0.515952 val loss: 0.693082
[Epoch 3] ogbg-molsider: 0.524025 test loss: 0.693268
[Epoch 4; Iter    12/   36] train: loss: 0.6936992
[Epoch 4] ogbg-molsider: 0.516213 val loss: 0.692961
[Epoch 4] ogbg-molsider: 0.522646 test loss: 0.693244
[Epoch 5; Iter     6/   36] train: loss: 0.6932373
[Epoch 5; Iter    36/   36] train: loss: 0.6934808
[Epoch 5] ogbg-molsider: 0.516092 val loss: 0.692883
[Epoch 5] ogbg-molsider: 0.521216 test loss: 0.693067
[Epoch 6; Iter    30/   36] train: loss: 0.6928728
[Epoch 6] ogbg-molsider: 0.520634 val loss: 0.692736
[Epoch 6] ogbg-molsider: 0.522189 test loss: 0.692945
[Epoch 7; Iter    24/   36] train: loss: 0.6930869
[Epoch 7] ogbg-molsider: 0.516094 val loss: 0.692650
[Epoch 7] ogbg-molsider: 0.521338 test loss: 0.692890
[Epoch 8; Iter    18/   36] train: loss: 0.6927783
[Epoch 8] ogbg-molsider: 0.516210 val loss: 0.692643
[Epoch 8] ogbg-molsider: 0.520560 test loss: 0.692809
[Epoch 9; Iter    12/   36] train: loss: 0.6934307
[Epoch 9] ogbg-molsider: 0.519573 val loss: 0.692593
[Epoch 9] ogbg-molsider: 0.523813 test loss: 0.692788
[Epoch 10; Iter     6/   36] train: loss: 0.6927520
[Epoch 10; Iter    36/   36] train: loss: 0.6921809
[Epoch 10] ogbg-molsider: 0.515571 val loss: 0.692436
[Epoch 10] ogbg-molsider: 0.524470 test loss: 0.692685
[Epoch 11; Iter    30/   36] train: loss: 0.6922271
[Epoch 11] ogbg-molsider: 0.518816 val loss: 0.692203
[Epoch 11] ogbg-molsider: 0.524258 test loss: 0.692430
[Epoch 12; Iter    24/   36] train: loss: 0.6926205
[Epoch 12] ogbg-molsider: 0.518845 val loss: 0.692063
[Epoch 12] ogbg-molsider: 0.523523 test loss: 0.692279
[Epoch 13; Iter    18/   36] train: loss: 0.6921979
[Epoch 13] ogbg-molsider: 0.519505 val loss: 0.691942
[Epoch 13] ogbg-molsider: 0.525343 test loss: 0.692136
[Epoch 14; Iter    12/   36] train: loss: 0.6925152
[Epoch 14] ogbg-molsider: 0.515243 val loss: 0.691764
[Epoch 14] ogbg-molsider: 0.522510 test loss: 0.691936
[Epoch 15; Iter     6/   36] train: loss: 0.6920321
[Epoch 15; Iter    36/   36] train: loss: 0.6914085
[Epoch 15] ogbg-molsider: 0.516954 val loss: 0.691581
[Epoch 15] ogbg-molsider: 0.523124 test loss: 0.691767
[Epoch 16; Iter    30/   36] train: loss: 0.6914978
[Epoch 16] ogbg-molsider: 0.515856 val loss: 0.691380
[Epoch 16] ogbg-molsider: 0.522523 test loss: 0.691577
[Epoch 17; Iter    24/   36] train: loss: 0.6918402
[Epoch 17] ogbg-molsider: 0.518684 val loss: 0.691072
[Epoch 17] ogbg-molsider: 0.523353 test loss: 0.691313
[Epoch 18; Iter    18/   36] train: loss: 0.6910167
[Epoch 18] ogbg-molsider: 0.515675 val loss: 0.690957
[Epoch 18] ogbg-molsider: 0.524477 test loss: 0.691152
[Epoch 19; Iter    12/   36] train: loss: 0.6915345
[Epoch 19] ogbg-molsider: 0.516908 val loss: 0.690684
[Epoch 19] ogbg-molsider: 0.525019 test loss: 0.690884
[Epoch 20; Iter     6/   36] train: loss: 0.6906663
[Epoch 20; Iter    36/   36] train: loss: 0.6862883
[Epoch 20] ogbg-molsider: 0.537245 val loss: 0.681538
[Epoch 20] ogbg-molsider: 0.571214 test loss: 0.681406
[Epoch 21; Iter    30/   36] train: loss: 0.6780910
[Epoch 21] ogbg-molsider: 0.520664 val loss: 0.662856
[Epoch 21] ogbg-molsider: 0.604330 test loss: 0.662514
[Epoch 22; Iter    24/   36] train: loss: 0.6567007
[Epoch 22] ogbg-molsider: 0.491540 val loss: 0.642001
[Epoch 22] ogbg-molsider: 0.592545 test loss: 0.638371
[Epoch 23; Iter    18/   36] train: loss: 0.6343639
[Epoch 23] ogbg-molsider: 0.531209 val loss: 0.602877
[Epoch 23] ogbg-molsider: 0.609525 test loss: 0.595668
[Epoch 24; Iter    12/   36] train: loss: 0.6129710
[Epoch 24] ogbg-molsider: 0.545014 val loss: 0.595576
[Epoch 24] ogbg-molsider: 0.588566 test loss: 0.597635
[Epoch 25; Iter     6/   36] train: loss: 0.5915238
[Epoch 25; Iter    36/   36] train: loss: 0.5730799
[Epoch 25] ogbg-molsider: 0.530956 val loss: 0.538076
[Epoch 25] ogbg-molsider: 0.602354 test loss: 0.536167
[Epoch 26; Iter    30/   36] train: loss: 0.5474578
[Epoch 26] ogbg-molsider: 0.556105 val loss: 0.508094
[Epoch 26] ogbg-molsider: 0.596440 test loss: 0.537685
[Epoch 27; Iter    24/   36] train: loss: 0.5316824
[Epoch 27] ogbg-molsider: 0.553854 val loss: 0.502826
[Epoch 27] ogbg-molsider: 0.585240 test loss: 0.528976
[Epoch 28; Iter    18/   36] train: loss: 0.5083334
[Epoch 28] ogbg-molsider: 0.535749 val loss: 0.486494
[Epoch 28] ogbg-molsider: 0.597267 test loss: 0.529221
[Epoch 29; Iter    12/   36] train: loss: 0.5151914
[Epoch 29] ogbg-molsider: 0.535506 val loss: 0.492968
[Epoch 29] ogbg-molsider: 0.587994 test loss: 0.522609
[Epoch 30; Iter     6/   36] train: loss: 0.5108786
[Epoch 30; Iter    36/   36] train: loss: 0.4881211
[Epoch 30] ogbg-molsider: 0.534922 val loss: 0.488686
[Epoch 30] ogbg-molsider: 0.616662 test loss: 0.551731
[Epoch 31; Iter    30/   36] train: loss: 0.4877305
[Epoch 31] ogbg-molsider: 0.532415 val loss: 0.487463
[Epoch 31] ogbg-molsider: 0.600483 test loss: 0.491842
[Epoch 32; Iter    24/   36] train: loss: 0.5385497
[Epoch 32] ogbg-molsider: 0.510961 val loss: 0.496164
[Epoch 32] ogbg-molsider: 0.608659 test loss: 0.492880
[Epoch 33; Iter    18/   36] train: loss: 0.4887319
[Epoch 33] ogbg-molsider: 0.527184 val loss: 0.494539
[Epoch 33] ogbg-molsider: 0.612357 test loss: 0.485989
[Epoch 34; Iter    12/   36] train: loss: 0.4817716
[Epoch 34] ogbg-molsider: 0.548565 val loss: 0.487500
[Epoch 34] ogbg-molsider: 0.584076 test loss: 0.499932
[Epoch 35; Iter     6/   36] train: loss: 0.5539057
[Epoch 35; Iter    36/   36] train: loss: 0.5095695
[Epoch 35] ogbg-molsider: 0.551350 val loss: 0.495501
[Epoch 35] ogbg-molsider: 0.579229 test loss: 0.504601
[Epoch 36; Iter    30/   36] train: loss: 0.4820097
[Epoch 36] ogbg-molsider: 0.522064 val loss: 0.495370
[Epoch 36] ogbg-molsider: 0.592051 test loss: 0.492684
[Epoch 37; Iter    24/   36] train: loss: 0.4744235
[Epoch 37] ogbg-molsider: 0.524614 val loss: 0.492723
[Epoch 37] ogbg-molsider: 0.594508 test loss: 0.494217
[Epoch 38; Iter    18/   36] train: loss: 0.5138699
[Epoch 38] ogbg-molsider: 0.535125 val loss: 0.526592
[Epoch 38] ogbg-molsider: 0.575885 test loss: 0.510721
[Epoch 39; Iter    12/   36] train: loss: 0.5081522
[Epoch 39] ogbg-molsider: 0.537697 val loss: 0.541291
[Epoch 39] ogbg-molsider: 0.603975 test loss: 0.498433
[Epoch 40; Iter     6/   36] train: loss: 0.4892988
[Epoch 40; Iter    36/   36] train: loss: 0.5117150
[Epoch 40] ogbg-molsider: 0.551804 val loss: 0.489661
[Epoch 40] ogbg-molsider: 0.604773 test loss: 0.501870
[Epoch 41; Iter    30/   36] train: loss: 0.5126802
[Epoch 41] ogbg-molsider: 0.559251 val loss: 0.580037
[Epoch 41] ogbg-molsider: 0.589604 test loss: 0.608523
[Epoch 42; Iter    24/   36] train: loss: 0.4842854
[Epoch 42] ogbg-molsider: 0.578176 val loss: 0.492511
[Epoch 42] ogbg-molsider: 0.563108 test loss: 0.527600
[Epoch 43; Iter    18/   36] train: loss: 0.4922630
[Epoch 43] ogbg-molsider: 0.570011 val loss: 0.487014
[Epoch 43] ogbg-molsider: 0.607447 test loss: 0.499754
[Epoch 44; Iter    12/   36] train: loss: 0.4649623
[Epoch 44] ogbg-molsider: 0.573581 val loss: 0.573501
[Epoch 44] ogbg-molsider: 0.585080 test loss: 0.609286
[Epoch 45; Iter     6/   36] train: loss: 0.4624697
[Epoch 45; Iter    36/   36] train: loss: 0.4458919
[Epoch 45] ogbg-molsider: 0.556565 val loss: 0.557479
[Epoch 45] ogbg-molsider: 0.610861 test loss: 0.562628
[Epoch 46; Iter    30/   36] train: loss: 0.5365781
[Epoch 46] ogbg-molsider: 0.538441 val loss: 0.556554
[Epoch 46] ogbg-molsider: 0.577800 test loss: 0.585179
[Epoch 47; Iter    24/   36] train: loss: 0.4434033
[Epoch 47] ogbg-molsider: 0.527034 val loss: 0.551606
[Epoch 47] ogbg-molsider: 0.537732 test loss: 0.532167
[Epoch 48; Iter    18/   36] train: loss: 0.4548841
[Epoch 48] ogbg-molsider: 0.580636 val loss: 0.597258
[Epoch 48] ogbg-molsider: 0.567245 test loss: 0.671744
[Epoch 49; Iter    12/   36] train: loss: 0.4356115
[Epoch 49] ogbg-molsider: 0.581152 val loss: 0.503667
[Epoch 49] ogbg-molsider: 0.561549 test loss: 0.539794
[Epoch 50; Iter     6/   36] train: loss: 0.4652982
[Epoch 50; Iter    36/   36] train: loss: 0.4880674
[Epoch 50] ogbg-molsider: 0.567582 val loss: 0.525753
[Epoch 50] ogbg-molsider: 0.605291 test loss: 0.540655
[Epoch 51; Iter    30/   36] train: loss: 0.4480771
[Epoch 51] ogbg-molsider: 0.613665 val loss: 0.564764
[Epoch 51] ogbg-molsider: 0.529688 test loss: 0.662234
[Epoch 52; Iter    24/   36] train: loss: 0.3946183
[Epoch 52] ogbg-molsider: 0.620247 val loss: 0.503989
[Epoch 52] ogbg-molsider: 0.566967 test loss: 0.562662
[Epoch 53; Iter    18/   36] train: loss: 0.3996083
[Epoch 53] ogbg-molsider: 0.600301 val loss: 0.558873
[Epoch 53] ogbg-molsider: 0.563843 test loss: 0.624162
[Epoch 54; Iter    12/   36] train: loss: 0.3883832
[Epoch 54] ogbg-molsider: 0.619771 val loss: 0.533847
[Epoch 54] ogbg-molsider: 0.580514 test loss: 0.550341
[Epoch 55; Iter     6/   36] train: loss: 0.3870905
[Epoch 55; Iter    36/   36] train: loss: 0.4065056
[Epoch 55] ogbg-molsider: 0.585686 val loss: 0.523119
[Epoch 55] ogbg-molsider: 0.572759 test loss: 0.550527
[Epoch 56; Iter    30/   36] train: loss: 0.3826863
[Epoch 56] ogbg-molsider: 0.620819 val loss: 0.553518
[Epoch 56] ogbg-molsider: 0.585044 test loss: 0.635361
[Epoch 57; Iter    24/   36] train: loss: 0.3809174
[Epoch 57] ogbg-molsider: 0.567265 val loss: 0.578720
[Epoch 57] ogbg-molsider: 0.579422 test loss: 0.627507
[Epoch 58; Iter    18/   36] train: loss: 0.3972129
[Epoch 58] ogbg-molsider: 0.607425 val loss: 3.273959
[Epoch 58] ogbg-molsider: 0.553070 test loss: 4.241533
[Epoch 59; Iter    12/   36] train: loss: 0.3741578
[Epoch 59] ogbg-molsider: 0.609513 val loss: 0.618320
[Epoch 59] ogbg-molsider: 0.538677 test loss: 0.733319
[Epoch 60; Iter     6/   36] train: loss: 0.3500915
[Epoch 60; Iter    36/   36] train: loss: 0.3986308
[Epoch 60] ogbg-molsider: 0.551676 val loss: 0.609540
[Epoch 60] ogbg-molsider: 0.573486 test loss: 0.660436
[Epoch 61; Iter    30/   36] train: loss: 0.3933288
[Epoch 61] ogbg-molsider: 0.569803 val loss: 0.587834
[Epoch 61] ogbg-molsider: 0.562385 test loss: 0.666125
[Epoch 62; Iter    24/   36] train: loss: 0.3572804
[Epoch 62] ogbg-molsider: 0.570115 val loss: 0.717697
[Epoch 62] ogbg-molsider: 0.567983 test loss: 0.798044
[Epoch 63; Iter    18/   36] train: loss: 0.3557838
[Epoch 63] ogbg-molsider: 0.588207 val loss: 0.930722
[Epoch 63] ogbg-molsider: 0.581766 test loss: 1.204145
[Epoch 64; Iter    12/   36] train: loss: 0.3453751
[Epoch 64] ogbg-molsider: 0.588977 val loss: 0.581216
[Epoch 64] ogbg-molsider: 0.580332 test loss: 0.603194
[Epoch 65; Iter     6/   36] train: loss: 0.2911979
[Epoch 65; Iter    36/   36] train: loss: 0.3305546
[Epoch 65] ogbg-molsider: 0.580963 val loss: 0.547931
[Epoch 65] ogbg-molsider: 0.582326 test loss: 0.592357
[Epoch 66; Iter    30/   36] train: loss: 0.3172104
[Epoch 66] ogbg-molsider: 0.622425 val loss: 0.649165
[Epoch 66] ogbg-molsider: 0.576005 test loss: 0.783995
[Epoch 67; Iter    24/   36] train: loss: 0.3155338
[Epoch 67] ogbg-molsider: 0.589389 val loss: 0.603746
[Epoch 67] ogbg-molsider: 0.568891 test loss: 0.703448
[Epoch 68; Iter    18/   36] train: loss: 0.3150356
[Epoch 68] ogbg-molsider: 0.601989 val loss: 0.555869
[Epoch 68] ogbg-molsider: 0.594769 test loss: 0.606191
[Epoch 69; Iter    12/   36] train: loss: 0.3284964
[Epoch 69] ogbg-molsider: 0.571163 val loss: 0.609265
[Epoch 69] ogbg-molsider: 0.591158 test loss: 0.624612
[Epoch 70; Iter     6/   36] train: loss: 0.3066820
[Epoch 70; Iter    36/   36] train: loss: 0.3230437
[Epoch 70] ogbg-molsider: 0.595533 val loss: 0.579779
[Epoch 70] ogbg-molsider: 0.568858 test loss: 0.636328
[Epoch 71; Iter    30/   36] train: loss: 0.2960750
[Epoch 71] ogbg-molsider: 0.589332 val loss: 0.618136
[Epoch 71] ogbg-molsider: 0.584760 test loss: 0.658229
[Epoch 72; Iter    24/   36] train: loss: 0.3059872
[Epoch 72] ogbg-molsider: 0.593308 val loss: 0.624760
[Epoch 72] ogbg-molsider: 0.582845 test loss: 0.683242
[Epoch 73; Iter    18/   36] train: loss: 0.2746435
[Epoch 73] ogbg-molsider: 0.574881 val loss: 0.568277
[Epoch 73] ogbg-molsider: 0.588844 test loss: 0.598134
[Epoch 74; Iter    12/   36] train: loss: 0.2613520
[Epoch 74] ogbg-molsider: 0.602187 val loss: 0.653358
[Epoch 74] ogbg-molsider: 0.565316 test loss: 0.727509
[Epoch 75; Iter     6/   36] train: loss: 0.2995542
[Epoch 75; Iter    36/   36] train: loss: 0.3003139
[Epoch 75] ogbg-molsider: 0.615128 val loss: 0.594526
[Epoch 75] ogbg-molsider: 0.579740 test loss: 0.676227
[Epoch 76; Iter    30/   36] train: loss: 0.2731980
[Epoch 76] ogbg-molsider: 0.594466 val loss: 0.625851
[Epoch 76] ogbg-molsider: 0.591265 test loss: 0.690639
[Epoch 77; Iter    24/   36] train: loss: 0.2951432
[Epoch 77] ogbg-molsider: 0.621899 val loss: 0.600986
[Epoch 77] ogbg-molsider: 0.573211 test loss: 0.676282
[Epoch 78; Iter    18/   36] train: loss: 0.2952524
[Epoch 78] ogbg-molsider: 0.597686 val loss: 0.655356
[Epoch 78] ogbg-molsider: 0.573063 test loss: 0.762369
[Epoch 79; Iter    12/   36] train: loss: 0.2727339
[Epoch 79] ogbg-molsider: 0.593367 val loss: 0.648226
[Epoch 79] ogbg-molsider: 0.584042 test loss: 0.701907
[Epoch 80; Iter     6/   36] train: loss: 0.3160362
[Epoch 80; Iter    36/   36] train: loss: 0.2763633
[Epoch 80] ogbg-molsider: 0.602877 val loss: 0.628755
[Epoch 80] ogbg-molsider: 0.581030 test loss: 0.694138
[Epoch 33] ogbg-molsider: 0.586711 test loss: 0.531787
[Epoch 34; Iter    12/   36] train: loss: 0.4979932
[Epoch 34] ogbg-molsider: 0.516629 val loss: 0.581286
[Epoch 34] ogbg-molsider: 0.583791 test loss: 0.538831
[Epoch 35; Iter     6/   36] train: loss: 0.5644924
[Epoch 35; Iter    36/   36] train: loss: 0.4536542
[Epoch 35] ogbg-molsider: 0.509883 val loss: 0.582826
[Epoch 35] ogbg-molsider: 0.583480 test loss: 0.674988
[Epoch 36; Iter    30/   36] train: loss: 0.4557109
[Epoch 36] ogbg-molsider: 0.544015 val loss: 0.551721
[Epoch 36] ogbg-molsider: 0.563349 test loss: 0.539246
[Epoch 37; Iter    24/   36] train: loss: 0.5373616
[Epoch 37] ogbg-molsider: 0.502207 val loss: 0.583637
[Epoch 37] ogbg-molsider: 0.563772 test loss: 0.556886
[Epoch 38; Iter    18/   36] train: loss: 0.5124919
[Epoch 38] ogbg-molsider: 0.528211 val loss: 0.587208
[Epoch 38] ogbg-molsider: 0.571510 test loss: 0.552904
[Epoch 39; Iter    12/   36] train: loss: 0.4935760
[Epoch 39] ogbg-molsider: 0.530968 val loss: 0.649229
[Epoch 39] ogbg-molsider: 0.561682 test loss: 0.576253
[Epoch 40; Iter     6/   36] train: loss: 0.4650522
[Epoch 40; Iter    36/   36] train: loss: 0.5117130
[Epoch 40] ogbg-molsider: 0.578586 val loss: 0.535511
[Epoch 40] ogbg-molsider: 0.549133 test loss: 0.571620
[Epoch 41; Iter    30/   36] train: loss: 0.5610027
[Epoch 41] ogbg-molsider: 0.566772 val loss: 0.565606
[Epoch 41] ogbg-molsider: 0.558378 test loss: 0.576136
[Epoch 42; Iter    24/   36] train: loss: 0.5042330
[Epoch 42] ogbg-molsider: 0.511533 val loss: 1.058660
[Epoch 42] ogbg-molsider: 0.540587 test loss: 0.926663
[Epoch 43; Iter    18/   36] train: loss: 0.4707091
[Epoch 43] ogbg-molsider: 0.538217 val loss: 0.870523
[Epoch 43] ogbg-molsider: 0.507728 test loss: 0.911813
[Epoch 44; Iter    12/   36] train: loss: 0.4485170
[Epoch 44] ogbg-molsider: 0.599102 val loss: 1.815171
[Epoch 44] ogbg-molsider: 0.498967 test loss: 2.966330
[Epoch 45; Iter     6/   36] train: loss: 0.4562622
[Epoch 45; Iter    36/   36] train: loss: 0.4420807
[Epoch 45] ogbg-molsider: 0.534823 val loss: 0.538843
[Epoch 45] ogbg-molsider: 0.568215 test loss: 0.528837
[Epoch 46; Iter    30/   36] train: loss: 0.4609070
[Epoch 46] ogbg-molsider: 0.555315 val loss: 0.930284
[Epoch 46] ogbg-molsider: 0.489569 test loss: 0.935482
[Epoch 47; Iter    24/   36] train: loss: 0.4349949
[Epoch 47] ogbg-molsider: 0.543604 val loss: 0.648621
[Epoch 47] ogbg-molsider: 0.497380 test loss: 0.677754
[Epoch 48; Iter    18/   36] train: loss: 0.4287854
[Epoch 48] ogbg-molsider: 0.573158 val loss: 0.887154
[Epoch 48] ogbg-molsider: 0.506287 test loss: 0.885574
[Epoch 49; Iter    12/   36] train: loss: 0.4006319
[Epoch 49] ogbg-molsider: 0.574838 val loss: 0.810249
[Epoch 49] ogbg-molsider: 0.501721 test loss: 0.950228
[Epoch 50; Iter     6/   36] train: loss: 0.4131340
[Epoch 50; Iter    36/   36] train: loss: 0.4261214
[Epoch 50] ogbg-molsider: 0.556913 val loss: 1.134919
[Epoch 50] ogbg-molsider: 0.471922 test loss: 1.301073
[Epoch 51; Iter    30/   36] train: loss: 0.3880968
[Epoch 51] ogbg-molsider: 0.556790 val loss: 1.563711
[Epoch 51] ogbg-molsider: 0.488596 test loss: 1.791940
[Epoch 52; Iter    24/   36] train: loss: 0.3816389
[Epoch 52] ogbg-molsider: 0.552242 val loss: 1.247787
[Epoch 52] ogbg-molsider: 0.513219 test loss: 1.401975
[Epoch 53; Iter    18/   36] train: loss: 0.3305763
[Epoch 53] ogbg-molsider: 0.521909 val loss: 1.502820
[Epoch 53] ogbg-molsider: 0.504095 test loss: 1.573614
[Epoch 54; Iter    12/   36] train: loss: 0.3730128
[Epoch 54] ogbg-molsider: 0.577399 val loss: 1.375651
[Epoch 54] ogbg-molsider: 0.517198 test loss: 2.106797
[Epoch 55; Iter     6/   36] train: loss: 0.3786851
[Epoch 55; Iter    36/   36] train: loss: 0.3566213
[Epoch 55] ogbg-molsider: 0.564286 val loss: 15.035040
[Epoch 55] ogbg-molsider: 0.532727 test loss: 24.144520
[Epoch 56; Iter    30/   36] train: loss: 0.3760318
[Epoch 56] ogbg-molsider: 0.566146 val loss: 14.705027
[Epoch 56] ogbg-molsider: 0.486536 test loss: 13.953628
[Epoch 57; Iter    24/   36] train: loss: 0.3405194
[Epoch 57] ogbg-molsider: 0.588150 val loss: 0.957051
[Epoch 57] ogbg-molsider: 0.500494 test loss: 1.141259
[Epoch 58; Iter    18/   36] train: loss: 0.3194612
[Epoch 58] ogbg-molsider: 0.586410 val loss: 0.984486
[Epoch 58] ogbg-molsider: 0.517391 test loss: 0.988697
[Epoch 59; Iter    12/   36] train: loss: 0.3629998
[Epoch 59] ogbg-molsider: 0.570479 val loss: 1.138187
[Epoch 59] ogbg-molsider: 0.512323 test loss: 1.297650
[Epoch 60; Iter     6/   36] train: loss: 0.3766411
[Epoch 60; Iter    36/   36] train: loss: 0.3292370
[Epoch 60] ogbg-molsider: 0.585770 val loss: 3.957091
[Epoch 60] ogbg-molsider: 0.533280 test loss: 3.759212
[Epoch 61; Iter    30/   36] train: loss: 0.3215207
[Epoch 61] ogbg-molsider: 0.592777 val loss: 0.959171
[Epoch 61] ogbg-molsider: 0.544252 test loss: 1.344512
[Epoch 62; Iter    24/   36] train: loss: 0.3371008
[Epoch 62] ogbg-molsider: 0.599885 val loss: 12.972046
[Epoch 62] ogbg-molsider: 0.541847 test loss: 14.539784
[Epoch 63; Iter    18/   36] train: loss: 0.3516798
[Epoch 63] ogbg-molsider: 0.585067 val loss: 13.119303
[Epoch 63] ogbg-molsider: 0.531087 test loss: 12.637780
[Epoch 64; Iter    12/   36] train: loss: 0.3265550
[Epoch 64] ogbg-molsider: 0.561138 val loss: 10.337699
[Epoch 64] ogbg-molsider: 0.535255 test loss: 9.954055
[Epoch 65; Iter     6/   36] train: loss: 0.2960753
[Epoch 65; Iter    36/   36] train: loss: 0.4276920
[Epoch 65] ogbg-molsider: 0.579582 val loss: 0.821931
[Epoch 65] ogbg-molsider: 0.551450 test loss: 0.876763
[Epoch 66; Iter    30/   36] train: loss: 0.3167843
[Epoch 66] ogbg-molsider: 0.603096 val loss: 13.866225
[Epoch 66] ogbg-molsider: 0.564505 test loss: 17.166722
[Epoch 67; Iter    24/   36] train: loss: 0.3166479
[Epoch 67] ogbg-molsider: 0.578363 val loss: 2.920878
[Epoch 67] ogbg-molsider: 0.528270 test loss: 1.507141
[Epoch 68; Iter    18/   36] train: loss: 0.3219659
[Epoch 68] ogbg-molsider: 0.580724 val loss: 0.924303
[Epoch 68] ogbg-molsider: 0.576508 test loss: 1.118277
[Epoch 69; Iter    12/   36] train: loss: 0.2922773
[Epoch 69] ogbg-molsider: 0.578899 val loss: 4.765564
[Epoch 69] ogbg-molsider: 0.556148 test loss: 9.804186
[Epoch 70; Iter     6/   36] train: loss: 0.2835250
[Epoch 70; Iter    36/   36] train: loss: 0.3000175
[Epoch 70] ogbg-molsider: 0.577994 val loss: 32.505549
[Epoch 70] ogbg-molsider: 0.559550 test loss: 48.871800
[Epoch 71; Iter    30/   36] train: loss: 0.3250684
[Epoch 71] ogbg-molsider: 0.558380 val loss: 1.968209
[Epoch 71] ogbg-molsider: 0.546102 test loss: 3.012722
[Epoch 72; Iter    24/   36] train: loss: 0.2739645
[Epoch 72] ogbg-molsider: 0.556036 val loss: 0.967952
[Epoch 72] ogbg-molsider: 0.545792 test loss: 1.031032
[Epoch 73; Iter    18/   36] train: loss: 0.2968356
[Epoch 73] ogbg-molsider: 0.566447 val loss: 0.980290
[Epoch 73] ogbg-molsider: 0.541708 test loss: 1.118436
[Epoch 74; Iter    12/   36] train: loss: 0.2546472
[Epoch 74] ogbg-molsider: 0.569111 val loss: 0.912591
[Epoch 74] ogbg-molsider: 0.529063 test loss: 0.958593
[Epoch 75; Iter     6/   36] train: loss: 0.2826037
[Epoch 75; Iter    36/   36] train: loss: 0.2767371
[Epoch 75] ogbg-molsider: 0.574455 val loss: 1.059269
[Epoch 75] ogbg-molsider: 0.545871 test loss: 1.089440
[Epoch 76; Iter    30/   36] train: loss: 0.3035867
[Epoch 76] ogbg-molsider: 0.572570 val loss: 1.358572
[Epoch 76] ogbg-molsider: 0.518188 test loss: 1.655458
[Epoch 77; Iter    24/   36] train: loss: 0.2833080
[Epoch 77] ogbg-molsider: 0.565853 val loss: 1.398106
[Epoch 77] ogbg-molsider: 0.531364 test loss: 1.637377
[Epoch 78; Iter    18/   36] train: loss: 0.2852621
[Epoch 78] ogbg-molsider: 0.550480 val loss: 0.885419
[Epoch 78] ogbg-molsider: 0.549569 test loss: 0.883208
[Epoch 79; Iter    12/   36] train: loss: 0.2432795
[Epoch 79] ogbg-molsider: 0.563762 val loss: 2.692186
[Epoch 79] ogbg-molsider: 0.529255 test loss: 1.726681
[Epoch 80; Iter     6/   36] train: loss: 0.2599453
[Epoch 80; Iter    36/   36] train: loss: 0.2292667
[Epoch 80] ogbg-molsider: 0.553009 val loss: 0.916271
[Epoch 80] ogbg-molsider: 0.542233 test loss: 0.878591
[Epoch 33] ogbg-molsider: 0.587692 test loss: 0.535308
[Epoch 34; Iter    12/   36] train: loss: 0.4934388
[Epoch 34] ogbg-molsider: 0.560876 val loss: 0.931485
[Epoch 34] ogbg-molsider: 0.582614 test loss: 1.260054
[Epoch 35; Iter     6/   36] train: loss: 0.5428511
[Epoch 35; Iter    36/   36] train: loss: 0.5171596
[Epoch 35] ogbg-molsider: 0.558031 val loss: 0.860623
[Epoch 35] ogbg-molsider: 0.566934 test loss: 1.122950
[Epoch 36; Iter    30/   36] train: loss: 0.4798756
[Epoch 36] ogbg-molsider: 0.546337 val loss: 0.613948
[Epoch 36] ogbg-molsider: 0.593191 test loss: 0.685785
[Epoch 37; Iter    24/   36] train: loss: 0.4650842
[Epoch 37] ogbg-molsider: 0.527223 val loss: 0.511115
[Epoch 37] ogbg-molsider: 0.588095 test loss: 0.526114
[Epoch 38; Iter    18/   36] train: loss: 0.4796754
[Epoch 38] ogbg-molsider: 0.535671 val loss: 0.495065
[Epoch 38] ogbg-molsider: 0.591033 test loss: 0.510848
[Epoch 39; Iter    12/   36] train: loss: 0.4874147
[Epoch 39] ogbg-molsider: 0.559656 val loss: 0.605834
[Epoch 39] ogbg-molsider: 0.599461 test loss: 0.552455
[Epoch 40; Iter     6/   36] train: loss: 0.4541486
[Epoch 40; Iter    36/   36] train: loss: 0.5071295
[Epoch 40] ogbg-molsider: 0.540232 val loss: 0.528674
[Epoch 40] ogbg-molsider: 0.617767 test loss: 0.517809
[Epoch 41; Iter    30/   36] train: loss: 0.4841893
[Epoch 41] ogbg-molsider: 0.572079 val loss: 0.578927
[Epoch 41] ogbg-molsider: 0.586973 test loss: 0.546485
[Epoch 42; Iter    24/   36] train: loss: 0.4671975
[Epoch 42] ogbg-molsider: 0.576277 val loss: 0.721280
[Epoch 42] ogbg-molsider: 0.581439 test loss: 0.521086
[Epoch 43; Iter    18/   36] train: loss: 0.4613537
[Epoch 43] ogbg-molsider: 0.576048 val loss: 1.078362
[Epoch 43] ogbg-molsider: 0.592640 test loss: 1.332012
[Epoch 44; Iter    12/   36] train: loss: 0.4708485
[Epoch 44] ogbg-molsider: 0.608747 val loss: 0.542300
[Epoch 44] ogbg-molsider: 0.591369 test loss: 0.585564
[Epoch 45; Iter     6/   36] train: loss: 0.4358584
[Epoch 45; Iter    36/   36] train: loss: 0.4630614
[Epoch 45] ogbg-molsider: 0.542237 val loss: 0.566047
[Epoch 45] ogbg-molsider: 0.615148 test loss: 0.633603
[Epoch 46; Iter    30/   36] train: loss: 0.4914236
[Epoch 46] ogbg-molsider: 0.545353 val loss: 0.742508
[Epoch 46] ogbg-molsider: 0.567633 test loss: 0.908552
[Epoch 47; Iter    24/   36] train: loss: 0.4808156
[Epoch 47] ogbg-molsider: 0.525922 val loss: 14.657312
[Epoch 47] ogbg-molsider: 0.548598 test loss: 12.132250
[Epoch 48; Iter    18/   36] train: loss: 0.4605018
[Epoch 48] ogbg-molsider: 0.576690 val loss: 0.526188
[Epoch 48] ogbg-molsider: 0.606792 test loss: 0.540496
[Epoch 49; Iter    12/   36] train: loss: 0.4100532
[Epoch 49] ogbg-molsider: 0.553746 val loss: 0.570470
[Epoch 49] ogbg-molsider: 0.604163 test loss: 0.650375
[Epoch 50; Iter     6/   36] train: loss: 0.4384796
[Epoch 50; Iter    36/   36] train: loss: 0.4330286
[Epoch 50] ogbg-molsider: 0.585357 val loss: 0.641849
[Epoch 50] ogbg-molsider: 0.597403 test loss: 0.603121
[Epoch 51; Iter    30/   36] train: loss: 0.4027542
[Epoch 51] ogbg-molsider: 0.581126 val loss: 0.802810
[Epoch 51] ogbg-molsider: 0.581235 test loss: 0.718070
[Epoch 52; Iter    24/   36] train: loss: 0.3702992
[Epoch 52] ogbg-molsider: 0.617630 val loss: 0.535959
[Epoch 52] ogbg-molsider: 0.597773 test loss: 0.600172
[Epoch 53; Iter    18/   36] train: loss: 0.3764044
[Epoch 53] ogbg-molsider: 0.583888 val loss: 0.553254
[Epoch 53] ogbg-molsider: 0.596373 test loss: 0.644446
[Epoch 54; Iter    12/   36] train: loss: 0.3594331
[Epoch 54] ogbg-molsider: 0.567231 val loss: 0.525812
[Epoch 54] ogbg-molsider: 0.605559 test loss: 0.551471
[Epoch 55; Iter     6/   36] train: loss: 0.3495129
[Epoch 55; Iter    36/   36] train: loss: 0.3714232
[Epoch 55] ogbg-molsider: 0.605887 val loss: 0.602668
[Epoch 55] ogbg-molsider: 0.579703 test loss: 0.687663
[Epoch 56; Iter    30/   36] train: loss: 0.3448120
[Epoch 56] ogbg-molsider: 0.574831 val loss: 0.591458
[Epoch 56] ogbg-molsider: 0.580830 test loss: 0.653151
[Epoch 57; Iter    24/   36] train: loss: 0.3622268
[Epoch 57] ogbg-molsider: 0.588307 val loss: 0.541556
[Epoch 57] ogbg-molsider: 0.591705 test loss: 0.601368
[Epoch 58; Iter    18/   36] train: loss: 0.3835662
[Epoch 58] ogbg-molsider: 0.561729 val loss: 0.587915
[Epoch 58] ogbg-molsider: 0.587684 test loss: 0.699943
[Epoch 59; Iter    12/   36] train: loss: 0.3244402
[Epoch 59] ogbg-molsider: 0.605926 val loss: 0.561554
[Epoch 59] ogbg-molsider: 0.594286 test loss: 0.631109
[Epoch 60; Iter     6/   36] train: loss: 0.3223308
[Epoch 60; Iter    36/   36] train: loss: 0.3912592
[Epoch 60] ogbg-molsider: 0.579423 val loss: 0.760926
[Epoch 60] ogbg-molsider: 0.566557 test loss: 0.756427
[Epoch 61; Iter    30/   36] train: loss: 0.3487805
[Epoch 61] ogbg-molsider: 0.567179 val loss: 0.575117
[Epoch 61] ogbg-molsider: 0.601458 test loss: 0.605175
[Epoch 62; Iter    24/   36] train: loss: 0.3275229
[Epoch 62] ogbg-molsider: 0.586809 val loss: 0.572223
[Epoch 62] ogbg-molsider: 0.593919 test loss: 0.631987
[Epoch 63; Iter    18/   36] train: loss: 0.3408658
[Epoch 63] ogbg-molsider: 0.588900 val loss: 1.022815
[Epoch 63] ogbg-molsider: 0.589715 test loss: 1.266798
[Epoch 64; Iter    12/   36] train: loss: 0.3052765
[Epoch 64] ogbg-molsider: 0.602264 val loss: 0.559507
[Epoch 64] ogbg-molsider: 0.606179 test loss: 0.683942
[Epoch 65; Iter     6/   36] train: loss: 0.2728176
[Epoch 65; Iter    36/   36] train: loss: 0.3052136
[Epoch 65] ogbg-molsider: 0.582062 val loss: 0.701357
[Epoch 65] ogbg-molsider: 0.591816 test loss: 0.786880
[Epoch 66; Iter    30/   36] train: loss: 0.3014704
[Epoch 66] ogbg-molsider: 0.583479 val loss: 0.647947
[Epoch 66] ogbg-molsider: 0.591517 test loss: 0.659287
[Epoch 67; Iter    24/   36] train: loss: 0.2810002
[Epoch 67] ogbg-molsider: 0.608506 val loss: 0.569464
[Epoch 67] ogbg-molsider: 0.569776 test loss: 0.632371
[Epoch 68; Iter    18/   36] train: loss: 0.2910732
[Epoch 68] ogbg-molsider: 0.591060 val loss: 0.613615
[Epoch 68] ogbg-molsider: 0.588187 test loss: 0.691431
[Epoch 69; Iter    12/   36] train: loss: 0.2920006
[Epoch 69] ogbg-molsider: 0.602261 val loss: 0.601221
[Epoch 69] ogbg-molsider: 0.572333 test loss: 0.710487
[Epoch 70; Iter     6/   36] train: loss: 0.3018261
[Epoch 70; Iter    36/   36] train: loss: 0.3331909
[Epoch 70] ogbg-molsider: 0.578487 val loss: 0.713383
[Epoch 70] ogbg-molsider: 0.565942 test loss: 0.825652
[Epoch 71; Iter    30/   36] train: loss: 0.2865820
[Epoch 71] ogbg-molsider: 0.585559 val loss: 0.608287
[Epoch 71] ogbg-molsider: 0.599246 test loss: 0.646537
[Epoch 72; Iter    24/   36] train: loss: 0.2946512
[Epoch 72] ogbg-molsider: 0.584550 val loss: 0.693740
[Epoch 72] ogbg-molsider: 0.588234 test loss: 0.838211
[Epoch 73; Iter    18/   36] train: loss: 0.2645623
[Epoch 73] ogbg-molsider: 0.589427 val loss: 0.627818
[Epoch 73] ogbg-molsider: 0.577638 test loss: 0.675668
[Epoch 74; Iter    12/   36] train: loss: 0.2498358
[Epoch 74] ogbg-molsider: 0.598730 val loss: 0.638445
[Epoch 74] ogbg-molsider: 0.578928 test loss: 0.718372
[Epoch 75; Iter     6/   36] train: loss: 0.2805513
[Epoch 75; Iter    36/   36] train: loss: 0.2943447
[Epoch 75] ogbg-molsider: 0.607273 val loss: 0.644807
[Epoch 75] ogbg-molsider: 0.580450 test loss: 0.760465
[Epoch 76; Iter    30/   36] train: loss: 0.2543047
[Epoch 76] ogbg-molsider: 0.583710 val loss: 1.022249
[Epoch 76] ogbg-molsider: 0.590441 test loss: 0.938159
[Epoch 77; Iter    24/   36] train: loss: 0.2591644
[Epoch 77] ogbg-molsider: 0.593873 val loss: 0.843196
[Epoch 77] ogbg-molsider: 0.583505 test loss: 0.828686
[Epoch 78; Iter    18/   36] train: loss: 0.2403144
[Epoch 78] ogbg-molsider: 0.588020 val loss: 0.780484
[Epoch 78] ogbg-molsider: 0.585868 test loss: 0.803551
[Epoch 79; Iter    12/   36] train: loss: 0.2203731
[Epoch 79] ogbg-molsider: 0.595414 val loss: 1.025687
[Epoch 79] ogbg-molsider: 0.587102 test loss: 1.001009
[Epoch 80; Iter     6/   36] train: loss: 0.2493692
[Epoch 80; Iter    36/   36] train: loss: 0.2271872
[Epoch 80] ogbg-molsider: 0.590958 val loss: 0.742625
[Epoch 80] ogbg-molsider: 0.583745 test loss: 0.864983
[Epoch 33] ogbg-molsider: 0.579997 test loss: 0.506384
[Epoch 34; Iter    12/   36] train: loss: 0.4775609
[Epoch 34] ogbg-molsider: 0.524951 val loss: 0.487962
[Epoch 34] ogbg-molsider: 0.573181 test loss: 0.492527
[Epoch 35; Iter     6/   36] train: loss: 0.5434155
[Epoch 35; Iter    36/   36] train: loss: 0.4637828
[Epoch 35] ogbg-molsider: 0.536251 val loss: 0.524798
[Epoch 35] ogbg-molsider: 0.561188 test loss: 0.507479
[Epoch 36; Iter    30/   36] train: loss: 0.4582161
[Epoch 36] ogbg-molsider: 0.545043 val loss: 0.525661
[Epoch 36] ogbg-molsider: 0.565610 test loss: 0.529233
[Epoch 37; Iter    24/   36] train: loss: 0.5336015
[Epoch 37] ogbg-molsider: 0.525378 val loss: 0.523817
[Epoch 37] ogbg-molsider: 0.588179 test loss: 0.528434
[Epoch 38; Iter    18/   36] train: loss: 0.5071319
[Epoch 38] ogbg-molsider: 0.537541 val loss: 0.553532
[Epoch 38] ogbg-molsider: 0.587525 test loss: 0.542767
[Epoch 39; Iter    12/   36] train: loss: 0.4920564
[Epoch 39] ogbg-molsider: 0.548359 val loss: 0.515179
[Epoch 39] ogbg-molsider: 0.554794 test loss: 0.522188
[Epoch 40; Iter     6/   36] train: loss: 0.4490789
[Epoch 40; Iter    36/   36] train: loss: 0.5005526
[Epoch 40] ogbg-molsider: 0.540664 val loss: 0.568670
[Epoch 40] ogbg-molsider: 0.561663 test loss: 0.588028
[Epoch 41; Iter    30/   36] train: loss: 0.5440958
[Epoch 41] ogbg-molsider: 0.588426 val loss: 0.490693
[Epoch 41] ogbg-molsider: 0.568226 test loss: 0.499730
[Epoch 42; Iter    24/   36] train: loss: 0.4477744
[Epoch 42] ogbg-molsider: 0.565892 val loss: 0.536706
[Epoch 42] ogbg-molsider: 0.612558 test loss: 0.560107
[Epoch 43; Iter    18/   36] train: loss: 0.4655615
[Epoch 43] ogbg-molsider: 0.554627 val loss: 0.515528
[Epoch 43] ogbg-molsider: 0.582536 test loss: 0.517324
[Epoch 44; Iter    12/   36] train: loss: 0.4565504
[Epoch 44] ogbg-molsider: 0.536969 val loss: 32.812580
[Epoch 44] ogbg-molsider: 0.534956 test loss: 57.840384
[Epoch 45; Iter     6/   36] train: loss: 0.4738953
[Epoch 45; Iter    36/   36] train: loss: 0.4320174
[Epoch 45] ogbg-molsider: 0.568437 val loss: 0.527065
[Epoch 45] ogbg-molsider: 0.579075 test loss: 0.526737
[Epoch 46; Iter    30/   36] train: loss: 0.4779405
[Epoch 46] ogbg-molsider: 0.537864 val loss: 0.703522
[Epoch 46] ogbg-molsider: 0.519405 test loss: 0.736707
[Epoch 47; Iter    24/   36] train: loss: 0.4965030
[Epoch 47] ogbg-molsider: 0.556169 val loss: 0.484001
[Epoch 47] ogbg-molsider: 0.560472 test loss: 0.497592
[Epoch 48; Iter    18/   36] train: loss: 0.4507970
[Epoch 48] ogbg-molsider: 0.592433 val loss: 0.520970
[Epoch 48] ogbg-molsider: 0.561089 test loss: 0.580380
[Epoch 49; Iter    12/   36] train: loss: 0.4286376
[Epoch 49] ogbg-molsider: 0.591244 val loss: 0.510756
[Epoch 49] ogbg-molsider: 0.584471 test loss: 0.551460
[Epoch 50; Iter     6/   36] train: loss: 0.4327435
[Epoch 50; Iter    36/   36] train: loss: 0.4160020
[Epoch 50] ogbg-molsider: 0.568705 val loss: 0.607826
[Epoch 50] ogbg-molsider: 0.567935 test loss: 0.621054
[Epoch 51; Iter    30/   36] train: loss: 0.4043600
[Epoch 51] ogbg-molsider: 0.554031 val loss: 0.564414
[Epoch 51] ogbg-molsider: 0.580135 test loss: 0.616373
[Epoch 52; Iter    24/   36] train: loss: 0.4015886
[Epoch 52] ogbg-molsider: 0.595571 val loss: 0.565075
[Epoch 52] ogbg-molsider: 0.587437 test loss: 0.582047
[Epoch 53; Iter    18/   36] train: loss: 0.3684861
[Epoch 53] ogbg-molsider: 0.597173 val loss: 1.090129
[Epoch 53] ogbg-molsider: 0.548226 test loss: 1.375452
[Epoch 54; Iter    12/   36] train: loss: 0.3574934
[Epoch 54] ogbg-molsider: 0.594994 val loss: 0.617216
[Epoch 54] ogbg-molsider: 0.588943 test loss: 0.690133
[Epoch 55; Iter     6/   36] train: loss: 0.3855276
[Epoch 55; Iter    36/   36] train: loss: 0.3568739
[Epoch 55] ogbg-molsider: 0.592692 val loss: 0.525061
[Epoch 55] ogbg-molsider: 0.592668 test loss: 0.544784
[Epoch 56; Iter    30/   36] train: loss: 0.3910430
[Epoch 56] ogbg-molsider: 0.591393 val loss: 0.587771
[Epoch 56] ogbg-molsider: 0.564496 test loss: 0.648286
[Epoch 57; Iter    24/   36] train: loss: 0.3442509
[Epoch 57] ogbg-molsider: 0.603593 val loss: 0.685280
[Epoch 57] ogbg-molsider: 0.574687 test loss: 0.732110
[Epoch 58; Iter    18/   36] train: loss: 0.3156206
[Epoch 58] ogbg-molsider: 0.585258 val loss: 0.596022
[Epoch 58] ogbg-molsider: 0.574466 test loss: 0.725910
[Epoch 59; Iter    12/   36] train: loss: 0.3683384
[Epoch 59] ogbg-molsider: 0.591679 val loss: 0.640718
[Epoch 59] ogbg-molsider: 0.575623 test loss: 0.675264
[Epoch 60; Iter     6/   36] train: loss: 0.4009833
[Epoch 60; Iter    36/   36] train: loss: 0.3257757
[Epoch 60] ogbg-molsider: 0.580688 val loss: 0.656993
[Epoch 60] ogbg-molsider: 0.583684 test loss: 0.672508
[Epoch 61; Iter    30/   36] train: loss: 0.3233520
[Epoch 61] ogbg-molsider: 0.601711 val loss: 0.546831
[Epoch 61] ogbg-molsider: 0.574828 test loss: 0.606577
[Epoch 62; Iter    24/   36] train: loss: 0.3481815
[Epoch 62] ogbg-molsider: 0.606724 val loss: 0.552251
[Epoch 62] ogbg-molsider: 0.594234 test loss: 0.569226
[Epoch 63; Iter    18/   36] train: loss: 0.3530430
[Epoch 63] ogbg-molsider: 0.598210 val loss: 0.648240
[Epoch 63] ogbg-molsider: 0.594544 test loss: 0.650228
[Epoch 64; Iter    12/   36] train: loss: 0.3589532
[Epoch 64] ogbg-molsider: 0.615055 val loss: 0.573140
[Epoch 64] ogbg-molsider: 0.571116 test loss: 0.636940
[Epoch 65; Iter     6/   36] train: loss: 0.3233734
[Epoch 65; Iter    36/   36] train: loss: 0.4479717
[Epoch 65] ogbg-molsider: 0.589334 val loss: 0.589741
[Epoch 65] ogbg-molsider: 0.586371 test loss: 0.793806
[Epoch 66; Iter    30/   36] train: loss: 0.3348812
[Epoch 66] ogbg-molsider: 0.610592 val loss: 0.821119
[Epoch 66] ogbg-molsider: 0.576489 test loss: 0.924643
[Epoch 67; Iter    24/   36] train: loss: 0.3103122
[Epoch 67] ogbg-molsider: 0.587243 val loss: 0.700208
[Epoch 67] ogbg-molsider: 0.573844 test loss: 0.686884
[Epoch 68; Iter    18/   36] train: loss: 0.3326567
[Epoch 68] ogbg-molsider: 0.613776 val loss: 1.974527
[Epoch 68] ogbg-molsider: 0.589787 test loss: 0.756974
[Epoch 69; Iter    12/   36] train: loss: 0.2942381
[Epoch 69] ogbg-molsider: 0.607822 val loss: 0.556428
[Epoch 69] ogbg-molsider: 0.586522 test loss: 0.623105
[Epoch 70; Iter     6/   36] train: loss: 0.2895408
[Epoch 70; Iter    36/   36] train: loss: 0.2999851
[Epoch 70] ogbg-molsider: 0.586659 val loss: 1.280701
[Epoch 70] ogbg-molsider: 0.575502 test loss: 0.961102
[Epoch 71; Iter    30/   36] train: loss: 0.3293465
[Epoch 71] ogbg-molsider: 0.572926 val loss: 0.631257
[Epoch 71] ogbg-molsider: 0.587999 test loss: 0.703654
[Epoch 72; Iter    24/   36] train: loss: 0.2905126
[Epoch 72] ogbg-molsider: 0.604915 val loss: 1.147326
[Epoch 72] ogbg-molsider: 0.598179 test loss: 0.668589
[Epoch 73; Iter    18/   36] train: loss: 0.2976686
[Epoch 73] ogbg-molsider: 0.598105 val loss: 1.959291
[Epoch 73] ogbg-molsider: 0.584742 test loss: 1.142009
[Epoch 74; Iter    12/   36] train: loss: 0.2735656
[Epoch 74] ogbg-molsider: 0.615151 val loss: 0.618614
[Epoch 74] ogbg-molsider: 0.593300 test loss: 0.642154
[Epoch 75; Iter     6/   36] train: loss: 0.2846350
[Epoch 75; Iter    36/   36] train: loss: 0.2963576
[Epoch 75] ogbg-molsider: 0.613250 val loss: 1.397289
[Epoch 75] ogbg-molsider: 0.585283 test loss: 0.769809
[Epoch 76; Iter    30/   36] train: loss: 0.3235550
[Epoch 76] ogbg-molsider: 0.564964 val loss: 0.845148
[Epoch 76] ogbg-molsider: 0.592550 test loss: 0.735697
[Epoch 77; Iter    24/   36] train: loss: 0.3028364
[Epoch 77] ogbg-molsider: 0.585357 val loss: 1.326911
[Epoch 77] ogbg-molsider: 0.576953 test loss: 1.093557
[Epoch 78; Iter    18/   36] train: loss: 0.2897054
[Epoch 78] ogbg-molsider: 0.608318 val loss: 1.776294
[Epoch 78] ogbg-molsider: 0.584439 test loss: 0.738969
[Epoch 79; Iter    12/   36] train: loss: 0.2395492
[Epoch 79] ogbg-molsider: 0.623778 val loss: 0.745379
[Epoch 79] ogbg-molsider: 0.599675 test loss: 0.670363
[Epoch 80; Iter     6/   36] train: loss: 0.2470253
[Epoch 80; Iter    36/   36] train: loss: 0.2477882
[Epoch 80] ogbg-molsider: 0.626324 val loss: 0.898124
[Epoch 80] ogbg-molsider: 0.591096 test loss: 0.696359
[Epoch 33] ogbg-molsider: 0.579111 test loss: 0.497152
[Epoch 34; Iter    12/   36] train: loss: 0.5082282
[Epoch 34] ogbg-molsider: 0.562340 val loss: 0.484555
[Epoch 34] ogbg-molsider: 0.592668 test loss: 0.489601
[Epoch 35; Iter     6/   36] train: loss: 0.5135212
[Epoch 35; Iter    36/   36] train: loss: 0.5141274
[Epoch 35] ogbg-molsider: 0.510880 val loss: 0.509169
[Epoch 35] ogbg-molsider: 0.601824 test loss: 0.508045
[Epoch 36; Iter    30/   36] train: loss: 0.4951767
[Epoch 36] ogbg-molsider: 0.526045 val loss: 0.499393
[Epoch 36] ogbg-molsider: 0.587507 test loss: 0.501647
[Epoch 37; Iter    24/   36] train: loss: 0.4975522
[Epoch 37] ogbg-molsider: 0.517206 val loss: 0.513626
[Epoch 37] ogbg-molsider: 0.591613 test loss: 0.511981
[Epoch 38; Iter    18/   36] train: loss: 0.4836089
[Epoch 38] ogbg-molsider: 0.535030 val loss: 0.486856
[Epoch 38] ogbg-molsider: 0.605078 test loss: 0.486651
[Epoch 39; Iter    12/   36] train: loss: 0.5040147
[Epoch 39] ogbg-molsider: 0.542277 val loss: 0.507111
[Epoch 39] ogbg-molsider: 0.619384 test loss: 0.489900
[Epoch 40; Iter     6/   36] train: loss: 0.4991671
[Epoch 40; Iter    36/   36] train: loss: 0.5109032
[Epoch 40] ogbg-molsider: 0.541756 val loss: 0.609917
[Epoch 40] ogbg-molsider: 0.558194 test loss: 0.690192
[Epoch 41; Iter    30/   36] train: loss: 0.4853044
[Epoch 41] ogbg-molsider: 0.582578 val loss: 0.521229
[Epoch 41] ogbg-molsider: 0.613423 test loss: 0.529991
[Epoch 42; Iter    24/   36] train: loss: 0.4693483
[Epoch 42] ogbg-molsider: 0.582133 val loss: 0.581185
[Epoch 42] ogbg-molsider: 0.602272 test loss: 0.643892
[Epoch 43; Iter    18/   36] train: loss: 0.4690512
[Epoch 43] ogbg-molsider: 0.578272 val loss: 0.497513
[Epoch 43] ogbg-molsider: 0.592797 test loss: 0.508011
[Epoch 44; Iter    12/   36] train: loss: 0.4353291
[Epoch 44] ogbg-molsider: 0.558689 val loss: 0.491302
[Epoch 44] ogbg-molsider: 0.566641 test loss: 0.508184
[Epoch 45; Iter     6/   36] train: loss: 0.4247917
[Epoch 45; Iter    36/   36] train: loss: 0.4376422
[Epoch 45] ogbg-molsider: 0.562260 val loss: 0.537763
[Epoch 45] ogbg-molsider: 0.541548 test loss: 0.616192
[Epoch 46; Iter    30/   36] train: loss: 0.4854147
[Epoch 46] ogbg-molsider: 0.558361 val loss: 0.613004
[Epoch 46] ogbg-molsider: 0.551746 test loss: 0.638787
[Epoch 47; Iter    24/   36] train: loss: 0.4197941
[Epoch 47] ogbg-molsider: 0.551084 val loss: 0.526955
[Epoch 47] ogbg-molsider: 0.599908 test loss: 0.541712
[Epoch 48; Iter    18/   36] train: loss: 0.5101978
[Epoch 48] ogbg-molsider: 0.612986 val loss: 0.563528
[Epoch 48] ogbg-molsider: 0.555850 test loss: 0.781570
[Epoch 49; Iter    12/   36] train: loss: 0.4063608
[Epoch 49] ogbg-molsider: 0.587383 val loss: 0.552916
[Epoch 49] ogbg-molsider: 0.553933 test loss: 0.637085
[Epoch 50; Iter     6/   36] train: loss: 0.4166366
[Epoch 50; Iter    36/   36] train: loss: 0.4494680
[Epoch 50] ogbg-molsider: 0.535004 val loss: 0.570367
[Epoch 50] ogbg-molsider: 0.593095 test loss: 0.594604
[Epoch 51; Iter    30/   36] train: loss: 0.3870593
[Epoch 51] ogbg-molsider: 0.547028 val loss: 0.579822
[Epoch 51] ogbg-molsider: 0.581530 test loss: 0.633098
[Epoch 52; Iter    24/   36] train: loss: 0.3918155
[Epoch 52] ogbg-molsider: 0.563718 val loss: 0.613960
[Epoch 52] ogbg-molsider: 0.564845 test loss: 1.018316
[Epoch 53; Iter    18/   36] train: loss: 0.3879857
[Epoch 53] ogbg-molsider: 0.582530 val loss: 0.544012
[Epoch 53] ogbg-molsider: 0.574341 test loss: 0.612871
[Epoch 54; Iter    12/   36] train: loss: 0.3810162
[Epoch 54] ogbg-molsider: 0.560702 val loss: 0.755555
[Epoch 54] ogbg-molsider: 0.537494 test loss: 0.876627
[Epoch 55; Iter     6/   36] train: loss: 0.3757138
[Epoch 55; Iter    36/   36] train: loss: 0.4443910
[Epoch 55] ogbg-molsider: 0.571173 val loss: 0.568395
[Epoch 55] ogbg-molsider: 0.561211 test loss: 0.630608
[Epoch 56; Iter    30/   36] train: loss: 0.3719847
[Epoch 56] ogbg-molsider: 0.588123 val loss: 0.648178
[Epoch 56] ogbg-molsider: 0.549509 test loss: 0.816644
[Epoch 57; Iter    24/   36] train: loss: 0.3445661
[Epoch 57] ogbg-molsider: 0.580523 val loss: 0.596021
[Epoch 57] ogbg-molsider: 0.581944 test loss: 0.762806
[Epoch 58; Iter    18/   36] train: loss: 0.3675712
[Epoch 58] ogbg-molsider: 0.554024 val loss: 0.757608
[Epoch 58] ogbg-molsider: 0.577064 test loss: 0.820774
[Epoch 59; Iter    12/   36] train: loss: 0.3445085
[Epoch 59] ogbg-molsider: 0.562532 val loss: 0.636654
[Epoch 59] ogbg-molsider: 0.577985 test loss: 0.763854
[Epoch 60; Iter     6/   36] train: loss: 0.4024954
[Epoch 60; Iter    36/   36] train: loss: 0.3384334
[Epoch 60] ogbg-molsider: 0.579005 val loss: 1.034619
[Epoch 60] ogbg-molsider: 0.580243 test loss: 0.730019
[Epoch 61; Iter    30/   36] train: loss: 0.3577008
[Epoch 61] ogbg-molsider: 0.577178 val loss: 0.701993
[Epoch 61] ogbg-molsider: 0.558734 test loss: 0.856975
[Epoch 62; Iter    24/   36] train: loss: 0.3307403
[Epoch 62] ogbg-molsider: 0.591273 val loss: 0.576619
[Epoch 62] ogbg-molsider: 0.586217 test loss: 0.637496
[Epoch 63; Iter    18/   36] train: loss: 0.3504263
[Epoch 63] ogbg-molsider: 0.574070 val loss: 0.733880
[Epoch 63] ogbg-molsider: 0.579890 test loss: 0.735027
[Epoch 64; Iter    12/   36] train: loss: 0.3106811
[Epoch 64] ogbg-molsider: 0.581271 val loss: 0.617040
[Epoch 64] ogbg-molsider: 0.596942 test loss: 0.681578
[Epoch 65; Iter     6/   36] train: loss: 0.3460886
[Epoch 65; Iter    36/   36] train: loss: 0.3758924
[Epoch 65] ogbg-molsider: 0.564207 val loss: 0.633002
[Epoch 65] ogbg-molsider: 0.567530 test loss: 0.738114
[Epoch 66; Iter    30/   36] train: loss: 0.3024171
[Epoch 66] ogbg-molsider: 0.582680 val loss: 0.637579
[Epoch 66] ogbg-molsider: 0.601684 test loss: 0.720986
[Epoch 67; Iter    24/   36] train: loss: 0.3314730
[Epoch 67] ogbg-molsider: 0.595386 val loss: 0.559436
[Epoch 67] ogbg-molsider: 0.590173 test loss: 0.615245
[Epoch 68; Iter    18/   36] train: loss: 0.3118086
[Epoch 68] ogbg-molsider: 0.581643 val loss: 0.748367
[Epoch 68] ogbg-molsider: 0.595638 test loss: 0.649564
[Epoch 69; Iter    12/   36] train: loss: 0.3199352
[Epoch 69] ogbg-molsider: 0.582017 val loss: 0.645364
[Epoch 69] ogbg-molsider: 0.577766 test loss: 0.754225
[Epoch 70; Iter     6/   36] train: loss: 0.2962297
[Epoch 70; Iter    36/   36] train: loss: 0.3314860
[Epoch 70] ogbg-molsider: 0.576041 val loss: 0.622914
[Epoch 70] ogbg-molsider: 0.576312 test loss: 0.656774
[Epoch 71; Iter    30/   36] train: loss: 0.3262020
[Epoch 71] ogbg-molsider: 0.595374 val loss: 0.647644
[Epoch 71] ogbg-molsider: 0.585927 test loss: 0.764644
[Epoch 72; Iter    24/   36] train: loss: 0.3148607
[Epoch 72] ogbg-molsider: 0.594470 val loss: 0.662991
[Epoch 72] ogbg-molsider: 0.597218 test loss: 0.641437
[Epoch 73; Iter    18/   36] train: loss: 0.3022442
[Epoch 73] ogbg-molsider: 0.571656 val loss: 0.620669
[Epoch 73] ogbg-molsider: 0.591225 test loss: 0.677310
[Epoch 74; Iter    12/   36] train: loss: 0.2876217
[Epoch 74] ogbg-molsider: 0.602010 val loss: 0.692102
[Epoch 74] ogbg-molsider: 0.593937 test loss: 0.795425
[Epoch 75; Iter     6/   36] train: loss: 0.2976031
[Epoch 75; Iter    36/   36] train: loss: 0.3151955
[Epoch 75] ogbg-molsider: 0.571017 val loss: 1.108174
[Epoch 75] ogbg-molsider: 0.579122 test loss: 1.025835
[Epoch 76; Iter    30/   36] train: loss: 0.2903942
[Epoch 76] ogbg-molsider: 0.599577 val loss: 0.736483
[Epoch 76] ogbg-molsider: 0.584417 test loss: 0.966803
[Epoch 77; Iter    24/   36] train: loss: 0.2512088
[Epoch 77] ogbg-molsider: 0.588315 val loss: 0.654425
[Epoch 77] ogbg-molsider: 0.587917 test loss: 0.778419
[Epoch 78; Iter    18/   36] train: loss: 0.2539561
[Epoch 78] ogbg-molsider: 0.588645 val loss: 0.667047
[Epoch 78] ogbg-molsider: 0.594862 test loss: 0.770822
[Epoch 79; Iter    12/   36] train: loss: 0.2478419
[Epoch 79] ogbg-molsider: 0.597466 val loss: 0.650998
[Epoch 79] ogbg-molsider: 0.578293 test loss: 0.799410
[Epoch 80; Iter     6/   36] train: loss: 0.2539692
[Epoch 80; Iter    36/   36] train: loss: 0.2657644
[Epoch 80] ogbg-molsider: 0.594396 val loss: 0.673093
[Epoch 80] ogbg-molsider: 0.582157 test loss: 0.813982
[Epoch 33] ogbg-molsider: 0.547679 test loss: 1.889116
[Epoch 34; Iter    12/   36] train: loss: 0.4898764
[Epoch 34] ogbg-molsider: 0.512284 val loss: 1.595629
[Epoch 34] ogbg-molsider: 0.543360 test loss: 1.869848
[Epoch 35; Iter     6/   36] train: loss: 0.5536614
[Epoch 35; Iter    36/   36] train: loss: 0.5123010
[Epoch 35] ogbg-molsider: 0.522511 val loss: 1.437823
[Epoch 35] ogbg-molsider: 0.530356 test loss: 1.750105
[Epoch 36; Iter    30/   36] train: loss: 0.4917839
[Epoch 36] ogbg-molsider: 0.493508 val loss: 1.494225
[Epoch 36] ogbg-molsider: 0.549574 test loss: 1.785739
[Epoch 37; Iter    24/   36] train: loss: 0.4553286
[Epoch 37] ogbg-molsider: 0.506118 val loss: 1.812720
[Epoch 37] ogbg-molsider: 0.512242 test loss: 2.002012
[Epoch 38; Iter    18/   36] train: loss: 0.5075426
[Epoch 38] ogbg-molsider: 0.516723 val loss: 4.777254
[Epoch 38] ogbg-molsider: 0.518044 test loss: 4.127207
[Epoch 39; Iter    12/   36] train: loss: 0.5057670
[Epoch 39] ogbg-molsider: 0.534613 val loss: 5.245240
[Epoch 39] ogbg-molsider: 0.529110 test loss: 5.018233
[Epoch 40; Iter     6/   36] train: loss: 0.4702196
[Epoch 40; Iter    36/   36] train: loss: 0.5330287
[Epoch 40] ogbg-molsider: 0.534713 val loss: 10.079253
[Epoch 40] ogbg-molsider: 0.518182 test loss: 13.008453
[Epoch 41; Iter    30/   36] train: loss: 0.5024595
[Epoch 41] ogbg-molsider: 0.554145 val loss: 2.829093
[Epoch 41] ogbg-molsider: 0.513064 test loss: 5.341133
[Epoch 42; Iter    24/   36] train: loss: 0.4719249
[Epoch 42] ogbg-molsider: 0.550930 val loss: 5.277360
[Epoch 42] ogbg-molsider: 0.515867 test loss: 6.703161
[Epoch 43; Iter    18/   36] train: loss: 0.4361971
[Epoch 43] ogbg-molsider: 0.546435 val loss: 1.985556
[Epoch 43] ogbg-molsider: 0.528947 test loss: 1.729801
[Epoch 44; Iter    12/   36] train: loss: 0.4499218
[Epoch 44] ogbg-molsider: 0.561959 val loss: 14.687266
[Epoch 44] ogbg-molsider: 0.537886 test loss: 22.549357
[Epoch 45; Iter     6/   36] train: loss: 0.4446319
[Epoch 45; Iter    36/   36] train: loss: 0.4351981
[Epoch 45] ogbg-molsider: 0.554934 val loss: 7.455040
[Epoch 45] ogbg-molsider: 0.566914 test loss: 10.636081
[Epoch 46; Iter    30/   36] train: loss: 0.4451758
[Epoch 46] ogbg-molsider: 0.539385 val loss: 0.741326
[Epoch 46] ogbg-molsider: 0.577239 test loss: 0.816200
[Epoch 47; Iter    24/   36] train: loss: 0.4375990
[Epoch 47] ogbg-molsider: 0.554004 val loss: 0.892336
[Epoch 47] ogbg-molsider: 0.546323 test loss: 1.139487
[Epoch 48; Iter    18/   36] train: loss: 0.4718778
[Epoch 48] ogbg-molsider: 0.596749 val loss: 1.269459
[Epoch 48] ogbg-molsider: 0.547397 test loss: 2.258420
[Epoch 49; Iter    12/   36] train: loss: 0.4067039
[Epoch 49] ogbg-molsider: 0.584108 val loss: 0.820628
[Epoch 49] ogbg-molsider: 0.567247 test loss: 1.007922
[Epoch 50; Iter     6/   36] train: loss: 0.4447690
[Epoch 50; Iter    36/   36] train: loss: 0.4169909
[Epoch 50] ogbg-molsider: 0.511233 val loss: 0.621648
[Epoch 50] ogbg-molsider: 0.568912 test loss: 0.604783
[Epoch 51; Iter    30/   36] train: loss: 0.3697263
[Epoch 51] ogbg-molsider: 0.558598 val loss: 0.783230
[Epoch 51] ogbg-molsider: 0.565064 test loss: 0.831270
[Epoch 52; Iter    24/   36] train: loss: 0.3670904
[Epoch 52] ogbg-molsider: 0.529607 val loss: 0.787935
[Epoch 52] ogbg-molsider: 0.561351 test loss: 0.791578
[Epoch 53; Iter    18/   36] train: loss: 0.3655834
[Epoch 53] ogbg-molsider: 0.537122 val loss: 0.694136
[Epoch 53] ogbg-molsider: 0.569693 test loss: 0.665600
[Epoch 54; Iter    12/   36] train: loss: 0.3807679
[Epoch 54] ogbg-molsider: 0.533605 val loss: 0.800101
[Epoch 54] ogbg-molsider: 0.560069 test loss: 0.797735
[Epoch 55; Iter     6/   36] train: loss: 0.3478615
[Epoch 55; Iter    36/   36] train: loss: 0.3607167
[Epoch 55] ogbg-molsider: 0.534798 val loss: 0.723206
[Epoch 55] ogbg-molsider: 0.576126 test loss: 0.685423
[Epoch 56; Iter    30/   36] train: loss: 0.3409691
[Epoch 56] ogbg-molsider: 0.523944 val loss: 0.665158
[Epoch 56] ogbg-molsider: 0.575380 test loss: 0.654643
[Epoch 57; Iter    24/   36] train: loss: 0.3421132
[Epoch 57] ogbg-molsider: 0.533931 val loss: 0.708323
[Epoch 57] ogbg-molsider: 0.564924 test loss: 1.082901
[Epoch 58; Iter    18/   36] train: loss: 0.3619463
[Epoch 58] ogbg-molsider: 0.538540 val loss: 0.698664
[Epoch 58] ogbg-molsider: 0.566988 test loss: 1.084618
[Epoch 59; Iter    12/   36] train: loss: 0.3287083
[Epoch 59] ogbg-molsider: 0.507735 val loss: 0.969728
[Epoch 59] ogbg-molsider: 0.539711 test loss: 1.417140
[Epoch 60; Iter     6/   36] train: loss: 0.3255189
[Epoch 60; Iter    36/   36] train: loss: 0.3577534
[Epoch 60] ogbg-molsider: 0.525371 val loss: 0.739615
[Epoch 60] ogbg-molsider: 0.558583 test loss: 1.109635
[Epoch 61; Iter    30/   36] train: loss: 0.3515372
[Epoch 61] ogbg-molsider: 0.521106 val loss: 0.901189
[Epoch 61] ogbg-molsider: 0.562928 test loss: 1.476286
[Epoch 62; Iter    24/   36] train: loss: 0.3441987
[Epoch 62] ogbg-molsider: 0.542023 val loss: 0.652249
[Epoch 62] ogbg-molsider: 0.577324 test loss: 0.821718
[Epoch 63; Iter    18/   36] train: loss: 0.3431947
[Epoch 63] ogbg-molsider: 0.526451 val loss: 0.705579
[Epoch 63] ogbg-molsider: 0.587386 test loss: 0.701516
[Epoch 64; Iter    12/   36] train: loss: 0.3170993
[Epoch 64] ogbg-molsider: 0.542557 val loss: 0.741204
[Epoch 64] ogbg-molsider: 0.576652 test loss: 0.724907
[Epoch 65; Iter     6/   36] train: loss: 0.2680441
[Epoch 65; Iter    36/   36] train: loss: 0.3104354
[Epoch 65] ogbg-molsider: 0.533554 val loss: 0.852253
[Epoch 65] ogbg-molsider: 0.556975 test loss: 1.195033
[Epoch 66; Iter    30/   36] train: loss: 0.3038076
[Epoch 66] ogbg-molsider: 0.531438 val loss: 0.756003
[Epoch 66] ogbg-molsider: 0.569254 test loss: 0.740888
[Epoch 67; Iter    24/   36] train: loss: 0.2729366
[Epoch 67] ogbg-molsider: 0.566533 val loss: 0.769197
[Epoch 67] ogbg-molsider: 0.555944 test loss: 1.192419
[Epoch 68; Iter    18/   36] train: loss: 0.2917195
[Epoch 68] ogbg-molsider: 0.531577 val loss: 0.820816
[Epoch 68] ogbg-molsider: 0.565339 test loss: 3.149774
[Epoch 69; Iter    12/   36] train: loss: 0.3140812
[Epoch 69] ogbg-molsider: 0.562523 val loss: 2.009259
[Epoch 69] ogbg-molsider: 0.569646 test loss: 3.007837
[Epoch 70; Iter     6/   36] train: loss: 0.2947572
[Epoch 70; Iter    36/   36] train: loss: 0.3030974
[Epoch 70] ogbg-molsider: 0.516019 val loss: 1.031426
[Epoch 70] ogbg-molsider: 0.550228 test loss: 1.929254
[Epoch 71; Iter    30/   36] train: loss: 0.3091005
[Epoch 71] ogbg-molsider: 0.553566 val loss: 0.935227
[Epoch 71] ogbg-molsider: 0.549016 test loss: 1.316222
[Epoch 72; Iter    24/   36] train: loss: 0.2942979
[Epoch 72] ogbg-molsider: 0.557938 val loss: 0.849763
[Epoch 72] ogbg-molsider: 0.546926 test loss: 0.940398
[Epoch 73; Iter    18/   36] train: loss: 0.2480025
[Epoch 73] ogbg-molsider: 0.574581 val loss: 1.657532
[Epoch 73] ogbg-molsider: 0.551546 test loss: 3.682034
[Epoch 74; Iter    12/   36] train: loss: 0.2463053
[Epoch 74] ogbg-molsider: 0.553271 val loss: 0.736852
[Epoch 74] ogbg-molsider: 0.573431 test loss: 0.771081
[Epoch 75; Iter     6/   36] train: loss: 0.2927492
[Epoch 75; Iter    36/   36] train: loss: 0.2658704
[Epoch 75] ogbg-molsider: 0.571667 val loss: 2.945772
[Epoch 75] ogbg-molsider: 0.553612 test loss: 9.206046
[Epoch 76; Iter    30/   36] train: loss: 0.2568459
[Epoch 76] ogbg-molsider: 0.539677 val loss: 1.235641
[Epoch 76] ogbg-molsider: 0.557512 test loss: 3.286635
[Epoch 77; Iter    24/   36] train: loss: 0.2695906
[Epoch 77] ogbg-molsider: 0.578144 val loss: 0.773283
[Epoch 77] ogbg-molsider: 0.552638 test loss: 0.807237
[Epoch 78; Iter    18/   36] train: loss: 0.2537188
[Epoch 78] ogbg-molsider: 0.567965 val loss: 0.814681
[Epoch 78] ogbg-molsider: 0.571962 test loss: 0.827557
[Epoch 79; Iter    12/   36] train: loss: 0.2253104
[Epoch 79] ogbg-molsider: 0.563664 val loss: 0.818659
[Epoch 79] ogbg-molsider: 0.575381 test loss: 1.710435
[Epoch 80; Iter     6/   36] train: loss: 0.2818481
[Epoch 80; Iter    36/   36] train: loss: 0.2373153
[Epoch 80] ogbg-molsider: 0.558936 val loss: 1.064633
[Epoch 80] ogbg-molsider: 0.571212 test loss: 2.100317
[Epoch 33] ogbg-molsider: 0.600484 test loss: 0.495684
[Epoch 34; Iter    12/   36] train: loss: 0.4888135
[Epoch 34] ogbg-molsider: 0.530895 val loss: 0.497537
[Epoch 34] ogbg-molsider: 0.583594 test loss: 0.504344
[Epoch 35; Iter     6/   36] train: loss: 0.5326390
[Epoch 35; Iter    36/   36] train: loss: 0.4624974
[Epoch 35] ogbg-molsider: 0.508213 val loss: 0.503192
[Epoch 35] ogbg-molsider: 0.597756 test loss: 0.492944
[Epoch 36; Iter    30/   36] train: loss: 0.4550773
[Epoch 36] ogbg-molsider: 0.561462 val loss: 0.494037
[Epoch 36] ogbg-molsider: 0.574955 test loss: 0.503429
[Epoch 37; Iter    24/   36] train: loss: 0.5296694
[Epoch 37] ogbg-molsider: 0.524028 val loss: 0.495223
[Epoch 37] ogbg-molsider: 0.603069 test loss: 0.502189
[Epoch 38; Iter    18/   36] train: loss: 0.5081436
[Epoch 38] ogbg-molsider: 0.547613 val loss: 0.485892
[Epoch 38] ogbg-molsider: 0.590254 test loss: 0.492030
[Epoch 39; Iter    12/   36] train: loss: 0.5037417
[Epoch 39] ogbg-molsider: 0.556700 val loss: 0.490131
[Epoch 39] ogbg-molsider: 0.585606 test loss: 0.508230
[Epoch 40; Iter     6/   36] train: loss: 0.4721322
[Epoch 40; Iter    36/   36] train: loss: 0.5437101
[Epoch 40] ogbg-molsider: 0.539883 val loss: 0.502632
[Epoch 40] ogbg-molsider: 0.586910 test loss: 0.528781
[Epoch 41; Iter    30/   36] train: loss: 0.5157706
[Epoch 41] ogbg-molsider: 0.533043 val loss: 1.767982
[Epoch 41] ogbg-molsider: 0.556975 test loss: 0.825431
[Epoch 42; Iter    24/   36] train: loss: 0.4829299
[Epoch 42] ogbg-molsider: 0.522357 val loss: 0.552061
[Epoch 42] ogbg-molsider: 0.620704 test loss: 0.558111
[Epoch 43; Iter    18/   36] train: loss: 0.4895422
[Epoch 43] ogbg-molsider: 0.561318 val loss: 0.556552
[Epoch 43] ogbg-molsider: 0.545392 test loss: 0.623062
[Epoch 44; Iter    12/   36] train: loss: 0.4943891
[Epoch 44] ogbg-molsider: 0.591909 val loss: 0.505158
[Epoch 44] ogbg-molsider: 0.584327 test loss: 0.566626
[Epoch 45; Iter     6/   36] train: loss: 0.4698718
[Epoch 45; Iter    36/   36] train: loss: 0.4509087
[Epoch 45] ogbg-molsider: 0.514869 val loss: 0.541509
[Epoch 45] ogbg-molsider: 0.575835 test loss: 0.551710
[Epoch 46; Iter    30/   36] train: loss: 0.4875160
[Epoch 46] ogbg-molsider: 0.560789 val loss: 0.519268
[Epoch 46] ogbg-molsider: 0.588722 test loss: 0.529823
[Epoch 47; Iter    24/   36] train: loss: 0.4595189
[Epoch 47] ogbg-molsider: 0.554151 val loss: 0.504078
[Epoch 47] ogbg-molsider: 0.581896 test loss: 0.518117
[Epoch 48; Iter    18/   36] train: loss: 0.4521352
[Epoch 48] ogbg-molsider: 0.556501 val loss: 0.538096
[Epoch 48] ogbg-molsider: 0.553565 test loss: 0.566075
[Epoch 49; Iter    12/   36] train: loss: 0.4596013
[Epoch 49] ogbg-molsider: 0.570122 val loss: 0.531681
[Epoch 49] ogbg-molsider: 0.579176 test loss: 0.573513
[Epoch 50; Iter     6/   36] train: loss: 0.4752048
[Epoch 50; Iter    36/   36] train: loss: 0.4487674
[Epoch 50] ogbg-molsider: 0.549082 val loss: 0.526122
[Epoch 50] ogbg-molsider: 0.568752 test loss: 0.534696
[Epoch 51; Iter    30/   36] train: loss: 0.4706919
[Epoch 51] ogbg-molsider: 0.591529 val loss: 1.296534
[Epoch 51] ogbg-molsider: 0.577110 test loss: 1.533982
[Epoch 52; Iter    24/   36] train: loss: 0.4140188
[Epoch 52] ogbg-molsider: 0.549832 val loss: 0.529358
[Epoch 52] ogbg-molsider: 0.579332 test loss: 0.541817
[Epoch 53; Iter    18/   36] train: loss: 0.3583055
[Epoch 53] ogbg-molsider: 0.568182 val loss: 0.551018
[Epoch 53] ogbg-molsider: 0.577090 test loss: 0.565637
[Epoch 54; Iter    12/   36] train: loss: 0.3540242
[Epoch 54] ogbg-molsider: 0.599822 val loss: 0.541590
[Epoch 54] ogbg-molsider: 0.565142 test loss: 0.607707
[Epoch 55; Iter     6/   36] train: loss: 0.3956126
[Epoch 55; Iter    36/   36] train: loss: 0.3686888
[Epoch 55] ogbg-molsider: 0.586953 val loss: 0.520213
[Epoch 55] ogbg-molsider: 0.566052 test loss: 0.552740
[Epoch 56; Iter    30/   36] train: loss: 0.4082004
[Epoch 56] ogbg-molsider: 0.607658 val loss: 0.518715
[Epoch 56] ogbg-molsider: 0.573459 test loss: 0.572797
[Epoch 57; Iter    24/   36] train: loss: 0.3686613
[Epoch 57] ogbg-molsider: 0.596860 val loss: 0.532344
[Epoch 57] ogbg-molsider: 0.612986 test loss: 0.537624
[Epoch 58; Iter    18/   36] train: loss: 0.3399876
[Epoch 58] ogbg-molsider: 0.595838 val loss: 0.530392
[Epoch 58] ogbg-molsider: 0.559130 test loss: 0.567083
[Epoch 59; Iter    12/   36] train: loss: 0.3566103
[Epoch 59] ogbg-molsider: 0.616295 val loss: 0.596079
[Epoch 59] ogbg-molsider: 0.573763 test loss: 0.667421
[Epoch 60; Iter     6/   36] train: loss: 0.4134924
[Epoch 60; Iter    36/   36] train: loss: 0.3335666
[Epoch 60] ogbg-molsider: 0.578266 val loss: 0.535938
[Epoch 60] ogbg-molsider: 0.590791 test loss: 0.573956
[Epoch 61; Iter    30/   36] train: loss: 0.3410346
[Epoch 61] ogbg-molsider: 0.585326 val loss: 0.556148
[Epoch 61] ogbg-molsider: 0.567525 test loss: 0.589841
[Epoch 62; Iter    24/   36] train: loss: 0.3565951
[Epoch 62] ogbg-molsider: 0.593512 val loss: 0.598920
[Epoch 62] ogbg-molsider: 0.583372 test loss: 0.662018
[Epoch 63; Iter    18/   36] train: loss: 0.3585817
[Epoch 63] ogbg-molsider: 0.581381 val loss: 0.545309
[Epoch 63] ogbg-molsider: 0.551440 test loss: 0.610194
[Epoch 64; Iter    12/   36] train: loss: 0.3485861
[Epoch 64] ogbg-molsider: 0.622928 val loss: 0.627280
[Epoch 64] ogbg-molsider: 0.572663 test loss: 0.732601
[Epoch 65; Iter     6/   36] train: loss: 0.3170941
[Epoch 65; Iter    36/   36] train: loss: 0.4465557
[Epoch 65] ogbg-molsider: 0.618117 val loss: 0.560150
[Epoch 65] ogbg-molsider: 0.564736 test loss: 0.628038
[Epoch 66; Iter    30/   36] train: loss: 0.3375193
[Epoch 66] ogbg-molsider: 0.598917 val loss: 0.603247
[Epoch 66] ogbg-molsider: 0.554104 test loss: 0.716478
[Epoch 67; Iter    24/   36] train: loss: 0.3359429
[Epoch 67] ogbg-molsider: 0.614992 val loss: 0.555944
[Epoch 67] ogbg-molsider: 0.569130 test loss: 0.629477
[Epoch 68; Iter    18/   36] train: loss: 0.3237811
[Epoch 68] ogbg-molsider: 0.571234 val loss: 0.575787
[Epoch 68] ogbg-molsider: 0.575236 test loss: 0.636030
[Epoch 69; Iter    12/   36] train: loss: 0.3056647
[Epoch 69] ogbg-molsider: 0.599202 val loss: 0.619134
[Epoch 69] ogbg-molsider: 0.587504 test loss: 0.682407
[Epoch 70; Iter     6/   36] train: loss: 0.3112713
[Epoch 70; Iter    36/   36] train: loss: 0.3035059
[Epoch 70] ogbg-molsider: 0.596232 val loss: 0.569927
[Epoch 70] ogbg-molsider: 0.580258 test loss: 0.615343
[Epoch 71; Iter    30/   36] train: loss: 0.3355276
[Epoch 71] ogbg-molsider: 0.608101 val loss: 0.531946
[Epoch 71] ogbg-molsider: 0.576406 test loss: 0.602660
[Epoch 72; Iter    24/   36] train: loss: 0.3123803
[Epoch 72] ogbg-molsider: 0.607104 val loss: 0.655597
[Epoch 72] ogbg-molsider: 0.592604 test loss: 0.698143
[Epoch 73; Iter    18/   36] train: loss: 0.3049166
[Epoch 73] ogbg-molsider: 0.585094 val loss: 0.606342
[Epoch 73] ogbg-molsider: 0.583789 test loss: 0.657949
[Epoch 74; Iter    12/   36] train: loss: 0.2663778
[Epoch 74] ogbg-molsider: 0.633983 val loss: 0.582425
[Epoch 74] ogbg-molsider: 0.593463 test loss: 0.639160
[Epoch 75; Iter     6/   36] train: loss: 0.2993866
[Epoch 75; Iter    36/   36] train: loss: 0.3312506
[Epoch 75] ogbg-molsider: 0.587323 val loss: 0.668229
[Epoch 75] ogbg-molsider: 0.568493 test loss: 0.708694
[Epoch 76; Iter    30/   36] train: loss: 0.3367025
[Epoch 76] ogbg-molsider: 0.567341 val loss: 0.573996
[Epoch 76] ogbg-molsider: 0.586716 test loss: 0.592012
[Epoch 77; Iter    24/   36] train: loss: 0.3026916
[Epoch 77] ogbg-molsider: 0.600391 val loss: 0.610007
[Epoch 77] ogbg-molsider: 0.584792 test loss: 0.647511
[Epoch 78; Iter    18/   36] train: loss: 0.2861117
[Epoch 78] ogbg-molsider: 0.597875 val loss: 0.604768
[Epoch 78] ogbg-molsider: 0.583565 test loss: 0.665073
[Epoch 79; Iter    12/   36] train: loss: 0.2493500
[Epoch 79] ogbg-molsider: 0.603193 val loss: 0.564855
[Epoch 79] ogbg-molsider: 0.589349 test loss: 0.613868
[Epoch 80; Iter     6/   36] train: loss: 0.2642069
[Epoch 80; Iter    36/   36] train: loss: 0.2574758
[Epoch 80] ogbg-molsider: 0.598337 val loss: 0.582535
[Epoch 80] ogbg-molsider: 0.592045 test loss: 0.629941
[Epoch 33] ogbg-molsider: 0.592132 test loss: 0.491012
[Epoch 34; Iter    12/   36] train: loss: 0.5176470
[Epoch 34] ogbg-molsider: 0.530290 val loss: 0.489689
[Epoch 34] ogbg-molsider: 0.577342 test loss: 0.499060
[Epoch 35; Iter     6/   36] train: loss: 0.4993771
[Epoch 35; Iter    36/   36] train: loss: 0.5180682
[Epoch 35] ogbg-molsider: 0.521504 val loss: 0.510153
[Epoch 35] ogbg-molsider: 0.578889 test loss: 0.522459
[Epoch 36; Iter    30/   36] train: loss: 0.4947937
[Epoch 36] ogbg-molsider: 0.525943 val loss: 0.496902
[Epoch 36] ogbg-molsider: 0.586285 test loss: 0.507370
[Epoch 37; Iter    24/   36] train: loss: 0.5067055
[Epoch 37] ogbg-molsider: 0.512714 val loss: 0.503930
[Epoch 37] ogbg-molsider: 0.598761 test loss: 0.511192
[Epoch 38; Iter    18/   36] train: loss: 0.4760057
[Epoch 38] ogbg-molsider: 0.529098 val loss: 0.495180
[Epoch 38] ogbg-molsider: 0.580392 test loss: 0.505151
[Epoch 39; Iter    12/   36] train: loss: 0.5178367
[Epoch 39] ogbg-molsider: 0.550491 val loss: 0.493450
[Epoch 39] ogbg-molsider: 0.600400 test loss: 0.503670
[Epoch 40; Iter     6/   36] train: loss: 0.4857013
[Epoch 40; Iter    36/   36] train: loss: 0.4949645
[Epoch 40] ogbg-molsider: 0.519589 val loss: 0.653158
[Epoch 40] ogbg-molsider: 0.566285 test loss: 0.860076
[Epoch 41; Iter    30/   36] train: loss: 0.4664798
[Epoch 41] ogbg-molsider: 0.587032 val loss: 0.514892
[Epoch 41] ogbg-molsider: 0.558095 test loss: 0.555914
[Epoch 42; Iter    24/   36] train: loss: 0.4106729
[Epoch 42] ogbg-molsider: 0.568681 val loss: 0.504175
[Epoch 42] ogbg-molsider: 0.602185 test loss: 0.517521
[Epoch 43; Iter    18/   36] train: loss: 0.4811954
[Epoch 43] ogbg-molsider: 0.559571 val loss: 0.695076
[Epoch 43] ogbg-molsider: 0.543421 test loss: 0.850954
[Epoch 44; Iter    12/   36] train: loss: 0.4674902
[Epoch 44] ogbg-molsider: 0.580543 val loss: 0.511355
[Epoch 44] ogbg-molsider: 0.588549 test loss: 0.542339
[Epoch 45; Iter     6/   36] train: loss: 0.4267866
[Epoch 45; Iter    36/   36] train: loss: 0.4165135
[Epoch 45] ogbg-molsider: 0.542920 val loss: 0.581424
[Epoch 45] ogbg-molsider: 0.587756 test loss: 0.579663
[Epoch 46; Iter    30/   36] train: loss: 0.4420316
[Epoch 46] ogbg-molsider: 0.589664 val loss: 0.566835
[Epoch 46] ogbg-molsider: 0.566654 test loss: 0.643728
[Epoch 47; Iter    24/   36] train: loss: 0.4106363
[Epoch 47] ogbg-molsider: 0.604356 val loss: 0.596776
[Epoch 47] ogbg-molsider: 0.565933 test loss: 0.720016
[Epoch 48; Iter    18/   36] train: loss: 0.5442246
[Epoch 48] ogbg-molsider: 0.590739 val loss: 6.489974
[Epoch 48] ogbg-molsider: 0.555717 test loss: 11.457111
[Epoch 49; Iter    12/   36] train: loss: 0.3918183
[Epoch 49] ogbg-molsider: 0.551726 val loss: 0.608494
[Epoch 49] ogbg-molsider: 0.539902 test loss: 0.670537
[Epoch 50; Iter     6/   36] train: loss: 0.3947658
[Epoch 50; Iter    36/   36] train: loss: 0.4716804
[Epoch 50] ogbg-molsider: 0.526285 val loss: 0.583816
[Epoch 50] ogbg-molsider: 0.559592 test loss: 0.613694
[Epoch 51; Iter    30/   36] train: loss: 0.4083986
[Epoch 51] ogbg-molsider: 0.558048 val loss: 0.686438
[Epoch 51] ogbg-molsider: 0.553559 test loss: 0.815194
[Epoch 52; Iter    24/   36] train: loss: 0.4326835
[Epoch 52] ogbg-molsider: 0.555611 val loss: 0.583332
[Epoch 52] ogbg-molsider: 0.555215 test loss: 0.675084
[Epoch 53; Iter    18/   36] train: loss: 0.3809186
[Epoch 53] ogbg-molsider: 0.538812 val loss: 1.569096
[Epoch 53] ogbg-molsider: 0.545276 test loss: 1.366551
[Epoch 54; Iter    12/   36] train: loss: 0.3937458
[Epoch 54] ogbg-molsider: 0.574111 val loss: 0.762664
[Epoch 54] ogbg-molsider: 0.541071 test loss: 0.967183
[Epoch 55; Iter     6/   36] train: loss: 0.3737441
[Epoch 55; Iter    36/   36] train: loss: 0.4516699
[Epoch 55] ogbg-molsider: 0.589193 val loss: 0.677348
[Epoch 55] ogbg-molsider: 0.549146 test loss: 0.856013
[Epoch 56; Iter    30/   36] train: loss: 0.3663532
[Epoch 56] ogbg-molsider: 0.574155 val loss: 0.723411
[Epoch 56] ogbg-molsider: 0.561010 test loss: 0.906505
[Epoch 57; Iter    24/   36] train: loss: 0.3524856
[Epoch 57] ogbg-molsider: 0.605712 val loss: 0.591393
[Epoch 57] ogbg-molsider: 0.556135 test loss: 0.912174
[Epoch 58; Iter    18/   36] train: loss: 0.3619337
[Epoch 58] ogbg-molsider: 0.583955 val loss: 0.729222
[Epoch 58] ogbg-molsider: 0.553823 test loss: 0.992679
[Epoch 59; Iter    12/   36] train: loss: 0.3220444
[Epoch 59] ogbg-molsider: 0.567961 val loss: 0.654371
[Epoch 59] ogbg-molsider: 0.567451 test loss: 0.787322
[Epoch 60; Iter     6/   36] train: loss: 0.3744312
[Epoch 60; Iter    36/   36] train: loss: 0.3440166
[Epoch 60] ogbg-molsider: 0.563386 val loss: 1.309351
[Epoch 60] ogbg-molsider: 0.543551 test loss: 0.988242
[Epoch 61; Iter    30/   36] train: loss: 0.3689344
[Epoch 61] ogbg-molsider: 0.569774 val loss: 0.761635
[Epoch 61] ogbg-molsider: 0.562229 test loss: 0.897745
[Epoch 62; Iter    24/   36] train: loss: 0.3524532
[Epoch 62] ogbg-molsider: 0.559001 val loss: 0.810124
[Epoch 62] ogbg-molsider: 0.576745 test loss: 1.028607
[Epoch 63; Iter    18/   36] train: loss: 0.3672053
[Epoch 63] ogbg-molsider: 0.575468 val loss: 0.912413
[Epoch 63] ogbg-molsider: 0.571050 test loss: 1.159024
[Epoch 64; Iter    12/   36] train: loss: 0.3188100
[Epoch 64] ogbg-molsider: 0.580303 val loss: 0.788485
[Epoch 64] ogbg-molsider: 0.569522 test loss: 1.056665
[Epoch 65; Iter     6/   36] train: loss: 0.3701716
[Epoch 65; Iter    36/   36] train: loss: 0.3522329
[Epoch 65] ogbg-molsider: 0.606773 val loss: 0.804018
[Epoch 65] ogbg-molsider: 0.570111 test loss: 0.707198
[Epoch 66; Iter    30/   36] train: loss: 0.3053139
[Epoch 66] ogbg-molsider: 0.588535 val loss: 1.388356
[Epoch 66] ogbg-molsider: 0.570842 test loss: 0.789457
[Epoch 67; Iter    24/   36] train: loss: 0.2948154
[Epoch 67] ogbg-molsider: 0.588290 val loss: 0.801571
[Epoch 67] ogbg-molsider: 0.556664 test loss: 1.075127
[Epoch 68; Iter    18/   36] train: loss: 0.3039118
[Epoch 68] ogbg-molsider: 0.579621 val loss: 0.609251
[Epoch 68] ogbg-molsider: 0.571360 test loss: 0.617204
[Epoch 69; Iter    12/   36] train: loss: 0.3074083
[Epoch 69] ogbg-molsider: 0.583916 val loss: 1.033754
[Epoch 69] ogbg-molsider: 0.564428 test loss: 1.004302
[Epoch 70; Iter     6/   36] train: loss: 0.2997080
[Epoch 70; Iter    36/   36] train: loss: 0.3318765
[Epoch 70] ogbg-molsider: 0.582854 val loss: 0.771167
[Epoch 70] ogbg-molsider: 0.559168 test loss: 0.917893
[Epoch 71; Iter    30/   36] train: loss: 0.3253036
[Epoch 71] ogbg-molsider: 0.597900 val loss: 3.103181
[Epoch 71] ogbg-molsider: 0.577760 test loss: 5.750383
[Epoch 72; Iter    24/   36] train: loss: 0.3095351
[Epoch 72] ogbg-molsider: 0.580124 val loss: 1.185408
[Epoch 72] ogbg-molsider: 0.559054 test loss: 1.173245
[Epoch 73; Iter    18/   36] train: loss: 0.3027651
[Epoch 73] ogbg-molsider: 0.544013 val loss: 0.901154
[Epoch 73] ogbg-molsider: 0.574433 test loss: 0.975137
[Epoch 74; Iter    12/   36] train: loss: 0.2992693
[Epoch 74] ogbg-molsider: 0.572010 val loss: 1.227460
[Epoch 74] ogbg-molsider: 0.572777 test loss: 1.050374
[Epoch 75; Iter     6/   36] train: loss: 0.2787668
[Epoch 75; Iter    36/   36] train: loss: 0.2980128
[Epoch 75] ogbg-molsider: 0.574373 val loss: 1.622426
[Epoch 75] ogbg-molsider: 0.554049 test loss: 1.174816
[Epoch 76; Iter    30/   36] train: loss: 0.2889862
[Epoch 76] ogbg-molsider: 0.579553 val loss: 1.195163
[Epoch 76] ogbg-molsider: 0.564772 test loss: 1.595104
[Epoch 77; Iter    24/   36] train: loss: 0.2504134
[Epoch 77] ogbg-molsider: 0.598551 val loss: 1.671987
[Epoch 77] ogbg-molsider: 0.580265 test loss: 1.587411
[Epoch 78; Iter    18/   36] train: loss: 0.2620195
[Epoch 78] ogbg-molsider: 0.578967 val loss: 1.488313
[Epoch 78] ogbg-molsider: 0.570096 test loss: 0.928024
[Epoch 79; Iter    12/   36] train: loss: 0.2481455
[Epoch 79] ogbg-molsider: 0.579302 val loss: 1.184008
[Epoch 79] ogbg-molsider: 0.560839 test loss: 1.032036
[Epoch 80; Iter     6/   36] train: loss: 0.2463788
[Epoch 80; Iter    36/   36] train: loss: 0.2798087
[Epoch 80] ogbg-molsider: 0.577137 val loss: 1.664397
[Epoch 80] ogbg-molsider: 0.562510 test loss: 1.364650
[Epoch 33] ogbg-molsider: 0.527509 test loss: 2.629961
[Epoch 34; Iter    12/   36] train: loss: 0.5224498
[Epoch 34] ogbg-molsider: 0.532937 val loss: 1.439543
[Epoch 34] ogbg-molsider: 0.495290 test loss: 2.876509
[Epoch 35; Iter     6/   36] train: loss: 0.4989090
[Epoch 35; Iter    36/   36] train: loss: 0.5291338
[Epoch 35] ogbg-molsider: 0.516052 val loss: 1.109210
[Epoch 35] ogbg-molsider: 0.522325 test loss: 2.466897
[Epoch 36; Iter    30/   36] train: loss: 0.4828439
[Epoch 36] ogbg-molsider: 0.502551 val loss: 1.183713
[Epoch 36] ogbg-molsider: 0.507111 test loss: 2.449855
[Epoch 37; Iter    24/   36] train: loss: 0.5011044
[Epoch 37] ogbg-molsider: 0.496202 val loss: 1.275576
[Epoch 37] ogbg-molsider: 0.520221 test loss: 2.595712
[Epoch 38; Iter    18/   36] train: loss: 0.4982482
[Epoch 38] ogbg-molsider: 0.505239 val loss: 1.114213
[Epoch 38] ogbg-molsider: 0.514500 test loss: 2.553937
[Epoch 39; Iter    12/   36] train: loss: 0.5114177
[Epoch 39] ogbg-molsider: 0.508631 val loss: 1.190417
[Epoch 39] ogbg-molsider: 0.529046 test loss: 2.286017
[Epoch 40; Iter     6/   36] train: loss: 0.5016860
[Epoch 40; Iter    36/   36] train: loss: 0.5072747
[Epoch 40] ogbg-molsider: 0.532502 val loss: 10.862788
[Epoch 40] ogbg-molsider: 0.456020 test loss: 17.676317
[Epoch 41; Iter    30/   36] train: loss: 0.5008522
[Epoch 41] ogbg-molsider: 0.537446 val loss: 0.861914
[Epoch 41] ogbg-molsider: 0.558634 test loss: 1.557987
[Epoch 42; Iter    24/   36] train: loss: 0.4320479
[Epoch 42] ogbg-molsider: 0.522752 val loss: 3.361749
[Epoch 42] ogbg-molsider: 0.526671 test loss: 4.920255
[Epoch 43; Iter    18/   36] train: loss: 0.4548240
[Epoch 43] ogbg-molsider: 0.524538 val loss: 5.341593
[Epoch 43] ogbg-molsider: 0.512358 test loss: 6.213220
[Epoch 44; Iter    12/   36] train: loss: 0.4524105
[Epoch 44] ogbg-molsider: 0.535232 val loss: 10.538948
[Epoch 44] ogbg-molsider: 0.529995 test loss: 21.520092
[Epoch 45; Iter     6/   36] train: loss: 0.4423628
[Epoch 45; Iter    36/   36] train: loss: 0.4365461
[Epoch 45] ogbg-molsider: 0.510259 val loss: 2.524122
[Epoch 45] ogbg-molsider: 0.525758 test loss: 3.684116
[Epoch 46; Iter    30/   36] train: loss: 0.4426878
[Epoch 46] ogbg-molsider: 0.529925 val loss: 64.942457
[Epoch 46] ogbg-molsider: 0.506569 test loss: 114.482704
[Epoch 47; Iter    24/   36] train: loss: 0.4334326
[Epoch 47] ogbg-molsider: 0.573789 val loss: 1.803919
[Epoch 47] ogbg-molsider: 0.556529 test loss: 2.969183
[Epoch 48; Iter    18/   36] train: loss: 0.4880192
[Epoch 48] ogbg-molsider: 0.562799 val loss: 9.320761
[Epoch 48] ogbg-molsider: 0.547015 test loss: 19.499781
[Epoch 49; Iter    12/   36] train: loss: 0.3953052
[Epoch 49] ogbg-molsider: 0.584185 val loss: 18.051821
[Epoch 49] ogbg-molsider: 0.543943 test loss: 33.431326
[Epoch 50; Iter     6/   36] train: loss: 0.3754428
[Epoch 50; Iter    36/   36] train: loss: 0.4549974
[Epoch 50] ogbg-molsider: 0.534711 val loss: 25.415127
[Epoch 50] ogbg-molsider: 0.526547 test loss: 51.902894
[Epoch 51; Iter    30/   36] train: loss: 0.3816355
[Epoch 51] ogbg-molsider: 0.566923 val loss: 11.225805
[Epoch 51] ogbg-molsider: 0.565045 test loss: 24.030950
[Epoch 52; Iter    24/   36] train: loss: 0.3863159
[Epoch 52] ogbg-molsider: 0.524065 val loss: 1.317565
[Epoch 52] ogbg-molsider: 0.549372 test loss: 2.793674
[Epoch 53; Iter    18/   36] train: loss: 0.3723471
[Epoch 53] ogbg-molsider: 0.556770 val loss: 1.760794
[Epoch 53] ogbg-molsider: 0.558557 test loss: 4.680317
[Epoch 54; Iter    12/   36] train: loss: 0.4111155
[Epoch 54] ogbg-molsider: 0.565149 val loss: 15.332017
[Epoch 54] ogbg-molsider: 0.535259 test loss: 33.898108
[Epoch 55; Iter     6/   36] train: loss: 0.3680998
[Epoch 55; Iter    36/   36] train: loss: 0.4589796
[Epoch 55] ogbg-molsider: 0.583852 val loss: 14.318639
[Epoch 55] ogbg-molsider: 0.563198 test loss: 29.854026
[Epoch 56; Iter    30/   36] train: loss: 0.3499005
[Epoch 56] ogbg-molsider: 0.580858 val loss: 25.536827
[Epoch 56] ogbg-molsider: 0.573162 test loss: 54.885254
[Epoch 57; Iter    24/   36] train: loss: 0.3587870
[Epoch 57] ogbg-molsider: 0.573432 val loss: 12.917347
[Epoch 57] ogbg-molsider: 0.563886 test loss: 28.740799
[Epoch 58; Iter    18/   36] train: loss: 0.3636473
[Epoch 58] ogbg-molsider: 0.545207 val loss: 4.073079
[Epoch 58] ogbg-molsider: 0.580248 test loss: 6.719650
[Epoch 59; Iter    12/   36] train: loss: 0.3164367
[Epoch 59] ogbg-molsider: 0.584377 val loss: 12.150267
[Epoch 59] ogbg-molsider: 0.564011 test loss: 23.366194
[Epoch 60; Iter     6/   36] train: loss: 0.3643959
[Epoch 60; Iter    36/   36] train: loss: 0.3262576
[Epoch 60] ogbg-molsider: 0.562405 val loss: 11.510208
[Epoch 60] ogbg-molsider: 0.595562 test loss: 24.829973
[Epoch 61; Iter    30/   36] train: loss: 0.3468157
[Epoch 61] ogbg-molsider: 0.568120 val loss: 34.733685
[Epoch 61] ogbg-molsider: 0.553675 test loss: 69.156734
[Epoch 62; Iter    24/   36] train: loss: 0.3306154
[Epoch 62] ogbg-molsider: 0.577250 val loss: 15.637988
[Epoch 62] ogbg-molsider: 0.575691 test loss: 33.514195
[Epoch 63; Iter    18/   36] train: loss: 0.3341644
[Epoch 63] ogbg-molsider: 0.565277 val loss: 34.413093
[Epoch 63] ogbg-molsider: 0.575376 test loss: 72.462458
[Epoch 64; Iter    12/   36] train: loss: 0.3181496
[Epoch 64] ogbg-molsider: 0.563937 val loss: 1.607868
[Epoch 64] ogbg-molsider: 0.600342 test loss: 2.211781
[Epoch 65; Iter     6/   36] train: loss: 0.3475734
[Epoch 65; Iter    36/   36] train: loss: 0.3555508
[Epoch 65] ogbg-molsider: 0.569478 val loss: 16.331443
[Epoch 65] ogbg-molsider: 0.568285 test loss: 37.492608
[Epoch 66; Iter    30/   36] train: loss: 0.2985475
[Epoch 66] ogbg-molsider: 0.572116 val loss: 9.966534
[Epoch 66] ogbg-molsider: 0.571566 test loss: 19.095220
[Epoch 67; Iter    24/   36] train: loss: 0.2931905
[Epoch 67] ogbg-molsider: 0.546932 val loss: 6.071523
[Epoch 67] ogbg-molsider: 0.600535 test loss: 15.262674
[Epoch 68; Iter    18/   36] train: loss: 0.2779071
[Epoch 68] ogbg-molsider: 0.570964 val loss: 2.312394
[Epoch 68] ogbg-molsider: 0.594745 test loss: 3.403944
[Epoch 69; Iter    12/   36] train: loss: 0.2951196
[Epoch 69] ogbg-molsider: 0.573822 val loss: 7.810683
[Epoch 69] ogbg-molsider: 0.592734 test loss: 11.985484
[Epoch 70; Iter     6/   36] train: loss: 0.2872405
[Epoch 70; Iter    36/   36] train: loss: 0.3334521
[Epoch 70] ogbg-molsider: 0.563686 val loss: 6.563125
[Epoch 70] ogbg-molsider: 0.598490 test loss: 10.476954
[Epoch 71; Iter    30/   36] train: loss: 0.3268110
[Epoch 71] ogbg-molsider: 0.590030 val loss: 15.270414
[Epoch 71] ogbg-molsider: 0.600932 test loss: 30.512281
[Epoch 72; Iter    24/   36] train: loss: 0.2882228
[Epoch 72] ogbg-molsider: 0.579962 val loss: 11.732172
[Epoch 72] ogbg-molsider: 0.576189 test loss: 27.433997
[Epoch 73; Iter    18/   36] train: loss: 0.2715505
[Epoch 73] ogbg-molsider: 0.558973 val loss: 1.455831
[Epoch 73] ogbg-molsider: 0.605954 test loss: 2.035163
[Epoch 74; Iter    12/   36] train: loss: 0.2863119
[Epoch 74] ogbg-molsider: 0.565152 val loss: 7.113878
[Epoch 74] ogbg-molsider: 0.601559 test loss: 10.285106
[Epoch 75; Iter     6/   36] train: loss: 0.2717221
[Epoch 75; Iter    36/   36] train: loss: 0.2985643
[Epoch 75] ogbg-molsider: 0.575776 val loss: 54.163844
[Epoch 75] ogbg-molsider: 0.520492 test loss: 104.460495
[Epoch 76; Iter    30/   36] train: loss: 0.2972912
[Epoch 76] ogbg-molsider: 0.598822 val loss: 3.996150
[Epoch 76] ogbg-molsider: 0.594286 test loss: 8.159096
[Epoch 77; Iter    24/   36] train: loss: 0.2678866
[Epoch 77] ogbg-molsider: 0.582151 val loss: 8.824036
[Epoch 77] ogbg-molsider: 0.591801 test loss: 16.579474
[Epoch 78; Iter    18/   36] train: loss: 0.2846768
[Epoch 78] ogbg-molsider: 0.548900 val loss: 4.143697
[Epoch 78] ogbg-molsider: 0.607231 test loss: 5.173933
[Epoch 79; Iter    12/   36] train: loss: 0.2345500
[Epoch 79] ogbg-molsider: 0.572436 val loss: 12.937930
[Epoch 79] ogbg-molsider: 0.594366 test loss: 25.807518
[Epoch 80; Iter     6/   36] train: loss: 0.2390019
[Epoch 80; Iter    36/   36] train: loss: 0.2630830
[Epoch 80] ogbg-molsider: 0.589043 val loss: 15.512115
[Epoch 80] ogbg-molsider: 0.569609 test loss: 31.165618[Epoch 33] ogbg-molsider: 0.616581 test loss: 0.499908
[Epoch 34; Iter    12/   36] train: loss: 0.4898159
[Epoch 34] ogbg-molsider: 0.586187 val loss: 0.481821
[Epoch 34] ogbg-molsider: 0.594765 test loss: 0.497477
[Epoch 35; Iter     6/   36] train: loss: 0.5295611
[Epoch 35; Iter    36/   36] train: loss: 0.5028744
[Epoch 35] ogbg-molsider: 0.571637 val loss: 0.488452
[Epoch 35] ogbg-molsider: 0.580962 test loss: 0.512383
[Epoch 36; Iter    30/   36] train: loss: 0.4725968
[Epoch 36] ogbg-molsider: 0.538202 val loss: 0.498147
[Epoch 36] ogbg-molsider: 0.606638 test loss: 0.493132
[Epoch 37; Iter    24/   36] train: loss: 0.4652956
[Epoch 37] ogbg-molsider: 0.537806 val loss: 0.490809
[Epoch 37] ogbg-molsider: 0.607198 test loss: 0.492479
[Epoch 38; Iter    18/   36] train: loss: 0.4953361
[Epoch 38] ogbg-molsider: 0.571051 val loss: 0.481175
[Epoch 38] ogbg-molsider: 0.605725 test loss: 0.494077
[Epoch 39; Iter    12/   36] train: loss: 0.5031835
[Epoch 39] ogbg-molsider: 0.558115 val loss: 0.506486
[Epoch 39] ogbg-molsider: 0.625812 test loss: 0.517705
[Epoch 40; Iter     6/   36] train: loss: 0.4637304
[Epoch 40; Iter    36/   36] train: loss: 0.5268506
[Epoch 40] ogbg-molsider: 0.563060 val loss: 0.488879
[Epoch 40] ogbg-molsider: 0.640619 test loss: 0.483491
[Epoch 41; Iter    30/   36] train: loss: 0.5107144
[Epoch 41] ogbg-molsider: 0.602595 val loss: 0.480801
[Epoch 41] ogbg-molsider: 0.617011 test loss: 0.505249
[Epoch 42; Iter    24/   36] train: loss: 0.4684023
[Epoch 42] ogbg-molsider: 0.604851 val loss: 0.505136
[Epoch 42] ogbg-molsider: 0.614785 test loss: 0.512963
[Epoch 43; Iter    18/   36] train: loss: 0.5029046
[Epoch 43] ogbg-molsider: 0.567628 val loss: 0.481198
[Epoch 43] ogbg-molsider: 0.612451 test loss: 0.490050
[Epoch 44; Iter    12/   36] train: loss: 0.4417697
[Epoch 44] ogbg-molsider: 0.596039 val loss: 0.489740
[Epoch 44] ogbg-molsider: 0.609421 test loss: 0.518352
[Epoch 45; Iter     6/   36] train: loss: 0.4802251
[Epoch 45; Iter    36/   36] train: loss: 0.4811678
[Epoch 45] ogbg-molsider: 0.558608 val loss: 0.494060
[Epoch 45] ogbg-molsider: 0.619796 test loss: 0.514142
[Epoch 46; Iter    30/   36] train: loss: 0.5170951
[Epoch 46] ogbg-molsider: 0.597523 val loss: 0.493594
[Epoch 46] ogbg-molsider: 0.584934 test loss: 0.525066
[Epoch 47; Iter    24/   36] train: loss: 0.4440258
[Epoch 47] ogbg-molsider: 0.601628 val loss: 0.483559
[Epoch 47] ogbg-molsider: 0.590126 test loss: 0.511832
[Epoch 48; Iter    18/   36] train: loss: 0.4945179
[Epoch 48] ogbg-molsider: 0.601932 val loss: 0.496374
[Epoch 48] ogbg-molsider: 0.597318 test loss: 0.538592
[Epoch 49; Iter    12/   36] train: loss: 0.4796662
[Epoch 49] ogbg-molsider: 0.613363 val loss: 0.482319
[Epoch 49] ogbg-molsider: 0.615023 test loss: 0.500785
[Epoch 50; Iter     6/   36] train: loss: 0.4752317
[Epoch 50; Iter    36/   36] train: loss: 0.5336662
[Epoch 50] ogbg-molsider: 0.599650 val loss: 0.487775
[Epoch 50] ogbg-molsider: 0.603528 test loss: 0.513298
[Epoch 51; Iter    30/   36] train: loss: 0.5055485
[Epoch 51] ogbg-molsider: 0.622522 val loss: 0.472755
[Epoch 51] ogbg-molsider: 0.590926 test loss: 0.519570
[Epoch 52; Iter    24/   36] train: loss: 0.4472384
[Epoch 52] ogbg-molsider: 0.624611 val loss: 0.494413
[Epoch 52] ogbg-molsider: 0.623473 test loss: 0.537104
[Epoch 53; Iter    18/   36] train: loss: 0.4576278
[Epoch 53] ogbg-molsider: 0.603354 val loss: 0.504733
[Epoch 53] ogbg-molsider: 0.575024 test loss: 0.529098
[Epoch 54; Iter    12/   36] train: loss: 0.4236245
[Epoch 54] ogbg-molsider: 0.609301 val loss: 0.491255
[Epoch 54] ogbg-molsider: 0.576818 test loss: 0.553565
[Epoch 55; Iter     6/   36] train: loss: 0.4748398
[Epoch 55; Iter    36/   36] train: loss: 0.5165634
[Epoch 55] ogbg-molsider: 0.614770 val loss: 0.492946
[Epoch 55] ogbg-molsider: 0.628459 test loss: 0.507948
[Epoch 56; Iter    30/   36] train: loss: 0.4416879
[Epoch 56] ogbg-molsider: 0.550817 val loss: 3.612915
[Epoch 56] ogbg-molsider: 0.525728 test loss: 5.349675
[Epoch 57; Iter    24/   36] train: loss: 0.4534627
[Epoch 57] ogbg-molsider: 0.589292 val loss: 0.529153
[Epoch 57] ogbg-molsider: 0.593914 test loss: 0.620696
[Epoch 58; Iter    18/   36] train: loss: 0.4733444
[Epoch 58] ogbg-molsider: 0.613620 val loss: 0.494730
[Epoch 58] ogbg-molsider: 0.577134 test loss: 0.548176
[Epoch 59; Iter    12/   36] train: loss: 0.4196991
[Epoch 59] ogbg-molsider: 0.574920 val loss: 0.616680
[Epoch 59] ogbg-molsider: 0.588970 test loss: 0.594555
[Epoch 60; Iter     6/   36] train: loss: 0.4351462
[Epoch 60; Iter    36/   36] train: loss: 0.4579373
[Epoch 60] ogbg-molsider: 0.570909 val loss: 0.519682
[Epoch 60] ogbg-molsider: 0.601406 test loss: 0.549137
[Epoch 61; Iter    30/   36] train: loss: 0.4733789
[Epoch 61] ogbg-molsider: 0.606970 val loss: 0.497690
[Epoch 61] ogbg-molsider: 0.586149 test loss: 0.523895
[Epoch 62; Iter    24/   36] train: loss: 0.3960853
[Epoch 62] ogbg-molsider: 0.627818 val loss: 0.489692
[Epoch 62] ogbg-molsider: 0.595789 test loss: 0.520629
[Epoch 63; Iter    18/   36] train: loss: 0.4564852
[Epoch 63] ogbg-molsider: 0.607340 val loss: 0.502624
[Epoch 63] ogbg-molsider: 0.579564 test loss: 0.574601
[Epoch 64; Iter    12/   36] train: loss: 0.3842510
[Epoch 64] ogbg-molsider: 0.635771 val loss: 0.483997
[Epoch 64] ogbg-molsider: 0.566509 test loss: 0.529239
[Epoch 65; Iter     6/   36] train: loss: 0.3482964
[Epoch 65; Iter    36/   36] train: loss: 0.3968509
[Epoch 65] ogbg-molsider: 0.635730 val loss: 0.642601
[Epoch 65] ogbg-molsider: 0.548414 test loss: 0.852024
[Epoch 66; Iter    30/   36] train: loss: 0.3696787
[Epoch 66] ogbg-molsider: 0.614961 val loss: 0.537201
[Epoch 66] ogbg-molsider: 0.552083 test loss: 0.564465
[Epoch 67; Iter    24/   36] train: loss: 0.4034456
[Epoch 67] ogbg-molsider: 0.624145 val loss: 0.515893
[Epoch 67] ogbg-molsider: 0.581736 test loss: 0.538652
[Epoch 68; Iter    18/   36] train: loss: 0.4021184
[Epoch 68] ogbg-molsider: 0.620393 val loss: 0.533406
[Epoch 68] ogbg-molsider: 0.561352 test loss: 0.604164
[Epoch 69; Iter    12/   36] train: loss: 0.3865334
[Epoch 69] ogbg-molsider: 0.644069 val loss: 0.499981
[Epoch 69] ogbg-molsider: 0.588487 test loss: 0.536319
[Epoch 70; Iter     6/   36] train: loss: 0.3941951
[Epoch 70; Iter    36/   36] train: loss: 0.3811295
[Epoch 70] ogbg-molsider: 0.626508 val loss: 0.530201
[Epoch 70] ogbg-molsider: 0.553167 test loss: 0.584354
[Epoch 71; Iter    30/   36] train: loss: 0.3785518
[Epoch 71] ogbg-molsider: 0.599992 val loss: 0.543016
[Epoch 71] ogbg-molsider: 0.611876 test loss: 0.559551
[Epoch 72; Iter    24/   36] train: loss: 0.3811829
[Epoch 72] ogbg-molsider: 0.629182 val loss: 0.520353
[Epoch 72] ogbg-molsider: 0.601594 test loss: 0.544360
[Epoch 73; Iter    18/   36] train: loss: 0.3915433
[Epoch 73] ogbg-molsider: 0.639996 val loss: 0.516591
[Epoch 73] ogbg-molsider: 0.579158 test loss: 0.570257
[Epoch 74; Iter    12/   36] train: loss: 0.3469166
[Epoch 74] ogbg-molsider: 0.653658 val loss: 0.517360
[Epoch 74] ogbg-molsider: 0.578131 test loss: 0.568363
[Epoch 75; Iter     6/   36] train: loss: 0.3530555
[Epoch 75; Iter    36/   36] train: loss: 0.3992401
[Epoch 75] ogbg-molsider: 0.643501 val loss: 0.572957
[Epoch 75] ogbg-molsider: 0.582868 test loss: 0.585972
[Epoch 76; Iter    30/   36] train: loss: 0.3481050
[Epoch 76] ogbg-molsider: 0.598953 val loss: 0.536829
[Epoch 76] ogbg-molsider: 0.582093 test loss: 0.571428
[Epoch 77; Iter    24/   36] train: loss: 0.3729014
[Epoch 77] ogbg-molsider: 0.636925 val loss: 0.840694
[Epoch 77] ogbg-molsider: 0.584837 test loss: 0.577426
[Epoch 78; Iter    18/   36] train: loss: 0.3636351
[Epoch 78] ogbg-molsider: 0.631605 val loss: 0.518883
[Epoch 78] ogbg-molsider: 0.575838 test loss: 0.571193
[Epoch 79; Iter    12/   36] train: loss: 0.3325397
[Epoch 79] ogbg-molsider: 0.641673 val loss: 0.551443
[Epoch 79] ogbg-molsider: 0.574129 test loss: 0.596346
[Epoch 80; Iter     6/   36] train: loss: 0.3734498
[Epoch 80; Iter    36/   36] train: loss: 0.3595803
[Epoch 80] ogbg-molsider: 0.621586 val loss: 0.582163
[Epoch 80] ogbg-molsider: 0.574998 test loss: 0.613441
[Epoch 33] ogbg-molsider: 0.602707 test loss: 0.494972
[Epoch 34; Iter    12/   36] train: loss: 0.4867727
[Epoch 34] ogbg-molsider: 0.520957 val loss: 0.496823
[Epoch 34] ogbg-molsider: 0.595440 test loss: 0.500432
[Epoch 35; Iter     6/   36] train: loss: 0.5237737
[Epoch 35; Iter    36/   36] train: loss: 0.4711577
[Epoch 35] ogbg-molsider: 0.556913 val loss: 0.504872
[Epoch 35] ogbg-molsider: 0.597765 test loss: 0.520603
[Epoch 36; Iter    30/   36] train: loss: 0.4637320
[Epoch 36] ogbg-molsider: 0.573780 val loss: 0.500335
[Epoch 36] ogbg-molsider: 0.591040 test loss: 0.518500
[Epoch 37; Iter    24/   36] train: loss: 0.5352866
[Epoch 37] ogbg-molsider: 0.571040 val loss: 0.480899
[Epoch 37] ogbg-molsider: 0.602553 test loss: 0.500402
[Epoch 38; Iter    18/   36] train: loss: 0.5077721
[Epoch 38] ogbg-molsider: 0.587733 val loss: 0.475209
[Epoch 38] ogbg-molsider: 0.596485 test loss: 0.487823
[Epoch 39; Iter    12/   36] train: loss: 0.4880374
[Epoch 39] ogbg-molsider: 0.605729 val loss: 0.477845
[Epoch 39] ogbg-molsider: 0.600988 test loss: 0.514808
[Epoch 40; Iter     6/   36] train: loss: 0.4686771
[Epoch 40; Iter    36/   36] train: loss: 0.5064388
[Epoch 40] ogbg-molsider: 0.495631 val loss: 0.518636
[Epoch 40] ogbg-molsider: 0.592754 test loss: 0.530677
[Epoch 41; Iter    30/   36] train: loss: 0.5318211
[Epoch 41] ogbg-molsider: 0.575294 val loss: 1.389043
[Epoch 41] ogbg-molsider: 0.564338 test loss: 1.514371
[Epoch 42; Iter    24/   36] train: loss: 0.4947645
[Epoch 42] ogbg-molsider: 0.557864 val loss: 0.501407
[Epoch 42] ogbg-molsider: 0.598756 test loss: 0.525923
[Epoch 43; Iter    18/   36] train: loss: 0.4837304
[Epoch 43] ogbg-molsider: 0.564908 val loss: 0.514066
[Epoch 43] ogbg-molsider: 0.607137 test loss: 0.550482
[Epoch 44; Iter    12/   36] train: loss: 0.4811908
[Epoch 44] ogbg-molsider: 0.543602 val loss: 0.494915
[Epoch 44] ogbg-molsider: 0.622799 test loss: 0.493003
[Epoch 45; Iter     6/   36] train: loss: 0.4912101
[Epoch 45; Iter    36/   36] train: loss: 0.4361289
[Epoch 45] ogbg-molsider: 0.557262 val loss: 0.492139
[Epoch 45] ogbg-molsider: 0.599218 test loss: 0.501436
[Epoch 46; Iter    30/   36] train: loss: 0.4883383
[Epoch 46] ogbg-molsider: 0.543029 val loss: 0.527831
[Epoch 46] ogbg-molsider: 0.603339 test loss: 0.610612
[Epoch 47; Iter    24/   36] train: loss: 0.4903899
[Epoch 47] ogbg-molsider: 0.612393 val loss: 0.535570
[Epoch 47] ogbg-molsider: 0.603960 test loss: 0.603363
[Epoch 48; Iter    18/   36] train: loss: 0.4432741
[Epoch 48] ogbg-molsider: 0.563363 val loss: 0.528665
[Epoch 48] ogbg-molsider: 0.605013 test loss: 0.575048
[Epoch 49; Iter    12/   36] train: loss: 0.4542834
[Epoch 49] ogbg-molsider: 0.588161 val loss: 0.501902
[Epoch 49] ogbg-molsider: 0.606243 test loss: 0.527275
[Epoch 50; Iter     6/   36] train: loss: 0.4575173
[Epoch 50; Iter    36/   36] train: loss: 0.4901491
[Epoch 50] ogbg-molsider: 0.585971 val loss: 0.497741
[Epoch 50] ogbg-molsider: 0.616843 test loss: 0.503215
[Epoch 51; Iter    30/   36] train: loss: 0.5231488
[Epoch 51] ogbg-molsider: 0.575550 val loss: 0.547279
[Epoch 51] ogbg-molsider: 0.597610 test loss: 0.543343
[Epoch 52; Iter    24/   36] train: loss: 0.4573604
[Epoch 52] ogbg-molsider: 0.595865 val loss: 0.501349
[Epoch 52] ogbg-molsider: 0.603819 test loss: 0.524345
[Epoch 53; Iter    18/   36] train: loss: 0.4490271
[Epoch 53] ogbg-molsider: 0.602281 val loss: 0.501182
[Epoch 53] ogbg-molsider: 0.585566 test loss: 0.537229
[Epoch 54; Iter    12/   36] train: loss: 0.4489627
[Epoch 54] ogbg-molsider: 0.568608 val loss: 0.498370
[Epoch 54] ogbg-molsider: 0.579470 test loss: 0.525484
[Epoch 55; Iter     6/   36] train: loss: 0.4703306
[Epoch 55; Iter    36/   36] train: loss: 0.4544548
[Epoch 55] ogbg-molsider: 0.596704 val loss: 0.551024
[Epoch 55] ogbg-molsider: 0.574176 test loss: 0.522414
[Epoch 56; Iter    30/   36] train: loss: 0.4259016
[Epoch 56] ogbg-molsider: 0.625468 val loss: 0.475085
[Epoch 56] ogbg-molsider: 0.576402 test loss: 0.519046
[Epoch 57; Iter    24/   36] train: loss: 0.4285305
[Epoch 57] ogbg-molsider: 0.579904 val loss: 0.514998
[Epoch 57] ogbg-molsider: 0.621135 test loss: 0.520542
[Epoch 58; Iter    18/   36] train: loss: 0.3852863
[Epoch 58] ogbg-molsider: 0.636542 val loss: 0.484931
[Epoch 58] ogbg-molsider: 0.594957 test loss: 0.529239
[Epoch 59; Iter    12/   36] train: loss: 0.4541280
[Epoch 59] ogbg-molsider: 0.545262 val loss: 0.530164
[Epoch 59] ogbg-molsider: 0.600355 test loss: 0.549980
[Epoch 60; Iter     6/   36] train: loss: 0.4848004
[Epoch 60; Iter    36/   36] train: loss: 0.3894241
[Epoch 60] ogbg-molsider: 0.597675 val loss: 0.503450
[Epoch 60] ogbg-molsider: 0.605460 test loss: 0.533052
[Epoch 61; Iter    30/   36] train: loss: 0.3852949
[Epoch 61] ogbg-molsider: 0.615941 val loss: 0.509815
[Epoch 61] ogbg-molsider: 0.599579 test loss: 0.542336
[Epoch 62; Iter    24/   36] train: loss: 0.4158817
[Epoch 62] ogbg-molsider: 0.622169 val loss: 0.494051
[Epoch 62] ogbg-molsider: 0.617491 test loss: 0.535273
[Epoch 63; Iter    18/   36] train: loss: 0.4522949
[Epoch 63] ogbg-molsider: 0.626502 val loss: 0.515627
[Epoch 63] ogbg-molsider: 0.570250 test loss: 0.578780
[Epoch 64; Iter    12/   36] train: loss: 0.4052784
[Epoch 64] ogbg-molsider: 0.637837 val loss: 0.546793
[Epoch 64] ogbg-molsider: 0.604503 test loss: 0.599827
[Epoch 65; Iter     6/   36] train: loss: 0.3703150
[Epoch 65; Iter    36/   36] train: loss: 0.4722776
[Epoch 65] ogbg-molsider: 0.580376 val loss: 0.558511
[Epoch 65] ogbg-molsider: 0.589899 test loss: 0.615509
[Epoch 66; Iter    30/   36] train: loss: 0.4050733
[Epoch 66] ogbg-molsider: 0.614399 val loss: 0.520436
[Epoch 66] ogbg-molsider: 0.591141 test loss: 0.570004
[Epoch 67; Iter    24/   36] train: loss: 0.4073486
[Epoch 67] ogbg-molsider: 0.648264 val loss: 0.508435
[Epoch 67] ogbg-molsider: 0.585083 test loss: 0.557461
[Epoch 68; Iter    18/   36] train: loss: 0.4043765
[Epoch 68] ogbg-molsider: 0.605935 val loss: 0.519133
[Epoch 68] ogbg-molsider: 0.590465 test loss: 0.573005
[Epoch 69; Iter    12/   36] train: loss: 0.3659666
[Epoch 69] ogbg-molsider: 0.628321 val loss: 0.526622
[Epoch 69] ogbg-molsider: 0.579695 test loss: 0.602932
[Epoch 70; Iter     6/   36] train: loss: 0.3482506
[Epoch 70; Iter    36/   36] train: loss: 0.3666950
[Epoch 70] ogbg-molsider: 0.631167 val loss: 0.534473
[Epoch 70] ogbg-molsider: 0.590024 test loss: 0.592264
[Epoch 71; Iter    30/   36] train: loss: 0.3786989
[Epoch 71] ogbg-molsider: 0.628658 val loss: 0.516739
[Epoch 71] ogbg-molsider: 0.580719 test loss: 0.577660
[Epoch 72; Iter    24/   36] train: loss: 0.3664230
[Epoch 72] ogbg-molsider: 0.616646 val loss: 0.529278
[Epoch 72] ogbg-molsider: 0.551791 test loss: 0.613019
[Epoch 73; Iter    18/   36] train: loss: 0.4064386
[Epoch 73] ogbg-molsider: 0.622070 val loss: 0.595645
[Epoch 73] ogbg-molsider: 0.576468 test loss: 0.588132
[Epoch 74; Iter    12/   36] train: loss: 0.3205985
[Epoch 74] ogbg-molsider: 0.614678 val loss: 0.532549
[Epoch 74] ogbg-molsider: 0.581191 test loss: 0.543017
[Epoch 75; Iter     6/   36] train: loss: 0.3534698
[Epoch 75; Iter    36/   36] train: loss: 0.3676403
[Epoch 75] ogbg-molsider: 0.590676 val loss: 0.532160
[Epoch 75] ogbg-molsider: 0.564539 test loss: 0.569417
[Epoch 76; Iter    30/   36] train: loss: 0.3927782
[Epoch 76] ogbg-molsider: 0.611191 val loss: 0.577336
[Epoch 76] ogbg-molsider: 0.583675 test loss: 0.700169
[Epoch 77; Iter    24/   36] train: loss: 0.3897567
[Epoch 77] ogbg-molsider: 0.629911 val loss: 0.556187
[Epoch 77] ogbg-molsider: 0.602247 test loss: 0.604401
[Epoch 78; Iter    18/   36] train: loss: 0.3463294
[Epoch 78] ogbg-molsider: 0.613316 val loss: 0.546640
[Epoch 78] ogbg-molsider: 0.583957 test loss: 0.603842
[Epoch 79; Iter    12/   36] train: loss: 0.3413166
[Epoch 79] ogbg-molsider: 0.625152 val loss: 0.569847
[Epoch 79] ogbg-molsider: 0.574319 test loss: 0.654706
[Epoch 80; Iter     6/   36] train: loss: 0.3536613
[Epoch 80; Iter    36/   36] train: loss: 0.3704253
[Epoch 80] ogbg-molsider: 0.649831 val loss: 0.523388
[Epoch 80] ogbg-molsider: 0.583914 test loss: 0.590540
[Epoch 33] ogbg-molsider: 0.601321 test loss: 0.490954
[Epoch 34; Iter    12/   36] train: loss: 0.5119124
[Epoch 34] ogbg-molsider: 0.574488 val loss: 0.482668
[Epoch 34] ogbg-molsider: 0.606644 test loss: 0.494002
[Epoch 35; Iter     6/   36] train: loss: 0.4804948
[Epoch 35; Iter    36/   36] train: loss: 0.4983312
[Epoch 35] ogbg-molsider: 0.551685 val loss: 0.497615
[Epoch 35] ogbg-molsider: 0.607434 test loss: 0.507979
[Epoch 36; Iter    30/   36] train: loss: 0.4582480
[Epoch 36] ogbg-molsider: 0.562431 val loss: 0.494337
[Epoch 36] ogbg-molsider: 0.607355 test loss: 0.506994
[Epoch 37; Iter    24/   36] train: loss: 0.5043446
[Epoch 37] ogbg-molsider: 0.572229 val loss: 0.483370
[Epoch 37] ogbg-molsider: 0.605173 test loss: 0.492883
[Epoch 38; Iter    18/   36] train: loss: 0.4550597
[Epoch 38] ogbg-molsider: 0.575383 val loss: 0.477843
[Epoch 38] ogbg-molsider: 0.607196 test loss: 0.486751
[Epoch 39; Iter    12/   36] train: loss: 0.5050503
[Epoch 39] ogbg-molsider: 0.589034 val loss: 0.487301
[Epoch 39] ogbg-molsider: 0.633900 test loss: 0.494980
[Epoch 40; Iter     6/   36] train: loss: 0.5078322
[Epoch 40; Iter    36/   36] train: loss: 0.4853355
[Epoch 40] ogbg-molsider: 0.538926 val loss: 0.626610
[Epoch 40] ogbg-molsider: 0.588441 test loss: 0.701259
[Epoch 41; Iter    30/   36] train: loss: 0.5103585
[Epoch 41] ogbg-molsider: 0.587602 val loss: 0.513968
[Epoch 41] ogbg-molsider: 0.599854 test loss: 0.525538
[Epoch 42; Iter    24/   36] train: loss: 0.4612973
[Epoch 42] ogbg-molsider: 0.630348 val loss: 0.472331
[Epoch 42] ogbg-molsider: 0.602524 test loss: 0.509095
[Epoch 43; Iter    18/   36] train: loss: 0.4738145
[Epoch 43] ogbg-molsider: 0.579810 val loss: 0.490865
[Epoch 43] ogbg-molsider: 0.586598 test loss: 0.521492
[Epoch 44; Iter    12/   36] train: loss: 0.4859414
[Epoch 44] ogbg-molsider: 0.628437 val loss: 0.473988
[Epoch 44] ogbg-molsider: 0.611192 test loss: 0.503153
[Epoch 45; Iter     6/   36] train: loss: 0.4549268
[Epoch 45; Iter    36/   36] train: loss: 0.4704609
[Epoch 45] ogbg-molsider: 0.591132 val loss: 0.489095
[Epoch 45] ogbg-molsider: 0.582775 test loss: 0.511823
[Epoch 46; Iter    30/   36] train: loss: 0.4660350
[Epoch 46] ogbg-molsider: 0.613321 val loss: 0.488077
[Epoch 46] ogbg-molsider: 0.601836 test loss: 0.523324
[Epoch 47; Iter    24/   36] train: loss: 0.4450943
[Epoch 47] ogbg-molsider: 0.566099 val loss: 0.540847
[Epoch 47] ogbg-molsider: 0.567313 test loss: 0.558234
[Epoch 48; Iter    18/   36] train: loss: 0.5119020
[Epoch 48] ogbg-molsider: 0.619399 val loss: 0.483131
[Epoch 48] ogbg-molsider: 0.598591 test loss: 0.521876
[Epoch 49; Iter    12/   36] train: loss: 0.4511277
[Epoch 49] ogbg-molsider: 0.602797 val loss: 0.479229
[Epoch 49] ogbg-molsider: 0.556640 test loss: 0.525198
[Epoch 50; Iter     6/   36] train: loss: 0.4463342
[Epoch 50; Iter    36/   36] train: loss: 0.5068766
[Epoch 50] ogbg-molsider: 0.562171 val loss: 0.512573
[Epoch 50] ogbg-molsider: 0.584654 test loss: 0.558466
[Epoch 51; Iter    30/   36] train: loss: 0.4282392
[Epoch 51] ogbg-molsider: 0.596851 val loss: 0.502930
[Epoch 51] ogbg-molsider: 0.605357 test loss: 0.535780
[Epoch 52; Iter    24/   36] train: loss: 0.4185721
[Epoch 52] ogbg-molsider: 0.582846 val loss: 0.510800
[Epoch 52] ogbg-molsider: 0.608925 test loss: 0.517391
[Epoch 53; Iter    18/   36] train: loss: 0.4972436
[Epoch 53] ogbg-molsider: 0.559071 val loss: 0.500745
[Epoch 53] ogbg-molsider: 0.598116 test loss: 0.519712
[Epoch 54; Iter    12/   36] train: loss: 0.4656290
[Epoch 54] ogbg-molsider: 0.556526 val loss: 0.539916
[Epoch 54] ogbg-molsider: 0.613275 test loss: 0.539190
[Epoch 55; Iter     6/   36] train: loss: 0.4028488
[Epoch 55; Iter    36/   36] train: loss: 0.4787343
[Epoch 55] ogbg-molsider: 0.618734 val loss: 0.729710
[Epoch 55] ogbg-molsider: 0.586256 test loss: 0.953993
[Epoch 56; Iter    30/   36] train: loss: 0.4535689
[Epoch 56] ogbg-molsider: 0.594634 val loss: 0.505795
[Epoch 56] ogbg-molsider: 0.585750 test loss: 0.533730
[Epoch 57; Iter    24/   36] train: loss: 0.3796918
[Epoch 57] ogbg-molsider: 0.589133 val loss: 0.509094
[Epoch 57] ogbg-molsider: 0.614319 test loss: 0.529973
[Epoch 58; Iter    18/   36] train: loss: 0.3971313
[Epoch 58] ogbg-molsider: 0.567295 val loss: 0.564850
[Epoch 58] ogbg-molsider: 0.574209 test loss: 0.612306
[Epoch 59; Iter    12/   36] train: loss: 0.4023370
[Epoch 59] ogbg-molsider: 0.616709 val loss: 0.537641
[Epoch 59] ogbg-molsider: 0.571789 test loss: 0.631760
[Epoch 60; Iter     6/   36] train: loss: 0.4308925
[Epoch 60; Iter    36/   36] train: loss: 0.4097903
[Epoch 60] ogbg-molsider: 0.609285 val loss: 0.518587
[Epoch 60] ogbg-molsider: 0.584665 test loss: 0.582646
[Epoch 61; Iter    30/   36] train: loss: 0.4205796
[Epoch 61] ogbg-molsider: 0.581502 val loss: 0.518420
[Epoch 61] ogbg-molsider: 0.586204 test loss: 0.539227
[Epoch 62; Iter    24/   36] train: loss: 0.4038189
[Epoch 62] ogbg-molsider: 0.585001 val loss: 0.540832
[Epoch 62] ogbg-molsider: 0.590160 test loss: 0.570364
[Epoch 63; Iter    18/   36] train: loss: 0.4719641
[Epoch 63] ogbg-molsider: 0.617196 val loss: 0.583844
[Epoch 63] ogbg-molsider: 0.587644 test loss: 0.958978
[Epoch 64; Iter    12/   36] train: loss: 0.4086672
[Epoch 64] ogbg-molsider: 0.602845 val loss: 0.535093
[Epoch 64] ogbg-molsider: 0.590395 test loss: 0.586529
[Epoch 65; Iter     6/   36] train: loss: 0.4072731
[Epoch 65; Iter    36/   36] train: loss: 0.3769098
[Epoch 65] ogbg-molsider: 0.607851 val loss: 0.796968
[Epoch 65] ogbg-molsider: 0.571312 test loss: 0.711683
[Epoch 66; Iter    30/   36] train: loss: 0.3687788
[Epoch 66] ogbg-molsider: 0.588125 val loss: 1.065727
[Epoch 66] ogbg-molsider: 0.585818 test loss: 0.609336
[Epoch 67; Iter    24/   36] train: loss: 0.3604829
[Epoch 67] ogbg-molsider: 0.598231 val loss: 0.528798
[Epoch 67] ogbg-molsider: 0.595713 test loss: 0.573877
[Epoch 68; Iter    18/   36] train: loss: 0.3487718
[Epoch 68] ogbg-molsider: 0.607570 val loss: 0.522433
[Epoch 68] ogbg-molsider: 0.583071 test loss: 0.548698
[Epoch 69; Iter    12/   36] train: loss: 0.3542133
[Epoch 69] ogbg-molsider: 0.606455 val loss: 0.655503
[Epoch 69] ogbg-molsider: 0.557513 test loss: 0.573032
[Epoch 70; Iter     6/   36] train: loss: 0.3366392
[Epoch 70; Iter    36/   36] train: loss: 0.3654064
[Epoch 70] ogbg-molsider: 0.602614 val loss: 0.532256
[Epoch 70] ogbg-molsider: 0.579763 test loss: 0.577370
[Epoch 71; Iter    30/   36] train: loss: 0.3723086
[Epoch 71] ogbg-molsider: 0.575848 val loss: 1.466584
[Epoch 71] ogbg-molsider: 0.553538 test loss: 0.608265
[Epoch 72; Iter    24/   36] train: loss: 0.3549922
[Epoch 72] ogbg-molsider: 0.628266 val loss: 0.518758
[Epoch 72] ogbg-molsider: 0.567395 test loss: 0.584370
[Epoch 73; Iter    18/   36] train: loss: 0.3797159
[Epoch 73] ogbg-molsider: 0.596482 val loss: 0.548774
[Epoch 73] ogbg-molsider: 0.584621 test loss: 0.567169
[Epoch 74; Iter    12/   36] train: loss: 0.3641362
[Epoch 74] ogbg-molsider: 0.617513 val loss: 0.636785
[Epoch 74] ogbg-molsider: 0.561317 test loss: 0.617806
[Epoch 75; Iter     6/   36] train: loss: 0.3276598
[Epoch 75; Iter    36/   36] train: loss: 0.3730260
[Epoch 75] ogbg-molsider: 0.602182 val loss: 0.915770
[Epoch 75] ogbg-molsider: 0.574666 test loss: 0.639932
[Epoch 76; Iter    30/   36] train: loss: 0.3389136
[Epoch 76] ogbg-molsider: 0.603929 val loss: 0.530291
[Epoch 76] ogbg-molsider: 0.587862 test loss: 0.568112
[Epoch 77; Iter    24/   36] train: loss: 0.3361677
[Epoch 77] ogbg-molsider: 0.612504 val loss: 0.523969
[Epoch 77] ogbg-molsider: 0.566342 test loss: 0.574797
[Epoch 78; Iter    18/   36] train: loss: 0.3434346
[Epoch 78] ogbg-molsider: 0.625129 val loss: 0.586968
[Epoch 78] ogbg-molsider: 0.580154 test loss: 0.609497
[Epoch 79; Iter    12/   36] train: loss: 0.3252276
[Epoch 79] ogbg-molsider: 0.596007 val loss: 0.694496
[Epoch 79] ogbg-molsider: 0.576336 test loss: 0.602960
[Epoch 80; Iter     6/   36] train: loss: 0.3434339
[Epoch 80; Iter    36/   36] train: loss: 0.3822846
[Epoch 80] ogbg-molsider: 0.620798 val loss: 0.555658
[Epoch 80] ogbg-molsider: 0.567706 test loss: 0.615902
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2029459
[Epoch 81] ogbg-molsider: 0.581155 val loss: 0.846900
[Epoch 81] ogbg-molsider: 0.593354 test loss: 0.808018
[Epoch 82; Iter    24/   36] train: loss: 0.2287726
[Epoch 82] ogbg-molsider: 0.601437 val loss: 0.826762
[Epoch 82] ogbg-molsider: 0.597125 test loss: 0.745326
[Epoch 83; Iter    18/   36] train: loss: 0.2091105
[Epoch 83] ogbg-molsider: 0.602851 val loss: 0.768132
[Epoch 83] ogbg-molsider: 0.588539 test loss: 0.742704
[Epoch 84; Iter    12/   36] train: loss: 0.2173471
[Epoch 84] ogbg-molsider: 0.589050 val loss: 0.918814
[Epoch 84] ogbg-molsider: 0.590203 test loss: 0.825416
[Epoch 85; Iter     6/   36] train: loss: 0.2083919
[Epoch 85; Iter    36/   36] train: loss: 0.2335059
[Epoch 85] ogbg-molsider: 0.596929 val loss: 0.766543
[Epoch 85] ogbg-molsider: 0.598127 test loss: 0.779506
[Epoch 86; Iter    30/   36] train: loss: 0.2302864
[Epoch 86] ogbg-molsider: 0.586607 val loss: 1.089781
[Epoch 86] ogbg-molsider: 0.597609 test loss: 1.031146
[Epoch 87; Iter    24/   36] train: loss: 0.2211575
[Epoch 87] ogbg-molsider: 0.588981 val loss: 1.139753
[Epoch 87] ogbg-molsider: 0.582093 test loss: 1.129816
[Epoch 88; Iter    18/   36] train: loss: 0.2408709
[Epoch 88] ogbg-molsider: 0.597842 val loss: 0.871711
[Epoch 88] ogbg-molsider: 0.592071 test loss: 0.823442
[Epoch 89; Iter    12/   36] train: loss: 0.2110872
[Epoch 89] ogbg-molsider: 0.602511 val loss: 1.193076
[Epoch 89] ogbg-molsider: 0.593044 test loss: 1.092795
[Epoch 90; Iter     6/   36] train: loss: 0.2064047
[Epoch 90; Iter    36/   36] train: loss: 0.2173209
[Epoch 90] ogbg-molsider: 0.598926 val loss: 1.110202
[Epoch 90] ogbg-molsider: 0.604281 test loss: 1.040099
[Epoch 91; Iter    30/   36] train: loss: 0.2077821
[Epoch 91] ogbg-molsider: 0.596723 val loss: 0.790531
[Epoch 91] ogbg-molsider: 0.589779 test loss: 0.773572
[Epoch 92; Iter    24/   36] train: loss: 0.2176796
[Epoch 92] ogbg-molsider: 0.587784 val loss: 0.906932
[Epoch 92] ogbg-molsider: 0.593526 test loss: 0.774523
[Epoch 93; Iter    18/   36] train: loss: 0.1781353
[Epoch 93] ogbg-molsider: 0.578818 val loss: 1.500405
[Epoch 93] ogbg-molsider: 0.593959 test loss: 1.497898
[Epoch 94; Iter    12/   36] train: loss: 0.1905841
[Epoch 94] ogbg-molsider: 0.571239 val loss: 0.968869
[Epoch 94] ogbg-molsider: 0.585751 test loss: 0.947927
[Epoch 95; Iter     6/   36] train: loss: 0.2200391
[Epoch 95; Iter    36/   36] train: loss: 0.2003021
[Epoch 95] ogbg-molsider: 0.578073 val loss: 1.347399
[Epoch 95] ogbg-molsider: 0.603204 test loss: 1.231731
[Epoch 96; Iter    30/   36] train: loss: 0.2075106
[Epoch 96] ogbg-molsider: 0.565711 val loss: 1.188196
[Epoch 96] ogbg-molsider: 0.600998 test loss: 1.043335
[Epoch 97; Iter    24/   36] train: loss: 0.2170890
[Epoch 97] ogbg-molsider: 0.578275 val loss: 1.569667
[Epoch 97] ogbg-molsider: 0.596876 test loss: 1.564496
[Epoch 98; Iter    18/   36] train: loss: 0.2127076
[Epoch 98] ogbg-molsider: 0.591537 val loss: 1.404484
[Epoch 98] ogbg-molsider: 0.607826 test loss: 1.375886
[Epoch 99; Iter    12/   36] train: loss: 0.2379870
[Epoch 99] ogbg-molsider: 0.580825 val loss: 1.626839
[Epoch 99] ogbg-molsider: 0.601749 test loss: 1.618479
[Epoch 100; Iter     6/   36] train: loss: 0.1493476
[Epoch 100; Iter    36/   36] train: loss: 0.1882148
[Epoch 100] ogbg-molsider: 0.575128 val loss: 1.227065
[Epoch 100] ogbg-molsider: 0.586447 test loss: 1.062620
[Epoch 101; Iter    30/   36] train: loss: 0.1543118
[Epoch 101] ogbg-molsider: 0.569452 val loss: 1.421873
[Epoch 101] ogbg-molsider: 0.594255 test loss: 1.283320
[Epoch 102; Iter    24/   36] train: loss: 0.1741898
[Epoch 102] ogbg-molsider: 0.581556 val loss: 1.361922
[Epoch 102] ogbg-molsider: 0.598718 test loss: 1.280381
[Epoch 103; Iter    18/   36] train: loss: 0.1629497
[Epoch 103] ogbg-molsider: 0.580212 val loss: 1.337339
[Epoch 103] ogbg-molsider: 0.595816 test loss: 1.260691
[Epoch 104; Iter    12/   36] train: loss: 0.1401865
[Epoch 104] ogbg-molsider: 0.585858 val loss: 1.352820
[Epoch 104] ogbg-molsider: 0.600069 test loss: 1.260018
[Epoch 105; Iter     6/   36] train: loss: 0.1350652
[Epoch 105; Iter    36/   36] train: loss: 0.1711327
[Epoch 105] ogbg-molsider: 0.583567 val loss: 1.618727
[Epoch 105] ogbg-molsider: 0.592431 test loss: 1.641050
[Epoch 106; Iter    30/   36] train: loss: 0.1770644
[Epoch 106] ogbg-molsider: 0.581381 val loss: 1.295602
[Epoch 106] ogbg-molsider: 0.591644 test loss: 1.191568
[Epoch 107; Iter    24/   36] train: loss: 0.1483282
[Epoch 107] ogbg-molsider: 0.583178 val loss: 1.393098
[Epoch 107] ogbg-molsider: 0.596501 test loss: 1.283568
[Epoch 108; Iter    18/   36] train: loss: 0.1444156
[Epoch 108] ogbg-molsider: 0.577657 val loss: 1.806537
[Epoch 108] ogbg-molsider: 0.597956 test loss: 1.893993
[Epoch 109; Iter    12/   36] train: loss: 0.1426944
[Epoch 109] ogbg-molsider: 0.583007 val loss: 1.528708
[Epoch 109] ogbg-molsider: 0.599320 test loss: 1.517121
[Epoch 110; Iter     6/   36] train: loss: 0.1602732
[Epoch 110; Iter    36/   36] train: loss: 0.1320791
[Epoch 110] ogbg-molsider: 0.575084 val loss: 1.385743
[Epoch 110] ogbg-molsider: 0.602698 test loss: 1.281401
[Epoch 111; Iter    30/   36] train: loss: 0.1179859
[Epoch 111] ogbg-molsider: 0.582771 val loss: 1.402598
[Epoch 111] ogbg-molsider: 0.598121 test loss: 1.394882
[Epoch 112; Iter    24/   36] train: loss: 0.1468904
[Epoch 112] ogbg-molsider: 0.572238 val loss: 1.664012
[Epoch 112] ogbg-molsider: 0.598199 test loss: 1.655135
[Epoch 113; Iter    18/   36] train: loss: 0.1087788
[Epoch 113] ogbg-molsider: 0.577284 val loss: 1.815183
[Epoch 113] ogbg-molsider: 0.604221 test loss: 1.933527
[Epoch 114; Iter    12/   36] train: loss: 0.1354213
[Epoch 114] ogbg-molsider: 0.569048 val loss: 1.578464
[Epoch 114] ogbg-molsider: 0.603774 test loss: 1.556525
[Epoch 115; Iter     6/   36] train: loss: 0.1238994
[Epoch 115; Iter    36/   36] train: loss: 0.1700105
[Epoch 115] ogbg-molsider: 0.572085 val loss: 1.597511
[Epoch 115] ogbg-molsider: 0.596521 test loss: 1.637629
[Epoch 116; Iter    30/   36] train: loss: 0.1191910
[Epoch 116] ogbg-molsider: 0.572001 val loss: 1.649786
[Epoch 116] ogbg-molsider: 0.601293 test loss: 1.651105
[Epoch 117; Iter    24/   36] train: loss: 0.1228157
[Epoch 117] ogbg-molsider: 0.572958 val loss: 1.554883
[Epoch 117] ogbg-molsider: 0.598832 test loss: 1.580745
[Epoch 118; Iter    18/   36] train: loss: 0.1326714
[Epoch 118] ogbg-molsider: 0.573753 val loss: 1.856706
[Epoch 118] ogbg-molsider: 0.595951 test loss: 1.899068
[Epoch 119; Iter    12/   36] train: loss: 0.1218941
[Epoch 119] ogbg-molsider: 0.573245 val loss: 1.554401
[Epoch 119] ogbg-molsider: 0.600074 test loss: 1.546525
[Epoch 120; Iter     6/   36] train: loss: 0.1489671
[Epoch 120; Iter    36/   36] train: loss: 0.1176892
[Epoch 120] ogbg-molsider: 0.566087 val loss: 1.493109
[Epoch 120] ogbg-molsider: 0.594737 test loss: 1.485474
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 52.
Statistics on  val_best_checkpoint
mean_pred: 0.8009383082389832
std_pred: 2.3271119594573975
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6591762100514568
rocauc: 0.6176304727075664
ogbg-molsider: 0.6176304727075664
OGBNanLabelBCEWithLogitsLoss: 0.5359585702419281
Statistics on  test
mean_pred: 0.8905991911888123
std_pred: 2.486049175262451
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6313520844226773
rocauc: 0.5977733525657749
ogbg-molsider: 0.5977733525657749
OGBNanLabelBCEWithLogitsLoss: 0.6001721382141113
Statistics on  train
mean_pred: 0.5100699663162231
std_pred: 2.2626636028289795
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8139231471911111
rocauc: 0.8594031674753557
ogbg-molsider: 0.8594031674753557
OGBNanLabelBCEWithLogitsLoss: 0.37032296342982185
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2093953
[Epoch 81] ogbg-molsider: 0.560128 val loss: 1.020433
[Epoch 81] ogbg-molsider: 0.554529 test loss: 1.082427
[Epoch 82; Iter    24/   36] train: loss: 0.2561723
[Epoch 82] ogbg-molsider: 0.580981 val loss: 1.001365
[Epoch 82] ogbg-molsider: 0.554344 test loss: 1.098143
[Epoch 83; Iter    18/   36] train: loss: 0.2372837
[Epoch 83] ogbg-molsider: 0.568088 val loss: 1.025410
[Epoch 83] ogbg-molsider: 0.553804 test loss: 1.076356
[Epoch 84; Iter    12/   36] train: loss: 0.2362812
[Epoch 84] ogbg-molsider: 0.558395 val loss: 0.934491
[Epoch 84] ogbg-molsider: 0.571922 test loss: 0.996548
[Epoch 85; Iter     6/   36] train: loss: 0.2387321
[Epoch 85; Iter    36/   36] train: loss: 0.2415821
[Epoch 85] ogbg-molsider: 0.582249 val loss: 0.908713
[Epoch 85] ogbg-molsider: 0.563613 test loss: 1.015468
[Epoch 86; Iter    30/   36] train: loss: 0.2327168
[Epoch 86] ogbg-molsider: 0.575294 val loss: 0.964876
[Epoch 86] ogbg-molsider: 0.557394 test loss: 1.092576
[Epoch 87; Iter    24/   36] train: loss: 0.1992688
[Epoch 87] ogbg-molsider: 0.568093 val loss: 0.938866
[Epoch 87] ogbg-molsider: 0.552518 test loss: 0.999147
[Epoch 88; Iter    18/   36] train: loss: 0.2034011
[Epoch 88] ogbg-molsider: 0.574573 val loss: 0.916622
[Epoch 88] ogbg-molsider: 0.549359 test loss: 0.971000
[Epoch 89; Iter    12/   36] train: loss: 0.1955763
[Epoch 89] ogbg-molsider: 0.570237 val loss: 0.948626
[Epoch 89] ogbg-molsider: 0.559322 test loss: 1.026068
[Epoch 90; Iter     6/   36] train: loss: 0.1713151
[Epoch 90; Iter    36/   36] train: loss: 0.2021962
[Epoch 90] ogbg-molsider: 0.572442 val loss: 0.953131
[Epoch 90] ogbg-molsider: 0.555014 test loss: 1.019487
[Epoch 91; Iter    30/   36] train: loss: 0.1846536
[Epoch 91] ogbg-molsider: 0.579246 val loss: 1.071065
[Epoch 91] ogbg-molsider: 0.553560 test loss: 1.131748
[Epoch 92; Iter    24/   36] train: loss: 0.2078003
[Epoch 92] ogbg-molsider: 0.573372 val loss: 0.948508
[Epoch 92] ogbg-molsider: 0.558401 test loss: 1.022172
[Epoch 93; Iter    18/   36] train: loss: 0.1504705
[Epoch 93] ogbg-molsider: 0.561846 val loss: 1.014633
[Epoch 93] ogbg-molsider: 0.557213 test loss: 1.057322
[Epoch 94; Iter    12/   36] train: loss: 0.1857987
[Epoch 94] ogbg-molsider: 0.578808 val loss: 1.034014
[Epoch 94] ogbg-molsider: 0.549608 test loss: 1.208771
[Epoch 95; Iter     6/   36] train: loss: 0.1952405
[Epoch 95; Iter    36/   36] train: loss: 0.1697035
[Epoch 95] ogbg-molsider: 0.569043 val loss: 1.006056
[Epoch 95] ogbg-molsider: 0.562180 test loss: 1.063006
[Epoch 96; Iter    30/   36] train: loss: 0.1895965
[Epoch 96] ogbg-molsider: 0.562667 val loss: 1.003196
[Epoch 96] ogbg-molsider: 0.553138 test loss: 1.088275
[Epoch 97; Iter    24/   36] train: loss: 0.1900596
[Epoch 97] ogbg-molsider: 0.564427 val loss: 1.028841
[Epoch 97] ogbg-molsider: 0.565444 test loss: 1.075062
[Epoch 98; Iter    18/   36] train: loss: 0.1952051
[Epoch 98] ogbg-molsider: 0.575332 val loss: 1.951705
[Epoch 98] ogbg-molsider: 0.548351 test loss: 2.827312
[Epoch 99; Iter    12/   36] train: loss: 0.2090350
[Epoch 99] ogbg-molsider: 0.558928 val loss: 1.149816
[Epoch 99] ogbg-molsider: 0.558448 test loss: 1.199534
[Epoch 100; Iter     6/   36] train: loss: 0.1315741
[Epoch 100; Iter    36/   36] train: loss: 0.1698737
[Epoch 100] ogbg-molsider: 0.558737 val loss: 1.056375
[Epoch 100] ogbg-molsider: 0.553226 test loss: 1.084056
[Epoch 101; Iter    30/   36] train: loss: 0.1371880
[Epoch 101] ogbg-molsider: 0.552254 val loss: 1.212717
[Epoch 101] ogbg-molsider: 0.554072 test loss: 1.138156
[Epoch 102; Iter    24/   36] train: loss: 0.1781341
[Epoch 102] ogbg-molsider: 0.560671 val loss: 1.191552
[Epoch 102] ogbg-molsider: 0.562827 test loss: 1.213050
[Epoch 103; Iter    18/   36] train: loss: 0.1965994
[Epoch 103] ogbg-molsider: 0.563599 val loss: 0.989769
[Epoch 103] ogbg-molsider: 0.561363 test loss: 1.082584
[Epoch 104; Iter    12/   36] train: loss: 0.1484236
[Epoch 104] ogbg-molsider: 0.575872 val loss: 1.199362
[Epoch 104] ogbg-molsider: 0.566754 test loss: 1.234307
[Epoch 105; Iter     6/   36] train: loss: 0.1253427
[Epoch 105; Iter    36/   36] train: loss: 0.1790684
[Epoch 105] ogbg-molsider: 0.561796 val loss: 1.168397
[Epoch 105] ogbg-molsider: 0.554516 test loss: 1.283221
[Epoch 106; Iter    30/   36] train: loss: 0.1779187
[Epoch 106] ogbg-molsider: 0.565053 val loss: 1.124922
[Epoch 106] ogbg-molsider: 0.569414 test loss: 1.144081
[Epoch 107; Iter    24/   36] train: loss: 0.1416135
[Epoch 107] ogbg-molsider: 0.575265 val loss: 1.610183
[Epoch 107] ogbg-molsider: 0.549284 test loss: 1.707345
[Epoch 108; Iter    18/   36] train: loss: 0.1383253
[Epoch 108] ogbg-molsider: 0.560027 val loss: 1.166375
[Epoch 108] ogbg-molsider: 0.574729 test loss: 1.328689
[Epoch 109; Iter    12/   36] train: loss: 0.1421378
[Epoch 109] ogbg-molsider: 0.579651 val loss: 1.143232
[Epoch 109] ogbg-molsider: 0.565450 test loss: 1.207557
[Epoch 110; Iter     6/   36] train: loss: 0.1569465
[Epoch 110; Iter    36/   36] train: loss: 0.1530639
[Epoch 110] ogbg-molsider: 0.564850 val loss: 1.387899
[Epoch 110] ogbg-molsider: 0.560133 test loss: 1.816187
[Epoch 111; Iter    30/   36] train: loss: 0.1249898
[Epoch 111] ogbg-molsider: 0.585896 val loss: 1.228281
[Epoch 111] ogbg-molsider: 0.555065 test loss: 1.663446
[Epoch 112; Iter    24/   36] train: loss: 0.1313490
[Epoch 112] ogbg-molsider: 0.583897 val loss: 1.030934
[Epoch 112] ogbg-molsider: 0.562323 test loss: 1.179836
[Epoch 113; Iter    18/   36] train: loss: 0.0886864
[Epoch 113] ogbg-molsider: 0.578319 val loss: 1.160834
[Epoch 113] ogbg-molsider: 0.561310 test loss: 1.251575
[Epoch 114; Iter    12/   36] train: loss: 0.1085101
[Epoch 114] ogbg-molsider: 0.573001 val loss: 1.161690
[Epoch 114] ogbg-molsider: 0.553746 test loss: 1.347693
[Epoch 115; Iter     6/   36] train: loss: 0.1001093
[Epoch 115; Iter    36/   36] train: loss: 0.1234858
[Epoch 115] ogbg-molsider: 0.574941 val loss: 1.277916
[Epoch 115] ogbg-molsider: 0.559298 test loss: 1.343827
[Epoch 116; Iter    30/   36] train: loss: 0.0965990
[Epoch 116] ogbg-molsider: 0.586275 val loss: 1.095598
[Epoch 116] ogbg-molsider: 0.564462 test loss: 1.162642
[Epoch 117; Iter    24/   36] train: loss: 0.0932174
[Epoch 117] ogbg-molsider: 0.577698 val loss: 1.093603
[Epoch 117] ogbg-molsider: 0.566624 test loss: 1.168314
[Epoch 118; Iter    18/   36] train: loss: 0.1190467
[Epoch 118] ogbg-molsider: 0.587040 val loss: 1.099829
[Epoch 118] ogbg-molsider: 0.562056 test loss: 1.169474
[Epoch 119; Iter    12/   36] train: loss: 0.1012574
[Epoch 119] ogbg-molsider: 0.587171 val loss: 1.110851
[Epoch 119] ogbg-molsider: 0.564298 test loss: 1.182736
[Epoch 120; Iter     6/   36] train: loss: 0.1296116
[Epoch 120; Iter    36/   36] train: loss: 0.0873836
[Epoch 120] ogbg-molsider: 0.584190 val loss: 1.293083
[Epoch 120] ogbg-molsider: 0.569294 test loss: 1.246208
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 48.
Statistics on  val_best_checkpoint
mean_pred: -0.40962231159210205
std_pred: 5.236819267272949
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6514564510889654
rocauc: 0.5967486188506229
ogbg-molsider: 0.5967486188506229
OGBNanLabelBCEWithLogitsLoss: 1.2694594740867615
Statistics on  test
mean_pred: -0.6670365929603577
std_pred: 14.970641136169434
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6093550543065526
rocauc: 0.5473974414625585
ogbg-molsider: 0.5473974414625585
OGBNanLabelBCEWithLogitsLoss: 2.2584200501441956
Statistics on  train
mean_pred: 1.0964161157608032
std_pred: 2.6411566734313965
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.7571503504516286
rocauc: 0.8106786670374801
ogbg-molsider: 0.8106786670374801
OGBNanLabelBCEWithLogitsLoss: 0.4589853228794204
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2151856
[Epoch 81] ogbg-molsider: 0.603675 val loss: 0.652229
[Epoch 81] ogbg-molsider: 0.588810 test loss: 0.789147
[Epoch 82; Iter    24/   36] train: loss: 0.2326759
[Epoch 82] ogbg-molsider: 0.590696 val loss: 0.673992
[Epoch 82] ogbg-molsider: 0.583422 test loss: 0.826019
[Epoch 83; Iter    18/   36] train: loss: 0.1987334
[Epoch 83] ogbg-molsider: 0.599918 val loss: 0.721848
[Epoch 83] ogbg-molsider: 0.578040 test loss: 0.917138
[Epoch 84; Iter    12/   36] train: loss: 0.2046978
[Epoch 84] ogbg-molsider: 0.596982 val loss: 0.728694
[Epoch 84] ogbg-molsider: 0.585931 test loss: 0.903634
[Epoch 85; Iter     6/   36] train: loss: 0.2345812
[Epoch 85; Iter    36/   36] train: loss: 0.2327238
[Epoch 85] ogbg-molsider: 0.607481 val loss: 0.755965
[Epoch 85] ogbg-molsider: 0.586210 test loss: 0.938862
[Epoch 86; Iter    30/   36] train: loss: 0.2654758
[Epoch 86] ogbg-molsider: 0.594038 val loss: 0.754230
[Epoch 86] ogbg-molsider: 0.571319 test loss: 0.958422
[Epoch 87; Iter    24/   36] train: loss: 0.2297764
[Epoch 87] ogbg-molsider: 0.608943 val loss: 0.691835
[Epoch 87] ogbg-molsider: 0.584727 test loss: 0.844880
[Epoch 88; Iter    18/   36] train: loss: 0.2472914
[Epoch 88] ogbg-molsider: 0.592501 val loss: 0.718347
[Epoch 88] ogbg-molsider: 0.581619 test loss: 0.852310
[Epoch 89; Iter    12/   36] train: loss: 0.2156923
[Epoch 89] ogbg-molsider: 0.605719 val loss: 0.651196
[Epoch 89] ogbg-molsider: 0.601417 test loss: 0.750183
[Epoch 90; Iter     6/   36] train: loss: 0.2078471
[Epoch 90; Iter    36/   36] train: loss: 0.2500226
[Epoch 90] ogbg-molsider: 0.591560 val loss: 0.757723
[Epoch 90] ogbg-molsider: 0.584401 test loss: 0.918230
[Epoch 91; Iter    30/   36] train: loss: 0.2700756
[Epoch 91] ogbg-molsider: 0.600037 val loss: 0.667143
[Epoch 91] ogbg-molsider: 0.590504 test loss: 0.718807
[Epoch 92; Iter    24/   36] train: loss: 0.2370606
[Epoch 92] ogbg-molsider: 0.591871 val loss: 0.726337
[Epoch 92] ogbg-molsider: 0.588425 test loss: 0.895979
[Epoch 93; Iter    18/   36] train: loss: 0.2444659
[Epoch 93] ogbg-molsider: 0.590649 val loss: 0.801719
[Epoch 93] ogbg-molsider: 0.591853 test loss: 0.861426
[Epoch 94; Iter    12/   36] train: loss: 0.1955211
[Epoch 94] ogbg-molsider: 0.597215 val loss: 0.741564
[Epoch 94] ogbg-molsider: 0.574941 test loss: 0.920470
[Epoch 95; Iter     6/   36] train: loss: 0.2043699
[Epoch 95; Iter    36/   36] train: loss: 0.2612513
[Epoch 95] ogbg-molsider: 0.589973 val loss: 0.836434
[Epoch 95] ogbg-molsider: 0.588796 test loss: 1.077228
[Epoch 96; Iter    30/   36] train: loss: 0.2053363
[Epoch 96] ogbg-molsider: 0.596380 val loss: 0.727799
[Epoch 96] ogbg-molsider: 0.591695 test loss: 0.827612
[Epoch 97; Iter    24/   36] train: loss: 0.1937469
[Epoch 97] ogbg-molsider: 0.592366 val loss: 0.809245
[Epoch 97] ogbg-molsider: 0.583030 test loss: 1.029612
[Epoch 98; Iter    18/   36] train: loss: 0.2530309
[Epoch 98] ogbg-molsider: 0.581357 val loss: 0.843825
[Epoch 98] ogbg-molsider: 0.582315 test loss: 1.000472
[Epoch 99; Iter    12/   36] train: loss: 0.1983865
[Epoch 99] ogbg-molsider: 0.586664 val loss: 0.754725
[Epoch 99] ogbg-molsider: 0.564544 test loss: 0.924259
[Epoch 100; Iter     6/   36] train: loss: 0.1735880
[Epoch 100; Iter    36/   36] train: loss: 0.2004323
[Epoch 100] ogbg-molsider: 0.595110 val loss: 0.712116
[Epoch 100] ogbg-molsider: 0.582134 test loss: 0.833741
[Epoch 101; Iter    30/   36] train: loss: 0.2378538
[Epoch 101] ogbg-molsider: 0.589628 val loss: 0.757154
[Epoch 101] ogbg-molsider: 0.591749 test loss: 0.872879
[Epoch 102; Iter    24/   36] train: loss: 0.2198461
[Epoch 102] ogbg-molsider: 0.598021 val loss: 0.785324
[Epoch 102] ogbg-molsider: 0.596687 test loss: 0.887218
[Epoch 103; Iter    18/   36] train: loss: 0.1779526
[Epoch 103] ogbg-molsider: 0.580391 val loss: 0.807172
[Epoch 103] ogbg-molsider: 0.574339 test loss: 0.971088
[Epoch 104; Iter    12/   36] train: loss: 0.1811114
[Epoch 104] ogbg-molsider: 0.580165 val loss: 0.747971
[Epoch 104] ogbg-molsider: 0.589660 test loss: 0.882530
[Epoch 105; Iter     6/   36] train: loss: 0.1820847
[Epoch 105; Iter    36/   36] train: loss: 0.1971475
[Epoch 105] ogbg-molsider: 0.593410 val loss: 0.782437
[Epoch 105] ogbg-molsider: 0.588779 test loss: 0.938883
[Epoch 106; Iter    30/   36] train: loss: 0.1638998
[Epoch 106] ogbg-molsider: 0.583320 val loss: 0.789058
[Epoch 106] ogbg-molsider: 0.583905 test loss: 0.942114
[Epoch 107; Iter    24/   36] train: loss: 0.1467584
[Epoch 107] ogbg-molsider: 0.586613 val loss: 0.804295
[Epoch 107] ogbg-molsider: 0.579455 test loss: 0.990114
[Epoch 108; Iter    18/   36] train: loss: 0.1662707
[Epoch 108] ogbg-molsider: 0.599878 val loss: 0.787836
[Epoch 108] ogbg-molsider: 0.588810 test loss: 0.943915
[Epoch 109; Iter    12/   36] train: loss: 0.1565938
[Epoch 109] ogbg-molsider: 0.582942 val loss: 0.833468
[Epoch 109] ogbg-molsider: 0.576732 test loss: 1.019698
[Epoch 110; Iter     6/   36] train: loss: 0.1570534
[Epoch 110; Iter    36/   36] train: loss: 0.1820052
[Epoch 110] ogbg-molsider: 0.583132 val loss: 0.832228
[Epoch 110] ogbg-molsider: 0.583349 test loss: 0.914872
[Epoch 111; Iter    30/   36] train: loss: 0.1877237
[Epoch 111] ogbg-molsider: 0.581229 val loss: 0.773973
[Epoch 111] ogbg-molsider: 0.594711 test loss: 0.831263
[Epoch 112; Iter    24/   36] train: loss: 0.1531713
[Epoch 112] ogbg-molsider: 0.596596 val loss: 0.826056
[Epoch 112] ogbg-molsider: 0.572845 test loss: 0.990120
[Epoch 113; Iter    18/   36] train: loss: 0.1622338
[Epoch 113] ogbg-molsider: 0.583891 val loss: 0.790079
[Epoch 113] ogbg-molsider: 0.580675 test loss: 0.928734
[Epoch 114; Iter    12/   36] train: loss: 0.1685969
[Epoch 114] ogbg-molsider: 0.596471 val loss: 0.834650
[Epoch 114] ogbg-molsider: 0.578367 test loss: 0.932369
[Epoch 115; Iter     6/   36] train: loss: 0.1341155
[Epoch 115; Iter    36/   36] train: loss: 0.1899825
[Epoch 115] ogbg-molsider: 0.583213 val loss: 0.835562
[Epoch 115] ogbg-molsider: 0.577949 test loss: 0.998483
[Epoch 116; Iter    30/   36] train: loss: 0.1492378
[Epoch 116] ogbg-molsider: 0.578727 val loss: 0.854838
[Epoch 116] ogbg-molsider: 0.577200 test loss: 1.038890
[Epoch 117; Iter    24/   36] train: loss: 0.1792554
[Epoch 117] ogbg-molsider: 0.578560 val loss: 0.840494
[Epoch 117] ogbg-molsider: 0.580189 test loss: 0.990591
[Epoch 118; Iter    18/   36] train: loss: 0.1507875
[Epoch 118] ogbg-molsider: 0.582272 val loss: 0.809444
[Epoch 118] ogbg-molsider: 0.578451 test loss: 0.949027
[Epoch 119; Iter    12/   36] train: loss: 0.1614391
[Epoch 119] ogbg-molsider: 0.580504 val loss: 0.949384
[Epoch 119] ogbg-molsider: 0.581911 test loss: 1.015397
[Epoch 120; Iter     6/   36] train: loss: 0.1262641
[Epoch 120; Iter    36/   36] train: loss: 0.1686143
[Epoch 120] ogbg-molsider: 0.578315 val loss: 0.945030
[Epoch 120] ogbg-molsider: 0.579865 test loss: 1.079952
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 48.
Statistics on  val_best_checkpoint
mean_pred: 1.5442546606063843
std_pred: 2.6195602416992188
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6582033391629359
rocauc: 0.6129859481103984
ogbg-molsider: 0.6129859481103984
OGBNanLabelBCEWithLogitsLoss: 0.5635281443595886
Statistics on  test
mean_pred: 1.422763466835022
std_pred: 3.5956485271453857
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.604642341448911
rocauc: 0.5558498614002622
ogbg-molsider: 0.5558498614002622
OGBNanLabelBCEWithLogitsLoss: 0.7815701246261597
Statistics on  train
mean_pred: 0.7040296196937561
std_pred: 2.5283007621765137
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.7401534665052517
rocauc: 0.7814855225158653
ogbg-molsider: 0.7814855225158653
OGBNanLabelBCEWithLogitsLoss: 0.4629622788892852
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2683613
[Epoch 81] ogbg-molsider: 0.555198 val loss: 0.981757
[Epoch 81] ogbg-molsider: 0.536596 test loss: 0.983373
[Epoch 82; Iter    24/   36] train: loss: 0.2374714
[Epoch 82] ogbg-molsider: 0.558711 val loss: 1.113257
[Epoch 82] ogbg-molsider: 0.528838 test loss: 1.189110
[Epoch 83; Iter    18/   36] train: loss: 0.2331944
[Epoch 83] ogbg-molsider: 0.539283 val loss: 0.924171
[Epoch 83] ogbg-molsider: 0.563779 test loss: 0.885597
[Epoch 84; Iter    12/   36] train: loss: 0.2096236
[Epoch 84] ogbg-molsider: 0.541830 val loss: 0.917628
[Epoch 84] ogbg-molsider: 0.556241 test loss: 0.914762
[Epoch 85; Iter     6/   36] train: loss: 0.2496714
[Epoch 85; Iter    36/   36] train: loss: 0.2364346
[Epoch 85] ogbg-molsider: 0.570701 val loss: 1.129619
[Epoch 85] ogbg-molsider: 0.540032 test loss: 1.206615
[Epoch 86; Iter    30/   36] train: loss: 0.1919340
[Epoch 86] ogbg-molsider: 0.547350 val loss: 1.093889
[Epoch 86] ogbg-molsider: 0.547672 test loss: 1.077816
[Epoch 87; Iter    24/   36] train: loss: 0.2143137
[Epoch 87] ogbg-molsider: 0.539261 val loss: 1.064705
[Epoch 87] ogbg-molsider: 0.552769 test loss: 1.016874
[Epoch 88; Iter    18/   36] train: loss: 0.1684151
[Epoch 88] ogbg-molsider: 0.540392 val loss: 1.150502
[Epoch 88] ogbg-molsider: 0.543019 test loss: 1.118687
[Epoch 89; Iter    12/   36] train: loss: 0.2092302
[Epoch 89] ogbg-molsider: 0.550527 val loss: 0.991344
[Epoch 89] ogbg-molsider: 0.552561 test loss: 1.009857
[Epoch 90; Iter     6/   36] train: loss: 0.1807641
[Epoch 90; Iter    36/   36] train: loss: 0.2040414
[Epoch 90] ogbg-molsider: 0.530480 val loss: 1.095933
[Epoch 90] ogbg-molsider: 0.545539 test loss: 1.084198
[Epoch 91; Iter    30/   36] train: loss: 0.2209412
[Epoch 91] ogbg-molsider: 0.549168 val loss: 1.068426
[Epoch 91] ogbg-molsider: 0.549318 test loss: 1.093372
[Epoch 92; Iter    24/   36] train: loss: 0.1989602
[Epoch 92] ogbg-molsider: 0.557156 val loss: 3.186773
[Epoch 92] ogbg-molsider: 0.542533 test loss: 5.488390
[Epoch 93; Iter    18/   36] train: loss: 0.1747092
[Epoch 93] ogbg-molsider: 0.557920 val loss: 2.261227
[Epoch 93] ogbg-molsider: 0.535519 test loss: 3.687991
[Epoch 94; Iter    12/   36] train: loss: 0.1778117
[Epoch 94] ogbg-molsider: 0.557449 val loss: 7.840283
[Epoch 94] ogbg-molsider: 0.549155 test loss: 14.372349
[Epoch 95; Iter     6/   36] train: loss: 0.2041846
[Epoch 95; Iter    36/   36] train: loss: 0.2349364
[Epoch 95] ogbg-molsider: 0.531613 val loss: 1.100886
[Epoch 95] ogbg-molsider: 0.549434 test loss: 1.087562
[Epoch 96; Iter    30/   36] train: loss: 0.2367961
[Epoch 96] ogbg-molsider: 0.563356 val loss: 5.586068
[Epoch 96] ogbg-molsider: 0.538267 test loss: 10.028426
[Epoch 97; Iter    24/   36] train: loss: 0.1982123
[Epoch 97] ogbg-molsider: 0.550027 val loss: 1.536368
[Epoch 97] ogbg-molsider: 0.537728 test loss: 1.316095
[Epoch 98; Iter    18/   36] train: loss: 0.1964588
[Epoch 98] ogbg-molsider: 0.544793 val loss: 1.336973
[Epoch 98] ogbg-molsider: 0.526487 test loss: 1.481018
[Epoch 99; Iter    12/   36] train: loss: 0.1932860
[Epoch 99] ogbg-molsider: 0.544469 val loss: 1.300784
[Epoch 99] ogbg-molsider: 0.537917 test loss: 1.277173
[Epoch 100; Iter     6/   36] train: loss: 0.1688713
[Epoch 100; Iter    36/   36] train: loss: 0.2030853
[Epoch 100] ogbg-molsider: 0.557487 val loss: 12.549998
[Epoch 100] ogbg-molsider: 0.551098 test loss: 25.550459
[Epoch 101; Iter    30/   36] train: loss: 0.1625615
[Epoch 101] ogbg-molsider: 0.568078 val loss: 1.573583
[Epoch 101] ogbg-molsider: 0.538812 test loss: 1.457912
[Epoch 102; Iter    24/   36] train: loss: 0.1606758
[Epoch 102] ogbg-molsider: 0.554965 val loss: 4.630675
[Epoch 102] ogbg-molsider: 0.544086 test loss: 5.056787
[Epoch 103; Iter    18/   36] train: loss: 0.1889673
[Epoch 103] ogbg-molsider: 0.523300 val loss: 1.465075
[Epoch 103] ogbg-molsider: 0.539401 test loss: 1.537386
[Epoch 104; Iter    12/   36] train: loss: 0.1638725
[Epoch 104] ogbg-molsider: 0.540512 val loss: 1.730864
[Epoch 104] ogbg-molsider: 0.545698 test loss: 1.557110
[Epoch 105; Iter     6/   36] train: loss: 0.1478397
[Epoch 105; Iter    36/   36] train: loss: 0.2184895
[Epoch 105] ogbg-molsider: 0.551015 val loss: 1.384090
[Epoch 105] ogbg-molsider: 0.552880 test loss: 1.574663
[Epoch 106; Iter    30/   36] train: loss: 0.1381812
[Epoch 106] ogbg-molsider: 0.532208 val loss: 1.397377
[Epoch 106] ogbg-molsider: 0.539663 test loss: 1.564677
[Epoch 107; Iter    24/   36] train: loss: 0.1413005
[Epoch 107] ogbg-molsider: 0.532962 val loss: 1.739531
[Epoch 107] ogbg-molsider: 0.545751 test loss: 1.672168
[Epoch 108; Iter    18/   36] train: loss: 0.1136615
[Epoch 108] ogbg-molsider: 0.551553 val loss: 1.513663
[Epoch 108] ogbg-molsider: 0.536108 test loss: 1.513844
[Epoch 109; Iter    12/   36] train: loss: 0.1380479
[Epoch 109] ogbg-molsider: 0.534244 val loss: 1.689416
[Epoch 109] ogbg-molsider: 0.542789 test loss: 1.629442
[Epoch 110; Iter     6/   36] train: loss: 0.1540511
[Epoch 110; Iter    36/   36] train: loss: 0.1454482
[Epoch 110] ogbg-molsider: 0.542135 val loss: 1.419286
[Epoch 110] ogbg-molsider: 0.535872 test loss: 1.514259
[Epoch 111; Iter    30/   36] train: loss: 0.1472738
[Epoch 111] ogbg-molsider: 0.542902 val loss: 1.788598
[Epoch 111] ogbg-molsider: 0.538554 test loss: 1.968378
[Epoch 112; Iter    24/   36] train: loss: 0.1342473
[Epoch 112] ogbg-molsider: 0.528072 val loss: 1.425953
[Epoch 112] ogbg-molsider: 0.537425 test loss: 1.582837
[Epoch 113; Iter    18/   36] train: loss: 0.1217805
[Epoch 113] ogbg-molsider: 0.548008 val loss: 2.326885
[Epoch 113] ogbg-molsider: 0.531701 test loss: 2.259885
[Epoch 114; Iter    12/   36] train: loss: 0.1330245
[Epoch 114] ogbg-molsider: 0.527642 val loss: 1.485931
[Epoch 114] ogbg-molsider: 0.538808 test loss: 1.661525
[Epoch 115; Iter     6/   36] train: loss: 0.1252502
[Epoch 115; Iter    36/   36] train: loss: 0.1116354
[Epoch 115] ogbg-molsider: 0.540758 val loss: 1.585606
[Epoch 115] ogbg-molsider: 0.531439 test loss: 1.488151
[Epoch 116; Iter    30/   36] train: loss: 0.1292984
[Epoch 116] ogbg-molsider: 0.527262 val loss: 1.354765
[Epoch 116] ogbg-molsider: 0.533748 test loss: 1.447163
[Epoch 117; Iter    24/   36] train: loss: 0.1534043
[Epoch 117] ogbg-molsider: 0.550590 val loss: 1.502523
[Epoch 117] ogbg-molsider: 0.528763 test loss: 1.726982
[Epoch 118; Iter    18/   36] train: loss: 0.1123378
[Epoch 118] ogbg-molsider: 0.543721 val loss: 1.591086
[Epoch 118] ogbg-molsider: 0.529104 test loss: 1.533228
[Epoch 119; Iter    12/   36] train: loss: 0.1294694
[Epoch 119] ogbg-molsider: 0.528487 val loss: 2.851284
[Epoch 119] ogbg-molsider: 0.534645 test loss: 3.030569
[Epoch 120; Iter     6/   36] train: loss: 0.1436193
[Epoch 120; Iter    36/   36] train: loss: 0.2388985
[Epoch 120] ogbg-molsider: 0.522481 val loss: 1.345398
[Epoch 120] ogbg-molsider: 0.530504 test loss: 1.405919
[Epoch 121; Iter    30/   36] train: loss: 0.1090538
[Epoch 121] ogbg-molsider: 0.526456 val loss: 1.374947
[Epoch 121] ogbg-molsider: 0.527354 test loss: 1.503980
[Epoch 122; Iter    24/   36] train: loss: 0.1185820
[Epoch 122] ogbg-molsider: 0.537798 val loss: 1.608820
[Epoch 122] ogbg-molsider: 0.531782 test loss: 1.565162
[Epoch 123; Iter    18/   36] train: loss: 0.1457358
[Epoch 123] ogbg-molsider: 0.543876 val loss: 1.387905
[Epoch 123] ogbg-molsider: 0.535224 test loss: 1.483877
[Epoch 124; Iter    12/   36] train: loss: 0.1301367
[Epoch 124] ogbg-molsider: 0.525365 val loss: 1.351122
[Epoch 124] ogbg-molsider: 0.533335 test loss: 1.450555
[Epoch 125; Iter     6/   36] train: loss: 0.1145110
[Epoch 125; Iter    36/   36] train: loss: 0.0919471
[Epoch 125] ogbg-molsider: 0.563621 val loss: 2.216433
[Epoch 125] ogbg-molsider: 0.536965 test loss: 2.121575
[Epoch 126; Iter    30/   36] train: loss: 0.1133451
[Epoch 126] ogbg-molsider: 0.530527 val loss: 1.450503
[Epoch 126] ogbg-molsider: 0.534902 test loss: 1.548982
Early stopping criterion based on -ogbg-molsider- that should be max reached after 126 epochs. Best model checkpoint was in epoch 66.
Statistics on  val_best_checkpoint
mean_pred: -9.080174446105957
std_pred: 152.19606018066406
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6597578668202325
rocauc: 0.6030962563587784
ogbg-molsider: 0.6030962563587784
OGBNanLabelBCEWithLogitsLoss: 13.866224730014801
Statistics on  test
mean_pred: -8.890790939331055
std_pred: 116.55860900878906
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6247072434622841
rocauc: 0.5645048605445064
ogbg-molsider: 0.5645048605445064
OGBNanLabelBCEWithLogitsLoss: 17.166722464561463
Statistics on  train
mean_pred: 1.3785762786865234
std_pred: 3.4857003688812256
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8525614084475464
rocauc: 0.9061654872993317
ogbg-molsider: 0.9061654872993317
OGBNanLabelBCEWithLogitsLoss: 0.3305433806445863
[Epoch 81; Iter    30/   36] train: loss: 0.2739536
[Epoch 81] ogbg-molsider: 0.581775 val loss: 0.838983
[Epoch 81] ogbg-molsider: 0.594456 test loss: 0.733528
[Epoch 82; Iter    24/   36] train: loss: 0.2399860
[Epoch 82] ogbg-molsider: 0.598239 val loss: 0.987751
[Epoch 82] ogbg-molsider: 0.599143 test loss: 0.685283
[Epoch 83; Iter    18/   36] train: loss: 0.2381241
[Epoch 83] ogbg-molsider: 0.592265 val loss: 0.962443
[Epoch 83] ogbg-molsider: 0.595561 test loss: 0.745090
[Epoch 84; Iter    12/   36] train: loss: 0.2149592
[Epoch 84] ogbg-molsider: 0.604028 val loss: 1.482649
[Epoch 84] ogbg-molsider: 0.589887 test loss: 0.796671
[Epoch 85; Iter     6/   36] train: loss: 0.2563200
[Epoch 85; Iter    36/   36] train: loss: 0.2391353
[Epoch 85] ogbg-molsider: 0.605089 val loss: 0.634743
[Epoch 85] ogbg-molsider: 0.592639 test loss: 0.782810
[Epoch 86; Iter    30/   36] train: loss: 0.2111332
[Epoch 86] ogbg-molsider: 0.582954 val loss: 0.955471
[Epoch 86] ogbg-molsider: 0.592627 test loss: 0.787677
[Epoch 87; Iter    24/   36] train: loss: 0.2166506
[Epoch 87] ogbg-molsider: 0.599354 val loss: 0.669197
[Epoch 87] ogbg-molsider: 0.573965 test loss: 0.747650
[Epoch 88; Iter    18/   36] train: loss: 0.1692947
[Epoch 88] ogbg-molsider: 0.593896 val loss: 1.412855
[Epoch 88] ogbg-molsider: 0.585161 test loss: 0.760524
[Epoch 89; Iter    12/   36] train: loss: 0.2333656
[Epoch 89] ogbg-molsider: 0.588305 val loss: 2.186964
[Epoch 89] ogbg-molsider: 0.582473 test loss: 1.612759
[Epoch 90; Iter     6/   36] train: loss: 0.1908527
[Epoch 90; Iter    36/   36] train: loss: 0.2347566
[Epoch 90] ogbg-molsider: 0.587060 val loss: 1.102455
[Epoch 90] ogbg-molsider: 0.581994 test loss: 0.799377
[Epoch 91; Iter    30/   36] train: loss: 0.2376640
[Epoch 91] ogbg-molsider: 0.572528 val loss: 0.936671
[Epoch 91] ogbg-molsider: 0.590183 test loss: 0.824965
[Epoch 92; Iter    24/   36] train: loss: 0.2029490
[Epoch 92] ogbg-molsider: 0.602461 val loss: 1.378820
[Epoch 92] ogbg-molsider: 0.574101 test loss: 1.987087
[Epoch 93; Iter    18/   36] train: loss: 0.1841429
[Epoch 93] ogbg-molsider: 0.587949 val loss: 5.164480
[Epoch 93] ogbg-molsider: 0.573394 test loss: 6.940466
[Epoch 94; Iter    12/   36] train: loss: 0.1990308
[Epoch 94] ogbg-molsider: 0.585151 val loss: 5.595439
[Epoch 94] ogbg-molsider: 0.571135 test loss: 8.568769
[Epoch 95; Iter     6/   36] train: loss: 0.2020023
[Epoch 95; Iter    36/   36] train: loss: 0.2393771
[Epoch 95] ogbg-molsider: 0.583931 val loss: 4.969956
[Epoch 95] ogbg-molsider: 0.559334 test loss: 9.930745
[Epoch 96; Iter    30/   36] train: loss: 0.2598146
[Epoch 96] ogbg-molsider: 0.601382 val loss: 7.410293
[Epoch 96] ogbg-molsider: 0.557862 test loss: 12.197940
[Epoch 97; Iter    24/   36] train: loss: 0.1831108
[Epoch 97] ogbg-molsider: 0.572516 val loss: 0.884808
[Epoch 97] ogbg-molsider: 0.584677 test loss: 0.815823
[Epoch 98; Iter    18/   36] train: loss: 0.2098071
[Epoch 98] ogbg-molsider: 0.583988 val loss: 0.831873
[Epoch 98] ogbg-molsider: 0.591668 test loss: 0.896363
[Epoch 99; Iter    12/   36] train: loss: 0.2085679
[Epoch 99] ogbg-molsider: 0.574166 val loss: 4.985993
[Epoch 99] ogbg-molsider: 0.566352 test loss: 10.012222
[Epoch 100; Iter     6/   36] train: loss: 0.1988462
[Epoch 100; Iter    36/   36] train: loss: 0.2297119
[Epoch 100] ogbg-molsider: 0.596104 val loss: 9.695348
[Epoch 100] ogbg-molsider: 0.561112 test loss: 16.609350
[Epoch 101; Iter    30/   36] train: loss: 0.1784364
[Epoch 101] ogbg-molsider: 0.580524 val loss: 1.571303
[Epoch 101] ogbg-molsider: 0.570206 test loss: 1.650852
[Epoch 102; Iter    24/   36] train: loss: 0.1685301
[Epoch 102] ogbg-molsider: 0.578881 val loss: 13.490827
[Epoch 102] ogbg-molsider: 0.566852 test loss: 23.005449
[Epoch 103; Iter    18/   36] train: loss: 0.1755645
[Epoch 103] ogbg-molsider: 0.584954 val loss: 12.312723
[Epoch 103] ogbg-molsider: 0.555044 test loss: 23.782770
[Epoch 104; Iter    12/   36] train: loss: 0.1657438
[Epoch 104] ogbg-molsider: 0.575102 val loss: 0.915387
[Epoch 104] ogbg-molsider: 0.571810 test loss: 0.959525
[Epoch 105; Iter     6/   36] train: loss: 0.1273948
[Epoch 105; Iter    36/   36] train: loss: 0.1844288
[Epoch 105] ogbg-molsider: 0.585898 val loss: 4.995454
[Epoch 105] ogbg-molsider: 0.556277 test loss: 8.965831
[Epoch 106; Iter    30/   36] train: loss: 0.1309583
[Epoch 106] ogbg-molsider: 0.586036 val loss: 4.901658
[Epoch 106] ogbg-molsider: 0.560870 test loss: 9.409799
[Epoch 107; Iter    24/   36] train: loss: 0.1436761
[Epoch 107] ogbg-molsider: 0.576987 val loss: 3.902108
[Epoch 107] ogbg-molsider: 0.562577 test loss: 6.051666
[Epoch 108; Iter    18/   36] train: loss: 0.1204899
[Epoch 108] ogbg-molsider: 0.583551 val loss: 4.577981
[Epoch 108] ogbg-molsider: 0.564876 test loss: 8.058164
[Epoch 109; Iter    12/   36] train: loss: 0.1404855
[Epoch 109] ogbg-molsider: 0.575036 val loss: 3.113914
[Epoch 109] ogbg-molsider: 0.561619 test loss: 4.472480
[Epoch 110; Iter     6/   36] train: loss: 0.1552254
[Epoch 110; Iter    36/   36] train: loss: 0.1489344
[Epoch 110] ogbg-molsider: 0.580749 val loss: 1.523836
[Epoch 110] ogbg-molsider: 0.568117 test loss: 1.733637
[Epoch 111; Iter    30/   36] train: loss: 0.1581448
[Epoch 111] ogbg-molsider: 0.583260 val loss: 3.585532
[Epoch 111] ogbg-molsider: 0.561477 test loss: 5.184664
[Epoch 112; Iter    24/   36] train: loss: 0.1462086
[Epoch 112] ogbg-molsider: 0.573884 val loss: 4.133065
[Epoch 112] ogbg-molsider: 0.545333 test loss: 8.519829
[Epoch 113; Iter    18/   36] train: loss: 0.1249106
[Epoch 113] ogbg-molsider: 0.582958 val loss: 4.050324
[Epoch 113] ogbg-molsider: 0.548056 test loss: 8.342820
[Epoch 114; Iter    12/   36] train: loss: 0.1442115
[Epoch 114] ogbg-molsider: 0.577153 val loss: 3.103171
[Epoch 114] ogbg-molsider: 0.555352 test loss: 6.257531
[Epoch 115; Iter     6/   36] train: loss: 0.1422805
[Epoch 115; Iter    36/   36] train: loss: 0.1131489
[Epoch 115] ogbg-molsider: 0.575967 val loss: 0.951241
[Epoch 115] ogbg-molsider: 0.568735 test loss: 0.991827
[Epoch 116; Iter    30/   36] train: loss: 0.1354500
[Epoch 116] ogbg-molsider: 0.585341 val loss: 1.594727
[Epoch 116] ogbg-molsider: 0.559438 test loss: 1.914570
[Epoch 117; Iter    24/   36] train: loss: 0.1661460
[Epoch 117] ogbg-molsider: 0.573017 val loss: 3.877730
[Epoch 117] ogbg-molsider: 0.559523 test loss: 8.685145
[Epoch 118; Iter    18/   36] train: loss: 0.1117858
[Epoch 118] ogbg-molsider: 0.568422 val loss: 7.066863
[Epoch 118] ogbg-molsider: 0.562229 test loss: 13.368589
[Epoch 119; Iter    12/   36] train: loss: 0.1393121
[Epoch 119] ogbg-molsider: 0.565461 val loss: 7.959020
[Epoch 119] ogbg-molsider: 0.551455 test loss: 12.682163
[Epoch 120; Iter     6/   36] train: loss: 0.1692353
[Epoch 120; Iter    36/   36] train: loss: 0.2605103
[Epoch 120] ogbg-molsider: 0.568430 val loss: 4.195874
[Epoch 120] ogbg-molsider: 0.547849 test loss: 7.014752
[Epoch 121; Iter    30/   36] train: loss: 0.1243840
[Epoch 121] ogbg-molsider: 0.574405 val loss: 5.133455
[Epoch 121] ogbg-molsider: 0.563354 test loss: 8.220809
[Epoch 122; Iter    24/   36] train: loss: 0.1344953
[Epoch 122] ogbg-molsider: 0.581247 val loss: 2.210029
[Epoch 122] ogbg-molsider: 0.562734 test loss: 3.732841
[Epoch 123; Iter    18/   36] train: loss: 0.1548297
[Epoch 123] ogbg-molsider: 0.568964 val loss: 3.484633
[Epoch 123] ogbg-molsider: 0.549686 test loss: 6.942099
[Epoch 124; Iter    12/   36] train: loss: 0.1400155
[Epoch 124] ogbg-molsider: 0.584955 val loss: 4.337443
[Epoch 124] ogbg-molsider: 0.563045 test loss: 6.752366
[Epoch 125; Iter     6/   36] train: loss: 0.1203748
[Epoch 125; Iter    36/   36] train: loss: 0.1092330
[Epoch 125] ogbg-molsider: 0.582149 val loss: 2.054460
[Epoch 125] ogbg-molsider: 0.564675 test loss: 2.750755
[Epoch 126; Iter    30/   36] train: loss: 0.1208555
[Epoch 126] ogbg-molsider: 0.573000 val loss: 4.418509
[Epoch 126] ogbg-molsider: 0.573237 test loss: 8.210778
[Epoch 127; Iter    24/   36] train: loss: 0.1148730
[Epoch 127] ogbg-molsider: 0.574352 val loss: 6.551000
[Epoch 127] ogbg-molsider: 0.542440 test loss: 11.063402
[Epoch 128; Iter    18/   36] train: loss: 0.1251710
[Epoch 81; Iter    30/   36] train: loss: 0.2303498
[Epoch 81] ogbg-molsider: 0.590690 val loss: 0.588062
[Epoch 81] ogbg-molsider: 0.594961 test loss: 0.632282
[Epoch 82; Iter    24/   36] train: loss: 0.2979229
[Epoch 82] ogbg-molsider: 0.621295 val loss: 0.627528
[Epoch 82] ogbg-molsider: 0.596597 test loss: 0.703910
[Epoch 83; Iter    18/   36] train: loss: 0.2604231
[Epoch 83] ogbg-molsider: 0.594941 val loss: 0.640554
[Epoch 83] ogbg-molsider: 0.582022 test loss: 0.725021
[Epoch 84; Iter    12/   36] train: loss: 0.2700919
[Epoch 84] ogbg-molsider: 0.597133 val loss: 0.692429
[Epoch 84] ogbg-molsider: 0.584508 test loss: 0.790150
[Epoch 85; Iter     6/   36] train: loss: 0.2391014
[Epoch 85; Iter    36/   36] train: loss: 0.2716476
[Epoch 85] ogbg-molsider: 0.609309 val loss: 0.659298
[Epoch 85] ogbg-molsider: 0.574428 test loss: 0.766510
[Epoch 86; Iter    30/   36] train: loss: 0.2853808
[Epoch 86] ogbg-molsider: 0.609043 val loss: 0.623881
[Epoch 86] ogbg-molsider: 0.573469 test loss: 0.763847
[Epoch 87; Iter    24/   36] train: loss: 0.2543236
[Epoch 87] ogbg-molsider: 0.600665 val loss: 0.596981
[Epoch 87] ogbg-molsider: 0.596206 test loss: 0.638675
[Epoch 88; Iter    18/   36] train: loss: 0.2543842
[Epoch 88] ogbg-molsider: 0.610151 val loss: 0.606659
[Epoch 88] ogbg-molsider: 0.594658 test loss: 0.709218
[Epoch 89; Iter    12/   36] train: loss: 0.2268990
[Epoch 89] ogbg-molsider: 0.612223 val loss: 0.617083
[Epoch 89] ogbg-molsider: 0.598738 test loss: 0.723384
[Epoch 90; Iter     6/   36] train: loss: 0.2072531
[Epoch 90; Iter    36/   36] train: loss: 0.2318765
[Epoch 90] ogbg-molsider: 0.615907 val loss: 0.618091
[Epoch 90] ogbg-molsider: 0.586885 test loss: 0.696802
[Epoch 91; Iter    30/   36] train: loss: 0.2115588
[Epoch 91] ogbg-molsider: 0.602681 val loss: 0.646350
[Epoch 91] ogbg-molsider: 0.585244 test loss: 0.733960
[Epoch 92; Iter    24/   36] train: loss: 0.2297040
[Epoch 92] ogbg-molsider: 0.601353 val loss: 0.633251
[Epoch 92] ogbg-molsider: 0.584581 test loss: 0.702158
[Epoch 93; Iter    18/   36] train: loss: 0.1752884
[Epoch 93] ogbg-molsider: 0.609518 val loss: 0.638209
[Epoch 93] ogbg-molsider: 0.578366 test loss: 0.748818
[Epoch 94; Iter    12/   36] train: loss: 0.1988611
[Epoch 94] ogbg-molsider: 0.624619 val loss: 0.706680
[Epoch 94] ogbg-molsider: 0.577207 test loss: 0.874460
[Epoch 95; Iter     6/   36] train: loss: 0.2004114
[Epoch 95; Iter    36/   36] train: loss: 0.2221106
[Epoch 95] ogbg-molsider: 0.610483 val loss: 0.628509
[Epoch 95] ogbg-molsider: 0.592297 test loss: 0.727477
[Epoch 96; Iter    30/   36] train: loss: 0.2123442
[Epoch 96] ogbg-molsider: 0.611210 val loss: 0.644792
[Epoch 96] ogbg-molsider: 0.592517 test loss: 0.757811
[Epoch 97; Iter    24/   36] train: loss: 0.2287793
[Epoch 97] ogbg-molsider: 0.601901 val loss: 0.677879
[Epoch 97] ogbg-molsider: 0.582165 test loss: 0.802350
[Epoch 98; Iter    18/   36] train: loss: 0.2186414
[Epoch 98] ogbg-molsider: 0.605092 val loss: 0.642879
[Epoch 98] ogbg-molsider: 0.590592 test loss: 0.736643
[Epoch 99; Iter    12/   36] train: loss: 0.2223150
[Epoch 99] ogbg-molsider: 0.603098 val loss: 0.666750
[Epoch 99] ogbg-molsider: 0.590178 test loss: 0.773306
[Epoch 100; Iter     6/   36] train: loss: 0.1529218
[Epoch 100; Iter    36/   36] train: loss: 0.1944716
[Epoch 100] ogbg-molsider: 0.620361 val loss: 0.680910
[Epoch 100] ogbg-molsider: 0.594925 test loss: 0.827796
[Epoch 101; Iter    30/   36] train: loss: 0.1705037
[Epoch 101] ogbg-molsider: 0.596722 val loss: 0.765990
[Epoch 101] ogbg-molsider: 0.580213 test loss: 0.924523
[Epoch 102; Iter    24/   36] train: loss: 0.1975643
[Epoch 102] ogbg-molsider: 0.608746 val loss: 0.734208
[Epoch 102] ogbg-molsider: 0.582652 test loss: 0.848756
[Epoch 103; Iter    18/   36] train: loss: 0.1793074
[Epoch 103] ogbg-molsider: 0.624417 val loss: 0.713692
[Epoch 103] ogbg-molsider: 0.589801 test loss: 0.869128
[Epoch 104; Iter    12/   36] train: loss: 0.1721356
[Epoch 104] ogbg-molsider: 0.618134 val loss: 0.704115
[Epoch 104] ogbg-molsider: 0.580545 test loss: 0.838378
[Epoch 105; Iter     6/   36] train: loss: 0.1521083
[Epoch 105; Iter    36/   36] train: loss: 0.2015274
[Epoch 105] ogbg-molsider: 0.612534 val loss: 0.684560
[Epoch 105] ogbg-molsider: 0.577741 test loss: 0.817877
[Epoch 106; Iter    30/   36] train: loss: 0.2006945
[Epoch 106] ogbg-molsider: 0.607633 val loss: 0.753539
[Epoch 106] ogbg-molsider: 0.583110 test loss: 0.853934
[Epoch 107; Iter    24/   36] train: loss: 0.1758155
[Epoch 107] ogbg-molsider: 0.621899 val loss: 0.715441
[Epoch 107] ogbg-molsider: 0.599017 test loss: 0.854150
[Epoch 108; Iter    18/   36] train: loss: 0.1836630
[Epoch 108] ogbg-molsider: 0.598561 val loss: 0.687385
[Epoch 108] ogbg-molsider: 0.589760 test loss: 0.813795
[Epoch 109; Iter    12/   36] train: loss: 0.1777799
[Epoch 109] ogbg-molsider: 0.601767 val loss: 0.859473
[Epoch 109] ogbg-molsider: 0.586280 test loss: 1.037980
[Epoch 110; Iter     6/   36] train: loss: 0.1880226
[Epoch 110; Iter    36/   36] train: loss: 0.1879458
[Epoch 110] ogbg-molsider: 0.605985 val loss: 0.841727
[Epoch 110] ogbg-molsider: 0.578129 test loss: 0.976994
[Epoch 111; Iter    30/   36] train: loss: 0.1469355
[Epoch 111] ogbg-molsider: 0.620112 val loss: 0.672749
[Epoch 111] ogbg-molsider: 0.593779 test loss: 0.805905
[Epoch 112; Iter    24/   36] train: loss: 0.1813353
[Epoch 112] ogbg-molsider: 0.615720 val loss: 0.808041
[Epoch 112] ogbg-molsider: 0.596322 test loss: 0.973301
[Epoch 113; Iter    18/   36] train: loss: 0.1298341
[Epoch 113] ogbg-molsider: 0.619131 val loss: 0.768547
[Epoch 113] ogbg-molsider: 0.579960 test loss: 0.961471
[Epoch 114; Iter    12/   36] train: loss: 0.1455585
[Epoch 114] ogbg-molsider: 0.617665 val loss: 0.748662
[Epoch 114] ogbg-molsider: 0.575189 test loss: 0.910577
[Epoch 115; Iter     6/   36] train: loss: 0.1215633
[Epoch 115; Iter    36/   36] train: loss: 0.1577968
[Epoch 115] ogbg-molsider: 0.618570 val loss: 0.814349
[Epoch 115] ogbg-molsider: 0.576444 test loss: 1.009068
[Epoch 116; Iter    30/   36] train: loss: 0.1243482
[Epoch 116] ogbg-molsider: 0.616137 val loss: 0.779388
[Epoch 116] ogbg-molsider: 0.581078 test loss: 0.957912
[Epoch 117; Iter    24/   36] train: loss: 0.1271335
[Epoch 117] ogbg-molsider: 0.618673 val loss: 0.730362
[Epoch 117] ogbg-molsider: 0.585225 test loss: 0.887266
[Epoch 118; Iter    18/   36] train: loss: 0.1477115
[Epoch 118] ogbg-molsider: 0.615478 val loss: 0.751910
[Epoch 118] ogbg-molsider: 0.577580 test loss: 0.913856
[Epoch 119; Iter    12/   36] train: loss: 0.1238881
[Epoch 119] ogbg-molsider: 0.603614 val loss: 0.751063
[Epoch 119] ogbg-molsider: 0.580283 test loss: 0.886282
[Epoch 120; Iter     6/   36] train: loss: 0.1490505
[Epoch 120; Iter    36/   36] train: loss: 0.1106327
[Epoch 120] ogbg-molsider: 0.606578 val loss: 0.816378
[Epoch 120] ogbg-molsider: 0.581142 test loss: 1.000401
[Epoch 121; Iter    30/   36] train: loss: 0.1694336
[Epoch 121] ogbg-molsider: 0.615146 val loss: 0.760030
[Epoch 121] ogbg-molsider: 0.586717 test loss: 0.923620
[Epoch 122; Iter    24/   36] train: loss: 0.1240822
[Epoch 122] ogbg-molsider: 0.613883 val loss: 0.773390
[Epoch 122] ogbg-molsider: 0.585019 test loss: 0.941606
[Epoch 123; Iter    18/   36] train: loss: 0.1491862
[Epoch 123] ogbg-molsider: 0.606917 val loss: 0.775321
[Epoch 123] ogbg-molsider: 0.583713 test loss: 0.910586
[Epoch 124; Iter    12/   36] train: loss: 0.1342751
[Epoch 124] ogbg-molsider: 0.620258 val loss: 0.771083
[Epoch 124] ogbg-molsider: 0.580241 test loss: 0.945447
[Epoch 125; Iter     6/   36] train: loss: 0.1187152
[Epoch 125; Iter    36/   36] train: loss: 0.1599432
[Epoch 125] ogbg-molsider: 0.612232 val loss: 0.827875
[Epoch 125] ogbg-molsider: 0.587561 test loss: 0.999300
[Epoch 126; Iter    30/   36] train: loss: 0.1154753
[Epoch 126] ogbg-molsider: 0.615013 val loss: 0.784574
[Epoch 126] ogbg-molsider: 0.580557 test loss: 0.933444
[Epoch 127; Iter    24/   36] train: loss: 0.1188429
[Epoch 127] ogbg-molsider: 0.613276 val loss: 0.804647
[Epoch 127] ogbg-molsider: 0.589126 test loss: 0.952079
[Epoch 128; Iter    18/   36] train: loss: 0.1253366
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.2195432
[Epoch 81] ogbg-molsider: 0.581349 val loss: 0.847171
[Epoch 81] ogbg-molsider: 0.571299 test loss: 0.859385
[Epoch 82; Iter    24/   36] train: loss: 0.2312099
[Epoch 82] ogbg-molsider: 0.586929 val loss: 1.404784
[Epoch 82] ogbg-molsider: 0.569077 test loss: 1.512561
[Epoch 83; Iter    18/   36] train: loss: 0.2008361
[Epoch 83] ogbg-molsider: 0.573687 val loss: 1.013942
[Epoch 83] ogbg-molsider: 0.567945 test loss: 1.037514
[Epoch 84; Iter    12/   36] train: loss: 0.2050430
[Epoch 84] ogbg-molsider: 0.575446 val loss: 1.007298
[Epoch 84] ogbg-molsider: 0.567830 test loss: 1.085002
[Epoch 85; Iter     6/   36] train: loss: 0.2329091
[Epoch 85; Iter    36/   36] train: loss: 0.2296242
[Epoch 85] ogbg-molsider: 0.587713 val loss: 2.360528
[Epoch 85] ogbg-molsider: 0.568928 test loss: 1.928552
[Epoch 86; Iter    30/   36] train: loss: 0.2555009
[Epoch 86] ogbg-molsider: 0.581063 val loss: 0.877460
[Epoch 86] ogbg-molsider: 0.568355 test loss: 1.014316
[Epoch 87; Iter    24/   36] train: loss: 0.2306244
[Epoch 87] ogbg-molsider: 0.559869 val loss: 1.174276
[Epoch 87] ogbg-molsider: 0.563538 test loss: 1.128028
[Epoch 88; Iter    18/   36] train: loss: 0.2249272
[Epoch 88] ogbg-molsider: 0.597194 val loss: 7.104278
[Epoch 88] ogbg-molsider: 0.567806 test loss: 14.146529
[Epoch 89; Iter    12/   36] train: loss: 0.2067427
[Epoch 89] ogbg-molsider: 0.576831 val loss: 1.487807
[Epoch 89] ogbg-molsider: 0.566500 test loss: 2.095658
[Epoch 90; Iter     6/   36] train: loss: 0.2042205
[Epoch 90; Iter    36/   36] train: loss: 0.2361731
[Epoch 90] ogbg-molsider: 0.577267 val loss: 0.897487
[Epoch 90] ogbg-molsider: 0.578812 test loss: 1.147134
[Epoch 91; Iter    30/   36] train: loss: 0.2389032
[Epoch 91] ogbg-molsider: 0.582612 val loss: 1.032955
[Epoch 91] ogbg-molsider: 0.565611 test loss: 1.089129
[Epoch 92; Iter    24/   36] train: loss: 0.2337134
[Epoch 92] ogbg-molsider: 0.554631 val loss: 1.513154
[Epoch 92] ogbg-molsider: 0.568290 test loss: 1.287864
[Epoch 93; Iter    18/   36] train: loss: 0.2402403
[Epoch 93] ogbg-molsider: 0.575357 val loss: 1.588396
[Epoch 93] ogbg-molsider: 0.573074 test loss: 1.559818
[Epoch 94; Iter    12/   36] train: loss: 0.1885387
[Epoch 94] ogbg-molsider: 0.582443 val loss: 1.946125
[Epoch 94] ogbg-molsider: 0.563516 test loss: 2.532120
[Epoch 95; Iter     6/   36] train: loss: 0.1870344
[Epoch 95; Iter    36/   36] train: loss: 0.2640092
[Epoch 95] ogbg-molsider: 0.573634 val loss: 3.634342
[Epoch 95] ogbg-molsider: 0.571640 test loss: 2.282641
[Epoch 96; Iter    30/   36] train: loss: 0.1938271
[Epoch 96] ogbg-molsider: 0.572609 val loss: 1.289660
[Epoch 96] ogbg-molsider: 0.563311 test loss: 1.410152
[Epoch 97; Iter    24/   36] train: loss: 0.1852274
[Epoch 97] ogbg-molsider: 0.581981 val loss: 3.525681
[Epoch 97] ogbg-molsider: 0.580996 test loss: 2.728513
[Epoch 98; Iter    18/   36] train: loss: 0.2341894
[Epoch 98] ogbg-molsider: 0.581523 val loss: 3.306481
[Epoch 98] ogbg-molsider: 0.571298 test loss: 2.141027
[Epoch 99; Iter    12/   36] train: loss: 0.1988291
[Epoch 99] ogbg-molsider: 0.571846 val loss: 3.092727
[Epoch 99] ogbg-molsider: 0.569125 test loss: 2.033061
[Epoch 100; Iter     6/   36] train: loss: 0.1663607
[Epoch 100; Iter    36/   36] train: loss: 0.1951658
[Epoch 100] ogbg-molsider: 0.574724 val loss: 1.441287
[Epoch 100] ogbg-molsider: 0.577015 test loss: 1.350434
[Epoch 101; Iter    30/   36] train: loss: 0.2349740
[Epoch 101] ogbg-molsider: 0.559504 val loss: 2.016008
[Epoch 101] ogbg-molsider: 0.565935 test loss: 1.886438
[Epoch 102; Iter    24/   36] train: loss: 0.2090529
[Epoch 102] ogbg-molsider: 0.579330 val loss: 1.635321
[Epoch 102] ogbg-molsider: 0.573630 test loss: 1.501782
[Epoch 103; Iter    18/   36] train: loss: 0.1634058
[Epoch 103] ogbg-molsider: 0.575536 val loss: 4.524404
[Epoch 103] ogbg-molsider: 0.577715 test loss: 4.625845
[Epoch 104; Iter    12/   36] train: loss: 0.1842750
[Epoch 104] ogbg-molsider: 0.583406 val loss: 4.793436
[Epoch 104] ogbg-molsider: 0.577818 test loss: 6.135041
[Epoch 105; Iter     6/   36] train: loss: 0.1938265
[Epoch 105; Iter    36/   36] train: loss: 0.1824231
[Epoch 105] ogbg-molsider: 0.578430 val loss: 5.646722
[Epoch 105] ogbg-molsider: 0.585115 test loss: 6.847410
[Epoch 106; Iter    30/   36] train: loss: 0.1670110
[Epoch 106] ogbg-molsider: 0.589386 val loss: 4.933357
[Epoch 106] ogbg-molsider: 0.581682 test loss: 3.932936
[Epoch 107; Iter    24/   36] train: loss: 0.1383137
[Epoch 107] ogbg-molsider: 0.579841 val loss: 4.818646
[Epoch 107] ogbg-molsider: 0.577515 test loss: 4.794675
[Epoch 108; Iter    18/   36] train: loss: 0.1663360
[Epoch 108] ogbg-molsider: 0.583412 val loss: 1.538277
[Epoch 108] ogbg-molsider: 0.571457 test loss: 1.521550
[Epoch 109; Iter    12/   36] train: loss: 0.1646335
[Epoch 109] ogbg-molsider: 0.583223 val loss: 5.529210
[Epoch 109] ogbg-molsider: 0.569233 test loss: 6.350608
[Epoch 110; Iter     6/   36] train: loss: 0.1496524
[Epoch 110; Iter    36/   36] train: loss: 0.1734040
[Epoch 110] ogbg-molsider: 0.584061 val loss: 3.546011
[Epoch 110] ogbg-molsider: 0.575026 test loss: 3.209419
[Epoch 111; Iter    30/   36] train: loss: 0.2023190
[Epoch 111] ogbg-molsider: 0.578890 val loss: 3.100189
[Epoch 111] ogbg-molsider: 0.575218 test loss: 3.544800
[Epoch 112; Iter    24/   36] train: loss: 0.1484036
[Epoch 112] ogbg-molsider: 0.581221 val loss: 10.923721
[Epoch 112] ogbg-molsider: 0.572277 test loss: 16.025287
[Epoch 113; Iter    18/   36] train: loss: 0.1591376
[Epoch 113] ogbg-molsider: 0.577810 val loss: 1.837250
[Epoch 113] ogbg-molsider: 0.571014 test loss: 1.287509
[Epoch 114; Iter    12/   36] train: loss: 0.1486307
[Epoch 114] ogbg-molsider: 0.580229 val loss: 3.310124
[Epoch 114] ogbg-molsider: 0.570839 test loss: 2.589128
[Epoch 115; Iter     6/   36] train: loss: 0.1295279
[Epoch 115; Iter    36/   36] train: loss: 0.2008692
[Epoch 115] ogbg-molsider: 0.575520 val loss: 3.305455
[Epoch 115] ogbg-molsider: 0.576485 test loss: 2.532423
[Epoch 116; Iter    30/   36] train: loss: 0.1417695
[Epoch 116] ogbg-molsider: 0.579966 val loss: 1.971748
[Epoch 116] ogbg-molsider: 0.576123 test loss: 1.735246
[Epoch 117; Iter    24/   36] train: loss: 0.1727165
[Epoch 117] ogbg-molsider: 0.568592 val loss: 2.866078
[Epoch 117] ogbg-molsider: 0.568662 test loss: 2.002447
[Epoch 118; Iter    18/   36] train: loss: 0.1451371
[Epoch 118] ogbg-molsider: 0.577869 val loss: 3.346252
[Epoch 118] ogbg-molsider: 0.584001 test loss: 2.293621
[Epoch 119; Iter    12/   36] train: loss: 0.1543438
[Epoch 119] ogbg-molsider: 0.574771 val loss: 8.940254
[Epoch 119] ogbg-molsider: 0.570914 test loss: 13.660783
[Epoch 120; Iter     6/   36] train: loss: 0.1315701
[Epoch 120; Iter    36/   36] train: loss: 0.1551904
[Epoch 120] ogbg-molsider: 0.581734 val loss: 4.137078
[Epoch 120] ogbg-molsider: 0.578310 test loss: 3.734096
[Epoch 121; Iter    30/   36] train: loss: 0.1206982
[Epoch 121] ogbg-molsider: 0.580329 val loss: 2.914418
[Epoch 121] ogbg-molsider: 0.564957 test loss: 2.371122
[Epoch 122; Iter    24/   36] train: loss: 0.1385762
[Epoch 122] ogbg-molsider: 0.587899 val loss: 5.906498
[Epoch 122] ogbg-molsider: 0.573079 test loss: 7.201041
[Epoch 123; Iter    18/   36] train: loss: 0.1130174
[Epoch 123] ogbg-molsider: 0.577177 val loss: 5.113751
[Epoch 123] ogbg-molsider: 0.580295 test loss: 6.004645
[Epoch 124; Iter    12/   36] train: loss: 0.1373992
[Epoch 124] ogbg-molsider: 0.570864 val loss: 2.847309
[Epoch 124] ogbg-molsider: 0.564637 test loss: 2.264235
[Epoch 125; Iter     6/   36] train: loss: 0.1207210
[Epoch 125; Iter    36/   36] train: loss: 0.1437932
[Epoch 125] ogbg-molsider: 0.586272 val loss: 2.278886
[Epoch 125] ogbg-molsider: 0.574203 test loss: 2.322213
Early stopping criterion based on -ogbg-molsider- that should be max reached after 125 epochs. Best model checkpoint was in epoch 65.
Statistics on  val_best_checkpoint
mean_pred: 0.30009764432907104
std_pred: 10.203302383422852
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6481760665394153
rocauc: 0.606773138621268
ogbg-molsider: 0.606773138621268
[Epoch 81; Iter    30/   36] train: loss: 0.2859917
[Epoch 81] ogbg-molsider: 0.590561 val loss: 0.575388
[Epoch 81] ogbg-molsider: 0.599508 test loss: 0.605001
[Epoch 82; Iter    24/   36] train: loss: 0.2494091
[Epoch 82] ogbg-molsider: 0.597663 val loss: 0.592088
[Epoch 82] ogbg-molsider: 0.581373 test loss: 0.652613
[Epoch 83; Iter    18/   36] train: loss: 0.2579609
[Epoch 83] ogbg-molsider: 0.591959 val loss: 0.581924
[Epoch 83] ogbg-molsider: 0.599081 test loss: 0.620311
[Epoch 84; Iter    12/   36] train: loss: 0.2317532
[Epoch 84] ogbg-molsider: 0.592064 val loss: 0.586408
[Epoch 84] ogbg-molsider: 0.577455 test loss: 0.640464
[Epoch 85; Iter     6/   36] train: loss: 0.2601565
[Epoch 85; Iter    36/   36] train: loss: 0.2560053
[Epoch 85] ogbg-molsider: 0.595272 val loss: 0.612961
[Epoch 85] ogbg-molsider: 0.588290 test loss: 0.660703
[Epoch 86; Iter    30/   36] train: loss: 0.2282649
[Epoch 86] ogbg-molsider: 0.584003 val loss: 0.602380
[Epoch 86] ogbg-molsider: 0.582502 test loss: 0.659829
[Epoch 87; Iter    24/   36] train: loss: 0.2131525
[Epoch 87] ogbg-molsider: 0.605123 val loss: 0.617573
[Epoch 87] ogbg-molsider: 0.589081 test loss: 0.688515
[Epoch 88; Iter    18/   36] train: loss: 0.1990746
[Epoch 88] ogbg-molsider: 0.589213 val loss: 0.612955
[Epoch 88] ogbg-molsider: 0.581624 test loss: 0.661884
[Epoch 89; Iter    12/   36] train: loss: 0.2374338
[Epoch 89] ogbg-molsider: 0.596484 val loss: 0.622240
[Epoch 89] ogbg-molsider: 0.584146 test loss: 0.707521
[Epoch 90; Iter     6/   36] train: loss: 0.1954129
[Epoch 90; Iter    36/   36] train: loss: 0.2380865
[Epoch 90] ogbg-molsider: 0.588357 val loss: 0.627404
[Epoch 90] ogbg-molsider: 0.577704 test loss: 0.696659
[Epoch 91; Iter    30/   36] train: loss: 0.2413838
[Epoch 91] ogbg-molsider: 0.587908 val loss: 0.623515
[Epoch 91] ogbg-molsider: 0.594894 test loss: 0.673043
[Epoch 92; Iter    24/   36] train: loss: 0.2181497
[Epoch 92] ogbg-molsider: 0.597072 val loss: 0.627199
[Epoch 92] ogbg-molsider: 0.582238 test loss: 0.686254
[Epoch 93; Iter    18/   36] train: loss: 0.1978551
[Epoch 93] ogbg-molsider: 0.596059 val loss: 0.665441
[Epoch 93] ogbg-molsider: 0.586969 test loss: 0.716667
[Epoch 94; Iter    12/   36] train: loss: 0.2456494
[Epoch 94] ogbg-molsider: 0.593870 val loss: 0.636154
[Epoch 94] ogbg-molsider: 0.585841 test loss: 0.690335
[Epoch 95; Iter     6/   36] train: loss: 0.2205499
[Epoch 95; Iter    36/   36] train: loss: 0.2491079
[Epoch 95] ogbg-molsider: 0.593019 val loss: 0.662027
[Epoch 95] ogbg-molsider: 0.595372 test loss: 0.718091
[Epoch 96; Iter    30/   36] train: loss: 0.2385510
[Epoch 96] ogbg-molsider: 0.609268 val loss: 0.673025
[Epoch 96] ogbg-molsider: 0.581939 test loss: 0.792679
[Epoch 97; Iter    24/   36] train: loss: 0.2016442
[Epoch 97] ogbg-molsider: 0.591920 val loss: 0.675803
[Epoch 97] ogbg-molsider: 0.588252 test loss: 0.734751
[Epoch 98; Iter    18/   36] train: loss: 0.2308193
[Epoch 98] ogbg-molsider: 0.590830 val loss: 0.693114
[Epoch 98] ogbg-molsider: 0.573632 test loss: 0.777662
[Epoch 99; Iter    12/   36] train: loss: 0.2308020
[Epoch 99] ogbg-molsider: 0.600825 val loss: 0.661819
[Epoch 99] ogbg-molsider: 0.582786 test loss: 0.747474
[Epoch 100; Iter     6/   36] train: loss: 0.1866656
[Epoch 100; Iter    36/   36] train: loss: 0.2145532
[Epoch 100] ogbg-molsider: 0.586716 val loss: 0.737770
[Epoch 100] ogbg-molsider: 0.579641 test loss: 0.823302
[Epoch 101; Iter    30/   36] train: loss: 0.1890713
[Epoch 101] ogbg-molsider: 0.579913 val loss: 0.680299
[Epoch 101] ogbg-molsider: 0.581279 test loss: 0.731468
[Epoch 102; Iter    24/   36] train: loss: 0.1842180
[Epoch 102] ogbg-molsider: 0.588480 val loss: 0.673012
[Epoch 102] ogbg-molsider: 0.571319 test loss: 0.750977
[Epoch 103; Iter    18/   36] train: loss: 0.1739860
[Epoch 103] ogbg-molsider: 0.581283 val loss: 0.685516
[Epoch 103] ogbg-molsider: 0.578705 test loss: 0.753239
[Epoch 104; Iter    12/   36] train: loss: 0.1645800
[Epoch 104] ogbg-molsider: 0.608801 val loss: 0.639752
[Epoch 104] ogbg-molsider: 0.579603 test loss: 0.728829
[Epoch 105; Iter     6/   36] train: loss: 0.1462151
[Epoch 105; Iter    36/   36] train: loss: 0.2086336
[Epoch 105] ogbg-molsider: 0.608042 val loss: 0.702821
[Epoch 105] ogbg-molsider: 0.585058 test loss: 0.822956
[Epoch 106; Iter    30/   36] train: loss: 0.1539803
[Epoch 106] ogbg-molsider: 0.609625 val loss: 0.644055
[Epoch 106] ogbg-molsider: 0.580509 test loss: 0.728943
[Epoch 107; Iter    24/   36] train: loss: 0.1614300
[Epoch 107] ogbg-molsider: 0.611009 val loss: 0.696311
[Epoch 107] ogbg-molsider: 0.577705 test loss: 0.810690
[Epoch 108; Iter    18/   36] train: loss: 0.1412316
[Epoch 108] ogbg-molsider: 0.609338 val loss: 0.667652
[Epoch 108] ogbg-molsider: 0.587348 test loss: 0.748558
[Epoch 109; Iter    12/   36] train: loss: 0.1452792
[Epoch 109] ogbg-molsider: 0.603062 val loss: 0.707293
[Epoch 109] ogbg-molsider: 0.580179 test loss: 0.796527
[Epoch 110; Iter     6/   36] train: loss: 0.1670855
[Epoch 110; Iter    36/   36] train: loss: 0.1773173
[Epoch 110] ogbg-molsider: 0.599928 val loss: 0.698865
[Epoch 110] ogbg-molsider: 0.575009 test loss: 0.805984
[Epoch 111; Iter    30/   36] train: loss: 0.1713008
[Epoch 111] ogbg-molsider: 0.597457 val loss: 0.705654
[Epoch 111] ogbg-molsider: 0.580065 test loss: 0.791309
[Epoch 112; Iter    24/   36] train: loss: 0.1591885
[Epoch 112] ogbg-molsider: 0.606454 val loss: 0.705896
[Epoch 112] ogbg-molsider: 0.584677 test loss: 0.797733
[Epoch 113; Iter    18/   36] train: loss: 0.1429958
[Epoch 113] ogbg-molsider: 0.602554 val loss: 0.702274
[Epoch 113] ogbg-molsider: 0.575525 test loss: 0.773684
[Epoch 114; Iter    12/   36] train: loss: 0.1524881
[Epoch 114] ogbg-molsider: 0.606931 val loss: 0.741679
[Epoch 114] ogbg-molsider: 0.581040 test loss: 0.862467
[Epoch 115; Iter     6/   36] train: loss: 0.1356630
[Epoch 115; Iter    36/   36] train: loss: 0.1363725
[Epoch 115] ogbg-molsider: 0.603645 val loss: 0.692605
[Epoch 115] ogbg-molsider: 0.587293 test loss: 0.773383
[Epoch 116; Iter    30/   36] train: loss: 0.1278034
[Epoch 116] ogbg-molsider: 0.598357 val loss: 0.731039
[Epoch 116] ogbg-molsider: 0.574212 test loss: 0.837262
[Epoch 117; Iter    24/   36] train: loss: 0.1675626
[Epoch 117] ogbg-molsider: 0.618624 val loss: 0.711338
[Epoch 117] ogbg-molsider: 0.575833 test loss: 0.822717
[Epoch 118; Iter    18/   36] train: loss: 0.1284829
[Epoch 118] ogbg-molsider: 0.610264 val loss: 0.719057
[Epoch 118] ogbg-molsider: 0.590073 test loss: 0.796701
[Epoch 119; Iter    12/   36] train: loss: 0.1578899
[Epoch 119] ogbg-molsider: 0.594614 val loss: 0.738284
[Epoch 119] ogbg-molsider: 0.570397 test loss: 0.830888
[Epoch 120; Iter     6/   36] train: loss: 0.1737954
[Epoch 120; Iter    36/   36] train: loss: 0.2552275
[Epoch 120] ogbg-molsider: 0.611868 val loss: 0.752795
[Epoch 120] ogbg-molsider: 0.581702 test loss: 0.867851
[Epoch 121; Iter    30/   36] train: loss: 0.1431908
[Epoch 121] ogbg-molsider: 0.594538 val loss: 0.720494
[Epoch 121] ogbg-molsider: 0.579273 test loss: 0.791022
[Epoch 122; Iter    24/   36] train: loss: 0.1462373
[Epoch 122] ogbg-molsider: 0.593721 val loss: 0.739502
[Epoch 122] ogbg-molsider: 0.588741 test loss: 0.799339
[Epoch 123; Iter    18/   36] train: loss: 0.1764359
[Epoch 123] ogbg-molsider: 0.601591 val loss: 0.739922
[Epoch 123] ogbg-molsider: 0.581244 test loss: 0.835323
[Epoch 124; Iter    12/   36] train: loss: 0.1400849
[Epoch 124] ogbg-molsider: 0.609834 val loss: 0.732286
[Epoch 124] ogbg-molsider: 0.586889 test loss: 0.822885
[Epoch 125; Iter     6/   36] train: loss: 0.1330001
[Epoch 125; Iter    36/   36] train: loss: 0.1217328
[Epoch 125] ogbg-molsider: 0.594579 val loss: 0.753897
[Epoch 125] ogbg-molsider: 0.586251 test loss: 0.838579
[Epoch 126; Iter    30/   36] train: loss: 0.1610203
[Epoch 126] ogbg-molsider: 0.609121 val loss: 0.777825
[Epoch 126] ogbg-molsider: 0.579516 test loss: 0.865984
[Epoch 127; Iter    24/   36] train: loss: 0.1379520
[Epoch 127] ogbg-molsider: 0.587088 val loss: 0.848588
[Epoch 127] ogbg-molsider: 0.559009 test loss: 0.973438
[Epoch 128; Iter    18/   36] train: loss: 0.1271336

[Epoch 81; Iter    30/   36] train: loss: 0.1953086
[Epoch 81] ogbg-molsider: 0.571210 val loss: 12.012576
[Epoch 81] ogbg-molsider: 0.594306 test loss: 23.504629
[Epoch 82; Iter    24/   36] train: loss: 0.2177147
[Epoch 82] ogbg-molsider: 0.573729 val loss: 11.422413
[Epoch 82] ogbg-molsider: 0.588949 test loss: 22.356279
[Epoch 83; Iter    18/   36] train: loss: 0.1818049
[Epoch 83] ogbg-molsider: 0.568659 val loss: 12.282075
[Epoch 83] ogbg-molsider: 0.590912 test loss: 24.442088
[Epoch 84; Iter    12/   36] train: loss: 0.1860202
[Epoch 84] ogbg-molsider: 0.588708 val loss: 15.029763
[Epoch 84] ogbg-molsider: 0.574373 test loss: 29.545192
[Epoch 85; Iter     6/   36] train: loss: 0.2076119
[Epoch 85; Iter    36/   36] train: loss: 0.2045662
[Epoch 85] ogbg-molsider: 0.589237 val loss: 14.400993
[Epoch 85] ogbg-molsider: 0.574093 test loss: 29.039036
[Epoch 86; Iter    30/   36] train: loss: 0.2169635
[Epoch 86] ogbg-molsider: 0.597367 val loss: 15.156010
[Epoch 86] ogbg-molsider: 0.580106 test loss: 29.410796
[Epoch 87; Iter    24/   36] train: loss: 0.2196493
[Epoch 87] ogbg-molsider: 0.569913 val loss: 10.058073
[Epoch 87] ogbg-molsider: 0.574391 test loss: 18.530184
[Epoch 88; Iter    18/   36] train: loss: 0.2065429
[Epoch 88] ogbg-molsider: 0.561221 val loss: 16.019216
[Epoch 88] ogbg-molsider: 0.567710 test loss: 31.769166
[Epoch 89; Iter    12/   36] train: loss: 0.1757975
[Epoch 89] ogbg-molsider: 0.575893 val loss: 11.063969
[Epoch 89] ogbg-molsider: 0.578083 test loss: 21.166578
[Epoch 90; Iter     6/   36] train: loss: 0.1859232
[Epoch 90; Iter    36/   36] train: loss: 0.2138379
[Epoch 90] ogbg-molsider: 0.566107 val loss: 7.847221
[Epoch 90] ogbg-molsider: 0.575917 test loss: 12.603578
[Epoch 91; Iter    30/   36] train: loss: 0.2286659
[Epoch 91] ogbg-molsider: 0.589720 val loss: 8.062021
[Epoch 91] ogbg-molsider: 0.582442 test loss: 14.661085
[Epoch 92; Iter    24/   36] train: loss: 0.1971748
[Epoch 92] ogbg-molsider: 0.561971 val loss: 9.911687
[Epoch 92] ogbg-molsider: 0.578810 test loss: 18.077853
[Epoch 93; Iter    18/   36] train: loss: 0.2306395
[Epoch 93] ogbg-molsider: 0.555137 val loss: 5.362176
[Epoch 93] ogbg-molsider: 0.573119 test loss: 7.662300
[Epoch 94; Iter    12/   36] train: loss: 0.1861155
[Epoch 94] ogbg-molsider: 0.580759 val loss: 6.747572
[Epoch 94] ogbg-molsider: 0.569139 test loss: 9.961436
[Epoch 95; Iter     6/   36] train: loss: 0.1721977
[Epoch 95; Iter    36/   36] train: loss: 0.2130802
[Epoch 95] ogbg-molsider: 0.559331 val loss: 8.065478
[Epoch 95] ogbg-molsider: 0.586600 test loss: 13.339142
[Epoch 96; Iter    30/   36] train: loss: 0.1690840
[Epoch 96] ogbg-molsider: 0.572200 val loss: 11.284753
[Epoch 96] ogbg-molsider: 0.575746 test loss: 19.964372
[Epoch 97; Iter    24/   36] train: loss: 0.1931697
[Epoch 97] ogbg-molsider: 0.559759 val loss: 11.654093
[Epoch 97] ogbg-molsider: 0.562788 test loss: 21.114198
[Epoch 98; Iter    18/   36] train: loss: 0.2091358
[Epoch 98] ogbg-molsider: 0.573669 val loss: 14.652202
[Epoch 98] ogbg-molsider: 0.567704 test loss: 25.810057
[Epoch 99; Iter    12/   36] train: loss: 0.1634298
[Epoch 99] ogbg-molsider: 0.542902 val loss: 3.404115
[Epoch 99] ogbg-molsider: 0.575839 test loss: 3.944087
[Epoch 100; Iter     6/   36] train: loss: 0.1431279
[Epoch 100; Iter    36/   36] train: loss: 0.1791860
[Epoch 100] ogbg-molsider: 0.573597 val loss: 17.396425
[Epoch 100] ogbg-molsider: 0.567534 test loss: 30.752633
[Epoch 101; Iter    30/   36] train: loss: 0.2117990
[Epoch 101] ogbg-molsider: 0.559824 val loss: 11.309777
[Epoch 101] ogbg-molsider: 0.572574 test loss: 20.691604
[Epoch 102; Iter    24/   36] train: loss: 0.1855064
[Epoch 102] ogbg-molsider: 0.563129 val loss: 14.827367
[Epoch 102] ogbg-molsider: 0.573739 test loss: 27.406535
[Epoch 103; Iter    18/   36] train: loss: 0.1683301
[Epoch 103] ogbg-molsider: 0.559786 val loss: 15.348529
[Epoch 103] ogbg-molsider: 0.561256 test loss: 29.920310
[Epoch 104; Iter    12/   36] train: loss: 0.1693376
[Epoch 104] ogbg-molsider: 0.546107 val loss: 22.450681
[Epoch 104] ogbg-molsider: 0.546905 test loss: 44.312860
[Epoch 105; Iter     6/   36] train: loss: 0.1850288
[Epoch 105; Iter    36/   36] train: loss: 0.1750811
[Epoch 105] ogbg-molsider: 0.561705 val loss: 14.452505
[Epoch 105] ogbg-molsider: 0.575644 test loss: 26.361853
[Epoch 106; Iter    30/   36] train: loss: 0.1497112
[Epoch 106] ogbg-molsider: 0.559556 val loss: 20.601539
[Epoch 106] ogbg-molsider: 0.554588 test loss: 39.090744
[Epoch 107; Iter    24/   36] train: loss: 0.1151256
[Epoch 107] ogbg-molsider: 0.568469 val loss: 17.745863
[Epoch 107] ogbg-molsider: 0.565292 test loss: 33.128659
[Epoch 108; Iter    18/   36] train: loss: 0.1394670
[Epoch 108] ogbg-molsider: 0.558891 val loss: 14.015156
[Epoch 108] ogbg-molsider: 0.572013 test loss: 24.786800
[Epoch 109; Iter    12/   36] train: loss: 0.1389959
[Epoch 109] ogbg-molsider: 0.548540 val loss: 10.094356
[Epoch 109] ogbg-molsider: 0.572980 test loss: 15.995008
[Epoch 110; Iter     6/   36] train: loss: 0.1248656
[Epoch 110; Iter    36/   36] train: loss: 0.1572707
[Epoch 110] ogbg-molsider: 0.578156 val loss: 18.631549
[Epoch 110] ogbg-molsider: 0.564862 test loss: 35.192559
[Epoch 111; Iter    30/   36] train: loss: 0.1561119
[Epoch 111] ogbg-molsider: 0.571072 val loss: 17.500933
[Epoch 111] ogbg-molsider: 0.571287 test loss: 31.700867
[Epoch 112; Iter    24/   36] train: loss: 0.1288795
[Epoch 112] ogbg-molsider: 0.560476 val loss: 16.750509
[Epoch 112] ogbg-molsider: 0.561239 test loss: 30.743935
[Epoch 113; Iter    18/   36] train: loss: 0.1318698
[Epoch 113] ogbg-molsider: 0.553492 val loss: 15.156066
[Epoch 113] ogbg-molsider: 0.569365 test loss: 26.173683
[Epoch 114; Iter    12/   36] train: loss: 0.1359226
[Epoch 114] ogbg-molsider: 0.553400 val loss: 22.078209
[Epoch 114] ogbg-molsider: 0.559430 test loss: 38.757619
[Epoch 115; Iter     6/   36] train: loss: 0.1026638
[Epoch 115; Iter    36/   36] train: loss: 0.1810203
[Epoch 115] ogbg-molsider: 0.558681 val loss: 30.595910
[Epoch 115] ogbg-molsider: 0.553699 test loss: 62.395804
[Epoch 116; Iter    30/   36] train: loss: 0.1324643
[Epoch 116] ogbg-molsider: 0.572427 val loss: 27.473022
[Epoch 116] ogbg-molsider: 0.553859 test loss: 54.669459
[Epoch 117; Iter    24/   36] train: loss: 0.1503929
[Epoch 117] ogbg-molsider: 0.549684 val loss: 21.538377
[Epoch 117] ogbg-molsider: 0.573004 test loss: 40.367467
[Epoch 118; Iter    18/   36] train: loss: 0.1217009
[Epoch 118] ogbg-molsider: 0.549711 val loss: 26.369341
[Epoch 118] ogbg-molsider: 0.569870 test loss: 50.403547
[Epoch 119; Iter    12/   36] train: loss: 0.1275103
[Epoch 119] ogbg-molsider: 0.556893 val loss: 28.816041
[Epoch 119] ogbg-molsider: 0.563521 test loss: 59.587137
[Epoch 120; Iter     6/   36] train: loss: 0.1043352
[Epoch 120; Iter    36/   36] train: loss: 0.1262303
[Epoch 120] ogbg-molsider: 0.564328 val loss: 19.068676
[Epoch 120] ogbg-molsider: 0.568006 test loss: 34.683714
[Epoch 121; Iter    30/   36] train: loss: 0.0975990
[Epoch 121] ogbg-molsider: 0.556530 val loss: 31.510989
[Epoch 121] ogbg-molsider: 0.566099 test loss: 62.979818
[Epoch 122; Iter    24/   36] train: loss: 0.1176913
[Epoch 122] ogbg-molsider: 0.546412 val loss: 34.263872
[Epoch 122] ogbg-molsider: 0.562347 test loss: 68.350928
[Epoch 123; Iter    18/   36] train: loss: 0.0933105
[Epoch 123] ogbg-molsider: 0.563843 val loss: 15.775441
[Epoch 123] ogbg-molsider: 0.565412 test loss: 27.917681
[Epoch 124; Iter    12/   36] train: loss: 0.1172931
[Epoch 124] ogbg-molsider: 0.545142 val loss: 38.064362
[Epoch 124] ogbg-molsider: 0.544272 test loss: 72.577756
[Epoch 125; Iter     6/   36] train: loss: 0.1194320
[Epoch 125; Iter    36/   36] train: loss: 0.1390714
[Epoch 125] ogbg-molsider: 0.558551 val loss: 9.579776
[Epoch 125] ogbg-molsider: 0.571530 test loss: 17.151399
[Epoch 126; Iter    30/   36] train: loss: 0.0997754
[Epoch 126] ogbg-molsider: 0.540287 val loss: 5.230312
[Epoch 126] ogbg-molsider: 0.563235 test loss: 9.224427
[Epoch 127; Iter    24/   36] train: loss: 0.1332297
[Epoch 127] ogbg-molsider: 0.556846 val loss: 11.789576
OGBNanLabelBCEWithLogitsLoss: 0.8040178656578064
Statistics on  test
mean_pred: 0.7411622405052185
std_pred: 5.435725212097168
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6111517080981483
rocauc: 0.5701107295201295
ogbg-molsider: 0.5701107295201295
OGBNanLabelBCEWithLogitsLoss: 0.7071975588798523
Statistics on  train
mean_pred: 0.5674997568130493
std_pred: 2.8866517543792725
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8595276168417549
rocauc: 0.901808371010529
ogbg-molsider: 0.901808371010529
OGBNanLabelBCEWithLogitsLoss: 0.31453374524911243
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.611498 val loss: 0.737810
[Epoch 128] ogbg-molsider: 0.580764 test loss: 0.819032
[Epoch 129; Iter    12/   36] train: loss: 0.1116251
[Epoch 129] ogbg-molsider: 0.602023 val loss: 0.776368
[Epoch 129] ogbg-molsider: 0.571994 test loss: 0.883392
[Epoch 130; Iter     6/   36] train: loss: 0.1375147
[Epoch 130; Iter    36/   36] train: loss: 0.1301438
[Epoch 130] ogbg-molsider: 0.612481 val loss: 0.741014
[Epoch 130] ogbg-molsider: 0.579383 test loss: 0.856524
[Epoch 131; Iter    30/   36] train: loss: 0.1427387
[Epoch 131] ogbg-molsider: 0.604653 val loss: 0.781179
[Epoch 131] ogbg-molsider: 0.575985 test loss: 0.877222
[Epoch 132; Iter    24/   36] train: loss: 0.1024702
[Epoch 132] ogbg-molsider: 0.600293 val loss: 0.747822
[Epoch 132] ogbg-molsider: 0.587506 test loss: 0.813468
[Epoch 133; Iter    18/   36] train: loss: 0.1361191
[Epoch 133] ogbg-molsider: 0.589817 val loss: 0.807524
[Epoch 133] ogbg-molsider: 0.578227 test loss: 0.894077
[Epoch 134; Iter    12/   36] train: loss: 0.1005915
[Epoch 134] ogbg-molsider: 0.591932 val loss: 0.789193
[Epoch 134] ogbg-molsider: 0.575612 test loss: 0.887743
Early stopping criterion based on -ogbg-molsider- that should be max reached after 134 epochs. Best model checkpoint was in epoch 74.
Statistics on  val_best_checkpoint
mean_pred: 0.869294285774231
std_pred: 3.377202033996582
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6679401417003127
rocauc: 0.6339828425626247
ogbg-molsider: 0.6339828425626247
OGBNanLabelBCEWithLogitsLoss: 0.5824250221252442
Statistics on  test
mean_pred: 0.6239215135574341
std_pred: 3.2835326194763184
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6298099485576315
rocauc: 0.5934630508196932
ogbg-molsider: 0.5934630508196932
OGBNanLabelBCEWithLogitsLoss: 0.6391599893569946
Statistics on  train
mean_pred: 1.0902363061904907
std_pred: 3.7587730884552
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8784286480604443
rocauc: 0.9161626834323905
ogbg-molsider: 0.9161626834323905
OGBNanLabelBCEWithLogitsLoss: 0.29537467327382827
[Epoch 127] ogbg-molsider: 0.565363 test loss: 19.563882
[Epoch 128; Iter    18/   36] train: loss: 0.1123765
[Epoch 128] ogbg-molsider: 0.566598 val loss: 11.447709
[Epoch 128] ogbg-molsider: 0.571731 test loss: 18.978522
[Epoch 129; Iter    12/   36] train: loss: 0.1290522
[Epoch 129] ogbg-molsider: 0.552641 val loss: 12.935249
[Epoch 129] ogbg-molsider: 0.565336 test loss: 21.585486
[Epoch 130; Iter     6/   36] train: loss: 0.1091862
[Epoch 130; Iter    36/   36] train: loss: 0.0964609
[Epoch 130] ogbg-molsider: 0.547106 val loss: 10.512341
[Epoch 130] ogbg-molsider: 0.569342 test loss: 16.497003
[Epoch 131; Iter    30/   36] train: loss: 0.1145026
[Epoch 131] ogbg-molsider: 0.561794 val loss: 13.687759
[Epoch 131] ogbg-molsider: 0.570272 test loss: 23.227175
[Epoch 132; Iter    24/   36] train: loss: 0.1118862
[Epoch 132] ogbg-molsider: 0.547038 val loss: 11.545278
[Epoch 132] ogbg-molsider: 0.561562 test loss: 18.691584
[Epoch 133; Iter    18/   36] train: loss: 0.1146005
[Epoch 133] ogbg-molsider: 0.550306 val loss: 12.446068
[Epoch 133] ogbg-molsider: 0.564388 test loss: 19.725900
[Epoch 134; Iter    12/   36] train: loss: 0.0820189
[Epoch 134] ogbg-molsider: 0.540020 val loss: 7.708620
[Epoch 134] ogbg-molsider: 0.564858 test loss: 10.799732
[Epoch 135; Iter     6/   36] train: loss: 0.0666385
[Epoch 135; Iter    36/   36] train: loss: 0.0857640
[Epoch 135] ogbg-molsider: 0.548262 val loss: 11.897422
[Epoch 135] ogbg-molsider: 0.560200 test loss: 18.874512
[Epoch 136; Iter    30/   36] train: loss: 0.0989459
[Epoch 136] ogbg-molsider: 0.538568 val loss: 12.998105
[Epoch 136] ogbg-molsider: 0.558603 test loss: 21.978991
Early stopping criterion based on -ogbg-molsider- that should be max reached after 136 epochs. Best model checkpoint was in epoch 76.
Statistics on  val_best_checkpoint
mean_pred: 2.470463514328003
std_pred: 30.94676399230957
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6437546958426454
rocauc: 0.5988215522128034
ogbg-molsider: 0.5988215522128034
OGBNanLabelBCEWithLogitsLoss: 3.9961497664451597
Statistics on  test
mean_pred: 2.4941165447235107
std_pred: 44.387996673583984
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6237874221562093
rocauc: 0.5942855913013726
ogbg-molsider: 0.5942855913013726
OGBNanLabelBCEWithLogitsLoss: 8.159096217155456
Statistics on  train
mean_pred: 0.48532941937446594
std_pred: 3.5599687099456787
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8804000465298089
rocauc: 0.9253578469723676
ogbg-molsider: 0.9253578469723676
OGBNanLabelBCEWithLogitsLoss: 0.2858731254107422
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.2.yml --seed 4 --device cuda:3
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.2.yml --seed 5 --device cuda:3
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.2.yml --seed 6 --device cuda:3
All runs completed.
[Epoch 128] ogbg-molsider: 0.569779 val loss: 2.836476
[Epoch 128] ogbg-molsider: 0.559578 test loss: 4.564031
[Epoch 129; Iter    12/   36] train: loss: 0.1068906
[Epoch 129] ogbg-molsider: 0.564903 val loss: 5.368074
[Epoch 129] ogbg-molsider: 0.556170 test loss: 7.182823
[Epoch 130; Iter     6/   36] train: loss: 0.1267479
[Epoch 130; Iter    36/   36] train: loss: 0.1162613
[Epoch 130] ogbg-molsider: 0.567305 val loss: 6.420105
[Epoch 130] ogbg-molsider: 0.549275 test loss: 10.375722
[Epoch 131; Iter    30/   36] train: loss: 0.1257205
[Epoch 131] ogbg-molsider: 0.571463 val loss: 4.035312
[Epoch 131] ogbg-molsider: 0.552192 test loss: 6.918155
[Epoch 132; Iter    24/   36] train: loss: 0.0888525
[Epoch 132] ogbg-molsider: 0.577728 val loss: 2.515808
[Epoch 132] ogbg-molsider: 0.554344 test loss: 4.638533
[Epoch 133; Iter    18/   36] train: loss: 0.1255784
[Epoch 133] ogbg-molsider: 0.572662 val loss: 6.230570
[Epoch 133] ogbg-molsider: 0.540801 test loss: 11.978257
[Epoch 134; Iter    12/   36] train: loss: 0.0852035
[Epoch 134] ogbg-molsider: 0.568537 val loss: 7.810934
[Epoch 134] ogbg-molsider: 0.548249 test loss: 14.136388
[Epoch 135; Iter     6/   36] train: loss: 0.1081487
[Epoch 135; Iter    36/   36] train: loss: 0.1662589
[Epoch 135] ogbg-molsider: 0.574260 val loss: 4.496343
[Epoch 135] ogbg-molsider: 0.544892 test loss: 7.672747
[Epoch 136; Iter    30/   36] train: loss: 0.1081426
[Epoch 136] ogbg-molsider: 0.568432 val loss: 4.742897
[Epoch 136] ogbg-molsider: 0.544672 test loss: 8.913316
[Epoch 137; Iter    24/   36] train: loss: 0.1006630
[Epoch 137] ogbg-molsider: 0.574052 val loss: 5.928134
[Epoch 137] ogbg-molsider: 0.545347 test loss: 10.494427
[Epoch 138; Iter    18/   36] train: loss: 0.1263143
[Epoch 138] ogbg-molsider: 0.570937 val loss: 5.286412
[Epoch 138] ogbg-molsider: 0.544531 test loss: 8.645641
[Epoch 139; Iter    12/   36] train: loss: 0.0820141
[Epoch 139] ogbg-molsider: 0.572745 val loss: 4.595226
[Epoch 139] ogbg-molsider: 0.549822 test loss: 7.693465
[Epoch 140; Iter     6/   36] train: loss: 0.0868058
[Epoch 140; Iter    36/   36] train: loss: 0.0873200
[Epoch 140] ogbg-molsider: 0.570896 val loss: 7.285378
[Epoch 140] ogbg-molsider: 0.538487 test loss: 14.533245
Early stopping criterion based on -ogbg-molsider- that should be max reached after 140 epochs. Best model checkpoint was in epoch 80.
Statistics on  val_best_checkpoint
mean_pred: 0.5711537599563599
std_pred: 6.925765514373779
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6698731068563396
rocauc: 0.6263235891317008
ogbg-molsider: 0.6263235891317008
OGBNanLabelBCEWithLogitsLoss: 0.8981243014335633
Statistics on  test
mean_pred: 0.4176909029483795
std_pred: 3.6901426315307617
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6283057893769032
rocauc: 0.5910964179554821
ogbg-molsider: 0.5910964179554821
OGBNanLabelBCEWithLogitsLoss: 0.6963593602180481
Statistics on  train
mean_pred: 1.096614122390747
std_pred: 3.89119553565979
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.9231757634493525
rocauc: 0.9521220773439194
ogbg-molsider: 0.9521220773439194
OGBNanLabelBCEWithLogitsLoss: 0.23514855197734302
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.1.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.1.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.1.yml --seed 6 --device cuda:2
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.594960 val loss: 0.793102
[Epoch 128] ogbg-molsider: 0.582111 test loss: 0.957348
[Epoch 129; Iter    12/   36] train: loss: 0.1298697
[Epoch 129] ogbg-molsider: 0.607325 val loss: 0.805288
[Epoch 129] ogbg-molsider: 0.590844 test loss: 0.985468
[Epoch 130; Iter     6/   36] train: loss: 0.1149512
[Epoch 130; Iter    36/   36] train: loss: 0.1196151
[Epoch 130] ogbg-molsider: 0.607962 val loss: 0.845305
[Epoch 130] ogbg-molsider: 0.578691 test loss: 1.008519
[Epoch 131; Iter    30/   36] train: loss: 0.1344328
[Epoch 131] ogbg-molsider: 0.609303 val loss: 0.816967
[Epoch 131] ogbg-molsider: 0.583995 test loss: 0.979680
[Epoch 132; Iter    24/   36] train: loss: 0.0926628
[Epoch 132] ogbg-molsider: 0.614591 val loss: 0.773425
[Epoch 132] ogbg-molsider: 0.581558 test loss: 0.955924
[Epoch 133; Iter    18/   36] train: loss: 0.0942273
[Epoch 133] ogbg-molsider: 0.611042 val loss: 0.780731
[Epoch 133] ogbg-molsider: 0.587117 test loss: 0.924988
[Epoch 134; Iter    12/   36] train: loss: 0.0941552
[Epoch 134] ogbg-molsider: 0.605016 val loss: 0.860809
[Epoch 134] ogbg-molsider: 0.591599 test loss: 1.018105
[Epoch 135; Iter     6/   36] train: loss: 0.1164398
[Epoch 135; Iter    36/   36] train: loss: 0.1531712
[Epoch 135] ogbg-molsider: 0.600720 val loss: 0.824204
[Epoch 135] ogbg-molsider: 0.586122 test loss: 0.979325
[Epoch 136; Iter    30/   36] train: loss: 0.0764352
[Epoch 136] ogbg-molsider: 0.600064 val loss: 0.835472
[Epoch 136] ogbg-molsider: 0.588578 test loss: 0.986021
[Epoch 137; Iter    24/   36] train: loss: 0.1074426
[Epoch 137] ogbg-molsider: 0.606768 val loss: 0.822907
[Epoch 137] ogbg-molsider: 0.584499 test loss: 1.002261
[Epoch 138; Iter    18/   36] train: loss: 0.1119542
[Epoch 138] ogbg-molsider: 0.610006 val loss: 0.833897
[Epoch 138] ogbg-molsider: 0.586394 test loss: 1.013806
[Epoch 139; Iter    12/   36] train: loss: 0.0887280
[Epoch 139] ogbg-molsider: 0.609482 val loss: 0.826533
[Epoch 139] ogbg-molsider: 0.579231 test loss: 0.993144
[Epoch 140; Iter     6/   36] train: loss: 0.0938569
[Epoch 140; Iter    36/   36] train: loss: 0.1258865
[Epoch 140] ogbg-molsider: 0.610017 val loss: 0.814076
[Epoch 140] ogbg-molsider: 0.584611 test loss: 0.972531
[Epoch 141; Iter    30/   36] train: loss: 0.1049528
[Epoch 141] ogbg-molsider: 0.606078 val loss: 0.866711
[Epoch 141] ogbg-molsider: 0.583502 test loss: 1.050404
[Epoch 142; Iter    24/   36] train: loss: 0.1175190
[Epoch 142] ogbg-molsider: 0.610964 val loss: 0.828816
[Epoch 142] ogbg-molsider: 0.584843 test loss: 1.013294
[Epoch 143; Iter    18/   36] train: loss: 0.0905312
[Epoch 143] ogbg-molsider: 0.609479 val loss: 0.863850
[Epoch 143] ogbg-molsider: 0.582639 test loss: 1.039037
[Epoch 144; Iter    12/   36] train: loss: 0.0814857
[Epoch 144] ogbg-molsider: 0.608885 val loss: 0.854211
[Epoch 144] ogbg-molsider: 0.581406 test loss: 1.022196
[Epoch 145; Iter     6/   36] train: loss: 0.1189764
[Epoch 145; Iter    36/   36] train: loss: 0.1073609
[Epoch 145] ogbg-molsider: 0.600804 val loss: 0.849209
[Epoch 145] ogbg-molsider: 0.583490 test loss: 1.044193
[Epoch 146; Iter    30/   36] train: loss: 0.0733339
[Epoch 146] ogbg-molsider: 0.606686 val loss: 0.852742
[Epoch 146] ogbg-molsider: 0.579544 test loss: 1.034435
[Epoch 147; Iter    24/   36] train: loss: 0.0935525
[Epoch 147] ogbg-molsider: 0.609972 val loss: 0.841406
[Epoch 147] ogbg-molsider: 0.581837 test loss: 1.020648
[Epoch 148; Iter    18/   36] train: loss: 0.0845053
[Epoch 148] ogbg-molsider: 0.605256 val loss: 0.861115
[Epoch 148] ogbg-molsider: 0.578879 test loss: 1.035693
[Epoch 149; Iter    12/   36] train: loss: 0.0869945
[Epoch 149] ogbg-molsider: 0.595284 val loss: 0.850686
[Epoch 149] ogbg-molsider: 0.577804 test loss: 1.007032
[Epoch 150; Iter     6/   36] train: loss: 0.1030253
[Epoch 150; Iter    36/   36] train: loss: 0.0990325
[Epoch 150] ogbg-molsider: 0.609858 val loss: 0.844950
[Epoch 150] ogbg-molsider: 0.577248 test loss: 1.003671
[Epoch 151; Iter    30/   36] train: loss: 0.0788549
[Epoch 151] ogbg-molsider: 0.604968 val loss: 0.859093
[Epoch 151] ogbg-molsider: 0.584012 test loss: 1.031467
[Epoch 152; Iter    24/   36] train: loss: 0.0783149
[Epoch 152] ogbg-molsider: 0.597125 val loss: 0.893728
[Epoch 152] ogbg-molsider: 0.576784 test loss: 1.068683
[Epoch 153; Iter    18/   36] train: loss: 0.0673830
[Epoch 153] ogbg-molsider: 0.603562 val loss: 0.852927
[Epoch 153] ogbg-molsider: 0.585097 test loss: 0.993771
[Epoch 154; Iter    12/   36] train: loss: 0.0807635
[Epoch 154] ogbg-molsider: 0.612332 val loss: 0.847735
[Epoch 154] ogbg-molsider: 0.585301 test loss: 1.023600
Early stopping criterion based on -ogbg-molsider- that should be max reached after 154 epochs. Best model checkpoint was in epoch 94.
Statistics on  val_best_checkpoint
mean_pred: -0.31186550855636597
std_pred: 3.3453009128570557
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6784881217212473
rocauc: 0.6246192159767382
ogbg-molsider: 0.6246192159767382
OGBNanLabelBCEWithLogitsLoss: 0.7066802501678466
Statistics on  test
mean_pred: -0.3575977683067322
std_pred: 3.6889710426330566
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.620902445042741
rocauc: 0.5772073185629193
ogbg-molsider: 0.5772073185629193
OGBNanLabelBCEWithLogitsLoss: 0.8744596838951111
Statistics on  train
mean_pred: 0.4260292947292328
std_pred: 3.8813703060150146
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.9557558898419594
rocauc: 0.9718175448231461
ogbg-molsider: 0.9718175448231461
OGBNanLabelBCEWithLogitsLoss: 0.19121729872292942
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.05.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.05.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.05.yml --seed 6 --device cuda:1
All runs completed.
[Epoch 81; Iter    30/   36] train: loss: 0.2716177
[Epoch 81] ogbg-molsider: 0.620075 val loss: 0.530386
[Epoch 81] ogbg-molsider: 0.583552 test loss: 0.592067
[Epoch 82; Iter    24/   36] train: loss: 0.2939909
[Epoch 82] ogbg-molsider: 0.622256 val loss: 0.726577
[Epoch 82] ogbg-molsider: 0.574803 test loss: 0.617569
[Epoch 83; Iter    18/   36] train: loss: 0.2709071
[Epoch 83] ogbg-molsider: 0.614509 val loss: 0.638909
[Epoch 83] ogbg-molsider: 0.569115 test loss: 0.611558
[Epoch 84; Iter    12/   36] train: loss: 0.2634694
[Epoch 84] ogbg-molsider: 0.628338 val loss: 0.544472
[Epoch 84] ogbg-molsider: 0.577932 test loss: 0.597926
[Epoch 85; Iter     6/   36] train: loss: 0.3059210
[Epoch 85; Iter    36/   36] train: loss: 0.2939510
[Epoch 85] ogbg-molsider: 0.622400 val loss: 0.729851
[Epoch 85] ogbg-molsider: 0.560760 test loss: 0.638948
[Epoch 86; Iter    30/   36] train: loss: 0.3211543
[Epoch 86] ogbg-molsider: 0.624070 val loss: 0.603546
[Epoch 86] ogbg-molsider: 0.573938 test loss: 0.627061
[Epoch 87; Iter    24/   36] train: loss: 0.2835831
[Epoch 87] ogbg-molsider: 0.630416 val loss: 0.561643
[Epoch 87] ogbg-molsider: 0.578805 test loss: 0.616422
[Epoch 88; Iter    18/   36] train: loss: 0.2799768
[Epoch 88] ogbg-molsider: 0.628760 val loss: 0.563827
[Epoch 88] ogbg-molsider: 0.577127 test loss: 0.615972
[Epoch 89; Iter    12/   36] train: loss: 0.2575139
[Epoch 89] ogbg-molsider: 0.624130 val loss: 0.588014
[Epoch 89] ogbg-molsider: 0.567319 test loss: 0.631035
[Epoch 90; Iter     6/   36] train: loss: 0.2631460
[Epoch 90; Iter    36/   36] train: loss: 0.3018519
[Epoch 90] ogbg-molsider: 0.632393 val loss: 0.569461
[Epoch 90] ogbg-molsider: 0.573378 test loss: 0.645871
[Epoch 91; Iter    30/   36] train: loss: 0.3043694
[Epoch 91] ogbg-molsider: 0.632471 val loss: 0.567335
[Epoch 91] ogbg-molsider: 0.570417 test loss: 0.665208
[Epoch 92; Iter    24/   36] train: loss: 0.2934285
[Epoch 92] ogbg-molsider: 0.644077 val loss: 0.558500
[Epoch 92] ogbg-molsider: 0.578093 test loss: 0.637141
[Epoch 93; Iter    18/   36] train: loss: 0.2937558
[Epoch 93] ogbg-molsider: 0.633290 val loss: 0.564781
[Epoch 93] ogbg-molsider: 0.564609 test loss: 0.651212
[Epoch 94; Iter    12/   36] train: loss: 0.2652455
[Epoch 94] ogbg-molsider: 0.625960 val loss: 0.573225
[Epoch 94] ogbg-molsider: 0.575387 test loss: 0.639004
[Epoch 95; Iter     6/   36] train: loss: 0.2586569
[Epoch 95; Iter    36/   36] train: loss: 0.2754122
[Epoch 95] ogbg-molsider: 0.631865 val loss: 0.681924
[Epoch 95] ogbg-molsider: 0.572969 test loss: 0.640454
[Epoch 96; Iter    30/   36] train: loss: 0.2714358
[Epoch 96] ogbg-molsider: 0.629462 val loss: 0.580944
[Epoch 96] ogbg-molsider: 0.571644 test loss: 0.663407
[Epoch 97; Iter    24/   36] train: loss: 0.2482283
[Epoch 97] ogbg-molsider: 0.627962 val loss: 0.574908
[Epoch 97] ogbg-molsider: 0.567450 test loss: 0.665478
[Epoch 98; Iter    18/   36] train: loss: 0.2817286
[Epoch 98] ogbg-molsider: 0.636405 val loss: 0.584804
[Epoch 98] ogbg-molsider: 0.566385 test loss: 0.658365
[Epoch 99; Iter    12/   36] train: loss: 0.2414606
[Epoch 99] ogbg-molsider: 0.638677 val loss: 0.593364
[Epoch 99] ogbg-molsider: 0.563333 test loss: 0.690164
[Epoch 100; Iter     6/   36] train: loss: 0.2358805
[Epoch 100; Iter    36/   36] train: loss: 0.2532208
[Epoch 100] ogbg-molsider: 0.641722 val loss: 0.806806
[Epoch 100] ogbg-molsider: 0.566700 test loss: 0.692675
[Epoch 101; Iter    30/   36] train: loss: 0.3013086
[Epoch 101] ogbg-molsider: 0.632747 val loss: 0.587969
[Epoch 101] ogbg-molsider: 0.567350 test loss: 0.669656
[Epoch 102; Iter    24/   36] train: loss: 0.2882648
[Epoch 102] ogbg-molsider: 0.639211 val loss: 0.581427
[Epoch 102] ogbg-molsider: 0.588326 test loss: 0.678332
[Epoch 103; Iter    18/   36] train: loss: 0.2725013
[Epoch 103] ogbg-molsider: 0.633074 val loss: 0.597342
[Epoch 103] ogbg-molsider: 0.572422 test loss: 0.697170
[Epoch 104; Iter    12/   36] train: loss: 0.2655987
[Epoch 104] ogbg-molsider: 0.620297 val loss: 0.628518
[Epoch 104] ogbg-molsider: 0.584283 test loss: 0.703679
[Epoch 105; Iter     6/   36] train: loss: 0.2969283
[Epoch 105; Iter    36/   36] train: loss: 0.2942354
[Epoch 105] ogbg-molsider: 0.619342 val loss: 0.617844
[Epoch 105] ogbg-molsider: 0.583999 test loss: 0.652219
[Epoch 106; Iter    30/   36] train: loss: 0.2556510
[Epoch 106] ogbg-molsider: 0.636897 val loss: 0.597920
[Epoch 106] ogbg-molsider: 0.572686 test loss: 0.676083
[Epoch 107; Iter    24/   36] train: loss: 0.2257577
[Epoch 107] ogbg-molsider: 0.633612 val loss: 0.599108
[Epoch 107] ogbg-molsider: 0.583701 test loss: 0.667391
[Epoch 108; Iter    18/   36] train: loss: 0.2533564
[Epoch 108] ogbg-molsider: 0.634649 val loss: 0.601933
[Epoch 108] ogbg-molsider: 0.581732 test loss: 0.656008
[Epoch 109; Iter    12/   36] train: loss: 0.2397816
[Epoch 109] ogbg-molsider: 0.636307 val loss: 0.605034
[Epoch 109] ogbg-molsider: 0.583689 test loss: 0.660781
[Epoch 110; Iter     6/   36] train: loss: 0.2200697
[Epoch 110; Iter    36/   36] train: loss: 0.2634615
[Epoch 110] ogbg-molsider: 0.638562 val loss: 0.605272
[Epoch 110] ogbg-molsider: 0.579865 test loss: 0.686760
[Epoch 111; Iter    30/   36] train: loss: 0.2641587
[Epoch 111] ogbg-molsider: 0.629635 val loss: 0.608740
[Epoch 111] ogbg-molsider: 0.587932 test loss: 0.659577
[Epoch 112; Iter    24/   36] train: loss: 0.2396593
[Epoch 112] ogbg-molsider: 0.632132 val loss: 0.615455
[Epoch 112] ogbg-molsider: 0.575933 test loss: 0.680101
[Epoch 113; Iter    18/   36] train: loss: 0.2368393
[Epoch 113] ogbg-molsider: 0.628563 val loss: 0.629717
[Epoch 113] ogbg-molsider: 0.573904 test loss: 0.702738
[Epoch 114; Iter    12/   36] train: loss: 0.2312958
[Epoch 114] ogbg-molsider: 0.631770 val loss: 0.634930
[Epoch 114] ogbg-molsider: 0.579111 test loss: 0.691315
[Epoch 115; Iter     6/   36] train: loss: 0.2089627
[Epoch 115; Iter    36/   36] train: loss: 0.2719617
[Epoch 115] ogbg-molsider: 0.634881 val loss: 0.628025
[Epoch 115] ogbg-molsider: 0.588539 test loss: 0.702196
[Epoch 116; Iter    30/   36] train: loss: 0.2244437
[Epoch 116] ogbg-molsider: 0.639141 val loss: 0.617846
[Epoch 116] ogbg-molsider: 0.580890 test loss: 0.707259
[Epoch 117; Iter    24/   36] train: loss: 0.2551567
[Epoch 117] ogbg-molsider: 0.637673 val loss: 0.629870
[Epoch 117] ogbg-molsider: 0.580860 test loss: 0.708665
[Epoch 118; Iter    18/   36] train: loss: 0.2262005
[Epoch 118] ogbg-molsider: 0.633390 val loss: 0.641908
[Epoch 118] ogbg-molsider: 0.579831 test loss: 0.720504
[Epoch 119; Iter    12/   36] train: loss: 0.2206234
[Epoch 119] ogbg-molsider: 0.634012 val loss: 0.637065
[Epoch 119] ogbg-molsider: 0.589282 test loss: 0.723561
[Epoch 120; Iter     6/   36] train: loss: 0.2029085
[Epoch 120; Iter    36/   36] train: loss: 0.2439613
[Epoch 120] ogbg-molsider: 0.616702 val loss: 0.652598
[Epoch 120] ogbg-molsider: 0.577493 test loss: 0.739220
[Epoch 121; Iter    30/   36] train: loss: 0.2031865
[Epoch 121] ogbg-molsider: 0.629804 val loss: 0.625518
[Epoch 121] ogbg-molsider: 0.579072 test loss: 0.724445
[Epoch 122; Iter    24/   36] train: loss: 0.2203375
[Epoch 122] ogbg-molsider: 0.623335 val loss: 0.656580
[Epoch 122] ogbg-molsider: 0.589535 test loss: 0.717524
[Epoch 123; Iter    18/   36] train: loss: 0.1883836
[Epoch 123] ogbg-molsider: 0.628052 val loss: 0.644941
[Epoch 123] ogbg-molsider: 0.586825 test loss: 0.720074
[Epoch 124; Iter    12/   36] train: loss: 0.2002730
[Epoch 124] ogbg-molsider: 0.634960 val loss: 0.623026
[Epoch 124] ogbg-molsider: 0.579478 test loss: 0.719681
[Epoch 125; Iter     6/   36] train: loss: 0.1956430
[Epoch 125; Iter    36/   36] train: loss: 0.2270494
[Epoch 125] ogbg-molsider: 0.629197 val loss: 0.647577
[Epoch 125] ogbg-molsider: 0.581965 test loss: 0.734564
[Epoch 126; Iter    30/   36] train: loss: 0.2183723
[Epoch 126] ogbg-molsider: 0.609159 val loss: 0.674266
[Epoch 126] ogbg-molsider: 0.588154 test loss: 0.739917
[Epoch 127; Iter    24/   36] train: loss: 0.2238523
[Epoch 127] ogbg-molsider: 0.622141 val loss: 0.672004
[Epoch 127] ogbg-molsider: 0.586961 test loss: 0.771741
[Epoch 128; Iter    18/   36] train: loss: 0.2023691
[Epoch 81; Iter    30/   36] train: loss: 0.4006839
[Epoch 81] ogbg-molsider: 0.617155 val loss: 0.542481
[Epoch 81] ogbg-molsider: 0.567779 test loss: 0.589443
[Epoch 82; Iter    24/   36] train: loss: 0.3415984
[Epoch 82] ogbg-molsider: 0.626604 val loss: 0.555106
[Epoch 82] ogbg-molsider: 0.576021 test loss: 0.614231
[Epoch 83; Iter    18/   36] train: loss: 0.3313384
[Epoch 83] ogbg-molsider: 0.619904 val loss: 0.545998
[Epoch 83] ogbg-molsider: 0.560271 test loss: 0.618007
[Epoch 84; Iter    12/   36] train: loss: 0.3098894
[Epoch 84] ogbg-molsider: 0.620172 val loss: 0.567028
[Epoch 84] ogbg-molsider: 0.550745 test loss: 0.654965
[Epoch 85; Iter     6/   36] train: loss: 0.3380974
[Epoch 85; Iter    36/   36] train: loss: 0.3742452
[Epoch 85] ogbg-molsider: 0.638330 val loss: 0.776190
[Epoch 85] ogbg-molsider: 0.588627 test loss: 0.602847
[Epoch 86; Iter    30/   36] train: loss: 0.2768618
[Epoch 86] ogbg-molsider: 0.638876 val loss: 0.530578
[Epoch 86] ogbg-molsider: 0.572903 test loss: 0.596238
[Epoch 87; Iter    24/   36] train: loss: 0.3111278
[Epoch 87] ogbg-molsider: 0.647876 val loss: 0.537352
[Epoch 87] ogbg-molsider: 0.577497 test loss: 0.601598
[Epoch 88; Iter    18/   36] train: loss: 0.2681088
[Epoch 88] ogbg-molsider: 0.630447 val loss: 0.545977
[Epoch 88] ogbg-molsider: 0.553435 test loss: 0.621562
[Epoch 89; Iter    12/   36] train: loss: 0.3055508
[Epoch 89] ogbg-molsider: 0.637486 val loss: 0.557206
[Epoch 89] ogbg-molsider: 0.580356 test loss: 0.631779
[Epoch 90; Iter     6/   36] train: loss: 0.2518313
[Epoch 90; Iter    36/   36] train: loss: 0.3413802
[Epoch 90] ogbg-molsider: 0.610753 val loss: 0.608397
[Epoch 90] ogbg-molsider: 0.571010 test loss: 1.083858
[Epoch 91; Iter    30/   36] train: loss: 0.3360008
[Epoch 91] ogbg-molsider: 0.619156 val loss: 0.579001
[Epoch 91] ogbg-molsider: 0.585205 test loss: 0.643010
[Epoch 92; Iter    24/   36] train: loss: 0.3075706
[Epoch 92] ogbg-molsider: 0.647267 val loss: 0.553137
[Epoch 92] ogbg-molsider: 0.580465 test loss: 0.625778
[Epoch 93; Iter    18/   36] train: loss: 0.2671650
[Epoch 93] ogbg-molsider: 0.629573 val loss: 0.554136
[Epoch 93] ogbg-molsider: 0.567736 test loss: 0.622877
[Epoch 94; Iter    12/   36] train: loss: 0.2915056
[Epoch 94] ogbg-molsider: 0.631986 val loss: 0.570416
[Epoch 94] ogbg-molsider: 0.578594 test loss: 0.618358
[Epoch 95; Iter     6/   36] train: loss: 0.2858477
[Epoch 95; Iter    36/   36] train: loss: 0.3695715
[Epoch 95] ogbg-molsider: 0.632149 val loss: 0.585779
[Epoch 95] ogbg-molsider: 0.573736 test loss: 0.641438
[Epoch 96; Iter    30/   36] train: loss: 0.3227125
[Epoch 96] ogbg-molsider: 0.626143 val loss: 0.591273
[Epoch 96] ogbg-molsider: 0.578854 test loss: 0.670215
[Epoch 97; Iter    24/   36] train: loss: 0.2770921
[Epoch 97] ogbg-molsider: 0.628861 val loss: 0.603985
[Epoch 97] ogbg-molsider: 0.575466 test loss: 0.668180
[Epoch 98; Iter    18/   36] train: loss: 0.2992436
[Epoch 98] ogbg-molsider: 0.640450 val loss: 0.568548
[Epoch 98] ogbg-molsider: 0.573852 test loss: 0.644149
[Epoch 99; Iter    12/   36] train: loss: 0.2936125
[Epoch 99] ogbg-molsider: 0.637385 val loss: 0.613935
[Epoch 99] ogbg-molsider: 0.590929 test loss: 0.676820
[Epoch 100; Iter     6/   36] train: loss: 0.2292213
[Epoch 100; Iter    36/   36] train: loss: 0.2821141
[Epoch 100] ogbg-molsider: 0.622058 val loss: 0.610913
[Epoch 100] ogbg-molsider: 0.574190 test loss: 0.663552
[Epoch 101; Iter    30/   36] train: loss: 0.2558571
[Epoch 101] ogbg-molsider: 0.628212 val loss: 0.626105
[Epoch 101] ogbg-molsider: 0.576447 test loss: 0.729634
[Epoch 102; Iter    24/   36] train: loss: 0.2731527
[Epoch 102] ogbg-molsider: 0.624356 val loss: 0.613503
[Epoch 102] ogbg-molsider: 0.575384 test loss: 0.696813
[Epoch 103; Iter    18/   36] train: loss: 0.2650782
[Epoch 103] ogbg-molsider: 0.612110 val loss: 0.630755
[Epoch 103] ogbg-molsider: 0.575647 test loss: 0.661531
[Epoch 104; Iter    12/   36] train: loss: 0.2444722
[Epoch 104] ogbg-molsider: 0.624356 val loss: 0.622276
[Epoch 104] ogbg-molsider: 0.588020 test loss: 0.681873
[Epoch 105; Iter     6/   36] train: loss: 0.2350995
[Epoch 105; Iter    36/   36] train: loss: 0.3076883
[Epoch 105] ogbg-molsider: 0.632772 val loss: 0.629173
[Epoch 105] ogbg-molsider: 0.587535 test loss: 0.675853
[Epoch 106; Iter    30/   36] train: loss: 0.2451803
[Epoch 106] ogbg-molsider: 0.633338 val loss: 0.615492
[Epoch 106] ogbg-molsider: 0.604439 test loss: 0.656088
[Epoch 107; Iter    24/   36] train: loss: 0.2708675
[Epoch 107] ogbg-molsider: 0.621920 val loss: 0.643174
[Epoch 107] ogbg-molsider: 0.592833 test loss: 0.685947
[Epoch 108; Iter    18/   36] train: loss: 0.2210301
[Epoch 108] ogbg-molsider: 0.619798 val loss: 0.609127
[Epoch 108] ogbg-molsider: 0.596525 test loss: 0.654289
[Epoch 109; Iter    12/   36] train: loss: 0.2311190
[Epoch 109] ogbg-molsider: 0.622026 val loss: 0.617253
[Epoch 109] ogbg-molsider: 0.578157 test loss: 0.676855
[Epoch 110; Iter     6/   36] train: loss: 0.2713634
[Epoch 110; Iter    36/   36] train: loss: 0.2690716
[Epoch 110] ogbg-molsider: 0.631500 val loss: 0.614635
[Epoch 110] ogbg-molsider: 0.590077 test loss: 0.685658
[Epoch 111; Iter    30/   36] train: loss: 0.2414800
[Epoch 111] ogbg-molsider: 0.629018 val loss: 0.636702
[Epoch 111] ogbg-molsider: 0.588945 test loss: 0.709287
[Epoch 112; Iter    24/   36] train: loss: 0.2046444
[Epoch 112] ogbg-molsider: 0.627478 val loss: 0.621691
[Epoch 112] ogbg-molsider: 0.592583 test loss: 0.691457
[Epoch 113; Iter    18/   36] train: loss: 0.2144407
[Epoch 113] ogbg-molsider: 0.622698 val loss: 0.615445
[Epoch 113] ogbg-molsider: 0.592323 test loss: 0.668418
[Epoch 114; Iter    12/   36] train: loss: 0.2317086
[Epoch 114] ogbg-molsider: 0.613056 val loss: 0.641474
[Epoch 114] ogbg-molsider: 0.593265 test loss: 0.688805
[Epoch 115; Iter     6/   36] train: loss: 0.2286428
[Epoch 115; Iter    36/   36] train: loss: 0.2148803
[Epoch 115] ogbg-molsider: 0.616945 val loss: 0.635414
[Epoch 115] ogbg-molsider: 0.586318 test loss: 0.684552
[Epoch 116; Iter    30/   36] train: loss: 0.2434959
[Epoch 116] ogbg-molsider: 0.619085 val loss: 0.639615
[Epoch 116] ogbg-molsider: 0.587052 test loss: 0.693767
[Epoch 117; Iter    24/   36] train: loss: 0.2742712
[Epoch 117] ogbg-molsider: 0.631711 val loss: 0.622897
[Epoch 117] ogbg-molsider: 0.598299 test loss: 0.682696
[Epoch 118; Iter    18/   36] train: loss: 0.1965533
[Epoch 118] ogbg-molsider: 0.625825 val loss: 0.636187
[Epoch 118] ogbg-molsider: 0.596536 test loss: 0.702390
[Epoch 119; Iter    12/   36] train: loss: 0.2296332
[Epoch 119] ogbg-molsider: 0.615337 val loss: 0.651749
[Epoch 119] ogbg-molsider: 0.592439 test loss: 0.709108
[Epoch 120; Iter     6/   36] train: loss: 0.2525286
[Epoch 120; Iter    36/   36] train: loss: 0.3315492
[Epoch 120] ogbg-molsider: 0.626832 val loss: 0.637717
[Epoch 120] ogbg-molsider: 0.589521 test loss: 0.713552
[Epoch 121; Iter    30/   36] train: loss: 0.2223501
[Epoch 121] ogbg-molsider: 0.624398 val loss: 0.632172
[Epoch 121] ogbg-molsider: 0.600207 test loss: 0.690077
[Epoch 122; Iter    24/   36] train: loss: 0.2324508
[Epoch 122] ogbg-molsider: 0.633363 val loss: 0.641889
[Epoch 122] ogbg-molsider: 0.599266 test loss: 0.704833
[Epoch 123; Iter    18/   36] train: loss: 0.2592441
[Epoch 123] ogbg-molsider: 0.627463 val loss: 0.647755
[Epoch 123] ogbg-molsider: 0.608015 test loss: 0.687860
[Epoch 124; Iter    12/   36] train: loss: 0.2365537
[Epoch 124] ogbg-molsider: 0.620894 val loss: 0.635636
[Epoch 124] ogbg-molsider: 0.593377 test loss: 0.692605
[Epoch 125; Iter     6/   36] train: loss: 0.2215922
[Epoch 125; Iter    36/   36] train: loss: 0.2057468
[Epoch 125] ogbg-molsider: 0.624340 val loss: 0.649397
[Epoch 125] ogbg-molsider: 0.589196 test loss: 0.698602
[Epoch 126; Iter    30/   36] train: loss: 0.2300218
[Epoch 126] ogbg-molsider: 0.626750 val loss: 0.643537
[Epoch 126] ogbg-molsider: 0.594712 test loss: 0.692097
[Epoch 127; Iter    24/   36] train: loss: 0.2358010
[Epoch 127] ogbg-molsider: 0.629109 val loss: 0.653436
[Epoch 127] ogbg-molsider: 0.587729 test loss: 0.702199
[Epoch 128; Iter    18/   36] train: loss: 0.2103738
[Epoch 81; Iter    30/   36] train: loss: 0.3218274
[Epoch 81] ogbg-molsider: 0.655435 val loss: 0.512196
[Epoch 81] ogbg-molsider: 0.587110 test loss: 0.587308
[Epoch 82; Iter    24/   36] train: loss: 0.3240303
[Epoch 82] ogbg-molsider: 0.630022 val loss: 0.562086
[Epoch 82] ogbg-molsider: 0.584707 test loss: 0.604812
[Epoch 83; Iter    18/   36] train: loss: 0.3219155
[Epoch 83] ogbg-molsider: 0.645006 val loss: 0.531973
[Epoch 83] ogbg-molsider: 0.576475 test loss: 0.590776
[Epoch 84; Iter    12/   36] train: loss: 0.3255418
[Epoch 84] ogbg-molsider: 0.640043 val loss: 0.546522
[Epoch 84] ogbg-molsider: 0.577219 test loss: 0.609751
[Epoch 85; Iter     6/   36] train: loss: 0.2930594
[Epoch 85; Iter    36/   36] train: loss: 0.3139274
[Epoch 85] ogbg-molsider: 0.637722 val loss: 0.535548
[Epoch 85] ogbg-molsider: 0.582760 test loss: 0.589485
[Epoch 86; Iter    30/   36] train: loss: 0.3204975
[Epoch 86] ogbg-molsider: 0.634194 val loss: 0.548619
[Epoch 86] ogbg-molsider: 0.567660 test loss: 0.611974
[Epoch 87; Iter    24/   36] train: loss: 0.3093330
[Epoch 87] ogbg-molsider: 0.648395 val loss: 0.542618
[Epoch 87] ogbg-molsider: 0.571279 test loss: 0.619643
[Epoch 88; Iter    18/   36] train: loss: 0.3206101
[Epoch 88] ogbg-molsider: 0.641162 val loss: 0.547810
[Epoch 88] ogbg-molsider: 0.570241 test loss: 0.624534
[Epoch 89; Iter    12/   36] train: loss: 0.2902550
[Epoch 89] ogbg-molsider: 0.642334 val loss: 0.554042
[Epoch 89] ogbg-molsider: 0.570613 test loss: 0.645946
[Epoch 90; Iter     6/   36] train: loss: 0.2782307
[Epoch 90; Iter    36/   36] train: loss: 0.3213691
[Epoch 90] ogbg-molsider: 0.639504 val loss: 0.560206
[Epoch 90] ogbg-molsider: 0.563226 test loss: 0.662288
[Epoch 91; Iter    30/   36] train: loss: 0.3009480
[Epoch 91] ogbg-molsider: 0.627266 val loss: 0.560621
[Epoch 91] ogbg-molsider: 0.560769 test loss: 0.633132
[Epoch 92; Iter    24/   36] train: loss: 0.2988607
[Epoch 92] ogbg-molsider: 0.630289 val loss: 0.586217
[Epoch 92] ogbg-molsider: 0.570048 test loss: 0.652664
[Epoch 93; Iter    18/   36] train: loss: 0.2615004
[Epoch 93] ogbg-molsider: 0.652919 val loss: 0.540798
[Epoch 93] ogbg-molsider: 0.568594 test loss: 0.622389
[Epoch 94; Iter    12/   36] train: loss: 0.2773220
[Epoch 94] ogbg-molsider: 0.626372 val loss: 0.562994
[Epoch 94] ogbg-molsider: 0.557405 test loss: 0.646895
[Epoch 95; Iter     6/   36] train: loss: 0.3173559
[Epoch 95; Iter    36/   36] train: loss: 0.2908935
[Epoch 95] ogbg-molsider: 0.640321 val loss: 0.561049
[Epoch 95] ogbg-molsider: 0.566730 test loss: 0.659448
[Epoch 96; Iter    30/   36] train: loss: 0.3069867
[Epoch 96] ogbg-molsider: 0.641398 val loss: 0.553235
[Epoch 96] ogbg-molsider: 0.568423 test loss: 0.635583
[Epoch 97; Iter    24/   36] train: loss: 0.3067123
[Epoch 97] ogbg-molsider: 0.622220 val loss: 0.575235
[Epoch 97] ogbg-molsider: 0.569252 test loss: 0.664989
[Epoch 98; Iter    18/   36] train: loss: 0.3101138
[Epoch 98] ogbg-molsider: 0.635024 val loss: 0.586126
[Epoch 98] ogbg-molsider: 0.586013 test loss: 0.672860
[Epoch 99; Iter    12/   36] train: loss: 0.3654338
[Epoch 99] ogbg-molsider: 0.630683 val loss: 0.568612
[Epoch 99] ogbg-molsider: 0.564503 test loss: 0.667133
[Epoch 100; Iter     6/   36] train: loss: 0.2426542
[Epoch 100; Iter    36/   36] train: loss: 0.3154791
[Epoch 100] ogbg-molsider: 0.628598 val loss: 0.607897
[Epoch 100] ogbg-molsider: 0.560384 test loss: 0.695385
[Epoch 101; Iter    30/   36] train: loss: 0.3041268
[Epoch 101] ogbg-molsider: 0.640024 val loss: 0.560050
[Epoch 101] ogbg-molsider: 0.595806 test loss: 0.629062
[Epoch 102; Iter    24/   36] train: loss: 0.3305951
[Epoch 102] ogbg-molsider: 0.633204 val loss: 0.589272
[Epoch 102] ogbg-molsider: 0.569290 test loss: 0.670569
[Epoch 103; Iter    18/   36] train: loss: 0.2798788
[Epoch 103] ogbg-molsider: 0.633548 val loss: 0.594482
[Epoch 103] ogbg-molsider: 0.559828 test loss: 0.676972
[Epoch 104; Iter    12/   36] train: loss: 0.2646051
[Epoch 104] ogbg-molsider: 0.634800 val loss: 0.591041
[Epoch 104] ogbg-molsider: 0.549476 test loss: 0.715946
[Epoch 105; Iter     6/   36] train: loss: 0.2607242
[Epoch 105; Iter    36/   36] train: loss: 0.2884213
[Epoch 105] ogbg-molsider: 0.626070 val loss: 0.578068
[Epoch 105] ogbg-molsider: 0.559375 test loss: 0.697170
[Epoch 106; Iter    30/   36] train: loss: 0.3012943
[Epoch 106] ogbg-molsider: 0.650276 val loss: 0.567903
[Epoch 106] ogbg-molsider: 0.572512 test loss: 0.676162
[Epoch 107; Iter    24/   36] train: loss: 0.2920816
[Epoch 107] ogbg-molsider: 0.637001 val loss: 0.574328
[Epoch 107] ogbg-molsider: 0.576449 test loss: 0.673761
[Epoch 108; Iter    18/   36] train: loss: 0.2546541
[Epoch 108] ogbg-molsider: 0.648634 val loss: 0.564573
[Epoch 108] ogbg-molsider: 0.561124 test loss: 0.677000
[Epoch 109; Iter    12/   36] train: loss: 0.2556068
[Epoch 109] ogbg-molsider: 0.641191 val loss: 0.580372
[Epoch 109] ogbg-molsider: 0.563824 test loss: 0.687129
[Epoch 110; Iter     6/   36] train: loss: 0.2886778
[Epoch 110; Iter    36/   36] train: loss: 0.2470166
[Epoch 110] ogbg-molsider: 0.653875 val loss: 0.610693
[Epoch 110] ogbg-molsider: 0.558241 test loss: 0.748964
[Epoch 111; Iter    30/   36] train: loss: 0.2417271
[Epoch 111] ogbg-molsider: 0.646714 val loss: 0.576463
[Epoch 111] ogbg-molsider: 0.574306 test loss: 0.696073
[Epoch 112; Iter    24/   36] train: loss: 0.2445787
[Epoch 112] ogbg-molsider: 0.647059 val loss: 0.581186
[Epoch 112] ogbg-molsider: 0.569103 test loss: 0.692284
[Epoch 113; Iter    18/   36] train: loss: 0.2159329
[Epoch 113] ogbg-molsider: 0.638541 val loss: 0.605049
[Epoch 113] ogbg-molsider: 0.567227 test loss: 0.714959
[Epoch 114; Iter    12/   36] train: loss: 0.2370776
[Epoch 114] ogbg-molsider: 0.652826 val loss: 0.582439
[Epoch 114] ogbg-molsider: 0.562035 test loss: 0.707523
[Epoch 115; Iter     6/   36] train: loss: 0.2327582
[Epoch 115; Iter    36/   36] train: loss: 0.2722702
[Epoch 115] ogbg-molsider: 0.644148 val loss: 0.587736
[Epoch 115] ogbg-molsider: 0.565007 test loss: 0.702202
[Epoch 116; Iter    30/   36] train: loss: 0.2420993
[Epoch 116] ogbg-molsider: 0.645286 val loss: 0.585659
[Epoch 116] ogbg-molsider: 0.564553 test loss: 0.711285
[Epoch 117; Iter    24/   36] train: loss: 0.2243239
[Epoch 117] ogbg-molsider: 0.641689 val loss: 0.588376
[Epoch 117] ogbg-molsider: 0.568459 test loss: 0.710254
[Epoch 118; Iter    18/   36] train: loss: 0.2561297
[Epoch 118] ogbg-molsider: 0.648800 val loss: 0.591511
[Epoch 118] ogbg-molsider: 0.563702 test loss: 0.717471
[Epoch 119; Iter    12/   36] train: loss: 0.2041032
[Epoch 119] ogbg-molsider: 0.638754 val loss: 0.597381
[Epoch 119] ogbg-molsider: 0.565134 test loss: 0.711465
[Epoch 120; Iter     6/   36] train: loss: 0.2773235
[Epoch 120; Iter    36/   36] train: loss: 0.2201030
[Epoch 120] ogbg-molsider: 0.642759 val loss: 0.618869
[Epoch 120] ogbg-molsider: 0.569473 test loss: 0.723659
[Epoch 121; Iter    30/   36] train: loss: 0.2871508
[Epoch 121] ogbg-molsider: 0.635979 val loss: 0.607760
[Epoch 121] ogbg-molsider: 0.577080 test loss: 0.713666
[Epoch 122; Iter    24/   36] train: loss: 0.2416883
[Epoch 122] ogbg-molsider: 0.632375 val loss: 0.606082
[Epoch 122] ogbg-molsider: 0.564372 test loss: 0.721877
[Epoch 123; Iter    18/   36] train: loss: 0.2509891
[Epoch 123] ogbg-molsider: 0.638925 val loss: 0.604238
[Epoch 123] ogbg-molsider: 0.572487 test loss: 0.727759
[Epoch 124; Iter    12/   36] train: loss: 0.2424870
[Epoch 124] ogbg-molsider: 0.631392 val loss: 0.630500
[Epoch 124] ogbg-molsider: 0.562513 test loss: 0.744495
[Epoch 125; Iter     6/   36] train: loss: 0.2156603
[Epoch 125; Iter    36/   36] train: loss: 0.2612205
[Epoch 125] ogbg-molsider: 0.644133 val loss: 0.612765
[Epoch 125] ogbg-molsider: 0.556210 test loss: 0.743351
[Epoch 126; Iter    30/   36] train: loss: 0.2236236
[Epoch 126] ogbg-molsider: 0.644646 val loss: 0.625188
[Epoch 126] ogbg-molsider: 0.570669 test loss: 0.733035
[Epoch 127; Iter    24/   36] train: loss: 0.2347736
[Epoch 127] ogbg-molsider: 0.644971 val loss: 0.607013
[Epoch 127] ogbg-molsider: 0.563429 test loss: 0.732232
[Epoch 128; Iter    18/   36] train: loss: 0.2386502
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.621173 val loss: 0.682424
[Epoch 128] ogbg-molsider: 0.586332 test loss: 0.736359
[Epoch 129; Iter    12/   36] train: loss: 0.1997805
[Epoch 129] ogbg-molsider: 0.622595 val loss: 0.665385
[Epoch 129] ogbg-molsider: 0.594644 test loss: 0.725203
[Epoch 130; Iter     6/   36] train: loss: 0.2121860
[Epoch 130; Iter    36/   36] train: loss: 0.2231615
[Epoch 130] ogbg-molsider: 0.625944 val loss: 0.676466
[Epoch 130] ogbg-molsider: 0.597901 test loss: 0.716921
[Epoch 131; Iter    30/   36] train: loss: 0.2376297
[Epoch 131] ogbg-molsider: 0.618149 val loss: 0.684511
[Epoch 131] ogbg-molsider: 0.600837 test loss: 0.716586
[Epoch 132; Iter    24/   36] train: loss: 0.1956067
[Epoch 132] ogbg-molsider: 0.602921 val loss: 0.694624
[Epoch 132] ogbg-molsider: 0.600443 test loss: 0.700770
[Epoch 133; Iter    18/   36] train: loss: 0.2152184
[Epoch 133] ogbg-molsider: 0.611488 val loss: 0.685043
[Epoch 133] ogbg-molsider: 0.589415 test loss: 0.709071
[Epoch 134; Iter    12/   36] train: loss: 0.1962775
[Epoch 134] ogbg-molsider: 0.616802 val loss: 0.695299
[Epoch 134] ogbg-molsider: 0.599286 test loss: 0.725232
[Epoch 135; Iter     6/   36] train: loss: 0.2234634
[Epoch 135; Iter    36/   36] train: loss: 0.2803303
[Epoch 135] ogbg-molsider: 0.622447 val loss: 0.679224
[Epoch 135] ogbg-molsider: 0.591468 test loss: 0.728606
[Epoch 136; Iter    30/   36] train: loss: 0.2116502
[Epoch 136] ogbg-molsider: 0.616696 val loss: 0.708129
[Epoch 136] ogbg-molsider: 0.594949 test loss: 0.743870
[Epoch 137; Iter    24/   36] train: loss: 0.1873827
[Epoch 137] ogbg-molsider: 0.613127 val loss: 0.677077
[Epoch 137] ogbg-molsider: 0.575833 test loss: 0.733488
[Epoch 138; Iter    18/   36] train: loss: 0.2248068
[Epoch 138] ogbg-molsider: 0.617316 val loss: 0.693814
[Epoch 138] ogbg-molsider: 0.587685 test loss: 0.726765
[Epoch 139; Iter    12/   36] train: loss: 0.1726887
[Epoch 139] ogbg-molsider: 0.610746 val loss: 0.692478
[Epoch 139] ogbg-molsider: 0.593205 test loss: 0.723735
[Epoch 140; Iter     6/   36] train: loss: 0.1746019
[Epoch 140; Iter    36/   36] train: loss: 0.1923661
[Epoch 140] ogbg-molsider: 0.611798 val loss: 0.689022
[Epoch 140] ogbg-molsider: 0.593736 test loss: 0.713369
Early stopping criterion based on -ogbg-molsider- that should be max reached after 140 epochs. Best model checkpoint was in epoch 80.
Statistics on  val_best_checkpoint
mean_pred: 0.46845299005508423
std_pred: 2.8729910850524902
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6871251319359212
rocauc: 0.6498306898873053
ogbg-molsider: 0.6498306898873053
OGBNanLabelBCEWithLogitsLoss: 0.5233879089355469
Statistics on  test
mean_pred: 0.5961921811103821
std_pred: 2.8924834728240967
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6216035150197052
rocauc: 0.5839138472233248
ogbg-molsider: 0.5839138472233248
OGBNanLabelBCEWithLogitsLoss: 0.5905403971672059
Statistics on  train
mean_pred: 0.44458481669425964
std_pred: 3.019023895263672
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8455062162466475
rocauc: 0.8876089590116791
ogbg-molsider: 0.8876089590116791
OGBNanLabelBCEWithLogitsLoss: 0.3283966581026713
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.645014 val loss: 0.658496
[Epoch 128] ogbg-molsider: 0.565496 test loss: 0.798272
[Epoch 129; Iter    12/   36] train: loss: 0.2594999
[Epoch 129] ogbg-molsider: 0.637521 val loss: 0.616481
[Epoch 129] ogbg-molsider: 0.563212 test loss: 0.733608
[Epoch 130; Iter     6/   36] train: loss: 0.2214635
[Epoch 130; Iter    36/   36] train: loss: 0.2083077
[Epoch 130] ogbg-molsider: 0.629423 val loss: 0.639446
[Epoch 130] ogbg-molsider: 0.566817 test loss: 0.748970
[Epoch 131; Iter    30/   36] train: loss: 0.2566182
[Epoch 131] ogbg-molsider: 0.646277 val loss: 0.609885
[Epoch 131] ogbg-molsider: 0.561059 test loss: 0.719742
[Epoch 132; Iter    24/   36] train: loss: 0.1983431
[Epoch 132] ogbg-molsider: 0.642962 val loss: 0.612466
[Epoch 132] ogbg-molsider: 0.559053 test loss: 0.738301
[Epoch 133; Iter    18/   36] train: loss: 0.1953114
[Epoch 133] ogbg-molsider: 0.641717 val loss: 0.629474
[Epoch 133] ogbg-molsider: 0.571035 test loss: 0.724377
[Epoch 134; Iter    12/   36] train: loss: 0.1982689
[Epoch 134] ogbg-molsider: 0.637302 val loss: 0.633106
[Epoch 134] ogbg-molsider: 0.566266 test loss: 0.753174
[Epoch 135; Iter     6/   36] train: loss: 0.2214185
[Epoch 135; Iter    36/   36] train: loss: 0.2343571
[Epoch 135] ogbg-molsider: 0.642083 val loss: 0.624733
[Epoch 135] ogbg-molsider: 0.569957 test loss: 0.735136
[Epoch 136; Iter    30/   36] train: loss: 0.1675946
[Epoch 136] ogbg-molsider: 0.646284 val loss: 0.631893
[Epoch 136] ogbg-molsider: 0.567661 test loss: 0.759886
[Epoch 137; Iter    24/   36] train: loss: 0.2199734
[Epoch 137] ogbg-molsider: 0.643195 val loss: 0.618541
[Epoch 137] ogbg-molsider: 0.569317 test loss: 0.748071
[Epoch 138; Iter    18/   36] train: loss: 0.2027305
[Epoch 138] ogbg-molsider: 0.642819 val loss: 0.636572
[Epoch 138] ogbg-molsider: 0.573911 test loss: 0.756583
[Epoch 139; Iter    12/   36] train: loss: 0.1778990
[Epoch 139] ogbg-molsider: 0.637310 val loss: 0.630835
[Epoch 139] ogbg-molsider: 0.570244 test loss: 0.750543
[Epoch 140; Iter     6/   36] train: loss: 0.1995540
[Epoch 140; Iter    36/   36] train: loss: 0.2093897
[Epoch 140] ogbg-molsider: 0.639162 val loss: 0.640389
[Epoch 140] ogbg-molsider: 0.570242 test loss: 0.769096
[Epoch 141; Iter    30/   36] train: loss: 0.1960330
[Epoch 141] ogbg-molsider: 0.638203 val loss: 0.644893
[Epoch 141] ogbg-molsider: 0.566683 test loss: 0.774549
Early stopping criterion based on -ogbg-molsider- that should be max reached after 141 epochs. Best model checkpoint was in epoch 81.
Statistics on  val_best_checkpoint
mean_pred: 0.6338549256324768
std_pred: 2.885928153991699
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6934475963825968
rocauc: 0.6554345064149097
ogbg-molsider: 0.6554345064149097
OGBNanLabelBCEWithLogitsLoss: 0.512195634841919
Statistics on  test
mean_pred: 0.7542380690574646
std_pred: 2.9789600372314453
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6371004069491284
rocauc: 0.5871095587225617
ogbg-molsider: 0.5871095587225617
OGBNanLabelBCEWithLogitsLoss: 0.5873080015182495
Statistics on  train
mean_pred: 0.4507080018520355
std_pred: 2.996042490005493
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8346892137119134
rocauc: 0.8837845741570015
ogbg-molsider: 0.8837845741570015
OGBNanLabelBCEWithLogitsLoss: 0.3307681083679199
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.628159 val loss: 0.657707
[Epoch 128] ogbg-molsider: 0.585616 test loss: 0.731090
[Epoch 129; Iter    12/   36] train: loss: 0.2114831
[Epoch 129] ogbg-molsider: 0.637385 val loss: 0.661050
[Epoch 129] ogbg-molsider: 0.579174 test loss: 0.761832
[Epoch 130; Iter     6/   36] train: loss: 0.2120692
[Epoch 130; Iter    36/   36] train: loss: 0.2039565
[Epoch 130] ogbg-molsider: 0.631041 val loss: 0.682405
[Epoch 130] ogbg-molsider: 0.584878 test loss: 0.758355
[Epoch 131; Iter    30/   36] train: loss: 0.2320708
[Epoch 131] ogbg-molsider: 0.634225 val loss: 0.670520
[Epoch 131] ogbg-molsider: 0.584511 test loss: 0.741630
[Epoch 132; Iter    24/   36] train: loss: 0.2046262
[Epoch 132] ogbg-molsider: 0.632469 val loss: 0.668560
[Epoch 132] ogbg-molsider: 0.587302 test loss: 0.774635
[Epoch 133; Iter    18/   36] train: loss: 0.2282078
[Epoch 133] ogbg-molsider: 0.627476 val loss: 0.677609
[Epoch 133] ogbg-molsider: 0.588437 test loss: 0.751555
[Epoch 134; Iter    12/   36] train: loss: 0.1866804
[Epoch 134] ogbg-molsider: 0.630810 val loss: 0.672866
[Epoch 134] ogbg-molsider: 0.583581 test loss: 0.757207
[Epoch 135; Iter     6/   36] train: loss: 0.1505270
[Epoch 135; Iter    36/   36] train: loss: 0.1729931
[Epoch 135] ogbg-molsider: 0.619249 val loss: 0.691035
[Epoch 135] ogbg-molsider: 0.578481 test loss: 0.776280
[Epoch 136; Iter    30/   36] train: loss: 0.1879977
[Epoch 136] ogbg-molsider: 0.622289 val loss: 0.683980
[Epoch 136] ogbg-molsider: 0.587040 test loss: 0.775623
[Epoch 137; Iter    24/   36] train: loss: 0.1956596
[Epoch 137] ogbg-molsider: 0.625546 val loss: 0.683226
[Epoch 137] ogbg-molsider: 0.587062 test loss: 0.763102
[Epoch 138; Iter    18/   36] train: loss: 0.1988334
[Epoch 138] ogbg-molsider: 0.627250 val loss: 0.688903
[Epoch 138] ogbg-molsider: 0.579929 test loss: 0.786310
[Epoch 139; Iter    12/   36] train: loss: 0.1587398
[Epoch 139] ogbg-molsider: 0.632141 val loss: 0.677657
[Epoch 139] ogbg-molsider: 0.584792 test loss: 0.777626
[Epoch 140; Iter     6/   36] train: loss: 0.1481353
[Epoch 140; Iter    36/   36] train: loss: 0.1923573
[Epoch 140] ogbg-molsider: 0.628286 val loss: 0.696083
[Epoch 140] ogbg-molsider: 0.587088 test loss: 0.783235
[Epoch 141; Iter    30/   36] train: loss: 0.1777448
[Epoch 141] ogbg-molsider: 0.621721 val loss: 0.675803
[Epoch 141] ogbg-molsider: 0.585276 test loss: 0.768007
[Epoch 142; Iter    24/   36] train: loss: 0.1788210
[Epoch 142] ogbg-molsider: 0.616393 val loss: 0.701749
[Epoch 142] ogbg-molsider: 0.585846 test loss: 0.775643
[Epoch 143; Iter    18/   36] train: loss: 0.1809415
[Epoch 143] ogbg-molsider: 0.621961 val loss: 0.698171
[Epoch 143] ogbg-molsider: 0.582515 test loss: 0.787546
[Epoch 144; Iter    12/   36] train: loss: 0.1497132
[Epoch 144] ogbg-molsider: 0.626492 val loss: 0.704433
[Epoch 144] ogbg-molsider: 0.582443 test loss: 0.787482
[Epoch 145; Iter     6/   36] train: loss: 0.1613072
[Epoch 145; Iter    36/   36] train: loss: 0.2312367
[Epoch 145] ogbg-molsider: 0.627643 val loss: 0.695537
[Epoch 145] ogbg-molsider: 0.585449 test loss: 0.794444
[Epoch 146; Iter    30/   36] train: loss: 0.1894180
[Epoch 146] ogbg-molsider: 0.628932 val loss: 0.704403
[Epoch 146] ogbg-molsider: 0.581260 test loss: 0.798710
[Epoch 147; Iter    24/   36] train: loss: 0.1945174
[Epoch 147] ogbg-molsider: 0.627860 val loss: 0.700451
[Epoch 147] ogbg-molsider: 0.581676 test loss: 0.791637
[Epoch 148; Iter    18/   36] train: loss: 0.2220912
[Epoch 148] ogbg-molsider: 0.626966 val loss: 0.704749
[Epoch 148] ogbg-molsider: 0.586201 test loss: 0.804591
[Epoch 149; Iter    12/   36] train: loss: 0.1765549
[Epoch 149] ogbg-molsider: 0.625367 val loss: 0.702076
[Epoch 149] ogbg-molsider: 0.587923 test loss: 0.809166
[Epoch 150; Iter     6/   36] train: loss: 0.1741708
[Epoch 150; Iter    36/   36] train: loss: 0.1888050
[Epoch 150] ogbg-molsider: 0.630835 val loss: 0.704602
[Epoch 150] ogbg-molsider: 0.593211 test loss: 0.809555
[Epoch 151; Iter    30/   36] train: loss: 0.1752255
[Epoch 151] ogbg-molsider: 0.620209 val loss: 0.712543
[Epoch 151] ogbg-molsider: 0.586000 test loss: 0.804634
[Epoch 152; Iter    24/   36] train: loss: 0.1550641
[Epoch 152] ogbg-molsider: 0.621527 val loss: 0.715046
[Epoch 152] ogbg-molsider: 0.582988 test loss: 0.804127
Early stopping criterion based on -ogbg-molsider- that should be max reached after 152 epochs. Best model checkpoint was in epoch 92.
Statistics on  val_best_checkpoint
mean_pred: 0.35486486554145813
std_pred: 2.9411962032318115
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6813053039399358
rocauc: 0.6440773138279184
ogbg-molsider: 0.6440773138279184
OGBNanLabelBCEWithLogitsLoss: 0.558500325679779
Statistics on  test
mean_pred: 0.41269993782043457
std_pred: 3.00730037689209
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6301676936254976
rocauc: 0.5780927131545543
ogbg-molsider: 0.5780927131545543
OGBNanLabelBCEWithLogitsLoss: 0.637140691280365
Statistics on  train
mean_pred: 0.37104710936546326
std_pred: 3.241966724395752
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8822368720980879
rocauc: 0.926471350321981
ogbg-molsider: 0.926471350321981
OGBNanLabelBCEWithLogitsLoss: 0.28546380665567184
Starting process for seed 4: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.0.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.0.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_static_noise_experiments/3DInfomax/sider/noise=0.0.yml --seed 6 --device cuda:0
All runs completed.
