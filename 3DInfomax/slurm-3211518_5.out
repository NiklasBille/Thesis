>>> Starting run for dataset: sider
Running SCAFF configs_split_experiments/GraphCL/sider/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/sider/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/sider/scaff/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.7.yml --seed 6 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/sider/scaff/train_prop=0.8.yml --seed 6 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.7/PNA_ogbg-molsider_GraphCL_sider_scaff=0.7_6_26-05_10-04-02
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.7
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6934270
[Epoch 1] ogbg-molsider: 0.484971 val loss: 0.693314
[Epoch 1] ogbg-molsider: 0.488348 test loss: 0.692875
[Epoch 2; Iter    28/   32] train: loss: 0.6929286
[Epoch 2] ogbg-molsider: 0.492062 val loss: 0.693508
[Epoch 2] ogbg-molsider: 0.496268 test loss: 0.692819
[Epoch 3; Iter    26/   32] train: loss: 0.6931803
[Epoch 3] ogbg-molsider: 0.488578 val loss: 0.693534
[Epoch 3] ogbg-molsider: 0.499429 test loss: 0.692711
[Epoch 4; Iter    24/   32] train: loss: 0.6931025
[Epoch 4] ogbg-molsider: 0.490024 val loss: 0.693545
[Epoch 4] ogbg-molsider: 0.500308 test loss: 0.692815
[Epoch 5; Iter    22/   32] train: loss: 0.6932401
[Epoch 5] ogbg-molsider: 0.490945 val loss: 0.693484
[Epoch 5] ogbg-molsider: 0.502018 test loss: 0.692689
[Epoch 6; Iter    20/   32] train: loss: 0.6927375
[Epoch 6] ogbg-molsider: 0.487950 val loss: 0.693572
[Epoch 6] ogbg-molsider: 0.499581 test loss: 0.692745
[Epoch 7; Iter    18/   32] train: loss: 0.6927890
[Epoch 7] ogbg-molsider: 0.490240 val loss: 0.693361
[Epoch 7] ogbg-molsider: 0.502297 test loss: 0.692665
[Epoch 8; Iter    16/   32] train: loss: 0.6933709
[Epoch 8] ogbg-molsider: 0.498246 val loss: 0.693331
[Epoch 8] ogbg-molsider: 0.501275 test loss: 0.692724
[Epoch 9; Iter    14/   32] train: loss: 0.6928328
[Epoch 9] ogbg-molsider: 0.488675 val loss: 0.693204
[Epoch 9] ogbg-molsider: 0.501298 test loss: 0.692484
[Epoch 10; Iter    12/   32] train: loss: 0.6925475
[Epoch 10] ogbg-molsider: 0.491668 val loss: 0.692983
[Epoch 10] ogbg-molsider: 0.500911 test loss: 0.692285
[Epoch 11; Iter    10/   32] train: loss: 0.6928181
[Epoch 11] ogbg-molsider: 0.489382 val loss: 0.693070
[Epoch 11] ogbg-molsider: 0.500630 test loss: 0.692360
[Epoch 12; Iter     8/   32] train: loss: 0.6922274
[Epoch 12] ogbg-molsider: 0.490345 val loss: 0.693004
[Epoch 12] ogbg-molsider: 0.500152 test loss: 0.692336
[Epoch 13; Iter     6/   32] train: loss: 0.6924682
[Epoch 13] ogbg-molsider: 0.485605 val loss: 0.692715
[Epoch 13] ogbg-molsider: 0.499320 test loss: 0.691885
[Epoch 14; Iter     4/   32] train: loss: 0.6917614
[Epoch 14] ogbg-molsider: 0.490004 val loss: 0.692790
[Epoch 14] ogbg-molsider: 0.501303 test loss: 0.692049
[Epoch 15; Iter     2/   32] train: loss: 0.6919835
[Epoch 15; Iter    32/   32] train: loss: 0.6920738
[Epoch 15] ogbg-molsider: 0.488353 val loss: 0.692453
[Epoch 15] ogbg-molsider: 0.497839 test loss: 0.691636
[Epoch 16; Iter    30/   32] train: loss: 0.6920750
[Epoch 16] ogbg-molsider: 0.490744 val loss: 0.692336
[Epoch 16] ogbg-molsider: 0.497961 test loss: 0.691745
[Epoch 17; Iter    28/   32] train: loss: 0.6919937
[Epoch 17] ogbg-molsider: 0.489722 val loss: 0.692147
[Epoch 17] ogbg-molsider: 0.499063 test loss: 0.691530
[Epoch 18; Iter    26/   32] train: loss: 0.6923622
[Epoch 18] ogbg-molsider: 0.489787 val loss: 0.692075
[Epoch 18] ogbg-molsider: 0.501099 test loss: 0.691286
[Epoch 19; Iter    24/   32] train: loss: 0.6914558
[Epoch 19] ogbg-molsider: 0.490984 val loss: 0.691878
[Epoch 19] ogbg-molsider: 0.498680 test loss: 0.691250
[Epoch 20; Iter    22/   32] train: loss: 0.6919364
[Epoch 20] ogbg-molsider: 0.491252 val loss: 0.691682
[Epoch 20] ogbg-molsider: 0.500756 test loss: 0.691034
[Epoch 21; Iter    20/   32] train: loss: 0.6918753
[Epoch 21] ogbg-molsider: 0.497519 val loss: 0.691529
[Epoch 21] ogbg-molsider: 0.499380 test loss: 0.690967
[Epoch 22; Iter    18/   32] train: loss: 0.6911782
[Epoch 22] ogbg-molsider: 0.533364 val loss: 0.685515
[Epoch 22] ogbg-molsider: 0.521615 test loss: 0.684788
[Epoch 23; Iter    16/   32] train: loss: 0.6850822
[Epoch 23] ogbg-molsider: 0.547093 val loss: 0.667059
[Epoch 23] ogbg-molsider: 0.552734 test loss: 0.665698
[Epoch 24; Iter    14/   32] train: loss: 0.6711537
[Epoch 24] ogbg-molsider: 0.538978 val loss: 0.647368
[Epoch 24] ogbg-molsider: 0.560187 test loss: 0.648572
[Epoch 25; Iter    12/   32] train: loss: 0.6613656
[Epoch 25] ogbg-molsider: 0.544149 val loss: 0.617243
[Epoch 25] ogbg-molsider: 0.558614 test loss: 0.621202
[Epoch 26; Iter    10/   32] train: loss: 0.6389180
[Epoch 26] ogbg-molsider: 0.530507 val loss: 0.582084
[Epoch 26] ogbg-molsider: 0.562355 test loss: 0.583386
[Epoch 27; Iter     8/   32] train: loss: 0.6132172
[Epoch 27] ogbg-molsider: 0.537133 val loss: 0.571723
[Epoch 27] ogbg-molsider: 0.543412 test loss: 0.581875
[Epoch 28; Iter     6/   32] train: loss: 0.5944566
[Epoch 28] ogbg-molsider: 0.526487 val loss: 0.535713
[Epoch 28] ogbg-molsider: 0.566666 test loss: 0.545464
[Epoch 29; Iter     4/   32] train: loss: 0.5706063
[Epoch 29] ogbg-molsider: 0.544591 val loss: 0.537640
[Epoch 29] ogbg-molsider: 0.579798 test loss: 0.541901
[Epoch 30; Iter     2/   32] train: loss: 0.5642079
[Epoch 30; Iter    32/   32] train: loss: 0.4644783
[Epoch 30] ogbg-molsider: 0.536141 val loss: 0.528728
[Epoch 30] ogbg-molsider: 0.584323 test loss: 0.529211
[Epoch 31; Iter    30/   32] train: loss: 0.5233312
[Epoch 31] ogbg-molsider: 0.539593 val loss: 0.500158
[Epoch 31] ogbg-molsider: 0.571819 test loss: 0.512030
[Epoch 32; Iter    28/   32] train: loss: 0.5272282
[Epoch 32] ogbg-molsider: 0.538672 val loss: 0.494417
[Epoch 32] ogbg-molsider: 0.585769 test loss: 0.503073
[Epoch 33; Iter    26/   32] train: loss: 0.4840960
[Epoch 33] ogbg-molsider: 0.541683 val loss: 0.496246
[Epoch 33] ogbg-molsider: 0.587167 test loss: 0.505869
[Epoch 34; Iter    24/   32] train: loss: 0.4917877
[Epoch 34] ogbg-molsider: 0.552835 val loss: 0.487107
[Epoch 34] ogbg-molsider: 0.594040 test loss: 0.501782
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.8/PNA_ogbg-molsider_GraphCL_sider_scaff=0.8_5_26-05_10-04-02
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.8
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6927666
[Epoch 1] ogbg-molsider: 0.470907 val loss: 0.693026
[Epoch 1] ogbg-molsider: 0.518256 test loss: 0.692729
[Epoch 2; Iter    24/   36] train: loss: 0.6928043
[Epoch 2] ogbg-molsider: 0.467259 val loss: 0.692918
[Epoch 2] ogbg-molsider: 0.509426 test loss: 0.692458
[Epoch 3; Iter    18/   36] train: loss: 0.6928941
[Epoch 3] ogbg-molsider: 0.466317 val loss: 0.692948
[Epoch 3] ogbg-molsider: 0.503969 test loss: 0.692369
[Epoch 4; Iter    12/   36] train: loss: 0.6940182
[Epoch 4] ogbg-molsider: 0.472369 val loss: 0.692835
[Epoch 4] ogbg-molsider: 0.502085 test loss: 0.692439
[Epoch 5; Iter     6/   36] train: loss: 0.6927276
[Epoch 5; Iter    36/   36] train: loss: 0.6929785
[Epoch 5] ogbg-molsider: 0.465347 val loss: 0.692665
[Epoch 5] ogbg-molsider: 0.504436 test loss: 0.692044
[Epoch 6; Iter    30/   36] train: loss: 0.6928712
[Epoch 6] ogbg-molsider: 0.464059 val loss: 0.692736
[Epoch 6] ogbg-molsider: 0.502695 test loss: 0.692245
[Epoch 7; Iter    24/   36] train: loss: 0.6931918
[Epoch 7] ogbg-molsider: 0.464694 val loss: 0.692641
[Epoch 7] ogbg-molsider: 0.503673 test loss: 0.692137
[Epoch 8; Iter    18/   36] train: loss: 0.6928407
[Epoch 8] ogbg-molsider: 0.472480 val loss: 0.692578
[Epoch 8] ogbg-molsider: 0.507325 test loss: 0.691990
[Epoch 9; Iter    12/   36] train: loss: 0.6926391
[Epoch 9] ogbg-molsider: 0.467280 val loss: 0.692309
[Epoch 9] ogbg-molsider: 0.503701 test loss: 0.691826
[Epoch 10; Iter     6/   36] train: loss: 0.6930600
[Epoch 10; Iter    36/   36] train: loss: 0.6928003
[Epoch 10] ogbg-molsider: 0.466410 val loss: 0.692248
[Epoch 10] ogbg-molsider: 0.502825 test loss: 0.691801
[Epoch 11; Iter    30/   36] train: loss: 0.6928711
[Epoch 11] ogbg-molsider: 0.472666 val loss: 0.692319
[Epoch 11] ogbg-molsider: 0.502607 test loss: 0.691763
[Epoch 12; Iter    24/   36] train: loss: 0.6927555
[Epoch 12] ogbg-molsider: 0.467929 val loss: 0.691996
[Epoch 12] ogbg-molsider: 0.503249 test loss: 0.691521
[Epoch 13; Iter    18/   36] train: loss: 0.6920398
[Epoch 13] ogbg-molsider: 0.470612 val loss: 0.691888
[Epoch 13] ogbg-molsider: 0.502339 test loss: 0.691520
[Epoch 14; Iter    12/   36] train: loss: 0.6925406
[Epoch 14] ogbg-molsider: 0.468067 val loss: 0.691736
[Epoch 14] ogbg-molsider: 0.502421 test loss: 0.691432
[Epoch 15; Iter     6/   36] train: loss: 0.6923411
[Epoch 15; Iter    36/   36] train: loss: 0.6922120
[Epoch 15] ogbg-molsider: 0.472721 val loss: 0.691465
[Epoch 15] ogbg-molsider: 0.501804 test loss: 0.691101
[Epoch 16; Iter    30/   36] train: loss: 0.6922432
[Epoch 16] ogbg-molsider: 0.468274 val loss: 0.691176
[Epoch 16] ogbg-molsider: 0.501560 test loss: 0.690806
[Epoch 17; Iter    24/   36] train: loss: 0.6914683
[Epoch 17] ogbg-molsider: 0.466829 val loss: 0.691068
[Epoch 17] ogbg-molsider: 0.502384 test loss: 0.690731
[Epoch 18; Iter    18/   36] train: loss: 0.6916913
[Epoch 18] ogbg-molsider: 0.469502 val loss: 0.690821
[Epoch 18] ogbg-molsider: 0.504158 test loss: 0.690475
[Epoch 19; Iter    12/   36] train: loss: 0.6914520
[Epoch 19] ogbg-molsider: 0.467819 val loss: 0.690582
[Epoch 19] ogbg-molsider: 0.504164 test loss: 0.690178
[Epoch 20; Iter     6/   36] train: loss: 0.6904233
[Epoch 20; Iter    36/   36] train: loss: 0.6852675
[Epoch 20] ogbg-molsider: 0.556428 val loss: 0.681383
[Epoch 20] ogbg-molsider: 0.540792 test loss: 0.681370
[Epoch 21; Iter    30/   36] train: loss: 0.6782297
[Epoch 21] ogbg-molsider: 0.526708 val loss: 0.661628
[Epoch 21] ogbg-molsider: 0.567245 test loss: 0.660192
[Epoch 22; Iter    24/   36] train: loss: 0.6581158
[Epoch 22] ogbg-molsider: 0.526854 val loss: 0.630513
[Epoch 22] ogbg-molsider: 0.561822 test loss: 0.629877
[Epoch 23; Iter    18/   36] train: loss: 0.6328252
[Epoch 23] ogbg-molsider: 0.536460 val loss: 0.597797
[Epoch 23] ogbg-molsider: 0.581449 test loss: 0.599655
[Epoch 24; Iter    12/   36] train: loss: 0.6301340
[Epoch 24] ogbg-molsider: 0.541864 val loss: 0.558747
[Epoch 24] ogbg-molsider: 0.575883 test loss: 0.561569
[Epoch 25; Iter     6/   36] train: loss: 0.5865523
[Epoch 25; Iter    36/   36] train: loss: 0.5560640
[Epoch 25] ogbg-molsider: 0.528946 val loss: 0.555613
[Epoch 25] ogbg-molsider: 0.567873 test loss: 0.556904
[Epoch 26; Iter    30/   36] train: loss: 0.5714481
[Epoch 26] ogbg-molsider: 0.547959 val loss: 0.522184
[Epoch 26] ogbg-molsider: 0.586042 test loss: 0.527968
[Epoch 27; Iter    24/   36] train: loss: 0.5317004
[Epoch 27] ogbg-molsider: 0.556506 val loss: 0.551537
[Epoch 27] ogbg-molsider: 0.578084 test loss: 0.522468
[Epoch 28; Iter    18/   36] train: loss: 0.5323139
[Epoch 28] ogbg-molsider: 0.570439 val loss: 0.495576
[Epoch 28] ogbg-molsider: 0.600412 test loss: 0.504854
[Epoch 29; Iter    12/   36] train: loss: 0.4946818
[Epoch 29] ogbg-molsider: 0.579151 val loss: 0.479828
[Epoch 29] ogbg-molsider: 0.602051 test loss: 0.488982
[Epoch 30; Iter     6/   36] train: loss: 0.4960810
[Epoch 30; Iter    36/   36] train: loss: 0.4559035
[Epoch 30] ogbg-molsider: 0.551406 val loss: 0.489279
[Epoch 30] ogbg-molsider: 0.595604 test loss: 0.499380
[Epoch 31; Iter    30/   36] train: loss: 0.5034415
[Epoch 31] ogbg-molsider: 0.565892 val loss: 0.497058
[Epoch 31] ogbg-molsider: 0.581490 test loss: 0.507726
[Epoch 32; Iter    24/   36] train: loss: 0.4795765
[Epoch 32] ogbg-molsider: 0.556922 val loss: 0.485797
[Epoch 32] ogbg-molsider: 0.611765 test loss: 0.493485
[Epoch 33; Iter    18/   36] train: loss: 0.4925794
[Epoch 33] ogbg-molsider: 0.561708 val loss: 0.485353
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.8/PNA_ogbg-molsider_GraphCL_sider_scaff=0.8_4_26-05_10-04-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.8
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6934346
[Epoch 1] ogbg-molsider: 0.517062 val loss: 0.693269
[Epoch 1] ogbg-molsider: 0.487229 test loss: 0.693540
[Epoch 2; Iter    24/   36] train: loss: 0.6932150
[Epoch 2] ogbg-molsider: 0.496588 val loss: 0.693226
[Epoch 2] ogbg-molsider: 0.494216 test loss: 0.693742
[Epoch 3; Iter    18/   36] train: loss: 0.6929246
[Epoch 3] ogbg-molsider: 0.494929 val loss: 0.693120
[Epoch 3] ogbg-molsider: 0.499175 test loss: 0.693727
[Epoch 4; Iter    12/   36] train: loss: 0.6936165
[Epoch 4] ogbg-molsider: 0.491360 val loss: 0.693248
[Epoch 4] ogbg-molsider: 0.501199 test loss: 0.693993
[Epoch 5; Iter     6/   36] train: loss: 0.6930395
[Epoch 5; Iter    36/   36] train: loss: 0.6938283
[Epoch 5] ogbg-molsider: 0.493785 val loss: 0.693021
[Epoch 5] ogbg-molsider: 0.501167 test loss: 0.693669
[Epoch 6; Iter    30/   36] train: loss: 0.6937210
[Epoch 6] ogbg-molsider: 0.497214 val loss: 0.692776
[Epoch 6] ogbg-molsider: 0.502237 test loss: 0.693334
[Epoch 7; Iter    24/   36] train: loss: 0.6931588
[Epoch 7] ogbg-molsider: 0.492556 val loss: 0.692751
[Epoch 7] ogbg-molsider: 0.500532 test loss: 0.693388
[Epoch 8; Iter    18/   36] train: loss: 0.6926011
[Epoch 8] ogbg-molsider: 0.493893 val loss: 0.692808
[Epoch 8] ogbg-molsider: 0.498751 test loss: 0.693443
[Epoch 9; Iter    12/   36] train: loss: 0.6935994
[Epoch 9] ogbg-molsider: 0.494922 val loss: 0.692633
[Epoch 9] ogbg-molsider: 0.498371 test loss: 0.693348
[Epoch 10; Iter     6/   36] train: loss: 0.6931671
[Epoch 10; Iter    36/   36] train: loss: 0.6930419
[Epoch 10] ogbg-molsider: 0.492844 val loss: 0.692702
[Epoch 10] ogbg-molsider: 0.501177 test loss: 0.693414
[Epoch 11; Iter    30/   36] train: loss: 0.6924747
[Epoch 11] ogbg-molsider: 0.495599 val loss: 0.692310
[Epoch 11] ogbg-molsider: 0.500103 test loss: 0.692999
[Epoch 12; Iter    24/   36] train: loss: 0.6924058
[Epoch 12] ogbg-molsider: 0.494151 val loss: 0.692222
[Epoch 12] ogbg-molsider: 0.500191 test loss: 0.692899
[Epoch 13; Iter    18/   36] train: loss: 0.6919406
[Epoch 13] ogbg-molsider: 0.494657 val loss: 0.692163
[Epoch 13] ogbg-molsider: 0.497504 test loss: 0.692801
[Epoch 14; Iter    12/   36] train: loss: 0.6922175
[Epoch 14] ogbg-molsider: 0.494748 val loss: 0.691822
[Epoch 14] ogbg-molsider: 0.500956 test loss: 0.692424
[Epoch 15; Iter     6/   36] train: loss: 0.6922642
[Epoch 15; Iter    36/   36] train: loss: 0.6918569
[Epoch 15] ogbg-molsider: 0.494233 val loss: 0.691641
[Epoch 15] ogbg-molsider: 0.497670 test loss: 0.692290
[Epoch 16; Iter    30/   36] train: loss: 0.6916514
[Epoch 16] ogbg-molsider: 0.493241 val loss: 0.691514
[Epoch 16] ogbg-molsider: 0.498376 test loss: 0.692222
[Epoch 17; Iter    24/   36] train: loss: 0.6917756
[Epoch 17] ogbg-molsider: 0.496347 val loss: 0.691206
[Epoch 17] ogbg-molsider: 0.500581 test loss: 0.691859
[Epoch 18; Iter    18/   36] train: loss: 0.6913220
[Epoch 18] ogbg-molsider: 0.496930 val loss: 0.690971
[Epoch 18] ogbg-molsider: 0.498736 test loss: 0.691616
[Epoch 19; Iter    12/   36] train: loss: 0.6915759
[Epoch 19] ogbg-molsider: 0.495092 val loss: 0.690679
[Epoch 19] ogbg-molsider: 0.498381 test loss: 0.691408
[Epoch 20; Iter     6/   36] train: loss: 0.6911590
[Epoch 20; Iter    36/   36] train: loss: 0.6859812
[Epoch 20] ogbg-molsider: 0.523212 val loss: 0.679632
[Epoch 20] ogbg-molsider: 0.567177 test loss: 0.679582
[Epoch 21; Iter    30/   36] train: loss: 0.6719510
[Epoch 21] ogbg-molsider: 0.529366 val loss: 0.668470
[Epoch 21] ogbg-molsider: 0.560364 test loss: 0.667669
[Epoch 22; Iter    24/   36] train: loss: 0.6578662
[Epoch 22] ogbg-molsider: 0.527395 val loss: 0.616824
[Epoch 22] ogbg-molsider: 0.560309 test loss: 0.618345
[Epoch 23; Iter    18/   36] train: loss: 0.6296055
[Epoch 23] ogbg-molsider: 0.545563 val loss: 0.597652
[Epoch 23] ogbg-molsider: 0.569389 test loss: 0.609850
[Epoch 24; Iter    12/   36] train: loss: 0.6154273
[Epoch 24] ogbg-molsider: 0.540302 val loss: 0.573624
[Epoch 24] ogbg-molsider: 0.578993 test loss: 0.581587
[Epoch 25; Iter     6/   36] train: loss: 0.5911117
[Epoch 25; Iter    36/   36] train: loss: 0.5668362
[Epoch 25] ogbg-molsider: 0.532083 val loss: 0.542490
[Epoch 25] ogbg-molsider: 0.574028 test loss: 0.544566
[Epoch 26; Iter    30/   36] train: loss: 0.5350565
[Epoch 26] ogbg-molsider: 0.543953 val loss: 0.501791
[Epoch 26] ogbg-molsider: 0.590497 test loss: 0.510996
[Epoch 27; Iter    24/   36] train: loss: 0.5206698
[Epoch 27] ogbg-molsider: 0.548912 val loss: 0.499007
[Epoch 27] ogbg-molsider: 0.574523 test loss: 0.511997
[Epoch 28; Iter    18/   36] train: loss: 0.5145035
[Epoch 28] ogbg-molsider: 0.553118 val loss: 0.484822
[Epoch 28] ogbg-molsider: 0.590743 test loss: 0.496456
[Epoch 29; Iter    12/   36] train: loss: 0.5277827
[Epoch 29] ogbg-molsider: 0.566481 val loss: 0.482683
[Epoch 29] ogbg-molsider: 0.590578 test loss: 0.492127
[Epoch 30; Iter     6/   36] train: loss: 0.5182301
[Epoch 30; Iter    36/   36] train: loss: 0.5050404
[Epoch 30] ogbg-molsider: 0.564688 val loss: 0.484159
[Epoch 30] ogbg-molsider: 0.610031 test loss: 0.488898
[Epoch 31; Iter    30/   36] train: loss: 0.4642969
[Epoch 31] ogbg-molsider: 0.570587 val loss: 0.481559
[Epoch 31] ogbg-molsider: 0.601890 test loss: 0.496772
[Epoch 32; Iter    24/   36] train: loss: 0.5458062
[Epoch 32] ogbg-molsider: 0.554737 val loss: 0.493074
[Epoch 32] ogbg-molsider: 0.597334 test loss: 0.502236
[Epoch 33; Iter    18/   36] train: loss: 0.4886954
[Epoch 33] ogbg-molsider: 0.558825 val loss: 0.487001
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.6/PNA_ogbg-molsider_GraphCL_sider_scaff=0.6_6_26-05_10-04-02
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.6
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.503036 val loss: 0.693220
[Epoch 1] ogbg-molsider: 0.484689 test loss: 0.692995
[Epoch 2; Iter     3/   27] train: loss: 0.6932994
[Epoch 2] ogbg-molsider: 0.504490 val loss: 0.693392
[Epoch 2] ogbg-molsider: 0.488229 test loss: 0.692997
[Epoch 3; Iter     6/   27] train: loss: 0.6926919
[Epoch 3] ogbg-molsider: 0.503758 val loss: 0.693450
[Epoch 3] ogbg-molsider: 0.492419 test loss: 0.692969
[Epoch 4; Iter     9/   27] train: loss: 0.6929273
[Epoch 4] ogbg-molsider: 0.499983 val loss: 0.693511
[Epoch 4] ogbg-molsider: 0.494753 test loss: 0.692940
[Epoch 5; Iter    12/   27] train: loss: 0.6924059
[Epoch 5] ogbg-molsider: 0.500577 val loss: 0.693529
[Epoch 5] ogbg-molsider: 0.496618 test loss: 0.693026
[Epoch 6; Iter    15/   27] train: loss: 0.6932173
[Epoch 6] ogbg-molsider: 0.499696 val loss: 0.693450
[Epoch 6] ogbg-molsider: 0.496848 test loss: 0.692850
[Epoch 7; Iter    18/   27] train: loss: 0.6929374
[Epoch 7] ogbg-molsider: 0.500288 val loss: 0.693422
[Epoch 7] ogbg-molsider: 0.496582 test loss: 0.692806
[Epoch 8; Iter    21/   27] train: loss: 0.6928433
[Epoch 8] ogbg-molsider: 0.500610 val loss: 0.693398
[Epoch 8] ogbg-molsider: 0.496948 test loss: 0.692891
[Epoch 9; Iter    24/   27] train: loss: 0.6932651
[Epoch 9] ogbg-molsider: 0.500178 val loss: 0.693437
[Epoch 9] ogbg-molsider: 0.496563 test loss: 0.692935
[Epoch 10; Iter    27/   27] train: loss: 0.6924284
[Epoch 10] ogbg-molsider: 0.499266 val loss: 0.693304
[Epoch 10] ogbg-molsider: 0.496830 test loss: 0.692755
[Epoch 11] ogbg-molsider: 0.501713 val loss: 0.693130
[Epoch 11] ogbg-molsider: 0.496260 test loss: 0.692633
[Epoch 12; Iter     3/   27] train: loss: 0.6925757
[Epoch 12] ogbg-molsider: 0.501140 val loss: 0.693169
[Epoch 12] ogbg-molsider: 0.497440 test loss: 0.692626
[Epoch 13; Iter     6/   27] train: loss: 0.6928379
[Epoch 13] ogbg-molsider: 0.503489 val loss: 0.692971
[Epoch 13] ogbg-molsider: 0.496279 test loss: 0.692417
[Epoch 14; Iter     9/   27] train: loss: 0.6923137
[Epoch 14] ogbg-molsider: 0.500413 val loss: 0.693068
[Epoch 14] ogbg-molsider: 0.496971 test loss: 0.692502
[Epoch 15; Iter    12/   27] train: loss: 0.6929509
[Epoch 15] ogbg-molsider: 0.502068 val loss: 0.692768
[Epoch 15] ogbg-molsider: 0.495138 test loss: 0.692220
[Epoch 16; Iter    15/   27] train: loss: 0.6915839
[Epoch 16] ogbg-molsider: 0.502318 val loss: 0.692703
[Epoch 16] ogbg-molsider: 0.496376 test loss: 0.692199
[Epoch 17; Iter    18/   27] train: loss: 0.6919249
[Epoch 17] ogbg-molsider: 0.501952 val loss: 0.692589
[Epoch 17] ogbg-molsider: 0.495838 test loss: 0.692064
[Epoch 18; Iter    21/   27] train: loss: 0.6922923
[Epoch 18] ogbg-molsider: 0.502987 val loss: 0.692511
[Epoch 18] ogbg-molsider: 0.497084 test loss: 0.691972
[Epoch 19; Iter    24/   27] train: loss: 0.6918789
[Epoch 19] ogbg-molsider: 0.502697 val loss: 0.692364
[Epoch 19] ogbg-molsider: 0.496527 test loss: 0.691746
[Epoch 20; Iter    27/   27] train: loss: 0.6921641
[Epoch 20] ogbg-molsider: 0.502024 val loss: 0.692373
[Epoch 20] ogbg-molsider: 0.496461 test loss: 0.691808
[Epoch 21] ogbg-molsider: 0.502849 val loss: 0.692243
[Epoch 21] ogbg-molsider: 0.496837 test loss: 0.691668
[Epoch 22; Iter     3/   27] train: loss: 0.6913444
[Epoch 22] ogbg-molsider: 0.504817 val loss: 0.691970
[Epoch 22] ogbg-molsider: 0.496979 test loss: 0.691427
[Epoch 23; Iter     6/   27] train: loss: 0.6922497
[Epoch 23] ogbg-molsider: 0.504423 val loss: 0.691831
[Epoch 23] ogbg-molsider: 0.497806 test loss: 0.691245
[Epoch 24; Iter     9/   27] train: loss: 0.6906314
[Epoch 24] ogbg-molsider: 0.504896 val loss: 0.691664
[Epoch 24] ogbg-molsider: 0.497253 test loss: 0.691143
[Epoch 25; Iter    12/   27] train: loss: 0.6911649
[Epoch 25] ogbg-molsider: 0.504338 val loss: 0.691508
[Epoch 25] ogbg-molsider: 0.497078 test loss: 0.690877
[Epoch 26; Iter    15/   27] train: loss: 0.6904954
[Epoch 26] ogbg-molsider: 0.545873 val loss: 0.685413
[Epoch 26] ogbg-molsider: 0.518602 test loss: 0.684296
[Epoch 27; Iter    18/   27] train: loss: 0.6814992
[Epoch 27] ogbg-molsider: 0.552339 val loss: 0.673923
[Epoch 27] ogbg-molsider: 0.548844 test loss: 0.674250
[Epoch 28; Iter    21/   27] train: loss: 0.6695904
[Epoch 28] ogbg-molsider: 0.547472 val loss: 0.655623
[Epoch 28] ogbg-molsider: 0.549166 test loss: 0.650229
[Epoch 29; Iter    24/   27] train: loss: 0.6475832
[Epoch 29] ogbg-molsider: 0.553429 val loss: 0.630141
[Epoch 29] ogbg-molsider: 0.552173 test loss: 0.632659
[Epoch 30; Iter    27/   27] train: loss: 0.6268396
[Epoch 30] ogbg-molsider: 0.555952 val loss: 0.601989
[Epoch 30] ogbg-molsider: 0.552422 test loss: 0.597450
[Epoch 31] ogbg-molsider: 0.553276 val loss: 0.596953
[Epoch 31] ogbg-molsider: 0.543508 test loss: 0.596673
[Epoch 32; Iter     3/   27] train: loss: 0.6107528
[Epoch 32] ogbg-molsider: 0.544342 val loss: 0.572326
[Epoch 32] ogbg-molsider: 0.544966 test loss: 0.568458
[Epoch 33; Iter     6/   27] train: loss: 0.5917515
[Epoch 33] ogbg-molsider: 0.561599 val loss: 0.541689
[Epoch 33] ogbg-molsider: 0.561565 test loss: 0.531454
[Epoch 34; Iter     9/   27] train: loss: 0.5491522
[Epoch 34] ogbg-molsider: 0.567975 val loss: 0.554295
[Epoch 34] ogbg-molsider: 0.556276 test loss: 0.549815
[Epoch 35; Iter    12/   27] train: loss: 0.5374016
[Epoch 35] ogbg-molsider: 0.553508 val loss: 0.519467
[Epoch 35] ogbg-molsider: 0.549592 test loss: 0.509325
[Epoch 36; Iter    15/   27] train: loss: 0.5288494
[Epoch 36] ogbg-molsider: 0.556156 val loss: 0.524092
[Epoch 36] ogbg-molsider: 0.563130 test loss: 0.510763
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.6/PNA_ogbg-molsider_GraphCL_sider_scaff=0.6_4_26-05_10-04-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.6
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.493864 val loss: 0.693282
[Epoch 1] ogbg-molsider: 0.501511 test loss: 0.693313
[Epoch 2; Iter     3/   27] train: loss: 0.6934153
[Epoch 2] ogbg-molsider: 0.494685 val loss: 0.693349
[Epoch 2] ogbg-molsider: 0.496719 test loss: 0.693431
[Epoch 3; Iter     6/   27] train: loss: 0.6932269
[Epoch 3] ogbg-molsider: 0.494241 val loss: 0.693271
[Epoch 3] ogbg-molsider: 0.497721 test loss: 0.693478
[Epoch 4; Iter     9/   27] train: loss: 0.6936660
[Epoch 4] ogbg-molsider: 0.492298 val loss: 0.693243
[Epoch 4] ogbg-molsider: 0.499473 test loss: 0.693503
[Epoch 5; Iter    12/   27] train: loss: 0.6929981
[Epoch 5] ogbg-molsider: 0.492224 val loss: 0.693214
[Epoch 5] ogbg-molsider: 0.499587 test loss: 0.693427
[Epoch 6; Iter    15/   27] train: loss: 0.6935584
[Epoch 6] ogbg-molsider: 0.491765 val loss: 0.693090
[Epoch 6] ogbg-molsider: 0.499544 test loss: 0.693273
[Epoch 7; Iter    18/   27] train: loss: 0.6932315
[Epoch 7] ogbg-molsider: 0.491954 val loss: 0.693041
[Epoch 7] ogbg-molsider: 0.499317 test loss: 0.693182
[Epoch 8; Iter    21/   27] train: loss: 0.6925457
[Epoch 8] ogbg-molsider: 0.492823 val loss: 0.692979
[Epoch 8] ogbg-molsider: 0.500851 test loss: 0.693094
[Epoch 9; Iter    24/   27] train: loss: 0.6928196
[Epoch 9] ogbg-molsider: 0.491288 val loss: 0.693045
[Epoch 9] ogbg-molsider: 0.500350 test loss: 0.693228
[Epoch 10; Iter    27/   27] train: loss: 0.6922765
[Epoch 10] ogbg-molsider: 0.492016 val loss: 0.692917
[Epoch 10] ogbg-molsider: 0.500364 test loss: 0.693095
[Epoch 11] ogbg-molsider: 0.493433 val loss: 0.692864
[Epoch 11] ogbg-molsider: 0.499340 test loss: 0.693064
[Epoch 12; Iter     3/   27] train: loss: 0.6928219
[Epoch 12] ogbg-molsider: 0.492732 val loss: 0.692737
[Epoch 12] ogbg-molsider: 0.501098 test loss: 0.692868
[Epoch 13; Iter     6/   27] train: loss: 0.6927552
[Epoch 13] ogbg-molsider: 0.491519 val loss: 0.692465
[Epoch 13] ogbg-molsider: 0.500011 test loss: 0.692526
[Epoch 14; Iter     9/   27] train: loss: 0.6927819
[Epoch 14] ogbg-molsider: 0.492136 val loss: 0.692575
[Epoch 14] ogbg-molsider: 0.500599 test loss: 0.692712
[Epoch 15; Iter    12/   27] train: loss: 0.6922751
[Epoch 15] ogbg-molsider: 0.493449 val loss: 0.692522
[Epoch 15] ogbg-molsider: 0.501122 test loss: 0.692669
[Epoch 16; Iter    15/   27] train: loss: 0.6920533
[Epoch 16] ogbg-molsider: 0.493605 val loss: 0.692407
[Epoch 16] ogbg-molsider: 0.500728 test loss: 0.692574
[Epoch 17; Iter    18/   27] train: loss: 0.6924763
[Epoch 17] ogbg-molsider: 0.492503 val loss: 0.692277
[Epoch 17] ogbg-molsider: 0.500409 test loss: 0.692389
[Epoch 18; Iter    21/   27] train: loss: 0.6919776
[Epoch 18] ogbg-molsider: 0.492363 val loss: 0.692041
[Epoch 18] ogbg-molsider: 0.499330 test loss: 0.692142
[Epoch 19; Iter    24/   27] train: loss: 0.6920894
[Epoch 19] ogbg-molsider: 0.491949 val loss: 0.691951
[Epoch 19] ogbg-molsider: 0.501441 test loss: 0.692026
[Epoch 20; Iter    27/   27] train: loss: 0.6928576
[Epoch 20] ogbg-molsider: 0.492750 val loss: 0.691788
[Epoch 20] ogbg-molsider: 0.500307 test loss: 0.691855
[Epoch 21] ogbg-molsider: 0.492182 val loss: 0.691745
[Epoch 21] ogbg-molsider: 0.500172 test loss: 0.691875
[Epoch 22; Iter     3/   27] train: loss: 0.6923478
[Epoch 22] ogbg-molsider: 0.494048 val loss: 0.691525
[Epoch 22] ogbg-molsider: 0.499343 test loss: 0.691685
[Epoch 23; Iter     6/   27] train: loss: 0.6913377
[Epoch 23] ogbg-molsider: 0.491177 val loss: 0.691294
[Epoch 23] ogbg-molsider: 0.501150 test loss: 0.691304
[Epoch 24; Iter     9/   27] train: loss: 0.6913669
[Epoch 24] ogbg-molsider: 0.491976 val loss: 0.691029
[Epoch 24] ogbg-molsider: 0.500710 test loss: 0.691005
[Epoch 25; Iter    12/   27] train: loss: 0.6914176
[Epoch 25] ogbg-molsider: 0.492442 val loss: 0.690959
[Epoch 25] ogbg-molsider: 0.499513 test loss: 0.691003
[Epoch 26; Iter    15/   27] train: loss: 0.6899894
[Epoch 26] ogbg-molsider: 0.525834 val loss: 0.683611
[Epoch 26] ogbg-molsider: 0.531591 test loss: 0.683178
[Epoch 27; Iter    18/   27] train: loss: 0.6838799
[Epoch 27] ogbg-molsider: 0.551149 val loss: 0.671644
[Epoch 27] ogbg-molsider: 0.551952 test loss: 0.670331
[Epoch 28; Iter    21/   27] train: loss: 0.6685641
[Epoch 28] ogbg-molsider: 0.549810 val loss: 0.645667
[Epoch 28] ogbg-molsider: 0.554928 test loss: 0.640223
[Epoch 29; Iter    24/   27] train: loss: 0.6496075
[Epoch 29] ogbg-molsider: 0.545701 val loss: 0.620998
[Epoch 29] ogbg-molsider: 0.550974 test loss: 0.616543
[Epoch 30; Iter    27/   27] train: loss: 0.6377636
[Epoch 30] ogbg-molsider: 0.545510 val loss: 0.630558
[Epoch 30] ogbg-molsider: 0.540741 test loss: 0.623445
[Epoch 31] ogbg-molsider: 0.541418 val loss: 0.610144
[Epoch 31] ogbg-molsider: 0.550516 test loss: 0.599799
[Epoch 32; Iter     3/   27] train: loss: 0.5918167
[Epoch 32] ogbg-molsider: 0.548910 val loss: 0.560827
[Epoch 32] ogbg-molsider: 0.542960 test loss: 0.552602
[Epoch 33; Iter     6/   27] train: loss: 0.5881114
[Epoch 33] ogbg-molsider: 0.547255 val loss: 0.538263
[Epoch 33] ogbg-molsider: 0.539031 test loss: 0.530519
[Epoch 34; Iter     9/   27] train: loss: 0.5611175
[Epoch 34] ogbg-molsider: 0.554397 val loss: 0.545532
[Epoch 34] ogbg-molsider: 0.546411 test loss: 0.538016
[Epoch 35; Iter    12/   27] train: loss: 0.5310898
[Epoch 35] ogbg-molsider: 0.552077 val loss: 0.525341
[Epoch 35] ogbg-molsider: 0.557060 test loss: 0.513075
[Epoch 36; Iter    15/   27] train: loss: 0.5350383
[Epoch 36] ogbg-molsider: 0.544781 val loss: 0.529421
[Epoch 36] ogbg-molsider: 0.553955 test loss: 0.515450
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.8/PNA_ogbg-molsider_GraphCL_sider_scaff=0.8_6_26-05_10-04-02
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.8
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932251
[Epoch 1] ogbg-molsider: 0.475188 val loss: 0.693091
[Epoch 1] ogbg-molsider: 0.487468 test loss: 0.692665
[Epoch 2; Iter    24/   36] train: loss: 0.6936162
[Epoch 2] ogbg-molsider: 0.483259 val loss: 0.693333
[Epoch 2] ogbg-molsider: 0.494448 test loss: 0.692781
[Epoch 3; Iter    18/   36] train: loss: 0.6937852
[Epoch 3] ogbg-molsider: 0.481916 val loss: 0.693251
[Epoch 3] ogbg-molsider: 0.495852 test loss: 0.692527
[Epoch 4; Iter    12/   36] train: loss: 0.6931652
[Epoch 4] ogbg-molsider: 0.484816 val loss: 0.693293
[Epoch 4] ogbg-molsider: 0.499551 test loss: 0.692624
[Epoch 5; Iter     6/   36] train: loss: 0.6927382
[Epoch 5; Iter    36/   36] train: loss: 0.6923722
[Epoch 5] ogbg-molsider: 0.483999 val loss: 0.693191
[Epoch 5] ogbg-molsider: 0.499833 test loss: 0.692471
[Epoch 6; Iter    30/   36] train: loss: 0.6933881
[Epoch 6] ogbg-molsider: 0.483554 val loss: 0.693122
[Epoch 6] ogbg-molsider: 0.497578 test loss: 0.692398
[Epoch 7; Iter    24/   36] train: loss: 0.6925113
[Epoch 7] ogbg-molsider: 0.483826 val loss: 0.693102
[Epoch 7] ogbg-molsider: 0.498720 test loss: 0.692464
[Epoch 8; Iter    18/   36] train: loss: 0.6927406
[Epoch 8] ogbg-molsider: 0.480618 val loss: 0.692852
[Epoch 8] ogbg-molsider: 0.498981 test loss: 0.692081
[Epoch 9; Iter    12/   36] train: loss: 0.6931607
[Epoch 9] ogbg-molsider: 0.481298 val loss: 0.692794
[Epoch 9] ogbg-molsider: 0.498189 test loss: 0.692018
[Epoch 10; Iter     6/   36] train: loss: 0.6927510
[Epoch 10; Iter    36/   36] train: loss: 0.6932729
[Epoch 10] ogbg-molsider: 0.483696 val loss: 0.692735
[Epoch 10] ogbg-molsider: 0.497800 test loss: 0.692060
[Epoch 11; Iter    30/   36] train: loss: 0.6922941
[Epoch 11] ogbg-molsider: 0.483690 val loss: 0.692607
[Epoch 11] ogbg-molsider: 0.499632 test loss: 0.691933
[Epoch 12; Iter    24/   36] train: loss: 0.6922039
[Epoch 12] ogbg-molsider: 0.484378 val loss: 0.692515
[Epoch 12] ogbg-molsider: 0.501044 test loss: 0.691892
[Epoch 13; Iter    18/   36] train: loss: 0.6923359
[Epoch 13] ogbg-molsider: 0.484329 val loss: 0.692347
[Epoch 13] ogbg-molsider: 0.500183 test loss: 0.691708
[Epoch 14; Iter    12/   36] train: loss: 0.6920405
[Epoch 14] ogbg-molsider: 0.482742 val loss: 0.692055
[Epoch 14] ogbg-molsider: 0.498248 test loss: 0.691322
[Epoch 15; Iter     6/   36] train: loss: 0.6918349
[Epoch 15; Iter    36/   36] train: loss: 0.6916741
[Epoch 15] ogbg-molsider: 0.482693 val loss: 0.692006
[Epoch 15] ogbg-molsider: 0.499932 test loss: 0.691260
[Epoch 16; Iter    30/   36] train: loss: 0.6923392
[Epoch 16] ogbg-molsider: 0.486074 val loss: 0.691824
[Epoch 16] ogbg-molsider: 0.500256 test loss: 0.691167
[Epoch 17; Iter    24/   36] train: loss: 0.6917476
[Epoch 17] ogbg-molsider: 0.482482 val loss: 0.691651
[Epoch 17] ogbg-molsider: 0.501051 test loss: 0.690935
[Epoch 18; Iter    18/   36] train: loss: 0.6917873
[Epoch 18] ogbg-molsider: 0.481579 val loss: 0.691432
[Epoch 18] ogbg-molsider: 0.499520 test loss: 0.690706
[Epoch 19; Iter    12/   36] train: loss: 0.6910781
[Epoch 19] ogbg-molsider: 0.483040 val loss: 0.691175
[Epoch 19] ogbg-molsider: 0.499059 test loss: 0.690396
[Epoch 20; Iter     6/   36] train: loss: 0.6901195
[Epoch 20; Iter    36/   36] train: loss: 0.6872858
[Epoch 20] ogbg-molsider: 0.532703 val loss: 0.680850
[Epoch 20] ogbg-molsider: 0.557857 test loss: 0.679399
[Epoch 21; Iter    30/   36] train: loss: 0.6758711
[Epoch 21] ogbg-molsider: 0.518390 val loss: 0.650811
[Epoch 21] ogbg-molsider: 0.560651 test loss: 0.651349
[Epoch 22; Iter    24/   36] train: loss: 0.6622298
[Epoch 22] ogbg-molsider: 0.531238 val loss: 0.621855
[Epoch 22] ogbg-molsider: 0.570501 test loss: 0.620218
[Epoch 23; Iter    18/   36] train: loss: 0.6309107
[Epoch 23] ogbg-molsider: 0.527992 val loss: 0.587983
[Epoch 23] ogbg-molsider: 0.577106 test loss: 0.590271
[Epoch 24; Iter    12/   36] train: loss: 0.6134252
[Epoch 24] ogbg-molsider: 0.541577 val loss: 0.555987
[Epoch 24] ogbg-molsider: 0.590502 test loss: 0.559049
[Epoch 25; Iter     6/   36] train: loss: 0.5730899
[Epoch 25; Iter    36/   36] train: loss: 0.5890972
[Epoch 25] ogbg-molsider: 0.550515 val loss: 0.532913
[Epoch 25] ogbg-molsider: 0.559815 test loss: 0.539503
[Epoch 26; Iter    30/   36] train: loss: 0.5630614
[Epoch 26] ogbg-molsider: 0.539935 val loss: 0.530809
[Epoch 26] ogbg-molsider: 0.589484 test loss: 0.533135
[Epoch 27; Iter    24/   36] train: loss: 0.5240155
[Epoch 27] ogbg-molsider: 0.569708 val loss: 0.501522
[Epoch 27] ogbg-molsider: 0.577891 test loss: 0.518668
[Epoch 28; Iter    18/   36] train: loss: 0.4704752
[Epoch 28] ogbg-molsider: 0.556601 val loss: 0.489122
[Epoch 28] ogbg-molsider: 0.581335 test loss: 0.498574
[Epoch 29; Iter    12/   36] train: loss: 0.5589181
[Epoch 29] ogbg-molsider: 0.573898 val loss: 0.486062
[Epoch 29] ogbg-molsider: 0.601157 test loss: 0.495226
[Epoch 30; Iter     6/   36] train: loss: 0.5131509
[Epoch 30; Iter    36/   36] train: loss: 0.4826609
[Epoch 30] ogbg-molsider: 0.582567 val loss: 0.481786
[Epoch 30] ogbg-molsider: 0.592026 test loss: 0.495194
[Epoch 31; Iter    30/   36] train: loss: 0.5102567
[Epoch 31] ogbg-molsider: 0.562999 val loss: 0.486401
[Epoch 31] ogbg-molsider: 0.602747 test loss: 0.496858
[Epoch 32; Iter    24/   36] train: loss: 0.4893579
[Epoch 32] ogbg-molsider: 0.581744 val loss: 0.478141
[Epoch 32] ogbg-molsider: 0.611555 test loss: 0.489153
[Epoch 33; Iter    18/   36] train: loss: 0.5150605
[Epoch 33] ogbg-molsider: 0.575777 val loss: 0.482730
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.7/PNA_ogbg-molsider_GraphCL_sider_scaff=0.7_5_26-05_10-04-02
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.7
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6931860
[Epoch 1] ogbg-molsider: 0.497707 val loss: 0.693008
[Epoch 1] ogbg-molsider: 0.504803 test loss: 0.692866
[Epoch 2; Iter    28/   32] train: loss: 0.6932344
[Epoch 2] ogbg-molsider: 0.494359 val loss: 0.692760
[Epoch 2] ogbg-molsider: 0.503271 test loss: 0.692665
[Epoch 3; Iter    26/   32] train: loss: 0.6932582
[Epoch 3] ogbg-molsider: 0.493987 val loss: 0.692768
[Epoch 3] ogbg-molsider: 0.502041 test loss: 0.692653
[Epoch 4; Iter    24/   32] train: loss: 0.6934046
[Epoch 4] ogbg-molsider: 0.498050 val loss: 0.692661
[Epoch 4] ogbg-molsider: 0.495512 test loss: 0.692566
[Epoch 5; Iter    22/   32] train: loss: 0.6931764
[Epoch 5] ogbg-molsider: 0.499957 val loss: 0.692722
[Epoch 5] ogbg-molsider: 0.495244 test loss: 0.692690
[Epoch 6; Iter    20/   32] train: loss: 0.6930069
[Epoch 6] ogbg-molsider: 0.500102 val loss: 0.692534
[Epoch 6] ogbg-molsider: 0.495876 test loss: 0.692649
[Epoch 7; Iter    18/   32] train: loss: 0.6932801
[Epoch 7] ogbg-molsider: 0.499789 val loss: 0.692551
[Epoch 7] ogbg-molsider: 0.494034 test loss: 0.692444
[Epoch 8; Iter    16/   32] train: loss: 0.6931026
[Epoch 8] ogbg-molsider: 0.497907 val loss: 0.692353
[Epoch 8] ogbg-molsider: 0.495479 test loss: 0.692370
[Epoch 9; Iter    14/   32] train: loss: 0.6926082
[Epoch 9] ogbg-molsider: 0.500407 val loss: 0.692485
[Epoch 9] ogbg-molsider: 0.494835 test loss: 0.692485
[Epoch 10; Iter    12/   32] train: loss: 0.6921943
[Epoch 10] ogbg-molsider: 0.500416 val loss: 0.692258
[Epoch 10] ogbg-molsider: 0.498760 test loss: 0.692331
[Epoch 11; Iter    10/   32] train: loss: 0.6930699
[Epoch 11] ogbg-molsider: 0.498081 val loss: 0.692111
[Epoch 11] ogbg-molsider: 0.500196 test loss: 0.692123
[Epoch 12; Iter     8/   32] train: loss: 0.6926347
[Epoch 12] ogbg-molsider: 0.502293 val loss: 0.692139
[Epoch 12] ogbg-molsider: 0.495626 test loss: 0.692161
[Epoch 13; Iter     6/   32] train: loss: 0.6924779
[Epoch 13] ogbg-molsider: 0.503381 val loss: 0.692013
[Epoch 13] ogbg-molsider: 0.495080 test loss: 0.691935
[Epoch 14; Iter     4/   32] train: loss: 0.6922158
[Epoch 14] ogbg-molsider: 0.500075 val loss: 0.691798
[Epoch 14] ogbg-molsider: 0.495600 test loss: 0.691737
[Epoch 15; Iter     2/   32] train: loss: 0.6920902
[Epoch 15; Iter    32/   32] train: loss: 0.6925457
[Epoch 15] ogbg-molsider: 0.500404 val loss: 0.691558
[Epoch 15] ogbg-molsider: 0.497989 test loss: 0.691659
[Epoch 16; Iter    30/   32] train: loss: 0.6922702
[Epoch 16] ogbg-molsider: 0.505183 val loss: 0.691424
[Epoch 16] ogbg-molsider: 0.495546 test loss: 0.691392
[Epoch 17; Iter    28/   32] train: loss: 0.6922288
[Epoch 17] ogbg-molsider: 0.499010 val loss: 0.691182
[Epoch 17] ogbg-molsider: 0.497775 test loss: 0.691229
[Epoch 18; Iter    26/   32] train: loss: 0.6917899
[Epoch 18] ogbg-molsider: 0.500966 val loss: 0.691091
[Epoch 18] ogbg-molsider: 0.496059 test loss: 0.691019
[Epoch 19; Iter    24/   32] train: loss: 0.6918387
[Epoch 19] ogbg-molsider: 0.500076 val loss: 0.691078
[Epoch 19] ogbg-molsider: 0.495359 test loss: 0.691050
[Epoch 20; Iter    22/   32] train: loss: 0.6916383
[Epoch 20] ogbg-molsider: 0.502566 val loss: 0.690735
[Epoch 20] ogbg-molsider: 0.495599 test loss: 0.690695
[Epoch 21; Iter    20/   32] train: loss: 0.6917199
[Epoch 21] ogbg-molsider: 0.501709 val loss: 0.690540
[Epoch 21] ogbg-molsider: 0.496983 test loss: 0.690427
[Epoch 22; Iter    18/   32] train: loss: 0.6903518
[Epoch 22] ogbg-molsider: 0.523834 val loss: 0.685817
[Epoch 22] ogbg-molsider: 0.540312 test loss: 0.687058
[Epoch 23; Iter    16/   32] train: loss: 0.6848212
[Epoch 23] ogbg-molsider: 0.536939 val loss: 0.672110
[Epoch 23] ogbg-molsider: 0.558973 test loss: 0.672772
[Epoch 24; Iter    14/   32] train: loss: 0.6748670
[Epoch 24] ogbg-molsider: 0.538633 val loss: 0.633232
[Epoch 24] ogbg-molsider: 0.550665 test loss: 0.627790
[Epoch 25; Iter    12/   32] train: loss: 0.6591281
[Epoch 25] ogbg-molsider: 0.517651 val loss: 0.637494
[Epoch 25] ogbg-molsider: 0.564243 test loss: 0.637671
[Epoch 26; Iter    10/   32] train: loss: 0.6332664
[Epoch 26] ogbg-molsider: 0.539952 val loss: 0.593450
[Epoch 26] ogbg-molsider: 0.569310 test loss: 0.594009
[Epoch 27; Iter     8/   32] train: loss: 0.6224626
[Epoch 27] ogbg-molsider: 0.533098 val loss: 0.572695
[Epoch 27] ogbg-molsider: 0.575312 test loss: 0.574380
[Epoch 28; Iter     6/   32] train: loss: 0.5845515
[Epoch 28] ogbg-molsider: 0.531987 val loss: 0.541976
[Epoch 28] ogbg-molsider: 0.565844 test loss: 0.554722
[Epoch 29; Iter     4/   32] train: loss: 0.5893958
[Epoch 29] ogbg-molsider: 0.514851 val loss: 0.563710
[Epoch 29] ogbg-molsider: 0.556649 test loss: 0.566117
[Epoch 30; Iter     2/   32] train: loss: 0.5307950
[Epoch 30; Iter    32/   32] train: loss: 0.5260401
[Epoch 30] ogbg-molsider: 0.527886 val loss: 0.513076
[Epoch 30] ogbg-molsider: 0.567682 test loss: 0.531213
[Epoch 31; Iter    30/   32] train: loss: 0.5161245
[Epoch 31] ogbg-molsider: 0.541822 val loss: 0.525037
[Epoch 31] ogbg-molsider: 0.577271 test loss: 0.517735
[Epoch 32; Iter    28/   32] train: loss: 0.5116399
[Epoch 32] ogbg-molsider: 0.535204 val loss: 0.502341
[Epoch 32] ogbg-molsider: 0.561298 test loss: 0.504768
[Epoch 33; Iter    26/   32] train: loss: 0.5506912
[Epoch 33] ogbg-molsider: 0.543550 val loss: 0.493860
[Epoch 33] ogbg-molsider: 0.580400 test loss: 0.506105
[Epoch 34; Iter    24/   32] train: loss: 0.5002345
[Epoch 34] ogbg-molsider: 0.539403 val loss: 0.496514
[Epoch 34] ogbg-molsider: 0.583849 test loss: 0.509187
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.7/PNA_ogbg-molsider_GraphCL_sider_scaff=0.7_4_26-05_10-04-02
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.7
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6934416
[Epoch 1] ogbg-molsider: 0.513275 val loss: 0.693241
[Epoch 1] ogbg-molsider: 0.503252 test loss: 0.693435
[Epoch 2; Iter    28/   32] train: loss: 0.6932529
[Epoch 2] ogbg-molsider: 0.512341 val loss: 0.693191
[Epoch 2] ogbg-molsider: 0.494723 test loss: 0.693664
[Epoch 3; Iter    26/   32] train: loss: 0.6929126
[Epoch 3] ogbg-molsider: 0.506945 val loss: 0.693040
[Epoch 3] ogbg-molsider: 0.498580 test loss: 0.693609
[Epoch 4; Iter    24/   32] train: loss: 0.6934811
[Epoch 4] ogbg-molsider: 0.505770 val loss: 0.693052
[Epoch 4] ogbg-molsider: 0.500888 test loss: 0.693667
[Epoch 5; Iter    22/   32] train: loss: 0.6930045
[Epoch 5] ogbg-molsider: 0.499836 val loss: 0.692942
[Epoch 5] ogbg-molsider: 0.501868 test loss: 0.693288
[Epoch 6; Iter    20/   32] train: loss: 0.6931548
[Epoch 6] ogbg-molsider: 0.504373 val loss: 0.693048
[Epoch 6] ogbg-molsider: 0.502002 test loss: 0.693677
[Epoch 7; Iter    18/   32] train: loss: 0.6930877
[Epoch 7] ogbg-molsider: 0.502972 val loss: 0.692868
[Epoch 7] ogbg-molsider: 0.500080 test loss: 0.693393
[Epoch 8; Iter    16/   32] train: loss: 0.6931012
[Epoch 8] ogbg-molsider: 0.505236 val loss: 0.692734
[Epoch 8] ogbg-molsider: 0.501621 test loss: 0.693260
[Epoch 9; Iter    14/   32] train: loss: 0.6932908
[Epoch 9] ogbg-molsider: 0.505162 val loss: 0.692730
[Epoch 9] ogbg-molsider: 0.500419 test loss: 0.693335
[Epoch 10; Iter    12/   32] train: loss: 0.6926325
[Epoch 10] ogbg-molsider: 0.502913 val loss: 0.692500
[Epoch 10] ogbg-molsider: 0.500776 test loss: 0.693010
[Epoch 11; Iter    10/   32] train: loss: 0.6930455
[Epoch 11] ogbg-molsider: 0.504666 val loss: 0.692487
[Epoch 11] ogbg-molsider: 0.500048 test loss: 0.693024
[Epoch 12; Iter     8/   32] train: loss: 0.6928461
[Epoch 12] ogbg-molsider: 0.504521 val loss: 0.692445
[Epoch 12] ogbg-molsider: 0.500390 test loss: 0.693138
[Epoch 13; Iter     6/   32] train: loss: 0.6927983
[Epoch 13] ogbg-molsider: 0.507040 val loss: 0.692258
[Epoch 13] ogbg-molsider: 0.499026 test loss: 0.692816
[Epoch 14; Iter     4/   32] train: loss: 0.6925845
[Epoch 14] ogbg-molsider: 0.504728 val loss: 0.692063
[Epoch 14] ogbg-molsider: 0.498876 test loss: 0.692727
[Epoch 15; Iter     2/   32] train: loss: 0.6929663
[Epoch 15; Iter    32/   32] train: loss: 0.6917093
[Epoch 15] ogbg-molsider: 0.502831 val loss: 0.691937
[Epoch 15] ogbg-molsider: 0.499654 test loss: 0.692302
[Epoch 16; Iter    30/   32] train: loss: 0.6920983
[Epoch 16] ogbg-molsider: 0.504043 val loss: 0.691634
[Epoch 16] ogbg-molsider: 0.500421 test loss: 0.692159
[Epoch 17; Iter    28/   32] train: loss: 0.6920646
[Epoch 17] ogbg-molsider: 0.505270 val loss: 0.691595
[Epoch 17] ogbg-molsider: 0.500874 test loss: 0.692092
[Epoch 18; Iter    26/   32] train: loss: 0.6919149
[Epoch 18] ogbg-molsider: 0.504936 val loss: 0.691382
[Epoch 18] ogbg-molsider: 0.501310 test loss: 0.691904
[Epoch 19; Iter    24/   32] train: loss: 0.6912453
[Epoch 19] ogbg-molsider: 0.507072 val loss: 0.691288
[Epoch 19] ogbg-molsider: 0.499433 test loss: 0.691893
[Epoch 20; Iter    22/   32] train: loss: 0.6913432
[Epoch 20] ogbg-molsider: 0.507325 val loss: 0.691003
[Epoch 20] ogbg-molsider: 0.498795 test loss: 0.691407
[Epoch 21; Iter    20/   32] train: loss: 0.6911055
[Epoch 21] ogbg-molsider: 0.505989 val loss: 0.690726
[Epoch 21] ogbg-molsider: 0.500022 test loss: 0.691236
[Epoch 22; Iter    18/   32] train: loss: 0.6904629
[Epoch 22] ogbg-molsider: 0.525203 val loss: 0.684792
[Epoch 22] ogbg-molsider: 0.548699 test loss: 0.685299
[Epoch 23; Iter    16/   32] train: loss: 0.6840669
[Epoch 23] ogbg-molsider: 0.534837 val loss: 0.674808
[Epoch 23] ogbg-molsider: 0.569479 test loss: 0.675147
[Epoch 24; Iter    14/   32] train: loss: 0.6678857
[Epoch 24] ogbg-molsider: 0.521959 val loss: 0.657103
[Epoch 24] ogbg-molsider: 0.551567 test loss: 0.658471
[Epoch 25; Iter    12/   32] train: loss: 0.6564507
[Epoch 25] ogbg-molsider: 0.520633 val loss: 0.624361
[Epoch 25] ogbg-molsider: 0.555813 test loss: 0.622793
[Epoch 26; Iter    10/   32] train: loss: 0.6485417
[Epoch 26] ogbg-molsider: 0.522267 val loss: 0.576637
[Epoch 26] ogbg-molsider: 0.557371 test loss: 0.583140
[Epoch 27; Iter     8/   32] train: loss: 0.6139607
[Epoch 27] ogbg-molsider: 0.531545 val loss: 0.562251
[Epoch 27] ogbg-molsider: 0.577395 test loss: 0.565883
[Epoch 28; Iter     6/   32] train: loss: 0.5942497
[Epoch 28] ogbg-molsider: 0.528022 val loss: 0.542448
[Epoch 28] ogbg-molsider: 0.566055 test loss: 0.552243
[Epoch 29; Iter     4/   32] train: loss: 0.5478145
[Epoch 29] ogbg-molsider: 0.538436 val loss: 0.529375
[Epoch 29] ogbg-molsider: 0.571635 test loss: 0.541569
[Epoch 30; Iter     2/   32] train: loss: 0.5515836
[Epoch 30; Iter    32/   32] train: loss: 0.5710261
[Epoch 30] ogbg-molsider: 0.541516 val loss: 0.522586
[Epoch 30] ogbg-molsider: 0.566021 test loss: 0.542874
[Epoch 31; Iter    30/   32] train: loss: 0.5358970
[Epoch 31] ogbg-molsider: 0.540451 val loss: 0.561312
[Epoch 31] ogbg-molsider: 0.569948 test loss: 0.514849
[Epoch 32; Iter    28/   32] train: loss: 0.5344343
[Epoch 32] ogbg-molsider: 0.532970 val loss: 0.593609
[Epoch 32] ogbg-molsider: 0.569044 test loss: 0.506985
[Epoch 33; Iter    26/   32] train: loss: 0.5201076
[Epoch 33] ogbg-molsider: 0.527370 val loss: 0.558728
[Epoch 33] ogbg-molsider: 0.562803 test loss: 0.514057
[Epoch 34; Iter    24/   32] train: loss: 0.5166691
[Epoch 34] ogbg-molsider: 0.539596 val loss: 0.562933
[Epoch 34] ogbg-molsider: 0.573314 test loss: 0.507165
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/sider/scaff/train_prop=0.6/PNA_ogbg-molsider_GraphCL_sider_scaff=0.6_5_26-05_10-04-02
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_scaff=0.6
logdir: runs/split/GraphCL/sider/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.506603 val loss: 0.692987
[Epoch 1] ogbg-molsider: 0.497481 test loss: 0.692840
[Epoch 2; Iter     3/   27] train: loss: 0.6934064
[Epoch 2] ogbg-molsider: 0.503620 val loss: 0.692737
[Epoch 2] ogbg-molsider: 0.498487 test loss: 0.692583
[Epoch 3; Iter     6/   27] train: loss: 0.6932975
[Epoch 3] ogbg-molsider: 0.499058 val loss: 0.692699
[Epoch 3] ogbg-molsider: 0.493498 test loss: 0.692494
[Epoch 4; Iter     9/   27] train: loss: 0.6930905
[Epoch 4] ogbg-molsider: 0.496508 val loss: 0.692692
[Epoch 4] ogbg-molsider: 0.490440 test loss: 0.692483
[Epoch 5; Iter    12/   27] train: loss: 0.6924731
[Epoch 5] ogbg-molsider: 0.495377 val loss: 0.692637
[Epoch 5] ogbg-molsider: 0.490246 test loss: 0.692394
[Epoch 6; Iter    15/   27] train: loss: 0.6929998
[Epoch 6] ogbg-molsider: 0.495927 val loss: 0.692540
[Epoch 6] ogbg-molsider: 0.490631 test loss: 0.692295
[Epoch 7; Iter    18/   27] train: loss: 0.6931859
[Epoch 7] ogbg-molsider: 0.496358 val loss: 0.692562
[Epoch 7] ogbg-molsider: 0.491590 test loss: 0.692359
[Epoch 8; Iter    21/   27] train: loss: 0.6923938
[Epoch 8] ogbg-molsider: 0.497769 val loss: 0.692547
[Epoch 8] ogbg-molsider: 0.490805 test loss: 0.692304
[Epoch 9; Iter    24/   27] train: loss: 0.6930651
[Epoch 9] ogbg-molsider: 0.496931 val loss: 0.692508
[Epoch 9] ogbg-molsider: 0.491261 test loss: 0.692251
[Epoch 10; Iter    27/   27] train: loss: 0.6929849
[Epoch 10] ogbg-molsider: 0.499036 val loss: 0.692440
[Epoch 10] ogbg-molsider: 0.491603 test loss: 0.692159
[Epoch 11] ogbg-molsider: 0.496889 val loss: 0.692360
[Epoch 11] ogbg-molsider: 0.490811 test loss: 0.692085
[Epoch 12; Iter     3/   27] train: loss: 0.6928112
[Epoch 12] ogbg-molsider: 0.497100 val loss: 0.692326
[Epoch 12] ogbg-molsider: 0.490240 test loss: 0.692038
[Epoch 13; Iter     6/   27] train: loss: 0.6930797
[Epoch 13] ogbg-molsider: 0.497515 val loss: 0.692247
[Epoch 13] ogbg-molsider: 0.489973 test loss: 0.691944
[Epoch 14; Iter     9/   27] train: loss: 0.6929921
[Epoch 14] ogbg-molsider: 0.497353 val loss: 0.692140
[Epoch 14] ogbg-molsider: 0.491145 test loss: 0.691889
[Epoch 15; Iter    12/   27] train: loss: 0.6925475
[Epoch 15] ogbg-molsider: 0.498156 val loss: 0.692086
[Epoch 15] ogbg-molsider: 0.491274 test loss: 0.691798
[Epoch 16; Iter    15/   27] train: loss: 0.6923252
[Epoch 16] ogbg-molsider: 0.496994 val loss: 0.691981
[Epoch 16] ogbg-molsider: 0.490570 test loss: 0.691669
[Epoch 17; Iter    18/   27] train: loss: 0.6923260
[Epoch 17] ogbg-molsider: 0.497455 val loss: 0.691741
[Epoch 17] ogbg-molsider: 0.491091 test loss: 0.691360
[Epoch 18; Iter    21/   27] train: loss: 0.6920990
[Epoch 18] ogbg-molsider: 0.497299 val loss: 0.691652
[Epoch 18] ogbg-molsider: 0.491938 test loss: 0.691312
[Epoch 19; Iter    24/   27] train: loss: 0.6922076
[Epoch 19] ogbg-molsider: 0.497551 val loss: 0.691549
[Epoch 19] ogbg-molsider: 0.491434 test loss: 0.691213
[Epoch 20; Iter    27/   27] train: loss: 0.6919476
[Epoch 20] ogbg-molsider: 0.499357 val loss: 0.691375
[Epoch 20] ogbg-molsider: 0.491731 test loss: 0.691042
[Epoch 21] ogbg-molsider: 0.499720 val loss: 0.691269
[Epoch 21] ogbg-molsider: 0.491609 test loss: 0.690879
[Epoch 22; Iter     3/   27] train: loss: 0.6917493
[Epoch 22] ogbg-molsider: 0.500666 val loss: 0.691158
[Epoch 22] ogbg-molsider: 0.492177 test loss: 0.690810
[Epoch 23; Iter     6/   27] train: loss: 0.6919208
[Epoch 23] ogbg-molsider: 0.498452 val loss: 0.691083
[Epoch 23] ogbg-molsider: 0.490764 test loss: 0.690653
[Epoch 24; Iter     9/   27] train: loss: 0.6913177
[Epoch 24] ogbg-molsider: 0.497891 val loss: 0.690854
[Epoch 24] ogbg-molsider: 0.492361 test loss: 0.690461
[Epoch 25; Iter    12/   27] train: loss: 0.6916816
[Epoch 25] ogbg-molsider: 0.499389 val loss: 0.690683
[Epoch 25] ogbg-molsider: 0.492787 test loss: 0.690327
[Epoch 26; Iter    15/   27] train: loss: 0.6904721
[Epoch 26] ogbg-molsider: 0.520855 val loss: 0.684821
[Epoch 26] ogbg-molsider: 0.540176 test loss: 0.684098
[Epoch 27; Iter    18/   27] train: loss: 0.6835245
[Epoch 27] ogbg-molsider: 0.544391 val loss: 0.674354
[Epoch 27] ogbg-molsider: 0.548721 test loss: 0.673784
[Epoch 28; Iter    21/   27] train: loss: 0.6728989
[Epoch 28] ogbg-molsider: 0.546578 val loss: 0.650532
[Epoch 28] ogbg-molsider: 0.553772 test loss: 0.645919
[Epoch 29; Iter    24/   27] train: loss: 0.6538180
[Epoch 29] ogbg-molsider: 0.553115 val loss: 0.625538
[Epoch 29] ogbg-molsider: 0.547635 test loss: 0.618577
[Epoch 30; Iter    27/   27] train: loss: 0.6300025
[Epoch 30] ogbg-molsider: 0.550752 val loss: 0.600798
[Epoch 30] ogbg-molsider: 0.561758 test loss: 0.594145
[Epoch 31] ogbg-molsider: 0.547959 val loss: 0.586534
[Epoch 31] ogbg-molsider: 0.548735 test loss: 0.579426
[Epoch 32; Iter     3/   27] train: loss: 0.6121442
[Epoch 32] ogbg-molsider: 0.543495 val loss: 0.590886
[Epoch 32] ogbg-molsider: 0.544175 test loss: 0.587796
[Epoch 33; Iter     6/   27] train: loss: 0.5720814
[Epoch 33] ogbg-molsider: 0.549493 val loss: 0.558273
[Epoch 33] ogbg-molsider: 0.562642 test loss: 0.546813
[Epoch 34; Iter     9/   27] train: loss: 0.5554139
[Epoch 34] ogbg-molsider: 0.548912 val loss: 0.557287
[Epoch 34] ogbg-molsider: 0.558627 test loss: 0.550908
[Epoch 35; Iter    12/   27] train: loss: 0.5488465
[Epoch 35] ogbg-molsider: 0.558451 val loss: 0.524565
[Epoch 35] ogbg-molsider: 0.564544 test loss: 0.511693
[Epoch 36; Iter    15/   27] train: loss: 0.5204072
[Epoch 36] ogbg-molsider: 0.550534 val loss: 0.522349
[Epoch 36] ogbg-molsider: 0.569384 test loss: 0.509291
[Epoch 35; Iter    22/   32] train: loss: 0.5061725
[Epoch 35] ogbg-molsider: 0.549896 val loss: 0.490869
[Epoch 35] ogbg-molsider: 0.582592 test loss: 0.504381
[Epoch 36; Iter    20/   32] train: loss: 0.5169007
[Epoch 36] ogbg-molsider: 0.554056 val loss: 0.484241
[Epoch 36] ogbg-molsider: 0.590586 test loss: 0.497465
[Epoch 37; Iter    18/   32] train: loss: 0.4979369
[Epoch 37] ogbg-molsider: 0.553932 val loss: 0.485003
[Epoch 37] ogbg-molsider: 0.587916 test loss: 0.498862
[Epoch 38; Iter    16/   32] train: loss: 0.5210938
[Epoch 38] ogbg-molsider: 0.548037 val loss: 0.486603
[Epoch 38] ogbg-molsider: 0.595419 test loss: 0.512699
[Epoch 39; Iter    14/   32] train: loss: 0.4720070
[Epoch 39] ogbg-molsider: 0.561724 val loss: 0.488799
[Epoch 39] ogbg-molsider: 0.608638 test loss: 0.499954
[Epoch 40; Iter    12/   32] train: loss: 0.4849935
[Epoch 40] ogbg-molsider: 0.553432 val loss: 0.486956
[Epoch 40] ogbg-molsider: 0.590938 test loss: 0.505402
[Epoch 41; Iter    10/   32] train: loss: 0.4733478
[Epoch 41] ogbg-molsider: 0.561555 val loss: 0.490460
[Epoch 41] ogbg-molsider: 0.593279 test loss: 0.519003
[Epoch 42; Iter     8/   32] train: loss: 0.4560006
[Epoch 42] ogbg-molsider: 0.553903 val loss: 0.485279
[Epoch 42] ogbg-molsider: 0.594124 test loss: 0.505117
[Epoch 43; Iter     6/   32] train: loss: 0.4555336
[Epoch 43] ogbg-molsider: 0.530481 val loss: 0.494326
[Epoch 43] ogbg-molsider: 0.590093 test loss: 0.496122
[Epoch 44; Iter     4/   32] train: loss: 0.5118659
[Epoch 44] ogbg-molsider: 0.548889 val loss: 0.496795
[Epoch 44] ogbg-molsider: 0.607728 test loss: 0.512511
[Epoch 45; Iter     2/   32] train: loss: 0.4845397
[Epoch 45; Iter    32/   32] train: loss: 0.5309215
[Epoch 45] ogbg-molsider: 0.543741 val loss: 0.493655
[Epoch 45] ogbg-molsider: 0.603766 test loss: 0.525553
[Epoch 46; Iter    30/   32] train: loss: 0.4471840
[Epoch 46] ogbg-molsider: 0.554887 val loss: 0.506686
[Epoch 46] ogbg-molsider: 0.577315 test loss: 0.553436
[Epoch 47; Iter    28/   32] train: loss: 0.4588603
[Epoch 47] ogbg-molsider: 0.580708 val loss: 0.504354
[Epoch 47] ogbg-molsider: 0.609592 test loss: 0.538834
[Epoch 48; Iter    26/   32] train: loss: 0.4474373
[Epoch 48] ogbg-molsider: 0.560551 val loss: 0.539657
[Epoch 48] ogbg-molsider: 0.586134 test loss: 0.529331
[Epoch 49; Iter    24/   32] train: loss: 0.4876285
[Epoch 49] ogbg-molsider: 0.568665 val loss: 0.531082
[Epoch 49] ogbg-molsider: 0.605424 test loss: 0.546660
[Epoch 50; Iter    22/   32] train: loss: 0.4645191
[Epoch 50] ogbg-molsider: 0.566899 val loss: 0.502515
[Epoch 50] ogbg-molsider: 0.611500 test loss: 0.558382
[Epoch 51; Iter    20/   32] train: loss: 0.4939317
[Epoch 51] ogbg-molsider: 0.551573 val loss: 0.513372
[Epoch 51] ogbg-molsider: 0.569863 test loss: 0.533471
[Epoch 52; Iter    18/   32] train: loss: 0.4716242
[Epoch 52] ogbg-molsider: 0.524320 val loss: 0.509164
[Epoch 52] ogbg-molsider: 0.580566 test loss: 0.529532
[Epoch 53; Iter    16/   32] train: loss: 0.5059161
[Epoch 53] ogbg-molsider: 0.550693 val loss: 0.504768
[Epoch 53] ogbg-molsider: 0.588582 test loss: 0.512355
[Epoch 54; Iter    14/   32] train: loss: 0.5061350
[Epoch 54] ogbg-molsider: 0.538418 val loss: 0.509838
[Epoch 54] ogbg-molsider: 0.606368 test loss: 0.527940
[Epoch 55; Iter    12/   32] train: loss: 0.4533636
[Epoch 55] ogbg-molsider: 0.584145 val loss: 0.501195
[Epoch 55] ogbg-molsider: 0.601989 test loss: 0.576918
[Epoch 56; Iter    10/   32] train: loss: 0.4980036
[Epoch 56] ogbg-molsider: 0.569648 val loss: 0.608639
[Epoch 56] ogbg-molsider: 0.605409 test loss: 0.571745
[Epoch 57; Iter     8/   32] train: loss: 0.4267699
[Epoch 57] ogbg-molsider: 0.566070 val loss: 0.503583
[Epoch 57] ogbg-molsider: 0.623671 test loss: 0.536734
[Epoch 58; Iter     6/   32] train: loss: 0.4673547
[Epoch 58] ogbg-molsider: 0.564671 val loss: 0.487434
[Epoch 58] ogbg-molsider: 0.607138 test loss: 0.493611
[Epoch 59; Iter     4/   32] train: loss: 0.4279659
[Epoch 59] ogbg-molsider: 0.572874 val loss: 0.491555
[Epoch 59] ogbg-molsider: 0.603168 test loss: 0.520898
[Epoch 60; Iter     2/   32] train: loss: 0.4698191
[Epoch 60; Iter    32/   32] train: loss: 0.6131896
[Epoch 60] ogbg-molsider: 0.567073 val loss: 0.496459
[Epoch 60] ogbg-molsider: 0.570469 test loss: 0.541267
[Epoch 61; Iter    30/   32] train: loss: 0.4801925
[Epoch 61] ogbg-molsider: 0.563268 val loss: 0.499841
[Epoch 61] ogbg-molsider: 0.590289 test loss: 0.644196
[Epoch 62; Iter    28/   32] train: loss: 0.5041734
[Epoch 62] ogbg-molsider: 0.588804 val loss: 0.505238
[Epoch 62] ogbg-molsider: 0.571480 test loss: 0.623943
[Epoch 63; Iter    26/   32] train: loss: 0.4758883
[Epoch 63] ogbg-molsider: 0.585156 val loss: 0.493268
[Epoch 63] ogbg-molsider: 0.609844 test loss: 0.498919
[Epoch 64; Iter    24/   32] train: loss: 0.4455921
[Epoch 64] ogbg-molsider: 0.587993 val loss: 0.501865
[Epoch 64] ogbg-molsider: 0.602367 test loss: 0.656324
[Epoch 65; Iter    22/   32] train: loss: 0.4967720
[Epoch 65] ogbg-molsider: 0.558335 val loss: 0.499344
[Epoch 65] ogbg-molsider: 0.608230 test loss: 0.551970
[Epoch 66; Iter    20/   32] train: loss: 0.4739726
[Epoch 66] ogbg-molsider: 0.573390 val loss: 0.501133
[Epoch 66] ogbg-molsider: 0.594154 test loss: 0.516608
[Epoch 67; Iter    18/   32] train: loss: 0.4567629
[Epoch 67] ogbg-molsider: 0.560868 val loss: 0.945975
[Epoch 67] ogbg-molsider: 0.593608 test loss: 0.602693
[Epoch 68; Iter    16/   32] train: loss: 0.4404598
[Epoch 68] ogbg-molsider: 0.569875 val loss: 0.521903
[Epoch 68] ogbg-molsider: 0.591772 test loss: 0.534168
[Epoch 69; Iter    14/   32] train: loss: 0.4176315
[Epoch 69] ogbg-molsider: 0.585039 val loss: 0.493177
[Epoch 69] ogbg-molsider: 0.599854 test loss: 0.524879
[Epoch 70; Iter    12/   32] train: loss: 0.4429798
[Epoch 70] ogbg-molsider: 0.563448 val loss: 0.506958
[Epoch 70] ogbg-molsider: 0.599790 test loss: 0.549148
[Epoch 71; Iter    10/   32] train: loss: 0.4635518
[Epoch 71] ogbg-molsider: 0.590073 val loss: 0.501523
[Epoch 71] ogbg-molsider: 0.602762 test loss: 0.539924
[Epoch 72; Iter     8/   32] train: loss: 0.4227844
[Epoch 72] ogbg-molsider: 0.573527 val loss: 0.754716
[Epoch 72] ogbg-molsider: 0.585923 test loss: 3.705574
[Epoch 73; Iter     6/   32] train: loss: 0.4417099
[Epoch 73] ogbg-molsider: 0.561499 val loss: 0.529542
[Epoch 73] ogbg-molsider: 0.571563 test loss: 0.538520
[Epoch 74; Iter     4/   32] train: loss: 0.4231051
[Epoch 74] ogbg-molsider: 0.573197 val loss: 0.509260
[Epoch 74] ogbg-molsider: 0.599532 test loss: 0.519533
[Epoch 75; Iter     2/   32] train: loss: 0.4215478
[Epoch 75; Iter    32/   32] train: loss: 0.4500703
[Epoch 75] ogbg-molsider: 0.578718 val loss: 0.513340
[Epoch 75] ogbg-molsider: 0.591125 test loss: 0.577831
[Epoch 76; Iter    30/   32] train: loss: 0.3975030
[Epoch 76] ogbg-molsider: 0.573366 val loss: 0.520133
[Epoch 76] ogbg-molsider: 0.579928 test loss: 0.531417
[Epoch 77; Iter    28/   32] train: loss: 0.4002830
[Epoch 77] ogbg-molsider: 0.555247 val loss: 0.542807
[Epoch 77] ogbg-molsider: 0.599641 test loss: 0.568183
[Epoch 78; Iter    26/   32] train: loss: 0.3893542
[Epoch 78] ogbg-molsider: 0.573913 val loss: 0.517821
[Epoch 78] ogbg-molsider: 0.580765 test loss: 0.528640
[Epoch 79; Iter    24/   32] train: loss: 0.4338205
[Epoch 79] ogbg-molsider: 0.549216 val loss: 0.533179
[Epoch 79] ogbg-molsider: 0.576879 test loss: 0.589777
[Epoch 80; Iter    22/   32] train: loss: 0.3992838
[Epoch 80] ogbg-molsider: 0.576887 val loss: 0.527230
[Epoch 80] ogbg-molsider: 0.578211 test loss: 0.542680
[Epoch 81; Iter    20/   32] train: loss: 0.4001818
[Epoch 81] ogbg-molsider: 0.575836 val loss: 0.523869
[Epoch 81] ogbg-molsider: 0.577278 test loss: 0.546970
[Epoch 82; Iter    18/   32] train: loss: 0.3936861
[Epoch 82] ogbg-molsider: 0.565074 val loss: 0.538497
[Epoch 82] ogbg-molsider: 0.581213 test loss: 0.557524
[Epoch 83; Iter    16/   32] train: loss: 0.3590782
[Epoch 83] ogbg-molsider: 0.569695 val loss: 0.543402
[Epoch 83] ogbg-molsider: 0.571609 test loss: 0.576142
[Epoch 84; Iter    14/   32] train: loss: 0.3544025
[Epoch 84] ogbg-molsider: 0.561731 val loss: 0.544265
[Epoch 33] ogbg-molsider: 0.611953 test loss: 0.494842
[Epoch 34; Iter    12/   36] train: loss: 0.5000725
[Epoch 34] ogbg-molsider: 0.578457 val loss: 0.490188
[Epoch 34] ogbg-molsider: 0.597721 test loss: 0.510233
[Epoch 35; Iter     6/   36] train: loss: 0.5462330
[Epoch 35; Iter    36/   36] train: loss: 0.5072761
[Epoch 35] ogbg-molsider: 0.587578 val loss: 0.491399
[Epoch 35] ogbg-molsider: 0.590006 test loss: 0.504726
[Epoch 36; Iter    30/   36] train: loss: 0.4811558
[Epoch 36] ogbg-molsider: 0.564890 val loss: 0.482126
[Epoch 36] ogbg-molsider: 0.592217 test loss: 0.491413
[Epoch 37; Iter    24/   36] train: loss: 0.4630575
[Epoch 37] ogbg-molsider: 0.575578 val loss: 0.489023
[Epoch 37] ogbg-molsider: 0.603729 test loss: 0.504952
[Epoch 38; Iter    18/   36] train: loss: 0.4957457
[Epoch 38] ogbg-molsider: 0.580506 val loss: 0.479880
[Epoch 38] ogbg-molsider: 0.618810 test loss: 0.491871
[Epoch 39; Iter    12/   36] train: loss: 0.5037125
[Epoch 39] ogbg-molsider: 0.589661 val loss: 0.484394
[Epoch 39] ogbg-molsider: 0.618633 test loss: 0.502284
[Epoch 40; Iter     6/   36] train: loss: 0.4773421
[Epoch 40; Iter    36/   36] train: loss: 0.5184150
[Epoch 40] ogbg-molsider: 0.541803 val loss: 0.502964
[Epoch 40] ogbg-molsider: 0.600158 test loss: 0.513022
[Epoch 41; Iter    30/   36] train: loss: 0.5156377
[Epoch 41] ogbg-molsider: 0.597964 val loss: 0.475261
[Epoch 41] ogbg-molsider: 0.593451 test loss: 0.497801
[Epoch 42; Iter    24/   36] train: loss: 0.4747066
[Epoch 42] ogbg-molsider: 0.594815 val loss: 0.483298
[Epoch 42] ogbg-molsider: 0.617878 test loss: 0.495276
[Epoch 43; Iter    18/   36] train: loss: 0.4947161
[Epoch 43] ogbg-molsider: 0.606171 val loss: 0.473198
[Epoch 43] ogbg-molsider: 0.608603 test loss: 0.490139
[Epoch 44; Iter    12/   36] train: loss: 0.4618759
[Epoch 44] ogbg-molsider: 0.600496 val loss: 0.485198
[Epoch 44] ogbg-molsider: 0.608218 test loss: 0.511112
[Epoch 45; Iter     6/   36] train: loss: 0.4816950
[Epoch 45; Iter    36/   36] train: loss: 0.4514899
[Epoch 45] ogbg-molsider: 0.591721 val loss: 0.764961
[Epoch 45] ogbg-molsider: 0.580366 test loss: 0.630946
[Epoch 46; Iter    30/   36] train: loss: 0.5126225
[Epoch 46] ogbg-molsider: 0.583790 val loss: 0.495453
[Epoch 46] ogbg-molsider: 0.625647 test loss: 0.503108
[Epoch 47; Iter    24/   36] train: loss: 0.4630098
[Epoch 47] ogbg-molsider: 0.580978 val loss: 0.489505
[Epoch 47] ogbg-molsider: 0.603840 test loss: 0.507207
[Epoch 48; Iter    18/   36] train: loss: 0.4991953
[Epoch 48] ogbg-molsider: 0.580553 val loss: 0.495281
[Epoch 48] ogbg-molsider: 0.618838 test loss: 0.506728
[Epoch 49; Iter    12/   36] train: loss: 0.4749456
[Epoch 49] ogbg-molsider: 0.588780 val loss: 0.479939
[Epoch 49] ogbg-molsider: 0.605835 test loss: 0.489594
[Epoch 50; Iter     6/   36] train: loss: 0.4886106
[Epoch 50; Iter    36/   36] train: loss: 0.5259469
[Epoch 50] ogbg-molsider: 0.586167 val loss: 0.501547
[Epoch 50] ogbg-molsider: 0.621065 test loss: 0.513745
[Epoch 51; Iter    30/   36] train: loss: 0.5406562
[Epoch 51] ogbg-molsider: 0.606074 val loss: 0.490386
[Epoch 51] ogbg-molsider: 0.615865 test loss: 0.516443
[Epoch 52; Iter    24/   36] train: loss: 0.4501581
[Epoch 52] ogbg-molsider: 0.633883 val loss: 0.473437
[Epoch 52] ogbg-molsider: 0.593531 test loss: 0.497737
[Epoch 53; Iter    18/   36] train: loss: 0.4598140
[Epoch 53] ogbg-molsider: 0.598583 val loss: 0.492689
[Epoch 53] ogbg-molsider: 0.595510 test loss: 0.521228
[Epoch 54; Iter    12/   36] train: loss: 0.4295415
[Epoch 54] ogbg-molsider: 0.604881 val loss: 0.480076
[Epoch 54] ogbg-molsider: 0.588359 test loss: 0.507933
[Epoch 55; Iter     6/   36] train: loss: 0.4674853
[Epoch 55; Iter    36/   36] train: loss: 0.5273484
[Epoch 55] ogbg-molsider: 0.563110 val loss: 3.290264
[Epoch 55] ogbg-molsider: 0.569687 test loss: 6.501547
[Epoch 56; Iter    30/   36] train: loss: 0.4719634
[Epoch 56] ogbg-molsider: 0.553233 val loss: 0.727841
[Epoch 56] ogbg-molsider: 0.564310 test loss: 0.584373
[Epoch 57; Iter    24/   36] train: loss: 0.4941161
[Epoch 57] ogbg-molsider: 0.566382 val loss: 0.507049
[Epoch 57] ogbg-molsider: 0.609805 test loss: 0.531503
[Epoch 58; Iter    18/   36] train: loss: 0.4947456
[Epoch 58] ogbg-molsider: 0.570011 val loss: 0.502642
[Epoch 58] ogbg-molsider: 0.576494 test loss: 0.528665
[Epoch 59; Iter    12/   36] train: loss: 0.4320258
[Epoch 59] ogbg-molsider: 0.573157 val loss: 0.502321
[Epoch 59] ogbg-molsider: 0.608127 test loss: 0.512446
[Epoch 60; Iter     6/   36] train: loss: 0.4104364
[Epoch 60; Iter    36/   36] train: loss: 0.4554821
[Epoch 60] ogbg-molsider: 0.623255 val loss: 0.535940
[Epoch 60] ogbg-molsider: 0.594349 test loss: 0.582283
[Epoch 61; Iter    30/   36] train: loss: 0.4497009
[Epoch 61] ogbg-molsider: 0.600080 val loss: 0.496376
[Epoch 61] ogbg-molsider: 0.584225 test loss: 0.531138
[Epoch 62; Iter    24/   36] train: loss: 0.3812849
[Epoch 62] ogbg-molsider: 0.585207 val loss: 0.511809
[Epoch 62] ogbg-molsider: 0.578026 test loss: 0.535818
[Epoch 63; Iter    18/   36] train: loss: 0.4533871
[Epoch 63] ogbg-molsider: 0.595058 val loss: 0.494750
[Epoch 63] ogbg-molsider: 0.584105 test loss: 0.516480
[Epoch 64; Iter    12/   36] train: loss: 0.4219020
[Epoch 64] ogbg-molsider: 0.594689 val loss: 0.514490
[Epoch 64] ogbg-molsider: 0.583239 test loss: 0.542697
[Epoch 65; Iter     6/   36] train: loss: 0.3828372
[Epoch 65; Iter    36/   36] train: loss: 0.3995831
[Epoch 65] ogbg-molsider: 0.587045 val loss: 0.512201
[Epoch 65] ogbg-molsider: 0.584292 test loss: 0.556780
[Epoch 66; Iter    30/   36] train: loss: 0.3819422
[Epoch 66] ogbg-molsider: 0.598965 val loss: 0.513904
[Epoch 66] ogbg-molsider: 0.592919 test loss: 0.552324
[Epoch 67; Iter    24/   36] train: loss: 0.3883826
[Epoch 67] ogbg-molsider: 0.603302 val loss: 0.506020
[Epoch 67] ogbg-molsider: 0.585004 test loss: 0.543520
[Epoch 68; Iter    18/   36] train: loss: 0.3878300
[Epoch 68] ogbg-molsider: 0.630078 val loss: 0.487419
[Epoch 68] ogbg-molsider: 0.589260 test loss: 0.524470
[Epoch 69; Iter    12/   36] train: loss: 0.3943435
[Epoch 69] ogbg-molsider: 0.602582 val loss: 0.526709
[Epoch 69] ogbg-molsider: 0.584828 test loss: 0.563331
[Epoch 70; Iter     6/   36] train: loss: 0.3759248
[Epoch 70; Iter    36/   36] train: loss: 0.4168877
[Epoch 70] ogbg-molsider: 0.607219 val loss: 0.502137
[Epoch 70] ogbg-molsider: 0.585987 test loss: 0.527981
[Epoch 71; Iter    30/   36] train: loss: 0.4023128
[Epoch 71] ogbg-molsider: 0.617553 val loss: 0.510600
[Epoch 71] ogbg-molsider: 0.581973 test loss: 0.570182
[Epoch 72; Iter    24/   36] train: loss: 0.3932273
[Epoch 72] ogbg-molsider: 0.591995 val loss: 0.527928
[Epoch 72] ogbg-molsider: 0.575100 test loss: 0.569668
[Epoch 73; Iter    18/   36] train: loss: 0.4229764
[Epoch 73] ogbg-molsider: 0.617968 val loss: 0.517121
[Epoch 73] ogbg-molsider: 0.576625 test loss: 0.592722
[Epoch 74; Iter    12/   36] train: loss: 0.3592120
[Epoch 74] ogbg-molsider: 0.593871 val loss: 0.525291
[Epoch 74] ogbg-molsider: 0.588357 test loss: 0.564588
[Epoch 75; Iter     6/   36] train: loss: 0.3923704
[Epoch 75; Iter    36/   36] train: loss: 0.4299344
[Epoch 75] ogbg-molsider: 0.625689 val loss: 0.528369
[Epoch 75] ogbg-molsider: 0.569834 test loss: 0.584172
[Epoch 76; Iter    30/   36] train: loss: 0.3697838
[Epoch 76] ogbg-molsider: 0.612250 val loss: 0.510224
[Epoch 76] ogbg-molsider: 0.562424 test loss: 0.563350
[Epoch 77; Iter    24/   36] train: loss: 0.3817094
[Epoch 77] ogbg-molsider: 0.582635 val loss: 0.526027
[Epoch 77] ogbg-molsider: 0.564351 test loss: 0.550763
[Epoch 78; Iter    18/   36] train: loss: 0.4080902
[Epoch 78] ogbg-molsider: 0.612968 val loss: 0.562519
[Epoch 78] ogbg-molsider: 0.574783 test loss: 0.644307
[Epoch 79; Iter    12/   36] train: loss: 0.3588941
[Epoch 79] ogbg-molsider: 0.610829 val loss: 0.532294
[Epoch 79] ogbg-molsider: 0.586671 test loss: 0.583064
[Epoch 80; Iter     6/   36] train: loss: 0.3911848
[Epoch 80; Iter    36/   36] train: loss: 0.3833808
[Epoch 80] ogbg-molsider: 0.634671 val loss: 0.529055
[Epoch 80] ogbg-molsider: 0.582061 test loss: 0.609251
[Epoch 33] ogbg-molsider: 0.597269 test loss: 0.495408
[Epoch 34; Iter    12/   36] train: loss: 0.4855094
[Epoch 34] ogbg-molsider: 0.545883 val loss: 0.487516
[Epoch 34] ogbg-molsider: 0.599926 test loss: 0.491746
[Epoch 35; Iter     6/   36] train: loss: 0.5457836
[Epoch 35; Iter    36/   36] train: loss: 0.4709366
[Epoch 35] ogbg-molsider: 0.571093 val loss: 0.483016
[Epoch 35] ogbg-molsider: 0.605163 test loss: 0.495103
[Epoch 36; Iter    30/   36] train: loss: 0.4503605
[Epoch 36] ogbg-molsider: 0.595608 val loss: 0.480553
[Epoch 36] ogbg-molsider: 0.614611 test loss: 0.495085
[Epoch 37; Iter    24/   36] train: loss: 0.5510948
[Epoch 37] ogbg-molsider: 0.573803 val loss: 0.489325
[Epoch 37] ogbg-molsider: 0.601215 test loss: 0.501233
[Epoch 38; Iter    18/   36] train: loss: 0.5003145
[Epoch 38] ogbg-molsider: 0.584592 val loss: 0.481215
[Epoch 38] ogbg-molsider: 0.601619 test loss: 0.491162
[Epoch 39; Iter    12/   36] train: loss: 0.4917309
[Epoch 39] ogbg-molsider: 0.573819 val loss: 0.485845
[Epoch 39] ogbg-molsider: 0.615600 test loss: 0.500927
[Epoch 40; Iter     6/   36] train: loss: 0.4781269
[Epoch 40; Iter    36/   36] train: loss: 0.5223055
[Epoch 40] ogbg-molsider: 0.595465 val loss: 0.482254
[Epoch 40] ogbg-molsider: 0.583847 test loss: 0.501872
[Epoch 41; Iter    30/   36] train: loss: 0.5209562
[Epoch 41] ogbg-molsider: 0.591817 val loss: 0.480690
[Epoch 41] ogbg-molsider: 0.585193 test loss: 0.493567
[Epoch 42; Iter    24/   36] train: loss: 0.4933637
[Epoch 42] ogbg-molsider: 0.565229 val loss: 0.499526
[Epoch 42] ogbg-molsider: 0.621681 test loss: 0.516668
[Epoch 43; Iter    18/   36] train: loss: 0.4956894
[Epoch 43] ogbg-molsider: 0.578042 val loss: 0.491394
[Epoch 43] ogbg-molsider: 0.591142 test loss: 0.518961
[Epoch 44; Iter    12/   36] train: loss: 0.4915101
[Epoch 44] ogbg-molsider: 0.566135 val loss: 0.495485
[Epoch 44] ogbg-molsider: 0.620170 test loss: 0.498509
[Epoch 45; Iter     6/   36] train: loss: 0.4845520
[Epoch 45; Iter    36/   36] train: loss: 0.4560557
[Epoch 45] ogbg-molsider: 0.550328 val loss: 0.583356
[Epoch 45] ogbg-molsider: 0.602772 test loss: 0.583211
[Epoch 46; Iter    30/   36] train: loss: 0.4899889
[Epoch 46] ogbg-molsider: 0.567540 val loss: 0.478761
[Epoch 46] ogbg-molsider: 0.605336 test loss: 0.495696
[Epoch 47; Iter    24/   36] train: loss: 0.5040683
[Epoch 47] ogbg-molsider: 0.583461 val loss: 0.478013
[Epoch 47] ogbg-molsider: 0.613870 test loss: 0.501175
[Epoch 48; Iter    18/   36] train: loss: 0.4659994
[Epoch 48] ogbg-molsider: 0.567864 val loss: 0.492933
[Epoch 48] ogbg-molsider: 0.630218 test loss: 0.509765
[Epoch 49; Iter    12/   36] train: loss: 0.4710050
[Epoch 49] ogbg-molsider: 0.616625 val loss: 0.476180
[Epoch 49] ogbg-molsider: 0.581022 test loss: 0.507298
[Epoch 50; Iter     6/   36] train: loss: 0.5163227
[Epoch 50; Iter    36/   36] train: loss: 0.5507436
[Epoch 50] ogbg-molsider: 0.559917 val loss: 0.541392
[Epoch 50] ogbg-molsider: 0.565030 test loss: 0.592893
[Epoch 51; Iter    30/   36] train: loss: 0.5342759
[Epoch 51] ogbg-molsider: 0.568473 val loss: 0.489315
[Epoch 51] ogbg-molsider: 0.602984 test loss: 0.507088
[Epoch 52; Iter    24/   36] train: loss: 0.4769329
[Epoch 52] ogbg-molsider: 0.607291 val loss: 0.479800
[Epoch 52] ogbg-molsider: 0.612312 test loss: 0.498962
[Epoch 53; Iter    18/   36] train: loss: 0.4703044
[Epoch 53] ogbg-molsider: 0.614629 val loss: 0.484077
[Epoch 53] ogbg-molsider: 0.598853 test loss: 0.508321
[Epoch 54; Iter    12/   36] train: loss: 0.4824056
[Epoch 54] ogbg-molsider: 0.619371 val loss: 0.477412
[Epoch 54] ogbg-molsider: 0.612237 test loss: 0.496302
[Epoch 55; Iter     6/   36] train: loss: 0.4743524
[Epoch 55; Iter    36/   36] train: loss: 0.4458753
[Epoch 55] ogbg-molsider: 0.598636 val loss: 0.492770
[Epoch 55] ogbg-molsider: 0.597261 test loss: 0.519252
[Epoch 56; Iter    30/   36] train: loss: 0.4436585
[Epoch 56] ogbg-molsider: 0.614874 val loss: 0.487318
[Epoch 56] ogbg-molsider: 0.596780 test loss: 0.516127
[Epoch 57; Iter    24/   36] train: loss: 0.4372514
[Epoch 57] ogbg-molsider: 0.615036 val loss: 0.488649
[Epoch 57] ogbg-molsider: 0.601476 test loss: 0.515024
[Epoch 58; Iter    18/   36] train: loss: 0.4362156
[Epoch 58] ogbg-molsider: 0.619332 val loss: 0.479417
[Epoch 58] ogbg-molsider: 0.590012 test loss: 0.515144
[Epoch 59; Iter    12/   36] train: loss: 0.4472038
[Epoch 59] ogbg-molsider: 0.609108 val loss: 0.597371
[Epoch 59] ogbg-molsider: 0.578096 test loss: 0.676001
[Epoch 60; Iter     6/   36] train: loss: 0.4645629
[Epoch 60; Iter    36/   36] train: loss: 0.4006911
[Epoch 60] ogbg-molsider: 0.583137 val loss: 0.525751
[Epoch 60] ogbg-molsider: 0.606162 test loss: 0.558744
[Epoch 61; Iter    30/   36] train: loss: 0.4046558
[Epoch 61] ogbg-molsider: 0.619061 val loss: 0.499260
[Epoch 61] ogbg-molsider: 0.589346 test loss: 0.538981
[Epoch 62; Iter    24/   36] train: loss: 0.4267903
[Epoch 62] ogbg-molsider: 0.574489 val loss: 0.534113
[Epoch 62] ogbg-molsider: 0.565495 test loss: 0.598689
[Epoch 63; Iter    18/   36] train: loss: 0.4726930
[Epoch 63] ogbg-molsider: 0.617241 val loss: 0.501471
[Epoch 63] ogbg-molsider: 0.572726 test loss: 0.536705
[Epoch 64; Iter    12/   36] train: loss: 0.4088888
[Epoch 64] ogbg-molsider: 0.593525 val loss: 0.524086
[Epoch 64] ogbg-molsider: 0.594073 test loss: 0.574022
[Epoch 65; Iter     6/   36] train: loss: 0.4058783
[Epoch 65; Iter    36/   36] train: loss: 0.5466138
[Epoch 65] ogbg-molsider: 0.555232 val loss: 0.960379
[Epoch 65] ogbg-molsider: 0.607318 test loss: 1.450139
[Epoch 66; Iter    30/   36] train: loss: 0.4455681
[Epoch 66] ogbg-molsider: 0.557575 val loss: 0.598469
[Epoch 66] ogbg-molsider: 0.612807 test loss: 1.012332
[Epoch 67; Iter    24/   36] train: loss: 0.4843024
[Epoch 67] ogbg-molsider: 0.611542 val loss: 0.488067
[Epoch 67] ogbg-molsider: 0.619128 test loss: 0.508920
[Epoch 68; Iter    18/   36] train: loss: 0.4905331
[Epoch 68] ogbg-molsider: 0.572491 val loss: 0.519515
[Epoch 68] ogbg-molsider: 0.612331 test loss: 0.533426
[Epoch 69; Iter    12/   36] train: loss: 0.4549685
[Epoch 69] ogbg-molsider: 0.606957 val loss: 0.483101
[Epoch 69] ogbg-molsider: 0.602531 test loss: 0.510033
[Epoch 70; Iter     6/   36] train: loss: 0.4070316
[Epoch 70; Iter    36/   36] train: loss: 0.4768788
[Epoch 70] ogbg-molsider: 0.614897 val loss: 0.486800
[Epoch 70] ogbg-molsider: 0.613626 test loss: 0.499511
[Epoch 71; Iter    30/   36] train: loss: 0.4522054
[Epoch 71] ogbg-molsider: 0.602299 val loss: 0.508454
[Epoch 71] ogbg-molsider: 0.588359 test loss: 0.546850
[Epoch 72; Iter    24/   36] train: loss: 0.4242524
[Epoch 72] ogbg-molsider: 0.567059 val loss: 0.586137
[Epoch 72] ogbg-molsider: 0.589006 test loss: 0.617753
[Epoch 73; Iter    18/   36] train: loss: 0.4219133
[Epoch 73] ogbg-molsider: 0.587548 val loss: 0.522590
[Epoch 73] ogbg-molsider: 0.606159 test loss: 0.556201
[Epoch 74; Iter    12/   36] train: loss: 0.4123028
[Epoch 74] ogbg-molsider: 0.604109 val loss: 0.505975
[Epoch 74] ogbg-molsider: 0.602180 test loss: 0.520712
[Epoch 75; Iter     6/   36] train: loss: 0.4254484
[Epoch 75; Iter    36/   36] train: loss: 0.5173570
[Epoch 75] ogbg-molsider: 0.576720 val loss: 0.500068
[Epoch 75] ogbg-molsider: 0.597348 test loss: 0.515165
[Epoch 76; Iter    30/   36] train: loss: 0.4924740
[Epoch 76] ogbg-molsider: 0.607808 val loss: 0.496551
[Epoch 76] ogbg-molsider: 0.598870 test loss: 0.529050
[Epoch 77; Iter    24/   36] train: loss: 0.4671274
[Epoch 77] ogbg-molsider: 0.601181 val loss: 0.511286
[Epoch 77] ogbg-molsider: 0.616590 test loss: 0.547355
[Epoch 78; Iter    18/   36] train: loss: 0.4352700
[Epoch 78] ogbg-molsider: 0.624642 val loss: 0.488377
[Epoch 78] ogbg-molsider: 0.602654 test loss: 0.523030
[Epoch 79; Iter    12/   36] train: loss: 0.3837354
[Epoch 79] ogbg-molsider: 0.616559 val loss: 0.505220
[Epoch 79] ogbg-molsider: 0.615186 test loss: 0.527532
[Epoch 80; Iter     6/   36] train: loss: 0.4300856
[Epoch 80; Iter    36/   36] train: loss: 0.3923671
[Epoch 80] ogbg-molsider: 0.620326 val loss: 0.519463
[Epoch 80] ogbg-molsider: 0.582251 test loss: 0.596590
[Epoch 37; Iter    18/   27] train: loss: 0.4642498
[Epoch 37] ogbg-molsider: 0.556300 val loss: 0.519090
[Epoch 37] ogbg-molsider: 0.553682 test loss: 0.499998
[Epoch 38; Iter    21/   27] train: loss: 0.5081457
[Epoch 38] ogbg-molsider: 0.557597 val loss: 0.519689
[Epoch 38] ogbg-molsider: 0.560961 test loss: 0.496469
[Epoch 39; Iter    24/   27] train: loss: 0.5218014
[Epoch 39] ogbg-molsider: 0.557837 val loss: 0.519576
[Epoch 39] ogbg-molsider: 0.563823 test loss: 0.495208
[Epoch 40; Iter    27/   27] train: loss: 0.4872971
[Epoch 40] ogbg-molsider: 0.520902 val loss: 0.531311
[Epoch 40] ogbg-molsider: 0.547403 test loss: 0.512099
[Epoch 41] ogbg-molsider: 0.558165 val loss: 0.515374
[Epoch 41] ogbg-molsider: 0.568982 test loss: 0.503458
[Epoch 42; Iter     3/   27] train: loss: 0.5096183
[Epoch 42] ogbg-molsider: 0.572244 val loss: 0.513638
[Epoch 42] ogbg-molsider: 0.572043 test loss: 0.501721
[Epoch 43; Iter     6/   27] train: loss: 0.5159154
[Epoch 43] ogbg-molsider: 0.572345 val loss: 0.509918
[Epoch 43] ogbg-molsider: 0.571941 test loss: 0.493514
[Epoch 44; Iter     9/   27] train: loss: 0.5078843
[Epoch 44] ogbg-molsider: 0.567948 val loss: 0.524052
[Epoch 44] ogbg-molsider: 0.556475 test loss: 0.526850
[Epoch 45; Iter    12/   27] train: loss: 0.4801036
[Epoch 45] ogbg-molsider: 0.573211 val loss: 0.510910
[Epoch 45] ogbg-molsider: 0.564010 test loss: 0.500202
[Epoch 46; Iter    15/   27] train: loss: 0.4804246
[Epoch 46] ogbg-molsider: 0.573246 val loss: 0.532765
[Epoch 46] ogbg-molsider: 0.558068 test loss: 0.522401
[Epoch 47; Iter    18/   27] train: loss: 0.4697061
[Epoch 47] ogbg-molsider: 0.560845 val loss: 0.547701
[Epoch 47] ogbg-molsider: 0.582394 test loss: 0.505321
[Epoch 48; Iter    21/   27] train: loss: 0.4617584
[Epoch 48] ogbg-molsider: 0.559250 val loss: 0.538970
[Epoch 48] ogbg-molsider: 0.572990 test loss: 0.503292
[Epoch 49; Iter    24/   27] train: loss: 0.4833833
[Epoch 49] ogbg-molsider: 0.564894 val loss: 0.519308
[Epoch 49] ogbg-molsider: 0.574732 test loss: 0.508871
[Epoch 50; Iter    27/   27] train: loss: 0.5220163
[Epoch 50] ogbg-molsider: 0.567958 val loss: 0.546733
[Epoch 50] ogbg-molsider: 0.557352 test loss: 0.538842
[Epoch 51] ogbg-molsider: 0.540431 val loss: 0.750723
[Epoch 51] ogbg-molsider: 0.560070 test loss: 0.615598
[Epoch 52; Iter     3/   27] train: loss: 0.4589712
[Epoch 52] ogbg-molsider: 0.567725 val loss: 0.524852
[Epoch 52] ogbg-molsider: 0.588750 test loss: 0.509029
[Epoch 53; Iter     6/   27] train: loss: 0.5230958
[Epoch 53] ogbg-molsider: 0.560433 val loss: 0.525579
[Epoch 53] ogbg-molsider: 0.573913 test loss: 0.513529
[Epoch 54; Iter     9/   27] train: loss: 0.5621570
[Epoch 54] ogbg-molsider: 0.565129 val loss: 0.550724
[Epoch 54] ogbg-molsider: 0.588850 test loss: 0.525202
[Epoch 55; Iter    12/   27] train: loss: 0.4949301
[Epoch 55] ogbg-molsider: 0.573007 val loss: 0.526047
[Epoch 55] ogbg-molsider: 0.567228 test loss: 0.542762
[Epoch 56; Iter    15/   27] train: loss: 0.4587503
[Epoch 56] ogbg-molsider: 0.568771 val loss: 0.597069
[Epoch 56] ogbg-molsider: 0.574053 test loss: 0.600450
[Epoch 57; Iter    18/   27] train: loss: 0.5027702
[Epoch 57] ogbg-molsider: 0.555504 val loss: 0.586591
[Epoch 57] ogbg-molsider: 0.575867 test loss: 0.568724
[Epoch 58; Iter    21/   27] train: loss: 0.5149402
[Epoch 58] ogbg-molsider: 0.581297 val loss: 0.553483
[Epoch 58] ogbg-molsider: 0.571434 test loss: 0.528272
[Epoch 59; Iter    24/   27] train: loss: 0.5233840
[Epoch 59] ogbg-molsider: 0.563544 val loss: 0.525143
[Epoch 59] ogbg-molsider: 0.581733 test loss: 0.510746
[Epoch 60; Iter    27/   27] train: loss: 0.4619211
[Epoch 60] ogbg-molsider: 0.579055 val loss: 0.526240
[Epoch 60] ogbg-molsider: 0.582941 test loss: 0.531767
[Epoch 61] ogbg-molsider: 0.580889 val loss: 0.535395
[Epoch 61] ogbg-molsider: 0.579068 test loss: 0.562433
[Epoch 62; Iter     3/   27] train: loss: 0.4906803
[Epoch 62] ogbg-molsider: 0.568158 val loss: 0.531887
[Epoch 62] ogbg-molsider: 0.589408 test loss: 0.542472
[Epoch 63; Iter     6/   27] train: loss: 0.4608920
[Epoch 63] ogbg-molsider: 0.563838 val loss: 0.566350
[Epoch 63] ogbg-molsider: 0.583323 test loss: 0.606069
[Epoch 64; Iter     9/   27] train: loss: 0.4528282
[Epoch 64] ogbg-molsider: 0.574225 val loss: 0.546424
[Epoch 64] ogbg-molsider: 0.603162 test loss: 0.531152
[Epoch 65; Iter    12/   27] train: loss: 0.5090989
[Epoch 65] ogbg-molsider: 0.546664 val loss: 0.692207
[Epoch 65] ogbg-molsider: 0.532229 test loss: 0.656313
[Epoch 66; Iter    15/   27] train: loss: 0.5132483
[Epoch 66] ogbg-molsider: 0.568258 val loss: 0.569780
[Epoch 66] ogbg-molsider: 0.575278 test loss: 0.558203
[Epoch 67; Iter    18/   27] train: loss: 0.5014352
[Epoch 67] ogbg-molsider: 0.571913 val loss: 0.513881
[Epoch 67] ogbg-molsider: 0.595448 test loss: 0.494166
[Epoch 68; Iter    21/   27] train: loss: 0.5150200
[Epoch 68] ogbg-molsider: 0.575439 val loss: 0.513571
[Epoch 68] ogbg-molsider: 0.603495 test loss: 0.498861
[Epoch 69; Iter    24/   27] train: loss: 0.4363385
[Epoch 69] ogbg-molsider: 0.579609 val loss: 0.516771
[Epoch 69] ogbg-molsider: 0.607111 test loss: 0.492849
[Epoch 70; Iter    27/   27] train: loss: 0.5317965
[Epoch 70] ogbg-molsider: 0.578888 val loss: 0.549495
[Epoch 70] ogbg-molsider: 0.594142 test loss: 0.573874
[Epoch 71] ogbg-molsider: 0.595786 val loss: 0.515672
[Epoch 71] ogbg-molsider: 0.600454 test loss: 0.505881
[Epoch 72; Iter     3/   27] train: loss: 0.4145703
[Epoch 72] ogbg-molsider: 0.590815 val loss: 0.519899
[Epoch 72] ogbg-molsider: 0.608739 test loss: 0.488348
[Epoch 73; Iter     6/   27] train: loss: 0.4120972
[Epoch 73] ogbg-molsider: 0.589368 val loss: 0.536302
[Epoch 73] ogbg-molsider: 0.607025 test loss: 0.521273
[Epoch 74; Iter     9/   27] train: loss: 0.4920162
[Epoch 74] ogbg-molsider: 0.599167 val loss: 0.521443
[Epoch 74] ogbg-molsider: 0.612818 test loss: 0.495949
[Epoch 75; Iter    12/   27] train: loss: 0.4471000
[Epoch 75] ogbg-molsider: 0.583425 val loss: 0.523525
[Epoch 75] ogbg-molsider: 0.599784 test loss: 0.493036
[Epoch 76; Iter    15/   27] train: loss: 0.4351097
[Epoch 76] ogbg-molsider: 0.594982 val loss: 0.519922
[Epoch 76] ogbg-molsider: 0.580113 test loss: 0.518839
[Epoch 77; Iter    18/   27] train: loss: 0.4341960
[Epoch 77] ogbg-molsider: 0.565312 val loss: 0.534214
[Epoch 77] ogbg-molsider: 0.571611 test loss: 0.540180
[Epoch 78; Iter    21/   27] train: loss: 0.4659199
[Epoch 78] ogbg-molsider: 0.585668 val loss: 0.540693
[Epoch 78] ogbg-molsider: 0.587490 test loss: 0.519837
[Epoch 79; Iter    24/   27] train: loss: 0.4395646
[Epoch 79] ogbg-molsider: 0.584940 val loss: 0.537729
[Epoch 79] ogbg-molsider: 0.587356 test loss: 0.516385
[Epoch 80; Iter    27/   27] train: loss: 0.4239258
[Epoch 80] ogbg-molsider: 0.595614 val loss: 0.523235
[Epoch 80] ogbg-molsider: 0.593492 test loss: 0.509176
[Epoch 81] ogbg-molsider: 0.601784 val loss: 0.534074
[Epoch 81] ogbg-molsider: 0.590485 test loss: 0.545672
[Epoch 82; Iter     3/   27] train: loss: 0.4262570
[Epoch 82] ogbg-molsider: 0.588110 val loss: 0.544114
[Epoch 82] ogbg-molsider: 0.595551 test loss: 0.535381
[Epoch 83; Iter     6/   27] train: loss: 0.4024329
[Epoch 83] ogbg-molsider: 0.606207 val loss: 0.525960
[Epoch 83] ogbg-molsider: 0.597253 test loss: 0.515032
[Epoch 84; Iter     9/   27] train: loss: 0.4484090
[Epoch 84] ogbg-molsider: 0.574640 val loss: 0.578805
[Epoch 84] ogbg-molsider: 0.597962 test loss: 0.553604
[Epoch 85; Iter    12/   27] train: loss: 0.3956915
[Epoch 85] ogbg-molsider: 0.595164 val loss: 0.538528
[Epoch 85] ogbg-molsider: 0.593289 test loss: 0.545526
[Epoch 86; Iter    15/   27] train: loss: 0.4463506
[Epoch 86] ogbg-molsider: 0.587503 val loss: 0.536207
[Epoch 86] ogbg-molsider: 0.596068 test loss: 0.508275
[Epoch 87; Iter    18/   27] train: loss: 0.4602850
[Epoch 87] ogbg-molsider: 0.589390 val loss: 0.540004
[Epoch 87] ogbg-molsider: 0.594369 test loss: 0.524964
[Epoch 88; Iter    21/   27] train: loss: 0.4223913
[Epoch 88] ogbg-molsider: 0.596652 val loss: 0.586792
[Epoch 88] ogbg-molsider: 0.576641 test loss: 0.602060
[Epoch 89; Iter    24/   27] train: loss: 0.4805185
[Epoch 35; Iter    22/   32] train: loss: 0.4906994
[Epoch 35] ogbg-molsider: 0.549095 val loss: 0.502728
[Epoch 35] ogbg-molsider: 0.582402 test loss: 0.519704
[Epoch 36; Iter    20/   32] train: loss: 0.5084488
[Epoch 36] ogbg-molsider: 0.532249 val loss: 0.499521
[Epoch 36] ogbg-molsider: 0.580362 test loss: 0.507194
[Epoch 37; Iter    18/   32] train: loss: 0.4986992
[Epoch 37] ogbg-molsider: 0.547175 val loss: 0.499998
[Epoch 37] ogbg-molsider: 0.592259 test loss: 0.502442
[Epoch 38; Iter    16/   32] train: loss: 0.4735086
[Epoch 38] ogbg-molsider: 0.528229 val loss: 0.496378
[Epoch 38] ogbg-molsider: 0.588686 test loss: 0.510587
[Epoch 39; Iter    14/   32] train: loss: 0.5000473
[Epoch 39] ogbg-molsider: 0.532694 val loss: 0.484336
[Epoch 39] ogbg-molsider: 0.583250 test loss: 0.499558
[Epoch 40; Iter    12/   32] train: loss: 0.4821515
[Epoch 40] ogbg-molsider: 0.526200 val loss: 0.493812
[Epoch 40] ogbg-molsider: 0.592797 test loss: 0.507419
[Epoch 41; Iter    10/   32] train: loss: 0.5046973
[Epoch 41] ogbg-molsider: 0.530991 val loss: 0.491720
[Epoch 41] ogbg-molsider: 0.603240 test loss: 0.496793
[Epoch 42; Iter     8/   32] train: loss: 0.4861335
[Epoch 42] ogbg-molsider: 0.570046 val loss: 0.486115
[Epoch 42] ogbg-molsider: 0.607661 test loss: 0.504431
[Epoch 43; Iter     6/   32] train: loss: 0.4726040
[Epoch 43] ogbg-molsider: 0.563004 val loss: 0.487282
[Epoch 43] ogbg-molsider: 0.594095 test loss: 0.499347
[Epoch 44; Iter     4/   32] train: loss: 0.5006573
[Epoch 44] ogbg-molsider: 0.555001 val loss: 0.490885
[Epoch 44] ogbg-molsider: 0.605861 test loss: 0.498892
[Epoch 45; Iter     2/   32] train: loss: 0.4436709
[Epoch 45; Iter    32/   32] train: loss: 0.5103672
[Epoch 45] ogbg-molsider: 0.535927 val loss: 0.492822
[Epoch 45] ogbg-molsider: 0.598772 test loss: 0.499936
[Epoch 46; Iter    30/   32] train: loss: 0.5172496
[Epoch 46] ogbg-molsider: 0.560491 val loss: 0.484197
[Epoch 46] ogbg-molsider: 0.594272 test loss: 0.499920
[Epoch 47; Iter    28/   32] train: loss: 0.4844097
[Epoch 47] ogbg-molsider: 0.597403 val loss: 0.487553
[Epoch 47] ogbg-molsider: 0.605789 test loss: 0.547007
[Epoch 48; Iter    26/   32] train: loss: 0.5026506
[Epoch 48] ogbg-molsider: 0.545604 val loss: 0.531914
[Epoch 48] ogbg-molsider: 0.603585 test loss: 0.533798
[Epoch 49; Iter    24/   32] train: loss: 0.4658000
[Epoch 49] ogbg-molsider: 0.558658 val loss: 0.490146
[Epoch 49] ogbg-molsider: 0.586253 test loss: 0.516199
[Epoch 50; Iter    22/   32] train: loss: 0.4530275
[Epoch 50] ogbg-molsider: 0.541197 val loss: 0.574828
[Epoch 50] ogbg-molsider: 0.572268 test loss: 0.590460
[Epoch 51; Iter    20/   32] train: loss: 0.4490667
[Epoch 51] ogbg-molsider: 0.570228 val loss: 0.489124
[Epoch 51] ogbg-molsider: 0.580960 test loss: 0.520054
[Epoch 52; Iter    18/   32] train: loss: 0.5109309
[Epoch 52] ogbg-molsider: 0.579881 val loss: 0.484872
[Epoch 52] ogbg-molsider: 0.601323 test loss: 0.522314
[Epoch 53; Iter    16/   32] train: loss: 0.4528967
[Epoch 53] ogbg-molsider: 0.535513 val loss: 0.505896
[Epoch 53] ogbg-molsider: 0.557650 test loss: 0.568269
[Epoch 54; Iter    14/   32] train: loss: 0.5493767
[Epoch 54] ogbg-molsider: 0.580150 val loss: 0.490062
[Epoch 54] ogbg-molsider: 0.603890 test loss: 0.526043
[Epoch 55; Iter    12/   32] train: loss: 0.4736626
[Epoch 55] ogbg-molsider: 0.550722 val loss: 0.486220
[Epoch 55] ogbg-molsider: 0.598392 test loss: 0.497032
[Epoch 56; Iter    10/   32] train: loss: 0.4780439
[Epoch 56] ogbg-molsider: 0.533278 val loss: 0.496064
[Epoch 56] ogbg-molsider: 0.584144 test loss: 0.509628
[Epoch 57; Iter     8/   32] train: loss: 0.5362491
[Epoch 57] ogbg-molsider: 0.557294 val loss: 0.486628
[Epoch 57] ogbg-molsider: 0.599080 test loss: 0.519829
[Epoch 58; Iter     6/   32] train: loss: 0.4666247
[Epoch 58] ogbg-molsider: 0.575345 val loss: 0.483571
[Epoch 58] ogbg-molsider: 0.607219 test loss: 0.516438
[Epoch 59; Iter     4/   32] train: loss: 0.4763222
[Epoch 59] ogbg-molsider: 0.583238 val loss: 0.495346
[Epoch 59] ogbg-molsider: 0.579598 test loss: 0.528879
[Epoch 60; Iter     2/   32] train: loss: 0.4625930
[Epoch 60; Iter    32/   32] train: loss: 0.4056927
[Epoch 60] ogbg-molsider: 0.587667 val loss: 0.480884
[Epoch 60] ogbg-molsider: 0.609933 test loss: 0.507514
[Epoch 61; Iter    30/   32] train: loss: 0.5161696
[Epoch 61] ogbg-molsider: 0.567238 val loss: 0.505993
[Epoch 61] ogbg-molsider: 0.604842 test loss: 0.497897
[Epoch 62; Iter    28/   32] train: loss: 0.4778801
[Epoch 62] ogbg-molsider: 0.590666 val loss: 0.489552
[Epoch 62] ogbg-molsider: 0.626237 test loss: 0.500226
[Epoch 63; Iter    26/   32] train: loss: 0.4657579
[Epoch 63] ogbg-molsider: 0.584444 val loss: 0.491728
[Epoch 63] ogbg-molsider: 0.614650 test loss: 0.507259
[Epoch 64; Iter    24/   32] train: loss: 0.5185578
[Epoch 64] ogbg-molsider: 0.590010 val loss: 0.497739
[Epoch 64] ogbg-molsider: 0.600260 test loss: 0.528157
[Epoch 65; Iter    22/   32] train: loss: 0.4599624
[Epoch 65] ogbg-molsider: 0.579004 val loss: 0.485742
[Epoch 65] ogbg-molsider: 0.613609 test loss: 0.509272
[Epoch 66; Iter    20/   32] train: loss: 0.4602011
[Epoch 66] ogbg-molsider: 0.590953 val loss: 0.498118
[Epoch 66] ogbg-molsider: 0.587089 test loss: 0.528743
[Epoch 67; Iter    18/   32] train: loss: 0.4493385
[Epoch 67] ogbg-molsider: 0.557535 val loss: 0.518977
[Epoch 67] ogbg-molsider: 0.615986 test loss: 0.519772
[Epoch 68; Iter    16/   32] train: loss: 0.4451517
[Epoch 68] ogbg-molsider: 0.599629 val loss: 0.491200
[Epoch 68] ogbg-molsider: 0.599444 test loss: 0.578297
[Epoch 69; Iter    14/   32] train: loss: 0.5348561
[Epoch 69] ogbg-molsider: 0.586017 val loss: 0.489739
[Epoch 69] ogbg-molsider: 0.616502 test loss: 0.519579
[Epoch 70; Iter    12/   32] train: loss: 0.5101605
[Epoch 70] ogbg-molsider: 0.576143 val loss: 0.484692
[Epoch 70] ogbg-molsider: 0.608901 test loss: 0.502120
[Epoch 71; Iter    10/   32] train: loss: 0.3968810
[Epoch 71] ogbg-molsider: 0.592521 val loss: 0.492292
[Epoch 71] ogbg-molsider: 0.622483 test loss: 0.509001
[Epoch 72; Iter     8/   32] train: loss: 0.3737178
[Epoch 72] ogbg-molsider: 0.582599 val loss: 0.494134
[Epoch 72] ogbg-molsider: 0.618999 test loss: 0.542384
[Epoch 73; Iter     6/   32] train: loss: 0.4043598
[Epoch 73] ogbg-molsider: 0.598210 val loss: 0.485667
[Epoch 73] ogbg-molsider: 0.599276 test loss: 0.522497
[Epoch 74; Iter     4/   32] train: loss: 0.4007490
[Epoch 74] ogbg-molsider: 0.585959 val loss: 0.510391
[Epoch 74] ogbg-molsider: 0.613945 test loss: 0.516267
[Epoch 75; Iter     2/   32] train: loss: 0.4181520
[Epoch 75; Iter    32/   32] train: loss: 0.4756531
[Epoch 75] ogbg-molsider: 0.599617 val loss: 0.507571
[Epoch 75] ogbg-molsider: 0.628808 test loss: 0.546554
[Epoch 76; Iter    30/   32] train: loss: 0.4482305
[Epoch 76] ogbg-molsider: 0.594355 val loss: 0.506714
[Epoch 76] ogbg-molsider: 0.598050 test loss: 0.549402
[Epoch 77; Iter    28/   32] train: loss: 0.4170718
[Epoch 77] ogbg-molsider: 0.581850 val loss: 0.542615
[Epoch 77] ogbg-molsider: 0.594984 test loss: 0.581075
[Epoch 78; Iter    26/   32] train: loss: 0.4521860
[Epoch 78] ogbg-molsider: 0.589622 val loss: 0.497884
[Epoch 78] ogbg-molsider: 0.606199 test loss: 0.540672
[Epoch 79; Iter    24/   32] train: loss: 0.3893633
[Epoch 79] ogbg-molsider: 0.575270 val loss: 0.517017
[Epoch 79] ogbg-molsider: 0.601705 test loss: 0.542484
[Epoch 80; Iter    22/   32] train: loss: 0.4466075
[Epoch 80] ogbg-molsider: 0.569840 val loss: 0.543201
[Epoch 80] ogbg-molsider: 0.611773 test loss: 0.534959
[Epoch 81; Iter    20/   32] train: loss: 0.4794399
[Epoch 81] ogbg-molsider: 0.560369 val loss: 0.535059
[Epoch 81] ogbg-molsider: 0.592833 test loss: 0.550371
[Epoch 82; Iter    18/   32] train: loss: 0.4738931
[Epoch 82] ogbg-molsider: 0.568051 val loss: 0.522309
[Epoch 82] ogbg-molsider: 0.606041 test loss: 0.522627
[Epoch 83; Iter    16/   32] train: loss: 0.4017379
[Epoch 83] ogbg-molsider: 0.575627 val loss: 0.531395
[Epoch 83] ogbg-molsider: 0.585917 test loss: 0.549069
[Epoch 84; Iter    14/   32] train: loss: 0.4011076
[Epoch 84] ogbg-molsider: 0.556106 val loss: 0.529300
[Epoch 37; Iter    18/   27] train: loss: 0.4851517
[Epoch 37] ogbg-molsider: 0.553507 val loss: 0.523001
[Epoch 37] ogbg-molsider: 0.563280 test loss: 0.514342
[Epoch 38; Iter    21/   27] train: loss: 0.5675229
[Epoch 38] ogbg-molsider: 0.567901 val loss: 0.513779
[Epoch 38] ogbg-molsider: 0.561304 test loss: 0.496617
[Epoch 39; Iter    24/   27] train: loss: 0.4988434
[Epoch 39] ogbg-molsider: 0.548341 val loss: 0.524697
[Epoch 39] ogbg-molsider: 0.546396 test loss: 0.514066
[Epoch 40; Iter    27/   27] train: loss: 0.5031244
[Epoch 40] ogbg-molsider: 0.562488 val loss: 0.519518
[Epoch 40] ogbg-molsider: 0.563694 test loss: 0.506701
[Epoch 41] ogbg-molsider: 0.558999 val loss: 0.517073
[Epoch 41] ogbg-molsider: 0.557305 test loss: 0.504515
[Epoch 42; Iter     3/   27] train: loss: 0.4945914
[Epoch 42] ogbg-molsider: 0.568119 val loss: 0.515628
[Epoch 42] ogbg-molsider: 0.564735 test loss: 0.504253
[Epoch 43; Iter     6/   27] train: loss: 0.5225162
[Epoch 43] ogbg-molsider: 0.558566 val loss: 0.545539
[Epoch 43] ogbg-molsider: 0.557976 test loss: 0.549232
[Epoch 44; Iter     9/   27] train: loss: 0.4310734
[Epoch 44] ogbg-molsider: 0.557130 val loss: 0.512447
[Epoch 44] ogbg-molsider: 0.573544 test loss: 0.491792
[Epoch 45; Iter    12/   27] train: loss: 0.5269892
[Epoch 45] ogbg-molsider: 0.562356 val loss: 0.521981
[Epoch 45] ogbg-molsider: 0.575414 test loss: 0.490835
[Epoch 46; Iter    15/   27] train: loss: 0.4715453
[Epoch 46] ogbg-molsider: 0.573639 val loss: 0.546011
[Epoch 46] ogbg-molsider: 0.573890 test loss: 0.496439
[Epoch 47; Iter    18/   27] train: loss: 0.4998861
[Epoch 47] ogbg-molsider: 0.579805 val loss: 0.525317
[Epoch 47] ogbg-molsider: 0.578049 test loss: 0.503184
[Epoch 48; Iter    21/   27] train: loss: 0.4412761
[Epoch 48] ogbg-molsider: 0.592697 val loss: 0.552713
[Epoch 48] ogbg-molsider: 0.581443 test loss: 0.505308
[Epoch 49; Iter    24/   27] train: loss: 0.4912900
[Epoch 49] ogbg-molsider: 0.567648 val loss: 0.526573
[Epoch 49] ogbg-molsider: 0.576356 test loss: 0.502174
[Epoch 50; Iter    27/   27] train: loss: 0.5291763
[Epoch 50] ogbg-molsider: 0.571920 val loss: 0.551237
[Epoch 50] ogbg-molsider: 0.580601 test loss: 0.497966
[Epoch 51] ogbg-molsider: 0.563785 val loss: 0.540864
[Epoch 51] ogbg-molsider: 0.576426 test loss: 0.501418
[Epoch 52; Iter     3/   27] train: loss: 0.4877661
[Epoch 52] ogbg-molsider: 0.560826 val loss: 0.537798
[Epoch 52] ogbg-molsider: 0.574361 test loss: 0.545030
[Epoch 53; Iter     6/   27] train: loss: 0.4231694
[Epoch 53] ogbg-molsider: 0.561473 val loss: 0.531820
[Epoch 53] ogbg-molsider: 0.584746 test loss: 0.525173
[Epoch 54; Iter     9/   27] train: loss: 0.4732648
[Epoch 54] ogbg-molsider: 0.557425 val loss: 0.541054
[Epoch 54] ogbg-molsider: 0.586398 test loss: 0.518588
[Epoch 55; Iter    12/   27] train: loss: 0.5218442
[Epoch 55] ogbg-molsider: 0.530321 val loss: 0.530795
[Epoch 55] ogbg-molsider: 0.562507 test loss: 0.513282
[Epoch 56; Iter    15/   27] train: loss: 0.4785801
[Epoch 56] ogbg-molsider: 0.564563 val loss: 0.533793
[Epoch 56] ogbg-molsider: 0.575275 test loss: 0.523245
[Epoch 57; Iter    18/   27] train: loss: 0.4866944
[Epoch 57] ogbg-molsider: 0.570414 val loss: 0.593696
[Epoch 57] ogbg-molsider: 0.583474 test loss: 0.522704
[Epoch 58; Iter    21/   27] train: loss: 0.4892468
[Epoch 58] ogbg-molsider: 0.578295 val loss: 0.522738
[Epoch 58] ogbg-molsider: 0.586073 test loss: 0.522537
[Epoch 59; Iter    24/   27] train: loss: 0.5138575
[Epoch 59] ogbg-molsider: 0.586236 val loss: 0.526030
[Epoch 59] ogbg-molsider: 0.568074 test loss: 0.555594
[Epoch 60; Iter    27/   27] train: loss: 0.5268886
[Epoch 60] ogbg-molsider: 0.573479 val loss: 0.524716
[Epoch 60] ogbg-molsider: 0.570304 test loss: 0.519226
[Epoch 61] ogbg-molsider: 0.574143 val loss: 0.520314
[Epoch 61] ogbg-molsider: 0.576048 test loss: 0.505003
[Epoch 62; Iter     3/   27] train: loss: 0.5288641
[Epoch 62] ogbg-molsider: 0.587223 val loss: 0.512584
[Epoch 62] ogbg-molsider: 0.618009 test loss: 0.496014
[Epoch 63; Iter     6/   27] train: loss: 0.4440898
[Epoch 63] ogbg-molsider: 0.567199 val loss: 0.517935
[Epoch 63] ogbg-molsider: 0.596452 test loss: 0.492991
[Epoch 64; Iter     9/   27] train: loss: 0.4731691
[Epoch 64] ogbg-molsider: 0.589049 val loss: 0.523305
[Epoch 64] ogbg-molsider: 0.591711 test loss: 0.511294
[Epoch 65; Iter    12/   27] train: loss: 0.4733806
[Epoch 65] ogbg-molsider: 0.579256 val loss: 0.523798
[Epoch 65] ogbg-molsider: 0.590397 test loss: 0.525808
[Epoch 66; Iter    15/   27] train: loss: 0.5059633
[Epoch 66] ogbg-molsider: 0.591649 val loss: 0.537311
[Epoch 66] ogbg-molsider: 0.590068 test loss: 0.515540
[Epoch 67; Iter    18/   27] train: loss: 0.4082455
[Epoch 67] ogbg-molsider: 0.593735 val loss: 0.546134
[Epoch 67] ogbg-molsider: 0.604280 test loss: 0.493055
[Epoch 68; Iter    21/   27] train: loss: 0.4561874
[Epoch 68] ogbg-molsider: 0.607432 val loss: 0.522959
[Epoch 68] ogbg-molsider: 0.601992 test loss: 0.505063
[Epoch 69; Iter    24/   27] train: loss: 0.4165483
[Epoch 69] ogbg-molsider: 0.587564 val loss: 0.525980
[Epoch 69] ogbg-molsider: 0.574515 test loss: 0.503999
[Epoch 70; Iter    27/   27] train: loss: 0.4227235
[Epoch 70] ogbg-molsider: 0.590935 val loss: 0.521765
[Epoch 70] ogbg-molsider: 0.595988 test loss: 0.507263
[Epoch 71] ogbg-molsider: 0.585967 val loss: 0.528091
[Epoch 71] ogbg-molsider: 0.599241 test loss: 0.514129
[Epoch 72; Iter     3/   27] train: loss: 0.4113447
[Epoch 72] ogbg-molsider: 0.589518 val loss: 0.520691
[Epoch 72] ogbg-molsider: 0.602934 test loss: 0.512088
[Epoch 73; Iter     6/   27] train: loss: 0.3785814
[Epoch 73] ogbg-molsider: 0.585224 val loss: 0.565353
[Epoch 73] ogbg-molsider: 0.562870 test loss: 0.585020
[Epoch 74; Iter     9/   27] train: loss: 0.4307123
[Epoch 74] ogbg-molsider: 0.593758 val loss: 0.553551
[Epoch 74] ogbg-molsider: 0.571573 test loss: 0.563201
[Epoch 75; Iter    12/   27] train: loss: 0.4461771
[Epoch 75] ogbg-molsider: 0.574777 val loss: 0.544913
[Epoch 75] ogbg-molsider: 0.578725 test loss: 0.529828
[Epoch 76; Iter    15/   27] train: loss: 0.4507668
[Epoch 76] ogbg-molsider: 0.594437 val loss: 0.530587
[Epoch 76] ogbg-molsider: 0.577961 test loss: 0.526109
[Epoch 77; Iter    18/   27] train: loss: 0.4211461
[Epoch 77] ogbg-molsider: 0.576862 val loss: 0.535177
[Epoch 77] ogbg-molsider: 0.583358 test loss: 0.539682
[Epoch 78; Iter    21/   27] train: loss: 0.4335848
[Epoch 78] ogbg-molsider: 0.598116 val loss: 0.540725
[Epoch 78] ogbg-molsider: 0.572885 test loss: 0.581006
[Epoch 79; Iter    24/   27] train: loss: 0.4258469
[Epoch 79] ogbg-molsider: 0.603903 val loss: 0.526532
[Epoch 79] ogbg-molsider: 0.591106 test loss: 0.516594
[Epoch 80; Iter    27/   27] train: loss: 0.4546312
[Epoch 80] ogbg-molsider: 0.575569 val loss: 0.564643
[Epoch 80] ogbg-molsider: 0.593519 test loss: 0.582976
[Epoch 81] ogbg-molsider: 0.569759 val loss: 0.562568
[Epoch 81] ogbg-molsider: 0.570191 test loss: 0.578557
[Epoch 82; Iter     3/   27] train: loss: 0.3890332
[Epoch 82] ogbg-molsider: 0.586403 val loss: 0.550158
[Epoch 82] ogbg-molsider: 0.586963 test loss: 0.529517
[Epoch 83; Iter     6/   27] train: loss: 0.4200357
[Epoch 83] ogbg-molsider: 0.586778 val loss: 0.563834
[Epoch 83] ogbg-molsider: 0.572029 test loss: 0.625422
[Epoch 84; Iter     9/   27] train: loss: 0.4598314
[Epoch 84] ogbg-molsider: 0.594465 val loss: 0.574400
[Epoch 84] ogbg-molsider: 0.551540 test loss: 0.626713
[Epoch 85; Iter    12/   27] train: loss: 0.4270374
[Epoch 85] ogbg-molsider: 0.580675 val loss: 0.611713
[Epoch 85] ogbg-molsider: 0.572806 test loss: 0.542604
[Epoch 86; Iter    15/   27] train: loss: 0.4279636
[Epoch 86] ogbg-molsider: 0.581608 val loss: 0.719456
[Epoch 86] ogbg-molsider: 0.575092 test loss: 0.729310
[Epoch 87; Iter    18/   27] train: loss: 0.4115265
[Epoch 87] ogbg-molsider: 0.600871 val loss: 0.592128
[Epoch 87] ogbg-molsider: 0.565648 test loss: 0.637164
[Epoch 88; Iter    21/   27] train: loss: 0.4543564
[Epoch 88] ogbg-molsider: 0.576814 val loss: 0.712769
[Epoch 88] ogbg-molsider: 0.577721 test loss: 0.686094
[Epoch 89; Iter    24/   27] train: loss: 0.3723675
[Epoch 33] ogbg-molsider: 0.606712 test loss: 0.490195
[Epoch 34; Iter    12/   36] train: loss: 0.5031945
[Epoch 34] ogbg-molsider: 0.593583 val loss: 0.478628
[Epoch 34] ogbg-molsider: 0.590959 test loss: 0.500398
[Epoch 35; Iter     6/   36] train: loss: 0.4920425
[Epoch 35; Iter    36/   36] train: loss: 0.5246127
[Epoch 35] ogbg-molsider: 0.575065 val loss: 0.491257
[Epoch 35] ogbg-molsider: 0.606498 test loss: 0.502237
[Epoch 36; Iter    30/   36] train: loss: 0.4686493
[Epoch 36] ogbg-molsider: 0.572235 val loss: 0.485523
[Epoch 36] ogbg-molsider: 0.610795 test loss: 0.499547
[Epoch 37; Iter    24/   36] train: loss: 0.4951315
[Epoch 37] ogbg-molsider: 0.587925 val loss: 0.478505
[Epoch 37] ogbg-molsider: 0.606757 test loss: 0.489643
[Epoch 38; Iter    18/   36] train: loss: 0.4733518
[Epoch 38] ogbg-molsider: 0.598528 val loss: 0.478354
[Epoch 38] ogbg-molsider: 0.605020 test loss: 0.492548
[Epoch 39; Iter    12/   36] train: loss: 0.5238388
[Epoch 39] ogbg-molsider: 0.575512 val loss: 0.481452
[Epoch 39] ogbg-molsider: 0.616218 test loss: 0.487645
[Epoch 40; Iter     6/   36] train: loss: 0.5347266
[Epoch 40; Iter    36/   36] train: loss: 0.4678488
[Epoch 40] ogbg-molsider: 0.569997 val loss: 0.495380
[Epoch 40] ogbg-molsider: 0.617531 test loss: 0.511129
[Epoch 41; Iter    30/   36] train: loss: 0.5242606
[Epoch 41] ogbg-molsider: 0.596609 val loss: 0.500509
[Epoch 41] ogbg-molsider: 0.590839 test loss: 0.498990
[Epoch 42; Iter    24/   36] train: loss: 0.4496145
[Epoch 42] ogbg-molsider: 0.609504 val loss: 0.477110
[Epoch 42] ogbg-molsider: 0.589959 test loss: 0.499520
[Epoch 43; Iter    18/   36] train: loss: 0.5101535
[Epoch 43] ogbg-molsider: 0.590858 val loss: 0.504617
[Epoch 43] ogbg-molsider: 0.580978 test loss: 0.542957
[Epoch 44; Iter    12/   36] train: loss: 0.4904064
[Epoch 44] ogbg-molsider: 0.588330 val loss: 0.496654
[Epoch 44] ogbg-molsider: 0.594215 test loss: 0.511181
[Epoch 45; Iter     6/   36] train: loss: 0.4574571
[Epoch 45; Iter    36/   36] train: loss: 0.4633884
[Epoch 45] ogbg-molsider: 0.638854 val loss: 0.471715
[Epoch 45] ogbg-molsider: 0.601123 test loss: 0.501997
[Epoch 46; Iter    30/   36] train: loss: 0.4725792
[Epoch 46] ogbg-molsider: 0.571182 val loss: 0.498117
[Epoch 46] ogbg-molsider: 0.605853 test loss: 0.498383
[Epoch 47; Iter    24/   36] train: loss: 0.4406542
[Epoch 47] ogbg-molsider: 0.570251 val loss: 0.507002
[Epoch 47] ogbg-molsider: 0.568368 test loss: 0.538610
[Epoch 48; Iter    18/   36] train: loss: 0.5400532
[Epoch 48] ogbg-molsider: 0.579483 val loss: 0.491358
[Epoch 48] ogbg-molsider: 0.586949 test loss: 0.500966
[Epoch 49; Iter    12/   36] train: loss: 0.4518847
[Epoch 49] ogbg-molsider: 0.589166 val loss: 0.505515
[Epoch 49] ogbg-molsider: 0.588149 test loss: 0.537116
[Epoch 50; Iter     6/   36] train: loss: 0.4611495
[Epoch 50; Iter    36/   36] train: loss: 0.4840206
[Epoch 50] ogbg-molsider: 0.588857 val loss: 0.486558
[Epoch 50] ogbg-molsider: 0.599366 test loss: 0.509247
[Epoch 51; Iter    30/   36] train: loss: 0.4401727
[Epoch 51] ogbg-molsider: 0.572385 val loss: 0.500866
[Epoch 51] ogbg-molsider: 0.600137 test loss: 0.513700
[Epoch 52; Iter    24/   36] train: loss: 0.4337056
[Epoch 52] ogbg-molsider: 0.582231 val loss: 0.507363
[Epoch 52] ogbg-molsider: 0.602448 test loss: 0.512839
[Epoch 53; Iter    18/   36] train: loss: 0.4651290
[Epoch 53] ogbg-molsider: 0.561519 val loss: 0.509679
[Epoch 53] ogbg-molsider: 0.561713 test loss: 0.532291
[Epoch 54; Iter    12/   36] train: loss: 0.4339281
[Epoch 54] ogbg-molsider: 0.592614 val loss: 0.484137
[Epoch 54] ogbg-molsider: 0.600337 test loss: 0.502936
[Epoch 55; Iter     6/   36] train: loss: 0.4115709
[Epoch 55; Iter    36/   36] train: loss: 0.5028076
[Epoch 55] ogbg-molsider: 0.597897 val loss: 0.528540
[Epoch 55] ogbg-molsider: 0.593210 test loss: 0.566393
[Epoch 56; Iter    30/   36] train: loss: 0.4459999
[Epoch 56] ogbg-molsider: 0.632431 val loss: 0.489036
[Epoch 56] ogbg-molsider: 0.584710 test loss: 0.516261
[Epoch 57; Iter    24/   36] train: loss: 0.3985216
[Epoch 57] ogbg-molsider: 0.613908 val loss: 0.491334
[Epoch 57] ogbg-molsider: 0.585468 test loss: 0.527107
[Epoch 58; Iter    18/   36] train: loss: 0.4309219
[Epoch 58] ogbg-molsider: 0.589173 val loss: 0.509007
[Epoch 58] ogbg-molsider: 0.608003 test loss: 0.510909
[Epoch 59; Iter    12/   36] train: loss: 0.4184252
[Epoch 59] ogbg-molsider: 0.604708 val loss: 0.570265
[Epoch 59] ogbg-molsider: 0.601549 test loss: 0.553290
[Epoch 60; Iter     6/   36] train: loss: 0.4391380
[Epoch 60; Iter    36/   36] train: loss: 0.4693292
[Epoch 60] ogbg-molsider: 0.608869 val loss: 0.520091
[Epoch 60] ogbg-molsider: 0.588408 test loss: 0.552202
[Epoch 61; Iter    30/   36] train: loss: 0.4508210
[Epoch 61] ogbg-molsider: 0.605415 val loss: 0.498844
[Epoch 61] ogbg-molsider: 0.600602 test loss: 0.534928
[Epoch 62; Iter    24/   36] train: loss: 0.4162078
[Epoch 62] ogbg-molsider: 0.592026 val loss: 0.534308
[Epoch 62] ogbg-molsider: 0.602566 test loss: 0.569833
[Epoch 63; Iter    18/   36] train: loss: 0.4532910
[Epoch 63] ogbg-molsider: 0.638218 val loss: 0.503855
[Epoch 63] ogbg-molsider: 0.587172 test loss: 0.536326
[Epoch 64; Iter    12/   36] train: loss: 0.3988233
[Epoch 64] ogbg-molsider: 0.580439 val loss: 0.518636
[Epoch 64] ogbg-molsider: 0.608151 test loss: 0.541684
[Epoch 65; Iter     6/   36] train: loss: 0.4035671
[Epoch 65; Iter    36/   36] train: loss: 0.4145777
[Epoch 65] ogbg-molsider: 0.602153 val loss: 0.505660
[Epoch 65] ogbg-molsider: 0.590232 test loss: 0.541305
[Epoch 66; Iter    30/   36] train: loss: 0.3816334
[Epoch 66] ogbg-molsider: 0.610158 val loss: 0.523749
[Epoch 66] ogbg-molsider: 0.596466 test loss: 0.560773
[Epoch 67; Iter    24/   36] train: loss: 0.3780785
[Epoch 67] ogbg-molsider: 0.609097 val loss: 0.528272
[Epoch 67] ogbg-molsider: 0.604189 test loss: 0.561675
[Epoch 68; Iter    18/   36] train: loss: 0.3484277
[Epoch 68] ogbg-molsider: 0.619174 val loss: 0.513465
[Epoch 68] ogbg-molsider: 0.604498 test loss: 0.556018
[Epoch 69; Iter    12/   36] train: loss: 0.3631628
[Epoch 69] ogbg-molsider: 0.641688 val loss: 0.496158
[Epoch 69] ogbg-molsider: 0.614015 test loss: 0.532723
[Epoch 70; Iter     6/   36] train: loss: 0.3313563
[Epoch 70; Iter    36/   36] train: loss: 0.4058430
[Epoch 70] ogbg-molsider: 0.604505 val loss: 0.550281
[Epoch 70] ogbg-molsider: 0.593222 test loss: 0.587184
[Epoch 71; Iter    30/   36] train: loss: 0.3788905
[Epoch 71] ogbg-molsider: 0.628757 val loss: 0.502379
[Epoch 71] ogbg-molsider: 0.590353 test loss: 0.575813
[Epoch 72; Iter    24/   36] train: loss: 0.3860427
[Epoch 72] ogbg-molsider: 0.618052 val loss: 0.517969
[Epoch 72] ogbg-molsider: 0.619728 test loss: 0.553279
[Epoch 73; Iter    18/   36] train: loss: 0.3805830
[Epoch 73] ogbg-molsider: 0.635601 val loss: 0.521108
[Epoch 73] ogbg-molsider: 0.584868 test loss: 0.610193
[Epoch 74; Iter    12/   36] train: loss: 0.3795007
[Epoch 74] ogbg-molsider: 0.597034 val loss: 0.554091
[Epoch 74] ogbg-molsider: 0.568588 test loss: 0.598084
[Epoch 75; Iter     6/   36] train: loss: 0.3298406
[Epoch 75; Iter    36/   36] train: loss: 0.3784290
[Epoch 75] ogbg-molsider: 0.585269 val loss: 0.589093
[Epoch 75] ogbg-molsider: 0.591498 test loss: 0.629915
[Epoch 76; Iter    30/   36] train: loss: 0.3531488
[Epoch 76] ogbg-molsider: 0.612237 val loss: 0.540809
[Epoch 76] ogbg-molsider: 0.570520 test loss: 0.583560
[Epoch 77; Iter    24/   36] train: loss: 0.3589747
[Epoch 77] ogbg-molsider: 0.606433 val loss: 0.538368
[Epoch 77] ogbg-molsider: 0.597461 test loss: 0.572123
[Epoch 78; Iter    18/   36] train: loss: 0.3715202
[Epoch 78] ogbg-molsider: 0.608338 val loss: 0.575246
[Epoch 78] ogbg-molsider: 0.587965 test loss: 0.611182
[Epoch 79; Iter    12/   36] train: loss: 0.3404821
[Epoch 79] ogbg-molsider: 0.616287 val loss: 0.577266
[Epoch 79] ogbg-molsider: 0.580156 test loss: 0.622808
[Epoch 80; Iter     6/   36] train: loss: 0.3342263
[Epoch 80; Iter    36/   36] train: loss: 0.3618311
[Epoch 80] ogbg-molsider: 0.622643 val loss: 0.550144
[Epoch 80] ogbg-molsider: 0.573749 test loss: 0.607837
[Epoch 37; Iter    18/   27] train: loss: 0.4796047
[Epoch 37] ogbg-molsider: 0.574222 val loss: 0.511226
[Epoch 37] ogbg-molsider: 0.566938 test loss: 0.493477
[Epoch 38; Iter    21/   27] train: loss: 0.4979703
[Epoch 38] ogbg-molsider: 0.546890 val loss: 0.524215
[Epoch 38] ogbg-molsider: 0.560673 test loss: 0.517365
[Epoch 39; Iter    24/   27] train: loss: 0.4941798
[Epoch 39] ogbg-molsider: 0.561445 val loss: 0.518064
[Epoch 39] ogbg-molsider: 0.548192 test loss: 0.511434
[Epoch 40; Iter    27/   27] train: loss: 0.5006899
[Epoch 40] ogbg-molsider: 0.564550 val loss: 0.510724
[Epoch 40] ogbg-molsider: 0.572597 test loss: 0.492907
[Epoch 41] ogbg-molsider: 0.561941 val loss: 0.510451
[Epoch 41] ogbg-molsider: 0.572217 test loss: 0.491005
[Epoch 42; Iter     3/   27] train: loss: 0.4759026
[Epoch 42] ogbg-molsider: 0.559043 val loss: 0.510686
[Epoch 42] ogbg-molsider: 0.565507 test loss: 0.495090
[Epoch 43; Iter     6/   27] train: loss: 0.4914390
[Epoch 43] ogbg-molsider: 0.573139 val loss: 0.515730
[Epoch 43] ogbg-molsider: 0.582992 test loss: 0.503754
[Epoch 44; Iter     9/   27] train: loss: 0.4952762
[Epoch 44] ogbg-molsider: 0.567607 val loss: 0.522323
[Epoch 44] ogbg-molsider: 0.563600 test loss: 0.512352
[Epoch 45; Iter    12/   27] train: loss: 0.4762890
[Epoch 45] ogbg-molsider: 0.561934 val loss: 0.513830
[Epoch 45] ogbg-molsider: 0.577615 test loss: 0.491527
[Epoch 46; Iter    15/   27] train: loss: 0.5393020
[Epoch 46] ogbg-molsider: 0.574287 val loss: 0.514553
[Epoch 46] ogbg-molsider: 0.570257 test loss: 0.499254
[Epoch 47; Iter    18/   27] train: loss: 0.5223996
[Epoch 47] ogbg-molsider: 0.561598 val loss: 0.512577
[Epoch 47] ogbg-molsider: 0.572776 test loss: 0.496621
[Epoch 48; Iter    21/   27] train: loss: 0.4301306
[Epoch 48] ogbg-molsider: 0.576479 val loss: 0.514583
[Epoch 48] ogbg-molsider: 0.581204 test loss: 0.504653
[Epoch 49; Iter    24/   27] train: loss: 0.4823391
[Epoch 49] ogbg-molsider: 0.586388 val loss: 0.506622
[Epoch 49] ogbg-molsider: 0.583235 test loss: 0.496207
[Epoch 50; Iter    27/   27] train: loss: 0.4472673
[Epoch 50] ogbg-molsider: 0.586446 val loss: 0.514770
[Epoch 50] ogbg-molsider: 0.575297 test loss: 0.505727
[Epoch 51] ogbg-molsider: 0.575802 val loss: 0.511752
[Epoch 51] ogbg-molsider: 0.581552 test loss: 0.496119
[Epoch 52; Iter     3/   27] train: loss: 0.5133799
[Epoch 52] ogbg-molsider: 0.578932 val loss: 0.514110
[Epoch 52] ogbg-molsider: 0.572459 test loss: 0.516093
[Epoch 53; Iter     6/   27] train: loss: 0.4940899
[Epoch 53] ogbg-molsider: 0.571095 val loss: 0.517555
[Epoch 53] ogbg-molsider: 0.579940 test loss: 0.497931
[Epoch 54; Iter     9/   27] train: loss: 0.4477260
[Epoch 54] ogbg-molsider: 0.563978 val loss: 0.571856
[Epoch 54] ogbg-molsider: 0.554926 test loss: 0.608710
[Epoch 55; Iter    12/   27] train: loss: 0.4362888
[Epoch 55] ogbg-molsider: 0.547579 val loss: 0.546014
[Epoch 55] ogbg-molsider: 0.583775 test loss: 0.527088
[Epoch 56; Iter    15/   27] train: loss: 0.4477177
[Epoch 56] ogbg-molsider: 0.579291 val loss: 0.523831
[Epoch 56] ogbg-molsider: 0.597102 test loss: 0.499798
[Epoch 57; Iter    18/   27] train: loss: 0.4818032
[Epoch 57] ogbg-molsider: 0.565414 val loss: 0.528685
[Epoch 57] ogbg-molsider: 0.577496 test loss: 0.512921
[Epoch 58; Iter    21/   27] train: loss: 0.4341511
[Epoch 58] ogbg-molsider: 0.580118 val loss: 0.572595
[Epoch 58] ogbg-molsider: 0.580047 test loss: 0.527950
[Epoch 59; Iter    24/   27] train: loss: 0.4873585
[Epoch 59] ogbg-molsider: 0.565749 val loss: 0.528106
[Epoch 59] ogbg-molsider: 0.572486 test loss: 0.517950
[Epoch 60; Iter    27/   27] train: loss: 0.4718540
[Epoch 60] ogbg-molsider: 0.578187 val loss: 0.528571
[Epoch 60] ogbg-molsider: 0.565613 test loss: 0.540470
[Epoch 61] ogbg-molsider: 0.574577 val loss: 0.582643
[Epoch 61] ogbg-molsider: 0.573176 test loss: 0.612091
[Epoch 62; Iter     3/   27] train: loss: 0.4990921
[Epoch 62] ogbg-molsider: 0.558143 val loss: 0.554245
[Epoch 62] ogbg-molsider: 0.555076 test loss: 0.573831
[Epoch 63; Iter     6/   27] train: loss: 0.4904675
[Epoch 63] ogbg-molsider: 0.549887 val loss: 0.523212
[Epoch 63] ogbg-molsider: 0.586069 test loss: 0.508997
[Epoch 64; Iter     9/   27] train: loss: 0.4687643
[Epoch 64] ogbg-molsider: 0.573451 val loss: 0.595343
[Epoch 64] ogbg-molsider: 0.570414 test loss: 0.574882
[Epoch 65; Iter    12/   27] train: loss: 0.4393706
[Epoch 65] ogbg-molsider: 0.576428 val loss: 0.538615
[Epoch 65] ogbg-molsider: 0.580251 test loss: 0.515213
[Epoch 66; Iter    15/   27] train: loss: 0.4864284
[Epoch 66] ogbg-molsider: 0.592445 val loss: 0.544603
[Epoch 66] ogbg-molsider: 0.589141 test loss: 0.540115
[Epoch 67; Iter    18/   27] train: loss: 0.4774163
[Epoch 67] ogbg-molsider: 0.582926 val loss: 0.546697
[Epoch 67] ogbg-molsider: 0.559168 test loss: 0.524295
[Epoch 68; Iter    21/   27] train: loss: 0.5197122
[Epoch 68] ogbg-molsider: 0.583772 val loss: 0.629120
[Epoch 68] ogbg-molsider: 0.576830 test loss: 0.537055
[Epoch 69; Iter    24/   27] train: loss: 0.4273504
[Epoch 69] ogbg-molsider: 0.571227 val loss: 0.532136
[Epoch 69] ogbg-molsider: 0.563396 test loss: 0.526212
[Epoch 70; Iter    27/   27] train: loss: 0.4417092
[Epoch 70] ogbg-molsider: 0.574768 val loss: 0.639279
[Epoch 70] ogbg-molsider: 0.588369 test loss: 0.524070
[Epoch 71] ogbg-molsider: 0.577304 val loss: 0.538566
[Epoch 71] ogbg-molsider: 0.595497 test loss: 0.510757
[Epoch 72; Iter     3/   27] train: loss: 0.4188845
[Epoch 72] ogbg-molsider: 0.572655 val loss: 0.568390
[Epoch 72] ogbg-molsider: 0.578867 test loss: 0.543592
[Epoch 73; Iter     6/   27] train: loss: 0.4118212
[Epoch 73] ogbg-molsider: 0.582814 val loss: 0.538963
[Epoch 73] ogbg-molsider: 0.592597 test loss: 0.511477
[Epoch 74; Iter     9/   27] train: loss: 0.4257436
[Epoch 74] ogbg-molsider: 0.578946 val loss: 0.539144
[Epoch 74] ogbg-molsider: 0.580059 test loss: 0.532817
[Epoch 75; Iter    12/   27] train: loss: 0.4550212
[Epoch 75] ogbg-molsider: 0.574002 val loss: 0.621692
[Epoch 75] ogbg-molsider: 0.570845 test loss: 0.596552
[Epoch 76; Iter    15/   27] train: loss: 0.4446916
[Epoch 76] ogbg-molsider: 0.572292 val loss: 0.547994
[Epoch 76] ogbg-molsider: 0.573086 test loss: 0.534025
[Epoch 77; Iter    18/   27] train: loss: 0.4262665
[Epoch 77] ogbg-molsider: 0.579725 val loss: 0.568889
[Epoch 77] ogbg-molsider: 0.561548 test loss: 0.549491
[Epoch 78; Iter    21/   27] train: loss: 0.4164622
[Epoch 78] ogbg-molsider: 0.575082 val loss: 0.673526
[Epoch 78] ogbg-molsider: 0.587954 test loss: 0.535784
[Epoch 79; Iter    24/   27] train: loss: 0.4175923
[Epoch 79] ogbg-molsider: 0.567869 val loss: 0.560991
[Epoch 79] ogbg-molsider: 0.595024 test loss: 0.529226
[Epoch 80; Iter    27/   27] train: loss: 0.3968154
[Epoch 80] ogbg-molsider: 0.567784 val loss: 0.554045
[Epoch 80] ogbg-molsider: 0.575525 test loss: 0.534722
[Epoch 81] ogbg-molsider: 0.562636 val loss: 0.569385
[Epoch 81] ogbg-molsider: 0.570563 test loss: 0.558486
[Epoch 82; Iter     3/   27] train: loss: 0.3519678
[Epoch 82] ogbg-molsider: 0.578325 val loss: 0.576352
[Epoch 82] ogbg-molsider: 0.587507 test loss: 0.548418
[Epoch 83; Iter     6/   27] train: loss: 0.3643640
[Epoch 83] ogbg-molsider: 0.587141 val loss: 0.570560
[Epoch 83] ogbg-molsider: 0.580969 test loss: 0.544490
[Epoch 84; Iter     9/   27] train: loss: 0.3786989
[Epoch 84] ogbg-molsider: 0.562562 val loss: 0.585703
[Epoch 84] ogbg-molsider: 0.585898 test loss: 0.548580
[Epoch 85; Iter    12/   27] train: loss: 0.4357694
[Epoch 85] ogbg-molsider: 0.583430 val loss: 0.590596
[Epoch 85] ogbg-molsider: 0.578464 test loss: 0.574832
[Epoch 86; Iter    15/   27] train: loss: 0.3869991
[Epoch 86] ogbg-molsider: 0.558160 val loss: 0.610133
[Epoch 86] ogbg-molsider: 0.562361 test loss: 0.590936
[Epoch 87; Iter    18/   27] train: loss: 0.3636255
[Epoch 87] ogbg-molsider: 0.580408 val loss: 0.608903
[Epoch 87] ogbg-molsider: 0.568873 test loss: 0.588413
[Epoch 88; Iter    21/   27] train: loss: 0.3966134
[Epoch 88] ogbg-molsider: 0.576727 val loss: 0.571563
[Epoch 88] ogbg-molsider: 0.589788 test loss: 0.547743
[Epoch 89; Iter    24/   27] train: loss: 0.4118125
[Epoch 35; Iter    22/   32] train: loss: 0.5121059
[Epoch 35] ogbg-molsider: 0.550968 val loss: 0.612245
[Epoch 35] ogbg-molsider: 0.578464 test loss: 0.499446
[Epoch 36; Iter    20/   32] train: loss: 0.4777950
[Epoch 36] ogbg-molsider: 0.549771 val loss: 0.560771
[Epoch 36] ogbg-molsider: 0.590135 test loss: 0.502330
[Epoch 37; Iter    18/   32] train: loss: 0.5229733
[Epoch 37] ogbg-molsider: 0.541479 val loss: 0.584756
[Epoch 37] ogbg-molsider: 0.569726 test loss: 0.517600
[Epoch 38; Iter    16/   32] train: loss: 0.4895190
[Epoch 38] ogbg-molsider: 0.546028 val loss: 0.566705
[Epoch 38] ogbg-molsider: 0.573026 test loss: 0.506670
[Epoch 39; Iter    14/   32] train: loss: 0.5225744
[Epoch 39] ogbg-molsider: 0.557040 val loss: 0.560942
[Epoch 39] ogbg-molsider: 0.588455 test loss: 0.520110
[Epoch 40; Iter    12/   32] train: loss: 0.5060538
[Epoch 40] ogbg-molsider: 0.549987 val loss: 0.592593
[Epoch 40] ogbg-molsider: 0.597720 test loss: 0.496696
[Epoch 41; Iter    10/   32] train: loss: 0.5279964
[Epoch 41] ogbg-molsider: 0.554811 val loss: 0.562436
[Epoch 41] ogbg-molsider: 0.601765 test loss: 0.500278
[Epoch 42; Iter     8/   32] train: loss: 0.4824738
[Epoch 42] ogbg-molsider: 0.534929 val loss: 0.604501
[Epoch 42] ogbg-molsider: 0.587516 test loss: 0.504543
[Epoch 43; Iter     6/   32] train: loss: 0.4781296
[Epoch 43] ogbg-molsider: 0.553043 val loss: 0.580567
[Epoch 43] ogbg-molsider: 0.603514 test loss: 0.496033
[Epoch 44; Iter     4/   32] train: loss: 0.4702133
[Epoch 44] ogbg-molsider: 0.543963 val loss: 0.517537
[Epoch 44] ogbg-molsider: 0.588917 test loss: 0.527839
[Epoch 45; Iter     2/   32] train: loss: 0.4678420
[Epoch 45; Iter    32/   32] train: loss: 0.4847689
[Epoch 45] ogbg-molsider: 0.556353 val loss: 0.488609
[Epoch 45] ogbg-molsider: 0.607146 test loss: 0.499746
[Epoch 46; Iter    30/   32] train: loss: 0.4680006
[Epoch 46] ogbg-molsider: 0.549061 val loss: 0.490702
[Epoch 46] ogbg-molsider: 0.611967 test loss: 0.503618
[Epoch 47; Iter    28/   32] train: loss: 0.4762943
[Epoch 47] ogbg-molsider: 0.560854 val loss: 0.567793
[Epoch 47] ogbg-molsider: 0.586336 test loss: 0.521845
[Epoch 48; Iter    26/   32] train: loss: 0.4651341
[Epoch 48] ogbg-molsider: 0.539673 val loss: 0.509914
[Epoch 48] ogbg-molsider: 0.587110 test loss: 0.526512
[Epoch 49; Iter    24/   32] train: loss: 0.5557489
[Epoch 49] ogbg-molsider: 0.537120 val loss: 0.497246
[Epoch 49] ogbg-molsider: 0.565523 test loss: 0.536860
[Epoch 50; Iter    22/   32] train: loss: 0.4868206
[Epoch 50] ogbg-molsider: 0.555179 val loss: 0.511239
[Epoch 50] ogbg-molsider: 0.573355 test loss: 0.550539
[Epoch 51; Iter    20/   32] train: loss: 0.4864134
[Epoch 51] ogbg-molsider: 0.531718 val loss: 0.507688
[Epoch 51] ogbg-molsider: 0.575999 test loss: 0.527843
[Epoch 52; Iter    18/   32] train: loss: 0.5219608
[Epoch 52] ogbg-molsider: 0.553341 val loss: 0.514978
[Epoch 52] ogbg-molsider: 0.569516 test loss: 0.523337
[Epoch 53; Iter    16/   32] train: loss: 0.4634167
[Epoch 53] ogbg-molsider: 0.538590 val loss: 0.501478
[Epoch 53] ogbg-molsider: 0.557655 test loss: 0.513782
[Epoch 54; Iter    14/   32] train: loss: 0.5593260
[Epoch 54] ogbg-molsider: 0.585201 val loss: 0.507371
[Epoch 54] ogbg-molsider: 0.594302 test loss: 0.509353
[Epoch 55; Iter    12/   32] train: loss: 0.4625863
[Epoch 55] ogbg-molsider: 0.568405 val loss: 0.523600
[Epoch 55] ogbg-molsider: 0.604284 test loss: 0.570343
[Epoch 56; Iter    10/   32] train: loss: 0.4595195
[Epoch 56] ogbg-molsider: 0.602624 val loss: 0.500005
[Epoch 56] ogbg-molsider: 0.605812 test loss: 0.539210
[Epoch 57; Iter     8/   32] train: loss: 0.4814063
[Epoch 57] ogbg-molsider: 0.565674 val loss: 0.496290
[Epoch 57] ogbg-molsider: 0.582338 test loss: 0.517554
[Epoch 58; Iter     6/   32] train: loss: 0.4802620
[Epoch 58] ogbg-molsider: 0.573434 val loss: 0.487344
[Epoch 58] ogbg-molsider: 0.598094 test loss: 0.496392
[Epoch 59; Iter     4/   32] train: loss: 0.4948593
[Epoch 59] ogbg-molsider: 0.563532 val loss: 0.504373
[Epoch 59] ogbg-molsider: 0.577480 test loss: 0.533442
[Epoch 60; Iter     2/   32] train: loss: 0.4575872
[Epoch 60; Iter    32/   32] train: loss: 0.7104675
[Epoch 60] ogbg-molsider: 0.584024 val loss: 0.482814
[Epoch 60] ogbg-molsider: 0.583447 test loss: 0.508585
[Epoch 61; Iter    30/   32] train: loss: 0.4948972
[Epoch 61] ogbg-molsider: 0.604601 val loss: 0.509091
[Epoch 61] ogbg-molsider: 0.581838 test loss: 0.547657
[Epoch 62; Iter    28/   32] train: loss: 0.4727412
[Epoch 62] ogbg-molsider: 0.588572 val loss: 0.492728
[Epoch 62] ogbg-molsider: 0.573656 test loss: 0.552205
[Epoch 63; Iter    26/   32] train: loss: 0.4917793
[Epoch 63] ogbg-molsider: 0.580173 val loss: 0.599134
[Epoch 63] ogbg-molsider: 0.580272 test loss: 0.546393
[Epoch 64; Iter    24/   32] train: loss: 0.4357758
[Epoch 64] ogbg-molsider: 0.604442 val loss: 0.484881
[Epoch 64] ogbg-molsider: 0.587983 test loss: 0.522010
[Epoch 65; Iter    22/   32] train: loss: 0.4150185
[Epoch 65] ogbg-molsider: 0.594243 val loss: 0.540991
[Epoch 65] ogbg-molsider: 0.597634 test loss: 0.583586
[Epoch 66; Iter    20/   32] train: loss: 0.4140997
[Epoch 66] ogbg-molsider: 0.588964 val loss: 0.585765
[Epoch 66] ogbg-molsider: 0.582916 test loss: 0.509890
[Epoch 67; Iter    18/   32] train: loss: 0.4814717
[Epoch 67] ogbg-molsider: 0.582071 val loss: 0.497952
[Epoch 67] ogbg-molsider: 0.578153 test loss: 0.524333
[Epoch 68; Iter    16/   32] train: loss: 0.4620091
[Epoch 68] ogbg-molsider: 0.593683 val loss: 0.848601
[Epoch 68] ogbg-molsider: 0.587385 test loss: 0.605460
[Epoch 69; Iter    14/   32] train: loss: 0.4459551
[Epoch 69] ogbg-molsider: 0.521496 val loss: 0.520798
[Epoch 69] ogbg-molsider: 0.543559 test loss: 0.566463
[Epoch 70; Iter    12/   32] train: loss: 0.4945328
[Epoch 70] ogbg-molsider: 0.577187 val loss: 0.606250
[Epoch 70] ogbg-molsider: 0.568514 test loss: 0.759071
[Epoch 71; Iter    10/   32] train: loss: 0.4144756
[Epoch 71] ogbg-molsider: 0.577644 val loss: 0.495306
[Epoch 71] ogbg-molsider: 0.584124 test loss: 0.518944
[Epoch 72; Iter     8/   32] train: loss: 0.4563193
[Epoch 72] ogbg-molsider: 0.586362 val loss: 0.491456
[Epoch 72] ogbg-molsider: 0.568969 test loss: 0.518797
[Epoch 73; Iter     6/   32] train: loss: 0.4410869
[Epoch 73] ogbg-molsider: 0.595225 val loss: 0.496997
[Epoch 73] ogbg-molsider: 0.592319 test loss: 0.525626
[Epoch 74; Iter     4/   32] train: loss: 0.4505021
[Epoch 74] ogbg-molsider: 0.584194 val loss: 0.502617
[Epoch 74] ogbg-molsider: 0.587706 test loss: 0.556279
[Epoch 75; Iter     2/   32] train: loss: 0.4169825
[Epoch 75; Iter    32/   32] train: loss: 0.3733800
[Epoch 75] ogbg-molsider: 0.579589 val loss: 0.514404
[Epoch 75] ogbg-molsider: 0.575087 test loss: 0.571865
[Epoch 76; Iter    30/   32] train: loss: 0.3965338
[Epoch 76] ogbg-molsider: 0.603867 val loss: 0.497595
[Epoch 76] ogbg-molsider: 0.569547 test loss: 0.543681
[Epoch 77; Iter    28/   32] train: loss: 0.4393494
[Epoch 77] ogbg-molsider: 0.580566 val loss: 0.508738
[Epoch 77] ogbg-molsider: 0.576607 test loss: 0.539707
[Epoch 78; Iter    26/   32] train: loss: 0.4183141
[Epoch 78] ogbg-molsider: 0.596931 val loss: 0.506830
[Epoch 78] ogbg-molsider: 0.578722 test loss: 0.562682
[Epoch 79; Iter    24/   32] train: loss: 0.4273239
[Epoch 79] ogbg-molsider: 0.583816 val loss: 0.515627
[Epoch 79] ogbg-molsider: 0.573897 test loss: 0.535810
[Epoch 80; Iter    22/   32] train: loss: 0.4310862
[Epoch 80] ogbg-molsider: 0.597902 val loss: 0.516350
[Epoch 80] ogbg-molsider: 0.588676 test loss: 0.557537
[Epoch 81; Iter    20/   32] train: loss: 0.3969328
[Epoch 81] ogbg-molsider: 0.598652 val loss: 0.506593
[Epoch 81] ogbg-molsider: 0.580776 test loss: 0.548047
[Epoch 82; Iter    18/   32] train: loss: 0.3804422
[Epoch 82] ogbg-molsider: 0.613898 val loss: 0.509759
[Epoch 82] ogbg-molsider: 0.555866 test loss: 0.574021
[Epoch 83; Iter    16/   32] train: loss: 0.3950262
[Epoch 83] ogbg-molsider: 0.594138 val loss: 0.518373
[Epoch 83] ogbg-molsider: 0.568549 test loss: 0.553818
[Epoch 84; Iter    14/   32] train: loss: 0.3924651
[Epoch 84] ogbg-molsider: 0.584580 val loss: 0.512184
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 84] ogbg-molsider: 0.588231 test loss: 0.545050
[Epoch 85; Iter    12/   32] train: loss: 0.4274887
[Epoch 85] ogbg-molsider: 0.561739 val loss: 0.558060
[Epoch 85] ogbg-molsider: 0.579508 test loss: 0.557845
[Epoch 86; Iter    10/   32] train: loss: 0.3545462
[Epoch 86] ogbg-molsider: 0.561576 val loss: 0.549346
[Epoch 86] ogbg-molsider: 0.589656 test loss: 0.577902
[Epoch 87; Iter     8/   32] train: loss: 0.4001925
[Epoch 87] ogbg-molsider: 0.567486 val loss: 0.555654
[Epoch 87] ogbg-molsider: 0.594126 test loss: 0.570536
[Epoch 88; Iter     6/   32] train: loss: 0.3326122
[Epoch 88] ogbg-molsider: 0.570615 val loss: 0.549384
[Epoch 88] ogbg-molsider: 0.583540 test loss: 0.610830
[Epoch 89; Iter     4/   32] train: loss: 0.3593881
[Epoch 89] ogbg-molsider: 0.562675 val loss: 0.545144
[Epoch 89] ogbg-molsider: 0.582319 test loss: 0.584091
[Epoch 90; Iter     2/   32] train: loss: 0.3689678
[Epoch 90; Iter    32/   32] train: loss: 0.3252185
[Epoch 90] ogbg-molsider: 0.548356 val loss: 0.569941
[Epoch 90] ogbg-molsider: 0.580804 test loss: 0.574180
[Epoch 91; Iter    30/   32] train: loss: 0.3321247
[Epoch 91] ogbg-molsider: 0.555345 val loss: 0.549497
[Epoch 91] ogbg-molsider: 0.589194 test loss: 0.567211
[Epoch 92; Iter    28/   32] train: loss: 0.3353683
[Epoch 92] ogbg-molsider: 0.551066 val loss: 0.565134
[Epoch 92] ogbg-molsider: 0.590265 test loss: 0.592504
[Epoch 93; Iter    26/   32] train: loss: 0.3282699
[Epoch 93] ogbg-molsider: 0.564431 val loss: 0.568422
[Epoch 93] ogbg-molsider: 0.584347 test loss: 0.581276
[Epoch 94; Iter    24/   32] train: loss: 0.3430430
[Epoch 94] ogbg-molsider: 0.543612 val loss: 0.575945
[Epoch 94] ogbg-molsider: 0.586675 test loss: 0.585460
[Epoch 95; Iter    22/   32] train: loss: 0.3974970
[Epoch 95] ogbg-molsider: 0.553892 val loss: 0.582482
[Epoch 95] ogbg-molsider: 0.579328 test loss: 0.675966
[Epoch 96; Iter    20/   32] train: loss: 0.3935764
[Epoch 96] ogbg-molsider: 0.562067 val loss: 0.563773
[Epoch 96] ogbg-molsider: 0.608947 test loss: 0.693339
[Epoch 97; Iter    18/   32] train: loss: 0.3809522
[Epoch 97] ogbg-molsider: 0.553959 val loss: 0.574761
[Epoch 97] ogbg-molsider: 0.579747 test loss: 0.655290
[Epoch 98; Iter    16/   32] train: loss: 0.3298112
[Epoch 98] ogbg-molsider: 0.562177 val loss: 0.554713
[Epoch 98] ogbg-molsider: 0.584574 test loss: 0.619631
[Epoch 99; Iter    14/   32] train: loss: 0.3821127
[Epoch 99] ogbg-molsider: 0.571203 val loss: 0.565152
[Epoch 99] ogbg-molsider: 0.572596 test loss: 0.646934
[Epoch 100; Iter    12/   32] train: loss: 0.3075021
[Epoch 100] ogbg-molsider: 0.567168 val loss: 0.570311
[Epoch 100] ogbg-molsider: 0.582043 test loss: 0.612346
[Epoch 101; Iter    10/   32] train: loss: 0.3379799
[Epoch 101] ogbg-molsider: 0.571145 val loss: 0.557887
[Epoch 101] ogbg-molsider: 0.580115 test loss: 0.635540
[Epoch 102; Iter     8/   32] train: loss: 0.3229240
[Epoch 102] ogbg-molsider: 0.573053 val loss: 0.576425
[Epoch 102] ogbg-molsider: 0.575211 test loss: 0.644781
[Epoch 103; Iter     6/   32] train: loss: 0.3585255
[Epoch 103] ogbg-molsider: 0.563868 val loss: 0.575276
[Epoch 103] ogbg-molsider: 0.580886 test loss: 0.589723
[Epoch 104; Iter     4/   32] train: loss: 0.3770006
[Epoch 104] ogbg-molsider: 0.536280 val loss: 0.610316
[Epoch 104] ogbg-molsider: 0.571808 test loss: 0.620906
[Epoch 105; Iter     2/   32] train: loss: 0.3200977
[Epoch 105; Iter    32/   32] train: loss: 0.3241760
[Epoch 105] ogbg-molsider: 0.555821 val loss: 0.601004
[Epoch 105] ogbg-molsider: 0.584981 test loss: 0.629388
[Epoch 106; Iter    30/   32] train: loss: 0.3483948
[Epoch 106] ogbg-molsider: 0.555865 val loss: 0.596202
[Epoch 106] ogbg-molsider: 0.575694 test loss: 0.606256
[Epoch 107; Iter    28/   32] train: loss: 0.3304008
[Epoch 107] ogbg-molsider: 0.560256 val loss: 0.592105
[Epoch 107] ogbg-molsider: 0.560371 test loss: 0.620960
[Epoch 108; Iter    26/   32] train: loss: 0.3790659
[Epoch 108] ogbg-molsider: 0.546654 val loss: 0.596897
[Epoch 108] ogbg-molsider: 0.591164 test loss: 0.627750
[Epoch 109; Iter    24/   32] train: loss: 0.3303724
[Epoch 109] ogbg-molsider: 0.555778 val loss: 0.607171
[Epoch 109] ogbg-molsider: 0.565619 test loss: 0.644181
[Epoch 110; Iter    22/   32] train: loss: 0.2977827
[Epoch 110] ogbg-molsider: 0.552575 val loss: 0.592091
[Epoch 110] ogbg-molsider: 0.568921 test loss: 0.631012
[Epoch 111; Iter    20/   32] train: loss: 0.2954468
[Epoch 111] ogbg-molsider: 0.553403 val loss: 0.620404
[Epoch 111] ogbg-molsider: 0.577160 test loss: 0.629549
[Epoch 112; Iter    18/   32] train: loss: 0.3956934
[Epoch 112] ogbg-molsider: 0.564087 val loss: 0.609207
[Epoch 112] ogbg-molsider: 0.568991 test loss: 0.664104
[Epoch 113; Iter    16/   32] train: loss: 0.3431474
[Epoch 113] ogbg-molsider: 0.574080 val loss: 0.597949
[Epoch 113] ogbg-molsider: 0.569244 test loss: 0.636742
[Epoch 114; Iter    14/   32] train: loss: 0.2975387
[Epoch 114] ogbg-molsider: 0.568269 val loss: 0.604842
[Epoch 114] ogbg-molsider: 0.585486 test loss: 0.629887
[Epoch 115; Iter    12/   32] train: loss: 0.2990522
[Epoch 115] ogbg-molsider: 0.556820 val loss: 0.606153
[Epoch 115] ogbg-molsider: 0.577428 test loss: 0.659276
[Epoch 116; Iter    10/   32] train: loss: 0.3043499
[Epoch 116] ogbg-molsider: 0.554232 val loss: 0.625969
[Epoch 116] ogbg-molsider: 0.570935 test loss: 0.658539
[Epoch 117; Iter     8/   32] train: loss: 0.3130347
[Epoch 117] ogbg-molsider: 0.548684 val loss: 0.623169
[Epoch 117] ogbg-molsider: 0.579331 test loss: 0.605448
[Epoch 118; Iter     6/   32] train: loss: 0.3055061
[Epoch 118] ogbg-molsider: 0.549507 val loss: 0.627245
[Epoch 118] ogbg-molsider: 0.587788 test loss: 0.604723
[Epoch 119; Iter     4/   32] train: loss: 0.3011105
[Epoch 119] ogbg-molsider: 0.570952 val loss: 0.606790
[Epoch 119] ogbg-molsider: 0.583329 test loss: 0.610684
[Epoch 120; Iter     2/   32] train: loss: 0.2961052
[Epoch 120; Iter    32/   32] train: loss: 0.3190804
[Epoch 120] ogbg-molsider: 0.573584 val loss: 0.595054
[Epoch 120] ogbg-molsider: 0.570102 test loss: 0.605886
[Epoch 121; Iter    30/   32] train: loss: 0.3396614
[Epoch 121] ogbg-molsider: 0.565631 val loss: 0.621976
[Epoch 121] ogbg-molsider: 0.585163 test loss: 0.618553
[Epoch 122; Iter    28/   32] train: loss: 0.3076182
[Epoch 122] ogbg-molsider: 0.574167 val loss: 0.596587
[Epoch 122] ogbg-molsider: 0.590940 test loss: 0.610112
[Epoch 123; Iter    26/   32] train: loss: 0.2856857
[Epoch 123] ogbg-molsider: 0.578268 val loss: 0.616906
[Epoch 123] ogbg-molsider: 0.582324 test loss: 0.661643
[Epoch 124; Iter    24/   32] train: loss: 0.2905499
[Epoch 124] ogbg-molsider: 0.581911 val loss: 0.594235
[Epoch 124] ogbg-molsider: 0.587306 test loss: 0.610860
[Epoch 125; Iter    22/   32] train: loss: 0.3344175
[Epoch 125] ogbg-molsider: 0.578655 val loss: 0.606717
[Epoch 125] ogbg-molsider: 0.587930 test loss: 0.615042
[Epoch 126; Iter    20/   32] train: loss: 0.3198792
[Epoch 126] ogbg-molsider: 0.576809 val loss: 0.617403
[Epoch 126] ogbg-molsider: 0.568479 test loss: 0.644794
[Epoch 127; Iter    18/   32] train: loss: 0.2924820
[Epoch 127] ogbg-molsider: 0.566655 val loss: 0.630912
[Epoch 127] ogbg-molsider: 0.578813 test loss: 0.625695
[Epoch 128; Iter    16/   32] train: loss: 0.2904951
[Epoch 128] ogbg-molsider: 0.589185 val loss: 0.612434
[Epoch 128] ogbg-molsider: 0.579577 test loss: 0.631367
[Epoch 129; Iter    14/   32] train: loss: 0.2770425
[Epoch 129] ogbg-molsider: 0.573878 val loss: 0.644884
[Epoch 129] ogbg-molsider: 0.569438 test loss: 0.655067
[Epoch 130; Iter    12/   32] train: loss: 0.3417228
[Epoch 130] ogbg-molsider: 0.567339 val loss: 0.633594
[Epoch 130] ogbg-molsider: 0.564087 test loss: 0.649436
[Epoch 131; Iter    10/   32] train: loss: 0.2940603
[Epoch 131] ogbg-molsider: 0.569059 val loss: 0.631704
[Epoch 131] ogbg-molsider: 0.576701 test loss: 0.638046
Early stopping criterion based on -ogbg-molsider- that should be max reached after 131 epochs. Best model checkpoint was in epoch 71.
Statistics on  val_best_checkpoint
mean_pred: 0.7636504769325256
std_pred: 2.0521178245544434
mean_targets: 0.6057459115982056
std_targets: 0.48873215913772583
[Epoch 84] ogbg-molsider: 0.603081 test loss: 0.532107
[Epoch 85; Iter    12/   32] train: loss: 0.4039453
[Epoch 85] ogbg-molsider: 0.560999 val loss: 0.532989
[Epoch 85] ogbg-molsider: 0.612079 test loss: 0.535161
[Epoch 86; Iter    10/   32] train: loss: 0.3811051
[Epoch 86] ogbg-molsider: 0.575266 val loss: 0.538960
[Epoch 86] ogbg-molsider: 0.608932 test loss: 0.559944
[Epoch 87; Iter     8/   32] train: loss: 0.3492647
[Epoch 87] ogbg-molsider: 0.551336 val loss: 0.533494
[Epoch 87] ogbg-molsider: 0.589203 test loss: 0.577489
[Epoch 88; Iter     6/   32] train: loss: 0.3474328
[Epoch 88] ogbg-molsider: 0.581368 val loss: 0.538582
[Epoch 88] ogbg-molsider: 0.598062 test loss: 0.566608
[Epoch 89; Iter     4/   32] train: loss: 0.3489704
[Epoch 89] ogbg-molsider: 0.559541 val loss: 0.543735
[Epoch 89] ogbg-molsider: 0.598152 test loss: 0.546371
[Epoch 90; Iter     2/   32] train: loss: 0.3286937
[Epoch 90; Iter    32/   32] train: loss: 0.3802384
[Epoch 90] ogbg-molsider: 0.582163 val loss: 0.535923
[Epoch 90] ogbg-molsider: 0.599634 test loss: 0.566751
[Epoch 91; Iter    30/   32] train: loss: 0.3701830
[Epoch 91] ogbg-molsider: 0.563553 val loss: 0.543769
[Epoch 91] ogbg-molsider: 0.591033 test loss: 0.565409
[Epoch 92; Iter    28/   32] train: loss: 0.3612621
[Epoch 92] ogbg-molsider: 0.571204 val loss: 0.540456
[Epoch 92] ogbg-molsider: 0.577464 test loss: 0.576258
[Epoch 93; Iter    26/   32] train: loss: 0.3980912
[Epoch 93] ogbg-molsider: 0.557922 val loss: 0.542111
[Epoch 93] ogbg-molsider: 0.580749 test loss: 0.573423
[Epoch 94; Iter    24/   32] train: loss: 0.3661686
[Epoch 94] ogbg-molsider: 0.564657 val loss: 0.557610
[Epoch 94] ogbg-molsider: 0.566402 test loss: 0.581772
[Epoch 95; Iter    22/   32] train: loss: 0.3227848
[Epoch 95] ogbg-molsider: 0.552485 val loss: 0.546415
[Epoch 95] ogbg-molsider: 0.579741 test loss: 0.569190
[Epoch 96; Iter    20/   32] train: loss: 0.3920263
[Epoch 96] ogbg-molsider: 0.573618 val loss: 0.554184
[Epoch 96] ogbg-molsider: 0.591455 test loss: 0.578927
[Epoch 97; Iter    18/   32] train: loss: 0.3295591
[Epoch 97] ogbg-molsider: 0.563874 val loss: 0.584733
[Epoch 97] ogbg-molsider: 0.580129 test loss: 0.618457
[Epoch 98; Iter    16/   32] train: loss: 0.3791900
[Epoch 98] ogbg-molsider: 0.552196 val loss: 0.577351
[Epoch 98] ogbg-molsider: 0.597994 test loss: 0.553529
[Epoch 99; Iter    14/   32] train: loss: 0.3580360
[Epoch 99] ogbg-molsider: 0.570861 val loss: 0.549560
[Epoch 99] ogbg-molsider: 0.581642 test loss: 0.586084
[Epoch 100; Iter    12/   32] train: loss: 0.3413339
[Epoch 100] ogbg-molsider: 0.572946 val loss: 0.567883
[Epoch 100] ogbg-molsider: 0.601742 test loss: 0.591946
[Epoch 101; Iter    10/   32] train: loss: 0.3319323
[Epoch 101] ogbg-molsider: 0.577069 val loss: 0.557982
[Epoch 101] ogbg-molsider: 0.586880 test loss: 0.597051
[Epoch 102; Iter     8/   32] train: loss: 0.3765026
[Epoch 102] ogbg-molsider: 0.551710 val loss: 0.578660
[Epoch 102] ogbg-molsider: 0.589981 test loss: 0.574009
[Epoch 103; Iter     6/   32] train: loss: 0.3429065
[Epoch 103] ogbg-molsider: 0.555092 val loss: 0.590211
[Epoch 103] ogbg-molsider: 0.597158 test loss: 0.608596
[Epoch 104; Iter     4/   32] train: loss: 0.3402045
[Epoch 104] ogbg-molsider: 0.576043 val loss: 0.573753
[Epoch 104] ogbg-molsider: 0.595473 test loss: 0.579579
[Epoch 105; Iter     2/   32] train: loss: 0.2964876
[Epoch 105; Iter    32/   32] train: loss: 0.3624102
[Epoch 105] ogbg-molsider: 0.575825 val loss: 0.580301
[Epoch 105] ogbg-molsider: 0.603718 test loss: 0.586178
[Epoch 106; Iter    30/   32] train: loss: 0.3488619
[Epoch 106] ogbg-molsider: 0.573032 val loss: 0.575010
[Epoch 106] ogbg-molsider: 0.587658 test loss: 0.614044
[Epoch 107; Iter    28/   32] train: loss: 0.3596655
[Epoch 107] ogbg-molsider: 0.562646 val loss: 0.577131
[Epoch 107] ogbg-molsider: 0.595925 test loss: 0.595830
[Epoch 108; Iter    26/   32] train: loss: 0.3377658
[Epoch 108] ogbg-molsider: 0.583150 val loss: 0.576589
[Epoch 108] ogbg-molsider: 0.593646 test loss: 0.602297
[Epoch 109; Iter    24/   32] train: loss: 0.3446774
[Epoch 109] ogbg-molsider: 0.564379 val loss: 0.581262
[Epoch 109] ogbg-molsider: 0.581610 test loss: 0.610303
[Epoch 110; Iter    22/   32] train: loss: 0.3332414
[Epoch 110] ogbg-molsider: 0.568587 val loss: 0.586954
[Epoch 110] ogbg-molsider: 0.591428 test loss: 0.615496
[Epoch 111; Iter    20/   32] train: loss: 0.3340832
[Epoch 111] ogbg-molsider: 0.570403 val loss: 0.583510
[Epoch 111] ogbg-molsider: 0.588234 test loss: 0.609328
[Epoch 112; Iter    18/   32] train: loss: 0.3606890
[Epoch 112] ogbg-molsider: 0.569467 val loss: 0.579915
[Epoch 112] ogbg-molsider: 0.584396 test loss: 0.611121
[Epoch 113; Iter    16/   32] train: loss: 0.3187942
[Epoch 113] ogbg-molsider: 0.562857 val loss: 0.599003
[Epoch 113] ogbg-molsider: 0.583160 test loss: 0.618511
[Epoch 114; Iter    14/   32] train: loss: 0.3898125
[Epoch 114] ogbg-molsider: 0.569047 val loss: 0.592949
[Epoch 114] ogbg-molsider: 0.578933 test loss: 0.622231
[Epoch 115; Iter    12/   32] train: loss: 0.2877185
[Epoch 115] ogbg-molsider: 0.566483 val loss: 0.608204
[Epoch 115] ogbg-molsider: 0.586231 test loss: 0.610128
[Epoch 116; Iter    10/   32] train: loss: 0.2652991
[Epoch 116] ogbg-molsider: 0.575873 val loss: 0.588721
[Epoch 116] ogbg-molsider: 0.584705 test loss: 0.614896
[Epoch 117; Iter     8/   32] train: loss: 0.3058067
[Epoch 117] ogbg-molsider: 0.566165 val loss: 0.602680
[Epoch 117] ogbg-molsider: 0.586387 test loss: 0.618917
[Epoch 118; Iter     6/   32] train: loss: 0.2712202
[Epoch 118] ogbg-molsider: 0.575514 val loss: 0.589967
[Epoch 118] ogbg-molsider: 0.585929 test loss: 0.620041
[Epoch 119; Iter     4/   32] train: loss: 0.3192234
[Epoch 119] ogbg-molsider: 0.566232 val loss: 0.605746
[Epoch 119] ogbg-molsider: 0.576322 test loss: 0.624270
[Epoch 120; Iter     2/   32] train: loss: 0.3106018
[Epoch 120; Iter    32/   32] train: loss: 0.3223069
[Epoch 120] ogbg-molsider: 0.557971 val loss: 0.628683
[Epoch 120] ogbg-molsider: 0.578142 test loss: 0.630836
[Epoch 121; Iter    30/   32] train: loss: 0.3051821
[Epoch 121] ogbg-molsider: 0.561791 val loss: 0.616164
[Epoch 121] ogbg-molsider: 0.573458 test loss: 0.656704
[Epoch 122; Iter    28/   32] train: loss: 0.2775272
[Epoch 122] ogbg-molsider: 0.565326 val loss: 0.601747
[Epoch 122] ogbg-molsider: 0.584497 test loss: 0.628880
[Epoch 123; Iter    26/   32] train: loss: 0.2959098
[Epoch 123] ogbg-molsider: 0.554398 val loss: 0.635187
[Epoch 123] ogbg-molsider: 0.582633 test loss: 0.641325
[Epoch 124; Iter    24/   32] train: loss: 0.3682907
[Epoch 124] ogbg-molsider: 0.558569 val loss: 0.641207
[Epoch 124] ogbg-molsider: 0.579828 test loss: 0.641430
[Epoch 125; Iter    22/   32] train: loss: 0.3022014
[Epoch 125] ogbg-molsider: 0.565469 val loss: 0.624255
[Epoch 125] ogbg-molsider: 0.584335 test loss: 0.639388
[Epoch 126; Iter    20/   32] train: loss: 0.3116167
[Epoch 126] ogbg-molsider: 0.559731 val loss: 0.625251
[Epoch 126] ogbg-molsider: 0.584127 test loss: 0.621196
[Epoch 127; Iter    18/   32] train: loss: 0.2883937
[Epoch 127] ogbg-molsider: 0.557959 val loss: 0.639594
[Epoch 127] ogbg-molsider: 0.586480 test loss: 0.632070
[Epoch 128; Iter    16/   32] train: loss: 0.2745743
[Epoch 128] ogbg-molsider: 0.563117 val loss: 0.646229
[Epoch 128] ogbg-molsider: 0.588280 test loss: 0.632398
Early stopping criterion based on -ogbg-molsider- that should be max reached after 128 epochs. Best model checkpoint was in epoch 68.
Statistics on  val_best_checkpoint
mean_pred: 0.8207716941833496
std_pred: 2.035412073135376
mean_targets: 0.6057459115982056
std_targets: 0.48873215913772583
prcauc: 0.658538138611618
rocauc: 0.5996294567245881
ogbg-molsider: 0.5996294567245881
OGBNanLabelBCEWithLogitsLoss: 0.491199791431427
Statistics on  test
mean_pred: 0.9619327187538147
std_pred: 2.079448938369751
mean_targets: 0.5677863955497742
std_targets: 0.49542638659477234
prcauc: 0.6151470569715807
rocauc: 0.5994435240272267
ogbg-molsider: 0.5994435240272267
OGBNanLabelBCEWithLogitsLoss: 0.578296789101192
Statistics on  train
mean_pred: 0.5132361054420471
std_pred: 2.129763603210449
mean_targets: 0.5593408942222595
std_targets: 0.49647536873817444
prcauc: 0.7333434214219507
rocauc: 0.7812804320502091
ogbg-molsider: 0.7812804320502091
OGBNanLabelBCEWithLogitsLoss: 0.4382478194311261
prcauc: 0.6639163155096018
rocauc: 0.5900726157286177
ogbg-molsider: 0.5900726157286177
OGBNanLabelBCEWithLogitsLoss: 0.5015233882835933
Statistics on  test
mean_pred: 0.7594037652015686
std_pred: 2.1229522228240967
mean_targets: 0.5677863955497742
std_targets: 0.49542638659477234
prcauc: 0.6286334708047983
rocauc: 0.6027624044088024
ogbg-molsider: 0.6027624044088024
OGBNanLabelBCEWithLogitsLoss: 0.539923506123679
Statistics on  train
mean_pred: 0.3937734365463257
std_pred: 2.1389882564544678
mean_targets: 0.5593408942222595
std_targets: 0.49647536873817444
prcauc: 0.7517835468107558
rocauc: 0.790996445022707
ogbg-molsider: 0.790996445022707
OGBNanLabelBCEWithLogitsLoss: 0.4256089124828577
[Epoch 81; Iter    30/   36] train: loss: 0.4747778
[Epoch 81] ogbg-molsider: 0.622443 val loss: 0.513920
[Epoch 81] ogbg-molsider: 0.602355 test loss: 0.551783
[Epoch 82; Iter    24/   36] train: loss: 0.4082930
[Epoch 82] ogbg-molsider: 0.613552 val loss: 0.512709
[Epoch 82] ogbg-molsider: 0.587755 test loss: 0.563503
[Epoch 83; Iter    18/   36] train: loss: 0.4014495
[Epoch 83] ogbg-molsider: 0.638248 val loss: 0.515559
[Epoch 83] ogbg-molsider: 0.605293 test loss: 0.550886
[Epoch 84; Iter    12/   36] train: loss: 0.3603890
[Epoch 84] ogbg-molsider: 0.625590 val loss: 0.499195
[Epoch 84] ogbg-molsider: 0.575597 test loss: 0.560686
[Epoch 85; Iter     6/   36] train: loss: 0.3953222
[Epoch 85; Iter    36/   36] train: loss: 0.4255466
[Epoch 85] ogbg-molsider: 0.631987 val loss: 0.520154
[Epoch 85] ogbg-molsider: 0.581404 test loss: 0.596823
[Epoch 86; Iter    30/   36] train: loss: 0.3988284
[Epoch 86] ogbg-molsider: 0.613216 val loss: 0.519422
[Epoch 86] ogbg-molsider: 0.585649 test loss: 0.580524
[Epoch 87; Iter    24/   36] train: loss: 0.3879648
[Epoch 87] ogbg-molsider: 0.653931 val loss: 0.496341
[Epoch 87] ogbg-molsider: 0.591803 test loss: 0.569153
[Epoch 88; Iter    18/   36] train: loss: 0.3583630
[Epoch 88] ogbg-molsider: 0.621145 val loss: 0.532891
[Epoch 88] ogbg-molsider: 0.579746 test loss: 0.558889
[Epoch 89; Iter    12/   36] train: loss: 0.3981431
[Epoch 89] ogbg-molsider: 0.639593 val loss: 0.508156
[Epoch 89] ogbg-molsider: 0.608489 test loss: 0.581842
[Epoch 90; Iter     6/   36] train: loss: 0.3149589
[Epoch 90; Iter    36/   36] train: loss: 0.3992333
[Epoch 90] ogbg-molsider: 0.621103 val loss: 0.533322
[Epoch 90] ogbg-molsider: 0.581391 test loss: 0.580069
[Epoch 91; Iter    30/   36] train: loss: 0.3822412
[Epoch 91] ogbg-molsider: 0.629633 val loss: 0.536691
[Epoch 91] ogbg-molsider: 0.610357 test loss: 0.593928
[Epoch 92; Iter    24/   36] train: loss: 0.3210093
[Epoch 92] ogbg-molsider: 0.648704 val loss: 0.519742
[Epoch 92] ogbg-molsider: 0.597499 test loss: 0.581293
[Epoch 93; Iter    18/   36] train: loss: 0.2950756
[Epoch 93] ogbg-molsider: 0.638201 val loss: 0.508074
[Epoch 93] ogbg-molsider: 0.588719 test loss: 0.570015
[Epoch 94; Iter    12/   36] train: loss: 0.3272060
[Epoch 94] ogbg-molsider: 0.642583 val loss: 0.517245
[Epoch 94] ogbg-molsider: 0.588345 test loss: 0.594646
[Epoch 95; Iter     6/   36] train: loss: 0.3391411
[Epoch 95; Iter    36/   36] train: loss: 0.3741849
[Epoch 95] ogbg-molsider: 0.629677 val loss: 0.528189
[Epoch 95] ogbg-molsider: 0.596609 test loss: 0.604957
[Epoch 96; Iter    30/   36] train: loss: 0.3682983
[Epoch 96] ogbg-molsider: 0.639900 val loss: 0.549390
[Epoch 96] ogbg-molsider: 0.607729 test loss: 0.612862
[Epoch 97; Iter    24/   36] train: loss: 0.3206202
[Epoch 97] ogbg-molsider: 0.641596 val loss: 0.523116
[Epoch 97] ogbg-molsider: 0.588925 test loss: 0.584955
[Epoch 98; Iter    18/   36] train: loss: 0.3478138
[Epoch 98] ogbg-molsider: 0.641489 val loss: 0.528584
[Epoch 98] ogbg-molsider: 0.597157 test loss: 0.595990
[Epoch 99; Iter    12/   36] train: loss: 0.3701443
[Epoch 99] ogbg-molsider: 0.637774 val loss: 0.536719
[Epoch 99] ogbg-molsider: 0.617189 test loss: 0.588001
[Epoch 100; Iter     6/   36] train: loss: 0.2740227
[Epoch 100; Iter    36/   36] train: loss: 0.3451576
[Epoch 100] ogbg-molsider: 0.633060 val loss: 0.540297
[Epoch 100] ogbg-molsider: 0.597762 test loss: 0.599362
[Epoch 101; Iter    30/   36] train: loss: 0.3126568
[Epoch 101] ogbg-molsider: 0.638413 val loss: 0.551696
[Epoch 101] ogbg-molsider: 0.603977 test loss: 0.634188
[Epoch 102; Iter    24/   36] train: loss: 0.3338098
[Epoch 102] ogbg-molsider: 0.628914 val loss: 0.556931
[Epoch 102] ogbg-molsider: 0.601206 test loss: 0.638210
[Epoch 103; Iter    18/   36] train: loss: 0.2953105
[Epoch 103] ogbg-molsider: 0.629501 val loss: 0.546891
[Epoch 103] ogbg-molsider: 0.604352 test loss: 0.613525
[Epoch 104; Iter    12/   36] train: loss: 0.3109313
[Epoch 104] ogbg-molsider: 0.633411 val loss: 0.537671
[Epoch 104] ogbg-molsider: 0.616106 test loss: 0.584542
[Epoch 105; Iter     6/   36] train: loss: 0.2901194
[Epoch 105; Iter    36/   36] train: loss: 0.3688649
[Epoch 105] ogbg-molsider: 0.631549 val loss: 0.552644
[Epoch 105] ogbg-molsider: 0.605696 test loss: 0.598022
[Epoch 106; Iter    30/   36] train: loss: 0.2797591
[Epoch 106] ogbg-molsider: 0.613062 val loss: 0.559544
[Epoch 106] ogbg-molsider: 0.579852 test loss: 0.644003
[Epoch 107; Iter    24/   36] train: loss: 0.3143395
[Epoch 107] ogbg-molsider: 0.629977 val loss: 0.554712
[Epoch 107] ogbg-molsider: 0.578148 test loss: 0.631464
[Epoch 108; Iter    18/   36] train: loss: 0.3012438
[Epoch 108] ogbg-molsider: 0.638623 val loss: 0.556057
[Epoch 108] ogbg-molsider: 0.598579 test loss: 0.630007
[Epoch 109; Iter    12/   36] train: loss: 0.2827467
[Epoch 109] ogbg-molsider: 0.617302 val loss: 0.580070
[Epoch 109] ogbg-molsider: 0.598107 test loss: 0.633085
[Epoch 110; Iter     6/   36] train: loss: 0.3104692
[Epoch 110; Iter    36/   36] train: loss: 0.3087770
[Epoch 110] ogbg-molsider: 0.619624 val loss: 0.583689
[Epoch 110] ogbg-molsider: 0.602467 test loss: 0.641287
[Epoch 111; Iter    30/   36] train: loss: 0.3054058
[Epoch 111] ogbg-molsider: 0.639045 val loss: 0.542917
[Epoch 111] ogbg-molsider: 0.598679 test loss: 0.622518
[Epoch 112; Iter    24/   36] train: loss: 0.2811757
[Epoch 112] ogbg-molsider: 0.626119 val loss: 0.595431
[Epoch 112] ogbg-molsider: 0.610161 test loss: 0.675417
[Epoch 113; Iter    18/   36] train: loss: 0.2793537
[Epoch 113] ogbg-molsider: 0.630585 val loss: 0.561643
[Epoch 113] ogbg-molsider: 0.603337 test loss: 0.639767
[Epoch 114; Iter    12/   36] train: loss: 0.3144613
[Epoch 114] ogbg-molsider: 0.633110 val loss: 0.581110
[Epoch 114] ogbg-molsider: 0.603611 test loss: 0.659433
[Epoch 115; Iter     6/   36] train: loss: 0.2915382
[Epoch 115; Iter    36/   36] train: loss: 0.2988537
[Epoch 115] ogbg-molsider: 0.629211 val loss: 0.575884
[Epoch 115] ogbg-molsider: 0.608454 test loss: 0.639057
[Epoch 116; Iter    30/   36] train: loss: 0.2789412
[Epoch 116] ogbg-molsider: 0.613241 val loss: 0.599047
[Epoch 116] ogbg-molsider: 0.592533 test loss: 0.663325
[Epoch 117; Iter    24/   36] train: loss: 0.3245573
[Epoch 117] ogbg-molsider: 0.630565 val loss: 0.578903
[Epoch 117] ogbg-molsider: 0.613152 test loss: 0.619311
[Epoch 118; Iter    18/   36] train: loss: 0.2460718
[Epoch 118] ogbg-molsider: 0.631059 val loss: 0.572116
[Epoch 118] ogbg-molsider: 0.606784 test loss: 0.639527
[Epoch 119; Iter    12/   36] train: loss: 0.2770346
[Epoch 119] ogbg-molsider: 0.630238 val loss: 0.583715
[Epoch 119] ogbg-molsider: 0.605062 test loss: 0.635182
[Epoch 120; Iter     6/   36] train: loss: 0.3052607
[Epoch 120; Iter    36/   36] train: loss: 0.3880497
[Epoch 120] ogbg-molsider: 0.640298 val loss: 0.574275
[Epoch 120] ogbg-molsider: 0.607451 test loss: 0.645636
[Epoch 121; Iter    30/   36] train: loss: 0.3023614
[Epoch 121] ogbg-molsider: 0.626744 val loss: 0.575953
[Epoch 121] ogbg-molsider: 0.604250 test loss: 0.636553
[Epoch 122; Iter    24/   36] train: loss: 0.2887164
[Epoch 122] ogbg-molsider: 0.630889 val loss: 0.589121
[Epoch 122] ogbg-molsider: 0.607707 test loss: 0.641358
[Epoch 123; Iter    18/   36] train: loss: 0.3120926
[Epoch 123] ogbg-molsider: 0.624292 val loss: 0.593468
[Epoch 123] ogbg-molsider: 0.596594 test loss: 0.675937
[Epoch 124; Iter    12/   36] train: loss: 0.3167555
[Epoch 124] ogbg-molsider: 0.630510 val loss: 0.584072
[Epoch 124] ogbg-molsider: 0.610249 test loss: 0.645787
[Epoch 125; Iter     6/   36] train: loss: 0.2725092
[Epoch 125; Iter    36/   36] train: loss: 0.2568264
[Epoch 125] ogbg-molsider: 0.633855 val loss: 0.583043
[Epoch 125] ogbg-molsider: 0.599646 test loss: 0.650668
[Epoch 126; Iter    30/   36] train: loss: 0.2591902
[Epoch 126] ogbg-molsider: 0.627555 val loss: 0.596111
[Epoch 126] ogbg-molsider: 0.604452 test loss: 0.654162
[Epoch 127; Iter    24/   36] train: loss: 0.2837183
[Epoch 127] ogbg-molsider: 0.625018 val loss: 0.592732
[Epoch 127] ogbg-molsider: 0.607394 test loss: 0.650698
[Epoch 128; Iter    18/   36] train: loss: 0.2798349
[Epoch 89] ogbg-molsider: 0.585576 val loss: 0.542430
[Epoch 89] ogbg-molsider: 0.586113 test loss: 0.546881
[Epoch 90; Iter    27/   27] train: loss: 0.4229896
[Epoch 90] ogbg-molsider: 0.568261 val loss: 0.583383
[Epoch 90] ogbg-molsider: 0.587165 test loss: 0.575181
[Epoch 91] ogbg-molsider: 0.589237 val loss: 0.570827
[Epoch 91] ogbg-molsider: 0.590979 test loss: 0.540961
[Epoch 92; Iter     3/   27] train: loss: 0.3979521
[Epoch 92] ogbg-molsider: 0.595497 val loss: 0.539112
[Epoch 92] ogbg-molsider: 0.592997 test loss: 0.537237
[Epoch 93; Iter     6/   27] train: loss: 0.3162808
[Epoch 93] ogbg-molsider: 0.580352 val loss: 0.575815
[Epoch 93] ogbg-molsider: 0.580839 test loss: 0.546892
[Epoch 94; Iter     9/   27] train: loss: 0.3514204
[Epoch 94] ogbg-molsider: 0.596002 val loss: 0.553472
[Epoch 94] ogbg-molsider: 0.586698 test loss: 0.532851
[Epoch 95; Iter    12/   27] train: loss: 0.3600522
[Epoch 95] ogbg-molsider: 0.593711 val loss: 0.556599
[Epoch 95] ogbg-molsider: 0.595334 test loss: 0.530747
[Epoch 96; Iter    15/   27] train: loss: 0.3833865
[Epoch 96] ogbg-molsider: 0.592368 val loss: 0.560250
[Epoch 96] ogbg-molsider: 0.588257 test loss: 0.540202
[Epoch 97; Iter    18/   27] train: loss: 0.3554437
[Epoch 97] ogbg-molsider: 0.597677 val loss: 0.552249
[Epoch 97] ogbg-molsider: 0.603347 test loss: 0.531203
[Epoch 98; Iter    21/   27] train: loss: 0.3827812
[Epoch 98] ogbg-molsider: 0.591254 val loss: 0.586273
[Epoch 98] ogbg-molsider: 0.588383 test loss: 0.572892
[Epoch 99; Iter    24/   27] train: loss: 0.3578890
[Epoch 99] ogbg-molsider: 0.606572 val loss: 0.579766
[Epoch 99] ogbg-molsider: 0.595675 test loss: 0.582979
[Epoch 100; Iter    27/   27] train: loss: 0.3286137
[Epoch 100] ogbg-molsider: 0.592330 val loss: 0.592870
[Epoch 100] ogbg-molsider: 0.599451 test loss: 0.582259
[Epoch 101] ogbg-molsider: 0.600047 val loss: 0.582242
[Epoch 101] ogbg-molsider: 0.610261 test loss: 0.540930
[Epoch 102; Iter     3/   27] train: loss: 0.3267623
[Epoch 102] ogbg-molsider: 0.594106 val loss: 0.566206
[Epoch 102] ogbg-molsider: 0.590940 test loss: 0.548480
[Epoch 103; Iter     6/   27] train: loss: 0.3329504
[Epoch 103] ogbg-molsider: 0.600311 val loss: 0.605084
[Epoch 103] ogbg-molsider: 0.601801 test loss: 0.594381
[Epoch 104; Iter     9/   27] train: loss: 0.3719456
[Epoch 104] ogbg-molsider: 0.593639 val loss: 0.578884
[Epoch 104] ogbg-molsider: 0.592852 test loss: 0.557221
[Epoch 105; Iter    12/   27] train: loss: 0.3635118
[Epoch 105] ogbg-molsider: 0.573952 val loss: 0.629486
[Epoch 105] ogbg-molsider: 0.581999 test loss: 0.630509
[Epoch 106; Iter    15/   27] train: loss: 0.3579066
[Epoch 106] ogbg-molsider: 0.587252 val loss: 0.608149
[Epoch 106] ogbg-molsider: 0.595094 test loss: 0.570469
[Epoch 107; Iter    18/   27] train: loss: 0.3425330
[Epoch 107] ogbg-molsider: 0.589754 val loss: 0.610982
[Epoch 107] ogbg-molsider: 0.586800 test loss: 0.606823
[Epoch 108; Iter    21/   27] train: loss: 0.3795763
[Epoch 108] ogbg-molsider: 0.581039 val loss: 0.624297
[Epoch 108] ogbg-molsider: 0.589903 test loss: 0.578934
[Epoch 109; Iter    24/   27] train: loss: 0.3226995
[Epoch 109] ogbg-molsider: 0.586562 val loss: 0.609687
[Epoch 109] ogbg-molsider: 0.582108 test loss: 0.622235
[Epoch 110; Iter    27/   27] train: loss: 0.2974675
[Epoch 110] ogbg-molsider: 0.592716 val loss: 0.600310
[Epoch 110] ogbg-molsider: 0.601982 test loss: 0.554398
[Epoch 111] ogbg-molsider: 0.597142 val loss: 0.621023
[Epoch 111] ogbg-molsider: 0.598673 test loss: 0.648931
[Epoch 112; Iter     3/   27] train: loss: 0.3131266
[Epoch 112] ogbg-molsider: 0.602367 val loss: 0.619983
[Epoch 112] ogbg-molsider: 0.600578 test loss: 0.630578
[Epoch 113; Iter     6/   27] train: loss: 0.3065505
[Epoch 113] ogbg-molsider: 0.595752 val loss: 0.612580
[Epoch 113] ogbg-molsider: 0.591677 test loss: 0.610966
[Epoch 114; Iter     9/   27] train: loss: 0.3145459
[Epoch 114] ogbg-molsider: 0.595323 val loss: 0.603670
[Epoch 114] ogbg-molsider: 0.601468 test loss: 0.570561
[Epoch 115; Iter    12/   27] train: loss: 0.3195177
[Epoch 115] ogbg-molsider: 0.592393 val loss: 0.623251
[Epoch 115] ogbg-molsider: 0.592060 test loss: 0.589194
[Epoch 116; Iter    15/   27] train: loss: 0.3108495
[Epoch 116] ogbg-molsider: 0.593647 val loss: 0.623817
[Epoch 116] ogbg-molsider: 0.605112 test loss: 0.580189
[Epoch 117; Iter    18/   27] train: loss: 0.2922663
[Epoch 117] ogbg-molsider: 0.589546 val loss: 0.609556
[Epoch 117] ogbg-molsider: 0.593007 test loss: 0.582573
[Epoch 118; Iter    21/   27] train: loss: 0.3175809
[Epoch 118] ogbg-molsider: 0.601388 val loss: 0.611274
[Epoch 118] ogbg-molsider: 0.601105 test loss: 0.584376
[Epoch 119; Iter    24/   27] train: loss: 0.3191629
[Epoch 119] ogbg-molsider: 0.599424 val loss: 0.610856
[Epoch 119] ogbg-molsider: 0.605064 test loss: 0.582035
[Epoch 120; Iter    27/   27] train: loss: 0.3176183
[Epoch 120] ogbg-molsider: 0.583760 val loss: 0.628310
[Epoch 120] ogbg-molsider: 0.596128 test loss: 0.614279
[Epoch 121] ogbg-molsider: 0.583435 val loss: 0.623106
[Epoch 121] ogbg-molsider: 0.600759 test loss: 0.596610
[Epoch 122; Iter     3/   27] train: loss: 0.3140897
[Epoch 122] ogbg-molsider: 0.592159 val loss: 0.623403
[Epoch 122] ogbg-molsider: 0.594340 test loss: 0.597455
[Epoch 123; Iter     6/   27] train: loss: 0.2743539
[Epoch 123] ogbg-molsider: 0.587479 val loss: 0.633066
[Epoch 123] ogbg-molsider: 0.600586 test loss: 0.623177
[Epoch 124; Iter     9/   27] train: loss: 0.3993715
[Epoch 124] ogbg-molsider: 0.586448 val loss: 0.646121
[Epoch 124] ogbg-molsider: 0.594728 test loss: 0.621402
[Epoch 125; Iter    12/   27] train: loss: 0.3197523
[Epoch 125] ogbg-molsider: 0.585531 val loss: 0.628784
[Epoch 125] ogbg-molsider: 0.595927 test loss: 0.597765
[Epoch 126; Iter    15/   27] train: loss: 0.2948234
[Epoch 126] ogbg-molsider: 0.587820 val loss: 0.634980
[Epoch 126] ogbg-molsider: 0.597508 test loss: 0.611642
[Epoch 127; Iter    18/   27] train: loss: 0.3075047
[Epoch 127] ogbg-molsider: 0.586200 val loss: 0.635952
[Epoch 127] ogbg-molsider: 0.606597 test loss: 0.593391
[Epoch 128; Iter    21/   27] train: loss: 0.3025470
[Epoch 128] ogbg-molsider: 0.586949 val loss: 0.635406
[Epoch 128] ogbg-molsider: 0.600382 test loss: 0.614765
[Epoch 129; Iter    24/   27] train: loss: 0.2932838
[Epoch 129] ogbg-molsider: 0.591624 val loss: 0.629647
[Epoch 129] ogbg-molsider: 0.594839 test loss: 0.611132
[Epoch 130; Iter    27/   27] train: loss: 0.3167294
[Epoch 130] ogbg-molsider: 0.581513 val loss: 0.654113
[Epoch 130] ogbg-molsider: 0.585090 test loss: 0.651989
[Epoch 131] ogbg-molsider: 0.584833 val loss: 0.650235
[Epoch 131] ogbg-molsider: 0.594589 test loss: 0.642440
[Epoch 132; Iter     3/   27] train: loss: 0.2886052
[Epoch 132] ogbg-molsider: 0.595369 val loss: 0.636194
[Epoch 132] ogbg-molsider: 0.594636 test loss: 0.608898
[Epoch 133; Iter     6/   27] train: loss: 0.2915763
[Epoch 133] ogbg-molsider: 0.586962 val loss: 0.640772
[Epoch 133] ogbg-molsider: 0.599943 test loss: 0.611022
[Epoch 134; Iter     9/   27] train: loss: 0.3075265
[Epoch 134] ogbg-molsider: 0.595538 val loss: 0.643820
[Epoch 134] ogbg-molsider: 0.599165 test loss: 0.627421
[Epoch 135; Iter    12/   27] train: loss: 0.3053611
[Epoch 135] ogbg-molsider: 0.587797 val loss: 0.652474
[Epoch 135] ogbg-molsider: 0.590260 test loss: 0.635874
[Epoch 136; Iter    15/   27] train: loss: 0.2893797
[Epoch 136] ogbg-molsider: 0.587853 val loss: 0.669365
[Epoch 136] ogbg-molsider: 0.594436 test loss: 0.639071
[Epoch 137; Iter    18/   27] train: loss: 0.2953613
[Epoch 137] ogbg-molsider: 0.588246 val loss: 0.660761
[Epoch 137] ogbg-molsider: 0.583121 test loss: 0.653595
[Epoch 138; Iter    21/   27] train: loss: 0.2856814
[Epoch 138] ogbg-molsider: 0.585882 val loss: 0.649026
[Epoch 138] ogbg-molsider: 0.582321 test loss: 0.647812
[Epoch 139; Iter    24/   27] train: loss: 0.2859642
[Epoch 139] ogbg-molsider: 0.597122 val loss: 0.654217
[Epoch 139] ogbg-molsider: 0.592310 test loss: 0.634425
[Epoch 140; Iter    27/   27] train: loss: 0.3360001
[Epoch 140] ogbg-molsider: 0.583689 val loss: 0.643244
[Epoch 140] ogbg-molsider: 0.580145 test loss: 0.638174
[Epoch 81; Iter    30/   36] train: loss: 0.3263279
[Epoch 81] ogbg-molsider: 0.620210 val loss: 0.518218
[Epoch 81] ogbg-molsider: 0.591097 test loss: 0.574180
[Epoch 82; Iter    24/   36] train: loss: 0.3559794
[Epoch 82] ogbg-molsider: 0.621649 val loss: 0.521027
[Epoch 82] ogbg-molsider: 0.569852 test loss: 0.579825
[Epoch 83; Iter    18/   36] train: loss: 0.3455671
[Epoch 83] ogbg-molsider: 0.617584 val loss: 0.524675
[Epoch 83] ogbg-molsider: 0.566423 test loss: 0.586787
[Epoch 84; Iter    12/   36] train: loss: 0.3473454
[Epoch 84] ogbg-molsider: 0.615462 val loss: 0.537816
[Epoch 84] ogbg-molsider: 0.563531 test loss: 0.614801
[Epoch 85; Iter     6/   36] train: loss: 0.3173071
[Epoch 85; Iter    36/   36] train: loss: 0.3236748
[Epoch 85] ogbg-molsider: 0.628640 val loss: 0.528473
[Epoch 85] ogbg-molsider: 0.562979 test loss: 0.586560
[Epoch 86; Iter    30/   36] train: loss: 0.3432424
[Epoch 86] ogbg-molsider: 0.606934 val loss: 0.543260
[Epoch 86] ogbg-molsider: 0.564031 test loss: 0.615621
[Epoch 87; Iter    24/   36] train: loss: 0.3299787
[Epoch 87] ogbg-molsider: 0.626452 val loss: 0.526488
[Epoch 87] ogbg-molsider: 0.569754 test loss: 0.589257
[Epoch 88; Iter    18/   36] train: loss: 0.3143231
[Epoch 88] ogbg-molsider: 0.603677 val loss: 0.537432
[Epoch 88] ogbg-molsider: 0.565950 test loss: 0.611033
[Epoch 89; Iter    12/   36] train: loss: 0.2948739
[Epoch 89] ogbg-molsider: 0.625588 val loss: 0.587144
[Epoch 89] ogbg-molsider: 0.564112 test loss: 0.707992
[Epoch 90; Iter     6/   36] train: loss: 0.2966947
[Epoch 90; Iter    36/   36] train: loss: 0.3382585
[Epoch 90] ogbg-molsider: 0.630264 val loss: 0.536457
[Epoch 90] ogbg-molsider: 0.571844 test loss: 0.616279
[Epoch 91; Iter    30/   36] train: loss: 0.3344169
[Epoch 91] ogbg-molsider: 0.627937 val loss: 0.538512
[Epoch 91] ogbg-molsider: 0.588973 test loss: 0.594720
[Epoch 92; Iter    24/   36] train: loss: 0.3253705
[Epoch 92] ogbg-molsider: 0.621798 val loss: 0.561180
[Epoch 92] ogbg-molsider: 0.566930 test loss: 0.616964
[Epoch 93; Iter    18/   36] train: loss: 0.2890851
[Epoch 93] ogbg-molsider: 0.634896 val loss: 0.538178
[Epoch 93] ogbg-molsider: 0.567895 test loss: 0.625280
[Epoch 94; Iter    12/   36] train: loss: 0.2932825
[Epoch 94] ogbg-molsider: 0.603049 val loss: 0.576205
[Epoch 94] ogbg-molsider: 0.578673 test loss: 0.642237
[Epoch 95; Iter     6/   36] train: loss: 0.3544260
[Epoch 95; Iter    36/   36] train: loss: 0.3106286
[Epoch 95] ogbg-molsider: 0.619663 val loss: 0.546526
[Epoch 95] ogbg-molsider: 0.573598 test loss: 0.632853
[Epoch 96; Iter    30/   36] train: loss: 0.3314142
[Epoch 96] ogbg-molsider: 0.631227 val loss: 0.543897
[Epoch 96] ogbg-molsider: 0.577998 test loss: 0.635191
[Epoch 97; Iter    24/   36] train: loss: 0.3289265
[Epoch 97] ogbg-molsider: 0.622425 val loss: 0.565224
[Epoch 97] ogbg-molsider: 0.566443 test loss: 0.672541
[Epoch 98; Iter    18/   36] train: loss: 0.3371843
[Epoch 98] ogbg-molsider: 0.610133 val loss: 0.558815
[Epoch 98] ogbg-molsider: 0.573447 test loss: 0.642228
[Epoch 99; Iter    12/   36] train: loss: 0.3618392
[Epoch 99] ogbg-molsider: 0.612049 val loss: 0.563413
[Epoch 99] ogbg-molsider: 0.571921 test loss: 0.652055
[Epoch 100; Iter     6/   36] train: loss: 0.2600614
[Epoch 100; Iter    36/   36] train: loss: 0.3110866
[Epoch 100] ogbg-molsider: 0.651824 val loss: 0.543663
[Epoch 100] ogbg-molsider: 0.590386 test loss: 0.649291
[Epoch 101; Iter    30/   36] train: loss: 0.2976381
[Epoch 101] ogbg-molsider: 0.634143 val loss: 0.563485
[Epoch 101] ogbg-molsider: 0.584162 test loss: 0.657960
[Epoch 102; Iter    24/   36] train: loss: 0.3220647
[Epoch 102] ogbg-molsider: 0.624854 val loss: 0.565633
[Epoch 102] ogbg-molsider: 0.572144 test loss: 0.643397
[Epoch 103; Iter    18/   36] train: loss: 0.2843980
[Epoch 103] ogbg-molsider: 0.628523 val loss: 0.564026
[Epoch 103] ogbg-molsider: 0.582075 test loss: 0.647631
[Epoch 104; Iter    12/   36] train: loss: 0.2808409
[Epoch 104] ogbg-molsider: 0.608394 val loss: 0.587774
[Epoch 104] ogbg-molsider: 0.563605 test loss: 0.692682
[Epoch 105; Iter     6/   36] train: loss: 0.2548145
[Epoch 105; Iter    36/   36] train: loss: 0.3212283
[Epoch 105] ogbg-molsider: 0.618389 val loss: 0.581712
[Epoch 105] ogbg-molsider: 0.597814 test loss: 0.634942
[Epoch 106; Iter    30/   36] train: loss: 0.3204786
[Epoch 106] ogbg-molsider: 0.633735 val loss: 0.559056
[Epoch 106] ogbg-molsider: 0.583930 test loss: 0.627677
[Epoch 107; Iter    24/   36] train: loss: 0.2958412
[Epoch 107] ogbg-molsider: 0.631915 val loss: 0.577031
[Epoch 107] ogbg-molsider: 0.579849 test loss: 0.668948
[Epoch 108; Iter    18/   36] train: loss: 0.2826604
[Epoch 108] ogbg-molsider: 0.638493 val loss: 0.559515
[Epoch 108] ogbg-molsider: 0.592871 test loss: 0.635205
[Epoch 109; Iter    12/   36] train: loss: 0.2750845
[Epoch 109] ogbg-molsider: 0.638736 val loss: 0.570013
[Epoch 109] ogbg-molsider: 0.582854 test loss: 0.661871
[Epoch 110; Iter     6/   36] train: loss: 0.3116097
[Epoch 110; Iter    36/   36] train: loss: 0.2497399
[Epoch 110] ogbg-molsider: 0.639995 val loss: 0.562745
[Epoch 110] ogbg-molsider: 0.573563 test loss: 0.644949
[Epoch 111; Iter    30/   36] train: loss: 0.2617089
[Epoch 111] ogbg-molsider: 0.642706 val loss: 0.568040
[Epoch 111] ogbg-molsider: 0.588769 test loss: 0.656456
[Epoch 112; Iter    24/   36] train: loss: 0.2720540
[Epoch 112] ogbg-molsider: 0.634917 val loss: 0.572934
[Epoch 112] ogbg-molsider: 0.580386 test loss: 0.653781
[Epoch 113; Iter    18/   36] train: loss: 0.2373116
[Epoch 113] ogbg-molsider: 0.635648 val loss: 0.587748
[Epoch 113] ogbg-molsider: 0.576492 test loss: 0.684994
[Epoch 114; Iter    12/   36] train: loss: 0.2610817
[Epoch 114] ogbg-molsider: 0.645598 val loss: 0.576449
[Epoch 114] ogbg-molsider: 0.572919 test loss: 0.670553
[Epoch 115; Iter     6/   36] train: loss: 0.2652768
[Epoch 115; Iter    36/   36] train: loss: 0.2829675
[Epoch 115] ogbg-molsider: 0.636934 val loss: 0.574970
[Epoch 115] ogbg-molsider: 0.578819 test loss: 0.653853
[Epoch 116; Iter    30/   36] train: loss: 0.2617829
[Epoch 116] ogbg-molsider: 0.634409 val loss: 0.585061
[Epoch 116] ogbg-molsider: 0.581439 test loss: 0.683240
[Epoch 117; Iter    24/   36] train: loss: 0.2358841
[Epoch 117] ogbg-molsider: 0.635890 val loss: 0.587805
[Epoch 117] ogbg-molsider: 0.580714 test loss: 0.668789
[Epoch 118; Iter    18/   36] train: loss: 0.2987283
[Epoch 118] ogbg-molsider: 0.639596 val loss: 0.579788
[Epoch 118] ogbg-molsider: 0.585064 test loss: 0.668415
[Epoch 119; Iter    12/   36] train: loss: 0.2382403
[Epoch 119] ogbg-molsider: 0.641725 val loss: 0.590005
[Epoch 119] ogbg-molsider: 0.585367 test loss: 0.674740
[Epoch 120; Iter     6/   36] train: loss: 0.3016045
[Epoch 120; Iter    36/   36] train: loss: 0.2404716
[Epoch 120] ogbg-molsider: 0.645866 val loss: 0.573138
[Epoch 120] ogbg-molsider: 0.591391 test loss: 0.662296
[Epoch 121; Iter    30/   36] train: loss: 0.3076175
[Epoch 121] ogbg-molsider: 0.644050 val loss: 0.592849
[Epoch 121] ogbg-molsider: 0.592197 test loss: 0.669658
[Epoch 122; Iter    24/   36] train: loss: 0.2710577
[Epoch 122] ogbg-molsider: 0.642222 val loss: 0.593326
[Epoch 122] ogbg-molsider: 0.584512 test loss: 0.678609
[Epoch 123; Iter    18/   36] train: loss: 0.2817143
[Epoch 123] ogbg-molsider: 0.641286 val loss: 0.579735
[Epoch 123] ogbg-molsider: 0.578515 test loss: 0.676949
[Epoch 124; Iter    12/   36] train: loss: 0.2582863
[Epoch 124] ogbg-molsider: 0.637325 val loss: 0.596946
[Epoch 124] ogbg-molsider: 0.578827 test loss: 0.694412
[Epoch 125; Iter     6/   36] train: loss: 0.2229450
[Epoch 125; Iter    36/   36] train: loss: 0.2800639
[Epoch 125] ogbg-molsider: 0.639234 val loss: 0.594751
[Epoch 125] ogbg-molsider: 0.580912 test loss: 0.675795
[Epoch 126; Iter    30/   36] train: loss: 0.2529105
[Epoch 126] ogbg-molsider: 0.639593 val loss: 0.596830
[Epoch 126] ogbg-molsider: 0.584326 test loss: 0.685492
[Epoch 127; Iter    24/   36] train: loss: 0.2479568
[Epoch 127] ogbg-molsider: 0.636528 val loss: 0.593800
[Epoch 127] ogbg-molsider: 0.598141 test loss: 0.675890
[Epoch 128; Iter    18/   36] train: loss: 0.2557129
[Epoch 84] ogbg-molsider: 0.579458 test loss: 0.531781
[Epoch 85; Iter    12/   32] train: loss: 0.3892225
[Epoch 85] ogbg-molsider: 0.598016 val loss: 0.501179
[Epoch 85] ogbg-molsider: 0.572246 test loss: 0.537097
[Epoch 86; Iter    10/   32] train: loss: 0.4410314
[Epoch 86] ogbg-molsider: 0.585429 val loss: 0.585700
[Epoch 86] ogbg-molsider: 0.574755 test loss: 0.696718
[Epoch 87; Iter     8/   32] train: loss: 0.3907818
[Epoch 87] ogbg-molsider: 0.591044 val loss: 0.517266
[Epoch 87] ogbg-molsider: 0.580832 test loss: 0.559005
[Epoch 88; Iter     6/   32] train: loss: 0.3789279
[Epoch 88] ogbg-molsider: 0.558207 val loss: 0.531401
[Epoch 88] ogbg-molsider: 0.585000 test loss: 0.552670
[Epoch 89; Iter     4/   32] train: loss: 0.3962792
[Epoch 89] ogbg-molsider: 0.575409 val loss: 0.534591
[Epoch 89] ogbg-molsider: 0.584991 test loss: 0.547371
[Epoch 90; Iter     2/   32] train: loss: 0.3977986
[Epoch 90; Iter    32/   32] train: loss: 0.3872052
[Epoch 90] ogbg-molsider: 0.571147 val loss: 0.558508
[Epoch 90] ogbg-molsider: 0.570796 test loss: 0.557454
[Epoch 91; Iter    30/   32] train: loss: 0.3898213
[Epoch 91] ogbg-molsider: 0.588706 val loss: 0.526963
[Epoch 91] ogbg-molsider: 0.574900 test loss: 0.556143
[Epoch 92; Iter    28/   32] train: loss: 0.3711250
[Epoch 92] ogbg-molsider: 0.580348 val loss: 0.562624
[Epoch 92] ogbg-molsider: 0.555335 test loss: 0.652555
[Epoch 93; Iter    26/   32] train: loss: 0.3854032
[Epoch 93] ogbg-molsider: 0.562825 val loss: 0.548824
[Epoch 93] ogbg-molsider: 0.571671 test loss: 0.615780
[Epoch 94; Iter    24/   32] train: loss: 0.3736742
[Epoch 94] ogbg-molsider: 0.579957 val loss: 0.563289
[Epoch 94] ogbg-molsider: 0.570977 test loss: 0.587893
[Epoch 95; Iter    22/   32] train: loss: 0.3601294
[Epoch 95] ogbg-molsider: 0.574512 val loss: 0.544015
[Epoch 95] ogbg-molsider: 0.594646 test loss: 0.559164
[Epoch 96; Iter    20/   32] train: loss: 0.3435214
[Epoch 96] ogbg-molsider: 0.590666 val loss: 0.552131
[Epoch 96] ogbg-molsider: 0.582290 test loss: 0.568583
[Epoch 97; Iter    18/   32] train: loss: 0.3620208
[Epoch 97] ogbg-molsider: 0.577106 val loss: 0.557443
[Epoch 97] ogbg-molsider: 0.584159 test loss: 0.584342
[Epoch 98; Iter    16/   32] train: loss: 0.3412042
[Epoch 98] ogbg-molsider: 0.592809 val loss: 0.544881
[Epoch 98] ogbg-molsider: 0.579919 test loss: 0.571364
[Epoch 99; Iter    14/   32] train: loss: 0.3547129
[Epoch 99] ogbg-molsider: 0.574598 val loss: 0.560514
[Epoch 99] ogbg-molsider: 0.568098 test loss: 0.586570
[Epoch 100; Iter    12/   32] train: loss: 0.3173181
[Epoch 100] ogbg-molsider: 0.584002 val loss: 0.555957
[Epoch 100] ogbg-molsider: 0.576753 test loss: 0.584956
[Epoch 101; Iter    10/   32] train: loss: 0.3085771
[Epoch 101] ogbg-molsider: 0.577936 val loss: 0.579108
[Epoch 101] ogbg-molsider: 0.582452 test loss: 0.607639
[Epoch 102; Iter     8/   32] train: loss: 0.3234821
[Epoch 102] ogbg-molsider: 0.577646 val loss: 0.580001
[Epoch 102] ogbg-molsider: 0.566622 test loss: 0.614008
[Epoch 103; Iter     6/   32] train: loss: 0.3016606
[Epoch 103] ogbg-molsider: 0.581776 val loss: 0.571182
[Epoch 103] ogbg-molsider: 0.583553 test loss: 0.577517
[Epoch 104; Iter     4/   32] train: loss: 0.3045186
[Epoch 104] ogbg-molsider: 0.573038 val loss: 0.575633
[Epoch 104] ogbg-molsider: 0.565663 test loss: 0.606399
[Epoch 105; Iter     2/   32] train: loss: 0.3342554
[Epoch 105; Iter    32/   32] train: loss: 0.3297276
[Epoch 105] ogbg-molsider: 0.578727 val loss: 0.584540
[Epoch 105] ogbg-molsider: 0.586472 test loss: 0.618403
[Epoch 106; Iter    30/   32] train: loss: 0.3114205
[Epoch 106] ogbg-molsider: 0.585090 val loss: 0.571923
[Epoch 106] ogbg-molsider: 0.587405 test loss: 0.625894
[Epoch 107; Iter    28/   32] train: loss: 0.3545409
[Epoch 107] ogbg-molsider: 0.590841 val loss: 0.578519
[Epoch 107] ogbg-molsider: 0.588023 test loss: 0.603527
[Epoch 108; Iter    26/   32] train: loss: 0.3350222
[Epoch 108] ogbg-molsider: 0.590142 val loss: 0.560998
[Epoch 108] ogbg-molsider: 0.582022 test loss: 0.589059
[Epoch 109; Iter    24/   32] train: loss: 0.3177540
[Epoch 109] ogbg-molsider: 0.587720 val loss: 0.580654
[Epoch 109] ogbg-molsider: 0.583161 test loss: 0.617051
[Epoch 110; Iter    22/   32] train: loss: 0.3154148
[Epoch 110] ogbg-molsider: 0.587970 val loss: 0.576172
[Epoch 110] ogbg-molsider: 0.585395 test loss: 0.608927
[Epoch 111; Iter    20/   32] train: loss: 0.3088503
[Epoch 111] ogbg-molsider: 0.604207 val loss: 0.611147
[Epoch 111] ogbg-molsider: 0.565910 test loss: 0.714275
[Epoch 112; Iter    18/   32] train: loss: 0.3421253
[Epoch 112] ogbg-molsider: 0.575275 val loss: 0.592932
[Epoch 112] ogbg-molsider: 0.584384 test loss: 0.621342
[Epoch 113; Iter    16/   32] train: loss: 0.3676293
[Epoch 113] ogbg-molsider: 0.587118 val loss: 0.584013
[Epoch 113] ogbg-molsider: 0.582570 test loss: 0.616177
[Epoch 114; Iter    14/   32] train: loss: 0.3292843
[Epoch 114] ogbg-molsider: 0.594624 val loss: 0.597389
[Epoch 114] ogbg-molsider: 0.579865 test loss: 0.643549
[Epoch 115; Iter    12/   32] train: loss: 0.3012900
[Epoch 115] ogbg-molsider: 0.572799 val loss: 0.608613
[Epoch 115] ogbg-molsider: 0.588335 test loss: 0.629101
[Epoch 116; Iter    10/   32] train: loss: 0.3318546
[Epoch 116] ogbg-molsider: 0.590997 val loss: 0.605935
[Epoch 116] ogbg-molsider: 0.590162 test loss: 0.642518
[Epoch 117; Iter     8/   32] train: loss: 0.3103195
[Epoch 117] ogbg-molsider: 0.577717 val loss: 0.629925
[Epoch 117] ogbg-molsider: 0.580188 test loss: 0.662304
[Epoch 118; Iter     6/   32] train: loss: 0.3432177
[Epoch 118] ogbg-molsider: 0.591815 val loss: 0.593634
[Epoch 118] ogbg-molsider: 0.573300 test loss: 0.642872
[Epoch 119; Iter     4/   32] train: loss: 0.2882854
[Epoch 119] ogbg-molsider: 0.594212 val loss: 0.602312
[Epoch 119] ogbg-molsider: 0.583695 test loss: 0.646529
[Epoch 120; Iter     2/   32] train: loss: 0.3013193
[Epoch 120; Iter    32/   32] train: loss: 0.4542752
[Epoch 120] ogbg-molsider: 0.603835 val loss: 0.627363
[Epoch 120] ogbg-molsider: 0.567352 test loss: 0.795477
[Epoch 121; Iter    30/   32] train: loss: 0.2856957
[Epoch 121] ogbg-molsider: 0.592731 val loss: 0.599404
[Epoch 121] ogbg-molsider: 0.567279 test loss: 0.671872
[Epoch 122; Iter    28/   32] train: loss: 0.3336333
[Epoch 122] ogbg-molsider: 0.597031 val loss: 0.590515
[Epoch 122] ogbg-molsider: 0.582671 test loss: 0.635388
[Epoch 123; Iter    26/   32] train: loss: 0.2905909
[Epoch 123] ogbg-molsider: 0.591137 val loss: 0.594422
[Epoch 123] ogbg-molsider: 0.577038 test loss: 0.632745
[Epoch 124; Iter    24/   32] train: loss: 0.2957928
[Epoch 124] ogbg-molsider: 0.590435 val loss: 0.588732
[Epoch 124] ogbg-molsider: 0.582619 test loss: 0.628520
[Epoch 125; Iter    22/   32] train: loss: 0.2844526
[Epoch 125] ogbg-molsider: 0.593160 val loss: 0.599080
[Epoch 125] ogbg-molsider: 0.584711 test loss: 0.646169
[Epoch 126; Iter    20/   32] train: loss: 0.2529425
[Epoch 126] ogbg-molsider: 0.589559 val loss: 0.604026
[Epoch 126] ogbg-molsider: 0.579311 test loss: 0.637581
[Epoch 127; Iter    18/   32] train: loss: 0.2758250
[Epoch 127] ogbg-molsider: 0.596942 val loss: 0.600575
[Epoch 127] ogbg-molsider: 0.578831 test loss: 0.638031
[Epoch 128; Iter    16/   32] train: loss: 0.2818637
[Epoch 128] ogbg-molsider: 0.593903 val loss: 0.610877
[Epoch 128] ogbg-molsider: 0.575956 test loss: 0.659175
[Epoch 129; Iter    14/   32] train: loss: 0.2504976
[Epoch 129] ogbg-molsider: 0.596461 val loss: 0.599917
[Epoch 129] ogbg-molsider: 0.585303 test loss: 0.629219
[Epoch 130; Iter    12/   32] train: loss: 0.2888163
[Epoch 130] ogbg-molsider: 0.594430 val loss: 0.599860
[Epoch 130] ogbg-molsider: 0.577747 test loss: 0.646998
[Epoch 131; Iter    10/   32] train: loss: 0.2748444
[Epoch 131] ogbg-molsider: 0.592220 val loss: 0.613701
[Epoch 131] ogbg-molsider: 0.580232 test loss: 0.669378
[Epoch 132; Iter     8/   32] train: loss: 0.2769904
[Epoch 132] ogbg-molsider: 0.593742 val loss: 0.609656
[Epoch 132] ogbg-molsider: 0.576748 test loss: 0.644542
[Epoch 133; Iter     6/   32] train: loss: 0.2877850
[Epoch 133] ogbg-molsider: 0.591607 val loss: 0.622685
[Epoch 81; Iter    30/   36] train: loss: 0.2820487
[Epoch 81] ogbg-molsider: 0.626540 val loss: 0.532790
[Epoch 81] ogbg-molsider: 0.599513 test loss: 0.586491
[Epoch 82; Iter    24/   36] train: loss: 0.3135066
[Epoch 82] ogbg-molsider: 0.604250 val loss: 0.589821
[Epoch 82] ogbg-molsider: 0.583871 test loss: 0.640050
[Epoch 83; Iter    18/   36] train: loss: 0.2930122
[Epoch 83] ogbg-molsider: 0.612853 val loss: 0.557034
[Epoch 83] ogbg-molsider: 0.573600 test loss: 0.595446
[Epoch 84; Iter    12/   36] train: loss: 0.2810468
[Epoch 84] ogbg-molsider: 0.614765 val loss: 0.567547
[Epoch 84] ogbg-molsider: 0.581524 test loss: 0.600706
[Epoch 85; Iter     6/   36] train: loss: 0.3244215
[Epoch 85; Iter    36/   36] train: loss: 0.3006735
[Epoch 85] ogbg-molsider: 0.620260 val loss: 0.562161
[Epoch 85] ogbg-molsider: 0.585582 test loss: 0.594598
[Epoch 86; Iter    30/   36] train: loss: 0.3259622
[Epoch 86] ogbg-molsider: 0.610838 val loss: 0.564659
[Epoch 86] ogbg-molsider: 0.593517 test loss: 0.593476
[Epoch 87; Iter    24/   36] train: loss: 0.3220463
[Epoch 87] ogbg-molsider: 0.602605 val loss: 0.569106
[Epoch 87] ogbg-molsider: 0.594909 test loss: 0.600718
[Epoch 88; Iter    18/   36] train: loss: 0.3079362
[Epoch 88] ogbg-molsider: 0.604008 val loss: 0.572517
[Epoch 88] ogbg-molsider: 0.579637 test loss: 0.607627
[Epoch 89; Iter    12/   36] train: loss: 0.2825350
[Epoch 89] ogbg-molsider: 0.622024 val loss: 0.574019
[Epoch 89] ogbg-molsider: 0.588030 test loss: 0.601385
[Epoch 90; Iter     6/   36] train: loss: 0.2873626
[Epoch 90; Iter    36/   36] train: loss: 0.3081556
[Epoch 90] ogbg-molsider: 0.614017 val loss: 0.582130
[Epoch 90] ogbg-molsider: 0.584959 test loss: 0.614350
[Epoch 91; Iter    30/   36] train: loss: 0.3430811
[Epoch 91] ogbg-molsider: 0.601702 val loss: 0.621965
[Epoch 91] ogbg-molsider: 0.578785 test loss: 0.661073
[Epoch 92; Iter    24/   36] train: loss: 0.3178396
[Epoch 92] ogbg-molsider: 0.606082 val loss: 0.582050
[Epoch 92] ogbg-molsider: 0.594540 test loss: 0.604796
[Epoch 93; Iter    18/   36] train: loss: 0.3077202
[Epoch 93] ogbg-molsider: 0.605786 val loss: 0.606502
[Epoch 93] ogbg-molsider: 0.586071 test loss: 0.638537
[Epoch 94; Iter    12/   36] train: loss: 0.2688508
[Epoch 94] ogbg-molsider: 0.605358 val loss: 0.610164
[Epoch 94] ogbg-molsider: 0.595836 test loss: 0.630013
[Epoch 95; Iter     6/   36] train: loss: 0.2756413
[Epoch 95; Iter    36/   36] train: loss: 0.2851505
[Epoch 95] ogbg-molsider: 0.602952 val loss: 0.609935
[Epoch 95] ogbg-molsider: 0.584631 test loss: 0.632133
[Epoch 96; Iter    30/   36] train: loss: 0.2738438
[Epoch 96] ogbg-molsider: 0.592732 val loss: 0.612443
[Epoch 96] ogbg-molsider: 0.579868 test loss: 0.633010
[Epoch 97; Iter    24/   36] train: loss: 0.2712475
[Epoch 97] ogbg-molsider: 0.622292 val loss: 0.599747
[Epoch 97] ogbg-molsider: 0.598720 test loss: 0.642409
[Epoch 98; Iter    18/   36] train: loss: 0.2814689
[Epoch 98] ogbg-molsider: 0.605814 val loss: 0.607471
[Epoch 98] ogbg-molsider: 0.583349 test loss: 0.638300
[Epoch 99; Iter    12/   36] train: loss: 0.2591732
[Epoch 99] ogbg-molsider: 0.602487 val loss: 0.636666
[Epoch 99] ogbg-molsider: 0.584264 test loss: 0.678230
[Epoch 100; Iter     6/   36] train: loss: 0.2615018
[Epoch 100; Iter    36/   36] train: loss: 0.2899708
[Epoch 100] ogbg-molsider: 0.603222 val loss: 0.611317
[Epoch 100] ogbg-molsider: 0.583149 test loss: 0.630272
[Epoch 101; Iter    30/   36] train: loss: 0.3088325
[Epoch 101] ogbg-molsider: 0.602040 val loss: 0.633035
[Epoch 101] ogbg-molsider: 0.594070 test loss: 0.635325
[Epoch 102; Iter    24/   36] train: loss: 0.3051458
[Epoch 102] ogbg-molsider: 0.605552 val loss: 0.642404
[Epoch 102] ogbg-molsider: 0.590088 test loss: 0.682390
[Epoch 103; Iter    18/   36] train: loss: 0.2935725
[Epoch 103] ogbg-molsider: 0.587363 val loss: 0.664924
[Epoch 103] ogbg-molsider: 0.578345 test loss: 0.715117
[Epoch 104; Iter    12/   36] train: loss: 0.2906132
[Epoch 104] ogbg-molsider: 0.610095 val loss: 0.621353
[Epoch 104] ogbg-molsider: 0.585499 test loss: 0.631978
[Epoch 105; Iter     6/   36] train: loss: 0.3042749
[Epoch 105; Iter    36/   36] train: loss: 0.2932114
[Epoch 105] ogbg-molsider: 0.598636 val loss: 0.622764
[Epoch 105] ogbg-molsider: 0.583270 test loss: 0.644013
[Epoch 106; Iter    30/   36] train: loss: 0.2514420
[Epoch 106] ogbg-molsider: 0.617575 val loss: 0.611617
[Epoch 106] ogbg-molsider: 0.585745 test loss: 0.640272
[Epoch 107; Iter    24/   36] train: loss: 0.2397031
[Epoch 107] ogbg-molsider: 0.613089 val loss: 0.616620
[Epoch 107] ogbg-molsider: 0.589592 test loss: 0.640895
[Epoch 108; Iter    18/   36] train: loss: 0.2668926
[Epoch 108] ogbg-molsider: 0.606814 val loss: 0.619572
[Epoch 108] ogbg-molsider: 0.584861 test loss: 0.634044
[Epoch 109; Iter    12/   36] train: loss: 0.2408449
[Epoch 109] ogbg-molsider: 0.613310 val loss: 0.620588
[Epoch 109] ogbg-molsider: 0.597629 test loss: 0.647238
[Epoch 110; Iter     6/   36] train: loss: 0.2480775
[Epoch 110; Iter    36/   36] train: loss: 0.2768344
[Epoch 110] ogbg-molsider: 0.604792 val loss: 0.644742
[Epoch 110] ogbg-molsider: 0.597176 test loss: 0.647482
[Epoch 111; Iter    30/   36] train: loss: 0.2788927
[Epoch 111] ogbg-molsider: 0.607255 val loss: 0.638893
[Epoch 111] ogbg-molsider: 0.597727 test loss: 0.649049
[Epoch 112; Iter    24/   36] train: loss: 0.2552307
[Epoch 112] ogbg-molsider: 0.604568 val loss: 0.650226
[Epoch 112] ogbg-molsider: 0.578754 test loss: 0.678327
[Epoch 113; Iter    18/   36] train: loss: 0.2625991
[Epoch 113] ogbg-molsider: 0.609456 val loss: 0.640016
[Epoch 113] ogbg-molsider: 0.597624 test loss: 0.654087
[Epoch 114; Iter    12/   36] train: loss: 0.2255092
[Epoch 114] ogbg-molsider: 0.608469 val loss: 0.649565
[Epoch 114] ogbg-molsider: 0.588858 test loss: 0.666258
[Epoch 115; Iter     6/   36] train: loss: 0.2398219
[Epoch 115; Iter    36/   36] train: loss: 0.2825623
[Epoch 115] ogbg-molsider: 0.608864 val loss: 0.646986
[Epoch 115] ogbg-molsider: 0.591197 test loss: 0.668609
[Epoch 116; Iter    30/   36] train: loss: 0.2524449
[Epoch 116] ogbg-molsider: 0.611088 val loss: 0.657009
[Epoch 116] ogbg-molsider: 0.591740 test loss: 0.681064
[Epoch 117; Iter    24/   36] train: loss: 0.2535421
[Epoch 117] ogbg-molsider: 0.607152 val loss: 0.652851
[Epoch 117] ogbg-molsider: 0.583643 test loss: 0.681490
[Epoch 118; Iter    18/   36] train: loss: 0.2483834
[Epoch 118] ogbg-molsider: 0.603140 val loss: 0.657170
[Epoch 118] ogbg-molsider: 0.596389 test loss: 0.672308
[Epoch 119; Iter    12/   36] train: loss: 0.2248347
[Epoch 119] ogbg-molsider: 0.604133 val loss: 0.677457
[Epoch 119] ogbg-molsider: 0.594866 test loss: 0.702348
[Epoch 120; Iter     6/   36] train: loss: 0.2286492
[Epoch 120; Iter    36/   36] train: loss: 0.2935643
[Epoch 120] ogbg-molsider: 0.602357 val loss: 0.676317
[Epoch 120] ogbg-molsider: 0.594747 test loss: 0.716511
[Epoch 121; Iter    30/   36] train: loss: 0.2278886
[Epoch 121] ogbg-molsider: 0.600894 val loss: 0.671945
[Epoch 121] ogbg-molsider: 0.596162 test loss: 0.670502
[Epoch 122; Iter    24/   36] train: loss: 0.2386089
[Epoch 122] ogbg-molsider: 0.608985 val loss: 0.657318
[Epoch 122] ogbg-molsider: 0.598149 test loss: 0.673529
[Epoch 123; Iter    18/   36] train: loss: 0.2140107
[Epoch 123] ogbg-molsider: 0.611232 val loss: 0.666707
[Epoch 123] ogbg-molsider: 0.592910 test loss: 0.674867
[Epoch 124; Iter    12/   36] train: loss: 0.2300157
[Epoch 124] ogbg-molsider: 0.612191 val loss: 0.655150
[Epoch 124] ogbg-molsider: 0.595466 test loss: 0.667364
[Epoch 125; Iter     6/   36] train: loss: 0.2247727
[Epoch 125; Iter    36/   36] train: loss: 0.2215186
[Epoch 125] ogbg-molsider: 0.611342 val loss: 0.670155
[Epoch 125] ogbg-molsider: 0.597499 test loss: 0.703127
[Epoch 126; Iter    30/   36] train: loss: 0.2313715
[Epoch 126] ogbg-molsider: 0.602799 val loss: 0.673808
[Epoch 126] ogbg-molsider: 0.589333 test loss: 0.695018
[Epoch 127; Iter    24/   36] train: loss: 0.2607642
[Epoch 127] ogbg-molsider: 0.601074 val loss: 0.725588
[Epoch 127] ogbg-molsider: 0.597267 test loss: 0.751938
[Epoch 128; Iter    18/   36] train: loss: 0.2411035
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 89] ogbg-molsider: 0.551628 val loss: 0.576282
[Epoch 89] ogbg-molsider: 0.595186 test loss: 0.550820
[Epoch 90; Iter    27/   27] train: loss: 0.4219976
[Epoch 90] ogbg-molsider: 0.598897 val loss: 0.555174
[Epoch 90] ogbg-molsider: 0.585947 test loss: 0.541595
[Epoch 91] ogbg-molsider: 0.587300 val loss: 0.582109
[Epoch 91] ogbg-molsider: 0.572162 test loss: 0.568090
[Epoch 92; Iter     3/   27] train: loss: 0.3819109
[Epoch 92] ogbg-molsider: 0.586270 val loss: 0.565118
[Epoch 92] ogbg-molsider: 0.575519 test loss: 0.561340
[Epoch 93; Iter     6/   27] train: loss: 0.3556653
[Epoch 93] ogbg-molsider: 0.595772 val loss: 0.574778
[Epoch 93] ogbg-molsider: 0.567081 test loss: 0.566328
[Epoch 94; Iter     9/   27] train: loss: 0.3517401
[Epoch 94] ogbg-molsider: 0.588112 val loss: 0.569697
[Epoch 94] ogbg-molsider: 0.576130 test loss: 0.542245
[Epoch 95; Iter    12/   27] train: loss: 0.4363446
[Epoch 95] ogbg-molsider: 0.588164 val loss: 0.559200
[Epoch 95] ogbg-molsider: 0.597662 test loss: 0.530803
[Epoch 96; Iter    15/   27] train: loss: 0.3460474
[Epoch 96] ogbg-molsider: 0.585230 val loss: 0.580755
[Epoch 96] ogbg-molsider: 0.569003 test loss: 0.566822
[Epoch 97; Iter    18/   27] train: loss: 0.4126769
[Epoch 97] ogbg-molsider: 0.586436 val loss: 0.559547
[Epoch 97] ogbg-molsider: 0.579942 test loss: 0.542947
[Epoch 98; Iter    21/   27] train: loss: 0.3831663
[Epoch 98] ogbg-molsider: 0.584705 val loss: 0.580458
[Epoch 98] ogbg-molsider: 0.597746 test loss: 0.551863
[Epoch 99; Iter    24/   27] train: loss: 0.3565224
[Epoch 99] ogbg-molsider: 0.598808 val loss: 0.579827
[Epoch 99] ogbg-molsider: 0.578206 test loss: 0.570194
[Epoch 100; Iter    27/   27] train: loss: 0.3549891
[Epoch 100] ogbg-molsider: 0.592314 val loss: 0.626689
[Epoch 100] ogbg-molsider: 0.571306 test loss: 0.627167
[Epoch 101] ogbg-molsider: 0.587024 val loss: 0.572211
[Epoch 101] ogbg-molsider: 0.569890 test loss: 0.567209
[Epoch 102; Iter     3/   27] train: loss: 0.3257295
[Epoch 102] ogbg-molsider: 0.604853 val loss: 0.581506
[Epoch 102] ogbg-molsider: 0.564204 test loss: 0.639744
[Epoch 103; Iter     6/   27] train: loss: 0.3553142
[Epoch 103] ogbg-molsider: 0.597122 val loss: 0.563698
[Epoch 103] ogbg-molsider: 0.592283 test loss: 0.556991
[Epoch 104; Iter     9/   27] train: loss: 0.3392643
[Epoch 104] ogbg-molsider: 0.605310 val loss: 0.568926
[Epoch 104] ogbg-molsider: 0.574434 test loss: 0.584086
[Epoch 105; Iter    12/   27] train: loss: 0.3492071
[Epoch 105] ogbg-molsider: 0.604251 val loss: 0.571700
[Epoch 105] ogbg-molsider: 0.586613 test loss: 0.558345
[Epoch 106; Iter    15/   27] train: loss: 0.3747311
[Epoch 106] ogbg-molsider: 0.587304 val loss: 0.616421
[Epoch 106] ogbg-molsider: 0.576726 test loss: 0.599801
[Epoch 107; Iter    18/   27] train: loss: 0.3189871
[Epoch 107] ogbg-molsider: 0.602032 val loss: 0.573063
[Epoch 107] ogbg-molsider: 0.587906 test loss: 0.556913
[Epoch 108; Iter    21/   27] train: loss: 0.3295358
[Epoch 108] ogbg-molsider: 0.598314 val loss: 0.606433
[Epoch 108] ogbg-molsider: 0.588218 test loss: 0.597926
[Epoch 109; Iter    24/   27] train: loss: 0.3238817
[Epoch 109] ogbg-molsider: 0.601839 val loss: 0.614444
[Epoch 109] ogbg-molsider: 0.581958 test loss: 0.620939
[Epoch 110; Iter    27/   27] train: loss: 0.3773293
[Epoch 110] ogbg-molsider: 0.588007 val loss: 0.614947
[Epoch 110] ogbg-molsider: 0.577724 test loss: 0.595145
[Epoch 111] ogbg-molsider: 0.601270 val loss: 0.589248
[Epoch 111] ogbg-molsider: 0.610320 test loss: 0.566858
[Epoch 112; Iter     3/   27] train: loss: 0.3083114
[Epoch 112] ogbg-molsider: 0.607759 val loss: 0.589273
[Epoch 112] ogbg-molsider: 0.594368 test loss: 0.575276
[Epoch 113; Iter     6/   27] train: loss: 0.3051212
[Epoch 113] ogbg-molsider: 0.604267 val loss: 0.622495
[Epoch 113] ogbg-molsider: 0.578893 test loss: 0.612794
[Epoch 114; Iter     9/   27] train: loss: 0.2924152
[Epoch 114] ogbg-molsider: 0.584441 val loss: 0.630716
[Epoch 114] ogbg-molsider: 0.593294 test loss: 0.595592
[Epoch 115; Iter    12/   27] train: loss: 0.2973472
[Epoch 115] ogbg-molsider: 0.597330 val loss: 0.628639
[Epoch 115] ogbg-molsider: 0.599941 test loss: 0.593620
[Epoch 116; Iter    15/   27] train: loss: 0.3010628
[Epoch 116] ogbg-molsider: 0.598125 val loss: 0.616656
[Epoch 116] ogbg-molsider: 0.594005 test loss: 0.598998
[Epoch 117; Iter    18/   27] train: loss: 0.3154095
[Epoch 117] ogbg-molsider: 0.594749 val loss: 0.613339
[Epoch 117] ogbg-molsider: 0.601804 test loss: 0.580509
[Epoch 118; Iter    21/   27] train: loss: 0.2924579
[Epoch 118] ogbg-molsider: 0.598519 val loss: 0.599559
[Epoch 118] ogbg-molsider: 0.597610 test loss: 0.580328
[Epoch 119; Iter    24/   27] train: loss: 0.3264357
[Epoch 119] ogbg-molsider: 0.596419 val loss: 0.624966
[Epoch 119] ogbg-molsider: 0.600492 test loss: 0.596552
[Epoch 120; Iter    27/   27] train: loss: 0.3138732
[Epoch 120] ogbg-molsider: 0.598680 val loss: 0.604465
[Epoch 120] ogbg-molsider: 0.599361 test loss: 0.592575
[Epoch 121] ogbg-molsider: 0.601843 val loss: 0.633940
[Epoch 121] ogbg-molsider: 0.603267 test loss: 0.610753
[Epoch 122; Iter     3/   27] train: loss: 0.2882049
[Epoch 122] ogbg-molsider: 0.591415 val loss: 0.625217
[Epoch 122] ogbg-molsider: 0.589970 test loss: 0.608524
[Epoch 123; Iter     6/   27] train: loss: 0.2755617
[Epoch 123] ogbg-molsider: 0.594761 val loss: 0.645281
[Epoch 123] ogbg-molsider: 0.591485 test loss: 0.626289
[Epoch 124; Iter     9/   27] train: loss: 0.2653488
[Epoch 124] ogbg-molsider: 0.591158 val loss: 0.633497
[Epoch 124] ogbg-molsider: 0.599063 test loss: 0.596106
[Epoch 125; Iter    12/   27] train: loss: 0.2982794
[Epoch 125] ogbg-molsider: 0.592720 val loss: 0.637945
[Epoch 125] ogbg-molsider: 0.585175 test loss: 0.634414
[Epoch 126; Iter    15/   27] train: loss: 0.2507795
[Epoch 126] ogbg-molsider: 0.595819 val loss: 0.635709
[Epoch 126] ogbg-molsider: 0.588756 test loss: 0.639760
[Epoch 127; Iter    18/   27] train: loss: 0.2843706
[Epoch 127] ogbg-molsider: 0.600024 val loss: 0.624166
[Epoch 127] ogbg-molsider: 0.598932 test loss: 0.607101
[Epoch 128; Iter    21/   27] train: loss: 0.3206580
[Epoch 128] ogbg-molsider: 0.592500 val loss: 0.645916
[Epoch 128] ogbg-molsider: 0.599224 test loss: 0.646953
[Epoch 129; Iter    24/   27] train: loss: 0.2992858
[Epoch 129] ogbg-molsider: 0.595846 val loss: 0.631589
[Epoch 129] ogbg-molsider: 0.596417 test loss: 0.616027
[Epoch 130; Iter    27/   27] train: loss: 0.3115174
[Epoch 130] ogbg-molsider: 0.594099 val loss: 0.650268
[Epoch 130] ogbg-molsider: 0.607430 test loss: 0.629037
[Epoch 131] ogbg-molsider: 0.595674 val loss: 0.680662
[Epoch 131] ogbg-molsider: 0.583111 test loss: 0.687600
[Epoch 132; Iter     3/   27] train: loss: 0.2742956
[Epoch 132] ogbg-molsider: 0.591941 val loss: 0.637063
[Epoch 132] ogbg-molsider: 0.589864 test loss: 0.633405
[Epoch 133; Iter     6/   27] train: loss: 0.2579205
[Epoch 133] ogbg-molsider: 0.596705 val loss: 0.648288
[Epoch 133] ogbg-molsider: 0.598556 test loss: 0.635858
[Epoch 134; Iter     9/   27] train: loss: 0.2822554
[Epoch 134] ogbg-molsider: 0.587450 val loss: 0.636252
[Epoch 134] ogbg-molsider: 0.591825 test loss: 0.618983
[Epoch 135; Iter    12/   27] train: loss: 0.2545580
[Epoch 135] ogbg-molsider: 0.595561 val loss: 0.676647
[Epoch 135] ogbg-molsider: 0.589578 test loss: 0.677101
[Epoch 136; Iter    15/   27] train: loss: 0.2622027
[Epoch 136] ogbg-molsider: 0.591361 val loss: 0.649282
[Epoch 136] ogbg-molsider: 0.595905 test loss: 0.624548
[Epoch 137; Iter    18/   27] train: loss: 0.2913008
[Epoch 137] ogbg-molsider: 0.589098 val loss: 0.686398
[Epoch 137] ogbg-molsider: 0.585015 test loss: 0.683186
[Epoch 138; Iter    21/   27] train: loss: 0.2619789
[Epoch 138] ogbg-molsider: 0.600109 val loss: 0.661270
[Epoch 138] ogbg-molsider: 0.596937 test loss: 0.646515
[Epoch 139; Iter    24/   27] train: loss: 0.2732158
[Epoch 139] ogbg-molsider: 0.588704 val loss: 0.667628
[Epoch 139] ogbg-molsider: 0.592647 test loss: 0.652617
[Epoch 140; Iter    27/   27] train: loss: 0.2985640
[Epoch 140] ogbg-molsider: 0.597676 val loss: 0.659509
[Epoch 140] ogbg-molsider: 0.602607 test loss: 0.644957
[Epoch 128] ogbg-molsider: 0.604020 val loss: 0.670119
[Epoch 128] ogbg-molsider: 0.593998 test loss: 0.693735
[Epoch 129; Iter    12/   36] train: loss: 0.2307645
[Epoch 129] ogbg-molsider: 0.616450 val loss: 0.681895
[Epoch 129] ogbg-molsider: 0.600354 test loss: 0.715927
Early stopping criterion based on -ogbg-molsider- that should be max reached after 129 epochs. Best model checkpoint was in epoch 69.
Statistics on  val_best_checkpoint
mean_pred: 0.574234664440155
std_pred: 2.690805673599243
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6741287452669502
rocauc: 0.641688248227934
ogbg-molsider: 0.641688248227934
OGBNanLabelBCEWithLogitsLoss: 0.49615824818611143
Statistics on  test
mean_pred: 0.47637927532196045
std_pred: 2.58229923248291
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6292765192040605
rocauc: 0.6140147950992225
ogbg-molsider: 0.6140147950992225
OGBNanLabelBCEWithLogitsLoss: 0.532723480463028
Statistics on  train
mean_pred: 0.15474160015583038
std_pred: 17.588672637939453
mean_targets: 0.5641575455665588
std_targets: 0.4958747327327728
prcauc: 0.8140396543960373
rocauc: 0.8678537447249568
ogbg-molsider: 0.8678537447249568
OGBNanLabelBCEWithLogitsLoss: 0.39190101209614014
[Epoch 89] ogbg-molsider: 0.591678 val loss: 0.593004
[Epoch 89] ogbg-molsider: 0.588696 test loss: 0.563502
[Epoch 90; Iter    27/   27] train: loss: 0.3685713
[Epoch 90] ogbg-molsider: 0.585455 val loss: 0.583623
[Epoch 90] ogbg-molsider: 0.586390 test loss: 0.548504
[Epoch 91] ogbg-molsider: 0.575831 val loss: 0.585867
[Epoch 91] ogbg-molsider: 0.572849 test loss: 0.558350
[Epoch 92; Iter     3/   27] train: loss: 0.3102991
[Epoch 92] ogbg-molsider: 0.584311 val loss: 0.584363
[Epoch 92] ogbg-molsider: 0.580838 test loss: 0.548875
[Epoch 93; Iter     6/   27] train: loss: 0.3346167
[Epoch 93] ogbg-molsider: 0.585675 val loss: 0.582733
[Epoch 93] ogbg-molsider: 0.573900 test loss: 0.554401
[Epoch 94; Iter     9/   27] train: loss: 0.3001160
[Epoch 94] ogbg-molsider: 0.590457 val loss: 0.593328
[Epoch 94] ogbg-molsider: 0.577751 test loss: 0.568020
[Epoch 95; Iter    12/   27] train: loss: 0.3071977
[Epoch 95] ogbg-molsider: 0.587224 val loss: 0.650393
[Epoch 95] ogbg-molsider: 0.587417 test loss: 0.607803
[Epoch 96; Iter    15/   27] train: loss: 0.3140647
[Epoch 96] ogbg-molsider: 0.586354 val loss: 0.586062
[Epoch 96] ogbg-molsider: 0.577954 test loss: 0.564551
[Epoch 97; Iter    18/   27] train: loss: 0.3347860
[Epoch 97] ogbg-molsider: 0.581549 val loss: 0.583918
[Epoch 97] ogbg-molsider: 0.567249 test loss: 0.575887
[Epoch 98; Iter    21/   27] train: loss: 0.3057022
[Epoch 98] ogbg-molsider: 0.592616 val loss: 0.602527
[Epoch 98] ogbg-molsider: 0.587978 test loss: 0.575408
[Epoch 99; Iter    24/   27] train: loss: 0.3021844
[Epoch 99] ogbg-molsider: 0.582212 val loss: 0.601290
[Epoch 99] ogbg-molsider: 0.570923 test loss: 0.578539
[Epoch 100; Iter    27/   27] train: loss: 0.3603372
[Epoch 100] ogbg-molsider: 0.589020 val loss: 0.613804
[Epoch 100] ogbg-molsider: 0.575474 test loss: 0.622158
[Epoch 101] ogbg-molsider: 0.585029 val loss: 0.621189
[Epoch 101] ogbg-molsider: 0.589045 test loss: 0.604487
[Epoch 102; Iter     3/   27] train: loss: 0.3124486
[Epoch 102] ogbg-molsider: 0.584357 val loss: 0.617124
[Epoch 102] ogbg-molsider: 0.589574 test loss: 0.593281
[Epoch 103; Iter     6/   27] train: loss: 0.3371762
[Epoch 103] ogbg-molsider: 0.585981 val loss: 0.610497
[Epoch 103] ogbg-molsider: 0.580702 test loss: 0.594401
[Epoch 104; Iter     9/   27] train: loss: 0.2902466
[Epoch 104] ogbg-molsider: 0.587585 val loss: 0.607106
[Epoch 104] ogbg-molsider: 0.579990 test loss: 0.595693
[Epoch 105; Iter    12/   27] train: loss: 0.3172111
[Epoch 105] ogbg-molsider: 0.594432 val loss: 0.628874
[Epoch 105] ogbg-molsider: 0.580835 test loss: 0.625109
[Epoch 106; Iter    15/   27] train: loss: 0.3415983
[Epoch 106] ogbg-molsider: 0.589654 val loss: 0.618838
[Epoch 106] ogbg-molsider: 0.590390 test loss: 0.597270
[Epoch 107; Iter    18/   27] train: loss: 0.3103294
[Epoch 107] ogbg-molsider: 0.599391 val loss: 0.617270
[Epoch 107] ogbg-molsider: 0.575405 test loss: 0.609084
[Epoch 108; Iter    21/   27] train: loss: 0.2964764
[Epoch 108] ogbg-molsider: 0.575745 val loss: 0.623681
[Epoch 108] ogbg-molsider: 0.581313 test loss: 0.603748
[Epoch 109; Iter    24/   27] train: loss: 0.3421524
[Epoch 109] ogbg-molsider: 0.582930 val loss: 0.647262
[Epoch 109] ogbg-molsider: 0.576305 test loss: 0.637383
[Epoch 110; Iter    27/   27] train: loss: 0.3491772
[Epoch 110] ogbg-molsider: 0.590518 val loss: 0.637999
[Epoch 110] ogbg-molsider: 0.566100 test loss: 0.691054
[Epoch 111] ogbg-molsider: 0.585460 val loss: 0.618404
[Epoch 111] ogbg-molsider: 0.577890 test loss: 0.612317
[Epoch 112; Iter     3/   27] train: loss: 0.3018997
[Epoch 112] ogbg-molsider: 0.588248 val loss: 0.627922
[Epoch 112] ogbg-molsider: 0.579868 test loss: 0.606198
[Epoch 113; Iter     6/   27] train: loss: 0.3188423
[Epoch 113] ogbg-molsider: 0.583580 val loss: 0.630461
[Epoch 113] ogbg-molsider: 0.581061 test loss: 0.622147
[Epoch 114; Iter     9/   27] train: loss: 0.2781841
[Epoch 114] ogbg-molsider: 0.585154 val loss: 0.637043
[Epoch 114] ogbg-molsider: 0.577853 test loss: 0.614114
[Epoch 115; Iter    12/   27] train: loss: 0.2735017
[Epoch 115] ogbg-molsider: 0.580652 val loss: 0.660267
[Epoch 115] ogbg-molsider: 0.584375 test loss: 0.664505
[Epoch 116; Iter    15/   27] train: loss: 0.2365243
[Epoch 116] ogbg-molsider: 0.592558 val loss: 0.631124
[Epoch 116] ogbg-molsider: 0.595610 test loss: 0.596256
[Epoch 117; Iter    18/   27] train: loss: 0.3056851
[Epoch 117] ogbg-molsider: 0.592907 val loss: 0.637248
[Epoch 117] ogbg-molsider: 0.590458 test loss: 0.612796
[Epoch 118; Iter    21/   27] train: loss: 0.2492222
[Epoch 118] ogbg-molsider: 0.587744 val loss: 0.640272
[Epoch 118] ogbg-molsider: 0.587382 test loss: 0.612852
[Epoch 119; Iter    24/   27] train: loss: 0.2852938
[Epoch 119] ogbg-molsider: 0.590282 val loss: 0.643953
[Epoch 119] ogbg-molsider: 0.589097 test loss: 0.615860
[Epoch 120; Iter    27/   27] train: loss: 0.2631162
[Epoch 120] ogbg-molsider: 0.588792 val loss: 0.645961
[Epoch 120] ogbg-molsider: 0.589359 test loss: 0.621513
[Epoch 121] ogbg-molsider: 0.589714 val loss: 0.643984
[Epoch 121] ogbg-molsider: 0.586645 test loss: 0.619267
[Epoch 122; Iter     3/   27] train: loss: 0.2466421
[Epoch 122] ogbg-molsider: 0.588859 val loss: 0.654772
[Epoch 122] ogbg-molsider: 0.584571 test loss: 0.635362
[Epoch 123; Iter     6/   27] train: loss: 0.2448260
[Epoch 123] ogbg-molsider: 0.593332 val loss: 0.661123
[Epoch 123] ogbg-molsider: 0.587802 test loss: 0.632110
[Epoch 124; Iter     9/   27] train: loss: 0.2805227
[Epoch 124] ogbg-molsider: 0.594579 val loss: 0.656311
[Epoch 124] ogbg-molsider: 0.590375 test loss: 0.637234
[Epoch 125; Iter    12/   27] train: loss: 0.2368197
[Epoch 125] ogbg-molsider: 0.591266 val loss: 0.659967
[Epoch 125] ogbg-molsider: 0.583331 test loss: 0.649077
[Epoch 126; Iter    15/   27] train: loss: 0.2537270
[Epoch 126] ogbg-molsider: 0.584899 val loss: 0.658779
[Epoch 126] ogbg-molsider: 0.578271 test loss: 0.638174
[Epoch 127; Iter    18/   27] train: loss: 0.2122103
[Epoch 127] ogbg-molsider: 0.588964 val loss: 0.654812
[Epoch 127] ogbg-molsider: 0.587739 test loss: 0.637030
[Epoch 128; Iter    21/   27] train: loss: 0.2631204
[Epoch 128] ogbg-molsider: 0.583985 val loss: 0.673661
[Epoch 128] ogbg-molsider: 0.591561 test loss: 0.641657
[Epoch 129; Iter    24/   27] train: loss: 0.2943641
[Epoch 129] ogbg-molsider: 0.589031 val loss: 0.656622
[Epoch 129] ogbg-molsider: 0.580406 test loss: 0.638505
[Epoch 130; Iter    27/   27] train: loss: 0.2435517
[Epoch 130] ogbg-molsider: 0.587469 val loss: 0.670210
[Epoch 130] ogbg-molsider: 0.577616 test loss: 0.664501
[Epoch 131] ogbg-molsider: 0.587314 val loss: 0.667573
[Epoch 131] ogbg-molsider: 0.574645 test loss: 0.664245
[Epoch 132; Iter     3/   27] train: loss: 0.2667404
[Epoch 132] ogbg-molsider: 0.584123 val loss: 0.665503
[Epoch 132] ogbg-molsider: 0.571816 test loss: 0.657074
[Epoch 133; Iter     6/   27] train: loss: 0.2595177
[Epoch 133] ogbg-molsider: 0.584092 val loss: 0.672433
[Epoch 133] ogbg-molsider: 0.583032 test loss: 0.655973
[Epoch 134; Iter     9/   27] train: loss: 0.2515633
[Epoch 134] ogbg-molsider: 0.589870 val loss: 0.671653
[Epoch 134] ogbg-molsider: 0.584013 test loss: 0.670579
[Epoch 135; Iter    12/   27] train: loss: 0.2458099
[Epoch 135] ogbg-molsider: 0.585797 val loss: 0.678122
[Epoch 135] ogbg-molsider: 0.576178 test loss: 0.676284
[Epoch 136; Iter    15/   27] train: loss: 0.2294505
[Epoch 136] ogbg-molsider: 0.582996 val loss: 0.714413
[Epoch 136] ogbg-molsider: 0.583158 test loss: 0.692953
[Epoch 137; Iter    18/   27] train: loss: 0.2330873
[Epoch 137] ogbg-molsider: 0.587506 val loss: 0.678672
[Epoch 137] ogbg-molsider: 0.580854 test loss: 0.654340
[Epoch 138; Iter    21/   27] train: loss: 0.2649373
[Epoch 138] ogbg-molsider: 0.583594 val loss: 0.690502
[Epoch 138] ogbg-molsider: 0.581013 test loss: 0.674964
[Epoch 139; Iter    24/   27] train: loss: 0.2209293
[Epoch 139] ogbg-molsider: 0.585710 val loss: 0.692152
[Epoch 139] ogbg-molsider: 0.577964 test loss: 0.681086
[Epoch 140; Iter    27/   27] train: loss: 0.2374765
[Epoch 140] ogbg-molsider: 0.582884 val loss: 0.690112
[Epoch 140] ogbg-molsider: 0.571394 test loss: 0.672059
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 133] ogbg-molsider: 0.570666 test loss: 0.673486
[Epoch 134; Iter     4/   32] train: loss: 0.2749400
[Epoch 134] ogbg-molsider: 0.595391 val loss: 0.616887
[Epoch 134] ogbg-molsider: 0.568231 test loss: 0.659227
[Epoch 135; Iter     2/   32] train: loss: 0.2957709
[Epoch 135; Iter    32/   32] train: loss: 0.2791282
[Epoch 135] ogbg-molsider: 0.593509 val loss: 0.615757
[Epoch 135] ogbg-molsider: 0.569399 test loss: 0.670951
[Epoch 136; Iter    30/   32] train: loss: 0.2944026
[Epoch 136] ogbg-molsider: 0.594377 val loss: 0.615612
[Epoch 136] ogbg-molsider: 0.577635 test loss: 0.639011
[Epoch 137; Iter    28/   32] train: loss: 0.3049953
[Epoch 137] ogbg-molsider: 0.589513 val loss: 0.615077
[Epoch 137] ogbg-molsider: 0.574575 test loss: 0.653702
[Epoch 138; Iter    26/   32] train: loss: 0.3165018
[Epoch 138] ogbg-molsider: 0.593637 val loss: 0.624254
[Epoch 138] ogbg-molsider: 0.571959 test loss: 0.683357
[Epoch 139; Iter    24/   32] train: loss: 0.2405758
[Epoch 139] ogbg-molsider: 0.597291 val loss: 0.625510
[Epoch 139] ogbg-molsider: 0.573389 test loss: 0.667388
[Epoch 140; Iter    22/   32] train: loss: 0.2933053
[Epoch 140] ogbg-molsider: 0.596492 val loss: 0.627282
[Epoch 140] ogbg-molsider: 0.578682 test loss: 0.662306
[Epoch 141; Iter    20/   32] train: loss: 0.2472723
[Epoch 141] ogbg-molsider: 0.602017 val loss: 0.611960
[Epoch 141] ogbg-molsider: 0.583919 test loss: 0.643757
[Epoch 142; Iter    18/   32] train: loss: 0.3032282
[Epoch 142] ogbg-molsider: 0.591356 val loss: 0.626333
[Epoch 142] ogbg-molsider: 0.572205 test loss: 0.666493
Early stopping criterion based on -ogbg-molsider- that should be max reached after 142 epochs. Best model checkpoint was in epoch 82.
Statistics on  val_best_checkpoint
mean_pred: 0.5107107758522034
std_pred: 2.1905388832092285
mean_targets: 0.6057459115982056
std_targets: 0.48873215913772583
prcauc: 0.6733414371653469
rocauc: 0.6138983490797131
ogbg-molsider: 0.6138983490797131
OGBNanLabelBCEWithLogitsLoss: 0.5097590897764478
Statistics on  test
mean_pred: 0.6386568546295166
std_pred: 2.229034662246704
mean_targets: 0.5677863955497742
std_targets: 0.49542638659477234
prcauc: 0.606367866667725
rocauc: 0.5558655345989542
ogbg-molsider: 0.5558655345989542
OGBNanLabelBCEWithLogitsLoss: 0.5740209988185337
Statistics on  train
mean_pred: 0.29456567764282227
std_pred: 2.2664239406585693
mean_targets: 0.5593408942222595
std_targets: 0.49647536873817444
prcauc: 0.7712494580157598
rocauc: 0.8273242093250173
ogbg-molsider: 0.8273242093250173
OGBNanLabelBCEWithLogitsLoss: 0.39586258586496115
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.624292 val loss: 0.607446
[Epoch 128] ogbg-molsider: 0.596443 test loss: 0.691543
[Epoch 129; Iter    12/   36] train: loss: 0.2811753
[Epoch 129] ogbg-molsider: 0.628173 val loss: 0.598757
[Epoch 129] ogbg-molsider: 0.604605 test loss: 0.664883
[Epoch 130; Iter     6/   36] train: loss: 0.2757368
[Epoch 130; Iter    36/   36] train: loss: 0.2618162
[Epoch 130] ogbg-molsider: 0.623014 val loss: 0.604046
[Epoch 130] ogbg-molsider: 0.604918 test loss: 0.666395
[Epoch 131; Iter    30/   36] train: loss: 0.3022570
[Epoch 131] ogbg-molsider: 0.627715 val loss: 0.597351
[Epoch 131] ogbg-molsider: 0.606908 test loss: 0.666269
[Epoch 132; Iter    24/   36] train: loss: 0.2628186
[Epoch 132] ogbg-molsider: 0.629530 val loss: 0.603678
[Epoch 132] ogbg-molsider: 0.606097 test loss: 0.674533
[Epoch 133; Iter    18/   36] train: loss: 0.2680746
[Epoch 133] ogbg-molsider: 0.622792 val loss: 0.605536
[Epoch 133] ogbg-molsider: 0.602810 test loss: 0.675931
[Epoch 134; Iter    12/   36] train: loss: 0.2701451
[Epoch 134] ogbg-molsider: 0.628065 val loss: 0.599039
[Epoch 134] ogbg-molsider: 0.614010 test loss: 0.656529
[Epoch 135; Iter     6/   36] train: loss: 0.2748336
[Epoch 135; Iter    36/   36] train: loss: 0.3559205
[Epoch 135] ogbg-molsider: 0.635076 val loss: 0.604513
[Epoch 135] ogbg-molsider: 0.614524 test loss: 0.692853
[Epoch 136; Iter    30/   36] train: loss: 0.2564547
[Epoch 136] ogbg-molsider: 0.628084 val loss: 0.619670
[Epoch 136] ogbg-molsider: 0.617436 test loss: 0.681198
[Epoch 137; Iter    24/   36] train: loss: 0.2566170
[Epoch 137] ogbg-molsider: 0.618636 val loss: 0.639500
[Epoch 137] ogbg-molsider: 0.611188 test loss: 0.698481
[Epoch 138; Iter    18/   36] train: loss: 0.3214477
[Epoch 138] ogbg-molsider: 0.625173 val loss: 0.614788
[Epoch 138] ogbg-molsider: 0.606747 test loss: 0.687901
[Epoch 139; Iter    12/   36] train: loss: 0.2350202
[Epoch 139] ogbg-molsider: 0.627783 val loss: 0.607239
[Epoch 139] ogbg-molsider: 0.607188 test loss: 0.692876
[Epoch 140; Iter     6/   36] train: loss: 0.2581821
[Epoch 140; Iter    36/   36] train: loss: 0.2552208
[Epoch 140] ogbg-molsider: 0.624871 val loss: 0.629748
[Epoch 140] ogbg-molsider: 0.612449 test loss: 0.718441
[Epoch 141; Iter    30/   36] train: loss: 0.2731266
[Epoch 141] ogbg-molsider: 0.619624 val loss: 0.618615
[Epoch 141] ogbg-molsider: 0.612139 test loss: 0.690971
[Epoch 142; Iter    24/   36] train: loss: 0.2631077
[Epoch 142] ogbg-molsider: 0.627045 val loss: 0.623549
[Epoch 142] ogbg-molsider: 0.619488 test loss: 0.685566
[Epoch 143; Iter    18/   36] train: loss: 0.2806585
[Epoch 143] ogbg-molsider: 0.615349 val loss: 0.628547
[Epoch 143] ogbg-molsider: 0.616163 test loss: 0.685244
[Epoch 144; Iter    12/   36] train: loss: 0.2327329
[Epoch 144] ogbg-molsider: 0.625523 val loss: 0.623132
[Epoch 144] ogbg-molsider: 0.616645 test loss: 0.679782
[Epoch 145; Iter     6/   36] train: loss: 0.2811841
[Epoch 145; Iter    36/   36] train: loss: 0.2782956
[Epoch 145] ogbg-molsider: 0.630052 val loss: 0.617571
[Epoch 145] ogbg-molsider: 0.612205 test loss: 0.684199
[Epoch 146; Iter    30/   36] train: loss: 0.2595369
[Epoch 146] ogbg-molsider: 0.624296 val loss: 0.628472
[Epoch 146] ogbg-molsider: 0.612428 test loss: 0.694850
[Epoch 147; Iter    24/   36] train: loss: 0.2623318
[Epoch 147] ogbg-molsider: 0.620003 val loss: 0.624012
[Epoch 147] ogbg-molsider: 0.611677 test loss: 0.689686
Early stopping criterion based on -ogbg-molsider- that should be max reached after 147 epochs. Best model checkpoint was in epoch 87.
Statistics on  val_best_checkpoint
mean_pred: 0.6894398331642151
std_pred: 2.6583240032196045
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6801489955395226
rocauc: 0.6539313957800325
ogbg-molsider: 0.6539313957800325
OGBNanLabelBCEWithLogitsLoss: 0.49634101390838625
Statistics on  test
mean_pred: 0.7169032096862793
std_pred: 2.70559024810791
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6224850919612822
rocauc: 0.5918028027588232
ogbg-molsider: 0.5918028027588232
OGBNanLabelBCEWithLogitsLoss: 0.569152569770813
Statistics on  train
mean_pred: 0.36802899837493896
std_pred: 2.6585214138031006
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8083499856042679
rocauc: 0.8546582053537347
ogbg-molsider: 0.8546582053537347
OGBNanLabelBCEWithLogitsLoss: 0.36758090886804795
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 141] ogbg-molsider: 0.592584 val loss: 0.646629
[Epoch 141] ogbg-molsider: 0.592748 test loss: 0.630944
[Epoch 142; Iter     3/   27] train: loss: 0.2776970
[Epoch 142] ogbg-molsider: 0.581884 val loss: 0.677772
[Epoch 142] ogbg-molsider: 0.580348 test loss: 0.700159
[Epoch 143; Iter     6/   27] train: loss: 0.2835937
[Epoch 143] ogbg-molsider: 0.595418 val loss: 0.655340
[Epoch 143] ogbg-molsider: 0.598823 test loss: 0.634298
[Epoch 144; Iter     9/   27] train: loss: 0.2882160
[Epoch 144] ogbg-molsider: 0.587021 val loss: 0.657966
[Epoch 144] ogbg-molsider: 0.590672 test loss: 0.654063
[Epoch 145; Iter    12/   27] train: loss: 0.2377998
[Epoch 145] ogbg-molsider: 0.595730 val loss: 0.658168
[Epoch 145] ogbg-molsider: 0.594281 test loss: 0.634621
[Epoch 146; Iter    15/   27] train: loss: 0.2464482
[Epoch 146] ogbg-molsider: 0.593983 val loss: 0.654608
[Epoch 146] ogbg-molsider: 0.587060 test loss: 0.641456
[Epoch 147; Iter    18/   27] train: loss: 0.2992724
[Epoch 147] ogbg-molsider: 0.589025 val loss: 0.656528
[Epoch 147] ogbg-molsider: 0.594078 test loss: 0.630688
[Epoch 148; Iter    21/   27] train: loss: 0.2584514
[Epoch 148] ogbg-molsider: 0.594368 val loss: 0.666108
[Epoch 148] ogbg-molsider: 0.589630 test loss: 0.640670
[Epoch 149; Iter    24/   27] train: loss: 0.3419863
[Epoch 149] ogbg-molsider: 0.590047 val loss: 0.660109
[Epoch 149] ogbg-molsider: 0.585548 test loss: 0.649132
[Epoch 150; Iter    27/   27] train: loss: 0.2417281
[Epoch 150] ogbg-molsider: 0.592511 val loss: 0.663785
[Epoch 150] ogbg-molsider: 0.586867 test loss: 0.655130
[Epoch 151] ogbg-molsider: 0.586498 val loss: 0.667585
[Epoch 151] ogbg-molsider: 0.592787 test loss: 0.635878
[Epoch 152; Iter     3/   27] train: loss: 0.2740384
[Epoch 152] ogbg-molsider: 0.591354 val loss: 0.661389
[Epoch 152] ogbg-molsider: 0.591475 test loss: 0.642448
[Epoch 153; Iter     6/   27] train: loss: 0.2741874
[Epoch 153] ogbg-molsider: 0.594895 val loss: 0.668558
[Epoch 153] ogbg-molsider: 0.590227 test loss: 0.659492
[Epoch 154; Iter     9/   27] train: loss: 0.2691589
[Epoch 154] ogbg-molsider: 0.593428 val loss: 0.669901
[Epoch 154] ogbg-molsider: 0.587310 test loss: 0.656557
[Epoch 155; Iter    12/   27] train: loss: 0.3079124
[Epoch 155] ogbg-molsider: 0.590886 val loss: 0.670335
[Epoch 155] ogbg-molsider: 0.591513 test loss: 0.661133
[Epoch 156; Iter    15/   27] train: loss: 0.2579249
[Epoch 156] ogbg-molsider: 0.593196 val loss: 0.659338
[Epoch 156] ogbg-molsider: 0.590232 test loss: 0.659480
[Epoch 157; Iter    18/   27] train: loss: 0.2684880
[Epoch 157] ogbg-molsider: 0.591127 val loss: 0.679021
[Epoch 157] ogbg-molsider: 0.590211 test loss: 0.663502
[Epoch 158; Iter    21/   27] train: loss: 0.2576889
[Epoch 158] ogbg-molsider: 0.591995 val loss: 0.676988
[Epoch 158] ogbg-molsider: 0.593422 test loss: 0.652427
[Epoch 159; Iter    24/   27] train: loss: 0.2657702
[Epoch 159] ogbg-molsider: 0.587144 val loss: 0.680439
[Epoch 159] ogbg-molsider: 0.587253 test loss: 0.671080
Early stopping criterion based on -ogbg-molsider- that should be max reached after 159 epochs. Best model checkpoint was in epoch 99.
Statistics on  val_best_checkpoint
mean_pred: 0.6404701471328735
std_pred: 2.710592746734619
mean_targets: 0.5918128490447998
std_targets: 0.49153006076812744
prcauc: 0.6531877722247411
rocauc: 0.6065718607120117
ogbg-molsider: 0.6065718607120117
OGBNanLabelBCEWithLogitsLoss: 0.579765839709176
Statistics on  test
mean_pred: 0.896210789680481
std_pred: 2.7328286170959473
mean_targets: 0.5811965465545654
std_targets: 0.49339503049850464
prcauc: 0.6255749774803527
rocauc: 0.5956752088810341
ogbg-molsider: 0.5956752088810341
OGBNanLabelBCEWithLogitsLoss: 0.5829793479707506
Statistics on  train
mean_pred: 0.3879312574863434
std_pred: 2.6173388957977295
mean_targets: 0.5549498200416565
std_targets: 0.4969820976257324
prcauc: 0.8137413548829124
rocauc: 0.8700078612599285
ogbg-molsider: 0.8700078612599285
OGBNanLabelBCEWithLogitsLoss: 0.34964161228250573
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.646637 val loss: 0.586239
[Epoch 128] ogbg-molsider: 0.577493 test loss: 0.690966
[Epoch 129; Iter    12/   36] train: loss: 0.2733988
[Epoch 129] ogbg-molsider: 0.640861 val loss: 0.598644
[Epoch 129] ogbg-molsider: 0.585491 test loss: 0.688135
[Epoch 130; Iter     6/   36] train: loss: 0.2166212
[Epoch 130; Iter    36/   36] train: loss: 0.2232078
[Epoch 130] ogbg-molsider: 0.637556 val loss: 0.606627
[Epoch 130] ogbg-molsider: 0.584575 test loss: 0.697425
[Epoch 131; Iter    30/   36] train: loss: 0.2729703
[Epoch 131] ogbg-molsider: 0.631945 val loss: 0.604528
[Epoch 131] ogbg-molsider: 0.582652 test loss: 0.690440
[Epoch 132; Iter    24/   36] train: loss: 0.2207426
[Epoch 132] ogbg-molsider: 0.627528 val loss: 0.615160
[Epoch 132] ogbg-molsider: 0.582215 test loss: 0.705114
[Epoch 133; Iter    18/   36] train: loss: 0.1962525
[Epoch 133] ogbg-molsider: 0.624717 val loss: 0.640686
[Epoch 133] ogbg-molsider: 0.575188 test loss: 0.756558
[Epoch 134; Iter    12/   36] train: loss: 0.2142273
[Epoch 134] ogbg-molsider: 0.646349 val loss: 0.607580
[Epoch 134] ogbg-molsider: 0.591474 test loss: 0.693897
[Epoch 135; Iter     6/   36] train: loss: 0.2458757
[Epoch 135; Iter    36/   36] train: loss: 0.2856344
[Epoch 135] ogbg-molsider: 0.639450 val loss: 0.606328
[Epoch 135] ogbg-molsider: 0.586820 test loss: 0.703932
[Epoch 136; Iter    30/   36] train: loss: 0.2054633
[Epoch 136] ogbg-molsider: 0.636378 val loss: 0.619582
[Epoch 136] ogbg-molsider: 0.584978 test loss: 0.716227
[Epoch 137; Iter    24/   36] train: loss: 0.2430008
[Epoch 137] ogbg-molsider: 0.636504 val loss: 0.621645
[Epoch 137] ogbg-molsider: 0.580965 test loss: 0.723021
[Epoch 138; Iter    18/   36] train: loss: 0.2196980
[Epoch 138] ogbg-molsider: 0.643971 val loss: 0.614912
[Epoch 138] ogbg-molsider: 0.585909 test loss: 0.710852
[Epoch 139; Iter    12/   36] train: loss: 0.2137108
[Epoch 139] ogbg-molsider: 0.640202 val loss: 0.621114
[Epoch 139] ogbg-molsider: 0.582420 test loss: 0.712697
[Epoch 140; Iter     6/   36] train: loss: 0.2209346
[Epoch 140; Iter    36/   36] train: loss: 0.2350484
[Epoch 140] ogbg-molsider: 0.642223 val loss: 0.620039
[Epoch 140] ogbg-molsider: 0.590602 test loss: 0.711155
[Epoch 141; Iter    30/   36] train: loss: 0.2298514
[Epoch 141] ogbg-molsider: 0.637819 val loss: 0.624405
[Epoch 141] ogbg-molsider: 0.580432 test loss: 0.720130
[Epoch 142; Iter    24/   36] train: loss: 0.2511122
[Epoch 142] ogbg-molsider: 0.633856 val loss: 0.633905
[Epoch 142] ogbg-molsider: 0.590500 test loss: 0.720512
[Epoch 143; Iter    18/   36] train: loss: 0.2196987
[Epoch 143] ogbg-molsider: 0.638445 val loss: 0.628277
[Epoch 143] ogbg-molsider: 0.583395 test loss: 0.719882
[Epoch 144; Iter    12/   36] train: loss: 0.2025890
[Epoch 144] ogbg-molsider: 0.635908 val loss: 0.641364
[Epoch 144] ogbg-molsider: 0.581671 test loss: 0.733132
[Epoch 145; Iter     6/   36] train: loss: 0.2431207
[Epoch 145; Iter    36/   36] train: loss: 0.2534490
[Epoch 145] ogbg-molsider: 0.635961 val loss: 0.639410
[Epoch 145] ogbg-molsider: 0.593379 test loss: 0.726663
[Epoch 146; Iter    30/   36] train: loss: 0.1964670
[Epoch 146] ogbg-molsider: 0.640222 val loss: 0.629418
[Epoch 146] ogbg-molsider: 0.592205 test loss: 0.709077
[Epoch 147; Iter    24/   36] train: loss: 0.2265364
[Epoch 147] ogbg-molsider: 0.635419 val loss: 0.632036
[Epoch 147] ogbg-molsider: 0.582978 test loss: 0.736242
[Epoch 148; Iter    18/   36] train: loss: 0.2046892
[Epoch 148] ogbg-molsider: 0.633890 val loss: 0.643289
[Epoch 148] ogbg-molsider: 0.584614 test loss: 0.741409
[Epoch 149; Iter    12/   36] train: loss: 0.2097597
[Epoch 149] ogbg-molsider: 0.645180 val loss: 0.632125
[Epoch 149] ogbg-molsider: 0.589485 test loss: 0.727509
[Epoch 150; Iter     6/   36] train: loss: 0.2559799
[Epoch 150; Iter    36/   36] train: loss: 0.2578498
[Epoch 150] ogbg-molsider: 0.638616 val loss: 0.637656
[Epoch 150] ogbg-molsider: 0.590671 test loss: 0.728032
[Epoch 151; Iter    30/   36] train: loss: 0.2088067
[Epoch 151] ogbg-molsider: 0.640823 val loss: 0.643365
[Epoch 151] ogbg-molsider: 0.594009 test loss: 0.737172
[Epoch 152; Iter    24/   36] train: loss: 0.2057638
[Epoch 152] ogbg-molsider: 0.637172 val loss: 0.645452
[Epoch 152] ogbg-molsider: 0.588135 test loss: 0.735685
[Epoch 153; Iter    18/   36] train: loss: 0.2056821
[Epoch 153] ogbg-molsider: 0.639907 val loss: 0.647206
[Epoch 153] ogbg-molsider: 0.588146 test loss: 0.735880
[Epoch 154; Iter    12/   36] train: loss: 0.1976015
[Epoch 154] ogbg-molsider: 0.647770 val loss: 0.641921
[Epoch 154] ogbg-molsider: 0.591781 test loss: 0.747291
[Epoch 155; Iter     6/   36] train: loss: 0.1843359
[Epoch 155; Iter    36/   36] train: loss: 0.2325758
[Epoch 155] ogbg-molsider: 0.638936 val loss: 0.638290
[Epoch 155] ogbg-molsider: 0.589781 test loss: 0.725487
[Epoch 156; Iter    30/   36] train: loss: 0.2420352
[Epoch 156] ogbg-molsider: 0.633372 val loss: 0.651083
[Epoch 156] ogbg-molsider: 0.592692 test loss: 0.736602
[Epoch 157; Iter    24/   36] train: loss: 0.2195570
[Epoch 157] ogbg-molsider: 0.635413 val loss: 0.643486
[Epoch 157] ogbg-molsider: 0.593998 test loss: 0.718902
[Epoch 158; Iter    18/   36] train: loss: 0.1972111
[Epoch 158] ogbg-molsider: 0.639033 val loss: 0.650037
[Epoch 158] ogbg-molsider: 0.595371 test loss: 0.727215
[Epoch 159; Iter    12/   36] train: loss: 0.1928777
[Epoch 159] ogbg-molsider: 0.642323 val loss: 0.646277
[Epoch 159] ogbg-molsider: 0.592972 test loss: 0.738059
[Epoch 160; Iter     6/   36] train: loss: 0.2412863
[Epoch 160; Iter    36/   36] train: loss: 0.2463937
[Epoch 160] ogbg-molsider: 0.644904 val loss: 0.642453
[Epoch 160] ogbg-molsider: 0.592813 test loss: 0.739383
Early stopping criterion based on -ogbg-molsider- that should be max reached after 160 epochs. Best model checkpoint was in epoch 100.
Statistics on  val_best_checkpoint
mean_pred: 0.9017665982246399
std_pred: 3.310075521469116
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6901372445952977
rocauc: 0.6518240744819352
ogbg-molsider: 0.6518240744819352
OGBNanLabelBCEWithLogitsLoss: 0.5436629951000214
Statistics on  test
mean_pred: 0.9692298769950867
std_pred: 3.3097774982452393
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6294389759588591
rocauc: 0.5903858846293931
ogbg-molsider: 0.5903858846293931
OGBNanLabelBCEWithLogitsLoss: 0.6492912173271179
Statistics on  train
mean_pred: 0.6947835683822632
std_pred: 3.305356025695801
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8698570454316195
rocauc: 0.9129198293055387
ogbg-molsider: 0.9129198293055387
OGBNanLabelBCEWithLogitsLoss: 0.28919317821661633
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 141] ogbg-molsider: 0.580245 val loss: 0.699228
[Epoch 141] ogbg-molsider: 0.573292 test loss: 0.685640
[Epoch 142; Iter     3/   27] train: loss: 0.2314794
[Epoch 142] ogbg-molsider: 0.582134 val loss: 0.684460
[Epoch 142] ogbg-molsider: 0.579303 test loss: 0.669253
[Epoch 143; Iter     6/   27] train: loss: 0.2256119
[Epoch 143] ogbg-molsider: 0.581896 val loss: 0.691822
[Epoch 143] ogbg-molsider: 0.573568 test loss: 0.681346
[Epoch 144; Iter     9/   27] train: loss: 0.2317187
[Epoch 144] ogbg-molsider: 0.579614 val loss: 0.694321
[Epoch 144] ogbg-molsider: 0.574173 test loss: 0.681429
[Epoch 145; Iter    12/   27] train: loss: 0.2348672
[Epoch 145] ogbg-molsider: 0.582699 val loss: 0.704206
[Epoch 145] ogbg-molsider: 0.583842 test loss: 0.681098
[Epoch 146; Iter    15/   27] train: loss: 0.2178000
[Epoch 146] ogbg-molsider: 0.577907 val loss: 0.696316
[Epoch 146] ogbg-molsider: 0.566582 test loss: 0.694616
[Epoch 147; Iter    18/   27] train: loss: 0.2021106
[Epoch 147] ogbg-molsider: 0.584183 val loss: 0.688460
[Epoch 147] ogbg-molsider: 0.582962 test loss: 0.666110
[Epoch 148; Iter    21/   27] train: loss: 0.2340235
[Epoch 148] ogbg-molsider: 0.580008 val loss: 0.702262
[Epoch 148] ogbg-molsider: 0.575859 test loss: 0.687671
[Epoch 149; Iter    24/   27] train: loss: 0.2063638
[Epoch 149] ogbg-molsider: 0.579643 val loss: 0.724097
[Epoch 149] ogbg-molsider: 0.575680 test loss: 0.702182
[Epoch 150; Iter    27/   27] train: loss: 0.2788347
[Epoch 150] ogbg-molsider: 0.580033 val loss: 0.701236
[Epoch 150] ogbg-molsider: 0.570268 test loss: 0.697334
[Epoch 151] ogbg-molsider: 0.580434 val loss: 0.707212
[Epoch 151] ogbg-molsider: 0.580178 test loss: 0.678634
[Epoch 152; Iter     3/   27] train: loss: 0.2145152
[Epoch 152] ogbg-molsider: 0.579809 val loss: 0.703800
[Epoch 152] ogbg-molsider: 0.578065 test loss: 0.686765
[Epoch 153; Iter     6/   27] train: loss: 0.2207521
[Epoch 153] ogbg-molsider: 0.573990 val loss: 0.714311
[Epoch 153] ogbg-molsider: 0.574139 test loss: 0.705330
[Epoch 154; Iter     9/   27] train: loss: 0.2676652
[Epoch 154] ogbg-molsider: 0.577845 val loss: 0.708513
[Epoch 154] ogbg-molsider: 0.575532 test loss: 0.700170
[Epoch 155; Iter    12/   27] train: loss: 0.1848812
[Epoch 155] ogbg-molsider: 0.580864 val loss: 0.710667
[Epoch 155] ogbg-molsider: 0.574000 test loss: 0.692268
[Epoch 156; Iter    15/   27] train: loss: 0.2379261
[Epoch 156] ogbg-molsider: 0.577035 val loss: 0.715650
[Epoch 156] ogbg-molsider: 0.571901 test loss: 0.705890
[Epoch 157; Iter    18/   27] train: loss: 0.2077623
[Epoch 157] ogbg-molsider: 0.582594 val loss: 0.723531
[Epoch 157] ogbg-molsider: 0.573836 test loss: 0.713338
[Epoch 158; Iter    21/   27] train: loss: 0.2211767
[Epoch 158] ogbg-molsider: 0.583632 val loss: 0.710453
[Epoch 158] ogbg-molsider: 0.574004 test loss: 0.709714
[Epoch 159; Iter    24/   27] train: loss: 0.2138152
[Epoch 159] ogbg-molsider: 0.580574 val loss: 0.716480
[Epoch 159] ogbg-molsider: 0.571179 test loss: 0.709741
[Epoch 160; Iter    27/   27] train: loss: 0.2209605
[Epoch 160] ogbg-molsider: 0.579543 val loss: 0.702004
[Epoch 160] ogbg-molsider: 0.572887 test loss: 0.697689
[Epoch 161] ogbg-molsider: 0.577827 val loss: 0.708872
[Epoch 161] ogbg-molsider: 0.565844 test loss: 0.717362
[Epoch 162; Iter     3/   27] train: loss: 0.2106002
[Epoch 162] ogbg-molsider: 0.578870 val loss: 0.732646
[Epoch 162] ogbg-molsider: 0.568983 test loss: 0.738358
[Epoch 163; Iter     6/   27] train: loss: 0.2114300
[Epoch 163] ogbg-molsider: 0.580103 val loss: 0.717025
[Epoch 163] ogbg-molsider: 0.574773 test loss: 0.703596
[Epoch 164; Iter     9/   27] train: loss: 0.2119097
[Epoch 164] ogbg-molsider: 0.579901 val loss: 0.735039
[Epoch 164] ogbg-molsider: 0.570777 test loss: 0.733093
[Epoch 165; Iter    12/   27] train: loss: 0.2105817
[Epoch 165] ogbg-molsider: 0.577948 val loss: 0.722746
[Epoch 165] ogbg-molsider: 0.568534 test loss: 0.737752
[Epoch 166; Iter    15/   27] train: loss: 0.2445296
[Epoch 166] ogbg-molsider: 0.579757 val loss: 0.717694
[Epoch 166] ogbg-molsider: 0.575304 test loss: 0.703289
[Epoch 167; Iter    18/   27] train: loss: 0.1777897
[Epoch 167] ogbg-molsider: 0.575381 val loss: 0.735043
[Epoch 167] ogbg-molsider: 0.568168 test loss: 0.740457
Early stopping criterion based on -ogbg-molsider- that should be max reached after 167 epochs. Best model checkpoint was in epoch 107.
Statistics on  val_best_checkpoint
mean_pred: 0.35280656814575195
std_pred: 2.912505865097046
mean_targets: 0.5918128490447998
std_targets: 0.49153006076812744
prcauc: 0.6461627611779391
rocauc: 0.5993910126984026
ogbg-molsider: 0.5993910126984026
OGBNanLabelBCEWithLogitsLoss: 0.6172703835699294
Statistics on  test
mean_pred: 0.28659188747406006
std_pred: 2.9080657958984375
mean_targets: 0.5811965465545654
std_targets: 0.49339503049850464
prcauc: 0.6249291787788744
rocauc: 0.5754050806508983
ogbg-molsider: 0.5754050806508983
OGBNanLabelBCEWithLogitsLoss: 0.6090840829743279
Statistics on  train
mean_pred: 0.14714200794696808
std_pred: 3.0412774085998535
mean_targets: 0.5549498200416565
std_targets: 0.4969820976257324
prcauc: 0.8687018323444261
rocauc: 0.9184400416287486
ogbg-molsider: 0.9184400416287486
OGBNanLabelBCEWithLogitsLoss: 0.2960984590980742
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 141] ogbg-molsider: 0.596892 val loss: 0.654937
[Epoch 141] ogbg-molsider: 0.600100 test loss: 0.620879
[Epoch 142; Iter     3/   27] train: loss: 0.3067676
[Epoch 142] ogbg-molsider: 0.592153 val loss: 0.671001
[Epoch 142] ogbg-molsider: 0.594690 test loss: 0.655132
[Epoch 143; Iter     6/   27] train: loss: 0.2713791
[Epoch 143] ogbg-molsider: 0.593227 val loss: 0.674026
[Epoch 143] ogbg-molsider: 0.596575 test loss: 0.656829
[Epoch 144; Iter     9/   27] train: loss: 0.3084269
[Epoch 144] ogbg-molsider: 0.591406 val loss: 0.669204
[Epoch 144] ogbg-molsider: 0.592345 test loss: 0.658826
[Epoch 145; Iter    12/   27] train: loss: 0.2404093
[Epoch 145] ogbg-molsider: 0.594609 val loss: 0.673076
[Epoch 145] ogbg-molsider: 0.601211 test loss: 0.647624
[Epoch 146; Iter    15/   27] train: loss: 0.2429625
[Epoch 146] ogbg-molsider: 0.594761 val loss: 0.675934
[Epoch 146] ogbg-molsider: 0.589280 test loss: 0.681667
[Epoch 147; Iter    18/   27] train: loss: 0.2518070
[Epoch 147] ogbg-molsider: 0.593756 val loss: 0.673508
[Epoch 147] ogbg-molsider: 0.601090 test loss: 0.645374
[Epoch 148; Iter    21/   27] train: loss: 0.2595806
[Epoch 148] ogbg-molsider: 0.591229 val loss: 0.682710
[Epoch 148] ogbg-molsider: 0.591217 test loss: 0.666349
[Epoch 149; Iter    24/   27] train: loss: 0.2636406
[Epoch 149] ogbg-molsider: 0.594995 val loss: 0.675391
[Epoch 149] ogbg-molsider: 0.596202 test loss: 0.663082
[Epoch 150; Iter    27/   27] train: loss: 0.2572412
[Epoch 150] ogbg-molsider: 0.587011 val loss: 0.686935
[Epoch 150] ogbg-molsider: 0.590515 test loss: 0.677398
[Epoch 151] ogbg-molsider: 0.591593 val loss: 0.693433
[Epoch 151] ogbg-molsider: 0.595900 test loss: 0.675789
[Epoch 152; Iter     3/   27] train: loss: 0.2430145
[Epoch 152] ogbg-molsider: 0.590024 val loss: 0.684187
[Epoch 152] ogbg-molsider: 0.592853 test loss: 0.666931
[Epoch 153; Iter     6/   27] train: loss: 0.2617906
[Epoch 153] ogbg-molsider: 0.589357 val loss: 0.696792
[Epoch 153] ogbg-molsider: 0.596038 test loss: 0.669342
[Epoch 154; Iter     9/   27] train: loss: 0.2362533
[Epoch 154] ogbg-molsider: 0.593116 val loss: 0.700051
[Epoch 154] ogbg-molsider: 0.607166 test loss: 0.675339
[Epoch 155; Iter    12/   27] train: loss: 0.1922627
[Epoch 155] ogbg-molsider: 0.586868 val loss: 0.708248
[Epoch 155] ogbg-molsider: 0.582133 test loss: 0.693446
[Epoch 156; Iter    15/   27] train: loss: 0.2813323
[Epoch 156] ogbg-molsider: 0.592759 val loss: 0.694060
[Epoch 156] ogbg-molsider: 0.593303 test loss: 0.672411
[Epoch 157; Iter    18/   27] train: loss: 0.2071741
[Epoch 157] ogbg-molsider: 0.590049 val loss: 0.702072
[Epoch 157] ogbg-molsider: 0.590363 test loss: 0.693395
[Epoch 158; Iter    21/   27] train: loss: 0.2051575
[Epoch 158] ogbg-molsider: 0.590627 val loss: 0.712631
[Epoch 158] ogbg-molsider: 0.600031 test loss: 0.685017
[Epoch 159; Iter    24/   27] train: loss: 0.2422156
[Epoch 159] ogbg-molsider: 0.590880 val loss: 0.720533
[Epoch 159] ogbg-molsider: 0.603437 test loss: 0.689824
[Epoch 160; Iter    27/   27] train: loss: 0.2325597
[Epoch 160] ogbg-molsider: 0.588327 val loss: 0.719903
[Epoch 160] ogbg-molsider: 0.591283 test loss: 0.695938
[Epoch 161] ogbg-molsider: 0.588367 val loss: 0.710416
[Epoch 161] ogbg-molsider: 0.583235 test loss: 0.714808
[Epoch 162; Iter     3/   27] train: loss: 0.2143817
[Epoch 162] ogbg-molsider: 0.589100 val loss: 0.727135
[Epoch 162] ogbg-molsider: 0.599441 test loss: 0.707906
[Epoch 163; Iter     6/   27] train: loss: 0.2561022
[Epoch 163] ogbg-molsider: 0.594041 val loss: 0.723696
[Epoch 163] ogbg-molsider: 0.603470 test loss: 0.705907
[Epoch 164; Iter     9/   27] train: loss: 0.2236533
[Epoch 164] ogbg-molsider: 0.589167 val loss: 0.723602
[Epoch 164] ogbg-molsider: 0.593336 test loss: 0.704192
[Epoch 165; Iter    12/   27] train: loss: 0.1917216
[Epoch 165] ogbg-molsider: 0.583294 val loss: 0.717533
[Epoch 165] ogbg-molsider: 0.593711 test loss: 0.689691
[Epoch 166; Iter    15/   27] train: loss: 0.1938143
[Epoch 166] ogbg-molsider: 0.584700 val loss: 0.728259
[Epoch 166] ogbg-molsider: 0.594193 test loss: 0.705632
[Epoch 167; Iter    18/   27] train: loss: 0.2070590
[Epoch 167] ogbg-molsider: 0.588977 val loss: 0.735288
[Epoch 167] ogbg-molsider: 0.596358 test loss: 0.707953
[Epoch 168; Iter    21/   27] train: loss: 0.2369044
[Epoch 168] ogbg-molsider: 0.589799 val loss: 0.720061
[Epoch 168] ogbg-molsider: 0.592811 test loss: 0.697604
[Epoch 169; Iter    24/   27] train: loss: 0.1910920
[Epoch 169] ogbg-molsider: 0.589215 val loss: 0.730116
[Epoch 169] ogbg-molsider: 0.605179 test loss: 0.693658
[Epoch 170; Iter    27/   27] train: loss: 0.2502249
[Epoch 170] ogbg-molsider: 0.585900 val loss: 0.731663
[Epoch 170] ogbg-molsider: 0.601477 test loss: 0.701634
[Epoch 171] ogbg-molsider: 0.583614 val loss: 0.726140
[Epoch 171] ogbg-molsider: 0.596803 test loss: 0.695968
[Epoch 172; Iter     3/   27] train: loss: 0.2191551
[Epoch 172] ogbg-molsider: 0.585419 val loss: 0.733038
[Epoch 172] ogbg-molsider: 0.601629 test loss: 0.701275
Early stopping criterion based on -ogbg-molsider- that should be max reached after 172 epochs. Best model checkpoint was in epoch 112.
Statistics on  val_best_checkpoint
mean_pred: 0.9889129400253296
std_pred: 2.961372137069702
mean_targets: 0.5918128490447998
std_targets: 0.49153006076812744
prcauc: 0.6499560651808627
rocauc: 0.6077594623831197
ogbg-molsider: 0.6077594623831197
OGBNanLabelBCEWithLogitsLoss: 0.5892733037471771
Statistics on  test
mean_pred: 0.9192480444908142
std_pred: 2.8782222270965576
mean_targets: 0.5811965465545654
std_targets: 0.49339503049850464
prcauc: 0.6387705932183833
rocauc: 0.5943683793472159
ogbg-molsider: 0.5943683793472159
OGBNanLabelBCEWithLogitsLoss: 0.5752759244706895
Statistics on  train
mean_pred: 0.641148030757904
std_pred: 3.3498494625091553
mean_targets: 0.5549498200416565
std_targets: 0.49698206782341003
prcauc: 0.8517797809348037
rocauc: 0.9019333194961219
ogbg-molsider: 0.9019333194961219
OGBNanLabelBCEWithLogitsLoss: 0.31821146497020014
All runs completed.
