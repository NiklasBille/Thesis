>>> Starting run for dataset: bbbp
Running RANDOM configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml --seed 6 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml --seed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.8/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.8_5_26-05_09-38-00
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.8
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6916990
[Epoch 1] ogbg-molbbbp: 0.553457 val loss: 0.692290
[Epoch 1] ogbg-molbbbp: 0.675779 test loss: 0.691761
[Epoch 2; Iter     5/   55] train: loss: 0.6900873
[Epoch 2; Iter    35/   55] train: loss: 0.6946584
[Epoch 2] ogbg-molbbbp: 0.518765 val loss: 0.692015
[Epoch 2] ogbg-molbbbp: 0.616402 test loss: 0.691231
[Epoch 3; Iter    10/   55] train: loss: 0.6908945
[Epoch 3; Iter    40/   55] train: loss: 0.6928069
[Epoch 3] ogbg-molbbbp: 0.524321 val loss: 0.691829
[Epoch 3] ogbg-molbbbp: 0.618607 test loss: 0.691097
[Epoch 4; Iter    15/   55] train: loss: 0.6932579
[Epoch 4; Iter    45/   55] train: loss: 0.6942009
[Epoch 4] ogbg-molbbbp: 0.531235 val loss: 0.690674
[Epoch 4] ogbg-molbbbp: 0.611405 test loss: 0.689670
[Epoch 5; Iter    20/   55] train: loss: 0.6866137
[Epoch 5; Iter    50/   55] train: loss: 0.6854991
[Epoch 5] ogbg-molbbbp: 0.532099 val loss: 0.690341
[Epoch 5] ogbg-molbbbp: 0.616843 test loss: 0.689232
[Epoch 6; Iter    25/   55] train: loss: 0.6871074
[Epoch 6; Iter    55/   55] train: loss: 0.6901207
[Epoch 6] ogbg-molbbbp: 0.560617 val loss: 0.689195
[Epoch 6] ogbg-molbbbp: 0.640212 test loss: 0.688029
[Epoch 7; Iter    30/   55] train: loss: 0.6904894
[Epoch 7] ogbg-molbbbp: 0.555926 val loss: 0.687426
[Epoch 7] ogbg-molbbbp: 0.648589 test loss: 0.685708
[Epoch 8; Iter     5/   55] train: loss: 0.6858962
[Epoch 8; Iter    35/   55] train: loss: 0.6855270
[Epoch 8] ogbg-molbbbp: 0.565679 val loss: 0.686414
[Epoch 8] ogbg-molbbbp: 0.650647 test loss: 0.684432
[Epoch 9; Iter    10/   55] train: loss: 0.6865864
[Epoch 9; Iter    40/   55] train: loss: 0.6843485
[Epoch 9] ogbg-molbbbp: 0.571358 val loss: 0.684574
[Epoch 9] ogbg-molbbbp: 0.647266 test loss: 0.682349
[Epoch 10; Iter    15/   55] train: loss: 0.6805279
[Epoch 10; Iter    45/   55] train: loss: 0.6829213
[Epoch 10] ogbg-molbbbp: 0.585679 val loss: 0.682882
[Epoch 10] ogbg-molbbbp: 0.657995 test loss: 0.680368
[Epoch 11; Iter    20/   55] train: loss: 0.6796302
[Epoch 11; Iter    50/   55] train: loss: 0.6815577
[Epoch 11] ogbg-molbbbp: 0.599136 val loss: 0.680814
[Epoch 11] ogbg-molbbbp: 0.664756 test loss: 0.677740
[Epoch 12; Iter    25/   55] train: loss: 0.6841726
[Epoch 12; Iter    55/   55] train: loss: 0.6736198
[Epoch 12] ogbg-molbbbp: 0.611975 val loss: 0.678676
[Epoch 12] ogbg-molbbbp: 0.682834 test loss: 0.675055
[Epoch 13; Iter    30/   55] train: loss: 0.6819870
[Epoch 13] ogbg-molbbbp: 0.766420 val loss: 0.652063
[Epoch 13] ogbg-molbbbp: 0.786155 test loss: 0.652824
[Epoch 14; Iter     5/   55] train: loss: 0.6533610
[Epoch 14; Iter    35/   55] train: loss: 0.6174830
[Epoch 14] ogbg-molbbbp: 0.893086 val loss: 0.519303
[Epoch 14] ogbg-molbbbp: 0.826132 test loss: 0.504585
[Epoch 15; Iter    10/   55] train: loss: 0.5834871
[Epoch 15; Iter    40/   55] train: loss: 0.4932642
[Epoch 15] ogbg-molbbbp: 0.871111 val loss: 0.481188
[Epoch 15] ogbg-molbbbp: 0.839506 test loss: 0.509324
[Epoch 16; Iter    15/   55] train: loss: 0.3993945
[Epoch 16; Iter    45/   55] train: loss: 0.4855758
[Epoch 16] ogbg-molbbbp: 0.841235 val loss: 0.415922
[Epoch 16] ogbg-molbbbp: 0.836567 test loss: 0.398508
[Epoch 17; Iter    20/   55] train: loss: 0.3868801
[Epoch 17; Iter    50/   55] train: loss: 0.3534638
[Epoch 17] ogbg-molbbbp: 0.896049 val loss: 0.366570
[Epoch 17] ogbg-molbbbp: 0.860964 test loss: 0.366653
[Epoch 18; Iter    25/   55] train: loss: 0.4582616
[Epoch 18; Iter    55/   55] train: loss: 0.3510426
[Epoch 18] ogbg-molbbbp: 0.895802 val loss: 0.404428
[Epoch 18] ogbg-molbbbp: 0.855820 test loss: 0.423860
[Epoch 19; Iter    30/   55] train: loss: 0.3210910
[Epoch 19] ogbg-molbbbp: 0.890123 val loss: 0.385382
[Epoch 19] ogbg-molbbbp: 0.875073 test loss: 0.346849
[Epoch 20; Iter     5/   55] train: loss: 0.3126695
[Epoch 20; Iter    35/   55] train: loss: 0.2693413
[Epoch 20] ogbg-molbbbp: 0.903210 val loss: 0.365531
[Epoch 20] ogbg-molbbbp: 0.871546 test loss: 0.370274
[Epoch 21; Iter    10/   55] train: loss: 0.3008029
[Epoch 21; Iter    40/   55] train: loss: 0.3794992
[Epoch 21] ogbg-molbbbp: 0.900617 val loss: 0.398896
[Epoch 21] ogbg-molbbbp: 0.869195 test loss: 0.364533
[Epoch 22; Iter    15/   55] train: loss: 0.2666473
[Epoch 22; Iter    45/   55] train: loss: 0.2416884
[Epoch 22] ogbg-molbbbp: 0.905185 val loss: 0.359297
[Epoch 22] ogbg-molbbbp: 0.880658 test loss: 0.335101
[Epoch 23; Iter    20/   55] train: loss: 0.3352884
[Epoch 23; Iter    50/   55] train: loss: 0.3705479
[Epoch 23] ogbg-molbbbp: 0.888765 val loss: 0.362403
[Epoch 23] ogbg-molbbbp: 0.881099 test loss: 0.332221
[Epoch 24; Iter    25/   55] train: loss: 0.3496716
[Epoch 24; Iter    55/   55] train: loss: 0.4647979
[Epoch 24] ogbg-molbbbp: 0.887531 val loss: 0.374006
[Epoch 24] ogbg-molbbbp: 0.865667 test loss: 0.368059
[Epoch 25; Iter    30/   55] train: loss: 0.2751752
[Epoch 25] ogbg-molbbbp: 0.853580 val loss: 0.404534
[Epoch 25] ogbg-molbbbp: 0.849059 test loss: 0.387519
[Epoch 26; Iter     5/   55] train: loss: 0.2940714
[Epoch 26; Iter    35/   55] train: loss: 0.4570614
[Epoch 26] ogbg-molbbbp: 0.926173 val loss: 0.321934
[Epoch 26] ogbg-molbbbp: 0.897560 test loss: 0.310777
[Epoch 27; Iter    10/   55] train: loss: 0.1661711
[Epoch 27; Iter    40/   55] train: loss: 0.3027312
[Epoch 27] ogbg-molbbbp: 0.909383 val loss: 0.356126
[Epoch 27] ogbg-molbbbp: 0.864932 test loss: 0.430948
[Epoch 28; Iter    15/   55] train: loss: 0.1817390
[Epoch 28; Iter    45/   55] train: loss: 0.3684288
[Epoch 28] ogbg-molbbbp: 0.928889 val loss: 0.308875
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.8/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.8_6_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.8
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6886628
[Epoch 1] ogbg-molbbbp: 0.656543 val loss: 0.692355
[Epoch 1] ogbg-molbbbp: 0.654027 test loss: 0.692132
[Epoch 2; Iter     5/   55] train: loss: 0.6917055
[Epoch 2; Iter    35/   55] train: loss: 0.6853133
[Epoch 2] ogbg-molbbbp: 0.658519 val loss: 0.690490
[Epoch 2] ogbg-molbbbp: 0.684744 test loss: 0.689315
[Epoch 3; Iter    10/   55] train: loss: 0.6885360
[Epoch 3; Iter    40/   55] train: loss: 0.6901648
[Epoch 3] ogbg-molbbbp: 0.640617 val loss: 0.690487
[Epoch 3] ogbg-molbbbp: 0.682687 test loss: 0.688943
[Epoch 4; Iter    15/   55] train: loss: 0.6886898
[Epoch 4; Iter    45/   55] train: loss: 0.6884135
[Epoch 4] ogbg-molbbbp: 0.649383 val loss: 0.689818
[Epoch 4] ogbg-molbbbp: 0.683128 test loss: 0.688284
[Epoch 5; Iter    20/   55] train: loss: 0.6887393
[Epoch 5; Iter    50/   55] train: loss: 0.6891672
[Epoch 5] ogbg-molbbbp: 0.649877 val loss: 0.689456
[Epoch 5] ogbg-molbbbp: 0.689741 test loss: 0.687685
[Epoch 6; Iter    25/   55] train: loss: 0.6891689
[Epoch 6; Iter    55/   55] train: loss: 0.6896947
[Epoch 6] ogbg-molbbbp: 0.656914 val loss: 0.688156
[Epoch 6] ogbg-molbbbp: 0.701352 test loss: 0.686132
[Epoch 7; Iter    30/   55] train: loss: 0.6837938
[Epoch 7] ogbg-molbbbp: 0.670370 val loss: 0.686638
[Epoch 7] ogbg-molbbbp: 0.702528 test loss: 0.684405
[Epoch 8; Iter     5/   55] train: loss: 0.6844631
[Epoch 8; Iter    35/   55] train: loss: 0.6800221
[Epoch 8] ogbg-molbbbp: 0.673457 val loss: 0.685413
[Epoch 8] ogbg-molbbbp: 0.704145 test loss: 0.683011
[Epoch 9; Iter    10/   55] train: loss: 0.6822714
[Epoch 9; Iter    40/   55] train: loss: 0.6862676
[Epoch 9] ogbg-molbbbp: 0.683457 val loss: 0.683780
[Epoch 9] ogbg-molbbbp: 0.715608 test loss: 0.680971
[Epoch 10; Iter    15/   55] train: loss: 0.6762046
[Epoch 10; Iter    45/   55] train: loss: 0.6818067
[Epoch 10] ogbg-molbbbp: 0.690494 val loss: 0.681596
[Epoch 10] ogbg-molbbbp: 0.720459 test loss: 0.678374
[Epoch 11; Iter    20/   55] train: loss: 0.6767352
[Epoch 11; Iter    50/   55] train: loss: 0.6797113
[Epoch 11] ogbg-molbbbp: 0.697284 val loss: 0.680073
[Epoch 11] ogbg-molbbbp: 0.725603 test loss: 0.676514
[Epoch 12; Iter    25/   55] train: loss: 0.6834102
[Epoch 12; Iter    55/   55] train: loss: 0.6843387
[Epoch 12] ogbg-molbbbp: 0.704691 val loss: 0.677361
[Epoch 12] ogbg-molbbbp: 0.727807 test loss: 0.673349
[Epoch 13; Iter    30/   55] train: loss: 0.6721516
[Epoch 13] ogbg-molbbbp: 0.792346 val loss: 0.649302
[Epoch 13] ogbg-molbbbp: 0.800265 test loss: 0.654014
[Epoch 14; Iter     5/   55] train: loss: 0.6394932
[Epoch 14; Iter    35/   55] train: loss: 0.5969236
[Epoch 14] ogbg-molbbbp: 0.854074 val loss: 0.499650
[Epoch 14] ogbg-molbbbp: 0.844062 test loss: 0.492299
[Epoch 15; Iter    10/   55] train: loss: 0.5510944
[Epoch 15; Iter    40/   55] train: loss: 0.5250598
[Epoch 15] ogbg-molbbbp: 0.889877 val loss: 0.478569
[Epoch 15] ogbg-molbbbp: 0.843327 test loss: 0.503057
[Epoch 16; Iter    15/   55] train: loss: 0.5442205
[Epoch 16; Iter    45/   55] train: loss: 0.3610451
[Epoch 16] ogbg-molbbbp: 0.854074 val loss: 0.443925
[Epoch 16] ogbg-molbbbp: 0.819371 test loss: 0.457971
[Epoch 17; Iter    20/   55] train: loss: 0.3530187
[Epoch 17; Iter    50/   55] train: loss: 0.2929026
[Epoch 17] ogbg-molbbbp: 0.885062 val loss: 0.411869
[Epoch 17] ogbg-molbbbp: 0.865667 test loss: 0.435552
[Epoch 18; Iter    25/   55] train: loss: 0.3306658
[Epoch 18; Iter    55/   55] train: loss: 0.7285417
[Epoch 18] ogbg-molbbbp: 0.849753 val loss: 0.400096
[Epoch 18] ogbg-molbbbp: 0.842740 test loss: 0.366346
[Epoch 19; Iter    30/   55] train: loss: 0.3789850
[Epoch 19] ogbg-molbbbp: 0.890864 val loss: 0.387788
[Epoch 19] ogbg-molbbbp: 0.856996 test loss: 0.369682
[Epoch 20; Iter     5/   55] train: loss: 0.3415465
[Epoch 20; Iter    35/   55] train: loss: 0.1496103
[Epoch 20] ogbg-molbbbp: 0.897160 val loss: 0.365670
[Epoch 20] ogbg-molbbbp: 0.865226 test loss: 0.364644
[Epoch 21; Iter    10/   55] train: loss: 0.3637038
[Epoch 21; Iter    40/   55] train: loss: 0.3891322
[Epoch 21] ogbg-molbbbp: 0.899630 val loss: 0.369884
[Epoch 21] ogbg-molbbbp: 0.885068 test loss: 0.332961
[Epoch 22; Iter    15/   55] train: loss: 0.3120121
[Epoch 22; Iter    45/   55] train: loss: 0.3844902
[Epoch 22] ogbg-molbbbp: 0.879383 val loss: 0.390289
[Epoch 22] ogbg-molbbbp: 0.858466 test loss: 0.346507
[Epoch 23; Iter    20/   55] train: loss: 0.3154209
[Epoch 23; Iter    50/   55] train: loss: 0.3011504
[Epoch 23] ogbg-molbbbp: 0.899877 val loss: 0.357102
[Epoch 23] ogbg-molbbbp: 0.875514 test loss: 0.343003
[Epoch 24; Iter    25/   55] train: loss: 0.3793954
[Epoch 24; Iter    55/   55] train: loss: 0.3658566
[Epoch 24] ogbg-molbbbp: 0.888025 val loss: 0.374173
[Epoch 24] ogbg-molbbbp: 0.866108 test loss: 0.351447
[Epoch 25; Iter    30/   55] train: loss: 0.3592105
[Epoch 25] ogbg-molbbbp: 0.887778 val loss: 0.401640
[Epoch 25] ogbg-molbbbp: 0.871693 test loss: 0.346588
[Epoch 26; Iter     5/   55] train: loss: 0.3006397
[Epoch 26; Iter    35/   55] train: loss: 0.2750034
[Epoch 26] ogbg-molbbbp: 0.911235 val loss: 0.328887
[Epoch 26] ogbg-molbbbp: 0.912992 test loss: 0.299455
[Epoch 27; Iter    10/   55] train: loss: 0.2944063
[Epoch 27; Iter    40/   55] train: loss: 0.2212293
[Epoch 27] ogbg-molbbbp: 0.913827 val loss: 0.331469
[Epoch 27] ogbg-molbbbp: 0.889036 test loss: 0.329825
[Epoch 28; Iter    15/   55] train: loss: 0.2743265
[Epoch 28; Iter    45/   55] train: loss: 0.2607235
[Epoch 28] ogbg-molbbbp: 0.904938 val loss: 0.395363
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.7/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.7_6_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.7
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6885897
[Epoch 1] ogbg-molbbbp: 0.615097 val loss: 0.692478
[Epoch 1] ogbg-molbbbp: 0.626875 test loss: 0.692388
[Epoch 2; Iter    12/   48] train: loss: 0.6901497
[Epoch 2; Iter    42/   48] train: loss: 0.6898118
[Epoch 2] ogbg-molbbbp: 0.657460 val loss: 0.690348
[Epoch 2] ogbg-molbbbp: 0.671944 test loss: 0.690079
[Epoch 3; Iter    24/   48] train: loss: 0.6887926
[Epoch 3] ogbg-molbbbp: 0.650655 val loss: 0.689929
[Epoch 3] ogbg-molbbbp: 0.668944 test loss: 0.689447
[Epoch 4; Iter     6/   48] train: loss: 0.6894301
[Epoch 4; Iter    36/   48] train: loss: 0.6899012
[Epoch 4] ogbg-molbbbp: 0.662565 val loss: 0.689397
[Epoch 4] ogbg-molbbbp: 0.675646 test loss: 0.689045
[Epoch 5; Iter    18/   48] train: loss: 0.6891473
[Epoch 5; Iter    48/   48] train: loss: 0.6857073
[Epoch 5] ogbg-molbbbp: 0.660353 val loss: 0.688908
[Epoch 5] ogbg-molbbbp: 0.677561 test loss: 0.688304
[Epoch 6; Iter    30/   48] train: loss: 0.6885267
[Epoch 6] ogbg-molbbbp: 0.658198 val loss: 0.688365
[Epoch 6] ogbg-molbbbp: 0.679796 test loss: 0.687568
[Epoch 7; Iter    12/   48] train: loss: 0.6888291
[Epoch 7; Iter    42/   48] train: loss: 0.6884634
[Epoch 7] ogbg-molbbbp: 0.668576 val loss: 0.687293
[Epoch 7] ogbg-molbbbp: 0.692627 test loss: 0.686313
[Epoch 8; Iter    24/   48] train: loss: 0.6865366
[Epoch 8] ogbg-molbbbp: 0.672603 val loss: 0.685745
[Epoch 8] ogbg-molbbbp: 0.690393 test loss: 0.684576
[Epoch 9; Iter     6/   48] train: loss: 0.6841658
[Epoch 9; Iter    36/   48] train: loss: 0.6806499
[Epoch 9] ogbg-molbbbp: 0.680599 val loss: 0.684584
[Epoch 9] ogbg-molbbbp: 0.697223 test loss: 0.683315
[Epoch 10; Iter    18/   48] train: loss: 0.6820206
[Epoch 10; Iter    48/   48] train: loss: 0.6778703
[Epoch 10] ogbg-molbbbp: 0.692282 val loss: 0.682981
[Epoch 10] ogbg-molbbbp: 0.704245 test loss: 0.681554
[Epoch 11; Iter    30/   48] train: loss: 0.6821797
[Epoch 11] ogbg-molbbbp: 0.695344 val loss: 0.681259
[Epoch 11] ogbg-molbbbp: 0.704117 test loss: 0.679485
[Epoch 12; Iter    12/   48] train: loss: 0.6835823
[Epoch 12; Iter    42/   48] train: loss: 0.6804735
[Epoch 12] ogbg-molbbbp: 0.704078 val loss: 0.679254
[Epoch 12] ogbg-molbbbp: 0.710756 test loss: 0.677327
[Epoch 13; Iter    24/   48] train: loss: 0.6750973
[Epoch 13] ogbg-molbbbp: 0.705155 val loss: 0.677967
[Epoch 13] ogbg-molbbbp: 0.715608 test loss: 0.675542
[Epoch 14; Iter     6/   48] train: loss: 0.6738379
[Epoch 14; Iter    36/   48] train: loss: 0.6694978
[Epoch 14] ogbg-molbbbp: 0.720637 val loss: 0.675694
[Epoch 14] ogbg-molbbbp: 0.714906 test loss: 0.673392
[Epoch 15; Iter    18/   48] train: loss: 0.6705834
[Epoch 15; Iter    48/   48] train: loss: 0.6569091
[Epoch 15] ogbg-molbbbp: 0.849146 val loss: 0.639421
[Epoch 15] ogbg-molbbbp: 0.823237 test loss: 0.640323
[Epoch 16; Iter    30/   48] train: loss: 0.5524081
[Epoch 16] ogbg-molbbbp: 0.877616 val loss: 0.518560
[Epoch 16] ogbg-molbbbp: 0.864092 test loss: 0.518881
[Epoch 17; Iter    12/   48] train: loss: 0.4789137
[Epoch 17; Iter    42/   48] train: loss: 0.4662079
[Epoch 17] ogbg-molbbbp: 0.868599 val loss: 0.435981
[Epoch 17] ogbg-molbbbp: 0.873029 test loss: 0.420957
[Epoch 18; Iter    24/   48] train: loss: 0.3722765
[Epoch 18] ogbg-molbbbp: 0.881642 val loss: 0.381477
[Epoch 18] ogbg-molbbbp: 0.873157 test loss: 0.369427
[Epoch 19; Iter     6/   48] train: loss: 0.4087566
[Epoch 19; Iter    36/   48] train: loss: 0.3178584
[Epoch 19] ogbg-molbbbp: 0.874724 val loss: 0.467912
[Epoch 19] ogbg-molbbbp: 0.854133 test loss: 0.495410
[Epoch 20; Iter    18/   48] train: loss: 0.3596888
[Epoch 20; Iter    48/   48] train: loss: 0.4430553
[Epoch 20] ogbg-molbbbp: 0.873419 val loss: 0.370657
[Epoch 20] ogbg-molbbbp: 0.883690 test loss: 0.377608
[Epoch 21; Iter    30/   48] train: loss: 0.2709280
[Epoch 21] ogbg-molbbbp: 0.868315 val loss: 0.370435
[Epoch 21] ogbg-molbbbp: 0.869071 test loss: 0.362710
[Epoch 22; Iter    12/   48] train: loss: 0.2766297
[Epoch 22; Iter    42/   48] train: loss: 0.2988894
[Epoch 22] ogbg-molbbbp: 0.890660 val loss: 0.340477
[Epoch 22] ogbg-molbbbp: 0.872327 test loss: 0.343416
[Epoch 23; Iter    24/   48] train: loss: 0.3936423
[Epoch 23] ogbg-molbbbp: 0.881132 val loss: 0.346857
[Epoch 23] ogbg-molbbbp: 0.872263 test loss: 0.347681
[Epoch 24; Iter     6/   48] train: loss: 0.1896523
[Epoch 24; Iter    36/   48] train: loss: 0.2249061
[Epoch 24] ogbg-molbbbp: 0.876028 val loss: 0.374834
[Epoch 24] ogbg-molbbbp: 0.883562 test loss: 0.361707
[Epoch 25; Iter    18/   48] train: loss: 0.2647820
[Epoch 25; Iter    48/   48] train: loss: 0.1781282
[Epoch 25] ogbg-molbbbp: 0.868202 val loss: 0.348483
[Epoch 25] ogbg-molbbbp: 0.855091 test loss: 0.352711
[Epoch 26; Iter    30/   48] train: loss: 0.3642511
[Epoch 26] ogbg-molbbbp: 0.894856 val loss: 0.330816
[Epoch 26] ogbg-molbbbp: 0.873221 test loss: 0.333632
[Epoch 27; Iter    12/   48] train: loss: 0.2079293
[Epoch 27; Iter    42/   48] train: loss: 0.7572885
[Epoch 27] ogbg-molbbbp: 0.889979 val loss: 0.332770
[Epoch 27] ogbg-molbbbp: 0.889371 test loss: 0.317213
[Epoch 28; Iter    24/   48] train: loss: 0.1447984
[Epoch 28] ogbg-molbbbp: 0.888334 val loss: 0.359037
[Epoch 28] ogbg-molbbbp: 0.881200 test loss: 0.359065
[Epoch 29; Iter     6/   48] train: loss: 0.3892854
[Epoch 29; Iter    36/   48] train: loss: 0.2370767
[Epoch 29] ogbg-molbbbp: 0.893609 val loss: 0.326485
[Epoch 29] ogbg-molbbbp: 0.899904 test loss: 0.308182
[Epoch 30; Iter    18/   48] train: loss: 0.2327095
[Epoch 30; Iter    48/   48] train: loss: 0.1648209
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.7/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.7_4_26-05_09-37-59
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.7
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6913316
[Epoch 1] ogbg-molbbbp: 0.681676 val loss: 0.691930
[Epoch 1] ogbg-molbbbp: 0.627705 test loss: 0.692409
[Epoch 2; Iter    12/   48] train: loss: 0.6934618
[Epoch 2; Iter    42/   48] train: loss: 0.6912625
[Epoch 2] ogbg-molbbbp: 0.717178 val loss: 0.689410
[Epoch 2] ogbg-molbbbp: 0.657836 test loss: 0.689803
[Epoch 3; Iter    24/   48] train: loss: 0.6924756
[Epoch 3] ogbg-molbbbp: 0.722225 val loss: 0.688728
[Epoch 3] ogbg-molbbbp: 0.663773 test loss: 0.688834
[Epoch 4; Iter     6/   48] train: loss: 0.6866258
[Epoch 4; Iter    36/   48] train: loss: 0.6875677
[Epoch 4] ogbg-molbbbp: 0.727386 val loss: 0.688145
[Epoch 4] ogbg-molbbbp: 0.664092 test loss: 0.688144
[Epoch 5; Iter    18/   48] train: loss: 0.6909475
[Epoch 5; Iter    48/   48] train: loss: 0.6915748
[Epoch 5] ogbg-molbbbp: 0.733624 val loss: 0.687897
[Epoch 5] ogbg-molbbbp: 0.671369 test loss: 0.688087
[Epoch 6; Iter    30/   48] train: loss: 0.6872995
[Epoch 6] ogbg-molbbbp: 0.730505 val loss: 0.686610
[Epoch 6] ogbg-molbbbp: 0.672455 test loss: 0.686511
[Epoch 7; Iter    12/   48] train: loss: 0.6875161
[Epoch 7; Iter    42/   48] train: loss: 0.6864387
[Epoch 7] ogbg-molbbbp: 0.734986 val loss: 0.685827
[Epoch 7] ogbg-molbbbp: 0.677370 test loss: 0.685587
[Epoch 8; Iter    24/   48] train: loss: 0.6854346
[Epoch 8] ogbg-molbbbp: 0.740316 val loss: 0.684966
[Epoch 8] ogbg-molbbbp: 0.679477 test loss: 0.684802
[Epoch 9; Iter     6/   48] train: loss: 0.6892810
[Epoch 9; Iter    36/   48] train: loss: 0.6857265
[Epoch 9] ogbg-molbbbp: 0.739069 val loss: 0.683030
[Epoch 9] ogbg-molbbbp: 0.680434 test loss: 0.682599
[Epoch 10; Iter    18/   48] train: loss: 0.6845385
[Epoch 10; Iter    48/   48] train: loss: 0.6744167
[Epoch 10] ogbg-molbbbp: 0.743549 val loss: 0.681514
[Epoch 10] ogbg-molbbbp: 0.683115 test loss: 0.680695
[Epoch 11; Iter    30/   48] train: loss: 0.6827313
[Epoch 11] ogbg-molbbbp: 0.751886 val loss: 0.679927
[Epoch 11] ogbg-molbbbp: 0.689435 test loss: 0.679033
[Epoch 12; Iter    12/   48] train: loss: 0.6795262
[Epoch 12; Iter    42/   48] train: loss: 0.6753844
[Epoch 12] ogbg-molbbbp: 0.758861 val loss: 0.678262
[Epoch 12] ogbg-molbbbp: 0.694478 test loss: 0.677098
[Epoch 13; Iter    24/   48] train: loss: 0.6800258
[Epoch 13] ogbg-molbbbp: 0.762774 val loss: 0.675940
[Epoch 13] ogbg-molbbbp: 0.695180 test loss: 0.674423
[Epoch 14; Iter     6/   48] train: loss: 0.6705138
[Epoch 14; Iter    36/   48] train: loss: 0.6749016
[Epoch 14] ogbg-molbbbp: 0.761924 val loss: 0.673817
[Epoch 14] ogbg-molbbbp: 0.697606 test loss: 0.671924
[Epoch 15; Iter    18/   48] train: loss: 0.6790898
[Epoch 15; Iter    48/   48] train: loss: 0.6507779
[Epoch 15] ogbg-molbbbp: 0.854137 val loss: 0.635817
[Epoch 15] ogbg-molbbbp: 0.827067 test loss: 0.638247
[Epoch 16; Iter    30/   48] train: loss: 0.5840600
[Epoch 16] ogbg-molbbbp: 0.854421 val loss: 0.519965
[Epoch 16] ogbg-molbbbp: 0.832301 test loss: 0.519827
[Epoch 17; Iter    12/   48] train: loss: 0.5375181
[Epoch 17; Iter    42/   48] train: loss: 0.4126740
[Epoch 17] ogbg-molbbbp: 0.870130 val loss: 0.505732
[Epoch 17] ogbg-molbbbp: 0.861666 test loss: 0.520169
[Epoch 18; Iter    24/   48] train: loss: 0.3998764
[Epoch 18] ogbg-molbbbp: 0.851188 val loss: 0.392180
[Epoch 18] ogbg-molbbbp: 0.877689 test loss: 0.348572
[Epoch 19; Iter     6/   48] train: loss: 0.4577509
[Epoch 19; Iter    36/   48] train: loss: 0.4536941
[Epoch 19] ogbg-molbbbp: 0.816197 val loss: 0.512193
[Epoch 19] ogbg-molbbbp: 0.836770 test loss: 0.480835
[Epoch 20; Iter    18/   48] train: loss: 0.4483452
[Epoch 20; Iter    48/   48] train: loss: 0.2049061
[Epoch 20] ogbg-molbbbp: 0.871888 val loss: 0.383979
[Epoch 20] ogbg-molbbbp: 0.851580 test loss: 0.403762
[Epoch 21; Iter    30/   48] train: loss: 0.3291725
[Epoch 21] ogbg-molbbbp: 0.876141 val loss: 0.364824
[Epoch 21] ogbg-molbbbp: 0.872838 test loss: 0.360505
[Epoch 22; Iter    12/   48] train: loss: 0.4677159
[Epoch 22; Iter    42/   48] train: loss: 0.3115645
[Epoch 22] ogbg-molbbbp: 0.885612 val loss: 0.340831
[Epoch 22] ogbg-molbbbp: 0.864220 test loss: 0.343061
[Epoch 23; Iter    24/   48] train: loss: 0.2953876
[Epoch 23] ogbg-molbbbp: 0.861396 val loss: 0.376278
[Epoch 23] ogbg-molbbbp: 0.880562 test loss: 0.335078
[Epoch 24; Iter     6/   48] train: loss: 0.3195154
[Epoch 24; Iter    36/   48] train: loss: 0.4326900
[Epoch 24] ogbg-molbbbp: 0.862247 val loss: 0.361984
[Epoch 24] ogbg-molbbbp: 0.868688 test loss: 0.352685
[Epoch 25; Iter    18/   48] train: loss: 0.3727570
[Epoch 25; Iter    48/   48] train: loss: 0.2764456
[Epoch 25] ogbg-molbbbp: 0.887200 val loss: 0.339212
[Epoch 25] ogbg-molbbbp: 0.866709 test loss: 0.343672
[Epoch 26; Iter    30/   48] train: loss: 0.2030330
[Epoch 26] ogbg-molbbbp: 0.855385 val loss: 0.401708
[Epoch 26] ogbg-molbbbp: 0.858857 test loss: 0.405642
[Epoch 27; Iter    12/   48] train: loss: 0.1949980
[Epoch 27; Iter    42/   48] train: loss: 0.5764491
[Epoch 27] ogbg-molbbbp: 0.896614 val loss: 0.321217
[Epoch 27] ogbg-molbbbp: 0.868050 test loss: 0.340869
[Epoch 28; Iter    24/   48] train: loss: 0.2084142
[Epoch 28] ogbg-molbbbp: 0.905972 val loss: 0.324164
[Epoch 28] ogbg-molbbbp: 0.875519 test loss: 0.341891
[Epoch 29; Iter     6/   48] train: loss: 0.1758234
[Epoch 29; Iter    36/   48] train: loss: 0.2879363
[Epoch 29] ogbg-molbbbp: 0.889072 val loss: 0.334577
[Epoch 29] ogbg-molbbbp: 0.903351 test loss: 0.315599
[Epoch 30; Iter    18/   48] train: loss: 0.3988942
[Epoch 30; Iter    48/   48] train: loss: 0.2156877
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.8/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.8_4_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.8
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6893182
[Epoch 1] ogbg-molbbbp: 0.608025 val loss: 0.692324
[Epoch 1] ogbg-molbbbp: 0.661670 test loss: 0.692137
[Epoch 2; Iter     5/   55] train: loss: 0.6920860
[Epoch 2; Iter    35/   55] train: loss: 0.6882710
[Epoch 2] ogbg-molbbbp: 0.660494 val loss: 0.689392
[Epoch 2] ogbg-molbbbp: 0.681658 test loss: 0.689439
[Epoch 3; Iter    10/   55] train: loss: 0.6922479
[Epoch 3; Iter    40/   55] train: loss: 0.6869908
[Epoch 3] ogbg-molbbbp: 0.667531 val loss: 0.689200
[Epoch 3] ogbg-molbbbp: 0.689594 test loss: 0.689266
[Epoch 4; Iter    15/   55] train: loss: 0.6904705
[Epoch 4; Iter    45/   55] train: loss: 0.6882935
[Epoch 4] ogbg-molbbbp: 0.672469 val loss: 0.688292
[Epoch 4] ogbg-molbbbp: 0.694444 test loss: 0.688289
[Epoch 5; Iter    20/   55] train: loss: 0.6914874
[Epoch 5; Iter    50/   55] train: loss: 0.6852200
[Epoch 5] ogbg-molbbbp: 0.670370 val loss: 0.687667
[Epoch 5] ogbg-molbbbp: 0.697678 test loss: 0.687411
[Epoch 6; Iter    25/   55] train: loss: 0.6812761
[Epoch 6; Iter    55/   55] train: loss: 0.6836445
[Epoch 6] ogbg-molbbbp: 0.674568 val loss: 0.686585
[Epoch 6] ogbg-molbbbp: 0.703557 test loss: 0.686322
[Epoch 7; Iter    30/   55] train: loss: 0.6896330
[Epoch 7] ogbg-molbbbp: 0.669630 val loss: 0.685416
[Epoch 7] ogbg-molbbbp: 0.704879 test loss: 0.684824
[Epoch 8; Iter     5/   55] train: loss: 0.6833681
[Epoch 8; Iter    35/   55] train: loss: 0.6827293
[Epoch 8] ogbg-molbbbp: 0.678148 val loss: 0.683642
[Epoch 8] ogbg-molbbbp: 0.711640 test loss: 0.682671
[Epoch 9; Iter    10/   55] train: loss: 0.6821458
[Epoch 9; Iter    40/   55] train: loss: 0.6848018
[Epoch 9] ogbg-molbbbp: 0.679506 val loss: 0.682107
[Epoch 9] ogbg-molbbbp: 0.713404 test loss: 0.680819
[Epoch 10; Iter    15/   55] train: loss: 0.6790220
[Epoch 10; Iter    45/   55] train: loss: 0.6854853
[Epoch 10] ogbg-molbbbp: 0.688148 val loss: 0.679537
[Epoch 10] ogbg-molbbbp: 0.721634 test loss: 0.677962
[Epoch 11; Iter    20/   55] train: loss: 0.6769285
[Epoch 11; Iter    50/   55] train: loss: 0.6824517
[Epoch 11] ogbg-molbbbp: 0.697407 val loss: 0.678349
[Epoch 11] ogbg-molbbbp: 0.726044 test loss: 0.676903
[Epoch 12; Iter    25/   55] train: loss: 0.6751983
[Epoch 12; Iter    55/   55] train: loss: 0.6734158
[Epoch 12] ogbg-molbbbp: 0.695432 val loss: 0.675756
[Epoch 12] ogbg-molbbbp: 0.722222 test loss: 0.673511
[Epoch 13; Iter    30/   55] train: loss: 0.6680245
[Epoch 13] ogbg-molbbbp: 0.784568 val loss: 0.654477
[Epoch 13] ogbg-molbbbp: 0.796149 test loss: 0.658337
[Epoch 14; Iter     5/   55] train: loss: 0.6424254
[Epoch 14; Iter    35/   55] train: loss: 0.6031487
[Epoch 14] ogbg-molbbbp: 0.894568 val loss: 0.539471
[Epoch 14] ogbg-molbbbp: 0.855967 test loss: 0.546711
[Epoch 15; Iter    10/   55] train: loss: 0.5518717
[Epoch 15; Iter    40/   55] train: loss: 0.4907789
[Epoch 15] ogbg-molbbbp: 0.866914 val loss: 0.464719
[Epoch 15] ogbg-molbbbp: 0.827307 test loss: 0.487359
[Epoch 16; Iter    15/   55] train: loss: 0.4641536
[Epoch 16; Iter    45/   55] train: loss: 0.3679442
[Epoch 16] ogbg-molbbbp: 0.883951 val loss: 0.440530
[Epoch 16] ogbg-molbbbp: 0.849059 test loss: 0.457236
[Epoch 17; Iter    20/   55] train: loss: 0.5010154
[Epoch 17; Iter    50/   55] train: loss: 0.3759379
[Epoch 17] ogbg-molbbbp: 0.881975 val loss: 0.399035
[Epoch 17] ogbg-molbbbp: 0.843474 test loss: 0.391125
[Epoch 18; Iter    25/   55] train: loss: 0.4726645
[Epoch 18; Iter    55/   55] train: loss: 0.4461956
[Epoch 18] ogbg-molbbbp: 0.882840 val loss: 0.379961
[Epoch 18] ogbg-molbbbp: 0.855232 test loss: 0.369391
[Epoch 19; Iter    30/   55] train: loss: 0.3012185
[Epoch 19] ogbg-molbbbp: 0.905556 val loss: 0.363000
[Epoch 19] ogbg-molbbbp: 0.842593 test loss: 0.372018
[Epoch 20; Iter     5/   55] train: loss: 0.3463696
[Epoch 20; Iter    35/   55] train: loss: 0.4672121
[Epoch 20] ogbg-molbbbp: 0.913827 val loss: 0.347606
[Epoch 20] ogbg-molbbbp: 0.877572 test loss: 0.342254
[Epoch 21; Iter    10/   55] train: loss: 0.3577563
[Epoch 21; Iter    40/   55] train: loss: 0.5802835
[Epoch 21] ogbg-molbbbp: 0.897778 val loss: 0.362163
[Epoch 21] ogbg-molbbbp: 0.869195 test loss: 0.340692
[Epoch 22; Iter    15/   55] train: loss: 0.4366646
[Epoch 22; Iter    45/   55] train: loss: 0.4329409
[Epoch 22] ogbg-molbbbp: 0.908642 val loss: 0.343372
[Epoch 22] ogbg-molbbbp: 0.885362 test loss: 0.333138
[Epoch 23; Iter    20/   55] train: loss: 0.4064896
[Epoch 23; Iter    50/   55] train: loss: 0.2287523
[Epoch 23] ogbg-molbbbp: 0.907284 val loss: 0.346904
[Epoch 23] ogbg-molbbbp: 0.878160 test loss: 0.337574
[Epoch 24; Iter    25/   55] train: loss: 0.3458656
[Epoch 24; Iter    55/   55] train: loss: 0.3264569
[Epoch 24] ogbg-molbbbp: 0.883457 val loss: 0.380041
[Epoch 24] ogbg-molbbbp: 0.860964 test loss: 0.353027
[Epoch 25; Iter    30/   55] train: loss: 0.4608667
[Epoch 25] ogbg-molbbbp: 0.911852 val loss: 0.374169
[Epoch 25] ogbg-molbbbp: 0.895650 test loss: 0.340223
[Epoch 26; Iter     5/   55] train: loss: 0.3079327
[Epoch 26; Iter    35/   55] train: loss: 0.4703318
[Epoch 26] ogbg-molbbbp: 0.931605 val loss: 0.325523
[Epoch 26] ogbg-molbbbp: 0.896384 test loss: 0.365846
[Epoch 27; Iter    10/   55] train: loss: 0.3493643
[Epoch 27; Iter    40/   55] train: loss: 0.3863191
[Epoch 27] ogbg-molbbbp: 0.935556 val loss: 0.289326
[Epoch 27] ogbg-molbbbp: 0.899765 test loss: 0.316797
[Epoch 28; Iter    15/   55] train: loss: 0.3441115
[Epoch 28; Iter    45/   55] train: loss: 0.2786247
[Epoch 28] ogbg-molbbbp: 0.913951 val loss: 0.352658
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.6/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.6_5_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.6
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6896363
[Epoch 1] ogbg-molbbbp: 0.601126 val loss: 0.692454
[Epoch 1] ogbg-molbbbp: 0.605202 test loss: 0.692400
[Epoch 2; Iter    19/   41] train: loss: 0.6954491
[Epoch 2] ogbg-molbbbp: 0.598293 val loss: 0.691691
[Epoch 2] ogbg-molbbbp: 0.592481 test loss: 0.691645
[Epoch 3; Iter     8/   41] train: loss: 0.6932165
[Epoch 3; Iter    38/   41] train: loss: 0.6915799
[Epoch 3] ogbg-molbbbp: 0.589418 val loss: 0.691777
[Epoch 3] ogbg-molbbbp: 0.560330 test loss: 0.692019
[Epoch 4; Iter    27/   41] train: loss: 0.6931078
[Epoch 4] ogbg-molbbbp: 0.590237 val loss: 0.690770
[Epoch 4] ogbg-molbbbp: 0.561432 test loss: 0.691034
[Epoch 5; Iter    16/   41] train: loss: 0.6917452
[Epoch 5] ogbg-molbbbp: 0.606554 val loss: 0.690192
[Epoch 5] ogbg-molbbbp: 0.573184 test loss: 0.690491
[Epoch 6; Iter     5/   41] train: loss: 0.6905116
[Epoch 6; Iter    35/   41] train: loss: 0.6884156
[Epoch 6] ogbg-molbbbp: 0.607066 val loss: 0.690063
[Epoch 6] ogbg-molbbbp: 0.582732 test loss: 0.690336
[Epoch 7; Iter    24/   41] train: loss: 0.6910410
[Epoch 7] ogbg-molbbbp: 0.611708 val loss: 0.689103
[Epoch 7] ogbg-molbbbp: 0.581397 test loss: 0.689442
[Epoch 8; Iter    13/   41] train: loss: 0.6867894
[Epoch 8] ogbg-molbbbp: 0.602082 val loss: 0.688182
[Epoch 8] ogbg-molbbbp: 0.582699 test loss: 0.688360
[Epoch 9; Iter     2/   41] train: loss: 0.6912749
[Epoch 9; Iter    32/   41] train: loss: 0.6856856
[Epoch 9] ogbg-molbbbp: 0.616795 val loss: 0.687400
[Epoch 9] ogbg-molbbbp: 0.597990 test loss: 0.687590
[Epoch 10; Iter    21/   41] train: loss: 0.6869539
[Epoch 10] ogbg-molbbbp: 0.624475 val loss: 0.685738
[Epoch 10] ogbg-molbbbp: 0.596488 test loss: 0.686007
[Epoch 11; Iter    10/   41] train: loss: 0.6899329
[Epoch 11; Iter    40/   41] train: loss: 0.6820689
[Epoch 11] ogbg-molbbbp: 0.630142 val loss: 0.684957
[Epoch 11] ogbg-molbbbp: 0.608240 test loss: 0.685210
[Epoch 12; Iter    29/   41] train: loss: 0.6825435
[Epoch 12] ogbg-molbbbp: 0.634409 val loss: 0.683652
[Epoch 12] ogbg-molbbbp: 0.615351 test loss: 0.683806
[Epoch 13; Iter    18/   41] train: loss: 0.6796154
[Epoch 13] ogbg-molbbbp: 0.649292 val loss: 0.682082
[Epoch 13] ogbg-molbbbp: 0.626202 test loss: 0.682350
[Epoch 14; Iter     7/   41] train: loss: 0.6828440
[Epoch 14; Iter    37/   41] train: loss: 0.6769066
[Epoch 14] ogbg-molbbbp: 0.654037 val loss: 0.680473
[Epoch 14] ogbg-molbbbp: 0.630308 test loss: 0.680757
[Epoch 15; Iter    26/   41] train: loss: 0.6781874
[Epoch 15] ogbg-molbbbp: 0.652466 val loss: 0.678769
[Epoch 15] ogbg-molbbbp: 0.624599 test loss: 0.679086
[Epoch 16; Iter    15/   41] train: loss: 0.6735964
[Epoch 16] ogbg-molbbbp: 0.665165 val loss: 0.676885
[Epoch 16] ogbg-molbbbp: 0.641760 test loss: 0.677167
[Epoch 17; Iter     4/   41] train: loss: 0.6844424
[Epoch 17; Iter    34/   41] train: loss: 0.6737261
[Epoch 17] ogbg-molbbbp: 0.778119 val loss: 0.667571
[Epoch 17] ogbg-molbbbp: 0.761752 test loss: 0.667628
[Epoch 18; Iter    23/   41] train: loss: 0.6439469
[Epoch 18] ogbg-molbbbp: 0.848199 val loss: 0.606386
[Epoch 18] ogbg-molbbbp: 0.847656 test loss: 0.606847
[Epoch 19; Iter    12/   41] train: loss: 0.6245908
[Epoch 19] ogbg-molbbbp: 0.859566 val loss: 0.551094
[Epoch 19] ogbg-molbbbp: 0.820680 test loss: 0.564471
[Epoch 20; Iter     1/   41] train: loss: 0.5643958
[Epoch 20; Iter    31/   41] train: loss: 0.5042047
[Epoch 20] ogbg-molbbbp: 0.868715 val loss: 0.463363
[Epoch 20] ogbg-molbbbp: 0.876603 test loss: 0.462676
[Epoch 21; Iter    20/   41] train: loss: 0.5012648
[Epoch 21] ogbg-molbbbp: 0.879331 val loss: 0.379626
[Epoch 21] ogbg-molbbbp: 0.873030 test loss: 0.395257
[Epoch 22; Iter     9/   41] train: loss: 0.3919985
[Epoch 22; Iter    39/   41] train: loss: 0.3914953
[Epoch 22] ogbg-molbbbp: 0.883291 val loss: 0.364776
[Epoch 22] ogbg-molbbbp: 0.876970 test loss: 0.386723
[Epoch 23; Iter    28/   41] train: loss: 0.3075214
[Epoch 23] ogbg-molbbbp: 0.890527 val loss: 0.361254
[Epoch 23] ogbg-molbbbp: 0.867755 test loss: 0.398726
[Epoch 24; Iter    17/   41] train: loss: 0.4767807
[Epoch 24] ogbg-molbbbp: 0.886056 val loss: 0.340254
[Epoch 24] ogbg-molbbbp: 0.889456 test loss: 0.358351
[Epoch 25; Iter     6/   41] train: loss: 0.2643588
[Epoch 25; Iter    36/   41] train: loss: 0.3230906
[Epoch 25] ogbg-molbbbp: 0.884383 val loss: 0.393365
[Epoch 25] ogbg-molbbbp: 0.868957 test loss: 0.414369
[Epoch 26; Iter    25/   41] train: loss: 0.3561723
[Epoch 26] ogbg-molbbbp: 0.880082 val loss: 0.342899
[Epoch 26] ogbg-molbbbp: 0.882312 test loss: 0.362443
[Epoch 27; Iter    14/   41] train: loss: 0.2701821
[Epoch 27] ogbg-molbbbp: 0.895784 val loss: 0.320579
[Epoch 27] ogbg-molbbbp: 0.889890 test loss: 0.358225
[Epoch 28; Iter     3/   41] train: loss: 0.3358271
[Epoch 28; Iter    33/   41] train: loss: 0.3841292
[Epoch 28] ogbg-molbbbp: 0.883359 val loss: 0.333969
[Epoch 28] ogbg-molbbbp: 0.889490 test loss: 0.365142
[Epoch 29; Iter    22/   41] train: loss: 0.3892709
[Epoch 29] ogbg-molbbbp: 0.881140 val loss: 0.352340
[Epoch 29] ogbg-molbbbp: 0.864683 test loss: 0.381030
[Epoch 30; Iter    11/   41] train: loss: 0.2336617
[Epoch 30; Iter    41/   41] train: loss: 0.2732765
[Epoch 30] ogbg-molbbbp: 0.899300 val loss: 0.325724
[Epoch 30] ogbg-molbbbp: 0.882846 test loss: 0.371072
[Epoch 31; Iter    30/   41] train: loss: 0.2431660
[Epoch 31] ogbg-molbbbp: 0.889913 val loss: 0.333800
[Epoch 31] ogbg-molbbbp: 0.874132 test loss: 0.366233
[Epoch 32; Iter    19/   41] train: loss: 0.4065241
[Epoch 32] ogbg-molbbbp: 0.895272 val loss: 0.318693
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.7/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.7_5_26-05_09-38-00
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.7
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6928528
[Epoch 1] ogbg-molbbbp: 0.652243 val loss: 0.691945
[Epoch 1] ogbg-molbbbp: 0.625215 test loss: 0.692242
[Epoch 2; Iter    12/   48] train: loss: 0.6946343
[Epoch 2; Iter    42/   48] train: loss: 0.6894509
[Epoch 2] ogbg-molbbbp: 0.607724 val loss: 0.691452
[Epoch 2] ogbg-molbbbp: 0.589914 test loss: 0.691682
[Epoch 3; Iter    24/   48] train: loss: 0.6902124
[Epoch 3] ogbg-molbbbp: 0.602450 val loss: 0.691033
[Epoch 3] ogbg-molbbbp: 0.580402 test loss: 0.691284
[Epoch 4; Iter     6/   48] train: loss: 0.6892571
[Epoch 4; Iter    36/   48] train: loss: 0.6919214
[Epoch 4] ogbg-molbbbp: 0.624624 val loss: 0.690350
[Epoch 4] ogbg-molbbbp: 0.591382 test loss: 0.690618
[Epoch 5; Iter    18/   48] train: loss: 0.6898866
[Epoch 5; Iter    48/   48] train: loss: 0.6903123
[Epoch 5] ogbg-molbbbp: 0.619293 val loss: 0.689524
[Epoch 5] ogbg-molbbbp: 0.588318 test loss: 0.689461
[Epoch 6; Iter    30/   48] train: loss: 0.6889793
[Epoch 6] ogbg-molbbbp: 0.618273 val loss: 0.689170
[Epoch 6] ogbg-molbbbp: 0.596297 test loss: 0.689166
[Epoch 7; Iter    12/   48] train: loss: 0.6874450
[Epoch 7; Iter    42/   48] train: loss: 0.6886820
[Epoch 7] ogbg-molbbbp: 0.623717 val loss: 0.688302
[Epoch 7] ogbg-molbbbp: 0.598340 test loss: 0.688102
[Epoch 8; Iter    24/   48] train: loss: 0.6888259
[Epoch 8] ogbg-molbbbp: 0.633585 val loss: 0.686617
[Epoch 8] ogbg-molbbbp: 0.597574 test loss: 0.686184
[Epoch 9; Iter     6/   48] train: loss: 0.6864225
[Epoch 9; Iter    36/   48] train: loss: 0.6853545
[Epoch 9] ogbg-molbbbp: 0.654795 val loss: 0.685295
[Epoch 9] ogbg-molbbbp: 0.610725 test loss: 0.684914
[Epoch 10; Iter    18/   48] train: loss: 0.6830246
[Epoch 10; Iter    48/   48] train: loss: 0.6861202
[Epoch 10] ogbg-molbbbp: 0.652697 val loss: 0.684260
[Epoch 10] ogbg-molbbbp: 0.613023 test loss: 0.683533
[Epoch 11; Iter    30/   48] train: loss: 0.6790416
[Epoch 11] ogbg-molbbbp: 0.668406 val loss: 0.682631
[Epoch 11] ogbg-molbbbp: 0.638366 test loss: 0.681559
[Epoch 12; Iter    12/   48] train: loss: 0.6803490
[Epoch 12; Iter    42/   48] train: loss: 0.6797449
[Epoch 12] ogbg-molbbbp: 0.681336 val loss: 0.681132
[Epoch 12] ogbg-molbbbp: 0.643600 test loss: 0.680129
[Epoch 13; Iter    24/   48] train: loss: 0.6756452
[Epoch 13] ogbg-molbbbp: 0.678160 val loss: 0.678873
[Epoch 13] ogbg-molbbbp: 0.636515 test loss: 0.677260
[Epoch 14; Iter     6/   48] train: loss: 0.6802146
[Epoch 14; Iter    36/   48] train: loss: 0.6815646
[Epoch 14] ogbg-molbbbp: 0.695231 val loss: 0.676900
[Epoch 14] ogbg-molbbbp: 0.658091 test loss: 0.675078
[Epoch 15; Iter    18/   48] train: loss: 0.6822492
[Epoch 15; Iter    48/   48] train: loss: 0.6632462
[Epoch 15] ogbg-molbbbp: 0.844326 val loss: 0.644097
[Epoch 15] ogbg-molbbbp: 0.824705 test loss: 0.640867
[Epoch 16; Iter    30/   48] train: loss: 0.6163697
[Epoch 16] ogbg-molbbbp: 0.838655 val loss: 0.541232
[Epoch 16] ogbg-molbbbp: 0.859496 test loss: 0.518034
[Epoch 17; Iter    12/   48] train: loss: 0.5127097
[Epoch 17; Iter    42/   48] train: loss: 0.5495897
[Epoch 17] ogbg-molbbbp: 0.877616 val loss: 0.502738
[Epoch 17] ogbg-molbbbp: 0.859113 test loss: 0.515387
[Epoch 18; Iter    24/   48] train: loss: 0.4746012
[Epoch 18] ogbg-molbbbp: 0.866444 val loss: 0.419238
[Epoch 18] ogbg-molbbbp: 0.880051 test loss: 0.403829
[Epoch 19; Iter     6/   48] train: loss: 0.3550069
[Epoch 19; Iter    36/   48] train: loss: 0.4022604
[Epoch 19] ogbg-molbbbp: 0.859638 val loss: 0.425160
[Epoch 19] ogbg-molbbbp: 0.863900 test loss: 0.424229
[Epoch 20; Iter    18/   48] train: loss: 0.4266789
[Epoch 20; Iter    48/   48] train: loss: 0.3002928
[Epoch 20] ogbg-molbbbp: 0.877049 val loss: 0.367450
[Epoch 20] ogbg-molbbbp: 0.876540 test loss: 0.357886
[Epoch 21; Iter    30/   48] train: loss: 0.4366129
[Epoch 21] ogbg-molbbbp: 0.867238 val loss: 0.360522
[Epoch 21] ogbg-molbbbp: 0.882349 test loss: 0.336495
[Epoch 22; Iter    12/   48] train: loss: 0.4496801
[Epoch 22; Iter    42/   48] train: loss: 0.3298737
[Epoch 22] ogbg-molbbbp: 0.886123 val loss: 0.347987
[Epoch 22] ogbg-molbbbp: 0.872646 test loss: 0.336156
[Epoch 23; Iter    24/   48] train: loss: 0.3422463
[Epoch 23] ogbg-molbbbp: 0.889695 val loss: 0.331024
[Epoch 23] ogbg-molbbbp: 0.867092 test loss: 0.333062
[Epoch 24; Iter     6/   48] train: loss: 0.2044107
[Epoch 24; Iter    36/   48] train: loss: 0.2301627
[Epoch 24] ogbg-molbbbp: 0.871321 val loss: 0.378101
[Epoch 24] ogbg-molbbbp: 0.869901 test loss: 0.380716
[Epoch 25; Iter    18/   48] train: loss: 0.2815763
[Epoch 25; Iter    48/   48] train: loss: 0.3358133
[Epoch 25] ogbg-molbbbp: 0.898202 val loss: 0.322619
[Epoch 25] ogbg-molbbbp: 0.878072 test loss: 0.328105
[Epoch 26; Iter    30/   48] train: loss: 0.1995558
[Epoch 26] ogbg-molbbbp: 0.866557 val loss: 0.352990
[Epoch 26] ogbg-molbbbp: 0.876604 test loss: 0.339141
[Epoch 27; Iter    12/   48] train: loss: 0.4802442
[Epoch 27; Iter    42/   48] train: loss: 0.5028375
[Epoch 27] ogbg-molbbbp: 0.883230 val loss: 0.340841
[Epoch 27] ogbg-molbbbp: 0.860198 test loss: 0.340806
[Epoch 28; Iter    24/   48] train: loss: 0.4197508
[Epoch 28] ogbg-molbbbp: 0.885612 val loss: 0.345281
[Epoch 28] ogbg-molbbbp: 0.881838 test loss: 0.328344
[Epoch 29; Iter     6/   48] train: loss: 0.2401379
[Epoch 29; Iter    36/   48] train: loss: 0.4723201
[Epoch 29] ogbg-molbbbp: 0.885782 val loss: 0.345188
[Epoch 29] ogbg-molbbbp: 0.881136 test loss: 0.349750
[Epoch 30; Iter    18/   48] train: loss: 0.2227652
[Epoch 30; Iter    48/   48] train: loss: 0.3012712
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.6/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.6_4_26-05_09-38-00
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.6
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6917866
[Epoch 1] ogbg-molbbbp: 0.613347 val loss: 0.692474
[Epoch 1] ogbg-molbbbp: 0.605168 test loss: 0.692549
[Epoch 2; Iter    19/   41] train: loss: 0.6894087
[Epoch 2] ogbg-molbbbp: 0.668032 val loss: 0.690529
[Epoch 2] ogbg-molbbbp: 0.661091 test loss: 0.690601
[Epoch 3; Iter     8/   41] train: loss: 0.6876114
[Epoch 3; Iter    38/   41] train: loss: 0.6889150
[Epoch 3] ogbg-molbbbp: 0.679467 val loss: 0.690141
[Epoch 3] ogbg-molbbbp: 0.679120 test loss: 0.690042
[Epoch 4; Iter    27/   41] train: loss: 0.6889830
[Epoch 4] ogbg-molbbbp: 0.670114 val loss: 0.689283
[Epoch 4] ogbg-molbbbp: 0.674913 test loss: 0.689066
[Epoch 5; Iter    16/   41] train: loss: 0.6906078
[Epoch 5] ogbg-molbbbp: 0.679126 val loss: 0.689152
[Epoch 5] ogbg-molbbbp: 0.677517 test loss: 0.689090
[Epoch 6; Iter     5/   41] train: loss: 0.6929627
[Epoch 6; Iter    35/   41] train: loss: 0.6865143
[Epoch 6] ogbg-molbbbp: 0.674825 val loss: 0.687777
[Epoch 6] ogbg-molbbbp: 0.676816 test loss: 0.687617
[Epoch 7; Iter    24/   41] train: loss: 0.6890219
[Epoch 7] ogbg-molbbbp: 0.681516 val loss: 0.687206
[Epoch 7] ogbg-molbbbp: 0.683427 test loss: 0.687079
[Epoch 8; Iter    13/   41] train: loss: 0.6924688
[Epoch 8] ogbg-molbbbp: 0.687865 val loss: 0.686508
[Epoch 8] ogbg-molbbbp: 0.688068 test loss: 0.686447
[Epoch 9; Iter     2/   41] train: loss: 0.6847900
[Epoch 9; Iter    32/   41] train: loss: 0.6856000
[Epoch 9] ogbg-molbbbp: 0.689469 val loss: 0.685606
[Epoch 9] ogbg-molbbbp: 0.690138 test loss: 0.685548
[Epoch 10; Iter    21/   41] train: loss: 0.6818454
[Epoch 10] ogbg-molbbbp: 0.687831 val loss: 0.684057
[Epoch 10] ogbg-molbbbp: 0.690271 test loss: 0.683915
[Epoch 11; Iter    10/   41] train: loss: 0.6820629
[Epoch 11; Iter    40/   41] train: loss: 0.6850677
[Epoch 11] ogbg-molbbbp: 0.693258 val loss: 0.682570
[Epoch 11] ogbg-molbbbp: 0.694778 test loss: 0.682436
[Epoch 12; Iter    29/   41] train: loss: 0.6819094
[Epoch 12] ogbg-molbbbp: 0.705445 val loss: 0.681819
[Epoch 12] ogbg-molbbbp: 0.701756 test loss: 0.681807
[Epoch 13; Iter    18/   41] train: loss: 0.6792008
[Epoch 13] ogbg-molbbbp: 0.704659 val loss: 0.680038
[Epoch 13] ogbg-molbbbp: 0.704294 test loss: 0.679991
[Epoch 14; Iter     7/   41] train: loss: 0.6771136
[Epoch 14; Iter    37/   41] train: loss: 0.6765751
[Epoch 14] ogbg-molbbbp: 0.703704 val loss: 0.677566
[Epoch 14] ogbg-molbbbp: 0.702624 test loss: 0.677423
[Epoch 15; Iter    26/   41] train: loss: 0.6717160
[Epoch 15] ogbg-molbbbp: 0.714286 val loss: 0.676231
[Epoch 15] ogbg-molbbbp: 0.710837 test loss: 0.676161
[Epoch 16; Iter    15/   41] train: loss: 0.6856787
[Epoch 16] ogbg-molbbbp: 0.708892 val loss: 0.674373
[Epoch 16] ogbg-molbbbp: 0.707966 test loss: 0.674207
[Epoch 17; Iter     4/   41] train: loss: 0.6701056
[Epoch 17; Iter    34/   41] train: loss: 0.6729767
[Epoch 17] ogbg-molbbbp: 0.782147 val loss: 0.663493
[Epoch 17] ogbg-molbbbp: 0.764323 test loss: 0.663742
[Epoch 18; Iter    23/   41] train: loss: 0.6511507
[Epoch 18] ogbg-molbbbp: 0.855265 val loss: 0.603271
[Epoch 18] ogbg-molbbbp: 0.852731 test loss: 0.604419
[Epoch 19; Iter    12/   41] train: loss: 0.5910521
[Epoch 19] ogbg-molbbbp: 0.846049 val loss: 0.465512
[Epoch 19] ogbg-molbbbp: 0.864416 test loss: 0.464549
[Epoch 20; Iter     1/   41] train: loss: 0.5107662
[Epoch 20; Iter    31/   41] train: loss: 0.4610890
[Epoch 20] ogbg-molbbbp: 0.878136 val loss: 0.490168
[Epoch 20] ogbg-molbbbp: 0.841513 test loss: 0.504015
[Epoch 21; Iter    20/   41] train: loss: 0.4164953
[Epoch 21] ogbg-molbbbp: 0.866633 val loss: 0.395419
[Epoch 21] ogbg-molbbbp: 0.863749 test loss: 0.410566
[Epoch 22; Iter     9/   41] train: loss: 0.3838634
[Epoch 22; Iter    39/   41] train: loss: 0.3729671
[Epoch 22] ogbg-molbbbp: 0.883359 val loss: 0.345287
[Epoch 22] ogbg-molbbbp: 0.871428 test loss: 0.370819
[Epoch 23; Iter    28/   41] train: loss: 0.3989960
[Epoch 23] ogbg-molbbbp: 0.879263 val loss: 0.338068
[Epoch 23] ogbg-molbbbp: 0.879841 test loss: 0.364814
[Epoch 24; Iter    17/   41] train: loss: 0.4808223
[Epoch 24] ogbg-molbbbp: 0.882676 val loss: 0.372111
[Epoch 24] ogbg-molbbbp: 0.879708 test loss: 0.395090
[Epoch 25; Iter     6/   41] train: loss: 0.2708065
[Epoch 25; Iter    36/   41] train: loss: 0.1795032
[Epoch 25] ogbg-molbbbp: 0.885168 val loss: 0.358516
[Epoch 25] ogbg-molbbbp: 0.886285 test loss: 0.384666
[Epoch 26; Iter    25/   41] train: loss: 0.3221485
[Epoch 26] ogbg-molbbbp: 0.893770 val loss: 0.323950
[Epoch 26] ogbg-molbbbp: 0.868423 test loss: 0.375346
[Epoch 27; Iter    14/   41] train: loss: 0.2820843
[Epoch 27] ogbg-molbbbp: 0.892678 val loss: 0.351146
[Epoch 27] ogbg-molbbbp: 0.860343 test loss: 0.399853
[Epoch 28; Iter     3/   41] train: loss: 0.3776917
[Epoch 28; Iter    33/   41] train: loss: 0.3826484
[Epoch 28] ogbg-molbbbp: 0.891722 val loss: 0.327263
[Epoch 28] ogbg-molbbbp: 0.873364 test loss: 0.370415
[Epoch 29; Iter    22/   41] train: loss: 0.3194164
[Epoch 29] ogbg-molbbbp: 0.879741 val loss: 0.359579
[Epoch 29] ogbg-molbbbp: 0.878239 test loss: 0.388525
[Epoch 30; Iter    11/   41] train: loss: 0.5085937
[Epoch 30; Iter    41/   41] train: loss: 0.1916055
[Epoch 30] ogbg-molbbbp: 0.888479 val loss: 0.324090
[Epoch 30] ogbg-molbbbp: 0.893830 test loss: 0.351727
[Epoch 31; Iter    30/   41] train: loss: 0.2603593
[Epoch 31] ogbg-molbbbp: 0.874484 val loss: 0.338520
[Epoch 31] ogbg-molbbbp: 0.883313 test loss: 0.364518
[Epoch 32; Iter    19/   41] train: loss: 0.3819032
[Epoch 32] ogbg-molbbbp: 0.893531 val loss: 0.326246
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bbbp/random/train_prop=0.6/PNA_ogbg-molbbbp_GraphCL_bbbp_random=0.6_6_26-05_09-38-01
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_random=0.6
logdir: runs/split/GraphCL/bbbp/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6933928
[Epoch 1] ogbg-molbbbp: 0.600137 val loss: 0.692771
[Epoch 1] ogbg-molbbbp: 0.635584 test loss: 0.692680
[Epoch 2; Iter    19/   41] train: loss: 0.6917765
[Epoch 2] ogbg-molbbbp: 0.625636 val loss: 0.690939
[Epoch 2] ogbg-molbbbp: 0.664630 test loss: 0.690643
[Epoch 3; Iter     8/   41] train: loss: 0.6891797
[Epoch 3; Iter    38/   41] train: loss: 0.6907784
[Epoch 3] ogbg-molbbbp: 0.634955 val loss: 0.690170
[Epoch 3] ogbg-molbbbp: 0.659255 test loss: 0.690021
[Epoch 4; Iter    27/   41] train: loss: 0.6896541
[Epoch 4] ogbg-molbbbp: 0.633248 val loss: 0.689949
[Epoch 4] ogbg-molbbbp: 0.659956 test loss: 0.689852
[Epoch 5; Iter    16/   41] train: loss: 0.6884916
[Epoch 5] ogbg-molbbbp: 0.633043 val loss: 0.689878
[Epoch 5] ogbg-molbbbp: 0.654113 test loss: 0.689821
[Epoch 6; Iter     5/   41] train: loss: 0.6891758
[Epoch 6; Iter    35/   41] train: loss: 0.6849240
[Epoch 6] ogbg-molbbbp: 0.642089 val loss: 0.688983
[Epoch 6] ogbg-molbbbp: 0.663929 test loss: 0.688912
[Epoch 7; Iter    24/   41] train: loss: 0.6891235
[Epoch 7] ogbg-molbbbp: 0.648097 val loss: 0.688399
[Epoch 7] ogbg-molbbbp: 0.669304 test loss: 0.688270
[Epoch 8; Iter    13/   41] train: loss: 0.6858563
[Epoch 8] ogbg-molbbbp: 0.649770 val loss: 0.687068
[Epoch 8] ogbg-molbbbp: 0.669338 test loss: 0.687004
[Epoch 9; Iter     2/   41] train: loss: 0.6910583
[Epoch 9; Iter    32/   41] train: loss: 0.6878142
[Epoch 9] ogbg-molbbbp: 0.660249 val loss: 0.686338
[Epoch 9] ogbg-molbbbp: 0.681557 test loss: 0.686240
[Epoch 10; Iter    21/   41] train: loss: 0.6870620
[Epoch 10] ogbg-molbbbp: 0.657757 val loss: 0.685035
[Epoch 10] ogbg-molbbbp: 0.676549 test loss: 0.685034
[Epoch 11; Iter    10/   41] train: loss: 0.6851500
[Epoch 11; Iter    40/   41] train: loss: 0.6895343
[Epoch 11] ogbg-molbbbp: 0.672606 val loss: 0.683670
[Epoch 11] ogbg-molbbbp: 0.690505 test loss: 0.683691
[Epoch 12; Iter    29/   41] train: loss: 0.6873000
[Epoch 12] ogbg-molbbbp: 0.677112 val loss: 0.682544
[Epoch 12] ogbg-molbbbp: 0.694378 test loss: 0.682513
[Epoch 13; Iter    18/   41] train: loss: 0.6833507
[Epoch 13] ogbg-molbbbp: 0.685475 val loss: 0.680840
[Epoch 13] ogbg-molbbbp: 0.702624 test loss: 0.680843
[Epoch 14; Iter     7/   41] train: loss: 0.6763170
[Epoch 14; Iter    37/   41] train: loss: 0.6767040
[Epoch 14] ogbg-molbbbp: 0.687182 val loss: 0.679040
[Epoch 14] ogbg-molbbbp: 0.703793 test loss: 0.679048
[Epoch 15; Iter    26/   41] train: loss: 0.6775008
[Epoch 15] ogbg-molbbbp: 0.697730 val loss: 0.676998
[Epoch 15] ogbg-molbbbp: 0.716346 test loss: 0.676963
[Epoch 16; Iter    15/   41] train: loss: 0.6769606
[Epoch 16] ogbg-molbbbp: 0.695443 val loss: 0.676242
[Epoch 16] ogbg-molbbbp: 0.712874 test loss: 0.676226
[Epoch 17; Iter     4/   41] train: loss: 0.6754262
[Epoch 17; Iter    34/   41] train: loss: 0.6753674
[Epoch 17] ogbg-molbbbp: 0.793139 val loss: 0.668898
[Epoch 17] ogbg-molbbbp: 0.780015 test loss: 0.669260
[Epoch 18; Iter    23/   41] train: loss: 0.6336964
[Epoch 18] ogbg-molbbbp: 0.856563 val loss: 0.584024
[Epoch 18] ogbg-molbbbp: 0.864817 test loss: 0.583939
[Epoch 19; Iter    12/   41] train: loss: 0.5633551
[Epoch 19] ogbg-molbbbp: 0.879980 val loss: 0.569922
[Epoch 19] ogbg-molbbbp: 0.867788 test loss: 0.582172
[Epoch 20; Iter     1/   41] train: loss: 0.5693873
[Epoch 20; Iter    31/   41] train: loss: 0.4701267
[Epoch 20] ogbg-molbbbp: 0.850009 val loss: 0.451671
[Epoch 20] ogbg-molbbbp: 0.862213 test loss: 0.456347
[Epoch 21; Iter    20/   41] train: loss: 0.3723079
[Epoch 21] ogbg-molbbbp: 0.878785 val loss: 0.354378
[Epoch 21] ogbg-molbbbp: 0.873698 test loss: 0.376117
[Epoch 22; Iter     9/   41] train: loss: 0.4897518
[Epoch 22; Iter    39/   41] train: loss: 0.3742364
[Epoch 22] ogbg-molbbbp: 0.891005 val loss: 0.363530
[Epoch 22] ogbg-molbbbp: 0.871595 test loss: 0.401466
[Epoch 23; Iter    28/   41] train: loss: 0.3830400
[Epoch 23] ogbg-molbbbp: 0.873562 val loss: 0.352086
[Epoch 23] ogbg-molbbbp: 0.876703 test loss: 0.375047
[Epoch 24; Iter    17/   41] train: loss: 0.3545576
[Epoch 24] ogbg-molbbbp: 0.875371 val loss: 0.357357
[Epoch 24] ogbg-molbbbp: 0.885317 test loss: 0.371431
[Epoch 25; Iter     6/   41] train: loss: 0.2979533
[Epoch 25; Iter    36/   41] train: loss: 0.4604826
[Epoch 25] ogbg-molbbbp: 0.888855 val loss: 0.336219
[Epoch 25] ogbg-molbbbp: 0.885617 test loss: 0.359842
[Epoch 26; Iter    25/   41] train: loss: 0.3450368
[Epoch 26] ogbg-molbbbp: 0.898174 val loss: 0.359710
[Epoch 26] ogbg-molbbbp: 0.856604 test loss: 0.423697
[Epoch 27; Iter    14/   41] train: loss: 0.3549239
[Epoch 27] ogbg-molbbbp: 0.893702 val loss: 0.325319
[Epoch 27] ogbg-molbbbp: 0.873731 test loss: 0.369447
[Epoch 28; Iter     3/   41] train: loss: 0.1960960
[Epoch 28; Iter    33/   41] train: loss: 0.2244636
[Epoch 28] ogbg-molbbbp: 0.877931 val loss: 0.338211
[Epoch 28] ogbg-molbbbp: 0.869424 test loss: 0.380776
[Epoch 29; Iter    22/   41] train: loss: 0.2796857
[Epoch 29] ogbg-molbbbp: 0.882335 val loss: 0.329322
[Epoch 29] ogbg-molbbbp: 0.886786 test loss: 0.355803
[Epoch 30; Iter    11/   41] train: loss: 0.3370377
[Epoch 30; Iter    41/   41] train: loss: 0.4256696
[Epoch 30] ogbg-molbbbp: 0.895716 val loss: 0.347004
[Epoch 30] ogbg-molbbbp: 0.886318 test loss: 0.377784
[Epoch 31; Iter    30/   41] train: loss: 0.4191715
[Epoch 31] ogbg-molbbbp: 0.889537 val loss: 0.317387
[Epoch 31] ogbg-molbbbp: 0.888421 test loss: 0.353501
[Epoch 32; Iter    19/   41] train: loss: 0.2651210
[Epoch 32] ogbg-molbbbp: 0.900427 val loss: 0.316797
[Epoch 28] ogbg-molbbbp: 0.913874 test loss: 0.281219
[Epoch 29; Iter    20/   55] train: loss: 0.4439100
[Epoch 29; Iter    50/   55] train: loss: 0.6773496
[Epoch 29] ogbg-molbbbp: 0.897654 val loss: 0.343455
[Epoch 29] ogbg-molbbbp: 0.899765 test loss: 0.410865
[Epoch 30; Iter    25/   55] train: loss: 0.3645784
[Epoch 30; Iter    55/   55] train: loss: 0.3170575
[Epoch 30] ogbg-molbbbp: 0.928519 val loss: 0.312030
[Epoch 30] ogbg-molbbbp: 0.937243 test loss: 0.278859
[Epoch 31; Iter    30/   55] train: loss: 0.1410968
[Epoch 31] ogbg-molbbbp: 0.882222 val loss: 0.426179
[Epoch 31] ogbg-molbbbp: 0.884921 test loss: 0.416175
[Epoch 32; Iter     5/   55] train: loss: 0.1719076
[Epoch 32; Iter    35/   55] train: loss: 0.2964484
[Epoch 32] ogbg-molbbbp: 0.907284 val loss: 0.349240
[Epoch 32] ogbg-molbbbp: 0.886978 test loss: 0.332958
[Epoch 33; Iter    10/   55] train: loss: 0.1861670
[Epoch 33; Iter    40/   55] train: loss: 0.2285961
[Epoch 33] ogbg-molbbbp: 0.912469 val loss: 0.342219
[Epoch 33] ogbg-molbbbp: 0.902410 test loss: 0.323553
[Epoch 34; Iter    15/   55] train: loss: 0.2978632
[Epoch 34; Iter    45/   55] train: loss: 0.2425226
[Epoch 34] ogbg-molbbbp: 0.920123 val loss: 0.319804
[Epoch 34] ogbg-molbbbp: 0.901235 test loss: 0.303233
[Epoch 35; Iter    20/   55] train: loss: 0.3345416
[Epoch 35; Iter    50/   55] train: loss: 0.3170535
[Epoch 35] ogbg-molbbbp: 0.941605 val loss: 0.293480
[Epoch 35] ogbg-molbbbp: 0.909171 test loss: 0.290518
[Epoch 36; Iter    25/   55] train: loss: 0.1007708
[Epoch 36; Iter    55/   55] train: loss: 0.1674310
[Epoch 36] ogbg-molbbbp: 0.913457 val loss: 0.335338
[Epoch 36] ogbg-molbbbp: 0.896972 test loss: 0.317831
[Epoch 37; Iter    30/   55] train: loss: 0.1504889
[Epoch 37] ogbg-molbbbp: 0.942593 val loss: 0.277829
[Epoch 37] ogbg-molbbbp: 0.898295 test loss: 0.327099
[Epoch 38; Iter     5/   55] train: loss: 0.3877974
[Epoch 38; Iter    35/   55] train: loss: 0.3136209
[Epoch 38] ogbg-molbbbp: 0.935679 val loss: 0.299547
[Epoch 38] ogbg-molbbbp: 0.916520 test loss: 0.297324
[Epoch 39; Iter    10/   55] train: loss: 0.2748037
[Epoch 39; Iter    40/   55] train: loss: 0.3130921
[Epoch 39] ogbg-molbbbp: 0.943333 val loss: 0.299742
[Epoch 39] ogbg-molbbbp: 0.895797 test loss: 0.336348
[Epoch 40; Iter    15/   55] train: loss: 0.3460162
[Epoch 40; Iter    45/   55] train: loss: 0.1166195
[Epoch 40] ogbg-molbbbp: 0.911605 val loss: 0.359905
[Epoch 40] ogbg-molbbbp: 0.894474 test loss: 0.394259
[Epoch 41; Iter    20/   55] train: loss: 0.1112190
[Epoch 41; Iter    50/   55] train: loss: 0.2222582
[Epoch 41] ogbg-molbbbp: 0.938519 val loss: 0.289359
[Epoch 41] ogbg-molbbbp: 0.915491 test loss: 0.323707
[Epoch 42; Iter    25/   55] train: loss: 0.1152355
[Epoch 42; Iter    55/   55] train: loss: 0.1135461
[Epoch 42] ogbg-molbbbp: 0.930988 val loss: 0.313986
[Epoch 42] ogbg-molbbbp: 0.914609 test loss: 0.330477
[Epoch 43; Iter    30/   55] train: loss: 0.0545947
[Epoch 43] ogbg-molbbbp: 0.920988 val loss: 0.347175
[Epoch 43] ogbg-molbbbp: 0.902557 test loss: 0.321711
[Epoch 44; Iter     5/   55] train: loss: 0.1108777
[Epoch 44; Iter    35/   55] train: loss: 0.2583914
[Epoch 44] ogbg-molbbbp: 0.934444 val loss: 0.319853
[Epoch 44] ogbg-molbbbp: 0.898442 test loss: 0.330729
[Epoch 45; Iter    10/   55] train: loss: 0.1399094
[Epoch 45; Iter    40/   55] train: loss: 0.0660010
[Epoch 45] ogbg-molbbbp: 0.927037 val loss: 0.324181
[Epoch 45] ogbg-molbbbp: 0.893004 test loss: 0.351702
[Epoch 46; Iter    15/   55] train: loss: 0.1754328
[Epoch 46; Iter    45/   55] train: loss: 0.4243594
[Epoch 46] ogbg-molbbbp: 0.920247 val loss: 0.350154
[Epoch 46] ogbg-molbbbp: 0.913727 test loss: 0.321229
[Epoch 47; Iter    20/   55] train: loss: 0.0811815
[Epoch 47; Iter    50/   55] train: loss: 0.0800148
[Epoch 47] ogbg-molbbbp: 0.932099 val loss: 0.329403
[Epoch 47] ogbg-molbbbp: 0.890653 test loss: 0.379769
[Epoch 48; Iter    25/   55] train: loss: 0.0617785
[Epoch 48; Iter    55/   55] train: loss: 0.1620356
[Epoch 48] ogbg-molbbbp: 0.921481 val loss: 0.363585
[Epoch 48] ogbg-molbbbp: 0.904468 test loss: 0.347777
[Epoch 49; Iter    30/   55] train: loss: 0.2119141
[Epoch 49] ogbg-molbbbp: 0.943086 val loss: 0.322608
[Epoch 49] ogbg-molbbbp: 0.905644 test loss: 0.325356
[Epoch 50; Iter     5/   55] train: loss: 0.0539832
[Epoch 50; Iter    35/   55] train: loss: 0.1228150
[Epoch 50] ogbg-molbbbp: 0.926296 val loss: 0.387727
[Epoch 50] ogbg-molbbbp: 0.914609 test loss: 0.341632
[Epoch 51; Iter    10/   55] train: loss: 0.0714238
[Epoch 51; Iter    40/   55] train: loss: 0.1560603
[Epoch 51] ogbg-molbbbp: 0.872346 val loss: 0.519313
[Epoch 51] ogbg-molbbbp: 0.876102 test loss: 0.412143
[Epoch 52; Iter    15/   55] train: loss: 0.1138384
[Epoch 52; Iter    45/   55] train: loss: 0.1117229
[Epoch 52] ogbg-molbbbp: 0.900988 val loss: 0.423416
[Epoch 52] ogbg-molbbbp: 0.892710 test loss: 0.421011
[Epoch 53; Iter    20/   55] train: loss: 0.0764493
[Epoch 53; Iter    50/   55] train: loss: 0.1142269
[Epoch 53] ogbg-molbbbp: 0.920123 val loss: 0.339946
[Epoch 53] ogbg-molbbbp: 0.871987 test loss: 0.393440
[Epoch 54; Iter    25/   55] train: loss: 0.0672005
[Epoch 54; Iter    55/   55] train: loss: 0.1273166
[Epoch 54] ogbg-molbbbp: 0.917778 val loss: 0.401397
[Epoch 54] ogbg-molbbbp: 0.887419 test loss: 0.400896
[Epoch 55; Iter    30/   55] train: loss: 0.0859404
[Epoch 55] ogbg-molbbbp: 0.915309 val loss: 0.385698
[Epoch 55] ogbg-molbbbp: 0.898736 test loss: 0.362962
[Epoch 56; Iter     5/   55] train: loss: 0.1426253
[Epoch 56; Iter    35/   55] train: loss: 0.0639020
[Epoch 56] ogbg-molbbbp: 0.911728 val loss: 0.395547
[Epoch 56] ogbg-molbbbp: 0.902410 test loss: 0.443410
[Epoch 57; Iter    10/   55] train: loss: 0.2075531
[Epoch 57; Iter    40/   55] train: loss: 0.1156622
[Epoch 57] ogbg-molbbbp: 0.888519 val loss: 0.444444
[Epoch 57] ogbg-molbbbp: 0.887419 test loss: 0.465954
[Epoch 58; Iter    15/   55] train: loss: 0.0439082
[Epoch 58; Iter    45/   55] train: loss: 0.0244006
[Epoch 58] ogbg-molbbbp: 0.918642 val loss: 0.453219
[Epoch 58] ogbg-molbbbp: 0.901529 test loss: 0.404514
[Epoch 59; Iter    20/   55] train: loss: 0.1392548
[Epoch 59; Iter    50/   55] train: loss: 0.1830596
[Epoch 59] ogbg-molbbbp: 0.887531 val loss: 0.497255
[Epoch 59] ogbg-molbbbp: 0.893445 test loss: 0.388978
[Epoch 60; Iter    25/   55] train: loss: 0.1461549
[Epoch 60; Iter    55/   55] train: loss: 0.5824434
[Epoch 60] ogbg-molbbbp: 0.940370 val loss: 0.316358
[Epoch 60] ogbg-molbbbp: 0.898736 test loss: 0.407233
[Epoch 61; Iter    30/   55] train: loss: 0.2719728
[Epoch 61] ogbg-molbbbp: 0.906667 val loss: 0.422408
[Epoch 61] ogbg-molbbbp: 0.885362 test loss: 0.471910
[Epoch 62; Iter     5/   55] train: loss: 0.2035530
[Epoch 62; Iter    35/   55] train: loss: 0.1446275
[Epoch 62] ogbg-molbbbp: 0.898148 val loss: 0.448643
[Epoch 62] ogbg-molbbbp: 0.884480 test loss: 0.429658
[Epoch 63; Iter    10/   55] train: loss: 0.1280652
[Epoch 63; Iter    40/   55] train: loss: 0.1525299
[Epoch 63] ogbg-molbbbp: 0.905185 val loss: 0.423915
[Epoch 63] ogbg-molbbbp: 0.893445 test loss: 0.423502
[Epoch 64; Iter    15/   55] train: loss: 0.2253806
[Epoch 64; Iter    45/   55] train: loss: 0.0612554
[Epoch 64] ogbg-molbbbp: 0.920988 val loss: 0.415929
[Epoch 64] ogbg-molbbbp: 0.889771 test loss: 0.435742
[Epoch 65; Iter    20/   55] train: loss: 0.0454102
[Epoch 65; Iter    50/   55] train: loss: 0.0788942
[Epoch 65] ogbg-molbbbp: 0.914691 val loss: 0.420208
[Epoch 65] ogbg-molbbbp: 0.905497 test loss: 0.365472
[Epoch 66; Iter    25/   55] train: loss: 0.1662612
[Epoch 66; Iter    55/   55] train: loss: 0.0229601
[Epoch 66] ogbg-molbbbp: 0.870864 val loss: 0.547730
[Epoch 66] ogbg-molbbbp: 0.879630 test loss: 0.547848
[Epoch 67; Iter    30/   55] train: loss: 0.0864807
[Epoch 67] ogbg-molbbbp: 0.931605 val loss: 0.388489
[Epoch 67] ogbg-molbbbp: 0.909612 test loss: 0.437508
[Epoch 68; Iter     5/   55] train: loss: 0.0238313
[Epoch 68; Iter    35/   55] train: loss: 0.2934105
[Epoch 68] ogbg-molbbbp: 0.921111 val loss: 0.493752
[Epoch 68] ogbg-molbbbp: 0.901235 test loss: 0.535438
[Epoch 69; Iter    10/   55] train: loss: 0.0717425
[Epoch 28] ogbg-molbbbp: 0.886537 test loss: 0.379149
[Epoch 29; Iter    20/   55] train: loss: 0.3981968
[Epoch 29; Iter    50/   55] train: loss: 0.3527985
[Epoch 29] ogbg-molbbbp: 0.878765 val loss: 0.433929
[Epoch 29] ogbg-molbbbp: 0.847884 test loss: 0.482855
[Epoch 30; Iter    25/   55] train: loss: 0.2154070
[Epoch 30; Iter    55/   55] train: loss: 0.1397856
[Epoch 30] ogbg-molbbbp: 0.930123 val loss: 0.325706
[Epoch 30] ogbg-molbbbp: 0.914903 test loss: 0.285721
[Epoch 31; Iter    30/   55] train: loss: 0.6688910
[Epoch 31] ogbg-molbbbp: 0.918148 val loss: 0.324604
[Epoch 31] ogbg-molbbbp: 0.891387 test loss: 0.334643
[Epoch 32; Iter     5/   55] train: loss: 0.2424384
[Epoch 32; Iter    35/   55] train: loss: 0.3134016
[Epoch 32] ogbg-molbbbp: 0.944691 val loss: 0.283171
[Epoch 32] ogbg-molbbbp: 0.903145 test loss: 0.307317
[Epoch 33; Iter    10/   55] train: loss: 0.2464463
[Epoch 33; Iter    40/   55] train: loss: 0.2150714
[Epoch 33] ogbg-molbbbp: 0.896049 val loss: 0.357681
[Epoch 33] ogbg-molbbbp: 0.881981 test loss: 0.325670
[Epoch 34; Iter    15/   55] train: loss: 0.2621770
[Epoch 34; Iter    45/   55] train: loss: 0.2271137
[Epoch 34] ogbg-molbbbp: 0.907160 val loss: 0.397235
[Epoch 34] ogbg-molbbbp: 0.885802 test loss: 0.443591
[Epoch 35; Iter    20/   55] train: loss: 0.0799230
[Epoch 35; Iter    50/   55] train: loss: 0.0991578
[Epoch 35] ogbg-molbbbp: 0.921605 val loss: 0.330417
[Epoch 35] ogbg-molbbbp: 0.891534 test loss: 0.344931
[Epoch 36; Iter    25/   55] train: loss: 0.1567859
[Epoch 36; Iter    55/   55] train: loss: 0.2506871
[Epoch 36] ogbg-molbbbp: 0.948395 val loss: 0.266551
[Epoch 36] ogbg-molbbbp: 0.898736 test loss: 0.365541
[Epoch 37; Iter    30/   55] train: loss: 0.2754475
[Epoch 37] ogbg-molbbbp: 0.913580 val loss: 0.359451
[Epoch 37] ogbg-molbbbp: 0.873604 test loss: 0.417855
[Epoch 38; Iter     5/   55] train: loss: 0.1317311
[Epoch 38; Iter    35/   55] train: loss: 0.3318986
[Epoch 38] ogbg-molbbbp: 0.887160 val loss: 0.392784
[Epoch 38] ogbg-molbbbp: 0.851411 test loss: 0.437927
[Epoch 39; Iter    10/   55] train: loss: 0.1690217
[Epoch 39; Iter    40/   55] train: loss: 0.1958954
[Epoch 39] ogbg-molbbbp: 0.910864 val loss: 0.370423
[Epoch 39] ogbg-molbbbp: 0.883598 test loss: 0.396396
[Epoch 40; Iter    15/   55] train: loss: 0.2800941
[Epoch 40; Iter    45/   55] train: loss: 0.2647521
[Epoch 40] ogbg-molbbbp: 0.906049 val loss: 0.354973
[Epoch 40] ogbg-molbbbp: 0.889771 test loss: 0.338995
[Epoch 41; Iter    20/   55] train: loss: 0.3207069
[Epoch 41; Iter    50/   55] train: loss: 0.2031585
[Epoch 41] ogbg-molbbbp: 0.899259 val loss: 0.355795
[Epoch 41] ogbg-molbbbp: 0.897707 test loss: 0.319865
[Epoch 42; Iter    25/   55] train: loss: 0.1293208
[Epoch 42; Iter    55/   55] train: loss: 0.4929202
[Epoch 42] ogbg-molbbbp: 0.917037 val loss: 0.389969
[Epoch 42] ogbg-molbbbp: 0.902998 test loss: 0.334704
[Epoch 43; Iter    30/   55] train: loss: 0.1813452
[Epoch 43] ogbg-molbbbp: 0.919753 val loss: 0.325278
[Epoch 43] ogbg-molbbbp: 0.903439 test loss: 0.356743
[Epoch 44; Iter     5/   55] train: loss: 0.1071703
[Epoch 44; Iter    35/   55] train: loss: 0.2872382
[Epoch 44] ogbg-molbbbp: 0.944321 val loss: 0.284662
[Epoch 44] ogbg-molbbbp: 0.909171 test loss: 0.327536
[Epoch 45; Iter    10/   55] train: loss: 0.2015881
[Epoch 45; Iter    40/   55] train: loss: 0.1515126
[Epoch 45] ogbg-molbbbp: 0.926667 val loss: 0.315977
[Epoch 45] ogbg-molbbbp: 0.889771 test loss: 0.363284
[Epoch 46; Iter    15/   55] train: loss: 0.1537466
[Epoch 46; Iter    45/   55] train: loss: 0.1477104
[Epoch 46] ogbg-molbbbp: 0.926543 val loss: 0.323976
[Epoch 46] ogbg-molbbbp: 0.903586 test loss: 0.332987
[Epoch 47; Iter    20/   55] train: loss: 0.3668083
[Epoch 47; Iter    50/   55] train: loss: 0.5566640
[Epoch 47] ogbg-molbbbp: 0.943457 val loss: 0.283608
[Epoch 47] ogbg-molbbbp: 0.915932 test loss: 0.282522
[Epoch 48; Iter    25/   55] train: loss: 0.1423588
[Epoch 48; Iter    55/   55] train: loss: 0.1075658
[Epoch 48] ogbg-molbbbp: 0.915556 val loss: 0.392444
[Epoch 48] ogbg-molbbbp: 0.877278 test loss: 0.442578
[Epoch 49; Iter    30/   55] train: loss: 0.1678823
[Epoch 49] ogbg-molbbbp: 0.916049 val loss: 0.370976
[Epoch 49] ogbg-molbbbp: 0.902116 test loss: 0.396903
[Epoch 50; Iter     5/   55] train: loss: 0.1182717
[Epoch 50; Iter    35/   55] train: loss: 0.2373404
[Epoch 50] ogbg-molbbbp: 0.947778 val loss: 0.280177
[Epoch 50] ogbg-molbbbp: 0.893151 test loss: 0.387180
[Epoch 51; Iter    10/   55] train: loss: 0.0249407
[Epoch 51; Iter    40/   55] train: loss: 0.2619562
[Epoch 51] ogbg-molbbbp: 0.873210 val loss: 0.525177
[Epoch 51] ogbg-molbbbp: 0.863463 test loss: 0.380606
[Epoch 52; Iter    15/   55] train: loss: 0.1510345
[Epoch 52; Iter    45/   55] train: loss: 0.0659976
[Epoch 52] ogbg-molbbbp: 0.931235 val loss: 0.325192
[Epoch 52] ogbg-molbbbp: 0.908877 test loss: 0.320112
[Epoch 53; Iter    20/   55] train: loss: 0.0989269
[Epoch 53; Iter    50/   55] train: loss: 0.1459264
[Epoch 53] ogbg-molbbbp: 0.895679 val loss: 0.423794
[Epoch 53] ogbg-molbbbp: 0.879924 test loss: 0.402186
[Epoch 54; Iter    25/   55] train: loss: 0.2410369
[Epoch 54; Iter    55/   55] train: loss: 0.2656982
[Epoch 54] ogbg-molbbbp: 0.925309 val loss: 0.678057
[Epoch 54] ogbg-molbbbp: 0.916226 test loss: 0.308868
[Epoch 55; Iter    30/   55] train: loss: 0.0905218
[Epoch 55] ogbg-molbbbp: 0.903086 val loss: 0.398098
[Epoch 55] ogbg-molbbbp: 0.883304 test loss: 0.362275
[Epoch 56; Iter     5/   55] train: loss: 0.3985317
[Epoch 56; Iter    35/   55] train: loss: 0.1712013
[Epoch 56] ogbg-molbbbp: 0.939259 val loss: 0.297430
[Epoch 56] ogbg-molbbbp: 0.868019 test loss: 0.448815
[Epoch 57; Iter    10/   55] train: loss: 0.1777350
[Epoch 57; Iter    40/   55] train: loss: 0.1400603
[Epoch 57] ogbg-molbbbp: 0.885185 val loss: 0.470404
[Epoch 57] ogbg-molbbbp: 0.861258 test loss: 0.421933
[Epoch 58; Iter    15/   55] train: loss: 0.2257720
[Epoch 58; Iter    45/   55] train: loss: 0.0873089
[Epoch 58] ogbg-molbbbp: 0.933210 val loss: 0.346767
[Epoch 58] ogbg-molbbbp: 0.891534 test loss: 0.413625
[Epoch 59; Iter    20/   55] train: loss: 0.1228440
[Epoch 59; Iter    50/   55] train: loss: 0.1182035
[Epoch 59] ogbg-molbbbp: 0.943827 val loss: 0.318913
[Epoch 59] ogbg-molbbbp: 0.912257 test loss: 0.352648
[Epoch 60; Iter    25/   55] train: loss: 0.2923834
[Epoch 60; Iter    55/   55] train: loss: 0.1778407
[Epoch 60] ogbg-molbbbp: 0.940988 val loss: 0.331776
[Epoch 60] ogbg-molbbbp: 0.899765 test loss: 0.409934
[Epoch 61; Iter    30/   55] train: loss: 0.0751220
[Epoch 61] ogbg-molbbbp: 0.901358 val loss: 0.452195
[Epoch 61] ogbg-molbbbp: 0.888595 test loss: 0.382842
[Epoch 62; Iter     5/   55] train: loss: 0.1946574
[Epoch 62; Iter    35/   55] train: loss: 0.0794651
[Epoch 62] ogbg-molbbbp: 0.921235 val loss: 0.365275
[Epoch 62] ogbg-molbbbp: 0.851999 test loss: 0.506090
[Epoch 63; Iter    10/   55] train: loss: 0.0336962
[Epoch 63; Iter    40/   55] train: loss: 0.1742188
[Epoch 63] ogbg-molbbbp: 0.913827 val loss: 0.365644
[Epoch 63] ogbg-molbbbp: 0.879189 test loss: 0.363563
[Epoch 64; Iter    15/   55] train: loss: 0.1087254
[Epoch 64; Iter    45/   55] train: loss: 0.0283758
[Epoch 64] ogbg-molbbbp: 0.917160 val loss: 0.414402
[Epoch 64] ogbg-molbbbp: 0.896678 test loss: 0.418857
[Epoch 65; Iter    20/   55] train: loss: 0.0881221
[Epoch 65; Iter    50/   55] train: loss: 0.1090618
[Epoch 65] ogbg-molbbbp: 0.917654 val loss: 0.499766
[Epoch 65] ogbg-molbbbp: 0.880658 test loss: 0.596873
[Epoch 66; Iter    25/   55] train: loss: 0.0176616
[Epoch 66; Iter    55/   55] train: loss: 0.1362282
[Epoch 66] ogbg-molbbbp: 0.895926 val loss: 0.505808
[Epoch 66] ogbg-molbbbp: 0.863904 test loss: 0.514326
[Epoch 67; Iter    30/   55] train: loss: 0.0551137
[Epoch 67] ogbg-molbbbp: 0.901111 val loss: 0.410160
[Epoch 67] ogbg-molbbbp: 0.876984 test loss: 0.469176
[Epoch 68; Iter     5/   55] train: loss: 0.0115437
[Epoch 68; Iter    35/   55] train: loss: 0.0415064
[Epoch 68] ogbg-molbbbp: 0.925679 val loss: 0.370459
[Epoch 68] ogbg-molbbbp: 0.868754 test loss: 0.463589
[Epoch 69; Iter    10/   55] train: loss: 0.0827650
[Epoch 30] ogbg-molbbbp: 0.897692 val loss: 0.365053
[Epoch 30] ogbg-molbbbp: 0.883690 test loss: 0.392309
[Epoch 31; Iter    30/   48] train: loss: 0.6006454
[Epoch 31] ogbg-molbbbp: 0.885385 val loss: 0.419303
[Epoch 31] ogbg-molbbbp: 0.905650 test loss: 0.392482
[Epoch 32; Iter    12/   48] train: loss: 0.1889299
[Epoch 32; Iter    42/   48] train: loss: 0.1469989
[Epoch 32] ogbg-molbbbp: 0.889412 val loss: 0.340549
[Epoch 32] ogbg-molbbbp: 0.915034 test loss: 0.295516
[Epoch 33; Iter    24/   48] train: loss: 0.2462212
[Epoch 33] ogbg-molbbbp: 0.898316 val loss: 0.458224
[Epoch 33] ogbg-molbbbp: 0.850495 test loss: 0.564518
[Epoch 34; Iter     6/   48] train: loss: 0.1489424
[Epoch 34; Iter    36/   48] train: loss: 0.1959488
[Epoch 34] ogbg-molbbbp: 0.874270 val loss: 0.367064
[Epoch 34] ogbg-molbbbp: 0.891286 test loss: 0.324964
[Epoch 35; Iter    18/   48] train: loss: 0.3021959
[Epoch 35; Iter    48/   48] train: loss: 0.2144843
[Epoch 35] ogbg-molbbbp: 0.892815 val loss: 0.599001
[Epoch 35] ogbg-molbbbp: 0.876285 test loss: 0.445228
[Epoch 36; Iter    30/   48] train: loss: 0.4188377
[Epoch 36] ogbg-molbbbp: 0.900868 val loss: 0.318804
[Epoch 36] ogbg-molbbbp: 0.912161 test loss: 0.289456
[Epoch 37; Iter    12/   48] train: loss: 0.2075822
[Epoch 37; Iter    42/   48] train: loss: 0.1924589
[Epoch 37] ogbg-molbbbp: 0.926445 val loss: 0.298341
[Epoch 37] ogbg-molbbbp: 0.922566 test loss: 0.289095
[Epoch 38; Iter    24/   48] train: loss: 0.2947883
[Epoch 38] ogbg-molbbbp: 0.902739 val loss: 0.295331
[Epoch 38] ogbg-molbbbp: 0.904245 test loss: 0.291067
[Epoch 39; Iter     6/   48] train: loss: 0.2023834
[Epoch 39; Iter    36/   48] train: loss: 0.1830775
[Epoch 39] ogbg-molbbbp: 0.896898 val loss: 0.335407
[Epoch 39] ogbg-molbbbp: 0.899138 test loss: 0.307200
[Epoch 40; Iter    18/   48] train: loss: 0.1929581
[Epoch 40; Iter    48/   48] train: loss: 0.0824125
[Epoch 40] ogbg-molbbbp: 0.904611 val loss: 0.342908
[Epoch 40] ogbg-molbbbp: 0.905713 test loss: 0.310375
[Epoch 41; Iter    30/   48] train: loss: 0.0918430
[Epoch 41] ogbg-molbbbp: 0.916690 val loss: 0.422849
[Epoch 41] ogbg-molbbbp: 0.906033 test loss: 0.382298
[Epoch 42; Iter    12/   48] train: loss: 0.1799379
[Epoch 42; Iter    42/   48] train: loss: 0.3449733
[Epoch 42] ogbg-molbbbp: 0.916180 val loss: 0.322574
[Epoch 42] ogbg-molbbbp: 0.911778 test loss: 0.288015
[Epoch 43; Iter    24/   48] train: loss: 0.1865221
[Epoch 43] ogbg-molbbbp: 0.916237 val loss: 0.319604
[Epoch 43] ogbg-molbbbp: 0.881136 test loss: 0.331284
[Epoch 44; Iter     6/   48] train: loss: 0.2115908
[Epoch 44; Iter    36/   48] train: loss: 0.1194407
[Epoch 44] ogbg-molbbbp: 0.911870 val loss: 0.376224
[Epoch 44] ogbg-molbbbp: 0.890393 test loss: 0.365246
[Epoch 45; Iter    18/   48] train: loss: 0.1988833
[Epoch 45; Iter    48/   48] train: loss: 0.2391749
[Epoch 45] ogbg-molbbbp: 0.930471 val loss: 0.295427
[Epoch 45] ogbg-molbbbp: 0.913118 test loss: 0.321005
[Epoch 46; Iter    30/   48] train: loss: 0.1833525
[Epoch 46] ogbg-molbbbp: 0.891567 val loss: 0.404147
[Epoch 46] ogbg-molbbbp: 0.877306 test loss: 0.439990
[Epoch 47; Iter    12/   48] train: loss: 0.0831660
[Epoch 47; Iter    42/   48] train: loss: 0.2277638
[Epoch 47] ogbg-molbbbp: 0.918222 val loss: 0.331123
[Epoch 47] ogbg-molbbbp: 0.903990 test loss: 0.326041
[Epoch 48; Iter    24/   48] train: loss: 0.3975642
[Epoch 48] ogbg-molbbbp: 0.915953 val loss: 0.347249
[Epoch 48] ogbg-molbbbp: 0.903160 test loss: 0.332918
[Epoch 49; Iter     6/   48] train: loss: 0.1531603
[Epoch 49; Iter    36/   48] train: loss: 0.1234038
[Epoch 49] ogbg-molbbbp: 0.912437 val loss: 0.321733
[Epoch 49] ogbg-molbbbp: 0.916246 test loss: 0.301177
[Epoch 50; Iter    18/   48] train: loss: 0.0770244
[Epoch 50; Iter    48/   48] train: loss: 0.0703382
[Epoch 50] ogbg-molbbbp: 0.907446 val loss: 0.353104
[Epoch 50] ogbg-molbbbp: 0.885988 test loss: 0.360082
[Epoch 51; Iter    30/   48] train: loss: 0.2624532
[Epoch 51] ogbg-molbbbp: 0.921738 val loss: 0.327617
[Epoch 51] ogbg-molbbbp: 0.902139 test loss: 0.332783
[Epoch 52; Iter    12/   48] train: loss: 0.0683207
[Epoch 52; Iter    42/   48] train: loss: 0.4498340
[Epoch 52] ogbg-molbbbp: 0.914932 val loss: 0.374946
[Epoch 52] ogbg-molbbbp: 0.897095 test loss: 0.366532
[Epoch 53; Iter    24/   48] train: loss: 0.1666988
[Epoch 53] ogbg-molbbbp: 0.894686 val loss: 0.398287
[Epoch 53] ogbg-molbbbp: 0.891350 test loss: 0.360968
[Epoch 54; Iter     6/   48] train: loss: 0.1047077
[Epoch 54; Iter    36/   48] train: loss: 0.0889134
[Epoch 54] ogbg-molbbbp: 0.922191 val loss: 0.308400
[Epoch 54] ogbg-molbbbp: 0.910246 test loss: 0.307699
[Epoch 55; Iter    18/   48] train: loss: 0.0969215
[Epoch 55; Iter    48/   48] train: loss: 0.0608737
[Epoch 55] ogbg-molbbbp: 0.910565 val loss: 0.558909
[Epoch 55] ogbg-molbbbp: 0.908331 test loss: 0.472531
[Epoch 56; Iter    30/   48] train: loss: 0.1295111
[Epoch 56] ogbg-molbbbp: 0.900527 val loss: 0.349711
[Epoch 56] ogbg-molbbbp: 0.910820 test loss: 0.339761
[Epoch 57; Iter    12/   48] train: loss: 0.0517475
[Epoch 57; Iter    42/   48] train: loss: 0.1945167
[Epoch 57] ogbg-molbbbp: 0.915102 val loss: 0.324718
[Epoch 57] ogbg-molbbbp: 0.894542 test loss: 0.372277
[Epoch 58; Iter    24/   48] train: loss: 0.0633201
[Epoch 58] ogbg-molbbbp: 0.895764 val loss: 0.737207
[Epoch 58] ogbg-molbbbp: 0.884903 test loss: 0.428337
[Epoch 59; Iter     6/   48] train: loss: 0.1439116
[Epoch 59; Iter    36/   48] train: loss: 0.0639385
[Epoch 59] ogbg-molbbbp: 0.905631 val loss: 0.403746
[Epoch 59] ogbg-molbbbp: 0.896266 test loss: 0.373110
[Epoch 60; Iter    18/   48] train: loss: 0.0357968
[Epoch 60; Iter    48/   48] train: loss: 0.2626940
[Epoch 60] ogbg-molbbbp: 0.896784 val loss: 0.431384
[Epoch 60] ogbg-molbbbp: 0.899777 test loss: 0.356464
[Epoch 61; Iter    30/   48] train: loss: 0.1098801
[Epoch 61] ogbg-molbbbp: 0.904327 val loss: 0.608715
[Epoch 61] ogbg-molbbbp: 0.894989 test loss: 0.373496
[Epoch 62; Iter    12/   48] train: loss: 0.1382231
[Epoch 62; Iter    42/   48] train: loss: 0.1141151
[Epoch 62] ogbg-molbbbp: 0.910168 val loss: 0.336121
[Epoch 62] ogbg-molbbbp: 0.887392 test loss: 0.353208
[Epoch 63; Iter    24/   48] train: loss: 0.1231055
[Epoch 63] ogbg-molbbbp: 0.923382 val loss: 0.287286
[Epoch 63] ogbg-molbbbp: 0.893776 test loss: 0.370148
[Epoch 64; Iter     6/   48] train: loss: 0.1204053
[Epoch 64; Iter    36/   48] train: loss: 0.1277498
[Epoch 64] ogbg-molbbbp: 0.900017 val loss: 0.382824
[Epoch 64] ogbg-molbbbp: 0.874114 test loss: 0.370904
[Epoch 65; Iter    18/   48] train: loss: 0.0355930
[Epoch 65; Iter    48/   48] train: loss: 0.9177547
[Epoch 65] ogbg-molbbbp: 0.910055 val loss: 0.376801
[Epoch 65] ogbg-molbbbp: 0.902522 test loss: 0.357199
[Epoch 66; Iter    30/   48] train: loss: 0.1254445
[Epoch 66] ogbg-molbbbp: 0.912494 val loss: 0.369714
[Epoch 66] ogbg-molbbbp: 0.903032 test loss: 0.369206
[Epoch 67; Iter    12/   48] train: loss: 0.0662290
[Epoch 67; Iter    42/   48] train: loss: 0.0269667
[Epoch 67] ogbg-molbbbp: 0.904724 val loss: 0.405634
[Epoch 67] ogbg-molbbbp: 0.901245 test loss: 0.388851
[Epoch 68; Iter    24/   48] train: loss: 0.0505918
[Epoch 68] ogbg-molbbbp: 0.891737 val loss: 0.387607
[Epoch 68] ogbg-molbbbp: 0.879604 test loss: 0.425440
[Epoch 69; Iter     6/   48] train: loss: 0.0994061
[Epoch 69; Iter    36/   48] train: loss: 0.0591447
[Epoch 69] ogbg-molbbbp: 0.895026 val loss: 0.407005
[Epoch 69] ogbg-molbbbp: 0.876732 test loss: 0.489065
[Epoch 70; Iter    18/   48] train: loss: 0.0856640
[Epoch 70; Iter    48/   48] train: loss: 0.2072801
[Epoch 70] ogbg-molbbbp: 0.912664 val loss: 0.353951
[Epoch 70] ogbg-molbbbp: 0.876157 test loss: 0.465779
[Epoch 71; Iter    30/   48] train: loss: 0.0268926
[Epoch 71] ogbg-molbbbp: 0.890092 val loss: 0.454457
[Epoch 71] ogbg-molbbbp: 0.902522 test loss: 0.401135
[Epoch 72; Iter    12/   48] train: loss: 0.0840276
[Epoch 72; Iter    42/   48] train: loss: 0.0242852
[Epoch 72] ogbg-molbbbp: 0.901832 val loss: 0.416175
[Epoch 72] ogbg-molbbbp: 0.892818 test loss: 0.488686
[Epoch 73; Iter    24/   48] train: loss: 0.0610506
[Epoch 73] ogbg-molbbbp: 0.908070 val loss: 0.396271
[Epoch 30] ogbg-molbbbp: 0.899790 val loss: 0.349866
[Epoch 30] ogbg-molbbbp: 0.895436 test loss: 0.354690
[Epoch 31; Iter    30/   48] train: loss: 0.3689790
[Epoch 31] ogbg-molbbbp: 0.909034 val loss: 0.303915
[Epoch 31] ogbg-molbbbp: 0.904437 test loss: 0.318080
[Epoch 32; Iter    12/   48] train: loss: 0.2714747
[Epoch 32; Iter    42/   48] train: loss: 0.4545480
[Epoch 32] ogbg-molbbbp: 0.881529 val loss: 0.422858
[Epoch 32] ogbg-molbbbp: 0.893521 test loss: 0.439366
[Epoch 33; Iter    24/   48] train: loss: 0.3144512
[Epoch 33] ogbg-molbbbp: 0.905178 val loss: 0.298747
[Epoch 33] ogbg-molbbbp: 0.910054 test loss: 0.284932
[Epoch 34; Iter     6/   48] train: loss: 0.1131742
[Epoch 34; Iter    36/   48] train: loss: 0.4120332
[Epoch 34] ogbg-molbbbp: 0.905405 val loss: 0.311365
[Epoch 34] ogbg-molbbbp: 0.876795 test loss: 0.352055
[Epoch 35; Iter    18/   48] train: loss: 0.2171826
[Epoch 35; Iter    48/   48] train: loss: 0.4428791
[Epoch 35] ogbg-molbbbp: 0.899110 val loss: 0.344973
[Epoch 35] ogbg-molbbbp: 0.899840 test loss: 0.319353
[Epoch 36; Iter    30/   48] train: loss: 0.4382568
[Epoch 36] ogbg-molbbbp: 0.887881 val loss: 0.345685
[Epoch 36] ogbg-molbbbp: 0.909033 test loss: 0.297367
[Epoch 37; Iter    12/   48] train: loss: 0.2472375
[Epoch 37; Iter    42/   48] train: loss: 0.3107021
[Epoch 37] ogbg-molbbbp: 0.925878 val loss: 0.287618
[Epoch 37] ogbg-molbbbp: 0.920715 test loss: 0.275517
[Epoch 38; Iter    24/   48] train: loss: 0.1495776
[Epoch 38] ogbg-molbbbp: 0.907333 val loss: 0.317083
[Epoch 38] ogbg-molbbbp: 0.909927 test loss: 0.331802
[Epoch 39; Iter     6/   48] train: loss: 0.1189198
[Epoch 39; Iter    36/   48] train: loss: 0.4243846
[Epoch 39] ogbg-molbbbp: 0.904441 val loss: 0.316874
[Epoch 39] ogbg-molbbbp: 0.885286 test loss: 0.306676
[Epoch 40; Iter    18/   48] train: loss: 0.1338978
[Epoch 40; Iter    48/   48] train: loss: 0.3089857
[Epoch 40] ogbg-molbbbp: 0.900754 val loss: 0.355885
[Epoch 40] ogbg-molbbbp: 0.890903 test loss: 0.359570
[Epoch 41; Iter    30/   48] train: loss: 0.1927034
[Epoch 41] ogbg-molbbbp: 0.912380 val loss: 0.289076
[Epoch 41] ogbg-molbbbp: 0.911523 test loss: 0.283429
[Epoch 42; Iter    12/   48] train: loss: 0.3140524
[Epoch 42; Iter    42/   48] train: loss: 0.2804352
[Epoch 42] ogbg-molbbbp: 0.915556 val loss: 0.309611
[Epoch 42] ogbg-molbbbp: 0.905713 test loss: 0.307699
[Epoch 43; Iter    24/   48] train: loss: 0.1138786
[Epoch 43] ogbg-molbbbp: 0.912947 val loss: 0.326967
[Epoch 43] ogbg-molbbbp: 0.887903 test loss: 0.337288
[Epoch 44; Iter     6/   48] train: loss: 0.4438550
[Epoch 44; Iter    36/   48] train: loss: 0.2829581
[Epoch 44] ogbg-molbbbp: 0.919412 val loss: 0.302137
[Epoch 44] ogbg-molbbbp: 0.909352 test loss: 0.285860
[Epoch 45; Iter    18/   48] train: loss: 0.0683502
[Epoch 45; Iter    48/   48] train: loss: 0.1563840
[Epoch 45] ogbg-molbbbp: 0.910452 val loss: 0.346535
[Epoch 45] ogbg-molbbbp: 0.907948 test loss: 0.349699
[Epoch 46; Iter    30/   48] train: loss: 0.3292576
[Epoch 46] ogbg-molbbbp: 0.904044 val loss: 0.360256
[Epoch 46] ogbg-molbbbp: 0.910756 test loss: 0.334683
[Epoch 47; Iter    12/   48] train: loss: 0.2850775
[Epoch 47; Iter    42/   48] train: loss: 0.1272638
[Epoch 47] ogbg-molbbbp: 0.910395 val loss: 0.308964
[Epoch 47] ogbg-molbbbp: 0.896202 test loss: 0.348393
[Epoch 48; Iter    24/   48] train: loss: 0.1684646
[Epoch 48] ogbg-molbbbp: 0.915273 val loss: 0.367212
[Epoch 48] ogbg-molbbbp: 0.887265 test loss: 0.402728
[Epoch 49; Iter     6/   48] train: loss: 0.1394555
[Epoch 49; Iter    36/   48] train: loss: 0.2132859
[Epoch 49] ogbg-molbbbp: 0.891283 val loss: 0.355777
[Epoch 49] ogbg-molbbbp: 0.899521 test loss: 0.331858
[Epoch 50; Iter    18/   48] train: loss: 0.2326261
[Epoch 50; Iter    48/   48] train: loss: 0.0521907
[Epoch 50] ogbg-molbbbp: 0.925594 val loss: 0.442987
[Epoch 50] ogbg-molbbbp: 0.883881 test loss: 0.480417
[Epoch 51; Iter    30/   48] train: loss: 0.1966110
[Epoch 51] ogbg-molbbbp: 0.906028 val loss: 0.340689
[Epoch 51] ogbg-molbbbp: 0.889499 test loss: 0.357852
[Epoch 52; Iter    12/   48] train: loss: 0.0443658
[Epoch 52; Iter    42/   48] train: loss: 0.2358456
[Epoch 52] ogbg-molbbbp: 0.913798 val loss: 0.342713
[Epoch 52] ogbg-molbbbp: 0.878072 test loss: 0.363165
[Epoch 53; Iter    24/   48] train: loss: 0.1155631
[Epoch 53] ogbg-molbbbp: 0.909318 val loss: 0.407266
[Epoch 53] ogbg-molbbbp: 0.899904 test loss: 0.363183
[Epoch 54; Iter     6/   48] train: loss: 0.3844077
[Epoch 54; Iter    36/   48] train: loss: 0.0770159
[Epoch 54] ogbg-molbbbp: 0.919469 val loss: 0.400610
[Epoch 54] ogbg-molbbbp: 0.919438 test loss: 0.385137
[Epoch 55; Iter    18/   48] train: loss: 0.0933071
[Epoch 55; Iter    48/   48] train: loss: 0.1965489
[Epoch 55] ogbg-molbbbp: 0.894629 val loss: 0.439074
[Epoch 55] ogbg-molbbbp: 0.887328 test loss: 0.443172
[Epoch 56; Iter    30/   48] train: loss: 0.1253800
[Epoch 56] ogbg-molbbbp: 0.898089 val loss: 0.618512
[Epoch 56] ogbg-molbbbp: 0.873412 test loss: 0.410740
[Epoch 57; Iter    12/   48] train: loss: 0.1063266
[Epoch 57; Iter    42/   48] train: loss: 0.2602269
[Epoch 57] ogbg-molbbbp: 0.895253 val loss: 0.459202
[Epoch 57] ogbg-molbbbp: 0.852538 test loss: 0.512339
[Epoch 58; Iter    24/   48] train: loss: 0.1991232
[Epoch 58] ogbg-molbbbp: 0.910055 val loss: 0.484617
[Epoch 58] ogbg-molbbbp: 0.893904 test loss: 0.427833
[Epoch 59; Iter     6/   48] train: loss: 0.1873053
[Epoch 59; Iter    36/   48] train: loss: 0.1491294
[Epoch 59] ogbg-molbbbp: 0.920547 val loss: 0.297871
[Epoch 59] ogbg-molbbbp: 0.880689 test loss: 0.363003
[Epoch 60; Iter    18/   48] train: loss: 0.1050111
[Epoch 60; Iter    48/   48] train: loss: 0.0689336
[Epoch 60] ogbg-molbbbp: 0.914025 val loss: 0.487478
[Epoch 60] ogbg-molbbbp: 0.919885 test loss: 0.298012
[Epoch 61; Iter    30/   48] train: loss: 0.2673187
[Epoch 61] ogbg-molbbbp: 0.915102 val loss: 0.346436
[Epoch 61] ogbg-molbbbp: 0.888541 test loss: 0.382058
[Epoch 62; Iter    12/   48] train: loss: 0.4111482
[Epoch 62; Iter    42/   48] train: loss: 0.1483156
[Epoch 62] ogbg-molbbbp: 0.893552 val loss: 0.438213
[Epoch 62] ogbg-molbbbp: 0.891159 test loss: 0.416190
[Epoch 63; Iter    24/   48] train: loss: 0.1852525
[Epoch 63] ogbg-molbbbp: 0.905008 val loss: 0.465984
[Epoch 63] ogbg-molbbbp: 0.875455 test loss: 0.469093
[Epoch 64; Iter     6/   48] train: loss: 0.1192733
[Epoch 64; Iter    36/   48] train: loss: 0.0527827
[Epoch 64] ogbg-molbbbp: 0.915102 val loss: 0.373750
[Epoch 64] ogbg-molbbbp: 0.884839 test loss: 0.430675
[Epoch 65; Iter    18/   48] train: loss: 0.0196712
[Epoch 65; Iter    48/   48] train: loss: 0.2964496
[Epoch 65] ogbg-molbbbp: 0.914876 val loss: 0.340681
[Epoch 65] ogbg-molbbbp: 0.884903 test loss: 0.402056
[Epoch 66; Iter    30/   48] train: loss: 0.1090302
[Epoch 66] ogbg-molbbbp: 0.908410 val loss: 0.379308
[Epoch 66] ogbg-molbbbp: 0.877242 test loss: 0.425470
[Epoch 67; Iter    12/   48] train: loss: 0.0363058
[Epoch 67; Iter    42/   48] train: loss: 0.0245593
[Epoch 67] ogbg-molbbbp: 0.909034 val loss: 0.394775
[Epoch 67] ogbg-molbbbp: 0.881966 test loss: 0.493046
[Epoch 68; Iter    24/   48] train: loss: 0.0965681
[Epoch 68] ogbg-molbbbp: 0.917541 val loss: 0.401217
[Epoch 68] ogbg-molbbbp: 0.894861 test loss: 0.465845
[Epoch 69; Iter     6/   48] train: loss: 0.0423511
[Epoch 69; Iter    36/   48] train: loss: 0.1345625
[Epoch 69] ogbg-molbbbp: 0.910282 val loss: 0.397706
[Epoch 69] ogbg-molbbbp: 0.908267 test loss: 0.415618
[Epoch 70; Iter    18/   48] train: loss: 0.0763346
[Epoch 70; Iter    48/   48] train: loss: 0.1588494
[Epoch 70] ogbg-molbbbp: 0.907333 val loss: 0.394725
[Epoch 70] ogbg-molbbbp: 0.887775 test loss: 0.457082
[Epoch 71; Iter    30/   48] train: loss: 0.0163014
[Epoch 71] ogbg-molbbbp: 0.907730 val loss: 0.431681
[Epoch 71] ogbg-molbbbp: 0.901883 test loss: 0.440815
[Epoch 72; Iter    12/   48] train: loss: 0.0333218
[Epoch 72; Iter    42/   48] train: loss: 0.0250874
[Epoch 72] ogbg-molbbbp: 0.913628 val loss: 0.419217
[Epoch 72] ogbg-molbbbp: 0.878647 test loss: 0.542958
[Epoch 73; Iter    24/   48] train: loss: 0.0284586
[Epoch 73] ogbg-molbbbp: 0.906085 val loss: 0.466260
[Epoch 30] ogbg-molbbbp: 0.911359 val loss: 0.299232
[Epoch 30] ogbg-molbbbp: 0.918225 test loss: 0.285752
[Epoch 31; Iter    30/   48] train: loss: 0.2503449
[Epoch 31] ogbg-molbbbp: 0.892871 val loss: 0.329017
[Epoch 31] ogbg-molbbbp: 0.923907 test loss: 0.280187
[Epoch 32; Iter    12/   48] train: loss: 0.2573304
[Epoch 32; Iter    42/   48] train: loss: 0.2481531
[Epoch 32] ogbg-molbbbp: 0.885385 val loss: 0.334954
[Epoch 32] ogbg-molbbbp: 0.889946 test loss: 0.308329
[Epoch 33; Iter    24/   48] train: loss: 0.2591235
[Epoch 33] ogbg-molbbbp: 0.881245 val loss: 0.356032
[Epoch 33] ogbg-molbbbp: 0.865113 test loss: 0.373246
[Epoch 34; Iter     6/   48] train: loss: 0.1916166
[Epoch 34; Iter    36/   48] train: loss: 0.2301255
[Epoch 34] ogbg-molbbbp: 0.889525 val loss: 0.337386
[Epoch 34] ogbg-molbbbp: 0.889307 test loss: 0.330068
[Epoch 35; Iter    18/   48] train: loss: 0.2037600
[Epoch 35; Iter    48/   48] train: loss: 0.4210527
[Epoch 35] ogbg-molbbbp: 0.895877 val loss: 0.327166
[Epoch 35] ogbg-molbbbp: 0.911203 test loss: 0.299963
[Epoch 36; Iter    30/   48] train: loss: 0.2226889
[Epoch 36] ogbg-molbbbp: 0.897862 val loss: 0.363989
[Epoch 36] ogbg-molbbbp: 0.923971 test loss: 0.302327
[Epoch 37; Iter    12/   48] train: loss: 0.1774280
[Epoch 37; Iter    42/   48] train: loss: 0.4069950
[Epoch 37] ogbg-molbbbp: 0.918959 val loss: 0.302774
[Epoch 37] ogbg-molbbbp: 0.896585 test loss: 0.315276
[Epoch 38; Iter    24/   48] train: loss: 0.2603585
[Epoch 38] ogbg-molbbbp: 0.907446 val loss: 0.315235
[Epoch 38] ogbg-molbbbp: 0.915034 test loss: 0.276863
[Epoch 39; Iter     6/   48] train: loss: 0.2454549
[Epoch 39; Iter    36/   48] train: loss: 0.3001772
[Epoch 39] ogbg-molbbbp: 0.901208 val loss: 0.326411
[Epoch 39] ogbg-molbbbp: 0.900032 test loss: 0.300408
[Epoch 40; Iter    18/   48] train: loss: 0.1920351
[Epoch 40; Iter    48/   48] train: loss: 0.3838620
[Epoch 40] ogbg-molbbbp: 0.903533 val loss: 0.341192
[Epoch 40] ogbg-molbbbp: 0.883881 test loss: 0.327428
[Epoch 41; Iter    30/   48] train: loss: 0.2637642
[Epoch 41] ogbg-molbbbp: 0.899166 val loss: 0.320579
[Epoch 41] ogbg-molbbbp: 0.921034 test loss: 0.272303
[Epoch 42; Iter    12/   48] train: loss: 0.0684694
[Epoch 42; Iter    42/   48] train: loss: 0.2704484
[Epoch 42] ogbg-molbbbp: 0.908581 val loss: 0.329104
[Epoch 42] ogbg-molbbbp: 0.884456 test loss: 0.387757
[Epoch 43; Iter    24/   48] train: loss: 0.1269010
[Epoch 43] ogbg-molbbbp: 0.866387 val loss: 0.456039
[Epoch 43] ogbg-molbbbp: 0.870731 test loss: 0.459593
[Epoch 44; Iter     6/   48] train: loss: 0.1760927
[Epoch 44; Iter    36/   48] train: loss: 0.3211287
[Epoch 44] ogbg-molbbbp: 0.895367 val loss: 0.326438
[Epoch 44] ogbg-molbbbp: 0.897159 test loss: 0.319151
[Epoch 45; Iter    18/   48] train: loss: 0.1833443
[Epoch 45; Iter    48/   48] train: loss: 0.0731916
[Epoch 45] ogbg-molbbbp: 0.912550 val loss: 0.355043
[Epoch 45] ogbg-molbbbp: 0.906990 test loss: 0.341814
[Epoch 46; Iter    30/   48] train: loss: 0.1977207
[Epoch 46] ogbg-molbbbp: 0.904554 val loss: 0.325081
[Epoch 46] ogbg-molbbbp: 0.879923 test loss: 0.349383
[Epoch 47; Iter    12/   48] train: loss: 0.2354447
[Epoch 47; Iter    42/   48] train: loss: 0.2440149
[Epoch 47] ogbg-molbbbp: 0.891283 val loss: 0.349153
[Epoch 47] ogbg-molbbbp: 0.893584 test loss: 0.322897
[Epoch 48; Iter    24/   48] train: loss: 0.1888783
[Epoch 48] ogbg-molbbbp: 0.916350 val loss: 0.302199
[Epoch 48] ogbg-molbbbp: 0.908522 test loss: 0.311377
[Epoch 49; Iter     6/   48] train: loss: 0.1841283
[Epoch 49; Iter    36/   48] train: loss: 0.1833566
[Epoch 49] ogbg-molbbbp: 0.910622 val loss: 0.321602
[Epoch 49] ogbg-molbbbp: 0.891478 test loss: 0.337879
[Epoch 50; Iter    18/   48] train: loss: 0.0934502
[Epoch 50; Iter    48/   48] train: loss: 0.0846633
[Epoch 50] ogbg-molbbbp: 0.915556 val loss: 0.348172
[Epoch 50] ogbg-molbbbp: 0.932908 test loss: 0.283388
[Epoch 51; Iter    30/   48] train: loss: 0.2429138
[Epoch 51] ogbg-molbbbp: 0.912040 val loss: 0.326739
[Epoch 51] ogbg-molbbbp: 0.907884 test loss: 0.318973
[Epoch 52; Iter    12/   48] train: loss: 0.1565555
[Epoch 52; Iter    42/   48] train: loss: 0.1379193
[Epoch 52] ogbg-molbbbp: 0.915896 val loss: 0.384064
[Epoch 52] ogbg-molbbbp: 0.880881 test loss: 0.492154
[Epoch 53; Iter    24/   48] train: loss: 0.3019812
[Epoch 53] ogbg-molbbbp: 0.894006 val loss: 0.335530
[Epoch 53] ogbg-molbbbp: 0.872391 test loss: 0.366516
[Epoch 54; Iter     6/   48] train: loss: 0.1081949
[Epoch 54; Iter    36/   48] train: loss: 0.1007559
[Epoch 54] ogbg-molbbbp: 0.902796 val loss: 0.433750
[Epoch 54] ogbg-molbbbp: 0.888350 test loss: 0.359888
[Epoch 55; Iter    18/   48] train: loss: 0.2485391
[Epoch 55; Iter    48/   48] train: loss: 0.4723827
[Epoch 55] ogbg-molbbbp: 0.900981 val loss: 0.403890
[Epoch 55] ogbg-molbbbp: 0.892499 test loss: 0.359324
[Epoch 56; Iter    30/   48] train: loss: 0.1824502
[Epoch 56] ogbg-molbbbp: 0.913855 val loss: 0.350549
[Epoch 56] ogbg-molbbbp: 0.880817 test loss: 0.400393
[Epoch 57; Iter    12/   48] train: loss: 0.0973357
[Epoch 57; Iter    42/   48] train: loss: 0.0825459
[Epoch 57] ogbg-molbbbp: 0.897749 val loss: 0.396570
[Epoch 57] ogbg-molbbbp: 0.899777 test loss: 0.413182
[Epoch 58; Iter    24/   48] train: loss: 0.0600528
[Epoch 58] ogbg-molbbbp: 0.905518 val loss: 0.377569
[Epoch 58] ogbg-molbbbp: 0.905458 test loss: 0.329550
[Epoch 59; Iter     6/   48] train: loss: 0.1500017
[Epoch 59; Iter    36/   48] train: loss: 0.1088071
[Epoch 59] ogbg-molbbbp: 0.919980 val loss: 0.317505
[Epoch 59] ogbg-molbbbp: 0.909352 test loss: 0.342712
[Epoch 60; Iter    18/   48] train: loss: 0.2088618
[Epoch 60; Iter    48/   48] train: loss: 0.0724473
[Epoch 60] ogbg-molbbbp: 0.907843 val loss: 0.368314
[Epoch 60] ogbg-molbbbp: 0.884264 test loss: 0.422871
[Epoch 61; Iter    30/   48] train: loss: 0.1377504
[Epoch 61] ogbg-molbbbp: 0.909148 val loss: 0.374751
[Epoch 61] ogbg-molbbbp: 0.904500 test loss: 0.374000
[Epoch 62; Iter    12/   48] train: loss: 0.1492893
[Epoch 62; Iter    42/   48] train: loss: 0.0793394
[Epoch 62] ogbg-molbbbp: 0.887540 val loss: 0.440927
[Epoch 62] ogbg-molbbbp: 0.884264 test loss: 0.470605
[Epoch 63; Iter    24/   48] train: loss: 0.1874513
[Epoch 63] ogbg-molbbbp: 0.903363 val loss: 0.387113
[Epoch 63] ogbg-molbbbp: 0.894095 test loss: 0.387660
[Epoch 64; Iter     6/   48] train: loss: 0.0986297
[Epoch 64; Iter    36/   48] train: loss: 0.1553971
[Epoch 64] ogbg-molbbbp: 0.900981 val loss: 0.431091
[Epoch 64] ogbg-molbbbp: 0.904373 test loss: 0.375724
[Epoch 65; Iter    18/   48] train: loss: 0.0197833
[Epoch 65; Iter    48/   48] train: loss: 0.3576779
[Epoch 65] ogbg-molbbbp: 0.880962 val loss: 0.513617
[Epoch 65] ogbg-molbbbp: 0.857198 test loss: 0.501971
[Epoch 66; Iter    30/   48] train: loss: 0.1248949
[Epoch 66] ogbg-molbbbp: 0.890206 val loss: 0.422092
[Epoch 66] ogbg-molbbbp: 0.868560 test loss: 0.525410
[Epoch 67; Iter    12/   48] train: loss: 0.0542237
[Epoch 67; Iter    42/   48] train: loss: 0.1526662
[Epoch 67] ogbg-molbbbp: 0.906482 val loss: 0.421791
[Epoch 67] ogbg-molbbbp: 0.882860 test loss: 0.453520
[Epoch 68; Iter    24/   48] train: loss: 0.0305498
[Epoch 68] ogbg-molbbbp: 0.908864 val loss: 0.423676
[Epoch 68] ogbg-molbbbp: 0.886754 test loss: 0.466009
[Epoch 69; Iter     6/   48] train: loss: 0.0628070
[Epoch 69; Iter    36/   48] train: loss: 0.1969041
[Epoch 69] ogbg-molbbbp: 0.910282 val loss: 0.440317
[Epoch 69] ogbg-molbbbp: 0.879349 test loss: 0.482663
[Epoch 70; Iter    18/   48] train: loss: 0.0341020
[Epoch 70; Iter    48/   48] train: loss: 0.1833304
[Epoch 70] ogbg-molbbbp: 0.905631 val loss: 0.449853
[Epoch 70] ogbg-molbbbp: 0.896649 test loss: 0.455100
[Epoch 71; Iter    30/   48] train: loss: 0.1232660
[Epoch 71] ogbg-molbbbp: 0.905688 val loss: 0.480803
[Epoch 71] ogbg-molbbbp: 0.882094 test loss: 0.505867
[Epoch 72; Iter    12/   48] train: loss: 0.0102935
[Epoch 72; Iter    42/   48] train: loss: 0.0067889
[Epoch 72] ogbg-molbbbp: 0.903420 val loss: 0.492668
[Epoch 72] ogbg-molbbbp: 0.882860 test loss: 0.518377
[Epoch 73; Iter    24/   48] train: loss: 0.0340023
[Epoch 73] ogbg-molbbbp: 0.908240 val loss: 0.418679
[Epoch 32] ogbg-molbbbp: 0.880509 test loss: 0.359836
[Epoch 33; Iter     8/   41] train: loss: 0.2877034
[Epoch 33; Iter    38/   41] train: loss: 0.3272940
[Epoch 33] ogbg-molbbbp: 0.900324 val loss: 0.309799
[Epoch 33] ogbg-molbbbp: 0.885817 test loss: 0.353969
[Epoch 34; Iter    27/   41] train: loss: 0.4552309
[Epoch 34] ogbg-molbbbp: 0.904455 val loss: 0.312109
[Epoch 34] ogbg-molbbbp: 0.896201 test loss: 0.337919
[Epoch 35; Iter    16/   41] train: loss: 0.3494584
[Epoch 35] ogbg-molbbbp: 0.903226 val loss: 0.303621
[Epoch 35] ogbg-molbbbp: 0.897069 test loss: 0.342458
[Epoch 36; Iter     5/   41] train: loss: 0.5432149
[Epoch 36; Iter    35/   41] train: loss: 0.1570550
[Epoch 36] ogbg-molbbbp: 0.917699 val loss: 0.290128
[Epoch 36] ogbg-molbbbp: 0.907018 test loss: 0.339937
[Epoch 37; Iter    24/   41] train: loss: 0.1489979
[Epoch 37] ogbg-molbbbp: 0.897013 val loss: 0.329886
[Epoch 37] ogbg-molbbbp: 0.885584 test loss: 0.357386
[Epoch 38; Iter    13/   41] train: loss: 0.3931492
[Epoch 38] ogbg-molbbbp: 0.899061 val loss: 0.345018
[Epoch 38] ogbg-molbbbp: 0.872563 test loss: 0.405826
[Epoch 39; Iter     2/   41] train: loss: 0.1960068
[Epoch 39; Iter    32/   41] train: loss: 0.2363069
[Epoch 39] ogbg-molbbbp: 0.890357 val loss: 0.334040
[Epoch 39] ogbg-molbbbp: 0.903312 test loss: 0.338455
[Epoch 40; Iter    21/   41] train: loss: 0.2941040
[Epoch 40] ogbg-molbbbp: 0.921488 val loss: 0.289270
[Epoch 40] ogbg-molbbbp: 0.919304 test loss: 0.308039
[Epoch 41; Iter    10/   41] train: loss: 0.2255867
[Epoch 41; Iter    40/   41] train: loss: 0.2933060
[Epoch 41] ogbg-molbbbp: 0.914491 val loss: 0.295687
[Epoch 41] ogbg-molbbbp: 0.890625 test loss: 0.348564
[Epoch 42; Iter    29/   41] train: loss: 0.3460381
[Epoch 42] ogbg-molbbbp: 0.913671 val loss: 0.355939
[Epoch 42] ogbg-molbbbp: 0.906417 test loss: 0.355162
[Epoch 43; Iter    18/   41] train: loss: 0.2463728
[Epoch 43] ogbg-molbbbp: 0.914729 val loss: 0.477130
[Epoch 43] ogbg-molbbbp: 0.885650 test loss: 0.548702
[Epoch 44; Iter     7/   41] train: loss: 0.1604315
[Epoch 44; Iter    37/   41] train: loss: 0.2041167
[Epoch 44] ogbg-molbbbp: 0.924765 val loss: 0.294008
[Epoch 44] ogbg-molbbbp: 0.892461 test loss: 0.345385
[Epoch 45; Iter    26/   41] train: loss: 0.1822824
[Epoch 45] ogbg-molbbbp: 0.920464 val loss: 0.292251
[Epoch 45] ogbg-molbbbp: 0.910156 test loss: 0.325790
[Epoch 46; Iter    15/   41] train: loss: 0.1861021
[Epoch 46] ogbg-molbbbp: 0.892405 val loss: 0.357469
[Epoch 46] ogbg-molbbbp: 0.859876 test loss: 0.409279
[Epoch 47; Iter     4/   41] train: loss: 0.2675940
[Epoch 47; Iter    34/   41] train: loss: 0.2445924
[Epoch 47] ogbg-molbbbp: 0.923639 val loss: 0.309284
[Epoch 47] ogbg-molbbbp: 0.904046 test loss: 0.335331
[Epoch 48; Iter    23/   41] train: loss: 0.3860777
[Epoch 48] ogbg-molbbbp: 0.927974 val loss: 0.286809
[Epoch 48] ogbg-molbbbp: 0.911759 test loss: 0.323637
[Epoch 49; Iter    12/   41] train: loss: 0.1465552
[Epoch 49] ogbg-molbbbp: 0.929817 val loss: 0.281089
[Epoch 49] ogbg-molbbbp: 0.908888 test loss: 0.324574
[Epoch 50; Iter     1/   41] train: loss: 0.1583794
[Epoch 50; Iter    31/   41] train: loss: 0.0879825
[Epoch 50] ogbg-molbbbp: 0.926062 val loss: 0.308442
[Epoch 50] ogbg-molbbbp: 0.912961 test loss: 0.357749
[Epoch 51; Iter    20/   41] train: loss: 0.1586725
[Epoch 51] ogbg-molbbbp: 0.915071 val loss: 0.354464
[Epoch 51] ogbg-molbbbp: 0.877404 test loss: 0.423574
[Epoch 52; Iter     9/   41] train: loss: 0.1445169
[Epoch 52; Iter    39/   41] train: loss: 0.1648622
[Epoch 52] ogbg-molbbbp: 0.914354 val loss: 0.364358
[Epoch 52] ogbg-molbbbp: 0.914229 test loss: 0.352133
[Epoch 53; Iter    28/   41] train: loss: 0.1351798
[Epoch 53] ogbg-molbbbp: 0.882779 val loss: 0.440069
[Epoch 53] ogbg-molbbbp: 0.881878 test loss: 0.429808
[Epoch 54; Iter    17/   41] train: loss: 0.3205149
[Epoch 54] ogbg-molbbbp: 0.916778 val loss: 0.325167
[Epoch 54] ogbg-molbbbp: 0.904380 test loss: 0.353957
[Epoch 55; Iter     6/   41] train: loss: 0.2003044
[Epoch 55; Iter    36/   41] train: loss: 0.1919638
[Epoch 55] ogbg-molbbbp: 0.899607 val loss: 0.642484
[Epoch 55] ogbg-molbbbp: 0.905849 test loss: 0.554218
[Epoch 56; Iter    25/   41] train: loss: 0.1509251
[Epoch 56] ogbg-molbbbp: 0.929886 val loss: 0.290500
[Epoch 56] ogbg-molbbbp: 0.903713 test loss: 0.360557
[Epoch 57; Iter    14/   41] train: loss: 0.1488040
[Epoch 57] ogbg-molbbbp: 0.930193 val loss: 0.332700
[Epoch 57] ogbg-molbbbp: 0.908019 test loss: 0.379957
[Epoch 58; Iter     3/   41] train: loss: 0.1103155
[Epoch 58; Iter    33/   41] train: loss: 0.2599299
[Epoch 58] ogbg-molbbbp: 0.916880 val loss: 0.348850
[Epoch 58] ogbg-molbbbp: 0.902811 test loss: 0.417841
[Epoch 59; Iter    22/   41] train: loss: 0.1223822
[Epoch 59] ogbg-molbbbp: 0.930705 val loss: 0.339224
[Epoch 59] ogbg-molbbbp: 0.882412 test loss: 0.466772
[Epoch 60; Iter    11/   41] train: loss: 0.1809786
[Epoch 60; Iter    41/   41] train: loss: 0.1396012
[Epoch 60] ogbg-molbbbp: 0.917870 val loss: 0.411269
[Epoch 60] ogbg-molbbbp: 0.892561 test loss: 0.480205
[Epoch 61; Iter    30/   41] train: loss: 0.2196769
[Epoch 61] ogbg-molbbbp: 0.904830 val loss: 0.375945
[Epoch 61] ogbg-molbbbp: 0.899873 test loss: 0.377739
[Epoch 62; Iter    19/   41] train: loss: 0.2467876
[Epoch 62] ogbg-molbbbp: 0.912954 val loss: 0.365579
[Epoch 62] ogbg-molbbbp: 0.897336 test loss: 0.391931
[Epoch 63; Iter     8/   41] train: loss: 0.0553924
[Epoch 63; Iter    38/   41] train: loss: 0.3428460
[Epoch 63] ogbg-molbbbp: 0.923571 val loss: 0.347352
[Epoch 63] ogbg-molbbbp: 0.912794 test loss: 0.384589
[Epoch 64; Iter    27/   41] train: loss: 0.2974564
[Epoch 64] ogbg-molbbbp: 0.929066 val loss: 0.313631
[Epoch 64] ogbg-molbbbp: 0.907118 test loss: 0.398542
[Epoch 65; Iter    16/   41] train: loss: 0.2891826
[Epoch 65] ogbg-molbbbp: 0.932924 val loss: 0.339901
[Epoch 65] ogbg-molbbbp: 0.892328 test loss: 0.435545
[Epoch 66; Iter     5/   41] train: loss: 0.1586013
[Epoch 66; Iter    35/   41] train: loss: 0.4837179
[Epoch 66] ogbg-molbbbp: 0.911487 val loss: 0.354970
[Epoch 66] ogbg-molbbbp: 0.890725 test loss: 0.403795
[Epoch 67; Iter    24/   41] train: loss: 0.1582732
[Epoch 67] ogbg-molbbbp: 0.928281 val loss: 0.401948
[Epoch 67] ogbg-molbbbp: 0.898037 test loss: 0.411153
[Epoch 68; Iter    13/   41] train: loss: 0.0820240
[Epoch 68] ogbg-molbbbp: 0.929988 val loss: 0.409718
[Epoch 68] ogbg-molbbbp: 0.889824 test loss: 0.524588
[Epoch 69; Iter     2/   41] train: loss: 0.1806634
[Epoch 69; Iter    32/   41] train: loss: 0.1229606
[Epoch 69] ogbg-molbbbp: 0.919884 val loss: 0.425672
[Epoch 69] ogbg-molbbbp: 0.885350 test loss: 0.522386
[Epoch 70; Iter    21/   41] train: loss: 0.0291067
[Epoch 70] ogbg-molbbbp: 0.924868 val loss: 0.348118
[Epoch 70] ogbg-molbbbp: 0.902143 test loss: 0.417368
[Epoch 71; Iter    10/   41] train: loss: 0.0352119
[Epoch 71; Iter    40/   41] train: loss: 0.1646723
[Epoch 71] ogbg-molbbbp: 0.926336 val loss: 0.371336
[Epoch 71] ogbg-molbbbp: 0.899940 test loss: 0.477416
[Epoch 72; Iter    29/   41] train: loss: 0.0208281
[Epoch 72] ogbg-molbbbp: 0.930876 val loss: 0.401388
[Epoch 72] ogbg-molbbbp: 0.909322 test loss: 0.470457
[Epoch 73; Iter    18/   41] train: loss: 0.0959670
[Epoch 73] ogbg-molbbbp: 0.929578 val loss: 0.375418
[Epoch 73] ogbg-molbbbp: 0.903112 test loss: 0.486005
[Epoch 74; Iter     7/   41] train: loss: 0.0618083
[Epoch 74; Iter    37/   41] train: loss: 0.1231137
[Epoch 74] ogbg-molbbbp: 0.932548 val loss: 0.363152
[Epoch 74] ogbg-molbbbp: 0.903279 test loss: 0.429093
[Epoch 75; Iter    26/   41] train: loss: 0.1377424
[Epoch 75] ogbg-molbbbp: 0.920601 val loss: 0.356544
[Epoch 75] ogbg-molbbbp: 0.894398 test loss: 0.435649
[Epoch 76; Iter    15/   41] train: loss: 0.0527480
[Epoch 76] ogbg-molbbbp: 0.926404 val loss: 0.369920
[Epoch 76] ogbg-molbbbp: 0.899339 test loss: 0.451702
[Epoch 77; Iter     4/   41] train: loss: 0.2241587
[Epoch 77; Iter    34/   41] train: loss: 0.1088323
[Epoch 77] ogbg-molbbbp: 0.902680 val loss: 0.432245
[Epoch 77] ogbg-molbbbp: 0.883280 test loss: 0.505604
[Epoch 78; Iter    23/   41] train: loss: 0.0252377
[Epoch 32] ogbg-molbbbp: 0.874766 test loss: 0.374577
[Epoch 33; Iter     8/   41] train: loss: 0.4246224
[Epoch 33; Iter    38/   41] train: loss: 0.1464920
[Epoch 33] ogbg-molbbbp: 0.901280 val loss: 0.312346
[Epoch 33] ogbg-molbbbp: 0.886084 test loss: 0.356480
[Epoch 34; Iter    27/   41] train: loss: 0.2469816
[Epoch 34] ogbg-molbbbp: 0.904318 val loss: 0.343437
[Epoch 34] ogbg-molbbbp: 0.879173 test loss: 0.388518
[Epoch 35; Iter    16/   41] train: loss: 0.3551484
[Epoch 35] ogbg-molbbbp: 0.889879 val loss: 0.439100
[Epoch 35] ogbg-molbbbp: 0.875668 test loss: 0.476562
[Epoch 36; Iter     5/   41] train: loss: 0.3236234
[Epoch 36; Iter    35/   41] train: loss: 0.3607927
[Epoch 36] ogbg-molbbbp: 0.899573 val loss: 0.349420
[Epoch 36] ogbg-molbbbp: 0.915298 test loss: 0.345582
[Epoch 37; Iter    24/   41] train: loss: 0.4532335
[Epoch 37] ogbg-molbbbp: 0.912477 val loss: 0.340489
[Epoch 37] ogbg-molbbbp: 0.884415 test loss: 0.373087
[Epoch 38; Iter    13/   41] train: loss: 0.1477155
[Epoch 38] ogbg-molbbbp: 0.904523 val loss: 0.327711
[Epoch 38] ogbg-molbbbp: 0.889824 test loss: 0.368196
[Epoch 39; Iter     2/   41] train: loss: 0.1762883
[Epoch 39; Iter    32/   41] train: loss: 0.2037822
[Epoch 39] ogbg-molbbbp: 0.901860 val loss: 0.334235
[Epoch 39] ogbg-molbbbp: 0.897202 test loss: 0.355341
[Epoch 40; Iter    21/   41] train: loss: 0.1542286
[Epoch 40] ogbg-molbbbp: 0.926575 val loss: 0.287560
[Epoch 40] ogbg-molbbbp: 0.910256 test loss: 0.354308
[Epoch 41; Iter    10/   41] train: loss: 0.2518853
[Epoch 41; Iter    40/   41] train: loss: 0.2315049
[Epoch 41] ogbg-molbbbp: 0.924287 val loss: 0.334397
[Epoch 41] ogbg-molbbbp: 0.899406 test loss: 0.375191
[Epoch 42; Iter    29/   41] train: loss: 0.3386189
[Epoch 42] ogbg-molbbbp: 0.920567 val loss: 0.310984
[Epoch 42] ogbg-molbbbp: 0.881510 test loss: 0.407528
[Epoch 43; Iter    18/   41] train: loss: 0.2156165
[Epoch 43] ogbg-molbbbp: 0.920771 val loss: 0.631324
[Epoch 43] ogbg-molbbbp: 0.911358 test loss: 0.796279
[Epoch 44; Iter     7/   41] train: loss: 0.2904880
[Epoch 44; Iter    37/   41] train: loss: 0.1561960
[Epoch 44] ogbg-molbbbp: 0.912989 val loss: 0.318675
[Epoch 44] ogbg-molbbbp: 0.906283 test loss: 0.335749
[Epoch 45; Iter    26/   41] train: loss: 0.3943526
[Epoch 45] ogbg-molbbbp: 0.906810 val loss: 0.313072
[Epoch 45] ogbg-molbbbp: 0.884649 test loss: 0.347154
[Epoch 46; Iter    15/   41] train: loss: 0.1631995
[Epoch 46] ogbg-molbbbp: 0.919986 val loss: 0.304612
[Epoch 46] ogbg-molbbbp: 0.878973 test loss: 0.378232
[Epoch 47; Iter     4/   41] train: loss: 0.4386778
[Epoch 47; Iter    34/   41] train: loss: 0.3048905
[Epoch 47] ogbg-molbbbp: 0.926540 val loss: 0.333836
[Epoch 47] ogbg-molbbbp: 0.911124 test loss: 0.354338
[Epoch 48; Iter    23/   41] train: loss: 0.1757737
[Epoch 48] ogbg-molbbbp: 0.921147 val loss: 0.333087
[Epoch 48] ogbg-molbbbp: 0.914062 test loss: 0.323359
[Epoch 49; Iter    12/   41] train: loss: 0.3223692
[Epoch 49] ogbg-molbbbp: 0.911760 val loss: 0.314343
[Epoch 49] ogbg-molbbbp: 0.895032 test loss: 0.378763
[Epoch 50; Iter     1/   41] train: loss: 0.1015671
[Epoch 50; Iter    31/   41] train: loss: 0.3169881
[Epoch 50] ogbg-molbbbp: 0.903055 val loss: 0.343248
[Epoch 50] ogbg-molbbbp: 0.885684 test loss: 0.380260
[Epoch 51; Iter    20/   41] train: loss: 0.1571139
[Epoch 51] ogbg-molbbbp: 0.923502 val loss: 0.292613
[Epoch 51] ogbg-molbbbp: 0.901976 test loss: 0.340836
[Epoch 52; Iter     9/   41] train: loss: 0.1209727
[Epoch 52; Iter    39/   41] train: loss: 0.2061794
[Epoch 52] ogbg-molbbbp: 0.913705 val loss: 0.330982
[Epoch 52] ogbg-molbbbp: 0.886318 test loss: 0.386716
[Epoch 53; Iter    28/   41] train: loss: 0.2141955
[Epoch 53] ogbg-molbbbp: 0.914388 val loss: 0.339264
[Epoch 53] ogbg-molbbbp: 0.898738 test loss: 0.351556
[Epoch 54; Iter    17/   41] train: loss: 0.1879204
[Epoch 54] ogbg-molbbbp: 0.911555 val loss: 0.472454
[Epoch 54] ogbg-molbbbp: 0.871595 test loss: 0.540652
[Epoch 55; Iter     6/   41] train: loss: 0.0966392
[Epoch 55; Iter    36/   41] train: loss: 0.2148816
[Epoch 55] ogbg-molbbbp: 0.915378 val loss: 0.308588
[Epoch 55] ogbg-molbbbp: 0.903379 test loss: 0.345824
[Epoch 56; Iter    25/   41] train: loss: 0.0394766
[Epoch 56] ogbg-molbbbp: 0.924492 val loss: 0.326442
[Epoch 56] ogbg-molbbbp: 0.912994 test loss: 0.361037
[Epoch 57; Iter    14/   41] train: loss: 0.0725250
[Epoch 57] ogbg-molbbbp: 0.929135 val loss: 0.279686
[Epoch 57] ogbg-molbbbp: 0.918470 test loss: 0.310878
[Epoch 58; Iter     3/   41] train: loss: 0.0785620
[Epoch 58; Iter    33/   41] train: loss: 0.1761376
[Epoch 58] ogbg-molbbbp: 0.925755 val loss: 0.304295
[Epoch 58] ogbg-molbbbp: 0.902077 test loss: 0.394614
[Epoch 59; Iter    22/   41] train: loss: 0.2224656
[Epoch 59] ogbg-molbbbp: 0.928213 val loss: 0.307468
[Epoch 59] ogbg-molbbbp: 0.905816 test loss: 0.369058
[Epoch 60; Iter    11/   41] train: loss: 0.1070550
[Epoch 60; Iter    41/   41] train: loss: 0.1892971
[Epoch 60] ogbg-molbbbp: 0.919611 val loss: 0.323345
[Epoch 60] ogbg-molbbbp: 0.902411 test loss: 0.379099
[Epoch 61; Iter    30/   41] train: loss: 0.1945148
[Epoch 61] ogbg-molbbbp: 0.928623 val loss: 0.310930
[Epoch 61] ogbg-molbbbp: 0.897169 test loss: 0.386699
[Epoch 62; Iter    19/   41] train: loss: 0.0524755
[Epoch 62] ogbg-molbbbp: 0.925721 val loss: 0.320184
[Epoch 62] ogbg-molbbbp: 0.883881 test loss: 0.410177
[Epoch 63; Iter     8/   41] train: loss: 0.1267310
[Epoch 63; Iter    38/   41] train: loss: 0.0969898
[Epoch 63] ogbg-molbbbp: 0.931524 val loss: 0.323262
[Epoch 63] ogbg-molbbbp: 0.905849 test loss: 0.398875
[Epoch 64; Iter    27/   41] train: loss: 0.2212771
[Epoch 64] ogbg-molbbbp: 0.916573 val loss: 0.390695
[Epoch 64] ogbg-molbbbp: 0.894264 test loss: 0.430468
[Epoch 65; Iter    16/   41] train: loss: 0.3770622
[Epoch 65] ogbg-molbbbp: 0.919304 val loss: 0.384494
[Epoch 65] ogbg-molbbbp: 0.894498 test loss: 0.478917
[Epoch 66; Iter     5/   41] train: loss: 0.3347217
[Epoch 66; Iter    35/   41] train: loss: 0.1038283
[Epoch 66] ogbg-molbbbp: 0.933367 val loss: 0.296830
[Epoch 66] ogbg-molbbbp: 0.898838 test loss: 0.357323
[Epoch 67; Iter    24/   41] train: loss: 0.0722101
[Epoch 67] ogbg-molbbbp: 0.923810 val loss: 0.323850
[Epoch 67] ogbg-molbbbp: 0.894631 test loss: 0.414942
[Epoch 68; Iter    13/   41] train: loss: 0.0268632
[Epoch 68] ogbg-molbbbp: 0.928111 val loss: 0.365766
[Epoch 68] ogbg-molbbbp: 0.889423 test loss: 0.443239
[Epoch 69; Iter     2/   41] train: loss: 0.1726226
[Epoch 69; Iter    32/   41] train: loss: 0.0622796
[Epoch 69] ogbg-molbbbp: 0.933402 val loss: 0.345553
[Epoch 69] ogbg-molbbbp: 0.902411 test loss: 0.422110
[Epoch 70; Iter    21/   41] train: loss: 0.0277681
[Epoch 70] ogbg-molbbbp: 0.936849 val loss: 0.331730
[Epoch 70] ogbg-molbbbp: 0.903045 test loss: 0.420191
[Epoch 71; Iter    10/   41] train: loss: 0.0672058
[Epoch 71; Iter    40/   41] train: loss: 0.0682284
[Epoch 71] ogbg-molbbbp: 0.928896 val loss: 0.316689
[Epoch 71] ogbg-molbbbp: 0.901209 test loss: 0.380409
[Epoch 72; Iter    29/   41] train: loss: 0.0918287
[Epoch 72] ogbg-molbbbp: 0.926984 val loss: 0.309672
[Epoch 72] ogbg-molbbbp: 0.892962 test loss: 0.405319
[Epoch 73; Iter    18/   41] train: loss: 0.0195877
[Epoch 73] ogbg-molbbbp: 0.930944 val loss: 0.302746
[Epoch 73] ogbg-molbbbp: 0.869491 test loss: 0.482389
[Epoch 74; Iter     7/   41] train: loss: 0.0503490
[Epoch 74; Iter    37/   41] train: loss: 0.0971383
[Epoch 74] ogbg-molbbbp: 0.925175 val loss: 0.402271
[Epoch 74] ogbg-molbbbp: 0.880909 test loss: 0.497003
[Epoch 75; Iter    26/   41] train: loss: 0.1136728
[Epoch 75] ogbg-molbbbp: 0.937805 val loss: 0.301980
[Epoch 75] ogbg-molbbbp: 0.901108 test loss: 0.396249
[Epoch 76; Iter    15/   41] train: loss: 0.0807122
[Epoch 76] ogbg-molbbbp: 0.940434 val loss: 0.290162
[Epoch 76] ogbg-molbbbp: 0.899907 test loss: 0.405153
[Epoch 77; Iter     4/   41] train: loss: 0.0220089
[Epoch 77; Iter    34/   41] train: loss: 0.0234052
[Epoch 77] ogbg-molbbbp: 0.935347 val loss: 0.355215
[Epoch 77] ogbg-molbbbp: 0.899072 test loss: 0.477549
[Epoch 78; Iter    23/   41] train: loss: 0.0189938
[Epoch 28] ogbg-molbbbp: 0.898148 test loss: 0.320565
[Epoch 29; Iter    20/   55] train: loss: 0.4186983
[Epoch 29; Iter    50/   55] train: loss: 0.2094177
[Epoch 29] ogbg-molbbbp: 0.920000 val loss: 0.335337
[Epoch 29] ogbg-molbbbp: 0.890947 test loss: 0.359863
[Epoch 30; Iter    25/   55] train: loss: 0.4710363
[Epoch 30; Iter    55/   55] train: loss: 0.2168986
[Epoch 30] ogbg-molbbbp: 0.914321 val loss: 0.336343
[Epoch 30] ogbg-molbbbp: 0.900500 test loss: 0.314215
[Epoch 31; Iter    30/   55] train: loss: 0.3917103
[Epoch 31] ogbg-molbbbp: 0.944568 val loss: 0.290851
[Epoch 31] ogbg-molbbbp: 0.913580 test loss: 0.294168
[Epoch 32; Iter     5/   55] train: loss: 0.3654905
[Epoch 32; Iter    35/   55] train: loss: 0.1977454
[Epoch 32] ogbg-molbbbp: 0.930247 val loss: 0.333804
[Epoch 32] ogbg-molbbbp: 0.900794 test loss: 0.312938
[Epoch 33; Iter    10/   55] train: loss: 0.3779814
[Epoch 33; Iter    40/   55] train: loss: 0.2983843
[Epoch 33] ogbg-molbbbp: 0.903704 val loss: 0.355509
[Epoch 33] ogbg-molbbbp: 0.889624 test loss: 0.360740
[Epoch 34; Iter    15/   55] train: loss: 0.1795628
[Epoch 34; Iter    45/   55] train: loss: 0.2424952
[Epoch 34] ogbg-molbbbp: 0.936667 val loss: 0.292998
[Epoch 34] ogbg-molbbbp: 0.917842 test loss: 0.307493
[Epoch 35; Iter    20/   55] train: loss: 0.3191665
[Epoch 35; Iter    50/   55] train: loss: 0.2658687
[Epoch 35] ogbg-molbbbp: 0.921481 val loss: 0.321111
[Epoch 35] ogbg-molbbbp: 0.906966 test loss: 0.344620
[Epoch 36; Iter    25/   55] train: loss: 0.1482786
[Epoch 36; Iter    55/   55] train: loss: 0.0684036
[Epoch 36] ogbg-molbbbp: 0.902593 val loss: 0.346627
[Epoch 36] ogbg-molbbbp: 0.892122 test loss: 0.356192
[Epoch 37; Iter    30/   55] train: loss: 0.2336421
[Epoch 37] ogbg-molbbbp: 0.903210 val loss: 0.375275
[Epoch 37] ogbg-molbbbp: 0.906820 test loss: 0.317848
[Epoch 38; Iter     5/   55] train: loss: 0.1785379
[Epoch 38; Iter    35/   55] train: loss: 0.1366924
[Epoch 38] ogbg-molbbbp: 0.942840 val loss: 0.278116
[Epoch 38] ogbg-molbbbp: 0.912845 test loss: 0.286701
[Epoch 39; Iter    10/   55] train: loss: 0.1787301
[Epoch 39; Iter    40/   55] train: loss: 0.2189012
[Epoch 39] ogbg-molbbbp: 0.927284 val loss: 0.324568
[Epoch 39] ogbg-molbbbp: 0.908436 test loss: 0.321957
[Epoch 40; Iter    15/   55] train: loss: 0.3086615
[Epoch 40; Iter    45/   55] train: loss: 0.0750341
[Epoch 40] ogbg-molbbbp: 0.946049 val loss: 0.289608
[Epoch 40] ogbg-molbbbp: 0.922693 test loss: 0.280865
[Epoch 41; Iter    20/   55] train: loss: 0.3700773
[Epoch 41; Iter    50/   55] train: loss: 0.0846894
[Epoch 41] ogbg-molbbbp: 0.924074 val loss: 0.344071
[Epoch 41] ogbg-molbbbp: 0.903145 test loss: 0.349591
[Epoch 42; Iter    25/   55] train: loss: 0.1591441
[Epoch 42; Iter    55/   55] train: loss: 0.0429005
[Epoch 42] ogbg-molbbbp: 0.913210 val loss: 0.366780
[Epoch 42] ogbg-molbbbp: 0.898442 test loss: 0.305567
[Epoch 43; Iter    30/   55] train: loss: 0.2946859
[Epoch 43] ogbg-molbbbp: 0.932593 val loss: 0.341091
[Epoch 43] ogbg-molbbbp: 0.912404 test loss: 0.312552
[Epoch 44; Iter     5/   55] train: loss: 0.2063251
[Epoch 44; Iter    35/   55] train: loss: 0.3606851
[Epoch 44] ogbg-molbbbp: 0.930864 val loss: 0.325545
[Epoch 44] ogbg-molbbbp: 0.900647 test loss: 0.345105
[Epoch 45; Iter    10/   55] train: loss: 0.1842316
[Epoch 45; Iter    40/   55] train: loss: 0.1020493
[Epoch 45] ogbg-molbbbp: 0.931852 val loss: 0.331892
[Epoch 45] ogbg-molbbbp: 0.903439 test loss: 0.332998
[Epoch 46; Iter    15/   55] train: loss: 0.2088912
[Epoch 46; Iter    45/   55] train: loss: 0.4477349
[Epoch 46] ogbg-molbbbp: 0.934568 val loss: 0.328960
[Epoch 46] ogbg-molbbbp: 0.911964 test loss: 0.356832
[Epoch 47; Iter    20/   55] train: loss: 0.1635781
[Epoch 47; Iter    50/   55] train: loss: 0.2671833
[Epoch 47] ogbg-molbbbp: 0.945062 val loss: 0.293235
[Epoch 47] ogbg-molbbbp: 0.888301 test loss: 0.345356
[Epoch 48; Iter    25/   55] train: loss: 0.1695076
[Epoch 48; Iter    55/   55] train: loss: 0.0693231
[Epoch 48] ogbg-molbbbp: 0.931605 val loss: 0.331574
[Epoch 48] ogbg-molbbbp: 0.906232 test loss: 0.372519
[Epoch 49; Iter    30/   55] train: loss: 0.1323256
[Epoch 49] ogbg-molbbbp: 0.911728 val loss: 0.408629
[Epoch 49] ogbg-molbbbp: 0.890506 test loss: 0.412691
[Epoch 50; Iter     5/   55] train: loss: 0.1739463
[Epoch 50; Iter    35/   55] train: loss: 0.1519825
[Epoch 50] ogbg-molbbbp: 0.915556 val loss: 0.378408
[Epoch 50] ogbg-molbbbp: 0.895944 test loss: 0.388766
[Epoch 51; Iter    10/   55] train: loss: 0.3385374
[Epoch 51; Iter    40/   55] train: loss: 0.0624175
[Epoch 51] ogbg-molbbbp: 0.930370 val loss: 0.331291
[Epoch 51] ogbg-molbbbp: 0.898295 test loss: 0.350707
[Epoch 52; Iter    15/   55] train: loss: 0.0461996
[Epoch 52; Iter    45/   55] train: loss: 0.3985986
[Epoch 52] ogbg-molbbbp: 0.915802 val loss: 0.368983
[Epoch 52] ogbg-molbbbp: 0.899324 test loss: 0.369854
[Epoch 53; Iter    20/   55] train: loss: 0.0662436
[Epoch 53; Iter    50/   55] train: loss: 0.1232546
[Epoch 53] ogbg-molbbbp: 0.931481 val loss: 0.356373
[Epoch 53] ogbg-molbbbp: 0.908730 test loss: 0.403475
[Epoch 54; Iter    25/   55] train: loss: 0.0455749
[Epoch 54; Iter    55/   55] train: loss: 0.0349978
[Epoch 54] ogbg-molbbbp: 0.913457 val loss: 0.443812
[Epoch 54] ogbg-molbbbp: 0.882128 test loss: 0.396827
[Epoch 55; Iter    30/   55] train: loss: 0.3245733
[Epoch 55] ogbg-molbbbp: 0.936049 val loss: 0.345984
[Epoch 55] ogbg-molbbbp: 0.917255 test loss: 0.413755
[Epoch 56; Iter     5/   55] train: loss: 0.0333868
[Epoch 56; Iter    35/   55] train: loss: 0.3059624
[Epoch 56] ogbg-molbbbp: 0.930247 val loss: 0.391209
[Epoch 56] ogbg-molbbbp: 0.913139 test loss: 0.430526
[Epoch 57; Iter    10/   55] train: loss: 0.3196709
[Epoch 57; Iter    40/   55] train: loss: 0.2431915
[Epoch 57] ogbg-molbbbp: 0.931605 val loss: 0.335669
[Epoch 57] ogbg-molbbbp: 0.888742 test loss: 0.423033
[Epoch 58; Iter    15/   55] train: loss: 0.2179628
[Epoch 58; Iter    45/   55] train: loss: 0.1991776
[Epoch 58] ogbg-molbbbp: 0.926420 val loss: 0.423149
[Epoch 58] ogbg-molbbbp: 0.903439 test loss: 0.445351
[Epoch 59; Iter    20/   55] train: loss: 0.2134991
[Epoch 59; Iter    50/   55] train: loss: 0.4111398
[Epoch 59] ogbg-molbbbp: 0.906173 val loss: 0.411056
[Epoch 59] ogbg-molbbbp: 0.871693 test loss: 0.487431
[Epoch 60; Iter    25/   55] train: loss: 0.1341857
[Epoch 60; Iter    55/   55] train: loss: 0.0085774
[Epoch 60] ogbg-molbbbp: 0.917284 val loss: 0.392873
[Epoch 60] ogbg-molbbbp: 0.891681 test loss: 0.462852
[Epoch 61; Iter    30/   55] train: loss: 0.3472338
[Epoch 61] ogbg-molbbbp: 0.925679 val loss: 0.401989
[Epoch 61] ogbg-molbbbp: 0.884186 test loss: 0.396934
[Epoch 62; Iter     5/   55] train: loss: 0.1737671
[Epoch 62; Iter    35/   55] train: loss: 0.0602397
[Epoch 62] ogbg-molbbbp: 0.897160 val loss: 0.500967
[Epoch 62] ogbg-molbbbp: 0.865961 test loss: 0.586151
[Epoch 63; Iter    10/   55] train: loss: 0.1065048
[Epoch 63; Iter    40/   55] train: loss: 0.1623204
[Epoch 63] ogbg-molbbbp: 0.928642 val loss: 0.387453
[Epoch 63] ogbg-molbbbp: 0.889918 test loss: 0.472238
[Epoch 64; Iter    15/   55] train: loss: 0.1064598
[Epoch 64; Iter    45/   55] train: loss: 0.0163229
[Epoch 64] ogbg-molbbbp: 0.925556 val loss: 0.386392
[Epoch 64] ogbg-molbbbp: 0.900059 test loss: 0.425920
[Epoch 65; Iter    20/   55] train: loss: 0.1551586
[Epoch 65; Iter    50/   55] train: loss: 0.0232402
[Epoch 65] ogbg-molbbbp: 0.886296 val loss: 0.631331
[Epoch 65] ogbg-molbbbp: 0.899030 test loss: 0.420164
[Epoch 66; Iter    25/   55] train: loss: 0.0220923
[Epoch 66; Iter    55/   55] train: loss: 0.1922364
[Epoch 66] ogbg-molbbbp: 0.914198 val loss: 0.442435
[Epoch 66] ogbg-molbbbp: 0.897560 test loss: 0.491181
[Epoch 67; Iter    30/   55] train: loss: 0.1208204
[Epoch 67] ogbg-molbbbp: 0.929753 val loss: 0.380039
[Epoch 67] ogbg-molbbbp: 0.911670 test loss: 0.390933
[Epoch 68; Iter     5/   55] train: loss: 0.1442766
[Epoch 68; Iter    35/   55] train: loss: 0.1304000
[Epoch 68] ogbg-molbbbp: 0.922593 val loss: 0.433004
[Epoch 68] ogbg-molbbbp: 0.893298 test loss: 0.608504
[Epoch 69; Iter    10/   55] train: loss: 0.1130285
[Epoch 32] ogbg-molbbbp: 0.881811 test loss: 0.365336
[Epoch 33; Iter     8/   41] train: loss: 0.3872941
[Epoch 33; Iter    38/   41] train: loss: 0.1750148
[Epoch 33] ogbg-molbbbp: 0.898925 val loss: 0.313937
[Epoch 33] ogbg-molbbbp: 0.885216 test loss: 0.355566
[Epoch 34; Iter    27/   41] train: loss: 0.2726528
[Epoch 34] ogbg-molbbbp: 0.907288 val loss: 0.312049
[Epoch 34] ogbg-molbbbp: 0.897503 test loss: 0.339775
[Epoch 35; Iter    16/   41] train: loss: 0.6226566
[Epoch 35] ogbg-molbbbp: 0.902509 val loss: 0.320763
[Epoch 35] ogbg-molbbbp: 0.848892 test loss: 0.435472
[Epoch 36; Iter     5/   41] train: loss: 0.4393880
[Epoch 36; Iter    35/   41] train: loss: 0.3301271
[Epoch 36] ogbg-molbbbp: 0.892576 val loss: 0.340369
[Epoch 36] ogbg-molbbbp: 0.889523 test loss: 0.361273
[Epoch 37; Iter    24/   41] train: loss: 0.1383888
[Epoch 37] ogbg-molbbbp: 0.892576 val loss: 0.342642
[Epoch 37] ogbg-molbbbp: 0.861311 test loss: 0.411792
[Epoch 38; Iter    13/   41] train: loss: 0.2149802
[Epoch 38] ogbg-molbbbp: 0.908414 val loss: 0.312617
[Epoch 38] ogbg-molbbbp: 0.890725 test loss: 0.338316
[Epoch 39; Iter     2/   41] train: loss: 0.1452198
[Epoch 39; Iter    32/   41] train: loss: 0.4503600
[Epoch 39] ogbg-molbbbp: 0.901929 val loss: 0.358592
[Epoch 39] ogbg-molbbbp: 0.894398 test loss: 0.367899
[Epoch 40; Iter    21/   41] train: loss: 0.1950594
[Epoch 40] ogbg-molbbbp: 0.924834 val loss: 0.332509
[Epoch 40] ogbg-molbbbp: 0.897069 test loss: 0.366124
[Epoch 41; Iter    10/   41] train: loss: 0.1947659
[Epoch 41; Iter    40/   41] train: loss: 0.2800805
[Epoch 41] ogbg-molbbbp: 0.933709 val loss: 0.268114
[Epoch 41] ogbg-molbbbp: 0.911859 test loss: 0.324520
[Epoch 42; Iter    29/   41] train: loss: 0.2130530
[Epoch 42] ogbg-molbbbp: 0.900017 val loss: 0.381754
[Epoch 42] ogbg-molbbbp: 0.906784 test loss: 0.378740
[Epoch 43; Iter    18/   41] train: loss: 0.3261267
[Epoch 43] ogbg-molbbbp: 0.916709 val loss: 0.324391
[Epoch 43] ogbg-molbbbp: 0.879708 test loss: 0.403329
[Epoch 44; Iter     7/   41] train: loss: 0.1783548
[Epoch 44; Iter    37/   41] train: loss: 0.1495959
[Epoch 44] ogbg-molbbbp: 0.893736 val loss: 0.331970
[Epoch 44] ogbg-molbbbp: 0.904247 test loss: 0.322610
[Epoch 45; Iter    26/   41] train: loss: 0.1119945
[Epoch 45] ogbg-molbbbp: 0.908414 val loss: 0.312309
[Epoch 45] ogbg-molbbbp: 0.895499 test loss: 0.388940
[Epoch 46; Iter    15/   41] train: loss: 0.1232906
[Epoch 46] ogbg-molbbbp: 0.927974 val loss: 0.322884
[Epoch 46] ogbg-molbbbp: 0.894865 test loss: 0.392856
[Epoch 47; Iter     4/   41] train: loss: 0.3300838
[Epoch 47; Iter    34/   41] train: loss: 0.3253457
[Epoch 47] ogbg-molbbbp: 0.918314 val loss: 0.288771
[Epoch 47] ogbg-molbbbp: 0.897903 test loss: 0.343396
[Epoch 48; Iter    23/   41] train: loss: 0.1421678
[Epoch 48] ogbg-molbbbp: 0.906093 val loss: 0.316474
[Epoch 48] ogbg-molbbbp: 0.898638 test loss: 0.342907
[Epoch 49; Iter    12/   41] train: loss: 0.1165295
[Epoch 49] ogbg-molbbbp: 0.924219 val loss: 0.279135
[Epoch 49] ogbg-molbbbp: 0.871561 test loss: 0.404341
[Epoch 50; Iter     1/   41] train: loss: 0.1584336
[Epoch 50; Iter    31/   41] train: loss: 0.1939594
[Epoch 50] ogbg-molbbbp: 0.919440 val loss: 0.309782
[Epoch 50] ogbg-molbbbp: 0.901209 test loss: 0.353099
[Epoch 51; Iter    20/   41] train: loss: 0.2195164
[Epoch 51] ogbg-molbbbp: 0.928520 val loss: 0.290371
[Epoch 51] ogbg-molbbbp: 0.892394 test loss: 0.409590
[Epoch 52; Iter     9/   41] train: loss: 0.1142664
[Epoch 52; Iter    39/   41] train: loss: 0.3652956
[Epoch 52] ogbg-molbbbp: 0.934494 val loss: 0.281938
[Epoch 52] ogbg-molbbbp: 0.907552 test loss: 0.355717
[Epoch 53; Iter    28/   41] train: loss: 0.1312909
[Epoch 53] ogbg-molbbbp: 0.912818 val loss: 0.339109
[Epoch 53] ogbg-molbbbp: 0.910190 test loss: 0.347997
[Epoch 54; Iter    17/   41] train: loss: 0.2219463
[Epoch 54] ogbg-molbbbp: 0.916300 val loss: 0.294118
[Epoch 54] ogbg-molbbbp: 0.888755 test loss: 0.352555
[Epoch 55; Iter     6/   41] train: loss: 0.0943443
[Epoch 55; Iter    36/   41] train: loss: 0.0777011
[Epoch 55] ogbg-molbbbp: 0.909609 val loss: 0.385306
[Epoch 55] ogbg-molbbbp: 0.888956 test loss: 0.413418
[Epoch 56; Iter    25/   41] train: loss: 0.0642518
[Epoch 56] ogbg-molbbbp: 0.915685 val loss: 0.364583
[Epoch 56] ogbg-molbbbp: 0.907686 test loss: 0.369425
[Epoch 57; Iter    14/   41] train: loss: 0.1130028
[Epoch 57] ogbg-molbbbp: 0.903431 val loss: 0.397234
[Epoch 57] ogbg-molbbbp: 0.882812 test loss: 0.436162
[Epoch 58; Iter     3/   41] train: loss: 0.2658338
[Epoch 58; Iter    33/   41] train: loss: 0.1346950
[Epoch 58] ogbg-molbbbp: 0.934323 val loss: 0.290540
[Epoch 58] ogbg-molbbbp: 0.896534 test loss: 0.401087
[Epoch 59; Iter    22/   41] train: loss: 0.0657811
[Epoch 59] ogbg-molbbbp: 0.921079 val loss: 0.340169
[Epoch 59] ogbg-molbbbp: 0.897736 test loss: 0.403775
[Epoch 60; Iter    11/   41] train: loss: 0.1773936
[Epoch 60; Iter    41/   41] train: loss: 0.0760891
[Epoch 60] ogbg-molbbbp: 0.924970 val loss: 0.332723
[Epoch 60] ogbg-molbbbp: 0.892561 test loss: 0.433619
[Epoch 61; Iter    30/   41] train: loss: 0.0455708
[Epoch 61] ogbg-molbbbp: 0.912715 val loss: 0.365017
[Epoch 61] ogbg-molbbbp: 0.875868 test loss: 0.462503
[Epoch 62; Iter    19/   41] train: loss: 0.1300350
[Epoch 62] ogbg-molbbbp: 0.919816 val loss: 0.323888
[Epoch 62] ogbg-molbbbp: 0.899205 test loss: 0.382384
[Epoch 63; Iter     8/   41] train: loss: 0.1194439
[Epoch 63; Iter    38/   41] train: loss: 0.0470982
[Epoch 63] ogbg-molbbbp: 0.905410 val loss: 0.355317
[Epoch 63] ogbg-molbbbp: 0.874933 test loss: 0.435824
[Epoch 64; Iter    27/   41] train: loss: 0.1161457
[Epoch 64] ogbg-molbbbp: 0.925311 val loss: 0.545567
[Epoch 64] ogbg-molbbbp: 0.896201 test loss: 0.641230
[Epoch 65; Iter    16/   41] train: loss: 0.0495798
[Epoch 65] ogbg-molbbbp: 0.929476 val loss: 0.347045
[Epoch 65] ogbg-molbbbp: 0.894464 test loss: 0.465910
[Epoch 66; Iter     5/   41] train: loss: 0.2058266
[Epoch 66; Iter    35/   41] train: loss: 0.0750456
[Epoch 66] ogbg-molbbbp: 0.921318 val loss: 0.414870
[Epoch 66] ogbg-molbbbp: 0.891693 test loss: 0.547803
[Epoch 67; Iter    24/   41] train: loss: 0.0535825
[Epoch 67] ogbg-molbbbp: 0.921659 val loss: 0.360267
[Epoch 67] ogbg-molbbbp: 0.882979 test loss: 0.513645
[Epoch 68; Iter    13/   41] train: loss: 0.0760461
[Epoch 68] ogbg-molbbbp: 0.918211 val loss: 0.331132
[Epoch 68] ogbg-molbbbp: 0.860443 test loss: 0.535284
[Epoch 69; Iter     2/   41] train: loss: 0.0279707
[Epoch 69; Iter    32/   41] train: loss: 0.1119535
[Epoch 69] ogbg-molbbbp: 0.929305 val loss: 0.344805
[Epoch 69] ogbg-molbbbp: 0.874032 test loss: 0.525748
[Epoch 70; Iter    21/   41] train: loss: 0.0213853
[Epoch 70] ogbg-molbbbp: 0.927940 val loss: 0.366740
[Epoch 70] ogbg-molbbbp: 0.904647 test loss: 0.478099
[Epoch 71; Iter    10/   41] train: loss: 0.1239361
[Epoch 71; Iter    40/   41] train: loss: 0.0270538
[Epoch 71] ogbg-molbbbp: 0.925824 val loss: 0.363182
[Epoch 71] ogbg-molbbbp: 0.878038 test loss: 0.542376
[Epoch 72; Iter    29/   41] train: loss: 0.0096484
[Epoch 72] ogbg-molbbbp: 0.931831 val loss: 0.360536
[Epoch 72] ogbg-molbbbp: 0.888922 test loss: 0.514370
[Epoch 73; Iter    18/   41] train: loss: 0.0151213
[Epoch 73] ogbg-molbbbp: 0.927052 val loss: 0.345608
[Epoch 73] ogbg-molbbbp: 0.881978 test loss: 0.505326
[Epoch 74; Iter     7/   41] train: loss: 0.0207253
[Epoch 74; Iter    37/   41] train: loss: 0.0058422
[Epoch 74] ogbg-molbbbp: 0.922239 val loss: 0.381672
[Epoch 74] ogbg-molbbbp: 0.886418 test loss: 0.503293
[Epoch 75; Iter    26/   41] train: loss: 0.0068345
[Epoch 75] ogbg-molbbbp: 0.934392 val loss: 0.345654
[Epoch 75] ogbg-molbbbp: 0.882579 test loss: 0.542538
[Epoch 76; Iter    15/   41] train: loss: 0.0401155
[Epoch 76] ogbg-molbbbp: 0.929203 val loss: 0.378892
[Epoch 76] ogbg-molbbbp: 0.879240 test loss: 0.607178
[Epoch 77; Iter     4/   41] train: loss: 0.0129237
[Epoch 77; Iter    34/   41] train: loss: 0.0596973
[Epoch 77] ogbg-molbbbp: 0.923980 val loss: 0.380012
[Epoch 77] ogbg-molbbbp: 0.883514 test loss: 0.556701
[Epoch 78; Iter    23/   41] train: loss: 0.1025986
[Epoch 69; Iter    40/   55] train: loss: 0.0969647
[Epoch 69] ogbg-molbbbp: 0.919753 val loss: 0.410425
[Epoch 69] ogbg-molbbbp: 0.889183 test loss: 0.480288
[Epoch 70; Iter    15/   55] train: loss: 0.2135716
[Epoch 70; Iter    45/   55] train: loss: 0.0259898
[Epoch 70] ogbg-molbbbp: 0.916543 val loss: 0.406655
[Epoch 70] ogbg-molbbbp: 0.885215 test loss: 0.465970
[Epoch 71; Iter    20/   55] train: loss: 0.0693747
[Epoch 71; Iter    50/   55] train: loss: 0.0529867
[Epoch 71] ogbg-molbbbp: 0.921358 val loss: 0.474815
[Epoch 71] ogbg-molbbbp: 0.892563 test loss: 0.488749
[Epoch 72; Iter    25/   55] train: loss: 0.0560268
[Epoch 72; Iter    55/   55] train: loss: 0.2462823
[Epoch 72] ogbg-molbbbp: 0.889877 val loss: 0.502982
[Epoch 72] ogbg-molbbbp: 0.911523 test loss: 0.368907
[Epoch 73; Iter    30/   55] train: loss: 0.1575306
[Epoch 73] ogbg-molbbbp: 0.926049 val loss: 0.450808
[Epoch 73] ogbg-molbbbp: 0.899471 test loss: 0.457382
[Epoch 74; Iter     5/   55] train: loss: 0.0874565
[Epoch 74; Iter    35/   55] train: loss: 0.0408758
[Epoch 74] ogbg-molbbbp: 0.913704 val loss: 0.456455
[Epoch 74] ogbg-molbbbp: 0.922105 test loss: 0.397577
[Epoch 75; Iter    10/   55] train: loss: 0.3244900
[Epoch 75; Iter    40/   55] train: loss: 0.1000992
[Epoch 75] ogbg-molbbbp: 0.917284 val loss: 0.465769
[Epoch 75] ogbg-molbbbp: 0.875808 test loss: 0.579197
[Epoch 76; Iter    15/   55] train: loss: 0.0580914
[Epoch 76; Iter    45/   55] train: loss: 0.0839044
[Epoch 76] ogbg-molbbbp: 0.919259 val loss: 0.463847
[Epoch 76] ogbg-molbbbp: 0.884774 test loss: 0.548503
[Epoch 77; Iter    20/   55] train: loss: 0.0067608
[Epoch 77; Iter    50/   55] train: loss: 0.2130779
[Epoch 77] ogbg-molbbbp: 0.905679 val loss: 0.479446
[Epoch 77] ogbg-molbbbp: 0.889477 test loss: 0.522579
[Epoch 78; Iter    25/   55] train: loss: 0.0180025
[Epoch 78; Iter    55/   55] train: loss: 0.0086735
[Epoch 78] ogbg-molbbbp: 0.914568 val loss: 0.434081
[Epoch 78] ogbg-molbbbp: 0.904762 test loss: 0.443225
[Epoch 79; Iter    30/   55] train: loss: 0.0293843
[Epoch 79] ogbg-molbbbp: 0.919136 val loss: 0.408071
[Epoch 79] ogbg-molbbbp: 0.879630 test loss: 0.495149
[Epoch 80; Iter     5/   55] train: loss: 0.0227165
[Epoch 80; Iter    35/   55] train: loss: 0.0784040
[Epoch 80] ogbg-molbbbp: 0.914321 val loss: 0.469189
[Epoch 80] ogbg-molbbbp: 0.880364 test loss: 0.643079
[Epoch 81; Iter    10/   55] train: loss: 0.0283191
[Epoch 81; Iter    40/   55] train: loss: 0.1254057
[Epoch 81] ogbg-molbbbp: 0.925432 val loss: 0.414035
[Epoch 81] ogbg-molbbbp: 0.892122 test loss: 0.524250
[Epoch 82; Iter    15/   55] train: loss: 0.0631105
[Epoch 82; Iter    45/   55] train: loss: 0.1784230
[Epoch 82] ogbg-molbbbp: 0.931975 val loss: 0.416032
[Epoch 82] ogbg-molbbbp: 0.900647 test loss: 0.474393
[Epoch 83; Iter    20/   55] train: loss: 0.1117170
[Epoch 83; Iter    50/   55] train: loss: 0.1205853
[Epoch 83] ogbg-molbbbp: 0.930494 val loss: 0.419431
[Epoch 83] ogbg-molbbbp: 0.906820 test loss: 0.513193
[Epoch 84; Iter    25/   55] train: loss: 0.0439846
[Epoch 84; Iter    55/   55] train: loss: 0.0070961
[Epoch 84] ogbg-molbbbp: 0.935556 val loss: 0.418108
[Epoch 84] ogbg-molbbbp: 0.879336 test loss: 0.585724
[Epoch 85; Iter    30/   55] train: loss: 0.0146665
[Epoch 85] ogbg-molbbbp: 0.908272 val loss: 0.548743
[Epoch 85] ogbg-molbbbp: 0.891534 test loss: 0.653748
[Epoch 86; Iter     5/   55] train: loss: 0.1668826
[Epoch 86; Iter    35/   55] train: loss: 0.0138790
[Epoch 86] ogbg-molbbbp: 0.923457 val loss: 0.461765
[Epoch 86] ogbg-molbbbp: 0.882569 test loss: 0.574689
[Epoch 87; Iter    10/   55] train: loss: 0.0796472
[Epoch 87; Iter    40/   55] train: loss: 0.0328443
[Epoch 87] ogbg-molbbbp: 0.923951 val loss: 0.488026
[Epoch 87] ogbg-molbbbp: 0.900794 test loss: 0.546212
[Epoch 88; Iter    15/   55] train: loss: 0.0446797
[Epoch 88; Iter    45/   55] train: loss: 0.0722085
[Epoch 88] ogbg-molbbbp: 0.928765 val loss: 0.475108
[Epoch 88] ogbg-molbbbp: 0.897707 test loss: 0.556325
[Epoch 89; Iter    20/   55] train: loss: 0.0860235
[Epoch 89; Iter    50/   55] train: loss: 0.0197261
[Epoch 89] ogbg-molbbbp: 0.928519 val loss: 0.419447
[Epoch 89] ogbg-molbbbp: 0.891828 test loss: 0.547581
[Epoch 90; Iter    25/   55] train: loss: 0.3116069
[Epoch 90; Iter    55/   55] train: loss: 0.0939269
[Epoch 90] ogbg-molbbbp: 0.925309 val loss: 0.456050
[Epoch 90] ogbg-molbbbp: 0.904027 test loss: 0.449685
[Epoch 91; Iter    30/   55] train: loss: 0.2192826
[Epoch 91] ogbg-molbbbp: 0.905556 val loss: 0.531729
[Epoch 91] ogbg-molbbbp: 0.863904 test loss: 0.647087
[Epoch 92; Iter     5/   55] train: loss: 0.0711314
[Epoch 92; Iter    35/   55] train: loss: 0.0308965
[Epoch 92] ogbg-molbbbp: 0.924568 val loss: 0.454259
[Epoch 92] ogbg-molbbbp: 0.894621 test loss: 0.548041
[Epoch 93; Iter    10/   55] train: loss: 0.0333469
[Epoch 93; Iter    40/   55] train: loss: 0.0249574
[Epoch 93] ogbg-molbbbp: 0.922469 val loss: 0.413686
[Epoch 93] ogbg-molbbbp: 0.901088 test loss: 0.488644
[Epoch 94; Iter    15/   55] train: loss: 0.0180025
[Epoch 94; Iter    45/   55] train: loss: 0.0423916
[Epoch 94] ogbg-molbbbp: 0.920494 val loss: 0.473552
[Epoch 94] ogbg-molbbbp: 0.908289 test loss: 0.531674
[Epoch 95; Iter    20/   55] train: loss: 0.0212270
[Epoch 95; Iter    50/   55] train: loss: 0.0345035
[Epoch 95] ogbg-molbbbp: 0.915062 val loss: 0.491666
[Epoch 95] ogbg-molbbbp: 0.897560 test loss: 0.539875
[Epoch 96; Iter    25/   55] train: loss: 0.1427917
[Epoch 96; Iter    55/   55] train: loss: 0.0101597
[Epoch 96] ogbg-molbbbp: 0.908272 val loss: 0.513839
[Epoch 96] ogbg-molbbbp: 0.899177 test loss: 0.580014
[Epoch 97; Iter    30/   55] train: loss: 0.0083754
[Epoch 97] ogbg-molbbbp: 0.916420 val loss: 0.504578
[Epoch 97] ogbg-molbbbp: 0.902851 test loss: 0.586956
[Epoch 98; Iter     5/   55] train: loss: 0.0052518
[Epoch 98; Iter    35/   55] train: loss: 0.0165360
[Epoch 98] ogbg-molbbbp: 0.902222 val loss: 0.554144
[Epoch 98] ogbg-molbbbp: 0.900206 test loss: 0.647314
[Epoch 99; Iter    10/   55] train: loss: 0.0130778
[Epoch 99; Iter    40/   55] train: loss: 0.0036899
[Epoch 99] ogbg-molbbbp: 0.917037 val loss: 0.518922
[Epoch 99] ogbg-molbbbp: 0.901382 test loss: 0.568183
[Epoch 100; Iter    15/   55] train: loss: 0.0125879
[Epoch 100; Iter    45/   55] train: loss: 0.0035941
[Epoch 100] ogbg-molbbbp: 0.918642 val loss: 0.522707
[Epoch 100] ogbg-molbbbp: 0.901088 test loss: 0.573539
[Epoch 101; Iter    20/   55] train: loss: 0.0078025
[Epoch 101; Iter    50/   55] train: loss: 0.0041496
[Epoch 101] ogbg-molbbbp: 0.915309 val loss: 0.513708
[Epoch 101] ogbg-molbbbp: 0.900206 test loss: 0.604222
[Epoch 102; Iter    25/   55] train: loss: 0.0025325
[Epoch 102; Iter    55/   55] train: loss: 0.0054908
[Epoch 102] ogbg-molbbbp: 0.923580 val loss: 0.512478
[Epoch 102] ogbg-molbbbp: 0.904615 test loss: 0.605913
[Epoch 103; Iter    30/   55] train: loss: 0.0883545
[Epoch 103] ogbg-molbbbp: 0.916667 val loss: 0.539355
[Epoch 103] ogbg-molbbbp: 0.909759 test loss: 0.607262
[Epoch 104; Iter     5/   55] train: loss: 0.0174870
[Epoch 104; Iter    35/   55] train: loss: 0.0129202
[Epoch 104] ogbg-molbbbp: 0.908889 val loss: 0.544921
[Epoch 104] ogbg-molbbbp: 0.906966 test loss: 0.556388
[Epoch 105; Iter    10/   55] train: loss: 0.0017607
[Epoch 105; Iter    40/   55] train: loss: 0.0026018
[Epoch 105] ogbg-molbbbp: 0.919383 val loss: 0.534117
[Epoch 105] ogbg-molbbbp: 0.904615 test loss: 0.668797
[Epoch 106; Iter    15/   55] train: loss: 0.0145346
[Epoch 106; Iter    45/   55] train: loss: 0.0699420
[Epoch 106] ogbg-molbbbp: 0.919136 val loss: 0.523830
[Epoch 106] ogbg-molbbbp: 0.910053 test loss: 0.543184
[Epoch 107; Iter    20/   55] train: loss: 0.0040796
[Epoch 107; Iter    50/   55] train: loss: 0.0017624
[Epoch 107] ogbg-molbbbp: 0.917160 val loss: 0.521301
[Epoch 107] ogbg-molbbbp: 0.902263 test loss: 0.580771
[Epoch 108; Iter    25/   55] train: loss: 0.0132071
[Epoch 108; Iter    55/   55] train: loss: 0.0029471
[Epoch 108] ogbg-molbbbp: 0.921852 val loss: 0.552321
[Epoch 108] ogbg-molbbbp: 0.906673 test loss: 0.596302
[Epoch 109; Iter    30/   55] train: loss: 0.0034749
[Epoch 69; Iter    40/   55] train: loss: 0.1160198
[Epoch 69] ogbg-molbbbp: 0.919012 val loss: 0.400223
[Epoch 69] ogbg-molbbbp: 0.885802 test loss: 0.444344
[Epoch 70; Iter    15/   55] train: loss: 0.0654997
[Epoch 70; Iter    45/   55] train: loss: 0.1859046
[Epoch 70] ogbg-molbbbp: 0.935185 val loss: 0.445822
[Epoch 70] ogbg-molbbbp: 0.873604 test loss: 0.563635
[Epoch 71; Iter    20/   55] train: loss: 0.1891001
[Epoch 71; Iter    50/   55] train: loss: 0.0637348
[Epoch 71] ogbg-molbbbp: 0.927407 val loss: 0.369556
[Epoch 71] ogbg-molbbbp: 0.879336 test loss: 0.440609
[Epoch 72; Iter    25/   55] train: loss: 0.0355608
[Epoch 72; Iter    55/   55] train: loss: 0.2167877
[Epoch 72] ogbg-molbbbp: 0.931975 val loss: 0.395289
[Epoch 72] ogbg-molbbbp: 0.861111 test loss: 0.474382
[Epoch 73; Iter    30/   55] train: loss: 0.0855464
[Epoch 73] ogbg-molbbbp: 0.904444 val loss: 0.498495
[Epoch 73] ogbg-molbbbp: 0.865226 test loss: 0.577467
[Epoch 74; Iter     5/   55] train: loss: 0.0439525
[Epoch 74; Iter    35/   55] train: loss: 0.0902582
[Epoch 74] ogbg-molbbbp: 0.910370 val loss: 0.508151
[Epoch 74] ogbg-molbbbp: 0.892563 test loss: 0.474630
[Epoch 75; Iter    10/   55] train: loss: 0.2677906
[Epoch 75; Iter    40/   55] train: loss: 0.1301821
[Epoch 75] ogbg-molbbbp: 0.902963 val loss: 0.479982
[Epoch 75] ogbg-molbbbp: 0.882716 test loss: 0.482972
[Epoch 76; Iter    15/   55] train: loss: 0.1702184
[Epoch 76; Iter    45/   55] train: loss: 0.1402545
[Epoch 76] ogbg-molbbbp: 0.919259 val loss: 0.462802
[Epoch 76] ogbg-molbbbp: 0.884333 test loss: 0.535755
[Epoch 77; Iter    20/   55] train: loss: 0.0157167
[Epoch 77; Iter    50/   55] train: loss: 0.0437210
[Epoch 77] ogbg-molbbbp: 0.924938 val loss: 0.430625
[Epoch 77] ogbg-molbbbp: 0.893151 test loss: 0.528089
[Epoch 78; Iter    25/   55] train: loss: 0.0236883
[Epoch 78; Iter    55/   55] train: loss: 0.2066716
[Epoch 78] ogbg-molbbbp: 0.931728 val loss: 0.416810
[Epoch 78] ogbg-molbbbp: 0.896091 test loss: 0.490070
[Epoch 79; Iter    30/   55] train: loss: 0.0366287
[Epoch 79] ogbg-molbbbp: 0.911975 val loss: 0.440539
[Epoch 79] ogbg-molbbbp: 0.884333 test loss: 0.500138
[Epoch 80; Iter     5/   55] train: loss: 0.0061211
[Epoch 80; Iter    35/   55] train: loss: 0.0262842
[Epoch 80] ogbg-molbbbp: 0.920864 val loss: 0.433125
[Epoch 80] ogbg-molbbbp: 0.889036 test loss: 0.496803
[Epoch 81; Iter    10/   55] train: loss: 0.0069208
[Epoch 81; Iter    40/   55] train: loss: 0.0860188
[Epoch 81] ogbg-molbbbp: 0.907778 val loss: 0.530629
[Epoch 81] ogbg-molbbbp: 0.888301 test loss: 0.559816
[Epoch 82; Iter    15/   55] train: loss: 0.0075603
[Epoch 82; Iter    45/   55] train: loss: 0.0283407
[Epoch 82] ogbg-molbbbp: 0.922469 val loss: 0.474791
[Epoch 82] ogbg-molbbbp: 0.887566 test loss: 0.523203
[Epoch 83; Iter    20/   55] train: loss: 0.0189155
[Epoch 83; Iter    50/   55] train: loss: 0.0033558
[Epoch 83] ogbg-molbbbp: 0.912099 val loss: 0.532191
[Epoch 83] ogbg-molbbbp: 0.882128 test loss: 0.599427
[Epoch 84; Iter    25/   55] train: loss: 0.0154612
[Epoch 84; Iter    55/   55] train: loss: 0.0119876
[Epoch 84] ogbg-molbbbp: 0.924321 val loss: 0.482876
[Epoch 84] ogbg-molbbbp: 0.880071 test loss: 0.562091
[Epoch 85; Iter    30/   55] train: loss: 0.0714972
[Epoch 85] ogbg-molbbbp: 0.927901 val loss: 0.457961
[Epoch 85] ogbg-molbbbp: 0.887419 test loss: 0.525283
[Epoch 86; Iter     5/   55] train: loss: 0.0279563
[Epoch 86; Iter    35/   55] train: loss: 0.0916869
[Epoch 86] ogbg-molbbbp: 0.923086 val loss: 0.463123
[Epoch 86] ogbg-molbbbp: 0.886390 test loss: 0.570476
[Epoch 87; Iter    10/   55] train: loss: 0.0144961
[Epoch 87; Iter    40/   55] train: loss: 0.1673897
[Epoch 87] ogbg-molbbbp: 0.917901 val loss: 0.456761
[Epoch 87] ogbg-molbbbp: 0.883157 test loss: 0.514273
[Epoch 88; Iter    15/   55] train: loss: 0.0066319
[Epoch 88; Iter    45/   55] train: loss: 0.0031796
[Epoch 88] ogbg-molbbbp: 0.920370 val loss: 0.468949
[Epoch 88] ogbg-molbbbp: 0.895209 test loss: 0.555264
[Epoch 89; Iter    20/   55] train: loss: 0.0032612
[Epoch 89; Iter    50/   55] train: loss: 0.0220033
[Epoch 89] ogbg-molbbbp: 0.915062 val loss: 0.513489
[Epoch 89] ogbg-molbbbp: 0.892710 test loss: 0.558918
[Epoch 90; Iter    25/   55] train: loss: 0.0021871
[Epoch 90; Iter    55/   55] train: loss: 0.0086578
[Epoch 90] ogbg-molbbbp: 0.924321 val loss: 0.474983
[Epoch 90] ogbg-molbbbp: 0.880218 test loss: 0.563712
[Epoch 91; Iter    30/   55] train: loss: 0.0084494
[Epoch 91] ogbg-molbbbp: 0.919259 val loss: 0.477419
[Epoch 91] ogbg-molbbbp: 0.877866 test loss: 0.568679
[Epoch 92; Iter     5/   55] train: loss: 0.0197019
[Epoch 92; Iter    35/   55] train: loss: 0.0952623
[Epoch 92] ogbg-molbbbp: 0.917407 val loss: 0.525179
[Epoch 92] ogbg-molbbbp: 0.883451 test loss: 0.615001
[Epoch 93; Iter    10/   55] train: loss: 0.0794025
[Epoch 93; Iter    40/   55] train: loss: 0.0328476
[Epoch 93] ogbg-molbbbp: 0.902222 val loss: 0.576509
[Epoch 93] ogbg-molbbbp: 0.889918 test loss: 0.548113
[Epoch 94; Iter    15/   55] train: loss: 0.2073332
[Epoch 94; Iter    45/   55] train: loss: 0.0233850
[Epoch 94] ogbg-molbbbp: 0.918025 val loss: 0.477480
[Epoch 94] ogbg-molbbbp: 0.896678 test loss: 0.505597
[Epoch 95; Iter    20/   55] train: loss: 0.0191328
[Epoch 95; Iter    50/   55] train: loss: 0.0058847
[Epoch 95] ogbg-molbbbp: 0.915556 val loss: 0.497004
[Epoch 95] ogbg-molbbbp: 0.888007 test loss: 0.571846
[Epoch 96; Iter    25/   55] train: loss: 0.0156272
[Epoch 96; Iter    55/   55] train: loss: 0.1205479
[Epoch 96] ogbg-molbbbp: 0.910988 val loss: 0.529906
[Epoch 96] ogbg-molbbbp: 0.879336 test loss: 0.546343
[Epoch 97; Iter    30/   55] train: loss: 0.0094575
[Epoch 97] ogbg-molbbbp: 0.918642 val loss: 0.539293
[Epoch 97] ogbg-molbbbp: 0.893298 test loss: 0.587682
[Epoch 98; Iter     5/   55] train: loss: 0.0062623
[Epoch 98; Iter    35/   55] train: loss: 0.0112012
[Epoch 98] ogbg-molbbbp: 0.922469 val loss: 0.486617
[Epoch 98] ogbg-molbbbp: 0.895797 test loss: 0.543440
[Epoch 99; Iter    10/   55] train: loss: 0.0073622
[Epoch 99; Iter    40/   55] train: loss: 0.0258444
[Epoch 99] ogbg-molbbbp: 0.924198 val loss: 0.467400
[Epoch 99] ogbg-molbbbp: 0.881687 test loss: 0.579718
[Epoch 100; Iter    15/   55] train: loss: 0.0036349
[Epoch 100; Iter    45/   55] train: loss: 0.0162275
[Epoch 100] ogbg-molbbbp: 0.921605 val loss: 0.514996
[Epoch 100] ogbg-molbbbp: 0.878895 test loss: 0.627919
[Epoch 101; Iter    20/   55] train: loss: 0.0099646
[Epoch 101; Iter    50/   55] train: loss: 0.0070482
[Epoch 101] ogbg-molbbbp: 0.905556 val loss: 0.506637
[Epoch 101] ogbg-molbbbp: 0.878454 test loss: 0.513260
[Epoch 102; Iter    25/   55] train: loss: 0.0156155
[Epoch 102; Iter    55/   55] train: loss: 0.0310689
[Epoch 102] ogbg-molbbbp: 0.913704 val loss: 0.549957
[Epoch 102] ogbg-molbbbp: 0.887125 test loss: 0.626265
[Epoch 103; Iter    30/   55] train: loss: 0.0020876
[Epoch 103] ogbg-molbbbp: 0.922469 val loss: 0.475200
[Epoch 103] ogbg-molbbbp: 0.898001 test loss: 0.530076
[Epoch 104; Iter     5/   55] train: loss: 0.0074153
[Epoch 104; Iter    35/   55] train: loss: 0.0029110
[Epoch 104] ogbg-molbbbp: 0.916296 val loss: 0.513269
[Epoch 104] ogbg-molbbbp: 0.893445 test loss: 0.559785
[Epoch 105; Iter    10/   55] train: loss: 0.0326458
[Epoch 105; Iter    40/   55] train: loss: 0.0025358
[Epoch 105] ogbg-molbbbp: 0.920988 val loss: 0.520325
[Epoch 105] ogbg-molbbbp: 0.898001 test loss: 0.590823
[Epoch 106; Iter    15/   55] train: loss: 0.0062808
[Epoch 106; Iter    45/   55] train: loss: 0.0787750
[Epoch 106] ogbg-molbbbp: 0.919259 val loss: 0.507312
[Epoch 106] ogbg-molbbbp: 0.892857 test loss: 0.596295
[Epoch 107; Iter    20/   55] train: loss: 0.0051384
[Epoch 107; Iter    50/   55] train: loss: 0.0218704
[Epoch 107] ogbg-molbbbp: 0.920000 val loss: 0.555403
[Epoch 107] ogbg-molbbbp: 0.888595 test loss: 0.661877
[Epoch 108; Iter    25/   55] train: loss: 0.0283187
[Epoch 108; Iter    55/   55] train: loss: 0.0159018
[Epoch 108] ogbg-molbbbp: 0.915309 val loss: 0.566102
[Epoch 108] ogbg-molbbbp: 0.884774 test loss: 0.652248
[Epoch 109; Iter    30/   55] train: loss: 0.0024642
[Epoch 73] ogbg-molbbbp: 0.888031 test loss: 0.461196
[Epoch 74; Iter     6/   48] train: loss: 0.0183824
[Epoch 74; Iter    36/   48] train: loss: 0.0824094
[Epoch 74] ogbg-molbbbp: 0.910962 val loss: 0.474542
[Epoch 74] ogbg-molbbbp: 0.890329 test loss: 0.487416
[Epoch 75; Iter    18/   48] train: loss: 0.0173482
[Epoch 75; Iter    48/   48] train: loss: 0.0155385
[Epoch 75] ogbg-molbbbp: 0.906142 val loss: 0.493174
[Epoch 75] ogbg-molbbbp: 0.890520 test loss: 0.491907
[Epoch 76; Iter    30/   48] train: loss: 0.0065224
[Epoch 76] ogbg-molbbbp: 0.907787 val loss: 0.419988
[Epoch 76] ogbg-molbbbp: 0.886499 test loss: 0.450546
[Epoch 77; Iter    12/   48] train: loss: 0.0048774
[Epoch 77; Iter    42/   48] train: loss: 0.0927455
[Epoch 77] ogbg-molbbbp: 0.916690 val loss: 0.474206
[Epoch 77] ogbg-molbbbp: 0.903607 test loss: 0.510360
[Epoch 78; Iter    24/   48] train: loss: 0.0113822
[Epoch 78] ogbg-molbbbp: 0.907957 val loss: 0.451678
[Epoch 78] ogbg-molbbbp: 0.887456 test loss: 0.524052
[Epoch 79; Iter     6/   48] train: loss: 0.0523042
[Epoch 79; Iter    36/   48] train: loss: 0.0183067
[Epoch 79] ogbg-molbbbp: 0.903476 val loss: 0.546838
[Epoch 79] ogbg-molbbbp: 0.885860 test loss: 0.581991
[Epoch 80; Iter    18/   48] train: loss: 0.0027510
[Epoch 80; Iter    48/   48] train: loss: 0.0489320
[Epoch 80] ogbg-molbbbp: 0.917654 val loss: 0.454347
[Epoch 80] ogbg-molbbbp: 0.890903 test loss: 0.549752
[Epoch 81; Iter    30/   48] train: loss: 0.0660661
[Epoch 81] ogbg-molbbbp: 0.902002 val loss: 0.515501
[Epoch 81] ogbg-molbbbp: 0.874625 test loss: 0.665103
[Epoch 82; Iter    12/   48] train: loss: 0.0037326
[Epoch 82; Iter    42/   48] train: loss: 0.1917690
[Epoch 82] ogbg-molbbbp: 0.911643 val loss: 0.462452
[Epoch 82] ogbg-molbbbp: 0.903096 test loss: 0.521101
[Epoch 83; Iter    24/   48] train: loss: 0.0183317
[Epoch 83] ogbg-molbbbp: 0.917938 val loss: 0.478947
[Epoch 83] ogbg-molbbbp: 0.890839 test loss: 0.555292
[Epoch 84; Iter     6/   48] train: loss: 0.2055355
[Epoch 84; Iter    36/   48] train: loss: 0.0066757
[Epoch 84] ogbg-molbbbp: 0.900301 val loss: 0.494663
[Epoch 84] ogbg-molbbbp: 0.870156 test loss: 0.652277
[Epoch 85; Iter    18/   48] train: loss: 0.0295256
[Epoch 85; Iter    48/   48] train: loss: 0.0344245
[Epoch 85] ogbg-molbbbp: 0.898032 val loss: 0.585193
[Epoch 85] ogbg-molbbbp: 0.881583 test loss: 0.586494
[Epoch 86; Iter    30/   48] train: loss: 0.0740357
[Epoch 86] ogbg-molbbbp: 0.911756 val loss: 0.518470
[Epoch 86] ogbg-molbbbp: 0.883626 test loss: 0.561026
[Epoch 87; Iter    12/   48] train: loss: 0.0071440
[Epoch 87; Iter    42/   48] train: loss: 0.0629620
[Epoch 87] ogbg-molbbbp: 0.907049 val loss: 0.515680
[Epoch 87] ogbg-molbbbp: 0.890073 test loss: 0.576931
[Epoch 88; Iter    24/   48] train: loss: 0.0127969
[Epoch 88] ogbg-molbbbp: 0.912607 val loss: 0.529470
[Epoch 88] ogbg-molbbbp: 0.870029 test loss: 0.630515
[Epoch 89; Iter     6/   48] train: loss: 0.0194910
[Epoch 89; Iter    36/   48] train: loss: 0.0065110
[Epoch 89] ogbg-molbbbp: 0.903647 val loss: 0.545248
[Epoch 89] ogbg-molbbbp: 0.904437 test loss: 0.590047
[Epoch 90; Iter    18/   48] train: loss: 0.0044911
[Epoch 90; Iter    48/   48] train: loss: 0.3258428
[Epoch 90] ogbg-molbbbp: 0.899280 val loss: 0.533903
[Epoch 90] ogbg-molbbbp: 0.893010 test loss: 0.553972
[Epoch 91; Iter    30/   48] train: loss: 0.0106902
[Epoch 91] ogbg-molbbbp: 0.910112 val loss: 0.584407
[Epoch 91] ogbg-molbbbp: 0.887265 test loss: 0.606788
[Epoch 92; Iter    12/   48] train: loss: 0.0131614
[Epoch 92; Iter    42/   48] train: loss: 0.0081490
[Epoch 92] ogbg-molbbbp: 0.911870 val loss: 0.480772
[Epoch 92] ogbg-molbbbp: 0.893265 test loss: 0.533132
[Epoch 93; Iter    24/   48] train: loss: 0.0447104
[Epoch 93] ogbg-molbbbp: 0.899053 val loss: 0.619772
[Epoch 93] ogbg-molbbbp: 0.893265 test loss: 0.568618
[Epoch 94; Iter     6/   48] train: loss: 0.0054163
[Epoch 94; Iter    36/   48] train: loss: 0.0049993
[Epoch 94] ogbg-molbbbp: 0.903817 val loss: 0.545215
[Epoch 94] ogbg-molbbbp: 0.893457 test loss: 0.529674
[Epoch 95; Iter    18/   48] train: loss: 0.0055732
[Epoch 95; Iter    48/   48] train: loss: 0.0084766
[Epoch 95] ogbg-molbbbp: 0.897749 val loss: 0.643988
[Epoch 95] ogbg-molbbbp: 0.895500 test loss: 0.634945
[Epoch 96; Iter    30/   48] train: loss: 0.0306825
[Epoch 96] ogbg-molbbbp: 0.902626 val loss: 0.631690
[Epoch 96] ogbg-molbbbp: 0.903543 test loss: 0.571202
[Epoch 97; Iter    12/   48] train: loss: 0.0038367
[Epoch 97; Iter    42/   48] train: loss: 0.0026143
[Epoch 97] ogbg-molbbbp: 0.899450 val loss: 0.612202
[Epoch 97] ogbg-molbbbp: 0.888797 test loss: 0.644025
[Epoch 98; Iter    24/   48] train: loss: 0.0020160
[Epoch 98] ogbg-molbbbp: 0.901945 val loss: 0.657351
[Epoch 98] ogbg-molbbbp: 0.897478 test loss: 0.620930
[Epoch 99; Iter     6/   48] train: loss: 0.0915887
[Epoch 99; Iter    36/   48] train: loss: 0.0109789
[Epoch 99] ogbg-molbbbp: 0.907446 val loss: 0.581017
[Epoch 99] ogbg-molbbbp: 0.901245 test loss: 0.608769
[Epoch 100; Iter    18/   48] train: loss: 0.0027583
[Epoch 100; Iter    48/   48] train: loss: 0.0183767
[Epoch 100] ogbg-molbbbp: 0.909545 val loss: 0.569415
[Epoch 100] ogbg-molbbbp: 0.902713 test loss: 0.617783
[Epoch 101; Iter    30/   48] train: loss: 0.0013353
[Epoch 101] ogbg-molbbbp: 0.906879 val loss: 0.551733
[Epoch 101] ogbg-molbbbp: 0.891797 test loss: 0.597457
[Epoch 102; Iter    12/   48] train: loss: 0.0046281
[Epoch 102; Iter    42/   48] train: loss: 0.0044209
[Epoch 102] ogbg-molbbbp: 0.905405 val loss: 0.528957
[Epoch 102] ogbg-molbbbp: 0.884392 test loss: 0.613589
[Epoch 103; Iter    24/   48] train: loss: 0.0194124
[Epoch 103] ogbg-molbbbp: 0.910452 val loss: 0.544050
[Epoch 103] ogbg-molbbbp: 0.897798 test loss: 0.607838
[Epoch 104; Iter     6/   48] train: loss: 0.0038724
[Epoch 104; Iter    36/   48] train: loss: 0.0034370
[Epoch 104] ogbg-molbbbp: 0.912040 val loss: 0.513568
[Epoch 104] ogbg-molbbbp: 0.901756 test loss: 0.597162
[Epoch 105; Iter    18/   48] train: loss: 0.0030951
[Epoch 105; Iter    48/   48] train: loss: 0.0016870
[Epoch 105] ogbg-molbbbp: 0.908694 val loss: 0.485768
[Epoch 105] ogbg-molbbbp: 0.890265 test loss: 0.600799
[Epoch 106; Iter    30/   48] train: loss: 0.0544329
[Epoch 106] ogbg-molbbbp: 0.908354 val loss: 0.524939
[Epoch 106] ogbg-molbbbp: 0.897032 test loss: 0.628201
[Epoch 107; Iter    12/   48] train: loss: 0.0043716
[Epoch 107; Iter    42/   48] train: loss: 0.0059324
[Epoch 107] ogbg-molbbbp: 0.906709 val loss: 0.560054
[Epoch 107] ogbg-molbbbp: 0.896968 test loss: 0.651134
[Epoch 108; Iter    24/   48] train: loss: 0.0145003
[Epoch 108] ogbg-molbbbp: 0.907163 val loss: 0.556935
[Epoch 108] ogbg-molbbbp: 0.896712 test loss: 0.658389
[Epoch 109; Iter     6/   48] train: loss: 0.0432593
[Epoch 109; Iter    36/   48] train: loss: 0.0187328
[Epoch 109] ogbg-molbbbp: 0.914365 val loss: 0.511667
[Epoch 109] ogbg-molbbbp: 0.895436 test loss: 0.684450
[Epoch 110; Iter    18/   48] train: loss: 0.0035893
[Epoch 110; Iter    48/   48] train: loss: 0.0016824
[Epoch 110] ogbg-molbbbp: 0.907560 val loss: 0.487145
[Epoch 110] ogbg-molbbbp: 0.892946 test loss: 0.639700
[Epoch 111; Iter    30/   48] train: loss: 0.0226492
[Epoch 111] ogbg-molbbbp: 0.915556 val loss: 0.502158
[Epoch 111] ogbg-molbbbp: 0.902458 test loss: 0.658580
[Epoch 112; Iter    12/   48] train: loss: 0.0024123
[Epoch 112; Iter    42/   48] train: loss: 0.0587379
[Epoch 112] ogbg-molbbbp: 0.914138 val loss: 0.470639
[Epoch 112] ogbg-molbbbp: 0.888861 test loss: 0.636199
[Epoch 113; Iter    24/   48] train: loss: 0.0140367
[Epoch 113] ogbg-molbbbp: 0.920206 val loss: 0.502558
[Epoch 113] ogbg-molbbbp: 0.890265 test loss: 0.760809
[Epoch 114; Iter     6/   48] train: loss: 0.0052995
[Epoch 114; Iter    36/   48] train: loss: 0.0013748
[Epoch 114] ogbg-molbbbp: 0.916010 val loss: 0.480723
[Epoch 114] ogbg-molbbbp: 0.903862 test loss: 0.631190
[Epoch 115; Iter    18/   48] train: loss: 0.0242266
[Epoch 115; Iter    48/   48] train: loss: 0.0019989
[Epoch 115] ogbg-molbbbp: 0.921568 val loss: 0.473439
[Epoch 115] ogbg-molbbbp: 0.900862 test loss: 0.659407
[Epoch 73] ogbg-molbbbp: 0.882860 test loss: 0.486436
[Epoch 74; Iter     6/   48] train: loss: 0.0530549
[Epoch 74; Iter    36/   48] train: loss: 0.0257568
[Epoch 74] ogbg-molbbbp: 0.901151 val loss: 0.527674
[Epoch 74] ogbg-molbbbp: 0.885924 test loss: 0.542984
[Epoch 75; Iter    18/   48] train: loss: 0.0350950
[Epoch 75; Iter    48/   48] train: loss: 0.0117957
[Epoch 75] ogbg-molbbbp: 0.904894 val loss: 0.426245
[Epoch 75] ogbg-molbbbp: 0.881392 test loss: 0.491258
[Epoch 76; Iter    30/   48] train: loss: 0.0117011
[Epoch 76] ogbg-molbbbp: 0.883911 val loss: 0.493149
[Epoch 76] ogbg-molbbbp: 0.894925 test loss: 0.457299
[Epoch 77; Iter    12/   48] train: loss: 0.0267728
[Epoch 77; Iter    42/   48] train: loss: 0.2514132
[Epoch 77] ogbg-molbbbp: 0.904100 val loss: 0.455461
[Epoch 77] ogbg-molbbbp: 0.894414 test loss: 0.491801
[Epoch 78; Iter    24/   48] train: loss: 0.0577978
[Epoch 78] ogbg-molbbbp: 0.905348 val loss: 0.442739
[Epoch 78] ogbg-molbbbp: 0.890456 test loss: 0.475889
[Epoch 79; Iter     6/   48] train: loss: 0.0620665
[Epoch 79; Iter    36/   48] train: loss: 0.3647986
[Epoch 79] ogbg-molbbbp: 0.922135 val loss: 0.451791
[Epoch 79] ogbg-molbbbp: 0.906160 test loss: 0.459238
[Epoch 80; Iter    18/   48] train: loss: 0.0580173
[Epoch 80; Iter    48/   48] train: loss: 0.1411678
[Epoch 80] ogbg-molbbbp: 0.909658 val loss: 0.430954
[Epoch 80] ogbg-molbbbp: 0.874242 test loss: 0.567873
[Epoch 81; Iter    30/   48] train: loss: 0.0728896
[Epoch 81] ogbg-molbbbp: 0.910168 val loss: 0.513731
[Epoch 81] ogbg-molbbbp: 0.904947 test loss: 0.469912
[Epoch 82; Iter    12/   48] train: loss: 0.1225524
[Epoch 82; Iter    42/   48] train: loss: 0.1309056
[Epoch 82] ogbg-molbbbp: 0.906936 val loss: 0.491138
[Epoch 82] ogbg-molbbbp: 0.878136 test loss: 0.498765
[Epoch 83; Iter    24/   48] train: loss: 0.0543164
[Epoch 83] ogbg-molbbbp: 0.908921 val loss: 0.436971
[Epoch 83] ogbg-molbbbp: 0.892627 test loss: 0.526062
[Epoch 84; Iter     6/   48] train: loss: 0.0784775
[Epoch 84; Iter    36/   48] train: loss: 0.0356170
[Epoch 84] ogbg-molbbbp: 0.913004 val loss: 0.483761
[Epoch 84] ogbg-molbbbp: 0.921226 test loss: 0.399323
[Epoch 85; Iter    18/   48] train: loss: 0.0588806
[Epoch 85; Iter    48/   48] train: loss: 0.0083977
[Epoch 85] ogbg-molbbbp: 0.885045 val loss: 0.585744
[Epoch 85] ogbg-molbbbp: 0.872391 test loss: 0.586787
[Epoch 86; Iter    30/   48] train: loss: 0.0764773
[Epoch 86] ogbg-molbbbp: 0.915273 val loss: 0.422565
[Epoch 86] ogbg-molbbbp: 0.896138 test loss: 0.512997
[Epoch 87; Iter    12/   48] train: loss: 0.0313477
[Epoch 87; Iter    42/   48] train: loss: 0.1763756
[Epoch 87] ogbg-molbbbp: 0.899960 val loss: 0.489259
[Epoch 87] ogbg-molbbbp: 0.905139 test loss: 0.438191
[Epoch 88; Iter    24/   48] train: loss: 0.0440333
[Epoch 88] ogbg-molbbbp: 0.903306 val loss: 0.567396
[Epoch 88] ogbg-molbbbp: 0.871178 test loss: 0.637822
[Epoch 89; Iter     6/   48] train: loss: 0.0185981
[Epoch 89; Iter    36/   48] train: loss: 0.0732005
[Epoch 89] ogbg-molbbbp: 0.889866 val loss: 0.539385
[Epoch 89] ogbg-molbbbp: 0.864794 test loss: 0.622466
[Epoch 90; Iter    18/   48] train: loss: 0.0188951
[Epoch 90; Iter    48/   48] train: loss: 0.2189746
[Epoch 90] ogbg-molbbbp: 0.897238 val loss: 0.496552
[Epoch 90] ogbg-molbbbp: 0.902394 test loss: 0.536666
[Epoch 91; Iter    30/   48] train: loss: 0.0303327
[Epoch 91] ogbg-molbbbp: 0.881586 val loss: 0.528748
[Epoch 91] ogbg-molbbbp: 0.866837 test loss: 0.576066
[Epoch 92; Iter    12/   48] train: loss: 0.1840241
[Epoch 92; Iter    42/   48] train: loss: 0.0960850
[Epoch 92] ogbg-molbbbp: 0.904838 val loss: 0.483637
[Epoch 92] ogbg-molbbbp: 0.893265 test loss: 0.509796
[Epoch 93; Iter    24/   48] train: loss: 0.0119726
[Epoch 93] ogbg-molbbbp: 0.901945 val loss: 0.591545
[Epoch 93] ogbg-molbbbp: 0.886882 test loss: 0.591208
[Epoch 94; Iter     6/   48] train: loss: 0.0292088
[Epoch 94; Iter    36/   48] train: loss: 0.0362923
[Epoch 94] ogbg-molbbbp: 0.902512 val loss: 0.517732
[Epoch 94] ogbg-molbbbp: 0.894159 test loss: 0.580190
[Epoch 95; Iter    18/   48] train: loss: 0.1794348
[Epoch 95; Iter    48/   48] train: loss: 0.0159578
[Epoch 95] ogbg-molbbbp: 0.886009 val loss: 0.639925
[Epoch 95] ogbg-molbbbp: 0.849601 test loss: 0.767843
[Epoch 96; Iter    30/   48] train: loss: 0.0666197
[Epoch 96] ogbg-molbbbp: 0.903306 val loss: 0.513805
[Epoch 96] ogbg-molbbbp: 0.888861 test loss: 0.501840
[Epoch 97; Iter    12/   48] train: loss: 0.1089809
[Epoch 97; Iter    42/   48] train: loss: 0.0666211
[Epoch 97] ogbg-molbbbp: 0.906596 val loss: 0.525230
[Epoch 97] ogbg-molbbbp: 0.878200 test loss: 0.660228
[Epoch 98; Iter    24/   48] train: loss: 0.0080452
[Epoch 98] ogbg-molbbbp: 0.895707 val loss: 0.579362
[Epoch 98] ogbg-molbbbp: 0.866326 test loss: 0.739984
[Epoch 99; Iter     6/   48] train: loss: 0.0565239
[Epoch 99; Iter    36/   48] train: loss: 0.0164219
[Epoch 99] ogbg-molbbbp: 0.899110 val loss: 0.483592
[Epoch 99] ogbg-molbbbp: 0.881838 test loss: 0.605552
[Epoch 100; Iter    18/   48] train: loss: 0.0281221
[Epoch 100; Iter    48/   48] train: loss: 0.0065879
[Epoch 100] ogbg-molbbbp: 0.898826 val loss: 0.498984
[Epoch 100] ogbg-molbbbp: 0.867475 test loss: 0.622663
[Epoch 101; Iter    30/   48] train: loss: 0.0279152
[Epoch 101] ogbg-molbbbp: 0.896501 val loss: 0.729057
[Epoch 101] ogbg-molbbbp: 0.873157 test loss: 0.528184
[Epoch 102; Iter    12/   48] train: loss: 0.0895903
[Epoch 102; Iter    42/   48] train: loss: 0.1551313
[Epoch 102] ogbg-molbbbp: 0.896671 val loss: 0.762779
[Epoch 102] ogbg-molbbbp: 0.871880 test loss: 0.669810
[Epoch 103; Iter    24/   48] train: loss: 0.0138818
[Epoch 103] ogbg-molbbbp: 0.914479 val loss: 0.561998
[Epoch 103] ogbg-molbbbp: 0.875519 test loss: 0.620243
[Epoch 104; Iter     6/   48] train: loss: 0.0181354
[Epoch 104; Iter    36/   48] train: loss: 0.1332809
[Epoch 104] ogbg-molbbbp: 0.905688 val loss: 0.509546
[Epoch 104] ogbg-molbbbp: 0.880179 test loss: 0.553674
[Epoch 105; Iter    18/   48] train: loss: 0.0266891
[Epoch 105; Iter    48/   48] train: loss: 0.0414587
[Epoch 105] ogbg-molbbbp: 0.920320 val loss: 0.501982
[Epoch 105] ogbg-molbbbp: 0.898819 test loss: 0.569781
[Epoch 106; Iter    30/   48] train: loss: 0.0327849
[Epoch 106] ogbg-molbbbp: 0.907560 val loss: 0.449868
[Epoch 106] ogbg-molbbbp: 0.889180 test loss: 0.518147
[Epoch 107; Iter    12/   48] train: loss: 0.0248499
[Epoch 107; Iter    42/   48] train: loss: 0.0343727
[Epoch 107] ogbg-molbbbp: 0.916293 val loss: 0.490173
[Epoch 107] ogbg-molbbbp: 0.898181 test loss: 0.589628
[Epoch 108; Iter    24/   48] train: loss: 0.0021974
[Epoch 108] ogbg-molbbbp: 0.914025 val loss: 0.500677
[Epoch 108] ogbg-molbbbp: 0.888733 test loss: 0.554822
[Epoch 109; Iter     6/   48] train: loss: 0.0707090
[Epoch 109; Iter    36/   48] train: loss: 0.0047407
[Epoch 109] ogbg-molbbbp: 0.919696 val loss: 0.476727
[Epoch 109] ogbg-molbbbp: 0.884073 test loss: 0.618898
[Epoch 110; Iter    18/   48] train: loss: 0.0364117
[Epoch 110; Iter    48/   48] train: loss: 0.0057496
[Epoch 110] ogbg-molbbbp: 0.921624 val loss: 0.515653
[Epoch 110] ogbg-molbbbp: 0.882796 test loss: 0.679166
[Epoch 111; Iter    30/   48] train: loss: 0.0180795
[Epoch 111] ogbg-molbbbp: 0.906085 val loss: 0.531561
[Epoch 111] ogbg-molbbbp: 0.869007 test loss: 0.732934
[Epoch 112; Iter    12/   48] train: loss: 0.1448475
[Epoch 112; Iter    42/   48] train: loss: 0.0047106
[Epoch 112] ogbg-molbbbp: 0.914989 val loss: 0.462509
[Epoch 112] ogbg-molbbbp: 0.888477 test loss: 0.632063
[Epoch 113; Iter    24/   48] train: loss: 0.0051845
[Epoch 113] ogbg-molbbbp: 0.912777 val loss: 0.500952
[Epoch 113] ogbg-molbbbp: 0.879796 test loss: 0.698889
[Epoch 114; Iter     6/   48] train: loss: 0.0154928
[Epoch 114; Iter    36/   48] train: loss: 0.0111478
[Epoch 114] ogbg-molbbbp: 0.902626 val loss: 0.623287
[Epoch 114] ogbg-molbbbp: 0.862305 test loss: 0.831822
[Epoch 115; Iter    18/   48] train: loss: 0.0148320
[Epoch 115; Iter    48/   48] train: loss: 0.0955694
[Epoch 115] ogbg-molbbbp: 0.906993 val loss: 0.560923
[Epoch 115] ogbg-molbbbp: 0.898436 test loss: 0.672999
[Epoch 78] ogbg-molbbbp: 0.899368 val loss: 0.457482
[Epoch 78] ogbg-molbbbp: 0.893630 test loss: 0.485766
[Epoch 79; Iter    12/   41] train: loss: 0.0889539
[Epoch 79] ogbg-molbbbp: 0.893258 val loss: 0.470796
[Epoch 79] ogbg-molbbbp: 0.879040 test loss: 0.521224
[Epoch 80; Iter     1/   41] train: loss: 0.0238454
[Epoch 80; Iter    31/   41] train: loss: 0.0098766
[Epoch 80] ogbg-molbbbp: 0.919065 val loss: 0.400752
[Epoch 80] ogbg-molbbbp: 0.902711 test loss: 0.505173
[Epoch 81; Iter    20/   41] train: loss: 0.0145442
[Epoch 81] ogbg-molbbbp: 0.917802 val loss: 0.416930
[Epoch 81] ogbg-molbbbp: 0.898738 test loss: 0.522629
[Epoch 82; Iter     9/   41] train: loss: 0.0092764
[Epoch 82; Iter    39/   41] train: loss: 0.0165572
[Epoch 82] ogbg-molbbbp: 0.922069 val loss: 0.417204
[Epoch 82] ogbg-molbbbp: 0.903846 test loss: 0.526382
[Epoch 83; Iter    28/   41] train: loss: 0.0164228
[Epoch 83] ogbg-molbbbp: 0.916095 val loss: 0.433351
[Epoch 83] ogbg-molbbbp: 0.900407 test loss: 0.544988
[Epoch 84; Iter    17/   41] train: loss: 0.0122522
[Epoch 84] ogbg-molbbbp: 0.912579 val loss: 0.413875
[Epoch 84] ogbg-molbbbp: 0.888555 test loss: 0.529182
[Epoch 85; Iter     6/   41] train: loss: 0.0416228
[Epoch 85; Iter    36/   41] train: loss: 0.0123438
[Epoch 85] ogbg-molbbbp: 0.917597 val loss: 0.422320
[Epoch 85] ogbg-molbbbp: 0.894832 test loss: 0.517056
[Epoch 86; Iter    25/   41] train: loss: 0.0070639
[Epoch 86] ogbg-molbbbp: 0.911623 val loss: 0.455720
[Epoch 86] ogbg-molbbbp: 0.891159 test loss: 0.562229
[Epoch 87; Iter    14/   41] train: loss: 0.0076518
[Epoch 87] ogbg-molbbbp: 0.902236 val loss: 0.485912
[Epoch 87] ogbg-molbbbp: 0.879841 test loss: 0.548550
[Epoch 88; Iter     3/   41] train: loss: 0.0324237
[Epoch 88; Iter    33/   41] train: loss: 0.0066520
[Epoch 88] ogbg-molbbbp: 0.917836 val loss: 0.481260
[Epoch 88] ogbg-molbbbp: 0.901175 test loss: 0.599558
[Epoch 89; Iter    22/   41] train: loss: 0.0708797
[Epoch 89] ogbg-molbbbp: 0.921147 val loss: 0.426945
[Epoch 89] ogbg-molbbbp: 0.884582 test loss: 0.621531
[Epoch 90; Iter    11/   41] train: loss: 0.1532911
[Epoch 90; Iter    41/   41] train: loss: 0.0163921
[Epoch 90] ogbg-molbbbp: 0.915754 val loss: 0.457226
[Epoch 90] ogbg-molbbbp: 0.887453 test loss: 0.598224
[Epoch 91; Iter    30/   41] train: loss: 0.0072886
[Epoch 91] ogbg-molbbbp: 0.914934 val loss: 0.476625
[Epoch 91] ogbg-molbbbp: 0.892294 test loss: 0.613026
[Epoch 92; Iter    19/   41] train: loss: 0.0074005
[Epoch 92] ogbg-molbbbp: 0.923741 val loss: 0.471536
[Epoch 92] ogbg-molbbbp: 0.900641 test loss: 0.581696
[Epoch 93; Iter     8/   41] train: loss: 0.0311701
[Epoch 93; Iter    38/   41] train: loss: 0.0221306
[Epoch 93] ogbg-molbbbp: 0.925550 val loss: 0.469816
[Epoch 93] ogbg-molbbbp: 0.897569 test loss: 0.613855
[Epoch 94; Iter    27/   41] train: loss: 0.1671186
[Epoch 94] ogbg-molbbbp: 0.927838 val loss: 0.439426
[Epoch 94] ogbg-molbbbp: 0.899472 test loss: 0.562799
[Epoch 95; Iter    16/   41] train: loss: 0.0075876
[Epoch 95] ogbg-molbbbp: 0.919201 val loss: 0.447069
[Epoch 95] ogbg-molbbbp: 0.893697 test loss: 0.578693
[Epoch 96; Iter     5/   41] train: loss: 0.0193746
[Epoch 96; Iter    35/   41] train: loss: 0.0078991
[Epoch 96] ogbg-molbbbp: 0.907937 val loss: 0.470861
[Epoch 96] ogbg-molbbbp: 0.883280 test loss: 0.571914
[Epoch 97; Iter    24/   41] train: loss: 0.0100809
[Epoch 97] ogbg-molbbbp: 0.920703 val loss: 0.453864
[Epoch 97] ogbg-molbbbp: 0.885951 test loss: 0.646448
[Epoch 98; Iter    13/   41] train: loss: 0.0473310
[Epoch 98] ogbg-molbbbp: 0.920601 val loss: 0.424100
[Epoch 98] ogbg-molbbbp: 0.884448 test loss: 0.580359
[Epoch 99; Iter     2/   41] train: loss: 0.0029491
[Epoch 99; Iter    32/   41] train: loss: 0.0043286
[Epoch 99] ogbg-molbbbp: 0.918962 val loss: 0.430689
[Epoch 99] ogbg-molbbbp: 0.881510 test loss: 0.604542
[Epoch 100; Iter    21/   41] train: loss: 0.0032080
[Epoch 100] ogbg-molbbbp: 0.909097 val loss: 0.485924
[Epoch 100] ogbg-molbbbp: 0.882679 test loss: 0.631491
[Epoch 101; Iter    10/   41] train: loss: 0.0086497
[Epoch 101; Iter    40/   41] train: loss: 0.1581629
[Epoch 101] ogbg-molbbbp: 0.907459 val loss: 0.478600
[Epoch 101] ogbg-molbbbp: 0.883080 test loss: 0.600995
[Epoch 102; Iter    29/   41] train: loss: 0.0187889
[Epoch 102] ogbg-molbbbp: 0.912920 val loss: 0.518197
[Epoch 102] ogbg-molbbbp: 0.883814 test loss: 0.649806
[Epoch 103; Iter    18/   41] train: loss: 0.0068691
[Epoch 103] ogbg-molbbbp: 0.914217 val loss: 0.531045
[Epoch 103] ogbg-molbbbp: 0.887420 test loss: 0.578366
[Epoch 104; Iter     7/   41] train: loss: 0.0116524
[Epoch 104; Iter    37/   41] train: loss: 0.0037798
[Epoch 104] ogbg-molbbbp: 0.919884 val loss: 0.702693
[Epoch 104] ogbg-molbbbp: 0.883847 test loss: 0.639542
[Epoch 105; Iter    26/   41] train: loss: 0.0070967
[Epoch 105] ogbg-molbbbp: 0.914047 val loss: 0.592027
[Epoch 105] ogbg-molbbbp: 0.879240 test loss: 0.618627
[Epoch 106; Iter    15/   41] train: loss: 0.0684483
[Epoch 106] ogbg-molbbbp: 0.919645 val loss: 0.617153
[Epoch 106] ogbg-molbbbp: 0.892995 test loss: 0.608861
[Epoch 107; Iter     4/   41] train: loss: 0.0036299
[Epoch 107; Iter    34/   41] train: loss: 0.1491078
[Epoch 107] ogbg-molbbbp: 0.915788 val loss: 0.617134
[Epoch 107] ogbg-molbbbp: 0.888455 test loss: 0.662655
[Epoch 108; Iter    23/   41] train: loss: 0.0042610
[Epoch 108] ogbg-molbbbp: 0.913535 val loss: 0.750274
[Epoch 108] ogbg-molbbbp: 0.888388 test loss: 0.642897
[Epoch 109; Iter    12/   41] train: loss: 0.0414580
[Epoch 109] ogbg-molbbbp: 0.911077 val loss: 0.935192
[Epoch 109] ogbg-molbbbp: 0.879808 test loss: 0.629642
[Epoch 110; Iter     1/   41] train: loss: 0.0110507
[Epoch 110; Iter    31/   41] train: loss: 0.0021315
[Epoch 110] ogbg-molbbbp: 0.913876 val loss: 1.008168
[Epoch 110] ogbg-molbbbp: 0.883347 test loss: 0.692980
[Epoch 111; Iter    20/   41] train: loss: 0.0031836
[Epoch 111] ogbg-molbbbp: 0.914354 val loss: 0.880219
[Epoch 111] ogbg-molbbbp: 0.886318 test loss: 0.640253
[Epoch 112; Iter     9/   41] train: loss: 0.0048058
[Epoch 112; Iter    39/   41] train: loss: 0.0802473
[Epoch 112] ogbg-molbbbp: 0.914832 val loss: 0.933100
[Epoch 112] ogbg-molbbbp: 0.892929 test loss: 0.630621
[Epoch 113; Iter    28/   41] train: loss: 0.0013258
[Epoch 113] ogbg-molbbbp: 0.914661 val loss: 0.962171
[Epoch 113] ogbg-molbbbp: 0.892795 test loss: 0.635926
[Epoch 114; Iter    17/   41] train: loss: 0.0359038
[Epoch 114] ogbg-molbbbp: 0.915412 val loss: 0.983388
[Epoch 114] ogbg-molbbbp: 0.891293 test loss: 0.619220
[Epoch 115; Iter     6/   41] train: loss: 0.0085409
[Epoch 115; Iter    36/   41] train: loss: 0.0372850
[Epoch 115] ogbg-molbbbp: 0.915549 val loss: 1.016200
[Epoch 115] ogbg-molbbbp: 0.891026 test loss: 0.698374
[Epoch 116; Iter    25/   41] train: loss: 0.0017062
[Epoch 116] ogbg-molbbbp: 0.915958 val loss: 0.964410
[Epoch 116] ogbg-molbbbp: 0.884782 test loss: 0.656015
[Epoch 117; Iter    14/   41] train: loss: 0.0051986
[Epoch 117] ogbg-molbbbp: 0.916470 val loss: 0.991341
[Epoch 117] ogbg-molbbbp: 0.885116 test loss: 0.670358
[Epoch 118; Iter     3/   41] train: loss: 0.0025749
[Epoch 118; Iter    33/   41] train: loss: 0.0018355
[Epoch 118] ogbg-molbbbp: 0.914900 val loss: 1.030974
[Epoch 118] ogbg-molbbbp: 0.881510 test loss: 0.693065
[Epoch 119; Iter    22/   41] train: loss: 0.0105964
[Epoch 119] ogbg-molbbbp: 0.915992 val loss: 1.028312
[Epoch 119] ogbg-molbbbp: 0.876436 test loss: 0.703855
[Epoch 120; Iter    11/   41] train: loss: 0.0015059
[Epoch 120; Iter    41/   41] train: loss: 0.0300051
[Epoch 120] ogbg-molbbbp: 0.916334 val loss: 1.062614
[Epoch 120] ogbg-molbbbp: 0.880509 test loss: 0.685762
[Epoch 121; Iter    30/   41] train: loss: 0.0020849
[Epoch 121] ogbg-molbbbp: 0.918723 val loss: 0.916729
[Epoch 121] ogbg-molbbbp: 0.888755 test loss: 0.706646
[Epoch 122; Iter    19/   41] train: loss: 0.0300781
[Epoch 122] ogbg-molbbbp: 0.916095 val loss: 0.989332
[Epoch 122] ogbg-molbbbp: 0.888789 test loss: 0.678114
[Epoch 123; Iter     8/   41] train: loss: 0.0012318
[Epoch 123; Iter    38/   41] train: loss: 0.1418420
[Epoch 123] ogbg-molbbbp: 0.914764 val loss: 0.973621
[Epoch 73] ogbg-molbbbp: 0.898628 test loss: 0.487618
[Epoch 74; Iter     6/   48] train: loss: 0.2088795
[Epoch 74; Iter    36/   48] train: loss: 0.0195904
[Epoch 74] ogbg-molbbbp: 0.909828 val loss: 0.460513
[Epoch 74] ogbg-molbbbp: 0.888605 test loss: 0.558106
[Epoch 75; Iter    18/   48] train: loss: 0.0113926
[Epoch 75; Iter    48/   48] train: loss: 0.0824697
[Epoch 75] ogbg-molbbbp: 0.905745 val loss: 0.506625
[Epoch 75] ogbg-molbbbp: 0.880817 test loss: 0.571491
[Epoch 76; Iter    30/   48] train: loss: 0.1676722
[Epoch 76] ogbg-molbbbp: 0.898089 val loss: 0.484931
[Epoch 76] ogbg-molbbbp: 0.875327 test loss: 0.578364
[Epoch 77; Iter    12/   48] train: loss: 0.0445413
[Epoch 77; Iter    42/   48] train: loss: 0.0540292
[Epoch 77] ogbg-molbbbp: 0.908410 val loss: 0.630365
[Epoch 77] ogbg-molbbbp: 0.897159 test loss: 0.503290
[Epoch 78; Iter    24/   48] train: loss: 0.0127349
[Epoch 78] ogbg-molbbbp: 0.917938 val loss: 0.394684
[Epoch 78] ogbg-molbbbp: 0.880179 test loss: 0.516260
[Epoch 79; Iter     6/   48] train: loss: 0.0278066
[Epoch 79; Iter    36/   48] train: loss: 0.1466083
[Epoch 79] ogbg-molbbbp: 0.910168 val loss: 0.462262
[Epoch 79] ogbg-molbbbp: 0.893584 test loss: 0.532568
[Epoch 80; Iter    18/   48] train: loss: 0.1097099
[Epoch 80; Iter    48/   48] train: loss: 0.0046243
[Epoch 80] ogbg-molbbbp: 0.909318 val loss: 0.456465
[Epoch 80] ogbg-molbbbp: 0.894287 test loss: 0.548348
[Epoch 81; Iter    30/   48] train: loss: 0.0079329
[Epoch 81] ogbg-molbbbp: 0.910962 val loss: 0.431074
[Epoch 81] ogbg-molbbbp: 0.893138 test loss: 0.502521
[Epoch 82; Iter    12/   48] train: loss: 0.0200552
[Epoch 82; Iter    42/   48] train: loss: 0.0148732
[Epoch 82] ogbg-molbbbp: 0.900924 val loss: 0.454431
[Epoch 82] ogbg-molbbbp: 0.884328 test loss: 0.618627
[Epoch 83; Iter    24/   48] train: loss: 0.0237098
[Epoch 83] ogbg-molbbbp: 0.891000 val loss: 0.512732
[Epoch 83] ogbg-molbbbp: 0.884583 test loss: 0.557343
[Epoch 84; Iter     6/   48] train: loss: 0.0241327
[Epoch 84; Iter    36/   48] train: loss: 0.0172484
[Epoch 84] ogbg-molbbbp: 0.902512 val loss: 0.502287
[Epoch 84] ogbg-molbbbp: 0.880753 test loss: 0.574861
[Epoch 85; Iter    18/   48] train: loss: 0.0892675
[Epoch 85; Iter    48/   48] train: loss: 0.0170576
[Epoch 85] ogbg-molbbbp: 0.895650 val loss: 0.523614
[Epoch 85] ogbg-molbbbp: 0.888158 test loss: 0.588457
[Epoch 86; Iter    30/   48] train: loss: 0.0146192
[Epoch 86] ogbg-molbbbp: 0.918392 val loss: 0.427980
[Epoch 86] ogbg-molbbbp: 0.899649 test loss: 0.523071
[Epoch 87; Iter    12/   48] train: loss: 0.0165974
[Epoch 87; Iter    42/   48] train: loss: 0.0806914
[Epoch 87] ogbg-molbbbp: 0.913855 val loss: 0.427050
[Epoch 87] ogbg-molbbbp: 0.894031 test loss: 0.487426
[Epoch 88; Iter    24/   48] train: loss: 0.0105740
[Epoch 88] ogbg-molbbbp: 0.911870 val loss: 0.466138
[Epoch 88] ogbg-molbbbp: 0.902968 test loss: 0.495018
[Epoch 89; Iter     6/   48] train: loss: 0.0144071
[Epoch 89; Iter    36/   48] train: loss: 0.0128210
[Epoch 89] ogbg-molbbbp: 0.912947 val loss: 0.441686
[Epoch 89] ogbg-molbbbp: 0.898691 test loss: 0.518569
[Epoch 90; Iter    18/   48] train: loss: 0.0238720
[Epoch 90; Iter    48/   48] train: loss: 0.0056613
[Epoch 90] ogbg-molbbbp: 0.899620 val loss: 0.477812
[Epoch 90] ogbg-molbbbp: 0.881009 test loss: 0.568223
[Epoch 91; Iter    30/   48] train: loss: 0.1210746
[Epoch 91] ogbg-molbbbp: 0.905178 val loss: 0.481986
[Epoch 91] ogbg-molbbbp: 0.899777 test loss: 0.587663
[Epoch 92; Iter    12/   48] train: loss: 0.2066268
[Epoch 92; Iter    42/   48] train: loss: 0.0058323
[Epoch 92] ogbg-molbbbp: 0.908354 val loss: 0.506083
[Epoch 92] ogbg-molbbbp: 0.894159 test loss: 0.631856
[Epoch 93; Iter    24/   48] train: loss: 0.0054519
[Epoch 93] ogbg-molbbbp: 0.905348 val loss: 0.527623
[Epoch 93] ogbg-molbbbp: 0.895244 test loss: 0.650439
[Epoch 94; Iter     6/   48] train: loss: 0.0180076
[Epoch 94; Iter    36/   48] train: loss: 0.0097206
[Epoch 94] ogbg-molbbbp: 0.914025 val loss: 0.479004
[Epoch 94] ogbg-molbbbp: 0.902968 test loss: 0.547805
[Epoch 95; Iter    18/   48] train: loss: 0.0028577
[Epoch 95; Iter    48/   48] train: loss: 0.0836187
[Epoch 95] ogbg-molbbbp: 0.912720 val loss: 0.476842
[Epoch 95] ogbg-molbbbp: 0.898308 test loss: 0.602735
[Epoch 96; Iter    30/   48] train: loss: 0.0043763
[Epoch 96] ogbg-molbbbp: 0.907276 val loss: 0.500987
[Epoch 96] ogbg-molbbbp: 0.892180 test loss: 0.676202
[Epoch 97; Iter    12/   48] train: loss: 0.7527018
[Epoch 97; Iter    42/   48] train: loss: 0.0029932
[Epoch 97] ogbg-molbbbp: 0.911870 val loss: 0.453996
[Epoch 97] ogbg-molbbbp: 0.906352 test loss: 0.556466
[Epoch 98; Iter    24/   48] train: loss: 0.1077201
[Epoch 98] ogbg-molbbbp: 0.906425 val loss: 0.467827
[Epoch 98] ogbg-molbbbp: 0.882094 test loss: 0.621487
[Epoch 99; Iter     6/   48] train: loss: 0.0401154
[Epoch 99; Iter    36/   48] train: loss: 0.2145663
[Epoch 99] ogbg-molbbbp: 0.912720 val loss: 0.496331
[Epoch 99] ogbg-molbbbp: 0.895244 test loss: 0.633796
[Epoch 100; Iter    18/   48] train: loss: 0.0249581
[Epoch 100; Iter    48/   48] train: loss: 0.0229527
[Epoch 100] ogbg-molbbbp: 0.900471 val loss: 0.533762
[Epoch 100] ogbg-molbbbp: 0.874178 test loss: 0.666698
[Epoch 101; Iter    30/   48] train: loss: 0.0038137
[Epoch 101] ogbg-molbbbp: 0.913174 val loss: 0.475797
[Epoch 101] ogbg-molbbbp: 0.893010 test loss: 0.610861
[Epoch 102; Iter    12/   48] train: loss: 0.0025676
[Epoch 102; Iter    42/   48] train: loss: 0.0098909
[Epoch 102] ogbg-molbbbp: 0.912891 val loss: 0.499590
[Epoch 102] ogbg-molbbbp: 0.897542 test loss: 0.617947
[Epoch 103; Iter    24/   48] train: loss: 0.0048521
[Epoch 103] ogbg-molbbbp: 0.909601 val loss: 0.490228
[Epoch 103] ogbg-molbbbp: 0.897670 test loss: 0.607487
[Epoch 104; Iter     6/   48] train: loss: 0.0015445
[Epoch 104; Iter    36/   48] train: loss: 0.0079282
[Epoch 104] ogbg-molbbbp: 0.909091 val loss: 0.525371
[Epoch 104] ogbg-molbbbp: 0.895244 test loss: 0.653363
[Epoch 105; Iter    18/   48] train: loss: 0.0038337
[Epoch 105; Iter    48/   48] train: loss: 0.0037387
[Epoch 105] ogbg-molbbbp: 0.906879 val loss: 0.517415
[Epoch 105] ogbg-molbbbp: 0.899521 test loss: 0.669548
[Epoch 106; Iter    30/   48] train: loss: 0.0023943
[Epoch 106] ogbg-molbbbp: 0.914989 val loss: 0.544368
[Epoch 106] ogbg-molbbbp: 0.897351 test loss: 0.702933
[Epoch 107; Iter    12/   48] train: loss: 0.0029783
[Epoch 107; Iter    42/   48] train: loss: 0.0028591
[Epoch 107] ogbg-molbbbp: 0.908694 val loss: 0.525198
[Epoch 107] ogbg-molbbbp: 0.884966 test loss: 0.692724
[Epoch 108; Iter    24/   48] train: loss: 0.0170635
[Epoch 108] ogbg-molbbbp: 0.906142 val loss: 0.497975
[Epoch 108] ogbg-molbbbp: 0.881838 test loss: 0.658272
[Epoch 109; Iter     6/   48] train: loss: 0.0014609
[Epoch 109; Iter    36/   48] train: loss: 0.0032443
[Epoch 109] ogbg-molbbbp: 0.906936 val loss: 0.536410
[Epoch 109] ogbg-molbbbp: 0.886882 test loss: 0.660815
[Epoch 110; Iter    18/   48] train: loss: 0.0020803
[Epoch 110; Iter    48/   48] train: loss: 0.0015354
[Epoch 110] ogbg-molbbbp: 0.903476 val loss: 0.553350
[Epoch 110] ogbg-molbbbp: 0.887392 test loss: 0.676194
[Epoch 111; Iter    30/   48] train: loss: 0.0027049
[Epoch 111] ogbg-molbbbp: 0.908354 val loss: 0.521702
[Epoch 111] ogbg-molbbbp: 0.885286 test loss: 0.645541
[Epoch 112; Iter    12/   48] train: loss: 0.0022347
[Epoch 112; Iter    42/   48] train: loss: 0.0020520
[Epoch 112] ogbg-molbbbp: 0.909545 val loss: 0.512569
[Epoch 112] ogbg-molbbbp: 0.888350 test loss: 0.599371
[Epoch 113; Iter    24/   48] train: loss: 0.0229077
[Epoch 113] ogbg-molbbbp: 0.916520 val loss: 0.514991
[Epoch 113] ogbg-molbbbp: 0.896904 test loss: 0.658336
[Epoch 114; Iter     6/   48] train: loss: 0.0046908
[Epoch 114; Iter    36/   48] train: loss: 0.0026039
[Epoch 114] ogbg-molbbbp: 0.907106 val loss: 0.513740
[Epoch 114] ogbg-molbbbp: 0.882030 test loss: 0.705743
[Epoch 115; Iter    18/   48] train: loss: 0.0033426
[Epoch 115; Iter    48/   48] train: loss: 0.0103121
[Epoch 115] ogbg-molbbbp: 0.909204 val loss: 0.516135
[Epoch 115] ogbg-molbbbp: 0.890010 test loss: 0.685974
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 69; Iter    40/   55] train: loss: 0.0484189
[Epoch 69] ogbg-molbbbp: 0.931728 val loss: 0.421062
[Epoch 69] ogbg-molbbbp: 0.913727 test loss: 0.440227
[Epoch 70; Iter    15/   55] train: loss: 0.0339065
[Epoch 70; Iter    45/   55] train: loss: 0.0926444
[Epoch 70] ogbg-molbbbp: 0.911111 val loss: 0.474835
[Epoch 70] ogbg-molbbbp: 0.903880 test loss: 0.449625
[Epoch 71; Iter    20/   55] train: loss: 0.0381409
[Epoch 71; Iter    50/   55] train: loss: 0.1261600
[Epoch 71] ogbg-molbbbp: 0.905309 val loss: 0.586054
[Epoch 71] ogbg-molbbbp: 0.871987 test loss: 0.703713
[Epoch 72; Iter    25/   55] train: loss: 0.0864147
[Epoch 72; Iter    55/   55] train: loss: 0.5230260
[Epoch 72] ogbg-molbbbp: 0.904198 val loss: 0.476292
[Epoch 72] ogbg-molbbbp: 0.885068 test loss: 0.524005
[Epoch 73; Iter    30/   55] train: loss: 0.0359224
[Epoch 73] ogbg-molbbbp: 0.927531 val loss: 0.444965
[Epoch 73] ogbg-molbbbp: 0.904615 test loss: 0.460218
[Epoch 74; Iter     5/   55] train: loss: 0.0310608
[Epoch 74; Iter    35/   55] train: loss: 0.0528623
[Epoch 74] ogbg-molbbbp: 0.914198 val loss: 0.493394
[Epoch 74] ogbg-molbbbp: 0.912845 test loss: 0.466523
[Epoch 75; Iter    10/   55] train: loss: 0.0140975
[Epoch 75; Iter    40/   55] train: loss: 0.0688958
[Epoch 75] ogbg-molbbbp: 0.917901 val loss: 0.417657
[Epoch 75] ogbg-molbbbp: 0.910053 test loss: 0.428447
[Epoch 76; Iter    15/   55] train: loss: 0.0828049
[Epoch 76; Iter    45/   55] train: loss: 0.0041237
[Epoch 76] ogbg-molbbbp: 0.909383 val loss: 0.476733
[Epoch 76] ogbg-molbbbp: 0.914462 test loss: 0.448090
[Epoch 77; Iter    20/   55] train: loss: 0.0139636
[Epoch 77; Iter    50/   55] train: loss: 0.1024482
[Epoch 77] ogbg-molbbbp: 0.916296 val loss: 0.466144
[Epoch 77] ogbg-molbbbp: 0.916520 test loss: 0.443897
[Epoch 78; Iter    25/   55] train: loss: 0.0554980
[Epoch 78; Iter    55/   55] train: loss: 0.0253826
[Epoch 78] ogbg-molbbbp: 0.897160 val loss: 0.547383
[Epoch 78] ogbg-molbbbp: 0.900206 test loss: 0.525540
[Epoch 79; Iter    30/   55] train: loss: 0.0503209
[Epoch 79] ogbg-molbbbp: 0.919136 val loss: 0.469527
[Epoch 79] ogbg-molbbbp: 0.895062 test loss: 0.524596
[Epoch 80; Iter     5/   55] train: loss: 0.0740974
[Epoch 80; Iter    35/   55] train: loss: 0.0121710
[Epoch 80] ogbg-molbbbp: 0.933333 val loss: 0.428191
[Epoch 80] ogbg-molbbbp: 0.906966 test loss: 0.581632
[Epoch 81; Iter    10/   55] train: loss: 0.0514629
[Epoch 81; Iter    40/   55] train: loss: 0.0188776
[Epoch 81] ogbg-molbbbp: 0.915062 val loss: 0.459105
[Epoch 81] ogbg-molbbbp: 0.899618 test loss: 0.485565
[Epoch 82; Iter    15/   55] train: loss: 0.0081895
[Epoch 82; Iter    45/   55] train: loss: 0.0096122
[Epoch 82] ogbg-molbbbp: 0.908765 val loss: 0.494095
[Epoch 82] ogbg-molbbbp: 0.889183 test loss: 0.489760
[Epoch 83; Iter    20/   55] train: loss: 0.0152053
[Epoch 83; Iter    50/   55] train: loss: 0.0208503
[Epoch 83] ogbg-molbbbp: 0.921235 val loss: 0.466282
[Epoch 83] ogbg-molbbbp: 0.900500 test loss: 0.516849
[Epoch 84; Iter    25/   55] train: loss: 0.0253447
[Epoch 84; Iter    55/   55] train: loss: 0.0033718
[Epoch 84] ogbg-molbbbp: 0.894815 val loss: 0.633364
[Epoch 84] ogbg-molbbbp: 0.902998 test loss: 0.673770
[Epoch 85; Iter    30/   55] train: loss: 0.0078405
[Epoch 85] ogbg-molbbbp: 0.917531 val loss: 0.533017
[Epoch 85] ogbg-molbbbp: 0.905791 test loss: 0.600046
[Epoch 86; Iter     5/   55] train: loss: 0.0247953
[Epoch 86; Iter    35/   55] train: loss: 0.0659790
[Epoch 86] ogbg-molbbbp: 0.909506 val loss: 0.490473
[Epoch 86] ogbg-molbbbp: 0.869048 test loss: 0.574776
[Epoch 87; Iter    10/   55] train: loss: 0.0870482
[Epoch 87; Iter    40/   55] train: loss: 0.2887755
[Epoch 87] ogbg-molbbbp: 0.908025 val loss: 0.551075
[Epoch 87] ogbg-molbbbp: 0.893298 test loss: 0.614544
[Epoch 88; Iter    15/   55] train: loss: 0.0176465
[Epoch 88; Iter    45/   55] train: loss: 0.1718740
[Epoch 88] ogbg-molbbbp: 0.925185 val loss: 0.455314
[Epoch 88] ogbg-molbbbp: 0.895062 test loss: 0.577658
[Epoch 89; Iter    20/   55] train: loss: 0.0226150
[Epoch 89; Iter    50/   55] train: loss: 0.0173272
[Epoch 89] ogbg-molbbbp: 0.915432 val loss: 0.517390
[Epoch 89] ogbg-molbbbp: 0.875514 test loss: 0.649909
[Epoch 90; Iter    25/   55] train: loss: 0.2081413
[Epoch 90; Iter    55/   55] train: loss: 0.0228278
[Epoch 90] ogbg-molbbbp: 0.918395 val loss: 0.451467
[Epoch 90] ogbg-molbbbp: 0.880511 test loss: 0.538868
[Epoch 91; Iter    30/   55] train: loss: 0.0254552
[Epoch 91] ogbg-molbbbp: 0.911852 val loss: 0.494593
[Epoch 91] ogbg-molbbbp: 0.855673 test loss: 0.625468
[Epoch 92; Iter     5/   55] train: loss: 0.0212504
[Epoch 92; Iter    35/   55] train: loss: 0.0073257
[Epoch 92] ogbg-molbbbp: 0.922469 val loss: 0.464168
[Epoch 92] ogbg-molbbbp: 0.893151 test loss: 0.550930
[Epoch 93; Iter    10/   55] train: loss: 0.0376088
[Epoch 93; Iter    40/   55] train: loss: 0.0085988
[Epoch 93] ogbg-molbbbp: 0.906914 val loss: 0.556438
[Epoch 93] ogbg-molbbbp: 0.897707 test loss: 0.600709
[Epoch 94; Iter    15/   55] train: loss: 0.0442241
[Epoch 94; Iter    45/   55] train: loss: 0.0078450
[Epoch 94] ogbg-molbbbp: 0.912099 val loss: 0.560188
[Epoch 94] ogbg-molbbbp: 0.897413 test loss: 0.613650
[Epoch 95; Iter    20/   55] train: loss: 0.0032097
[Epoch 95; Iter    50/   55] train: loss: 0.0022925
[Epoch 95] ogbg-molbbbp: 0.908889 val loss: 0.565801
[Epoch 95] ogbg-molbbbp: 0.897266 test loss: 0.648455
[Epoch 96; Iter    25/   55] train: loss: 0.0063930
[Epoch 96; Iter    55/   55] train: loss: 0.0009031
[Epoch 96] ogbg-molbbbp: 0.915062 val loss: 0.561021
[Epoch 96] ogbg-molbbbp: 0.898589 test loss: 0.657069
[Epoch 97; Iter    30/   55] train: loss: 0.0085094
[Epoch 97] ogbg-molbbbp: 0.910247 val loss: 0.557815
[Epoch 97] ogbg-molbbbp: 0.899177 test loss: 0.636052
[Epoch 98; Iter     5/   55] train: loss: 0.0300540
[Epoch 98; Iter    35/   55] train: loss: 0.0020968
[Epoch 98] ogbg-molbbbp: 0.909877 val loss: 0.573839
[Epoch 98] ogbg-molbbbp: 0.904615 test loss: 0.603205
[Epoch 99; Iter    10/   55] train: loss: 0.2520867
[Epoch 99; Iter    40/   55] train: loss: 0.0033974
[Epoch 99] ogbg-molbbbp: 0.903457 val loss: 0.598436
[Epoch 99] ogbg-molbbbp: 0.901529 test loss: 0.590025
[Epoch 100; Iter    15/   55] train: loss: 0.0226374
[Epoch 100; Iter    45/   55] train: loss: 0.0056293
[Epoch 100] ogbg-molbbbp: 0.913457 val loss: 0.571982
[Epoch 100] ogbg-molbbbp: 0.900941 test loss: 0.626325
[Epoch 101; Iter    20/   55] train: loss: 0.0016421
[Epoch 101; Iter    50/   55] train: loss: 0.0022141
[Epoch 101] ogbg-molbbbp: 0.919506 val loss: 0.564800
[Epoch 101] ogbg-molbbbp: 0.902116 test loss: 0.612787
[Epoch 102; Iter    25/   55] train: loss: 0.0029686
[Epoch 102; Iter    55/   55] train: loss: 0.0054336
[Epoch 102] ogbg-molbbbp: 0.911728 val loss: 0.581490
[Epoch 102] ogbg-molbbbp: 0.894621 test loss: 0.656206
[Epoch 103; Iter    30/   55] train: loss: 0.0045542
[Epoch 103] ogbg-molbbbp: 0.910988 val loss: 0.565698
[Epoch 103] ogbg-molbbbp: 0.912551 test loss: 0.595235
[Epoch 104; Iter     5/   55] train: loss: 0.0054446
[Epoch 104; Iter    35/   55] train: loss: 0.1057160
[Epoch 104] ogbg-molbbbp: 0.907284 val loss: 0.566056
[Epoch 104] ogbg-molbbbp: 0.902557 test loss: 0.683850
[Epoch 105; Iter    10/   55] train: loss: 0.0267678
[Epoch 105; Iter    40/   55] train: loss: 0.0779060
[Epoch 105] ogbg-molbbbp: 0.909753 val loss: 0.579406
[Epoch 105] ogbg-molbbbp: 0.906379 test loss: 0.645679
[Epoch 106; Iter    15/   55] train: loss: 0.0071542
[Epoch 106; Iter    45/   55] train: loss: 0.0013388
[Epoch 106] ogbg-molbbbp: 0.912840 val loss: 0.562572
[Epoch 106] ogbg-molbbbp: 0.913286 test loss: 0.635120
[Epoch 107; Iter    20/   55] train: loss: 0.0035052
[Epoch 107; Iter    50/   55] train: loss: 0.0031757
[Epoch 107] ogbg-molbbbp: 0.917037 val loss: 0.584156
[Epoch 107] ogbg-molbbbp: 0.900500 test loss: 0.723625
[Epoch 108; Iter    25/   55] train: loss: 0.0037098
[Epoch 108; Iter    55/   55] train: loss: 0.0040392
[Epoch 108] ogbg-molbbbp: 0.920988 val loss: 0.586913
[Epoch 108] ogbg-molbbbp: 0.898001 test loss: 0.674424
[Epoch 109; Iter    30/   55] train: loss: 0.0404530
[Epoch 78] ogbg-molbbbp: 0.929408 val loss: 0.364759
[Epoch 78] ogbg-molbbbp: 0.886452 test loss: 0.560209
[Epoch 79; Iter    12/   41] train: loss: 0.0051892
[Epoch 79] ogbg-molbbbp: 0.922581 val loss: 0.403570
[Epoch 79] ogbg-molbbbp: 0.894531 test loss: 0.529988
[Epoch 80; Iter     1/   41] train: loss: 0.0134369
[Epoch 80; Iter    31/   41] train: loss: 0.0217078
[Epoch 80] ogbg-molbbbp: 0.924083 val loss: 0.400350
[Epoch 80] ogbg-molbbbp: 0.883747 test loss: 0.605029
[Epoch 81; Iter    20/   41] train: loss: 0.1553763
[Epoch 81] ogbg-molbbbp: 0.919372 val loss: 0.392895
[Epoch 81] ogbg-molbbbp: 0.887987 test loss: 0.536622
[Epoch 82; Iter     9/   41] train: loss: 0.0050134
[Epoch 82; Iter    39/   41] train: loss: 0.0614168
[Epoch 82] ogbg-molbbbp: 0.923297 val loss: 0.396767
[Epoch 82] ogbg-molbbbp: 0.876936 test loss: 0.589079
[Epoch 83; Iter    28/   41] train: loss: 0.0222485
[Epoch 83] ogbg-molbbbp: 0.934050 val loss: 0.412694
[Epoch 83] ogbg-molbbbp: 0.897436 test loss: 0.584791
[Epoch 84; Iter    17/   41] train: loss: 0.0607937
[Epoch 84] ogbg-molbbbp: 0.924390 val loss: 0.397264
[Epoch 84] ogbg-molbbbp: 0.876736 test loss: 0.597385
[Epoch 85; Iter     6/   41] train: loss: 0.0165887
[Epoch 85; Iter    36/   41] train: loss: 0.0079927
[Epoch 85] ogbg-molbbbp: 0.916436 val loss: 0.440002
[Epoch 85] ogbg-molbbbp: 0.882779 test loss: 0.563161
[Epoch 86; Iter    25/   41] train: loss: 0.0163613
[Epoch 86] ogbg-molbbbp: 0.925516 val loss: 0.454176
[Epoch 86] ogbg-molbbbp: 0.885417 test loss: 0.631790
[Epoch 87; Iter    14/   41] train: loss: 0.0077884
[Epoch 87] ogbg-molbbbp: 0.917699 val loss: 0.418393
[Epoch 87] ogbg-molbbbp: 0.873965 test loss: 0.589803
[Epoch 88; Iter     3/   41] train: loss: 0.0173191
[Epoch 88; Iter    33/   41] train: loss: 0.0286335
[Epoch 88] ogbg-molbbbp: 0.929066 val loss: 0.397791
[Epoch 88] ogbg-molbbbp: 0.896568 test loss: 0.521121
[Epoch 89; Iter    22/   41] train: loss: 0.0079548
[Epoch 89] ogbg-molbbbp: 0.916539 val loss: 0.443050
[Epoch 89] ogbg-molbbbp: 0.882045 test loss: 0.575747
[Epoch 90; Iter    11/   41] train: loss: 0.0584761
[Epoch 90; Iter    41/   41] train: loss: 0.0973304
[Epoch 90] ogbg-molbbbp: 0.923332 val loss: 0.438089
[Epoch 90] ogbg-molbbbp: 0.888555 test loss: 0.597881
[Epoch 91; Iter    30/   41] train: loss: 0.0079943
[Epoch 91] ogbg-molbbbp: 0.913978 val loss: 0.422393
[Epoch 91] ogbg-molbbbp: 0.864383 test loss: 0.636510
[Epoch 92; Iter    19/   41] train: loss: 0.0057883
[Epoch 92] ogbg-molbbbp: 0.924185 val loss: 0.401384
[Epoch 92] ogbg-molbbbp: 0.891627 test loss: 0.549010
[Epoch 93; Iter     8/   41] train: loss: 0.0669931
[Epoch 93; Iter    38/   41] train: loss: 0.0269663
[Epoch 93] ogbg-molbbbp: 0.928759 val loss: 0.430905
[Epoch 93] ogbg-molbbbp: 0.885150 test loss: 0.607881
[Epoch 94; Iter    27/   41] train: loss: 0.0061169
[Epoch 94] ogbg-molbbbp: 0.927974 val loss: 0.371976
[Epoch 94] ogbg-molbbbp: 0.867054 test loss: 0.577166
[Epoch 95; Iter    16/   41] train: loss: 0.0029251
[Epoch 95] ogbg-molbbbp: 0.932173 val loss: 0.340277
[Epoch 95] ogbg-molbbbp: 0.874065 test loss: 0.522405
[Epoch 96; Iter     5/   41] train: loss: 0.0152745
[Epoch 96; Iter    35/   41] train: loss: 0.0504400
[Epoch 96] ogbg-molbbbp: 0.929578 val loss: 0.414833
[Epoch 96] ogbg-molbbbp: 0.886619 test loss: 0.560967
[Epoch 97; Iter    24/   41] train: loss: 0.1822789
[Epoch 97] ogbg-molbbbp: 0.932173 val loss: 0.380880
[Epoch 97] ogbg-molbbbp: 0.888822 test loss: 0.546048
[Epoch 98; Iter    13/   41] train: loss: 0.0044051
[Epoch 98] ogbg-molbbbp: 0.932719 val loss: 0.432439
[Epoch 98] ogbg-molbbbp: 0.889757 test loss: 0.613874
[Epoch 99; Iter     2/   41] train: loss: 0.0041657
[Epoch 99; Iter    32/   41] train: loss: 0.0027371
[Epoch 99] ogbg-molbbbp: 0.933777 val loss: 0.396737
[Epoch 99] ogbg-molbbbp: 0.895132 test loss: 0.555689
[Epoch 100; Iter    21/   41] train: loss: 0.0122172
[Epoch 100] ogbg-molbbbp: 0.931251 val loss: 0.429809
[Epoch 100] ogbg-molbbbp: 0.894164 test loss: 0.593997
[Epoch 101; Iter    10/   41] train: loss: 0.0053096
[Epoch 101; Iter    40/   41] train: loss: 0.0015325
[Epoch 101] ogbg-molbbbp: 0.929988 val loss: 0.412671
[Epoch 101] ogbg-molbbbp: 0.882979 test loss: 0.600006
[Epoch 102; Iter    29/   41] train: loss: 0.0170680
[Epoch 102] ogbg-molbbbp: 0.928418 val loss: 0.419579
[Epoch 102] ogbg-molbbbp: 0.888021 test loss: 0.583956
[Epoch 103; Iter    18/   41] train: loss: 0.0102145
[Epoch 103] ogbg-molbbbp: 0.927462 val loss: 0.420650
[Epoch 103] ogbg-molbbbp: 0.888922 test loss: 0.584421
[Epoch 104; Iter     7/   41] train: loss: 0.0295100
[Epoch 104; Iter    37/   41] train: loss: 0.0101044
[Epoch 104] ogbg-molbbbp: 0.928418 val loss: 0.432058
[Epoch 104] ogbg-molbbbp: 0.886318 test loss: 0.589050
[Epoch 105; Iter    26/   41] train: loss: 0.0446951
[Epoch 105] ogbg-molbbbp: 0.927564 val loss: 0.427180
[Epoch 105] ogbg-molbbbp: 0.887887 test loss: 0.569681
[Epoch 106; Iter    15/   41] train: loss: 0.0023863
[Epoch 106] ogbg-molbbbp: 0.929237 val loss: 0.431719
[Epoch 106] ogbg-molbbbp: 0.884315 test loss: 0.580185
[Epoch 107; Iter     4/   41] train: loss: 0.0313095
[Epoch 107; Iter    34/   41] train: loss: 0.0020356
[Epoch 107] ogbg-molbbbp: 0.925482 val loss: 0.455868
[Epoch 107] ogbg-molbbbp: 0.879307 test loss: 0.633564
[Epoch 108; Iter    23/   41] train: loss: 0.0018825
[Epoch 108] ogbg-molbbbp: 0.923434 val loss: 0.434045
[Epoch 108] ogbg-molbbbp: 0.882746 test loss: 0.605358
[Epoch 109; Iter    12/   41] train: loss: 0.0155286
[Epoch 109] ogbg-molbbbp: 0.923434 val loss: 0.438118
[Epoch 109] ogbg-molbbbp: 0.883347 test loss: 0.609778
[Epoch 110; Iter     1/   41] train: loss: 0.0019512
[Epoch 110; Iter    31/   41] train: loss: 0.0070658
[Epoch 110] ogbg-molbbbp: 0.923980 val loss: 0.442075
[Epoch 110] ogbg-molbbbp: 0.880342 test loss: 0.633847
[Epoch 111; Iter    20/   41] train: loss: 0.0863203
[Epoch 111] ogbg-molbbbp: 0.924322 val loss: 0.434852
[Epoch 111] ogbg-molbbbp: 0.874499 test loss: 0.638729
[Epoch 112; Iter     9/   41] train: loss: 0.0534951
[Epoch 112; Iter    39/   41] train: loss: 0.0031013
[Epoch 112] ogbg-molbbbp: 0.923673 val loss: 0.452229
[Epoch 112] ogbg-molbbbp: 0.874700 test loss: 0.648816
[Epoch 113; Iter    28/   41] train: loss: 0.0018309
[Epoch 113] ogbg-molbbbp: 0.926677 val loss: 0.437538
[Epoch 113] ogbg-molbbbp: 0.881210 test loss: 0.635486
[Epoch 114; Iter    17/   41] train: loss: 0.0018407
[Epoch 114] ogbg-molbbbp: 0.926267 val loss: 0.434798
[Epoch 114] ogbg-molbbbp: 0.883881 test loss: 0.623016
[Epoch 115; Iter     6/   41] train: loss: 0.0083591
[Epoch 115; Iter    36/   41] train: loss: 0.0012063
[Epoch 115] ogbg-molbbbp: 0.927974 val loss: 0.422865
[Epoch 115] ogbg-molbbbp: 0.880976 test loss: 0.641406
[Epoch 116; Iter    25/   41] train: loss: 0.0721165
[Epoch 116] ogbg-molbbbp: 0.926165 val loss: 0.423074
[Epoch 116] ogbg-molbbbp: 0.879006 test loss: 0.635030
[Epoch 117; Iter    14/   41] train: loss: 0.0027914
[Epoch 117] ogbg-molbbbp: 0.929339 val loss: 0.453243
[Epoch 117] ogbg-molbbbp: 0.882746 test loss: 0.688790
[Epoch 118; Iter     3/   41] train: loss: 0.0017371
[Epoch 118; Iter    33/   41] train: loss: 0.0922350
[Epoch 118] ogbg-molbbbp: 0.924390 val loss: 0.487803
[Epoch 118] ogbg-molbbbp: 0.879474 test loss: 0.696221
[Epoch 119; Iter    22/   41] train: loss: 0.0014934
[Epoch 119] ogbg-molbbbp: 0.925619 val loss: 0.491417
[Epoch 119] ogbg-molbbbp: 0.877671 test loss: 0.708913
[Epoch 120; Iter    11/   41] train: loss: 0.0023955
[Epoch 120; Iter    41/   41] train: loss: 0.0020658
[Epoch 120] ogbg-molbbbp: 0.927940 val loss: 0.501813
[Epoch 120] ogbg-molbbbp: 0.883013 test loss: 0.693583
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 52.
Statistics on  val_best_checkpoint
mean_pred: 2.7903735637664795
std_pred: 3.8985090255737305
mean_targets: 0.7720588445663452
std_targets: 0.4200195074081421
prcauc: 0.9726363957447166
rocauc: 0.9344939409455539
ogbg-molbbbp: 0.9344939409455539
BCEWithLogitsLoss: 0.28193757576601847
Statistics on  test
mean_pred: 2.7106857299804688
std_pred: 3.7742629051208496
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbbbp: 0.941321 val loss: 0.325870
[Epoch 78] ogbg-molbbbp: 0.903412 test loss: 0.452100
[Epoch 79; Iter    12/   41] train: loss: 0.0230495
[Epoch 79] ogbg-molbbbp: 0.937498 val loss: 0.385769
[Epoch 79] ogbg-molbbbp: 0.906083 test loss: 0.471849
[Epoch 80; Iter     1/   41] train: loss: 0.0058792
[Epoch 80; Iter    31/   41] train: loss: 0.0093905
[Epoch 80] ogbg-molbbbp: 0.939580 val loss: 0.333379
[Epoch 80] ogbg-molbbbp: 0.894231 test loss: 0.505652
[Epoch 81; Iter    20/   41] train: loss: 0.0221824
[Epoch 81] ogbg-molbbbp: 0.934801 val loss: 0.339978
[Epoch 81] ogbg-molbbbp: 0.882612 test loss: 0.527947
[Epoch 82; Iter     9/   41] train: loss: 0.0102342
[Epoch 82; Iter    39/   41] train: loss: 0.0149529
[Epoch 82] ogbg-molbbbp: 0.935211 val loss: 0.373307
[Epoch 82] ogbg-molbbbp: 0.904814 test loss: 0.480261
[Epoch 83; Iter    28/   41] train: loss: 0.0065508
[Epoch 83] ogbg-molbbbp: 0.933675 val loss: 0.384584
[Epoch 83] ogbg-molbbbp: 0.900942 test loss: 0.497035
[Epoch 84; Iter    17/   41] train: loss: 0.0544069
[Epoch 84] ogbg-molbbbp: 0.931558 val loss: 0.404918
[Epoch 84] ogbg-molbbbp: 0.907118 test loss: 0.496138
[Epoch 85; Iter     6/   41] train: loss: 0.0838565
[Epoch 85; Iter    36/   41] train: loss: 0.0304806
[Epoch 85] ogbg-molbbbp: 0.934187 val loss: 0.379715
[Epoch 85] ogbg-molbbbp: 0.893563 test loss: 0.529558
[Epoch 86; Iter    25/   41] train: loss: 0.0197198
[Epoch 86] ogbg-molbbbp: 0.932958 val loss: 0.386328
[Epoch 86] ogbg-molbbbp: 0.891026 test loss: 0.505253
[Epoch 87; Iter    14/   41] train: loss: 0.0069115
[Epoch 87] ogbg-molbbbp: 0.932241 val loss: 0.396758
[Epoch 87] ogbg-molbbbp: 0.901509 test loss: 0.506036
[Epoch 88; Iter     3/   41] train: loss: 0.0046980
[Epoch 88; Iter    33/   41] train: loss: 0.0241601
[Epoch 88] ogbg-molbbbp: 0.940092 val loss: 0.362298
[Epoch 88] ogbg-molbbbp: 0.895333 test loss: 0.533640
[Epoch 89; Iter    22/   41] train: loss: 0.0350061
[Epoch 89] ogbg-molbbbp: 0.934835 val loss: 0.421443
[Epoch 89] ogbg-molbbbp: 0.892361 test loss: 0.572073
[Epoch 90; Iter    11/   41] train: loss: 0.0365315
[Epoch 90; Iter    41/   41] train: loss: 0.0203698
[Epoch 90] ogbg-molbbbp: 0.926062 val loss: 0.389161
[Epoch 90] ogbg-molbbbp: 0.889957 test loss: 0.547435
[Epoch 91; Iter    30/   41] train: loss: 0.0081808
[Epoch 91] ogbg-molbbbp: 0.932719 val loss: 0.415065
[Epoch 91] ogbg-molbbbp: 0.897870 test loss: 0.523748
[Epoch 92; Iter    19/   41] train: loss: 0.0138597
[Epoch 92] ogbg-molbbbp: 0.933163 val loss: 0.395291
[Epoch 92] ogbg-molbbbp: 0.899372 test loss: 0.505581
[Epoch 93; Iter     8/   41] train: loss: 0.0046614
[Epoch 93; Iter    38/   41] train: loss: 0.0198696
[Epoch 93] ogbg-molbbbp: 0.931388 val loss: 0.412313
[Epoch 93] ogbg-molbbbp: 0.894097 test loss: 0.548589
[Epoch 94; Iter    27/   41] train: loss: 0.0078052
[Epoch 94] ogbg-molbbbp: 0.926438 val loss: 0.446338
[Epoch 94] ogbg-molbbbp: 0.892428 test loss: 0.566176
[Epoch 95; Iter    16/   41] train: loss: 0.0207945
[Epoch 95] ogbg-molbbbp: 0.931627 val loss: 0.430294
[Epoch 95] ogbg-molbbbp: 0.903446 test loss: 0.512854
[Epoch 96; Iter     5/   41] train: loss: 0.0039237
[Epoch 96; Iter    35/   41] train: loss: 0.0415707
[Epoch 96] ogbg-molbbbp: 0.923980 val loss: 0.440406
[Epoch 96] ogbg-molbbbp: 0.893396 test loss: 0.573352
[Epoch 97; Iter    24/   41] train: loss: 0.0078488
[Epoch 97] ogbg-molbbbp: 0.916607 val loss: 0.849689
[Epoch 97] ogbg-molbbbp: 0.887921 test loss: 0.655740
[Epoch 98; Iter    13/   41] train: loss: 0.0047365
[Epoch 98] ogbg-molbbbp: 0.927701 val loss: 0.484335
[Epoch 98] ogbg-molbbbp: 0.896167 test loss: 0.559434
[Epoch 99; Iter     2/   41] train: loss: 0.0025854
[Epoch 99; Iter    32/   41] train: loss: 0.0252737
[Epoch 99] ogbg-molbbbp: 0.923707 val loss: 0.436704
[Epoch 99] ogbg-molbbbp: 0.892628 test loss: 0.581822
[Epoch 100; Iter    21/   41] train: loss: 0.0031159
[Epoch 100] ogbg-molbbbp: 0.929271 val loss: 0.439962
[Epoch 100] ogbg-molbbbp: 0.895166 test loss: 0.554666
[Epoch 101; Iter    10/   41] train: loss: 0.0707447
[Epoch 101; Iter    40/   41] train: loss: 0.0342919
[Epoch 101] ogbg-molbbbp: 0.929817 val loss: 0.439273
[Epoch 101] ogbg-molbbbp: 0.897369 test loss: 0.537970
[Epoch 102; Iter    29/   41] train: loss: 0.1375275
[Epoch 102] ogbg-molbbbp: 0.916914 val loss: 0.499432
[Epoch 102] ogbg-molbbbp: 0.886585 test loss: 0.608956
[Epoch 103; Iter    18/   41] train: loss: 0.0058931
[Epoch 103] ogbg-molbbbp: 0.921727 val loss: 0.456360
[Epoch 103] ogbg-molbbbp: 0.888355 test loss: 0.580161
[Epoch 104; Iter     7/   41] train: loss: 0.0103591
[Epoch 104; Iter    37/   41] train: loss: 0.0277668
[Epoch 104] ogbg-molbbbp: 0.929715 val loss: 0.433670
[Epoch 104] ogbg-molbbbp: 0.894999 test loss: 0.575776
[Epoch 105; Iter    26/   41] train: loss: 0.0048193
[Epoch 105] ogbg-molbbbp: 0.931353 val loss: 0.450618
[Epoch 105] ogbg-molbbbp: 0.895933 test loss: 0.606515
[Epoch 106; Iter    15/   41] train: loss: 0.0079735
[Epoch 106] ogbg-molbbbp: 0.930022 val loss: 0.436379
[Epoch 106] ogbg-molbbbp: 0.893429 test loss: 0.572335
[Epoch 107; Iter     4/   41] train: loss: 0.0424878
[Epoch 107; Iter    34/   41] train: loss: 0.0032606
[Epoch 107] ogbg-molbbbp: 0.930602 val loss: 0.447386
[Epoch 107] ogbg-molbbbp: 0.894798 test loss: 0.599838
[Epoch 108; Iter    23/   41] train: loss: 0.0333332
[Epoch 108] ogbg-molbbbp: 0.922410 val loss: 0.428457
[Epoch 108] ogbg-molbbbp: 0.884348 test loss: 0.583127
[Epoch 109; Iter    12/   41] train: loss: 0.0043385
[Epoch 109] ogbg-molbbbp: 0.926779 val loss: 0.450816
[Epoch 109] ogbg-molbbbp: 0.886886 test loss: 0.615641
[Epoch 110; Iter     1/   41] train: loss: 0.0056325
[Epoch 110; Iter    31/   41] train: loss: 0.0113304
[Epoch 110] ogbg-molbbbp: 0.927360 val loss: 0.476675
[Epoch 110] ogbg-molbbbp: 0.891560 test loss: 0.632155
[Epoch 111; Iter    20/   41] train: loss: 0.0034844
[Epoch 111] ogbg-molbbbp: 0.925004 val loss: 0.464805
[Epoch 111] ogbg-molbbbp: 0.888054 test loss: 0.615397
[Epoch 112; Iter     9/   41] train: loss: 0.0101084
[Epoch 112; Iter    39/   41] train: loss: 0.0019540
[Epoch 112] ogbg-molbbbp: 0.925414 val loss: 0.448431
[Epoch 112] ogbg-molbbbp: 0.887821 test loss: 0.604567
[Epoch 113; Iter    28/   41] train: loss: 0.0012860
[Epoch 113] ogbg-molbbbp: 0.926984 val loss: 0.497636
[Epoch 113] ogbg-molbbbp: 0.891193 test loss: 0.635265
[Epoch 114; Iter    17/   41] train: loss: 0.0288549
[Epoch 114] ogbg-molbbbp: 0.925824 val loss: 0.488339
[Epoch 114] ogbg-molbbbp: 0.896134 test loss: 0.616057
[Epoch 115; Iter     6/   41] train: loss: 0.0016350
[Epoch 115; Iter    36/   41] train: loss: 0.0042664
[Epoch 115] ogbg-molbbbp: 0.925994 val loss: 0.513346
[Epoch 115] ogbg-molbbbp: 0.889590 test loss: 0.628567
[Epoch 116; Iter    25/   41] train: loss: 0.0075708
[Epoch 116] ogbg-molbbbp: 0.926267 val loss: 0.486042
[Epoch 116] ogbg-molbbbp: 0.893363 test loss: 0.601375
[Epoch 117; Iter    14/   41] train: loss: 0.0043723
[Epoch 117] ogbg-molbbbp: 0.925619 val loss: 0.478196
[Epoch 117] ogbg-molbbbp: 0.886886 test loss: 0.625148
[Epoch 118; Iter     3/   41] train: loss: 0.0072433
[Epoch 118; Iter    33/   41] train: loss: 0.0825312
[Epoch 118] ogbg-molbbbp: 0.926677 val loss: 0.455151
[Epoch 118] ogbg-molbbbp: 0.886619 test loss: 0.624168
[Epoch 119; Iter    22/   41] train: loss: 0.0029869
[Epoch 119] ogbg-molbbbp: 0.923741 val loss: 0.492700
[Epoch 119] ogbg-molbbbp: 0.886719 test loss: 0.651040
[Epoch 120; Iter    11/   41] train: loss: 0.0449162
[Epoch 120; Iter    41/   41] train: loss: 0.0021577
[Epoch 120] ogbg-molbbbp: 0.925755 val loss: 0.490373
[Epoch 120] ogbg-molbbbp: 0.878005 test loss: 0.685942
[Epoch 121; Iter    30/   41] train: loss: 0.0231749
[Epoch 121] ogbg-molbbbp: 0.924287 val loss: 0.467370
[Epoch 121] ogbg-molbbbp: 0.882145 test loss: 0.618926
[Epoch 122; Iter    19/   41] train: loss: 0.0042499
[Epoch 122] ogbg-molbbbp: 0.924526 val loss: 0.471110
[Epoch 122] ogbg-molbbbp: 0.884615 test loss: 0.626056
[Epoch 123; Iter     8/   41] train: loss: 0.0290685
[Epoch 123; Iter    38/   41] train: loss: 0.0470382
[Epoch 123] ogbg-molbbbp: 0.923912 val loss: 0.467027
mean_targets: 0.7647058963775635
std_targets: 0.42470329999923706
prcauc: 0.9681483297124575
rocauc: 0.9075520833333334
ogbg-molbbbp: 0.9075520833333334
BCEWithLogitsLoss: 0.35571729711123873
Statistics on  train
mean_pred: 2.855268955230713
std_pred: 3.6999595165252686
mean_targets: 0.7628781199455261
std_targets: 0.425491601228714
prcauc: 0.9951314726567909
rocauc: 0.983882174668293
ogbg-molbbbp: 0.983882174668293
BCEWithLogitsLoss: 0.14368244228748286
[Epoch 123] ogbg-molbbbp: 0.884482 test loss: 0.719459
[Epoch 124; Iter    27/   41] train: loss: 0.0013016
[Epoch 124] ogbg-molbbbp: 0.913432 val loss: 0.782166
[Epoch 124] ogbg-molbbbp: 0.888388 test loss: 0.673807
[Epoch 125; Iter    16/   41] train: loss: 0.0016301
[Epoch 125] ogbg-molbbbp: 0.909712 val loss: 1.020468
[Epoch 125] ogbg-molbbbp: 0.879941 test loss: 0.695745
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 125 epochs. Best model checkpoint was in epoch 65.
Statistics on  val_best_checkpoint
mean_pred: 1.907967448234558
std_pred: 4.236993312835693
mean_targets: 0.7720588445663452
std_targets: 0.4200195074081421
prcauc: 0.976157659839658
rocauc: 0.9329237071172555
ogbg-molbbbp: 0.9329237071172555
BCEWithLogitsLoss: 0.33990064369780676
Statistics on  test
mean_pred: 1.844740390777588
std_pred: 4.192826271057129
mean_targets: 0.7647058963775635
std_targets: 0.42470329999923706
prcauc: 0.954237995638294
rocauc: 0.8923277243589743
ogbg-molbbbp: 0.8923277243589743
BCEWithLogitsLoss: 0.4355447164603642
Statistics on  train
mean_pred: 2.267025947570801
std_pred: 4.232906818389893
mean_targets: 0.7628781199455261
std_targets: 0.425491601228714
prcauc: 0.9964306997098201
rocauc: 0.9885870569538383
ogbg-molbbbp: 0.9885870569538383
BCEWithLogitsLoss: 0.13035934554730974
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.0735240
[Epoch 116] ogbg-molbbbp: 0.910962 val loss: 0.541285
[Epoch 116] ogbg-molbbbp: 0.902330 test loss: 0.529276
[Epoch 117; Iter    12/   48] train: loss: 0.0684465
[Epoch 117; Iter    42/   48] train: loss: 0.0339375
[Epoch 117] ogbg-molbbbp: 0.910906 val loss: 0.543718
[Epoch 117] ogbg-molbbbp: 0.890201 test loss: 0.744427
[Epoch 118; Iter    24/   48] train: loss: 0.0245533
[Epoch 118] ogbg-molbbbp: 0.911756 val loss: 0.535227
[Epoch 118] ogbg-molbbbp: 0.892499 test loss: 0.714856
[Epoch 119; Iter     6/   48] train: loss: 0.0213936
[Epoch 119; Iter    36/   48] train: loss: 0.0014283
[Epoch 119] ogbg-molbbbp: 0.904781 val loss: 0.547974
[Epoch 119] ogbg-molbbbp: 0.899138 test loss: 0.651127
[Epoch 120; Iter    18/   48] train: loss: 0.0031485
[Epoch 120; Iter    48/   48] train: loss: 0.0017456
[Epoch 120] ogbg-molbbbp: 0.910055 val loss: 0.527362
[Epoch 120] ogbg-molbbbp: 0.894670 test loss: 0.653689
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 45.
Statistics on  val_best_checkpoint
mean_pred: 2.2298755645751953
std_pred: 3.4850857257843018
mean_targets: 0.7483659982681274
std_targets: 0.434662789106369
prcauc: 0.9688706323609363
rocauc: 0.9304712754494413
ogbg-molbbbp: 0.9304712754494413
BCEWithLogitsLoss: 0.29542695510793815
Statistics on  test
mean_pred: 2.4706037044525146
std_pred: 3.3762786388397217
mean_targets: 0.7875816822052002
std_targets: 0.4096892774105072
prcauc: 0.971659972507425
rocauc: 0.9131184168528567
ogbg-molbbbp: 0.9131184168528567
BCEWithLogitsLoss: 0.3210054757920178
Statistics on  train
mean_pred: 2.3642220497131348
std_pred: 3.387230157852173
mean_targets: 0.7638401985168457
std_targets: 0.42487040162086487
prcauc: 0.9919407719834228
rocauc: 0.9757112133503935
ogbg-molbbbp: 0.9757112133503935
BCEWithLogitsLoss: 0.167267680944254
[Epoch 109] ogbg-molbbbp: 0.916790 val loss: 0.578096
[Epoch 109] ogbg-molbbbp: 0.906232 test loss: 0.683489
[Epoch 110; Iter     5/   55] train: loss: 0.0060897
[Epoch 110; Iter    35/   55] train: loss: 0.0024501
[Epoch 110] ogbg-molbbbp: 0.928519 val loss: 0.509677
[Epoch 110] ogbg-molbbbp: 0.908436 test loss: 0.573819
[Epoch 111; Iter    10/   55] train: loss: 0.0028797
[Epoch 111; Iter    40/   55] train: loss: 0.0465710
[Epoch 111] ogbg-molbbbp: 0.918025 val loss: 0.552046
[Epoch 111] ogbg-molbbbp: 0.903145 test loss: 0.622427
[Epoch 112; Iter    15/   55] train: loss: 0.0970587
[Epoch 112; Iter    45/   55] train: loss: 0.0042120
[Epoch 112] ogbg-molbbbp: 0.921605 val loss: 0.537031
[Epoch 112] ogbg-molbbbp: 0.905644 test loss: 0.600028
[Epoch 113; Iter    20/   55] train: loss: 0.0168014
[Epoch 113; Iter    50/   55] train: loss: 0.0170547
[Epoch 113] ogbg-molbbbp: 0.918395 val loss: 0.531780
[Epoch 113] ogbg-molbbbp: 0.905791 test loss: 0.564404
[Epoch 114; Iter    25/   55] train: loss: 0.0019202
[Epoch 114; Iter    55/   55] train: loss: 0.1555373
[Epoch 114] ogbg-molbbbp: 0.914074 val loss: 0.575060
[Epoch 114] ogbg-molbbbp: 0.883451 test loss: 0.734339
[Epoch 115; Iter    30/   55] train: loss: 0.0058090
[Epoch 115] ogbg-molbbbp: 0.925185 val loss: 0.515365
[Epoch 115] ogbg-molbbbp: 0.886831 test loss: 0.705797
[Epoch 116; Iter     5/   55] train: loss: 0.0306172
[Epoch 116; Iter    35/   55] train: loss: 0.0087956
[Epoch 116] ogbg-molbbbp: 0.912346 val loss: 0.582774
[Epoch 116] ogbg-molbbbp: 0.897560 test loss: 0.617488
[Epoch 117; Iter    10/   55] train: loss: 0.0036522
[Epoch 117; Iter    40/   55] train: loss: 0.0034425
[Epoch 117] ogbg-molbbbp: 0.932346 val loss: 0.516332
[Epoch 117] ogbg-molbbbp: 0.894915 test loss: 0.661728
[Epoch 118; Iter    15/   55] train: loss: 0.0043178
[Epoch 118; Iter    45/   55] train: loss: 0.0073791
[Epoch 118] ogbg-molbbbp: 0.924938 val loss: 0.562635
[Epoch 118] ogbg-molbbbp: 0.904468 test loss: 0.668217
[Epoch 119; Iter    20/   55] train: loss: 0.0026748
[Epoch 119; Iter    50/   55] train: loss: 0.0088001
[Epoch 119] ogbg-molbbbp: 0.926173 val loss: 0.535576
[Epoch 119] ogbg-molbbbp: 0.907995 test loss: 0.649343
[Epoch 120; Iter    25/   55] train: loss: 0.0013154
[Epoch 120; Iter    55/   55] train: loss: 0.0006732
[Epoch 120] ogbg-molbbbp: 0.926914 val loss: 0.555250
[Epoch 120] ogbg-molbbbp: 0.908436 test loss: 0.676699
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 39.
Statistics on  val_best_checkpoint
mean_pred: 1.7554771900177002
std_pred: 3.0697107315063477
mean_targets: 0.7352941632270813
std_targets: 0.44226178526878357
prcauc: 0.9796314940441659
rocauc: 0.9433333333333334
ogbg-molbbbp: 0.9433333333333334
BCEWithLogitsLoss: 0.29974242406232016
Statistics on  test
mean_pred: 2.001836061477661
std_pred: 3.2454278469085693
mean_targets: 0.7941176891326904
std_targets: 0.40533962845802307
prcauc: 0.967057127601402
rocauc: 0.8957965902410346
ogbg-molbbbp: 0.8957965902410346
BCEWithLogitsLoss: 0.336348437837192
Statistics on  train
mean_pred: 1.995871663093567
std_pred: 3.106961250305176
mean_targets: 0.7651747465133667
std_targets: 0.42401954531669617
prcauc: 0.9895134152993351
rocauc: 0.9689016787172792
ogbg-molbbbp: 0.9689016787172792
BCEWithLogitsLoss: 0.19419498646801167
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.916543 val loss: 0.528898
[Epoch 109] ogbg-molbbbp: 0.894915 test loss: 0.597721
[Epoch 110; Iter     5/   55] train: loss: 0.0012270
[Epoch 110; Iter    35/   55] train: loss: 0.0073744
[Epoch 110] ogbg-molbbbp: 0.917284 val loss: 0.542487
[Epoch 110] ogbg-molbbbp: 0.896531 test loss: 0.603398
[Epoch 111; Iter    10/   55] train: loss: 0.0014354
[Epoch 111; Iter    40/   55] train: loss: 0.0055114
[Epoch 111] ogbg-molbbbp: 0.908395 val loss: 0.567119
[Epoch 111] ogbg-molbbbp: 0.875367 test loss: 0.644193
[Epoch 112; Iter    15/   55] train: loss: 0.0109413
[Epoch 112; Iter    45/   55] train: loss: 0.0156182
[Epoch 112] ogbg-molbbbp: 0.918148 val loss: 0.540513
[Epoch 112] ogbg-molbbbp: 0.894327 test loss: 0.609840
[Epoch 113; Iter    20/   55] train: loss: 0.0651352
[Epoch 113; Iter    50/   55] train: loss: 0.0598707
[Epoch 113] ogbg-molbbbp: 0.922963 val loss: 0.521284
[Epoch 113] ogbg-molbbbp: 0.890359 test loss: 0.573519
[Epoch 114; Iter    25/   55] train: loss: 0.0448698
[Epoch 114; Iter    55/   55] train: loss: 0.0145612
[Epoch 114] ogbg-molbbbp: 0.925802 val loss: 0.512765
[Epoch 114] ogbg-molbbbp: 0.893739 test loss: 0.582774
[Epoch 115; Iter    30/   55] train: loss: 0.0097815
[Epoch 115] ogbg-molbbbp: 0.922469 val loss: 0.558062
[Epoch 115] ogbg-molbbbp: 0.895209 test loss: 0.614000
[Epoch 116; Iter     5/   55] train: loss: 0.0014986
[Epoch 116; Iter    35/   55] train: loss: 0.0684223
[Epoch 116] ogbg-molbbbp: 0.920988 val loss: 0.570371
[Epoch 116] ogbg-molbbbp: 0.896678 test loss: 0.605513
[Epoch 117; Iter    10/   55] train: loss: 0.0028015
[Epoch 117; Iter    40/   55] train: loss: 0.0464538
[Epoch 117] ogbg-molbbbp: 0.923951 val loss: 0.507409
[Epoch 117] ogbg-molbbbp: 0.898148 test loss: 0.541715
[Epoch 118; Iter    15/   55] train: loss: 0.1448757
[Epoch 118; Iter    45/   55] train: loss: 0.0291786
[Epoch 118] ogbg-molbbbp: 0.910864 val loss: 0.597501
[Epoch 118] ogbg-molbbbp: 0.887713 test loss: 0.626931
[Epoch 119; Iter    20/   55] train: loss: 0.0030708
[Epoch 119; Iter    50/   55] train: loss: 0.0059700
[Epoch 119] ogbg-molbbbp: 0.921111 val loss: 0.568834
[Epoch 119] ogbg-molbbbp: 0.888301 test loss: 0.622327
[Epoch 120; Iter    25/   55] train: loss: 0.0244807
[Epoch 120; Iter    55/   55] train: loss: 0.0016680
[Epoch 120] ogbg-molbbbp: 0.912222 val loss: 0.606072
[Epoch 120] ogbg-molbbbp: 0.887125 test loss: 0.648960
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 36.
Statistics on  val_best_checkpoint
mean_pred: 1.3179665803909302
std_pred: 3.309342622756958
mean_targets: 0.7352941632270813
std_targets: 0.44226178526878357
prcauc: 0.979751585361305
rocauc: 0.948395061728395
ogbg-molbbbp: 0.948395061728395
BCEWithLogitsLoss: 0.2665510560785021
Statistics on  test
mean_pred: 0.06465104222297668
std_pred: 21.81636619567871
mean_targets: 0.7941176891326904
std_targets: 0.40533962845802307
prcauc: 0.9661053771722761
rocauc: 0.8987360376249265
ogbg-molbbbp: 0.8987360376249265
BCEWithLogitsLoss: 0.36554051084177835
Statistics on  train
mean_pred: 1.5260876417160034
std_pred: 3.214700698852539
mean_targets: 0.7651747465133667
std_targets: 0.42401954531669617
prcauc: 0.9879247565719155
rocauc: 0.9661474024235122
ogbg-molbbbp: 0.9661474024235122
BCEWithLogitsLoss: 0.21675700653683055
[Epoch 116; Iter    30/   48] train: loss: 0.0013296
[Epoch 116] ogbg-molbbbp: 0.915783 val loss: 0.508581
[Epoch 116] ogbg-molbbbp: 0.892818 test loss: 0.667429
[Epoch 117; Iter    12/   48] train: loss: 0.0017353
[Epoch 117; Iter    42/   48] train: loss: 0.0016833
[Epoch 117] ogbg-molbbbp: 0.909601 val loss: 0.525817
[Epoch 117] ogbg-molbbbp: 0.890329 test loss: 0.708536
[Epoch 118; Iter    24/   48] train: loss: 0.0191715
[Epoch 118] ogbg-molbbbp: 0.911303 val loss: 0.525465
[Epoch 118] ogbg-molbbbp: 0.891286 test loss: 0.735661
[Epoch 119; Iter     6/   48] train: loss: 0.0047941
[Epoch 119; Iter    36/   48] train: loss: 0.0032062
[Epoch 119] ogbg-molbbbp: 0.910906 val loss: 0.527245
[Epoch 119] ogbg-molbbbp: 0.891478 test loss: 0.709486
[Epoch 120; Iter    18/   48] train: loss: 0.0008428
[Epoch 120; Iter    48/   48] train: loss: 0.0023520
[Epoch 120] ogbg-molbbbp: 0.908581 val loss: 0.536166
[Epoch 120] ogbg-molbbbp: 0.886307 test loss: 0.755117
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 37.
Statistics on  val_best_checkpoint
mean_pred: 2.4353020191192627
std_pred: 3.043142318725586
mean_targets: 0.7483659982681274
std_targets: 0.434662789106369
prcauc: 0.9720551248116969
rocauc: 0.9258776158339477
ogbg-molbbbp: 0.9258776158339477
BCEWithLogitsLoss: 0.2876175838437947
Statistics on  test
mean_pred: 2.7116646766662598
std_pred: 2.820148468017578
mean_targets: 0.7875816822052002
std_targets: 0.4096892774105072
prcauc: 0.9774873549553331
rocauc: 0.9207149696776252
ogbg-molbbbp: 0.9207149696776252
BCEWithLogitsLoss: 0.27551686560565775
Statistics on  train
mean_pred: 2.6396801471710205
std_pred: 2.9395086765289307
mean_targets: 0.7638401985168457
std_targets: 0.4248703718185425
prcauc: 0.9882578369827029
rocauc: 0.9670527863229249
ogbg-molbbbp: 0.9670527863229249
BCEWithLogitsLoss: 0.19698102384184799
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbbbp: 0.880709 test loss: 0.642673
[Epoch 124; Iter    27/   41] train: loss: 0.0426415
[Epoch 124] ogbg-molbbbp: 0.926677 val loss: 0.510733
[Epoch 124] ogbg-molbbbp: 0.881878 test loss: 0.668865
[Epoch 125; Iter    16/   41] train: loss: 0.0013098
[Epoch 125] ogbg-molbbbp: 0.926813 val loss: 0.506610
[Epoch 125] ogbg-molbbbp: 0.881010 test loss: 0.663800
[Epoch 126; Iter     5/   41] train: loss: 0.0671418
[Epoch 126; Iter    35/   41] train: loss: 0.0263758
[Epoch 126] ogbg-molbbbp: 0.925346 val loss: 0.483295
[Epoch 126] ogbg-molbbbp: 0.881644 test loss: 0.661374
[Epoch 127; Iter    24/   41] train: loss: 0.0008785
[Epoch 127] ogbg-molbbbp: 0.919474 val loss: 0.485163
[Epoch 127] ogbg-molbbbp: 0.871227 test loss: 0.715784
[Epoch 128; Iter    13/   41] train: loss: 0.0014000
[Epoch 128] ogbg-molbbbp: 0.915037 val loss: 0.509414
[Epoch 128] ogbg-molbbbp: 0.877037 test loss: 0.675506
[Epoch 129; Iter     2/   41] train: loss: 0.0014613
[Epoch 129; Iter    32/   41] train: loss: 0.0216181
[Epoch 129] ogbg-molbbbp: 0.919440 val loss: 0.510123
[Epoch 129] ogbg-molbbbp: 0.878005 test loss: 0.697040
[Epoch 130; Iter    21/   41] train: loss: 0.0011806
[Epoch 130] ogbg-molbbbp: 0.922376 val loss: 0.498031
[Epoch 130] ogbg-molbbbp: 0.879274 test loss: 0.681687
[Epoch 131; Iter    10/   41] train: loss: 0.0008557
[Epoch 131; Iter    40/   41] train: loss: 0.0007701
[Epoch 131] ogbg-molbbbp: 0.923127 val loss: 0.510826
[Epoch 131] ogbg-molbbbp: 0.878138 test loss: 0.708834
[Epoch 132; Iter    29/   41] train: loss: 0.0013911
[Epoch 132] ogbg-molbbbp: 0.924731 val loss: 0.499136
[Epoch 132] ogbg-molbbbp: 0.878205 test loss: 0.701167
[Epoch 133; Iter    18/   41] train: loss: 0.0740512
[Epoch 133] ogbg-molbbbp: 0.918962 val loss: 0.518488
[Epoch 133] ogbg-molbbbp: 0.872463 test loss: 0.711910
[Epoch 134; Iter     7/   41] train: loss: 0.0021891
[Epoch 134; Iter    37/   41] train: loss: 0.0006960
[Epoch 134] ogbg-molbbbp: 0.921522 val loss: 0.516285
[Epoch 134] ogbg-molbbbp: 0.871461 test loss: 0.722776
[Epoch 135; Iter    26/   41] train: loss: 0.0037843
[Epoch 135] ogbg-molbbbp: 0.922000 val loss: 0.522781
[Epoch 135] ogbg-molbbbp: 0.878739 test loss: 0.721202
[Epoch 136; Iter    15/   41] train: loss: 0.0007092
[Epoch 136] ogbg-molbbbp: 0.923127 val loss: 0.487819
[Epoch 136] ogbg-molbbbp: 0.877604 test loss: 0.688069
[Epoch 137; Iter     4/   41] train: loss: 0.0004903
[Epoch 137; Iter    34/   41] train: loss: 0.0020266
[Epoch 137] ogbg-molbbbp: 0.921796 val loss: 0.514859
[Epoch 137] ogbg-molbbbp: 0.877971 test loss: 0.717757
[Epoch 138; Iter    23/   41] train: loss: 0.0013910
[Epoch 138] ogbg-molbbbp: 0.923400 val loss: 0.502921
[Epoch 138] ogbg-molbbbp: 0.879340 test loss: 0.718782
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 138 epochs. Best model checkpoint was in epoch 78.
Statistics on  val_best_checkpoint
mean_pred: 3.977708578109741
std_pred: 5.413637161254883
mean_targets: 0.7720588445663452
std_targets: 0.4200195074081421
prcauc: 0.9752879045251404
rocauc: 0.941321044546851
ogbg-molbbbp: 0.941321044546851
BCEWithLogitsLoss: 0.32586984150111675
Statistics on  test
mean_pred: 3.771012306213379
std_pred: 5.334135055541992
mean_targets: 0.7647058963775635
std_targets: 0.42470329999923706
prcauc: 0.9587448923485945
rocauc: 0.9034121260683761
ogbg-molbbbp: 0.9034121260683761
BCEWithLogitsLoss: 0.4520996653341821
Statistics on  train
mean_pred: 4.065442085266113
std_pred: 5.438073635101318
mean_targets: 0.7628781199455261
std_targets: 0.425491601228714
prcauc: 0.9996842005827495
rocauc: 0.9987286099715417
ogbg-molbbbp: 0.9987286099715417
BCEWithLogitsLoss: 0.03591134191331703
All runs completed.
[Epoch 109] ogbg-molbbbp: 0.917407 val loss: 0.594148
[Epoch 109] ogbg-molbbbp: 0.905203 test loss: 0.729038
[Epoch 110; Iter     5/   55] train: loss: 0.0053456
[Epoch 110; Iter    35/   55] train: loss: 0.0016699
[Epoch 110] ogbg-molbbbp: 0.920247 val loss: 0.570249
[Epoch 110] ogbg-molbbbp: 0.904321 test loss: 0.648057
[Epoch 111; Iter    10/   55] train: loss: 0.0026210
[Epoch 111; Iter    40/   55] train: loss: 0.0028103
[Epoch 111] ogbg-molbbbp: 0.913827 val loss: 0.590530
[Epoch 111] ogbg-molbbbp: 0.907848 test loss: 0.600998
[Epoch 112; Iter    15/   55] train: loss: 0.0023857
[Epoch 112; Iter    45/   55] train: loss: 0.0919984
[Epoch 112] ogbg-molbbbp: 0.911111 val loss: 0.635442
[Epoch 112] ogbg-molbbbp: 0.895209 test loss: 0.770603
[Epoch 113; Iter    20/   55] train: loss: 0.0115335
[Epoch 113; Iter    50/   55] train: loss: 0.0129151
[Epoch 113] ogbg-molbbbp: 0.923457 val loss: 0.603008
[Epoch 113] ogbg-molbbbp: 0.904174 test loss: 0.705926
[Epoch 114; Iter    25/   55] train: loss: 0.0117808
[Epoch 114; Iter    55/   55] train: loss: 0.0241033
[Epoch 114] ogbg-molbbbp: 0.916790 val loss: 0.562010
[Epoch 114] ogbg-molbbbp: 0.903586 test loss: 0.597287
[Epoch 115; Iter    30/   55] train: loss: 0.0016638
[Epoch 115] ogbg-molbbbp: 0.909506 val loss: 0.592302
[Epoch 115] ogbg-molbbbp: 0.892416 test loss: 0.669972
[Epoch 116; Iter     5/   55] train: loss: 0.0181253
[Epoch 116; Iter    35/   55] train: loss: 0.1351891
[Epoch 116] ogbg-molbbbp: 0.911235 val loss: 0.643245
[Epoch 116] ogbg-molbbbp: 0.870223 test loss: 0.706333
[Epoch 117; Iter    10/   55] train: loss: 0.0452843
[Epoch 117; Iter    40/   55] train: loss: 0.0696170
[Epoch 117] ogbg-molbbbp: 0.920370 val loss: 0.584285
[Epoch 117] ogbg-molbbbp: 0.894621 test loss: 0.692706
[Epoch 118; Iter    15/   55] train: loss: 0.0049628
[Epoch 118; Iter    45/   55] train: loss: 0.0753913
[Epoch 118] ogbg-molbbbp: 0.922716 val loss: 0.578973
[Epoch 118] ogbg-molbbbp: 0.900647 test loss: 0.670295
[Epoch 119; Iter    20/   55] train: loss: 0.0612806
[Epoch 119; Iter    50/   55] train: loss: 0.0040124
[Epoch 119] ogbg-molbbbp: 0.922222 val loss: 0.580642
[Epoch 119] ogbg-molbbbp: 0.901382 test loss: 0.658486
[Epoch 120; Iter    25/   55] train: loss: 0.0011012
[Epoch 120; Iter    55/   55] train: loss: 0.1527038
[Epoch 120] ogbg-molbbbp: 0.916173 val loss: 0.622820
[Epoch 120] ogbg-molbbbp: 0.901235 test loss: 0.685041
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 40.
Statistics on  val_best_checkpoint
mean_pred: 2.7276628017425537
std_pred: 3.2532126903533936
mean_targets: 0.7352941632270813
std_targets: 0.44226178526878357
prcauc: 0.9808268005057075
rocauc: 0.9460493827160493
ogbg-molbbbp: 0.9460493827160493
BCEWithLogitsLoss: 0.28960834230695454
Statistics on  test
mean_pred: 2.923858642578125
std_pred: 3.273164987564087
mean_targets: 0.7941176891326904
std_targets: 0.40533962845802307
prcauc: 0.9775600452521203
rocauc: 0.9226925338036449
ogbg-molbbbp: 0.9226925338036449
BCEWithLogitsLoss: 0.2808653233306749
Statistics on  train
mean_pred: 2.887225389480591
std_pred: 3.2009150981903076
mean_targets: 0.7651747465133667
std_targets: 0.42401960492134094
prcauc: 0.9908802683873571
rocauc: 0.9730670901118029
ogbg-molbbbp: 0.9730670901118029
BCEWithLogitsLoss: 0.18555674342946574
All runs completed.
[Epoch 116; Iter    30/   48] train: loss: 0.0024782
[Epoch 116] ogbg-molbbbp: 0.913968 val loss: 0.505150
[Epoch 116] ogbg-molbbbp: 0.887839 test loss: 0.669838
[Epoch 117; Iter    12/   48] train: loss: 0.0036100
[Epoch 117; Iter    42/   48] train: loss: 0.0068062
[Epoch 117] ogbg-molbbbp: 0.914649 val loss: 0.512665
[Epoch 117] ogbg-molbbbp: 0.883051 test loss: 0.724664
[Epoch 118; Iter    24/   48] train: loss: 0.0662906
[Epoch 118] ogbg-molbbbp: 0.912097 val loss: 0.537551
[Epoch 118] ogbg-molbbbp: 0.889627 test loss: 0.704184
[Epoch 119; Iter     6/   48] train: loss: 0.0022404
[Epoch 119; Iter    36/   48] train: loss: 0.1681648
[Epoch 119] ogbg-molbbbp: 0.909091 val loss: 0.477219
[Epoch 119] ogbg-molbbbp: 0.870348 test loss: 0.641252
[Epoch 120; Iter    18/   48] train: loss: 0.0088146
[Epoch 120; Iter    48/   48] train: loss: 0.0228167
[Epoch 120] ogbg-molbbbp: 0.903533 val loss: 0.641251
[Epoch 120] ogbg-molbbbp: 0.886307 test loss: 0.731983
[Epoch 121; Iter    30/   48] train: loss: 0.0070672
[Epoch 121] ogbg-molbbbp: 0.903079 val loss: 0.695136
[Epoch 121] ogbg-molbbbp: 0.885158 test loss: 0.746771
[Epoch 122; Iter    12/   48] train: loss: 0.0024285
[Epoch 122; Iter    42/   48] train: loss: 0.0014434
[Epoch 122] ogbg-molbbbp: 0.905802 val loss: 0.575033
[Epoch 122] ogbg-molbbbp: 0.879732 test loss: 0.763803
[Epoch 123; Iter    24/   48] train: loss: 0.0024828
[Epoch 123] ogbg-molbbbp: 0.909828 val loss: 0.501054
[Epoch 123] ogbg-molbbbp: 0.877051 test loss: 0.729227
[Epoch 124; Iter     6/   48] train: loss: 0.0537650
[Epoch 124; Iter    36/   48] train: loss: 0.0008184
[Epoch 124] ogbg-molbbbp: 0.910055 val loss: 0.519901
[Epoch 124] ogbg-molbbbp: 0.878711 test loss: 0.767331
[Epoch 125; Iter    18/   48] train: loss: 0.1250516
[Epoch 125; Iter    48/   48] train: loss: 0.0157824
[Epoch 125] ogbg-molbbbp: 0.912210 val loss: 0.511934
[Epoch 125] ogbg-molbbbp: 0.876221 test loss: 0.770029
[Epoch 126; Iter    30/   48] train: loss: 0.0366484
[Epoch 126] ogbg-molbbbp: 0.908864 val loss: 0.535741
[Epoch 126] ogbg-molbbbp: 0.876859 test loss: 0.716086
[Epoch 127; Iter    12/   48] train: loss: 0.0169798
[Epoch 127; Iter    42/   48] train: loss: 0.0042921
[Epoch 127] ogbg-molbbbp: 0.906085 val loss: 0.584814
[Epoch 127] ogbg-molbbbp: 0.877051 test loss: 0.744050
[Epoch 128; Iter    24/   48] train: loss: 0.1221102
[Epoch 128] ogbg-molbbbp: 0.908013 val loss: 0.526238
[Epoch 128] ogbg-molbbbp: 0.874433 test loss: 0.776483
[Epoch 129; Iter     6/   48] train: loss: 0.0016213
[Epoch 129; Iter    36/   48] train: loss: 0.0018167
[Epoch 129] ogbg-molbbbp: 0.912380 val loss: 0.504823
[Epoch 129] ogbg-molbbbp: 0.875263 test loss: 0.753328
[Epoch 130; Iter    18/   48] train: loss: 0.0494260
[Epoch 130; Iter    48/   48] train: loss: 0.0098357
[Epoch 130] ogbg-molbbbp: 0.907843 val loss: 0.519493
[Epoch 130] ogbg-molbbbp: 0.868944 test loss: 0.756354
[Epoch 131; Iter    30/   48] train: loss: 0.0094143
[Epoch 131] ogbg-molbbbp: 0.905461 val loss: 0.570792
[Epoch 131] ogbg-molbbbp: 0.874753 test loss: 0.802504
[Epoch 132; Iter    12/   48] train: loss: 0.0008034
[Epoch 132; Iter    42/   48] train: loss: 0.0149483
[Epoch 132] ogbg-molbbbp: 0.906482 val loss: 0.583723
[Epoch 132] ogbg-molbbbp: 0.876412 test loss: 0.754826
[Epoch 133; Iter    24/   48] train: loss: 0.0580355
[Epoch 133] ogbg-molbbbp: 0.906369 val loss: 0.614947
[Epoch 133] ogbg-molbbbp: 0.873923 test loss: 0.813213
[Epoch 134; Iter     6/   48] train: loss: 0.0164048
[Epoch 134; Iter    36/   48] train: loss: 0.1566618
[Epoch 134] ogbg-molbbbp: 0.905915 val loss: 0.606147
[Epoch 134] ogbg-molbbbp: 0.877944 test loss: 0.755599
[Epoch 135; Iter    18/   48] train: loss: 0.0009950
[Epoch 135; Iter    48/   48] train: loss: 0.0003506
[Epoch 135] ogbg-molbbbp: 0.908921 val loss: 0.530019
[Epoch 135] ogbg-molbbbp: 0.877370 test loss: 0.759922
[Epoch 136; Iter    30/   48] train: loss: 0.0053010
[Epoch 136] ogbg-molbbbp: 0.905008 val loss: 0.556960
[Epoch 136] ogbg-molbbbp: 0.875455 test loss: 0.761464
[Epoch 137; Iter    12/   48] train: loss: 0.0021334
[Epoch 137; Iter    42/   48] train: loss: 0.0058055
[Epoch 137] ogbg-molbbbp: 0.909998 val loss: 0.525872
[Epoch 137] ogbg-molbbbp: 0.878455 test loss: 0.781164
[Epoch 138; Iter    24/   48] train: loss: 0.0387980
[Epoch 138] ogbg-molbbbp: 0.910339 val loss: 0.518296
[Epoch 138] ogbg-molbbbp: 0.883690 test loss: 0.750505
[Epoch 139; Iter     6/   48] train: loss: 0.0936384
[Epoch 139; Iter    36/   48] train: loss: 0.0039873
[Epoch 139] ogbg-molbbbp: 0.911813 val loss: 0.492236
[Epoch 139] ogbg-molbbbp: 0.883115 test loss: 0.670806
[Epoch 140; Iter    18/   48] train: loss: 0.0008739
[Epoch 140; Iter    48/   48] train: loss: 0.0023552
[Epoch 140] ogbg-molbbbp: 0.912097 val loss: 0.518753
[Epoch 140] ogbg-molbbbp: 0.884775 test loss: 0.733290
[Epoch 141; Iter    30/   48] train: loss: 0.0171328
[Epoch 141] ogbg-molbbbp: 0.910055 val loss: 0.564135
[Epoch 141] ogbg-molbbbp: 0.888924 test loss: 0.721817
[Epoch 142; Iter    12/   48] train: loss: 0.0378649
[Epoch 142; Iter    42/   48] train: loss: 0.0408574
[Epoch 142] ogbg-molbbbp: 0.908581 val loss: 0.506362
[Epoch 142] ogbg-molbbbp: 0.884839 test loss: 0.723503
[Epoch 143; Iter    24/   48] train: loss: 0.0078412
[Epoch 143] ogbg-molbbbp: 0.911416 val loss: 0.510885
[Epoch 143] ogbg-molbbbp: 0.880562 test loss: 0.767402
[Epoch 144; Iter     6/   48] train: loss: 0.0127619
[Epoch 144; Iter    36/   48] train: loss: 0.0156930
[Epoch 144] ogbg-molbbbp: 0.912323 val loss: 0.505087
[Epoch 144] ogbg-molbbbp: 0.883945 test loss: 0.738066
[Epoch 145; Iter    18/   48] train: loss: 0.0045314
[Epoch 145; Iter    48/   48] train: loss: 0.0001565
[Epoch 145] ogbg-molbbbp: 0.910736 val loss: 0.512277
[Epoch 145] ogbg-molbbbp: 0.891350 test loss: 0.698647
[Epoch 146; Iter    30/   48] train: loss: 0.0011411
[Epoch 146] ogbg-molbbbp: 0.904838 val loss: 0.527663
[Epoch 146] ogbg-molbbbp: 0.878264 test loss: 0.720411
[Epoch 147; Iter    12/   48] train: loss: 0.0017427
[Epoch 147; Iter    42/   48] train: loss: 0.0034044
[Epoch 147] ogbg-molbbbp: 0.904781 val loss: 0.515756
[Epoch 147] ogbg-molbbbp: 0.882030 test loss: 0.676511
[Epoch 148; Iter    24/   48] train: loss: 0.0009732
[Epoch 148] ogbg-molbbbp: 0.905178 val loss: 0.529416
[Epoch 148] ogbg-molbbbp: 0.884966 test loss: 0.705200
[Epoch 149; Iter     6/   48] train: loss: 0.0013919
[Epoch 149; Iter    36/   48] train: loss: 0.0799268
[Epoch 149] ogbg-molbbbp: 0.902626 val loss: 0.587092
[Epoch 149] ogbg-molbbbp: 0.881072 test loss: 0.735425
[Epoch 150; Iter    18/   48] train: loss: 0.0043556
[Epoch 150; Iter    48/   48] train: loss: 0.4196097
[Epoch 150] ogbg-molbbbp: 0.902285 val loss: 0.621504
[Epoch 150] ogbg-molbbbp: 0.882668 test loss: 0.773523
[Epoch 151; Iter    30/   48] train: loss: 0.0185083
[Epoch 151] ogbg-molbbbp: 0.908127 val loss: 0.517875
[Epoch 151] ogbg-molbbbp: 0.882285 test loss: 0.731380
[Epoch 152; Iter    12/   48] train: loss: 0.0102352
[Epoch 152; Iter    42/   48] train: loss: 0.0007664
[Epoch 152] ogbg-molbbbp: 0.909488 val loss: 0.533107
[Epoch 152] ogbg-molbbbp: 0.887009 test loss: 0.757231
[Epoch 153; Iter    24/   48] train: loss: 0.0076461
[Epoch 153] ogbg-molbbbp: 0.908354 val loss: 0.528481
[Epoch 153] ogbg-molbbbp: 0.887711 test loss: 0.735146
[Epoch 154; Iter     6/   48] train: loss: 0.0256791
[Epoch 154; Iter    36/   48] train: loss: 0.0006658
[Epoch 154] ogbg-molbbbp: 0.907616 val loss: 0.535942
[Epoch 154] ogbg-molbbbp: 0.882030 test loss: 0.753855
[Epoch 155; Iter    18/   48] train: loss: 0.0014441
[Epoch 155; Iter    48/   48] train: loss: 0.0024507
[Epoch 155] ogbg-molbbbp: 0.905858 val loss: 0.525559
[Epoch 155] ogbg-molbbbp: 0.887711 test loss: 0.712840
[Epoch 156; Iter    30/   48] train: loss: 0.0007655
[Epoch 156] ogbg-molbbbp: 0.905802 val loss: 0.545363
[Epoch 156] ogbg-molbbbp: 0.888286 test loss: 0.762465
[Epoch 157; Iter    12/   48] train: loss: 0.0012378
[Epoch 157; Iter    42/   48] train: loss: 0.0319119
[Epoch 157] ogbg-molbbbp: 0.906255 val loss: 0.534923
[Epoch 157] ogbg-molbbbp: 0.884775 test loss: 0.743576
[Epoch 158; Iter    24/   48] train: loss: 0.0017062
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 158] ogbg-molbbbp: 0.907673 val loss: 0.535352
[Epoch 158] ogbg-molbbbp: 0.887711 test loss: 0.723519
[Epoch 159; Iter     6/   48] train: loss: 0.0032981
[Epoch 159; Iter    36/   48] train: loss: 0.0011522
[Epoch 159] ogbg-molbbbp: 0.907843 val loss: 0.548735
[Epoch 159] ogbg-molbbbp: 0.887839 test loss: 0.758993
[Epoch 160; Iter    18/   48] train: loss: 0.0007587
[Epoch 160; Iter    48/   48] train: loss: 0.0005737
[Epoch 160] ogbg-molbbbp: 0.905745 val loss: 0.576788
[Epoch 160] ogbg-molbbbp: 0.887328 test loss: 0.805870
[Epoch 161; Iter    30/   48] train: loss: 0.0016064
[Epoch 161] ogbg-molbbbp: 0.907276 val loss: 0.524796
[Epoch 161] ogbg-molbbbp: 0.885669 test loss: 0.700670
[Epoch 162; Iter    12/   48] train: loss: 0.0235406
[Epoch 162; Iter    42/   48] train: loss: 0.0023819
[Epoch 162] ogbg-molbbbp: 0.908184 val loss: 0.542244
[Epoch 162] ogbg-molbbbp: 0.883754 test loss: 0.748217
[Epoch 163; Iter    24/   48] train: loss: 0.0499567
[Epoch 163] ogbg-molbbbp: 0.906028 val loss: 0.564066
[Epoch 163] ogbg-molbbbp: 0.880434 test loss: 0.785510
[Epoch 164; Iter     6/   48] train: loss: 0.0013509
[Epoch 164; Iter    36/   48] train: loss: 0.0006789
[Epoch 164] ogbg-molbbbp: 0.908127 val loss: 0.556924
[Epoch 164] ogbg-molbbbp: 0.882349 test loss: 0.814347
[Epoch 165; Iter    18/   48] train: loss: 0.0027997
[Epoch 165; Iter    48/   48] train: loss: 0.0003913
[Epoch 165] ogbg-molbbbp: 0.907616 val loss: 0.541707
[Epoch 165] ogbg-molbbbp: 0.880243 test loss: 0.755214
[Epoch 166; Iter    30/   48] train: loss: 0.0261166
[Epoch 166] ogbg-molbbbp: 0.905688 val loss: 0.541809
[Epoch 166] ogbg-molbbbp: 0.874689 test loss: 0.748600
[Epoch 167; Iter    12/   48] train: loss: 0.0004906
[Epoch 167; Iter    42/   48] train: loss: 0.0008729
[Epoch 167] ogbg-molbbbp: 0.904611 val loss: 0.555804
[Epoch 167] ogbg-molbbbp: 0.878774 test loss: 0.761072
[Epoch 168; Iter    24/   48] train: loss: 0.0008450
[Epoch 168] ogbg-molbbbp: 0.900414 val loss: 0.572068
[Epoch 168] ogbg-molbbbp: 0.877561 test loss: 0.772453
[Epoch 169; Iter     6/   48] train: loss: 0.0007880
[Epoch 169; Iter    36/   48] train: loss: 0.0009457
[Epoch 169] ogbg-molbbbp: 0.903590 val loss: 0.551164
[Epoch 169] ogbg-molbbbp: 0.879923 test loss: 0.736026
[Epoch 170; Iter    18/   48] train: loss: 0.0020025
[Epoch 170; Iter    48/   48] train: loss: 0.0093326
[Epoch 170] ogbg-molbbbp: 0.905348 val loss: 0.552529
[Epoch 170] ogbg-molbbbp: 0.880626 test loss: 0.759164
[Epoch 171; Iter    30/   48] train: loss: 0.0014469
[Epoch 171] ogbg-molbbbp: 0.904724 val loss: 0.575606
[Epoch 171] ogbg-molbbbp: 0.881136 test loss: 0.798650
[Epoch 172; Iter    12/   48] train: loss: 0.0025268
[Epoch 172; Iter    42/   48] train: loss: 0.0010622
[Epoch 172] ogbg-molbbbp: 0.905972 val loss: 0.582398
[Epoch 172] ogbg-molbbbp: 0.883371 test loss: 0.823756
[Epoch 173; Iter    24/   48] train: loss: 0.0197526
[Epoch 173] ogbg-molbbbp: 0.905802 val loss: 0.567158
[Epoch 173] ogbg-molbbbp: 0.877434 test loss: 0.799330
[Epoch 174; Iter     6/   48] train: loss: 0.0049031
[Epoch 174; Iter    36/   48] train: loss: 0.0083102
[Epoch 174] ogbg-molbbbp: 0.904157 val loss: 0.566707
[Epoch 174] ogbg-molbbbp: 0.878264 test loss: 0.785236
[Epoch 175; Iter    18/   48] train: loss: 0.0018957
[Epoch 175; Iter    48/   48] train: loss: 0.0008019
[Epoch 175] ogbg-molbbbp: 0.904667 val loss: 0.590528
[Epoch 175] ogbg-molbbbp: 0.876285 test loss: 0.846571
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 175 epochs. Best model checkpoint was in epoch 115.
Statistics on  val_best_checkpoint
mean_pred: 4.438530445098877
std_pred: 7.303267478942871
mean_targets: 0.7483659982681274
std_targets: 0.434662789106369
prcauc: 0.9554448821728709
rocauc: 0.9215675154539784
ogbg-molbbbp: 0.9215675154539784
BCEWithLogitsLoss: 0.4734389158812436
Statistics on  test
mean_pred: 5.030623435974121
std_pred: 6.946233749389648
mean_targets: 0.7875816822052002
std_targets: 0.4096892774105072
prcauc: 0.9670833962707355
rocauc: 0.9008617938078519
ogbg-molbbbp: 0.9008617938078519
BCEWithLogitsLoss: 0.6594071361151609
Statistics on  train
mean_pred: 5.057180404663086
std_pred: 7.144924163818359
mean_targets: 0.7638401985168457
std_targets: 0.4248703718185425
prcauc: 0.9999815504354707
rocauc: 0.9999401083494406
ogbg-molbbbp: 0.9999401083494406
BCEWithLogitsLoss: 0.009517956970133431
All runs completed.
