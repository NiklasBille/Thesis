>>> Starting run for dataset: sider
Running SCAFF configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.7.yml --seed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.8/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.8_4_26-05_09-18-15
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.8
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932428
[Epoch 1] ogbg-molsider: 0.508463 val loss: 0.693147
[Epoch 1] ogbg-molsider: 0.525730 test loss: 0.693261
[Epoch 2; Iter    24/   36] train: loss: 0.6936606
[Epoch 2] ogbg-molsider: 0.513912 val loss: 0.692988
[Epoch 2] ogbg-molsider: 0.522213 test loss: 0.693174
[Epoch 3; Iter    18/   36] train: loss: 0.6932107
[Epoch 3] ogbg-molsider: 0.515952 val loss: 0.693082
[Epoch 3] ogbg-molsider: 0.524025 test loss: 0.693268
[Epoch 4; Iter    12/   36] train: loss: 0.6936992
[Epoch 4] ogbg-molsider: 0.516213 val loss: 0.692961
[Epoch 4] ogbg-molsider: 0.522646 test loss: 0.693244
[Epoch 5; Iter     6/   36] train: loss: 0.6932373
[Epoch 5; Iter    36/   36] train: loss: 0.6934808
[Epoch 5] ogbg-molsider: 0.516092 val loss: 0.692883
[Epoch 5] ogbg-molsider: 0.521216 test loss: 0.693067
[Epoch 6; Iter    30/   36] train: loss: 0.6928728
[Epoch 6] ogbg-molsider: 0.520634 val loss: 0.692736
[Epoch 6] ogbg-molsider: 0.522189 test loss: 0.692945
[Epoch 7; Iter    24/   36] train: loss: 0.6930869
[Epoch 7] ogbg-molsider: 0.516094 val loss: 0.692650
[Epoch 7] ogbg-molsider: 0.521338 test loss: 0.692890
[Epoch 8; Iter    18/   36] train: loss: 0.6927783
[Epoch 8] ogbg-molsider: 0.516210 val loss: 0.692643
[Epoch 8] ogbg-molsider: 0.520560 test loss: 0.692809
[Epoch 9; Iter    12/   36] train: loss: 0.6934307
[Epoch 9] ogbg-molsider: 0.519573 val loss: 0.692593
[Epoch 9] ogbg-molsider: 0.523813 test loss: 0.692788
[Epoch 10; Iter     6/   36] train: loss: 0.6927520
[Epoch 10; Iter    36/   36] train: loss: 0.6921809
[Epoch 10] ogbg-molsider: 0.515571 val loss: 0.692436
[Epoch 10] ogbg-molsider: 0.524470 test loss: 0.692685
[Epoch 11; Iter    30/   36] train: loss: 0.6922271
[Epoch 11] ogbg-molsider: 0.518816 val loss: 0.692203
[Epoch 11] ogbg-molsider: 0.524258 test loss: 0.692430
[Epoch 12; Iter    24/   36] train: loss: 0.6926205
[Epoch 12] ogbg-molsider: 0.518845 val loss: 0.692063
[Epoch 12] ogbg-molsider: 0.523523 test loss: 0.692279
[Epoch 13; Iter    18/   36] train: loss: 0.6921979
[Epoch 13] ogbg-molsider: 0.519505 val loss: 0.691942
[Epoch 13] ogbg-molsider: 0.525343 test loss: 0.692136
[Epoch 14; Iter    12/   36] train: loss: 0.6925152
[Epoch 14] ogbg-molsider: 0.515243 val loss: 0.691764
[Epoch 14] ogbg-molsider: 0.522510 test loss: 0.691936
[Epoch 15; Iter     6/   36] train: loss: 0.6920321
[Epoch 15; Iter    36/   36] train: loss: 0.6914085
[Epoch 15] ogbg-molsider: 0.516954 val loss: 0.691581
[Epoch 15] ogbg-molsider: 0.523124 test loss: 0.691767
[Epoch 16; Iter    30/   36] train: loss: 0.6914978
[Epoch 16] ogbg-molsider: 0.515856 val loss: 0.691380
[Epoch 16] ogbg-molsider: 0.522523 test loss: 0.691577
[Epoch 17; Iter    24/   36] train: loss: 0.6918402
[Epoch 17] ogbg-molsider: 0.518684 val loss: 0.691072
[Epoch 17] ogbg-molsider: 0.523353 test loss: 0.691313
[Epoch 18; Iter    18/   36] train: loss: 0.6910167
[Epoch 18] ogbg-molsider: 0.515675 val loss: 0.690957
[Epoch 18] ogbg-molsider: 0.524477 test loss: 0.691152
[Epoch 19; Iter    12/   36] train: loss: 0.6915345
[Epoch 19] ogbg-molsider: 0.516908 val loss: 0.690684
[Epoch 19] ogbg-molsider: 0.525019 test loss: 0.690884
[Epoch 20; Iter     6/   36] train: loss: 0.6906663
[Epoch 20; Iter    36/   36] train: loss: 0.6862883
[Epoch 20] ogbg-molsider: 0.537245 val loss: 0.681538
[Epoch 20] ogbg-molsider: 0.571214 test loss: 0.681406
[Epoch 21; Iter    30/   36] train: loss: 0.6780910
[Epoch 21] ogbg-molsider: 0.520664 val loss: 0.662856
[Epoch 21] ogbg-molsider: 0.604330 test loss: 0.662514
[Epoch 22; Iter    24/   36] train: loss: 0.6567007
[Epoch 22] ogbg-molsider: 0.491540 val loss: 0.642001
[Epoch 22] ogbg-molsider: 0.592545 test loss: 0.638371
[Epoch 23; Iter    18/   36] train: loss: 0.6343639
[Epoch 23] ogbg-molsider: 0.531209 val loss: 0.602877
[Epoch 23] ogbg-molsider: 0.609525 test loss: 0.595668
[Epoch 24; Iter    12/   36] train: loss: 0.6129710
[Epoch 24] ogbg-molsider: 0.545014 val loss: 0.595576
[Epoch 24] ogbg-molsider: 0.588566 test loss: 0.597635
[Epoch 25; Iter     6/   36] train: loss: 0.5915238
[Epoch 25; Iter    36/   36] train: loss: 0.5730799
[Epoch 25] ogbg-molsider: 0.530956 val loss: 0.538076
[Epoch 25] ogbg-molsider: 0.602354 test loss: 0.536167
[Epoch 26; Iter    30/   36] train: loss: 0.5474578
[Epoch 26] ogbg-molsider: 0.556105 val loss: 0.508094
[Epoch 26] ogbg-molsider: 0.596440 test loss: 0.537685
[Epoch 27; Iter    24/   36] train: loss: 0.5316824
[Epoch 27] ogbg-molsider: 0.553854 val loss: 0.502826
[Epoch 27] ogbg-molsider: 0.585240 test loss: 0.528976
[Epoch 28; Iter    18/   36] train: loss: 0.5083334
[Epoch 28] ogbg-molsider: 0.535749 val loss: 0.486494
[Epoch 28] ogbg-molsider: 0.597267 test loss: 0.529221
[Epoch 29; Iter    12/   36] train: loss: 0.5151914
[Epoch 29] ogbg-molsider: 0.535506 val loss: 0.492968
[Epoch 29] ogbg-molsider: 0.587994 test loss: 0.522609
[Epoch 30; Iter     6/   36] train: loss: 0.5108786
[Epoch 30; Iter    36/   36] train: loss: 0.4881211
[Epoch 30] ogbg-molsider: 0.534922 val loss: 0.488686
[Epoch 30] ogbg-molsider: 0.616662 test loss: 0.551731
[Epoch 31; Iter    30/   36] train: loss: 0.4877305
[Epoch 31] ogbg-molsider: 0.532415 val loss: 0.487463
[Epoch 31] ogbg-molsider: 0.600483 test loss: 0.491842
[Epoch 32; Iter    24/   36] train: loss: 0.5385497
[Epoch 32] ogbg-molsider: 0.510961 val loss: 0.496164
[Epoch 32] ogbg-molsider: 0.608659 test loss: 0.492880
[Epoch 33; Iter    18/   36] train: loss: 0.4887319
[Epoch 33] ogbg-molsider: 0.527184 val loss: 0.494539
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.6/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.6_6_26-05_09-18-15
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.6
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.496788 val loss: 0.693150
[Epoch 1] ogbg-molsider: 0.501480 test loss: 0.693025
[Epoch 2; Iter     3/   27] train: loss: 0.6930000
[Epoch 2] ogbg-molsider: 0.492671 val loss: 0.693154
[Epoch 2] ogbg-molsider: 0.497740 test loss: 0.692837
[Epoch 3; Iter     6/   27] train: loss: 0.6927409
[Epoch 3] ogbg-molsider: 0.491921 val loss: 0.693202
[Epoch 3] ogbg-molsider: 0.499330 test loss: 0.692855
[Epoch 4; Iter     9/   27] train: loss: 0.6931965
[Epoch 4] ogbg-molsider: 0.492610 val loss: 0.693166
[Epoch 4] ogbg-molsider: 0.500383 test loss: 0.692755
[Epoch 5; Iter    12/   27] train: loss: 0.6927896
[Epoch 5] ogbg-molsider: 0.492327 val loss: 0.693183
[Epoch 5] ogbg-molsider: 0.502276 test loss: 0.692858
[Epoch 6; Iter    15/   27] train: loss: 0.6930548
[Epoch 6] ogbg-molsider: 0.493080 val loss: 0.693075
[Epoch 6] ogbg-molsider: 0.499817 test loss: 0.692644
[Epoch 7; Iter    18/   27] train: loss: 0.6929733
[Epoch 7] ogbg-molsider: 0.491281 val loss: 0.693095
[Epoch 7] ogbg-molsider: 0.499439 test loss: 0.692685
[Epoch 8; Iter    21/   27] train: loss: 0.6929917
[Epoch 8] ogbg-molsider: 0.491964 val loss: 0.693018
[Epoch 8] ogbg-molsider: 0.503633 test loss: 0.692648
[Epoch 9; Iter    24/   27] train: loss: 0.6931935
[Epoch 9] ogbg-molsider: 0.489949 val loss: 0.693080
[Epoch 9] ogbg-molsider: 0.501466 test loss: 0.692716
[Epoch 10; Iter    27/   27] train: loss: 0.6932070
[Epoch 10] ogbg-molsider: 0.489132 val loss: 0.693003
[Epoch 10] ogbg-molsider: 0.500410 test loss: 0.692619
[Epoch 11] ogbg-molsider: 0.490707 val loss: 0.692913
[Epoch 11] ogbg-molsider: 0.502668 test loss: 0.692526
[Epoch 12; Iter     3/   27] train: loss: 0.6929046
[Epoch 12] ogbg-molsider: 0.489334 val loss: 0.692908
[Epoch 12] ogbg-molsider: 0.501404 test loss: 0.692538
[Epoch 13; Iter     6/   27] train: loss: 0.6926314
[Epoch 13] ogbg-molsider: 0.491815 val loss: 0.692770
[Epoch 13] ogbg-molsider: 0.501172 test loss: 0.692370
[Epoch 14; Iter     9/   27] train: loss: 0.6927454
[Epoch 14] ogbg-molsider: 0.490922 val loss: 0.692656
[Epoch 14] ogbg-molsider: 0.501126 test loss: 0.692250
[Epoch 15; Iter    12/   27] train: loss: 0.6927868
[Epoch 15] ogbg-molsider: 0.492545 val loss: 0.692533
[Epoch 15] ogbg-molsider: 0.501024 test loss: 0.692134
[Epoch 16; Iter    15/   27] train: loss: 0.6921428
[Epoch 16] ogbg-molsider: 0.491126 val loss: 0.692515
[Epoch 16] ogbg-molsider: 0.503759 test loss: 0.692119
[Epoch 17; Iter    18/   27] train: loss: 0.6919469
[Epoch 17] ogbg-molsider: 0.490819 val loss: 0.692370
[Epoch 17] ogbg-molsider: 0.503076 test loss: 0.691957
[Epoch 18; Iter    21/   27] train: loss: 0.6924405
[Epoch 18] ogbg-molsider: 0.491936 val loss: 0.692283
[Epoch 18] ogbg-molsider: 0.501780 test loss: 0.691888
[Epoch 19; Iter    24/   27] train: loss: 0.6924872
[Epoch 19] ogbg-molsider: 0.491479 val loss: 0.692238
[Epoch 19] ogbg-molsider: 0.499789 test loss: 0.691774
[Epoch 20; Iter    27/   27] train: loss: 0.6922947
[Epoch 20] ogbg-molsider: 0.493164 val loss: 0.691970
[Epoch 20] ogbg-molsider: 0.503772 test loss: 0.691547
[Epoch 21] ogbg-molsider: 0.491104 val loss: 0.691975
[Epoch 21] ogbg-molsider: 0.499502 test loss: 0.691569
[Epoch 22; Iter     3/   27] train: loss: 0.6914248
[Epoch 22] ogbg-molsider: 0.490108 val loss: 0.691844
[Epoch 22] ogbg-molsider: 0.499940 test loss: 0.691405
[Epoch 23; Iter     6/   27] train: loss: 0.6915790
[Epoch 23] ogbg-molsider: 0.490880 val loss: 0.691649
[Epoch 23] ogbg-molsider: 0.502396 test loss: 0.691209
[Epoch 24; Iter     9/   27] train: loss: 0.6910487
[Epoch 24] ogbg-molsider: 0.490708 val loss: 0.691594
[Epoch 24] ogbg-molsider: 0.500334 test loss: 0.691118
[Epoch 25; Iter    12/   27] train: loss: 0.6916789
[Epoch 25] ogbg-molsider: 0.491069 val loss: 0.691359
[Epoch 25] ogbg-molsider: 0.499563 test loss: 0.690850
[Epoch 26; Iter    15/   27] train: loss: 0.6912743
[Epoch 26] ogbg-molsider: 0.505216 val loss: 0.687117
[Epoch 26] ogbg-molsider: 0.525177 test loss: 0.685736
[Epoch 27; Iter    18/   27] train: loss: 0.6833774
[Epoch 27] ogbg-molsider: 0.541601 val loss: 0.670977
[Epoch 27] ogbg-molsider: 0.555911 test loss: 0.668922
[Epoch 28; Iter    21/   27] train: loss: 0.6712654
[Epoch 28] ogbg-molsider: 0.535270 val loss: 0.650902
[Epoch 28] ogbg-molsider: 0.573181 test loss: 0.641345
[Epoch 29; Iter    24/   27] train: loss: 0.6470065
[Epoch 29] ogbg-molsider: 0.549056 val loss: 0.619187
[Epoch 29] ogbg-molsider: 0.569844 test loss: 0.609544
[Epoch 30; Iter    27/   27] train: loss: 0.6264439
[Epoch 30] ogbg-molsider: 0.558033 val loss: 0.605131
[Epoch 30] ogbg-molsider: 0.569195 test loss: 0.594542
[Epoch 31] ogbg-molsider: 0.556135 val loss: 0.607784
[Epoch 31] ogbg-molsider: 0.557585 test loss: 0.599873
[Epoch 32; Iter     3/   27] train: loss: 0.6068429
[Epoch 32] ogbg-molsider: 0.567907 val loss: 0.558578
[Epoch 32] ogbg-molsider: 0.564422 test loss: 0.544620
[Epoch 33; Iter     6/   27] train: loss: 0.5917988
[Epoch 33] ogbg-molsider: 0.559995 val loss: 0.575567
[Epoch 33] ogbg-molsider: 0.569703 test loss: 0.563556
[Epoch 34; Iter     9/   27] train: loss: 0.5344906
[Epoch 34] ogbg-molsider: 0.556692 val loss: 0.546246
[Epoch 34] ogbg-molsider: 0.565499 test loss: 0.541344
[Epoch 35; Iter    12/   27] train: loss: 0.5369327
[Epoch 35] ogbg-molsider: 0.556258 val loss: 0.528288
[Epoch 35] ogbg-molsider: 0.567854 test loss: 0.516270
[Epoch 36; Iter    15/   27] train: loss: 0.5291320
[Epoch 36] ogbg-molsider: 0.566886 val loss: 0.523689
[Epoch 36] ogbg-molsider: 0.576885 test loss: 0.508496
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.7/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.7_5_26-05_09-18-14
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.7
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6931875
[Epoch 1] ogbg-molsider: 0.470676 val loss: 0.693145
[Epoch 1] ogbg-molsider: 0.501248 test loss: 0.693121
[Epoch 2; Iter    28/   32] train: loss: 0.6930664
[Epoch 2] ogbg-molsider: 0.475733 val loss: 0.693182
[Epoch 2] ogbg-molsider: 0.502437 test loss: 0.693234
[Epoch 3; Iter    26/   32] train: loss: 0.6932304
[Epoch 3] ogbg-molsider: 0.477945 val loss: 0.693188
[Epoch 3] ogbg-molsider: 0.499589 test loss: 0.693222
[Epoch 4; Iter    24/   32] train: loss: 0.6929662
[Epoch 4] ogbg-molsider: 0.479076 val loss: 0.693195
[Epoch 4] ogbg-molsider: 0.503371 test loss: 0.693327
[Epoch 5; Iter    22/   32] train: loss: 0.6926658
[Epoch 5] ogbg-molsider: 0.480183 val loss: 0.693096
[Epoch 5] ogbg-molsider: 0.500248 test loss: 0.693214
[Epoch 6; Iter    20/   32] train: loss: 0.6928322
[Epoch 6] ogbg-molsider: 0.482034 val loss: 0.693107
[Epoch 6] ogbg-molsider: 0.506696 test loss: 0.693215
[Epoch 7; Iter    18/   32] train: loss: 0.6926278
[Epoch 7] ogbg-molsider: 0.480732 val loss: 0.693021
[Epoch 7] ogbg-molsider: 0.502761 test loss: 0.693171
[Epoch 8; Iter    16/   32] train: loss: 0.6926080
[Epoch 8] ogbg-molsider: 0.478725 val loss: 0.693000
[Epoch 8] ogbg-molsider: 0.504539 test loss: 0.693218
[Epoch 9; Iter    14/   32] train: loss: 0.6931170
[Epoch 9] ogbg-molsider: 0.481649 val loss: 0.692789
[Epoch 9] ogbg-molsider: 0.507571 test loss: 0.692870
[Epoch 10; Iter    12/   32] train: loss: 0.6928475
[Epoch 10] ogbg-molsider: 0.481888 val loss: 0.692725
[Epoch 10] ogbg-molsider: 0.504086 test loss: 0.692777
[Epoch 11; Iter    10/   32] train: loss: 0.6931657
[Epoch 11] ogbg-molsider: 0.478522 val loss: 0.692692
[Epoch 11] ogbg-molsider: 0.502303 test loss: 0.692876
[Epoch 12; Iter     8/   32] train: loss: 0.6925246
[Epoch 12] ogbg-molsider: 0.481570 val loss: 0.692527
[Epoch 12] ogbg-molsider: 0.506570 test loss: 0.692710
[Epoch 13; Iter     6/   32] train: loss: 0.6923152
[Epoch 13] ogbg-molsider: 0.478496 val loss: 0.692533
[Epoch 13] ogbg-molsider: 0.504922 test loss: 0.692622
[Epoch 14; Iter     4/   32] train: loss: 0.6922510
[Epoch 14] ogbg-molsider: 0.479786 val loss: 0.692247
[Epoch 14] ogbg-molsider: 0.507121 test loss: 0.692371
[Epoch 15; Iter     2/   32] train: loss: 0.6918724
[Epoch 15; Iter    32/   32] train: loss: 0.6920673
[Epoch 15] ogbg-molsider: 0.482320 val loss: 0.692218
[Epoch 15] ogbg-molsider: 0.506986 test loss: 0.692319
[Epoch 16; Iter    30/   32] train: loss: 0.6920731
[Epoch 16] ogbg-molsider: 0.480671 val loss: 0.692015
[Epoch 16] ogbg-molsider: 0.506348 test loss: 0.692087
[Epoch 17; Iter    28/   32] train: loss: 0.6916994
[Epoch 17] ogbg-molsider: 0.478791 val loss: 0.691854
[Epoch 17] ogbg-molsider: 0.503403 test loss: 0.691990
[Epoch 18; Iter    26/   32] train: loss: 0.6917022
[Epoch 18] ogbg-molsider: 0.479463 val loss: 0.691848
[Epoch 18] ogbg-molsider: 0.503995 test loss: 0.692054
[Epoch 19; Iter    24/   32] train: loss: 0.6919170
[Epoch 19] ogbg-molsider: 0.482385 val loss: 0.691483
[Epoch 19] ogbg-molsider: 0.506929 test loss: 0.691510
[Epoch 20; Iter    22/   32] train: loss: 0.6914936
[Epoch 20] ogbg-molsider: 0.478611 val loss: 0.691343
[Epoch 20] ogbg-molsider: 0.505215 test loss: 0.691454
[Epoch 21; Iter    20/   32] train: loss: 0.6911467
[Epoch 21] ogbg-molsider: 0.480965 val loss: 0.691185
[Epoch 21] ogbg-molsider: 0.499626 test loss: 0.691230
[Epoch 22; Iter    18/   32] train: loss: 0.6904145
[Epoch 22] ogbg-molsider: 0.511664 val loss: 0.688459
[Epoch 22] ogbg-molsider: 0.543580 test loss: 0.688733
[Epoch 23; Iter    16/   32] train: loss: 0.6850854
[Epoch 23] ogbg-molsider: 0.526868 val loss: 0.673597
[Epoch 23] ogbg-molsider: 0.566241 test loss: 0.674750
[Epoch 24; Iter    14/   32] train: loss: 0.6755068
[Epoch 24] ogbg-molsider: 0.548311 val loss: 0.648819
[Epoch 24] ogbg-molsider: 0.586455 test loss: 0.643066
[Epoch 25; Iter    12/   32] train: loss: 0.6577510
[Epoch 25] ogbg-molsider: 0.550802 val loss: 0.622194
[Epoch 25] ogbg-molsider: 0.585376 test loss: 0.618830
[Epoch 26; Iter    10/   32] train: loss: 0.6353007
[Epoch 26] ogbg-molsider: 0.541206 val loss: 0.600528
[Epoch 26] ogbg-molsider: 0.586367 test loss: 0.596810
[Epoch 27; Iter     8/   32] train: loss: 0.6226394
[Epoch 27] ogbg-molsider: 0.541765 val loss: 0.575370
[Epoch 27] ogbg-molsider: 0.574834 test loss: 0.580457
[Epoch 28; Iter     6/   32] train: loss: 0.5905259
[Epoch 28] ogbg-molsider: 0.559850 val loss: 0.546821
[Epoch 28] ogbg-molsider: 0.568695 test loss: 0.556092
[Epoch 29; Iter     4/   32] train: loss: 0.5923764
[Epoch 29] ogbg-molsider: 0.545335 val loss: 0.548610
[Epoch 29] ogbg-molsider: 0.575787 test loss: 0.556132
[Epoch 30; Iter     2/   32] train: loss: 0.5384337
[Epoch 30; Iter    32/   32] train: loss: 0.5327260
[Epoch 30] ogbg-molsider: 0.552990 val loss: 0.515467
[Epoch 30] ogbg-molsider: 0.569726 test loss: 0.527029
[Epoch 31; Iter    30/   32] train: loss: 0.5174806
[Epoch 31] ogbg-molsider: 0.555141 val loss: 0.497007
[Epoch 31] ogbg-molsider: 0.589092 test loss: 0.505115
[Epoch 32; Iter    28/   32] train: loss: 0.5064448
[Epoch 32] ogbg-molsider: 0.574901 val loss: 0.488614
[Epoch 32] ogbg-molsider: 0.579608 test loss: 0.506338
[Epoch 33; Iter    26/   32] train: loss: 0.5515100
[Epoch 33] ogbg-molsider: 0.557150 val loss: 0.489744
[Epoch 33] ogbg-molsider: 0.581963 test loss: 0.499792
[Epoch 34; Iter    24/   32] train: loss: 0.5148738
[Epoch 34] ogbg-molsider: 0.573696 val loss: 0.485730
[Epoch 34] ogbg-molsider: 0.584174 test loss: 0.500253
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.8/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.8_5_26-05_09-18-14
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.8
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6936331
[Epoch 1] ogbg-molsider: 0.483690 val loss: 0.693155
[Epoch 1] ogbg-molsider: 0.503371 test loss: 0.693228
[Epoch 2; Iter    24/   36] train: loss: 0.6937493
[Epoch 2] ogbg-molsider: 0.499742 val loss: 0.693115
[Epoch 2] ogbg-molsider: 0.498544 test loss: 0.693428
[Epoch 3; Iter    18/   36] train: loss: 0.6935792
[Epoch 3] ogbg-molsider: 0.503120 val loss: 0.693016
[Epoch 3] ogbg-molsider: 0.498856 test loss: 0.693424
[Epoch 4; Iter    12/   36] train: loss: 0.6933743
[Epoch 4] ogbg-molsider: 0.506100 val loss: 0.693076
[Epoch 4] ogbg-molsider: 0.498171 test loss: 0.693434
[Epoch 5; Iter     6/   36] train: loss: 0.6931689
[Epoch 5; Iter    36/   36] train: loss: 0.6929625
[Epoch 5] ogbg-molsider: 0.500667 val loss: 0.693055
[Epoch 5] ogbg-molsider: 0.497338 test loss: 0.693482
[Epoch 6; Iter    30/   36] train: loss: 0.6932262
[Epoch 6] ogbg-molsider: 0.503284 val loss: 0.692926
[Epoch 6] ogbg-molsider: 0.502133 test loss: 0.693212
[Epoch 7; Iter    24/   36] train: loss: 0.6932282
[Epoch 7] ogbg-molsider: 0.507675 val loss: 0.692746
[Epoch 7] ogbg-molsider: 0.500213 test loss: 0.693114
[Epoch 8; Iter    18/   36] train: loss: 0.6924952
[Epoch 8] ogbg-molsider: 0.500417 val loss: 0.692816
[Epoch 8] ogbg-molsider: 0.502040 test loss: 0.693236
[Epoch 9; Iter    12/   36] train: loss: 0.6931766
[Epoch 9] ogbg-molsider: 0.500951 val loss: 0.692719
[Epoch 9] ogbg-molsider: 0.500490 test loss: 0.693116
[Epoch 10; Iter     6/   36] train: loss: 0.6926487
[Epoch 10; Iter    36/   36] train: loss: 0.6925964
[Epoch 10] ogbg-molsider: 0.503446 val loss: 0.692483
[Epoch 10] ogbg-molsider: 0.500008 test loss: 0.692806
[Epoch 11; Iter    30/   36] train: loss: 0.6926727
[Epoch 11] ogbg-molsider: 0.508322 val loss: 0.692318
[Epoch 11] ogbg-molsider: 0.500958 test loss: 0.692698
[Epoch 12; Iter    24/   36] train: loss: 0.6924053
[Epoch 12] ogbg-molsider: 0.505031 val loss: 0.692115
[Epoch 12] ogbg-molsider: 0.501400 test loss: 0.692495
[Epoch 13; Iter    18/   36] train: loss: 0.6925083
[Epoch 13] ogbg-molsider: 0.506162 val loss: 0.692111
[Epoch 13] ogbg-molsider: 0.500341 test loss: 0.692417
[Epoch 14; Iter    12/   36] train: loss: 0.6922555
[Epoch 14] ogbg-molsider: 0.507015 val loss: 0.691930
[Epoch 14] ogbg-molsider: 0.503859 test loss: 0.692312
[Epoch 15; Iter     6/   36] train: loss: 0.6926001
[Epoch 15; Iter    36/   36] train: loss: 0.6916689
[Epoch 15] ogbg-molsider: 0.510460 val loss: 0.691762
[Epoch 15] ogbg-molsider: 0.501479 test loss: 0.692044
[Epoch 16; Iter    30/   36] train: loss: 0.6923040
[Epoch 16] ogbg-molsider: 0.505530 val loss: 0.691555
[Epoch 16] ogbg-molsider: 0.502183 test loss: 0.691920
[Epoch 17; Iter    24/   36] train: loss: 0.6917722
[Epoch 17] ogbg-molsider: 0.509580 val loss: 0.691361
[Epoch 17] ogbg-molsider: 0.500816 test loss: 0.691727
[Epoch 18; Iter    18/   36] train: loss: 0.6916637
[Epoch 18] ogbg-molsider: 0.504801 val loss: 0.691030
[Epoch 18] ogbg-molsider: 0.504960 test loss: 0.691314
[Epoch 19; Iter    12/   36] train: loss: 0.6917031
[Epoch 19] ogbg-molsider: 0.507032 val loss: 0.690880
[Epoch 19] ogbg-molsider: 0.505516 test loss: 0.691205
[Epoch 20; Iter     6/   36] train: loss: 0.6909327
[Epoch 20; Iter    36/   36] train: loss: 0.6863186
[Epoch 20] ogbg-molsider: 0.540658 val loss: 0.682393
[Epoch 20] ogbg-molsider: 0.563531 test loss: 0.682920
[Epoch 21; Iter    30/   36] train: loss: 0.6790473
[Epoch 21] ogbg-molsider: 0.532132 val loss: 0.657990
[Epoch 21] ogbg-molsider: 0.598657 test loss: 0.656422
[Epoch 22; Iter    24/   36] train: loss: 0.6603025
[Epoch 22] ogbg-molsider: 0.524145 val loss: 0.635209
[Epoch 22] ogbg-molsider: 0.588564 test loss: 0.631409
[Epoch 23; Iter    18/   36] train: loss: 0.6343783
[Epoch 23] ogbg-molsider: 0.524446 val loss: 0.624035
[Epoch 23] ogbg-molsider: 0.592573 test loss: 0.621016
[Epoch 24; Iter    12/   36] train: loss: 0.6304240
[Epoch 24] ogbg-molsider: 0.518792 val loss: 0.584454
[Epoch 24] ogbg-molsider: 0.605914 test loss: 0.579633
[Epoch 25; Iter     6/   36] train: loss: 0.5884835
[Epoch 25; Iter    36/   36] train: loss: 0.5748572
[Epoch 25] ogbg-molsider: 0.546613 val loss: 0.556347
[Epoch 25] ogbg-molsider: 0.576834 test loss: 0.557699
[Epoch 26; Iter    30/   36] train: loss: 0.5646052
[Epoch 26] ogbg-molsider: 0.541455 val loss: 0.516047
[Epoch 26] ogbg-molsider: 0.616528 test loss: 0.513307
[Epoch 27; Iter    24/   36] train: loss: 0.5343263
[Epoch 27] ogbg-molsider: 0.564166 val loss: 0.500281
[Epoch 27] ogbg-molsider: 0.584414 test loss: 0.506693
[Epoch 28; Iter    18/   36] train: loss: 0.5266917
[Epoch 28] ogbg-molsider: 0.556186 val loss: 0.506421
[Epoch 28] ogbg-molsider: 0.589240 test loss: 0.511249
[Epoch 29; Iter    12/   36] train: loss: 0.4922637
[Epoch 29] ogbg-molsider: 0.577750 val loss: 0.483618
[Epoch 29] ogbg-molsider: 0.586111 test loss: 0.496318
[Epoch 30; Iter     6/   36] train: loss: 0.4977187
[Epoch 30; Iter    36/   36] train: loss: 0.4504667
[Epoch 30] ogbg-molsider: 0.572794 val loss: 0.481923
[Epoch 30] ogbg-molsider: 0.591383 test loss: 0.489832
[Epoch 31; Iter    30/   36] train: loss: 0.4858133
[Epoch 31] ogbg-molsider: 0.542017 val loss: 0.496543
[Epoch 31] ogbg-molsider: 0.603267 test loss: 0.500724
[Epoch 32; Iter    24/   36] train: loss: 0.4832959
[Epoch 32] ogbg-molsider: 0.552524 val loss: 0.489654
[Epoch 32] ogbg-molsider: 0.601478 test loss: 0.502903
[Epoch 33; Iter    18/   36] train: loss: 0.4870285
[Epoch 33] ogbg-molsider: 0.544017 val loss: 0.491450
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.6/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.6_4_26-05_09-18-14
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.6
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.492339 val loss: 0.693293
[Epoch 1] ogbg-molsider: 0.519627 test loss: 0.693276
[Epoch 2; Iter     3/   27] train: loss: 0.6930735
[Epoch 2] ogbg-molsider: 0.492921 val loss: 0.693295
[Epoch 2] ogbg-molsider: 0.517371 test loss: 0.693196
[Epoch 3; Iter     6/   27] train: loss: 0.6932296
[Epoch 3] ogbg-molsider: 0.494625 val loss: 0.693401
[Epoch 3] ogbg-molsider: 0.517974 test loss: 0.693188
[Epoch 4; Iter     9/   27] train: loss: 0.6932921
[Epoch 4] ogbg-molsider: 0.498689 val loss: 0.693386
[Epoch 4] ogbg-molsider: 0.520543 test loss: 0.693158
[Epoch 5; Iter    12/   27] train: loss: 0.6931247
[Epoch 5] ogbg-molsider: 0.500485 val loss: 0.693338
[Epoch 5] ogbg-molsider: 0.521273 test loss: 0.693128
[Epoch 6; Iter    15/   27] train: loss: 0.6928139
[Epoch 6] ogbg-molsider: 0.499643 val loss: 0.693336
[Epoch 6] ogbg-molsider: 0.521830 test loss: 0.693176
[Epoch 7; Iter    18/   27] train: loss: 0.6934432
[Epoch 7] ogbg-molsider: 0.499084 val loss: 0.693260
[Epoch 7] ogbg-molsider: 0.521126 test loss: 0.693085
[Epoch 8; Iter    21/   27] train: loss: 0.6929905
[Epoch 8] ogbg-molsider: 0.500444 val loss: 0.693227
[Epoch 8] ogbg-molsider: 0.520850 test loss: 0.693084
[Epoch 9; Iter    24/   27] train: loss: 0.6926705
[Epoch 9] ogbg-molsider: 0.499035 val loss: 0.693155
[Epoch 9] ogbg-molsider: 0.519270 test loss: 0.692937
[Epoch 10; Iter    27/   27] train: loss: 0.6928465
[Epoch 10] ogbg-molsider: 0.500431 val loss: 0.693031
[Epoch 10] ogbg-molsider: 0.519811 test loss: 0.692805
[Epoch 11] ogbg-molsider: 0.498740 val loss: 0.693070
[Epoch 11] ogbg-molsider: 0.518611 test loss: 0.692868
[Epoch 12; Iter     3/   27] train: loss: 0.6929730
[Epoch 12] ogbg-molsider: 0.501216 val loss: 0.692840
[Epoch 12] ogbg-molsider: 0.520868 test loss: 0.692676
[Epoch 13; Iter     6/   27] train: loss: 0.6934863
[Epoch 13] ogbg-molsider: 0.501015 val loss: 0.692681
[Epoch 13] ogbg-molsider: 0.518804 test loss: 0.692445
[Epoch 14; Iter     9/   27] train: loss: 0.6926511
[Epoch 14] ogbg-molsider: 0.498398 val loss: 0.692726
[Epoch 14] ogbg-molsider: 0.522143 test loss: 0.692431
[Epoch 15; Iter    12/   27] train: loss: 0.6927875
[Epoch 15] ogbg-molsider: 0.500572 val loss: 0.692582
[Epoch 15] ogbg-molsider: 0.520824 test loss: 0.692298
[Epoch 16; Iter    15/   27] train: loss: 0.6923356
[Epoch 16] ogbg-molsider: 0.499314 val loss: 0.692545
[Epoch 16] ogbg-molsider: 0.520009 test loss: 0.692268
[Epoch 17; Iter    18/   27] train: loss: 0.6923985
[Epoch 17] ogbg-molsider: 0.499115 val loss: 0.692342
[Epoch 17] ogbg-molsider: 0.521694 test loss: 0.692023
[Epoch 18; Iter    21/   27] train: loss: 0.6917316
[Epoch 18] ogbg-molsider: 0.499552 val loss: 0.692187
[Epoch 18] ogbg-molsider: 0.520540 test loss: 0.691891
[Epoch 19; Iter    24/   27] train: loss: 0.6922011
[Epoch 19] ogbg-molsider: 0.500201 val loss: 0.692127
[Epoch 19] ogbg-molsider: 0.521572 test loss: 0.691823
[Epoch 20; Iter    27/   27] train: loss: 0.6921564
[Epoch 20] ogbg-molsider: 0.501286 val loss: 0.691959
[Epoch 20] ogbg-molsider: 0.519337 test loss: 0.691697
[Epoch 21] ogbg-molsider: 0.499009 val loss: 0.691900
[Epoch 21] ogbg-molsider: 0.520862 test loss: 0.691584
[Epoch 22; Iter     3/   27] train: loss: 0.6918128
[Epoch 22] ogbg-molsider: 0.499684 val loss: 0.691723
[Epoch 22] ogbg-molsider: 0.520956 test loss: 0.691396
[Epoch 23; Iter     6/   27] train: loss: 0.6911139
[Epoch 23] ogbg-molsider: 0.502033 val loss: 0.691474
[Epoch 23] ogbg-molsider: 0.522017 test loss: 0.691168
[Epoch 24; Iter     9/   27] train: loss: 0.6911518
[Epoch 24] ogbg-molsider: 0.500663 val loss: 0.691324
[Epoch 24] ogbg-molsider: 0.520006 test loss: 0.691044
[Epoch 25; Iter    12/   27] train: loss: 0.6913851
[Epoch 25] ogbg-molsider: 0.498167 val loss: 0.691182
[Epoch 25] ogbg-molsider: 0.519836 test loss: 0.690811
[Epoch 26; Iter    15/   27] train: loss: 0.6899332
[Epoch 26] ogbg-molsider: 0.527839 val loss: 0.686828
[Epoch 26] ogbg-molsider: 0.548522 test loss: 0.685351
[Epoch 27; Iter    18/   27] train: loss: 0.6839362
[Epoch 27] ogbg-molsider: 0.535512 val loss: 0.673723
[Epoch 27] ogbg-molsider: 0.553864 test loss: 0.670544
[Epoch 28; Iter    21/   27] train: loss: 0.6668691
[Epoch 28] ogbg-molsider: 0.545026 val loss: 0.652380
[Epoch 28] ogbg-molsider: 0.575080 test loss: 0.645750
[Epoch 29; Iter    24/   27] train: loss: 0.6542693
[Epoch 29] ogbg-molsider: 0.539694 val loss: 0.622955
[Epoch 29] ogbg-molsider: 0.569776 test loss: 0.611438
[Epoch 30; Iter    27/   27] train: loss: 0.6351643
[Epoch 30] ogbg-molsider: 0.541130 val loss: 0.609760
[Epoch 30] ogbg-molsider: 0.569216 test loss: 0.597895
[Epoch 31] ogbg-molsider: 0.533814 val loss: 0.597284
[Epoch 31] ogbg-molsider: 0.561492 test loss: 0.581012
[Epoch 32; Iter     3/   27] train: loss: 0.5986016
[Epoch 32] ogbg-molsider: 0.532220 val loss: 0.568296
[Epoch 32] ogbg-molsider: 0.523986 test loss: 0.565222
[Epoch 33; Iter     6/   27] train: loss: 0.5876557
[Epoch 33] ogbg-molsider: 0.559101 val loss: 0.557692
[Epoch 33] ogbg-molsider: 0.558047 test loss: 0.542126
[Epoch 34; Iter     9/   27] train: loss: 0.5527655
[Epoch 34] ogbg-molsider: 0.561424 val loss: 0.537048
[Epoch 34] ogbg-molsider: 0.573078 test loss: 0.519992
[Epoch 35; Iter    12/   27] train: loss: 0.5274209
[Epoch 35] ogbg-molsider: 0.554785 val loss: 0.529642
[Epoch 35] ogbg-molsider: 0.575151 test loss: 0.514365
[Epoch 36; Iter    15/   27] train: loss: 0.5138138
[Epoch 36] ogbg-molsider: 0.530411 val loss: 0.557010
[Epoch 36] ogbg-molsider: 0.553745 test loss: 0.560622
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.7/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.7_4_26-05_09-18-14
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.7
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6935924
[Epoch 1] ogbg-molsider: 0.500908 val loss: 0.693283
[Epoch 1] ogbg-molsider: 0.527108 test loss: 0.693216
[Epoch 2; Iter    28/   32] train: loss: 0.6931946
[Epoch 2] ogbg-molsider: 0.496082 val loss: 0.693330
[Epoch 2] ogbg-molsider: 0.524442 test loss: 0.693079
[Epoch 3; Iter    26/   32] train: loss: 0.6933525
[Epoch 3] ogbg-molsider: 0.504738 val loss: 0.693398
[Epoch 3] ogbg-molsider: 0.525872 test loss: 0.693145
[Epoch 4; Iter    24/   32] train: loss: 0.6931521
[Epoch 4] ogbg-molsider: 0.503684 val loss: 0.693362
[Epoch 4] ogbg-molsider: 0.527430 test loss: 0.692977
[Epoch 5; Iter    22/   32] train: loss: 0.6932921
[Epoch 5] ogbg-molsider: 0.502341 val loss: 0.693176
[Epoch 5] ogbg-molsider: 0.524510 test loss: 0.692935
[Epoch 6; Iter    20/   32] train: loss: 0.6931449
[Epoch 6] ogbg-molsider: 0.502163 val loss: 0.693213
[Epoch 6] ogbg-molsider: 0.527451 test loss: 0.692834
[Epoch 7; Iter    18/   32] train: loss: 0.6927588
[Epoch 7] ogbg-molsider: 0.501150 val loss: 0.693173
[Epoch 7] ogbg-molsider: 0.529030 test loss: 0.692811
[Epoch 8; Iter    16/   32] train: loss: 0.6928347
[Epoch 8] ogbg-molsider: 0.505448 val loss: 0.693013
[Epoch 8] ogbg-molsider: 0.525781 test loss: 0.692709
[Epoch 9; Iter    14/   32] train: loss: 0.6934295
[Epoch 9] ogbg-molsider: 0.502878 val loss: 0.693037
[Epoch 9] ogbg-molsider: 0.526409 test loss: 0.692699
[Epoch 10; Iter    12/   32] train: loss: 0.6926151
[Epoch 10] ogbg-molsider: 0.503873 val loss: 0.692909
[Epoch 10] ogbg-molsider: 0.524108 test loss: 0.692653
[Epoch 11; Iter    10/   32] train: loss: 0.6923127
[Epoch 11] ogbg-molsider: 0.502926 val loss: 0.692743
[Epoch 11] ogbg-molsider: 0.526288 test loss: 0.692413
[Epoch 12; Iter     8/   32] train: loss: 0.6930435
[Epoch 12] ogbg-molsider: 0.503642 val loss: 0.692662
[Epoch 12] ogbg-molsider: 0.526026 test loss: 0.692321
[Epoch 13; Iter     6/   32] train: loss: 0.6925481
[Epoch 13] ogbg-molsider: 0.503896 val loss: 0.692641
[Epoch 13] ogbg-molsider: 0.530056 test loss: 0.692366
[Epoch 14; Iter     4/   32] train: loss: 0.6923163
[Epoch 14] ogbg-molsider: 0.501506 val loss: 0.692332
[Epoch 14] ogbg-molsider: 0.525502 test loss: 0.691924
[Epoch 15; Iter     2/   32] train: loss: 0.6924347
[Epoch 15; Iter    32/   32] train: loss: 0.6919976
[Epoch 15] ogbg-molsider: 0.504836 val loss: 0.692142
[Epoch 15] ogbg-molsider: 0.528564 test loss: 0.691885
[Epoch 16; Iter    30/   32] train: loss: 0.6919055
[Epoch 16] ogbg-molsider: 0.503430 val loss: 0.692112
[Epoch 16] ogbg-molsider: 0.526320 test loss: 0.691856
[Epoch 17; Iter    28/   32] train: loss: 0.6919167
[Epoch 17] ogbg-molsider: 0.502268 val loss: 0.691900
[Epoch 17] ogbg-molsider: 0.528494 test loss: 0.691560
[Epoch 18; Iter    26/   32] train: loss: 0.6915208
[Epoch 18] ogbg-molsider: 0.499019 val loss: 0.691729
[Epoch 18] ogbg-molsider: 0.527326 test loss: 0.691356
[Epoch 19; Iter    24/   32] train: loss: 0.6917663
[Epoch 19] ogbg-molsider: 0.503218 val loss: 0.691504
[Epoch 19] ogbg-molsider: 0.526955 test loss: 0.691140
[Epoch 20; Iter    22/   32] train: loss: 0.6913827
[Epoch 20] ogbg-molsider: 0.506792 val loss: 0.691151
[Epoch 20] ogbg-molsider: 0.527359 test loss: 0.690818
[Epoch 21; Iter    20/   32] train: loss: 0.6911172
[Epoch 21] ogbg-molsider: 0.503167 val loss: 0.691094
[Epoch 21] ogbg-molsider: 0.528219 test loss: 0.690801
[Epoch 22; Iter    18/   32] train: loss: 0.6905411
[Epoch 22] ogbg-molsider: 0.529681 val loss: 0.686978
[Epoch 22] ogbg-molsider: 0.549193 test loss: 0.685603
[Epoch 23; Iter    16/   32] train: loss: 0.6837376
[Epoch 23] ogbg-molsider: 0.527844 val loss: 0.671247
[Epoch 23] ogbg-molsider: 0.574736 test loss: 0.669500
[Epoch 24; Iter    14/   32] train: loss: 0.6725978
[Epoch 24] ogbg-molsider: 0.532636 val loss: 0.651991
[Epoch 24] ogbg-molsider: 0.567427 test loss: 0.648714
[Epoch 25; Iter    12/   32] train: loss: 0.6580219
[Epoch 25] ogbg-molsider: 0.528024 val loss: 0.617354
[Epoch 25] ogbg-molsider: 0.563209 test loss: 0.613986
[Epoch 26; Iter    10/   32] train: loss: 0.6453422
[Epoch 26] ogbg-molsider: 0.535103 val loss: 0.586328
[Epoch 26] ogbg-molsider: 0.578202 test loss: 0.588432
[Epoch 27; Iter     8/   32] train: loss: 0.6189242
[Epoch 27] ogbg-molsider: 0.542693 val loss: 0.563917
[Epoch 27] ogbg-molsider: 0.579444 test loss: 0.569872
[Epoch 28; Iter     6/   32] train: loss: 0.5831203
[Epoch 28] ogbg-molsider: 0.563637 val loss: 0.549195
[Epoch 28] ogbg-molsider: 0.588616 test loss: 0.555560
[Epoch 29; Iter     4/   32] train: loss: 0.5480155
[Epoch 29] ogbg-molsider: 0.540498 val loss: 0.538164
[Epoch 29] ogbg-molsider: 0.560221 test loss: 0.546490
[Epoch 30; Iter     2/   32] train: loss: 0.5636273
[Epoch 30; Iter    32/   32] train: loss: 0.5861414
[Epoch 30] ogbg-molsider: 0.538613 val loss: 0.503553
[Epoch 30] ogbg-molsider: 0.566529 test loss: 0.520979
[Epoch 31; Iter    30/   32] train: loss: 0.5353040
[Epoch 31] ogbg-molsider: 0.560294 val loss: 0.492781
[Epoch 31] ogbg-molsider: 0.577017 test loss: 0.515133
[Epoch 32; Iter    28/   32] train: loss: 0.5389534
[Epoch 32] ogbg-molsider: 0.549196 val loss: 0.492745
[Epoch 32] ogbg-molsider: 0.577225 test loss: 0.504580
[Epoch 33; Iter    26/   32] train: loss: 0.5165259
[Epoch 33] ogbg-molsider: 0.553831 val loss: 0.491387
[Epoch 33] ogbg-molsider: 0.549586 test loss: 0.504611
[Epoch 34; Iter    24/   32] train: loss: 0.5099015
[Epoch 34] ogbg-molsider: 0.559105 val loss: 0.484194
[Epoch 34] ogbg-molsider: 0.570585 test loss: 0.505106
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.7/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.7_6_26-05_09-18-14
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.7
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6928998
[Epoch 1] ogbg-molsider: 0.473255 val loss: 0.693168
[Epoch 1] ogbg-molsider: 0.505218 test loss: 0.692971
[Epoch 2; Iter    28/   32] train: loss: 0.6933002
[Epoch 2] ogbg-molsider: 0.470802 val loss: 0.693226
[Epoch 2] ogbg-molsider: 0.508307 test loss: 0.692791
[Epoch 3; Iter    26/   32] train: loss: 0.6929410
[Epoch 3] ogbg-molsider: 0.473937 val loss: 0.693275
[Epoch 3] ogbg-molsider: 0.506295 test loss: 0.692786
[Epoch 4; Iter    24/   32] train: loss: 0.6934041
[Epoch 4] ogbg-molsider: 0.477835 val loss: 0.693180
[Epoch 4] ogbg-molsider: 0.507191 test loss: 0.692774
[Epoch 5; Iter    22/   32] train: loss: 0.6934947
[Epoch 5] ogbg-molsider: 0.480107 val loss: 0.693112
[Epoch 5] ogbg-molsider: 0.507869 test loss: 0.692652
[Epoch 6; Iter    20/   32] train: loss: 0.6928589
[Epoch 6] ogbg-molsider: 0.475973 val loss: 0.693122
[Epoch 6] ogbg-molsider: 0.503826 test loss: 0.692737
[Epoch 7; Iter    18/   32] train: loss: 0.6932296
[Epoch 7] ogbg-molsider: 0.478918 val loss: 0.692988
[Epoch 7] ogbg-molsider: 0.507974 test loss: 0.692533
[Epoch 8; Iter    16/   32] train: loss: 0.6929047
[Epoch 8] ogbg-molsider: 0.481681 val loss: 0.693079
[Epoch 8] ogbg-molsider: 0.509122 test loss: 0.692783
[Epoch 9; Iter    14/   32] train: loss: 0.6927168
[Epoch 9] ogbg-molsider: 0.478255 val loss: 0.692902
[Epoch 9] ogbg-molsider: 0.505553 test loss: 0.692467
[Epoch 10; Iter    12/   32] train: loss: 0.6925436
[Epoch 10] ogbg-molsider: 0.482921 val loss: 0.692761
[Epoch 10] ogbg-molsider: 0.508604 test loss: 0.692348
[Epoch 11; Iter    10/   32] train: loss: 0.6932892
[Epoch 11] ogbg-molsider: 0.475127 val loss: 0.692751
[Epoch 11] ogbg-molsider: 0.505577 test loss: 0.692324
[Epoch 12; Iter     8/   32] train: loss: 0.6921630
[Epoch 12] ogbg-molsider: 0.479396 val loss: 0.692653
[Epoch 12] ogbg-molsider: 0.507141 test loss: 0.692294
[Epoch 13; Iter     6/   32] train: loss: 0.6925811
[Epoch 13] ogbg-molsider: 0.481233 val loss: 0.692410
[Epoch 13] ogbg-molsider: 0.505719 test loss: 0.691964
[Epoch 14; Iter     4/   32] train: loss: 0.6925621
[Epoch 14] ogbg-molsider: 0.478738 val loss: 0.692469
[Epoch 14] ogbg-molsider: 0.506731 test loss: 0.692124
[Epoch 15; Iter     2/   32] train: loss: 0.6924794
[Epoch 15; Iter    32/   32] train: loss: 0.6915519
[Epoch 15] ogbg-molsider: 0.481884 val loss: 0.692065
[Epoch 15] ogbg-molsider: 0.507859 test loss: 0.691676
[Epoch 16; Iter    30/   32] train: loss: 0.6929200
[Epoch 16] ogbg-molsider: 0.479392 val loss: 0.692053
[Epoch 16] ogbg-molsider: 0.508995 test loss: 0.691657
[Epoch 17; Iter    28/   32] train: loss: 0.6920558
[Epoch 17] ogbg-molsider: 0.480918 val loss: 0.691943
[Epoch 17] ogbg-molsider: 0.510031 test loss: 0.691569
[Epoch 18; Iter    26/   32] train: loss: 0.6921370
[Epoch 18] ogbg-molsider: 0.480943 val loss: 0.691749
[Epoch 18] ogbg-molsider: 0.507391 test loss: 0.691390
[Epoch 19; Iter    24/   32] train: loss: 0.6913293
[Epoch 19] ogbg-molsider: 0.481012 val loss: 0.691631
[Epoch 19] ogbg-molsider: 0.506534 test loss: 0.691320
[Epoch 20; Iter    22/   32] train: loss: 0.6920134
[Epoch 20] ogbg-molsider: 0.484407 val loss: 0.691454
[Epoch 20] ogbg-molsider: 0.506598 test loss: 0.691141
[Epoch 21; Iter    20/   32] train: loss: 0.6918858
[Epoch 21] ogbg-molsider: 0.484470 val loss: 0.691310
[Epoch 21] ogbg-molsider: 0.510289 test loss: 0.691086
[Epoch 22; Iter    18/   32] train: loss: 0.6905227
[Epoch 22] ogbg-molsider: 0.506777 val loss: 0.687516
[Epoch 22] ogbg-molsider: 0.538015 test loss: 0.686485
[Epoch 23; Iter    16/   32] train: loss: 0.6846524
[Epoch 23] ogbg-molsider: 0.527740 val loss: 0.666349
[Epoch 23] ogbg-molsider: 0.576659 test loss: 0.666373
[Epoch 24; Iter    14/   32] train: loss: 0.6719886
[Epoch 24] ogbg-molsider: 0.545751 val loss: 0.633159
[Epoch 24] ogbg-molsider: 0.575921 test loss: 0.628858
[Epoch 25; Iter    12/   32] train: loss: 0.6586647
[Epoch 25] ogbg-molsider: 0.537960 val loss: 0.630977
[Epoch 25] ogbg-molsider: 0.585226 test loss: 0.627523
[Epoch 26; Iter    10/   32] train: loss: 0.6468195
[Epoch 26] ogbg-molsider: 0.536712 val loss: 0.587386
[Epoch 26] ogbg-molsider: 0.575126 test loss: 0.585305
[Epoch 27; Iter     8/   32] train: loss: 0.6137840
[Epoch 27] ogbg-molsider: 0.538176 val loss: 0.587469
[Epoch 27] ogbg-molsider: 0.574447 test loss: 0.587136
[Epoch 28; Iter     6/   32] train: loss: 0.5950540
[Epoch 28] ogbg-molsider: 0.541801 val loss: 0.540347
[Epoch 28] ogbg-molsider: 0.580157 test loss: 0.539071
[Epoch 29; Iter     4/   32] train: loss: 0.5714397
[Epoch 29] ogbg-molsider: 0.562332 val loss: 0.564109
[Epoch 29] ogbg-molsider: 0.572507 test loss: 0.564394
[Epoch 30; Iter     2/   32] train: loss: 0.5680802
[Epoch 30; Iter    32/   32] train: loss: 0.4585950
[Epoch 30] ogbg-molsider: 0.560663 val loss: 0.514534
[Epoch 30] ogbg-molsider: 0.589651 test loss: 0.514262
[Epoch 31; Iter    30/   32] train: loss: 0.5219944
[Epoch 31] ogbg-molsider: 0.559703 val loss: 0.506886
[Epoch 31] ogbg-molsider: 0.570507 test loss: 0.519466
[Epoch 32; Iter    28/   32] train: loss: 0.5454106
[Epoch 32] ogbg-molsider: 0.565423 val loss: 0.499994
[Epoch 32] ogbg-molsider: 0.583508 test loss: 0.501373
[Epoch 33; Iter    26/   32] train: loss: 0.4857425
[Epoch 33] ogbg-molsider: 0.562102 val loss: 0.509053
[Epoch 33] ogbg-molsider: 0.579116 test loss: 0.516269
[Epoch 34; Iter    24/   32] train: loss: 0.4986828
[Epoch 34] ogbg-molsider: 0.572871 val loss: 0.497300
[Epoch 34] ogbg-molsider: 0.584120 test loss: 0.504228
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.6/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.6_5_26-05_09-18-14
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.6
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.499947 val loss: 0.693161
[Epoch 1] ogbg-molsider: 0.496774 test loss: 0.693170
[Epoch 2; Iter     3/   27] train: loss: 0.6934064
[Epoch 2] ogbg-molsider: 0.499910 val loss: 0.693181
[Epoch 2] ogbg-molsider: 0.499738 test loss: 0.693210
[Epoch 3; Iter     6/   27] train: loss: 0.6935979
[Epoch 3] ogbg-molsider: 0.500587 val loss: 0.693215
[Epoch 3] ogbg-molsider: 0.502959 test loss: 0.693280
[Epoch 4; Iter     9/   27] train: loss: 0.6938114
[Epoch 4] ogbg-molsider: 0.500945 val loss: 0.693232
[Epoch 4] ogbg-molsider: 0.502482 test loss: 0.693303
[Epoch 5; Iter    12/   27] train: loss: 0.6932006
[Epoch 5] ogbg-molsider: 0.501219 val loss: 0.693167
[Epoch 5] ogbg-molsider: 0.504412 test loss: 0.693213
[Epoch 6; Iter    15/   27] train: loss: 0.6929653
[Epoch 6] ogbg-molsider: 0.501151 val loss: 0.693134
[Epoch 6] ogbg-molsider: 0.500617 test loss: 0.693160
[Epoch 7; Iter    18/   27] train: loss: 0.6928786
[Epoch 7] ogbg-molsider: 0.499683 val loss: 0.693144
[Epoch 7] ogbg-molsider: 0.502388 test loss: 0.693212
[Epoch 8; Iter    21/   27] train: loss: 0.6933773
[Epoch 8] ogbg-molsider: 0.499512 val loss: 0.693037
[Epoch 8] ogbg-molsider: 0.500436 test loss: 0.693093
[Epoch 9; Iter    24/   27] train: loss: 0.6927480
[Epoch 9] ogbg-molsider: 0.500796 val loss: 0.692919
[Epoch 9] ogbg-molsider: 0.503964 test loss: 0.692921
[Epoch 10; Iter    27/   27] train: loss: 0.6932940
[Epoch 10] ogbg-molsider: 0.501909 val loss: 0.692953
[Epoch 10] ogbg-molsider: 0.504006 test loss: 0.693026
[Epoch 11] ogbg-molsider: 0.501565 val loss: 0.692860
[Epoch 11] ogbg-molsider: 0.503810 test loss: 0.692900
[Epoch 12; Iter     3/   27] train: loss: 0.6924648
[Epoch 12] ogbg-molsider: 0.501430 val loss: 0.692846
[Epoch 12] ogbg-molsider: 0.504485 test loss: 0.692917
[Epoch 13; Iter     6/   27] train: loss: 0.6924642
[Epoch 13] ogbg-molsider: 0.501279 val loss: 0.692748
[Epoch 13] ogbg-molsider: 0.503134 test loss: 0.692764
[Epoch 14; Iter     9/   27] train: loss: 0.6929246
[Epoch 14] ogbg-molsider: 0.499564 val loss: 0.692614
[Epoch 14] ogbg-molsider: 0.504746 test loss: 0.692575
[Epoch 15; Iter    12/   27] train: loss: 0.6924263
[Epoch 15] ogbg-molsider: 0.501784 val loss: 0.692533
[Epoch 15] ogbg-molsider: 0.504157 test loss: 0.692504
[Epoch 16; Iter    15/   27] train: loss: 0.6918091
[Epoch 16] ogbg-molsider: 0.501327 val loss: 0.692446
[Epoch 16] ogbg-molsider: 0.505086 test loss: 0.692432
[Epoch 17; Iter    18/   27] train: loss: 0.6925345
[Epoch 17] ogbg-molsider: 0.503057 val loss: 0.692285
[Epoch 17] ogbg-molsider: 0.501100 test loss: 0.692261
[Epoch 18; Iter    21/   27] train: loss: 0.6923370
[Epoch 18] ogbg-molsider: 0.501800 val loss: 0.692217
[Epoch 18] ogbg-molsider: 0.502931 test loss: 0.692148
[Epoch 19; Iter    24/   27] train: loss: 0.6918491
[Epoch 19] ogbg-molsider: 0.502605 val loss: 0.692100
[Epoch 19] ogbg-molsider: 0.505424 test loss: 0.692017
[Epoch 20; Iter    27/   27] train: loss: 0.6917827
[Epoch 20] ogbg-molsider: 0.502647 val loss: 0.691950
[Epoch 20] ogbg-molsider: 0.504384 test loss: 0.691879
[Epoch 21] ogbg-molsider: 0.501911 val loss: 0.691817
[Epoch 21] ogbg-molsider: 0.503618 test loss: 0.691722
[Epoch 22; Iter     3/   27] train: loss: 0.6914945
[Epoch 22] ogbg-molsider: 0.502603 val loss: 0.691681
[Epoch 22] ogbg-molsider: 0.504908 test loss: 0.691604
[Epoch 23; Iter     6/   27] train: loss: 0.6918315
[Epoch 23] ogbg-molsider: 0.502214 val loss: 0.691535
[Epoch 23] ogbg-molsider: 0.505206 test loss: 0.691393
[Epoch 24; Iter     9/   27] train: loss: 0.6914347
[Epoch 24] ogbg-molsider: 0.502798 val loss: 0.691382
[Epoch 24] ogbg-molsider: 0.506344 test loss: 0.691242
[Epoch 25; Iter    12/   27] train: loss: 0.6915730
[Epoch 25] ogbg-molsider: 0.504110 val loss: 0.691221
[Epoch 25] ogbg-molsider: 0.506859 test loss: 0.691022
[Epoch 26; Iter    15/   27] train: loss: 0.6902867
[Epoch 26] ogbg-molsider: 0.535489 val loss: 0.687135
[Epoch 26] ogbg-molsider: 0.534723 test loss: 0.686599
[Epoch 27; Iter    18/   27] train: loss: 0.6843581
[Epoch 27] ogbg-molsider: 0.547353 val loss: 0.675190
[Epoch 27] ogbg-molsider: 0.561340 test loss: 0.672937
[Epoch 28; Iter    21/   27] train: loss: 0.6719441
[Epoch 28] ogbg-molsider: 0.534586 val loss: 0.653703
[Epoch 28] ogbg-molsider: 0.558714 test loss: 0.647144
[Epoch 29; Iter    24/   27] train: loss: 0.6548926
[Epoch 29] ogbg-molsider: 0.540094 val loss: 0.640473
[Epoch 29] ogbg-molsider: 0.565851 test loss: 0.633998
[Epoch 30; Iter    27/   27] train: loss: 0.6253720
[Epoch 30] ogbg-molsider: 0.555213 val loss: 0.600539
[Epoch 30] ogbg-molsider: 0.572243 test loss: 0.590710
[Epoch 31] ogbg-molsider: 0.535427 val loss: 0.586375
[Epoch 31] ogbg-molsider: 0.544521 test loss: 0.573701
[Epoch 32; Iter     3/   27] train: loss: 0.6108266
[Epoch 32] ogbg-molsider: 0.538698 val loss: 0.599352
[Epoch 32] ogbg-molsider: 0.549063 test loss: 0.588302
[Epoch 33; Iter     6/   27] train: loss: 0.5710754
[Epoch 33] ogbg-molsider: 0.545535 val loss: 0.570713
[Epoch 33] ogbg-molsider: 0.562519 test loss: 0.554396
[Epoch 34; Iter     9/   27] train: loss: 0.5566964
[Epoch 34] ogbg-molsider: 0.553668 val loss: 0.545475
[Epoch 34] ogbg-molsider: 0.557966 test loss: 0.529018
[Epoch 35; Iter    12/   27] train: loss: 0.5506182
[Epoch 35] ogbg-molsider: 0.552600 val loss: 0.536146
[Epoch 35] ogbg-molsider: 0.552879 test loss: 0.511853
[Epoch 36; Iter    15/   27] train: loss: 0.5084113
[Epoch 36] ogbg-molsider: 0.567985 val loss: 0.523777
[Epoch 36] ogbg-molsider: 0.574616 test loss: 0.503585
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/sider/scaff/train_prop=0.8/PNA_ogbg-molsider_3DInfomax_sider_scaff=0.8_6_26-05_09-18-14
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_scaff=0.8
logdir: runs/split/3DInfomax/sider/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6931961
[Epoch 1] ogbg-molsider: 0.490884 val loss: 0.693054
[Epoch 1] ogbg-molsider: 0.509410 test loss: 0.692827
[Epoch 2; Iter    24/   36] train: loss: 0.6936196
[Epoch 2] ogbg-molsider: 0.489924 val loss: 0.693133
[Epoch 2] ogbg-molsider: 0.516967 test loss: 0.692477
[Epoch 3; Iter    18/   36] train: loss: 0.6939726
[Epoch 3] ogbg-molsider: 0.491759 val loss: 0.693116
[Epoch 3] ogbg-molsider: 0.511385 test loss: 0.692419
[Epoch 4; Iter    12/   36] train: loss: 0.6926282
[Epoch 4] ogbg-molsider: 0.491560 val loss: 0.693117
[Epoch 4] ogbg-molsider: 0.513804 test loss: 0.692416
[Epoch 5; Iter     6/   36] train: loss: 0.6931393
[Epoch 5; Iter    36/   36] train: loss: 0.6932732
[Epoch 5] ogbg-molsider: 0.489966 val loss: 0.693072
[Epoch 5] ogbg-molsider: 0.509569 test loss: 0.692477
[Epoch 6; Iter    30/   36] train: loss: 0.6932129
[Epoch 6] ogbg-molsider: 0.492337 val loss: 0.692958
[Epoch 6] ogbg-molsider: 0.511323 test loss: 0.692295
[Epoch 7; Iter    24/   36] train: loss: 0.6926952
[Epoch 7] ogbg-molsider: 0.491181 val loss: 0.692911
[Epoch 7] ogbg-molsider: 0.510374 test loss: 0.692301
[Epoch 8; Iter    18/   36] train: loss: 0.6929107
[Epoch 8] ogbg-molsider: 0.488013 val loss: 0.692732
[Epoch 8] ogbg-molsider: 0.510594 test loss: 0.692068
[Epoch 9; Iter    12/   36] train: loss: 0.6928544
[Epoch 9] ogbg-molsider: 0.493199 val loss: 0.692690
[Epoch 9] ogbg-molsider: 0.511060 test loss: 0.692027
[Epoch 10; Iter     6/   36] train: loss: 0.6922631
[Epoch 10; Iter    36/   36] train: loss: 0.6929717
[Epoch 10] ogbg-molsider: 0.492938 val loss: 0.692558
[Epoch 10] ogbg-molsider: 0.513193 test loss: 0.691881
[Epoch 11; Iter    30/   36] train: loss: 0.6932401
[Epoch 11] ogbg-molsider: 0.490834 val loss: 0.692513
[Epoch 11] ogbg-molsider: 0.511071 test loss: 0.691871
[Epoch 12; Iter    24/   36] train: loss: 0.6923122
[Epoch 12] ogbg-molsider: 0.491703 val loss: 0.692338
[Epoch 12] ogbg-molsider: 0.513121 test loss: 0.691658
[Epoch 13; Iter    18/   36] train: loss: 0.6922776
[Epoch 13] ogbg-molsider: 0.491281 val loss: 0.692257
[Epoch 13] ogbg-molsider: 0.513698 test loss: 0.691583
[Epoch 14; Iter    12/   36] train: loss: 0.6925902
[Epoch 14] ogbg-molsider: 0.494063 val loss: 0.691995
[Epoch 14] ogbg-molsider: 0.513760 test loss: 0.691305
[Epoch 15; Iter     6/   36] train: loss: 0.6922340
[Epoch 15; Iter    36/   36] train: loss: 0.6927293
[Epoch 15] ogbg-molsider: 0.490850 val loss: 0.691932
[Epoch 15] ogbg-molsider: 0.511715 test loss: 0.691305
[Epoch 16; Iter    30/   36] train: loss: 0.6916839
[Epoch 16] ogbg-molsider: 0.492200 val loss: 0.691784
[Epoch 16] ogbg-molsider: 0.513469 test loss: 0.691110
[Epoch 17; Iter    24/   36] train: loss: 0.6917000
[Epoch 17] ogbg-molsider: 0.492984 val loss: 0.691490
[Epoch 17] ogbg-molsider: 0.510831 test loss: 0.690845
[Epoch 18; Iter    18/   36] train: loss: 0.6913743
[Epoch 18] ogbg-molsider: 0.492640 val loss: 0.691267
[Epoch 18] ogbg-molsider: 0.510537 test loss: 0.690630
[Epoch 19; Iter    12/   36] train: loss: 0.6915489
[Epoch 19] ogbg-molsider: 0.491967 val loss: 0.691043
[Epoch 19] ogbg-molsider: 0.509310 test loss: 0.690393
[Epoch 20; Iter     6/   36] train: loss: 0.6906611
[Epoch 20; Iter    36/   36] train: loss: 0.6875112
[Epoch 20] ogbg-molsider: 0.545491 val loss: 0.682009
[Epoch 20] ogbg-molsider: 0.565039 test loss: 0.681641
[Epoch 21; Iter    30/   36] train: loss: 0.6754796
[Epoch 21] ogbg-molsider: 0.548758 val loss: 0.654561
[Epoch 21] ogbg-molsider: 0.589446 test loss: 0.654506
[Epoch 22; Iter    24/   36] train: loss: 0.6661252
[Epoch 22] ogbg-molsider: 0.511655 val loss: 0.625296
[Epoch 22] ogbg-molsider: 0.589097 test loss: 0.624218
[Epoch 23; Iter    18/   36] train: loss: 0.6336802
[Epoch 23] ogbg-molsider: 0.517355 val loss: 0.596698
[Epoch 23] ogbg-molsider: 0.585489 test loss: 0.591106
[Epoch 24; Iter    12/   36] train: loss: 0.6128667
[Epoch 24] ogbg-molsider: 0.537016 val loss: 0.559121
[Epoch 24] ogbg-molsider: 0.600061 test loss: 0.552700
[Epoch 25; Iter     6/   36] train: loss: 0.5809156
[Epoch 25; Iter    36/   36] train: loss: 0.5864478
[Epoch 25] ogbg-molsider: 0.520711 val loss: 0.539366
[Epoch 25] ogbg-molsider: 0.604724 test loss: 0.535821
[Epoch 26; Iter    30/   36] train: loss: 0.5590328
[Epoch 26] ogbg-molsider: 0.539743 val loss: 0.537725
[Epoch 26] ogbg-molsider: 0.594552 test loss: 0.537141
[Epoch 27; Iter    24/   36] train: loss: 0.5181759
[Epoch 27] ogbg-molsider: 0.532578 val loss: 0.515637
[Epoch 27] ogbg-molsider: 0.574374 test loss: 0.523513
[Epoch 28; Iter    18/   36] train: loss: 0.4803813
[Epoch 28] ogbg-molsider: 0.579646 val loss: 0.487071
[Epoch 28] ogbg-molsider: 0.588851 test loss: 0.502601
[Epoch 29; Iter    12/   36] train: loss: 0.5443570
[Epoch 29] ogbg-molsider: 0.574885 val loss: 0.488329
[Epoch 29] ogbg-molsider: 0.597064 test loss: 0.499656
[Epoch 30; Iter     6/   36] train: loss: 0.5037438
[Epoch 30; Iter    36/   36] train: loss: 0.4780190
[Epoch 30] ogbg-molsider: 0.582283 val loss: 0.486423
[Epoch 30] ogbg-molsider: 0.585864 test loss: 0.505241
[Epoch 31; Iter    30/   36] train: loss: 0.5053059
[Epoch 31] ogbg-molsider: 0.526004 val loss: 0.489297
[Epoch 31] ogbg-molsider: 0.629304 test loss: 0.485332
[Epoch 32; Iter    24/   36] train: loss: 0.4848566
[Epoch 32] ogbg-molsider: 0.601409 val loss: 0.474621
[Epoch 32] ogbg-molsider: 0.583086 test loss: 0.496300
[Epoch 33; Iter    18/   36] train: loss: 0.5130961
[Epoch 33] ogbg-molsider: 0.547576 val loss: 0.486108
[Epoch 33] ogbg-molsider: 0.602707 test loss: 0.494972
[Epoch 34; Iter    12/   36] train: loss: 0.4867727
[Epoch 34] ogbg-molsider: 0.520957 val loss: 0.496823
[Epoch 34] ogbg-molsider: 0.595440 test loss: 0.500432
[Epoch 35; Iter     6/   36] train: loss: 0.5237737
[Epoch 35; Iter    36/   36] train: loss: 0.4711577
[Epoch 35] ogbg-molsider: 0.556913 val loss: 0.504872
[Epoch 35] ogbg-molsider: 0.597765 test loss: 0.520603
[Epoch 36; Iter    30/   36] train: loss: 0.4637320
[Epoch 36] ogbg-molsider: 0.573780 val loss: 0.500335
[Epoch 36] ogbg-molsider: 0.591040 test loss: 0.518500
[Epoch 37; Iter    24/   36] train: loss: 0.5352866
[Epoch 37] ogbg-molsider: 0.571040 val loss: 0.480899
[Epoch 37] ogbg-molsider: 0.602553 test loss: 0.500402
[Epoch 38; Iter    18/   36] train: loss: 0.5077721
[Epoch 38] ogbg-molsider: 0.587733 val loss: 0.475209
[Epoch 38] ogbg-molsider: 0.596485 test loss: 0.487823
[Epoch 39; Iter    12/   36] train: loss: 0.4880374
[Epoch 39] ogbg-molsider: 0.605729 val loss: 0.477845
[Epoch 39] ogbg-molsider: 0.600988 test loss: 0.514808
[Epoch 40; Iter     6/   36] train: loss: 0.4686771
[Epoch 40; Iter    36/   36] train: loss: 0.5064388
[Epoch 40] ogbg-molsider: 0.495631 val loss: 0.518636
[Epoch 40] ogbg-molsider: 0.592754 test loss: 0.530677
[Epoch 41; Iter    30/   36] train: loss: 0.5318211
[Epoch 41] ogbg-molsider: 0.575294 val loss: 1.389043
[Epoch 41] ogbg-molsider: 0.564338 test loss: 1.514371
[Epoch 42; Iter    24/   36] train: loss: 0.4947645
[Epoch 42] ogbg-molsider: 0.557864 val loss: 0.501407
[Epoch 42] ogbg-molsider: 0.598756 test loss: 0.525923
[Epoch 43; Iter    18/   36] train: loss: 0.4837304
[Epoch 43] ogbg-molsider: 0.564908 val loss: 0.514066
[Epoch 43] ogbg-molsider: 0.607137 test loss: 0.550482
[Epoch 44; Iter    12/   36] train: loss: 0.4811908
[Epoch 44] ogbg-molsider: 0.543602 val loss: 0.494915
[Epoch 44] ogbg-molsider: 0.622799 test loss: 0.493003
[Epoch 45; Iter     6/   36] train: loss: 0.4912101
[Epoch 45; Iter    36/   36] train: loss: 0.4361289
[Epoch 45] ogbg-molsider: 0.557262 val loss: 0.492139
[Epoch 45] ogbg-molsider: 0.599218 test loss: 0.501436
[Epoch 46; Iter    30/   36] train: loss: 0.4883383
[Epoch 46] ogbg-molsider: 0.543029 val loss: 0.527831
[Epoch 46] ogbg-molsider: 0.603339 test loss: 0.610612
[Epoch 47; Iter    24/   36] train: loss: 0.4903899
[Epoch 47] ogbg-molsider: 0.612393 val loss: 0.535570
[Epoch 47] ogbg-molsider: 0.603960 test loss: 0.603363
[Epoch 48; Iter    18/   36] train: loss: 0.4432741
[Epoch 48] ogbg-molsider: 0.563363 val loss: 0.528665
[Epoch 48] ogbg-molsider: 0.605013 test loss: 0.575048
[Epoch 49; Iter    12/   36] train: loss: 0.4542834
[Epoch 49] ogbg-molsider: 0.588161 val loss: 0.501902
[Epoch 49] ogbg-molsider: 0.606243 test loss: 0.527275
[Epoch 50; Iter     6/   36] train: loss: 0.4575173
[Epoch 50; Iter    36/   36] train: loss: 0.4901491
[Epoch 50] ogbg-molsider: 0.585971 val loss: 0.497741
[Epoch 50] ogbg-molsider: 0.616843 test loss: 0.503215
[Epoch 51; Iter    30/   36] train: loss: 0.5231488
[Epoch 51] ogbg-molsider: 0.575550 val loss: 0.547279
[Epoch 51] ogbg-molsider: 0.597610 test loss: 0.543343
[Epoch 52; Iter    24/   36] train: loss: 0.4573604
[Epoch 52] ogbg-molsider: 0.595865 val loss: 0.501349
[Epoch 52] ogbg-molsider: 0.603819 test loss: 0.524345
[Epoch 53; Iter    18/   36] train: loss: 0.4490271
[Epoch 53] ogbg-molsider: 0.602281 val loss: 0.501182
[Epoch 53] ogbg-molsider: 0.585566 test loss: 0.537229
[Epoch 54; Iter    12/   36] train: loss: 0.4489627
[Epoch 54] ogbg-molsider: 0.568608 val loss: 0.498370
[Epoch 54] ogbg-molsider: 0.579470 test loss: 0.525484
[Epoch 55; Iter     6/   36] train: loss: 0.4703306
[Epoch 55; Iter    36/   36] train: loss: 0.4544548
[Epoch 55] ogbg-molsider: 0.596704 val loss: 0.551024
[Epoch 55] ogbg-molsider: 0.574176 test loss: 0.522414
[Epoch 56; Iter    30/   36] train: loss: 0.4259016
[Epoch 56] ogbg-molsider: 0.625468 val loss: 0.475085
[Epoch 56] ogbg-molsider: 0.576402 test loss: 0.519046
[Epoch 57; Iter    24/   36] train: loss: 0.4285305
[Epoch 57] ogbg-molsider: 0.579904 val loss: 0.514998
[Epoch 57] ogbg-molsider: 0.621135 test loss: 0.520542
[Epoch 58; Iter    18/   36] train: loss: 0.3852863
[Epoch 58] ogbg-molsider: 0.636542 val loss: 0.484931
[Epoch 58] ogbg-molsider: 0.594957 test loss: 0.529239
[Epoch 59; Iter    12/   36] train: loss: 0.4541280
[Epoch 59] ogbg-molsider: 0.545262 val loss: 0.530164
[Epoch 59] ogbg-molsider: 0.600355 test loss: 0.549980
[Epoch 60; Iter     6/   36] train: loss: 0.4848004
[Epoch 60; Iter    36/   36] train: loss: 0.3894241
[Epoch 60] ogbg-molsider: 0.597675 val loss: 0.503450
[Epoch 60] ogbg-molsider: 0.605460 test loss: 0.533052
[Epoch 61; Iter    30/   36] train: loss: 0.3852949
[Epoch 61] ogbg-molsider: 0.615941 val loss: 0.509815
[Epoch 61] ogbg-molsider: 0.599579 test loss: 0.542336
[Epoch 62; Iter    24/   36] train: loss: 0.4158817
[Epoch 62] ogbg-molsider: 0.622169 val loss: 0.494051
[Epoch 62] ogbg-molsider: 0.617491 test loss: 0.535273
[Epoch 63; Iter    18/   36] train: loss: 0.4522949
[Epoch 63] ogbg-molsider: 0.626502 val loss: 0.515627
[Epoch 63] ogbg-molsider: 0.570250 test loss: 0.578780
[Epoch 64; Iter    12/   36] train: loss: 0.4052784
[Epoch 64] ogbg-molsider: 0.637837 val loss: 0.546793
[Epoch 64] ogbg-molsider: 0.604503 test loss: 0.599827
[Epoch 65; Iter     6/   36] train: loss: 0.3703150
[Epoch 65; Iter    36/   36] train: loss: 0.4722776
[Epoch 65] ogbg-molsider: 0.580376 val loss: 0.558511
[Epoch 65] ogbg-molsider: 0.589899 test loss: 0.615509
[Epoch 66; Iter    30/   36] train: loss: 0.4050733
[Epoch 66] ogbg-molsider: 0.614399 val loss: 0.520436
[Epoch 66] ogbg-molsider: 0.591141 test loss: 0.570004
[Epoch 67; Iter    24/   36] train: loss: 0.4073486
[Epoch 67] ogbg-molsider: 0.648264 val loss: 0.508435
[Epoch 67] ogbg-molsider: 0.585083 test loss: 0.557461
[Epoch 68; Iter    18/   36] train: loss: 0.4043765
[Epoch 68] ogbg-molsider: 0.605935 val loss: 0.519133
[Epoch 68] ogbg-molsider: 0.590465 test loss: 0.573005
[Epoch 69; Iter    12/   36] train: loss: 0.3659666
[Epoch 69] ogbg-molsider: 0.628321 val loss: 0.526622
[Epoch 69] ogbg-molsider: 0.579695 test loss: 0.602932
[Epoch 70; Iter     6/   36] train: loss: 0.3482506
[Epoch 70; Iter    36/   36] train: loss: 0.3666950
[Epoch 70] ogbg-molsider: 0.631167 val loss: 0.534473
[Epoch 70] ogbg-molsider: 0.590024 test loss: 0.592264
[Epoch 71; Iter    30/   36] train: loss: 0.3786989
[Epoch 71] ogbg-molsider: 0.628658 val loss: 0.516739
[Epoch 71] ogbg-molsider: 0.580719 test loss: 0.577660
[Epoch 72; Iter    24/   36] train: loss: 0.3664230
[Epoch 72] ogbg-molsider: 0.616646 val loss: 0.529278
[Epoch 72] ogbg-molsider: 0.551791 test loss: 0.613019
[Epoch 73; Iter    18/   36] train: loss: 0.4064386
[Epoch 73] ogbg-molsider: 0.622070 val loss: 0.595645
[Epoch 73] ogbg-molsider: 0.576468 test loss: 0.588132
[Epoch 74; Iter    12/   36] train: loss: 0.3205985
[Epoch 74] ogbg-molsider: 0.614678 val loss: 0.532549
[Epoch 74] ogbg-molsider: 0.581191 test loss: 0.543017
[Epoch 75; Iter     6/   36] train: loss: 0.3534698
[Epoch 75; Iter    36/   36] train: loss: 0.3676403
[Epoch 75] ogbg-molsider: 0.590676 val loss: 0.532160
[Epoch 75] ogbg-molsider: 0.564539 test loss: 0.569417
[Epoch 76; Iter    30/   36] train: loss: 0.3927782
[Epoch 76] ogbg-molsider: 0.611191 val loss: 0.577336
[Epoch 76] ogbg-molsider: 0.583675 test loss: 0.700169
[Epoch 77; Iter    24/   36] train: loss: 0.3897567
[Epoch 77] ogbg-molsider: 0.629911 val loss: 0.556187
[Epoch 77] ogbg-molsider: 0.602247 test loss: 0.604401
[Epoch 78; Iter    18/   36] train: loss: 0.3463294
[Epoch 78] ogbg-molsider: 0.613316 val loss: 0.546640
[Epoch 78] ogbg-molsider: 0.583957 test loss: 0.603842
[Epoch 79; Iter    12/   36] train: loss: 0.3413166
[Epoch 79] ogbg-molsider: 0.625152 val loss: 0.569847
[Epoch 79] ogbg-molsider: 0.574319 test loss: 0.654706
[Epoch 80; Iter     6/   36] train: loss: 0.3536613
[Epoch 80; Iter    36/   36] train: loss: 0.3704253
[Epoch 80] ogbg-molsider: 0.649831 val loss: 0.523388
[Epoch 80] ogbg-molsider: 0.583914 test loss: 0.590540
[Epoch 33] ogbg-molsider: 0.616581 test loss: 0.499908
[Epoch 34; Iter    12/   36] train: loss: 0.4898159
[Epoch 34] ogbg-molsider: 0.586187 val loss: 0.481821
[Epoch 34] ogbg-molsider: 0.594765 test loss: 0.497477
[Epoch 35; Iter     6/   36] train: loss: 0.5295611
[Epoch 35; Iter    36/   36] train: loss: 0.5028744
[Epoch 35] ogbg-molsider: 0.571637 val loss: 0.488452
[Epoch 35] ogbg-molsider: 0.580962 test loss: 0.512383
[Epoch 36; Iter    30/   36] train: loss: 0.4725968
[Epoch 36] ogbg-molsider: 0.538202 val loss: 0.498147
[Epoch 36] ogbg-molsider: 0.606638 test loss: 0.493132
[Epoch 37; Iter    24/   36] train: loss: 0.4652956
[Epoch 37] ogbg-molsider: 0.537806 val loss: 0.490809
[Epoch 37] ogbg-molsider: 0.607198 test loss: 0.492479
[Epoch 38; Iter    18/   36] train: loss: 0.4953361
[Epoch 38] ogbg-molsider: 0.571051 val loss: 0.481175
[Epoch 38] ogbg-molsider: 0.605725 test loss: 0.494077
[Epoch 39; Iter    12/   36] train: loss: 0.5031835
[Epoch 39] ogbg-molsider: 0.558115 val loss: 0.506486
[Epoch 39] ogbg-molsider: 0.625812 test loss: 0.517705
[Epoch 40; Iter     6/   36] train: loss: 0.4637304
[Epoch 40; Iter    36/   36] train: loss: 0.5268506
[Epoch 40] ogbg-molsider: 0.563060 val loss: 0.488879
[Epoch 40] ogbg-molsider: 0.640619 test loss: 0.483491
[Epoch 41; Iter    30/   36] train: loss: 0.5107144
[Epoch 41] ogbg-molsider: 0.602595 val loss: 0.480801
[Epoch 41] ogbg-molsider: 0.617011 test loss: 0.505249
[Epoch 42; Iter    24/   36] train: loss: 0.4684023
[Epoch 42] ogbg-molsider: 0.604851 val loss: 0.505136
[Epoch 42] ogbg-molsider: 0.614785 test loss: 0.512963
[Epoch 43; Iter    18/   36] train: loss: 0.5029046
[Epoch 43] ogbg-molsider: 0.567628 val loss: 0.481198
[Epoch 43] ogbg-molsider: 0.612451 test loss: 0.490050
[Epoch 44; Iter    12/   36] train: loss: 0.4417697
[Epoch 44] ogbg-molsider: 0.596039 val loss: 0.489740
[Epoch 44] ogbg-molsider: 0.609421 test loss: 0.518352
[Epoch 45; Iter     6/   36] train: loss: 0.4802251
[Epoch 45; Iter    36/   36] train: loss: 0.4811678
[Epoch 45] ogbg-molsider: 0.558608 val loss: 0.494060
[Epoch 45] ogbg-molsider: 0.619796 test loss: 0.514142
[Epoch 46; Iter    30/   36] train: loss: 0.5170951
[Epoch 46] ogbg-molsider: 0.597523 val loss: 0.493594
[Epoch 46] ogbg-molsider: 0.584934 test loss: 0.525066
[Epoch 47; Iter    24/   36] train: loss: 0.4440258
[Epoch 47] ogbg-molsider: 0.601628 val loss: 0.483559
[Epoch 47] ogbg-molsider: 0.590126 test loss: 0.511832
[Epoch 48; Iter    18/   36] train: loss: 0.4945179
[Epoch 48] ogbg-molsider: 0.601932 val loss: 0.496374
[Epoch 48] ogbg-molsider: 0.597318 test loss: 0.538592
[Epoch 49; Iter    12/   36] train: loss: 0.4796662
[Epoch 49] ogbg-molsider: 0.613363 val loss: 0.482319
[Epoch 49] ogbg-molsider: 0.615023 test loss: 0.500785
[Epoch 50; Iter     6/   36] train: loss: 0.4752317
[Epoch 50; Iter    36/   36] train: loss: 0.5336662
[Epoch 50] ogbg-molsider: 0.599650 val loss: 0.487775
[Epoch 50] ogbg-molsider: 0.603528 test loss: 0.513298
[Epoch 51; Iter    30/   36] train: loss: 0.5055485
[Epoch 51] ogbg-molsider: 0.622522 val loss: 0.472755
[Epoch 51] ogbg-molsider: 0.590926 test loss: 0.519570
[Epoch 52; Iter    24/   36] train: loss: 0.4472384
[Epoch 52] ogbg-molsider: 0.624611 val loss: 0.494413
[Epoch 52] ogbg-molsider: 0.623473 test loss: 0.537104
[Epoch 53; Iter    18/   36] train: loss: 0.4576278
[Epoch 53] ogbg-molsider: 0.603354 val loss: 0.504733
[Epoch 53] ogbg-molsider: 0.575024 test loss: 0.529098
[Epoch 54; Iter    12/   36] train: loss: 0.4236245
[Epoch 54] ogbg-molsider: 0.609301 val loss: 0.491255
[Epoch 54] ogbg-molsider: 0.576818 test loss: 0.553565
[Epoch 55; Iter     6/   36] train: loss: 0.4748398
[Epoch 55; Iter    36/   36] train: loss: 0.5165634
[Epoch 55] ogbg-molsider: 0.614770 val loss: 0.492946
[Epoch 55] ogbg-molsider: 0.628459 test loss: 0.507948
[Epoch 56; Iter    30/   36] train: loss: 0.4416879
[Epoch 56] ogbg-molsider: 0.550817 val loss: 3.612915
[Epoch 56] ogbg-molsider: 0.525728 test loss: 5.349675
[Epoch 57; Iter    24/   36] train: loss: 0.4534627
[Epoch 57] ogbg-molsider: 0.589292 val loss: 0.529153
[Epoch 57] ogbg-molsider: 0.593914 test loss: 0.620696
[Epoch 58; Iter    18/   36] train: loss: 0.4733444
[Epoch 58] ogbg-molsider: 0.613620 val loss: 0.494730
[Epoch 58] ogbg-molsider: 0.577134 test loss: 0.548176
[Epoch 59; Iter    12/   36] train: loss: 0.4196991
[Epoch 59] ogbg-molsider: 0.574920 val loss: 0.616680
[Epoch 59] ogbg-molsider: 0.588970 test loss: 0.594555
[Epoch 60; Iter     6/   36] train: loss: 0.4351462
[Epoch 60; Iter    36/   36] train: loss: 0.4579373
[Epoch 60] ogbg-molsider: 0.570909 val loss: 0.519682
[Epoch 60] ogbg-molsider: 0.601406 test loss: 0.549137
[Epoch 61; Iter    30/   36] train: loss: 0.4733789
[Epoch 61] ogbg-molsider: 0.606970 val loss: 0.497690
[Epoch 61] ogbg-molsider: 0.586149 test loss: 0.523895
[Epoch 62; Iter    24/   36] train: loss: 0.3960853
[Epoch 62] ogbg-molsider: 0.627818 val loss: 0.489692
[Epoch 62] ogbg-molsider: 0.595789 test loss: 0.520629
[Epoch 63; Iter    18/   36] train: loss: 0.4564852
[Epoch 63] ogbg-molsider: 0.607340 val loss: 0.502624
[Epoch 63] ogbg-molsider: 0.579564 test loss: 0.574601
[Epoch 64; Iter    12/   36] train: loss: 0.3842510
[Epoch 64] ogbg-molsider: 0.635771 val loss: 0.483997
[Epoch 64] ogbg-molsider: 0.566509 test loss: 0.529239
[Epoch 65; Iter     6/   36] train: loss: 0.3482964
[Epoch 65; Iter    36/   36] train: loss: 0.3968509
[Epoch 65] ogbg-molsider: 0.635730 val loss: 0.642601
[Epoch 65] ogbg-molsider: 0.548414 test loss: 0.852024
[Epoch 66; Iter    30/   36] train: loss: 0.3696787
[Epoch 66] ogbg-molsider: 0.614961 val loss: 0.537201
[Epoch 66] ogbg-molsider: 0.552083 test loss: 0.564465
[Epoch 67; Iter    24/   36] train: loss: 0.4034456
[Epoch 67] ogbg-molsider: 0.624145 val loss: 0.515893
[Epoch 67] ogbg-molsider: 0.581736 test loss: 0.538652
[Epoch 68; Iter    18/   36] train: loss: 0.4021184
[Epoch 68] ogbg-molsider: 0.620393 val loss: 0.533406
[Epoch 68] ogbg-molsider: 0.561352 test loss: 0.604164
[Epoch 69; Iter    12/   36] train: loss: 0.3865334
[Epoch 69] ogbg-molsider: 0.644069 val loss: 0.499981
[Epoch 69] ogbg-molsider: 0.588487 test loss: 0.536319
[Epoch 70; Iter     6/   36] train: loss: 0.3941951
[Epoch 70; Iter    36/   36] train: loss: 0.3811295
[Epoch 70] ogbg-molsider: 0.626508 val loss: 0.530201
[Epoch 70] ogbg-molsider: 0.553167 test loss: 0.584354
[Epoch 71; Iter    30/   36] train: loss: 0.3785518
[Epoch 71] ogbg-molsider: 0.599992 val loss: 0.543016
[Epoch 71] ogbg-molsider: 0.611876 test loss: 0.559551
[Epoch 72; Iter    24/   36] train: loss: 0.3811829
[Epoch 72] ogbg-molsider: 0.629182 val loss: 0.520353
[Epoch 72] ogbg-molsider: 0.601594 test loss: 0.544360
[Epoch 73; Iter    18/   36] train: loss: 0.3915433
[Epoch 73] ogbg-molsider: 0.639996 val loss: 0.516591
[Epoch 73] ogbg-molsider: 0.579158 test loss: 0.570257
[Epoch 74; Iter    12/   36] train: loss: 0.3469166
[Epoch 74] ogbg-molsider: 0.653658 val loss: 0.517360
[Epoch 74] ogbg-molsider: 0.578131 test loss: 0.568363
[Epoch 75; Iter     6/   36] train: loss: 0.3530555
[Epoch 75; Iter    36/   36] train: loss: 0.3992401
[Epoch 75] ogbg-molsider: 0.643501 val loss: 0.572957
[Epoch 75] ogbg-molsider: 0.582868 test loss: 0.585972
[Epoch 76; Iter    30/   36] train: loss: 0.3481050
[Epoch 76] ogbg-molsider: 0.598953 val loss: 0.536829
[Epoch 76] ogbg-molsider: 0.582093 test loss: 0.571428
[Epoch 77; Iter    24/   36] train: loss: 0.3729014
[Epoch 77] ogbg-molsider: 0.636925 val loss: 0.840694
[Epoch 77] ogbg-molsider: 0.584837 test loss: 0.577426
[Epoch 78; Iter    18/   36] train: loss: 0.3636351
[Epoch 78] ogbg-molsider: 0.631605 val loss: 0.518883
[Epoch 78] ogbg-molsider: 0.575838 test loss: 0.571193
[Epoch 79; Iter    12/   36] train: loss: 0.3325397
[Epoch 79] ogbg-molsider: 0.641673 val loss: 0.551443
[Epoch 79] ogbg-molsider: 0.574129 test loss: 0.596346
[Epoch 80; Iter     6/   36] train: loss: 0.3734498
[Epoch 80; Iter    36/   36] train: loss: 0.3595803
[Epoch 80] ogbg-molsider: 0.621586 val loss: 0.582163
[Epoch 80] ogbg-molsider: 0.574998 test loss: 0.613441
[Epoch 35; Iter    22/   32] train: loss: 0.5153242
[Epoch 35] ogbg-molsider: 0.574725 val loss: 0.491019
[Epoch 35] ogbg-molsider: 0.573588 test loss: 0.518081
[Epoch 36; Iter    20/   32] train: loss: 0.5202175
[Epoch 36] ogbg-molsider: 0.563919 val loss: 0.492701
[Epoch 36] ogbg-molsider: 0.588335 test loss: 0.510479
[Epoch 37; Iter    18/   32] train: loss: 0.5003091
[Epoch 37] ogbg-molsider: 0.572977 val loss: 0.482336
[Epoch 37] ogbg-molsider: 0.594861 test loss: 0.500016
[Epoch 38; Iter    16/   32] train: loss: 0.5115910
[Epoch 38] ogbg-molsider: 0.567391 val loss: 0.483221
[Epoch 38] ogbg-molsider: 0.588763 test loss: 0.507978
[Epoch 39; Iter    14/   32] train: loss: 0.4713720
[Epoch 39] ogbg-molsider: 0.574290 val loss: 0.484452
[Epoch 39] ogbg-molsider: 0.575178 test loss: 0.528134
[Epoch 40; Iter    12/   32] train: loss: 0.4825018
[Epoch 40] ogbg-molsider: 0.557836 val loss: 0.488817
[Epoch 40] ogbg-molsider: 0.583273 test loss: 0.503316
[Epoch 41; Iter    10/   32] train: loss: 0.4553173
[Epoch 41] ogbg-molsider: 0.559240 val loss: 0.494154
[Epoch 41] ogbg-molsider: 0.585623 test loss: 0.540811
[Epoch 42; Iter     8/   32] train: loss: 0.4432299
[Epoch 42] ogbg-molsider: 0.562857 val loss: 0.492260
[Epoch 42] ogbg-molsider: 0.578987 test loss: 0.511867
[Epoch 43; Iter     6/   32] train: loss: 0.4770342
[Epoch 43] ogbg-molsider: 0.564887 val loss: 0.490052
[Epoch 43] ogbg-molsider: 0.598569 test loss: 0.502066
[Epoch 44; Iter     4/   32] train: loss: 0.4486136
[Epoch 44] ogbg-molsider: 0.531838 val loss: 0.527975
[Epoch 44] ogbg-molsider: 0.554883 test loss: 0.533686
[Epoch 45; Iter     2/   32] train: loss: 0.5151008
[Epoch 45; Iter    32/   32] train: loss: 0.5430180
[Epoch 45] ogbg-molsider: 0.562179 val loss: 0.523320
[Epoch 45] ogbg-molsider: 0.585004 test loss: 0.574382
[Epoch 46; Iter    30/   32] train: loss: 0.4412756
[Epoch 46] ogbg-molsider: 0.562284 val loss: 0.514730
[Epoch 46] ogbg-molsider: 0.599689 test loss: 0.532232
[Epoch 47; Iter    28/   32] train: loss: 0.4732576
[Epoch 47] ogbg-molsider: 0.586163 val loss: 0.491594
[Epoch 47] ogbg-molsider: 0.575487 test loss: 0.520426
[Epoch 48; Iter    26/   32] train: loss: 0.4328130
[Epoch 48] ogbg-molsider: 0.579760 val loss: 0.514179
[Epoch 48] ogbg-molsider: 0.576760 test loss: 0.634802
[Epoch 49; Iter    24/   32] train: loss: 0.4784222
[Epoch 49] ogbg-molsider: 0.583284 val loss: 0.502747
[Epoch 49] ogbg-molsider: 0.609006 test loss: 0.540552
[Epoch 50; Iter    22/   32] train: loss: 0.4656250
[Epoch 50] ogbg-molsider: 0.572664 val loss: 0.499755
[Epoch 50] ogbg-molsider: 0.601093 test loss: 0.540761
[Epoch 51; Iter    20/   32] train: loss: 0.4934641
[Epoch 51] ogbg-molsider: 0.574018 val loss: 0.499059
[Epoch 51] ogbg-molsider: 0.561678 test loss: 0.530020
[Epoch 52; Iter    18/   32] train: loss: 0.4747507
[Epoch 52] ogbg-molsider: 0.561542 val loss: 0.501167
[Epoch 52] ogbg-molsider: 0.581327 test loss: 0.536037
[Epoch 53; Iter    16/   32] train: loss: 0.5048888
[Epoch 53] ogbg-molsider: 0.582470 val loss: 0.526723
[Epoch 53] ogbg-molsider: 0.611501 test loss: 0.555618
[Epoch 54; Iter    14/   32] train: loss: 0.4861332
[Epoch 54] ogbg-molsider: 0.563152 val loss: 0.500556
[Epoch 54] ogbg-molsider: 0.576799 test loss: 0.534694
[Epoch 55; Iter    12/   32] train: loss: 0.4650675
[Epoch 55] ogbg-molsider: 0.577542 val loss: 0.500163
[Epoch 55] ogbg-molsider: 0.607503 test loss: 0.526026
[Epoch 56; Iter    10/   32] train: loss: 0.4985848
[Epoch 56] ogbg-molsider: 0.585948 val loss: 0.485757
[Epoch 56] ogbg-molsider: 0.603508 test loss: 0.497408
[Epoch 57; Iter     8/   32] train: loss: 0.3784638
[Epoch 57] ogbg-molsider: 0.580281 val loss: 0.510899
[Epoch 57] ogbg-molsider: 0.589399 test loss: 0.556976
[Epoch 58; Iter     6/   32] train: loss: 0.4207233
[Epoch 58] ogbg-molsider: 0.576050 val loss: 0.507935
[Epoch 58] ogbg-molsider: 0.621065 test loss: 0.502568
[Epoch 59; Iter     4/   32] train: loss: 0.4240092
[Epoch 59] ogbg-molsider: 0.593145 val loss: 0.500141
[Epoch 59] ogbg-molsider: 0.584813 test loss: 0.541696
[Epoch 60; Iter     2/   32] train: loss: 0.4452585
[Epoch 60; Iter    32/   32] train: loss: 0.4401498
[Epoch 60] ogbg-molsider: 0.606946 val loss: 0.493542
[Epoch 60] ogbg-molsider: 0.622734 test loss: 0.523012
[Epoch 61; Iter    30/   32] train: loss: 0.4564328
[Epoch 61] ogbg-molsider: 0.580949 val loss: 0.489767
[Epoch 61] ogbg-molsider: 0.608661 test loss: 0.506822
[Epoch 62; Iter    28/   32] train: loss: 0.4849127
[Epoch 62] ogbg-molsider: 0.584860 val loss: 0.509108
[Epoch 62] ogbg-molsider: 0.594601 test loss: 0.523191
[Epoch 63; Iter    26/   32] train: loss: 0.4675457
[Epoch 63] ogbg-molsider: 0.598514 val loss: 0.502894
[Epoch 63] ogbg-molsider: 0.580296 test loss: 0.563979
[Epoch 64; Iter    24/   32] train: loss: 0.4265848
[Epoch 64] ogbg-molsider: 0.599933 val loss: 0.512300
[Epoch 64] ogbg-molsider: 0.609904 test loss: 0.534071
[Epoch 65; Iter    22/   32] train: loss: 0.4545702
[Epoch 65] ogbg-molsider: 0.606974 val loss: 0.521900
[Epoch 65] ogbg-molsider: 0.582762 test loss: 0.534722
[Epoch 66; Iter    20/   32] train: loss: 0.4712764
[Epoch 66] ogbg-molsider: 0.598472 val loss: 0.506905
[Epoch 66] ogbg-molsider: 0.571836 test loss: 0.547326
[Epoch 67; Iter    18/   32] train: loss: 0.4428850
[Epoch 67] ogbg-molsider: 0.580014 val loss: 0.540173
[Epoch 67] ogbg-molsider: 0.586888 test loss: 0.594138
[Epoch 68; Iter    16/   32] train: loss: 0.3928508
[Epoch 68] ogbg-molsider: 0.599165 val loss: 0.527426
[Epoch 68] ogbg-molsider: 0.608383 test loss: 0.540702
[Epoch 69; Iter    14/   32] train: loss: 0.4064375
[Epoch 69] ogbg-molsider: 0.608984 val loss: 0.545974
[Epoch 69] ogbg-molsider: 0.569199 test loss: 0.631390
[Epoch 70; Iter    12/   32] train: loss: 0.3897237
[Epoch 70] ogbg-molsider: 0.591071 val loss: 0.554956
[Epoch 70] ogbg-molsider: 0.584438 test loss: 0.558155
[Epoch 71; Iter    10/   32] train: loss: 0.4261568
[Epoch 71] ogbg-molsider: 0.582775 val loss: 0.522918
[Epoch 71] ogbg-molsider: 0.553869 test loss: 0.576706
[Epoch 72; Iter     8/   32] train: loss: 0.4030096
[Epoch 72] ogbg-molsider: 0.587509 val loss: 0.573534
[Epoch 72] ogbg-molsider: 0.586177 test loss: 0.580374
[Epoch 73; Iter     6/   32] train: loss: 0.5344583
[Epoch 73] ogbg-molsider: 0.579738 val loss: 0.555238
[Epoch 73] ogbg-molsider: 0.598101 test loss: 0.694497
[Epoch 74; Iter     4/   32] train: loss: 0.4808885
[Epoch 74] ogbg-molsider: 0.613029 val loss: 0.491078
[Epoch 74] ogbg-molsider: 0.600589 test loss: 0.521765
[Epoch 75; Iter     2/   32] train: loss: 0.4207588
[Epoch 75; Iter    32/   32] train: loss: 0.4983334
[Epoch 75] ogbg-molsider: 0.580380 val loss: 0.533251
[Epoch 75] ogbg-molsider: 0.579362 test loss: 0.561868
[Epoch 76; Iter    30/   32] train: loss: 0.4229543
[Epoch 76] ogbg-molsider: 0.588454 val loss: 0.521825
[Epoch 76] ogbg-molsider: 0.582713 test loss: 0.531711
[Epoch 77; Iter    28/   32] train: loss: 0.4248984
[Epoch 77] ogbg-molsider: 0.581524 val loss: 0.523427
[Epoch 77] ogbg-molsider: 0.569475 test loss: 0.543620
[Epoch 78; Iter    26/   32] train: loss: 0.3646850
[Epoch 78] ogbg-molsider: 0.572109 val loss: 0.523462
[Epoch 78] ogbg-molsider: 0.555098 test loss: 0.556437
[Epoch 79; Iter    24/   32] train: loss: 0.4158010
[Epoch 79] ogbg-molsider: 0.578316 val loss: 0.535576
[Epoch 79] ogbg-molsider: 0.565353 test loss: 0.564722
[Epoch 80; Iter    22/   32] train: loss: 0.4000597
[Epoch 80] ogbg-molsider: 0.598952 val loss: 0.524577
[Epoch 80] ogbg-molsider: 0.573436 test loss: 0.537598
[Epoch 81; Iter    20/   32] train: loss: 0.3805050
[Epoch 81] ogbg-molsider: 0.582841 val loss: 0.538521
[Epoch 81] ogbg-molsider: 0.548657 test loss: 0.565370
[Epoch 82; Iter    18/   32] train: loss: 0.4161046
[Epoch 82] ogbg-molsider: 0.583171 val loss: 0.537201
[Epoch 82] ogbg-molsider: 0.553024 test loss: 0.574319
[Epoch 83; Iter    16/   32] train: loss: 0.3537444
[Epoch 83] ogbg-molsider: 0.579326 val loss: 0.546155
[Epoch 83] ogbg-molsider: 0.548603 test loss: 0.578274
[Epoch 84; Iter    14/   32] train: loss: 0.3673186
[Epoch 84] ogbg-molsider: 0.561052 val loss: 0.589033
[Epoch 37; Iter    18/   27] train: loss: 0.4804673
[Epoch 37] ogbg-molsider: 0.558779 val loss: 0.523994
[Epoch 37] ogbg-molsider: 0.563258 test loss: 0.514555
[Epoch 38; Iter    21/   27] train: loss: 0.5615222
[Epoch 38] ogbg-molsider: 0.563250 val loss: 0.519998
[Epoch 38] ogbg-molsider: 0.562231 test loss: 0.499815
[Epoch 39; Iter    24/   27] train: loss: 0.4935671
[Epoch 39] ogbg-molsider: 0.561622 val loss: 0.520069
[Epoch 39] ogbg-molsider: 0.562527 test loss: 0.497910
[Epoch 40; Iter    27/   27] train: loss: 0.4957331
[Epoch 40] ogbg-molsider: 0.572172 val loss: 0.515738
[Epoch 40] ogbg-molsider: 0.578898 test loss: 0.501798
[Epoch 41] ogbg-molsider: 0.555399 val loss: 0.516880
[Epoch 41] ogbg-molsider: 0.567756 test loss: 0.503635
[Epoch 42; Iter     3/   27] train: loss: 0.4893126
[Epoch 42] ogbg-molsider: 0.562634 val loss: 0.517061
[Epoch 42] ogbg-molsider: 0.573056 test loss: 0.504983
[Epoch 43; Iter     6/   27] train: loss: 0.5083504
[Epoch 43] ogbg-molsider: 0.571773 val loss: 0.514725
[Epoch 43] ogbg-molsider: 0.562654 test loss: 0.499810
[Epoch 44; Iter     9/   27] train: loss: 0.4188095
[Epoch 44] ogbg-molsider: 0.572173 val loss: 0.515558
[Epoch 44] ogbg-molsider: 0.571077 test loss: 0.499375
[Epoch 45; Iter    12/   27] train: loss: 0.5270134
[Epoch 45] ogbg-molsider: 0.553952 val loss: 0.533703
[Epoch 45] ogbg-molsider: 0.570046 test loss: 0.517688
[Epoch 46; Iter    15/   27] train: loss: 0.4460092
[Epoch 46] ogbg-molsider: 0.559839 val loss: 0.521688
[Epoch 46] ogbg-molsider: 0.564812 test loss: 0.498375
[Epoch 47; Iter    18/   27] train: loss: 0.4967039
[Epoch 47] ogbg-molsider: 0.560948 val loss: 0.520050
[Epoch 47] ogbg-molsider: 0.572365 test loss: 0.514901
[Epoch 48; Iter    21/   27] train: loss: 0.4455454
[Epoch 48] ogbg-molsider: 0.567208 val loss: 0.522926
[Epoch 48] ogbg-molsider: 0.564517 test loss: 0.513761
[Epoch 49; Iter    24/   27] train: loss: 0.4947485
[Epoch 49] ogbg-molsider: 0.559831 val loss: 0.531076
[Epoch 49] ogbg-molsider: 0.568427 test loss: 0.512628
[Epoch 50; Iter    27/   27] train: loss: 0.5259412
[Epoch 50] ogbg-molsider: 0.572417 val loss: 0.523472
[Epoch 50] ogbg-molsider: 0.577211 test loss: 0.508655
[Epoch 51] ogbg-molsider: 0.555351 val loss: 0.553680
[Epoch 51] ogbg-molsider: 0.574071 test loss: 0.568508
[Epoch 52; Iter     3/   27] train: loss: 0.4856688
[Epoch 52] ogbg-molsider: 0.557742 val loss: 0.543465
[Epoch 52] ogbg-molsider: 0.578074 test loss: 0.553903
[Epoch 53; Iter     6/   27] train: loss: 0.4077043
[Epoch 53] ogbg-molsider: 0.583672 val loss: 0.550460
[Epoch 53] ogbg-molsider: 0.593728 test loss: 0.520995
[Epoch 54; Iter     9/   27] train: loss: 0.4679920
[Epoch 54] ogbg-molsider: 0.583722 val loss: 0.549769
[Epoch 54] ogbg-molsider: 0.584084 test loss: 0.629797
[Epoch 55; Iter    12/   27] train: loss: 0.5115523
[Epoch 55] ogbg-molsider: 0.537863 val loss: 0.685901
[Epoch 55] ogbg-molsider: 0.541865 test loss: 0.722359
[Epoch 56; Iter    15/   27] train: loss: 0.5204514
[Epoch 56] ogbg-molsider: 0.574165 val loss: 0.531979
[Epoch 56] ogbg-molsider: 0.571568 test loss: 0.544788
[Epoch 57; Iter    18/   27] train: loss: 0.5010407
[Epoch 57] ogbg-molsider: 0.552129 val loss: 0.532931
[Epoch 57] ogbg-molsider: 0.585726 test loss: 0.530677
[Epoch 58; Iter    21/   27] train: loss: 0.4628770
[Epoch 58] ogbg-molsider: 0.585867 val loss: 0.515206
[Epoch 58] ogbg-molsider: 0.589495 test loss: 0.502575
[Epoch 59; Iter    24/   27] train: loss: 0.4844755
[Epoch 59] ogbg-molsider: 0.588059 val loss: 0.514105
[Epoch 59] ogbg-molsider: 0.604482 test loss: 0.504293
[Epoch 60; Iter    27/   27] train: loss: 0.5199307
[Epoch 60] ogbg-molsider: 0.580015 val loss: 0.531872
[Epoch 60] ogbg-molsider: 0.588238 test loss: 0.520125
[Epoch 61] ogbg-molsider: 0.572719 val loss: 0.560313
[Epoch 61] ogbg-molsider: 0.584531 test loss: 0.524115
[Epoch 62; Iter     3/   27] train: loss: 0.5225759
[Epoch 62] ogbg-molsider: 0.577678 val loss: 0.529783
[Epoch 62] ogbg-molsider: 0.552060 test loss: 0.563136
[Epoch 63; Iter     6/   27] train: loss: 0.4166756
[Epoch 63] ogbg-molsider: 0.540509 val loss: 0.571772
[Epoch 63] ogbg-molsider: 0.537507 test loss: 0.554661
[Epoch 64; Iter     9/   27] train: loss: 0.4645577
[Epoch 64] ogbg-molsider: 0.570164 val loss: 0.558977
[Epoch 64] ogbg-molsider: 0.606132 test loss: 0.552124
[Epoch 65; Iter    12/   27] train: loss: 0.4293686
[Epoch 65] ogbg-molsider: 0.555976 val loss: 0.548890
[Epoch 65] ogbg-molsider: 0.597091 test loss: 0.518566
[Epoch 66; Iter    15/   27] train: loss: 0.4890899
[Epoch 66] ogbg-molsider: 0.524082 val loss: 0.587988
[Epoch 66] ogbg-molsider: 0.570414 test loss: 0.541075
[Epoch 67; Iter    18/   27] train: loss: 0.4119598
[Epoch 67] ogbg-molsider: 0.569538 val loss: 0.539014
[Epoch 67] ogbg-molsider: 0.591212 test loss: 0.514901
[Epoch 68; Iter    21/   27] train: loss: 0.4379789
[Epoch 68] ogbg-molsider: 0.561793 val loss: 0.553112
[Epoch 68] ogbg-molsider: 0.578540 test loss: 0.548466
[Epoch 69; Iter    24/   27] train: loss: 0.4080775
[Epoch 69] ogbg-molsider: 0.570492 val loss: 0.551582
[Epoch 69] ogbg-molsider: 0.549743 test loss: 0.557909
[Epoch 70; Iter    27/   27] train: loss: 0.3846448
[Epoch 70] ogbg-molsider: 0.575499 val loss: 0.540516
[Epoch 70] ogbg-molsider: 0.588151 test loss: 0.520318
[Epoch 71] ogbg-molsider: 0.582080 val loss: 0.587064
[Epoch 71] ogbg-molsider: 0.572902 test loss: 0.572511
[Epoch 72; Iter     3/   27] train: loss: 0.3771443
[Epoch 72] ogbg-molsider: 0.579174 val loss: 0.548775
[Epoch 72] ogbg-molsider: 0.591735 test loss: 0.551169
[Epoch 73; Iter     6/   27] train: loss: 0.3782512
[Epoch 73] ogbg-molsider: 0.570639 val loss: 0.555146
[Epoch 73] ogbg-molsider: 0.574171 test loss: 0.532244
[Epoch 74; Iter     9/   27] train: loss: 0.3726148
[Epoch 74] ogbg-molsider: 0.550447 val loss: 0.628980
[Epoch 74] ogbg-molsider: 0.586723 test loss: 0.740077
[Epoch 75; Iter    12/   27] train: loss: 0.4019746
[Epoch 75] ogbg-molsider: 0.559106 val loss: 0.598082
[Epoch 75] ogbg-molsider: 0.567619 test loss: 0.570295
[Epoch 76; Iter    15/   27] train: loss: 0.3812342
[Epoch 76] ogbg-molsider: 0.590548 val loss: 0.565313
[Epoch 76] ogbg-molsider: 0.576945 test loss: 0.570483
[Epoch 77; Iter    18/   27] train: loss: 0.3672109
[Epoch 77] ogbg-molsider: 0.564267 val loss: 0.602980
[Epoch 77] ogbg-molsider: 0.569509 test loss: 0.608525
[Epoch 78; Iter    21/   27] train: loss: 0.4039032
[Epoch 78] ogbg-molsider: 0.563699 val loss: 0.591918
[Epoch 78] ogbg-molsider: 0.570658 test loss: 0.564430
[Epoch 79; Iter    24/   27] train: loss: 0.4344270
[Epoch 79] ogbg-molsider: 0.559665 val loss: 0.664665
[Epoch 79] ogbg-molsider: 0.557515 test loss: 0.684491
[Epoch 80; Iter    27/   27] train: loss: 0.4937054
[Epoch 80] ogbg-molsider: 0.566003 val loss: 0.585982
[Epoch 80] ogbg-molsider: 0.584048 test loss: 0.589613
[Epoch 81] ogbg-molsider: 0.556227 val loss: 0.582594
[Epoch 81] ogbg-molsider: 0.569124 test loss: 0.560816
[Epoch 82; Iter     3/   27] train: loss: 0.3645087
[Epoch 82] ogbg-molsider: 0.554414 val loss: 0.575288
[Epoch 82] ogbg-molsider: 0.553302 test loss: 0.560161
[Epoch 83; Iter     6/   27] train: loss: 0.3660143
[Epoch 83] ogbg-molsider: 0.582485 val loss: 0.559066
[Epoch 83] ogbg-molsider: 0.566325 test loss: 0.547808
[Epoch 84; Iter     9/   27] train: loss: 0.3724313
[Epoch 84] ogbg-molsider: 0.583654 val loss: 0.560220
[Epoch 84] ogbg-molsider: 0.580450 test loss: 0.553603
[Epoch 85; Iter    12/   27] train: loss: 0.3577317
[Epoch 85] ogbg-molsider: 0.565998 val loss: 0.587981
[Epoch 85] ogbg-molsider: 0.577122 test loss: 0.578728
[Epoch 86; Iter    15/   27] train: loss: 0.4045958
[Epoch 86] ogbg-molsider: 0.597011 val loss: 0.569777
[Epoch 86] ogbg-molsider: 0.606229 test loss: 0.557886
[Epoch 87; Iter    18/   27] train: loss: 0.4160116
[Epoch 87] ogbg-molsider: 0.571003 val loss: 0.585116
[Epoch 87] ogbg-molsider: 0.583626 test loss: 0.569927
[Epoch 88; Iter    21/   27] train: loss: 0.3942521
[Epoch 88] ogbg-molsider: 0.544240 val loss: 0.628087
[Epoch 88] ogbg-molsider: 0.578186 test loss: 0.603842
[Epoch 89; Iter    24/   27] train: loss: 0.3119558
[Epoch 37; Iter    18/   27] train: loss: 0.4582362
[Epoch 37] ogbg-molsider: 0.561067 val loss: 0.517811
[Epoch 37] ogbg-molsider: 0.563996 test loss: 0.505626
[Epoch 38; Iter    21/   27] train: loss: 0.4930322
[Epoch 38] ogbg-molsider: 0.554822 val loss: 0.519659
[Epoch 38] ogbg-molsider: 0.566454 test loss: 0.502056
[Epoch 39; Iter    24/   27] train: loss: 0.5281731
[Epoch 39] ogbg-molsider: 0.555178 val loss: 0.517455
[Epoch 39] ogbg-molsider: 0.554072 test loss: 0.500960
[Epoch 40; Iter    27/   27] train: loss: 0.4783572
[Epoch 40] ogbg-molsider: 0.540307 val loss: 0.537606
[Epoch 40] ogbg-molsider: 0.566407 test loss: 0.526884
[Epoch 41] ogbg-molsider: 0.557985 val loss: 0.518215
[Epoch 41] ogbg-molsider: 0.563797 test loss: 0.502463
[Epoch 42; Iter     3/   27] train: loss: 0.4742801
[Epoch 42] ogbg-molsider: 0.559871 val loss: 0.518677
[Epoch 42] ogbg-molsider: 0.571920 test loss: 0.500572
[Epoch 43; Iter     6/   27] train: loss: 0.5140257
[Epoch 43] ogbg-molsider: 0.569387 val loss: 0.522329
[Epoch 43] ogbg-molsider: 0.580661 test loss: 0.497292
[Epoch 44; Iter     9/   27] train: loss: 0.5034551
[Epoch 44] ogbg-molsider: 0.556890 val loss: 0.527715
[Epoch 44] ogbg-molsider: 0.563399 test loss: 0.528109
[Epoch 45; Iter    12/   27] train: loss: 0.4788547
[Epoch 45] ogbg-molsider: 0.567875 val loss: 0.526432
[Epoch 45] ogbg-molsider: 0.576846 test loss: 0.507937
[Epoch 46; Iter    15/   27] train: loss: 0.4583736
[Epoch 46] ogbg-molsider: 0.560515 val loss: 0.526221
[Epoch 46] ogbg-molsider: 0.573127 test loss: 0.510051
[Epoch 47; Iter    18/   27] train: loss: 0.4658388
[Epoch 47] ogbg-molsider: 0.563795 val loss: 0.531716
[Epoch 47] ogbg-molsider: 0.579224 test loss: 0.514410
[Epoch 48; Iter    21/   27] train: loss: 0.4783087
[Epoch 48] ogbg-molsider: 0.563573 val loss: 0.520110
[Epoch 48] ogbg-molsider: 0.577512 test loss: 0.497286
[Epoch 49; Iter    24/   27] train: loss: 0.4891767
[Epoch 49] ogbg-molsider: 0.567256 val loss: 0.529214
[Epoch 49] ogbg-molsider: 0.577962 test loss: 0.513654
[Epoch 50; Iter    27/   27] train: loss: 0.5074242
[Epoch 50] ogbg-molsider: 0.581135 val loss: 0.523151
[Epoch 50] ogbg-molsider: 0.571896 test loss: 0.506820
[Epoch 51] ogbg-molsider: 0.580394 val loss: 0.519933
[Epoch 51] ogbg-molsider: 0.581719 test loss: 0.505738
[Epoch 52; Iter     3/   27] train: loss: 0.4347157
[Epoch 52] ogbg-molsider: 0.570166 val loss: 0.533235
[Epoch 52] ogbg-molsider: 0.577559 test loss: 0.532283
[Epoch 53; Iter     6/   27] train: loss: 0.4969383
[Epoch 53] ogbg-molsider: 0.565374 val loss: 0.533930
[Epoch 53] ogbg-molsider: 0.593155 test loss: 0.500406
[Epoch 54; Iter     9/   27] train: loss: 0.5459073
[Epoch 54] ogbg-molsider: 0.577150 val loss: 0.543712
[Epoch 54] ogbg-molsider: 0.571284 test loss: 0.542731
[Epoch 55; Iter    12/   27] train: loss: 0.4692406
[Epoch 55] ogbg-molsider: 0.564325 val loss: 0.543573
[Epoch 55] ogbg-molsider: 0.580087 test loss: 0.546103
[Epoch 56; Iter    15/   27] train: loss: 0.4228042
[Epoch 56] ogbg-molsider: 0.555228 val loss: 0.562800
[Epoch 56] ogbg-molsider: 0.558901 test loss: 0.655332
[Epoch 57; Iter    18/   27] train: loss: 0.4250500
[Epoch 57] ogbg-molsider: 0.557016 val loss: 0.537233
[Epoch 57] ogbg-molsider: 0.566514 test loss: 0.535157
[Epoch 58; Iter    21/   27] train: loss: 0.4867615
[Epoch 58] ogbg-molsider: 0.563898 val loss: 0.570885
[Epoch 58] ogbg-molsider: 0.587844 test loss: 0.554892
[Epoch 59; Iter    24/   27] train: loss: 0.5113645
[Epoch 59] ogbg-molsider: 0.564918 val loss: 0.591659
[Epoch 59] ogbg-molsider: 0.566944 test loss: 0.549866
[Epoch 60; Iter    27/   27] train: loss: 0.4484418
[Epoch 60] ogbg-molsider: 0.584440 val loss: 0.556344
[Epoch 60] ogbg-molsider: 0.591282 test loss: 0.515288
[Epoch 61] ogbg-molsider: 0.590608 val loss: 0.537727
[Epoch 61] ogbg-molsider: 0.593485 test loss: 0.544807
[Epoch 62; Iter     3/   27] train: loss: 0.4430887
[Epoch 62] ogbg-molsider: 0.584381 val loss: 0.620883
[Epoch 62] ogbg-molsider: 0.598413 test loss: 0.592463
[Epoch 63; Iter     6/   27] train: loss: 0.4357302
[Epoch 63] ogbg-molsider: 0.574558 val loss: 0.551824
[Epoch 63] ogbg-molsider: 0.594539 test loss: 0.561288
[Epoch 64; Iter     9/   27] train: loss: 0.4090688
[Epoch 64] ogbg-molsider: 0.555745 val loss: 0.576544
[Epoch 64] ogbg-molsider: 0.588737 test loss: 0.560775
[Epoch 65; Iter    12/   27] train: loss: 0.4474677
[Epoch 65] ogbg-molsider: 0.580142 val loss: 1.311506
[Epoch 65] ogbg-molsider: 0.581035 test loss: 0.671999
[Epoch 66; Iter    15/   27] train: loss: 0.4562085
[Epoch 66] ogbg-molsider: 0.573070 val loss: 0.576143
[Epoch 66] ogbg-molsider: 0.595911 test loss: 0.559621
[Epoch 67; Iter    18/   27] train: loss: 0.4724374
[Epoch 67] ogbg-molsider: 0.565788 val loss: 0.571291
[Epoch 67] ogbg-molsider: 0.593680 test loss: 0.540677
[Epoch 68; Iter    21/   27] train: loss: 0.4297140
[Epoch 68] ogbg-molsider: 0.577234 val loss: 0.541183
[Epoch 68] ogbg-molsider: 0.595169 test loss: 0.528270
[Epoch 69; Iter    24/   27] train: loss: 0.3611822
[Epoch 69] ogbg-molsider: 0.568219 val loss: 0.567999
[Epoch 69] ogbg-molsider: 0.581803 test loss: 0.558093
[Epoch 70; Iter    27/   27] train: loss: 0.4846934
[Epoch 70] ogbg-molsider: 0.540365 val loss: 0.564983
[Epoch 70] ogbg-molsider: 0.568527 test loss: 0.548388
[Epoch 71] ogbg-molsider: 0.579580 val loss: 0.596481
[Epoch 71] ogbg-molsider: 0.559747 test loss: 0.622368
[Epoch 72; Iter     3/   27] train: loss: 0.3473977
[Epoch 72] ogbg-molsider: 0.575271 val loss: 0.578723
[Epoch 72] ogbg-molsider: 0.586640 test loss: 0.541338
[Epoch 73; Iter     6/   27] train: loss: 0.3616550
[Epoch 73] ogbg-molsider: 0.575206 val loss: 0.575497
[Epoch 73] ogbg-molsider: 0.569915 test loss: 0.577383
[Epoch 74; Iter     9/   27] train: loss: 0.4034633
[Epoch 74] ogbg-molsider: 0.576840 val loss: 0.588743
[Epoch 74] ogbg-molsider: 0.557264 test loss: 0.567168
[Epoch 75; Iter    12/   27] train: loss: 0.4019718
[Epoch 75] ogbg-molsider: 0.561666 val loss: 0.581906
[Epoch 75] ogbg-molsider: 0.561579 test loss: 0.558801
[Epoch 76; Iter    15/   27] train: loss: 0.3693778
[Epoch 76] ogbg-molsider: 0.573100 val loss: 0.588113
[Epoch 76] ogbg-molsider: 0.579041 test loss: 0.586010
[Epoch 77; Iter    18/   27] train: loss: 0.3806585
[Epoch 77] ogbg-molsider: 0.555897 val loss: 0.601319
[Epoch 77] ogbg-molsider: 0.560049 test loss: 0.636929
[Epoch 78; Iter    21/   27] train: loss: 0.3918621
[Epoch 78] ogbg-molsider: 0.585490 val loss: 0.586749
[Epoch 78] ogbg-molsider: 0.556889 test loss: 0.588479
[Epoch 79; Iter    24/   27] train: loss: 0.3972024
[Epoch 79] ogbg-molsider: 0.561841 val loss: 0.606196
[Epoch 79] ogbg-molsider: 0.568639 test loss: 0.622680
[Epoch 80; Iter    27/   27] train: loss: 0.3929691
[Epoch 80] ogbg-molsider: 0.584602 val loss: 0.556334
[Epoch 80] ogbg-molsider: 0.551085 test loss: 0.570009
[Epoch 81] ogbg-molsider: 0.566069 val loss: 0.580476
[Epoch 81] ogbg-molsider: 0.550596 test loss: 0.587309
[Epoch 82; Iter     3/   27] train: loss: 0.3378696
[Epoch 82] ogbg-molsider: 0.583036 val loss: 0.580143
[Epoch 82] ogbg-molsider: 0.552704 test loss: 0.640665
[Epoch 83; Iter     6/   27] train: loss: 0.3141580
[Epoch 83] ogbg-molsider: 0.569174 val loss: 0.608125
[Epoch 83] ogbg-molsider: 0.555057 test loss: 0.600584
[Epoch 84; Iter     9/   27] train: loss: 0.3674259
[Epoch 84] ogbg-molsider: 0.557323 val loss: 0.638124
[Epoch 84] ogbg-molsider: 0.564255 test loss: 0.674769
[Epoch 85; Iter    12/   27] train: loss: 0.3440248
[Epoch 85] ogbg-molsider: 0.582899 val loss: 0.612546
[Epoch 85] ogbg-molsider: 0.564216 test loss: 0.642719
[Epoch 86; Iter    15/   27] train: loss: 0.3580284
[Epoch 86] ogbg-molsider: 0.575558 val loss: 0.599786
[Epoch 86] ogbg-molsider: 0.566814 test loss: 0.583424
[Epoch 87; Iter    18/   27] train: loss: 0.3862371
[Epoch 87] ogbg-molsider: 0.575734 val loss: 0.570762
[Epoch 87] ogbg-molsider: 0.552866 test loss: 0.576233
[Epoch 88; Iter    21/   27] train: loss: 0.3529952
[Epoch 88] ogbg-molsider: 0.573575 val loss: 0.584674
[Epoch 88] ogbg-molsider: 0.550643 test loss: 0.603495
[Epoch 89; Iter    24/   27] train: loss: 0.3993866
[Epoch 37; Iter    18/   27] train: loss: 0.4825788
[Epoch 37] ogbg-molsider: 0.562254 val loss: 0.516184
[Epoch 37] ogbg-molsider: 0.567622 test loss: 0.496219
[Epoch 38; Iter    21/   27] train: loss: 0.4902677
[Epoch 38] ogbg-molsider: 0.555451 val loss: 0.517644
[Epoch 38] ogbg-molsider: 0.555199 test loss: 0.501188
[Epoch 39; Iter    24/   27] train: loss: 0.5070294
[Epoch 39] ogbg-molsider: 0.561919 val loss: 0.518969
[Epoch 39] ogbg-molsider: 0.557545 test loss: 0.509791
[Epoch 40; Iter    27/   27] train: loss: 0.5018904
[Epoch 40] ogbg-molsider: 0.568060 val loss: 0.518974
[Epoch 40] ogbg-molsider: 0.580599 test loss: 0.498730
[Epoch 41] ogbg-molsider: 0.551862 val loss: 0.518782
[Epoch 41] ogbg-molsider: 0.567982 test loss: 0.496416
[Epoch 42; Iter     3/   27] train: loss: 0.4606159
[Epoch 42] ogbg-molsider: 0.564170 val loss: 0.517861
[Epoch 42] ogbg-molsider: 0.565657 test loss: 0.503728
[Epoch 43; Iter     6/   27] train: loss: 0.4908797
[Epoch 43] ogbg-molsider: 0.558295 val loss: 0.524206
[Epoch 43] ogbg-molsider: 0.558201 test loss: 0.518000
[Epoch 44; Iter     9/   27] train: loss: 0.4782013
[Epoch 44] ogbg-molsider: 0.558193 val loss: 0.528067
[Epoch 44] ogbg-molsider: 0.579202 test loss: 0.505907
[Epoch 45; Iter    12/   27] train: loss: 0.4814869
[Epoch 45] ogbg-molsider: 0.575890 val loss: 0.515057
[Epoch 45] ogbg-molsider: 0.570823 test loss: 0.513608
[Epoch 46; Iter    15/   27] train: loss: 0.5411834
[Epoch 46] ogbg-molsider: 0.579376 val loss: 0.518972
[Epoch 46] ogbg-molsider: 0.572849 test loss: 0.499424
[Epoch 47; Iter    18/   27] train: loss: 0.5250700
[Epoch 47] ogbg-molsider: 0.568050 val loss: 0.519316
[Epoch 47] ogbg-molsider: 0.574203 test loss: 0.496966
[Epoch 48; Iter    21/   27] train: loss: 0.4040467
[Epoch 48] ogbg-molsider: 0.581564 val loss: 0.516715
[Epoch 48] ogbg-molsider: 0.571514 test loss: 0.506155
[Epoch 49; Iter    24/   27] train: loss: 0.4419369
[Epoch 49] ogbg-molsider: 0.571434 val loss: 0.517971
[Epoch 49] ogbg-molsider: 0.568589 test loss: 0.501188
[Epoch 50; Iter    27/   27] train: loss: 0.4557759
[Epoch 50] ogbg-molsider: 0.559944 val loss: 0.524461
[Epoch 50] ogbg-molsider: 0.574590 test loss: 0.509034
[Epoch 51] ogbg-molsider: 0.567831 val loss: 0.523587
[Epoch 51] ogbg-molsider: 0.583592 test loss: 0.500728
[Epoch 52; Iter     3/   27] train: loss: 0.4961374
[Epoch 52] ogbg-molsider: 0.569644 val loss: 0.528623
[Epoch 52] ogbg-molsider: 0.569123 test loss: 0.524065
[Epoch 53; Iter     6/   27] train: loss: 0.4996757
[Epoch 53] ogbg-molsider: 0.538351 val loss: 0.544315
[Epoch 53] ogbg-molsider: 0.563140 test loss: 0.528499
[Epoch 54; Iter     9/   27] train: loss: 0.4435869
[Epoch 54] ogbg-molsider: 0.587630 val loss: 0.587387
[Epoch 54] ogbg-molsider: 0.585520 test loss: 0.583599
[Epoch 55; Iter    12/   27] train: loss: 0.4382350
[Epoch 55] ogbg-molsider: 0.575168 val loss: 0.527539
[Epoch 55] ogbg-molsider: 0.580478 test loss: 0.522780
[Epoch 56; Iter    15/   27] train: loss: 0.4262707
[Epoch 56] ogbg-molsider: 0.577408 val loss: 0.543610
[Epoch 56] ogbg-molsider: 0.590709 test loss: 0.542836
[Epoch 57; Iter    18/   27] train: loss: 0.4437981
[Epoch 57] ogbg-molsider: 0.566436 val loss: 0.539050
[Epoch 57] ogbg-molsider: 0.573144 test loss: 0.541629
[Epoch 58; Iter    21/   27] train: loss: 0.4595371
[Epoch 58] ogbg-molsider: 0.537812 val loss: 1.856140
[Epoch 58] ogbg-molsider: 0.547058 test loss: 2.475931
[Epoch 59; Iter    24/   27] train: loss: 0.4810902
[Epoch 59] ogbg-molsider: 0.592044 val loss: 0.535136
[Epoch 59] ogbg-molsider: 0.583452 test loss: 0.548695
[Epoch 60; Iter    27/   27] train: loss: 0.4864819
[Epoch 60] ogbg-molsider: 0.585506 val loss: 0.519661
[Epoch 60] ogbg-molsider: 0.613500 test loss: 0.502471
[Epoch 61] ogbg-molsider: 0.587248 val loss: 0.544467
[Epoch 61] ogbg-molsider: 0.567323 test loss: 0.562313
[Epoch 62; Iter     3/   27] train: loss: 0.5136168
[Epoch 62] ogbg-molsider: 0.592802 val loss: 0.506788
[Epoch 62] ogbg-molsider: 0.592859 test loss: 0.493261
[Epoch 63; Iter     6/   27] train: loss: 0.4397127
[Epoch 63] ogbg-molsider: 0.573592 val loss: 0.576127
[Epoch 63] ogbg-molsider: 0.595375 test loss: 0.567540
[Epoch 64; Iter     9/   27] train: loss: 0.4414226
[Epoch 64] ogbg-molsider: 0.533242 val loss: 0.589807
[Epoch 64] ogbg-molsider: 0.538752 test loss: 0.596607
[Epoch 65; Iter    12/   27] train: loss: 0.5099938
[Epoch 65] ogbg-molsider: 0.582606 val loss: 0.544806
[Epoch 65] ogbg-molsider: 0.583341 test loss: 0.521699
[Epoch 66; Iter    15/   27] train: loss: 0.5373579
[Epoch 66] ogbg-molsider: 0.587536 val loss: 0.569635
[Epoch 66] ogbg-molsider: 0.570041 test loss: 1.021641
[Epoch 67; Iter    18/   27] train: loss: 0.4713319
[Epoch 67] ogbg-molsider: 0.563137 val loss: 0.527224
[Epoch 67] ogbg-molsider: 0.588162 test loss: 0.501997
[Epoch 68; Iter    21/   27] train: loss: 0.5052906
[Epoch 68] ogbg-molsider: 0.578262 val loss: 0.544896
[Epoch 68] ogbg-molsider: 0.582293 test loss: 0.526557
[Epoch 69; Iter    24/   27] train: loss: 0.4093275
[Epoch 69] ogbg-molsider: 0.559684 val loss: 0.552196
[Epoch 69] ogbg-molsider: 0.550357 test loss: 0.536361
[Epoch 70; Iter    27/   27] train: loss: 0.4425447
[Epoch 70] ogbg-molsider: 0.577865 val loss: 0.543603
[Epoch 70] ogbg-molsider: 0.583286 test loss: 0.518203
[Epoch 71] ogbg-molsider: 0.581655 val loss: 0.539482
[Epoch 71] ogbg-molsider: 0.602991 test loss: 0.505882
[Epoch 72; Iter     3/   27] train: loss: 0.3981618
[Epoch 72] ogbg-molsider: 0.571621 val loss: 0.553407
[Epoch 72] ogbg-molsider: 0.578719 test loss: 0.553993
[Epoch 73; Iter     6/   27] train: loss: 0.4115559
[Epoch 73] ogbg-molsider: 0.562824 val loss: 0.573110
[Epoch 73] ogbg-molsider: 0.564519 test loss: 0.541247
[Epoch 74; Iter     9/   27] train: loss: 0.4141673
[Epoch 74] ogbg-molsider: 0.564016 val loss: 0.596774
[Epoch 74] ogbg-molsider: 0.589786 test loss: 0.574604
[Epoch 75; Iter    12/   27] train: loss: 0.4451415
[Epoch 75] ogbg-molsider: 0.566149 val loss: 0.574591
[Epoch 75] ogbg-molsider: 0.551547 test loss: 0.595300
[Epoch 76; Iter    15/   27] train: loss: 0.4617229
[Epoch 76] ogbg-molsider: 0.572316 val loss: 0.553848
[Epoch 76] ogbg-molsider: 0.606328 test loss: 0.516289
[Epoch 77; Iter    18/   27] train: loss: 0.3925208
[Epoch 77] ogbg-molsider: 0.573501 val loss: 0.556876
[Epoch 77] ogbg-molsider: 0.578584 test loss: 0.533459
[Epoch 78; Iter    21/   27] train: loss: 0.4071869
[Epoch 78] ogbg-molsider: 0.562907 val loss: 0.574405
[Epoch 78] ogbg-molsider: 0.588211 test loss: 0.548192
[Epoch 79; Iter    24/   27] train: loss: 0.4006175
[Epoch 79] ogbg-molsider: 0.582911 val loss: 0.564222
[Epoch 79] ogbg-molsider: 0.568827 test loss: 0.556010
[Epoch 80; Iter    27/   27] train: loss: 0.3784921
[Epoch 80] ogbg-molsider: 0.553096 val loss: 0.585847
[Epoch 80] ogbg-molsider: 0.562542 test loss: 0.551415
[Epoch 81] ogbg-molsider: 0.566509 val loss: 0.551312
[Epoch 81] ogbg-molsider: 0.571042 test loss: 0.527864
[Epoch 82; Iter     3/   27] train: loss: 0.3740551
[Epoch 82] ogbg-molsider: 0.558850 val loss: 0.603341
[Epoch 82] ogbg-molsider: 0.571672 test loss: 0.574225
[Epoch 83; Iter     6/   27] train: loss: 0.3939838
[Epoch 83] ogbg-molsider: 0.594450 val loss: 0.590919
[Epoch 83] ogbg-molsider: 0.594619 test loss: 0.566427
[Epoch 84; Iter     9/   27] train: loss: 0.3926413
[Epoch 84] ogbg-molsider: 0.556076 val loss: 0.599500
[Epoch 84] ogbg-molsider: 0.553098 test loss: 0.603643
[Epoch 85; Iter    12/   27] train: loss: 0.4136617
[Epoch 85] ogbg-molsider: 0.588096 val loss: 0.562593
[Epoch 85] ogbg-molsider: 0.588406 test loss: 0.545593
[Epoch 86; Iter    15/   27] train: loss: 0.3966233
[Epoch 86] ogbg-molsider: 0.575929 val loss: 0.617590
[Epoch 86] ogbg-molsider: 0.576945 test loss: 0.615260
[Epoch 87; Iter    18/   27] train: loss: 0.3466726
[Epoch 87] ogbg-molsider: 0.563838 val loss: 0.609070
[Epoch 87] ogbg-molsider: 0.567774 test loss: 0.587575
[Epoch 88; Iter    21/   27] train: loss: 0.4014587
[Epoch 88] ogbg-molsider: 0.552021 val loss: 0.642368
[Epoch 88] ogbg-molsider: 0.573939 test loss: 0.665776
[Epoch 89; Iter    24/   27] train: loss: 0.4296446
[Epoch 35; Iter    22/   32] train: loss: 0.4855376
[Epoch 35] ogbg-molsider: 0.560312 val loss: 0.490470
[Epoch 35] ogbg-molsider: 0.568423 test loss: 0.506971
[Epoch 36; Iter    20/   32] train: loss: 0.4617019
[Epoch 36] ogbg-molsider: 0.560442 val loss: 0.485058
[Epoch 36] ogbg-molsider: 0.574874 test loss: 0.500699
[Epoch 37; Iter    18/   32] train: loss: 0.5241302
[Epoch 37] ogbg-molsider: 0.561945 val loss: 0.489114
[Epoch 37] ogbg-molsider: 0.572457 test loss: 0.505739
[Epoch 38; Iter    16/   32] train: loss: 0.4838997
[Epoch 38] ogbg-molsider: 0.550615 val loss: 0.495514
[Epoch 38] ogbg-molsider: 0.565340 test loss: 0.511148
[Epoch 39; Iter    14/   32] train: loss: 0.5125105
[Epoch 39] ogbg-molsider: 0.572857 val loss: 0.486245
[Epoch 39] ogbg-molsider: 0.568596 test loss: 0.506523
[Epoch 40; Iter    12/   32] train: loss: 0.4950613
[Epoch 40] ogbg-molsider: 0.565614 val loss: 0.487626
[Epoch 40] ogbg-molsider: 0.584097 test loss: 0.506835
[Epoch 41; Iter    10/   32] train: loss: 0.5264319
[Epoch 41] ogbg-molsider: 0.555026 val loss: 0.492759
[Epoch 41] ogbg-molsider: 0.590902 test loss: 0.500795
[Epoch 42; Iter     8/   32] train: loss: 0.4734865
[Epoch 42] ogbg-molsider: 0.566722 val loss: 0.489067
[Epoch 42] ogbg-molsider: 0.582522 test loss: 0.504114
[Epoch 43; Iter     6/   32] train: loss: 0.4499387
[Epoch 43] ogbg-molsider: 0.566122 val loss: 0.493606
[Epoch 43] ogbg-molsider: 0.580437 test loss: 0.531704
[Epoch 44; Iter     4/   32] train: loss: 0.4418637
[Epoch 44] ogbg-molsider: 0.571754 val loss: 0.501717
[Epoch 44] ogbg-molsider: 0.588339 test loss: 0.542189
[Epoch 45; Iter     2/   32] train: loss: 0.4661886
[Epoch 45; Iter    32/   32] train: loss: 0.4354977
[Epoch 45] ogbg-molsider: 0.563011 val loss: 0.486124
[Epoch 45] ogbg-molsider: 0.613684 test loss: 0.557368
[Epoch 46; Iter    30/   32] train: loss: 0.4713691
[Epoch 46] ogbg-molsider: 0.574544 val loss: 0.493246
[Epoch 46] ogbg-molsider: 0.565398 test loss: 0.562810
[Epoch 47; Iter    28/   32] train: loss: 0.4940087
[Epoch 47] ogbg-molsider: 0.524506 val loss: 1.082408
[Epoch 47] ogbg-molsider: 0.574202 test loss: 0.765372
[Epoch 48; Iter    26/   32] train: loss: 0.4652472
[Epoch 48] ogbg-molsider: 0.562653 val loss: 0.500862
[Epoch 48] ogbg-molsider: 0.614315 test loss: 0.512215
[Epoch 49; Iter    24/   32] train: loss: 0.5341924
[Epoch 49] ogbg-molsider: 0.566598 val loss: 0.500092
[Epoch 49] ogbg-molsider: 0.600541 test loss: 0.527418
[Epoch 50; Iter    22/   32] train: loss: 0.4583526
[Epoch 50] ogbg-molsider: 0.564348 val loss: 0.492811
[Epoch 50] ogbg-molsider: 0.601944 test loss: 0.506158
[Epoch 51; Iter    20/   32] train: loss: 0.4826555
[Epoch 51] ogbg-molsider: 0.572672 val loss: 0.484491
[Epoch 51] ogbg-molsider: 0.607534 test loss: 0.511540
[Epoch 52; Iter    18/   32] train: loss: 0.4872869
[Epoch 52] ogbg-molsider: 0.565213 val loss: 0.497191
[Epoch 52] ogbg-molsider: 0.576608 test loss: 0.515705
[Epoch 53; Iter    16/   32] train: loss: 0.4623134
[Epoch 53] ogbg-molsider: 0.571418 val loss: 0.485935
[Epoch 53] ogbg-molsider: 0.606409 test loss: 0.509450
[Epoch 54; Iter    14/   32] train: loss: 0.5658303
[Epoch 54] ogbg-molsider: 0.553503 val loss: 0.504207
[Epoch 54] ogbg-molsider: 0.595683 test loss: 0.509539
[Epoch 55; Iter    12/   32] train: loss: 0.4314882
[Epoch 55] ogbg-molsider: 0.561151 val loss: 0.498405
[Epoch 55] ogbg-molsider: 0.601107 test loss: 0.509187
[Epoch 56; Iter    10/   32] train: loss: 0.4520418
[Epoch 56] ogbg-molsider: 0.581751 val loss: 0.592916
[Epoch 56] ogbg-molsider: 0.545748 test loss: 0.697813
[Epoch 57; Iter     8/   32] train: loss: 0.4668278
[Epoch 57] ogbg-molsider: 0.581510 val loss: 0.483804
[Epoch 57] ogbg-molsider: 0.596211 test loss: 0.506699
[Epoch 58; Iter     6/   32] train: loss: 0.4554808
[Epoch 58] ogbg-molsider: 0.588108 val loss: 0.485534
[Epoch 58] ogbg-molsider: 0.578984 test loss: 0.517410
[Epoch 59; Iter     4/   32] train: loss: 0.4640211
[Epoch 59] ogbg-molsider: 0.567445 val loss: 0.533426
[Epoch 59] ogbg-molsider: 0.572273 test loss: 0.538751
[Epoch 60; Iter     2/   32] train: loss: 0.4572003
[Epoch 60; Iter    32/   32] train: loss: 0.6081394
[Epoch 60] ogbg-molsider: 0.573150 val loss: 0.505241
[Epoch 60] ogbg-molsider: 0.568060 test loss: 0.550236
[Epoch 61; Iter    30/   32] train: loss: 0.4923542
[Epoch 61] ogbg-molsider: 0.596779 val loss: 0.546344
[Epoch 61] ogbg-molsider: 0.600303 test loss: 0.556543
[Epoch 62; Iter    28/   32] train: loss: 0.4317041
[Epoch 62] ogbg-molsider: 0.609005 val loss: 0.489980
[Epoch 62] ogbg-molsider: 0.584761 test loss: 0.546185
[Epoch 63; Iter    26/   32] train: loss: 0.4772316
[Epoch 63] ogbg-molsider: 0.578731 val loss: 0.521592
[Epoch 63] ogbg-molsider: 0.571590 test loss: 0.574709
[Epoch 64; Iter    24/   32] train: loss: 0.3989593
[Epoch 64] ogbg-molsider: 0.608344 val loss: 0.492913
[Epoch 64] ogbg-molsider: 0.587342 test loss: 0.528251
[Epoch 65; Iter    22/   32] train: loss: 0.3931351
[Epoch 65] ogbg-molsider: 0.581984 val loss: 0.509958
[Epoch 65] ogbg-molsider: 0.588267 test loss: 0.542112
[Epoch 66; Iter    20/   32] train: loss: 0.4131598
[Epoch 66] ogbg-molsider: 0.590935 val loss: 0.516237
[Epoch 66] ogbg-molsider: 0.607235 test loss: 0.530798
[Epoch 67; Iter    18/   32] train: loss: 0.4447428
[Epoch 67] ogbg-molsider: 0.597264 val loss: 0.498277
[Epoch 67] ogbg-molsider: 0.601459 test loss: 0.532611
[Epoch 68; Iter    16/   32] train: loss: 0.4418373
[Epoch 68] ogbg-molsider: 0.606524 val loss: 0.563709
[Epoch 68] ogbg-molsider: 0.593561 test loss: 0.606632
[Epoch 69; Iter    14/   32] train: loss: 0.4478784
[Epoch 69] ogbg-molsider: 0.577664 val loss: 0.514303
[Epoch 69] ogbg-molsider: 0.580408 test loss: 0.625943
[Epoch 70; Iter    12/   32] train: loss: 0.4307205
[Epoch 70] ogbg-molsider: 0.590453 val loss: 0.527977
[Epoch 70] ogbg-molsider: 0.607294 test loss: 0.540899
[Epoch 71; Iter    10/   32] train: loss: 0.3567925
[Epoch 71] ogbg-molsider: 0.563558 val loss: 0.544898
[Epoch 71] ogbg-molsider: 0.609680 test loss: 0.559200
[Epoch 72; Iter     8/   32] train: loss: 0.4150312
[Epoch 72] ogbg-molsider: 0.606079 val loss: 0.506311
[Epoch 72] ogbg-molsider: 0.582224 test loss: 0.530716
[Epoch 73; Iter     6/   32] train: loss: 0.4164814
[Epoch 73] ogbg-molsider: 0.588221 val loss: 0.516879
[Epoch 73] ogbg-molsider: 0.603569 test loss: 0.526765
[Epoch 74; Iter     4/   32] train: loss: 0.4408870
[Epoch 74] ogbg-molsider: 0.599338 val loss: 0.524090
[Epoch 74] ogbg-molsider: 0.600306 test loss: 0.536685
[Epoch 75; Iter     2/   32] train: loss: 0.3820687
[Epoch 75; Iter    32/   32] train: loss: 0.4230424
[Epoch 75] ogbg-molsider: 0.614779 val loss: 0.519455
[Epoch 75] ogbg-molsider: 0.602762 test loss: 0.540974
[Epoch 76; Iter    30/   32] train: loss: 0.3562835
[Epoch 76] ogbg-molsider: 0.605931 val loss: 0.522859
[Epoch 76] ogbg-molsider: 0.595080 test loss: 0.538850
[Epoch 77; Iter    28/   32] train: loss: 0.3915392
[Epoch 77] ogbg-molsider: 0.595524 val loss: 0.525586
[Epoch 77] ogbg-molsider: 0.570590 test loss: 0.553583
[Epoch 78; Iter    26/   32] train: loss: 0.3803308
[Epoch 78] ogbg-molsider: 0.571513 val loss: 0.560600
[Epoch 78] ogbg-molsider: 0.587340 test loss: 0.562133
[Epoch 79; Iter    24/   32] train: loss: 0.3697990
[Epoch 79] ogbg-molsider: 0.590534 val loss: 0.545981
[Epoch 79] ogbg-molsider: 0.597583 test loss: 0.555394
[Epoch 80; Iter    22/   32] train: loss: 0.3689474
[Epoch 80] ogbg-molsider: 0.604226 val loss: 0.571472
[Epoch 80] ogbg-molsider: 0.598345 test loss: 0.586631
[Epoch 81; Iter    20/   32] train: loss: 0.3603156
[Epoch 81] ogbg-molsider: 0.600314 val loss: 0.534587
[Epoch 81] ogbg-molsider: 0.603632 test loss: 0.540236
[Epoch 82; Iter    18/   32] train: loss: 0.3237721
[Epoch 82] ogbg-molsider: 0.596841 val loss: 0.554624
[Epoch 82] ogbg-molsider: 0.600055 test loss: 0.569478
[Epoch 83; Iter    16/   32] train: loss: 0.3327211
[Epoch 83] ogbg-molsider: 0.593627 val loss: 0.564022
[Epoch 83] ogbg-molsider: 0.600413 test loss: 0.562059
[Epoch 84; Iter    14/   32] train: loss: 0.3096175
[Epoch 84] ogbg-molsider: 0.593222 val loss: 0.559752
[Epoch 33] ogbg-molsider: 0.601321 test loss: 0.490954
[Epoch 34; Iter    12/   36] train: loss: 0.5119124
[Epoch 34] ogbg-molsider: 0.574488 val loss: 0.482668
[Epoch 34] ogbg-molsider: 0.606644 test loss: 0.494002
[Epoch 35; Iter     6/   36] train: loss: 0.4804948
[Epoch 35; Iter    36/   36] train: loss: 0.4983312
[Epoch 35] ogbg-molsider: 0.551685 val loss: 0.497615
[Epoch 35] ogbg-molsider: 0.607434 test loss: 0.507979
[Epoch 36; Iter    30/   36] train: loss: 0.4582480
[Epoch 36] ogbg-molsider: 0.562431 val loss: 0.494337
[Epoch 36] ogbg-molsider: 0.607355 test loss: 0.506994
[Epoch 37; Iter    24/   36] train: loss: 0.5043446
[Epoch 37] ogbg-molsider: 0.572229 val loss: 0.483370
[Epoch 37] ogbg-molsider: 0.605173 test loss: 0.492883
[Epoch 38; Iter    18/   36] train: loss: 0.4550597
[Epoch 38] ogbg-molsider: 0.575383 val loss: 0.477843
[Epoch 38] ogbg-molsider: 0.607196 test loss: 0.486751
[Epoch 39; Iter    12/   36] train: loss: 0.5050503
[Epoch 39] ogbg-molsider: 0.589034 val loss: 0.487301
[Epoch 39] ogbg-molsider: 0.633900 test loss: 0.494980
[Epoch 40; Iter     6/   36] train: loss: 0.5078322
[Epoch 40; Iter    36/   36] train: loss: 0.4853355
[Epoch 40] ogbg-molsider: 0.538926 val loss: 0.626610
[Epoch 40] ogbg-molsider: 0.588441 test loss: 0.701259
[Epoch 41; Iter    30/   36] train: loss: 0.5103585
[Epoch 41] ogbg-molsider: 0.587602 val loss: 0.513968
[Epoch 41] ogbg-molsider: 0.599854 test loss: 0.525538
[Epoch 42; Iter    24/   36] train: loss: 0.4612973
[Epoch 42] ogbg-molsider: 0.630348 val loss: 0.472331
[Epoch 42] ogbg-molsider: 0.602524 test loss: 0.509095
[Epoch 43; Iter    18/   36] train: loss: 0.4738145
[Epoch 43] ogbg-molsider: 0.579810 val loss: 0.490865
[Epoch 43] ogbg-molsider: 0.586598 test loss: 0.521492
[Epoch 44; Iter    12/   36] train: loss: 0.4859414
[Epoch 44] ogbg-molsider: 0.628437 val loss: 0.473988
[Epoch 44] ogbg-molsider: 0.611192 test loss: 0.503153
[Epoch 45; Iter     6/   36] train: loss: 0.4549268
[Epoch 45; Iter    36/   36] train: loss: 0.4704609
[Epoch 45] ogbg-molsider: 0.591132 val loss: 0.489095
[Epoch 45] ogbg-molsider: 0.582775 test loss: 0.511823
[Epoch 46; Iter    30/   36] train: loss: 0.4660350
[Epoch 46] ogbg-molsider: 0.613321 val loss: 0.488077
[Epoch 46] ogbg-molsider: 0.601836 test loss: 0.523324
[Epoch 47; Iter    24/   36] train: loss: 0.4450943
[Epoch 47] ogbg-molsider: 0.566099 val loss: 0.540847
[Epoch 47] ogbg-molsider: 0.567313 test loss: 0.558234
[Epoch 48; Iter    18/   36] train: loss: 0.5119020
[Epoch 48] ogbg-molsider: 0.619399 val loss: 0.483131
[Epoch 48] ogbg-molsider: 0.598591 test loss: 0.521876
[Epoch 49; Iter    12/   36] train: loss: 0.4511277
[Epoch 49] ogbg-molsider: 0.602797 val loss: 0.479229
[Epoch 49] ogbg-molsider: 0.556640 test loss: 0.525198
[Epoch 50; Iter     6/   36] train: loss: 0.4463342
[Epoch 50; Iter    36/   36] train: loss: 0.5068766
[Epoch 50] ogbg-molsider: 0.562171 val loss: 0.512573
[Epoch 50] ogbg-molsider: 0.584654 test loss: 0.558466
[Epoch 51; Iter    30/   36] train: loss: 0.4282392
[Epoch 51] ogbg-molsider: 0.596851 val loss: 0.502930
[Epoch 51] ogbg-molsider: 0.605357 test loss: 0.535780
[Epoch 52; Iter    24/   36] train: loss: 0.4185721
[Epoch 52] ogbg-molsider: 0.582846 val loss: 0.510800
[Epoch 52] ogbg-molsider: 0.608925 test loss: 0.517391
[Epoch 53; Iter    18/   36] train: loss: 0.4972436
[Epoch 53] ogbg-molsider: 0.559071 val loss: 0.500745
[Epoch 53] ogbg-molsider: 0.598116 test loss: 0.519712
[Epoch 54; Iter    12/   36] train: loss: 0.4656290
[Epoch 54] ogbg-molsider: 0.556526 val loss: 0.539916
[Epoch 54] ogbg-molsider: 0.613275 test loss: 0.539190
[Epoch 55; Iter     6/   36] train: loss: 0.4028488
[Epoch 55; Iter    36/   36] train: loss: 0.4787343
[Epoch 55] ogbg-molsider: 0.618734 val loss: 0.729710
[Epoch 55] ogbg-molsider: 0.586256 test loss: 0.953993
[Epoch 56; Iter    30/   36] train: loss: 0.4535689
[Epoch 56] ogbg-molsider: 0.594634 val loss: 0.505795
[Epoch 56] ogbg-molsider: 0.585750 test loss: 0.533730
[Epoch 57; Iter    24/   36] train: loss: 0.3796918
[Epoch 57] ogbg-molsider: 0.589133 val loss: 0.509094
[Epoch 57] ogbg-molsider: 0.614319 test loss: 0.529973
[Epoch 58; Iter    18/   36] train: loss: 0.3971313
[Epoch 58] ogbg-molsider: 0.567295 val loss: 0.564850
[Epoch 58] ogbg-molsider: 0.574209 test loss: 0.612306
[Epoch 59; Iter    12/   36] train: loss: 0.4023370
[Epoch 59] ogbg-molsider: 0.616709 val loss: 0.537641
[Epoch 59] ogbg-molsider: 0.571789 test loss: 0.631760
[Epoch 60; Iter     6/   36] train: loss: 0.4308925
[Epoch 60; Iter    36/   36] train: loss: 0.4097903
[Epoch 60] ogbg-molsider: 0.609285 val loss: 0.518587
[Epoch 60] ogbg-molsider: 0.584665 test loss: 0.582646
[Epoch 61; Iter    30/   36] train: loss: 0.4205796
[Epoch 61] ogbg-molsider: 0.581502 val loss: 0.518420
[Epoch 61] ogbg-molsider: 0.586204 test loss: 0.539227
[Epoch 62; Iter    24/   36] train: loss: 0.4038189
[Epoch 62] ogbg-molsider: 0.585001 val loss: 0.540832
[Epoch 62] ogbg-molsider: 0.590160 test loss: 0.570364
[Epoch 63; Iter    18/   36] train: loss: 0.4719641
[Epoch 63] ogbg-molsider: 0.617196 val loss: 0.583844
[Epoch 63] ogbg-molsider: 0.587644 test loss: 0.958978
[Epoch 64; Iter    12/   36] train: loss: 0.4086672
[Epoch 64] ogbg-molsider: 0.602845 val loss: 0.535093
[Epoch 64] ogbg-molsider: 0.590395 test loss: 0.586529
[Epoch 65; Iter     6/   36] train: loss: 0.4072731
[Epoch 65; Iter    36/   36] train: loss: 0.3769098
[Epoch 65] ogbg-molsider: 0.607851 val loss: 0.796968
[Epoch 65] ogbg-molsider: 0.571312 test loss: 0.711683
[Epoch 66; Iter    30/   36] train: loss: 0.3687788
[Epoch 66] ogbg-molsider: 0.588125 val loss: 1.065727
[Epoch 66] ogbg-molsider: 0.585818 test loss: 0.609336
[Epoch 67; Iter    24/   36] train: loss: 0.3604829
[Epoch 67] ogbg-molsider: 0.598231 val loss: 0.528798
[Epoch 67] ogbg-molsider: 0.595713 test loss: 0.573877
[Epoch 68; Iter    18/   36] train: loss: 0.3487718
[Epoch 68] ogbg-molsider: 0.607570 val loss: 0.522433
[Epoch 68] ogbg-molsider: 0.583071 test loss: 0.548698
[Epoch 69; Iter    12/   36] train: loss: 0.3542133
[Epoch 69] ogbg-molsider: 0.606455 val loss: 0.655503
[Epoch 69] ogbg-molsider: 0.557513 test loss: 0.573032
[Epoch 70; Iter     6/   36] train: loss: 0.3366392
[Epoch 70; Iter    36/   36] train: loss: 0.3654064
[Epoch 70] ogbg-molsider: 0.602614 val loss: 0.532256
[Epoch 70] ogbg-molsider: 0.579763 test loss: 0.577370
[Epoch 71; Iter    30/   36] train: loss: 0.3723086
[Epoch 71] ogbg-molsider: 0.575848 val loss: 1.466584
[Epoch 71] ogbg-molsider: 0.553538 test loss: 0.608265
[Epoch 72; Iter    24/   36] train: loss: 0.3549922
[Epoch 72] ogbg-molsider: 0.628266 val loss: 0.518758
[Epoch 72] ogbg-molsider: 0.567395 test loss: 0.584370
[Epoch 73; Iter    18/   36] train: loss: 0.3797159
[Epoch 73] ogbg-molsider: 0.596482 val loss: 0.548774
[Epoch 73] ogbg-molsider: 0.584621 test loss: 0.567169
[Epoch 74; Iter    12/   36] train: loss: 0.3641362
[Epoch 74] ogbg-molsider: 0.617513 val loss: 0.636785
[Epoch 74] ogbg-molsider: 0.561317 test loss: 0.617806
[Epoch 75; Iter     6/   36] train: loss: 0.3276598
[Epoch 75; Iter    36/   36] train: loss: 0.3730260
[Epoch 75] ogbg-molsider: 0.602182 val loss: 0.915770
[Epoch 75] ogbg-molsider: 0.574666 test loss: 0.639932
[Epoch 76; Iter    30/   36] train: loss: 0.3389136
[Epoch 76] ogbg-molsider: 0.603929 val loss: 0.530291
[Epoch 76] ogbg-molsider: 0.587862 test loss: 0.568112
[Epoch 77; Iter    24/   36] train: loss: 0.3361677
[Epoch 77] ogbg-molsider: 0.612504 val loss: 0.523969
[Epoch 77] ogbg-molsider: 0.566342 test loss: 0.574797
[Epoch 78; Iter    18/   36] train: loss: 0.3434346
[Epoch 78] ogbg-molsider: 0.625129 val loss: 0.586968
[Epoch 78] ogbg-molsider: 0.580154 test loss: 0.609497
[Epoch 79; Iter    12/   36] train: loss: 0.3252276
[Epoch 79] ogbg-molsider: 0.596007 val loss: 0.694496
[Epoch 79] ogbg-molsider: 0.576336 test loss: 0.602960
[Epoch 80; Iter     6/   36] train: loss: 0.3434339
[Epoch 80; Iter    36/   36] train: loss: 0.3822846
[Epoch 80] ogbg-molsider: 0.620798 val loss: 0.555658
[Epoch 80] ogbg-molsider: 0.567706 test loss: 0.615902
[Epoch 35; Iter    22/   32] train: loss: 0.4836058
[Epoch 35] ogbg-molsider: 0.564363 val loss: 0.486872
[Epoch 35] ogbg-molsider: 0.581651 test loss: 0.517407
[Epoch 36; Iter    20/   32] train: loss: 0.4889400
[Epoch 36] ogbg-molsider: 0.561401 val loss: 0.488168
[Epoch 36] ogbg-molsider: 0.582261 test loss: 0.499988
[Epoch 37; Iter    18/   32] train: loss: 0.5008826
[Epoch 37] ogbg-molsider: 0.564754 val loss: 0.486733
[Epoch 37] ogbg-molsider: 0.577247 test loss: 0.501368
[Epoch 38; Iter    16/   32] train: loss: 0.4804938
[Epoch 38] ogbg-molsider: 0.569638 val loss: 0.488571
[Epoch 38] ogbg-molsider: 0.587030 test loss: 0.493593
[Epoch 39; Iter    14/   32] train: loss: 0.5169750
[Epoch 39] ogbg-molsider: 0.551212 val loss: 0.492578
[Epoch 39] ogbg-molsider: 0.595714 test loss: 0.505229
[Epoch 40; Iter    12/   32] train: loss: 0.4826719
[Epoch 40] ogbg-molsider: 0.576651 val loss: 0.482357
[Epoch 40] ogbg-molsider: 0.575109 test loss: 0.512470
[Epoch 41; Iter    10/   32] train: loss: 0.5117339
[Epoch 41] ogbg-molsider: 0.560972 val loss: 0.493197
[Epoch 41] ogbg-molsider: 0.591943 test loss: 0.503952
[Epoch 42; Iter     8/   32] train: loss: 0.4821298
[Epoch 42] ogbg-molsider: 0.563582 val loss: 0.492370
[Epoch 42] ogbg-molsider: 0.600794 test loss: 0.499602
[Epoch 43; Iter     6/   32] train: loss: 0.4582480
[Epoch 43] ogbg-molsider: 0.574922 val loss: 0.487029
[Epoch 43] ogbg-molsider: 0.588993 test loss: 0.495362
[Epoch 44; Iter     4/   32] train: loss: 0.5041624
[Epoch 44] ogbg-molsider: 0.592143 val loss: 0.509518
[Epoch 44] ogbg-molsider: 0.596010 test loss: 0.541862
[Epoch 45; Iter     2/   32] train: loss: 0.4388559
[Epoch 45; Iter    32/   32] train: loss: 0.5579699
[Epoch 45] ogbg-molsider: 0.568981 val loss: 0.583944
[Epoch 45] ogbg-molsider: 0.614349 test loss: 0.508751
[Epoch 46; Iter    30/   32] train: loss: 0.5088531
[Epoch 46] ogbg-molsider: 0.548285 val loss: 0.533778
[Epoch 46] ogbg-molsider: 0.572886 test loss: 0.666222
[Epoch 47; Iter    28/   32] train: loss: 0.4617840
[Epoch 47] ogbg-molsider: 0.589023 val loss: 0.499883
[Epoch 47] ogbg-molsider: 0.572630 test loss: 0.569679
[Epoch 48; Iter    26/   32] train: loss: 0.4739196
[Epoch 48] ogbg-molsider: 0.585789 val loss: 0.494170
[Epoch 48] ogbg-molsider: 0.583516 test loss: 0.546912
[Epoch 49; Iter    24/   32] train: loss: 0.4543564
[Epoch 49] ogbg-molsider: 0.558732 val loss: 0.490359
[Epoch 49] ogbg-molsider: 0.593932 test loss: 0.524521
[Epoch 50; Iter    22/   32] train: loss: 0.4707643
[Epoch 50] ogbg-molsider: 0.574600 val loss: 0.491983
[Epoch 50] ogbg-molsider: 0.591788 test loss: 0.508788
[Epoch 51; Iter    20/   32] train: loss: 0.4227586
[Epoch 51] ogbg-molsider: 0.570824 val loss: 0.507895
[Epoch 51] ogbg-molsider: 0.578459 test loss: 0.566834
[Epoch 52; Iter    18/   32] train: loss: 0.5004956
[Epoch 52] ogbg-molsider: 0.584859 val loss: 0.491006
[Epoch 52] ogbg-molsider: 0.597271 test loss: 0.517459
[Epoch 53; Iter    16/   32] train: loss: 0.4378653
[Epoch 53] ogbg-molsider: 0.605487 val loss: 0.486938
[Epoch 53] ogbg-molsider: 0.576652 test loss: 0.533094
[Epoch 54; Iter    14/   32] train: loss: 0.5253240
[Epoch 54] ogbg-molsider: 0.605819 val loss: 0.517434
[Epoch 54] ogbg-molsider: 0.558456 test loss: 0.545118
[Epoch 55; Iter    12/   32] train: loss: 0.4966015
[Epoch 55] ogbg-molsider: 0.577351 val loss: 0.495339
[Epoch 55] ogbg-molsider: 0.577210 test loss: 0.551961
[Epoch 56; Iter    10/   32] train: loss: 0.4731548
[Epoch 56] ogbg-molsider: 0.557635 val loss: 0.520853
[Epoch 56] ogbg-molsider: 0.600953 test loss: 0.523324
[Epoch 57; Iter     8/   32] train: loss: 0.5103199
[Epoch 57] ogbg-molsider: 0.541063 val loss: 0.616191
[Epoch 57] ogbg-molsider: 0.535375 test loss: 0.731965
[Epoch 58; Iter     6/   32] train: loss: 0.5042022
[Epoch 58] ogbg-molsider: 0.568056 val loss: 0.541724
[Epoch 58] ogbg-molsider: 0.571354 test loss: 0.564984
[Epoch 59; Iter     4/   32] train: loss: 0.5069326
[Epoch 59] ogbg-molsider: 0.579797 val loss: 0.510086
[Epoch 59] ogbg-molsider: 0.577365 test loss: 0.578019
[Epoch 60; Iter     2/   32] train: loss: 0.5155785
[Epoch 60; Iter    32/   32] train: loss: 0.4511705
[Epoch 60] ogbg-molsider: 0.573030 val loss: 0.508186
[Epoch 60] ogbg-molsider: 0.594211 test loss: 0.568641
[Epoch 61; Iter    30/   32] train: loss: 0.5347789
[Epoch 61] ogbg-molsider: 0.557041 val loss: 0.528663
[Epoch 61] ogbg-molsider: 0.554407 test loss: 0.575509
[Epoch 62; Iter    28/   32] train: loss: 0.4787119
[Epoch 62] ogbg-molsider: 0.564133 val loss: 0.484669
[Epoch 62] ogbg-molsider: 0.568584 test loss: 0.647129
[Epoch 63; Iter    26/   32] train: loss: 0.5003594
[Epoch 63] ogbg-molsider: 0.573938 val loss: 0.493574
[Epoch 63] ogbg-molsider: 0.571027 test loss: 0.575269
[Epoch 64; Iter    24/   32] train: loss: 0.5367855
[Epoch 64] ogbg-molsider: 0.571757 val loss: 0.486668
[Epoch 64] ogbg-molsider: 0.594960 test loss: 0.510337
[Epoch 65; Iter    22/   32] train: loss: 0.5049708
[Epoch 65] ogbg-molsider: 0.580407 val loss: 0.483491
[Epoch 65] ogbg-molsider: 0.625851 test loss: 0.499395
[Epoch 66; Iter    20/   32] train: loss: 0.4841641
[Epoch 66] ogbg-molsider: 0.596630 val loss: 0.481766
[Epoch 66] ogbg-molsider: 0.600644 test loss: 0.514108
[Epoch 67; Iter    18/   32] train: loss: 0.4718927
[Epoch 67] ogbg-molsider: 0.607303 val loss: 0.480050
[Epoch 67] ogbg-molsider: 0.625726 test loss: 0.498504
[Epoch 68; Iter    16/   32] train: loss: 0.4519619
[Epoch 68] ogbg-molsider: 0.603011 val loss: 0.485625
[Epoch 68] ogbg-molsider: 0.609049 test loss: 0.522118
[Epoch 69; Iter    14/   32] train: loss: 0.5278026
[Epoch 69] ogbg-molsider: 0.599613 val loss: 0.479861
[Epoch 69] ogbg-molsider: 0.617275 test loss: 0.509881
[Epoch 70; Iter    12/   32] train: loss: 0.5149391
[Epoch 70] ogbg-molsider: 0.614817 val loss: 0.480499
[Epoch 70] ogbg-molsider: 0.620174 test loss: 0.562334
[Epoch 71; Iter    10/   32] train: loss: 0.4176582
[Epoch 71] ogbg-molsider: 0.576368 val loss: 0.506860
[Epoch 71] ogbg-molsider: 0.613814 test loss: 0.525690
[Epoch 72; Iter     8/   32] train: loss: 0.4388298
[Epoch 72] ogbg-molsider: 0.570762 val loss: 0.528437
[Epoch 72] ogbg-molsider: 0.587889 test loss: 0.608759
[Epoch 73; Iter     6/   32] train: loss: 0.4360649
[Epoch 73] ogbg-molsider: 0.574152 val loss: 0.492412
[Epoch 73] ogbg-molsider: 0.602637 test loss: 0.541737
[Epoch 74; Iter     4/   32] train: loss: 0.4450191
[Epoch 74] ogbg-molsider: 0.597867 val loss: 0.481586
[Epoch 74] ogbg-molsider: 0.637747 test loss: 0.499970
[Epoch 75; Iter     2/   32] train: loss: 0.4619685
[Epoch 75; Iter    32/   32] train: loss: 0.5235243
[Epoch 75] ogbg-molsider: 0.579673 val loss: 0.493306
[Epoch 75] ogbg-molsider: 0.613733 test loss: 0.508046
[Epoch 76; Iter    30/   32] train: loss: 0.4952802
[Epoch 76] ogbg-molsider: 0.610564 val loss: 0.500322
[Epoch 76] ogbg-molsider: 0.583363 test loss: 0.524698
[Epoch 77; Iter    28/   32] train: loss: 0.4595245
[Epoch 77] ogbg-molsider: 0.614484 val loss: 0.494635
[Epoch 77] ogbg-molsider: 0.611981 test loss: 0.525514
[Epoch 78; Iter    26/   32] train: loss: 0.4831734
[Epoch 78] ogbg-molsider: 0.605115 val loss: 0.491434
[Epoch 78] ogbg-molsider: 0.624646 test loss: 0.513893
[Epoch 79; Iter    24/   32] train: loss: 0.4340053
[Epoch 79] ogbg-molsider: 0.608464 val loss: 0.484582
[Epoch 79] ogbg-molsider: 0.630038 test loss: 0.502155
[Epoch 80; Iter    22/   32] train: loss: 0.4715720
[Epoch 80] ogbg-molsider: 0.600527 val loss: 0.505337
[Epoch 80] ogbg-molsider: 0.618859 test loss: 0.535997
[Epoch 81; Iter    20/   32] train: loss: 0.4929627
[Epoch 81] ogbg-molsider: 0.607676 val loss: 0.485874
[Epoch 81] ogbg-molsider: 0.614061 test loss: 0.510107
[Epoch 82; Iter    18/   32] train: loss: 0.5010272
[Epoch 82] ogbg-molsider: 0.602483 val loss: 0.500610
[Epoch 82] ogbg-molsider: 0.629896 test loss: 0.522940
[Epoch 83; Iter    16/   32] train: loss: 0.4468258
[Epoch 83] ogbg-molsider: 0.592048 val loss: 0.504782
[Epoch 83] ogbg-molsider: 0.598156 test loss: 0.549869
[Epoch 84; Iter    14/   32] train: loss: 0.4530576
[Epoch 84] ogbg-molsider: 0.602487 val loss: 0.490698
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 89] ogbg-molsider: 0.573910 val loss: 0.617423
[Epoch 89] ogbg-molsider: 0.570251 test loss: 0.605710
[Epoch 90; Iter    27/   27] train: loss: 0.3559155
[Epoch 90] ogbg-molsider: 0.579564 val loss: 0.600447
[Epoch 90] ogbg-molsider: 0.551659 test loss: 0.610841
[Epoch 91] ogbg-molsider: 0.565722 val loss: 0.619532
[Epoch 91] ogbg-molsider: 0.570829 test loss: 0.611759
[Epoch 92; Iter     3/   27] train: loss: 0.3007253
[Epoch 92] ogbg-molsider: 0.577851 val loss: 0.619176
[Epoch 92] ogbg-molsider: 0.555562 test loss: 0.628804
[Epoch 93; Iter     6/   27] train: loss: 0.2697859
[Epoch 93] ogbg-molsider: 0.566336 val loss: 0.624165
[Epoch 93] ogbg-molsider: 0.569271 test loss: 0.602293
[Epoch 94; Iter     9/   27] train: loss: 0.3071020
[Epoch 94] ogbg-molsider: 0.552560 val loss: 0.668857
[Epoch 94] ogbg-molsider: 0.570769 test loss: 0.705507
[Epoch 95; Iter    12/   27] train: loss: 0.3205149
[Epoch 95] ogbg-molsider: 0.575527 val loss: 0.629696
[Epoch 95] ogbg-molsider: 0.576108 test loss: 0.614497
[Epoch 96; Iter    15/   27] train: loss: 0.3296002
[Epoch 96] ogbg-molsider: 0.557606 val loss: 0.635960
[Epoch 96] ogbg-molsider: 0.563778 test loss: 0.645653
[Epoch 97; Iter    18/   27] train: loss: 0.2838099
[Epoch 97] ogbg-molsider: 0.570087 val loss: 0.611851
[Epoch 97] ogbg-molsider: 0.567707 test loss: 0.613410
[Epoch 98; Iter    21/   27] train: loss: 0.3180041
[Epoch 98] ogbg-molsider: 0.574277 val loss: 0.614518
[Epoch 98] ogbg-molsider: 0.563171 test loss: 0.618711
[Epoch 99; Iter    24/   27] train: loss: 0.2588504
[Epoch 99] ogbg-molsider: 0.576263 val loss: 0.621816
[Epoch 99] ogbg-molsider: 0.576021 test loss: 0.615855
[Epoch 100; Iter    27/   27] train: loss: 0.2729425
[Epoch 100] ogbg-molsider: 0.567155 val loss: 0.636495
[Epoch 100] ogbg-molsider: 0.563087 test loss: 0.644592
[Epoch 101] ogbg-molsider: 0.572417 val loss: 0.628436
[Epoch 101] ogbg-molsider: 0.572989 test loss: 0.620030
[Epoch 102; Iter     3/   27] train: loss: 0.2635927
[Epoch 102] ogbg-molsider: 0.564994 val loss: 0.630826
[Epoch 102] ogbg-molsider: 0.571107 test loss: 0.616695
[Epoch 103; Iter     6/   27] train: loss: 0.2605221
[Epoch 103] ogbg-molsider: 0.570355 val loss: 0.634557
[Epoch 103] ogbg-molsider: 0.570862 test loss: 0.631896
[Epoch 104; Iter     9/   27] train: loss: 0.2642244
[Epoch 104] ogbg-molsider: 0.565351 val loss: 0.633620
[Epoch 104] ogbg-molsider: 0.564949 test loss: 0.629329
[Epoch 105; Iter    12/   27] train: loss: 0.2770738
[Epoch 105] ogbg-molsider: 0.564631 val loss: 0.665849
[Epoch 105] ogbg-molsider: 0.568836 test loss: 0.679657
[Epoch 106; Iter    15/   27] train: loss: 0.2733660
[Epoch 106] ogbg-molsider: 0.572287 val loss: 0.652661
[Epoch 106] ogbg-molsider: 0.571296 test loss: 0.652990
[Epoch 107; Iter    18/   27] train: loss: 0.2755820
[Epoch 107] ogbg-molsider: 0.573072 val loss: 0.673935
[Epoch 107] ogbg-molsider: 0.573751 test loss: 0.659057
[Epoch 108; Iter    21/   27] train: loss: 0.3088315
[Epoch 108] ogbg-molsider: 0.566827 val loss: 0.660867
[Epoch 108] ogbg-molsider: 0.572875 test loss: 0.650569
[Epoch 109; Iter    24/   27] train: loss: 0.2527830
[Epoch 109] ogbg-molsider: 0.571775 val loss: 0.656721
[Epoch 109] ogbg-molsider: 0.580244 test loss: 0.639815
[Epoch 110; Iter    27/   27] train: loss: 0.2455294
[Epoch 110] ogbg-molsider: 0.579397 val loss: 0.656586
[Epoch 110] ogbg-molsider: 0.594928 test loss: 0.634184
[Epoch 111] ogbg-molsider: 0.575494 val loss: 0.660010
[Epoch 111] ogbg-molsider: 0.577382 test loss: 0.682689
[Epoch 112; Iter     3/   27] train: loss: 0.2396470
[Epoch 112] ogbg-molsider: 0.572209 val loss: 0.654466
[Epoch 112] ogbg-molsider: 0.576616 test loss: 0.658535
[Epoch 113; Iter     6/   27] train: loss: 0.2469408
[Epoch 113] ogbg-molsider: 0.561721 val loss: 0.699025
[Epoch 113] ogbg-molsider: 0.569645 test loss: 0.705929
[Epoch 114; Iter     9/   27] train: loss: 0.2614515
[Epoch 114] ogbg-molsider: 0.573496 val loss: 0.656695
[Epoch 114] ogbg-molsider: 0.575597 test loss: 0.643661
[Epoch 115; Iter    12/   27] train: loss: 0.2593147
[Epoch 115] ogbg-molsider: 0.564900 val loss: 0.682795
[Epoch 115] ogbg-molsider: 0.580881 test loss: 0.666930
[Epoch 116; Iter    15/   27] train: loss: 0.2538729
[Epoch 116] ogbg-molsider: 0.566145 val loss: 0.693004
[Epoch 116] ogbg-molsider: 0.577460 test loss: 0.686352
[Epoch 117; Iter    18/   27] train: loss: 0.2360330
[Epoch 117] ogbg-molsider: 0.582584 val loss: 0.654214
[Epoch 117] ogbg-molsider: 0.580896 test loss: 0.631772
[Epoch 118; Iter    21/   27] train: loss: 0.2642272
[Epoch 118] ogbg-molsider: 0.574167 val loss: 0.685183
[Epoch 118] ogbg-molsider: 0.586592 test loss: 0.683384
[Epoch 119; Iter    24/   27] train: loss: 0.2720855
[Epoch 119] ogbg-molsider: 0.569203 val loss: 0.692808
[Epoch 119] ogbg-molsider: 0.576658 test loss: 0.684338
[Epoch 120; Iter    27/   27] train: loss: 0.2454948
[Epoch 120] ogbg-molsider: 0.568183 val loss: 0.696804
[Epoch 120] ogbg-molsider: 0.578182 test loss: 0.708368
[Epoch 121] ogbg-molsider: 0.566080 val loss: 0.708594
[Epoch 121] ogbg-molsider: 0.580963 test loss: 0.670936
Early stopping criterion based on -ogbg-molsider- that should be max reached after 121 epochs. Best model checkpoint was in epoch 61.
Statistics on  val_best_checkpoint
mean_pred: 0.914198637008667
std_pred: 2.1488966941833496
mean_targets: 0.5918128490447998
std_targets: 0.49153006076812744
prcauc: 0.6372525420524583
rocauc: 0.5906080686087319
ogbg-molsider: 0.5906080686087319
OGBNanLabelBCEWithLogitsLoss: 0.5377267565992143
Statistics on  test
mean_pred: 1.0455362796783447
std_pred: 2.2945287227630615
mean_targets: 0.5811965465545654
std_targets: 0.49339503049850464
prcauc: 0.6198248934498672
rocauc: 0.5934846574685203
ogbg-molsider: 0.5934846574685203
OGBNanLabelBCEWithLogitsLoss: 0.5448068016105228
Statistics on  train
mean_pred: 0.5861179232597351
std_pred: 2.1520540714263916
mean_targets: 0.5549498200416565
std_targets: 0.4969820976257324
prcauc: 0.7329913873857017
rocauc: 0.7762804492490377
ogbg-molsider: 0.7762804492490377
OGBNanLabelBCEWithLogitsLoss: 0.44884364030979296
[Epoch 89] ogbg-molsider: 0.571573 val loss: 0.593199
[Epoch 89] ogbg-molsider: 0.574407 test loss: 0.596969
[Epoch 90; Iter    27/   27] train: loss: 0.3585047
[Epoch 90] ogbg-molsider: 0.559851 val loss: 0.604088
[Epoch 90] ogbg-molsider: 0.589829 test loss: 0.561029
[Epoch 91] ogbg-molsider: 0.589098 val loss: 0.581345
[Epoch 91] ogbg-molsider: 0.580541 test loss: 0.570714
[Epoch 92; Iter     3/   27] train: loss: 0.3167872
[Epoch 92] ogbg-molsider: 0.571431 val loss: 0.600370
[Epoch 92] ogbg-molsider: 0.594936 test loss: 0.589707
[Epoch 93; Iter     6/   27] train: loss: 0.3050514
[Epoch 93] ogbg-molsider: 0.574208 val loss: 0.590257
[Epoch 93] ogbg-molsider: 0.585103 test loss: 0.570744
[Epoch 94; Iter     9/   27] train: loss: 0.2944414
[Epoch 94] ogbg-molsider: 0.575848 val loss: 0.582602
[Epoch 94] ogbg-molsider: 0.586612 test loss: 0.561337
[Epoch 95; Iter    12/   27] train: loss: 0.3431033
[Epoch 95] ogbg-molsider: 0.583397 val loss: 0.595708
[Epoch 95] ogbg-molsider: 0.589207 test loss: 0.574937
[Epoch 96; Iter    15/   27] train: loss: 0.2857565
[Epoch 96] ogbg-molsider: 0.572869 val loss: 0.604235
[Epoch 96] ogbg-molsider: 0.584550 test loss: 0.595620
[Epoch 97; Iter    18/   27] train: loss: 0.3125386
[Epoch 97] ogbg-molsider: 0.580879 val loss: 0.597160
[Epoch 97] ogbg-molsider: 0.583486 test loss: 0.610366
[Epoch 98; Iter    21/   27] train: loss: 0.3193256
[Epoch 98] ogbg-molsider: 0.577900 val loss: 0.594323
[Epoch 98] ogbg-molsider: 0.588762 test loss: 0.580594
[Epoch 99; Iter    24/   27] train: loss: 0.2930202
[Epoch 99] ogbg-molsider: 0.584377 val loss: 0.598737
[Epoch 99] ogbg-molsider: 0.579399 test loss: 0.610539
[Epoch 100; Iter    27/   27] train: loss: 0.2946959
[Epoch 100] ogbg-molsider: 0.571460 val loss: 0.621936
[Epoch 100] ogbg-molsider: 0.586389 test loss: 0.609852
[Epoch 101] ogbg-molsider: 0.572970 val loss: 0.612026
[Epoch 101] ogbg-molsider: 0.575356 test loss: 0.596498
[Epoch 102; Iter     3/   27] train: loss: 0.2742955
[Epoch 102] ogbg-molsider: 0.584896 val loss: 0.600448
[Epoch 102] ogbg-molsider: 0.592861 test loss: 0.575823
[Epoch 103; Iter     6/   27] train: loss: 0.2926502
[Epoch 103] ogbg-molsider: 0.577075 val loss: 0.628912
[Epoch 103] ogbg-molsider: 0.599814 test loss: 0.593696
[Epoch 104; Iter     9/   27] train: loss: 0.2833811
[Epoch 104] ogbg-molsider: 0.579835 val loss: 0.606835
[Epoch 104] ogbg-molsider: 0.592510 test loss: 0.599192
[Epoch 105; Iter    12/   27] train: loss: 0.3198594
[Epoch 105] ogbg-molsider: 0.553247 val loss: 0.643182
[Epoch 105] ogbg-molsider: 0.575504 test loss: 0.622602
[Epoch 106; Iter    15/   27] train: loss: 0.3883566
[Epoch 106] ogbg-molsider: 0.587515 val loss: 0.610109
[Epoch 106] ogbg-molsider: 0.586264 test loss: 0.597915
[Epoch 107; Iter    18/   27] train: loss: 0.3042990
[Epoch 107] ogbg-molsider: 0.572849 val loss: 0.623308
[Epoch 107] ogbg-molsider: 0.595227 test loss: 0.590471
[Epoch 108; Iter    21/   27] train: loss: 0.2747285
[Epoch 108] ogbg-molsider: 0.584703 val loss: 0.624426
[Epoch 108] ogbg-molsider: 0.594898 test loss: 0.612559
[Epoch 109; Iter    24/   27] train: loss: 0.2993143
[Epoch 109] ogbg-molsider: 0.582400 val loss: 0.620316
[Epoch 109] ogbg-molsider: 0.588731 test loss: 0.599334
[Epoch 110; Iter    27/   27] train: loss: 0.3412385
[Epoch 110] ogbg-molsider: 0.571475 val loss: 0.623856
[Epoch 110] ogbg-molsider: 0.576935 test loss: 0.612655
[Epoch 111] ogbg-molsider: 0.585397 val loss: 0.621902
[Epoch 111] ogbg-molsider: 0.592645 test loss: 0.601034
[Epoch 112; Iter     3/   27] train: loss: 0.2865614
[Epoch 112] ogbg-molsider: 0.588344 val loss: 0.616004
[Epoch 112] ogbg-molsider: 0.596507 test loss: 0.608734
[Epoch 113; Iter     6/   27] train: loss: 0.2853614
[Epoch 113] ogbg-molsider: 0.576786 val loss: 0.624716
[Epoch 113] ogbg-molsider: 0.578018 test loss: 0.617060
[Epoch 114; Iter     9/   27] train: loss: 0.2687384
[Epoch 114] ogbg-molsider: 0.566170 val loss: 0.662822
[Epoch 114] ogbg-molsider: 0.578612 test loss: 0.645646
[Epoch 115; Iter    12/   27] train: loss: 0.2592084
[Epoch 115] ogbg-molsider: 0.580327 val loss: 0.640891
[Epoch 115] ogbg-molsider: 0.581556 test loss: 0.631207
[Epoch 116; Iter    15/   27] train: loss: 0.2724104
[Epoch 116] ogbg-molsider: 0.574300 val loss: 0.657497
[Epoch 116] ogbg-molsider: 0.588213 test loss: 0.639011
[Epoch 117; Iter    18/   27] train: loss: 0.2781653
[Epoch 117] ogbg-molsider: 0.582969 val loss: 0.654538
[Epoch 117] ogbg-molsider: 0.594821 test loss: 0.634206
[Epoch 118; Iter    21/   27] train: loss: 0.2871026
[Epoch 118] ogbg-molsider: 0.582015 val loss: 0.650391
[Epoch 118] ogbg-molsider: 0.586408 test loss: 0.637343
[Epoch 119; Iter    24/   27] train: loss: 0.3096951
[Epoch 119] ogbg-molsider: 0.584666 val loss: 0.636466
[Epoch 119] ogbg-molsider: 0.596628 test loss: 0.608830
[Epoch 120; Iter    27/   27] train: loss: 0.2912476
[Epoch 120] ogbg-molsider: 0.575777 val loss: 0.652327
[Epoch 120] ogbg-molsider: 0.588460 test loss: 0.616191
[Epoch 121] ogbg-molsider: 0.585191 val loss: 0.644302
[Epoch 121] ogbg-molsider: 0.595688 test loss: 0.621791
[Epoch 122; Iter     3/   27] train: loss: 0.2534079
[Epoch 122] ogbg-molsider: 0.575677 val loss: 0.649217
[Epoch 122] ogbg-molsider: 0.591472 test loss: 0.626525
[Epoch 123; Iter     6/   27] train: loss: 0.2285510
[Epoch 123] ogbg-molsider: 0.584750 val loss: 0.654322
[Epoch 123] ogbg-molsider: 0.595383 test loss: 0.653692
[Epoch 124; Iter     9/   27] train: loss: 0.2404944
[Epoch 124] ogbg-molsider: 0.576598 val loss: 0.660686
[Epoch 124] ogbg-molsider: 0.596548 test loss: 0.627217
[Epoch 125; Iter    12/   27] train: loss: 0.2715116
[Epoch 125] ogbg-molsider: 0.574606 val loss: 0.670527
[Epoch 125] ogbg-molsider: 0.590916 test loss: 0.648190
[Epoch 126; Iter    15/   27] train: loss: 0.2133195
[Epoch 126] ogbg-molsider: 0.574874 val loss: 0.685384
[Epoch 126] ogbg-molsider: 0.590966 test loss: 0.665475
[Epoch 127; Iter    18/   27] train: loss: 0.2362652
[Epoch 127] ogbg-molsider: 0.570785 val loss: 0.679382
[Epoch 127] ogbg-molsider: 0.595482 test loss: 0.632029
[Epoch 128; Iter    21/   27] train: loss: 0.3050108
[Epoch 128] ogbg-molsider: 0.578737 val loss: 0.673647
[Epoch 128] ogbg-molsider: 0.591337 test loss: 0.660805
[Epoch 129; Iter    24/   27] train: loss: 0.2639319
[Epoch 129] ogbg-molsider: 0.575343 val loss: 0.671312
[Epoch 129] ogbg-molsider: 0.596484 test loss: 0.650295
[Epoch 130; Iter    27/   27] train: loss: 0.3001075
[Epoch 130] ogbg-molsider: 0.571317 val loss: 0.669557
[Epoch 130] ogbg-molsider: 0.589008 test loss: 0.637803
[Epoch 131] ogbg-molsider: 0.571098 val loss: 0.686948
[Epoch 131] ogbg-molsider: 0.588647 test loss: 0.700072
[Epoch 132; Iter     3/   27] train: loss: 0.2584689
[Epoch 132] ogbg-molsider: 0.576103 val loss: 0.687836
[Epoch 132] ogbg-molsider: 0.598295 test loss: 0.650503
[Epoch 133; Iter     6/   27] train: loss: 0.2395779
[Epoch 133] ogbg-molsider: 0.575932 val loss: 0.687932
[Epoch 133] ogbg-molsider: 0.597723 test loss: 0.662803
[Epoch 134; Iter     9/   27] train: loss: 0.2502254
[Epoch 134] ogbg-molsider: 0.569009 val loss: 0.684330
[Epoch 134] ogbg-molsider: 0.599557 test loss: 0.652946
[Epoch 135; Iter    12/   27] train: loss: 0.2261968
[Epoch 135] ogbg-molsider: 0.576919 val loss: 0.685890
[Epoch 135] ogbg-molsider: 0.597844 test loss: 0.671322
[Epoch 136; Iter    15/   27] train: loss: 0.2357658
[Epoch 136] ogbg-molsider: 0.569327 val loss: 0.671366
[Epoch 136] ogbg-molsider: 0.585025 test loss: 0.651667
[Epoch 137; Iter    18/   27] train: loss: 0.2561449
[Epoch 137] ogbg-molsider: 0.579424 val loss: 0.695083
[Epoch 137] ogbg-molsider: 0.594526 test loss: 0.695958
[Epoch 138; Iter    21/   27] train: loss: 0.2416293
[Epoch 138] ogbg-molsider: 0.575004 val loss: 0.694694
[Epoch 138] ogbg-molsider: 0.598990 test loss: 0.677565
[Epoch 139; Iter    24/   27] train: loss: 0.2193912
[Epoch 139] ogbg-molsider: 0.574040 val loss: 0.694994
[Epoch 139] ogbg-molsider: 0.585763 test loss: 0.670553
[Epoch 140; Iter    27/   27] train: loss: 0.2602337
[Epoch 140] ogbg-molsider: 0.578369 val loss: 0.695676
[Epoch 140] ogbg-molsider: 0.601824 test loss: 0.665813
[Epoch 81; Iter    30/   36] train: loss: 0.4006839
[Epoch 81] ogbg-molsider: 0.617155 val loss: 0.542481
[Epoch 81] ogbg-molsider: 0.567779 test loss: 0.589443
[Epoch 82; Iter    24/   36] train: loss: 0.3415984
[Epoch 82] ogbg-molsider: 0.626604 val loss: 0.555106
[Epoch 82] ogbg-molsider: 0.576021 test loss: 0.614231
[Epoch 83; Iter    18/   36] train: loss: 0.3313384
[Epoch 83] ogbg-molsider: 0.619904 val loss: 0.545998
[Epoch 83] ogbg-molsider: 0.560271 test loss: 0.618007
[Epoch 84; Iter    12/   36] train: loss: 0.3098894
[Epoch 84] ogbg-molsider: 0.620172 val loss: 0.567028
[Epoch 84] ogbg-molsider: 0.550745 test loss: 0.654965
[Epoch 85; Iter     6/   36] train: loss: 0.3380974
[Epoch 85; Iter    36/   36] train: loss: 0.3742452
[Epoch 85] ogbg-molsider: 0.638330 val loss: 0.776190
[Epoch 85] ogbg-molsider: 0.588627 test loss: 0.602847
[Epoch 86; Iter    30/   36] train: loss: 0.2768618
[Epoch 86] ogbg-molsider: 0.638876 val loss: 0.530578
[Epoch 86] ogbg-molsider: 0.572903 test loss: 0.596238
[Epoch 87; Iter    24/   36] train: loss: 0.3111278
[Epoch 87] ogbg-molsider: 0.647876 val loss: 0.537352
[Epoch 87] ogbg-molsider: 0.577497 test loss: 0.601598
[Epoch 88; Iter    18/   36] train: loss: 0.2681088
[Epoch 88] ogbg-molsider: 0.630447 val loss: 0.545977
[Epoch 88] ogbg-molsider: 0.553435 test loss: 0.621562
[Epoch 89; Iter    12/   36] train: loss: 0.3055508
[Epoch 89] ogbg-molsider: 0.637486 val loss: 0.557206
[Epoch 89] ogbg-molsider: 0.580356 test loss: 0.631779
[Epoch 90; Iter     6/   36] train: loss: 0.2518313
[Epoch 90; Iter    36/   36] train: loss: 0.3413802
[Epoch 90] ogbg-molsider: 0.610753 val loss: 0.608397
[Epoch 90] ogbg-molsider: 0.571010 test loss: 1.083858
[Epoch 91; Iter    30/   36] train: loss: 0.3360008
[Epoch 91] ogbg-molsider: 0.619156 val loss: 0.579001
[Epoch 91] ogbg-molsider: 0.585205 test loss: 0.643010
[Epoch 92; Iter    24/   36] train: loss: 0.3075706
[Epoch 92] ogbg-molsider: 0.647267 val loss: 0.553137
[Epoch 92] ogbg-molsider: 0.580465 test loss: 0.625778
[Epoch 93; Iter    18/   36] train: loss: 0.2671650
[Epoch 93] ogbg-molsider: 0.629573 val loss: 0.554136
[Epoch 93] ogbg-molsider: 0.567736 test loss: 0.622877
[Epoch 94; Iter    12/   36] train: loss: 0.2915056
[Epoch 94] ogbg-molsider: 0.631986 val loss: 0.570416
[Epoch 94] ogbg-molsider: 0.578594 test loss: 0.618358
[Epoch 95; Iter     6/   36] train: loss: 0.2858477
[Epoch 95; Iter    36/   36] train: loss: 0.3695715
[Epoch 95] ogbg-molsider: 0.632149 val loss: 0.585779
[Epoch 95] ogbg-molsider: 0.573736 test loss: 0.641438
[Epoch 96; Iter    30/   36] train: loss: 0.3227125
[Epoch 96] ogbg-molsider: 0.626143 val loss: 0.591273
[Epoch 96] ogbg-molsider: 0.578854 test loss: 0.670215
[Epoch 97; Iter    24/   36] train: loss: 0.2770921
[Epoch 97] ogbg-molsider: 0.628861 val loss: 0.603985
[Epoch 97] ogbg-molsider: 0.575466 test loss: 0.668180
[Epoch 98; Iter    18/   36] train: loss: 0.2992436
[Epoch 98] ogbg-molsider: 0.640450 val loss: 0.568548
[Epoch 98] ogbg-molsider: 0.573852 test loss: 0.644149
[Epoch 99; Iter    12/   36] train: loss: 0.2936125
[Epoch 99] ogbg-molsider: 0.637385 val loss: 0.613935
[Epoch 99] ogbg-molsider: 0.590929 test loss: 0.676820
[Epoch 100; Iter     6/   36] train: loss: 0.2292213
[Epoch 100; Iter    36/   36] train: loss: 0.2821141
[Epoch 100] ogbg-molsider: 0.622058 val loss: 0.610913
[Epoch 100] ogbg-molsider: 0.574190 test loss: 0.663552
[Epoch 101; Iter    30/   36] train: loss: 0.2558571
[Epoch 101] ogbg-molsider: 0.628212 val loss: 0.626105
[Epoch 101] ogbg-molsider: 0.576447 test loss: 0.729634
[Epoch 102; Iter    24/   36] train: loss: 0.2731527
[Epoch 102] ogbg-molsider: 0.624356 val loss: 0.613503
[Epoch 102] ogbg-molsider: 0.575384 test loss: 0.696813
[Epoch 103; Iter    18/   36] train: loss: 0.2650782
[Epoch 103] ogbg-molsider: 0.612110 val loss: 0.630755
[Epoch 103] ogbg-molsider: 0.575647 test loss: 0.661531
[Epoch 104; Iter    12/   36] train: loss: 0.2444722
[Epoch 104] ogbg-molsider: 0.624356 val loss: 0.622276
[Epoch 104] ogbg-molsider: 0.588020 test loss: 0.681873
[Epoch 105; Iter     6/   36] train: loss: 0.2350995
[Epoch 105; Iter    36/   36] train: loss: 0.3076883
[Epoch 105] ogbg-molsider: 0.632772 val loss: 0.629173
[Epoch 105] ogbg-molsider: 0.587535 test loss: 0.675853
[Epoch 106; Iter    30/   36] train: loss: 0.2451803
[Epoch 106] ogbg-molsider: 0.633338 val loss: 0.615492
[Epoch 106] ogbg-molsider: 0.604439 test loss: 0.656088
[Epoch 107; Iter    24/   36] train: loss: 0.2708675
[Epoch 107] ogbg-molsider: 0.621920 val loss: 0.643174
[Epoch 107] ogbg-molsider: 0.592833 test loss: 0.685947
[Epoch 108; Iter    18/   36] train: loss: 0.2210301
[Epoch 108] ogbg-molsider: 0.619798 val loss: 0.609127
[Epoch 108] ogbg-molsider: 0.596525 test loss: 0.654289
[Epoch 109; Iter    12/   36] train: loss: 0.2311190
[Epoch 109] ogbg-molsider: 0.622026 val loss: 0.617253
[Epoch 109] ogbg-molsider: 0.578157 test loss: 0.676855
[Epoch 110; Iter     6/   36] train: loss: 0.2713634
[Epoch 110; Iter    36/   36] train: loss: 0.2690716
[Epoch 110] ogbg-molsider: 0.631500 val loss: 0.614635
[Epoch 110] ogbg-molsider: 0.590077 test loss: 0.685658
[Epoch 111; Iter    30/   36] train: loss: 0.2414800
[Epoch 111] ogbg-molsider: 0.629018 val loss: 0.636702
[Epoch 111] ogbg-molsider: 0.588945 test loss: 0.709287
[Epoch 112; Iter    24/   36] train: loss: 0.2046444
[Epoch 112] ogbg-molsider: 0.627478 val loss: 0.621691
[Epoch 112] ogbg-molsider: 0.592583 test loss: 0.691457
[Epoch 113; Iter    18/   36] train: loss: 0.2144407
[Epoch 113] ogbg-molsider: 0.622698 val loss: 0.615445
[Epoch 113] ogbg-molsider: 0.592323 test loss: 0.668418
[Epoch 114; Iter    12/   36] train: loss: 0.2317086
[Epoch 114] ogbg-molsider: 0.613056 val loss: 0.641474
[Epoch 114] ogbg-molsider: 0.593265 test loss: 0.688805
[Epoch 115; Iter     6/   36] train: loss: 0.2286428
[Epoch 115; Iter    36/   36] train: loss: 0.2148803
[Epoch 115] ogbg-molsider: 0.616945 val loss: 0.635414
[Epoch 115] ogbg-molsider: 0.586318 test loss: 0.684552
[Epoch 116; Iter    30/   36] train: loss: 0.2434959
[Epoch 116] ogbg-molsider: 0.619085 val loss: 0.639615
[Epoch 116] ogbg-molsider: 0.587052 test loss: 0.693767
[Epoch 117; Iter    24/   36] train: loss: 0.2742712
[Epoch 117] ogbg-molsider: 0.631711 val loss: 0.622897
[Epoch 117] ogbg-molsider: 0.598299 test loss: 0.682696
[Epoch 118; Iter    18/   36] train: loss: 0.1965533
[Epoch 118] ogbg-molsider: 0.625825 val loss: 0.636187
[Epoch 118] ogbg-molsider: 0.596536 test loss: 0.702390
[Epoch 119; Iter    12/   36] train: loss: 0.2296332
[Epoch 119] ogbg-molsider: 0.615337 val loss: 0.651749
[Epoch 119] ogbg-molsider: 0.592439 test loss: 0.709108
[Epoch 120; Iter     6/   36] train: loss: 0.2525286
[Epoch 120; Iter    36/   36] train: loss: 0.3315492
[Epoch 120] ogbg-molsider: 0.626832 val loss: 0.637717
[Epoch 120] ogbg-molsider: 0.589521 test loss: 0.713552
[Epoch 121; Iter    30/   36] train: loss: 0.2223501
[Epoch 121] ogbg-molsider: 0.624398 val loss: 0.632172
[Epoch 121] ogbg-molsider: 0.600207 test loss: 0.690077
[Epoch 122; Iter    24/   36] train: loss: 0.2324508
[Epoch 122] ogbg-molsider: 0.633363 val loss: 0.641889
[Epoch 122] ogbg-molsider: 0.599266 test loss: 0.704833
[Epoch 123; Iter    18/   36] train: loss: 0.2592441
[Epoch 123] ogbg-molsider: 0.627463 val loss: 0.647755
[Epoch 123] ogbg-molsider: 0.608015 test loss: 0.687860
[Epoch 124; Iter    12/   36] train: loss: 0.2365537
[Epoch 124] ogbg-molsider: 0.620894 val loss: 0.635636
[Epoch 124] ogbg-molsider: 0.593377 test loss: 0.692605
[Epoch 125; Iter     6/   36] train: loss: 0.2215922
[Epoch 125; Iter    36/   36] train: loss: 0.2057468
[Epoch 125] ogbg-molsider: 0.624340 val loss: 0.649397
[Epoch 125] ogbg-molsider: 0.589196 test loss: 0.698602
[Epoch 126; Iter    30/   36] train: loss: 0.2300218
[Epoch 126] ogbg-molsider: 0.626750 val loss: 0.643537
[Epoch 126] ogbg-molsider: 0.594712 test loss: 0.692097
[Epoch 127; Iter    24/   36] train: loss: 0.2358010
[Epoch 127] ogbg-molsider: 0.629109 val loss: 0.653436
[Epoch 127] ogbg-molsider: 0.587729 test loss: 0.702199
[Epoch 128; Iter    18/   36] train: loss: 0.2103738
[Epoch 89] ogbg-molsider: 0.577113 val loss: 0.600109
[Epoch 89] ogbg-molsider: 0.557457 test loss: 0.561148
[Epoch 90; Iter    27/   27] train: loss: 0.3914054
[Epoch 90] ogbg-molsider: 0.587410 val loss: 0.589561
[Epoch 90] ogbg-molsider: 0.585441 test loss: 0.568252
[Epoch 91] ogbg-molsider: 0.582454 val loss: 0.560078
[Epoch 91] ogbg-molsider: 0.563247 test loss: 0.569318
[Epoch 92; Iter     3/   27] train: loss: 0.3240712
[Epoch 92] ogbg-molsider: 0.577971 val loss: 0.593314
[Epoch 92] ogbg-molsider: 0.576642 test loss: 0.579948
[Epoch 93; Iter     6/   27] train: loss: 0.3475960
[Epoch 93] ogbg-molsider: 0.574221 val loss: 0.594987
[Epoch 93] ogbg-molsider: 0.573291 test loss: 0.585743
[Epoch 94; Iter     9/   27] train: loss: 0.3116646
[Epoch 94] ogbg-molsider: 0.580982 val loss: 0.588168
[Epoch 94] ogbg-molsider: 0.571269 test loss: 0.570765
[Epoch 95; Iter    12/   27] train: loss: 0.2992705
[Epoch 95] ogbg-molsider: 0.568004 val loss: 0.604898
[Epoch 95] ogbg-molsider: 0.575271 test loss: 0.574674
[Epoch 96; Iter    15/   27] train: loss: 0.3167845
[Epoch 96] ogbg-molsider: 0.574663 val loss: 0.604745
[Epoch 96] ogbg-molsider: 0.570333 test loss: 0.586811
[Epoch 97; Iter    18/   27] train: loss: 0.3417887
[Epoch 97] ogbg-molsider: 0.576242 val loss: 0.600090
[Epoch 97] ogbg-molsider: 0.562867 test loss: 0.594840
[Epoch 98; Iter    21/   27] train: loss: 0.2942100
[Epoch 98] ogbg-molsider: 0.572108 val loss: 0.619452
[Epoch 98] ogbg-molsider: 0.568668 test loss: 0.599487
[Epoch 99; Iter    24/   27] train: loss: 0.3193081
[Epoch 99] ogbg-molsider: 0.578502 val loss: 0.608790
[Epoch 99] ogbg-molsider: 0.563461 test loss: 0.599233
[Epoch 100; Iter    27/   27] train: loss: 0.3583454
[Epoch 100] ogbg-molsider: 0.573601 val loss: 0.618536
[Epoch 100] ogbg-molsider: 0.568313 test loss: 0.602217
[Epoch 101] ogbg-molsider: 0.569378 val loss: 0.637110
[Epoch 101] ogbg-molsider: 0.575524 test loss: 0.616630
[Epoch 102; Iter     3/   27] train: loss: 0.2820039
[Epoch 102] ogbg-molsider: 0.581609 val loss: 0.618140
[Epoch 102] ogbg-molsider: 0.567483 test loss: 0.603563
[Epoch 103; Iter     6/   27] train: loss: 0.3350875
[Epoch 103] ogbg-molsider: 0.572561 val loss: 0.613892
[Epoch 103] ogbg-molsider: 0.565173 test loss: 0.590885
[Epoch 104; Iter     9/   27] train: loss: 0.2999660
[Epoch 104] ogbg-molsider: 0.584756 val loss: 0.629554
[Epoch 104] ogbg-molsider: 0.576675 test loss: 0.613224
[Epoch 105; Iter    12/   27] train: loss: 0.2945915
[Epoch 105] ogbg-molsider: 0.579713 val loss: 0.642630
[Epoch 105] ogbg-molsider: 0.567909 test loss: 0.612765
[Epoch 106; Iter    15/   27] train: loss: 0.3266525
[Epoch 106] ogbg-molsider: 0.577522 val loss: 0.622388
[Epoch 106] ogbg-molsider: 0.568863 test loss: 0.608576
[Epoch 107; Iter    18/   27] train: loss: 0.2947639
[Epoch 107] ogbg-molsider: 0.563159 val loss: 0.627580
[Epoch 107] ogbg-molsider: 0.565837 test loss: 0.605206
[Epoch 108; Iter    21/   27] train: loss: 0.2833626
[Epoch 108] ogbg-molsider: 0.572581 val loss: 0.637838
[Epoch 108] ogbg-molsider: 0.566371 test loss: 0.623391
[Epoch 109; Iter    24/   27] train: loss: 0.3567725
[Epoch 109] ogbg-molsider: 0.569456 val loss: 0.645395
[Epoch 109] ogbg-molsider: 0.577516 test loss: 0.616274
[Epoch 110; Iter    27/   27] train: loss: 0.3694068
[Epoch 110] ogbg-molsider: 0.571551 val loss: 0.675725
[Epoch 110] ogbg-molsider: 0.565608 test loss: 0.699673
[Epoch 111] ogbg-molsider: 0.584291 val loss: 0.643667
[Epoch 111] ogbg-molsider: 0.561925 test loss: 0.634205
[Epoch 112; Iter     3/   27] train: loss: 0.3011706
[Epoch 112] ogbg-molsider: 0.576265 val loss: 0.641833
[Epoch 112] ogbg-molsider: 0.567186 test loss: 0.630133
[Epoch 113; Iter     6/   27] train: loss: 0.3130533
[Epoch 113] ogbg-molsider: 0.573023 val loss: 0.650425
[Epoch 113] ogbg-molsider: 0.570378 test loss: 0.651935
[Epoch 114; Iter     9/   27] train: loss: 0.2797358
[Epoch 114] ogbg-molsider: 0.572338 val loss: 0.655619
[Epoch 114] ogbg-molsider: 0.581973 test loss: 0.625729
[Epoch 115; Iter    12/   27] train: loss: 0.2629446
[Epoch 115] ogbg-molsider: 0.577000 val loss: 0.635554
[Epoch 115] ogbg-molsider: 0.584987 test loss: 0.611576
[Epoch 116; Iter    15/   27] train: loss: 0.2382961
[Epoch 116] ogbg-molsider: 0.571028 val loss: 0.639284
[Epoch 116] ogbg-molsider: 0.576610 test loss: 0.631246
[Epoch 117; Iter    18/   27] train: loss: 0.3099600
[Epoch 117] ogbg-molsider: 0.572053 val loss: 0.670949
[Epoch 117] ogbg-molsider: 0.578069 test loss: 0.635806
[Epoch 118; Iter    21/   27] train: loss: 0.2579758
[Epoch 118] ogbg-molsider: 0.569595 val loss: 0.653659
[Epoch 118] ogbg-molsider: 0.580608 test loss: 0.632202
[Epoch 119; Iter    24/   27] train: loss: 0.2806787
[Epoch 119] ogbg-molsider: 0.571982 val loss: 0.662553
[Epoch 119] ogbg-molsider: 0.575075 test loss: 0.652827
[Epoch 120; Iter    27/   27] train: loss: 0.2749755
[Epoch 120] ogbg-molsider: 0.570433 val loss: 0.661455
[Epoch 120] ogbg-molsider: 0.583826 test loss: 0.631439
[Epoch 121] ogbg-molsider: 0.571256 val loss: 0.667511
[Epoch 121] ogbg-molsider: 0.575915 test loss: 0.647084
[Epoch 122; Iter     3/   27] train: loss: 0.2531457
[Epoch 122] ogbg-molsider: 0.569872 val loss: 0.671036
[Epoch 122] ogbg-molsider: 0.587619 test loss: 0.635121
[Epoch 123; Iter     6/   27] train: loss: 0.2460418
[Epoch 123] ogbg-molsider: 0.579048 val loss: 0.672248
[Epoch 123] ogbg-molsider: 0.588582 test loss: 0.645247
[Epoch 124; Iter     9/   27] train: loss: 0.2811323
[Epoch 124] ogbg-molsider: 0.566914 val loss: 0.677658
[Epoch 124] ogbg-molsider: 0.575261 test loss: 0.654152
[Epoch 125; Iter    12/   27] train: loss: 0.2464120
[Epoch 125] ogbg-molsider: 0.571550 val loss: 0.675052
[Epoch 125] ogbg-molsider: 0.575424 test loss: 0.656807
[Epoch 126; Iter    15/   27] train: loss: 0.2355360
[Epoch 126] ogbg-molsider: 0.566536 val loss: 0.671344
[Epoch 126] ogbg-molsider: 0.570105 test loss: 0.659619
[Epoch 127; Iter    18/   27] train: loss: 0.2018500
[Epoch 127] ogbg-molsider: 0.571602 val loss: 0.675241
[Epoch 127] ogbg-molsider: 0.577537 test loss: 0.654064
[Epoch 128; Iter    21/   27] train: loss: 0.2678609
[Epoch 128] ogbg-molsider: 0.571187 val loss: 0.663794
[Epoch 128] ogbg-molsider: 0.579119 test loss: 0.639794
[Epoch 129; Iter    24/   27] train: loss: 0.2820722
[Epoch 129] ogbg-molsider: 0.573792 val loss: 0.679214
[Epoch 129] ogbg-molsider: 0.583255 test loss: 0.652605
[Epoch 130; Iter    27/   27] train: loss: 0.2564079
[Epoch 130] ogbg-molsider: 0.572914 val loss: 0.681980
[Epoch 130] ogbg-molsider: 0.575083 test loss: 0.677389
[Epoch 131] ogbg-molsider: 0.572034 val loss: 0.678382
[Epoch 131] ogbg-molsider: 0.578656 test loss: 0.645268
[Epoch 132; Iter     3/   27] train: loss: 0.2650377
[Epoch 132] ogbg-molsider: 0.577010 val loss: 0.687854
[Epoch 132] ogbg-molsider: 0.587264 test loss: 0.664541
[Epoch 133; Iter     6/   27] train: loss: 0.2653171
[Epoch 133] ogbg-molsider: 0.564859 val loss: 0.687203
[Epoch 133] ogbg-molsider: 0.582215 test loss: 0.649707
[Epoch 134; Iter     9/   27] train: loss: 0.2389267
[Epoch 134] ogbg-molsider: 0.574950 val loss: 0.693087
[Epoch 134] ogbg-molsider: 0.590887 test loss: 0.654375
[Epoch 135; Iter    12/   27] train: loss: 0.2421322
[Epoch 135] ogbg-molsider: 0.577797 val loss: 0.697491
[Epoch 135] ogbg-molsider: 0.579078 test loss: 0.679522
[Epoch 136; Iter    15/   27] train: loss: 0.2312335
[Epoch 136] ogbg-molsider: 0.568031 val loss: 0.719890
[Epoch 136] ogbg-molsider: 0.583712 test loss: 0.680253
[Epoch 137; Iter    18/   27] train: loss: 0.2416003
[Epoch 137] ogbg-molsider: 0.572566 val loss: 0.699040
[Epoch 137] ogbg-molsider: 0.582236 test loss: 0.665697
[Epoch 138; Iter    21/   27] train: loss: 0.2693202
[Epoch 138] ogbg-molsider: 0.565832 val loss: 0.718024
[Epoch 138] ogbg-molsider: 0.585413 test loss: 0.671271
[Epoch 139; Iter    24/   27] train: loss: 0.2151547
[Epoch 139] ogbg-molsider: 0.567154 val loss: 0.707483
[Epoch 139] ogbg-molsider: 0.582820 test loss: 0.661466
[Epoch 140; Iter    27/   27] train: loss: 0.2409602
[Epoch 140] ogbg-molsider: 0.563738 val loss: 0.721311
[Epoch 140] ogbg-molsider: 0.574703 test loss: 0.687816
[Epoch 81; Iter    30/   36] train: loss: 0.3218274
[Epoch 81] ogbg-molsider: 0.655435 val loss: 0.512196
[Epoch 81] ogbg-molsider: 0.587110 test loss: 0.587308
[Epoch 82; Iter    24/   36] train: loss: 0.3240303
[Epoch 82] ogbg-molsider: 0.630022 val loss: 0.562086
[Epoch 82] ogbg-molsider: 0.584707 test loss: 0.604812
[Epoch 83; Iter    18/   36] train: loss: 0.3219155
[Epoch 83] ogbg-molsider: 0.645006 val loss: 0.531973
[Epoch 83] ogbg-molsider: 0.576475 test loss: 0.590776
[Epoch 84; Iter    12/   36] train: loss: 0.3255418
[Epoch 84] ogbg-molsider: 0.640043 val loss: 0.546522
[Epoch 84] ogbg-molsider: 0.577219 test loss: 0.609751
[Epoch 85; Iter     6/   36] train: loss: 0.2930594
[Epoch 85; Iter    36/   36] train: loss: 0.3139274
[Epoch 85] ogbg-molsider: 0.637722 val loss: 0.535548
[Epoch 85] ogbg-molsider: 0.582760 test loss: 0.589485
[Epoch 86; Iter    30/   36] train: loss: 0.3204975
[Epoch 86] ogbg-molsider: 0.634194 val loss: 0.548619
[Epoch 86] ogbg-molsider: 0.567660 test loss: 0.611974
[Epoch 87; Iter    24/   36] train: loss: 0.3093330
[Epoch 87] ogbg-molsider: 0.648395 val loss: 0.542618
[Epoch 87] ogbg-molsider: 0.571279 test loss: 0.619643
[Epoch 88; Iter    18/   36] train: loss: 0.3206101
[Epoch 88] ogbg-molsider: 0.641162 val loss: 0.547810
[Epoch 88] ogbg-molsider: 0.570241 test loss: 0.624534
[Epoch 89; Iter    12/   36] train: loss: 0.2902550
[Epoch 89] ogbg-molsider: 0.642334 val loss: 0.554042
[Epoch 89] ogbg-molsider: 0.570613 test loss: 0.645946
[Epoch 90; Iter     6/   36] train: loss: 0.2782307
[Epoch 90; Iter    36/   36] train: loss: 0.3213691
[Epoch 90] ogbg-molsider: 0.639504 val loss: 0.560206
[Epoch 90] ogbg-molsider: 0.563226 test loss: 0.662288
[Epoch 91; Iter    30/   36] train: loss: 0.3009480
[Epoch 91] ogbg-molsider: 0.627266 val loss: 0.560621
[Epoch 91] ogbg-molsider: 0.560769 test loss: 0.633132
[Epoch 92; Iter    24/   36] train: loss: 0.2988607
[Epoch 92] ogbg-molsider: 0.630289 val loss: 0.586217
[Epoch 92] ogbg-molsider: 0.570048 test loss: 0.652664
[Epoch 93; Iter    18/   36] train: loss: 0.2615004
[Epoch 93] ogbg-molsider: 0.652919 val loss: 0.540798
[Epoch 93] ogbg-molsider: 0.568594 test loss: 0.622389
[Epoch 94; Iter    12/   36] train: loss: 0.2773220
[Epoch 94] ogbg-molsider: 0.626372 val loss: 0.562994
[Epoch 94] ogbg-molsider: 0.557405 test loss: 0.646895
[Epoch 95; Iter     6/   36] train: loss: 0.3173559
[Epoch 95; Iter    36/   36] train: loss: 0.2908935
[Epoch 95] ogbg-molsider: 0.640321 val loss: 0.561049
[Epoch 95] ogbg-molsider: 0.566730 test loss: 0.659448
[Epoch 96; Iter    30/   36] train: loss: 0.3069867
[Epoch 96] ogbg-molsider: 0.641398 val loss: 0.553235
[Epoch 96] ogbg-molsider: 0.568423 test loss: 0.635583
[Epoch 97; Iter    24/   36] train: loss: 0.3067123
[Epoch 97] ogbg-molsider: 0.622220 val loss: 0.575235
[Epoch 97] ogbg-molsider: 0.569252 test loss: 0.664989
[Epoch 98; Iter    18/   36] train: loss: 0.3101138
[Epoch 98] ogbg-molsider: 0.635024 val loss: 0.586126
[Epoch 98] ogbg-molsider: 0.586013 test loss: 0.672860
[Epoch 99; Iter    12/   36] train: loss: 0.3654338
[Epoch 99] ogbg-molsider: 0.630683 val loss: 0.568612
[Epoch 99] ogbg-molsider: 0.564503 test loss: 0.667133
[Epoch 100; Iter     6/   36] train: loss: 0.2426542
[Epoch 100; Iter    36/   36] train: loss: 0.3154791
[Epoch 100] ogbg-molsider: 0.628598 val loss: 0.607897
[Epoch 100] ogbg-molsider: 0.560384 test loss: 0.695385
[Epoch 101; Iter    30/   36] train: loss: 0.3041268
[Epoch 101] ogbg-molsider: 0.640024 val loss: 0.560050
[Epoch 101] ogbg-molsider: 0.595806 test loss: 0.629062
[Epoch 102; Iter    24/   36] train: loss: 0.3305951
[Epoch 102] ogbg-molsider: 0.633204 val loss: 0.589272
[Epoch 102] ogbg-molsider: 0.569290 test loss: 0.670569
[Epoch 103; Iter    18/   36] train: loss: 0.2798788
[Epoch 103] ogbg-molsider: 0.633548 val loss: 0.594482
[Epoch 103] ogbg-molsider: 0.559828 test loss: 0.676972
[Epoch 104; Iter    12/   36] train: loss: 0.2646051
[Epoch 104] ogbg-molsider: 0.634800 val loss: 0.591041
[Epoch 104] ogbg-molsider: 0.549476 test loss: 0.715946
[Epoch 105; Iter     6/   36] train: loss: 0.2607242
[Epoch 105; Iter    36/   36] train: loss: 0.2884213
[Epoch 105] ogbg-molsider: 0.626070 val loss: 0.578068
[Epoch 105] ogbg-molsider: 0.559375 test loss: 0.697170
[Epoch 106; Iter    30/   36] train: loss: 0.3012943
[Epoch 106] ogbg-molsider: 0.650276 val loss: 0.567903
[Epoch 106] ogbg-molsider: 0.572512 test loss: 0.676162
[Epoch 107; Iter    24/   36] train: loss: 0.2920816
[Epoch 107] ogbg-molsider: 0.637001 val loss: 0.574328
[Epoch 107] ogbg-molsider: 0.576449 test loss: 0.673761
[Epoch 108; Iter    18/   36] train: loss: 0.2546541
[Epoch 108] ogbg-molsider: 0.648634 val loss: 0.564573
[Epoch 108] ogbg-molsider: 0.561124 test loss: 0.677000
[Epoch 109; Iter    12/   36] train: loss: 0.2556068
[Epoch 109] ogbg-molsider: 0.641191 val loss: 0.580372
[Epoch 109] ogbg-molsider: 0.563824 test loss: 0.687129
[Epoch 110; Iter     6/   36] train: loss: 0.2886778
[Epoch 110; Iter    36/   36] train: loss: 0.2470166
[Epoch 110] ogbg-molsider: 0.653875 val loss: 0.610693
[Epoch 110] ogbg-molsider: 0.558241 test loss: 0.748964
[Epoch 111; Iter    30/   36] train: loss: 0.2417271
[Epoch 111] ogbg-molsider: 0.646714 val loss: 0.576463
[Epoch 111] ogbg-molsider: 0.574306 test loss: 0.696073
[Epoch 112; Iter    24/   36] train: loss: 0.2445787
[Epoch 112] ogbg-molsider: 0.647059 val loss: 0.581186
[Epoch 112] ogbg-molsider: 0.569103 test loss: 0.692284
[Epoch 113; Iter    18/   36] train: loss: 0.2159329
[Epoch 113] ogbg-molsider: 0.638541 val loss: 0.605049
[Epoch 113] ogbg-molsider: 0.567227 test loss: 0.714959
[Epoch 114; Iter    12/   36] train: loss: 0.2370776
[Epoch 114] ogbg-molsider: 0.652826 val loss: 0.582439
[Epoch 114] ogbg-molsider: 0.562035 test loss: 0.707523
[Epoch 115; Iter     6/   36] train: loss: 0.2327582
[Epoch 115; Iter    36/   36] train: loss: 0.2722702
[Epoch 115] ogbg-molsider: 0.644148 val loss: 0.587736
[Epoch 115] ogbg-molsider: 0.565007 test loss: 0.702202
[Epoch 116; Iter    30/   36] train: loss: 0.2420993
[Epoch 116] ogbg-molsider: 0.645286 val loss: 0.585659
[Epoch 116] ogbg-molsider: 0.564553 test loss: 0.711285
[Epoch 117; Iter    24/   36] train: loss: 0.2243239
[Epoch 117] ogbg-molsider: 0.641689 val loss: 0.588376
[Epoch 117] ogbg-molsider: 0.568459 test loss: 0.710254
[Epoch 118; Iter    18/   36] train: loss: 0.2561297
[Epoch 118] ogbg-molsider: 0.648800 val loss: 0.591511
[Epoch 118] ogbg-molsider: 0.563702 test loss: 0.717471
[Epoch 119; Iter    12/   36] train: loss: 0.2041032
[Epoch 119] ogbg-molsider: 0.638754 val loss: 0.597381
[Epoch 119] ogbg-molsider: 0.565134 test loss: 0.711465
[Epoch 120; Iter     6/   36] train: loss: 0.2773235
[Epoch 120; Iter    36/   36] train: loss: 0.2201030
[Epoch 120] ogbg-molsider: 0.642759 val loss: 0.618869
[Epoch 120] ogbg-molsider: 0.569473 test loss: 0.723659
[Epoch 121; Iter    30/   36] train: loss: 0.2871508
[Epoch 121] ogbg-molsider: 0.635979 val loss: 0.607760
[Epoch 121] ogbg-molsider: 0.577080 test loss: 0.713666
[Epoch 122; Iter    24/   36] train: loss: 0.2416883
[Epoch 122] ogbg-molsider: 0.632375 val loss: 0.606082
[Epoch 122] ogbg-molsider: 0.564372 test loss: 0.721877
[Epoch 123; Iter    18/   36] train: loss: 0.2509891
[Epoch 123] ogbg-molsider: 0.638925 val loss: 0.604238
[Epoch 123] ogbg-molsider: 0.572487 test loss: 0.727759
[Epoch 124; Iter    12/   36] train: loss: 0.2424870
[Epoch 124] ogbg-molsider: 0.631392 val loss: 0.630500
[Epoch 124] ogbg-molsider: 0.562513 test loss: 0.744495
[Epoch 125; Iter     6/   36] train: loss: 0.2156603
[Epoch 125; Iter    36/   36] train: loss: 0.2612205
[Epoch 125] ogbg-molsider: 0.644133 val loss: 0.612765
[Epoch 125] ogbg-molsider: 0.556210 test loss: 0.743351
[Epoch 126; Iter    30/   36] train: loss: 0.2236236
[Epoch 126] ogbg-molsider: 0.644646 val loss: 0.625188
[Epoch 126] ogbg-molsider: 0.570669 test loss: 0.733035
[Epoch 127; Iter    24/   36] train: loss: 0.2347736
[Epoch 127] ogbg-molsider: 0.644971 val loss: 0.607013
[Epoch 127] ogbg-molsider: 0.563429 test loss: 0.732232
[Epoch 128; Iter    18/   36] train: loss: 0.2386502
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 84] ogbg-molsider: 0.546747 test loss: 0.627905
[Epoch 85; Iter    12/   32] train: loss: 0.4064944
[Epoch 85] ogbg-molsider: 0.569301 val loss: 0.566373
[Epoch 85] ogbg-molsider: 0.561179 test loss: 0.589915
[Epoch 86; Iter    10/   32] train: loss: 0.3524809
[Epoch 86] ogbg-molsider: 0.587934 val loss: 0.558155
[Epoch 86] ogbg-molsider: 0.563668 test loss: 0.583974
[Epoch 87; Iter     8/   32] train: loss: 0.3969424
[Epoch 87] ogbg-molsider: 0.581752 val loss: 0.549019
[Epoch 87] ogbg-molsider: 0.563348 test loss: 0.561806
[Epoch 88; Iter     6/   32] train: loss: 0.3260368
[Epoch 88] ogbg-molsider: 0.573783 val loss: 0.560134
[Epoch 88] ogbg-molsider: 0.572920 test loss: 0.563129
[Epoch 89; Iter     4/   32] train: loss: 0.3532729
[Epoch 89] ogbg-molsider: 0.575468 val loss: 0.559716
[Epoch 89] ogbg-molsider: 0.563976 test loss: 0.589504
[Epoch 90; Iter     2/   32] train: loss: 0.3391668
[Epoch 90; Iter    32/   32] train: loss: 0.3182003
[Epoch 90] ogbg-molsider: 0.567673 val loss: 0.591780
[Epoch 90] ogbg-molsider: 0.565375 test loss: 0.593439
[Epoch 91; Iter    30/   32] train: loss: 0.3187892
[Epoch 91] ogbg-molsider: 0.575213 val loss: 0.572319
[Epoch 91] ogbg-molsider: 0.564730 test loss: 0.610288
[Epoch 92; Iter    28/   32] train: loss: 0.3246216
[Epoch 92] ogbg-molsider: 0.580341 val loss: 0.574055
[Epoch 92] ogbg-molsider: 0.555210 test loss: 0.598852
[Epoch 93; Iter    26/   32] train: loss: 0.3374601
[Epoch 93] ogbg-molsider: 0.581768 val loss: 0.581105
[Epoch 93] ogbg-molsider: 0.578609 test loss: 0.575004
[Epoch 94; Iter    24/   32] train: loss: 0.3436071
[Epoch 94] ogbg-molsider: 0.579751 val loss: 0.584315
[Epoch 94] ogbg-molsider: 0.562119 test loss: 0.622280
[Epoch 95; Iter    22/   32] train: loss: 0.3215704
[Epoch 95] ogbg-molsider: 0.572545 val loss: 0.606042
[Epoch 95] ogbg-molsider: 0.563345 test loss: 0.625248
[Epoch 96; Iter    20/   32] train: loss: 0.3677011
[Epoch 96] ogbg-molsider: 0.570572 val loss: 0.584958
[Epoch 96] ogbg-molsider: 0.565588 test loss: 0.596134
[Epoch 97; Iter    18/   32] train: loss: 0.3376070
[Epoch 97] ogbg-molsider: 0.575136 val loss: 0.586859
[Epoch 97] ogbg-molsider: 0.568076 test loss: 0.601042
[Epoch 98; Iter    16/   32] train: loss: 0.3213040
[Epoch 98] ogbg-molsider: 0.571840 val loss: 0.594569
[Epoch 98] ogbg-molsider: 0.571334 test loss: 0.589832
[Epoch 99; Iter    14/   32] train: loss: 0.3417844
[Epoch 99] ogbg-molsider: 0.577804 val loss: 0.590402
[Epoch 99] ogbg-molsider: 0.579427 test loss: 0.609911
[Epoch 100; Iter    12/   32] train: loss: 0.3172169
[Epoch 100] ogbg-molsider: 0.587721 val loss: 0.577872
[Epoch 100] ogbg-molsider: 0.567664 test loss: 0.602433
[Epoch 101; Iter    10/   32] train: loss: 0.3167980
[Epoch 101] ogbg-molsider: 0.574363 val loss: 0.599854
[Epoch 101] ogbg-molsider: 0.573527 test loss: 0.612955
[Epoch 102; Iter     8/   32] train: loss: 0.3041985
[Epoch 102] ogbg-molsider: 0.575553 val loss: 0.586145
[Epoch 102] ogbg-molsider: 0.577140 test loss: 0.598777
[Epoch 103; Iter     6/   32] train: loss: 0.3345939
[Epoch 103] ogbg-molsider: 0.584305 val loss: 0.619887
[Epoch 103] ogbg-molsider: 0.588766 test loss: 0.620607
[Epoch 104; Iter     4/   32] train: loss: 0.3629358
[Epoch 104] ogbg-molsider: 0.560994 val loss: 0.627757
[Epoch 104] ogbg-molsider: 0.591618 test loss: 0.587255
[Epoch 105; Iter     2/   32] train: loss: 0.3057884
[Epoch 105; Iter    32/   32] train: loss: 0.3338626
[Epoch 105] ogbg-molsider: 0.578145 val loss: 0.592908
[Epoch 105] ogbg-molsider: 0.581564 test loss: 0.595646
[Epoch 106; Iter    30/   32] train: loss: 0.3372812
[Epoch 106] ogbg-molsider: 0.565098 val loss: 0.609946
[Epoch 106] ogbg-molsider: 0.587496 test loss: 0.591487
[Epoch 107; Iter    28/   32] train: loss: 0.3198664
[Epoch 107] ogbg-molsider: 0.568916 val loss: 0.615090
[Epoch 107] ogbg-molsider: 0.569831 test loss: 0.617919
[Epoch 108; Iter    26/   32] train: loss: 0.3793398
[Epoch 108] ogbg-molsider: 0.589881 val loss: 0.613563
[Epoch 108] ogbg-molsider: 0.582593 test loss: 0.633346
[Epoch 109; Iter    24/   32] train: loss: 0.3246594
[Epoch 109] ogbg-molsider: 0.602216 val loss: 0.606503
[Epoch 109] ogbg-molsider: 0.594017 test loss: 0.607390
[Epoch 110; Iter    22/   32] train: loss: 0.2890006
[Epoch 110] ogbg-molsider: 0.589556 val loss: 0.618763
[Epoch 110] ogbg-molsider: 0.563169 test loss: 0.694323
[Epoch 111; Iter    20/   32] train: loss: 0.2914686
[Epoch 111] ogbg-molsider: 0.577056 val loss: 0.612710
[Epoch 111] ogbg-molsider: 0.584695 test loss: 0.602273
[Epoch 112; Iter    18/   32] train: loss: 0.3895521
[Epoch 112] ogbg-molsider: 0.592573 val loss: 0.606957
[Epoch 112] ogbg-molsider: 0.574548 test loss: 0.642627
[Epoch 113; Iter    16/   32] train: loss: 0.3270773
[Epoch 113] ogbg-molsider: 0.601031 val loss: 0.606134
[Epoch 113] ogbg-molsider: 0.601266 test loss: 0.612105
[Epoch 114; Iter    14/   32] train: loss: 0.3108988
[Epoch 114] ogbg-molsider: 0.591854 val loss: 0.590714
[Epoch 114] ogbg-molsider: 0.578196 test loss: 0.609778
[Epoch 115; Iter    12/   32] train: loss: 0.2870875
[Epoch 115] ogbg-molsider: 0.592864 val loss: 0.605190
[Epoch 115] ogbg-molsider: 0.587665 test loss: 0.630911
[Epoch 116; Iter    10/   32] train: loss: 0.3005518
[Epoch 116] ogbg-molsider: 0.600616 val loss: 0.592587
[Epoch 116] ogbg-molsider: 0.593136 test loss: 0.614905
[Epoch 117; Iter     8/   32] train: loss: 0.3045003
[Epoch 117] ogbg-molsider: 0.585942 val loss: 0.615479
[Epoch 117] ogbg-molsider: 0.582173 test loss: 0.628979
[Epoch 118; Iter     6/   32] train: loss: 0.2744673
[Epoch 118] ogbg-molsider: 0.597356 val loss: 0.608734
[Epoch 118] ogbg-molsider: 0.577202 test loss: 0.627570
[Epoch 119; Iter     4/   32] train: loss: 0.2999013
[Epoch 119] ogbg-molsider: 0.600066 val loss: 0.614029
[Epoch 119] ogbg-molsider: 0.571951 test loss: 0.643127
[Epoch 120; Iter     2/   32] train: loss: 0.3042015
[Epoch 120; Iter    32/   32] train: loss: 0.2941627
[Epoch 120] ogbg-molsider: 0.600593 val loss: 0.593617
[Epoch 120] ogbg-molsider: 0.583525 test loss: 0.607753
[Epoch 121; Iter    30/   32] train: loss: 0.3150778
[Epoch 121] ogbg-molsider: 0.595423 val loss: 0.652575
[Epoch 121] ogbg-molsider: 0.591277 test loss: 0.676900
[Epoch 122; Iter    28/   32] train: loss: 0.2861162
[Epoch 122] ogbg-molsider: 0.580142 val loss: 0.623819
[Epoch 122] ogbg-molsider: 0.573084 test loss: 0.626702
[Epoch 123; Iter    26/   32] train: loss: 0.2729800
[Epoch 123] ogbg-molsider: 0.602701 val loss: 0.609826
[Epoch 123] ogbg-molsider: 0.591883 test loss: 0.625352
[Epoch 124; Iter    24/   32] train: loss: 0.2730858
[Epoch 124] ogbg-molsider: 0.591077 val loss: 0.628570
[Epoch 124] ogbg-molsider: 0.584766 test loss: 0.614980
[Epoch 125; Iter    22/   32] train: loss: 0.3240432
[Epoch 125] ogbg-molsider: 0.587793 val loss: 0.619427
[Epoch 125] ogbg-molsider: 0.601623 test loss: 0.606669
[Epoch 126; Iter    20/   32] train: loss: 0.2962728
[Epoch 126] ogbg-molsider: 0.599450 val loss: 0.630221
[Epoch 126] ogbg-molsider: 0.579031 test loss: 0.659707
[Epoch 127; Iter    18/   32] train: loss: 0.2746964
[Epoch 127] ogbg-molsider: 0.587165 val loss: 0.620244
[Epoch 127] ogbg-molsider: 0.598762 test loss: 0.632517
[Epoch 128; Iter    16/   32] train: loss: 0.2823087
[Epoch 128] ogbg-molsider: 0.585042 val loss: 0.623673
[Epoch 128] ogbg-molsider: 0.571502 test loss: 0.624648
[Epoch 129; Iter    14/   32] train: loss: 0.2841028
[Epoch 129] ogbg-molsider: 0.579684 val loss: 0.647580
[Epoch 129] ogbg-molsider: 0.581230 test loss: 0.647252
[Epoch 130; Iter    12/   32] train: loss: 0.3062693
[Epoch 130] ogbg-molsider: 0.583169 val loss: 0.623812
[Epoch 130] ogbg-molsider: 0.576933 test loss: 0.634738
[Epoch 131; Iter    10/   32] train: loss: 0.2868877
[Epoch 131] ogbg-molsider: 0.591497 val loss: 0.617827
[Epoch 131] ogbg-molsider: 0.588194 test loss: 0.621231
[Epoch 132; Iter     8/   32] train: loss: 0.2796303
[Epoch 132] ogbg-molsider: 0.593093 val loss: 0.620231
[Epoch 132] ogbg-molsider: 0.583486 test loss: 0.643249
[Epoch 133; Iter     6/   32] train: loss: 0.2831438
[Epoch 133] ogbg-molsider: 0.590252 val loss: 0.633659
[Epoch 141] ogbg-molsider: 0.570856 val loss: 0.715959
[Epoch 141] ogbg-molsider: 0.582915 test loss: 0.680984
[Epoch 142; Iter     3/   27] train: loss: 0.2284474
[Epoch 142] ogbg-molsider: 0.564475 val loss: 0.717456
[Epoch 142] ogbg-molsider: 0.587575 test loss: 0.672940
[Epoch 143; Iter     6/   27] train: loss: 0.2344355
[Epoch 143] ogbg-molsider: 0.570358 val loss: 0.704357
[Epoch 143] ogbg-molsider: 0.581810 test loss: 0.671954
Early stopping criterion based on -ogbg-molsider- that should be max reached after 143 epochs. Best model checkpoint was in epoch 83.
Statistics on  val_best_checkpoint
mean_pred: 1.1043930053710938
std_pred: 2.9367685317993164
mean_targets: 0.5918128490447998
std_targets: 0.49153006076812744
prcauc: 0.6511319981005235
rocauc: 0.5944496843087742
ogbg-molsider: 0.5944496843087742
OGBNanLabelBCEWithLogitsLoss: 0.5909194217787849
Statistics on  test
mean_pred: 1.0596179962158203
std_pred: 2.842967987060547
mean_targets: 0.5811965465545654
std_targets: 0.49339503049850464
prcauc: 0.6395691650302753
rocauc: 0.5946190755532624
ogbg-molsider: 0.5946190755532624
OGBNanLabelBCEWithLogitsLoss: 0.5664266149202982
Statistics on  train
mean_pred: 0.7675830721855164
std_pred: 2.8794121742248535
mean_targets: 0.5549498200416565
std_targets: 0.4969821274280548
prcauc: 0.8057410142865491
rocauc: 0.8492647228627154
ogbg-molsider: 0.8492647228627154
OGBNanLabelBCEWithLogitsLoss: 0.3878304163614909
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 141] ogbg-molsider: 0.580669 val loss: 0.679988
[Epoch 141] ogbg-molsider: 0.594403 test loss: 0.650792
[Epoch 142; Iter     3/   27] train: loss: 0.2850035
[Epoch 142] ogbg-molsider: 0.578521 val loss: 0.696040
[Epoch 142] ogbg-molsider: 0.594780 test loss: 0.674375
[Epoch 143; Iter     6/   27] train: loss: 0.2595245
[Epoch 143] ogbg-molsider: 0.582082 val loss: 0.698012
[Epoch 143] ogbg-molsider: 0.591236 test loss: 0.691588
[Epoch 144; Iter     9/   27] train: loss: 0.2852467
[Epoch 144] ogbg-molsider: 0.575602 val loss: 0.689622
[Epoch 144] ogbg-molsider: 0.589661 test loss: 0.691285
[Epoch 145; Iter    12/   27] train: loss: 0.2167431
[Epoch 145] ogbg-molsider: 0.573162 val loss: 0.699995
[Epoch 145] ogbg-molsider: 0.599793 test loss: 0.659407
[Epoch 146; Iter    15/   27] train: loss: 0.2168084
[Epoch 146] ogbg-molsider: 0.580417 val loss: 0.691046
[Epoch 146] ogbg-molsider: 0.591054 test loss: 0.660715
Early stopping criterion based on -ogbg-molsider- that should be max reached after 146 epochs. Best model checkpoint was in epoch 86.
Statistics on  val_best_checkpoint
mean_pred: 0.8414016366004944
std_pred: 2.5411505699157715
mean_targets: 0.5918128490447998
std_targets: 0.49153006076812744
prcauc: 0.6401658484582633
rocauc: 0.5970113764984535
ogbg-molsider: 0.5970113764984535
OGBNanLabelBCEWithLogitsLoss: 0.5697768827279409
Statistics on  test
mean_pred: 0.9597082734107971
std_pred: 2.643293857574463
mean_targets: 0.5811965465545654
std_targets: 0.49339503049850464
prcauc: 0.6377651217444681
rocauc: 0.6062285418697239
ogbg-molsider: 0.6062285418697239
OGBNanLabelBCEWithLogitsLoss: 0.5578858521249559
Statistics on  train
mean_pred: 0.5396376252174377
std_pred: 2.7506701946258545
mean_targets: 0.5549498200416565
std_targets: 0.4969820976257324
prcauc: 0.8222169153659652
rocauc: 0.8754251109869202
ogbg-molsider: 0.8754251109869202
OGBNanLabelBCEWithLogitsLoss: 0.3436400868274547
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
All runs completed.
[Epoch 133] ogbg-molsider: 0.590899 test loss: 0.622883
[Epoch 134; Iter     4/   32] train: loss: 0.2456209
[Epoch 134] ogbg-molsider: 0.593090 val loss: 0.624380
[Epoch 134] ogbg-molsider: 0.592118 test loss: 0.636535
Early stopping criterion based on -ogbg-molsider- that should be max reached after 134 epochs. Best model checkpoint was in epoch 74.
Statistics on  val_best_checkpoint
mean_pred: 0.7643818259239197
std_pred: 2.1977460384368896
mean_targets: 0.6057459115982056
std_targets: 0.48873215913772583
prcauc: 0.6632177184714689
rocauc: 0.6130287556865627
ogbg-molsider: 0.6130287556865627
OGBNanLabelBCEWithLogitsLoss: 0.49107771686145235
Statistics on  test
mean_pred: 0.7438027262687683
std_pred: 2.3549129962921143
mean_targets: 0.5677863955497742
std_targets: 0.49542638659477234
prcauc: 0.6258539626613759
rocauc: 0.6005886110129167
ogbg-molsider: 0.6005886110129167
OGBNanLabelBCEWithLogitsLoss: 0.5217647509915488
Statistics on  train
mean_pred: 0.3763641119003296
std_pred: 3.542426824569702
mean_targets: 0.5593408942222595
std_targets: 0.49647536873817444
prcauc: 0.7363416342665869
rocauc: 0.77889737391779
ogbg-molsider: 0.77889737391779
OGBNanLabelBCEWithLogitsLoss: 0.4993095900863409
[Epoch 84] ogbg-molsider: 0.596093 test loss: 0.558111
[Epoch 85; Iter    12/   32] train: loss: 0.3307068
[Epoch 85] ogbg-molsider: 0.586517 val loss: 0.554003
[Epoch 85] ogbg-molsider: 0.592797 test loss: 0.547252
[Epoch 86; Iter    10/   32] train: loss: 0.3902180
[Epoch 86] ogbg-molsider: 0.596086 val loss: 0.569577
[Epoch 86] ogbg-molsider: 0.583567 test loss: 0.590862
[Epoch 87; Iter     8/   32] train: loss: 0.3618435
[Epoch 87] ogbg-molsider: 0.587489 val loss: 0.554604
[Epoch 87] ogbg-molsider: 0.578781 test loss: 0.578745
[Epoch 88; Iter     6/   32] train: loss: 0.3150082
[Epoch 88] ogbg-molsider: 0.603048 val loss: 0.572116
[Epoch 88] ogbg-molsider: 0.585998 test loss: 0.578835
[Epoch 89; Iter     4/   32] train: loss: 0.3105612
[Epoch 89] ogbg-molsider: 0.590264 val loss: 0.571984
[Epoch 89] ogbg-molsider: 0.576347 test loss: 0.583865
[Epoch 90; Iter     2/   32] train: loss: 0.3119968
[Epoch 90; Iter    32/   32] train: loss: 0.3430060
[Epoch 90] ogbg-molsider: 0.587270 val loss: 0.593885
[Epoch 90] ogbg-molsider: 0.597683 test loss: 0.572356
[Epoch 91; Iter    30/   32] train: loss: 0.3039809
[Epoch 91] ogbg-molsider: 0.563236 val loss: 0.594764
[Epoch 91] ogbg-molsider: 0.569234 test loss: 0.583196
[Epoch 92; Iter    28/   32] train: loss: 0.3175764
[Epoch 92] ogbg-molsider: 0.588583 val loss: 0.586952
[Epoch 92] ogbg-molsider: 0.579877 test loss: 0.608582
[Epoch 93; Iter    26/   32] train: loss: 0.3738979
[Epoch 93] ogbg-molsider: 0.567883 val loss: 0.579683
[Epoch 93] ogbg-molsider: 0.583941 test loss: 0.571046
[Epoch 94; Iter    24/   32] train: loss: 0.3320713
[Epoch 94] ogbg-molsider: 0.582836 val loss: 0.579550
[Epoch 94] ogbg-molsider: 0.587387 test loss: 1.212696
[Epoch 95; Iter    22/   32] train: loss: 0.3254952
[Epoch 95] ogbg-molsider: 0.586193 val loss: 0.590475
[Epoch 95] ogbg-molsider: 0.586639 test loss: 0.581132
[Epoch 96; Iter    20/   32] train: loss: 0.3071318
[Epoch 96] ogbg-molsider: 0.595049 val loss: 0.587867
[Epoch 96] ogbg-molsider: 0.602563 test loss: 0.579984
[Epoch 97; Iter    18/   32] train: loss: 0.3593471
[Epoch 97] ogbg-molsider: 0.582993 val loss: 0.590258
[Epoch 97] ogbg-molsider: 0.579915 test loss: 0.601090
[Epoch 98; Iter    16/   32] train: loss: 0.2972681
[Epoch 98] ogbg-molsider: 0.574436 val loss: 0.578717
[Epoch 98] ogbg-molsider: 0.558715 test loss: 0.972109
[Epoch 99; Iter    14/   32] train: loss: 0.3374559
[Epoch 99] ogbg-molsider: 0.594802 val loss: 0.603380
[Epoch 99] ogbg-molsider: 0.584424 test loss: 0.600157
[Epoch 100; Iter    12/   32] train: loss: 0.3080543
[Epoch 100] ogbg-molsider: 0.587341 val loss: 0.588377
[Epoch 100] ogbg-molsider: 0.585802 test loss: 0.579411
[Epoch 101; Iter    10/   32] train: loss: 0.3187923
[Epoch 101] ogbg-molsider: 0.610943 val loss: 0.579753
[Epoch 101] ogbg-molsider: 0.597689 test loss: 0.580364
[Epoch 102; Iter     8/   32] train: loss: 0.3257862
[Epoch 102] ogbg-molsider: 0.586029 val loss: 0.599498
[Epoch 102] ogbg-molsider: 0.595288 test loss: 0.603781
[Epoch 103; Iter     6/   32] train: loss: 0.2910725
[Epoch 103] ogbg-molsider: 0.586318 val loss: 0.603936
[Epoch 103] ogbg-molsider: 0.598638 test loss: 0.592340
[Epoch 104; Iter     4/   32] train: loss: 0.2974945
[Epoch 104] ogbg-molsider: 0.583772 val loss: 0.590593
[Epoch 104] ogbg-molsider: 0.592065 test loss: 0.583656
[Epoch 105; Iter     2/   32] train: loss: 0.3170992
[Epoch 105; Iter    32/   32] train: loss: 0.3297991
[Epoch 105] ogbg-molsider: 0.590154 val loss: 0.610106
[Epoch 105] ogbg-molsider: 0.583771 test loss: 0.606111
[Epoch 106; Iter    30/   32] train: loss: 0.3218893
[Epoch 106] ogbg-molsider: 0.592675 val loss: 0.594460
[Epoch 106] ogbg-molsider: 0.598884 test loss: 0.601311
[Epoch 107; Iter    28/   32] train: loss: 0.3408978
[Epoch 107] ogbg-molsider: 0.596287 val loss: 0.590515
[Epoch 107] ogbg-molsider: 0.600258 test loss: 0.578880
[Epoch 108; Iter    26/   32] train: loss: 0.3192570
[Epoch 108] ogbg-molsider: 0.595058 val loss: 0.588827
[Epoch 108] ogbg-molsider: 0.604103 test loss: 0.587691
[Epoch 109; Iter    24/   32] train: loss: 0.2962864
[Epoch 109] ogbg-molsider: 0.592368 val loss: 0.588481
[Epoch 109] ogbg-molsider: 0.607254 test loss: 0.570564
[Epoch 110; Iter    22/   32] train: loss: 0.2946709
[Epoch 110] ogbg-molsider: 0.589961 val loss: 0.597478
[Epoch 110] ogbg-molsider: 0.592910 test loss: 0.583566
[Epoch 111; Iter    20/   32] train: loss: 0.2755812
[Epoch 111] ogbg-molsider: 0.593244 val loss: 0.611878
[Epoch 111] ogbg-molsider: 0.593594 test loss: 0.661311
[Epoch 112; Iter    18/   32] train: loss: 0.3113752
[Epoch 112] ogbg-molsider: 0.594476 val loss: 0.609372
[Epoch 112] ogbg-molsider: 0.600281 test loss: 0.605782
[Epoch 113; Iter    16/   32] train: loss: 0.3332127
[Epoch 113] ogbg-molsider: 0.602837 val loss: 0.608479
[Epoch 113] ogbg-molsider: 0.602391 test loss: 0.602905
[Epoch 114; Iter    14/   32] train: loss: 0.2897072
[Epoch 114] ogbg-molsider: 0.601463 val loss: 0.594126
[Epoch 114] ogbg-molsider: 0.605371 test loss: 0.591464
[Epoch 115; Iter    12/   32] train: loss: 0.2609978
[Epoch 115] ogbg-molsider: 0.601310 val loss: 0.613654
[Epoch 115] ogbg-molsider: 0.586563 test loss: 0.605941
[Epoch 116; Iter    10/   32] train: loss: 0.3023271
[Epoch 116] ogbg-molsider: 0.604594 val loss: 0.611319
[Epoch 116] ogbg-molsider: 0.598150 test loss: 0.610380
[Epoch 117; Iter     8/   32] train: loss: 0.2784667
[Epoch 117] ogbg-molsider: 0.603963 val loss: 0.612895
[Epoch 117] ogbg-molsider: 0.589434 test loss: 0.644441
[Epoch 118; Iter     6/   32] train: loss: 0.3016880
[Epoch 118] ogbg-molsider: 0.607030 val loss: 0.603356
[Epoch 118] ogbg-molsider: 0.593082 test loss: 0.610590
[Epoch 119; Iter     4/   32] train: loss: 0.2615616
[Epoch 119] ogbg-molsider: 0.600572 val loss: 0.640667
[Epoch 119] ogbg-molsider: 0.599946 test loss: 0.609863
[Epoch 120; Iter     2/   32] train: loss: 0.2701877
[Epoch 120; Iter    32/   32] train: loss: 0.3605580
[Epoch 120] ogbg-molsider: 0.602145 val loss: 0.636125
[Epoch 120] ogbg-molsider: 0.579268 test loss: 0.935844
[Epoch 121; Iter    30/   32] train: loss: 0.2525846
[Epoch 121] ogbg-molsider: 0.591779 val loss: 0.632522
[Epoch 121] ogbg-molsider: 0.592252 test loss: 0.761919
[Epoch 122; Iter    28/   32] train: loss: 0.2845465
[Epoch 122] ogbg-molsider: 0.601364 val loss: 0.616921
[Epoch 122] ogbg-molsider: 0.607268 test loss: 0.601892
[Epoch 123; Iter    26/   32] train: loss: 0.2585524
[Epoch 123] ogbg-molsider: 0.599346 val loss: 0.616915
[Epoch 123] ogbg-molsider: 0.607415 test loss: 0.601494
[Epoch 124; Iter    24/   32] train: loss: 0.2759996
[Epoch 124] ogbg-molsider: 0.611782 val loss: 0.611846
[Epoch 124] ogbg-molsider: 0.599651 test loss: 0.615063
[Epoch 125; Iter    22/   32] train: loss: 0.2803450
[Epoch 125] ogbg-molsider: 0.601513 val loss: 0.625422
[Epoch 125] ogbg-molsider: 0.595346 test loss: 0.611919
[Epoch 126; Iter    20/   32] train: loss: 0.2444558
[Epoch 126] ogbg-molsider: 0.594808 val loss: 0.653660
[Epoch 126] ogbg-molsider: 0.613332 test loss: 0.620725
[Epoch 127; Iter    18/   32] train: loss: 0.2786477
[Epoch 127] ogbg-molsider: 0.596829 val loss: 0.635236
[Epoch 127] ogbg-molsider: 0.596728 test loss: 0.613952
[Epoch 128; Iter    16/   32] train: loss: 0.2462266
[Epoch 128] ogbg-molsider: 0.607922 val loss: 0.640285
[Epoch 128] ogbg-molsider: 0.598891 test loss: 0.636163
[Epoch 129; Iter    14/   32] train: loss: 0.2364297
[Epoch 129] ogbg-molsider: 0.602205 val loss: 0.618061
[Epoch 129] ogbg-molsider: 0.604790 test loss: 0.605869
[Epoch 130; Iter    12/   32] train: loss: 0.2594585
[Epoch 130] ogbg-molsider: 0.595001 val loss: 0.644675
[Epoch 130] ogbg-molsider: 0.592791 test loss: 0.637365
[Epoch 131; Iter    10/   32] train: loss: 0.2493190
[Epoch 131] ogbg-molsider: 0.602275 val loss: 0.631763
[Epoch 131] ogbg-molsider: 0.614371 test loss: 0.618617
[Epoch 132; Iter     8/   32] train: loss: 0.2579802
[Epoch 132] ogbg-molsider: 0.590809 val loss: 0.646711
[Epoch 132] ogbg-molsider: 0.599795 test loss: 0.611457
[Epoch 133; Iter     6/   32] train: loss: 0.2700466
[Epoch 133] ogbg-molsider: 0.603596 val loss: 0.644564
[Epoch 84] ogbg-molsider: 0.600583 test loss: 0.528054
[Epoch 85; Iter    12/   32] train: loss: 0.4597908
[Epoch 85] ogbg-molsider: 0.608232 val loss: 0.495124
[Epoch 85] ogbg-molsider: 0.589272 test loss: 0.551869
[Epoch 86; Iter    10/   32] train: loss: 0.4143570
[Epoch 86] ogbg-molsider: 0.591138 val loss: 0.503083
[Epoch 86] ogbg-molsider: 0.608402 test loss: 0.521941
[Epoch 87; Iter     8/   32] train: loss: 0.3835175
[Epoch 87] ogbg-molsider: 0.613388 val loss: 0.499472
[Epoch 87] ogbg-molsider: 0.609662 test loss: 0.529710
[Epoch 88; Iter     6/   32] train: loss: 0.3672088
[Epoch 88] ogbg-molsider: 0.603052 val loss: 0.500916
[Epoch 88] ogbg-molsider: 0.609417 test loss: 0.517132
[Epoch 89; Iter     4/   32] train: loss: 0.4130577
[Epoch 89] ogbg-molsider: 0.597545 val loss: 0.506187
[Epoch 89] ogbg-molsider: 0.585165 test loss: 0.562153
[Epoch 90; Iter     2/   32] train: loss: 0.3975506
[Epoch 90; Iter    32/   32] train: loss: 0.4225922
[Epoch 90] ogbg-molsider: 0.612115 val loss: 0.508962
[Epoch 90] ogbg-molsider: 0.601462 test loss: 0.531897
[Epoch 91; Iter    30/   32] train: loss: 0.4200424
[Epoch 91] ogbg-molsider: 0.592554 val loss: 0.517983
[Epoch 91] ogbg-molsider: 0.595178 test loss: 0.537440
[Epoch 92; Iter    28/   32] train: loss: 0.4240122
[Epoch 92] ogbg-molsider: 0.606289 val loss: 0.518210
[Epoch 92] ogbg-molsider: 0.595723 test loss: 0.578308
[Epoch 93; Iter    26/   32] train: loss: 0.4470827
[Epoch 93] ogbg-molsider: 0.602006 val loss: 0.517896
[Epoch 93] ogbg-molsider: 0.599490 test loss: 0.556804
[Epoch 94; Iter    24/   32] train: loss: 0.4200419
[Epoch 94] ogbg-molsider: 0.620119 val loss: 0.504850
[Epoch 94] ogbg-molsider: 0.593117 test loss: 0.551192
[Epoch 95; Iter    22/   32] train: loss: 0.3592023
[Epoch 95] ogbg-molsider: 0.610779 val loss: 0.534146
[Epoch 95] ogbg-molsider: 0.621463 test loss: 0.571864
[Epoch 96; Iter    20/   32] train: loss: 0.4233831
[Epoch 96] ogbg-molsider: 0.617554 val loss: 0.518999
[Epoch 96] ogbg-molsider: 0.610269 test loss: 0.561888
[Epoch 97; Iter    18/   32] train: loss: 0.3855596
[Epoch 97] ogbg-molsider: 0.587839 val loss: 0.533093
[Epoch 97] ogbg-molsider: 0.594582 test loss: 0.579042
[Epoch 98; Iter    16/   32] train: loss: 0.3766599
[Epoch 98] ogbg-molsider: 0.591537 val loss: 0.534687
[Epoch 98] ogbg-molsider: 0.616771 test loss: 0.535005
[Epoch 99; Iter    14/   32] train: loss: 0.4147958
[Epoch 99] ogbg-molsider: 0.599997 val loss: 0.518663
[Epoch 99] ogbg-molsider: 0.614405 test loss: 0.547493
[Epoch 100; Iter    12/   32] train: loss: 0.3956628
[Epoch 100] ogbg-molsider: 0.601033 val loss: 0.515833
[Epoch 100] ogbg-molsider: 0.624190 test loss: 0.541188
[Epoch 101; Iter    10/   32] train: loss: 0.3756696
[Epoch 101] ogbg-molsider: 0.606043 val loss: 0.516244
[Epoch 101] ogbg-molsider: 0.600550 test loss: 0.562635
[Epoch 102; Iter     8/   32] train: loss: 0.4044175
[Epoch 102] ogbg-molsider: 0.596305 val loss: 0.528719
[Epoch 102] ogbg-molsider: 0.602436 test loss: 0.569271
[Epoch 103; Iter     6/   32] train: loss: 0.3716508
[Epoch 103] ogbg-molsider: 0.597262 val loss: 0.536966
[Epoch 103] ogbg-molsider: 0.604603 test loss: 0.581645
[Epoch 104; Iter     4/   32] train: loss: 0.3923631
[Epoch 104] ogbg-molsider: 0.593883 val loss: 0.543113
[Epoch 104] ogbg-molsider: 0.603926 test loss: 0.564512
[Epoch 105; Iter     2/   32] train: loss: 0.3318938
[Epoch 105; Iter    32/   32] train: loss: 0.3526478
[Epoch 105] ogbg-molsider: 0.602708 val loss: 0.554857
[Epoch 105] ogbg-molsider: 0.630530 test loss: 0.593388
[Epoch 106; Iter    30/   32] train: loss: 0.3902841
[Epoch 106] ogbg-molsider: 0.612958 val loss: 0.523332
[Epoch 106] ogbg-molsider: 0.610582 test loss: 0.557833
[Epoch 107; Iter    28/   32] train: loss: 0.3957212
[Epoch 107] ogbg-molsider: 0.587565 val loss: 0.563339
[Epoch 107] ogbg-molsider: 0.608593 test loss: 0.580187
[Epoch 108; Iter    26/   32] train: loss: 0.3697355
[Epoch 108] ogbg-molsider: 0.584970 val loss: 0.576644
[Epoch 108] ogbg-molsider: 0.602462 test loss: 0.613104
[Epoch 109; Iter    24/   32] train: loss: 0.3388715
[Epoch 109] ogbg-molsider: 0.609471 val loss: 0.525406
[Epoch 109] ogbg-molsider: 0.592522 test loss: 0.567650
[Epoch 110; Iter    22/   32] train: loss: 0.3620514
[Epoch 110] ogbg-molsider: 0.602324 val loss: 0.547682
[Epoch 110] ogbg-molsider: 0.613811 test loss: 0.584789
[Epoch 111; Iter    20/   32] train: loss: 0.3565447
[Epoch 111] ogbg-molsider: 0.607754 val loss: 0.539570
[Epoch 111] ogbg-molsider: 0.620167 test loss: 0.569380
[Epoch 112; Iter    18/   32] train: loss: 0.3681090
[Epoch 112] ogbg-molsider: 0.595423 val loss: 0.549291
[Epoch 112] ogbg-molsider: 0.608440 test loss: 0.579310
[Epoch 113; Iter    16/   32] train: loss: 0.3197692
[Epoch 113] ogbg-molsider: 0.583848 val loss: 0.562121
[Epoch 113] ogbg-molsider: 0.610846 test loss: 0.583062
[Epoch 114; Iter    14/   32] train: loss: 0.3837161
[Epoch 114] ogbg-molsider: 0.588634 val loss: 0.562188
[Epoch 114] ogbg-molsider: 0.610712 test loss: 0.583776
[Epoch 115; Iter    12/   32] train: loss: 0.3047629
[Epoch 115] ogbg-molsider: 0.589916 val loss: 0.572980
[Epoch 115] ogbg-molsider: 0.614000 test loss: 0.589588
[Epoch 116; Iter    10/   32] train: loss: 0.3046451
[Epoch 116] ogbg-molsider: 0.597885 val loss: 0.557679
[Epoch 116] ogbg-molsider: 0.609104 test loss: 0.588078
[Epoch 117; Iter     8/   32] train: loss: 0.3398507
[Epoch 117] ogbg-molsider: 0.591322 val loss: 0.562464
[Epoch 117] ogbg-molsider: 0.601246 test loss: 0.600771
[Epoch 118; Iter     6/   32] train: loss: 0.3008979
[Epoch 118] ogbg-molsider: 0.587441 val loss: 0.563980
[Epoch 118] ogbg-molsider: 0.610330 test loss: 0.585174
[Epoch 119; Iter     4/   32] train: loss: 0.3497466
[Epoch 119] ogbg-molsider: 0.594891 val loss: 0.568166
[Epoch 119] ogbg-molsider: 0.615798 test loss: 0.579923
[Epoch 120; Iter     2/   32] train: loss: 0.3308302
[Epoch 120; Iter    32/   32] train: loss: 0.3450013
[Epoch 120] ogbg-molsider: 0.576202 val loss: 0.581215
[Epoch 120] ogbg-molsider: 0.602473 test loss: 0.599019
[Epoch 121; Iter    30/   32] train: loss: 0.3204378
[Epoch 121] ogbg-molsider: 0.593095 val loss: 0.582023
[Epoch 121] ogbg-molsider: 0.610347 test loss: 0.617695
[Epoch 122; Iter    28/   32] train: loss: 0.3302605
[Epoch 122] ogbg-molsider: 0.584441 val loss: 0.564558
[Epoch 122] ogbg-molsider: 0.593149 test loss: 0.603604
[Epoch 123; Iter    26/   32] train: loss: 0.3047732
[Epoch 123] ogbg-molsider: 0.592245 val loss: 0.558587
[Epoch 123] ogbg-molsider: 0.613084 test loss: 0.585946
[Epoch 124; Iter    24/   32] train: loss: 0.3718933
[Epoch 124] ogbg-molsider: 0.582174 val loss: 0.582229
[Epoch 124] ogbg-molsider: 0.598987 test loss: 0.620045
[Epoch 125; Iter    22/   32] train: loss: 0.3278882
[Epoch 125] ogbg-molsider: 0.591052 val loss: 0.579125
[Epoch 125] ogbg-molsider: 0.595756 test loss: 0.621371
[Epoch 126; Iter    20/   32] train: loss: 0.3437277
[Epoch 126] ogbg-molsider: 0.593869 val loss: 0.574840
[Epoch 126] ogbg-molsider: 0.603413 test loss: 0.620553
[Epoch 127; Iter    18/   32] train: loss: 0.3451113
[Epoch 127] ogbg-molsider: 0.585026 val loss: 0.586510
[Epoch 127] ogbg-molsider: 0.606455 test loss: 0.595102
[Epoch 128; Iter    16/   32] train: loss: 0.2904544
[Epoch 128] ogbg-molsider: 0.591810 val loss: 0.570160
[Epoch 128] ogbg-molsider: 0.600120 test loss: 0.604389
[Epoch 129; Iter    14/   32] train: loss: 0.3055561
[Epoch 129] ogbg-molsider: 0.586580 val loss: 0.580077
[Epoch 129] ogbg-molsider: 0.607912 test loss: 0.595162
[Epoch 130; Iter    12/   32] train: loss: 0.2867841
[Epoch 130] ogbg-molsider: 0.587384 val loss: 0.595905
[Epoch 130] ogbg-molsider: 0.617381 test loss: 0.594296
[Epoch 131; Iter    10/   32] train: loss: 0.2891622
[Epoch 131] ogbg-molsider: 0.587632 val loss: 0.578322
[Epoch 131] ogbg-molsider: 0.606242 test loss: 0.597123
[Epoch 132; Iter     8/   32] train: loss: 0.3325324
[Epoch 132] ogbg-molsider: 0.589988 val loss: 0.586715
[Epoch 132] ogbg-molsider: 0.605279 test loss: 0.629579
[Epoch 133; Iter     6/   32] train: loss: 0.3487104
[Epoch 133] ogbg-molsider: 0.585017 val loss: 0.590321
[Epoch 81; Iter    30/   36] train: loss: 0.2716177
[Epoch 81] ogbg-molsider: 0.620075 val loss: 0.530386
[Epoch 81] ogbg-molsider: 0.583552 test loss: 0.592067
[Epoch 82; Iter    24/   36] train: loss: 0.2939909
[Epoch 82] ogbg-molsider: 0.622256 val loss: 0.726577
[Epoch 82] ogbg-molsider: 0.574803 test loss: 0.617569
[Epoch 83; Iter    18/   36] train: loss: 0.2709071
[Epoch 83] ogbg-molsider: 0.614509 val loss: 0.638909
[Epoch 83] ogbg-molsider: 0.569115 test loss: 0.611558
[Epoch 84; Iter    12/   36] train: loss: 0.2634694
[Epoch 84] ogbg-molsider: 0.628338 val loss: 0.544472
[Epoch 84] ogbg-molsider: 0.577932 test loss: 0.597926
[Epoch 85; Iter     6/   36] train: loss: 0.3059210
[Epoch 85; Iter    36/   36] train: loss: 0.2939510
[Epoch 85] ogbg-molsider: 0.622400 val loss: 0.729851
[Epoch 85] ogbg-molsider: 0.560760 test loss: 0.638948
[Epoch 86; Iter    30/   36] train: loss: 0.3211543
[Epoch 86] ogbg-molsider: 0.624070 val loss: 0.603546
[Epoch 86] ogbg-molsider: 0.573938 test loss: 0.627061
[Epoch 87; Iter    24/   36] train: loss: 0.2835831
[Epoch 87] ogbg-molsider: 0.630416 val loss: 0.561643
[Epoch 87] ogbg-molsider: 0.578805 test loss: 0.616422
[Epoch 88; Iter    18/   36] train: loss: 0.2799768
[Epoch 88] ogbg-molsider: 0.628760 val loss: 0.563827
[Epoch 88] ogbg-molsider: 0.577127 test loss: 0.615972
[Epoch 89; Iter    12/   36] train: loss: 0.2575139
[Epoch 89] ogbg-molsider: 0.624130 val loss: 0.588014
[Epoch 89] ogbg-molsider: 0.567319 test loss: 0.631035
[Epoch 90; Iter     6/   36] train: loss: 0.2631460
[Epoch 90; Iter    36/   36] train: loss: 0.3018519
[Epoch 90] ogbg-molsider: 0.632393 val loss: 0.569461
[Epoch 90] ogbg-molsider: 0.573378 test loss: 0.645871
[Epoch 91; Iter    30/   36] train: loss: 0.3043694
[Epoch 91] ogbg-molsider: 0.632471 val loss: 0.567335
[Epoch 91] ogbg-molsider: 0.570417 test loss: 0.665208
[Epoch 92; Iter    24/   36] train: loss: 0.2934285
[Epoch 92] ogbg-molsider: 0.644077 val loss: 0.558500
[Epoch 92] ogbg-molsider: 0.578093 test loss: 0.637141
[Epoch 93; Iter    18/   36] train: loss: 0.2937558
[Epoch 93] ogbg-molsider: 0.633290 val loss: 0.564781
[Epoch 93] ogbg-molsider: 0.564609 test loss: 0.651212
[Epoch 94; Iter    12/   36] train: loss: 0.2652455
[Epoch 94] ogbg-molsider: 0.625960 val loss: 0.573225
[Epoch 94] ogbg-molsider: 0.575387 test loss: 0.639004
[Epoch 95; Iter     6/   36] train: loss: 0.2586569
[Epoch 95; Iter    36/   36] train: loss: 0.2754122
[Epoch 95] ogbg-molsider: 0.631865 val loss: 0.681924
[Epoch 95] ogbg-molsider: 0.572969 test loss: 0.640454
[Epoch 96; Iter    30/   36] train: loss: 0.2714358
[Epoch 96] ogbg-molsider: 0.629462 val loss: 0.580944
[Epoch 96] ogbg-molsider: 0.571644 test loss: 0.663407
[Epoch 97; Iter    24/   36] train: loss: 0.2482283
[Epoch 97] ogbg-molsider: 0.627962 val loss: 0.574908
[Epoch 97] ogbg-molsider: 0.567450 test loss: 0.665478
[Epoch 98; Iter    18/   36] train: loss: 0.2817286
[Epoch 98] ogbg-molsider: 0.636405 val loss: 0.584804
[Epoch 98] ogbg-molsider: 0.566385 test loss: 0.658365
[Epoch 99; Iter    12/   36] train: loss: 0.2414606
[Epoch 99] ogbg-molsider: 0.638677 val loss: 0.593364
[Epoch 99] ogbg-molsider: 0.563333 test loss: 0.690164
[Epoch 100; Iter     6/   36] train: loss: 0.2358805
[Epoch 100; Iter    36/   36] train: loss: 0.2532208
[Epoch 100] ogbg-molsider: 0.641722 val loss: 0.806806
[Epoch 100] ogbg-molsider: 0.566700 test loss: 0.692675
[Epoch 101; Iter    30/   36] train: loss: 0.3013086
[Epoch 101] ogbg-molsider: 0.632747 val loss: 0.587969
[Epoch 101] ogbg-molsider: 0.567350 test loss: 0.669656
[Epoch 102; Iter    24/   36] train: loss: 0.2882648
[Epoch 102] ogbg-molsider: 0.639211 val loss: 0.581427
[Epoch 102] ogbg-molsider: 0.588326 test loss: 0.678332
[Epoch 103; Iter    18/   36] train: loss: 0.2725013
[Epoch 103] ogbg-molsider: 0.633074 val loss: 0.597342
[Epoch 103] ogbg-molsider: 0.572422 test loss: 0.697170
[Epoch 104; Iter    12/   36] train: loss: 0.2655987
[Epoch 104] ogbg-molsider: 0.620297 val loss: 0.628518
[Epoch 104] ogbg-molsider: 0.584283 test loss: 0.703679
[Epoch 105; Iter     6/   36] train: loss: 0.2969283
[Epoch 105; Iter    36/   36] train: loss: 0.2942354
[Epoch 105] ogbg-molsider: 0.619342 val loss: 0.617844
[Epoch 105] ogbg-molsider: 0.583999 test loss: 0.652219
[Epoch 106; Iter    30/   36] train: loss: 0.2556510
[Epoch 106] ogbg-molsider: 0.636897 val loss: 0.597920
[Epoch 106] ogbg-molsider: 0.572686 test loss: 0.676083
[Epoch 107; Iter    24/   36] train: loss: 0.2257577
[Epoch 107] ogbg-molsider: 0.633612 val loss: 0.599108
[Epoch 107] ogbg-molsider: 0.583701 test loss: 0.667391
[Epoch 108; Iter    18/   36] train: loss: 0.2533564
[Epoch 108] ogbg-molsider: 0.634649 val loss: 0.601933
[Epoch 108] ogbg-molsider: 0.581732 test loss: 0.656008
[Epoch 109; Iter    12/   36] train: loss: 0.2397816
[Epoch 109] ogbg-molsider: 0.636307 val loss: 0.605034
[Epoch 109] ogbg-molsider: 0.583689 test loss: 0.660781
[Epoch 110; Iter     6/   36] train: loss: 0.2200697
[Epoch 110; Iter    36/   36] train: loss: 0.2634615
[Epoch 110] ogbg-molsider: 0.638562 val loss: 0.605272
[Epoch 110] ogbg-molsider: 0.579865 test loss: 0.686760
[Epoch 111; Iter    30/   36] train: loss: 0.2641587
[Epoch 111] ogbg-molsider: 0.629635 val loss: 0.608740
[Epoch 111] ogbg-molsider: 0.587932 test loss: 0.659577
[Epoch 112; Iter    24/   36] train: loss: 0.2396593
[Epoch 112] ogbg-molsider: 0.632132 val loss: 0.615455
[Epoch 112] ogbg-molsider: 0.575933 test loss: 0.680101
[Epoch 113; Iter    18/   36] train: loss: 0.2368393
[Epoch 113] ogbg-molsider: 0.628563 val loss: 0.629717
[Epoch 113] ogbg-molsider: 0.573904 test loss: 0.702738
[Epoch 114; Iter    12/   36] train: loss: 0.2312958
[Epoch 114] ogbg-molsider: 0.631770 val loss: 0.634930
[Epoch 114] ogbg-molsider: 0.579111 test loss: 0.691315
[Epoch 115; Iter     6/   36] train: loss: 0.2089627
[Epoch 115; Iter    36/   36] train: loss: 0.2719617
[Epoch 115] ogbg-molsider: 0.634881 val loss: 0.628025
[Epoch 115] ogbg-molsider: 0.588539 test loss: 0.702196
[Epoch 116; Iter    30/   36] train: loss: 0.2244437
[Epoch 116] ogbg-molsider: 0.639141 val loss: 0.617846
[Epoch 116] ogbg-molsider: 0.580890 test loss: 0.707259
[Epoch 117; Iter    24/   36] train: loss: 0.2551567
[Epoch 117] ogbg-molsider: 0.637673 val loss: 0.629870
[Epoch 117] ogbg-molsider: 0.580860 test loss: 0.708665
[Epoch 118; Iter    18/   36] train: loss: 0.2262005
[Epoch 118] ogbg-molsider: 0.633390 val loss: 0.641908
[Epoch 118] ogbg-molsider: 0.579831 test loss: 0.720504
[Epoch 119; Iter    12/   36] train: loss: 0.2206234
[Epoch 119] ogbg-molsider: 0.634012 val loss: 0.637065
[Epoch 119] ogbg-molsider: 0.589282 test loss: 0.723561
[Epoch 120; Iter     6/   36] train: loss: 0.2029085
[Epoch 120; Iter    36/   36] train: loss: 0.2439613
[Epoch 120] ogbg-molsider: 0.616702 val loss: 0.652598
[Epoch 120] ogbg-molsider: 0.577493 test loss: 0.739220
[Epoch 121; Iter    30/   36] train: loss: 0.2031865
[Epoch 121] ogbg-molsider: 0.629804 val loss: 0.625518
[Epoch 121] ogbg-molsider: 0.579072 test loss: 0.724445
[Epoch 122; Iter    24/   36] train: loss: 0.2203375
[Epoch 122] ogbg-molsider: 0.623335 val loss: 0.656580
[Epoch 122] ogbg-molsider: 0.589535 test loss: 0.717524
[Epoch 123; Iter    18/   36] train: loss: 0.1883836
[Epoch 123] ogbg-molsider: 0.628052 val loss: 0.644941
[Epoch 123] ogbg-molsider: 0.586825 test loss: 0.720074
[Epoch 124; Iter    12/   36] train: loss: 0.2002730
[Epoch 124] ogbg-molsider: 0.634960 val loss: 0.623026
[Epoch 124] ogbg-molsider: 0.579478 test loss: 0.719681
[Epoch 125; Iter     6/   36] train: loss: 0.1956430
[Epoch 125; Iter    36/   36] train: loss: 0.2270494
[Epoch 125] ogbg-molsider: 0.629197 val loss: 0.647577
[Epoch 125] ogbg-molsider: 0.581965 test loss: 0.734564
[Epoch 126; Iter    30/   36] train: loss: 0.2183723
[Epoch 126] ogbg-molsider: 0.609159 val loss: 0.674266
[Epoch 126] ogbg-molsider: 0.588154 test loss: 0.739917
[Epoch 127; Iter    24/   36] train: loss: 0.2238523
[Epoch 127] ogbg-molsider: 0.622141 val loss: 0.672004
[Epoch 127] ogbg-molsider: 0.586961 test loss: 0.771741
[Epoch 128; Iter    18/   36] train: loss: 0.2023691
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 133] ogbg-molsider: 0.606283 test loss: 0.626725
[Epoch 134; Iter     4/   32] train: loss: 0.2465520
[Epoch 134] ogbg-molsider: 0.606625 val loss: 0.633895
[Epoch 134] ogbg-molsider: 0.606633 test loss: 0.615020
[Epoch 135; Iter     2/   32] train: loss: 0.2495537
[Epoch 135; Iter    32/   32] train: loss: 0.2468445
[Epoch 135] ogbg-molsider: 0.604480 val loss: 0.634022
[Epoch 135] ogbg-molsider: 0.604184 test loss: 0.612273
Early stopping criterion based on -ogbg-molsider- that should be max reached after 135 epochs. Best model checkpoint was in epoch 75.
Statistics on  val_best_checkpoint
mean_pred: 0.7431557774543762
std_pred: 2.4344770908355713
mean_targets: 0.6057459115982056
std_targets: 0.48873215913772583
prcauc: 0.670190872034862
rocauc: 0.614778926377369
ogbg-molsider: 0.614778926377369
OGBNanLabelBCEWithLogitsLoss: 0.5194552966526577
Statistics on  test
mean_pred: 0.6801228523254395
std_pred: 2.4296531677246094
mean_targets: 0.5677863955497742
std_targets: 0.49542638659477234
prcauc: 0.6223862299428513
rocauc: 0.6027620885295691
ogbg-molsider: 0.6027620885295691
OGBNanLabelBCEWithLogitsLoss: 0.540974497795105
Statistics on  train
mean_pred: 0.43205851316452026
std_pred: 2.457831382751465
mean_targets: 0.5593408942222595
std_targets: 0.4964753985404968
prcauc: 0.7980704516470423
rocauc: 0.8430729563356792
ogbg-molsider: 0.8430729563356792
OGBNanLabelBCEWithLogitsLoss: 0.3766526635736227
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.621173 val loss: 0.682424
[Epoch 128] ogbg-molsider: 0.586332 test loss: 0.736359
[Epoch 129; Iter    12/   36] train: loss: 0.1997805
[Epoch 129] ogbg-molsider: 0.622595 val loss: 0.665385
[Epoch 129] ogbg-molsider: 0.594644 test loss: 0.725203
[Epoch 130; Iter     6/   36] train: loss: 0.2121860
[Epoch 130; Iter    36/   36] train: loss: 0.2231615
[Epoch 130] ogbg-molsider: 0.625944 val loss: 0.676466
[Epoch 130] ogbg-molsider: 0.597901 test loss: 0.716921
[Epoch 131; Iter    30/   36] train: loss: 0.2376297
[Epoch 131] ogbg-molsider: 0.618149 val loss: 0.684511
[Epoch 131] ogbg-molsider: 0.600837 test loss: 0.716586
[Epoch 132; Iter    24/   36] train: loss: 0.1956067
[Epoch 132] ogbg-molsider: 0.602921 val loss: 0.694624
[Epoch 132] ogbg-molsider: 0.600443 test loss: 0.700770
[Epoch 133; Iter    18/   36] train: loss: 0.2152184
[Epoch 133] ogbg-molsider: 0.611488 val loss: 0.685043
[Epoch 133] ogbg-molsider: 0.589415 test loss: 0.709071
[Epoch 134; Iter    12/   36] train: loss: 0.1962775
[Epoch 134] ogbg-molsider: 0.616802 val loss: 0.695299
[Epoch 134] ogbg-molsider: 0.599286 test loss: 0.725232
[Epoch 135; Iter     6/   36] train: loss: 0.2234634
[Epoch 135; Iter    36/   36] train: loss: 0.2803303
[Epoch 135] ogbg-molsider: 0.622447 val loss: 0.679224
[Epoch 135] ogbg-molsider: 0.591468 test loss: 0.728606
[Epoch 136; Iter    30/   36] train: loss: 0.2116502
[Epoch 136] ogbg-molsider: 0.616696 val loss: 0.708129
[Epoch 136] ogbg-molsider: 0.594949 test loss: 0.743870
[Epoch 137; Iter    24/   36] train: loss: 0.1873827
[Epoch 137] ogbg-molsider: 0.613127 val loss: 0.677077
[Epoch 137] ogbg-molsider: 0.575833 test loss: 0.733488
[Epoch 138; Iter    18/   36] train: loss: 0.2248068
[Epoch 138] ogbg-molsider: 0.617316 val loss: 0.693814
[Epoch 138] ogbg-molsider: 0.587685 test loss: 0.726765
[Epoch 139; Iter    12/   36] train: loss: 0.1726887
[Epoch 139] ogbg-molsider: 0.610746 val loss: 0.692478
[Epoch 139] ogbg-molsider: 0.593205 test loss: 0.723735
[Epoch 140; Iter     6/   36] train: loss: 0.1746019
[Epoch 140; Iter    36/   36] train: loss: 0.1923661
[Epoch 140] ogbg-molsider: 0.611798 val loss: 0.689022
[Epoch 140] ogbg-molsider: 0.593736 test loss: 0.713369
Early stopping criterion based on -ogbg-molsider- that should be max reached after 140 epochs. Best model checkpoint was in epoch 80.
Statistics on  val_best_checkpoint
mean_pred: 0.46845299005508423
std_pred: 2.8729910850524902
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6871251319359212
rocauc: 0.6498306898873053
ogbg-molsider: 0.6498306898873053
OGBNanLabelBCEWithLogitsLoss: 0.5233879089355469
Statistics on  test
mean_pred: 0.5961921811103821
std_pred: 2.8924834728240967
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6216035150197052
rocauc: 0.5839138472233248
ogbg-molsider: 0.5839138472233248
OGBNanLabelBCEWithLogitsLoss: 0.5905403971672059
Statistics on  train
mean_pred: 0.44458481669425964
std_pred: 3.019023895263672
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8455062162466475
rocauc: 0.8876089590116791
ogbg-molsider: 0.8876089590116791
OGBNanLabelBCEWithLogitsLoss: 0.3283966581026713
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 133] ogbg-molsider: 0.602120 test loss: 0.613960
[Epoch 134; Iter     4/   32] train: loss: 0.3205936
[Epoch 134] ogbg-molsider: 0.589059 val loss: 0.582117
[Epoch 134] ogbg-molsider: 0.605232 test loss: 0.602334
[Epoch 135; Iter     2/   32] train: loss: 0.3025123
[Epoch 135; Iter    32/   32] train: loss: 0.2859908
[Epoch 135] ogbg-molsider: 0.577473 val loss: 0.598237
[Epoch 135] ogbg-molsider: 0.599812 test loss: 0.618186
[Epoch 136; Iter    30/   32] train: loss: 0.2819197
[Epoch 136] ogbg-molsider: 0.583327 val loss: 0.587676
[Epoch 136] ogbg-molsider: 0.608033 test loss: 0.604975
[Epoch 137; Iter    28/   32] train: loss: 0.2544035
[Epoch 137] ogbg-molsider: 0.584197 val loss: 0.588528
[Epoch 137] ogbg-molsider: 0.613306 test loss: 0.598546
[Epoch 138; Iter    26/   32] train: loss: 0.3083830
[Epoch 138] ogbg-molsider: 0.586241 val loss: 0.585993
[Epoch 138] ogbg-molsider: 0.615805 test loss: 0.598758
[Epoch 139; Iter    24/   32] train: loss: 0.3126467
[Epoch 139] ogbg-molsider: 0.590830 val loss: 0.582894
[Epoch 139] ogbg-molsider: 0.608039 test loss: 0.600551
[Epoch 140; Iter    22/   32] train: loss: 0.2969855
[Epoch 140] ogbg-molsider: 0.586882 val loss: 0.593751
[Epoch 140] ogbg-molsider: 0.613571 test loss: 0.610928
[Epoch 141; Iter    20/   32] train: loss: 0.2963491
[Epoch 141] ogbg-molsider: 0.585574 val loss: 0.592679
[Epoch 141] ogbg-molsider: 0.613877 test loss: 0.603836
[Epoch 142; Iter    18/   32] train: loss: 0.2656915
[Epoch 142] ogbg-molsider: 0.589099 val loss: 0.589214
[Epoch 142] ogbg-molsider: 0.618489 test loss: 0.592870
[Epoch 143; Iter    16/   32] train: loss: 0.3496601
[Epoch 143] ogbg-molsider: 0.581371 val loss: 0.597956
[Epoch 143] ogbg-molsider: 0.614094 test loss: 0.614129
[Epoch 144; Iter    14/   32] train: loss: 0.3890808
[Epoch 144] ogbg-molsider: 0.581535 val loss: 0.600474
[Epoch 144] ogbg-molsider: 0.619122 test loss: 0.592225
[Epoch 145; Iter    12/   32] train: loss: 0.2763327
[Epoch 145] ogbg-molsider: 0.576156 val loss: 0.591807
[Epoch 145] ogbg-molsider: 0.616821 test loss: 0.599300
[Epoch 146; Iter    10/   32] train: loss: 0.2902367
[Epoch 146] ogbg-molsider: 0.584124 val loss: 0.605287
[Epoch 146] ogbg-molsider: 0.615453 test loss: 0.616023
[Epoch 147; Iter     8/   32] train: loss: 0.3456230
[Epoch 147] ogbg-molsider: 0.586101 val loss: 0.605091
[Epoch 147] ogbg-molsider: 0.615452 test loss: 0.614400
[Epoch 148; Iter     6/   32] train: loss: 0.3081063
[Epoch 148] ogbg-molsider: 0.584863 val loss: 0.600826
[Epoch 148] ogbg-molsider: 0.611407 test loss: 0.638722
[Epoch 149; Iter     4/   32] train: loss: 0.3063095
[Epoch 149] ogbg-molsider: 0.581455 val loss: 0.603388
[Epoch 149] ogbg-molsider: 0.614994 test loss: 0.623177
[Epoch 150; Iter     2/   32] train: loss: 0.2582917
[Epoch 150; Iter    32/   32] train: loss: 0.3483554
[Epoch 150] ogbg-molsider: 0.588540 val loss: 0.595636
[Epoch 150] ogbg-molsider: 0.610250 test loss: 0.605810
[Epoch 151; Iter    30/   32] train: loss: 0.2628036
[Epoch 151] ogbg-molsider: 0.585630 val loss: 0.591101
[Epoch 151] ogbg-molsider: 0.609848 test loss: 0.615777
[Epoch 152; Iter    28/   32] train: loss: 0.3190833
[Epoch 152] ogbg-molsider: 0.580215 val loss: 0.606544
[Epoch 152] ogbg-molsider: 0.612828 test loss: 0.613637
[Epoch 153; Iter    26/   32] train: loss: 0.3187619
[Epoch 153] ogbg-molsider: 0.581931 val loss: 0.601680
[Epoch 153] ogbg-molsider: 0.607502 test loss: 0.633069
[Epoch 154; Iter    24/   32] train: loss: 0.2887844
[Epoch 154] ogbg-molsider: 0.584835 val loss: 0.602148
[Epoch 154] ogbg-molsider: 0.616376 test loss: 0.629627
Early stopping criterion based on -ogbg-molsider- that should be max reached after 154 epochs. Best model checkpoint was in epoch 94.
Statistics on  val_best_checkpoint
mean_pred: 0.7109777927398682
std_pred: 2.3635635375976562
mean_targets: 0.6057459115982056
std_targets: 0.48873215913772583
prcauc: 0.6684562937304132
rocauc: 0.620118911398397
ogbg-molsider: 0.620118911398397
OGBNanLabelBCEWithLogitsLoss: 0.5048502215317318
Statistics on  test
mean_pred: 0.8170480132102966
std_pred: 2.457913637161255
mean_targets: 0.5677863955497742
std_targets: 0.49542638659477234
prcauc: 0.635588591069225
rocauc: 0.5931173859837552
ogbg-molsider: 0.5931173859837552
OGBNanLabelBCEWithLogitsLoss: 0.5511919472898755
Statistics on  train
mean_pred: 0.5239301323890686
std_pred: 2.364872455596924
mean_targets: 0.5593408942222595
std_targets: 0.4964753985404968
prcauc: 0.7945463794650605
rocauc: 0.8413903352635236
ogbg-molsider: 0.8413903352635236
OGBNanLabelBCEWithLogitsLoss: 0.38216058257967234
All runs completed.
[Epoch 128] ogbg-molsider: 0.645014 val loss: 0.658496
[Epoch 128] ogbg-molsider: 0.565496 test loss: 0.798272
[Epoch 129; Iter    12/   36] train: loss: 0.2594999
[Epoch 129] ogbg-molsider: 0.637521 val loss: 0.616481
[Epoch 129] ogbg-molsider: 0.563212 test loss: 0.733608
[Epoch 130; Iter     6/   36] train: loss: 0.2214635
[Epoch 130; Iter    36/   36] train: loss: 0.2083077
[Epoch 130] ogbg-molsider: 0.629423 val loss: 0.639446
[Epoch 130] ogbg-molsider: 0.566817 test loss: 0.748970
[Epoch 131; Iter    30/   36] train: loss: 0.2566182
[Epoch 131] ogbg-molsider: 0.646277 val loss: 0.609885
[Epoch 131] ogbg-molsider: 0.561059 test loss: 0.719742
[Epoch 132; Iter    24/   36] train: loss: 0.1983431
[Epoch 132] ogbg-molsider: 0.642962 val loss: 0.612466
[Epoch 132] ogbg-molsider: 0.559053 test loss: 0.738301
[Epoch 133; Iter    18/   36] train: loss: 0.1953114
[Epoch 133] ogbg-molsider: 0.641717 val loss: 0.629474
[Epoch 133] ogbg-molsider: 0.571035 test loss: 0.724377
[Epoch 134; Iter    12/   36] train: loss: 0.1982689
[Epoch 134] ogbg-molsider: 0.637302 val loss: 0.633106
[Epoch 134] ogbg-molsider: 0.566266 test loss: 0.753174
[Epoch 135; Iter     6/   36] train: loss: 0.2214185
[Epoch 135; Iter    36/   36] train: loss: 0.2343571
[Epoch 135] ogbg-molsider: 0.642083 val loss: 0.624733
[Epoch 135] ogbg-molsider: 0.569957 test loss: 0.735136
[Epoch 136; Iter    30/   36] train: loss: 0.1675946
[Epoch 136] ogbg-molsider: 0.646284 val loss: 0.631893
[Epoch 136] ogbg-molsider: 0.567661 test loss: 0.759886
[Epoch 137; Iter    24/   36] train: loss: 0.2199734
[Epoch 137] ogbg-molsider: 0.643195 val loss: 0.618541
[Epoch 137] ogbg-molsider: 0.569317 test loss: 0.748071
[Epoch 138; Iter    18/   36] train: loss: 0.2027305
[Epoch 138] ogbg-molsider: 0.642819 val loss: 0.636572
[Epoch 138] ogbg-molsider: 0.573911 test loss: 0.756583
[Epoch 139; Iter    12/   36] train: loss: 0.1778990
[Epoch 139] ogbg-molsider: 0.637310 val loss: 0.630835
[Epoch 139] ogbg-molsider: 0.570244 test loss: 0.750543
[Epoch 140; Iter     6/   36] train: loss: 0.1995540
[Epoch 140; Iter    36/   36] train: loss: 0.2093897
[Epoch 140] ogbg-molsider: 0.639162 val loss: 0.640389
[Epoch 140] ogbg-molsider: 0.570242 test loss: 0.769096
[Epoch 141; Iter    30/   36] train: loss: 0.1960330
[Epoch 141] ogbg-molsider: 0.638203 val loss: 0.644893
[Epoch 141] ogbg-molsider: 0.566683 test loss: 0.774549
Early stopping criterion based on -ogbg-molsider- that should be max reached after 141 epochs. Best model checkpoint was in epoch 81.
Statistics on  val_best_checkpoint
mean_pred: 0.6338549256324768
std_pred: 2.885928153991699
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6934475963825968
rocauc: 0.6554345064149097
ogbg-molsider: 0.6554345064149097
OGBNanLabelBCEWithLogitsLoss: 0.512195634841919
Statistics on  test
mean_pred: 0.7542380690574646
std_pred: 2.9789600372314453
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6371004069491284
rocauc: 0.5871095587225617
ogbg-molsider: 0.5871095587225617
OGBNanLabelBCEWithLogitsLoss: 0.5873080015182495
Statistics on  train
mean_pred: 0.4507080018520355
std_pred: 2.996042490005493
mean_targets: 0.5641575455665588
std_targets: 0.4958747923374176
prcauc: 0.8346892137119134
rocauc: 0.8837845741570015
ogbg-molsider: 0.8837845741570015
OGBNanLabelBCEWithLogitsLoss: 0.3307681083679199
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.628159 val loss: 0.657707
[Epoch 128] ogbg-molsider: 0.585616 test loss: 0.731090
[Epoch 129; Iter    12/   36] train: loss: 0.2114831
[Epoch 129] ogbg-molsider: 0.637385 val loss: 0.661050
[Epoch 129] ogbg-molsider: 0.579174 test loss: 0.761832
[Epoch 130; Iter     6/   36] train: loss: 0.2120692
[Epoch 130; Iter    36/   36] train: loss: 0.2039565
[Epoch 130] ogbg-molsider: 0.631041 val loss: 0.682405
[Epoch 130] ogbg-molsider: 0.584878 test loss: 0.758355
[Epoch 131; Iter    30/   36] train: loss: 0.2320708
[Epoch 131] ogbg-molsider: 0.634225 val loss: 0.670520
[Epoch 131] ogbg-molsider: 0.584511 test loss: 0.741630
[Epoch 132; Iter    24/   36] train: loss: 0.2046262
[Epoch 132] ogbg-molsider: 0.632469 val loss: 0.668560
[Epoch 132] ogbg-molsider: 0.587302 test loss: 0.774635
[Epoch 133; Iter    18/   36] train: loss: 0.2282078
[Epoch 133] ogbg-molsider: 0.627476 val loss: 0.677609
[Epoch 133] ogbg-molsider: 0.588437 test loss: 0.751555
[Epoch 134; Iter    12/   36] train: loss: 0.1866804
[Epoch 134] ogbg-molsider: 0.630810 val loss: 0.672866
[Epoch 134] ogbg-molsider: 0.583581 test loss: 0.757207
[Epoch 135; Iter     6/   36] train: loss: 0.1505270
[Epoch 135; Iter    36/   36] train: loss: 0.1729931
[Epoch 135] ogbg-molsider: 0.619249 val loss: 0.691035
[Epoch 135] ogbg-molsider: 0.578481 test loss: 0.776280
[Epoch 136; Iter    30/   36] train: loss: 0.1879977
[Epoch 136] ogbg-molsider: 0.622289 val loss: 0.683980
[Epoch 136] ogbg-molsider: 0.587040 test loss: 0.775623
[Epoch 137; Iter    24/   36] train: loss: 0.1956596
[Epoch 137] ogbg-molsider: 0.625546 val loss: 0.683226
[Epoch 137] ogbg-molsider: 0.587062 test loss: 0.763102
[Epoch 138; Iter    18/   36] train: loss: 0.1988334
[Epoch 138] ogbg-molsider: 0.627250 val loss: 0.688903
[Epoch 138] ogbg-molsider: 0.579929 test loss: 0.786310
[Epoch 139; Iter    12/   36] train: loss: 0.1587398
[Epoch 139] ogbg-molsider: 0.632141 val loss: 0.677657
[Epoch 139] ogbg-molsider: 0.584792 test loss: 0.777626
[Epoch 140; Iter     6/   36] train: loss: 0.1481353
[Epoch 140; Iter    36/   36] train: loss: 0.1923573
[Epoch 140] ogbg-molsider: 0.628286 val loss: 0.696083
[Epoch 140] ogbg-molsider: 0.587088 test loss: 0.783235
[Epoch 141; Iter    30/   36] train: loss: 0.1777448
[Epoch 141] ogbg-molsider: 0.621721 val loss: 0.675803
[Epoch 141] ogbg-molsider: 0.585276 test loss: 0.768007
[Epoch 142; Iter    24/   36] train: loss: 0.1788210
[Epoch 142] ogbg-molsider: 0.616393 val loss: 0.701749
[Epoch 142] ogbg-molsider: 0.585846 test loss: 0.775643
[Epoch 143; Iter    18/   36] train: loss: 0.1809415
[Epoch 143] ogbg-molsider: 0.621961 val loss: 0.698171
[Epoch 143] ogbg-molsider: 0.582515 test loss: 0.787546
[Epoch 144; Iter    12/   36] train: loss: 0.1497132
[Epoch 144] ogbg-molsider: 0.626492 val loss: 0.704433
[Epoch 144] ogbg-molsider: 0.582443 test loss: 0.787482
[Epoch 145; Iter     6/   36] train: loss: 0.1613072
[Epoch 145; Iter    36/   36] train: loss: 0.2312367
[Epoch 145] ogbg-molsider: 0.627643 val loss: 0.695537
[Epoch 145] ogbg-molsider: 0.585449 test loss: 0.794444
[Epoch 146; Iter    30/   36] train: loss: 0.1894180
[Epoch 146] ogbg-molsider: 0.628932 val loss: 0.704403
[Epoch 146] ogbg-molsider: 0.581260 test loss: 0.798710
[Epoch 147; Iter    24/   36] train: loss: 0.1945174
[Epoch 147] ogbg-molsider: 0.627860 val loss: 0.700451
[Epoch 147] ogbg-molsider: 0.581676 test loss: 0.791637
[Epoch 148; Iter    18/   36] train: loss: 0.2220912
[Epoch 148] ogbg-molsider: 0.626966 val loss: 0.704749
[Epoch 148] ogbg-molsider: 0.586201 test loss: 0.804591
[Epoch 149; Iter    12/   36] train: loss: 0.1765549
[Epoch 149] ogbg-molsider: 0.625367 val loss: 0.702076
[Epoch 149] ogbg-molsider: 0.587923 test loss: 0.809166
[Epoch 150; Iter     6/   36] train: loss: 0.1741708
[Epoch 150; Iter    36/   36] train: loss: 0.1888050
[Epoch 150] ogbg-molsider: 0.630835 val loss: 0.704602
[Epoch 150] ogbg-molsider: 0.593211 test loss: 0.809555
[Epoch 151; Iter    30/   36] train: loss: 0.1752255
[Epoch 151] ogbg-molsider: 0.620209 val loss: 0.712543
[Epoch 151] ogbg-molsider: 0.586000 test loss: 0.804634
[Epoch 152; Iter    24/   36] train: loss: 0.1550641
[Epoch 152] ogbg-molsider: 0.621527 val loss: 0.715046
[Epoch 152] ogbg-molsider: 0.582988 test loss: 0.804127
Early stopping criterion based on -ogbg-molsider- that should be max reached after 152 epochs. Best model checkpoint was in epoch 92.
Statistics on  val_best_checkpoint
mean_pred: 0.35486486554145813
std_pred: 2.9411962032318115
mean_targets: 0.5918155908584595
std_targets: 0.49156126379966736
prcauc: 0.6813053039399358
rocauc: 0.6440773138279184
ogbg-molsider: 0.6440773138279184
OGBNanLabelBCEWithLogitsLoss: 0.558500325679779
Statistics on  test
mean_pred: 0.41269993782043457
std_pred: 3.00730037689209
mean_targets: 0.5705775618553162
std_targets: 0.4950578212738037
prcauc: 0.6301676936254976
rocauc: 0.5780927131545543
ogbg-molsider: 0.5780927131545543
OGBNanLabelBCEWithLogitsLoss: 0.637140691280365
Statistics on  train
mean_pred: 0.37104710936546326
std_pred: 3.241966724395752
mean_targets: 0.5641575455665588
std_targets: 0.4958747625350952
prcauc: 0.8822368720980879
rocauc: 0.926471350321981
ogbg-molsider: 0.926471350321981
OGBNanLabelBCEWithLogitsLoss: 0.28546380665567184
All runs completed.
