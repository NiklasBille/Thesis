>>> Starting run for dataset: bbbp
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --seed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.6/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.6_6_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.6
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'Traceback (most recent call last):
  File "/workspace/train.py", line 691, in <module>
max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6900484
    train(args)
  File "/workspace/train.py", line 297, in train
    return train_ogbg(args, device, metrics_dict)
  File "/workspace/train.py", line 481, in train_ogbg
    val_metrics = trainer.train(train_loader, val_loader, test_loader)
  File "/workspace/trainer/trainer.py", line 91, in train
    val_metrics = self.predict(val_loader, epoch)
  File "/workspace/trainer/trainer.py", line 182, in predict
    total_metrics = self.evaluate_metrics(epoch_predictions, epoch_targets, val=True)
  File "/workspace/trainer/trainer.py", line 201, in evaluate_metrics
    metrics[key] = metric(predictions, targets).item()
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/trainer/metrics.py", line 184, in forward
    raise RuntimeError('No positively labeled data available. Cannot compute Average Precision.')
RuntimeError: No positively labeled data available. Cannot compute Average Precision.
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.6/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.6_4_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.6
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'Traceback (most recent call last):
  File "/workspace/train.py", line 691, in <module>
max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6914535
    train(args)
  File "/workspace/train.py", line 297, in train
    return train_ogbg(args, device, metrics_dict)
  File "/workspace/train.py", line 481, in train_ogbg
    val_metrics = trainer.train(train_loader, val_loader, test_loader)
  File "/workspace/trainer/trainer.py", line 91, in train
    val_metrics = self.predict(val_loader, epoch)
  File "/workspace/trainer/trainer.py", line 182, in predict
    total_metrics = self.evaluate_metrics(epoch_predictions, epoch_targets, val=True)
  File "/workspace/trainer/trainer.py", line 201, in evaluate_metrics
    metrics[key] = metric(predictions, targets).item()
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/trainer/metrics.py", line 184, in forward
    raise RuntimeError('No positively labeled data available. Cannot compute Average Precision.')
RuntimeError: No positively labeled data available. Cannot compute Average Precision.
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.6/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.6_5_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.6
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6934155
Traceback (most recent call last):
  File "/workspace/train.py", line 691, in <module>
    train(args)
  File "/workspace/train.py", line 297, in train
    return train_ogbg(args, device, metrics_dict)
  File "/workspace/train.py", line 481, in train_ogbg
    val_metrics = trainer.train(train_loader, val_loader, test_loader)
  File "/workspace/trainer/trainer.py", line 91, in train
    val_metrics = self.predict(val_loader, epoch)
  File "/workspace/trainer/trainer.py", line 182, in predict
    total_metrics = self.evaluate_metrics(epoch_predictions, epoch_targets, val=True)
  File "/workspace/trainer/trainer.py", line 201, in evaluate_metrics
    metrics[key] = metric(predictions, targets).item()
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/trainer/metrics.py", line 184, in forward
    raise RuntimeError('No positively labeled data available. Cannot compute Average Precision.')
RuntimeError: No positively labeled data available. Cannot compute Average Precision.
All runs completed.
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.8/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.8_5_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.8
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6953800
[Epoch 1] ogbg-molbbbp: 0.681968 val loss: 0.690484
[Epoch 1] ogbg-molbbbp: 0.504919 test loss: 0.692650
[Epoch 2; Iter     5/   55] train: loss: 0.6891797
[Epoch 2; Iter    35/   55] train: loss: 0.6913388
[Epoch 2] ogbg-molbbbp: 0.659863 val loss: 0.689653
[Epoch 2] ogbg-molbbbp: 0.511285 test loss: 0.692963
[Epoch 3; Iter    10/   55] train: loss: 0.6917677
[Epoch 3; Iter    40/   55] train: loss: 0.6895813
[Epoch 3] ogbg-molbbbp: 0.662750 val loss: 0.689699
[Epoch 3] ogbg-molbbbp: 0.513792 test loss: 0.692928
[Epoch 4; Iter    15/   55] train: loss: 0.6932127
[Epoch 4; Iter    45/   55] train: loss: 0.6917215
[Epoch 4] ogbg-molbbbp: 0.668724 val loss: 0.689766
[Epoch 4] ogbg-molbbbp: 0.507620 test loss: 0.692964
[Epoch 5; Iter    20/   55] train: loss: 0.6900438
[Epoch 5; Iter    50/   55] train: loss: 0.6886819
[Epoch 5] ogbg-molbbbp: 0.676392 val loss: 0.689756
[Epoch 5] ogbg-molbbbp: 0.524306 test loss: 0.692562
[Epoch 6; Iter    25/   55] train: loss: 0.6907805
[Epoch 6; Iter    55/   55] train: loss: 0.6868229
[Epoch 6] ogbg-molbbbp: 0.688440 val loss: 0.689352
[Epoch 6] ogbg-molbbbp: 0.515625 test loss: 0.692586
[Epoch 7; Iter    30/   55] train: loss: 0.6843200
[Epoch 7] ogbg-molbbbp: 0.687743 val loss: 0.689706
[Epoch 7] ogbg-molbbbp: 0.520158 test loss: 0.692286
[Epoch 8; Iter     5/   55] train: loss: 0.6848198
[Epoch 8; Iter    35/   55] train: loss: 0.6829073
[Epoch 8] ogbg-molbbbp: 0.693020 val loss: 0.689998
[Epoch 8] ogbg-molbbbp: 0.520544 test loss: 0.692166
[Epoch 9; Iter    10/   55] train: loss: 0.6824759
[Epoch 9; Iter    40/   55] train: loss: 0.6832919
[Epoch 9] ogbg-molbbbp: 0.712237 val loss: 0.689589
[Epoch 9] ogbg-molbbbp: 0.525367 test loss: 0.691818
[Epoch 10; Iter    15/   55] train: loss: 0.6793813
[Epoch 10; Iter    45/   55] train: loss: 0.6761037
[Epoch 10] ogbg-molbbbp: 0.716619 val loss: 0.690044
[Epoch 10] ogbg-molbbbp: 0.526331 test loss: 0.691658
[Epoch 11; Iter    20/   55] train: loss: 0.6775451
[Epoch 11; Iter    50/   55] train: loss: 0.6749845
[Epoch 11] ogbg-molbbbp: 0.739022 val loss: 0.689276
[Epoch 11] ogbg-molbbbp: 0.530285 test loss: 0.691331
[Epoch 12; Iter    25/   55] train: loss: 0.6756749
[Epoch 12; Iter    55/   55] train: loss: 0.6737819
[Epoch 12] ogbg-molbbbp: 0.745196 val loss: 0.689362
[Epoch 12] ogbg-molbbbp: 0.533565 test loss: 0.691045
[Epoch 13; Iter    30/   55] train: loss: 0.6755737
[Epoch 13] ogbg-molbbbp: 0.912775 val loss: 0.638866
[Epoch 13] ogbg-molbbbp: 0.614294 test loss: 0.680838
[Epoch 14; Iter     5/   55] train: loss: 0.6600859
[Epoch 14; Iter    35/   55] train: loss: 0.6125320
[Epoch 14] ogbg-molbbbp: 0.912576 val loss: 0.527694
[Epoch 14] ogbg-molbbbp: 0.676215 test loss: 0.653872
[Epoch 15; Iter    10/   55] train: loss: 0.6011708
[Epoch 15; Iter    40/   55] train: loss: 0.5246513
[Epoch 15] ogbg-molbbbp: 0.932391 val loss: 0.429690
[Epoch 15] ogbg-molbbbp: 0.668499 test loss: 0.661629
[Epoch 16; Iter    15/   55] train: loss: 0.4103695
[Epoch 16; Iter    45/   55] train: loss: 0.4990442
[Epoch 16] ogbg-molbbbp: 0.936971 val loss: 0.396988
[Epoch 16] ogbg-molbbbp: 0.644194 test loss: 0.720797
[Epoch 17; Iter    20/   55] train: loss: 0.3242468
[Epoch 17; Iter    50/   55] train: loss: 0.4411244
[Epoch 17] ogbg-molbbbp: 0.931295 val loss: 0.358905
[Epoch 17] ogbg-molbbbp: 0.680170 test loss: 0.706807
[Epoch 18; Iter    25/   55] train: loss: 0.3736207
[Epoch 18; Iter    55/   55] train: loss: 0.5158485
[Epoch 18] ogbg-molbbbp: 0.941153 val loss: 0.375839
[Epoch 18] ogbg-molbbbp: 0.665895 test loss: 0.877649
[Epoch 19; Iter    30/   55] train: loss: 0.3345467
[Epoch 19] ogbg-molbbbp: 0.943443 val loss: 0.314649
[Epoch 19] ogbg-molbbbp: 0.646894 test loss: 0.925791
[Epoch 20; Iter     5/   55] train: loss: 0.3231257
[Epoch 20; Iter    35/   55] train: loss: 0.2377951
[Epoch 20] ogbg-molbbbp: 0.944738 val loss: 0.454619
[Epoch 20] ogbg-molbbbp: 0.685185 test loss: 0.901042
[Epoch 21; Iter    10/   55] train: loss: 0.2851139
[Epoch 21; Iter    40/   55] train: loss: 0.5542256
[Epoch 21] ogbg-molbbbp: 0.957483 val loss: 0.315918
[Epoch 21] ogbg-molbbbp: 0.680941 test loss: 0.836815
[Epoch 22; Iter    15/   55] train: loss: 0.3117546
[Epoch 22; Iter    45/   55] train: loss: 0.3554611
[Epoch 22] ogbg-molbbbp: 0.933586 val loss: 0.557752
[Epoch 22] ogbg-molbbbp: 0.684606 test loss: 0.970966
[Epoch 23; Iter    20/   55] train: loss: 0.2494872
[Epoch 23; Iter    50/   55] train: loss: 0.2501745
[Epoch 23] ogbg-molbbbp: 0.957682 val loss: 0.419206
[Epoch 23] ogbg-molbbbp: 0.687596 test loss: 1.119974
[Epoch 24; Iter    25/   55] train: loss: 0.4575572
[Epoch 24; Iter    55/   55] train: loss: 0.1451092
[Epoch 24] ogbg-molbbbp: 0.939759 val loss: 0.469392
[Epoch 24] ogbg-molbbbp: 0.691358 test loss: 0.998601
[Epoch 25; Iter    30/   55] train: loss: 0.2689428
[Epoch 25] ogbg-molbbbp: 0.950812 val loss: 0.333008
[Epoch 25] ogbg-molbbbp: 0.709973 test loss: 0.962775
[Epoch 26; Iter     5/   55] train: loss: 0.2391358
[Epoch 26; Iter    35/   55] train: loss: 0.2025697
[Epoch 26] ogbg-molbbbp: 0.932889 val loss: 0.433971
[Epoch 26] ogbg-molbbbp: 0.674383 test loss: 1.052091
[Epoch 27; Iter    10/   55] train: loss: 0.2705702
[Epoch 27; Iter    40/   55] train: loss: 0.2238509
[Epoch 27] ogbg-molbbbp: 0.960769 val loss: 0.271955
[Epoch 27] ogbg-molbbbp: 0.698978 test loss: 0.823703
[Epoch 28; Iter    15/   55] train: loss: 0.3020680
[Epoch 28; Iter    45/   55] train: loss: 0.1625158
[Epoch 28] ogbg-molbbbp: 0.953699 val loss: 0.273873
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.7/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.7_6_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.7
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6906257
[Epoch 1] ogbg-molbbbp: 0.698534 val loss: 0.692650
[Epoch 1] ogbg-molbbbp: 0.602957 test loss: 0.691826
[Epoch 2; Iter    12/   48] train: loss: 0.6899332
[Epoch 2; Iter    42/   48] train: loss: 0.6945339
[Epoch 2] ogbg-molbbbp: 0.793314 val loss: 0.688748
[Epoch 2] ogbg-molbbbp: 0.645206 test loss: 0.688139
[Epoch 3; Iter    24/   48] train: loss: 0.6912053
[Epoch 3] ogbg-molbbbp: 0.785337 val loss: 0.687196
[Epoch 3] ogbg-molbbbp: 0.640771 test loss: 0.688770
[Epoch 4; Iter     6/   48] train: loss: 0.6873144
[Epoch 4; Iter    36/   48] train: loss: 0.6873167
[Epoch 4] ogbg-molbbbp: 0.802111 val loss: 0.686495
[Epoch 4] ogbg-molbbbp: 0.646774 test loss: 0.688274
[Epoch 5; Iter    18/   48] train: loss: 0.6893399
[Epoch 5; Iter    48/   48] train: loss: 0.6914814
[Epoch 5] ogbg-molbbbp: 0.771261 val loss: 0.685733
[Epoch 5] ogbg-molbbbp: 0.633961 test loss: 0.689304
[Epoch 6; Iter    30/   48] train: loss: 0.6868055
[Epoch 6] ogbg-molbbbp: 0.822053 val loss: 0.684271
[Epoch 6] ogbg-molbbbp: 0.656183 test loss: 0.688415
[Epoch 7; Iter    12/   48] train: loss: 0.6911626
[Epoch 7; Iter    42/   48] train: loss: 0.6880922
[Epoch 7] ogbg-molbbbp: 0.817361 val loss: 0.683416
[Epoch 7] ogbg-molbbbp: 0.656810 test loss: 0.688659
[Epoch 8; Iter    24/   48] train: loss: 0.6853527
[Epoch 8] ogbg-molbbbp: 0.833783 val loss: 0.681240
[Epoch 8] ogbg-molbbbp: 0.665950 test loss: 0.688786
[Epoch 9; Iter     6/   48] train: loss: 0.6840798
[Epoch 9; Iter    36/   48] train: loss: 0.6849536
[Epoch 9] ogbg-molbbbp: 0.832493 val loss: 0.678731
[Epoch 9] ogbg-molbbbp: 0.657706 test loss: 0.689438
[Epoch 10; Iter    18/   48] train: loss: 0.6804606
[Epoch 10; Iter    48/   48] train: loss: 0.6785380
[Epoch 10] ogbg-molbbbp: 0.846334 val loss: 0.676200
[Epoch 10] ogbg-molbbbp: 0.673208 test loss: 0.689833
[Epoch 11; Iter    30/   48] train: loss: 0.6808973
[Epoch 11] ogbg-molbbbp: 0.853842 val loss: 0.674122
[Epoch 11] ogbg-molbbbp: 0.677643 test loss: 0.689836
[Epoch 12; Iter    12/   48] train: loss: 0.6820886
[Epoch 12; Iter    42/   48] train: loss: 0.6846358
[Epoch 12] ogbg-molbbbp: 0.858534 val loss: 0.671020
[Epoch 12] ogbg-molbbbp: 0.677867 test loss: 0.690533
[Epoch 13; Iter    24/   48] train: loss: 0.6723617
[Epoch 13] ogbg-molbbbp: 0.882933 val loss: 0.668432
[Epoch 13] ogbg-molbbbp: 0.687634 test loss: 0.690409
[Epoch 14; Iter     6/   48] train: loss: 0.6771677
[Epoch 14; Iter    36/   48] train: loss: 0.6761363
[Epoch 14] ogbg-molbbbp: 0.886334 val loss: 0.664696
[Epoch 14] ogbg-molbbbp: 0.693280 test loss: 0.691301
[Epoch 15; Iter    18/   48] train: loss: 0.6689470
[Epoch 15; Iter    48/   48] train: loss: 0.6595894
[Epoch 15] ogbg-molbbbp: 0.959179 val loss: 0.653548
[Epoch 15] ogbg-molbbbp: 0.782482 test loss: 0.618123
[Epoch 16; Iter    30/   48] train: loss: 0.5866780
[Epoch 16] ogbg-molbbbp: 0.923284 val loss: 0.590083
[Epoch 16] ogbg-molbbbp: 0.761738 test loss: 0.562163
[Epoch 17; Iter    12/   48] train: loss: 0.6282388
[Epoch 17; Iter    42/   48] train: loss: 0.4993056
[Epoch 17] ogbg-molbbbp: 0.964575 val loss: 0.433213
[Epoch 17] ogbg-molbbbp: 0.778405 test loss: 0.539300
[Epoch 18; Iter    24/   48] train: loss: 0.4380372
[Epoch 18] ogbg-molbbbp: 0.965044 val loss: 0.405523
[Epoch 18] ogbg-molbbbp: 0.785753 test loss: 0.534200
[Epoch 19; Iter     6/   48] train: loss: 0.4454785
[Epoch 19; Iter    36/   48] train: loss: 0.3552455
[Epoch 19] ogbg-molbbbp: 0.962581 val loss: 0.541879
[Epoch 19] ogbg-molbbbp: 0.785439 test loss: 0.548762
[Epoch 20; Iter    18/   48] train: loss: 0.3819457
[Epoch 20; Iter    48/   48] train: loss: 0.2852449
[Epoch 20] ogbg-molbbbp: 0.971496 val loss: 0.237630
[Epoch 20] ogbg-molbbbp: 0.767652 test loss: 0.636836
[Epoch 21; Iter    30/   48] train: loss: 0.3189030
[Epoch 21] ogbg-molbbbp: 0.966334 val loss: 0.200817
[Epoch 21] ogbg-molbbbp: 0.782168 test loss: 0.692981
[Epoch 22; Iter    12/   48] train: loss: 0.3369920
[Epoch 22; Iter    42/   48] train: loss: 0.3117015
[Epoch 22] ogbg-molbbbp: 0.972082 val loss: 0.221879
[Epoch 22] ogbg-molbbbp: 0.784095 test loss: 0.650050
[Epoch 23; Iter    24/   48] train: loss: 0.3083789
[Epoch 23] ogbg-molbbbp: 0.976891 val loss: 0.173243
[Epoch 23] ogbg-molbbbp: 0.798611 test loss: 0.720281
[Epoch 24; Iter     6/   48] train: loss: 0.3066725
[Epoch 24; Iter    36/   48] train: loss: 0.2157625
[Epoch 24] ogbg-molbbbp: 0.951906 val loss: 0.169571
[Epoch 24] ogbg-molbbbp: 0.775627 test loss: 0.792094
[Epoch 25; Iter    18/   48] train: loss: 0.2828526
[Epoch 25; Iter    48/   48] train: loss: 0.1762233
[Epoch 25] ogbg-molbbbp: 0.971144 val loss: 0.156438
[Epoch 25] ogbg-molbbbp: 0.765547 test loss: 0.759165
[Epoch 26; Iter    30/   48] train: loss: 0.4955655
[Epoch 26] ogbg-molbbbp: 0.940880 val loss: 0.176777
[Epoch 26] ogbg-molbbbp: 0.775896 test loss: 0.857132
[Epoch 27; Iter    12/   48] train: loss: 0.1958082
[Epoch 27; Iter    42/   48] train: loss: 0.3513821
[Epoch 27] ogbg-molbbbp: 0.977595 val loss: 0.198856
[Epoch 27] ogbg-molbbbp: 0.791353 test loss: 0.647939
[Epoch 28; Iter    24/   48] train: loss: 0.2332905
[Epoch 28] ogbg-molbbbp: 0.980880 val loss: 0.210401
[Epoch 28] ogbg-molbbbp: 0.796416 test loss: 0.623273
[Epoch 29; Iter     6/   48] train: loss: 0.3334680
[Epoch 29; Iter    36/   48] train: loss: 0.2586492
[Epoch 29] ogbg-molbbbp: 0.974780 val loss: 0.176300
[Epoch 29] ogbg-molbbbp: 0.770027 test loss: 0.774372
[Epoch 30; Iter    18/   48] train: loss: 0.3396993
[Epoch 30; Iter    48/   48] train: loss: 0.2399173
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.8/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.8_4_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.8
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6884762
[Epoch 1] ogbg-molbbbp: 0.786618 val loss: 0.688927
[Epoch 1] ogbg-molbbbp: 0.451003 test loss: 0.693395
[Epoch 2; Iter     5/   55] train: loss: 0.6923946
[Epoch 2; Iter    35/   55] train: loss: 0.6876841
[Epoch 2] ogbg-molbbbp: 0.837997 val loss: 0.682949
[Epoch 2] ogbg-molbbbp: 0.473187 test loss: 0.692958
[Epoch 3; Iter    10/   55] train: loss: 0.6927744
[Epoch 3; Iter    40/   55] train: loss: 0.6897194
[Epoch 3] ogbg-molbbbp: 0.844867 val loss: 0.682070
[Epoch 3] ogbg-molbbbp: 0.473283 test loss: 0.692994
[Epoch 4; Iter    15/   55] train: loss: 0.6907960
[Epoch 4; Iter    45/   55] train: loss: 0.6890080
[Epoch 4] ogbg-molbbbp: 0.845962 val loss: 0.682295
[Epoch 4] ogbg-molbbbp: 0.474151 test loss: 0.692845
[Epoch 5; Iter    20/   55] train: loss: 0.6906404
[Epoch 5; Iter    50/   55] train: loss: 0.6897183
[Epoch 5] ogbg-molbbbp: 0.859703 val loss: 0.681333
[Epoch 5] ogbg-molbbbp: 0.485822 test loss: 0.692551
[Epoch 6; Iter    25/   55] train: loss: 0.6867520
[Epoch 6; Iter    55/   55] train: loss: 0.6828100
[Epoch 6] ogbg-molbbbp: 0.858409 val loss: 0.681337
[Epoch 6] ogbg-molbbbp: 0.485050 test loss: 0.692353
[Epoch 7; Iter    30/   55] train: loss: 0.6822934
[Epoch 7] ogbg-molbbbp: 0.859803 val loss: 0.681038
[Epoch 7] ogbg-molbbbp: 0.483989 test loss: 0.692293
[Epoch 8; Iter     5/   55] train: loss: 0.6849414
[Epoch 8; Iter    35/   55] train: loss: 0.6830447
[Epoch 8] ogbg-molbbbp: 0.859703 val loss: 0.681340
[Epoch 8] ogbg-molbbbp: 0.485243 test loss: 0.692142
[Epoch 9; Iter    10/   55] train: loss: 0.6818204
[Epoch 9; Iter    40/   55] train: loss: 0.6791931
[Epoch 9] ogbg-molbbbp: 0.873743 val loss: 0.680515
[Epoch 9] ogbg-molbbbp: 0.495949 test loss: 0.691665
[Epoch 10; Iter    15/   55] train: loss: 0.6797972
[Epoch 10; Iter    45/   55] train: loss: 0.6759865
[Epoch 10] ogbg-molbbbp: 0.867171 val loss: 0.680906
[Epoch 10] ogbg-molbbbp: 0.488619 test loss: 0.691683
[Epoch 11; Iter    20/   55] train: loss: 0.6755283
[Epoch 11; Iter    50/   55] train: loss: 0.6691516
[Epoch 11] ogbg-molbbbp: 0.875734 val loss: 0.679810
[Epoch 11] ogbg-molbbbp: 0.500579 test loss: 0.691127
[Epoch 12; Iter    25/   55] train: loss: 0.6697974
[Epoch 12; Iter    55/   55] train: loss: 0.6743339
[Epoch 12] ogbg-molbbbp: 0.875834 val loss: 0.679601
[Epoch 12] ogbg-molbbbp: 0.503569 test loss: 0.690819
[Epoch 13; Iter    30/   55] train: loss: 0.6696041
[Epoch 13] ogbg-molbbbp: 0.921040 val loss: 0.619850
[Epoch 13] ogbg-molbbbp: 0.634742 test loss: 0.679863
[Epoch 14; Iter     5/   55] train: loss: 0.6582856
[Epoch 14; Iter    35/   55] train: loss: 0.6013725
[Epoch 14] ogbg-molbbbp: 0.945335 val loss: 0.447476
[Epoch 14] ogbg-molbbbp: 0.661555 test loss: 0.671061
[Epoch 15; Iter    10/   55] train: loss: 0.5356821
[Epoch 15; Iter    40/   55] train: loss: 0.5460823
[Epoch 15] ogbg-molbbbp: 0.932490 val loss: 0.448589
[Epoch 15] ogbg-molbbbp: 0.644001 test loss: 0.711707
[Epoch 16; Iter    15/   55] train: loss: 0.5255836
[Epoch 16; Iter    45/   55] train: loss: 0.3604421
[Epoch 16] ogbg-molbbbp: 0.928010 val loss: 0.423684
[Epoch 16] ogbg-molbbbp: 0.667149 test loss: 0.740127
[Epoch 17; Iter    20/   55] train: loss: 0.3566412
[Epoch 17; Iter    50/   55] train: loss: 0.5332732
[Epoch 17] ogbg-molbbbp: 0.940854 val loss: 0.351445
[Epoch 17] ogbg-molbbbp: 0.665799 test loss: 0.837979
[Epoch 18; Iter    25/   55] train: loss: 0.3472274
[Epoch 18; Iter    55/   55] train: loss: 0.3417291
[Epoch 18] ogbg-molbbbp: 0.944339 val loss: 0.379286
[Epoch 18] ogbg-molbbbp: 0.683931 test loss: 0.876071
[Epoch 19; Iter    30/   55] train: loss: 0.3280640
[Epoch 19] ogbg-molbbbp: 0.937270 val loss: 0.351573
[Epoch 19] ogbg-molbbbp: 0.654417 test loss: 0.838074
[Epoch 20; Iter     5/   55] train: loss: 0.2684816
[Epoch 20; Iter    35/   55] train: loss: 0.1711666
[Epoch 20] ogbg-molbbbp: 0.928507 val loss: 0.421668
[Epoch 20] ogbg-molbbbp: 0.682099 test loss: 0.783989
[Epoch 21; Iter    10/   55] train: loss: 0.3836500
[Epoch 21; Iter    40/   55] train: loss: 0.4311933
[Epoch 21] ogbg-molbbbp: 0.950513 val loss: 0.328312
[Epoch 21] ogbg-molbbbp: 0.663098 test loss: 0.889632
[Epoch 22; Iter    15/   55] train: loss: 0.2950646
[Epoch 22; Iter    45/   55] train: loss: 0.3558119
[Epoch 22] ogbg-molbbbp: 0.946231 val loss: 0.370667
[Epoch 22] ogbg-molbbbp: 0.685378 test loss: 0.947215
[Epoch 23; Iter    20/   55] train: loss: 0.4546368
[Epoch 23; Iter    50/   55] train: loss: 0.2135662
[Epoch 23] ogbg-molbbbp: 0.946530 val loss: 0.411343
[Epoch 23] ogbg-molbbbp: 0.679977 test loss: 1.003616
[Epoch 24; Iter    25/   55] train: loss: 0.2922603
[Epoch 24; Iter    55/   55] train: loss: 0.8133190
[Epoch 24] ogbg-molbbbp: 0.946629 val loss: 0.395135
[Epoch 24] ogbg-molbbbp: 0.677855 test loss: 0.911598
[Epoch 25; Iter    30/   55] train: loss: 0.1332224
[Epoch 25] ogbg-molbbbp: 0.956188 val loss: 0.308469
[Epoch 25] ogbg-molbbbp: 0.704090 test loss: 0.804687
[Epoch 26; Iter     5/   55] train: loss: 0.3537517
[Epoch 26; Iter    35/   55] train: loss: 0.1261112
[Epoch 26] ogbg-molbbbp: 0.929105 val loss: 0.380102
[Epoch 26] ogbg-molbbbp: 0.666667 test loss: 0.860764
[Epoch 27; Iter    10/   55] train: loss: 0.1517573
[Epoch 27; Iter    40/   55] train: loss: 0.1663667
[Epoch 27] ogbg-molbbbp: 0.967141 val loss: 0.247041
[Epoch 27] ogbg-molbbbp: 0.729745 test loss: 0.773834
[Epoch 28; Iter    15/   55] train: loss: 0.1882667
[Epoch 28; Iter    45/   55] train: loss: 0.2517363
[Epoch 28] ogbg-molbbbp: 0.918052 val loss: 0.887085
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.8/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.8_6_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.8
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6932197
[Epoch 1] ogbg-molbbbp: 0.762720 val loss: 0.690579
[Epoch 1] ogbg-molbbbp: 0.533083 test loss: 0.692731
[Epoch 2; Iter     5/   55] train: loss: 0.6927039
[Epoch 2; Iter    35/   55] train: loss: 0.6903805
[Epoch 2] ogbg-molbbbp: 0.797272 val loss: 0.685792
[Epoch 2] ogbg-molbbbp: 0.534722 test loss: 0.692064
[Epoch 3; Iter    10/   55] train: loss: 0.6901688
[Epoch 3; Iter    40/   55] train: loss: 0.6889755
[Epoch 3] ogbg-molbbbp: 0.794882 val loss: 0.686051
[Epoch 3] ogbg-molbbbp: 0.536073 test loss: 0.692082
[Epoch 4; Iter    15/   55] train: loss: 0.6905053
[Epoch 4; Iter    45/   55] train: loss: 0.6917712
[Epoch 4] ogbg-molbbbp: 0.820671 val loss: 0.685113
[Epoch 4] ogbg-molbbbp: 0.534144 test loss: 0.691986
[Epoch 5; Iter    20/   55] train: loss: 0.6911498
[Epoch 5; Iter    50/   55] train: loss: 0.6849086
[Epoch 5] ogbg-molbbbp: 0.804441 val loss: 0.685635
[Epoch 5] ogbg-molbbbp: 0.544078 test loss: 0.691602
[Epoch 6; Iter    25/   55] train: loss: 0.6898429
[Epoch 6; Iter    55/   55] train: loss: 0.6864198
[Epoch 6] ogbg-molbbbp: 0.818182 val loss: 0.685093
[Epoch 6] ogbg-molbbbp: 0.542728 test loss: 0.691411
[Epoch 7; Iter    30/   55] train: loss: 0.6864702
[Epoch 7] ogbg-molbbbp: 0.820671 val loss: 0.685014
[Epoch 7] ogbg-molbbbp: 0.550637 test loss: 0.691104
[Epoch 8; Iter     5/   55] train: loss: 0.6864006
[Epoch 8; Iter    35/   55] train: loss: 0.6850717
[Epoch 8] ogbg-molbbbp: 0.823758 val loss: 0.685118
[Epoch 8] ogbg-molbbbp: 0.547261 test loss: 0.691080
[Epoch 9; Iter    10/   55] train: loss: 0.6809853
[Epoch 9; Iter    40/   55] train: loss: 0.6821335
[Epoch 9] ogbg-molbbbp: 0.840984 val loss: 0.684847
[Epoch 9] ogbg-molbbbp: 0.547164 test loss: 0.690830
[Epoch 10; Iter    15/   55] train: loss: 0.6796424
[Epoch 10; Iter    45/   55] train: loss: 0.6761301
[Epoch 10] ogbg-molbbbp: 0.844668 val loss: 0.685269
[Epoch 10] ogbg-molbbbp: 0.547068 test loss: 0.690670
[Epoch 11; Iter    20/   55] train: loss: 0.6766856
[Epoch 11; Iter    50/   55] train: loss: 0.6718633
[Epoch 11] ogbg-molbbbp: 0.851339 val loss: 0.685120
[Epoch 11] ogbg-molbbbp: 0.547647 test loss: 0.690257
[Epoch 12; Iter    25/   55] train: loss: 0.6706553
[Epoch 12; Iter    55/   55] train: loss: 0.6854111
[Epoch 12] ogbg-molbbbp: 0.861097 val loss: 0.684722
[Epoch 12] ogbg-molbbbp: 0.551698 test loss: 0.690112
[Epoch 13; Iter    30/   55] train: loss: 0.6700866
[Epoch 13] ogbg-molbbbp: 0.912277 val loss: 0.609498
[Epoch 13] ogbg-molbbbp: 0.658951 test loss: 0.675043
[Epoch 14; Iter     5/   55] train: loss: 0.6314537
[Epoch 14; Iter    35/   55] train: loss: 0.5839542
[Epoch 14] ogbg-molbbbp: 0.942447 val loss: 0.476285
[Epoch 14] ogbg-molbbbp: 0.656346 test loss: 0.676354
[Epoch 15; Iter    10/   55] train: loss: 0.5738180
[Epoch 15; Iter    40/   55] train: loss: 0.4984138
[Epoch 15] ogbg-molbbbp: 0.915464 val loss: 0.430677
[Epoch 15] ogbg-molbbbp: 0.616802 test loss: 0.716303
[Epoch 16; Iter    15/   55] train: loss: 0.4568293
[Epoch 16; Iter    45/   55] train: loss: 0.4049587
[Epoch 16] ogbg-molbbbp: 0.923131 val loss: 0.420508
[Epoch 16] ogbg-molbbbp: 0.685378 test loss: 0.705190
[Epoch 17; Iter    20/   55] train: loss: 0.3968272
[Epoch 17; Iter    50/   55] train: loss: 0.3701334
[Epoch 17] ogbg-molbbbp: 0.945733 val loss: 0.354612
[Epoch 17] ogbg-molbbbp: 0.683353 test loss: 0.721750
[Epoch 18; Iter    25/   55] train: loss: 0.2525442
[Epoch 18; Iter    55/   55] train: loss: 0.2801490
[Epoch 18] ogbg-molbbbp: 0.942547 val loss: 0.411497
[Epoch 18] ogbg-molbbbp: 0.663002 test loss: 0.857327
[Epoch 19; Iter    30/   55] train: loss: 0.2537983
[Epoch 19] ogbg-molbbbp: 0.948820 val loss: 0.316019
[Epoch 19] ogbg-molbbbp: 0.677566 test loss: 0.813521
[Epoch 20; Iter     5/   55] train: loss: 0.3644337
[Epoch 20; Iter    35/   55] train: loss: 0.4354169
[Epoch 20] ogbg-molbbbp: 0.955491 val loss: 0.286527
[Epoch 20] ogbg-molbbbp: 0.655671 test loss: 0.819072
[Epoch 21; Iter    10/   55] train: loss: 0.3622795
[Epoch 21; Iter    40/   55] train: loss: 0.2449801
[Epoch 21] ogbg-molbbbp: 0.935776 val loss: 0.493187
[Epoch 21] ogbg-molbbbp: 0.674093 test loss: 1.120458
[Epoch 22; Iter    15/   55] train: loss: 0.3581019
[Epoch 22; Iter    45/   55] train: loss: 0.2594362
[Epoch 22] ogbg-molbbbp: 0.929204 val loss: 0.470772
[Epoch 22] ogbg-molbbbp: 0.692226 test loss: 0.952291
[Epoch 23; Iter    20/   55] train: loss: 0.3943421
[Epoch 23; Iter    50/   55] train: loss: 0.3807874
[Epoch 23] ogbg-molbbbp: 0.932391 val loss: 0.453190
[Epoch 23] ogbg-molbbbp: 0.682485 test loss: 0.927744
[Epoch 24; Iter    25/   55] train: loss: 0.2283857
[Epoch 24; Iter    55/   55] train: loss: 0.0774461
[Epoch 24] ogbg-molbbbp: 0.951210 val loss: 0.305556
[Epoch 24] ogbg-molbbbp: 0.684606 test loss: 0.994863
[Epoch 25; Iter    30/   55] train: loss: 0.2697252
[Epoch 25] ogbg-molbbbp: 0.949816 val loss: 0.510730
[Epoch 25] ogbg-molbbbp: 0.679688 test loss: 1.205852
[Epoch 26; Iter     5/   55] train: loss: 0.2486286
[Epoch 26; Iter    35/   55] train: loss: 0.1863474
[Epoch 26] ogbg-molbbbp: 0.920442 val loss: 0.364678
[Epoch 26] ogbg-molbbbp: 0.621528 test loss: 0.926933
[Epoch 27; Iter    10/   55] train: loss: 0.1926475
[Epoch 27; Iter    40/   55] train: loss: 0.1212564
[Epoch 27] ogbg-molbbbp: 0.961764 val loss: 0.274758
[Epoch 27] ogbg-molbbbp: 0.682099 test loss: 1.022249
[Epoch 28; Iter    15/   55] train: loss: 0.1506365
[Epoch 28; Iter    45/   55] train: loss: 0.2235475
[Epoch 28] ogbg-molbbbp: 0.947426 val loss: 0.328096
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.7/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.7_4_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.7
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6901863
[Epoch 1] ogbg-molbbbp: 0.815953 val loss: 0.691170
[Epoch 1] ogbg-molbbbp: 0.611604 test loss: 0.691728
[Epoch 2; Iter    12/   48] train: loss: 0.6913084
[Epoch 2; Iter    42/   48] train: loss: 0.6906883
[Epoch 2] ogbg-molbbbp: 0.856305 val loss: 0.686664
[Epoch 2] ogbg-molbbbp: 0.644624 test loss: 0.688346
[Epoch 3; Iter    24/   48] train: loss: 0.6870868
[Epoch 3] ogbg-molbbbp: 0.862287 val loss: 0.684623
[Epoch 3] ogbg-molbbbp: 0.652240 test loss: 0.688260
[Epoch 4; Iter     6/   48] train: loss: 0.6925020
[Epoch 4; Iter    36/   48] train: loss: 0.6914425
[Epoch 4] ogbg-molbbbp: 0.866276 val loss: 0.684064
[Epoch 4] ogbg-molbbbp: 0.651703 test loss: 0.688250
[Epoch 5; Iter    18/   48] train: loss: 0.6913630
[Epoch 5; Iter    48/   48] train: loss: 0.6837044
[Epoch 5] ogbg-molbbbp: 0.869443 val loss: 0.682166
[Epoch 5] ogbg-molbbbp: 0.656004 test loss: 0.688529
[Epoch 6; Iter    30/   48] train: loss: 0.6874417
[Epoch 6] ogbg-molbbbp: 0.874135 val loss: 0.682541
[Epoch 6] ogbg-molbbbp: 0.658916 test loss: 0.688224
[Epoch 7; Iter    12/   48] train: loss: 0.6864911
[Epoch 7; Iter    42/   48] train: loss: 0.6840315
[Epoch 7] ogbg-molbbbp: 0.880117 val loss: 0.680237
[Epoch 7] ogbg-molbbbp: 0.665726 test loss: 0.688108
[Epoch 8; Iter    24/   48] train: loss: 0.6857542
[Epoch 8] ogbg-molbbbp: 0.885865 val loss: 0.679006
[Epoch 8] ogbg-molbbbp: 0.665412 test loss: 0.687915
[Epoch 9; Iter     6/   48] train: loss: 0.6815360
[Epoch 9; Iter    36/   48] train: loss: 0.6798682
[Epoch 9] ogbg-molbbbp: 0.885630 val loss: 0.675842
[Epoch 9] ogbg-molbbbp: 0.669534 test loss: 0.689190
[Epoch 10; Iter    18/   48] train: loss: 0.6804165
[Epoch 10; Iter    48/   48] train: loss: 0.6829693
[Epoch 10] ogbg-molbbbp: 0.888446 val loss: 0.672735
[Epoch 10] ogbg-molbbbp: 0.671550 test loss: 0.689552
[Epoch 11; Iter    30/   48] train: loss: 0.6799290
[Epoch 11] ogbg-molbbbp: 0.896422 val loss: 0.670996
[Epoch 11] ogbg-molbbbp: 0.673746 test loss: 0.689017
[Epoch 12; Iter    12/   48] train: loss: 0.6791303
[Epoch 12; Iter    42/   48] train: loss: 0.6752552
[Epoch 12] ogbg-molbbbp: 0.895484 val loss: 0.668619
[Epoch 12] ogbg-molbbbp: 0.675672 test loss: 0.689974
[Epoch 13; Iter    24/   48] train: loss: 0.6804712
[Epoch 13] ogbg-molbbbp: 0.906510 val loss: 0.665140
[Epoch 13] ogbg-molbbbp: 0.683961 test loss: 0.689873
[Epoch 14; Iter     6/   48] train: loss: 0.6775936
[Epoch 14; Iter    36/   48] train: loss: 0.6793779
[Epoch 14] ogbg-molbbbp: 0.908504 val loss: 0.661805
[Epoch 14] ogbg-molbbbp: 0.687052 test loss: 0.689939
[Epoch 15; Iter    18/   48] train: loss: 0.6726156
[Epoch 15; Iter    48/   48] train: loss: 0.6646257
[Epoch 15] ogbg-molbbbp: 0.960469 val loss: 0.610696
[Epoch 15] ogbg-molbbbp: 0.768817 test loss: 0.633506
[Epoch 16; Iter    30/   48] train: loss: 0.6172341
[Epoch 16] ogbg-molbbbp: 0.965982 val loss: 0.516890
[Epoch 16] ogbg-molbbbp: 0.777419 test loss: 0.564725
[Epoch 17; Iter    12/   48] train: loss: 0.5428323
[Epoch 17; Iter    42/   48] train: loss: 0.5019163
[Epoch 17] ogbg-molbbbp: 0.965513 val loss: 0.488310
[Epoch 17] ogbg-molbbbp: 0.781496 test loss: 0.523908
[Epoch 18; Iter    24/   48] train: loss: 0.4212844
[Epoch 18] ogbg-molbbbp: 0.896422 val loss: 0.380199
[Epoch 18] ogbg-molbbbp: 0.750986 test loss: 0.591610
[Epoch 19; Iter     6/   48] train: loss: 0.4646217
[Epoch 19; Iter    36/   48] train: loss: 0.4049275
[Epoch 19] ogbg-molbbbp: 0.974076 val loss: 0.369562
[Epoch 19] ogbg-molbbbp: 0.773656 test loss: 0.553096
[Epoch 20; Iter    18/   48] train: loss: 0.3563314
[Epoch 20; Iter    48/   48] train: loss: 0.3276920
[Epoch 20] ogbg-molbbbp: 0.965748 val loss: 0.314343
[Epoch 20] ogbg-molbbbp: 0.769937 test loss: 0.578507
[Epoch 21; Iter    30/   48] train: loss: 0.3397556
[Epoch 21] ogbg-molbbbp: 0.955894 val loss: 0.321264
[Epoch 21] ogbg-molbbbp: 0.774373 test loss: 0.572305
[Epoch 22; Iter    12/   48] train: loss: 0.3617420
[Epoch 22; Iter    42/   48] train: loss: 0.3412688
[Epoch 22] ogbg-molbbbp: 0.925748 val loss: 0.194794
[Epoch 22] ogbg-molbbbp: 0.766174 test loss: 0.760518
[Epoch 23; Iter    24/   48] train: loss: 0.3172962
[Epoch 23] ogbg-molbbbp: 0.984633 val loss: 0.150130
[Epoch 23] ogbg-molbbbp: 0.800762 test loss: 0.706049
[Epoch 24; Iter     6/   48] train: loss: 0.2003821
[Epoch 24; Iter    36/   48] train: loss: 0.3771253
[Epoch 24] ogbg-molbbbp: 0.965044 val loss: 0.165788
[Epoch 24] ogbg-molbbbp: 0.792876 test loss: 0.750417
[Epoch 25; Iter    18/   48] train: loss: 0.3183318
[Epoch 25; Iter    48/   48] train: loss: 0.2070773
[Epoch 25] ogbg-molbbbp: 0.961173 val loss: 0.192575
[Epoch 25] ogbg-molbbbp: 0.784901 test loss: 0.703742
[Epoch 26; Iter    30/   48] train: loss: 0.3276660
[Epoch 26] ogbg-molbbbp: 0.944047 val loss: 0.180319
[Epoch 26] ogbg-molbbbp: 0.771505 test loss: 0.820226
[Epoch 27; Iter    12/   48] train: loss: 0.1566227
[Epoch 27; Iter    42/   48] train: loss: 0.2323383
[Epoch 27] ogbg-molbbbp: 0.984751 val loss: 0.114032
[Epoch 27] ogbg-molbbbp: 0.782751 test loss: 0.947854
[Epoch 28; Iter    24/   48] train: loss: 0.2532763
[Epoch 28] ogbg-molbbbp: 0.974311 val loss: 0.137810
[Epoch 28] ogbg-molbbbp: 0.783647 test loss: 0.839420
[Epoch 29; Iter     6/   48] train: loss: 0.2504075
[Epoch 29; Iter    36/   48] train: loss: 0.1728624
[Epoch 29] ogbg-molbbbp: 0.980059 val loss: 0.170459
[Epoch 29] ogbg-molbbbp: 0.785663 test loss: 0.673398
[Epoch 30; Iter    18/   48] train: loss: 0.2011306
[Epoch 30; Iter    48/   48] train: loss: 0.1805956
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bbbp/scaff/train_prop=0.7/PNA_ogbg-molbbbp_GraphCL_bbbp_scaff=0.7_5_26-05_09-43-37
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bbbp_scaff=0.7
logdir: runs/split/GraphCL/bbbp/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6918203
[Epoch 1] ogbg-molbbbp: 0.685279 val loss: 0.690903
[Epoch 1] ogbg-molbbbp: 0.628943 test loss: 0.692029
[Epoch 2; Iter    12/   48] train: loss: 0.6905054
[Epoch 2; Iter    42/   48] train: loss: 0.6909816
[Epoch 2] ogbg-molbbbp: 0.635660 val loss: 0.688533
[Epoch 2] ogbg-molbbbp: 0.628136 test loss: 0.691773
[Epoch 3; Iter    24/   48] train: loss: 0.6920494
[Epoch 3] ogbg-molbbbp: 0.628035 val loss: 0.688454
[Epoch 3] ogbg-molbbbp: 0.608020 test loss: 0.692274
[Epoch 4; Iter     6/   48] train: loss: 0.6900611
[Epoch 4; Iter    36/   48] train: loss: 0.6924483
[Epoch 4] ogbg-molbbbp: 0.643167 val loss: 0.686923
[Epoch 4] ogbg-molbbbp: 0.617697 test loss: 0.692412
[Epoch 5; Iter    18/   48] train: loss: 0.6916550
[Epoch 5; Iter    48/   48] train: loss: 0.6882816
[Epoch 5] ogbg-molbbbp: 0.653021 val loss: 0.686169
[Epoch 5] ogbg-molbbbp: 0.625493 test loss: 0.692331
[Epoch 6; Iter    30/   48] train: loss: 0.6912886
[Epoch 6] ogbg-molbbbp: 0.667918 val loss: 0.684834
[Epoch 6] ogbg-molbbbp: 0.634722 test loss: 0.692601
[Epoch 7; Iter    12/   48] train: loss: 0.6896749
[Epoch 7; Iter    42/   48] train: loss: 0.6906480
[Epoch 7] ogbg-molbbbp: 0.693138 val loss: 0.683647
[Epoch 7] ogbg-molbbbp: 0.639964 test loss: 0.692224
[Epoch 8; Iter    24/   48] train: loss: 0.6893251
[Epoch 8] ogbg-molbbbp: 0.709208 val loss: 0.681240
[Epoch 8] ogbg-molbbbp: 0.641801 test loss: 0.692892
[Epoch 9; Iter     6/   48] train: loss: 0.6858118
[Epoch 9; Iter    36/   48] train: loss: 0.6873041
[Epoch 9] ogbg-molbbbp: 0.710029 val loss: 0.679360
[Epoch 9] ogbg-molbbbp: 0.644579 test loss: 0.693536
[Epoch 10; Iter    18/   48] train: loss: 0.6860488
[Epoch 10; Iter    48/   48] train: loss: 0.6833835
[Epoch 10] ogbg-molbbbp: 0.709912 val loss: 0.676571
[Epoch 10] ogbg-molbbbp: 0.654749 test loss: 0.694000
[Epoch 11; Iter    30/   48] train: loss: 0.6838430
[Epoch 11] ogbg-molbbbp: 0.724223 val loss: 0.673805
[Epoch 11] ogbg-molbbbp: 0.657034 test loss: 0.694613
[Epoch 12; Iter    12/   48] train: loss: 0.6783108
[Epoch 12; Iter    42/   48] train: loss: 0.6788939
[Epoch 12] ogbg-molbbbp: 0.743109 val loss: 0.671027
[Epoch 12] ogbg-molbbbp: 0.663799 test loss: 0.695206
[Epoch 13; Iter    24/   48] train: loss: 0.6788905
[Epoch 13] ogbg-molbbbp: 0.769736 val loss: 0.667394
[Epoch 13] ogbg-molbbbp: 0.671505 test loss: 0.695506
[Epoch 14; Iter     6/   48] train: loss: 0.6765576
[Epoch 14; Iter    36/   48] train: loss: 0.6756955
[Epoch 14] ogbg-molbbbp: 0.777361 val loss: 0.664942
[Epoch 14] ogbg-molbbbp: 0.675134 test loss: 0.696175
[Epoch 15; Iter    18/   48] train: loss: 0.6730422
[Epoch 15; Iter    48/   48] train: loss: 0.6390493
[Epoch 15] ogbg-molbbbp: 0.960587 val loss: 0.622344
[Epoch 15] ogbg-molbbbp: 0.773029 test loss: 0.651376
[Epoch 16; Iter    30/   48] train: loss: 0.6204489
[Epoch 16] ogbg-molbbbp: 0.963050 val loss: 0.553495
[Epoch 16] ogbg-molbbbp: 0.780600 test loss: 0.573879
[Epoch 17; Iter    12/   48] train: loss: 0.5762127
[Epoch 17; Iter    42/   48] train: loss: 0.5333180
[Epoch 17] ogbg-molbbbp: 0.963519 val loss: 0.511751
[Epoch 17] ogbg-molbbbp: 0.783020 test loss: 0.522131
[Epoch 18; Iter    24/   48] train: loss: 0.4273360
[Epoch 18] ogbg-molbbbp: 0.953900 val loss: 0.513702
[Epoch 18] ogbg-molbbbp: 0.777554 test loss: 0.529652
[Epoch 19; Iter     6/   48] train: loss: 0.4583009
[Epoch 19; Iter    36/   48] train: loss: 0.4523845
[Epoch 19] ogbg-molbbbp: 0.972082 val loss: 0.400603
[Epoch 19] ogbg-molbbbp: 0.778629 test loss: 0.539760
[Epoch 20; Iter    18/   48] train: loss: 0.3440038
[Epoch 20; Iter    48/   48] train: loss: 0.3285626
[Epoch 20] ogbg-molbbbp: 0.975249 val loss: 0.300550
[Epoch 20] ogbg-molbbbp: 0.792294 test loss: 0.567560
[Epoch 21; Iter    30/   48] train: loss: 0.3474248
[Epoch 21] ogbg-molbbbp: 0.967273 val loss: 0.233647
[Epoch 21] ogbg-molbbbp: 0.783244 test loss: 0.610560
[Epoch 22; Iter    12/   48] train: loss: 0.3187206
[Epoch 22; Iter    42/   48] train: loss: 0.3073172
[Epoch 22] ogbg-molbbbp: 0.844340 val loss: 0.305221
[Epoch 22] ogbg-molbbbp: 0.737142 test loss: 0.720414
[Epoch 23; Iter    24/   48] train: loss: 0.3666099
[Epoch 23] ogbg-molbbbp: 0.977126 val loss: 0.146202
[Epoch 23] ogbg-molbbbp: 0.794220 test loss: 0.798069
[Epoch 24; Iter     6/   48] train: loss: 0.1915685
[Epoch 24; Iter    36/   48] train: loss: 0.3343111
[Epoch 24] ogbg-molbbbp: 0.977243 val loss: 0.178220
[Epoch 24] ogbg-molbbbp: 0.789651 test loss: 0.705084
[Epoch 25; Iter    18/   48] train: loss: 0.2513284
[Epoch 25; Iter    48/   48] train: loss: 0.3215640
[Epoch 25] ogbg-molbbbp: 0.969267 val loss: 0.188350
[Epoch 25] ogbg-molbbbp: 0.779391 test loss: 0.706971
[Epoch 26; Iter    30/   48] train: loss: 0.1175294
[Epoch 26] ogbg-molbbbp: 0.974311 val loss: 0.157567
[Epoch 26] ogbg-molbbbp: 0.791263 test loss: 0.739857
[Epoch 27; Iter    12/   48] train: loss: 0.3049795
[Epoch 27; Iter    42/   48] train: loss: 0.2783884
[Epoch 27] ogbg-molbbbp: 0.976070 val loss: 0.186198
[Epoch 27] ogbg-molbbbp: 0.791353 test loss: 0.679670
[Epoch 28; Iter    24/   48] train: loss: 0.1669649
[Epoch 28] ogbg-molbbbp: 0.984164 val loss: 0.207004
[Epoch 28] ogbg-molbbbp: 0.794848 test loss: 0.628632
[Epoch 29; Iter     6/   48] train: loss: 0.3057412
[Epoch 29; Iter    36/   48] train: loss: 0.3057329
[Epoch 29] ogbg-molbbbp: 0.974428 val loss: 0.204668
[Epoch 29] ogbg-molbbbp: 0.780108 test loss: 0.668019
[Epoch 30; Iter    18/   48] train: loss: 0.1328786
[Epoch 30; Iter    48/   48] train: loss: 0.3330691
[Epoch 28] ogbg-molbbbp: 0.695795 test loss: 0.834424
[Epoch 29; Iter    20/   55] train: loss: 0.2572624
[Epoch 29; Iter    50/   55] train: loss: 0.3511262
[Epoch 29] ogbg-molbbbp: 0.956985 val loss: 0.447576
[Epoch 29] ogbg-molbbbp: 0.699942 test loss: 1.446907
[Epoch 30; Iter    25/   55] train: loss: 0.1621436
[Epoch 30; Iter    55/   55] train: loss: 0.4067048
[Epoch 30] ogbg-molbbbp: 0.944837 val loss: 0.457208
[Epoch 30] ogbg-molbbbp: 0.707369 test loss: 1.020358
[Epoch 31; Iter    30/   55] train: loss: 0.2758201
[Epoch 31] ogbg-molbbbp: 0.939859 val loss: 0.463213
[Epoch 31] ogbg-molbbbp: 0.654996 test loss: 1.271590
[Epoch 32; Iter     5/   55] train: loss: 0.1420801
[Epoch 32; Iter    35/   55] train: loss: 0.1069882
[Epoch 32] ogbg-molbbbp: 0.968834 val loss: 0.245653
[Epoch 32] ogbg-molbbbp: 0.686535 test loss: 1.034900
[Epoch 33; Iter    10/   55] train: loss: 0.1383043
[Epoch 33; Iter    40/   55] train: loss: 0.1963476
[Epoch 33] ogbg-molbbbp: 0.951309 val loss: 0.283978
[Epoch 33] ogbg-molbbbp: 0.699171 test loss: 1.073332
[Epoch 34; Iter    15/   55] train: loss: 0.3253691
[Epoch 34; Iter    45/   55] train: loss: 0.3529561
[Epoch 34] ogbg-molbbbp: 0.962163 val loss: 0.266756
[Epoch 34] ogbg-molbbbp: 0.688850 test loss: 1.098710
[Epoch 35; Iter    20/   55] train: loss: 0.2030814
[Epoch 35; Iter    50/   55] train: loss: 0.3046133
[Epoch 35] ogbg-molbbbp: 0.963855 val loss: 0.302603
[Epoch 35] ogbg-molbbbp: 0.680556 test loss: 1.048169
[Epoch 36; Iter    25/   55] train: loss: 0.3347963
[Epoch 36; Iter    55/   55] train: loss: 0.2056999
[Epoch 36] ogbg-molbbbp: 0.942149 val loss: 0.363768
[Epoch 36] ogbg-molbbbp: 0.670718 test loss: 1.105514
[Epoch 37; Iter    30/   55] train: loss: 0.2018259
[Epoch 37] ogbg-molbbbp: 0.928109 val loss: 0.485658
[Epoch 37] ogbg-molbbbp: 0.694348 test loss: 1.058980
[Epoch 38; Iter     5/   55] train: loss: 0.1643714
[Epoch 38; Iter    35/   55] train: loss: 0.1252498
[Epoch 38] ogbg-molbbbp: 0.955093 val loss: 0.291118
[Epoch 38] ogbg-molbbbp: 0.656250 test loss: 1.200496
[Epoch 39; Iter    10/   55] train: loss: 0.0806185
[Epoch 39; Iter    40/   55] train: loss: 0.0790977
[Epoch 39] ogbg-molbbbp: 0.888081 val loss: 0.709347
[Epoch 39] ogbg-molbbbp: 0.627701 test loss: 1.217126
[Epoch 40; Iter    15/   55] train: loss: 0.1007071
[Epoch 40; Iter    45/   55] train: loss: 0.3435299
[Epoch 40] ogbg-molbbbp: 0.962461 val loss: 0.276329
[Epoch 40] ogbg-molbbbp: 0.702643 test loss: 1.096844
[Epoch 41; Iter    20/   55] train: loss: 0.2115120
[Epoch 41; Iter    50/   55] train: loss: 0.0840942
[Epoch 41] ogbg-molbbbp: 0.951409 val loss: 0.330381
[Epoch 41] ogbg-molbbbp: 0.680556 test loss: 1.161217
[Epoch 42; Iter    25/   55] train: loss: 0.1520083
[Epoch 42; Iter    55/   55] train: loss: 0.0405774
[Epoch 42] ogbg-molbbbp: 0.948422 val loss: 0.409877
[Epoch 42] ogbg-molbbbp: 0.688175 test loss: 1.327154
[Epoch 43; Iter    30/   55] train: loss: 0.1313603
[Epoch 43] ogbg-molbbbp: 0.944439 val loss: 0.348215
[Epoch 43] ogbg-molbbbp: 0.672550 test loss: 1.080322
[Epoch 44; Iter     5/   55] train: loss: 0.0986983
[Epoch 44; Iter    35/   55] train: loss: 0.0326762
[Epoch 44] ogbg-molbbbp: 0.956587 val loss: 0.341649
[Epoch 44] ogbg-molbbbp: 0.688079 test loss: 1.357454
[Epoch 45; Iter    10/   55] train: loss: 0.0773007
[Epoch 45; Iter    40/   55] train: loss: 0.1650857
[Epoch 45] ogbg-molbbbp: 0.934382 val loss: 0.428431
[Epoch 45] ogbg-molbbbp: 0.684510 test loss: 1.087497
[Epoch 46; Iter    15/   55] train: loss: 0.2775276
[Epoch 46; Iter    45/   55] train: loss: 0.1999334
[Epoch 46] ogbg-molbbbp: 0.928607 val loss: 0.635860
[Epoch 46] ogbg-molbbbp: 0.634356 test loss: 1.464355
[Epoch 47; Iter    20/   55] train: loss: 0.1268014
[Epoch 47; Iter    50/   55] train: loss: 0.1315223
[Epoch 47] ogbg-molbbbp: 0.952504 val loss: 0.349989
[Epoch 47] ogbg-molbbbp: 0.676022 test loss: 1.393708
[Epoch 48; Iter    25/   55] train: loss: 0.0753964
[Epoch 48; Iter    55/   55] train: loss: 0.0836827
[Epoch 48] ogbg-molbbbp: 0.932092 val loss: 0.470680
[Epoch 48] ogbg-molbbbp: 0.630305 test loss: 1.501559
[Epoch 49; Iter    30/   55] train: loss: 0.1392997
[Epoch 49] ogbg-molbbbp: 0.933386 val loss: 0.479708
[Epoch 49] ogbg-molbbbp: 0.661458 test loss: 1.265371
[Epoch 50; Iter     5/   55] train: loss: 0.1838287
[Epoch 50; Iter    35/   55] train: loss: 0.0273761
[Epoch 50] ogbg-molbbbp: 0.949119 val loss: 0.435812
[Epoch 50] ogbg-molbbbp: 0.666956 test loss: 1.537112
[Epoch 51; Iter    10/   55] train: loss: 0.0609581
[Epoch 51; Iter    40/   55] train: loss: 0.1277563
[Epoch 51] ogbg-molbbbp: 0.934083 val loss: 0.615827
[Epoch 51] ogbg-molbbbp: 0.631559 test loss: 2.056944
[Epoch 52; Iter    15/   55] train: loss: 0.0603076
[Epoch 52; Iter    45/   55] train: loss: 0.0691083
[Epoch 52] ogbg-molbbbp: 0.957184 val loss: 0.412794
[Epoch 52] ogbg-molbbbp: 0.650945 test loss: 1.574086
[Epoch 53; Iter    20/   55] train: loss: 0.1502081
[Epoch 53; Iter    50/   55] train: loss: 0.1030487
[Epoch 53] ogbg-molbbbp: 0.931096 val loss: 0.599022
[Epoch 53] ogbg-molbbbp: 0.640721 test loss: 1.528351
[Epoch 54; Iter    25/   55] train: loss: 0.0466247
[Epoch 54; Iter    55/   55] train: loss: 0.2046984
[Epoch 54] ogbg-molbbbp: 0.903515 val loss: 0.859978
[Epoch 54] ogbg-molbbbp: 0.633681 test loss: 1.551117
[Epoch 55; Iter    30/   55] train: loss: 0.0447564
[Epoch 55] ogbg-molbbbp: 0.958678 val loss: 0.354425
[Epoch 55] ogbg-molbbbp: 0.687404 test loss: 1.457832
[Epoch 56; Iter     5/   55] train: loss: 0.0154964
[Epoch 56; Iter    35/   55] train: loss: 0.1473507
[Epoch 56] ogbg-molbbbp: 0.961665 val loss: 0.353358
[Epoch 56] ogbg-molbbbp: 0.660590 test loss: 1.592314
[Epoch 57; Iter    10/   55] train: loss: 0.0713173
[Epoch 57; Iter    40/   55] train: loss: 0.0517145
[Epoch 57] ogbg-molbbbp: 0.953998 val loss: 0.430242
[Epoch 57] ogbg-molbbbp: 0.658854 test loss: 1.445042
[Epoch 58; Iter    15/   55] train: loss: 0.1115530
[Epoch 58; Iter    45/   55] train: loss: 0.0788312
[Epoch 58] ogbg-molbbbp: 0.937469 val loss: 0.529185
[Epoch 58] ogbg-molbbbp: 0.671296 test loss: 1.539282
[Epoch 59; Iter    20/   55] train: loss: 0.0549490
[Epoch 59; Iter    50/   55] train: loss: 0.1057086
[Epoch 59] ogbg-molbbbp: 0.965648 val loss: 0.306624
[Epoch 59] ogbg-molbbbp: 0.675733 test loss: 1.585930
[Epoch 60; Iter    25/   55] train: loss: 0.0799826
[Epoch 60; Iter    55/   55] train: loss: 0.1453222
[Epoch 60] ogbg-molbbbp: 0.948721 val loss: 0.624203
[Epoch 60] ogbg-molbbbp: 0.633584 test loss: 2.166641
[Epoch 61; Iter    30/   55] train: loss: 0.0605494
[Epoch 61] ogbg-molbbbp: 0.956388 val loss: 0.393805
[Epoch 61] ogbg-molbbbp: 0.682967 test loss: 1.506445
[Epoch 62; Iter     5/   55] train: loss: 0.0349184
[Epoch 62; Iter    35/   55] train: loss: 0.1296166
[Epoch 62] ogbg-molbbbp: 0.952305 val loss: 0.424906
[Epoch 62] ogbg-molbbbp: 0.649981 test loss: 1.519916
[Epoch 63; Iter    10/   55] train: loss: 0.0766734
[Epoch 63; Iter    40/   55] train: loss: 0.0768832
[Epoch 63] ogbg-molbbbp: 0.948920 val loss: 0.439560
[Epoch 63] ogbg-molbbbp: 0.647087 test loss: 1.545591
[Epoch 64; Iter    15/   55] train: loss: 0.0876850
[Epoch 64; Iter    45/   55] train: loss: 0.0970817
[Epoch 64] ogbg-molbbbp: 0.940058 val loss: 0.726170
[Epoch 64] ogbg-molbbbp: 0.668210 test loss: 2.010318
[Epoch 65; Iter    20/   55] train: loss: 0.1442471
[Epoch 65; Iter    50/   55] train: loss: 0.1122759
[Epoch 65] ogbg-molbbbp: 0.953898 val loss: 0.429718
[Epoch 65] ogbg-molbbbp: 0.675154 test loss: 1.420679
[Epoch 66; Iter    25/   55] train: loss: 0.0255534
[Epoch 66; Iter    55/   55] train: loss: 0.0501235
[Epoch 66] ogbg-molbbbp: 0.957483 val loss: 0.387189
[Epoch 66] ogbg-molbbbp: 0.665702 test loss: 1.673984
[Epoch 67; Iter    30/   55] train: loss: 0.0203808
[Epoch 67] ogbg-molbbbp: 0.961366 val loss: 0.360410
[Epoch 67] ogbg-molbbbp: 0.687596 test loss: 1.568069
[Epoch 68; Iter     5/   55] train: loss: 0.0212479
[Epoch 68; Iter    35/   55] train: loss: 0.2186521
[Epoch 68] ogbg-molbbbp: 0.945634 val loss: 0.477884
[Epoch 68] ogbg-molbbbp: 0.623457 test loss: 1.700280
[Epoch 69; Iter    10/   55] train: loss: 0.1154042
[Epoch 28] ogbg-molbbbp: 0.706597 test loss: 5.356084
[Epoch 29; Iter    20/   55] train: loss: 0.1391781
[Epoch 29; Iter    50/   55] train: loss: 0.2384129
[Epoch 29] ogbg-molbbbp: 0.665936 val loss: 40.700312
[Epoch 29] ogbg-molbbbp: 0.591242 test loss: 23.540148
[Epoch 30; Iter    25/   55] train: loss: 0.2685767
[Epoch 30; Iter    55/   55] train: loss: 0.3560107
[Epoch 30] ogbg-molbbbp: 0.933287 val loss: 0.410964
[Epoch 30] ogbg-molbbbp: 0.707948 test loss: 0.815034
[Epoch 31; Iter    30/   55] train: loss: 0.2625220
[Epoch 31] ogbg-molbbbp: 0.942149 val loss: 0.344841
[Epoch 31] ogbg-molbbbp: 0.703607 test loss: 0.918413
[Epoch 32; Iter     5/   55] train: loss: 0.2587498
[Epoch 32; Iter    35/   55] train: loss: 0.3643477
[Epoch 32] ogbg-molbbbp: 0.955790 val loss: 0.285029
[Epoch 32] ogbg-molbbbp: 0.678048 test loss: 0.987085
[Epoch 33; Iter    10/   55] train: loss: 0.2192195
[Epoch 33; Iter    40/   55] train: loss: 0.2873611
[Epoch 33] ogbg-molbbbp: 0.941053 val loss: 0.489459
[Epoch 33] ogbg-molbbbp: 0.699846 test loss: 1.191136
[Epoch 34; Iter    15/   55] train: loss: 0.0950423
[Epoch 34; Iter    45/   55] train: loss: 0.2651731
[Epoch 34] ogbg-molbbbp: 0.959076 val loss: 0.275057
[Epoch 34] ogbg-molbbbp: 0.715085 test loss: 1.033680
[Epoch 35; Iter    20/   55] train: loss: 0.1461640
[Epoch 35; Iter    50/   55] train: loss: 0.1584471
[Epoch 35] ogbg-molbbbp: 0.935278 val loss: 0.485012
[Epoch 35] ogbg-molbbbp: 0.662905 test loss: 1.341902
[Epoch 36; Iter    25/   55] train: loss: 0.3208841
[Epoch 36; Iter    55/   55] train: loss: 0.7168037
[Epoch 36] ogbg-molbbbp: 0.924126 val loss: 0.563353
[Epoch 36] ogbg-molbbbp: 0.666088 test loss: 1.264344
[Epoch 37; Iter    30/   55] train: loss: 0.1310593
[Epoch 37] ogbg-molbbbp: 0.917156 val loss: 0.469411
[Epoch 37] ogbg-molbbbp: 0.660301 test loss: 1.069549
[Epoch 38; Iter     5/   55] train: loss: 0.1820215
[Epoch 38; Iter    35/   55] train: loss: 0.1267001
[Epoch 38] ogbg-molbbbp: 0.954297 val loss: 0.327341
[Epoch 38] ogbg-molbbbp: 0.694155 test loss: 1.334323
[Epoch 39; Iter    10/   55] train: loss: 0.0897391
[Epoch 39; Iter    40/   55] train: loss: 0.1390057
[Epoch 39] ogbg-molbbbp: 0.953500 val loss: 0.318584
[Epoch 39] ogbg-molbbbp: 0.650849 test loss: 1.284735
[Epoch 40; Iter    15/   55] train: loss: 0.0614541
[Epoch 40; Iter    45/   55] train: loss: 0.2649996
[Epoch 40] ogbg-molbbbp: 0.964353 val loss: 0.302158
[Epoch 40] ogbg-molbbbp: 0.708719 test loss: 1.236288
[Epoch 41; Iter    20/   55] train: loss: 0.1436417
[Epoch 41; Iter    50/   55] train: loss: 0.1790774
[Epoch 41] ogbg-molbbbp: 0.966942 val loss: 0.251464
[Epoch 41] ogbg-molbbbp: 0.698688 test loss: 1.169841
[Epoch 42; Iter    25/   55] train: loss: 0.1466627
[Epoch 42; Iter    55/   55] train: loss: 0.3142493
[Epoch 42] ogbg-molbbbp: 0.962959 val loss: 0.300568
[Epoch 42] ogbg-molbbbp: 0.699267 test loss: 1.271651
[Epoch 43; Iter    30/   55] train: loss: 0.1952517
[Epoch 43] ogbg-molbbbp: 0.939659 val loss: 0.535466
[Epoch 43] ogbg-molbbbp: 0.649402 test loss: 1.363938
[Epoch 44; Iter     5/   55] train: loss: 0.2165106
[Epoch 44; Iter    35/   55] train: loss: 0.0842070
[Epoch 44] ogbg-molbbbp: 0.963557 val loss: 0.251844
[Epoch 44] ogbg-molbbbp: 0.702546 test loss: 1.110997
[Epoch 45; Iter    10/   55] train: loss: 0.1741407
[Epoch 45; Iter    40/   55] train: loss: 0.2071454
[Epoch 45] ogbg-molbbbp: 0.952903 val loss: 0.349096
[Epoch 45] ogbg-molbbbp: 0.689140 test loss: 1.243377
[Epoch 46; Iter    15/   55] train: loss: 0.0773660
[Epoch 46; Iter    45/   55] train: loss: 0.1945463
[Epoch 46] ogbg-molbbbp: 0.946928 val loss: 0.429047
[Epoch 46] ogbg-molbbbp: 0.690490 test loss: 1.232244
[Epoch 47; Iter    20/   55] train: loss: 0.1824735
[Epoch 47; Iter    50/   55] train: loss: 0.1447271
[Epoch 47] ogbg-molbbbp: 0.969631 val loss: 0.255613
[Epoch 47] ogbg-molbbbp: 0.701389 test loss: 1.135471
[Epoch 48; Iter    25/   55] train: loss: 0.2425041
[Epoch 48; Iter    55/   55] train: loss: 0.1118491
[Epoch 48] ogbg-molbbbp: 0.960669 val loss: 0.290037
[Epoch 48] ogbg-molbbbp: 0.682195 test loss: 1.110738
[Epoch 49; Iter    30/   55] train: loss: 0.0738671
[Epoch 49] ogbg-molbbbp: 0.968734 val loss: 0.275217
[Epoch 49] ogbg-molbbbp: 0.687596 test loss: 1.383441
[Epoch 50; Iter     5/   55] train: loss: 0.0328903
[Epoch 50; Iter    35/   55] train: loss: 0.0660565
[Epoch 50] ogbg-molbbbp: 0.954197 val loss: 0.372291
[Epoch 50] ogbg-molbbbp: 0.677276 test loss: 1.273064
[Epoch 51; Iter    10/   55] train: loss: 0.1451700
[Epoch 51; Iter    40/   55] train: loss: 0.0711897
[Epoch 51] ogbg-molbbbp: 0.948023 val loss: 0.629660
[Epoch 51] ogbg-molbbbp: 0.688079 test loss: 1.910464
[Epoch 52; Iter    15/   55] train: loss: 0.0522832
[Epoch 52; Iter    45/   55] train: loss: 0.1321314
[Epoch 52] ogbg-molbbbp: 0.972219 val loss: 0.223336
[Epoch 52] ogbg-molbbbp: 0.669464 test loss: 1.188095
[Epoch 53; Iter    20/   55] train: loss: 0.2139199
[Epoch 53; Iter    50/   55] train: loss: 0.1787579
[Epoch 53] ogbg-molbbbp: 0.966444 val loss: 0.299866
[Epoch 53] ogbg-molbbbp: 0.708430 test loss: 1.242184
[Epoch 54; Iter    25/   55] train: loss: 0.0622222
[Epoch 54; Iter    55/   55] train: loss: 0.0474324
[Epoch 54] ogbg-molbbbp: 0.970825 val loss: 0.288635
[Epoch 54] ogbg-molbbbp: 0.710745 test loss: 1.324186
[Epoch 55; Iter    30/   55] train: loss: 0.0638814
[Epoch 55] ogbg-molbbbp: 0.963059 val loss: 0.305986
[Epoch 55] ogbg-molbbbp: 0.694830 test loss: 1.378956
[Epoch 56; Iter     5/   55] train: loss: 0.1439508
[Epoch 56; Iter    35/   55] train: loss: 0.0498423
[Epoch 56] ogbg-molbbbp: 0.965747 val loss: 0.335431
[Epoch 56] ogbg-molbbbp: 0.705150 test loss: 1.430857
[Epoch 57; Iter    10/   55] train: loss: 0.1020616
[Epoch 57; Iter    40/   55] train: loss: 0.2433648
[Epoch 57] ogbg-molbbbp: 0.946331 val loss: 0.566647
[Epoch 57] ogbg-molbbbp: 0.709394 test loss: 1.634215
[Epoch 58; Iter    15/   55] train: loss: 0.0625066
[Epoch 58; Iter    45/   55] train: loss: 0.0278533
[Epoch 58] ogbg-molbbbp: 0.958080 val loss: 0.401358
[Epoch 58] ogbg-molbbbp: 0.695698 test loss: 1.547846
[Epoch 59; Iter    20/   55] train: loss: 0.2554801
[Epoch 59; Iter    50/   55] train: loss: 0.1521993
[Epoch 59] ogbg-molbbbp: 0.941253 val loss: 0.464504
[Epoch 59] ogbg-molbbbp: 0.670814 test loss: 1.682005
[Epoch 60; Iter    25/   55] train: loss: 0.0776599
[Epoch 60; Iter    55/   55] train: loss: 0.5255183
[Epoch 60] ogbg-molbbbp: 0.943642 val loss: 0.515669
[Epoch 60] ogbg-molbbbp: 0.707851 test loss: 1.700519
[Epoch 61; Iter    30/   55] train: loss: 0.1060924
[Epoch 61] ogbg-molbbbp: 0.967639 val loss: 0.304963
[Epoch 61] ogbg-molbbbp: 0.685185 test loss: 1.542782
[Epoch 62; Iter     5/   55] train: loss: 0.0394186
[Epoch 62; Iter    35/   55] train: loss: 0.1073775
[Epoch 62] ogbg-molbbbp: 0.966544 val loss: 0.355979
[Epoch 62] ogbg-molbbbp: 0.685089 test loss: 1.677297
[Epoch 63; Iter    10/   55] train: loss: 0.0665461
[Epoch 63; Iter    40/   55] train: loss: 0.0504424
[Epoch 63] ogbg-molbbbp: 0.970427 val loss: 0.269676
[Epoch 63] ogbg-molbbbp: 0.697145 test loss: 1.474224
[Epoch 64; Iter    15/   55] train: loss: 0.0346270
[Epoch 64; Iter    45/   55] train: loss: 0.1574691
[Epoch 64] ogbg-molbbbp: 0.964254 val loss: 0.350014
[Epoch 64] ogbg-molbbbp: 0.688657 test loss: 1.597573
[Epoch 65; Iter    20/   55] train: loss: 0.0349778
[Epoch 65; Iter    50/   55] train: loss: 0.0070580
[Epoch 65] ogbg-molbbbp: 0.962959 val loss: 0.382134
[Epoch 65] ogbg-molbbbp: 0.689043 test loss: 1.746097
[Epoch 66; Iter    25/   55] train: loss: 0.0647716
[Epoch 66; Iter    55/   55] train: loss: 0.0070498
[Epoch 66] ogbg-molbbbp: 0.968734 val loss: 0.368431
[Epoch 66] ogbg-molbbbp: 0.708140 test loss: 1.636652
[Epoch 67; Iter    30/   55] train: loss: 0.0148622
[Epoch 67] ogbg-molbbbp: 0.970626 val loss: 0.346775
[Epoch 67] ogbg-molbbbp: 0.708623 test loss: 1.703232
[Epoch 68; Iter     5/   55] train: loss: 0.1447750
[Epoch 68; Iter    35/   55] train: loss: 0.0152149
[Epoch 68] ogbg-molbbbp: 0.962561 val loss: 0.404299
[Epoch 68] ogbg-molbbbp: 0.690683 test loss: 1.638688
[Epoch 69; Iter    10/   55] train: loss: 0.0087729
[Epoch 30] ogbg-molbbbp: 0.984868 val loss: 0.174602
[Epoch 30] ogbg-molbbbp: 0.786738 test loss: 0.758387
[Epoch 31; Iter    30/   48] train: loss: 0.1897253
[Epoch 31] ogbg-molbbbp: 0.973607 val loss: 0.129425
[Epoch 31] ogbg-molbbbp: 0.786290 test loss: 0.874937
[Epoch 32; Iter    12/   48] train: loss: 0.1824716
[Epoch 32; Iter    42/   48] train: loss: 0.3089021
[Epoch 32] ogbg-molbbbp: 0.972669 val loss: 0.192759
[Epoch 32] ogbg-molbbbp: 0.794086 test loss: 0.690153
[Epoch 33; Iter    24/   48] train: loss: 0.3250464
[Epoch 33] ogbg-molbbbp: 0.968094 val loss: 0.293033
[Epoch 33] ogbg-molbbbp: 0.812545 test loss: 0.725619
[Epoch 34; Iter     6/   48] train: loss: 0.1871011
[Epoch 34; Iter    36/   48] train: loss: 0.1207759
[Epoch 34] ogbg-molbbbp: 0.957654 val loss: 0.181510
[Epoch 34] ogbg-molbbbp: 0.781541 test loss: 0.663985
[Epoch 35; Iter    18/   48] train: loss: 0.1219587
[Epoch 35; Iter    48/   48] train: loss: 0.2905451
[Epoch 35] ogbg-molbbbp: 0.856422 val loss: 0.227780
[Epoch 35] ogbg-molbbbp: 0.752330 test loss: 0.984162
[Epoch 36; Iter    30/   48] train: loss: 0.1936172
[Epoch 36] ogbg-molbbbp: 0.984985 val loss: 0.174800
[Epoch 36] ogbg-molbbbp: 0.792518 test loss: 0.674691
[Epoch 37; Iter    12/   48] train: loss: 0.4540844
[Epoch 37; Iter    42/   48] train: loss: 0.3013494
[Epoch 37] ogbg-molbbbp: 0.987097 val loss: 0.096146
[Epoch 37] ogbg-molbbbp: 0.795161 test loss: 0.840859
[Epoch 38; Iter    24/   48] train: loss: 0.2104794
[Epoch 38] ogbg-molbbbp: 0.990968 val loss: 0.212359
[Epoch 38] ogbg-molbbbp: 0.788351 test loss: 0.852584
[Epoch 39; Iter     6/   48] train: loss: 0.3239942
[Epoch 39; Iter    36/   48] train: loss: 0.3437927
[Epoch 39] ogbg-molbbbp: 0.985924 val loss: 0.253304
[Epoch 39] ogbg-molbbbp: 0.809095 test loss: 0.624340
[Epoch 40; Iter    18/   48] train: loss: 0.2610340
[Epoch 40; Iter    48/   48] train: loss: 0.4135394
[Epoch 40] ogbg-molbbbp: 0.974897 val loss: 0.145303
[Epoch 40] ogbg-molbbbp: 0.781855 test loss: 0.767730
[Epoch 41; Iter    30/   48] train: loss: 0.1556809
[Epoch 41] ogbg-molbbbp: 0.979707 val loss: 0.106021
[Epoch 41] ogbg-molbbbp: 0.772043 test loss: 0.932335
[Epoch 42; Iter    12/   48] train: loss: 0.0805276
[Epoch 42; Iter    42/   48] train: loss: 0.1177926
[Epoch 42] ogbg-molbbbp: 0.986041 val loss: 0.112046
[Epoch 42] ogbg-molbbbp: 0.814785 test loss: 0.680342
[Epoch 43; Iter    24/   48] train: loss: 0.2435253
[Epoch 43] ogbg-molbbbp: 0.990029 val loss: 0.097615
[Epoch 43] ogbg-molbbbp: 0.795520 test loss: 0.790314
[Epoch 44; Iter     6/   48] train: loss: 0.0498717
[Epoch 44; Iter    36/   48] train: loss: 0.3691072
[Epoch 44] ogbg-molbbbp: 0.989091 val loss: 0.109826
[Epoch 44] ogbg-molbbbp: 0.818056 test loss: 0.726384
[Epoch 45; Iter    18/   48] train: loss: 0.2072647
[Epoch 45; Iter    48/   48] train: loss: 0.7648426
[Epoch 45] ogbg-molbbbp: 0.918240 val loss: 0.163102
[Epoch 45] ogbg-molbbbp: 0.720789 test loss: 1.195827
[Epoch 46; Iter    30/   48] train: loss: 0.3718292
[Epoch 46] ogbg-molbbbp: 0.981935 val loss: 0.091344
[Epoch 46] ogbg-molbbbp: 0.800762 test loss: 0.995711
[Epoch 47; Iter    12/   48] train: loss: 0.1173398
[Epoch 47; Iter    42/   48] train: loss: 0.1704675
[Epoch 47] ogbg-molbbbp: 0.979707 val loss: 0.097593
[Epoch 47] ogbg-molbbbp: 0.786738 test loss: 0.903421
[Epoch 48; Iter    24/   48] train: loss: 0.2697891
[Epoch 48] ogbg-molbbbp: 0.987331 val loss: 0.151940
[Epoch 48] ogbg-molbbbp: 0.813217 test loss: 0.780456
[Epoch 49; Iter     6/   48] train: loss: 0.1840166
[Epoch 49; Iter    36/   48] train: loss: 0.1267842
[Epoch 49] ogbg-molbbbp: 0.992962 val loss: 0.082564
[Epoch 49] ogbg-molbbbp: 0.785125 test loss: 0.894636
[Epoch 50; Iter    18/   48] train: loss: 0.0449851
[Epoch 50; Iter    48/   48] train: loss: 0.4877729
[Epoch 50] ogbg-molbbbp: 0.973255 val loss: 0.117684
[Epoch 50] ogbg-molbbbp: 0.779525 test loss: 0.933225
[Epoch 51; Iter    30/   48] train: loss: 0.0753203
[Epoch 51] ogbg-molbbbp: 0.973372 val loss: 0.125313
[Epoch 51] ogbg-molbbbp: 0.796013 test loss: 0.892471
[Epoch 52; Iter    12/   48] train: loss: 0.1836001
[Epoch 52; Iter    42/   48] train: loss: 0.0907533
[Epoch 52] ogbg-molbbbp: 0.968211 val loss: 0.115007
[Epoch 52] ogbg-molbbbp: 0.770251 test loss: 1.028648
[Epoch 53; Iter    24/   48] train: loss: 0.1413291
[Epoch 53] ogbg-molbbbp: 0.980880 val loss: 0.085588
[Epoch 53] ogbg-molbbbp: 0.795161 test loss: 1.178476
[Epoch 54; Iter     6/   48] train: loss: 0.1628946
[Epoch 54; Iter    36/   48] train: loss: 0.3807477
[Epoch 54] ogbg-molbbbp: 0.992493 val loss: 0.263729
[Epoch 54] ogbg-molbbbp: 0.798656 test loss: 0.783716
[Epoch 55; Iter    18/   48] train: loss: 0.2009150
[Epoch 55; Iter    48/   48] train: loss: 0.2646889
[Epoch 55] ogbg-molbbbp: 0.986393 val loss: 0.134769
[Epoch 55] ogbg-molbbbp: 0.797581 test loss: 0.895549
[Epoch 56; Iter    30/   48] train: loss: 0.0523054
[Epoch 56] ogbg-molbbbp: 0.981349 val loss: 0.111262
[Epoch 56] ogbg-molbbbp: 0.797625 test loss: 0.962332
[Epoch 57; Iter    12/   48] train: loss: 0.1897295
[Epoch 57; Iter    42/   48] train: loss: 0.1787529
[Epoch 57] ogbg-molbbbp: 0.979472 val loss: 0.101677
[Epoch 57] ogbg-molbbbp: 0.791129 test loss: 1.208157
[Epoch 58; Iter    24/   48] train: loss: 0.1168158
[Epoch 58] ogbg-molbbbp: 0.977595 val loss: 0.110251
[Epoch 58] ogbg-molbbbp: 0.793459 test loss: 0.955509
[Epoch 59; Iter     6/   48] train: loss: 0.0524603
[Epoch 59; Iter    36/   48] train: loss: 0.0942027
[Epoch 59] ogbg-molbbbp: 0.982874 val loss: 0.190504
[Epoch 59] ogbg-molbbbp: 0.793369 test loss: 0.898870
[Epoch 60; Iter    18/   48] train: loss: 0.0649688
[Epoch 60; Iter    48/   48] train: loss: 0.1409840
[Epoch 60] ogbg-molbbbp: 0.975601 val loss: 0.106227
[Epoch 60] ogbg-molbbbp: 0.799418 test loss: 1.018845
[Epoch 61; Iter    30/   48] train: loss: 0.2685747
[Epoch 61] ogbg-molbbbp: 0.963871 val loss: 0.141334
[Epoch 61] ogbg-molbbbp: 0.781452 test loss: 1.070887
[Epoch 62; Iter    12/   48] train: loss: 0.0960866
[Epoch 62; Iter    42/   48] train: loss: 0.0380952
[Epoch 62] ogbg-molbbbp: 0.983930 val loss: 0.087936
[Epoch 62] ogbg-molbbbp: 0.789337 test loss: 1.212478
[Epoch 63; Iter    24/   48] train: loss: 0.0671853
[Epoch 63] ogbg-molbbbp: 0.971965 val loss: 0.175682
[Epoch 63] ogbg-molbbbp: 0.770341 test loss: 0.955612
[Epoch 64; Iter     6/   48] train: loss: 0.0814419
[Epoch 64; Iter    36/   48] train: loss: 0.1482804
[Epoch 64] ogbg-molbbbp: 0.911906 val loss: 0.736470
[Epoch 64] ogbg-molbbbp: 0.773611 test loss: 1.559317
[Epoch 65; Iter    18/   48] train: loss: 0.1078097
[Epoch 65; Iter    48/   48] train: loss: 0.2031581
[Epoch 65] ogbg-molbbbp: 0.966334 val loss: 0.113804
[Epoch 65] ogbg-molbbbp: 0.786604 test loss: 1.305424
[Epoch 66; Iter    30/   48] train: loss: 0.1374504
[Epoch 66] ogbg-molbbbp: 0.968680 val loss: 0.201620
[Epoch 66] ogbg-molbbbp: 0.774462 test loss: 1.106952
[Epoch 67; Iter    12/   48] train: loss: 0.1300589
[Epoch 67; Iter    42/   48] train: loss: 0.2192283
[Epoch 67] ogbg-molbbbp: 0.964106 val loss: 0.137690
[Epoch 67] ogbg-molbbbp: 0.765233 test loss: 0.922279
[Epoch 68; Iter    24/   48] train: loss: 0.0552853
[Epoch 68] ogbg-molbbbp: 0.977009 val loss: 0.131676
[Epoch 68] ogbg-molbbbp: 0.785797 test loss: 1.181375
[Epoch 69; Iter     6/   48] train: loss: 0.0114205
[Epoch 69; Iter    36/   48] train: loss: 0.0680828
[Epoch 69] ogbg-molbbbp: 0.964692 val loss: 0.107956
[Epoch 69] ogbg-molbbbp: 0.764158 test loss: 1.283719
[Epoch 70; Iter    18/   48] train: loss: 0.0851474
[Epoch 70; Iter    48/   48] train: loss: 0.1078796
[Epoch 70] ogbg-molbbbp: 0.977947 val loss: 0.133857
[Epoch 70] ogbg-molbbbp: 0.773297 test loss: 1.144804
[Epoch 71; Iter    30/   48] train: loss: 0.1682098
[Epoch 71] ogbg-molbbbp: 0.978416 val loss: 0.105603
[Epoch 71] ogbg-molbbbp: 0.786066 test loss: 1.235430
[Epoch 72; Iter    12/   48] train: loss: 0.0630059
[Epoch 72; Iter    42/   48] train: loss: 0.1293289
[Epoch 72] ogbg-molbbbp: 0.975484 val loss: 0.155003
[Epoch 72] ogbg-molbbbp: 0.787366 test loss: 1.087562
[Epoch 73; Iter    24/   48] train: loss: 0.1932244
[Epoch 73] ogbg-molbbbp: 0.990029 val loss: 0.100575
[Epoch 28] ogbg-molbbbp: 0.706983 test loss: 0.925503
[Epoch 29; Iter    20/   55] train: loss: 0.1297160
[Epoch 29; Iter    50/   55] train: loss: 0.3029278
[Epoch 29] ogbg-molbbbp: 0.931893 val loss: 0.433750
[Epoch 29] ogbg-molbbbp: 0.689815 test loss: 0.972341
[Epoch 30; Iter    25/   55] train: loss: 0.1840866
[Epoch 30; Iter    55/   55] train: loss: 0.1644728
[Epoch 30] ogbg-molbbbp: 0.944439 val loss: 0.319015
[Epoch 30] ogbg-molbbbp: 0.635899 test loss: 1.155672
[Epoch 31; Iter    30/   55] train: loss: 0.3910246
[Epoch 31] ogbg-molbbbp: 0.927213 val loss: 0.561637
[Epoch 31] ogbg-molbbbp: 0.625482 test loss: 1.340804
[Epoch 32; Iter     5/   55] train: loss: 0.1224753
[Epoch 32; Iter    35/   55] train: loss: 0.1822701
[Epoch 32] ogbg-molbbbp: 0.921338 val loss: 0.461787
[Epoch 32] ogbg-molbbbp: 0.673804 test loss: 0.914640
[Epoch 33; Iter    10/   55] train: loss: 0.1012750
[Epoch 33; Iter    40/   55] train: loss: 0.1100212
[Epoch 33] ogbg-molbbbp: 0.946331 val loss: 0.400627
[Epoch 33] ogbg-molbbbp: 0.726852 test loss: 1.128783
[Epoch 34; Iter    15/   55] train: loss: 0.5140907
[Epoch 34; Iter    45/   55] train: loss: 0.1057590
[Epoch 34] ogbg-molbbbp: 0.963855 val loss: 0.273490
[Epoch 34] ogbg-molbbbp: 0.702643 test loss: 1.090434
[Epoch 35; Iter    20/   55] train: loss: 0.1417210
[Epoch 35; Iter    50/   55] train: loss: 0.1859246
[Epoch 35] ogbg-molbbbp: 0.952405 val loss: 0.319161
[Epoch 35] ogbg-molbbbp: 0.698206 test loss: 1.219809
[Epoch 36; Iter    25/   55] train: loss: 0.2108904
[Epoch 36; Iter    55/   55] train: loss: 0.0938122
[Epoch 36] ogbg-molbbbp: 0.962262 val loss: 0.305504
[Epoch 36] ogbg-molbbbp: 0.717593 test loss: 1.092444
[Epoch 37; Iter    30/   55] train: loss: 0.4786937
[Epoch 37] ogbg-molbbbp: 0.932789 val loss: 0.415089
[Epoch 37] ogbg-molbbbp: 0.699074 test loss: 0.978603
[Epoch 38; Iter     5/   55] train: loss: 0.1098131
[Epoch 38; Iter    35/   55] train: loss: 0.1328393
[Epoch 38] ogbg-molbbbp: 0.971921 val loss: 0.243860
[Epoch 38] ogbg-molbbbp: 0.726755 test loss: 0.966725
[Epoch 39; Iter    10/   55] train: loss: 0.0976179
[Epoch 39; Iter    40/   55] train: loss: 0.1688185
[Epoch 39] ogbg-molbbbp: 0.958877 val loss: 0.325001
[Epoch 39] ogbg-molbbbp: 0.704090 test loss: 1.176862
[Epoch 40; Iter    15/   55] train: loss: 0.1660212
[Epoch 40; Iter    45/   55] train: loss: 0.0872102
[Epoch 40] ogbg-molbbbp: 0.968436 val loss: 0.256990
[Epoch 40] ogbg-molbbbp: 0.684317 test loss: 1.207931
[Epoch 41; Iter    20/   55] train: loss: 0.3117264
[Epoch 41; Iter    50/   55] train: loss: 0.0361628
[Epoch 41] ogbg-molbbbp: 0.960370 val loss: 0.330006
[Epoch 41] ogbg-molbbbp: 0.690104 test loss: 1.241251
[Epoch 42; Iter    25/   55] train: loss: 0.0669958
[Epoch 42; Iter    55/   55] train: loss: 0.2693722
[Epoch 42] ogbg-molbbbp: 0.956686 val loss: 0.297495
[Epoch 42] ogbg-molbbbp: 0.692612 test loss: 1.064860
[Epoch 43; Iter    30/   55] train: loss: 0.0803158
[Epoch 43] ogbg-molbbbp: 0.952903 val loss: 0.415743
[Epoch 43] ogbg-molbbbp: 0.709008 test loss: 1.113630
[Epoch 44; Iter     5/   55] train: loss: 0.1297622
[Epoch 44; Iter    35/   55] train: loss: 0.2188177
[Epoch 44] ogbg-molbbbp: 0.962461 val loss: 0.321451
[Epoch 44] ogbg-molbbbp: 0.703511 test loss: 1.464100
[Epoch 45; Iter    10/   55] train: loss: 0.1116227
[Epoch 45; Iter    40/   55] train: loss: 0.0580058
[Epoch 45] ogbg-molbbbp: 0.960669 val loss: 0.309379
[Epoch 45] ogbg-molbbbp: 0.674383 test loss: 1.185209
[Epoch 46; Iter    15/   55] train: loss: 0.2476228
[Epoch 46; Iter    45/   55] train: loss: 0.1618935
[Epoch 46] ogbg-molbbbp: 0.947824 val loss: 0.491959
[Epoch 46] ogbg-molbbbp: 0.704186 test loss: 1.551737
[Epoch 47; Iter    20/   55] train: loss: 0.2164210
[Epoch 47; Iter    50/   55] train: loss: 0.1407318
[Epoch 47] ogbg-molbbbp: 0.950513 val loss: 0.415920
[Epoch 47] ogbg-molbbbp: 0.668885 test loss: 1.395967
[Epoch 48; Iter    25/   55] train: loss: 0.0317342
[Epoch 48; Iter    55/   55] train: loss: 0.2673764
[Epoch 48] ogbg-molbbbp: 0.963258 val loss: 0.294513
[Epoch 48] ogbg-molbbbp: 0.658854 test loss: 1.389332
[Epoch 49; Iter    30/   55] train: loss: 0.1238773
[Epoch 49] ogbg-molbbbp: 0.960171 val loss: 0.293144
[Epoch 49] ogbg-molbbbp: 0.687693 test loss: 1.109277
[Epoch 50; Iter     5/   55] train: loss: 0.1353153
[Epoch 50; Iter    35/   55] train: loss: 0.0860732
[Epoch 50] ogbg-molbbbp: 0.946829 val loss: 0.429626
[Epoch 50] ogbg-molbbbp: 0.619020 test loss: 1.771556
[Epoch 51; Iter    10/   55] train: loss: 0.0652721
[Epoch 51; Iter    40/   55] train: loss: 0.1191994
[Epoch 51] ogbg-molbbbp: 0.945833 val loss: 0.629931
[Epoch 51] ogbg-molbbbp: 0.599344 test loss: 2.157666
[Epoch 52; Iter    15/   55] train: loss: 0.1286474
[Epoch 52; Iter    45/   55] train: loss: 0.0331154
[Epoch 52] ogbg-molbbbp: 0.957483 val loss: 0.346562
[Epoch 52] ogbg-molbbbp: 0.647569 test loss: 1.541018
[Epoch 53; Iter    20/   55] train: loss: 0.1343932
[Epoch 53; Iter    50/   55] train: loss: 0.0881102
[Epoch 53] ogbg-molbbbp: 0.943742 val loss: 0.400133
[Epoch 53] ogbg-molbbbp: 0.632137 test loss: 1.331212
[Epoch 54; Iter    25/   55] train: loss: 0.0255518
[Epoch 54; Iter    55/   55] train: loss: 0.0625148
[Epoch 54] ogbg-molbbbp: 0.909887 val loss: 0.795578
[Epoch 54] ogbg-molbbbp: 0.619695 test loss: 1.775690
[Epoch 55; Iter    30/   55] train: loss: 0.0572973
[Epoch 55] ogbg-molbbbp: 0.964154 val loss: 0.318687
[Epoch 55] ogbg-molbbbp: 0.647377 test loss: 1.526264
[Epoch 56; Iter     5/   55] train: loss: 0.0280947
[Epoch 56; Iter    35/   55] train: loss: 0.0483272
[Epoch 56] ogbg-molbbbp: 0.943344 val loss: 0.406894
[Epoch 56] ogbg-molbbbp: 0.642458 test loss: 1.578027
[Epoch 57; Iter    10/   55] train: loss: 0.0966681
[Epoch 57; Iter    40/   55] train: loss: 0.0996561
[Epoch 57] ogbg-molbbbp: 0.951011 val loss: 0.344831
[Epoch 57] ogbg-molbbbp: 0.653935 test loss: 1.219868
[Epoch 58; Iter    15/   55] train: loss: 0.1600789
[Epoch 58; Iter    45/   55] train: loss: 0.0557140
[Epoch 58] ogbg-molbbbp: 0.962561 val loss: 0.312102
[Epoch 58] ogbg-molbbbp: 0.660204 test loss: 1.450805
[Epoch 59; Iter    20/   55] train: loss: 0.2284932
[Epoch 59; Iter    50/   55] train: loss: 0.2446961
[Epoch 59] ogbg-molbbbp: 0.948422 val loss: 0.353973
[Epoch 59] ogbg-molbbbp: 0.665509 test loss: 1.050781
[Epoch 60; Iter    25/   55] train: loss: 0.0227791
[Epoch 60; Iter    55/   55] train: loss: 0.0616158
[Epoch 60] ogbg-molbbbp: 0.958279 val loss: 0.302592
[Epoch 60] ogbg-molbbbp: 0.656636 test loss: 1.324436
[Epoch 61; Iter    30/   55] train: loss: 0.0707805
[Epoch 61] ogbg-molbbbp: 0.957981 val loss: 0.404352
[Epoch 61] ogbg-molbbbp: 0.636285 test loss: 1.949182
[Epoch 62; Iter     5/   55] train: loss: 0.1535522
[Epoch 62; Iter    35/   55] train: loss: 0.0661519
[Epoch 62] ogbg-molbbbp: 0.949617 val loss: 0.428232
[Epoch 62] ogbg-molbbbp: 0.606674 test loss: 1.992187
[Epoch 63; Iter    10/   55] train: loss: 0.0140405
[Epoch 63; Iter    40/   55] train: loss: 0.1114818
[Epoch 63] ogbg-molbbbp: 0.967340 val loss: 0.306250
[Epoch 63] ogbg-molbbbp: 0.654032 test loss: 1.638757
[Epoch 64; Iter    15/   55] train: loss: 0.1074535
[Epoch 64; Iter    45/   55] train: loss: 0.0722127
[Epoch 64] ogbg-molbbbp: 0.944638 val loss: 0.470772
[Epoch 64] ogbg-molbbbp: 0.634066 test loss: 1.726568
[Epoch 65; Iter    20/   55] train: loss: 0.0324065
[Epoch 65; Iter    50/   55] train: loss: 0.0389975
[Epoch 65] ogbg-molbbbp: 0.966146 val loss: 0.362099
[Epoch 65] ogbg-molbbbp: 0.653453 test loss: 1.785397
[Epoch 66; Iter    25/   55] train: loss: 0.0110549
[Epoch 66; Iter    55/   55] train: loss: 0.0193336
[Epoch 66] ogbg-molbbbp: 0.949119 val loss: 0.481120
[Epoch 66] ogbg-molbbbp: 0.649788 test loss: 1.645578
[Epoch 67; Iter    30/   55] train: loss: 0.1192943
[Epoch 67] ogbg-molbbbp: 0.952703 val loss: 0.367051
[Epoch 67] ogbg-molbbbp: 0.642940 test loss: 1.568468
[Epoch 68; Iter     5/   55] train: loss: 0.0133253
[Epoch 68; Iter    35/   55] train: loss: 0.0206876
[Epoch 68] ogbg-molbbbp: 0.916658 val loss: 0.684924
[Epoch 68] ogbg-molbbbp: 0.598090 test loss: 2.162116
[Epoch 69; Iter    10/   55] train: loss: 0.1218441
[Epoch 30] ogbg-molbbbp: 0.851496 val loss: 0.277136
[Epoch 30] ogbg-molbbbp: 0.759229 test loss: 0.711472
[Epoch 31; Iter    30/   48] train: loss: 0.2631802
[Epoch 31] ogbg-molbbbp: 0.974663 val loss: 0.191937
[Epoch 31] ogbg-molbbbp: 0.790009 test loss: 0.617099
[Epoch 32; Iter    12/   48] train: loss: 0.1947701
[Epoch 32; Iter    42/   48] train: loss: 0.2103074
[Epoch 32] ogbg-molbbbp: 0.969384 val loss: 0.272174
[Epoch 32] ogbg-molbbbp: 0.788217 test loss: 0.579072
[Epoch 33; Iter    24/   48] train: loss: 0.1747306
[Epoch 33] ogbg-molbbbp: 0.963636 val loss: 0.232043
[Epoch 33] ogbg-molbbbp: 0.804928 test loss: 0.571377
[Epoch 34; Iter     6/   48] train: loss: 0.2547317
[Epoch 34; Iter    36/   48] train: loss: 0.2382129
[Epoch 34] ogbg-molbbbp: 0.969032 val loss: 0.109851
[Epoch 34] ogbg-molbbbp: 0.787231 test loss: 0.977522
[Epoch 35; Iter    18/   48] train: loss: 0.1815279
[Epoch 35; Iter    48/   48] train: loss: 0.1753986
[Epoch 35] ogbg-molbbbp: 0.980059 val loss: 0.136125
[Epoch 35] ogbg-molbbbp: 0.800134 test loss: 0.741092
[Epoch 36; Iter    30/   48] train: loss: 0.5961472
[Epoch 36] ogbg-molbbbp: 0.975601 val loss: 2.484289
[Epoch 36] ogbg-molbbbp: 0.738306 test loss: 3.103585
[Epoch 37; Iter    12/   48] train: loss: 0.3214452
[Epoch 37; Iter    42/   48] train: loss: 0.1480359
[Epoch 37] ogbg-molbbbp: 0.967977 val loss: 0.122569
[Epoch 37] ogbg-molbbbp: 0.755287 test loss: 1.075005
[Epoch 38; Iter    24/   48] train: loss: 0.4724330
[Epoch 38] ogbg-molbbbp: 0.977243 val loss: 0.154712
[Epoch 38] ogbg-molbbbp: 0.764382 test loss: 0.826880
[Epoch 39; Iter     6/   48] train: loss: 0.2946133
[Epoch 39; Iter    36/   48] train: loss: 0.2253290
[Epoch 39] ogbg-molbbbp: 0.984633 val loss: 0.245336
[Epoch 39] ogbg-molbbbp: 0.780735 test loss: 0.702884
[Epoch 40; Iter    18/   48] train: loss: 0.2160842
[Epoch 40; Iter    48/   48] train: loss: 0.2554067
[Epoch 40] ogbg-molbbbp: 0.962229 val loss: 0.200571
[Epoch 40] ogbg-molbbbp: 0.772222 test loss: 0.821688
[Epoch 41; Iter    30/   48] train: loss: 0.0694198
[Epoch 41] ogbg-molbbbp: 0.986041 val loss: 0.093657
[Epoch 41] ogbg-molbbbp: 0.792339 test loss: 0.931300
[Epoch 42; Iter    12/   48] train: loss: 0.2198894
[Epoch 42; Iter    42/   48] train: loss: 0.2824280
[Epoch 42] ogbg-molbbbp: 0.970088 val loss: 0.172896
[Epoch 42] ogbg-molbbbp: 0.801254 test loss: 0.662347
[Epoch 43; Iter    24/   48] train: loss: 0.3545206
[Epoch 43] ogbg-molbbbp: 0.971730 val loss: 0.123200
[Epoch 43] ogbg-molbbbp: 0.786918 test loss: 1.118003
[Epoch 44; Iter     6/   48] train: loss: 0.1491479
[Epoch 44; Iter    36/   48] train: loss: 0.2348278
[Epoch 44] ogbg-molbbbp: 0.950499 val loss: 0.157434
[Epoch 44] ogbg-molbbbp: 0.783199 test loss: 0.801908
[Epoch 45; Iter    18/   48] train: loss: 0.2039933
[Epoch 45; Iter    48/   48] train: loss: 0.2078690
[Epoch 45] ogbg-molbbbp: 0.952610 val loss: 0.148614
[Epoch 45] ogbg-molbbbp: 0.797133 test loss: 0.860917
[Epoch 46; Iter    30/   48] train: loss: 0.1754207
[Epoch 46] ogbg-molbbbp: 0.964692 val loss: 0.148568
[Epoch 46] ogbg-molbbbp: 0.808647 test loss: 0.750173
[Epoch 47; Iter    12/   48] train: loss: 0.1072403
[Epoch 47; Iter    42/   48] train: loss: 0.1198345
[Epoch 47] ogbg-molbbbp: 0.988739 val loss: 0.090852
[Epoch 47] ogbg-molbbbp: 0.798746 test loss: 0.893322
[Epoch 48; Iter    24/   48] train: loss: 0.1939811
[Epoch 48] ogbg-molbbbp: 0.992258 val loss: 0.196663
[Epoch 48] ogbg-molbbbp: 0.785125 test loss: 0.665588
[Epoch 49; Iter     6/   48] train: loss: 0.1048025
[Epoch 49; Iter    36/   48] train: loss: 0.1687208
[Epoch 49] ogbg-molbbbp: 0.997537 val loss: 0.085115
[Epoch 49] ogbg-molbbbp: 0.762590 test loss: 1.115560
[Epoch 50; Iter    18/   48] train: loss: 0.1005447
[Epoch 50; Iter    48/   48] train: loss: 0.0623612
[Epoch 50] ogbg-molbbbp: 0.989326 val loss: 0.090675
[Epoch 50] ogbg-molbbbp: 0.794758 test loss: 1.069953
[Epoch 51; Iter    30/   48] train: loss: 0.1001443
[Epoch 51] ogbg-molbbbp: 0.986510 val loss: 0.217370
[Epoch 51] ogbg-molbbbp: 0.782168 test loss: 0.723104
[Epoch 52; Iter    12/   48] train: loss: 0.1663681
[Epoch 52; Iter    42/   48] train: loss: 0.2013799
[Epoch 52] ogbg-molbbbp: 0.938768 val loss: 0.168753
[Epoch 52] ogbg-molbbbp: 0.786066 test loss: 0.890342
[Epoch 53; Iter    24/   48] train: loss: 0.2167351
[Epoch 53] ogbg-molbbbp: 0.993196 val loss: 0.128822
[Epoch 53] ogbg-molbbbp: 0.800179 test loss: 0.902617
[Epoch 54; Iter     6/   48] train: loss: 0.1445341
[Epoch 54; Iter    36/   48] train: loss: 0.1520672
[Epoch 54] ogbg-molbbbp: 0.975367 val loss: 0.123829
[Epoch 54] ogbg-molbbbp: 0.797894 test loss: 0.762680
[Epoch 55; Iter    18/   48] train: loss: 0.1932599
[Epoch 55; Iter    48/   48] train: loss: 0.2355360
[Epoch 55] ogbg-molbbbp: 0.987449 val loss: 0.101966
[Epoch 55] ogbg-molbbbp: 0.765054 test loss: 1.000229
[Epoch 56; Iter    30/   48] train: loss: 0.3626481
[Epoch 56] ogbg-molbbbp: 0.974194 val loss: 0.121121
[Epoch 56] ogbg-molbbbp: 0.759364 test loss: 0.951225
[Epoch 57; Iter    12/   48] train: loss: 0.5801619
[Epoch 57; Iter    42/   48] train: loss: 0.1532857
[Epoch 57] ogbg-molbbbp: 0.992375 val loss: 0.087447
[Epoch 57] ogbg-molbbbp: 0.794400 test loss: 1.036310
[Epoch 58; Iter    24/   48] train: loss: 0.0744842
[Epoch 58] ogbg-molbbbp: 0.986276 val loss: 0.209326
[Epoch 58] ogbg-molbbbp: 0.759050 test loss: 0.830626
[Epoch 59; Iter     6/   48] train: loss: 0.0749494
[Epoch 59; Iter    36/   48] train: loss: 0.1383127
[Epoch 59] ogbg-molbbbp: 0.981466 val loss: 0.100880
[Epoch 59] ogbg-molbbbp: 0.794310 test loss: 1.091512
[Epoch 60; Iter    18/   48] train: loss: 0.2141451
[Epoch 60; Iter    48/   48] train: loss: 0.0765168
[Epoch 60] ogbg-molbbbp: 0.973724 val loss: 0.138786
[Epoch 60] ogbg-molbbbp: 0.779391 test loss: 0.943185
[Epoch 61; Iter    30/   48] train: loss: 0.2582668
[Epoch 61] ogbg-molbbbp: 0.980176 val loss: 0.113913
[Epoch 61] ogbg-molbbbp: 0.763351 test loss: 1.379644
[Epoch 62; Iter    12/   48] train: loss: 0.0501552
[Epoch 62; Iter    42/   48] train: loss: 0.0525971
[Epoch 62] ogbg-molbbbp: 0.973607 val loss: 0.163952
[Epoch 62] ogbg-molbbbp: 0.787186 test loss: 0.868536
[Epoch 63; Iter    24/   48] train: loss: 0.2564813
[Epoch 63] ogbg-molbbbp: 0.987566 val loss: 0.152260
[Epoch 63] ogbg-molbbbp: 0.778719 test loss: 1.134014
[Epoch 64; Iter     6/   48] train: loss: 0.0885852
[Epoch 64; Iter    36/   48] train: loss: 0.0992997
[Epoch 64] ogbg-molbbbp: 0.993196 val loss: 0.068046
[Epoch 64] ogbg-molbbbp: 0.791577 test loss: 1.097345
[Epoch 65; Iter    18/   48] train: loss: 0.0858094
[Epoch 65; Iter    48/   48] train: loss: 0.2749858
[Epoch 65] ogbg-molbbbp: 0.973372 val loss: 0.121558
[Epoch 65] ogbg-molbbbp: 0.720744 test loss: 1.331782
[Epoch 66; Iter    30/   48] train: loss: 0.1092247
[Epoch 66] ogbg-molbbbp: 0.981232 val loss: 0.085487
[Epoch 66] ogbg-molbbbp: 0.778898 test loss: 1.210930
[Epoch 67; Iter    12/   48] train: loss: 0.0518533
[Epoch 67; Iter    42/   48] train: loss: 0.0954819
[Epoch 67] ogbg-molbbbp: 0.974780 val loss: 0.130320
[Epoch 67] ogbg-molbbbp: 0.789337 test loss: 1.138613
[Epoch 68; Iter    24/   48] train: loss: 0.2505828
[Epoch 68] ogbg-molbbbp: 0.990968 val loss: 0.093802
[Epoch 68] ogbg-molbbbp: 0.796505 test loss: 1.195794
[Epoch 69; Iter     6/   48] train: loss: 0.1341394
[Epoch 69; Iter    36/   48] train: loss: 0.1948415
[Epoch 69] ogbg-molbbbp: 0.987449 val loss: 0.132555
[Epoch 69] ogbg-molbbbp: 0.791263 test loss: 0.955541
[Epoch 70; Iter    18/   48] train: loss: 0.2198712
[Epoch 70; Iter    48/   48] train: loss: 0.5065879
[Epoch 70] ogbg-molbbbp: 0.980528 val loss: 0.107817
[Epoch 70] ogbg-molbbbp: 0.771685 test loss: 1.098756
[Epoch 71; Iter    30/   48] train: loss: 0.0288812
[Epoch 71] ogbg-molbbbp: 0.981584 val loss: 0.100273
[Epoch 71] ogbg-molbbbp: 0.796192 test loss: 1.245561
[Epoch 72; Iter    12/   48] train: loss: 0.0262824
[Epoch 72; Iter    42/   48] train: loss: 0.0511084
[Epoch 72] ogbg-molbbbp: 0.980059 val loss: 0.098151
[Epoch 72] ogbg-molbbbp: 0.769131 test loss: 1.214592
[Epoch 73; Iter    24/   48] train: loss: 0.0501922
[Epoch 73] ogbg-molbbbp: 0.988622 val loss: 0.141795
[Epoch 30] ogbg-molbbbp: 0.977361 val loss: 0.321869
[Epoch 30] ogbg-molbbbp: 0.795609 test loss: 0.583833
[Epoch 31; Iter    30/   48] train: loss: 0.2365052
[Epoch 31] ogbg-molbbbp: 0.985455 val loss: 0.141678
[Epoch 31] ogbg-molbbbp: 0.805063 test loss: 0.668395
[Epoch 32; Iter    12/   48] train: loss: 0.1744045
[Epoch 32; Iter    42/   48] train: loss: 0.3918281
[Epoch 32] ogbg-molbbbp: 0.984164 val loss: 0.147401
[Epoch 32] ogbg-molbbbp: 0.805376 test loss: 0.737010
[Epoch 33; Iter    24/   48] train: loss: 0.3038498
[Epoch 33] ogbg-molbbbp: 0.978299 val loss: 0.291337
[Epoch 33] ogbg-molbbbp: 0.788262 test loss: 0.735742
[Epoch 34; Iter     6/   48] train: loss: 0.1848127
[Epoch 34; Iter    36/   48] train: loss: 0.2109194
[Epoch 34] ogbg-molbbbp: 0.980411 val loss: 0.206055
[Epoch 34] ogbg-molbbbp: 0.813844 test loss: 0.609518
[Epoch 35; Iter    18/   48] train: loss: 0.1380221
[Epoch 35; Iter    48/   48] train: loss: 0.2080552
[Epoch 35] ogbg-molbbbp: 0.926100 val loss: 0.434580
[Epoch 35] ogbg-molbbbp: 0.795565 test loss: 0.762817
[Epoch 36; Iter    30/   48] train: loss: 0.1545731
[Epoch 36] ogbg-molbbbp: 0.976070 val loss: 0.149021
[Epoch 36] ogbg-molbbbp: 0.804211 test loss: 0.716880
[Epoch 37; Iter    12/   48] train: loss: 0.1214620
[Epoch 37; Iter    42/   48] train: loss: 0.3576118
[Epoch 37] ogbg-molbbbp: 0.979355 val loss: 0.218682
[Epoch 37] ogbg-molbbbp: 0.802599 test loss: 0.624533
[Epoch 38; Iter    24/   48] train: loss: 0.2293669
[Epoch 38] ogbg-molbbbp: 0.974076 val loss: 0.196180
[Epoch 38] ogbg-molbbbp: 0.777957 test loss: 0.802352
[Epoch 39; Iter     6/   48] train: loss: 0.1676155
[Epoch 39; Iter    36/   48] train: loss: 0.1256301
[Epoch 39] ogbg-molbbbp: 0.982170 val loss: 0.228923
[Epoch 39] ogbg-molbbbp: 0.812097 test loss: 0.628693
[Epoch 40; Iter    18/   48] train: loss: 0.2264397
[Epoch 40; Iter    48/   48] train: loss: 0.0869580
[Epoch 40] ogbg-molbbbp: 0.980997 val loss: 0.127646
[Epoch 40] ogbg-molbbbp: 0.819892 test loss: 0.728919
[Epoch 41; Iter    30/   48] train: loss: 0.1313034
[Epoch 41] ogbg-molbbbp: 0.988856 val loss: 0.112596
[Epoch 41] ogbg-molbbbp: 0.796281 test loss: 0.803937
[Epoch 42; Iter    12/   48] train: loss: 0.2151874
[Epoch 42; Iter    42/   48] train: loss: 0.3667795
[Epoch 42] ogbg-molbbbp: 0.975367 val loss: 0.100287
[Epoch 42] ogbg-molbbbp: 0.803450 test loss: 0.918201
[Epoch 43; Iter    24/   48] train: loss: 0.1451175
[Epoch 43] ogbg-molbbbp: 0.978416 val loss: 0.106359
[Epoch 43] ogbg-molbbbp: 0.801165 test loss: 0.967174
[Epoch 44; Iter     6/   48] train: loss: 0.0640322
[Epoch 44; Iter    36/   48] train: loss: 0.1573784
[Epoch 44] ogbg-molbbbp: 0.980176 val loss: 0.294680
[Epoch 44] ogbg-molbbbp: 0.809588 test loss: 0.733440
[Epoch 45; Iter    18/   48] train: loss: 0.2325349
[Epoch 45; Iter    48/   48] train: loss: 0.0855044
[Epoch 45] ogbg-molbbbp: 0.977243 val loss: 0.142911
[Epoch 45] ogbg-molbbbp: 0.805959 test loss: 0.783595
[Epoch 46; Iter    30/   48] train: loss: 0.1363092
[Epoch 46] ogbg-molbbbp: 0.965279 val loss: 0.121408
[Epoch 46] ogbg-molbbbp: 0.799955 test loss: 0.837362
[Epoch 47; Iter    12/   48] train: loss: 0.0739025
[Epoch 47; Iter    42/   48] train: loss: 0.0964508
[Epoch 47] ogbg-molbbbp: 0.988387 val loss: 0.200442
[Epoch 47] ogbg-molbbbp: 0.816667 test loss: 0.620961
[Epoch 48; Iter    24/   48] train: loss: 0.1053111
[Epoch 48] ogbg-molbbbp: 0.966569 val loss: 0.134013
[Epoch 48] ogbg-molbbbp: 0.776254 test loss: 0.963566
[Epoch 49; Iter     6/   48] train: loss: 0.1588866
[Epoch 49; Iter    36/   48] train: loss: 0.0904594
[Epoch 49] ogbg-molbbbp: 0.988035 val loss: 0.100965
[Epoch 49] ogbg-molbbbp: 0.784543 test loss: 0.881184
[Epoch 50; Iter    18/   48] train: loss: 0.0996995
[Epoch 50; Iter    48/   48] train: loss: 0.0956533
[Epoch 50] ogbg-molbbbp: 0.975718 val loss: 0.174142
[Epoch 50] ogbg-molbbbp: 0.824194 test loss: 0.838270
[Epoch 51; Iter    30/   48] train: loss: 0.1513700
[Epoch 51] ogbg-molbbbp: 0.986510 val loss: 0.096751
[Epoch 51] ogbg-molbbbp: 0.800045 test loss: 1.130741
[Epoch 52; Iter    12/   48] train: loss: 0.1324518
[Epoch 52; Iter    42/   48] train: loss: 0.0543069
[Epoch 52] ogbg-molbbbp: 0.938886 val loss: 0.140701
[Epoch 52] ogbg-molbbbp: 0.773387 test loss: 0.988157
[Epoch 53; Iter    24/   48] train: loss: 0.3261484
[Epoch 53] ogbg-molbbbp: 0.992141 val loss: 0.146303
[Epoch 53] ogbg-molbbbp: 0.812007 test loss: 0.863473
[Epoch 54; Iter     6/   48] train: loss: 0.2572507
[Epoch 54; Iter    36/   48] train: loss: 0.3061136
[Epoch 54] ogbg-molbbbp: 0.953079 val loss: 0.216798
[Epoch 54] ogbg-molbbbp: 0.766711 test loss: 0.976540
[Epoch 55; Iter    18/   48] train: loss: 0.1854393
[Epoch 55; Iter    48/   48] train: loss: 0.3190692
[Epoch 55] ogbg-molbbbp: 0.990147 val loss: 0.137913
[Epoch 55] ogbg-molbbbp: 0.808647 test loss: 0.801051
[Epoch 56; Iter    30/   48] train: loss: 0.0977633
[Epoch 56] ogbg-molbbbp: 0.980997 val loss: 0.186843
[Epoch 56] ogbg-molbbbp: 0.782527 test loss: 1.171046
[Epoch 57; Iter    12/   48] train: loss: 0.0666483
[Epoch 57; Iter    42/   48] train: loss: 0.1039748
[Epoch 57] ogbg-molbbbp: 0.989326 val loss: 0.186714
[Epoch 57] ogbg-molbbbp: 0.797088 test loss: 0.946924
[Epoch 58; Iter    24/   48] train: loss: 0.0848005
[Epoch 58] ogbg-molbbbp: 0.991437 val loss: 0.082941
[Epoch 58] ogbg-molbbbp: 0.780242 test loss: 1.023441
[Epoch 59; Iter     6/   48] train: loss: 0.0952586
[Epoch 59; Iter    36/   48] train: loss: 0.2652191
[Epoch 59] ogbg-molbbbp: 0.985924 val loss: 0.114196
[Epoch 59] ogbg-molbbbp: 0.778226 test loss: 0.972868
[Epoch 60; Iter    18/   48] train: loss: 0.0690433
[Epoch 60; Iter    48/   48] train: loss: 0.1255881
[Epoch 60] ogbg-molbbbp: 0.972317 val loss: 0.164048
[Epoch 60] ogbg-molbbbp: 0.795968 test loss: 0.926772
[Epoch 61; Iter    30/   48] train: loss: 0.1697850
[Epoch 61] ogbg-molbbbp: 0.979355 val loss: 0.124200
[Epoch 61] ogbg-molbbbp: 0.778091 test loss: 1.136015
[Epoch 62; Iter    12/   48] train: loss: 0.0773041
[Epoch 62; Iter    42/   48] train: loss: 0.0462262
[Epoch 62] ogbg-molbbbp: 0.985689 val loss: 0.127318
[Epoch 62] ogbg-molbbbp: 0.786111 test loss: 1.095575
[Epoch 63; Iter    24/   48] train: loss: 0.3349411
[Epoch 63] ogbg-molbbbp: 0.973490 val loss: 0.120752
[Epoch 63] ogbg-molbbbp: 0.757168 test loss: 1.339128
[Epoch 64; Iter     6/   48] train: loss: 0.0192166
[Epoch 64; Iter    36/   48] train: loss: 0.0844921
[Epoch 64] ogbg-molbbbp: 0.980997 val loss: 0.209369
[Epoch 64] ogbg-molbbbp: 0.789471 test loss: 0.861152
[Epoch 65; Iter    18/   48] train: loss: 0.0088988
[Epoch 65; Iter    48/   48] train: loss: 0.0667012
[Epoch 65] ogbg-molbbbp: 0.979941 val loss: 0.174049
[Epoch 65] ogbg-molbbbp: 0.788799 test loss: 0.998492
[Epoch 66; Iter    30/   48] train: loss: 0.0460614
[Epoch 66] ogbg-molbbbp: 0.980645 val loss: 0.110582
[Epoch 66] ogbg-molbbbp: 0.782348 test loss: 1.134502
[Epoch 67; Iter    12/   48] train: loss: 0.1070228
[Epoch 67; Iter    42/   48] train: loss: 0.0244923
[Epoch 67] ogbg-molbbbp: 0.991554 val loss: 0.123627
[Epoch 67] ogbg-molbbbp: 0.778763 test loss: 1.072377
[Epoch 68; Iter    24/   48] train: loss: 0.2780343
[Epoch 68] ogbg-molbbbp: 0.983695 val loss: 0.131628
[Epoch 68] ogbg-molbbbp: 0.764337 test loss: 1.390878
[Epoch 69; Iter     6/   48] train: loss: 0.0319465
[Epoch 69; Iter    36/   48] train: loss: 0.0190261
[Epoch 69] ogbg-molbbbp: 0.989326 val loss: 0.204928
[Epoch 69] ogbg-molbbbp: 0.769489 test loss: 1.309094
[Epoch 70; Iter    18/   48] train: loss: 0.1016867
[Epoch 70; Iter    48/   48] train: loss: 0.1597692
[Epoch 70] ogbg-molbbbp: 0.984282 val loss: 0.118719
[Epoch 70] ogbg-molbbbp: 0.774597 test loss: 1.146423
[Epoch 71; Iter    30/   48] train: loss: 0.0260141
[Epoch 71] ogbg-molbbbp: 0.974897 val loss: 0.144560
[Epoch 71] ogbg-molbbbp: 0.754346 test loss: 1.314858
[Epoch 72; Iter    12/   48] train: loss: 0.1974456
[Epoch 72; Iter    42/   48] train: loss: 0.0299604
[Epoch 72] ogbg-molbbbp: 0.983343 val loss: 0.088212
[Epoch 72] ogbg-molbbbp: 0.774642 test loss: 1.468816
[Epoch 73; Iter    24/   48] train: loss: 0.0450149
[Epoch 73] ogbg-molbbbp: 0.986979 val loss: 0.098254
[Epoch 69; Iter    40/   55] train: loss: 0.0112418
[Epoch 69] ogbg-molbbbp: 0.949915 val loss: 0.526210
[Epoch 69] ogbg-molbbbp: 0.625000 test loss: 2.027904
[Epoch 70; Iter    15/   55] train: loss: 0.0989129
[Epoch 70; Iter    45/   55] train: loss: 0.0696618
[Epoch 70] ogbg-molbbbp: 0.964353 val loss: 0.384892
[Epoch 70] ogbg-molbbbp: 0.640914 test loss: 1.971848
[Epoch 71; Iter    20/   55] train: loss: 0.0535085
[Epoch 71; Iter    50/   55] train: loss: 0.2045204
[Epoch 71] ogbg-molbbbp: 0.962262 val loss: 0.422646
[Epoch 71] ogbg-molbbbp: 0.667824 test loss: 1.829180
[Epoch 72; Iter    25/   55] train: loss: 0.1450841
[Epoch 72; Iter    55/   55] train: loss: 0.2894095
[Epoch 72] ogbg-molbbbp: 0.958578 val loss: 0.424225
[Epoch 72] ogbg-molbbbp: 0.665992 test loss: 1.650537
[Epoch 73; Iter    30/   55] train: loss: 0.0996367
[Epoch 73] ogbg-molbbbp: 0.958279 val loss: 0.430178
[Epoch 73] ogbg-molbbbp: 0.631848 test loss: 1.905508
[Epoch 74; Iter     5/   55] train: loss: 0.0072786
[Epoch 74; Iter    35/   55] train: loss: 0.0331044
[Epoch 74] ogbg-molbbbp: 0.955193 val loss: 0.435035
[Epoch 74] ogbg-molbbbp: 0.639371 test loss: 1.882197
[Epoch 75; Iter    10/   55] train: loss: 0.0248690
[Epoch 75; Iter    40/   55] train: loss: 0.0880536
[Epoch 75] ogbg-molbbbp: 0.956686 val loss: 0.474159
[Epoch 75] ogbg-molbbbp: 0.632909 test loss: 1.820105
[Epoch 76; Iter    15/   55] train: loss: 0.1259567
[Epoch 76; Iter    45/   55] train: loss: 0.0780520
[Epoch 76] ogbg-molbbbp: 0.957582 val loss: 0.485646
[Epoch 76] ogbg-molbbbp: 0.671779 test loss: 1.789438
[Epoch 77; Iter    20/   55] train: loss: 0.0031291
[Epoch 77; Iter    50/   55] train: loss: 0.0049758
[Epoch 77] ogbg-molbbbp: 0.965050 val loss: 0.379404
[Epoch 77] ogbg-molbbbp: 0.676312 test loss: 1.752494
[Epoch 78; Iter    25/   55] train: loss: 0.0494426
[Epoch 78; Iter    55/   55] train: loss: 0.3750969
[Epoch 78] ogbg-molbbbp: 0.953301 val loss: 0.447530
[Epoch 78] ogbg-molbbbp: 0.635610 test loss: 1.824751
[Epoch 79; Iter    30/   55] train: loss: 0.0189604
[Epoch 79] ogbg-molbbbp: 0.954097 val loss: 0.524620
[Epoch 79] ogbg-molbbbp: 0.633005 test loss: 2.052222
[Epoch 80; Iter     5/   55] train: loss: 0.1100257
[Epoch 80; Iter    35/   55] train: loss: 0.0479065
[Epoch 80] ogbg-molbbbp: 0.956885 val loss: 0.444525
[Epoch 80] ogbg-molbbbp: 0.655961 test loss: 1.644109
[Epoch 81; Iter    10/   55] train: loss: 0.0090727
[Epoch 81; Iter    40/   55] train: loss: 0.0206689
[Epoch 81] ogbg-molbbbp: 0.950115 val loss: 0.557949
[Epoch 81] ogbg-molbbbp: 0.646508 test loss: 1.934923
[Epoch 82; Iter    15/   55] train: loss: 0.0175496
[Epoch 82; Iter    45/   55] train: loss: 0.0780118
[Epoch 82] ogbg-molbbbp: 0.951409 val loss: 0.465676
[Epoch 82] ogbg-molbbbp: 0.649402 test loss: 1.726466
[Epoch 83; Iter    20/   55] train: loss: 0.0179833
[Epoch 83; Iter    50/   55] train: loss: 0.0450212
[Epoch 83] ogbg-molbbbp: 0.950712 val loss: 0.501831
[Epoch 83] ogbg-molbbbp: 0.646123 test loss: 2.049297
[Epoch 84; Iter    25/   55] train: loss: 0.0039534
[Epoch 84; Iter    55/   55] train: loss: 0.1139684
[Epoch 84] ogbg-molbbbp: 0.943742 val loss: 0.668863
[Epoch 84] ogbg-molbbbp: 0.665606 test loss: 1.935276
[Epoch 85; Iter    30/   55] train: loss: 0.0058480
[Epoch 85] ogbg-molbbbp: 0.953600 val loss: 0.512234
[Epoch 85] ogbg-molbbbp: 0.660204 test loss: 1.973209
[Epoch 86; Iter     5/   55] train: loss: 0.0111522
[Epoch 86; Iter    35/   55] train: loss: 0.0029256
[Epoch 86] ogbg-molbbbp: 0.954496 val loss: 0.493101
[Epoch 86] ogbg-molbbbp: 0.659915 test loss: 1.899483
[Epoch 87; Iter    10/   55] train: loss: 0.0468008
[Epoch 87; Iter    40/   55] train: loss: 0.0686022
[Epoch 87] ogbg-molbbbp: 0.949816 val loss: 0.473200
[Epoch 87] ogbg-molbbbp: 0.666763 test loss: 1.606195
[Epoch 88; Iter    15/   55] train: loss: 0.0167100
[Epoch 88; Iter    45/   55] train: loss: 0.0570945
[Epoch 88] ogbg-molbbbp: 0.961167 val loss: 0.496089
[Epoch 88] ogbg-molbbbp: 0.660301 test loss: 1.926307
[Epoch 89; Iter    20/   55] train: loss: 0.0926416
[Epoch 89; Iter    50/   55] train: loss: 0.0311982
[Epoch 89] ogbg-molbbbp: 0.939659 val loss: 0.631418
[Epoch 89] ogbg-molbbbp: 0.642747 test loss: 1.965957
[Epoch 90; Iter    25/   55] train: loss: 0.0015953
[Epoch 90; Iter    55/   55] train: loss: 0.0781535
[Epoch 90] ogbg-molbbbp: 0.959574 val loss: 0.498215
[Epoch 90] ogbg-molbbbp: 0.658951 test loss: 1.941714
[Epoch 91; Iter    30/   55] train: loss: 0.0203988
[Epoch 91] ogbg-molbbbp: 0.954396 val loss: 0.529076
[Epoch 91] ogbg-molbbbp: 0.619406 test loss: 2.144700
[Epoch 92; Iter     5/   55] train: loss: 0.0056239
[Epoch 92; Iter    35/   55] train: loss: 0.0110183
[Epoch 92] ogbg-molbbbp: 0.949915 val loss: 0.497762
[Epoch 92] ogbg-molbbbp: 0.642940 test loss: 1.962233
[Epoch 93; Iter    10/   55] train: loss: 0.0102241
[Epoch 93; Iter    40/   55] train: loss: 0.0062792
[Epoch 93] ogbg-molbbbp: 0.950812 val loss: 0.500849
[Epoch 93] ogbg-molbbbp: 0.633777 test loss: 2.018051
[Epoch 94; Iter    15/   55] train: loss: 0.0118457
[Epoch 94; Iter    45/   55] train: loss: 0.0138893
[Epoch 94] ogbg-molbbbp: 0.953002 val loss: 0.533421
[Epoch 94] ogbg-molbbbp: 0.633777 test loss: 2.160982
[Epoch 95; Iter    20/   55] train: loss: 0.0728453
[Epoch 95; Iter    50/   55] train: loss: 0.0033822
[Epoch 95] ogbg-molbbbp: 0.955691 val loss: 0.458807
[Epoch 95] ogbg-molbbbp: 0.603684 test loss: 2.182034
[Epoch 96; Iter    25/   55] train: loss: 0.0711774
[Epoch 96; Iter    55/   55] train: loss: 0.0110219
[Epoch 96] ogbg-molbbbp: 0.948721 val loss: 0.555423
[Epoch 96] ogbg-molbbbp: 0.649884 test loss: 2.158071
[Epoch 97; Iter    30/   55] train: loss: 0.0197104
[Epoch 97] ogbg-molbbbp: 0.943841 val loss: 0.508270
[Epoch 97] ogbg-molbbbp: 0.631944 test loss: 2.005888
[Epoch 98; Iter     5/   55] train: loss: 0.0019425
[Epoch 98; Iter    35/   55] train: loss: 0.0157201
[Epoch 98] ogbg-molbbbp: 0.951807 val loss: 0.484958
[Epoch 98] ogbg-molbbbp: 0.654514 test loss: 1.999140
[Epoch 99; Iter    10/   55] train: loss: 0.1500742
[Epoch 99; Iter    40/   55] train: loss: 0.0020143
[Epoch 99] ogbg-molbbbp: 0.949418 val loss: 0.520203
[Epoch 99] ogbg-molbbbp: 0.662809 test loss: 1.975600
[Epoch 100; Iter    15/   55] train: loss: 0.0024502
[Epoch 100; Iter    45/   55] train: loss: 0.0083001
[Epoch 100] ogbg-molbbbp: 0.954994 val loss: 0.463244
[Epoch 100] ogbg-molbbbp: 0.652585 test loss: 2.022288
[Epoch 101; Iter    20/   55] train: loss: 0.0014954
[Epoch 101; Iter    50/   55] train: loss: 0.0780269
[Epoch 101] ogbg-molbbbp: 0.950314 val loss: 0.491690
[Epoch 101] ogbg-molbbbp: 0.648052 test loss: 2.067910
[Epoch 102; Iter    25/   55] train: loss: 0.0198197
[Epoch 102; Iter    55/   55] train: loss: 0.0079688
[Epoch 102] ogbg-molbbbp: 0.951210 val loss: 0.518044
[Epoch 102] ogbg-molbbbp: 0.636188 test loss: 2.192483
[Epoch 103; Iter    30/   55] train: loss: 0.0010349
[Epoch 103] ogbg-molbbbp: 0.944837 val loss: 0.611486
[Epoch 103] ogbg-molbbbp: 0.637056 test loss: 2.184420
[Epoch 104; Iter     5/   55] train: loss: 0.0018848
[Epoch 104; Iter    35/   55] train: loss: 0.0045269
[Epoch 104] ogbg-molbbbp: 0.944538 val loss: 0.540932
[Epoch 104] ogbg-molbbbp: 0.636381 test loss: 2.056855
[Epoch 105; Iter    10/   55] train: loss: 0.0080792
[Epoch 105; Iter    40/   55] train: loss: 0.0493480
[Epoch 105] ogbg-molbbbp: 0.949716 val loss: 0.593756
[Epoch 105] ogbg-molbbbp: 0.633873 test loss: 2.107055
[Epoch 106; Iter    15/   55] train: loss: 0.0680370
[Epoch 106; Iter    45/   55] train: loss: 0.0013484
[Epoch 106] ogbg-molbbbp: 0.945634 val loss: 0.598322
[Epoch 106] ogbg-molbbbp: 0.645062 test loss: 1.979841
[Epoch 107; Iter    20/   55] train: loss: 0.0070676
[Epoch 107; Iter    50/   55] train: loss: 0.0042699
[Epoch 107] ogbg-molbbbp: 0.945435 val loss: 0.562088
[Epoch 107] ogbg-molbbbp: 0.606964 test loss: 2.276383
[Epoch 108; Iter    25/   55] train: loss: 0.0097720
[Epoch 108; Iter    55/   55] train: loss: 0.0366057
[Epoch 108] ogbg-molbbbp: 0.945733 val loss: 0.537615
[Epoch 108] ogbg-molbbbp: 0.622975 test loss: 2.260198
[Epoch 109; Iter    30/   55] train: loss: 0.0323843
[Epoch 69; Iter    40/   55] train: loss: 0.0650941
[Epoch 69] ogbg-molbbbp: 0.944140 val loss: 0.618295
[Epoch 69] ogbg-molbbbp: 0.670235 test loss: 2.514189
[Epoch 70; Iter    15/   55] train: loss: 0.6380295
[Epoch 70; Iter    45/   55] train: loss: 0.2060047
[Epoch 70] ogbg-molbbbp: 0.944240 val loss: 0.526186
[Epoch 70] ogbg-molbbbp: 0.665606 test loss: 1.773795
[Epoch 71; Iter    20/   55] train: loss: 0.0464401
[Epoch 71; Iter    50/   55] train: loss: 0.0549491
[Epoch 71] ogbg-molbbbp: 0.967241 val loss: 0.299660
[Epoch 71] ogbg-molbbbp: 0.670235 test loss: 1.417489
[Epoch 72; Iter    25/   55] train: loss: 0.0772799
[Epoch 72; Iter    55/   55] train: loss: 0.1403587
[Epoch 72] ogbg-molbbbp: 0.961665 val loss: 0.453609
[Epoch 72] ogbg-molbbbp: 0.644579 test loss: 1.918913
[Epoch 73; Iter    30/   55] train: loss: 0.0530777
[Epoch 73] ogbg-molbbbp: 0.953699 val loss: 0.439765
[Epoch 73] ogbg-molbbbp: 0.635031 test loss: 1.759055
[Epoch 74; Iter     5/   55] train: loss: 0.0467408
[Epoch 74; Iter    35/   55] train: loss: 0.0277627
[Epoch 74] ogbg-molbbbp: 0.944240 val loss: 0.458371
[Epoch 74] ogbg-molbbbp: 0.621721 test loss: 1.750231
[Epoch 75; Iter    10/   55] train: loss: 0.0350639
[Epoch 75; Iter    40/   55] train: loss: 0.0493945
[Epoch 75] ogbg-molbbbp: 0.950314 val loss: 0.438143
[Epoch 75] ogbg-molbbbp: 0.636574 test loss: 1.718003
[Epoch 76; Iter    15/   55] train: loss: 0.1700301
[Epoch 76; Iter    45/   55] train: loss: 0.0225003
[Epoch 76] ogbg-molbbbp: 0.941850 val loss: 0.497165
[Epoch 76] ogbg-molbbbp: 0.636671 test loss: 1.812487
[Epoch 77; Iter    20/   55] train: loss: 0.0240622
[Epoch 77; Iter    50/   55] train: loss: 0.0463820
[Epoch 77] ogbg-molbbbp: 0.945235 val loss: 0.511934
[Epoch 77] ogbg-molbbbp: 0.630498 test loss: 1.802310
[Epoch 78; Iter    25/   55] train: loss: 0.1252587
[Epoch 78; Iter    55/   55] train: loss: 0.0476001
[Epoch 78] ogbg-molbbbp: 0.956388 val loss: 0.450434
[Epoch 78] ogbg-molbbbp: 0.649595 test loss: 1.933693
[Epoch 79; Iter    30/   55] train: loss: 0.0141070
[Epoch 79] ogbg-molbbbp: 0.934681 val loss: 0.689109
[Epoch 79] ogbg-molbbbp: 0.582658 test loss: 2.239114
[Epoch 80; Iter     5/   55] train: loss: 0.0445536
[Epoch 80; Iter    35/   55] train: loss: 0.0406511
[Epoch 80] ogbg-molbbbp: 0.953699 val loss: 0.381914
[Epoch 80] ogbg-molbbbp: 0.658179 test loss: 1.525696
[Epoch 81; Iter    10/   55] train: loss: 0.0106943
[Epoch 81; Iter    40/   55] train: loss: 0.0499421
[Epoch 81] ogbg-molbbbp: 0.949915 val loss: 0.415029
[Epoch 81] ogbg-molbbbp: 0.641686 test loss: 1.823184
[Epoch 82; Iter    15/   55] train: loss: 0.0605605
[Epoch 82; Iter    45/   55] train: loss: 0.0132995
[Epoch 82] ogbg-molbbbp: 0.951210 val loss: 0.494694
[Epoch 82] ogbg-molbbbp: 0.635802 test loss: 2.027093
[Epoch 83; Iter    20/   55] train: loss: 0.0646165
[Epoch 83; Iter    50/   55] train: loss: 0.0318508
[Epoch 83] ogbg-molbbbp: 0.947028 val loss: 0.491748
[Epoch 83] ogbg-molbbbp: 0.625289 test loss: 1.985561
[Epoch 84; Iter    25/   55] train: loss: 0.0915967
[Epoch 84; Iter    55/   55] train: loss: 0.0361589
[Epoch 84] ogbg-molbbbp: 0.953898 val loss: 0.486913
[Epoch 84] ogbg-molbbbp: 0.647859 test loss: 1.980808
[Epoch 85; Iter    30/   55] train: loss: 0.0070399
[Epoch 85] ogbg-molbbbp: 0.958578 val loss: 0.408254
[Epoch 85] ogbg-molbbbp: 0.664738 test loss: 1.808480
[Epoch 86; Iter     5/   55] train: loss: 0.1191566
[Epoch 86; Iter    35/   55] train: loss: 0.0484832
[Epoch 86] ogbg-molbbbp: 0.950413 val loss: 0.497046
[Epoch 86] ogbg-molbbbp: 0.640336 test loss: 1.865126
[Epoch 87; Iter    10/   55] train: loss: 0.0122922
[Epoch 87; Iter    40/   55] train: loss: 0.0069219
[Epoch 87] ogbg-molbbbp: 0.945036 val loss: 0.581041
[Epoch 87] ogbg-molbbbp: 0.639275 test loss: 2.185380
[Epoch 88; Iter    15/   55] train: loss: 0.0372957
[Epoch 88; Iter    45/   55] train: loss: 0.0089783
[Epoch 88] ogbg-molbbbp: 0.955989 val loss: 0.426829
[Epoch 88] ogbg-molbbbp: 0.648920 test loss: 1.773827
[Epoch 89; Iter    20/   55] train: loss: 0.0219845
[Epoch 89; Iter    50/   55] train: loss: 0.0596407
[Epoch 89] ogbg-molbbbp: 0.950214 val loss: 0.532366
[Epoch 89] ogbg-molbbbp: 0.636960 test loss: 2.154443
[Epoch 90; Iter    25/   55] train: loss: 0.0071308
[Epoch 90; Iter    55/   55] train: loss: 0.1153220
[Epoch 90] ogbg-molbbbp: 0.954396 val loss: 0.493655
[Epoch 90] ogbg-molbbbp: 0.636285 test loss: 2.220438
[Epoch 91; Iter    30/   55] train: loss: 0.0098144
[Epoch 91] ogbg-molbbbp: 0.956885 val loss: 0.394464
[Epoch 91] ogbg-molbbbp: 0.645351 test loss: 1.767331
[Epoch 92; Iter     5/   55] train: loss: 0.0075290
[Epoch 92; Iter    35/   55] train: loss: 0.0064362
[Epoch 92] ogbg-molbbbp: 0.951608 val loss: 0.449720
[Epoch 92] ogbg-molbbbp: 0.638214 test loss: 1.920498
[Epoch 93; Iter    10/   55] train: loss: 0.0061105
[Epoch 93; Iter    40/   55] train: loss: 0.0021991
[Epoch 93] ogbg-molbbbp: 0.941053 val loss: 0.606304
[Epoch 93] ogbg-molbbbp: 0.628472 test loss: 2.174167
[Epoch 94; Iter    15/   55] train: loss: 0.0731803
[Epoch 94; Iter    45/   55] train: loss: 0.0025182
[Epoch 94] ogbg-molbbbp: 0.949218 val loss: 0.591090
[Epoch 94] ogbg-molbbbp: 0.638117 test loss: 2.268844
[Epoch 95; Iter    20/   55] train: loss: 0.3944264
[Epoch 95; Iter    50/   55] train: loss: 0.0042802
[Epoch 95] ogbg-molbbbp: 0.944339 val loss: 0.553728
[Epoch 95] ogbg-molbbbp: 0.625772 test loss: 2.215194
[Epoch 96; Iter    25/   55] train: loss: 0.0078427
[Epoch 96; Iter    55/   55] train: loss: 0.0056776
[Epoch 96] ogbg-molbbbp: 0.955093 val loss: 0.474485
[Epoch 96] ogbg-molbbbp: 0.676987 test loss: 2.136089
[Epoch 97; Iter    30/   55] train: loss: 0.0116322
[Epoch 97] ogbg-molbbbp: 0.939759 val loss: 0.651748
[Epoch 97] ogbg-molbbbp: 0.653260 test loss: 2.190809
[Epoch 98; Iter     5/   55] train: loss: 0.0213318
[Epoch 98; Iter    35/   55] train: loss: 0.0085507
[Epoch 98] ogbg-molbbbp: 0.948223 val loss: 0.536632
[Epoch 98] ogbg-molbbbp: 0.643808 test loss: 2.076785
[Epoch 99; Iter    10/   55] train: loss: 0.0058840
[Epoch 99; Iter    40/   55] train: loss: 0.0049227
[Epoch 99] ogbg-molbbbp: 0.957184 val loss: 0.433248
[Epoch 99] ogbg-molbbbp: 0.657697 test loss: 1.923027
[Epoch 100; Iter    15/   55] train: loss: 0.0096419
[Epoch 100; Iter    45/   55] train: loss: 0.0192840
[Epoch 100] ogbg-molbbbp: 0.938863 val loss: 0.637783
[Epoch 100] ogbg-molbbbp: 0.616609 test loss: 2.557878
[Epoch 101; Iter    20/   55] train: loss: 0.1085727
[Epoch 101; Iter    50/   55] train: loss: 0.0088522
[Epoch 101] ogbg-molbbbp: 0.943144 val loss: 0.643949
[Epoch 101] ogbg-molbbbp: 0.635031 test loss: 2.359534
[Epoch 102; Iter    25/   55] train: loss: 0.1123745
[Epoch 102; Iter    55/   55] train: loss: 0.0055273
[Epoch 102] ogbg-molbbbp: 0.951509 val loss: 0.551007
[Epoch 102] ogbg-molbbbp: 0.635802 test loss: 2.356478
[Epoch 103; Iter    30/   55] train: loss: 0.0094640
[Epoch 103] ogbg-molbbbp: 0.948322 val loss: 0.507624
[Epoch 103] ogbg-molbbbp: 0.645158 test loss: 1.994379
[Epoch 104; Iter     5/   55] train: loss: 0.0745878
[Epoch 104; Iter    35/   55] train: loss: 0.0079931
[Epoch 104] ogbg-molbbbp: 0.950214 val loss: 0.486198
[Epoch 104] ogbg-molbbbp: 0.651427 test loss: 2.127552
[Epoch 105; Iter    10/   55] train: loss: 0.1552465
[Epoch 105; Iter    40/   55] train: loss: 0.0069122
[Epoch 105] ogbg-molbbbp: 0.941253 val loss: 0.642301
[Epoch 105] ogbg-molbbbp: 0.630787 test loss: 2.198387
[Epoch 106; Iter    15/   55] train: loss: 0.0189219
[Epoch 106; Iter    45/   55] train: loss: 0.0086732
[Epoch 106] ogbg-molbbbp: 0.944041 val loss: 0.633762
[Epoch 106] ogbg-molbbbp: 0.637731 test loss: 2.305326
[Epoch 107; Iter    20/   55] train: loss: 0.0102051
[Epoch 107; Iter    50/   55] train: loss: 0.0036788
[Epoch 107] ogbg-molbbbp: 0.953998 val loss: 0.434342
[Epoch 107] ogbg-molbbbp: 0.650752 test loss: 1.926317
[Epoch 108; Iter    25/   55] train: loss: 0.0012507
[Epoch 108; Iter    55/   55] train: loss: 0.0016351
[Epoch 108] ogbg-molbbbp: 0.947426 val loss: 0.608661
[Epoch 108] ogbg-molbbbp: 0.639275 test loss: 2.323313
[Epoch 109; Iter    30/   55] train: loss: 0.0209348
[Epoch 69; Iter    40/   55] train: loss: 0.0716489
[Epoch 69] ogbg-molbbbp: 0.965150 val loss: 0.423196
[Epoch 69] ogbg-molbbbp: 0.713156 test loss: 1.755026
[Epoch 70; Iter    15/   55] train: loss: 0.0236045
[Epoch 70; Iter    45/   55] train: loss: 0.0277191
[Epoch 70] ogbg-molbbbp: 0.962661 val loss: 0.341414
[Epoch 70] ogbg-molbbbp: 0.683835 test loss: 1.703109
[Epoch 71; Iter    20/   55] train: loss: 0.0048718
[Epoch 71; Iter    50/   55] train: loss: 0.0088770
[Epoch 71] ogbg-molbbbp: 0.960470 val loss: 0.381126
[Epoch 71] ogbg-molbbbp: 0.691744 test loss: 1.710748
[Epoch 72; Iter    25/   55] train: loss: 0.0033043
[Epoch 72; Iter    55/   55] train: loss: 0.0041191
[Epoch 72] ogbg-molbbbp: 0.960072 val loss: 0.410212
[Epoch 72] ogbg-molbbbp: 0.690104 test loss: 1.791590
[Epoch 73; Iter    30/   55] train: loss: 0.0785801
[Epoch 73] ogbg-molbbbp: 0.949418 val loss: 0.474822
[Epoch 73] ogbg-molbbbp: 0.678627 test loss: 1.890144
[Epoch 74; Iter     5/   55] train: loss: 0.0115323
[Epoch 74; Iter    35/   55] train: loss: 0.0095602
[Epoch 74] ogbg-molbbbp: 0.960968 val loss: 0.355493
[Epoch 74] ogbg-molbbbp: 0.661555 test loss: 1.880461
[Epoch 75; Iter    10/   55] train: loss: 0.0286297
[Epoch 75; Iter    40/   55] train: loss: 0.0185420
[Epoch 75] ogbg-molbbbp: 0.960271 val loss: 0.407631
[Epoch 75] ogbg-molbbbp: 0.669946 test loss: 1.961835
[Epoch 76; Iter    15/   55] train: loss: 0.0531314
[Epoch 76; Iter    45/   55] train: loss: 0.1044625
[Epoch 76] ogbg-molbbbp: 0.964752 val loss: 0.426695
[Epoch 76] ogbg-molbbbp: 0.691551 test loss: 2.072205
[Epoch 77; Iter    20/   55] train: loss: 0.0305734
[Epoch 77; Iter    50/   55] train: loss: 0.1376411
[Epoch 77] ogbg-molbbbp: 0.950214 val loss: 0.490166
[Epoch 77] ogbg-molbbbp: 0.673900 test loss: 1.859370
[Epoch 78; Iter    25/   55] train: loss: 0.0031049
[Epoch 78; Iter    55/   55] train: loss: 0.0035429
[Epoch 78] ogbg-molbbbp: 0.960171 val loss: 0.402024
[Epoch 78] ogbg-molbbbp: 0.678241 test loss: 2.097655
[Epoch 79; Iter    30/   55] train: loss: 0.0090410
[Epoch 79] ogbg-molbbbp: 0.954695 val loss: 0.559295
[Epoch 79] ogbg-molbbbp: 0.688368 test loss: 2.288113
[Epoch 80; Iter     5/   55] train: loss: 0.0069744
[Epoch 80; Iter    35/   55] train: loss: 0.0032556
[Epoch 80] ogbg-molbbbp: 0.957284 val loss: 0.452544
[Epoch 80] ogbg-molbbbp: 0.702064 test loss: 1.923259
[Epoch 81; Iter    10/   55] train: loss: 0.0840797
[Epoch 81; Iter    40/   55] train: loss: 0.1457571
[Epoch 81] ogbg-molbbbp: 0.962760 val loss: 0.371841
[Epoch 81] ogbg-molbbbp: 0.698206 test loss: 1.795393
[Epoch 82; Iter    15/   55] train: loss: 0.0028901
[Epoch 82; Iter    45/   55] train: loss: 0.0059778
[Epoch 82] ogbg-molbbbp: 0.952006 val loss: 0.418124
[Epoch 82] ogbg-molbbbp: 0.664931 test loss: 1.973646
[Epoch 83; Iter    20/   55] train: loss: 0.0657296
[Epoch 83; Iter    50/   55] train: loss: 0.0177802
[Epoch 83] ogbg-molbbbp: 0.956786 val loss: 0.416346
[Epoch 83] ogbg-molbbbp: 0.671971 test loss: 2.106480
[Epoch 84; Iter    25/   55] train: loss: 0.0636851
[Epoch 84; Iter    55/   55] train: loss: 0.0022880
[Epoch 84] ogbg-molbbbp: 0.959375 val loss: 0.381997
[Epoch 84] ogbg-molbbbp: 0.685089 test loss: 1.899805
[Epoch 85; Iter    30/   55] train: loss: 0.0829719
[Epoch 85] ogbg-molbbbp: 0.958479 val loss: 0.517738
[Epoch 85] ogbg-molbbbp: 0.710745 test loss: 2.091534
[Epoch 86; Iter     5/   55] train: loss: 0.0064167
[Epoch 86; Iter    35/   55] train: loss: 0.0499683
[Epoch 86] ogbg-molbbbp: 0.951608 val loss: 0.606931
[Epoch 86] ogbg-molbbbp: 0.724537 test loss: 2.007580
[Epoch 87; Iter    10/   55] train: loss: 0.0638436
[Epoch 87; Iter    40/   55] train: loss: 0.0075210
[Epoch 87] ogbg-molbbbp: 0.949218 val loss: 0.499325
[Epoch 87] ogbg-molbbbp: 0.686246 test loss: 1.972397
[Epoch 88; Iter    15/   55] train: loss: 0.0467502
[Epoch 88; Iter    45/   55] train: loss: 0.0017527
[Epoch 88] ogbg-molbbbp: 0.950314 val loss: 0.522230
[Epoch 88] ogbg-molbbbp: 0.703607 test loss: 2.049074
[Epoch 89; Iter    20/   55] train: loss: 0.1097981
[Epoch 89; Iter    50/   55] train: loss: 0.0195349
[Epoch 89] ogbg-molbbbp: 0.953600 val loss: 0.396279
[Epoch 89] ogbg-molbbbp: 0.684510 test loss: 1.805602
[Epoch 90; Iter    25/   55] train: loss: 0.0046104
[Epoch 90; Iter    55/   55] train: loss: 0.0017862
[Epoch 90] ogbg-molbbbp: 0.962362 val loss: 0.381944
[Epoch 90] ogbg-molbbbp: 0.701292 test loss: 1.921916
[Epoch 91; Iter    30/   55] train: loss: 0.0048293
[Epoch 91] ogbg-molbbbp: 0.957881 val loss: 0.426961
[Epoch 91] ogbg-molbbbp: 0.689622 test loss: 1.944584
[Epoch 92; Iter     5/   55] train: loss: 0.0074616
[Epoch 92; Iter    35/   55] train: loss: 0.0140710
[Epoch 92] ogbg-molbbbp: 0.954197 val loss: 0.447546
[Epoch 92] ogbg-molbbbp: 0.693769 test loss: 2.067615
[Epoch 93; Iter    10/   55] train: loss: 0.0043573
[Epoch 93; Iter    40/   55] train: loss: 0.0018822
[Epoch 93] ogbg-molbbbp: 0.953699 val loss: 0.546073
[Epoch 93] ogbg-molbbbp: 0.688561 test loss: 2.192521
[Epoch 94; Iter    15/   55] train: loss: 0.0129458
[Epoch 94; Iter    45/   55] train: loss: 0.0084625
[Epoch 94] ogbg-molbbbp: 0.956587 val loss: 0.431541
[Epoch 94] ogbg-molbbbp: 0.686053 test loss: 1.992113
[Epoch 95; Iter    20/   55] train: loss: 0.0025036
[Epoch 95; Iter    50/   55] train: loss: 0.0111858
[Epoch 95] ogbg-molbbbp: 0.952903 val loss: 0.482961
[Epoch 95] ogbg-molbbbp: 0.694348 test loss: 2.135779
[Epoch 96; Iter    25/   55] train: loss: 0.0279018
[Epoch 96; Iter    55/   55] train: loss: 0.0021514
[Epoch 96] ogbg-molbbbp: 0.955491 val loss: 0.432325
[Epoch 96] ogbg-molbbbp: 0.689140 test loss: 1.917345
[Epoch 97; Iter    30/   55] train: loss: 0.0041067
[Epoch 97] ogbg-molbbbp: 0.951907 val loss: 0.563039
[Epoch 97] ogbg-molbbbp: 0.686728 test loss: 2.242934
[Epoch 98; Iter     5/   55] train: loss: 0.0046384
[Epoch 98; Iter    35/   55] train: loss: 0.0024298
[Epoch 98] ogbg-molbbbp: 0.959176 val loss: 0.387166
[Epoch 98] ogbg-molbbbp: 0.682774 test loss: 1.925749
[Epoch 99; Iter    10/   55] train: loss: 0.0970378
[Epoch 99; Iter    40/   55] train: loss: 0.0180889
[Epoch 99] ogbg-molbbbp: 0.954695 val loss: 0.510173
[Epoch 99] ogbg-molbbbp: 0.698592 test loss: 2.109799
[Epoch 100; Iter    15/   55] train: loss: 0.0018461
[Epoch 100; Iter    45/   55] train: loss: 0.0031569
[Epoch 100] ogbg-molbbbp: 0.958976 val loss: 0.425723
[Epoch 100] ogbg-molbbbp: 0.693673 test loss: 1.931243
[Epoch 101; Iter    20/   55] train: loss: 0.1021599
[Epoch 101; Iter    50/   55] train: loss: 0.0128181
[Epoch 101] ogbg-molbbbp: 0.960769 val loss: 0.483671
[Epoch 101] ogbg-molbbbp: 0.693383 test loss: 2.179659
[Epoch 102; Iter    25/   55] train: loss: 0.0300631
[Epoch 102; Iter    55/   55] train: loss: 0.0019927
[Epoch 102] ogbg-molbbbp: 0.961366 val loss: 0.432523
[Epoch 102] ogbg-molbbbp: 0.680170 test loss: 2.035860
[Epoch 103; Iter    30/   55] train: loss: 0.0011606
[Epoch 103] ogbg-molbbbp: 0.958080 val loss: 0.467250
[Epoch 103] ogbg-molbbbp: 0.673997 test loss: 2.265918
[Epoch 104; Iter     5/   55] train: loss: 0.0054387
[Epoch 104; Iter    35/   55] train: loss: 0.0067553
[Epoch 104] ogbg-molbbbp: 0.962860 val loss: 0.394883
[Epoch 104] ogbg-molbbbp: 0.678048 test loss: 2.125207
[Epoch 105; Iter    10/   55] train: loss: 0.0030507
[Epoch 105; Iter    40/   55] train: loss: 0.0217470
[Epoch 105] ogbg-molbbbp: 0.960570 val loss: 0.458680
[Epoch 105] ogbg-molbbbp: 0.684028 test loss: 2.212278
[Epoch 106; Iter    15/   55] train: loss: 0.0014652
[Epoch 106; Iter    45/   55] train: loss: 0.0109256
[Epoch 106] ogbg-molbbbp: 0.954794 val loss: 0.515485
[Epoch 106] ogbg-molbbbp: 0.669078 test loss: 2.275990
[Epoch 107; Iter    20/   55] train: loss: 0.0015381
[Epoch 107; Iter    50/   55] train: loss: 0.0282707
[Epoch 107] ogbg-molbbbp: 0.953102 val loss: 0.479335
[Epoch 107] ogbg-molbbbp: 0.679880 test loss: 2.021689
[Epoch 108; Iter    25/   55] train: loss: 0.0104508
[Epoch 108; Iter    55/   55] train: loss: 0.1367371
[Epoch 108] ogbg-molbbbp: 0.953799 val loss: 0.586504
[Epoch 108] ogbg-molbbbp: 0.696470 test loss: 2.247551
[Epoch 109; Iter    30/   55] train: loss: 0.0026135
[Epoch 73] ogbg-molbbbp: 0.792832 test loss: 1.503772
[Epoch 74; Iter     6/   48] train: loss: 0.0397099
[Epoch 74; Iter    36/   48] train: loss: 0.0579289
[Epoch 74] ogbg-molbbbp: 0.925044 val loss: 0.219025
[Epoch 74] ogbg-molbbbp: 0.782841 test loss: 1.192894
[Epoch 75; Iter    18/   48] train: loss: 0.4922083
[Epoch 75; Iter    48/   48] train: loss: 0.1070471
[Epoch 75] ogbg-molbbbp: 0.974780 val loss: 0.107529
[Epoch 75] ogbg-molbbbp: 0.786022 test loss: 1.157514
[Epoch 76; Iter    30/   48] train: loss: 0.0379182
[Epoch 76] ogbg-molbbbp: 0.977947 val loss: 0.109944
[Epoch 76] ogbg-molbbbp: 0.784946 test loss: 1.335946
[Epoch 77; Iter    12/   48] train: loss: 0.0327612
[Epoch 77; Iter    42/   48] train: loss: 0.0190030
[Epoch 77] ogbg-molbbbp: 0.970440 val loss: 0.143538
[Epoch 77] ogbg-molbbbp: 0.777599 test loss: 1.052721
[Epoch 78; Iter    24/   48] train: loss: 0.0822770
[Epoch 78] ogbg-molbbbp: 0.993196 val loss: 0.156860
[Epoch 78] ogbg-molbbbp: 0.781183 test loss: 0.942308
[Epoch 79; Iter     6/   48] train: loss: 0.0741012
[Epoch 79; Iter    36/   48] train: loss: 0.0255768
[Epoch 79] ogbg-molbbbp: 0.964575 val loss: 0.178866
[Epoch 79] ogbg-molbbbp: 0.758826 test loss: 1.441699
[Epoch 80; Iter    18/   48] train: loss: 0.0593455
[Epoch 80; Iter    48/   48] train: loss: 0.0058229
[Epoch 80] ogbg-molbbbp: 0.973255 val loss: 0.161895
[Epoch 80] ogbg-molbbbp: 0.797357 test loss: 1.272395
[Epoch 81; Iter    30/   48] train: loss: 0.1471847
[Epoch 81] ogbg-molbbbp: 0.966100 val loss: 0.204652
[Epoch 81] ogbg-molbbbp: 0.786649 test loss: 1.061021
[Epoch 82; Iter    12/   48] train: loss: 0.1030198
[Epoch 82; Iter    42/   48] train: loss: 0.1478328
[Epoch 82] ogbg-molbbbp: 0.969150 val loss: 0.132498
[Epoch 82] ogbg-molbbbp: 0.772536 test loss: 1.313984
[Epoch 83; Iter    24/   48] train: loss: 0.0132574
[Epoch 83] ogbg-molbbbp: 0.973255 val loss: 0.147481
[Epoch 83] ogbg-molbbbp: 0.799776 test loss: 1.157401
[Epoch 84; Iter     6/   48] train: loss: 0.1493240
[Epoch 84; Iter    36/   48] train: loss: 0.0201627
[Epoch 84] ogbg-molbbbp: 0.972551 val loss: 0.159855
[Epoch 84] ogbg-molbbbp: 0.782213 test loss: 1.162969
[Epoch 85; Iter    18/   48] train: loss: 0.0356792
[Epoch 85; Iter    48/   48] train: loss: 0.0335101
[Epoch 85] ogbg-molbbbp: 0.967155 val loss: 0.142891
[Epoch 85] ogbg-molbbbp: 0.775986 test loss: 1.388860
[Epoch 86; Iter    30/   48] train: loss: 0.0185073
[Epoch 86] ogbg-molbbbp: 0.956950 val loss: 0.171400
[Epoch 86] ogbg-molbbbp: 0.775358 test loss: 1.326594
[Epoch 87; Iter    12/   48] train: loss: 0.1421688
[Epoch 87; Iter    42/   48] train: loss: 0.0188476
[Epoch 87] ogbg-molbbbp: 0.965982 val loss: 0.126240
[Epoch 87] ogbg-molbbbp: 0.758781 test loss: 1.288835
[Epoch 88; Iter    24/   48] train: loss: 0.0692233
[Epoch 88] ogbg-molbbbp: 0.974311 val loss: 0.140067
[Epoch 88] ogbg-molbbbp: 0.775493 test loss: 1.151462
[Epoch 89; Iter     6/   48] train: loss: 0.1085225
[Epoch 89; Iter    36/   48] train: loss: 0.1688534
[Epoch 89] ogbg-molbbbp: 0.973490 val loss: 0.165998
[Epoch 89] ogbg-molbbbp: 0.789830 test loss: 1.112366
[Epoch 90; Iter    18/   48] train: loss: 0.0482544
[Epoch 90; Iter    48/   48] train: loss: 0.0102071
[Epoch 90] ogbg-molbbbp: 0.988152 val loss: 0.086543
[Epoch 90] ogbg-molbbbp: 0.775672 test loss: 1.193040
[Epoch 91; Iter    30/   48] train: loss: 0.0210585
[Epoch 91] ogbg-molbbbp: 0.980411 val loss: 0.121450
[Epoch 91] ogbg-molbbbp: 0.784543 test loss: 1.313031
[Epoch 92; Iter    12/   48] train: loss: 0.0045902
[Epoch 92; Iter    42/   48] train: loss: 0.0122019
[Epoch 92] ogbg-molbbbp: 0.982639 val loss: 0.140396
[Epoch 92] ogbg-molbbbp: 0.791891 test loss: 1.040156
[Epoch 93; Iter    24/   48] train: loss: 0.0202055
[Epoch 93] ogbg-molbbbp: 0.982757 val loss: 0.150587
[Epoch 93] ogbg-molbbbp: 0.796192 test loss: 1.177762
[Epoch 94; Iter     6/   48] train: loss: 0.0944693
[Epoch 94; Iter    36/   48] train: loss: 0.0095546
[Epoch 94] ogbg-molbbbp: 0.975601 val loss: 0.163238
[Epoch 94] ogbg-molbbbp: 0.787007 test loss: 1.106850
[Epoch 95; Iter    18/   48] train: loss: 0.0066891
[Epoch 95; Iter    48/   48] train: loss: 0.1199148
[Epoch 95] ogbg-molbbbp: 0.976070 val loss: 0.119933
[Epoch 95] ogbg-molbbbp: 0.788262 test loss: 1.482077
[Epoch 96; Iter    30/   48] train: loss: 0.0068972
[Epoch 96] ogbg-molbbbp: 0.973724 val loss: 0.120668
[Epoch 96] ogbg-molbbbp: 0.789247 test loss: 1.295296
[Epoch 97; Iter    12/   48] train: loss: 0.0068374
[Epoch 97; Iter    42/   48] train: loss: 0.0039913
[Epoch 97] ogbg-molbbbp: 0.971730 val loss: 0.130063
[Epoch 97] ogbg-molbbbp: 0.788799 test loss: 1.285786
[Epoch 98; Iter    24/   48] train: loss: 0.0021925
[Epoch 98] ogbg-molbbbp: 0.977947 val loss: 0.151739
[Epoch 98] ogbg-molbbbp: 0.781631 test loss: 1.295118
[Epoch 99; Iter     6/   48] train: loss: 0.0196351
[Epoch 99; Iter    36/   48] train: loss: 0.0307783
[Epoch 99] ogbg-molbbbp: 0.970674 val loss: 0.135826
[Epoch 99] ogbg-molbbbp: 0.782527 test loss: 1.335273
[Epoch 100; Iter    18/   48] train: loss: 0.0087990
[Epoch 100; Iter    48/   48] train: loss: 0.0393098
[Epoch 100] ogbg-molbbbp: 0.974897 val loss: 0.147317
[Epoch 100] ogbg-molbbbp: 0.777151 test loss: 1.299140
[Epoch 101; Iter    30/   48] train: loss: 0.0043143
[Epoch 101] ogbg-molbbbp: 0.980176 val loss: 0.117092
[Epoch 101] ogbg-molbbbp: 0.794265 test loss: 1.300640
[Epoch 102; Iter    12/   48] train: loss: 0.0049767
[Epoch 102; Iter    42/   48] train: loss: 0.0030134
[Epoch 102] ogbg-molbbbp: 0.978065 val loss: 0.137005
[Epoch 102] ogbg-molbbbp: 0.790233 test loss: 1.334012
[Epoch 103; Iter    24/   48] train: loss: 0.0301639
[Epoch 103] ogbg-molbbbp: 0.981935 val loss: 0.138994
[Epoch 103] ogbg-molbbbp: 0.785484 test loss: 1.434137
[Epoch 104; Iter     6/   48] train: loss: 0.0049895
[Epoch 104; Iter    36/   48] train: loss: 0.0647109
[Epoch 104] ogbg-molbbbp: 0.972551 val loss: 0.135027
[Epoch 104] ogbg-molbbbp: 0.773297 test loss: 1.317954
[Epoch 105; Iter    18/   48] train: loss: 0.0106253
[Epoch 105; Iter    48/   48] train: loss: 0.0004313
[Epoch 105] ogbg-molbbbp: 0.980645 val loss: 0.177418
[Epoch 105] ogbg-molbbbp: 0.791801 test loss: 1.433725
[Epoch 106; Iter    30/   48] train: loss: 0.0137717
[Epoch 106] ogbg-molbbbp: 0.976657 val loss: 0.131259
[Epoch 106] ogbg-molbbbp: 0.785842 test loss: 1.380147
[Epoch 107; Iter    12/   48] train: loss: 0.0011082
[Epoch 107; Iter    42/   48] train: loss: 0.0093310
[Epoch 107] ogbg-molbbbp: 0.976540 val loss: 0.161301
[Epoch 107] ogbg-molbbbp: 0.775224 test loss: 1.434858
[Epoch 108; Iter    24/   48] train: loss: 0.0025398
[Epoch 108] ogbg-molbbbp: 0.969032 val loss: 0.145513
[Epoch 108] ogbg-molbbbp: 0.780869 test loss: 1.496913
[Epoch 109; Iter     6/   48] train: loss: 0.0013813
[Epoch 109; Iter    36/   48] train: loss: 0.0609870
[Epoch 109] ogbg-molbbbp: 0.979238 val loss: 0.129050
[Epoch 109] ogbg-molbbbp: 0.754928 test loss: 1.431749
[Epoch 110; Iter    18/   48] train: loss: 0.0103217
[Epoch 110; Iter    48/   48] train: loss: 0.0111757
[Epoch 110] ogbg-molbbbp: 0.968328 val loss: 0.143755
[Epoch 110] ogbg-molbbbp: 0.770699 test loss: 1.504033
[Epoch 111; Iter    30/   48] train: loss: 0.0091546
[Epoch 111] ogbg-molbbbp: 0.979589 val loss: 0.122299
[Epoch 111] ogbg-molbbbp: 0.795251 test loss: 1.408914
[Epoch 112; Iter    12/   48] train: loss: 0.0293485
[Epoch 112; Iter    42/   48] train: loss: 0.0314237
[Epoch 112] ogbg-molbbbp: 0.968915 val loss: 0.141158
[Epoch 112] ogbg-molbbbp: 0.771729 test loss: 1.525770
[Epoch 113; Iter    24/   48] train: loss: 0.0246239
[Epoch 113] ogbg-molbbbp: 0.977126 val loss: 0.142632
[Epoch 113] ogbg-molbbbp: 0.777375 test loss: 1.476688
[Epoch 114; Iter     6/   48] train: loss: 0.0047053
[Epoch 114; Iter    36/   48] train: loss: 0.0715758
[Epoch 114] ogbg-molbbbp: 0.971261 val loss: 0.153482
[Epoch 114] ogbg-molbbbp: 0.780421 test loss: 1.509177
[Epoch 115; Iter    18/   48] train: loss: 0.0026483
[Epoch 115; Iter    48/   48] train: loss: 0.0131000
[Epoch 115] ogbg-molbbbp: 0.975718 val loss: 0.121753
[Epoch 115] ogbg-molbbbp: 0.790591 test loss: 1.503797
[Epoch 73] ogbg-molbbbp: 0.795923 test loss: 1.021702
[Epoch 74; Iter     6/   48] train: loss: 0.1069517
[Epoch 74; Iter    36/   48] train: loss: 0.0765205
[Epoch 74] ogbg-molbbbp: 0.978651 val loss: 0.174623
[Epoch 74] ogbg-molbbbp: 0.789516 test loss: 1.077649
[Epoch 75; Iter    18/   48] train: loss: 0.0997491
[Epoch 75; Iter    48/   48] train: loss: 0.1625614
[Epoch 75] ogbg-molbbbp: 0.988856 val loss: 0.108479
[Epoch 75] ogbg-molbbbp: 0.766129 test loss: 1.143554
[Epoch 76; Iter    30/   48] train: loss: 0.1018975
[Epoch 76] ogbg-molbbbp: 0.985689 val loss: 0.153026
[Epoch 76] ogbg-molbbbp: 0.760529 test loss: 1.058540
[Epoch 77; Iter    12/   48] train: loss: 0.1270533
[Epoch 77; Iter    42/   48] train: loss: 0.0319824
[Epoch 77] ogbg-molbbbp: 0.988270 val loss: 0.144332
[Epoch 77] ogbg-molbbbp: 0.767608 test loss: 1.184127
[Epoch 78; Iter    24/   48] train: loss: 0.0839578
[Epoch 78] ogbg-molbbbp: 0.974545 val loss: 0.117666
[Epoch 78] ogbg-molbbbp: 0.761783 test loss: 1.173061
[Epoch 79; Iter     6/   48] train: loss: 0.0608123
[Epoch 79; Iter    36/   48] train: loss: 0.0364188
[Epoch 79] ogbg-molbbbp: 0.979003 val loss: 0.113613
[Epoch 79] ogbg-molbbbp: 0.759185 test loss: 1.264496
[Epoch 80; Iter    18/   48] train: loss: 0.0837718
[Epoch 80; Iter    48/   48] train: loss: 0.1024255
[Epoch 80] ogbg-molbbbp: 0.977478 val loss: 0.145662
[Epoch 80] ogbg-molbbbp: 0.774194 test loss: 1.148771
[Epoch 81; Iter    30/   48] train: loss: 0.0495456
[Epoch 81] ogbg-molbbbp: 0.977713 val loss: 0.116427
[Epoch 81] ogbg-molbbbp: 0.750986 test loss: 1.334473
[Epoch 82; Iter    12/   48] train: loss: 0.0377843
[Epoch 82; Iter    42/   48] train: loss: 0.0496280
[Epoch 82] ogbg-molbbbp: 0.984047 val loss: 0.120674
[Epoch 82] ogbg-molbbbp: 0.746505 test loss: 1.399118
[Epoch 83; Iter    24/   48] train: loss: 0.0375075
[Epoch 83] ogbg-molbbbp: 0.979824 val loss: 0.095517
[Epoch 83] ogbg-molbbbp: 0.767832 test loss: 1.450301
[Epoch 84; Iter     6/   48] train: loss: 0.0100866
[Epoch 84; Iter    36/   48] train: loss: 0.0151249
[Epoch 84] ogbg-molbbbp: 0.983930 val loss: 0.139459
[Epoch 84] ogbg-molbbbp: 0.767384 test loss: 1.212020
[Epoch 85; Iter    18/   48] train: loss: 0.1396950
[Epoch 85; Iter    48/   48] train: loss: 0.0100918
[Epoch 85] ogbg-molbbbp: 0.986276 val loss: 0.106216
[Epoch 85] ogbg-molbbbp: 0.757124 test loss: 1.373897
[Epoch 86; Iter    30/   48] train: loss: 0.0155977
[Epoch 86] ogbg-molbbbp: 0.986979 val loss: 0.113114
[Epoch 86] ogbg-molbbbp: 0.760708 test loss: 1.358669
[Epoch 87; Iter    12/   48] train: loss: 0.0042101
[Epoch 87; Iter    42/   48] train: loss: 0.0386396
[Epoch 87] ogbg-molbbbp: 0.972551 val loss: 0.106525
[Epoch 87] ogbg-molbbbp: 0.755376 test loss: 1.396566
[Epoch 88; Iter    24/   48] train: loss: 0.0387009
[Epoch 88] ogbg-molbbbp: 0.986158 val loss: 0.113141
[Epoch 88] ogbg-molbbbp: 0.765636 test loss: 1.294689
[Epoch 89; Iter     6/   48] train: loss: 0.0296897
[Epoch 89; Iter    36/   48] train: loss: 0.0233263
[Epoch 89] ogbg-molbbbp: 0.986862 val loss: 0.091886
[Epoch 89] ogbg-molbbbp: 0.782168 test loss: 1.457169
[Epoch 90; Iter    18/   48] train: loss: 0.0061413
[Epoch 90; Iter    48/   48] train: loss: 0.0313026
[Epoch 90] ogbg-molbbbp: 0.979472 val loss: 0.134345
[Epoch 90] ogbg-molbbbp: 0.764471 test loss: 1.305192
[Epoch 91; Iter    30/   48] train: loss: 0.1346353
[Epoch 91] ogbg-molbbbp: 0.978182 val loss: 0.151993
[Epoch 91] ogbg-molbbbp: 0.788754 test loss: 1.208437
[Epoch 92; Iter    12/   48] train: loss: 0.0148641
[Epoch 92; Iter    42/   48] train: loss: 0.0335416
[Epoch 92] ogbg-molbbbp: 0.983460 val loss: 0.210908
[Epoch 92] ogbg-molbbbp: 0.778629 test loss: 1.208291
[Epoch 93; Iter    24/   48] train: loss: 0.0854779
[Epoch 93] ogbg-molbbbp: 0.985455 val loss: 0.110606
[Epoch 93] ogbg-molbbbp: 0.764740 test loss: 1.444974
[Epoch 94; Iter     6/   48] train: loss: 0.0142518
[Epoch 94; Iter    36/   48] train: loss: 0.0217283
[Epoch 94] ogbg-molbbbp: 0.979941 val loss: 0.192913
[Epoch 94] ogbg-molbbbp: 0.774821 test loss: 1.379224
[Epoch 95; Iter    18/   48] train: loss: 0.1563346
[Epoch 95; Iter    48/   48] train: loss: 0.1741297
[Epoch 95] ogbg-molbbbp: 0.982639 val loss: 0.534157
[Epoch 95] ogbg-molbbbp: 0.759005 test loss: 1.435986
[Epoch 96; Iter    30/   48] train: loss: 0.0431579
[Epoch 96] ogbg-molbbbp: 0.976657 val loss: 0.293263
[Epoch 96] ogbg-molbbbp: 0.763530 test loss: 1.476907
[Epoch 97; Iter    12/   48] train: loss: 0.0089921
[Epoch 97; Iter    42/   48] train: loss: 0.0173587
[Epoch 97] ogbg-molbbbp: 0.981349 val loss: 0.299136
[Epoch 97] ogbg-molbbbp: 0.776075 test loss: 1.281724
[Epoch 98; Iter    24/   48] train: loss: 0.0196530
[Epoch 98] ogbg-molbbbp: 0.984751 val loss: 0.126909
[Epoch 98] ogbg-molbbbp: 0.781004 test loss: 1.477155
[Epoch 99; Iter     6/   48] train: loss: 0.0118076
[Epoch 99; Iter    36/   48] train: loss: 0.0048832
[Epoch 99] ogbg-molbbbp: 0.971730 val loss: 0.333518
[Epoch 99] ogbg-molbbbp: 0.770430 test loss: 1.426583
[Epoch 100; Iter    18/   48] train: loss: 0.0504503
[Epoch 100; Iter    48/   48] train: loss: 0.0045015
[Epoch 100] ogbg-molbbbp: 0.991320 val loss: 0.129955
[Epoch 100] ogbg-molbbbp: 0.788262 test loss: 1.321978
[Epoch 101; Iter    30/   48] train: loss: 0.0114551
[Epoch 101] ogbg-molbbbp: 0.981584 val loss: 0.678214
[Epoch 101] ogbg-molbbbp: 0.774194 test loss: 1.353096
[Epoch 102; Iter    12/   48] train: loss: 0.0294565
[Epoch 102; Iter    42/   48] train: loss: 0.0229986
[Epoch 102] ogbg-molbbbp: 0.989912 val loss: 0.108445
[Epoch 102] ogbg-molbbbp: 0.792204 test loss: 1.425721
[Epoch 103; Iter    24/   48] train: loss: 0.0151796
[Epoch 103] ogbg-molbbbp: 0.985572 val loss: 0.090202
[Epoch 103] ogbg-molbbbp: 0.770699 test loss: 1.536330
[Epoch 104; Iter     6/   48] train: loss: 0.1000234
[Epoch 104; Iter    36/   48] train: loss: 0.0089609
[Epoch 104] ogbg-molbbbp: 0.987331 val loss: 0.124682
[Epoch 104] ogbg-molbbbp: 0.761201 test loss: 1.406174
[Epoch 105; Iter    18/   48] train: loss: 0.0092367
[Epoch 105; Iter    48/   48] train: loss: 0.0111009
[Epoch 105] ogbg-molbbbp: 0.983812 val loss: 0.185866
[Epoch 105] ogbg-molbbbp: 0.777061 test loss: 1.379136
[Epoch 106; Iter    30/   48] train: loss: 0.0679123
[Epoch 106] ogbg-molbbbp: 0.982522 val loss: 0.351610
[Epoch 106] ogbg-molbbbp: 0.779346 test loss: 1.398536
[Epoch 107; Iter    12/   48] train: loss: 0.0105173
[Epoch 107; Iter    42/   48] train: loss: 0.0654921
[Epoch 107] ogbg-molbbbp: 0.984282 val loss: 0.260347
[Epoch 107] ogbg-molbbbp: 0.771550 test loss: 1.447204
[Epoch 108; Iter    24/   48] train: loss: 0.0262107
[Epoch 108] ogbg-molbbbp: 0.982170 val loss: 0.556760
[Epoch 108] ogbg-molbbbp: 0.778181 test loss: 1.342842
[Epoch 109; Iter     6/   48] train: loss: 0.0413375
[Epoch 109; Iter    36/   48] train: loss: 0.0270540
[Epoch 109] ogbg-molbbbp: 0.984985 val loss: 0.263176
[Epoch 109] ogbg-molbbbp: 0.778629 test loss: 1.511268
[Epoch 110; Iter    18/   48] train: loss: 0.0047535
[Epoch 110; Iter    48/   48] train: loss: 0.1365517
[Epoch 110] ogbg-molbbbp: 0.982522 val loss: 0.228178
[Epoch 110] ogbg-molbbbp: 0.770923 test loss: 1.511376
[Epoch 111; Iter    30/   48] train: loss: 0.0086275
[Epoch 111] ogbg-molbbbp: 0.983812 val loss: 0.400585
[Epoch 111] ogbg-molbbbp: 0.777419 test loss: 1.475572
[Epoch 112; Iter    12/   48] train: loss: 0.0091177
[Epoch 112; Iter    42/   48] train: loss: 0.0056335
[Epoch 112] ogbg-molbbbp: 0.985924 val loss: 0.597289
[Epoch 112] ogbg-molbbbp: 0.774059 test loss: 1.402347
[Epoch 113; Iter    24/   48] train: loss: 0.0033797
[Epoch 113] ogbg-molbbbp: 0.983226 val loss: 0.153947
[Epoch 113] ogbg-molbbbp: 0.767563 test loss: 1.508206
[Epoch 114; Iter     6/   48] train: loss: 0.0062013
[Epoch 114; Iter    36/   48] train: loss: 0.0014443
[Epoch 114] ogbg-molbbbp: 0.981466 val loss: 0.518226
[Epoch 114] ogbg-molbbbp: 0.776478 test loss: 1.381860
[Epoch 115; Iter    18/   48] train: loss: 0.0017366
[Epoch 115; Iter    48/   48] train: loss: 0.2195079
[Epoch 115] ogbg-molbbbp: 0.983578 val loss: 0.206385
[Epoch 115] ogbg-molbbbp: 0.770251 test loss: 1.483447
[Epoch 73] ogbg-molbbbp: 0.755511 test loss: 1.333619
[Epoch 74; Iter     6/   48] train: loss: 0.0247427
[Epoch 74; Iter    36/   48] train: loss: 0.0946392
[Epoch 74] ogbg-molbbbp: 0.975836 val loss: 0.133627
[Epoch 74] ogbg-molbbbp: 0.781138 test loss: 1.083073
[Epoch 75; Iter    18/   48] train: loss: 0.0119785
[Epoch 75; Iter    48/   48] train: loss: 0.1830408
[Epoch 75] ogbg-molbbbp: 0.988856 val loss: 0.142602
[Epoch 75] ogbg-molbbbp: 0.771640 test loss: 1.039342
[Epoch 76; Iter    30/   48] train: loss: 0.0205310
[Epoch 76] ogbg-molbbbp: 0.987801 val loss: 0.101229
[Epoch 76] ogbg-molbbbp: 0.766756 test loss: 1.447274
[Epoch 77; Iter    12/   48] train: loss: 0.0068990
[Epoch 77; Iter    42/   48] train: loss: 0.0331284
[Epoch 77] ogbg-molbbbp: 0.988739 val loss: 0.178164
[Epoch 77] ogbg-molbbbp: 0.801299 test loss: 1.009890
[Epoch 78; Iter    24/   48] train: loss: 0.0926360
[Epoch 78] ogbg-molbbbp: 0.963519 val loss: 0.138700
[Epoch 78] ogbg-molbbbp: 0.781900 test loss: 1.111161
[Epoch 79; Iter     6/   48] train: loss: 0.0980444
[Epoch 79; Iter    36/   48] train: loss: 0.1170104
[Epoch 79] ogbg-molbbbp: 0.976305 val loss: 0.110463
[Epoch 79] ogbg-molbbbp: 0.776568 test loss: 1.193592
[Epoch 80; Iter    18/   48] train: loss: 0.0093491
[Epoch 80; Iter    48/   48] train: loss: 0.0043082
[Epoch 80] ogbg-molbbbp: 0.986158 val loss: 0.111775
[Epoch 80] ogbg-molbbbp: 0.787814 test loss: 1.219193
[Epoch 81; Iter    30/   48] train: loss: 0.0162184
[Epoch 81] ogbg-molbbbp: 0.985455 val loss: 0.093793
[Epoch 81] ogbg-molbbbp: 0.786066 test loss: 1.330291
[Epoch 82; Iter    12/   48] train: loss: 0.0989041
[Epoch 82; Iter    42/   48] train: loss: 0.0078831
[Epoch 82] ogbg-molbbbp: 0.980762 val loss: 0.110711
[Epoch 82] ogbg-molbbbp: 0.784005 test loss: 1.242006
[Epoch 83; Iter    24/   48] train: loss: 0.0078872
[Epoch 83] ogbg-molbbbp: 0.985806 val loss: 0.112406
[Epoch 83] ogbg-molbbbp: 0.775851 test loss: 1.171888
[Epoch 84; Iter     6/   48] train: loss: 0.0064082
[Epoch 84; Iter    36/   48] train: loss: 0.0713432
[Epoch 84] ogbg-molbbbp: 0.983695 val loss: 0.111346
[Epoch 84] ogbg-molbbbp: 0.778629 test loss: 1.405201
[Epoch 85; Iter    18/   48] train: loss: 0.0878387
[Epoch 85; Iter    48/   48] train: loss: 0.0063668
[Epoch 85] ogbg-molbbbp: 0.988504 val loss: 0.104973
[Epoch 85] ogbg-molbbbp: 0.767697 test loss: 1.334993
[Epoch 86; Iter    30/   48] train: loss: 0.0023070
[Epoch 86] ogbg-molbbbp: 0.976657 val loss: 0.106339
[Epoch 86] ogbg-molbbbp: 0.764651 test loss: 1.236064
[Epoch 87; Iter    12/   48] train: loss: 0.0417888
[Epoch 87; Iter    42/   48] train: loss: 0.0614605
[Epoch 87] ogbg-molbbbp: 0.973372 val loss: 0.148116
[Epoch 87] ogbg-molbbbp: 0.763262 test loss: 1.210621
[Epoch 88; Iter    24/   48] train: loss: 0.0074944
[Epoch 88] ogbg-molbbbp: 0.987683 val loss: 0.119778
[Epoch 88] ogbg-molbbbp: 0.773253 test loss: 1.397349
[Epoch 89; Iter     6/   48] train: loss: 0.0211838
[Epoch 89; Iter    36/   48] train: loss: 0.0477421
[Epoch 89] ogbg-molbbbp: 0.982405 val loss: 0.100451
[Epoch 89] ogbg-molbbbp: 0.773656 test loss: 1.512958
[Epoch 90; Iter    18/   48] train: loss: 0.0061993
[Epoch 90; Iter    48/   48] train: loss: 0.0089300
[Epoch 90] ogbg-molbbbp: 0.975718 val loss: 0.102173
[Epoch 90] ogbg-molbbbp: 0.769848 test loss: 1.438012
[Epoch 91; Iter    30/   48] train: loss: 0.0288056
[Epoch 91] ogbg-molbbbp: 0.987801 val loss: 0.101381
[Epoch 91] ogbg-molbbbp: 0.786694 test loss: 1.337479
[Epoch 92; Iter    12/   48] train: loss: 0.0363986
[Epoch 92; Iter    42/   48] train: loss: 0.0627002
[Epoch 92] ogbg-molbbbp: 0.975601 val loss: 0.130080
[Epoch 92] ogbg-molbbbp: 0.778315 test loss: 1.313556
[Epoch 93; Iter    24/   48] train: loss: 0.0070997
[Epoch 93] ogbg-molbbbp: 0.982639 val loss: 0.096837
[Epoch 93] ogbg-molbbbp: 0.759946 test loss: 1.376344
[Epoch 94; Iter     6/   48] train: loss: 0.0117397
[Epoch 94; Iter    36/   48] train: loss: 0.0228876
[Epoch 94] ogbg-molbbbp: 0.987683 val loss: 0.105126
[Epoch 94] ogbg-molbbbp: 0.783826 test loss: 1.332614
[Epoch 95; Iter    18/   48] train: loss: 0.0837558
[Epoch 95; Iter    48/   48] train: loss: 0.0167382
[Epoch 95] ogbg-molbbbp: 0.990029 val loss: 0.089745
[Epoch 95] ogbg-molbbbp: 0.786918 test loss: 1.366997
[Epoch 96; Iter    30/   48] train: loss: 0.0054380
[Epoch 96] ogbg-molbbbp: 0.990381 val loss: 0.102776
[Epoch 96] ogbg-molbbbp: 0.785081 test loss: 1.378793
[Epoch 97; Iter    12/   48] train: loss: 0.0284587
[Epoch 97; Iter    42/   48] train: loss: 0.0394491
[Epoch 97] ogbg-molbbbp: 0.984399 val loss: 0.119949
[Epoch 97] ogbg-molbbbp: 0.784991 test loss: 1.308845
[Epoch 98; Iter    24/   48] train: loss: 0.0089287
[Epoch 98] ogbg-molbbbp: 0.987918 val loss: 0.101906
[Epoch 98] ogbg-molbbbp: 0.770789 test loss: 1.275040
[Epoch 99; Iter     6/   48] train: loss: 0.0156454
[Epoch 99; Iter    36/   48] train: loss: 0.0185357
[Epoch 99] ogbg-molbbbp: 0.985572 val loss: 0.126078
[Epoch 99] ogbg-molbbbp: 0.776837 test loss: 1.284457
[Epoch 100; Iter    18/   48] train: loss: 0.0222658
[Epoch 100; Iter    48/   48] train: loss: 0.0034606
[Epoch 100] ogbg-molbbbp: 0.951202 val loss: 0.152646
[Epoch 100] ogbg-molbbbp: 0.773387 test loss: 1.707212
[Epoch 101; Iter    30/   48] train: loss: 0.0590404
[Epoch 101] ogbg-molbbbp: 0.984047 val loss: 0.114968
[Epoch 101] ogbg-molbbbp: 0.779077 test loss: 1.326832
[Epoch 102; Iter    12/   48] train: loss: 0.0530475
[Epoch 102; Iter    42/   48] train: loss: 0.0550588
[Epoch 102] ogbg-molbbbp: 0.985455 val loss: 0.114670
[Epoch 102] ogbg-molbbbp: 0.793145 test loss: 1.264070
[Epoch 103; Iter    24/   48] train: loss: 0.0615057
[Epoch 103] ogbg-molbbbp: 0.986041 val loss: 0.103971
[Epoch 103] ogbg-molbbbp: 0.789113 test loss: 1.556228
[Epoch 104; Iter     6/   48] train: loss: 0.0023377
[Epoch 104; Iter    36/   48] train: loss: 0.0081893
[Epoch 104] ogbg-molbbbp: 0.983695 val loss: 0.106994
[Epoch 104] ogbg-molbbbp: 0.772446 test loss: 1.422960
[Epoch 105; Iter    18/   48] train: loss: 0.0309474
[Epoch 105; Iter    48/   48] train: loss: 0.0030921
[Epoch 105] ogbg-molbbbp: 0.981349 val loss: 0.104541
[Epoch 105] ogbg-molbbbp: 0.769310 test loss: 1.427347
[Epoch 106; Iter    30/   48] train: loss: 0.0035030
[Epoch 106] ogbg-molbbbp: 0.986276 val loss: 0.102827
[Epoch 106] ogbg-molbbbp: 0.778002 test loss: 1.471004
[Epoch 107; Iter    12/   48] train: loss: 0.0313463
[Epoch 107; Iter    42/   48] train: loss: 0.1390396
[Epoch 107] ogbg-molbbbp: 0.986862 val loss: 0.103881
[Epoch 107] ogbg-molbbbp: 0.775717 test loss: 1.381673
[Epoch 108; Iter    24/   48] train: loss: 0.0059956
[Epoch 108] ogbg-molbbbp: 0.984868 val loss: 0.109226
[Epoch 108] ogbg-molbbbp: 0.775448 test loss: 1.347756
[Epoch 109; Iter     6/   48] train: loss: 0.0029978
[Epoch 109; Iter    36/   48] train: loss: 0.0871867
[Epoch 109] ogbg-molbbbp: 0.976774 val loss: 0.107869
[Epoch 109] ogbg-molbbbp: 0.769937 test loss: 1.468837
[Epoch 110; Iter    18/   48] train: loss: 0.0404703
[Epoch 110; Iter    48/   48] train: loss: 0.0618486
[Epoch 110] ogbg-molbbbp: 0.984399 val loss: 0.109271
[Epoch 110] ogbg-molbbbp: 0.774910 test loss: 1.467934
[Epoch 111; Iter    30/   48] train: loss: 0.0031397
[Epoch 111] ogbg-molbbbp: 0.985689 val loss: 0.111855
[Epoch 111] ogbg-molbbbp: 0.777643 test loss: 1.454985
[Epoch 112; Iter    12/   48] train: loss: 0.0010185
[Epoch 112; Iter    42/   48] train: loss: 0.0012297
[Epoch 112] ogbg-molbbbp: 0.979941 val loss: 0.139160
[Epoch 112] ogbg-molbbbp: 0.768369 test loss: 1.363125
[Epoch 113; Iter    24/   48] train: loss: 0.0019920
[Epoch 113] ogbg-molbbbp: 0.977713 val loss: 0.117441
[Epoch 113] ogbg-molbbbp: 0.767742 test loss: 1.433536
[Epoch 114; Iter     6/   48] train: loss: 0.0028068
[Epoch 114; Iter    36/   48] train: loss: 0.0449158
[Epoch 114] ogbg-molbbbp: 0.981818 val loss: 0.099345
[Epoch 114] ogbg-molbbbp: 0.769265 test loss: 1.494293
[Epoch 115; Iter    18/   48] train: loss: 0.0387522
[Epoch 115; Iter    48/   48] train: loss: 0.0005364
[Epoch 115] ogbg-molbbbp: 0.980059 val loss: 0.106684
[Epoch 115] ogbg-molbbbp: 0.756989 test loss: 1.524127
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.0249871
[Epoch 116] ogbg-molbbbp: 0.982405 val loss: 0.170251
[Epoch 116] ogbg-molbbbp: 0.778315 test loss: 1.454073
[Epoch 117; Iter    12/   48] train: loss: 0.0020656
[Epoch 117; Iter    42/   48] train: loss: 0.0019300
[Epoch 117] ogbg-molbbbp: 0.978886 val loss: 0.262301
[Epoch 117] ogbg-molbbbp: 0.774149 test loss: 1.465973
[Epoch 118; Iter    24/   48] train: loss: 0.0053084
[Epoch 118] ogbg-molbbbp: 0.985103 val loss: 0.181897
[Epoch 118] ogbg-molbbbp: 0.779839 test loss: 1.474518
[Epoch 119; Iter     6/   48] train: loss: 0.0018694
[Epoch 119; Iter    36/   48] train: loss: 0.0094659
[Epoch 119] ogbg-molbbbp: 0.983695 val loss: 0.560346
[Epoch 119] ogbg-molbbbp: 0.771909 test loss: 1.512482
[Epoch 120; Iter    18/   48] train: loss: 0.0860965
[Epoch 120; Iter    48/   48] train: loss: 0.0022059
[Epoch 120] ogbg-molbbbp: 0.978886 val loss: 0.946178
[Epoch 120] ogbg-molbbbp: 0.765502 test loss: 1.446710
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 49.
Statistics on  val_best_checkpoint
mean_pred: 3.6597445011138916
std_pred: 4.328078269958496
mean_targets: 0.8986928462982178
std_targets: 0.30222928524017334
prcauc: 0.9997198231774087
rocauc: 0.9975366568914956
ogbg-molbbbp: 0.9975366568914956
BCEWithLogitsLoss: 0.08511450548063625
Statistics on  test
mean_pred: 0.46794000267982483
std_pred: 5.688467979431152
mean_targets: 0.3921568691730499
std_targets: 0.4890310764312744
prcauc: 0.6179482274198325
rocauc: 0.762589605734767
ogbg-molbbbp: 0.762589605734767
BCEWithLogitsLoss: 1.115560309453444
Statistics on  train
mean_pred: 2.8632020950317383
std_pred: 17.72496223449707
mean_targets: 0.816398024559021
std_targets: 0.3872950077056885
prcauc: 0.9928688140312967
rocauc: 0.9768240343347638
ogbg-molbbbp: 0.9768240343347638
BCEWithLogitsLoss: 0.5942124032881111
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.0272668
[Epoch 116] ogbg-molbbbp: 0.980997 val loss: 0.110320
[Epoch 116] ogbg-molbbbp: 0.761425 test loss: 1.440243
[Epoch 117; Iter    12/   48] train: loss: 0.0360107
[Epoch 117; Iter    42/   48] train: loss: 0.0526769
[Epoch 117] ogbg-molbbbp: 0.985689 val loss: 0.097051
[Epoch 117] ogbg-molbbbp: 0.774776 test loss: 1.499557
[Epoch 118; Iter    24/   48] train: loss: 0.0039187
[Epoch 118] ogbg-molbbbp: 0.982522 val loss: 0.108589
[Epoch 118] ogbg-molbbbp: 0.771192 test loss: 1.530464
[Epoch 119; Iter     6/   48] train: loss: 0.0035465
[Epoch 119; Iter    36/   48] train: loss: 0.0528008
[Epoch 119] ogbg-molbbbp: 0.980293 val loss: 0.104474
[Epoch 119] ogbg-molbbbp: 0.771192 test loss: 1.588004
[Epoch 120; Iter    18/   48] train: loss: 0.0040233
[Epoch 120; Iter    48/   48] train: loss: 0.0405188
[Epoch 120] ogbg-molbbbp: 0.978886 val loss: 0.122130
[Epoch 120] ogbg-molbbbp: 0.772536 test loss: 1.428277
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 53.
Statistics on  val_best_checkpoint
mean_pred: 3.4117887020111084
std_pred: 4.315451145172119
mean_targets: 0.8986928462982178
std_targets: 0.30222928524017334
prcauc: 0.9991003194738506
rocauc: 0.9921407624633432
ogbg-molbbbp: 0.9921407624633432
BCEWithLogitsLoss: 0.146302942186594
Statistics on  test
mean_pred: -0.5806529521942139
std_pred: 5.430288314819336
mean_targets: 0.3921568691730499
std_targets: 0.4890310764312744
prcauc: 0.725953752663691
rocauc: 0.8120071684587813
ogbg-molbbbp: 0.8120071684587813
BCEWithLogitsLoss: 0.863473430275917
Statistics on  train
mean_pred: 2.907565116882324
std_pred: 4.911768913269043
mean_targets: 0.816398024559021
std_targets: 0.3872950077056885
prcauc: 0.9948334329310067
rocauc: 0.9800887855060119
ogbg-molbbbp: 0.9800887855060119
BCEWithLogitsLoss: 0.14318671104653427
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.950612 val loss: 0.458649
[Epoch 109] ogbg-molbbbp: 0.590664 test loss: 2.276110
[Epoch 110; Iter     5/   55] train: loss: 0.0117051
[Epoch 110; Iter    35/   55] train: loss: 0.0601507
[Epoch 110] ogbg-molbbbp: 0.951011 val loss: 0.572203
[Epoch 110] ogbg-molbbbp: 0.661073 test loss: 2.121447
[Epoch 111; Iter    10/   55] train: loss: 0.0035314
[Epoch 111; Iter    40/   55] train: loss: 0.0015829
[Epoch 111] ogbg-molbbbp: 0.951807 val loss: 0.475161
[Epoch 111] ogbg-molbbbp: 0.648630 test loss: 2.063926
[Epoch 112; Iter    15/   55] train: loss: 0.0473916
[Epoch 112; Iter    45/   55] train: loss: 0.0019386
[Epoch 112] ogbg-molbbbp: 0.948123 val loss: 0.534620
[Epoch 112] ogbg-molbbbp: 0.643711 test loss: 2.140732
[Epoch 113; Iter    20/   55] train: loss: 0.0032004
[Epoch 113; Iter    50/   55] train: loss: 0.0621041
[Epoch 113] ogbg-molbbbp: 0.950214 val loss: 0.561995
[Epoch 113] ogbg-molbbbp: 0.631848 test loss: 2.370315
[Epoch 114; Iter    25/   55] train: loss: 0.0039374
[Epoch 114; Iter    55/   55] train: loss: 0.0017752
[Epoch 114] ogbg-molbbbp: 0.942248 val loss: 0.609024
[Epoch 114] ogbg-molbbbp: 0.652874 test loss: 2.065623
[Epoch 115; Iter    30/   55] train: loss: 0.0040273
[Epoch 115] ogbg-molbbbp: 0.951708 val loss: 0.476154
[Epoch 115] ogbg-molbbbp: 0.654032 test loss: 1.920951
[Epoch 116; Iter     5/   55] train: loss: 0.0024275
[Epoch 116; Iter    35/   55] train: loss: 0.0055425
[Epoch 116] ogbg-molbbbp: 0.950214 val loss: 0.511369
[Epoch 116] ogbg-molbbbp: 0.655382 test loss: 2.117874
[Epoch 117; Iter    10/   55] train: loss: 0.0193521
[Epoch 117; Iter    40/   55] train: loss: 0.0041569
[Epoch 117] ogbg-molbbbp: 0.952206 val loss: 0.510935
[Epoch 117] ogbg-molbbbp: 0.645544 test loss: 2.222705
[Epoch 118; Iter    15/   55] train: loss: 0.0910357
[Epoch 118; Iter    45/   55] train: loss: 0.0017761
[Epoch 118] ogbg-molbbbp: 0.952006 val loss: 0.534324
[Epoch 118] ogbg-molbbbp: 0.652874 test loss: 2.120046
[Epoch 119; Iter    20/   55] train: loss: 0.0033090
[Epoch 119; Iter    50/   55] train: loss: 0.0913206
[Epoch 119] ogbg-molbbbp: 0.950812 val loss: 0.486369
[Epoch 119] ogbg-molbbbp: 0.652199 test loss: 2.102361
[Epoch 120; Iter    25/   55] train: loss: 0.0796499
[Epoch 120; Iter    55/   55] train: loss: 0.1867776
[Epoch 120] ogbg-molbbbp: 0.957184 val loss: 0.489726
[Epoch 120] ogbg-molbbbp: 0.644965 test loss: 2.168533
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 32.
Statistics on  val_best_checkpoint
mean_pred: -0.451416552066803
std_pred: 4.535043716430664
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9502048173898439
rocauc: 0.9688340137409142
ogbg-molbbbp: 0.9688340137409142
BCEWithLogitsLoss: 0.24565320994172776
Statistics on  test
mean_pred: 1.0038633346557617
std_pred: 6.191842555999756
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6868821851248439
rocauc: 0.6865354938271605
ogbg-molbbbp: 0.6865354938271605
BCEWithLogitsLoss: 1.0349004098347254
Statistics on  train
mean_pred: 2.8244266510009766
std_pred: 2.5762248039245605
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9888852956744517
rocauc: 0.9483352756511412
ogbg-molbbbp: 0.9483352756511412
BCEWithLogitsLoss: 0.2143646881661632
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.952903 val loss: 0.546740
[Epoch 109] ogbg-molbbbp: 0.660301 test loss: 2.220105
[Epoch 110; Iter     5/   55] train: loss: 0.0095246
[Epoch 110; Iter    35/   55] train: loss: 0.0047425
[Epoch 110] ogbg-molbbbp: 0.953898 val loss: 0.480332
[Epoch 110] ogbg-molbbbp: 0.661169 test loss: 2.033178
[Epoch 111; Iter    10/   55] train: loss: 0.0044021
[Epoch 111; Iter    40/   55] train: loss: 0.0012641
[Epoch 111] ogbg-molbbbp: 0.949617 val loss: 0.522873
[Epoch 111] ogbg-molbbbp: 0.651235 test loss: 2.120814
[Epoch 112; Iter    15/   55] train: loss: 0.0112908
[Epoch 112; Iter    45/   55] train: loss: 0.0044072
[Epoch 112] ogbg-molbbbp: 0.947028 val loss: 0.610285
[Epoch 112] ogbg-molbbbp: 0.654610 test loss: 2.309303
[Epoch 113; Iter    20/   55] train: loss: 0.0105983
[Epoch 113; Iter    50/   55] train: loss: 0.0222637
[Epoch 113] ogbg-molbbbp: 0.951110 val loss: 0.483257
[Epoch 113] ogbg-molbbbp: 0.652103 test loss: 2.019717
[Epoch 114; Iter    25/   55] train: loss: 0.0100835
[Epoch 114; Iter    55/   55] train: loss: 0.0052894
[Epoch 114] ogbg-molbbbp: 0.946829 val loss: 0.573021
[Epoch 114] ogbg-molbbbp: 0.645930 test loss: 2.230175
[Epoch 115; Iter    30/   55] train: loss: 0.0150169
[Epoch 115] ogbg-molbbbp: 0.949716 val loss: 0.546557
[Epoch 115] ogbg-molbbbp: 0.651717 test loss: 2.236024
[Epoch 116; Iter     5/   55] train: loss: 0.0137681
[Epoch 116; Iter    35/   55] train: loss: 0.0404490
[Epoch 116] ogbg-molbbbp: 0.942547 val loss: 0.633204
[Epoch 116] ogbg-molbbbp: 0.634259 test loss: 2.312287
[Epoch 117; Iter    10/   55] train: loss: 0.0626184
[Epoch 117; Iter    40/   55] train: loss: 0.0039186
[Epoch 117] ogbg-molbbbp: 0.957682 val loss: 0.511273
[Epoch 117] ogbg-molbbbp: 0.676312 test loss: 2.164862
[Epoch 118; Iter    15/   55] train: loss: 0.0038276
[Epoch 118; Iter    45/   55] train: loss: 0.0048329
[Epoch 118] ogbg-molbbbp: 0.955890 val loss: 0.473685
[Epoch 118] ogbg-molbbbp: 0.661458 test loss: 2.111975
[Epoch 119; Iter    20/   55] train: loss: 0.0915988
[Epoch 119; Iter    50/   55] train: loss: 0.0039791
[Epoch 119] ogbg-molbbbp: 0.950612 val loss: 0.609734
[Epoch 119] ogbg-molbbbp: 0.653356 test loss: 2.435317
[Epoch 120; Iter    25/   55] train: loss: 0.0595958
[Epoch 120; Iter    55/   55] train: loss: 0.0031136
[Epoch 120] ogbg-molbbbp: 0.950812 val loss: 0.550129
[Epoch 120] ogbg-molbbbp: 0.668692 test loss: 2.167000
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 38.
Statistics on  val_best_checkpoint
mean_pred: -1.9303609132766724
std_pred: 5.049192905426025
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9533436654019398
rocauc: 0.9719207408144978
ogbg-molbbbp: 0.9719207408144978
BCEWithLogitsLoss: 0.24385982086615904
Statistics on  test
mean_pred: 0.33478105068206787
std_pred: 3.8050708770751953
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.7529435537633864
rocauc: 0.7267554012345679
ogbg-molbbbp: 0.7267554012345679
BCEWithLogitsLoss: 0.9667250939777919
Statistics on  train
mean_pred: 2.0023996829986572
std_pred: 3.536742925643921
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.993463428917915
rocauc: 0.9708791729629361
ogbg-molbbbp: 0.9708791729629361
BCEWithLogitsLoss: 0.21431659216230567
[Epoch 109] ogbg-molbbbp: 0.954894 val loss: 0.542405
[Epoch 109] ogbg-molbbbp: 0.680459 test loss: 2.112907
[Epoch 110; Iter     5/   55] train: loss: 0.0064640
[Epoch 110; Iter    35/   55] train: loss: 0.0027934
[Epoch 110] ogbg-molbbbp: 0.954695 val loss: 0.477511
[Epoch 110] ogbg-molbbbp: 0.680170 test loss: 2.030493
[Epoch 111; Iter    10/   55] train: loss: 0.0023918
[Epoch 111; Iter    40/   55] train: loss: 0.0031919
[Epoch 111] ogbg-molbbbp: 0.954297 val loss: 0.491210
[Epoch 111] ogbg-molbbbp: 0.687789 test loss: 2.107033
[Epoch 112; Iter    15/   55] train: loss: 0.0025080
[Epoch 112; Iter    45/   55] train: loss: 0.0038112
[Epoch 112] ogbg-molbbbp: 0.952504 val loss: 0.533774
[Epoch 112] ogbg-molbbbp: 0.690779 test loss: 2.292601
[Epoch 113; Iter    20/   55] train: loss: 0.0144800
[Epoch 113; Iter    50/   55] train: loss: 0.0025283
[Epoch 113] ogbg-molbbbp: 0.950513 val loss: 0.538382
[Epoch 113] ogbg-molbbbp: 0.684896 test loss: 2.141181
[Epoch 114; Iter    25/   55] train: loss: 0.0078027
[Epoch 114; Iter    55/   55] train: loss: 0.2984863
[Epoch 114] ogbg-molbbbp: 0.959375 val loss: 0.446482
[Epoch 114] ogbg-molbbbp: 0.691454 test loss: 2.078296
[Epoch 115; Iter    30/   55] train: loss: 0.0043800
[Epoch 115] ogbg-molbbbp: 0.954097 val loss: 0.453771
[Epoch 115] ogbg-molbbbp: 0.691165 test loss: 2.003499
[Epoch 116; Iter     5/   55] train: loss: 0.0015926
[Epoch 116; Iter    35/   55] train: loss: 0.0053413
[Epoch 116] ogbg-molbbbp: 0.947028 val loss: 0.567755
[Epoch 116] ogbg-molbbbp: 0.675733 test loss: 2.248397
[Epoch 117; Iter    10/   55] train: loss: 0.0321400
[Epoch 117; Iter    40/   55] train: loss: 0.0074114
[Epoch 117] ogbg-molbbbp: 0.953699 val loss: 0.506036
[Epoch 117] ogbg-molbbbp: 0.682292 test loss: 2.182896
[Epoch 118; Iter    15/   55] train: loss: 0.0026697
[Epoch 118; Iter    45/   55] train: loss: 0.0026539
[Epoch 118] ogbg-molbbbp: 0.951608 val loss: 0.574040
[Epoch 118] ogbg-molbbbp: 0.681520 test loss: 2.367482
[Epoch 119; Iter    20/   55] train: loss: 0.0847156
[Epoch 119; Iter    50/   55] train: loss: 0.0310283
[Epoch 119] ogbg-molbbbp: 0.951708 val loss: 0.520634
[Epoch 119] ogbg-molbbbp: 0.690490 test loss: 2.212993
[Epoch 120; Iter    25/   55] train: loss: 0.0018823
[Epoch 120; Iter    55/   55] train: loss: 0.0012214
[Epoch 120] ogbg-molbbbp: 0.954794 val loss: 0.479865
[Epoch 120] ogbg-molbbbp: 0.680266 test loss: 2.219759
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 52.
Statistics on  val_best_checkpoint
mean_pred: -0.7731286287307739
std_pred: 5.416553974151611
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.948327140452455
rocauc: 0.9722194563377476
ogbg-molbbbp: 0.9722194563377476
BCEWithLogitsLoss: 0.22333588238273347
Statistics on  test
mean_pred: 1.1432939767837524
std_pred: 4.270082950592041
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.6757884428047443
rocauc: 0.6694637345679013
ogbg-molbbbp: 0.6694637345679013
BCEWithLogitsLoss: 1.188095475946154
Statistics on  train
mean_pred: 3.125520944595337
std_pred: 3.5434136390686035
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9979744764347671
rocauc: 0.9896271865015418
ogbg-molbbbp: 0.9896271865015418
BCEWithLogitsLoss: 0.12356924990361387
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.1236684
[Epoch 116] ogbg-molbbbp: 0.979120 val loss: 0.131758
[Epoch 116] ogbg-molbbbp: 0.791935 test loss: 1.355286
[Epoch 117; Iter    12/   48] train: loss: 0.0015684
[Epoch 117; Iter    42/   48] train: loss: 0.0349416
[Epoch 117] ogbg-molbbbp: 0.973255 val loss: 0.148037
[Epoch 117] ogbg-molbbbp: 0.783826 test loss: 1.480483
[Epoch 118; Iter    24/   48] train: loss: 0.0063833
[Epoch 118] ogbg-molbbbp: 0.973255 val loss: 0.150362
[Epoch 118] ogbg-molbbbp: 0.787186 test loss: 1.386259
[Epoch 119; Iter     6/   48] train: loss: 0.0285478
[Epoch 119; Iter    36/   48] train: loss: 0.0029379
[Epoch 119] ogbg-molbbbp: 0.973372 val loss: 0.153237
[Epoch 119] ogbg-molbbbp: 0.788486 test loss: 1.369444
[Epoch 120; Iter    18/   48] train: loss: 0.0077689
[Epoch 120; Iter    48/   48] train: loss: 0.0429988
[Epoch 120] ogbg-molbbbp: 0.972551 val loss: 0.127392
[Epoch 120] ogbg-molbbbp: 0.789158 test loss: 1.619008
[Epoch 121; Iter    30/   48] train: loss: 0.0012610
[Epoch 121] ogbg-molbbbp: 0.973842 val loss: 0.145971
[Epoch 121] ogbg-molbbbp: 0.792159 test loss: 1.531159
[Epoch 122; Iter    12/   48] train: loss: 0.0106045
[Epoch 122; Iter    42/   48] train: loss: 0.0034557
[Epoch 122] ogbg-molbbbp: 0.978768 val loss: 0.148433
[Epoch 122] ogbg-molbbbp: 0.787814 test loss: 1.347240
[Epoch 123; Iter    24/   48] train: loss: 0.0614017
[Epoch 123] ogbg-molbbbp: 0.974780 val loss: 0.171965
[Epoch 123] ogbg-molbbbp: 0.780108 test loss: 1.356242
[Epoch 124; Iter     6/   48] train: loss: 0.1231287
[Epoch 124; Iter    36/   48] train: loss: 0.0309091
[Epoch 124] ogbg-molbbbp: 0.970205 val loss: 0.144045
[Epoch 124] ogbg-molbbbp: 0.791308 test loss: 1.413912
[Epoch 125; Iter    18/   48] train: loss: 0.0036846
[Epoch 125; Iter    48/   48] train: loss: 0.0031277
[Epoch 125] ogbg-molbbbp: 0.975249 val loss: 0.173391
[Epoch 125] ogbg-molbbbp: 0.790412 test loss: 1.468126
[Epoch 126; Iter    30/   48] train: loss: 0.0080177
[Epoch 126] ogbg-molbbbp: 0.973372 val loss: 0.148172
[Epoch 126] ogbg-molbbbp: 0.794400 test loss: 1.433564
[Epoch 127; Iter    12/   48] train: loss: 0.0407296
[Epoch 127; Iter    42/   48] train: loss: 0.0349811
[Epoch 127] ogbg-molbbbp: 0.973255 val loss: 0.150133
[Epoch 127] ogbg-molbbbp: 0.799821 test loss: 1.395908
[Epoch 128; Iter    24/   48] train: loss: 0.0022921
[Epoch 128] ogbg-molbbbp: 0.974194 val loss: 0.165885
[Epoch 128] ogbg-molbbbp: 0.799866 test loss: 1.384600
[Epoch 129; Iter     6/   48] train: loss: 0.0030991
[Epoch 129; Iter    36/   48] train: loss: 0.0012688
[Epoch 129] ogbg-molbbbp: 0.973607 val loss: 0.172250
[Epoch 129] ogbg-molbbbp: 0.800403 test loss: 1.380514
[Epoch 130; Iter    18/   48] train: loss: 0.0014298
[Epoch 130; Iter    48/   48] train: loss: 0.0091649
[Epoch 130] ogbg-molbbbp: 0.974076 val loss: 0.157918
[Epoch 130] ogbg-molbbbp: 0.802912 test loss: 1.298162
[Epoch 131; Iter    30/   48] train: loss: 0.0051747
[Epoch 131] ogbg-molbbbp: 0.968915 val loss: 0.126699
[Epoch 131] ogbg-molbbbp: 0.793011 test loss: 1.469534
[Epoch 132; Iter    12/   48] train: loss: 0.0204630
[Epoch 132; Iter    42/   48] train: loss: 0.0028371
[Epoch 132] ogbg-molbbbp: 0.977243 val loss: 0.127838
[Epoch 132] ogbg-molbbbp: 0.791846 test loss: 1.533023
[Epoch 133; Iter    24/   48] train: loss: 0.0008159
[Epoch 133] ogbg-molbbbp: 0.975601 val loss: 0.177249
[Epoch 133] ogbg-molbbbp: 0.796819 test loss: 1.329338
[Epoch 134; Iter     6/   48] train: loss: 0.0391369
[Epoch 134; Iter    36/   48] train: loss: 0.0048715
[Epoch 134] ogbg-molbbbp: 0.971026 val loss: 0.146635
[Epoch 134] ogbg-molbbbp: 0.792966 test loss: 1.405900
[Epoch 135; Iter    18/   48] train: loss: 0.0490922
[Epoch 135; Iter    48/   48] train: loss: 0.0004623
[Epoch 135] ogbg-molbbbp: 0.972434 val loss: 0.144858
[Epoch 135] ogbg-molbbbp: 0.795161 test loss: 1.588619
[Epoch 136; Iter    30/   48] train: loss: 0.0005714
[Epoch 136] ogbg-molbbbp: 0.980411 val loss: 0.124718
[Epoch 136] ogbg-molbbbp: 0.793056 test loss: 1.512949
[Epoch 137; Iter    12/   48] train: loss: 0.0058295
[Epoch 137; Iter    42/   48] train: loss: 0.0044070
[Epoch 137] ogbg-molbbbp: 0.971378 val loss: 0.163257
[Epoch 137] ogbg-molbbbp: 0.794265 test loss: 1.386088
[Epoch 138; Iter    24/   48] train: loss: 0.0009161
[Epoch 138] ogbg-molbbbp: 0.968915 val loss: 0.142294
[Epoch 138] ogbg-molbbbp: 0.771819 test loss: 1.683585
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 138 epochs. Best model checkpoint was in epoch 78.
Statistics on  val_best_checkpoint
mean_pred: 3.76179575920105
std_pred: 4.255802154541016
mean_targets: 0.8986928462982178
std_targets: 0.30222928524017334
prcauc: 0.9992426454623478
rocauc: 0.9931964809384164
ogbg-molbbbp: 0.9931964809384164
BCEWithLogitsLoss: 0.1568599221381274
Statistics on  test
mean_pred: -0.11306378990411758
std_pred: 5.141162872314453
mean_targets: 0.3921568691730499
std_targets: 0.4890310764312744
prcauc: 0.6708806824392415
rocauc: 0.7811827956989248
ogbg-molbbbp: 0.7811827956989248
BCEWithLogitsLoss: 0.9423084756867453
Statistics on  train
mean_pred: 3.261446237564087
std_pred: 4.530749797821045
mean_targets: 0.816398024559021
std_targets: 0.3872950077056885
prcauc: 0.999697132016068
rocauc: 0.9986813222815581
ogbg-molbbbp: 0.9986813222815581
BCEWithLogitsLoss: 0.06465253672407319
All runs completed.
