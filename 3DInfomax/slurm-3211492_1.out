>>> Starting run for dataset: bbbp
Running RANDOM configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.7.yml --seed 6 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.8.yml --seed 6 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.8/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.8_4_26-05_09-18-11
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.8
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6935468
[Epoch 1] ogbg-molbbbp: 0.614815 val loss: 0.692474
[Epoch 1] ogbg-molbbbp: 0.468842 test loss: 0.692946
[Epoch 2; Iter     5/   55] train: loss: 0.6948677
[Epoch 2; Iter    35/   55] train: loss: 0.6943694
[Epoch 2] ogbg-molbbbp: 0.574321 val loss: 0.692222
[Epoch 2] ogbg-molbbbp: 0.458995 test loss: 0.693190
[Epoch 3; Iter    10/   55] train: loss: 0.6947260
[Epoch 3; Iter    40/   55] train: loss: 0.6951745
[Epoch 3] ogbg-molbbbp: 0.565185 val loss: 0.691894
[Epoch 3] ogbg-molbbbp: 0.438713 test loss: 0.692930
[Epoch 4; Iter    15/   55] train: loss: 0.6917427
[Epoch 4; Iter    45/   55] train: loss: 0.6895662
[Epoch 4] ogbg-molbbbp: 0.585926 val loss: 0.690971
[Epoch 4] ogbg-molbbbp: 0.457966 test loss: 0.691895
[Epoch 5; Iter    20/   55] train: loss: 0.6915697
[Epoch 5; Iter    50/   55] train: loss: 0.6902010
[Epoch 5] ogbg-molbbbp: 0.590741 val loss: 0.690216
[Epoch 5] ogbg-molbbbp: 0.464874 test loss: 0.690857
[Epoch 6; Iter    25/   55] train: loss: 0.6909554
[Epoch 6; Iter    55/   55] train: loss: 0.6901693
[Epoch 6] ogbg-molbbbp: 0.614074 val loss: 0.688779
[Epoch 6] ogbg-molbbbp: 0.475603 test loss: 0.689348
[Epoch 7; Iter    30/   55] train: loss: 0.6917283
[Epoch 7] ogbg-molbbbp: 0.609012 val loss: 0.687395
[Epoch 7] ogbg-molbbbp: 0.470899 test loss: 0.687706
[Epoch 8; Iter     5/   55] train: loss: 0.6886346
[Epoch 8; Iter    35/   55] train: loss: 0.6893002
[Epoch 8] ogbg-molbbbp: 0.635556 val loss: 0.685909
[Epoch 8] ogbg-molbbbp: 0.491770 test loss: 0.686048
[Epoch 9; Iter    10/   55] train: loss: 0.6837604
[Epoch 9; Iter    40/   55] train: loss: 0.6850562
[Epoch 9] ogbg-molbbbp: 0.647531 val loss: 0.683911
[Epoch 9] ogbg-molbbbp: 0.508524 test loss: 0.683690
[Epoch 10; Iter    15/   55] train: loss: 0.6841615
[Epoch 10; Iter    45/   55] train: loss: 0.6873772
[Epoch 10] ogbg-molbbbp: 0.656790 val loss: 0.682731
[Epoch 10] ogbg-molbbbp: 0.517784 test loss: 0.682154
[Epoch 11; Iter    20/   55] train: loss: 0.6842259
[Epoch 11; Iter    50/   55] train: loss: 0.6836987
[Epoch 11] ogbg-molbbbp: 0.665679 val loss: 0.680608
[Epoch 11] ogbg-molbbbp: 0.523075 test loss: 0.679799
[Epoch 12; Iter    25/   55] train: loss: 0.6786382
[Epoch 12; Iter    55/   55] train: loss: 0.6816971
[Epoch 12] ogbg-molbbbp: 0.672222 val loss: 0.678265
[Epoch 12] ogbg-molbbbp: 0.547031 test loss: 0.676822
[Epoch 13; Iter    30/   55] train: loss: 0.6743447
[Epoch 13] ogbg-molbbbp: 0.791728 val loss: 0.645690
[Epoch 13] ogbg-molbbbp: 0.754703 test loss: 0.648854
[Epoch 14; Iter     5/   55] train: loss: 0.6524170
[Epoch 14; Iter    35/   55] train: loss: 0.6011886
[Epoch 14] ogbg-molbbbp: 0.871111 val loss: 0.553596
[Epoch 14] ogbg-molbbbp: 0.821429 test loss: 0.567377
[Epoch 15; Iter    10/   55] train: loss: 0.5472618
[Epoch 15; Iter    40/   55] train: loss: 0.5396421
[Epoch 15] ogbg-molbbbp: 0.853086 val loss: 0.474720
[Epoch 15] ogbg-molbbbp: 0.826720 test loss: 0.483838
[Epoch 16; Iter    15/   55] train: loss: 0.4818821
[Epoch 16; Iter    45/   55] train: loss: 0.3968001
[Epoch 16] ogbg-molbbbp: 0.895556 val loss: 0.421863
[Epoch 16] ogbg-molbbbp: 0.865373 test loss: 0.414287
[Epoch 17; Iter    20/   55] train: loss: 0.5151952
[Epoch 17; Iter    50/   55] train: loss: 0.4956604
[Epoch 17] ogbg-molbbbp: 0.898395 val loss: 0.391082
[Epoch 17] ogbg-molbbbp: 0.852587 test loss: 0.407495
[Epoch 18; Iter    25/   55] train: loss: 0.4766007
[Epoch 18; Iter    55/   55] train: loss: 0.6115574
[Epoch 18] ogbg-molbbbp: 0.874938 val loss: 0.399290
[Epoch 18] ogbg-molbbbp: 0.864638 test loss: 0.357858
[Epoch 19; Iter    30/   55] train: loss: 0.2262670
[Epoch 19] ogbg-molbbbp: 0.883210 val loss: 0.388022
[Epoch 19] ogbg-molbbbp: 0.882128 test loss: 0.340099
[Epoch 20; Iter     5/   55] train: loss: 0.3064669
[Epoch 20; Iter    35/   55] train: loss: 0.3857270
[Epoch 20] ogbg-molbbbp: 0.892840 val loss: 0.377032
[Epoch 20] ogbg-molbbbp: 0.864345 test loss: 0.347685
[Epoch 21; Iter    10/   55] train: loss: 0.3951375
[Epoch 21; Iter    40/   55] train: loss: 0.4193763
[Epoch 21] ogbg-molbbbp: 0.852593 val loss: 0.432226
[Epoch 21] ogbg-molbbbp: 0.844356 test loss: 0.358660
[Epoch 22; Iter    15/   55] train: loss: 0.5004639
[Epoch 22; Iter    45/   55] train: loss: 0.4579189
[Epoch 22] ogbg-molbbbp: 0.900617 val loss: 0.366230
[Epoch 22] ogbg-molbbbp: 0.885802 test loss: 0.319438
[Epoch 23; Iter    20/   55] train: loss: 0.5095612
[Epoch 23; Iter    50/   55] train: loss: 0.2830675
[Epoch 23] ogbg-molbbbp: 0.909383 val loss: 0.356179
[Epoch 23] ogbg-molbbbp: 0.896238 test loss: 0.313720
[Epoch 24; Iter    25/   55] train: loss: 0.3372062
[Epoch 24; Iter    55/   55] train: loss: 0.3836617
[Epoch 24] ogbg-molbbbp: 0.911481 val loss: 0.350769
[Epoch 24] ogbg-molbbbp: 0.894180 test loss: 0.317183
[Epoch 25; Iter    30/   55] train: loss: 0.4775238
[Epoch 25] ogbg-molbbbp: 0.912716 val loss: 0.362481
[Epoch 25] ogbg-molbbbp: 0.887125 test loss: 0.320045
[Epoch 26; Iter     5/   55] train: loss: 0.2887154
[Epoch 26; Iter    35/   55] train: loss: 0.3365281
[Epoch 26] ogbg-molbbbp: 0.938148 val loss: 0.326097
[Epoch 26] ogbg-molbbbp: 0.925926 test loss: 0.278186
[Epoch 27; Iter    10/   55] train: loss: 0.3340474
[Epoch 27; Iter    40/   55] train: loss: 0.4544271
[Epoch 27] ogbg-molbbbp: 0.920494 val loss: 0.338655
[Epoch 27] ogbg-molbbbp: 0.827601 test loss: 0.420562
[Epoch 28; Iter    15/   55] train: loss: 0.3052693
[Epoch 28; Iter    45/   55] train: loss: 0.2481931
[Epoch 28] ogbg-molbbbp: 0.939259 val loss: 0.291049
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.8/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.8_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.8
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6933892
[Epoch 1] ogbg-molbbbp: 0.612963 val loss: 0.692284
[Epoch 1] ogbg-molbbbp: 0.568342 test loss: 0.692142
[Epoch 2; Iter     5/   55] train: loss: 0.6904556
[Epoch 2; Iter    35/   55] train: loss: 0.6910750
[Epoch 2] ogbg-molbbbp: 0.516049 val loss: 0.693317
[Epoch 2] ogbg-molbbbp: 0.475750 test loss: 0.692703
[Epoch 3; Iter    10/   55] train: loss: 0.6911787
[Epoch 3; Iter    40/   55] train: loss: 0.6920096
[Epoch 3] ogbg-molbbbp: 0.536049 val loss: 0.692263
[Epoch 3] ogbg-molbbbp: 0.482363 test loss: 0.691731
[Epoch 4; Iter    15/   55] train: loss: 0.6919031
[Epoch 4; Iter    45/   55] train: loss: 0.6903815
[Epoch 4] ogbg-molbbbp: 0.539136 val loss: 0.692079
[Epoch 4] ogbg-molbbbp: 0.483392 test loss: 0.691413
[Epoch 5; Iter    20/   55] train: loss: 0.6888528
[Epoch 5; Iter    50/   55] train: loss: 0.6902616
[Epoch 5] ogbg-molbbbp: 0.535556 val loss: 0.691037
[Epoch 5] ogbg-molbbbp: 0.491917 test loss: 0.690128
[Epoch 6; Iter    25/   55] train: loss: 0.6886723
[Epoch 6; Iter    55/   55] train: loss: 0.6910537
[Epoch 6] ogbg-molbbbp: 0.541728 val loss: 0.689978
[Epoch 6] ogbg-molbbbp: 0.488683 test loss: 0.689075
[Epoch 7; Iter    30/   55] train: loss: 0.6869641
[Epoch 7] ogbg-molbbbp: 0.567531 val loss: 0.688602
[Epoch 7] ogbg-molbbbp: 0.504556 test loss: 0.687370
[Epoch 8; Iter     5/   55] train: loss: 0.6878617
[Epoch 8; Iter    35/   55] train: loss: 0.6852578
[Epoch 8] ogbg-molbbbp: 0.564074 val loss: 0.687889
[Epoch 8] ogbg-molbbbp: 0.502939 test loss: 0.686553
[Epoch 9; Iter    10/   55] train: loss: 0.6863909
[Epoch 9; Iter    40/   55] train: loss: 0.6848989
[Epoch 9] ogbg-molbbbp: 0.582469 val loss: 0.685799
[Epoch 9] ogbg-molbbbp: 0.514256 test loss: 0.684170
[Epoch 10; Iter    15/   55] train: loss: 0.6800910
[Epoch 10; Iter    45/   55] train: loss: 0.6842209
[Epoch 10] ogbg-molbbbp: 0.589383 val loss: 0.684097
[Epoch 10] ogbg-molbbbp: 0.528072 test loss: 0.682187
[Epoch 11; Iter    20/   55] train: loss: 0.6835077
[Epoch 11; Iter    50/   55] train: loss: 0.6824496
[Epoch 11] ogbg-molbbbp: 0.601481 val loss: 0.682189
[Epoch 11] ogbg-molbbbp: 0.535714 test loss: 0.679940
[Epoch 12; Iter    25/   55] train: loss: 0.6844140
[Epoch 12; Iter    55/   55] train: loss: 0.6855080
[Epoch 12] ogbg-molbbbp: 0.607901 val loss: 0.680037
[Epoch 12] ogbg-molbbbp: 0.539683 test loss: 0.677312
[Epoch 13; Iter    30/   55] train: loss: 0.6736628
[Epoch 13] ogbg-molbbbp: 0.780494 val loss: 0.647470
[Epoch 13] ogbg-molbbbp: 0.735450 test loss: 0.650465
[Epoch 14; Iter     5/   55] train: loss: 0.6456773
[Epoch 14; Iter    35/   55] train: loss: 0.5997681
[Epoch 14] ogbg-molbbbp: 0.850247 val loss: 0.541832
[Epoch 14] ogbg-molbbbp: 0.823045 test loss: 0.540913
[Epoch 15; Iter    10/   55] train: loss: 0.5347150
[Epoch 15; Iter    40/   55] train: loss: 0.5281358
[Epoch 15] ogbg-molbbbp: 0.890000 val loss: 0.469797
[Epoch 15] ogbg-molbbbp: 0.857143 test loss: 0.469214
[Epoch 16; Iter    15/   55] train: loss: 0.5788506
[Epoch 16; Iter    45/   55] train: loss: 0.3854609
[Epoch 16] ogbg-molbbbp: 0.826914 val loss: 0.452577
[Epoch 16] ogbg-molbbbp: 0.830835 test loss: 0.448590
[Epoch 17; Iter    20/   55] train: loss: 0.3834111
[Epoch 17; Iter    50/   55] train: loss: 0.2899125
[Epoch 17] ogbg-molbbbp: 0.889012 val loss: 0.402739
[Epoch 17] ogbg-molbbbp: 0.852440 test loss: 0.412116
[Epoch 18; Iter    25/   55] train: loss: 0.3092476
[Epoch 18; Iter    55/   55] train: loss: 0.7579475
[Epoch 18] ogbg-molbbbp: 0.816296 val loss: 0.444512
[Epoch 18] ogbg-molbbbp: 0.837008 test loss: 0.364890
[Epoch 19; Iter    30/   55] train: loss: 0.3926141
[Epoch 19] ogbg-molbbbp: 0.795679 val loss: 0.475361
[Epoch 19] ogbg-molbbbp: 0.814227 test loss: 0.398682
[Epoch 20; Iter     5/   55] train: loss: 0.4484012
[Epoch 20; Iter    35/   55] train: loss: 0.1874418
[Epoch 20] ogbg-molbbbp: 0.901111 val loss: 0.378874
[Epoch 20] ogbg-molbbbp: 0.867284 test loss: 0.338658
[Epoch 21; Iter    10/   55] train: loss: 0.5292850
[Epoch 21; Iter    40/   55] train: loss: 0.4522837
[Epoch 21] ogbg-molbbbp: 0.893827 val loss: 0.390541
[Epoch 21] ogbg-molbbbp: 0.871252 test loss: 0.339488
[Epoch 22; Iter    15/   55] train: loss: 0.3605535
[Epoch 22; Iter    45/   55] train: loss: 0.3373968
[Epoch 22] ogbg-molbbbp: 0.860000 val loss: 0.418618
[Epoch 22] ogbg-molbbbp: 0.846561 test loss: 0.347325
[Epoch 23; Iter    20/   55] train: loss: 0.2995669
[Epoch 23; Iter    50/   55] train: loss: 0.3280717
[Epoch 23] ogbg-molbbbp: 0.902346 val loss: 0.375875
[Epoch 23] ogbg-molbbbp: 0.884186 test loss: 0.323190
[Epoch 24; Iter    25/   55] train: loss: 0.3087674
[Epoch 24; Iter    55/   55] train: loss: 0.4496570
[Epoch 24] ogbg-molbbbp: 0.899383 val loss: 0.378074
[Epoch 24] ogbg-molbbbp: 0.877572 test loss: 0.334373
[Epoch 25; Iter    30/   55] train: loss: 0.4263289
[Epoch 25] ogbg-molbbbp: 0.877778 val loss: 0.408232
[Epoch 25] ogbg-molbbbp: 0.872134 test loss: 0.327410
[Epoch 26; Iter     5/   55] train: loss: 0.2848072
[Epoch 26; Iter    35/   55] train: loss: 0.1861938
[Epoch 26] ogbg-molbbbp: 0.898272 val loss: 0.370737
[Epoch 26] ogbg-molbbbp: 0.892857 test loss: 0.363723
[Epoch 27; Iter    10/   55] train: loss: 0.2459400
[Epoch 27; Iter    40/   55] train: loss: 0.3378987
[Epoch 27] ogbg-molbbbp: 0.886543 val loss: 0.356977
[Epoch 27] ogbg-molbbbp: 0.868313 test loss: 0.361015
[Epoch 28; Iter    15/   55] train: loss: 0.2663082
[Epoch 28; Iter    45/   55] train: loss: 0.5213276
[Epoch 28] ogbg-molbbbp: 0.893210 val loss: 0.539015
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.8/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.8_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.8
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6925297
[Epoch 1] ogbg-molbbbp: 0.665926 val loss: 0.691669
[Epoch 1] ogbg-molbbbp: 0.600235 test loss: 0.692279
[Epoch 2; Iter     5/   55] train: loss: 0.6928993
[Epoch 2; Iter    35/   55] train: loss: 0.6884786
[Epoch 2] ogbg-molbbbp: 0.665185 val loss: 0.689624
[Epoch 2] ogbg-molbbbp: 0.565109 test loss: 0.691176
[Epoch 3; Iter    10/   55] train: loss: 0.6928765
[Epoch 3; Iter    40/   55] train: loss: 0.6919283
[Epoch 3] ogbg-molbbbp: 0.667160 val loss: 0.690277
[Epoch 3] ogbg-molbbbp: 0.562022 test loss: 0.691968
[Epoch 4; Iter    15/   55] train: loss: 0.6887037
[Epoch 4; Iter    45/   55] train: loss: 0.6913815
[Epoch 4] ogbg-molbbbp: 0.662222 val loss: 0.689099
[Epoch 4] ogbg-molbbbp: 0.569518 test loss: 0.690356
[Epoch 5; Iter    20/   55] train: loss: 0.6880069
[Epoch 5; Iter    50/   55] train: loss: 0.6907690
[Epoch 5] ogbg-molbbbp: 0.669753 val loss: 0.688778
[Epoch 5] ogbg-molbbbp: 0.573927 test loss: 0.690081
[Epoch 6; Iter    25/   55] train: loss: 0.6893025
[Epoch 6; Iter    55/   55] train: loss: 0.6939592
[Epoch 6] ogbg-molbbbp: 0.680370 val loss: 0.687000
[Epoch 6] ogbg-molbbbp: 0.569665 test loss: 0.688521
[Epoch 7; Iter    30/   55] train: loss: 0.6876671
[Epoch 7] ogbg-molbbbp: 0.695556 val loss: 0.685511
[Epoch 7] ogbg-molbbbp: 0.590829 test loss: 0.686647
[Epoch 8; Iter     5/   55] train: loss: 0.6893849
[Epoch 8; Iter    35/   55] train: loss: 0.6844033
[Epoch 8] ogbg-molbbbp: 0.693086 val loss: 0.684237
[Epoch 8] ogbg-molbbbp: 0.588918 test loss: 0.685059
[Epoch 9; Iter    10/   55] train: loss: 0.6824195
[Epoch 9; Iter    40/   55] train: loss: 0.6836209
[Epoch 9] ogbg-molbbbp: 0.701481 val loss: 0.682533
[Epoch 9] ogbg-molbbbp: 0.593180 test loss: 0.682964
[Epoch 10; Iter    15/   55] train: loss: 0.6786410
[Epoch 10; Iter    45/   55] train: loss: 0.6809848
[Epoch 10] ogbg-molbbbp: 0.706173 val loss: 0.681006
[Epoch 10] ogbg-molbbbp: 0.595385 test loss: 0.681103
[Epoch 11; Iter    20/   55] train: loss: 0.6819729
[Epoch 11; Iter    50/   55] train: loss: 0.6829999
[Epoch 11] ogbg-molbbbp: 0.701852 val loss: 0.678818
[Epoch 11] ogbg-molbbbp: 0.609935 test loss: 0.678471
[Epoch 12; Iter    25/   55] train: loss: 0.6830076
[Epoch 12; Iter    55/   55] train: loss: 0.6761020
[Epoch 12] ogbg-molbbbp: 0.709259 val loss: 0.676593
[Epoch 12] ogbg-molbbbp: 0.614345 test loss: 0.675543
[Epoch 13; Iter    30/   55] train: loss: 0.6755521
[Epoch 13] ogbg-molbbbp: 0.791975 val loss: 0.640189
[Epoch 13] ogbg-molbbbp: 0.757055 test loss: 0.641780
[Epoch 14; Iter     5/   55] train: loss: 0.6578957
[Epoch 14; Iter    35/   55] train: loss: 0.6115200
[Epoch 14] ogbg-molbbbp: 0.870617 val loss: 0.524253
[Epoch 14] ogbg-molbbbp: 0.820400 test loss: 0.528514
[Epoch 15; Iter    10/   55] train: loss: 0.5397355
[Epoch 15; Iter    40/   55] train: loss: 0.5307007
[Epoch 15] ogbg-molbbbp: 0.821111 val loss: 0.481327
[Epoch 15] ogbg-molbbbp: 0.838183 test loss: 0.475726
[Epoch 16; Iter    15/   55] train: loss: 0.3812348
[Epoch 16; Iter    45/   55] train: loss: 0.5178875
[Epoch 16] ogbg-molbbbp: 0.860370 val loss: 0.409488
[Epoch 16] ogbg-molbbbp: 0.834362 test loss: 0.401790
[Epoch 17; Iter    20/   55] train: loss: 0.4509811
[Epoch 17; Iter    50/   55] train: loss: 0.3853758
[Epoch 17] ogbg-molbbbp: 0.884198 val loss: 0.397793
[Epoch 17] ogbg-molbbbp: 0.856114 test loss: 0.385245
[Epoch 18; Iter    25/   55] train: loss: 0.3900036
[Epoch 18; Iter    55/   55] train: loss: 0.5240566
[Epoch 18] ogbg-molbbbp: 0.861605 val loss: 0.423765
[Epoch 18] ogbg-molbbbp: 0.835979 test loss: 0.421001
[Epoch 19; Iter    30/   55] train: loss: 0.3122945
[Epoch 19] ogbg-molbbbp: 0.855556 val loss: 0.410240
[Epoch 19] ogbg-molbbbp: 0.839212 test loss: 0.369669
[Epoch 20; Iter     5/   55] train: loss: 0.3620573
[Epoch 20; Iter    35/   55] train: loss: 0.3618836
[Epoch 20] ogbg-molbbbp: 0.886296 val loss: 0.381562
[Epoch 20] ogbg-molbbbp: 0.881099 test loss: 0.341703
[Epoch 21; Iter    10/   55] train: loss: 0.3480042
[Epoch 21; Iter    40/   55] train: loss: 0.3826655
[Epoch 21] ogbg-molbbbp: 0.888889 val loss: 0.417685
[Epoch 21] ogbg-molbbbp: 0.886537 test loss: 0.324603
[Epoch 22; Iter    15/   55] train: loss: 0.2626441
[Epoch 22; Iter    45/   55] train: loss: 0.2951004
[Epoch 22] ogbg-molbbbp: 0.889506 val loss: 0.407665
[Epoch 22] ogbg-molbbbp: 0.889477 test loss: 0.324686
[Epoch 23; Iter    20/   55] train: loss: 0.3448879
[Epoch 23; Iter    50/   55] train: loss: 0.3260955
[Epoch 23] ogbg-molbbbp: 0.859753 val loss: 0.427455
[Epoch 23] ogbg-molbbbp: 0.864932 test loss: 0.338173
[Epoch 24; Iter    25/   55] train: loss: 0.3347594
[Epoch 24; Iter    55/   55] train: loss: 0.3991466
[Epoch 24] ogbg-molbbbp: 0.908148 val loss: 0.358989
[Epoch 24] ogbg-molbbbp: 0.875514 test loss: 0.337801
[Epoch 25; Iter    30/   55] train: loss: 0.3002588
[Epoch 25] ogbg-molbbbp: 0.826914 val loss: 0.430145
[Epoch 25] ogbg-molbbbp: 0.834509 test loss: 0.397475
[Epoch 26; Iter     5/   55] train: loss: 0.2880629
[Epoch 26; Iter    35/   55] train: loss: 0.3999471
[Epoch 26] ogbg-molbbbp: 0.896173 val loss: 0.346242
[Epoch 26] ogbg-molbbbp: 0.899030 test loss: 0.313052
[Epoch 27; Iter    10/   55] train: loss: 0.2692611
[Epoch 27; Iter    40/   55] train: loss: 0.6203849
[Epoch 27] ogbg-molbbbp: 0.834938 val loss: 0.457212
[Epoch 27] ogbg-molbbbp: 0.822163 test loss: 0.473080
[Epoch 28; Iter    15/   55] train: loss: 0.2237754
[Epoch 28; Iter    45/   55] train: loss: 0.3524950
[Epoch 28] ogbg-molbbbp: 0.925679 val loss: 0.312854
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.7/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.7_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.7
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6914344
[Epoch 1] ogbg-molbbbp: 0.680372 val loss: 0.691852
[Epoch 1] ogbg-molbbbp: 0.608746 test loss: 0.692138
[Epoch 2; Iter    12/   48] train: loss: 0.6930096
[Epoch 2; Iter    42/   48] train: loss: 0.6897632
[Epoch 2] ogbg-molbbbp: 0.670277 val loss: 0.689983
[Epoch 2] ogbg-molbbbp: 0.583211 test loss: 0.691100
[Epoch 3; Iter    24/   48] train: loss: 0.6963868
[Epoch 3] ogbg-molbbbp: 0.674871 val loss: 0.689841
[Epoch 3] ogbg-molbbbp: 0.586211 test loss: 0.691242
[Epoch 4; Iter     6/   48] train: loss: 0.6939727
[Epoch 4; Iter    36/   48] train: loss: 0.6941550
[Epoch 4] ogbg-molbbbp: 0.675722 val loss: 0.688885
[Epoch 4] ogbg-molbbbp: 0.589148 test loss: 0.690035
[Epoch 5; Iter    18/   48] train: loss: 0.6969739
[Epoch 5; Iter    48/   48] train: loss: 0.6899245
[Epoch 5] ogbg-molbbbp: 0.675552 val loss: 0.688509
[Epoch 5] ogbg-molbbbp: 0.595723 test loss: 0.689490
[Epoch 6; Iter    30/   48] train: loss: 0.6901650
[Epoch 6] ogbg-molbbbp: 0.676232 val loss: 0.687253
[Epoch 6] ogbg-molbbbp: 0.603958 test loss: 0.687892
[Epoch 7; Iter    12/   48] train: loss: 0.6882648
[Epoch 7; Iter    42/   48] train: loss: 0.6894666
[Epoch 7] ogbg-molbbbp: 0.684739 val loss: 0.686258
[Epoch 7] ogbg-molbbbp: 0.603256 test loss: 0.686894
[Epoch 8; Iter    24/   48] train: loss: 0.6896173
[Epoch 8] ogbg-molbbbp: 0.686327 val loss: 0.685287
[Epoch 8] ogbg-molbbbp: 0.610469 test loss: 0.685795
[Epoch 9; Iter     6/   48] train: loss: 0.6814363
[Epoch 9; Iter    36/   48] train: loss: 0.6832355
[Epoch 9] ogbg-molbbbp: 0.690410 val loss: 0.684140
[Epoch 9] ogbg-molbbbp: 0.612193 test loss: 0.684436
[Epoch 10; Iter    18/   48] train: loss: 0.6833645
[Epoch 10; Iter    48/   48] train: loss: 0.6844031
[Epoch 10] ogbg-molbbbp: 0.691544 val loss: 0.682811
[Epoch 10] ogbg-molbbbp: 0.617172 test loss: 0.682815
[Epoch 11; Iter    30/   48] train: loss: 0.6810442
[Epoch 11] ogbg-molbbbp: 0.691374 val loss: 0.681449
[Epoch 11] ogbg-molbbbp: 0.621002 test loss: 0.681105
[Epoch 12; Iter    12/   48] train: loss: 0.6802530
[Epoch 12; Iter    42/   48] train: loss: 0.6805454
[Epoch 12] ogbg-molbbbp: 0.692452 val loss: 0.679370
[Epoch 12] ogbg-molbbbp: 0.632557 test loss: 0.678534
[Epoch 13; Iter    24/   48] train: loss: 0.6785759
[Epoch 13] ogbg-molbbbp: 0.698974 val loss: 0.677155
[Epoch 13] ogbg-molbbbp: 0.632301 test loss: 0.676168
[Epoch 14; Iter     6/   48] train: loss: 0.6799054
[Epoch 14; Iter    36/   48] train: loss: 0.6836629
[Epoch 14] ogbg-molbbbp: 0.699484 val loss: 0.675374
[Epoch 14] ogbg-molbbbp: 0.641175 test loss: 0.673975
[Epoch 15; Iter    18/   48] train: loss: 0.6806174
[Epoch 15; Iter    48/   48] train: loss: 0.6564049
[Epoch 15] ogbg-molbbbp: 0.768162 val loss: 0.635513
[Epoch 15] ogbg-molbbbp: 0.813789 test loss: 0.622072
[Epoch 16; Iter    30/   48] train: loss: 0.6187947
[Epoch 16] ogbg-molbbbp: 0.810129 val loss: 0.570723
[Epoch 16] ogbg-molbbbp: 0.841302 test loss: 0.547564
[Epoch 17; Iter    12/   48] train: loss: 0.5121853
[Epoch 17; Iter    42/   48] train: loss: 0.5662611
[Epoch 17] ogbg-molbbbp: 0.825271 val loss: 0.464158
[Epoch 17] ogbg-molbbbp: 0.838876 test loss: 0.447115
[Epoch 18; Iter    24/   48] train: loss: 0.5228151
[Epoch 18] ogbg-molbbbp: 0.841434 val loss: 0.442247
[Epoch 18] ogbg-molbbbp: 0.873987 test loss: 0.412041
[Epoch 19; Iter     6/   48] train: loss: 0.4007499
[Epoch 19; Iter    36/   48] train: loss: 0.4131792
[Epoch 19] ogbg-molbbbp: 0.823626 val loss: 0.446827
[Epoch 19] ogbg-molbbbp: 0.800000 test loss: 0.443436
[Epoch 20; Iter    18/   48] train: loss: 0.4331580
[Epoch 20; Iter    48/   48] train: loss: 0.3068381
[Epoch 20] ogbg-molbbbp: 0.865763 val loss: 0.388718
[Epoch 20] ogbg-molbbbp: 0.868688 test loss: 0.379826
[Epoch 21; Iter    30/   48] train: loss: 0.4171258
[Epoch 21] ogbg-molbbbp: 0.841831 val loss: 0.450152
[Epoch 21] ogbg-molbbbp: 0.872071 test loss: 0.413430
[Epoch 22; Iter    12/   48] train: loss: 0.5045822
[Epoch 22; Iter    42/   48] train: loss: 0.3090282
[Epoch 22] ogbg-molbbbp: 0.869733 val loss: 0.361519
[Epoch 22] ogbg-molbbbp: 0.874433 test loss: 0.332802
[Epoch 23; Iter    24/   48] train: loss: 0.3679545
[Epoch 23] ogbg-molbbbp: 0.886009 val loss: 0.349616
[Epoch 23] ogbg-molbbbp: 0.864794 test loss: 0.345365
[Epoch 24; Iter     6/   48] train: loss: 0.2097237
[Epoch 24; Iter    36/   48] train: loss: 0.2296801
[Epoch 24] ogbg-molbbbp: 0.858561 val loss: 0.377579
[Epoch 24] ogbg-molbbbp: 0.864794 test loss: 0.351696
[Epoch 25; Iter    18/   48] train: loss: 0.3652531
[Epoch 25; Iter    48/   48] train: loss: 0.3264322
[Epoch 25] ogbg-molbbbp: 0.890830 val loss: 0.351786
[Epoch 25] ogbg-molbbbp: 0.883051 test loss: 0.329079
[Epoch 26; Iter    30/   48] train: loss: 0.2655303
[Epoch 26] ogbg-molbbbp: 0.872909 val loss: 0.363795
[Epoch 26] ogbg-molbbbp: 0.867092 test loss: 0.364713
[Epoch 27; Iter    12/   48] train: loss: 0.6052994
[Epoch 27; Iter    42/   48] train: loss: 0.4572811
[Epoch 27] ogbg-molbbbp: 0.868655 val loss: 0.365163
[Epoch 27] ogbg-molbbbp: 0.887201 test loss: 0.328214
[Epoch 28; Iter    24/   48] train: loss: 0.4007247
[Epoch 28] ogbg-molbbbp: 0.890943 val loss: 0.375629
[Epoch 28] ogbg-molbbbp: 0.884711 test loss: 0.340633
[Epoch 29; Iter     6/   48] train: loss: 0.2264567
[Epoch 29; Iter    36/   48] train: loss: 0.5099841
[Epoch 29] ogbg-molbbbp: 0.909601 val loss: 0.303002
[Epoch 29] ogbg-molbbbp: 0.902394 test loss: 0.304650
[Epoch 30; Iter    18/   48] train: loss: 0.2052639
[Epoch 30; Iter    48/   48] train: loss: 0.3170070
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.6/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.6_6_26-05_09-18-11
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.6
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6926152
[Epoch 1] ogbg-molbbbp: 0.590988 val loss: 0.692899
[Epoch 1] ogbg-molbbbp: 0.616052 test loss: 0.692702
[Epoch 2; Iter    19/   41] train: loss: 0.6944498
[Epoch 2] ogbg-molbbbp: 0.515037 val loss: 0.692770
[Epoch 2] ogbg-molbbbp: 0.544204 test loss: 0.692176
[Epoch 3; Iter     8/   41] train: loss: 0.6926083
[Epoch 3; Iter    38/   41] train: loss: 0.6935326
[Epoch 3] ogbg-molbbbp: 0.515754 val loss: 0.693079
[Epoch 3] ogbg-molbbbp: 0.519431 test loss: 0.692661
[Epoch 4; Iter    27/   41] train: loss: 0.6935880
[Epoch 4] ogbg-molbbbp: 0.498208 val loss: 0.692421
[Epoch 4] ogbg-molbbbp: 0.505008 test loss: 0.692065
[Epoch 5; Iter    16/   41] train: loss: 0.6936468
[Epoch 5] ogbg-molbbbp: 0.511043 val loss: 0.691961
[Epoch 5] ogbg-molbbbp: 0.516126 test loss: 0.691640
[Epoch 6; Iter     5/   41] train: loss: 0.6898628
[Epoch 6; Iter    35/   41] train: loss: 0.6879439
[Epoch 6] ogbg-molbbbp: 0.522820 val loss: 0.691714
[Epoch 6] ogbg-molbbbp: 0.522569 test loss: 0.691433
[Epoch 7; Iter    24/   41] train: loss: 0.6888652
[Epoch 7] ogbg-molbbbp: 0.521693 val loss: 0.690767
[Epoch 7] ogbg-molbbbp: 0.523471 test loss: 0.690448
[Epoch 8; Iter    13/   41] train: loss: 0.6875777
[Epoch 8] ogbg-molbbbp: 0.524731 val loss: 0.689968
[Epoch 8] ogbg-molbbbp: 0.521301 test loss: 0.689690
[Epoch 9; Iter     2/   41] train: loss: 0.6885424
[Epoch 9; Iter    32/   41] train: loss: 0.6879708
[Epoch 9] ogbg-molbbbp: 0.533026 val loss: 0.688765
[Epoch 9] ogbg-molbbbp: 0.529447 test loss: 0.688479
[Epoch 10; Iter    21/   41] train: loss: 0.6884148
[Epoch 10] ogbg-molbbbp: 0.539136 val loss: 0.687601
[Epoch 10] ogbg-molbbbp: 0.533854 test loss: 0.687345
[Epoch 11; Iter    10/   41] train: loss: 0.6897544
[Epoch 11; Iter    40/   41] train: loss: 0.6888542
[Epoch 11] ogbg-molbbbp: 0.558286 val loss: 0.686978
[Epoch 11] ogbg-molbbbp: 0.548511 test loss: 0.686722
[Epoch 12; Iter    29/   41] train: loss: 0.6858343
[Epoch 12] ogbg-molbbbp: 0.556580 val loss: 0.685422
[Epoch 12] ogbg-molbbbp: 0.547175 test loss: 0.685158
[Epoch 13; Iter    18/   41] train: loss: 0.6829501
[Epoch 13] ogbg-molbbbp: 0.563782 val loss: 0.683859
[Epoch 13] ogbg-molbbbp: 0.555055 test loss: 0.683616
[Epoch 14; Iter     7/   41] train: loss: 0.6804961
[Epoch 14; Iter    37/   41] train: loss: 0.6804857
[Epoch 14] ogbg-molbbbp: 0.581840 val loss: 0.682408
[Epoch 14] ogbg-molbbbp: 0.571982 test loss: 0.682134
[Epoch 15; Iter    26/   41] train: loss: 0.6790869
[Epoch 15] ogbg-molbbbp: 0.584195 val loss: 0.680547
[Epoch 15] ogbg-molbbbp: 0.573985 test loss: 0.680286
[Epoch 16; Iter    15/   41] train: loss: 0.6801984
[Epoch 16] ogbg-molbbbp: 0.603721 val loss: 0.679138
[Epoch 16] ogbg-molbbbp: 0.593583 test loss: 0.678843
[Epoch 17; Iter     4/   41] train: loss: 0.6820477
[Epoch 17; Iter    34/   41] train: loss: 0.6730562
[Epoch 17] ogbg-molbbbp: 0.752483 val loss: 0.673377
[Epoch 17] ogbg-molbbbp: 0.745376 test loss: 0.672543
[Epoch 18; Iter    23/   41] train: loss: 0.6474040
[Epoch 18] ogbg-molbbbp: 0.826318 val loss: 0.602126
[Epoch 18] ogbg-molbbbp: 0.838208 test loss: 0.602244
[Epoch 19; Iter    12/   41] train: loss: 0.5898503
[Epoch 19] ogbg-molbbbp: 0.838539 val loss: 0.493103
[Epoch 19] ogbg-molbbbp: 0.839577 test loss: 0.498605
[Epoch 20; Iter     1/   41] train: loss: 0.5727776
[Epoch 20; Iter    31/   41] train: loss: 0.6032994
[Epoch 20] ogbg-molbbbp: 0.835330 val loss: 0.486187
[Epoch 20] ogbg-molbbbp: 0.857939 test loss: 0.480771
[Epoch 21; Iter    20/   41] train: loss: 0.4117340
[Epoch 21] ogbg-molbbbp: 0.834716 val loss: 0.403022
[Epoch 21] ogbg-molbbbp: 0.868523 test loss: 0.398069
[Epoch 22; Iter     9/   41] train: loss: 0.4646163
[Epoch 22; Iter    39/   41] train: loss: 0.3760608
[Epoch 22] ogbg-molbbbp: 0.875303 val loss: 0.398485
[Epoch 22] ogbg-molbbbp: 0.876903 test loss: 0.416394
[Epoch 23; Iter    28/   41] train: loss: 0.4200695
[Epoch 23] ogbg-molbbbp: 0.859805 val loss: 0.361589
[Epoch 23] ogbg-molbbbp: 0.869257 test loss: 0.383979
[Epoch 24; Iter    17/   41] train: loss: 0.4386177
[Epoch 24] ogbg-molbbbp: 0.834750 val loss: 0.390259
[Epoch 24] ogbg-molbbbp: 0.854300 test loss: 0.398729
[Epoch 25; Iter     6/   41] train: loss: 0.2807230
[Epoch 25; Iter    36/   41] train: loss: 0.3751805
[Epoch 25] ogbg-molbbbp: 0.883700 val loss: 0.337346
[Epoch 25] ogbg-molbbbp: 0.886185 test loss: 0.366245
[Epoch 26; Iter    25/   41] train: loss: 0.3189388
[Epoch 26] ogbg-molbbbp: 0.884451 val loss: 0.406081
[Epoch 26] ogbg-molbbbp: 0.834368 test loss: 0.456118
[Epoch 27; Iter    14/   41] train: loss: 0.3273700
[Epoch 27] ogbg-molbbbp: 0.884793 val loss: 0.339843
[Epoch 27] ogbg-molbbbp: 0.882946 test loss: 0.366650
[Epoch 28; Iter     3/   41] train: loss: 0.2311388
[Epoch 28; Iter    33/   41] train: loss: 0.2765247
[Epoch 28] ogbg-molbbbp: 0.851408 val loss: 0.429797
[Epoch 28] ogbg-molbbbp: 0.871828 test loss: 0.414290
[Epoch 29; Iter    22/   41] train: loss: 0.2176028
[Epoch 29] ogbg-molbbbp: 0.866155 val loss: 0.350849
[Epoch 29] ogbg-molbbbp: 0.885851 test loss: 0.355270
[Epoch 30; Iter    11/   41] train: loss: 0.2976430
[Epoch 30; Iter    41/   41] train: loss: 0.4242722
[Epoch 30] ogbg-molbbbp: 0.888718 val loss: 0.322681
[Epoch 30] ogbg-molbbbp: 0.884949 test loss: 0.352209
[Epoch 31; Iter    30/   41] train: loss: 0.3786596
[Epoch 31] ogbg-molbbbp: 0.891586 val loss: 0.323054
[Epoch 31] ogbg-molbbbp: 0.899339 test loss: 0.344260
[Epoch 32; Iter    19/   41] train: loss: 0.2561414
[Epoch 32] ogbg-molbbbp: 0.881447 val loss: 0.328890
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.7/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.7_4_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.7
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6935842
[Epoch 1] ogbg-molbbbp: 0.509896 val loss: 0.692993
[Epoch 1] ogbg-molbbbp: 0.512097 test loss: 0.692716
[Epoch 2; Iter    12/   48] train: loss: 0.6914111
[Epoch 2; Iter    42/   48] train: loss: 0.6932908
[Epoch 2] ogbg-molbbbp: 0.469234 val loss: 0.693900
[Epoch 2] ogbg-molbbbp: 0.479349 test loss: 0.692753
[Epoch 3; Iter    24/   48] train: loss: 0.6910983
[Epoch 3] ogbg-molbbbp: 0.469801 val loss: 0.693463
[Epoch 3] ogbg-molbbbp: 0.473476 test loss: 0.692213
[Epoch 4; Iter     6/   48] train: loss: 0.6942453
[Epoch 4; Iter    36/   48] train: loss: 0.6939795
[Epoch 4] ogbg-molbbbp: 0.481937 val loss: 0.692535
[Epoch 4] ogbg-molbbbp: 0.484903 test loss: 0.691224
[Epoch 5; Iter    18/   48] train: loss: 0.6905215
[Epoch 5; Iter    48/   48] train: loss: 0.6924326
[Epoch 5] ogbg-molbbbp: 0.467532 val loss: 0.692324
[Epoch 5] ogbg-molbbbp: 0.481455 test loss: 0.690809
[Epoch 6; Iter    30/   48] train: loss: 0.6919489
[Epoch 6] ogbg-molbbbp: 0.487382 val loss: 0.691312
[Epoch 6] ogbg-molbbbp: 0.500862 test loss: 0.689700
[Epoch 7; Iter    12/   48] train: loss: 0.6922113
[Epoch 7; Iter    42/   48] train: loss: 0.6879218
[Epoch 7] ogbg-molbbbp: 0.496285 val loss: 0.690037
[Epoch 7] ogbg-molbbbp: 0.500926 test loss: 0.688283
[Epoch 8; Iter    24/   48] train: loss: 0.6870048
[Epoch 8] ogbg-molbbbp: 0.498270 val loss: 0.688894
[Epoch 8] ogbg-molbbbp: 0.496649 test loss: 0.687015
[Epoch 9; Iter     6/   48] train: loss: 0.6886299
[Epoch 9; Iter    36/   48] train: loss: 0.6820148
[Epoch 9] ogbg-molbbbp: 0.526059 val loss: 0.687469
[Epoch 9] ogbg-molbbbp: 0.524928 test loss: 0.685470
[Epoch 10; Iter    18/   48] train: loss: 0.6884351
[Epoch 10; Iter    48/   48] train: loss: 0.6828460
[Epoch 10] ogbg-molbbbp: 0.551239 val loss: 0.686192
[Epoch 10] ogbg-molbbbp: 0.539611 test loss: 0.684106
[Epoch 11; Iter    30/   48] train: loss: 0.6842677
[Epoch 11] ogbg-molbbbp: 0.548971 val loss: 0.684667
[Epoch 11] ogbg-molbbbp: 0.544590 test loss: 0.682135
[Epoch 12; Iter    12/   48] train: loss: 0.6841362
[Epoch 12; Iter    42/   48] train: loss: 0.6790006
[Epoch 12] ogbg-molbbbp: 0.553167 val loss: 0.682765
[Epoch 12] ogbg-molbbbp: 0.541462 test loss: 0.679916
[Epoch 13; Iter    24/   48] train: loss: 0.6846143
[Epoch 13] ogbg-molbbbp: 0.586854 val loss: 0.680527
[Epoch 13] ogbg-molbbbp: 0.582317 test loss: 0.677538
[Epoch 14; Iter     6/   48] train: loss: 0.6764281
[Epoch 14; Iter    36/   48] train: loss: 0.6769081
[Epoch 14] ogbg-molbbbp: 0.605512 val loss: 0.678583
[Epoch 14] ogbg-molbbbp: 0.599425 test loss: 0.675291
[Epoch 15; Iter    18/   48] train: loss: 0.6776871
[Epoch 15; Iter    48/   48] train: loss: 0.6523095
[Epoch 15] ogbg-molbbbp: 0.778030 val loss: 0.640477
[Epoch 15] ogbg-molbbbp: 0.789531 test loss: 0.632746
[Epoch 16; Iter    30/   48] train: loss: 0.5919229
[Epoch 16] ogbg-molbbbp: 0.829411 val loss: 0.565552
[Epoch 16] ogbg-molbbbp: 0.851963 test loss: 0.552896
[Epoch 17; Iter    12/   48] train: loss: 0.5420996
[Epoch 17; Iter    42/   48] train: loss: 0.4352461
[Epoch 17] ogbg-molbbbp: 0.778313 val loss: 0.517274
[Epoch 17] ogbg-molbbbp: 0.850048 test loss: 0.486801
[Epoch 18; Iter    24/   48] train: loss: 0.4329810
[Epoch 18] ogbg-molbbbp: 0.812170 val loss: 0.418105
[Epoch 18] ogbg-molbbbp: 0.877625 test loss: 0.354608
[Epoch 19; Iter     6/   48] train: loss: 0.5199912
[Epoch 19; Iter    36/   48] train: loss: 0.4640544
[Epoch 19] ogbg-molbbbp: 0.824023 val loss: 0.487461
[Epoch 19] ogbg-molbbbp: 0.845005 test loss: 0.449806
[Epoch 20; Iter    18/   48] train: loss: 0.4004211
[Epoch 20; Iter    48/   48] train: loss: 0.2344264
[Epoch 20] ogbg-molbbbp: 0.856803 val loss: 0.438672
[Epoch 20] ogbg-molbbbp: 0.841685 test loss: 0.440885
[Epoch 21; Iter    30/   48] train: loss: 0.3512748
[Epoch 21] ogbg-molbbbp: 0.854648 val loss: 0.409282
[Epoch 21] ogbg-molbbbp: 0.866837 test loss: 0.392728
[Epoch 22; Iter    12/   48] train: loss: 0.4977288
[Epoch 22; Iter    42/   48] train: loss: 0.3042977
[Epoch 22] ogbg-molbbbp: 0.854251 val loss: 0.377048
[Epoch 22] ogbg-molbbbp: 0.858410 test loss: 0.352614
[Epoch 23; Iter    24/   48] train: loss: 0.3187957
[Epoch 23] ogbg-molbbbp: 0.841207 val loss: 0.398559
[Epoch 23] ogbg-molbbbp: 0.891478 test loss: 0.329811
[Epoch 24; Iter     6/   48] train: loss: 0.3411392
[Epoch 24; Iter    36/   48] train: loss: 0.4355398
[Epoch 24] ogbg-molbbbp: 0.876935 val loss: 0.359201
[Epoch 24] ogbg-molbbbp: 0.868497 test loss: 0.339223
[Epoch 25; Iter    18/   48] train: loss: 0.4045829
[Epoch 25; Iter    48/   48] train: loss: 0.2522261
[Epoch 25] ogbg-molbbbp: 0.874497 val loss: 0.357550
[Epoch 25] ogbg-molbbbp: 0.881455 test loss: 0.333947
[Epoch 26; Iter    30/   48] train: loss: 0.2243847
[Epoch 26] ogbg-molbbbp: 0.865196 val loss: 0.367440
[Epoch 26] ogbg-molbbbp: 0.863262 test loss: 0.356534
[Epoch 27; Iter    12/   48] train: loss: 0.3049441
[Epoch 27; Iter    42/   48] train: loss: 0.5174732
[Epoch 27] ogbg-molbbbp: 0.890036 val loss: 0.371031
[Epoch 27] ogbg-molbbbp: 0.883754 test loss: 0.325941
[Epoch 28; Iter    24/   48] train: loss: 0.2403078
[Epoch 28] ogbg-molbbbp: 0.882096 val loss: 0.367310
[Epoch 28] ogbg-molbbbp: 0.871752 test loss: 0.376381
[Epoch 29; Iter     6/   48] train: loss: 0.1291635
[Epoch 29; Iter    36/   48] train: loss: 0.2952801
[Epoch 29] ogbg-molbbbp: 0.883174 val loss: 0.327699
[Epoch 29] ogbg-molbbbp: 0.909671 test loss: 0.300807
[Epoch 30; Iter    18/   48] train: loss: 0.4689653
[Epoch 30; Iter    48/   48] train: loss: 0.2903755
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.7/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.7_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.7
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6916667
[Epoch 1] ogbg-molbbbp: 0.539897 val loss: 0.693080
[Epoch 1] ogbg-molbbbp: 0.638685 test loss: 0.692238
[Epoch 2; Iter    12/   48] train: loss: 0.6884851
[Epoch 2; Iter    42/   48] train: loss: 0.6914790
[Epoch 2] ogbg-molbbbp: 0.458459 val loss: 0.694563
[Epoch 2] ogbg-molbbbp: 0.554612 test loss: 0.692485
[Epoch 3; Iter    24/   48] train: loss: 0.6909691
[Epoch 3] ogbg-molbbbp: 0.447343 val loss: 0.694300
[Epoch 3] ogbg-molbbbp: 0.534312 test loss: 0.692168
[Epoch 4; Iter     6/   48] train: loss: 0.6942346
[Epoch 4; Iter    36/   48] train: loss: 0.6890310
[Epoch 4] ogbg-molbbbp: 0.452447 val loss: 0.693414
[Epoch 4] ogbg-molbbbp: 0.539419 test loss: 0.691049
[Epoch 5; Iter    18/   48] train: loss: 0.6942822
[Epoch 5; Iter    48/   48] train: loss: 0.6896762
[Epoch 5] ogbg-molbbbp: 0.466058 val loss: 0.692250
[Epoch 5] ogbg-molbbbp: 0.547079 test loss: 0.689836
[Epoch 6; Iter    30/   48] train: loss: 0.6896101
[Epoch 6] ogbg-molbbbp: 0.475415 val loss: 0.691603
[Epoch 6] ogbg-molbbbp: 0.568401 test loss: 0.688941
[Epoch 7; Iter    12/   48] train: loss: 0.6883151
[Epoch 7; Iter    42/   48] train: loss: 0.6879133
[Epoch 7] ogbg-molbbbp: 0.482504 val loss: 0.690528
[Epoch 7] ogbg-molbbbp: 0.560868 test loss: 0.687931
[Epoch 8; Iter    24/   48] train: loss: 0.6879059
[Epoch 8] ogbg-molbbbp: 0.479215 val loss: 0.689483
[Epoch 8] ogbg-molbbbp: 0.566677 test loss: 0.686444
[Epoch 9; Iter     6/   48] train: loss: 0.6838872
[Epoch 9; Iter    36/   48] train: loss: 0.6883819
[Epoch 9] ogbg-molbbbp: 0.498837 val loss: 0.688194
[Epoch 9] ogbg-molbbbp: 0.574976 test loss: 0.685123
[Epoch 10; Iter    18/   48] train: loss: 0.6832650
[Epoch 10; Iter    48/   48] train: loss: 0.6834748
[Epoch 10] ogbg-molbbbp: 0.495888 val loss: 0.687500
[Epoch 10] ogbg-molbbbp: 0.570508 test loss: 0.684220
[Epoch 11; Iter    30/   48] train: loss: 0.6831412
[Epoch 11] ogbg-molbbbp: 0.511087 val loss: 0.685750
[Epoch 11] ogbg-molbbbp: 0.583722 test loss: 0.682309
[Epoch 12; Iter    12/   48] train: loss: 0.6808711
[Epoch 12; Iter    42/   48] train: loss: 0.6822753
[Epoch 12] ogbg-molbbbp: 0.534849 val loss: 0.683586
[Epoch 12] ogbg-molbbbp: 0.608746 test loss: 0.679696
[Epoch 13; Iter    24/   48] train: loss: 0.6754858
[Epoch 13] ogbg-molbbbp: 0.530426 val loss: 0.682080
[Epoch 13] ogbg-molbbbp: 0.600894 test loss: 0.677935
[Epoch 14; Iter     6/   48] train: loss: 0.6758323
[Epoch 14; Iter    36/   48] train: loss: 0.6711096
[Epoch 14] ogbg-molbbbp: 0.535303 val loss: 0.680104
[Epoch 14] ogbg-molbbbp: 0.603256 test loss: 0.675599
[Epoch 15; Iter    18/   48] train: loss: 0.6738411
[Epoch 15; Iter    48/   48] train: loss: 0.6725889
[Epoch 15] ogbg-molbbbp: 0.769863 val loss: 0.636405
[Epoch 15] ogbg-molbbbp: 0.791318 test loss: 0.627422
[Epoch 16; Iter    30/   48] train: loss: 0.5801051
[Epoch 16] ogbg-molbbbp: 0.810526 val loss: 0.570905
[Epoch 16] ogbg-molbbbp: 0.868624 test loss: 0.551602
[Epoch 17; Iter    12/   48] train: loss: 0.4999028
[Epoch 17; Iter    42/   48] train: loss: 0.4937561
[Epoch 17] ogbg-molbbbp: 0.816310 val loss: 0.532764
[Epoch 17] ogbg-molbbbp: 0.863071 test loss: 0.491900
[Epoch 18; Iter    24/   48] train: loss: 0.4383457
[Epoch 18] ogbg-molbbbp: 0.845460 val loss: 0.415801
[Epoch 18] ogbg-molbbbp: 0.865943 test loss: 0.380362
[Epoch 19; Iter     6/   48] train: loss: 0.4855867
[Epoch 19; Iter    36/   48] train: loss: 0.3591572
[Epoch 19] ogbg-molbbbp: 0.856576 val loss: 0.411525
[Epoch 19] ogbg-molbbbp: 0.870986 test loss: 0.395502
[Epoch 20; Iter    18/   48] train: loss: 0.3651663
[Epoch 20; Iter    48/   48] train: loss: 0.4439963
[Epoch 20] ogbg-molbbbp: 0.791300 val loss: 0.431470
[Epoch 20] ogbg-molbbbp: 0.807980 test loss: 0.387283
[Epoch 21; Iter    30/   48] train: loss: 0.3206883
[Epoch 21] ogbg-molbbbp: 0.868542 val loss: 0.373359
[Epoch 21] ogbg-molbbbp: 0.843728 test loss: 0.371967
[Epoch 22; Iter    12/   48] train: loss: 0.3430307
[Epoch 22; Iter    42/   48] train: loss: 0.3032878
[Epoch 22] ogbg-molbbbp: 0.877786 val loss: 0.367149
[Epoch 22] ogbg-molbbbp: 0.882413 test loss: 0.350427
[Epoch 23; Iter    24/   48] train: loss: 0.3522995
[Epoch 23] ogbg-molbbbp: 0.859638 val loss: 0.371878
[Epoch 23] ogbg-molbbbp: 0.887009 test loss: 0.335312
[Epoch 24; Iter     6/   48] train: loss: 0.2624466
[Epoch 24; Iter    36/   48] train: loss: 0.2402040
[Epoch 24] ogbg-molbbbp: 0.857313 val loss: 0.367284
[Epoch 24] ogbg-molbbbp: 0.862815 test loss: 0.341011
[Epoch 25; Iter    18/   48] train: loss: 0.2251476
[Epoch 25; Iter    48/   48] train: loss: 0.2017074
[Epoch 25] ogbg-molbbbp: 0.883627 val loss: 0.346721
[Epoch 25] ogbg-molbbbp: 0.889563 test loss: 0.331304
[Epoch 26; Iter    30/   48] train: loss: 0.2961519
[Epoch 26] ogbg-molbbbp: 0.894629 val loss: 0.333590
[Epoch 26] ogbg-molbbbp: 0.874242 test loss: 0.333543
[Epoch 27; Iter    12/   48] train: loss: 0.2501478
[Epoch 27; Iter    42/   48] train: loss: 0.6812596
[Epoch 27] ogbg-molbbbp: 0.882436 val loss: 0.346884
[Epoch 27] ogbg-molbbbp: 0.893712 test loss: 0.321472
[Epoch 28; Iter    24/   48] train: loss: 0.1567149
[Epoch 28] ogbg-molbbbp: 0.891057 val loss: 0.346424
[Epoch 28] ogbg-molbbbp: 0.875774 test loss: 0.342174
[Epoch 29; Iter     6/   48] train: loss: 0.3282577
[Epoch 29; Iter    36/   48] train: loss: 0.2806199
[Epoch 29] ogbg-molbbbp: 0.890943 val loss: 0.330195
[Epoch 29] ogbg-molbbbp: 0.893904 test loss: 0.331216
[Epoch 30; Iter    18/   48] train: loss: 0.1627001
[Epoch 30; Iter    48/   48] train: loss: 0.1886426
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.6/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.6_4_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.6
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6913597
[Epoch 1] ogbg-molbbbp: 0.485714 val loss: 0.693074
[Epoch 1] ogbg-molbbbp: 0.552651 test loss: 0.692904
[Epoch 2; Iter    19/   41] train: loss: 0.6927155
[Epoch 2] ogbg-molbbbp: 0.444649 val loss: 0.693711
[Epoch 2] ogbg-molbbbp: 0.522970 test loss: 0.692684
[Epoch 3; Iter     8/   41] train: loss: 0.6929514
[Epoch 3; Iter    38/   41] train: loss: 0.6927564
[Epoch 3] ogbg-molbbbp: 0.440212 val loss: 0.693866
[Epoch 3] ogbg-molbbbp: 0.515024 test loss: 0.692528
[Epoch 4; Iter    27/   41] train: loss: 0.6931570
[Epoch 4] ogbg-molbbbp: 0.449565 val loss: 0.693115
[Epoch 4] ogbg-molbbbp: 0.530015 test loss: 0.691883
[Epoch 5; Iter    16/   41] train: loss: 0.6905599
[Epoch 5] ogbg-molbbbp: 0.446288 val loss: 0.692852
[Epoch 5] ogbg-molbbbp: 0.524973 test loss: 0.691545
[Epoch 6; Iter     5/   41] train: loss: 0.6911651
[Epoch 6; Iter    35/   41] train: loss: 0.6938124
[Epoch 6] ogbg-molbbbp: 0.454958 val loss: 0.692033
[Epoch 6] ogbg-molbbbp: 0.536124 test loss: 0.690762
[Epoch 7; Iter    24/   41] train: loss: 0.6886508
[Epoch 7] ogbg-molbbbp: 0.472845 val loss: 0.691178
[Epoch 7] ogbg-molbbbp: 0.550114 test loss: 0.689879
[Epoch 8; Iter    13/   41] train: loss: 0.6939428
[Epoch 8] ogbg-molbbbp: 0.471719 val loss: 0.690558
[Epoch 8] ogbg-molbbbp: 0.543737 test loss: 0.689203
[Epoch 9; Iter     2/   41] train: loss: 0.6918962
[Epoch 9; Iter    32/   41] train: loss: 0.6914837
[Epoch 9] ogbg-molbbbp: 0.474211 val loss: 0.689285
[Epoch 9] ogbg-molbbbp: 0.545206 test loss: 0.688044
[Epoch 10; Iter    21/   41] train: loss: 0.6869606
[Epoch 10] ogbg-molbbbp: 0.488957 val loss: 0.688715
[Epoch 10] ogbg-molbbbp: 0.553118 test loss: 0.687515
[Epoch 11; Iter    10/   41] train: loss: 0.6858602
[Epoch 11; Iter    40/   41] train: loss: 0.6838846
[Epoch 11] ogbg-molbbbp: 0.498891 val loss: 0.687022
[Epoch 11] ogbg-molbbbp: 0.560296 test loss: 0.685867
[Epoch 12; Iter    29/   41] train: loss: 0.6856677
[Epoch 12] ogbg-molbbbp: 0.517324 val loss: 0.685367
[Epoch 12] ogbg-molbbbp: 0.579026 test loss: 0.684108
[Epoch 13; Iter    18/   41] train: loss: 0.6861035
[Epoch 13] ogbg-molbbbp: 0.523536 val loss: 0.684122
[Epoch 13] ogbg-molbbbp: 0.585837 test loss: 0.682913
[Epoch 14; Iter     7/   41] train: loss: 0.6810261
[Epoch 14; Iter    37/   41] train: loss: 0.6795514
[Epoch 14] ogbg-molbbbp: 0.561256 val loss: 0.682343
[Epoch 14] ogbg-molbbbp: 0.613615 test loss: 0.681219
[Epoch 15; Iter    26/   41] train: loss: 0.6791449
[Epoch 15] ogbg-molbbbp: 0.561324 val loss: 0.680877
[Epoch 15] ogbg-molbbbp: 0.606838 test loss: 0.679704
[Epoch 16; Iter    15/   41] train: loss: 0.6846582
[Epoch 16] ogbg-molbbbp: 0.578460 val loss: 0.678790
[Epoch 16] ogbg-molbbbp: 0.624065 test loss: 0.677755
[Epoch 17; Iter     4/   41] train: loss: 0.6744598
[Epoch 17; Iter    34/   41] train: loss: 0.6754645
[Epoch 17] ogbg-molbbbp: 0.730056 val loss: 0.671166
[Epoch 17] ogbg-molbbbp: 0.742087 test loss: 0.669048
[Epoch 18; Iter    23/   41] train: loss: 0.6599316
[Epoch 18] ogbg-molbbbp: 0.806554 val loss: 0.612972
[Epoch 18] ogbg-molbbbp: 0.815071 test loss: 0.612220
[Epoch 19; Iter    12/   41] train: loss: 0.6133181
[Epoch 19] ogbg-molbbbp: 0.830278 val loss: 0.562051
[Epoch 19] ogbg-molbbbp: 0.848190 test loss: 0.555930
[Epoch 20; Iter     1/   41] train: loss: 0.5158917
[Epoch 20; Iter    31/   41] train: loss: 0.4957594
[Epoch 20] ogbg-molbbbp: 0.813688 val loss: 0.503915
[Epoch 20] ogbg-molbbbp: 0.789697 test loss: 0.523569
[Epoch 21; Iter    20/   41] train: loss: 0.4428020
[Epoch 21] ogbg-molbbbp: 0.840655 val loss: 0.462363
[Epoch 21] ogbg-molbbbp: 0.869491 test loss: 0.448794
[Epoch 22; Iter     9/   41] train: loss: 0.3497165
[Epoch 22; Iter    39/   41] train: loss: 0.4525422
[Epoch 22] ogbg-molbbbp: 0.866837 val loss: 0.407543
[Epoch 22] ogbg-molbbbp: 0.840411 test loss: 0.433987
[Epoch 23; Iter    28/   41] train: loss: 0.4046486
[Epoch 23] ogbg-molbbbp: 0.853286 val loss: 0.370072
[Epoch 23] ogbg-molbbbp: 0.880409 test loss: 0.375628
[Epoch 24; Iter    17/   41] train: loss: 0.4338471
[Epoch 24] ogbg-molbbbp: 0.872197 val loss: 0.404668
[Epoch 24] ogbg-molbbbp: 0.864416 test loss: 0.425619
[Epoch 25; Iter     6/   41] train: loss: 0.2767140
[Epoch 25; Iter    36/   41] train: loss: 0.2302225
[Epoch 25] ogbg-molbbbp: 0.874723 val loss: 0.364178
[Epoch 25] ogbg-molbbbp: 0.874499 test loss: 0.387343
[Epoch 26; Iter    25/   41] train: loss: 0.3888972
[Epoch 26] ogbg-molbbbp: 0.865438 val loss: 0.350519
[Epoch 26] ogbg-molbbbp: 0.886485 test loss: 0.369849
[Epoch 27; Iter    14/   41] train: loss: 0.3980219
[Epoch 27] ogbg-molbbbp: 0.891142 val loss: 0.361248
[Epoch 27] ogbg-molbbbp: 0.843817 test loss: 0.426094
[Epoch 28; Iter     3/   41] train: loss: 0.4551141
[Epoch 28; Iter    33/   41] train: loss: 0.4020144
[Epoch 28] ogbg-molbbbp: 0.857279 val loss: 0.396981
[Epoch 28] ogbg-molbbbp: 0.865151 test loss: 0.425355
[Epoch 29; Iter    22/   41] train: loss: 0.2930351
[Epoch 29] ogbg-molbbbp: 0.884315 val loss: 0.326125
[Epoch 29] ogbg-molbbbp: 0.890558 test loss: 0.354837
[Epoch 30; Iter    11/   41] train: loss: 0.5099937
[Epoch 30; Iter    41/   41] train: loss: 0.1923447
[Epoch 30] ogbg-molbbbp: 0.881925 val loss: 0.348881
[Epoch 30] ogbg-molbbbp: 0.889824 test loss: 0.381803
[Epoch 31; Iter    30/   41] train: loss: 0.1840885
[Epoch 31] ogbg-molbbbp: 0.862127 val loss: 0.376513
[Epoch 31] ogbg-molbbbp: 0.886185 test loss: 0.377857
[Epoch 32; Iter    19/   41] train: loss: 0.4430976
[Epoch 32] ogbg-molbbbp: 0.896330 val loss: 0.327266
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bbbp/random/train_prop=0.6/PNA_ogbg-molbbbp_3DInfomax_bbbp_random=0.6_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_random=0.6
logdir: runs/split/3DInfomax/bbbp/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6908110
[Epoch 1] ogbg-molbbbp: 0.621915 val loss: 0.692498
[Epoch 1] ogbg-molbbbp: 0.639189 test loss: 0.692385
[Epoch 2; Iter    19/   41] train: loss: 0.6884907
[Epoch 2] ogbg-molbbbp: 0.612425 val loss: 0.691802
[Epoch 2] ogbg-molbbbp: 0.633948 test loss: 0.691162
[Epoch 3; Iter     8/   41] train: loss: 0.6917577
[Epoch 3; Iter    38/   41] train: loss: 0.6903417
[Epoch 3] ogbg-molbbbp: 0.601434 val loss: 0.692426
[Epoch 3] ogbg-molbbbp: 0.620960 test loss: 0.691724
[Epoch 4; Iter    27/   41] train: loss: 0.6912121
[Epoch 4] ogbg-molbbbp: 0.602389 val loss: 0.691411
[Epoch 4] ogbg-molbbbp: 0.625033 test loss: 0.690670
[Epoch 5; Iter    16/   41] train: loss: 0.6908927
[Epoch 5] ogbg-molbbbp: 0.600546 val loss: 0.691303
[Epoch 5] ogbg-molbbbp: 0.621361 test loss: 0.690631
[Epoch 6; Iter     5/   41] train: loss: 0.6892291
[Epoch 6; Iter    35/   41] train: loss: 0.6895577
[Epoch 6] ogbg-molbbbp: 0.600888 val loss: 0.690458
[Epoch 6] ogbg-molbbbp: 0.622796 test loss: 0.689689
[Epoch 7; Iter    24/   41] train: loss: 0.6919470
[Epoch 7] ogbg-molbbbp: 0.612391 val loss: 0.689298
[Epoch 7] ogbg-molbbbp: 0.637954 test loss: 0.688483
[Epoch 8; Iter    13/   41] train: loss: 0.6901947
[Epoch 8] ogbg-molbbbp: 0.617136 val loss: 0.688349
[Epoch 8] ogbg-molbbbp: 0.637921 test loss: 0.687614
[Epoch 9; Iter     2/   41] train: loss: 0.6852729
[Epoch 9; Iter    32/   41] train: loss: 0.6856258
[Epoch 9] ogbg-molbbbp: 0.619628 val loss: 0.687698
[Epoch 9] ogbg-molbbbp: 0.637320 test loss: 0.687007
[Epoch 10; Iter    21/   41] train: loss: 0.6890130
[Epoch 10] ogbg-molbbbp: 0.624714 val loss: 0.686703
[Epoch 10] ogbg-molbbbp: 0.640759 test loss: 0.686071
[Epoch 11; Iter    10/   41] train: loss: 0.6880288
[Epoch 11; Iter    40/   41] train: loss: 0.6818696
[Epoch 11] ogbg-molbbbp: 0.631200 val loss: 0.684935
[Epoch 11] ogbg-molbbbp: 0.648972 test loss: 0.684323
[Epoch 12; Iter    29/   41] train: loss: 0.6798357
[Epoch 12] ogbg-molbbbp: 0.628230 val loss: 0.683525
[Epoch 12] ogbg-molbbbp: 0.646368 test loss: 0.682901
[Epoch 13; Iter    18/   41] train: loss: 0.6819306
[Epoch 13] ogbg-molbbbp: 0.640007 val loss: 0.682755
[Epoch 13] ogbg-molbbbp: 0.655849 test loss: 0.682063
[Epoch 14; Iter     7/   41] train: loss: 0.6822565
[Epoch 14; Iter    37/   41] train: loss: 0.6845528
[Epoch 14] ogbg-molbbbp: 0.642840 val loss: 0.680427
[Epoch 14] ogbg-molbbbp: 0.661458 test loss: 0.679806
[Epoch 15; Iter    26/   41] train: loss: 0.6823533
[Epoch 15] ogbg-molbbbp: 0.652808 val loss: 0.679307
[Epoch 15] ogbg-molbbbp: 0.666800 test loss: 0.678682
[Epoch 16; Iter    15/   41] train: loss: 0.6763738
[Epoch 16] ogbg-molbbbp: 0.656631 val loss: 0.677293
[Epoch 16] ogbg-molbbbp: 0.669171 test loss: 0.676683
[Epoch 17; Iter     4/   41] train: loss: 0.6825779
[Epoch 17; Iter    34/   41] train: loss: 0.6716037
[Epoch 17] ogbg-molbbbp: 0.749821 val loss: 0.668494
[Epoch 17] ogbg-molbbbp: 0.753439 test loss: 0.666958
[Epoch 18; Iter    23/   41] train: loss: 0.6426018
[Epoch 18] ogbg-molbbbp: 0.819355 val loss: 0.623846
[Epoch 18] ogbg-molbbbp: 0.829961 test loss: 0.619247
[Epoch 19; Iter    12/   41] train: loss: 0.6253926
[Epoch 19] ogbg-molbbbp: 0.839563 val loss: 0.533565
[Epoch 19] ogbg-molbbbp: 0.835437 test loss: 0.538844
[Epoch 20; Iter     1/   41] train: loss: 0.5848199
[Epoch 20; Iter    31/   41] train: loss: 0.5241285
[Epoch 20] ogbg-molbbbp: 0.839427 val loss: 0.432577
[Epoch 20] ogbg-molbbbp: 0.863949 test loss: 0.438779
[Epoch 21; Iter    20/   41] train: loss: 0.4885396
[Epoch 21] ogbg-molbbbp: 0.861922 val loss: 0.424912
[Epoch 21] ogbg-molbbbp: 0.870793 test loss: 0.432841
[Epoch 22; Iter     9/   41] train: loss: 0.4402068
[Epoch 22; Iter    39/   41] train: loss: 0.3952175
[Epoch 22] ogbg-molbbbp: 0.846936 val loss: 0.373704
[Epoch 22] ogbg-molbbbp: 0.857071 test loss: 0.382103
[Epoch 23; Iter    28/   41] train: loss: 0.3610352
[Epoch 23] ogbg-molbbbp: 0.876771 val loss: 0.364560
[Epoch 23] ogbg-molbbbp: 0.829093 test loss: 0.408058
[Epoch 24; Iter    17/   41] train: loss: 0.5456104
[Epoch 24] ogbg-molbbbp: 0.878341 val loss: 0.360495
[Epoch 24] ogbg-molbbbp: 0.875234 test loss: 0.383509
[Epoch 25; Iter     6/   41] train: loss: 0.2794520
[Epoch 25; Iter    36/   41] train: loss: 0.3427503
[Epoch 25] ogbg-molbbbp: 0.870353 val loss: 0.376652
[Epoch 25] ogbg-molbbbp: 0.864717 test loss: 0.404811
[Epoch 26; Iter    25/   41] train: loss: 0.4194710
[Epoch 26] ogbg-molbbbp: 0.858918 val loss: 0.361558
[Epoch 26] ogbg-molbbbp: 0.863649 test loss: 0.379030
[Epoch 27; Iter    14/   41] train: loss: 0.2407038
[Epoch 27] ogbg-molbbbp: 0.891313 val loss: 0.335935
[Epoch 27] ogbg-molbbbp: 0.887186 test loss: 0.369237
[Epoch 28; Iter     3/   41] train: loss: 0.3944410
[Epoch 28; Iter    33/   41] train: loss: 0.3543102
[Epoch 28] ogbg-molbbbp: 0.871855 val loss: 0.373061
[Epoch 28] ogbg-molbbbp: 0.878138 test loss: 0.388961
[Epoch 29; Iter    22/   41] train: loss: 0.3630021
[Epoch 29] ogbg-molbbbp: 0.890527 val loss: 0.326001
[Epoch 29] ogbg-molbbbp: 0.876603 test loss: 0.371560
[Epoch 30; Iter    11/   41] train: loss: 0.3281109
[Epoch 30; Iter    41/   41] train: loss: 0.3101511
[Epoch 30] ogbg-molbbbp: 0.887080 val loss: 0.339811
[Epoch 30] ogbg-molbbbp: 0.870760 test loss: 0.376551
[Epoch 31; Iter    30/   41] train: loss: 0.1865671
[Epoch 31] ogbg-molbbbp: 0.868237 val loss: 0.358921
[Epoch 31] ogbg-molbbbp: 0.878940 test loss: 0.364482
[Epoch 32; Iter    19/   41] train: loss: 0.4432585
[Epoch 32] ogbg-molbbbp: 0.883359 val loss: 0.340602
[Epoch 28] ogbg-molbbbp: 0.899618 test loss: 0.353995
[Epoch 29; Iter    20/   55] train: loss: 0.3075878
[Epoch 29; Iter    50/   55] train: loss: 0.5171445
[Epoch 29] ogbg-molbbbp: 0.875926 val loss: 0.586879
[Epoch 29] ogbg-molbbbp: 0.853909 test loss: 0.638466
[Epoch 30; Iter    25/   55] train: loss: 0.2161307
[Epoch 30; Iter    55/   55] train: loss: 0.2229426
[Epoch 30] ogbg-molbbbp: 0.917160 val loss: 0.355351
[Epoch 30] ogbg-molbbbp: 0.889036 test loss: 0.312725
[Epoch 31; Iter    30/   55] train: loss: 0.6528261
[Epoch 31] ogbg-molbbbp: 0.897778 val loss: 0.356080
[Epoch 31] ogbg-molbbbp: 0.871399 test loss: 0.348365
[Epoch 32; Iter     5/   55] train: loss: 0.1465756
[Epoch 32; Iter    35/   55] train: loss: 0.2974901
[Epoch 32] ogbg-molbbbp: 0.927778 val loss: 0.374260
[Epoch 32] ogbg-molbbbp: 0.905938 test loss: 0.396382
[Epoch 33; Iter    10/   55] train: loss: 0.2136253
[Epoch 33; Iter    40/   55] train: loss: 0.1557708
[Epoch 33] ogbg-molbbbp: 0.894321 val loss: 0.355633
[Epoch 33] ogbg-molbbbp: 0.873457 test loss: 0.324661
[Epoch 34; Iter    15/   55] train: loss: 0.2936758
[Epoch 34; Iter    45/   55] train: loss: 0.1703124
[Epoch 34] ogbg-molbbbp: 0.921358 val loss: 0.331816
[Epoch 34] ogbg-molbbbp: 0.877278 test loss: 0.351235
[Epoch 35; Iter    20/   55] train: loss: 0.0710025
[Epoch 35; Iter    50/   55] train: loss: 0.0809532
[Epoch 35] ogbg-molbbbp: 0.926049 val loss: 0.310107
[Epoch 35] ogbg-molbbbp: 0.894915 test loss: 0.304481
[Epoch 36; Iter    25/   55] train: loss: 0.1613248
[Epoch 36; Iter    55/   55] train: loss: 0.4298684
[Epoch 36] ogbg-molbbbp: 0.906173 val loss: 0.467068
[Epoch 36] ogbg-molbbbp: 0.849353 test loss: 0.571963
[Epoch 37; Iter    30/   55] train: loss: 0.3490121
[Epoch 37] ogbg-molbbbp: 0.919877 val loss: 0.316255
[Epoch 37] ogbg-molbbbp: 0.904321 test loss: 0.320470
[Epoch 38; Iter     5/   55] train: loss: 0.1605601
[Epoch 38; Iter    35/   55] train: loss: 0.3391476
[Epoch 38] ogbg-molbbbp: 0.912222 val loss: 0.325391
[Epoch 38] ogbg-molbbbp: 0.892122 test loss: 0.324073
[Epoch 39; Iter    10/   55] train: loss: 0.1494413
[Epoch 39; Iter    40/   55] train: loss: 0.1641751
[Epoch 39] ogbg-molbbbp: 0.925309 val loss: 0.304851
[Epoch 39] ogbg-molbbbp: 0.907995 test loss: 0.311998
[Epoch 40; Iter    15/   55] train: loss: 0.1804425
[Epoch 40; Iter    45/   55] train: loss: 0.2714556
[Epoch 40] ogbg-molbbbp: 0.921605 val loss: 0.328155
[Epoch 40] ogbg-molbbbp: 0.900500 test loss: 0.328866
[Epoch 41; Iter    20/   55] train: loss: 0.2744906
[Epoch 41; Iter    50/   55] train: loss: 0.1828370
[Epoch 41] ogbg-molbbbp: 0.914691 val loss: 0.374976
[Epoch 41] ogbg-molbbbp: 0.882422 test loss: 0.458314
[Epoch 42; Iter    25/   55] train: loss: 0.1235747
[Epoch 42; Iter    55/   55] train: loss: 0.3745866
[Epoch 42] ogbg-molbbbp: 0.944074 val loss: 0.273218
[Epoch 42] ogbg-molbbbp: 0.915491 test loss: 0.300409
[Epoch 43; Iter    30/   55] train: loss: 0.1264819
[Epoch 43] ogbg-molbbbp: 0.932963 val loss: 0.322506
[Epoch 43] ogbg-molbbbp: 0.895356 test loss: 0.446777
[Epoch 44; Iter     5/   55] train: loss: 0.1155515
[Epoch 44; Iter    35/   55] train: loss: 0.1858210
[Epoch 44] ogbg-molbbbp: 0.940864 val loss: 0.301566
[Epoch 44] ogbg-molbbbp: 0.914021 test loss: 0.326639
[Epoch 45; Iter    10/   55] train: loss: 0.1428875
[Epoch 45; Iter    40/   55] train: loss: 0.1500605
[Epoch 45] ogbg-molbbbp: 0.932346 val loss: 0.304377
[Epoch 45] ogbg-molbbbp: 0.916079 test loss: 0.311808
[Epoch 46; Iter    15/   55] train: loss: 0.1641642
[Epoch 46; Iter    45/   55] train: loss: 0.1267248
[Epoch 46] ogbg-molbbbp: 0.930000 val loss: 0.309654
[Epoch 46] ogbg-molbbbp: 0.893886 test loss: 0.343132
[Epoch 47; Iter    20/   55] train: loss: 0.4024186
[Epoch 47; Iter    50/   55] train: loss: 0.4480939
[Epoch 47] ogbg-molbbbp: 0.944938 val loss: 0.278397
[Epoch 47] ogbg-molbbbp: 0.907701 test loss: 0.318489
[Epoch 48; Iter    25/   55] train: loss: 0.1347903
[Epoch 48; Iter    55/   55] train: loss: 0.0899776
[Epoch 48] ogbg-molbbbp: 0.900000 val loss: 0.389362
[Epoch 48] ogbg-molbbbp: 0.862287 test loss: 0.392668
[Epoch 49; Iter    30/   55] train: loss: 0.2304422
[Epoch 49] ogbg-molbbbp: 0.935802 val loss: 0.320626
[Epoch 49] ogbg-molbbbp: 0.888448 test loss: 0.412723
[Epoch 50; Iter     5/   55] train: loss: 0.0867900
[Epoch 50; Iter    35/   55] train: loss: 0.1375341
[Epoch 50] ogbg-molbbbp: 0.931235 val loss: 0.379774
[Epoch 50] ogbg-molbbbp: 0.900500 test loss: 0.452702
[Epoch 51; Iter    10/   55] train: loss: 0.0681273
[Epoch 51; Iter    40/   55] train: loss: 0.3208205
[Epoch 51] ogbg-molbbbp: 0.930370 val loss: 0.320012
[Epoch 51] ogbg-molbbbp: 0.892122 test loss: 0.374118
[Epoch 52; Iter    15/   55] train: loss: 0.1212302
[Epoch 52; Iter    45/   55] train: loss: 0.0683494
[Epoch 52] ogbg-molbbbp: 0.936543 val loss: 0.315345
[Epoch 52] ogbg-molbbbp: 0.895797 test loss: 0.342942
[Epoch 53; Iter    20/   55] train: loss: 0.0728401
[Epoch 53; Iter    50/   55] train: loss: 0.1234937
[Epoch 53] ogbg-molbbbp: 0.907778 val loss: 0.392896
[Epoch 53] ogbg-molbbbp: 0.887713 test loss: 0.443453
[Epoch 54; Iter    25/   55] train: loss: 0.1770940
[Epoch 54; Iter    55/   55] train: loss: 0.2350342
[Epoch 54] ogbg-molbbbp: 0.921358 val loss: 0.330696
[Epoch 54] ogbg-molbbbp: 0.858466 test loss: 0.398691
[Epoch 55; Iter    30/   55] train: loss: 0.0867662
[Epoch 55] ogbg-molbbbp: 0.906790 val loss: 0.385595
[Epoch 55] ogbg-molbbbp: 0.876984 test loss: 0.390638
[Epoch 56; Iter     5/   55] train: loss: 0.1685722
[Epoch 56; Iter    35/   55] train: loss: 0.1492346
[Epoch 56] ogbg-molbbbp: 0.936173 val loss: 0.330270
[Epoch 56] ogbg-molbbbp: 0.902410 test loss: 0.368544
[Epoch 57; Iter    10/   55] train: loss: 0.1261508
[Epoch 57; Iter    40/   55] train: loss: 0.0898154
[Epoch 57] ogbg-molbbbp: 0.911481 val loss: 0.411867
[Epoch 57] ogbg-molbbbp: 0.885509 test loss: 0.407238
[Epoch 58; Iter    15/   55] train: loss: 0.0816159
[Epoch 58; Iter    45/   55] train: loss: 0.1406195
[Epoch 58] ogbg-molbbbp: 0.945556 val loss: 0.303932
[Epoch 58] ogbg-molbbbp: 0.894768 test loss: 0.403539
[Epoch 59; Iter    20/   55] train: loss: 0.1066903
[Epoch 59; Iter    50/   55] train: loss: 0.0550697
[Epoch 59] ogbg-molbbbp: 0.923827 val loss: 0.373558
[Epoch 59] ogbg-molbbbp: 0.875661 test loss: 0.485698
[Epoch 60; Iter    25/   55] train: loss: 0.1984156
[Epoch 60; Iter    55/   55] train: loss: 0.0959706
[Epoch 60] ogbg-molbbbp: 0.937654 val loss: 0.353331
[Epoch 60] ogbg-molbbbp: 0.889918 test loss: 0.437644
[Epoch 61; Iter    30/   55] train: loss: 0.0390443
[Epoch 61] ogbg-molbbbp: 0.934074 val loss: 0.379881
[Epoch 61] ogbg-molbbbp: 0.897266 test loss: 0.441275
[Epoch 62; Iter     5/   55] train: loss: 0.2039149
[Epoch 62; Iter    35/   55] train: loss: 0.0369222
[Epoch 62] ogbg-molbbbp: 0.919630 val loss: 0.449619
[Epoch 62] ogbg-molbbbp: 0.891387 test loss: 0.526990
[Epoch 63; Iter    10/   55] train: loss: 0.0235046
[Epoch 63; Iter    40/   55] train: loss: 0.0378773
[Epoch 63] ogbg-molbbbp: 0.929753 val loss: 0.404791
[Epoch 63] ogbg-molbbbp: 0.899324 test loss: 0.412135
[Epoch 64; Iter    15/   55] train: loss: 0.1749992
[Epoch 64; Iter    45/   55] train: loss: 0.0248236
[Epoch 64] ogbg-molbbbp: 0.927654 val loss: 0.394837
[Epoch 64] ogbg-molbbbp: 0.892416 test loss: 0.472577
[Epoch 65; Iter    20/   55] train: loss: 0.0299922
[Epoch 65; Iter    50/   55] train: loss: 0.0405088
[Epoch 65] ogbg-molbbbp: 0.927407 val loss: 0.448026
[Epoch 65] ogbg-molbbbp: 0.890506 test loss: 0.584867
[Epoch 66; Iter    25/   55] train: loss: 0.0180171
[Epoch 66; Iter    55/   55] train: loss: 0.0418918
[Epoch 66] ogbg-molbbbp: 0.911852 val loss: 0.431750
[Epoch 66] ogbg-molbbbp: 0.881099 test loss: 0.478911
[Epoch 67; Iter    30/   55] train: loss: 0.0178409
[Epoch 67] ogbg-molbbbp: 0.916914 val loss: 0.479076
[Epoch 67] ogbg-molbbbp: 0.875808 test loss: 0.654072
[Epoch 68; Iter     5/   55] train: loss: 0.0072719
[Epoch 68; Iter    35/   55] train: loss: 0.0069586
[Epoch 68] ogbg-molbbbp: 0.930247 val loss: 0.409427
[Epoch 68] ogbg-molbbbp: 0.897707 test loss: 0.501080
[Epoch 69; Iter    10/   55] train: loss: 0.0143221
[Epoch 28] ogbg-molbbbp: 0.886537 test loss: 0.343981
[Epoch 29; Iter    20/   55] train: loss: 0.3670892
[Epoch 29; Iter    50/   55] train: loss: 0.1757877
[Epoch 29] ogbg-molbbbp: 0.897160 val loss: 0.370766
[Epoch 29] ogbg-molbbbp: 0.877278 test loss: 0.352187
[Epoch 30; Iter    25/   55] train: loss: 0.4450252
[Epoch 30; Iter    55/   55] train: loss: 0.1361194
[Epoch 30] ogbg-molbbbp: 0.925679 val loss: 0.303587
[Epoch 30] ogbg-molbbbp: 0.926661 test loss: 0.285140
[Epoch 31; Iter    30/   55] train: loss: 0.3560537
[Epoch 31] ogbg-molbbbp: 0.917160 val loss: 0.326112
[Epoch 31] ogbg-molbbbp: 0.891387 test loss: 0.327711
[Epoch 32; Iter     5/   55] train: loss: 0.3876443
[Epoch 32; Iter    35/   55] train: loss: 0.1678513
[Epoch 32] ogbg-molbbbp: 0.906420 val loss: 0.396029
[Epoch 32] ogbg-molbbbp: 0.881981 test loss: 0.467517
[Epoch 33; Iter    10/   55] train: loss: 0.4275461
[Epoch 33; Iter    40/   55] train: loss: 0.2353926
[Epoch 33] ogbg-molbbbp: 0.916173 val loss: 0.327620
[Epoch 33] ogbg-molbbbp: 0.905497 test loss: 0.299820
[Epoch 34; Iter    15/   55] train: loss: 0.1736886
[Epoch 34; Iter    45/   55] train: loss: 0.1961877
[Epoch 34] ogbg-molbbbp: 0.923457 val loss: 0.342824
[Epoch 34] ogbg-molbbbp: 0.910935 test loss: 0.354173
[Epoch 35; Iter    20/   55] train: loss: 0.2845412
[Epoch 35; Iter    50/   55] train: loss: 0.1472410
[Epoch 35] ogbg-molbbbp: 0.916420 val loss: 0.391899
[Epoch 35] ogbg-molbbbp: 0.903880 test loss: 0.450351
[Epoch 36; Iter    25/   55] train: loss: 0.1531207
[Epoch 36; Iter    55/   55] train: loss: 0.2113211
[Epoch 36] ogbg-molbbbp: 0.886173 val loss: 0.452482
[Epoch 36] ogbg-molbbbp: 0.872869 test loss: 0.515821
[Epoch 37; Iter    30/   55] train: loss: 0.2629656
[Epoch 37] ogbg-molbbbp: 0.894074 val loss: 0.382448
[Epoch 37] ogbg-molbbbp: 0.869342 test loss: 0.352383
[Epoch 38; Iter     5/   55] train: loss: 0.1025926
[Epoch 38; Iter    35/   55] train: loss: 0.1066126
[Epoch 38] ogbg-molbbbp: 0.938889 val loss: 0.288149
[Epoch 38] ogbg-molbbbp: 0.910200 test loss: 0.284483
[Epoch 39; Iter    10/   55] train: loss: 0.1386765
[Epoch 39; Iter    40/   55] train: loss: 0.1426243
[Epoch 39] ogbg-molbbbp: 0.906914 val loss: 0.386518
[Epoch 39] ogbg-molbbbp: 0.873457 test loss: 0.405359
[Epoch 40; Iter    15/   55] train: loss: 0.2083529
[Epoch 40; Iter    45/   55] train: loss: 0.0883498
[Epoch 40] ogbg-molbbbp: 0.916420 val loss: 0.367319
[Epoch 40] ogbg-molbbbp: 0.895209 test loss: 0.345514
[Epoch 41; Iter    20/   55] train: loss: 0.2896913
[Epoch 41; Iter    50/   55] train: loss: 0.1350943
[Epoch 41] ogbg-molbbbp: 0.904938 val loss: 0.370777
[Epoch 41] ogbg-molbbbp: 0.868754 test loss: 0.451773
[Epoch 42; Iter    25/   55] train: loss: 0.1025798
[Epoch 42; Iter    55/   55] train: loss: 0.0454281
[Epoch 42] ogbg-molbbbp: 0.926543 val loss: 0.358337
[Epoch 42] ogbg-molbbbp: 0.894621 test loss: 0.333332
[Epoch 43; Iter    30/   55] train: loss: 0.4123664
[Epoch 43] ogbg-molbbbp: 0.922593 val loss: 0.388928
[Epoch 43] ogbg-molbbbp: 0.911523 test loss: 0.313162
[Epoch 44; Iter     5/   55] train: loss: 0.3565463
[Epoch 44; Iter    35/   55] train: loss: 0.1664030
[Epoch 44] ogbg-molbbbp: 0.941358 val loss: 0.331689
[Epoch 44] ogbg-molbbbp: 0.910935 test loss: 0.396817
[Epoch 45; Iter    10/   55] train: loss: 0.1382796
[Epoch 45; Iter    40/   55] train: loss: 0.1286308
[Epoch 45] ogbg-molbbbp: 0.915556 val loss: 0.415763
[Epoch 45] ogbg-molbbbp: 0.892122 test loss: 0.412126
[Epoch 46; Iter    15/   55] train: loss: 0.1841750
[Epoch 46; Iter    45/   55] train: loss: 0.2813013
[Epoch 46] ogbg-molbbbp: 0.942593 val loss: 0.304471
[Epoch 46] ogbg-molbbbp: 0.898001 test loss: 0.339671
[Epoch 47; Iter    20/   55] train: loss: 0.0999867
[Epoch 47; Iter    50/   55] train: loss: 0.3584424
[Epoch 47] ogbg-molbbbp: 0.910494 val loss: 0.385172
[Epoch 47] ogbg-molbbbp: 0.838918 test loss: 0.482501
[Epoch 48; Iter    25/   55] train: loss: 0.3736632
[Epoch 48; Iter    55/   55] train: loss: 0.0947684
[Epoch 48] ogbg-molbbbp: 0.911728 val loss: 0.356861
[Epoch 48] ogbg-molbbbp: 0.864198 test loss: 0.370530
[Epoch 49; Iter    30/   55] train: loss: 0.1471579
[Epoch 49] ogbg-molbbbp: 0.929877 val loss: 0.306424
[Epoch 49] ogbg-molbbbp: 0.892563 test loss: 0.346980
[Epoch 50; Iter     5/   55] train: loss: 0.2288711
[Epoch 50; Iter    35/   55] train: loss: 0.1643940
[Epoch 50] ogbg-molbbbp: 0.923704 val loss: 0.349978
[Epoch 50] ogbg-molbbbp: 0.880218 test loss: 0.365747
[Epoch 51; Iter    10/   55] train: loss: 0.2240843
[Epoch 51; Iter    40/   55] train: loss: 0.1737770
[Epoch 51] ogbg-molbbbp: 0.931728 val loss: 0.299994
[Epoch 51] ogbg-molbbbp: 0.883598 test loss: 0.365626
[Epoch 52; Iter    15/   55] train: loss: 0.1448449
[Epoch 52; Iter    45/   55] train: loss: 0.3151141
[Epoch 52] ogbg-molbbbp: 0.941728 val loss: 0.298596
[Epoch 52] ogbg-molbbbp: 0.899765 test loss: 0.333220
[Epoch 53; Iter    20/   55] train: loss: 0.2080117
[Epoch 53; Iter    50/   55] train: loss: 0.2636465
[Epoch 53] ogbg-molbbbp: 0.920000 val loss: 0.382646
[Epoch 53] ogbg-molbbbp: 0.885215 test loss: 0.423536
[Epoch 54; Iter    25/   55] train: loss: 0.1461682
[Epoch 54; Iter    55/   55] train: loss: 0.2073579
[Epoch 54] ogbg-molbbbp: 0.899630 val loss: 0.450334
[Epoch 54] ogbg-molbbbp: 0.864491 test loss: 0.549332
[Epoch 55; Iter    30/   55] train: loss: 0.2230648
[Epoch 55] ogbg-molbbbp: 0.922346 val loss: 0.327442
[Epoch 55] ogbg-molbbbp: 0.862581 test loss: 0.383686
[Epoch 56; Iter     5/   55] train: loss: 0.0910792
[Epoch 56; Iter    35/   55] train: loss: 0.1958444
[Epoch 56] ogbg-molbbbp: 0.914691 val loss: 0.381118
[Epoch 56] ogbg-molbbbp: 0.896972 test loss: 0.378664
[Epoch 57; Iter    10/   55] train: loss: 0.3119955
[Epoch 57; Iter    40/   55] train: loss: 0.1785656
[Epoch 57] ogbg-molbbbp: 0.928765 val loss: 0.364989
[Epoch 57] ogbg-molbbbp: 0.897119 test loss: 0.431352
[Epoch 58; Iter    15/   55] train: loss: 0.1435078
[Epoch 58; Iter    45/   55] train: loss: 0.1430651
[Epoch 58] ogbg-molbbbp: 0.944815 val loss: 0.314344
[Epoch 58] ogbg-molbbbp: 0.899471 test loss: 0.383678
[Epoch 59; Iter    20/   55] train: loss: 0.0911278
[Epoch 59; Iter    50/   55] train: loss: 0.1780407
[Epoch 59] ogbg-molbbbp: 0.932963 val loss: 0.376020
[Epoch 59] ogbg-molbbbp: 0.892563 test loss: 0.390149
[Epoch 60; Iter    25/   55] train: loss: 0.0965397
[Epoch 60; Iter    55/   55] train: loss: 0.0142950
[Epoch 60] ogbg-molbbbp: 0.927654 val loss: 0.357725
[Epoch 60] ogbg-molbbbp: 0.888154 test loss: 0.445072
[Epoch 61; Iter    30/   55] train: loss: 0.3423190
[Epoch 61] ogbg-molbbbp: 0.917160 val loss: 0.398029
[Epoch 61] ogbg-molbbbp: 0.878013 test loss: 0.431428
[Epoch 62; Iter     5/   55] train: loss: 0.1364735
[Epoch 62; Iter    35/   55] train: loss: 0.0853605
[Epoch 62] ogbg-molbbbp: 0.898889 val loss: 0.480481
[Epoch 62] ogbg-molbbbp: 0.865373 test loss: 0.630263
[Epoch 63; Iter    10/   55] train: loss: 0.0214959
[Epoch 63; Iter    40/   55] train: loss: 0.1267578
[Epoch 63] ogbg-molbbbp: 0.933333 val loss: 0.374569
[Epoch 63] ogbg-molbbbp: 0.900353 test loss: 0.446064
[Epoch 64; Iter    15/   55] train: loss: 0.1150923
[Epoch 64; Iter    45/   55] train: loss: 0.0187612
[Epoch 64] ogbg-molbbbp: 0.936790 val loss: 0.387187
[Epoch 64] ogbg-molbbbp: 0.899324 test loss: 0.475498
[Epoch 65; Iter    20/   55] train: loss: 0.0589300
[Epoch 65; Iter    50/   55] train: loss: 0.0256824
[Epoch 65] ogbg-molbbbp: 0.928272 val loss: 0.404709
[Epoch 65] ogbg-molbbbp: 0.896531 test loss: 0.443301
[Epoch 66; Iter    25/   55] train: loss: 0.0072615
[Epoch 66; Iter    55/   55] train: loss: 0.0206335
[Epoch 66] ogbg-molbbbp: 0.923951 val loss: 0.417249
[Epoch 66] ogbg-molbbbp: 0.883010 test loss: 0.538907
[Epoch 67; Iter    30/   55] train: loss: 0.0719583
[Epoch 67] ogbg-molbbbp: 0.922963 val loss: 0.442579
[Epoch 67] ogbg-molbbbp: 0.878013 test loss: 0.589947
[Epoch 68; Iter     5/   55] train: loss: 0.0273598
[Epoch 68; Iter    35/   55] train: loss: 0.0279366
[Epoch 68] ogbg-molbbbp: 0.931481 val loss: 0.457676
[Epoch 68] ogbg-molbbbp: 0.906966 test loss: 0.526292
[Epoch 69; Iter    10/   55] train: loss: 0.0659096
[Epoch 30] ogbg-molbbbp: 0.883571 val loss: 0.363885
[Epoch 30] ogbg-molbbbp: 0.891797 test loss: 0.346149
[Epoch 31; Iter    30/   48] train: loss: 0.2816178
[Epoch 31] ogbg-molbbbp: 0.882380 val loss: 0.328678
[Epoch 31] ogbg-molbbbp: 0.895372 test loss: 0.338848
[Epoch 32; Iter    12/   48] train: loss: 0.1921604
[Epoch 32; Iter    42/   48] train: loss: 0.3438645
[Epoch 32] ogbg-molbbbp: 0.891283 val loss: 0.374420
[Epoch 32] ogbg-molbbbp: 0.876795 test loss: 0.474097
[Epoch 33; Iter    24/   48] train: loss: 0.2949508
[Epoch 33] ogbg-molbbbp: 0.887030 val loss: 0.318576
[Epoch 33] ogbg-molbbbp: 0.907692 test loss: 0.333303
[Epoch 34; Iter     6/   48] train: loss: 0.1266002
[Epoch 34; Iter    36/   48] train: loss: 0.4644171
[Epoch 34] ogbg-molbbbp: 0.911870 val loss: 0.364825
[Epoch 34] ogbg-molbbbp: 0.891350 test loss: 0.446064
[Epoch 35; Iter    18/   48] train: loss: 0.2910630
[Epoch 35; Iter    48/   48] train: loss: 0.4618143
[Epoch 35] ogbg-molbbbp: 0.892644 val loss: 0.344813
[Epoch 35] ogbg-molbbbp: 0.900543 test loss: 0.328627
[Epoch 36; Iter    30/   48] train: loss: 0.4267025
[Epoch 36] ogbg-molbbbp: 0.899620 val loss: 0.324634
[Epoch 36] ogbg-molbbbp: 0.921864 test loss: 0.293980
[Epoch 37; Iter    12/   48] train: loss: 0.2141408
[Epoch 37; Iter    42/   48] train: loss: 0.2755123
[Epoch 37] ogbg-molbbbp: 0.907616 val loss: 0.315775
[Epoch 37] ogbg-molbbbp: 0.914267 test loss: 0.293336
[Epoch 38; Iter    24/   48] train: loss: 0.1458176
[Epoch 38] ogbg-molbbbp: 0.907163 val loss: 0.291497
[Epoch 38] ogbg-molbbbp: 0.886754 test loss: 0.339757
[Epoch 39; Iter     6/   48] train: loss: 0.1356353
[Epoch 39; Iter    36/   48] train: loss: 0.3293415
[Epoch 39] ogbg-molbbbp: 0.923949 val loss: 0.297641
[Epoch 39] ogbg-molbbbp: 0.906033 test loss: 0.301857
[Epoch 40; Iter    18/   48] train: loss: 0.1307230
[Epoch 40; Iter    48/   48] train: loss: 0.1916344
[Epoch 40] ogbg-molbbbp: 0.904327 val loss: 0.341879
[Epoch 40] ogbg-molbbbp: 0.916821 test loss: 0.292314
[Epoch 41; Iter    30/   48] train: loss: 0.1574475
[Epoch 41] ogbg-molbbbp: 0.910679 val loss: 0.309865
[Epoch 41] ogbg-molbbbp: 0.889690 test loss: 0.374658
[Epoch 42; Iter    12/   48] train: loss: 0.1997226
[Epoch 42; Iter    42/   48] train: loss: 0.2764600
[Epoch 42] ogbg-molbbbp: 0.912494 val loss: 0.348701
[Epoch 42] ogbg-molbbbp: 0.876987 test loss: 0.409426
[Epoch 43; Iter    24/   48] train: loss: 0.0927571
[Epoch 43] ogbg-molbbbp: 0.911643 val loss: 0.348900
[Epoch 43] ogbg-molbbbp: 0.898181 test loss: 0.372460
[Epoch 44; Iter     6/   48] train: loss: 0.4443228
[Epoch 44; Iter    36/   48] train: loss: 0.1830291
[Epoch 44] ogbg-molbbbp: 0.930528 val loss: 0.294823
[Epoch 44] ogbg-molbbbp: 0.911395 test loss: 0.306168
[Epoch 45; Iter    18/   48] train: loss: 0.0952800
[Epoch 45; Iter    48/   48] train: loss: 0.2205313
[Epoch 45] ogbg-molbbbp: 0.897635 val loss: 0.347629
[Epoch 45] ogbg-molbbbp: 0.877561 test loss: 0.401505
[Epoch 46; Iter    30/   48] train: loss: 0.2528958
[Epoch 46] ogbg-molbbbp: 0.922532 val loss: 0.363825
[Epoch 46] ogbg-molbbbp: 0.899330 test loss: 0.395908
[Epoch 47; Iter    12/   48] train: loss: 0.2228545
[Epoch 47; Iter    42/   48] train: loss: 0.1302755
[Epoch 47] ogbg-molbbbp: 0.918448 val loss: 0.321000
[Epoch 47] ogbg-molbbbp: 0.902777 test loss: 0.342369
[Epoch 48; Iter    24/   48] train: loss: 0.1677297
[Epoch 48] ogbg-molbbbp: 0.897862 val loss: 0.338188
[Epoch 48] ogbg-molbbbp: 0.877561 test loss: 0.376742
[Epoch 49; Iter     6/   48] train: loss: 0.0902463
[Epoch 49; Iter    36/   48] train: loss: 0.1968520
[Epoch 49] ogbg-molbbbp: 0.898599 val loss: 0.360664
[Epoch 49] ogbg-molbbbp: 0.902458 test loss: 0.401978
[Epoch 50; Iter    18/   48] train: loss: 0.3873513
[Epoch 50; Iter    48/   48] train: loss: 0.1669673
[Epoch 50] ogbg-molbbbp: 0.917484 val loss: 0.421854
[Epoch 50] ogbg-molbbbp: 0.863390 test loss: 0.518779
[Epoch 51; Iter    30/   48] train: loss: 0.1186873
[Epoch 51] ogbg-molbbbp: 0.898769 val loss: 0.362435
[Epoch 51] ogbg-molbbbp: 0.862113 test loss: 0.451761
[Epoch 52; Iter    12/   48] train: loss: 0.0517081
[Epoch 52; Iter    42/   48] train: loss: 0.1220380
[Epoch 52] ogbg-molbbbp: 0.925991 val loss: 0.281727
[Epoch 52] ogbg-molbbbp: 0.859623 test loss: 0.392681
[Epoch 53; Iter    24/   48] train: loss: 0.1157472
[Epoch 53] ogbg-molbbbp: 0.910282 val loss: 0.324354
[Epoch 53] ogbg-molbbbp: 0.884264 test loss: 0.403452
[Epoch 54; Iter     6/   48] train: loss: 0.3292204
[Epoch 54; Iter    36/   48] train: loss: 0.0735846
[Epoch 54] ogbg-molbbbp: 0.881472 val loss: 1.047108
[Epoch 54] ogbg-molbbbp: 0.841941 test loss: 7.051034
[Epoch 55; Iter    18/   48] train: loss: 0.1137118
[Epoch 55; Iter    48/   48] train: loss: 0.3761039
[Epoch 55] ogbg-molbbbp: 0.906766 val loss: 0.419633
[Epoch 55] ogbg-molbbbp: 0.862496 test loss: 0.577682
[Epoch 56; Iter    30/   48] train: loss: 0.0929242
[Epoch 56] ogbg-molbbbp: 0.904214 val loss: 0.329127
[Epoch 56] ogbg-molbbbp: 0.851452 test loss: 0.409344
[Epoch 57; Iter    12/   48] train: loss: 0.1233251
[Epoch 57; Iter    42/   48] train: loss: 0.2412101
[Epoch 57] ogbg-molbbbp: 0.918051 val loss: 0.316186
[Epoch 57] ogbg-molbbbp: 0.890584 test loss: 0.354517
[Epoch 58; Iter    24/   48] train: loss: 0.2051176
[Epoch 58] ogbg-molbbbp: 0.925594 val loss: 0.397867
[Epoch 58] ogbg-molbbbp: 0.883626 test loss: 0.458015
[Epoch 59; Iter     6/   48] train: loss: 0.0548753
[Epoch 59; Iter    36/   48] train: loss: 0.0437275
[Epoch 59] ogbg-molbbbp: 0.913174 val loss: 0.373114
[Epoch 59] ogbg-molbbbp: 0.887456 test loss: 0.451138
[Epoch 60; Iter    18/   48] train: loss: 0.0543236
[Epoch 60; Iter    48/   48] train: loss: 0.0344259
[Epoch 60] ogbg-molbbbp: 0.910339 val loss: 0.353041
[Epoch 60] ogbg-molbbbp: 0.891031 test loss: 0.430143
[Epoch 61; Iter    30/   48] train: loss: 0.0917377
[Epoch 61] ogbg-molbbbp: 0.898032 val loss: 0.404352
[Epoch 61] ogbg-molbbbp: 0.870412 test loss: 0.515162
[Epoch 62; Iter    12/   48] train: loss: 0.2762948
[Epoch 62; Iter    42/   48] train: loss: 0.1893963
[Epoch 62] ogbg-molbbbp: 0.914762 val loss: 0.372653
[Epoch 62] ogbg-molbbbp: 0.894733 test loss: 0.467474
[Epoch 63; Iter    24/   48] train: loss: 0.2656993
[Epoch 63] ogbg-molbbbp: 0.907673 val loss: 0.396521
[Epoch 63] ogbg-molbbbp: 0.854580 test loss: 0.483048
[Epoch 64; Iter     6/   48] train: loss: 0.0631684
[Epoch 64; Iter    36/   48] train: loss: 0.0527236
[Epoch 64] ogbg-molbbbp: 0.923893 val loss: 0.351322
[Epoch 64] ogbg-molbbbp: 0.881647 test loss: 0.447865
[Epoch 65; Iter    18/   48] train: loss: 0.0195229
[Epoch 65; Iter    48/   48] train: loss: 0.2337367
[Epoch 65] ogbg-molbbbp: 0.918675 val loss: 0.369455
[Epoch 65] ogbg-molbbbp: 0.884520 test loss: 0.444303
[Epoch 66; Iter    30/   48] train: loss: 0.2246838
[Epoch 66] ogbg-molbbbp: 0.906993 val loss: 0.364716
[Epoch 66] ogbg-molbbbp: 0.871497 test loss: 0.408258
[Epoch 67; Iter    12/   48] train: loss: 0.1639682
[Epoch 67; Iter    42/   48] train: loss: 0.0651167
[Epoch 67] ogbg-molbbbp: 0.920774 val loss: 0.362586
[Epoch 67] ogbg-molbbbp: 0.886882 test loss: 0.460807
[Epoch 68; Iter    24/   48] train: loss: 0.2290525
[Epoch 68] ogbg-molbbbp: 0.920660 val loss: 0.355996
[Epoch 68] ogbg-molbbbp: 0.888924 test loss: 0.410277
[Epoch 69; Iter     6/   48] train: loss: 0.1006719
[Epoch 69; Iter    36/   48] train: loss: 0.3039485
[Epoch 69] ogbg-molbbbp: 0.901265 val loss: 0.378676
[Epoch 69] ogbg-molbbbp: 0.894989 test loss: 0.406378
[Epoch 70; Iter    18/   48] train: loss: 0.2197156
[Epoch 70; Iter    48/   48] train: loss: 0.2131242
[Epoch 70] ogbg-molbbbp: 0.932626 val loss: 0.359166
[Epoch 70] ogbg-molbbbp: 0.879923 test loss: 0.454480
[Epoch 71; Iter    30/   48] train: loss: 0.0428341
[Epoch 71] ogbg-molbbbp: 0.925594 val loss: 0.350667
[Epoch 71] ogbg-molbbbp: 0.888094 test loss: 0.472593
[Epoch 72; Iter    12/   48] train: loss: 0.1280452
[Epoch 72; Iter    42/   48] train: loss: 0.0372974
[Epoch 72] ogbg-molbbbp: 0.914989 val loss: 0.403858
[Epoch 72] ogbg-molbbbp: 0.871114 test loss: 0.576038
[Epoch 73; Iter    24/   48] train: loss: 0.0226817
[Epoch 73] ogbg-molbbbp: 0.928373 val loss: 0.335240
[Epoch 32] ogbg-molbbbp: 0.889757 test loss: 0.352382
[Epoch 33; Iter     8/   41] train: loss: 0.4670195
[Epoch 33; Iter    38/   41] train: loss: 0.1791112
[Epoch 33] ogbg-molbbbp: 0.885817 val loss: 0.330770
[Epoch 33] ogbg-molbbbp: 0.871928 test loss: 0.366687
[Epoch 34; Iter    27/   41] train: loss: 0.3061292
[Epoch 34] ogbg-molbbbp: 0.914661 val loss: 0.303413
[Epoch 34] ogbg-molbbbp: 0.891526 test loss: 0.349656
[Epoch 35; Iter    16/   41] train: loss: 0.2727273
[Epoch 35] ogbg-molbbbp: 0.889776 val loss: 0.392171
[Epoch 35] ogbg-molbbbp: 0.867555 test loss: 0.512239
[Epoch 36; Iter     5/   41] train: loss: 0.2460061
[Epoch 36; Iter    35/   41] train: loss: 0.3271604
[Epoch 36] ogbg-molbbbp: 0.902919 val loss: 0.327735
[Epoch 36] ogbg-molbbbp: 0.907385 test loss: 0.328186
[Epoch 37; Iter    24/   41] train: loss: 0.2756431
[Epoch 37] ogbg-molbbbp: 0.889776 val loss: 0.389363
[Epoch 37] ogbg-molbbbp: 0.877237 test loss: 0.408420
[Epoch 38; Iter    13/   41] train: loss: 0.1088428
[Epoch 38] ogbg-molbbbp: 0.901587 val loss: 0.370832
[Epoch 38] ogbg-molbbbp: 0.895333 test loss: 0.381192
[Epoch 39; Iter     2/   41] train: loss: 0.1693718
[Epoch 39; Iter    32/   41] train: loss: 0.1688125
[Epoch 39] ogbg-molbbbp: 0.887046 val loss: 0.430318
[Epoch 39] ogbg-molbbbp: 0.864116 test loss: 0.426656
[Epoch 40; Iter    21/   41] train: loss: 0.1679088
[Epoch 40] ogbg-molbbbp: 0.904864 val loss: 0.330261
[Epoch 40] ogbg-molbbbp: 0.888121 test loss: 0.372825
[Epoch 41; Iter    10/   41] train: loss: 0.2151508
[Epoch 41; Iter    40/   41] train: loss: 0.2664897
[Epoch 41] ogbg-molbbbp: 0.910189 val loss: 0.321518
[Epoch 41] ogbg-molbbbp: 0.889590 test loss: 0.368927
[Epoch 42; Iter    29/   41] train: loss: 0.3636677
[Epoch 42] ogbg-molbbbp: 0.918894 val loss: 0.307232
[Epoch 42] ogbg-molbbbp: 0.908287 test loss: 0.335989
[Epoch 43; Iter    18/   41] train: loss: 0.1552503
[Epoch 43] ogbg-molbbbp: 0.930841 val loss: 0.297986
[Epoch 43] ogbg-molbbbp: 0.924279 test loss: 0.309817
[Epoch 44; Iter     7/   41] train: loss: 0.1822563
[Epoch 44; Iter    37/   41] train: loss: 0.1777137
[Epoch 44] ogbg-molbbbp: 0.890084 val loss: 0.388989
[Epoch 44] ogbg-molbbbp: 0.885650 test loss: 0.399719
[Epoch 45; Iter    26/   41] train: loss: 0.3863930
[Epoch 45] ogbg-molbbbp: 0.876600 val loss: 0.336517
[Epoch 45] ogbg-molbbbp: 0.859776 test loss: 0.390176
[Epoch 46; Iter    15/   41] train: loss: 0.1197117
[Epoch 46] ogbg-molbbbp: 0.908346 val loss: 0.318888
[Epoch 46] ogbg-molbbbp: 0.859241 test loss: 0.389735
[Epoch 47; Iter     4/   41] train: loss: 0.3396770
[Epoch 47; Iter    34/   41] train: loss: 0.2564591
[Epoch 47] ogbg-molbbbp: 0.922171 val loss: 0.280926
[Epoch 47] ogbg-molbbbp: 0.905415 test loss: 0.330905
[Epoch 48; Iter    23/   41] train: loss: 0.1672232
[Epoch 48] ogbg-molbbbp: 0.899539 val loss: 0.349843
[Epoch 48] ogbg-molbbbp: 0.861044 test loss: 0.411587
[Epoch 49; Iter    12/   41] train: loss: 0.3408178
[Epoch 49] ogbg-molbbbp: 0.923161 val loss: 0.305044
[Epoch 49] ogbg-molbbbp: 0.911625 test loss: 0.349321
[Epoch 50; Iter     1/   41] train: loss: 0.0810651
[Epoch 50; Iter    31/   41] train: loss: 0.2985665
[Epoch 50] ogbg-molbbbp: 0.905308 val loss: 0.300511
[Epoch 50] ogbg-molbbbp: 0.889490 test loss: 0.351638
[Epoch 51; Iter    20/   41] train: loss: 0.1530242
[Epoch 51] ogbg-molbbbp: 0.915958 val loss: 0.316910
[Epoch 51] ogbg-molbbbp: 0.906384 test loss: 0.365131
[Epoch 52; Iter     9/   41] train: loss: 0.1406615
[Epoch 52; Iter    39/   41] train: loss: 0.1587483
[Epoch 52] ogbg-molbbbp: 0.900495 val loss: 0.338193
[Epoch 52] ogbg-molbbbp: 0.885917 test loss: 0.383637
[Epoch 53; Iter    28/   41] train: loss: 0.1443883
[Epoch 53] ogbg-molbbbp: 0.885714 val loss: 0.361380
[Epoch 53] ogbg-molbbbp: 0.878405 test loss: 0.389262
[Epoch 54; Iter    17/   41] train: loss: 0.1858725
[Epoch 54] ogbg-molbbbp: 0.916163 val loss: 0.376058
[Epoch 54] ogbg-molbbbp: 0.887654 test loss: 0.517307
[Epoch 55; Iter     6/   41] train: loss: 0.0515315
[Epoch 55; Iter    36/   41] train: loss: 0.1500466
[Epoch 55] ogbg-molbbbp: 0.914661 val loss: 0.320954
[Epoch 55] ogbg-molbbbp: 0.901275 test loss: 0.385580
[Epoch 56; Iter    25/   41] train: loss: 0.1138639
[Epoch 56] ogbg-molbbbp: 0.905513 val loss: 0.433740
[Epoch 56] ogbg-molbbbp: 0.889957 test loss: 0.424141
[Epoch 57; Iter    14/   41] train: loss: 0.0231253
[Epoch 57] ogbg-molbbbp: 0.916129 val loss: 0.394526
[Epoch 57] ogbg-molbbbp: 0.894598 test loss: 0.445979
[Epoch 58; Iter     3/   41] train: loss: 0.0347653
[Epoch 58; Iter    33/   41] train: loss: 0.1195515
[Epoch 58] ogbg-molbbbp: 0.910940 val loss: 0.385880
[Epoch 58] ogbg-molbbbp: 0.892127 test loss: 0.498859
[Epoch 59; Iter    22/   41] train: loss: 0.1532895
[Epoch 59] ogbg-molbbbp: 0.911009 val loss: 0.359263
[Epoch 59] ogbg-molbbbp: 0.879874 test loss: 0.447035
[Epoch 60; Iter    11/   41] train: loss: 0.0994915
[Epoch 60; Iter    41/   41] train: loss: 0.1406203
[Epoch 60] ogbg-molbbbp: 0.919406 val loss: 0.464243
[Epoch 60] ogbg-molbbbp: 0.885851 test loss: 0.547483
[Epoch 61; Iter    30/   41] train: loss: 0.1920360
[Epoch 61] ogbg-molbbbp: 0.899915 val loss: 0.452224
[Epoch 61] ogbg-molbbbp: 0.864416 test loss: 0.500795
[Epoch 62; Iter    19/   41] train: loss: 0.0373404
[Epoch 62] ogbg-molbbbp: 0.905718 val loss: 0.386724
[Epoch 62] ogbg-molbbbp: 0.887420 test loss: 0.438549
[Epoch 63; Iter     8/   41] train: loss: 0.1214675
[Epoch 63; Iter    38/   41] train: loss: 0.1081037
[Epoch 63] ogbg-molbbbp: 0.923024 val loss: 0.349247
[Epoch 63] ogbg-molbbbp: 0.896601 test loss: 0.418924
[Epoch 64; Iter    27/   41] train: loss: 0.1552316
[Epoch 64] ogbg-molbbbp: 0.923844 val loss: 0.361439
[Epoch 64] ogbg-molbbbp: 0.909756 test loss: 0.424905
[Epoch 65; Iter    16/   41] train: loss: 0.2485716
[Epoch 65] ogbg-molbbbp: 0.919850 val loss: 0.383836
[Epoch 65] ogbg-molbbbp: 0.919371 test loss: 0.438682
[Epoch 66; Iter     5/   41] train: loss: 0.0701309
[Epoch 66; Iter    35/   41] train: loss: 0.1062831
[Epoch 66] ogbg-molbbbp: 0.919850 val loss: 0.381621
[Epoch 66] ogbg-molbbbp: 0.883280 test loss: 0.509081
[Epoch 67; Iter    24/   41] train: loss: 0.0230461
[Epoch 67] ogbg-molbbbp: 0.922273 val loss: 0.350263
[Epoch 67] ogbg-molbbbp: 0.887854 test loss: 0.471417
[Epoch 68; Iter    13/   41] train: loss: 0.0209085
[Epoch 68] ogbg-molbbbp: 0.911521 val loss: 0.564515
[Epoch 68] ogbg-molbbbp: 0.890725 test loss: 0.609040
[Epoch 69; Iter     2/   41] train: loss: 0.2421646
[Epoch 69; Iter    32/   41] train: loss: 0.1949338
[Epoch 69] ogbg-molbbbp: 0.900700 val loss: 0.435827
[Epoch 69] ogbg-molbbbp: 0.861712 test loss: 0.543949
[Epoch 70; Iter    21/   41] train: loss: 0.0622589
[Epoch 70] ogbg-molbbbp: 0.920259 val loss: 0.432898
[Epoch 70] ogbg-molbbbp: 0.889623 test loss: 0.553390
[Epoch 71; Iter    10/   41] train: loss: 0.0947067
[Epoch 71; Iter    40/   41] train: loss: 0.0653011
[Epoch 71] ogbg-molbbbp: 0.925004 val loss: 0.436713
[Epoch 71] ogbg-molbbbp: 0.886752 test loss: 0.555402
[Epoch 72; Iter    29/   41] train: loss: 0.0928556
[Epoch 72] ogbg-molbbbp: 0.921215 val loss: 0.368799
[Epoch 72] ogbg-molbbbp: 0.887720 test loss: 0.471679
[Epoch 73; Iter    18/   41] train: loss: 0.0125941
[Epoch 73] ogbg-molbbbp: 0.917358 val loss: 0.376202
[Epoch 73] ogbg-molbbbp: 0.875935 test loss: 0.537429
[Epoch 74; Iter     7/   41] train: loss: 0.0319396
[Epoch 74; Iter    37/   41] train: loss: 0.1628337
[Epoch 74] ogbg-molbbbp: 0.921454 val loss: 0.434641
[Epoch 74] ogbg-molbbbp: 0.887787 test loss: 0.587945
[Epoch 75; Iter    26/   41] train: loss: 0.0561458
[Epoch 75] ogbg-molbbbp: 0.917119 val loss: 0.425507
[Epoch 75] ogbg-molbbbp: 0.884949 test loss: 0.558468
[Epoch 76; Iter    15/   41] train: loss: 0.0202226
[Epoch 76] ogbg-molbbbp: 0.924117 val loss: 0.392891
[Epoch 76] ogbg-molbbbp: 0.889123 test loss: 0.511849
[Epoch 77; Iter     4/   41] train: loss: 0.0092950
[Epoch 77; Iter    34/   41] train: loss: 0.0065676
[Epoch 77] ogbg-molbbbp: 0.914286 val loss: 0.431583
[Epoch 77] ogbg-molbbbp: 0.889056 test loss: 0.565677
[Epoch 78; Iter    23/   41] train: loss: 0.0045387
[Epoch 28] ogbg-molbbbp: 0.885362 test loss: 0.336424
[Epoch 29; Iter    20/   55] train: loss: 0.3703319
[Epoch 29; Iter    50/   55] train: loss: 0.6176960
[Epoch 29] ogbg-molbbbp: 0.928272 val loss: 0.313805
[Epoch 29] ogbg-molbbbp: 0.924750 test loss: 0.278586
[Epoch 30; Iter    25/   55] train: loss: 0.3559329
[Epoch 30; Iter    55/   55] train: loss: 0.2347026
[Epoch 30] ogbg-molbbbp: 0.926296 val loss: 0.380356
[Epoch 30] ogbg-molbbbp: 0.922986 test loss: 0.273356
[Epoch 31; Iter    30/   55] train: loss: 0.1770625
[Epoch 31] ogbg-molbbbp: 0.914815 val loss: 0.350408
[Epoch 31] ogbg-molbbbp: 0.893151 test loss: 0.594331
[Epoch 32; Iter     5/   55] train: loss: 0.2186016
[Epoch 32; Iter    35/   55] train: loss: 0.3676155
[Epoch 32] ogbg-molbbbp: 0.911605 val loss: 0.359882
[Epoch 32] ogbg-molbbbp: 0.888595 test loss: 0.342929
[Epoch 33; Iter    10/   55] train: loss: 0.2249682
[Epoch 33; Iter    40/   55] train: loss: 0.1957714
[Epoch 33] ogbg-molbbbp: 0.919136 val loss: 0.322864
[Epoch 33] ogbg-molbbbp: 0.895650 test loss: 0.314289
[Epoch 34; Iter    15/   55] train: loss: 0.2553319
[Epoch 34; Iter    45/   55] train: loss: 0.2211370
[Epoch 34] ogbg-molbbbp: 0.937037 val loss: 0.281593
[Epoch 34] ogbg-molbbbp: 0.901969 test loss: 0.331052
[Epoch 35; Iter    20/   55] train: loss: 0.3001936
[Epoch 35; Iter    50/   55] train: loss: 0.3648027
[Epoch 35] ogbg-molbbbp: 0.911605 val loss: 0.349459
[Epoch 35] ogbg-molbbbp: 0.881099 test loss: 0.381007
[Epoch 36; Iter    25/   55] train: loss: 0.0635293
[Epoch 36; Iter    55/   55] train: loss: 0.2286767
[Epoch 36] ogbg-molbbbp: 0.891852 val loss: 0.403144
[Epoch 36] ogbg-molbbbp: 0.893298 test loss: 0.406064
[Epoch 37; Iter    30/   55] train: loss: 0.1508127
[Epoch 37] ogbg-molbbbp: 0.917778 val loss: 0.331581
[Epoch 37] ogbg-molbbbp: 0.890947 test loss: 0.374767
[Epoch 38; Iter     5/   55] train: loss: 0.3653728
[Epoch 38; Iter    35/   55] train: loss: 0.3392276
[Epoch 38] ogbg-molbbbp: 0.942716 val loss: 0.289919
[Epoch 38] ogbg-molbbbp: 0.914609 test loss: 0.327838
[Epoch 39; Iter    10/   55] train: loss: 0.1921891
[Epoch 39; Iter    40/   55] train: loss: 0.1024431
[Epoch 39] ogbg-molbbbp: 0.940864 val loss: 0.312543
[Epoch 39] ogbg-molbbbp: 0.878748 test loss: 0.381710
[Epoch 40; Iter    15/   55] train: loss: 0.2366535
[Epoch 40; Iter    45/   55] train: loss: 0.0926884
[Epoch 40] ogbg-molbbbp: 0.898889 val loss: 0.388804
[Epoch 40] ogbg-molbbbp: 0.891093 test loss: 0.354720
[Epoch 41; Iter    20/   55] train: loss: 0.1835779
[Epoch 41; Iter    50/   55] train: loss: 0.2130758
[Epoch 41] ogbg-molbbbp: 0.924691 val loss: 0.315769
[Epoch 41] ogbg-molbbbp: 0.915197 test loss: 0.321124
[Epoch 42; Iter    25/   55] train: loss: 0.0783388
[Epoch 42; Iter    55/   55] train: loss: 0.1077335
[Epoch 42] ogbg-molbbbp: 0.914691 val loss: 0.354042
[Epoch 42] ogbg-molbbbp: 0.902263 test loss: 0.330554
[Epoch 43; Iter    30/   55] train: loss: 0.1321523
[Epoch 43] ogbg-molbbbp: 0.896420 val loss: 0.434946
[Epoch 43] ogbg-molbbbp: 0.876984 test loss: 0.484341
[Epoch 44; Iter     5/   55] train: loss: 0.1812799
[Epoch 44; Iter    35/   55] train: loss: 0.1316448
[Epoch 44] ogbg-molbbbp: 0.920370 val loss: 0.341663
[Epoch 44] ogbg-molbbbp: 0.902263 test loss: 0.322623
[Epoch 45; Iter    10/   55] train: loss: 0.1220745
[Epoch 45; Iter    40/   55] train: loss: 0.0586770
[Epoch 45] ogbg-molbbbp: 0.924321 val loss: 0.368444
[Epoch 45] ogbg-molbbbp: 0.908583 test loss: 0.333533
[Epoch 46; Iter    15/   55] train: loss: 0.1721125
[Epoch 46; Iter    45/   55] train: loss: 0.2712317
[Epoch 46] ogbg-molbbbp: 0.926173 val loss: 0.335084
[Epoch 46] ogbg-molbbbp: 0.898589 test loss: 0.371570
[Epoch 47; Iter    20/   55] train: loss: 0.2061128
[Epoch 47; Iter    50/   55] train: loss: 0.0562366
[Epoch 47] ogbg-molbbbp: 0.936914 val loss: 0.355382
[Epoch 47] ogbg-molbbbp: 0.891093 test loss: 0.368316
[Epoch 48; Iter    25/   55] train: loss: 0.0809867
[Epoch 48; Iter    55/   55] train: loss: 0.0638069
[Epoch 48] ogbg-molbbbp: 0.910494 val loss: 0.425608
[Epoch 48] ogbg-molbbbp: 0.881687 test loss: 0.424073
[Epoch 49; Iter    30/   55] train: loss: 0.2272299
[Epoch 49] ogbg-molbbbp: 0.929383 val loss: 0.360301
[Epoch 49] ogbg-molbbbp: 0.892122 test loss: 0.375222
[Epoch 50; Iter     5/   55] train: loss: 0.0573773
[Epoch 50; Iter    35/   55] train: loss: 0.0692029
[Epoch 50] ogbg-molbbbp: 0.909136 val loss: 0.406672
[Epoch 50] ogbg-molbbbp: 0.918724 test loss: 0.319350
[Epoch 51; Iter    10/   55] train: loss: 0.0624605
[Epoch 51; Iter    40/   55] train: loss: 0.1878721
[Epoch 51] ogbg-molbbbp: 0.896790 val loss: 0.401158
[Epoch 51] ogbg-molbbbp: 0.889918 test loss: 0.428270
[Epoch 52; Iter    15/   55] train: loss: 0.0923582
[Epoch 52; Iter    45/   55] train: loss: 0.0785507
[Epoch 52] ogbg-molbbbp: 0.927778 val loss: 0.349965
[Epoch 52] ogbg-molbbbp: 0.893298 test loss: 0.352987
[Epoch 53; Iter    20/   55] train: loss: 0.1142766
[Epoch 53; Iter    50/   55] train: loss: 0.0927529
[Epoch 53] ogbg-molbbbp: 0.920617 val loss: 0.356053
[Epoch 53] ogbg-molbbbp: 0.903880 test loss: 0.373191
[Epoch 54; Iter    25/   55] train: loss: 0.0271090
[Epoch 54; Iter    55/   55] train: loss: 0.0339749
[Epoch 54] ogbg-molbbbp: 0.892222 val loss: 0.423858
[Epoch 54] ogbg-molbbbp: 0.861111 test loss: 0.450243
[Epoch 55; Iter    30/   55] train: loss: 0.1431700
[Epoch 55] ogbg-molbbbp: 0.917778 val loss: 0.423869
[Epoch 55] ogbg-molbbbp: 0.899030 test loss: 0.385819
[Epoch 56; Iter     5/   55] train: loss: 0.0814942
[Epoch 56; Iter    35/   55] train: loss: 0.1330072
[Epoch 56] ogbg-molbbbp: 0.934198 val loss: 0.343677
[Epoch 56] ogbg-molbbbp: 0.909759 test loss: 0.395460
[Epoch 57; Iter    10/   55] train: loss: 0.1106383
[Epoch 57; Iter    40/   55] train: loss: 0.1090889
[Epoch 57] ogbg-molbbbp: 0.894938 val loss: 0.425805
[Epoch 57] ogbg-molbbbp: 0.890947 test loss: 0.450737
[Epoch 58; Iter    15/   55] train: loss: 0.0314994
[Epoch 58; Iter    45/   55] train: loss: 0.0303361
[Epoch 58] ogbg-molbbbp: 0.927531 val loss: 0.459502
[Epoch 58] ogbg-molbbbp: 0.919753 test loss: 0.407823
[Epoch 59; Iter    20/   55] train: loss: 0.1163371
[Epoch 59; Iter    50/   55] train: loss: 0.1674237
[Epoch 59] ogbg-molbbbp: 0.914074 val loss: 0.370730
[Epoch 59] ogbg-molbbbp: 0.887566 test loss: 0.422085
[Epoch 60; Iter    25/   55] train: loss: 0.1033781
[Epoch 60; Iter    55/   55] train: loss: 0.7192375
[Epoch 60] ogbg-molbbbp: 0.918395 val loss: 0.449949
[Epoch 60] ogbg-molbbbp: 0.867872 test loss: 0.514322
[Epoch 61; Iter    30/   55] train: loss: 0.3404269
[Epoch 61] ogbg-molbbbp: 0.925802 val loss: 0.339181
[Epoch 61] ogbg-molbbbp: 0.906379 test loss: 0.422404
[Epoch 62; Iter     5/   55] train: loss: 0.1227873
[Epoch 62; Iter    35/   55] train: loss: 0.1006587
[Epoch 62] ogbg-molbbbp: 0.921235 val loss: 0.387228
[Epoch 62] ogbg-molbbbp: 0.876984 test loss: 0.457765
[Epoch 63; Iter    10/   55] train: loss: 0.0826864
[Epoch 63; Iter    40/   55] train: loss: 0.1583294
[Epoch 63] ogbg-molbbbp: 0.932469 val loss: 0.364727
[Epoch 63] ogbg-molbbbp: 0.891387 test loss: 0.503387
[Epoch 64; Iter    15/   55] train: loss: 0.0801514
[Epoch 64; Iter    45/   55] train: loss: 0.0217661
[Epoch 64] ogbg-molbbbp: 0.935309 val loss: 0.373165
[Epoch 64] ogbg-molbbbp: 0.889771 test loss: 0.533009
[Epoch 65; Iter    20/   55] train: loss: 0.0140564
[Epoch 65; Iter    50/   55] train: loss: 0.0120834
[Epoch 65] ogbg-molbbbp: 0.930988 val loss: 0.351074
[Epoch 65] ogbg-molbbbp: 0.883451 test loss: 0.489156
[Epoch 66; Iter    25/   55] train: loss: 0.1091974
[Epoch 66; Iter    55/   55] train: loss: 0.0257452
[Epoch 66] ogbg-molbbbp: 0.929012 val loss: 0.425524
[Epoch 66] ogbg-molbbbp: 0.898736 test loss: 0.552789
[Epoch 67; Iter    30/   55] train: loss: 0.0525916
[Epoch 67] ogbg-molbbbp: 0.936667 val loss: 0.379713
[Epoch 67] ogbg-molbbbp: 0.896531 test loss: 0.586325
[Epoch 68; Iter     5/   55] train: loss: 0.0122531
[Epoch 68; Iter    35/   55] train: loss: 0.0633059
[Epoch 68] ogbg-molbbbp: 0.939630 val loss: 0.379280
[Epoch 68] ogbg-molbbbp: 0.900206 test loss: 0.572770
[Epoch 69; Iter    10/   55] train: loss: 0.0098635
[Epoch 30] ogbg-molbbbp: 0.879544 val loss: 0.383696
[Epoch 30] ogbg-molbbbp: 0.887456 test loss: 0.357906
[Epoch 31; Iter    30/   48] train: loss: 0.2920443
[Epoch 31] ogbg-molbbbp: 0.888901 val loss: 0.426311
[Epoch 31] ogbg-molbbbp: 0.894031 test loss: 0.388074
[Epoch 32; Iter    12/   48] train: loss: 0.3323108
[Epoch 32; Iter    42/   48] train: loss: 0.2180041
[Epoch 32] ogbg-molbbbp: 0.874724 val loss: 0.337678
[Epoch 32] ogbg-molbbbp: 0.878264 test loss: 0.324617
[Epoch 33; Iter    24/   48] train: loss: 0.2096955
[Epoch 33] ogbg-molbbbp: 0.889922 val loss: 0.339451
[Epoch 33] ogbg-molbbbp: 0.857581 test loss: 0.416850
[Epoch 34; Iter     6/   48] train: loss: 0.1238172
[Epoch 34; Iter    36/   48] train: loss: 0.1980320
[Epoch 34] ogbg-molbbbp: 0.880508 val loss: 0.421375
[Epoch 34] ogbg-molbbbp: 0.895946 test loss: 0.659549
[Epoch 35; Iter    18/   48] train: loss: 0.2428224
[Epoch 35; Iter    48/   48] train: loss: 0.4025441
[Epoch 35] ogbg-molbbbp: 0.893949 val loss: 0.592303
[Epoch 35] ogbg-molbbbp: 0.901756 test loss: 0.587224
[Epoch 36; Iter    30/   48] train: loss: 0.1746197
[Epoch 36] ogbg-molbbbp: 0.897635 val loss: 0.346935
[Epoch 36] ogbg-molbbbp: 0.918928 test loss: 0.328574
[Epoch 37; Iter    12/   48] train: loss: 0.2281528
[Epoch 37; Iter    42/   48] train: loss: 0.2881972
[Epoch 37] ogbg-molbbbp: 0.913685 val loss: 0.344767
[Epoch 37] ogbg-molbbbp: 0.875646 test loss: 0.437358
[Epoch 38; Iter    24/   48] train: loss: 0.2370450
[Epoch 38] ogbg-molbbbp: 0.889185 val loss: 0.343929
[Epoch 38] ogbg-molbbbp: 0.870412 test loss: 0.384458
[Epoch 39; Iter     6/   48] train: loss: 0.2662009
[Epoch 39; Iter    36/   48] train: loss: 0.2816537
[Epoch 39] ogbg-molbbbp: 0.883911 val loss: 0.366706
[Epoch 39] ogbg-molbbbp: 0.875327 test loss: 0.371097
[Epoch 40; Iter    18/   48] train: loss: 0.2514202
[Epoch 40; Iter    48/   48] train: loss: 0.4028891
[Epoch 40] ogbg-molbbbp: 0.897805 val loss: 0.324025
[Epoch 40] ogbg-molbbbp: 0.886371 test loss: 0.340665
[Epoch 41; Iter    30/   48] train: loss: 0.2392734
[Epoch 41] ogbg-molbbbp: 0.901775 val loss: 0.322793
[Epoch 41] ogbg-molbbbp: 0.902202 test loss: 0.332284
[Epoch 42; Iter    12/   48] train: loss: 0.0935676
[Epoch 42; Iter    42/   48] train: loss: 0.2695861
[Epoch 42] ogbg-molbbbp: 0.897125 val loss: 0.352293
[Epoch 42] ogbg-molbbbp: 0.878072 test loss: 0.388326
[Epoch 43; Iter    24/   48] train: loss: 0.1284461
[Epoch 43] ogbg-molbbbp: 0.895026 val loss: 0.352116
[Epoch 43] ogbg-molbbbp: 0.895563 test loss: 0.339909
[Epoch 44; Iter     6/   48] train: loss: 0.1400589
[Epoch 44; Iter    36/   48] train: loss: 0.2883192
[Epoch 44] ogbg-molbbbp: 0.899677 val loss: 0.328999
[Epoch 44] ogbg-molbbbp: 0.896968 test loss: 0.355580
[Epoch 45; Iter    18/   48] train: loss: 0.1021166
[Epoch 45; Iter    48/   48] train: loss: 0.0409439
[Epoch 45] ogbg-molbbbp: 0.892701 val loss: 0.365874
[Epoch 45] ogbg-molbbbp: 0.917651 test loss: 0.485215
[Epoch 46; Iter    30/   48] train: loss: 0.0974538
[Epoch 46] ogbg-molbbbp: 0.894232 val loss: 0.363905
[Epoch 46] ogbg-molbbbp: 0.873731 test loss: 0.406107
[Epoch 47; Iter    12/   48] train: loss: 0.1797296
[Epoch 47; Iter    42/   48] train: loss: 0.3199522
[Epoch 47] ogbg-molbbbp: 0.895423 val loss: 0.359297
[Epoch 47] ogbg-molbbbp: 0.878519 test loss: 0.388577
[Epoch 48; Iter    24/   48] train: loss: 0.1280567
[Epoch 48] ogbg-molbbbp: 0.891227 val loss: 0.364004
[Epoch 48] ogbg-molbbbp: 0.884775 test loss: 0.406118
[Epoch 49; Iter     6/   48] train: loss: 0.1614230
[Epoch 49; Iter    36/   48] train: loss: 0.1552326
[Epoch 49] ogbg-molbbbp: 0.924290 val loss: 0.308071
[Epoch 49] ogbg-molbbbp: 0.888286 test loss: 0.388072
[Epoch 50; Iter    18/   48] train: loss: 0.0459504
[Epoch 50; Iter    48/   48] train: loss: 0.0574083
[Epoch 50] ogbg-molbbbp: 0.895480 val loss: 0.433336
[Epoch 50] ogbg-molbbbp: 0.912672 test loss: 0.391398
[Epoch 51; Iter    30/   48] train: loss: 0.1111179
[Epoch 51] ogbg-molbbbp: 0.899223 val loss: 0.387397
[Epoch 51] ogbg-molbbbp: 0.899968 test loss: 0.404293
[Epoch 52; Iter    12/   48] train: loss: 0.0820183
[Epoch 52; Iter    42/   48] train: loss: 0.0661072
[Epoch 52] ogbg-molbbbp: 0.912097 val loss: 0.422757
[Epoch 52] ogbg-molbbbp: 0.908331 test loss: 0.421181
[Epoch 53; Iter    24/   48] train: loss: 0.2922179
[Epoch 53] ogbg-molbbbp: 0.902626 val loss: 0.376780
[Epoch 53] ogbg-molbbbp: 0.903288 test loss: 0.365868
[Epoch 54; Iter     6/   48] train: loss: 0.0571089
[Epoch 54; Iter    36/   48] train: loss: 0.0999061
[Epoch 54] ogbg-molbbbp: 0.927239 val loss: 0.363670
[Epoch 54] ogbg-molbbbp: 0.903926 test loss: 0.453995
[Epoch 55; Iter    18/   48] train: loss: 0.2911749
[Epoch 55; Iter    48/   48] train: loss: 0.6327085
[Epoch 55] ogbg-molbbbp: 0.917428 val loss: 0.309325
[Epoch 55] ogbg-molbbbp: 0.900479 test loss: 0.360254
[Epoch 56; Iter    30/   48] train: loss: 0.1536157
[Epoch 56] ogbg-molbbbp: 0.920887 val loss: 0.353869
[Epoch 56] ogbg-molbbbp: 0.896393 test loss: 0.382849
[Epoch 57; Iter    12/   48] train: loss: 0.0591279
[Epoch 57; Iter    42/   48] train: loss: 0.0901954
[Epoch 57] ogbg-molbbbp: 0.901378 val loss: 0.364207
[Epoch 57] ogbg-molbbbp: 0.898755 test loss: 0.389964
[Epoch 58; Iter    24/   48] train: loss: 0.0944140
[Epoch 58] ogbg-molbbbp: 0.907560 val loss: 0.370315
[Epoch 58] ogbg-molbbbp: 0.895627 test loss: 0.401934
[Epoch 59; Iter     6/   48] train: loss: 0.2127851
[Epoch 59; Iter    36/   48] train: loss: 0.0461174
[Epoch 59] ogbg-molbbbp: 0.922815 val loss: 0.338358
[Epoch 59] ogbg-molbbbp: 0.916183 test loss: 0.396326
[Epoch 60; Iter    18/   48] train: loss: 0.0398950
[Epoch 60; Iter    48/   48] train: loss: 0.0699070
[Epoch 60] ogbg-molbbbp: 0.921397 val loss: 0.354653
[Epoch 60] ogbg-molbbbp: 0.884264 test loss: 0.448379
[Epoch 61; Iter    30/   48] train: loss: 0.1427728
[Epoch 61] ogbg-molbbbp: 0.914479 val loss: 0.364270
[Epoch 61] ogbg-molbbbp: 0.891733 test loss: 0.551319
[Epoch 62; Iter    12/   48] train: loss: 0.2652389
[Epoch 62; Iter    42/   48] train: loss: 0.1272792
[Epoch 62] ogbg-molbbbp: 0.911927 val loss: 0.362950
[Epoch 62] ogbg-molbbbp: 0.884009 test loss: 0.465649
[Epoch 63; Iter    24/   48] train: loss: 0.1928242
[Epoch 63] ogbg-molbbbp: 0.916237 val loss: 0.361509
[Epoch 63] ogbg-molbbbp: 0.899011 test loss: 0.447670
[Epoch 64; Iter     6/   48] train: loss: 0.0439536
[Epoch 64; Iter    36/   48] train: loss: 0.1591003
[Epoch 64] ogbg-molbbbp: 0.913458 val loss: 0.373107
[Epoch 64] ogbg-molbbbp: 0.888414 test loss: 0.465595
[Epoch 65; Iter    18/   48] train: loss: 0.1271102
[Epoch 65; Iter    48/   48] train: loss: 0.1524107
[Epoch 65] ogbg-molbbbp: 0.896104 val loss: 0.415511
[Epoch 65] ogbg-molbbbp: 0.895053 test loss: 0.466987
[Epoch 66; Iter    30/   48] train: loss: 0.0398014
[Epoch 66] ogbg-molbbbp: 0.908921 val loss: 0.366669
[Epoch 66] ogbg-molbbbp: 0.890393 test loss: 0.463581
[Epoch 67; Iter    12/   48] train: loss: 0.0381097
[Epoch 67; Iter    42/   48] train: loss: 0.1284144
[Epoch 67] ogbg-molbbbp: 0.915102 val loss: 0.370857
[Epoch 67] ogbg-molbbbp: 0.903160 test loss: 0.450481
[Epoch 68; Iter    24/   48] train: loss: 0.0150269
[Epoch 68] ogbg-molbbbp: 0.915556 val loss: 0.383472
[Epoch 68] ogbg-molbbbp: 0.886945 test loss: 0.547773
[Epoch 69; Iter     6/   48] train: loss: 0.0293248
[Epoch 69; Iter    36/   48] train: loss: 0.0397071
[Epoch 69] ogbg-molbbbp: 0.916463 val loss: 0.372573
[Epoch 69] ogbg-molbbbp: 0.889371 test loss: 0.500507
[Epoch 70; Iter    18/   48] train: loss: 0.0219072
[Epoch 70; Iter    48/   48] train: loss: 0.1113806
[Epoch 70] ogbg-molbbbp: 0.913685 val loss: 0.392548
[Epoch 70] ogbg-molbbbp: 0.897606 test loss: 0.524454
[Epoch 71; Iter    30/   48] train: loss: 0.0145227
[Epoch 71] ogbg-molbbbp: 0.920320 val loss: 0.384526
[Epoch 71] ogbg-molbbbp: 0.889882 test loss: 0.538865
[Epoch 72; Iter    12/   48] train: loss: 0.0103858
[Epoch 72; Iter    42/   48] train: loss: 0.0057215
[Epoch 72] ogbg-molbbbp: 0.911813 val loss: 0.417930
[Epoch 72] ogbg-molbbbp: 0.871305 test loss: 0.591970
[Epoch 73; Iter    24/   48] train: loss: 0.0407157
[Epoch 73] ogbg-molbbbp: 0.910452 val loss: 0.406379
[Epoch 30] ogbg-molbbbp: 0.871718 val loss: 0.360168
[Epoch 30] ogbg-molbbbp: 0.886435 test loss: 0.341241
[Epoch 31; Iter    30/   48] train: loss: 0.4885066
[Epoch 31] ogbg-molbbbp: 0.893268 val loss: 0.358893
[Epoch 31] ogbg-molbbbp: 0.891605 test loss: 0.364684
[Epoch 32; Iter    12/   48] train: loss: 0.2056754
[Epoch 32; Iter    42/   48] train: loss: 0.2001098
[Epoch 32] ogbg-molbbbp: 0.914762 val loss: 0.321610
[Epoch 32] ogbg-molbbbp: 0.913246 test loss: 0.324835
[Epoch 33; Iter    24/   48] train: loss: 0.2635682
[Epoch 33] ogbg-molbbbp: 0.891737 val loss: 0.368806
[Epoch 33] ogbg-molbbbp: 0.892563 test loss: 0.397903
[Epoch 34; Iter     6/   48] train: loss: 0.1495714
[Epoch 34; Iter    36/   48] train: loss: 0.3211015
[Epoch 34] ogbg-molbbbp: 0.905745 val loss: 0.368871
[Epoch 34] ogbg-molbbbp: 0.903862 test loss: 0.363623
[Epoch 35; Iter    18/   48] train: loss: 0.3335503
[Epoch 35; Iter    48/   48] train: loss: 0.3847831
[Epoch 35] ogbg-molbbbp: 0.907446 val loss: 0.348930
[Epoch 35] ogbg-molbbbp: 0.864028 test loss: 0.372234
[Epoch 36; Iter    30/   48] train: loss: 0.4093038
[Epoch 36] ogbg-molbbbp: 0.909488 val loss: 0.312702
[Epoch 36] ogbg-molbbbp: 0.903862 test loss: 0.306230
[Epoch 37; Iter    12/   48] train: loss: 0.1886122
[Epoch 37; Iter    42/   48] train: loss: 0.1827756
[Epoch 37] ogbg-molbbbp: 0.910849 val loss: 0.425891
[Epoch 37] ogbg-molbbbp: 0.903543 test loss: 0.370239
[Epoch 38; Iter    24/   48] train: loss: 0.2937837
[Epoch 38] ogbg-molbbbp: 0.891227 val loss: 0.346517
[Epoch 38] ogbg-molbbbp: 0.878774 test loss: 0.335602
[Epoch 39; Iter     6/   48] train: loss: 0.2437343
[Epoch 39; Iter    36/   48] train: loss: 0.2095602
[Epoch 39] ogbg-molbbbp: 0.897862 val loss: 0.329321
[Epoch 39] ogbg-molbbbp: 0.873795 test loss: 0.348729
[Epoch 40; Iter    18/   48] train: loss: 0.2091666
[Epoch 40; Iter    48/   48] train: loss: 0.0741033
[Epoch 40] ogbg-molbbbp: 0.904157 val loss: 0.342821
[Epoch 40] ogbg-molbbbp: 0.905586 test loss: 0.334943
[Epoch 41; Iter    30/   48] train: loss: 0.2142542
[Epoch 41] ogbg-molbbbp: 0.896898 val loss: 0.546255
[Epoch 41] ogbg-molbbbp: 0.863134 test loss: 0.714101
[Epoch 42; Iter    12/   48] train: loss: 0.2273258
[Epoch 42; Iter    42/   48] train: loss: 0.4117709
[Epoch 42] ogbg-molbbbp: 0.911983 val loss: 0.312297
[Epoch 42] ogbg-molbbbp: 0.908458 test loss: 0.318101
[Epoch 43; Iter    24/   48] train: loss: 0.2331453
[Epoch 43] ogbg-molbbbp: 0.918335 val loss: 0.291960
[Epoch 43] ogbg-molbbbp: 0.877689 test loss: 0.351344
[Epoch 44; Iter     6/   48] train: loss: 0.2101923
[Epoch 44; Iter    36/   48] train: loss: 0.0861860
[Epoch 44] ogbg-molbbbp: 0.914479 val loss: 0.319426
[Epoch 44] ogbg-molbbbp: 0.897925 test loss: 0.360963
[Epoch 45; Iter    18/   48] train: loss: 0.2877713
[Epoch 45; Iter    48/   48] train: loss: 0.2765971
[Epoch 45] ogbg-molbbbp: 0.911189 val loss: 0.341255
[Epoch 45] ogbg-molbbbp: 0.892180 test loss: 0.351970
[Epoch 46; Iter    30/   48] train: loss: 0.1595933
[Epoch 46] ogbg-molbbbp: 0.907276 val loss: 0.321466
[Epoch 46] ogbg-molbbbp: 0.896521 test loss: 0.337003
[Epoch 47; Iter    12/   48] train: loss: 0.1015377
[Epoch 47; Iter    42/   48] train: loss: 0.1483071
[Epoch 47] ogbg-molbbbp: 0.916747 val loss: 0.354258
[Epoch 47] ogbg-molbbbp: 0.897478 test loss: 0.362311
[Epoch 48; Iter    24/   48] train: loss: 0.4222079
[Epoch 48] ogbg-molbbbp: 0.925707 val loss: 0.350782
[Epoch 48] ogbg-molbbbp: 0.916438 test loss: 0.379860
[Epoch 49; Iter     6/   48] train: loss: 0.1717446
[Epoch 49; Iter    36/   48] train: loss: 0.0900775
[Epoch 49] ogbg-molbbbp: 0.914592 val loss: 0.297720
[Epoch 49] ogbg-molbbbp: 0.896393 test loss: 0.324546
[Epoch 50; Iter    18/   48] train: loss: 0.0509230
[Epoch 50; Iter    48/   48] train: loss: 0.0541258
[Epoch 50] ogbg-molbbbp: 0.914365 val loss: 0.318213
[Epoch 50] ogbg-molbbbp: 0.886243 test loss: 0.372445
[Epoch 51; Iter    30/   48] train: loss: 0.1378760
[Epoch 51] ogbg-molbbbp: 0.916860 val loss: 0.382132
[Epoch 51] ogbg-molbbbp: 0.907948 test loss: 0.373554
[Epoch 52; Iter    12/   48] train: loss: 0.0942133
[Epoch 52; Iter    42/   48] train: loss: 0.3064549
[Epoch 52] ogbg-molbbbp: 0.914876 val loss: 0.354203
[Epoch 52] ogbg-molbbbp: 0.899202 test loss: 0.357090
[Epoch 53; Iter    24/   48] train: loss: 0.2351739
[Epoch 53] ogbg-molbbbp: 0.895140 val loss: 0.385595
[Epoch 53] ogbg-molbbbp: 0.885669 test loss: 0.440130
[Epoch 54; Iter     6/   48] train: loss: 0.0697626
[Epoch 54; Iter    36/   48] train: loss: 0.1523093
[Epoch 54] ogbg-molbbbp: 0.919583 val loss: 0.319728
[Epoch 54] ogbg-molbbbp: 0.893201 test loss: 0.359040
[Epoch 55; Iter    18/   48] train: loss: 0.1499220
[Epoch 55; Iter    48/   48] train: loss: 0.0630666
[Epoch 55] ogbg-molbbbp: 0.941076 val loss: 0.271728
[Epoch 55] ogbg-molbbbp: 0.932844 test loss: 0.279632
[Epoch 56; Iter    30/   48] train: loss: 0.0922736
[Epoch 56] ogbg-molbbbp: 0.923723 val loss: 0.348113
[Epoch 56] ogbg-molbbbp: 0.913246 test loss: 0.389130
[Epoch 57; Iter    12/   48] train: loss: 0.0478849
[Epoch 57; Iter    42/   48] train: loss: 0.1725343
[Epoch 57] ogbg-molbbbp: 0.920433 val loss: 0.364241
[Epoch 57] ogbg-molbbbp: 0.894733 test loss: 0.446167
[Epoch 58; Iter    24/   48] train: loss: 0.0695961
[Epoch 58] ogbg-molbbbp: 0.922418 val loss: 0.309933
[Epoch 58] ogbg-molbbbp: 0.886307 test loss: 0.393645
[Epoch 59; Iter     6/   48] train: loss: 0.0991670
[Epoch 59; Iter    36/   48] train: loss: 0.0849426
[Epoch 59] ogbg-molbbbp: 0.914195 val loss: 0.362717
[Epoch 59] ogbg-molbbbp: 0.890520 test loss: 0.396477
[Epoch 60; Iter    18/   48] train: loss: 0.1038323
[Epoch 60; Iter    48/   48] train: loss: 0.2478018
[Epoch 60] ogbg-molbbbp: 0.923042 val loss: 0.310305
[Epoch 60] ogbg-molbbbp: 0.879477 test loss: 0.415452
[Epoch 61; Iter    30/   48] train: loss: 0.0211449
[Epoch 61] ogbg-molbbbp: 0.914308 val loss: 0.342420
[Epoch 61] ogbg-molbbbp: 0.885286 test loss: 0.437971
[Epoch 62; Iter    12/   48] train: loss: 0.1338643
[Epoch 62; Iter    42/   48] train: loss: 0.1445982
[Epoch 62] ogbg-molbbbp: 0.917711 val loss: 0.349354
[Epoch 62] ogbg-molbbbp: 0.879157 test loss: 0.409307
[Epoch 63; Iter    24/   48] train: loss: 0.0735858
[Epoch 63] ogbg-molbbbp: 0.928940 val loss: 0.333089
[Epoch 63] ogbg-molbbbp: 0.891925 test loss: 0.437030
[Epoch 64; Iter     6/   48] train: loss: 0.0853242
[Epoch 64; Iter    36/   48] train: loss: 0.0439539
[Epoch 64] ogbg-molbbbp: 0.912380 val loss: 0.364372
[Epoch 64] ogbg-molbbbp: 0.886818 test loss: 0.419637
[Epoch 65; Iter    18/   48] train: loss: 0.0270056
[Epoch 65; Iter    48/   48] train: loss: 0.6267934
[Epoch 65] ogbg-molbbbp: 0.918959 val loss: 0.365318
[Epoch 65] ogbg-molbbbp: 0.894095 test loss: 0.421135
[Epoch 66; Iter    30/   48] train: loss: 0.1301135
[Epoch 66] ogbg-molbbbp: 0.902739 val loss: 0.404254
[Epoch 66] ogbg-molbbbp: 0.863198 test loss: 0.520263
[Epoch 67; Iter    12/   48] train: loss: 0.0143209
[Epoch 67; Iter    42/   48] train: loss: 0.0105851
[Epoch 67] ogbg-molbbbp: 0.911133 val loss: 0.465479
[Epoch 67] ogbg-molbbbp: 0.890456 test loss: 0.495947
[Epoch 68; Iter    24/   48] train: loss: 0.0120672
[Epoch 68] ogbg-molbbbp: 0.914025 val loss: 0.451240
[Epoch 68] ogbg-molbbbp: 0.887265 test loss: 0.498799
[Epoch 69; Iter     6/   48] train: loss: 0.0154199
[Epoch 69; Iter    36/   48] train: loss: 0.0516155
[Epoch 69] ogbg-molbbbp: 0.922418 val loss: 0.389359
[Epoch 69] ogbg-molbbbp: 0.891669 test loss: 0.476225
[Epoch 70; Iter    18/   48] train: loss: 0.0417563
[Epoch 70; Iter    48/   48] train: loss: 0.1338792
[Epoch 70] ogbg-molbbbp: 0.913117 val loss: 0.410789
[Epoch 70] ogbg-molbbbp: 0.876349 test loss: 0.510054
[Epoch 71; Iter    30/   48] train: loss: 0.0326944
[Epoch 71] ogbg-molbbbp: 0.908127 val loss: 0.452678
[Epoch 71] ogbg-molbbbp: 0.883243 test loss: 0.553676
[Epoch 72; Iter    12/   48] train: loss: 0.0114579
[Epoch 72; Iter    42/   48] train: loss: 0.0168054
[Epoch 72] ogbg-molbbbp: 0.904157 val loss: 0.495335
[Epoch 72] ogbg-molbbbp: 0.894733 test loss: 0.522857
[Epoch 73; Iter    24/   48] train: loss: 0.0066049
[Epoch 73] ogbg-molbbbp: 0.909261 val loss: 0.620075
[Epoch 32] ogbg-molbbbp: 0.869892 test loss: 0.374866
[Epoch 33; Iter     8/   41] train: loss: 0.4595226
[Epoch 33; Iter    38/   41] train: loss: 0.1857438
[Epoch 33] ogbg-molbbbp: 0.887353 val loss: 0.345562
[Epoch 33] ogbg-molbbbp: 0.876870 test loss: 0.371633
[Epoch 34; Iter    27/   41] train: loss: 0.2488695
[Epoch 34] ogbg-molbbbp: 0.921113 val loss: 0.283199
[Epoch 34] ogbg-molbbbp: 0.919905 test loss: 0.308186
[Epoch 35; Iter    16/   41] train: loss: 0.6436503
[Epoch 35] ogbg-molbbbp: 0.906230 val loss: 0.350576
[Epoch 35] ogbg-molbbbp: 0.874065 test loss: 0.404407
[Epoch 36; Iter     5/   41] train: loss: 0.3328511
[Epoch 36; Iter    35/   41] train: loss: 0.3846835
[Epoch 36] ogbg-molbbbp: 0.873391 val loss: 0.365712
[Epoch 36] ogbg-molbbbp: 0.892294 test loss: 0.361528
[Epoch 37; Iter    24/   41] train: loss: 0.1691663
[Epoch 37] ogbg-molbbbp: 0.875917 val loss: 0.351789
[Epoch 37] ogbg-molbbbp: 0.887487 test loss: 0.357840
[Epoch 38; Iter    13/   41] train: loss: 0.1986213
[Epoch 38] ogbg-molbbbp: 0.908483 val loss: 0.416836
[Epoch 38] ogbg-molbbbp: 0.898070 test loss: 0.462638
[Epoch 39; Iter     2/   41] train: loss: 0.1164057
[Epoch 39; Iter    32/   41] train: loss: 0.3696304
[Epoch 39] ogbg-molbbbp: 0.903909 val loss: 0.355901
[Epoch 39] ogbg-molbbbp: 0.886285 test loss: 0.431033
[Epoch 40; Iter    21/   41] train: loss: 0.1857829
[Epoch 40] ogbg-molbbbp: 0.919065 val loss: 0.292010
[Epoch 40] ogbg-molbbbp: 0.920439 test loss: 0.296143
[Epoch 41; Iter    10/   41] train: loss: 0.1627743
[Epoch 41; Iter    40/   41] train: loss: 0.3023654
[Epoch 41] ogbg-molbbbp: 0.904421 val loss: 0.366471
[Epoch 41] ogbg-molbbbp: 0.884716 test loss: 0.480424
[Epoch 42; Iter    29/   41] train: loss: 0.1992907
[Epoch 42] ogbg-molbbbp: 0.890425 val loss: 0.345357
[Epoch 42] ogbg-molbbbp: 0.906918 test loss: 0.320438
[Epoch 43; Iter    18/   41] train: loss: 0.3014893
[Epoch 43] ogbg-molbbbp: 0.910258 val loss: 0.305660
[Epoch 43] ogbg-molbbbp: 0.887386 test loss: 0.389617
[Epoch 44; Iter     7/   41] train: loss: 0.1631589
[Epoch 44; Iter    37/   41] train: loss: 0.2253142
[Epoch 44] ogbg-molbbbp: 0.930022 val loss: 0.281915
[Epoch 44] ogbg-molbbbp: 0.901643 test loss: 0.365227
[Epoch 45; Iter    26/   41] train: loss: 0.1325164
[Epoch 45] ogbg-molbbbp: 0.912886 val loss: 0.465876
[Epoch 45] ogbg-molbbbp: 0.893763 test loss: 0.488257
[Epoch 46; Iter    15/   41] train: loss: 0.1015403
[Epoch 46] ogbg-molbbbp: 0.917836 val loss: 0.384476
[Epoch 46] ogbg-molbbbp: 0.888488 test loss: 0.460523
[Epoch 47; Iter     4/   41] train: loss: 0.3017555
[Epoch 47; Iter    34/   41] train: loss: 0.2661719
[Epoch 47] ogbg-molbbbp: 0.910121 val loss: 0.452652
[Epoch 47] ogbg-molbbbp: 0.884882 test loss: 0.565793
[Epoch 48; Iter    23/   41] train: loss: 0.1330104
[Epoch 48] ogbg-molbbbp: 0.888991 val loss: 0.376975
[Epoch 48] ogbg-molbbbp: 0.883714 test loss: 0.403977
[Epoch 49; Iter    12/   41] train: loss: 0.0617400
[Epoch 49] ogbg-molbbbp: 0.929647 val loss: 0.293261
[Epoch 49] ogbg-molbbbp: 0.886185 test loss: 0.398119
[Epoch 50; Iter     1/   41] train: loss: 0.1070104
[Epoch 50; Iter    31/   41] train: loss: 0.1297718
[Epoch 50] ogbg-molbbbp: 0.913569 val loss: 0.360894
[Epoch 50] ogbg-molbbbp: 0.879207 test loss: 0.432336
[Epoch 51; Iter    20/   41] train: loss: 0.1479350
[Epoch 51] ogbg-molbbbp: 0.918075 val loss: 0.307083
[Epoch 51] ogbg-molbbbp: 0.903178 test loss: 0.371770
[Epoch 52; Iter     9/   41] train: loss: 0.1055352
[Epoch 52; Iter    39/   41] train: loss: 0.2319938
[Epoch 52] ogbg-molbbbp: 0.909029 val loss: 0.315991
[Epoch 52] ogbg-molbbbp: 0.869324 test loss: 0.415758
[Epoch 53; Iter    28/   41] train: loss: 0.1601370
[Epoch 53] ogbg-molbbbp: 0.904762 val loss: 0.330548
[Epoch 53] ogbg-molbbbp: 0.890925 test loss: 0.389837
[Epoch 54; Iter    17/   41] train: loss: 0.1036904
[Epoch 54] ogbg-molbbbp: 0.911487 val loss: 0.364358
[Epoch 54] ogbg-molbbbp: 0.876002 test loss: 0.437648
[Epoch 55; Iter     6/   41] train: loss: 0.0997141
[Epoch 55; Iter    36/   41] train: loss: 0.0965374
[Epoch 55] ogbg-molbbbp: 0.909165 val loss: 0.356301
[Epoch 55] ogbg-molbbbp: 0.891393 test loss: 0.437704
[Epoch 56; Iter    25/   41] train: loss: 0.0612112
[Epoch 56] ogbg-molbbbp: 0.926165 val loss: 0.321221
[Epoch 56] ogbg-molbbbp: 0.882045 test loss: 0.454474
[Epoch 57; Iter    14/   41] train: loss: 0.2050663
[Epoch 57] ogbg-molbbbp: 0.924526 val loss: 0.301001
[Epoch 57] ogbg-molbbbp: 0.882278 test loss: 0.399966
[Epoch 58; Iter     3/   41] train: loss: 0.1695767
[Epoch 58; Iter    33/   41] train: loss: 0.0897317
[Epoch 58] ogbg-molbbbp: 0.923229 val loss: 0.353216
[Epoch 58] ogbg-molbbbp: 0.878739 test loss: 0.469869
[Epoch 59; Iter    22/   41] train: loss: 0.0319339
[Epoch 59] ogbg-molbbbp: 0.926336 val loss: 0.321303
[Epoch 59] ogbg-molbbbp: 0.885851 test loss: 0.450976
[Epoch 60; Iter    11/   41] train: loss: 0.1788449
[Epoch 60; Iter    41/   41] train: loss: 0.0176522
[Epoch 60] ogbg-molbbbp: 0.921522 val loss: 0.374573
[Epoch 60] ogbg-molbbbp: 0.878806 test loss: 0.548833
[Epoch 61; Iter    30/   41] train: loss: 0.0424529
[Epoch 61] ogbg-molbbbp: 0.896604 val loss: 0.416544
[Epoch 61] ogbg-molbbbp: 0.864784 test loss: 0.591644
[Epoch 62; Iter    19/   41] train: loss: 0.0633916
[Epoch 62] ogbg-molbbbp: 0.857177 val loss: 0.509927
[Epoch 62] ogbg-molbbbp: 0.818176 test loss: 0.618692
[Epoch 63; Iter     8/   41] train: loss: 0.1214264
[Epoch 63; Iter    38/   41] train: loss: 0.0727335
[Epoch 63] ogbg-molbbbp: 0.907220 val loss: 0.405726
[Epoch 63] ogbg-molbbbp: 0.872963 test loss: 0.489628
[Epoch 64; Iter    27/   41] train: loss: 0.1448423
[Epoch 64] ogbg-molbbbp: 0.918450 val loss: 0.332045
[Epoch 64] ogbg-molbbbp: 0.874967 test loss: 0.472933
[Epoch 65; Iter    16/   41] train: loss: 0.0740191
[Epoch 65] ogbg-molbbbp: 0.916231 val loss: 0.362097
[Epoch 65] ogbg-molbbbp: 0.885216 test loss: 0.455867
[Epoch 66; Iter     5/   41] train: loss: 0.1748647
[Epoch 66; Iter    35/   41] train: loss: 0.0318713
[Epoch 66] ogbg-molbbbp: 0.914115 val loss: 0.592503
[Epoch 66] ogbg-molbbbp: 0.884482 test loss: 0.650578
[Epoch 67; Iter    24/   41] train: loss: 0.0180054
[Epoch 67] ogbg-molbbbp: 0.921761 val loss: 0.347418
[Epoch 67] ogbg-molbbbp: 0.868957 test loss: 0.479910
[Epoch 68; Iter    13/   41] train: loss: 0.1125619
[Epoch 68] ogbg-molbbbp: 0.905684 val loss: 0.564260
[Epoch 68] ogbg-molbbbp: 0.867989 test loss: 0.525778
[Epoch 69; Iter     2/   41] train: loss: 0.0464128
[Epoch 69; Iter    32/   41] train: loss: 0.0905352
[Epoch 69] ogbg-molbbbp: 0.906400 val loss: 0.425057
[Epoch 69] ogbg-molbbbp: 0.857973 test loss: 0.552085
[Epoch 70; Iter    21/   41] train: loss: 0.1231240
[Epoch 70] ogbg-molbbbp: 0.915754 val loss: 0.356922
[Epoch 70] ogbg-molbbbp: 0.871194 test loss: 0.580313
[Epoch 71; Iter    10/   41] train: loss: 0.1126646
[Epoch 71; Iter    40/   41] train: loss: 0.1605442
[Epoch 71] ogbg-molbbbp: 0.918962 val loss: 0.343055
[Epoch 71] ogbg-molbbbp: 0.874833 test loss: 0.518660
[Epoch 72; Iter    29/   41] train: loss: 0.0190820
[Epoch 72] ogbg-molbbbp: 0.914593 val loss: 0.374334
[Epoch 72] ogbg-molbbbp: 0.862947 test loss: 0.623195
[Epoch 73; Iter    18/   41] train: loss: 0.0173216
[Epoch 73] ogbg-molbbbp: 0.905923 val loss: 0.498520
[Epoch 73] ogbg-molbbbp: 0.889089 test loss: 0.573039
[Epoch 74; Iter     7/   41] train: loss: 0.0353385
[Epoch 74; Iter    37/   41] train: loss: 0.0344872
[Epoch 74] ogbg-molbbbp: 0.922888 val loss: 0.356362
[Epoch 74] ogbg-molbbbp: 0.886652 test loss: 0.518872
[Epoch 75; Iter    26/   41] train: loss: 0.0064532
[Epoch 75] ogbg-molbbbp: 0.901246 val loss: 0.365546
[Epoch 75] ogbg-molbbbp: 0.860343 test loss: 0.524299
[Epoch 76; Iter    15/   41] train: loss: 0.1100846
[Epoch 76] ogbg-molbbbp: 0.922239 val loss: 0.340294
[Epoch 76] ogbg-molbbbp: 0.884649 test loss: 0.504149
[Epoch 77; Iter     4/   41] train: loss: 0.0128280
[Epoch 77; Iter    34/   41] train: loss: 0.1748133
[Epoch 77] ogbg-molbbbp: 0.915344 val loss: 0.390770
[Epoch 77] ogbg-molbbbp: 0.883847 test loss: 0.565201
[Epoch 78; Iter    23/   41] train: loss: 0.0831946
[Epoch 32] ogbg-molbbbp: 0.844885 test loss: 0.395959
[Epoch 33; Iter     8/   41] train: loss: 0.3617308
[Epoch 33; Iter    38/   41] train: loss: 0.2117737
[Epoch 33] ogbg-molbbbp: 0.881174 val loss: 0.336372
[Epoch 33] ogbg-molbbbp: 0.886485 test loss: 0.356742
[Epoch 34; Iter    27/   41] train: loss: 0.4963553
[Epoch 34] ogbg-molbbbp: 0.908892 val loss: 0.292316
[Epoch 34] ogbg-molbbbp: 0.904848 test loss: 0.323327
[Epoch 35; Iter    16/   41] train: loss: 0.2875625
[Epoch 35] ogbg-molbbbp: 0.773169 val loss: 0.939743
[Epoch 35] ogbg-molbbbp: 0.747596 test loss: 1.100618
[Epoch 36; Iter     5/   41] train: loss: 0.5864635
[Epoch 36; Iter    35/   41] train: loss: 0.1212249
[Epoch 36] ogbg-molbbbp: 0.913705 val loss: 0.309341
[Epoch 36] ogbg-molbbbp: 0.897569 test loss: 0.350789
[Epoch 37; Iter    24/   41] train: loss: 0.1770609
[Epoch 37] ogbg-molbbbp: 0.906366 val loss: 0.343493
[Epoch 37] ogbg-molbbbp: 0.878639 test loss: 0.383806
[Epoch 38; Iter    13/   41] train: loss: 0.3719489
[Epoch 38] ogbg-molbbbp: 0.916163 val loss: 0.273414
[Epoch 38] ogbg-molbbbp: 0.879708 test loss: 0.355140
[Epoch 39; Iter     2/   41] train: loss: 0.1395762
[Epoch 39; Iter    32/   41] train: loss: 0.2804959
[Epoch 39] ogbg-molbbbp: 0.880526 val loss: 0.381194
[Epoch 39] ogbg-molbbbp: 0.890057 test loss: 0.363462
[Epoch 40; Iter    21/   41] train: loss: 0.2710227
[Epoch 40] ogbg-molbbbp: 0.920669 val loss: 0.290953
[Epoch 40] ogbg-molbbbp: 0.915198 test loss: 0.325126
[Epoch 41; Iter    10/   41] train: loss: 0.1779090
[Epoch 41; Iter    40/   41] train: loss: 0.3032825
[Epoch 41] ogbg-molbbbp: 0.910531 val loss: 0.333647
[Epoch 41] ogbg-molbbbp: 0.885517 test loss: 0.375704
[Epoch 42; Iter    29/   41] train: loss: 0.3028805
[Epoch 42] ogbg-molbbbp: 0.905240 val loss: 0.370428
[Epoch 42] ogbg-molbbbp: 0.885717 test loss: 0.415552
[Epoch 43; Iter    18/   41] train: loss: 0.2421043
[Epoch 43] ogbg-molbbbp: 0.927564 val loss: 0.319757
[Epoch 43] ogbg-molbbbp: 0.898571 test loss: 0.377839
[Epoch 44; Iter     7/   41] train: loss: 0.1646211
[Epoch 44; Iter    37/   41] train: loss: 0.1399138
[Epoch 44] ogbg-molbbbp: 0.919611 val loss: 0.340429
[Epoch 44] ogbg-molbbbp: 0.875000 test loss: 0.409284
[Epoch 45; Iter    26/   41] train: loss: 0.1249069
[Epoch 45] ogbg-molbbbp: 0.917563 val loss: 0.309393
[Epoch 45] ogbg-molbbbp: 0.897569 test loss: 0.360905
[Epoch 46; Iter    15/   41] train: loss: 0.1908288
[Epoch 46] ogbg-molbbbp: 0.910445 val loss: 0.469430
[Epoch 46] ogbg-molbbbp: 0.859876 test loss: 0.544397
[Epoch 47; Iter     4/   41] train: loss: 0.3065377
[Epoch 47; Iter    34/   41] train: loss: 0.1696819
[Epoch 47] ogbg-molbbbp: 0.910394 val loss: 0.357436
[Epoch 47] ogbg-molbbbp: 0.862814 test loss: 0.433725
[Epoch 48; Iter    23/   41] train: loss: 0.2728542
[Epoch 48] ogbg-molbbbp: 0.927257 val loss: 0.307237
[Epoch 48] ogbg-molbbbp: 0.901476 test loss: 0.355316
[Epoch 49; Iter    12/   41] train: loss: 0.1207720
[Epoch 49] ogbg-molbbbp: 0.931422 val loss: 0.297959
[Epoch 49] ogbg-molbbbp: 0.885517 test loss: 0.380957
[Epoch 50; Iter     1/   41] train: loss: 0.1240053
[Epoch 50; Iter    31/   41] train: loss: 0.0623086
[Epoch 50] ogbg-molbbbp: 0.935757 val loss: 0.284478
[Epoch 50] ogbg-molbbbp: 0.890325 test loss: 0.523694
[Epoch 51; Iter    20/   41] train: loss: 0.1490418
[Epoch 51] ogbg-molbbbp: 0.925550 val loss: 0.321825
[Epoch 51] ogbg-molbbbp: 0.898104 test loss: 0.390948
[Epoch 52; Iter     9/   41] train: loss: 0.1304268
[Epoch 52; Iter    39/   41] train: loss: 0.2419300
[Epoch 52] ogbg-molbbbp: 0.908756 val loss: 0.368030
[Epoch 52] ogbg-molbbbp: 0.891860 test loss: 0.409606
[Epoch 53; Iter    28/   41] train: loss: 0.1086914
[Epoch 53] ogbg-molbbbp: 0.924151 val loss: 0.471067
[Epoch 53] ogbg-molbbbp: 0.900274 test loss: 0.522589
[Epoch 54; Iter    17/   41] train: loss: 0.2317094
[Epoch 54] ogbg-molbbbp: 0.917426 val loss: 0.372517
[Epoch 54] ogbg-molbbbp: 0.892428 test loss: 0.434966
[Epoch 55; Iter     6/   41] train: loss: 0.1543877
[Epoch 55; Iter    36/   41] train: loss: 0.1311665
[Epoch 55] ogbg-molbbbp: 0.926438 val loss: 0.329412
[Epoch 55] ogbg-molbbbp: 0.909923 test loss: 0.385439
[Epoch 56; Iter    25/   41] train: loss: 0.1508050
[Epoch 56] ogbg-molbbbp: 0.918621 val loss: 0.393087
[Epoch 56] ogbg-molbbbp: 0.865919 test loss: 0.539806
[Epoch 57; Iter    14/   41] train: loss: 0.2611670
[Epoch 57] ogbg-molbbbp: 0.919201 val loss: 0.379764
[Epoch 57] ogbg-molbbbp: 0.879774 test loss: 0.472074
[Epoch 58; Iter     3/   41] train: loss: 0.1972309
[Epoch 58; Iter    33/   41] train: loss: 0.3290702
[Epoch 58] ogbg-molbbbp: 0.929988 val loss: 0.334590
[Epoch 58] ogbg-molbbbp: 0.890491 test loss: 0.468613
[Epoch 59; Iter    22/   41] train: loss: 0.0735643
[Epoch 59] ogbg-molbbbp: 0.931115 val loss: 0.323997
[Epoch 59] ogbg-molbbbp: 0.893530 test loss: 0.421225
[Epoch 60; Iter    11/   41] train: loss: 0.1259278
[Epoch 60; Iter    41/   41] train: loss: 0.0806584
[Epoch 60] ogbg-molbbbp: 0.929647 val loss: 0.330486
[Epoch 60] ogbg-molbbbp: 0.878739 test loss: 0.477982
[Epoch 61; Iter    30/   41] train: loss: 0.0741848
[Epoch 61] ogbg-molbbbp: 0.930944 val loss: 0.361958
[Epoch 61] ogbg-molbbbp: 0.876235 test loss: 0.502275
[Epoch 62; Iter    19/   41] train: loss: 0.0593798
[Epoch 62] ogbg-molbbbp: 0.921522 val loss: 0.381707
[Epoch 62] ogbg-molbbbp: 0.873831 test loss: 0.482858
[Epoch 63; Iter     8/   41] train: loss: 0.0874641
[Epoch 63; Iter    38/   41] train: loss: 0.3233010
[Epoch 63] ogbg-molbbbp: 0.918792 val loss: 0.377962
[Epoch 63] ogbg-molbbbp: 0.868556 test loss: 0.496425
[Epoch 64; Iter    27/   41] train: loss: 0.0904323
[Epoch 64] ogbg-molbbbp: 0.933197 val loss: 0.366762
[Epoch 64] ogbg-molbbbp: 0.890925 test loss: 0.444023
[Epoch 65; Iter    16/   41] train: loss: 0.1595353
[Epoch 65] ogbg-molbbbp: 0.920430 val loss: 0.398422
[Epoch 65] ogbg-molbbbp: 0.889423 test loss: 0.488773
[Epoch 66; Iter     5/   41] train: loss: 0.0508266
[Epoch 66; Iter    35/   41] train: loss: 0.1552459
[Epoch 66] ogbg-molbbbp: 0.921283 val loss: 0.360883
[Epoch 66] ogbg-molbbbp: 0.861679 test loss: 0.529372
[Epoch 67; Iter    24/   41] train: loss: 0.0938549
[Epoch 67] ogbg-molbbbp: 0.921079 val loss: 0.593112
[Epoch 67] ogbg-molbbbp: 0.893196 test loss: 0.550614
[Epoch 68; Iter    13/   41] train: loss: 0.0600767
[Epoch 68] ogbg-molbbbp: 0.931763 val loss: 0.364739
[Epoch 68] ogbg-molbbbp: 0.877070 test loss: 0.527457
[Epoch 69; Iter     2/   41] train: loss: 0.1351919
[Epoch 69; Iter    32/   41] train: loss: 0.0777921
[Epoch 69] ogbg-molbbbp: 0.934153 val loss: 0.720438
[Epoch 69] ogbg-molbbbp: 0.886151 test loss: 0.514836
[Epoch 70; Iter    21/   41] train: loss: 0.0092560
[Epoch 70] ogbg-molbbbp: 0.932412 val loss: 0.356353
[Epoch 70] ogbg-molbbbp: 0.893129 test loss: 0.496739
[Epoch 71; Iter    10/   41] train: loss: 0.0131435
[Epoch 71; Iter    40/   41] train: loss: 0.1054624
[Epoch 71] ogbg-molbbbp: 0.938522 val loss: 0.363953
[Epoch 71] ogbg-molbbbp: 0.884782 test loss: 0.564326
[Epoch 72; Iter    29/   41] train: loss: 0.0090433
[Epoch 72] ogbg-molbbbp: 0.936337 val loss: 0.388117
[Epoch 72] ogbg-molbbbp: 0.886919 test loss: 0.587937
[Epoch 73; Iter    18/   41] train: loss: 0.0726284
[Epoch 73] ogbg-molbbbp: 0.933163 val loss: 0.366582
[Epoch 73] ogbg-molbbbp: 0.886285 test loss: 0.564245
[Epoch 74; Iter     7/   41] train: loss: 0.0148183
[Epoch 74; Iter    37/   41] train: loss: 0.0581226
[Epoch 74] ogbg-molbbbp: 0.926950 val loss: 0.418194
[Epoch 74] ogbg-molbbbp: 0.875334 test loss: 0.589887
[Epoch 75; Iter    26/   41] train: loss: 0.0158132
[Epoch 75] ogbg-molbbbp: 0.925721 val loss: 0.451396
[Epoch 75] ogbg-molbbbp: 0.877971 test loss: 0.548276
[Epoch 76; Iter    15/   41] train: loss: 0.0041805
[Epoch 76] ogbg-molbbbp: 0.924868 val loss: 0.489041
[Epoch 76] ogbg-molbbbp: 0.890024 test loss: 0.585598
[Epoch 77; Iter     4/   41] train: loss: 0.0883114
[Epoch 77; Iter    34/   41] train: loss: 0.0694452
[Epoch 77] ogbg-molbbbp: 0.921898 val loss: 0.416953
[Epoch 77] ogbg-molbbbp: 0.881343 test loss: 0.583283
[Epoch 78; Iter    23/   41] train: loss: 0.0042237
[Epoch 69; Iter    40/   55] train: loss: 0.0708536
[Epoch 69] ogbg-molbbbp: 0.931481 val loss: 0.402142
[Epoch 69] ogbg-molbbbp: 0.888007 test loss: 0.497963
[Epoch 70; Iter    15/   55] train: loss: 0.0126755
[Epoch 70; Iter    45/   55] train: loss: 0.1483925
[Epoch 70] ogbg-molbbbp: 0.927778 val loss: 0.429509
[Epoch 70] ogbg-molbbbp: 0.878895 test loss: 0.590177
[Epoch 71; Iter    20/   55] train: loss: 0.0343087
[Epoch 71; Iter    50/   55] train: loss: 0.0493641
[Epoch 71] ogbg-molbbbp: 0.933210 val loss: 0.424166
[Epoch 71] ogbg-molbbbp: 0.880511 test loss: 0.526612
[Epoch 72; Iter    25/   55] train: loss: 0.0109852
[Epoch 72; Iter    55/   55] train: loss: 0.0541782
[Epoch 72] ogbg-molbbbp: 0.932099 val loss: 0.440809
[Epoch 72] ogbg-molbbbp: 0.895062 test loss: 0.536277
[Epoch 73; Iter    30/   55] train: loss: 0.0523224
[Epoch 73] ogbg-molbbbp: 0.938889 val loss: 0.438271
[Epoch 73] ogbg-molbbbp: 0.888448 test loss: 0.558473
[Epoch 74; Iter     5/   55] train: loss: 0.0122919
[Epoch 74; Iter    35/   55] train: loss: 0.0311995
[Epoch 74] ogbg-molbbbp: 0.912099 val loss: 0.490439
[Epoch 74] ogbg-molbbbp: 0.882275 test loss: 0.535989
[Epoch 75; Iter    10/   55] train: loss: 0.1677730
[Epoch 75; Iter    40/   55] train: loss: 0.0104482
[Epoch 75] ogbg-molbbbp: 0.919506 val loss: 0.496458
[Epoch 75] ogbg-molbbbp: 0.868607 test loss: 0.572919
[Epoch 76; Iter    15/   55] train: loss: 0.2395420
[Epoch 76; Iter    45/   55] train: loss: 0.0887565
[Epoch 76] ogbg-molbbbp: 0.923827 val loss: 0.454563
[Epoch 76] ogbg-molbbbp: 0.889330 test loss: 0.564089
[Epoch 77; Iter    20/   55] train: loss: 0.0455142
[Epoch 77; Iter    50/   55] train: loss: 0.0052773
[Epoch 77] ogbg-molbbbp: 0.931235 val loss: 0.469792
[Epoch 77] ogbg-molbbbp: 0.893151 test loss: 0.561807
[Epoch 78; Iter    25/   55] train: loss: 0.0221942
[Epoch 78; Iter    55/   55] train: loss: 0.0866602
[Epoch 78] ogbg-molbbbp: 0.928025 val loss: 0.521491
[Epoch 78] ogbg-molbbbp: 0.879924 test loss: 0.689945
[Epoch 79; Iter    30/   55] train: loss: 0.0038229
[Epoch 79] ogbg-molbbbp: 0.903333 val loss: 0.469687
[Epoch 79] ogbg-molbbbp: 0.875073 test loss: 0.555108
[Epoch 80; Iter     5/   55] train: loss: 0.0138582
[Epoch 80; Iter    35/   55] train: loss: 0.0126430
[Epoch 80] ogbg-molbbbp: 0.919012 val loss: 0.474169
[Epoch 80] ogbg-molbbbp: 0.874045 test loss: 0.589999
[Epoch 81; Iter    10/   55] train: loss: 0.0065770
[Epoch 81; Iter    40/   55] train: loss: 0.0218596
[Epoch 81] ogbg-molbbbp: 0.909753 val loss: 0.524464
[Epoch 81] ogbg-molbbbp: 0.904174 test loss: 0.570137
[Epoch 82; Iter    15/   55] train: loss: 0.0130476
[Epoch 82; Iter    45/   55] train: loss: 0.0221513
[Epoch 82] ogbg-molbbbp: 0.928395 val loss: 0.490805
[Epoch 82] ogbg-molbbbp: 0.878748 test loss: 0.678545
[Epoch 83; Iter    20/   55] train: loss: 0.0610977
[Epoch 83; Iter    50/   55] train: loss: 0.0082553
[Epoch 83] ogbg-molbbbp: 0.903210 val loss: 0.545191
[Epoch 83] ogbg-molbbbp: 0.870370 test loss: 0.684400
[Epoch 84; Iter    25/   55] train: loss: 0.0204091
[Epoch 84; Iter    55/   55] train: loss: 0.1631503
[Epoch 84] ogbg-molbbbp: 0.911975 val loss: 0.489236
[Epoch 84] ogbg-molbbbp: 0.883304 test loss: 0.542653
[Epoch 85; Iter    30/   55] train: loss: 0.0621724
[Epoch 85] ogbg-molbbbp: 0.931728 val loss: 0.479357
[Epoch 85] ogbg-molbbbp: 0.881981 test loss: 0.535574
[Epoch 86; Iter     5/   55] train: loss: 0.0478984
[Epoch 86; Iter    35/   55] train: loss: 0.0380446
[Epoch 86] ogbg-molbbbp: 0.924815 val loss: 0.521953
[Epoch 86] ogbg-molbbbp: 0.883157 test loss: 0.617723
[Epoch 87; Iter    10/   55] train: loss: 0.0087816
[Epoch 87; Iter    40/   55] train: loss: 0.0707796
[Epoch 87] ogbg-molbbbp: 0.923210 val loss: 0.531735
[Epoch 87] ogbg-molbbbp: 0.885362 test loss: 0.600420
[Epoch 88; Iter    15/   55] train: loss: 0.0030753
[Epoch 88; Iter    45/   55] train: loss: 0.0135666
[Epoch 88] ogbg-molbbbp: 0.922469 val loss: 0.557861
[Epoch 88] ogbg-molbbbp: 0.882422 test loss: 0.643122
[Epoch 89; Iter    20/   55] train: loss: 0.0020841
[Epoch 89; Iter    50/   55] train: loss: 0.0092999
[Epoch 89] ogbg-molbbbp: 0.921728 val loss: 0.559576
[Epoch 89] ogbg-molbbbp: 0.884921 test loss: 0.644660
[Epoch 90; Iter    25/   55] train: loss: 0.0046368
[Epoch 90; Iter    55/   55] train: loss: 0.0463579
[Epoch 90] ogbg-molbbbp: 0.920494 val loss: 0.574482
[Epoch 90] ogbg-molbbbp: 0.889183 test loss: 0.621583
[Epoch 91; Iter    30/   55] train: loss: 0.0133185
[Epoch 91] ogbg-molbbbp: 0.921605 val loss: 0.578504
[Epoch 91] ogbg-molbbbp: 0.875955 test loss: 0.661640
[Epoch 92; Iter     5/   55] train: loss: 0.0095271
[Epoch 92; Iter    35/   55] train: loss: 0.0707528
[Epoch 92] ogbg-molbbbp: 0.918148 val loss: 0.569024
[Epoch 92] ogbg-molbbbp: 0.891093 test loss: 0.636790
[Epoch 93; Iter    10/   55] train: loss: 0.0573680
[Epoch 93; Iter    40/   55] train: loss: 0.0025656
[Epoch 93] ogbg-molbbbp: 0.908519 val loss: 0.626256
[Epoch 93] ogbg-molbbbp: 0.886831 test loss: 0.703733
[Epoch 94; Iter    15/   55] train: loss: 0.0439807
[Epoch 94; Iter    45/   55] train: loss: 0.0618673
[Epoch 94] ogbg-molbbbp: 0.918025 val loss: 0.594371
[Epoch 94] ogbg-molbbbp: 0.883304 test loss: 0.662419
[Epoch 95; Iter    20/   55] train: loss: 0.0058280
[Epoch 95; Iter    50/   55] train: loss: 0.0037112
[Epoch 95] ogbg-molbbbp: 0.909506 val loss: 0.580923
[Epoch 95] ogbg-molbbbp: 0.885949 test loss: 0.664459
[Epoch 96; Iter    25/   55] train: loss: 0.0047148
[Epoch 96; Iter    55/   55] train: loss: 0.0012023
[Epoch 96] ogbg-molbbbp: 0.912963 val loss: 0.580794
[Epoch 96] ogbg-molbbbp: 0.887419 test loss: 0.663497
[Epoch 97; Iter    30/   55] train: loss: 0.0059525
[Epoch 97] ogbg-molbbbp: 0.918889 val loss: 0.553668
[Epoch 97] ogbg-molbbbp: 0.877278 test loss: 0.646036
[Epoch 98; Iter     5/   55] train: loss: 0.0027694
[Epoch 98; Iter    35/   55] train: loss: 0.0030871
[Epoch 98] ogbg-molbbbp: 0.916049 val loss: 0.554895
[Epoch 98] ogbg-molbbbp: 0.884480 test loss: 0.634422
[Epoch 99; Iter    10/   55] train: loss: 0.0024303
[Epoch 99; Iter    40/   55] train: loss: 0.0014371
[Epoch 99] ogbg-molbbbp: 0.911358 val loss: 0.560551
[Epoch 99] ogbg-molbbbp: 0.881687 test loss: 0.628703
[Epoch 100; Iter    15/   55] train: loss: 0.0010472
[Epoch 100; Iter    45/   55] train: loss: 0.0099718
[Epoch 100] ogbg-molbbbp: 0.909259 val loss: 0.591092
[Epoch 100] ogbg-molbbbp: 0.880218 test loss: 0.690016
[Epoch 101; Iter    20/   55] train: loss: 0.0371741
[Epoch 101; Iter    50/   55] train: loss: 0.0022049
[Epoch 101] ogbg-molbbbp: 0.920494 val loss: 0.558784
[Epoch 101] ogbg-molbbbp: 0.885949 test loss: 0.655796
[Epoch 102; Iter    25/   55] train: loss: 0.0093778
[Epoch 102; Iter    55/   55] train: loss: 0.0128463
[Epoch 102] ogbg-molbbbp: 0.920000 val loss: 0.573535
[Epoch 102] ogbg-molbbbp: 0.891093 test loss: 0.658667
[Epoch 103; Iter    30/   55] train: loss: 0.0010600
[Epoch 103] ogbg-molbbbp: 0.929012 val loss: 0.548079
[Epoch 103] ogbg-molbbbp: 0.887272 test loss: 0.640501
[Epoch 104; Iter     5/   55] train: loss: 0.0022071
[Epoch 104; Iter    35/   55] train: loss: 0.0029043
[Epoch 104] ogbg-molbbbp: 0.917531 val loss: 0.579016
[Epoch 104] ogbg-molbbbp: 0.879042 test loss: 0.673770
[Epoch 105; Iter    10/   55] train: loss: 0.0110719
[Epoch 105; Iter    40/   55] train: loss: 0.0020986
[Epoch 105] ogbg-molbbbp: 0.924815 val loss: 0.586722
[Epoch 105] ogbg-molbbbp: 0.887125 test loss: 0.716301
[Epoch 106; Iter    15/   55] train: loss: 0.0065459
[Epoch 106; Iter    45/   55] train: loss: 0.1219460
[Epoch 106] ogbg-molbbbp: 0.928642 val loss: 0.549537
[Epoch 106] ogbg-molbbbp: 0.891828 test loss: 0.636137
[Epoch 107; Iter    20/   55] train: loss: 0.0015195
[Epoch 107; Iter    50/   55] train: loss: 0.0139586
[Epoch 107] ogbg-molbbbp: 0.916420 val loss: 0.575283
[Epoch 107] ogbg-molbbbp: 0.884333 test loss: 0.652354
[Epoch 108; Iter    25/   55] train: loss: 0.0175910
[Epoch 108; Iter    55/   55] train: loss: 0.0274109
[Epoch 108] ogbg-molbbbp: 0.913457 val loss: 0.585326
[Epoch 108] ogbg-molbbbp: 0.871105 test loss: 0.791312
[Epoch 109; Iter    30/   55] train: loss: 0.0025637
[Epoch 69; Iter    40/   55] train: loss: 0.1112706
[Epoch 69] ogbg-molbbbp: 0.933704 val loss: 0.419328
[Epoch 69] ogbg-molbbbp: 0.893886 test loss: 0.541423
[Epoch 70; Iter    15/   55] train: loss: 0.0133371
[Epoch 70; Iter    45/   55] train: loss: 0.0680782
[Epoch 70] ogbg-molbbbp: 0.924815 val loss: 0.483079
[Epoch 70] ogbg-molbbbp: 0.885362 test loss: 0.525189
[Epoch 71; Iter    20/   55] train: loss: 0.0446079
[Epoch 71; Iter    50/   55] train: loss: 0.0319065
[Epoch 71] ogbg-molbbbp: 0.914815 val loss: 0.474900
[Epoch 71] ogbg-molbbbp: 0.889918 test loss: 0.503988
[Epoch 72; Iter    25/   55] train: loss: 0.0413628
[Epoch 72; Iter    55/   55] train: loss: 0.1510807
[Epoch 72] ogbg-molbbbp: 0.925432 val loss: 0.449420
[Epoch 72] ogbg-molbbbp: 0.875955 test loss: 0.598611
[Epoch 73; Iter    30/   55] train: loss: 0.0366318
[Epoch 73] ogbg-molbbbp: 0.934938 val loss: 0.415536
[Epoch 73] ogbg-molbbbp: 0.892710 test loss: 0.552916
[Epoch 74; Iter     5/   55] train: loss: 0.0044718
[Epoch 74; Iter    35/   55] train: loss: 0.0725327
[Epoch 74] ogbg-molbbbp: 0.945185 val loss: 0.340809
[Epoch 74] ogbg-molbbbp: 0.890947 test loss: 0.577672
[Epoch 75; Iter    10/   55] train: loss: 0.0120662
[Epoch 75; Iter    40/   55] train: loss: 0.0319704
[Epoch 75] ogbg-molbbbp: 0.922222 val loss: 0.488371
[Epoch 75] ogbg-molbbbp: 0.889624 test loss: 0.509599
[Epoch 76; Iter    15/   55] train: loss: 0.0379321
[Epoch 76; Iter    45/   55] train: loss: 0.0102346
[Epoch 76] ogbg-molbbbp: 0.922963 val loss: 0.450546
[Epoch 76] ogbg-molbbbp: 0.896678 test loss: 0.571819
[Epoch 77; Iter    20/   55] train: loss: 0.0101986
[Epoch 77; Iter    50/   55] train: loss: 0.0405321
[Epoch 77] ogbg-molbbbp: 0.922469 val loss: 0.445393
[Epoch 77] ogbg-molbbbp: 0.899471 test loss: 0.458250
[Epoch 78; Iter    25/   55] train: loss: 0.0368432
[Epoch 78; Iter    55/   55] train: loss: 0.0032534
[Epoch 78] ogbg-molbbbp: 0.926420 val loss: 0.470417
[Epoch 78] ogbg-molbbbp: 0.905791 test loss: 0.611434
[Epoch 79; Iter    30/   55] train: loss: 0.0414937
[Epoch 79] ogbg-molbbbp: 0.911852 val loss: 0.509016
[Epoch 79] ogbg-molbbbp: 0.907260 test loss: 0.481997
[Epoch 80; Iter     5/   55] train: loss: 0.0592443
[Epoch 80; Iter    35/   55] train: loss: 0.0120402
[Epoch 80] ogbg-molbbbp: 0.912840 val loss: 0.487208
[Epoch 80] ogbg-molbbbp: 0.891975 test loss: 0.587512
[Epoch 81; Iter    10/   55] train: loss: 0.0195233
[Epoch 81; Iter    40/   55] train: loss: 0.0097870
[Epoch 81] ogbg-molbbbp: 0.896543 val loss: 0.511263
[Epoch 81] ogbg-molbbbp: 0.869048 test loss: 0.541200
[Epoch 82; Iter    15/   55] train: loss: 0.0279053
[Epoch 82; Iter    45/   55] train: loss: 0.0208458
[Epoch 82] ogbg-molbbbp: 0.922840 val loss: 0.483758
[Epoch 82] ogbg-molbbbp: 0.876984 test loss: 0.565375
[Epoch 83; Iter    20/   55] train: loss: 0.0074328
[Epoch 83; Iter    50/   55] train: loss: 0.0097701
[Epoch 83] ogbg-molbbbp: 0.922346 val loss: 0.481806
[Epoch 83] ogbg-molbbbp: 0.884774 test loss: 0.606043
[Epoch 84; Iter    25/   55] train: loss: 0.0071903
[Epoch 84; Iter    55/   55] train: loss: 0.0032282
[Epoch 84] ogbg-molbbbp: 0.926667 val loss: 0.511780
[Epoch 84] ogbg-molbbbp: 0.889918 test loss: 0.577471
[Epoch 85; Iter    30/   55] train: loss: 0.0057855
[Epoch 85] ogbg-molbbbp: 0.917407 val loss: 0.526489
[Epoch 85] ogbg-molbbbp: 0.887125 test loss: 0.632493
[Epoch 86; Iter     5/   55] train: loss: 0.0053545
[Epoch 86; Iter    35/   55] train: loss: 0.0297083
[Epoch 86] ogbg-molbbbp: 0.911852 val loss: 0.499768
[Epoch 86] ogbg-molbbbp: 0.868313 test loss: 0.616025
[Epoch 87; Iter    10/   55] train: loss: 0.0663986
[Epoch 87; Iter    40/   55] train: loss: 0.1280233
[Epoch 87] ogbg-molbbbp: 0.906914 val loss: 0.633475
[Epoch 87] ogbg-molbbbp: 0.885949 test loss: 0.749649
[Epoch 88; Iter    15/   55] train: loss: 0.0090760
[Epoch 88; Iter    45/   55] train: loss: 0.1765932
[Epoch 88] ogbg-molbbbp: 0.882840 val loss: 0.604842
[Epoch 88] ogbg-molbbbp: 0.867137 test loss: 0.622850
[Epoch 89; Iter    20/   55] train: loss: 0.0086135
[Epoch 89; Iter    50/   55] train: loss: 0.0109327
[Epoch 89] ogbg-molbbbp: 0.924691 val loss: 0.441295
[Epoch 89] ogbg-molbbbp: 0.902998 test loss: 0.566851
[Epoch 90; Iter    25/   55] train: loss: 0.1773913
[Epoch 90; Iter    55/   55] train: loss: 0.0115385
[Epoch 90] ogbg-molbbbp: 0.932963 val loss: 0.443160
[Epoch 90] ogbg-molbbbp: 0.867284 test loss: 0.563390
[Epoch 91; Iter    30/   55] train: loss: 0.0096861
[Epoch 91] ogbg-molbbbp: 0.924321 val loss: 0.426878
[Epoch 91] ogbg-molbbbp: 0.893739 test loss: 0.522099
[Epoch 92; Iter     5/   55] train: loss: 0.1264385
[Epoch 92; Iter    35/   55] train: loss: 0.0021885
[Epoch 92] ogbg-molbbbp: 0.929383 val loss: 0.473786
[Epoch 92] ogbg-molbbbp: 0.898883 test loss: 0.577388
[Epoch 93; Iter    10/   55] train: loss: 0.0737539
[Epoch 93; Iter    40/   55] train: loss: 0.0057836
[Epoch 93] ogbg-molbbbp: 0.921358 val loss: 0.501726
[Epoch 93] ogbg-molbbbp: 0.879483 test loss: 0.669687
[Epoch 94; Iter    15/   55] train: loss: 0.2216465
[Epoch 94; Iter    45/   55] train: loss: 0.0140855
[Epoch 94] ogbg-molbbbp: 0.933704 val loss: 0.441141
[Epoch 94] ogbg-molbbbp: 0.897266 test loss: 0.601762
[Epoch 95; Iter    20/   55] train: loss: 0.0373321
[Epoch 95; Iter    50/   55] train: loss: 0.0212404
[Epoch 95] ogbg-molbbbp: 0.935926 val loss: 0.469283
[Epoch 95] ogbg-molbbbp: 0.889183 test loss: 0.685688
[Epoch 96; Iter    25/   55] train: loss: 0.0343150
[Epoch 96; Iter    55/   55] train: loss: 0.0050854
[Epoch 96] ogbg-molbbbp: 0.925309 val loss: 0.474021
[Epoch 96] ogbg-molbbbp: 0.892416 test loss: 0.555091
[Epoch 97; Iter    30/   55] train: loss: 0.0449546
[Epoch 97] ogbg-molbbbp: 0.926420 val loss: 0.520364
[Epoch 97] ogbg-molbbbp: 0.889624 test loss: 0.632097
[Epoch 98; Iter     5/   55] train: loss: 0.0311510
[Epoch 98; Iter    35/   55] train: loss: 0.0022589
[Epoch 98] ogbg-molbbbp: 0.931358 val loss: 0.477995
[Epoch 98] ogbg-molbbbp: 0.902116 test loss: 0.566835
[Epoch 99; Iter    10/   55] train: loss: 0.2542477
[Epoch 99; Iter    40/   55] train: loss: 0.0100143
[Epoch 99] ogbg-molbbbp: 0.926914 val loss: 0.454656
[Epoch 99] ogbg-molbbbp: 0.892416 test loss: 0.645119
[Epoch 100; Iter    15/   55] train: loss: 0.0136563
[Epoch 100; Iter    45/   55] train: loss: 0.0049716
[Epoch 100] ogbg-molbbbp: 0.929753 val loss: 0.488455
[Epoch 100] ogbg-molbbbp: 0.903145 test loss: 0.726679
[Epoch 101; Iter    20/   55] train: loss: 0.0103259
[Epoch 101; Iter    50/   55] train: loss: 0.0070409
[Epoch 101] ogbg-molbbbp: 0.926790 val loss: 0.534128
[Epoch 101] ogbg-molbbbp: 0.896825 test loss: 0.646594
[Epoch 102; Iter    25/   55] train: loss: 0.0055286
[Epoch 102; Iter    55/   55] train: loss: 0.0515295
[Epoch 102] ogbg-molbbbp: 0.925062 val loss: 0.562946
[Epoch 102] ogbg-molbbbp: 0.911817 test loss: 0.570927
[Epoch 103; Iter    30/   55] train: loss: 0.0115651
[Epoch 103] ogbg-molbbbp: 0.923086 val loss: 0.508176
[Epoch 103] ogbg-molbbbp: 0.885215 test loss: 0.613563
[Epoch 104; Iter     5/   55] train: loss: 0.0426843
[Epoch 104; Iter    35/   55] train: loss: 0.0951870
[Epoch 104] ogbg-molbbbp: 0.920123 val loss: 0.468694
[Epoch 104] ogbg-molbbbp: 0.895944 test loss: 0.600155
[Epoch 105; Iter    10/   55] train: loss: 0.0090446
[Epoch 105; Iter    40/   55] train: loss: 0.1211519
[Epoch 105] ogbg-molbbbp: 0.929753 val loss: 0.468472
[Epoch 105] ogbg-molbbbp: 0.882863 test loss: 0.631454
[Epoch 106; Iter    15/   55] train: loss: 0.0284163
[Epoch 106; Iter    45/   55] train: loss: 0.0070156
[Epoch 106] ogbg-molbbbp: 0.932099 val loss: 0.440229
[Epoch 106] ogbg-molbbbp: 0.894327 test loss: 0.555594
[Epoch 107; Iter    20/   55] train: loss: 0.0036970
[Epoch 107; Iter    50/   55] train: loss: 0.0111924
[Epoch 107] ogbg-molbbbp: 0.929877 val loss: 0.501770
[Epoch 107] ogbg-molbbbp: 0.915050 test loss: 0.527885
[Epoch 108; Iter    25/   55] train: loss: 0.0045246
[Epoch 108; Iter    55/   55] train: loss: 0.0087325
[Epoch 108] ogbg-molbbbp: 0.931235 val loss: 0.536189
[Epoch 108] ogbg-molbbbp: 0.920929 test loss: 0.593951
[Epoch 109; Iter    30/   55] train: loss: 0.0658821
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbbbp: 0.920362 val loss: 0.401396
[Epoch 78] ogbg-molbbbp: 0.887553 test loss: 0.546745
[Epoch 79; Iter    12/   41] train: loss: 0.0113822
[Epoch 79] ogbg-molbbbp: 0.926984 val loss: 0.426586
[Epoch 79] ogbg-molbbbp: 0.892829 test loss: 0.599899
[Epoch 80; Iter     1/   41] train: loss: 0.0080643
[Epoch 80; Iter    31/   41] train: loss: 0.0039399
[Epoch 80] ogbg-molbbbp: 0.920976 val loss: 0.410527
[Epoch 80] ogbg-molbbbp: 0.880175 test loss: 0.595907
[Epoch 81; Iter    20/   41] train: loss: 0.0176300
[Epoch 81] ogbg-molbbbp: 0.925073 val loss: 0.392623
[Epoch 81] ogbg-molbbbp: 0.882479 test loss: 0.579994
[Epoch 82; Iter     9/   41] train: loss: 0.0136911
[Epoch 82; Iter    39/   41] train: loss: 0.0057484
[Epoch 82] ogbg-molbbbp: 0.928827 val loss: 0.371710
[Epoch 82] ogbg-molbbbp: 0.879374 test loss: 0.575973
[Epoch 83; Iter    28/   41] train: loss: 0.0041850
[Epoch 83] ogbg-molbbbp: 0.923639 val loss: 0.429236
[Epoch 83] ogbg-molbbbp: 0.890024 test loss: 0.582749
[Epoch 84; Iter    17/   41] train: loss: 0.1052389
[Epoch 84] ogbg-molbbbp: 0.923024 val loss: 0.450564
[Epoch 84] ogbg-molbbbp: 0.885517 test loss: 0.618969
[Epoch 85; Iter     6/   41] train: loss: 0.0609745
[Epoch 85; Iter    36/   41] train: loss: 0.0217455
[Epoch 85] ogbg-molbbbp: 0.921113 val loss: 0.407476
[Epoch 85] ogbg-molbbbp: 0.887553 test loss: 0.584466
[Epoch 86; Iter    25/   41] train: loss: 0.0033247
[Epoch 86] ogbg-molbbbp: 0.917631 val loss: 0.451479
[Epoch 86] ogbg-molbbbp: 0.881577 test loss: 0.609653
[Epoch 87; Iter    14/   41] train: loss: 0.0078876
[Epoch 87] ogbg-molbbbp: 0.913774 val loss: 0.492668
[Epoch 87] ogbg-molbbbp: 0.884649 test loss: 0.656387
[Epoch 88; Iter     3/   41] train: loss: 0.0040771
[Epoch 88; Iter    33/   41] train: loss: 0.0074197
[Epoch 88] ogbg-molbbbp: 0.922581 val loss: 0.430415
[Epoch 88] ogbg-molbbbp: 0.888522 test loss: 0.619088
[Epoch 89; Iter    22/   41] train: loss: 0.0211704
[Epoch 89] ogbg-molbbbp: 0.910497 val loss: 0.437788
[Epoch 89] ogbg-molbbbp: 0.887153 test loss: 0.575120
[Epoch 90; Iter    11/   41] train: loss: 0.0182584
[Epoch 90; Iter    41/   41] train: loss: 0.0064762
[Epoch 90] ogbg-molbbbp: 0.921079 val loss: 0.425160
[Epoch 90] ogbg-molbbbp: 0.886919 test loss: 0.597671
[Epoch 91; Iter    30/   41] train: loss: 0.0047233
[Epoch 91] ogbg-molbbbp: 0.928281 val loss: 0.375605
[Epoch 91] ogbg-molbbbp: 0.899339 test loss: 0.537188
[Epoch 92; Iter    19/   41] train: loss: 0.0084413
[Epoch 92] ogbg-molbbbp: 0.922547 val loss: 0.418355
[Epoch 92] ogbg-molbbbp: 0.887787 test loss: 0.583251
[Epoch 93; Iter     8/   41] train: loss: 0.0204336
[Epoch 93; Iter    38/   41] train: loss: 0.0695374
[Epoch 93] ogbg-molbbbp: 0.928111 val loss: 0.447957
[Epoch 93] ogbg-molbbbp: 0.887086 test loss: 0.662099
[Epoch 94; Iter    27/   41] train: loss: 0.0019027
[Epoch 94] ogbg-molbbbp: 0.919406 val loss: 0.487410
[Epoch 94] ogbg-molbbbp: 0.891960 test loss: 0.648850
[Epoch 95; Iter    16/   41] train: loss: 0.0114761
[Epoch 95] ogbg-molbbbp: 0.923741 val loss: 0.439189
[Epoch 95] ogbg-molbbbp: 0.890625 test loss: 0.604039
[Epoch 96; Iter     5/   41] train: loss: 0.0106524
[Epoch 96; Iter    35/   41] train: loss: 0.0179225
[Epoch 96] ogbg-molbbbp: 0.920157 val loss: 0.428907
[Epoch 96] ogbg-molbbbp: 0.887253 test loss: 0.575315
[Epoch 97; Iter    24/   41] train: loss: 0.0076764
[Epoch 97] ogbg-molbbbp: 0.920362 val loss: 0.472551
[Epoch 97] ogbg-molbbbp: 0.875501 test loss: 0.651443
[Epoch 98; Iter    13/   41] train: loss: 0.0091517
[Epoch 98] ogbg-molbbbp: 0.920362 val loss: 0.481423
[Epoch 98] ogbg-molbbbp: 0.883180 test loss: 0.640537
[Epoch 99; Iter     2/   41] train: loss: 0.0015187
[Epoch 99; Iter    32/   41] train: loss: 0.0272535
[Epoch 99] ogbg-molbbbp: 0.924117 val loss: 0.464242
[Epoch 99] ogbg-molbbbp: 0.885550 test loss: 0.648317
[Epoch 100; Iter    21/   41] train: loss: 0.0034894
[Epoch 100] ogbg-molbbbp: 0.922069 val loss: 0.465865
[Epoch 100] ogbg-molbbbp: 0.886351 test loss: 0.634552
[Epoch 101; Iter    10/   41] train: loss: 0.0045959
[Epoch 101; Iter    40/   41] train: loss: 0.0195576
[Epoch 101] ogbg-molbbbp: 0.924526 val loss: 0.454732
[Epoch 101] ogbg-molbbbp: 0.886485 test loss: 0.625618
[Epoch 102; Iter    29/   41] train: loss: 0.1015700
[Epoch 102] ogbg-molbbbp: 0.924458 val loss: 0.472092
[Epoch 102] ogbg-molbbbp: 0.875868 test loss: 0.680998
[Epoch 103; Iter    18/   41] train: loss: 0.0052690
[Epoch 103] ogbg-molbbbp: 0.923161 val loss: 0.454343
[Epoch 103] ogbg-molbbbp: 0.871494 test loss: 0.652038
[Epoch 104; Iter     7/   41] train: loss: 0.0073930
[Epoch 104; Iter    37/   41] train: loss: 0.0453928
[Epoch 104] ogbg-molbbbp: 0.926131 val loss: 0.455768
[Epoch 104] ogbg-molbbbp: 0.879440 test loss: 0.653948
[Epoch 105; Iter    26/   41] train: loss: 0.0056468
[Epoch 105] ogbg-molbbbp: 0.924287 val loss: 0.483966
[Epoch 105] ogbg-molbbbp: 0.876536 test loss: 0.688294
[Epoch 106; Iter    15/   41] train: loss: 0.0125393
[Epoch 106] ogbg-molbbbp: 0.921932 val loss: 0.457051
[Epoch 106] ogbg-molbbbp: 0.870927 test loss: 0.645129
[Epoch 107; Iter     4/   41] train: loss: 0.0406403
[Epoch 107; Iter    34/   41] train: loss: 0.0044970
[Epoch 107] ogbg-molbbbp: 0.923878 val loss: 0.493891
[Epoch 107] ogbg-molbbbp: 0.886518 test loss: 0.671328
[Epoch 108; Iter    23/   41] train: loss: 0.0638990
[Epoch 108] ogbg-molbbbp: 0.921898 val loss: 0.459911
[Epoch 108] ogbg-molbbbp: 0.878138 test loss: 0.637594
[Epoch 109; Iter    12/   41] train: loss: 0.0056038
[Epoch 109] ogbg-molbbbp: 0.923502 val loss: 0.486935
[Epoch 109] ogbg-molbbbp: 0.875200 test loss: 0.700325
[Epoch 110; Iter     1/   41] train: loss: 0.0016944
[Epoch 110; Iter    31/   41] train: loss: 0.0073304
[Epoch 110] ogbg-molbbbp: 0.923093 val loss: 0.487057
[Epoch 110] ogbg-molbbbp: 0.880909 test loss: 0.696088
[Epoch 111; Iter    20/   41] train: loss: 0.0049564
[Epoch 111] ogbg-molbbbp: 0.920055 val loss: 0.490695
[Epoch 111] ogbg-molbbbp: 0.882445 test loss: 0.664134
[Epoch 112; Iter     9/   41] train: loss: 0.0151130
[Epoch 112; Iter    39/   41] train: loss: 0.0027911
[Epoch 112] ogbg-molbbbp: 0.921010 val loss: 0.477875
[Epoch 112] ogbg-molbbbp: 0.880642 test loss: 0.670631
[Epoch 113; Iter    28/   41] train: loss: 0.0015074
[Epoch 113] ogbg-molbbbp: 0.921830 val loss: 0.526428
[Epoch 113] ogbg-molbbbp: 0.880442 test loss: 0.741642
[Epoch 114; Iter    17/   41] train: loss: 0.0100210
[Epoch 114] ogbg-molbbbp: 0.922205 val loss: 0.480444
[Epoch 114] ogbg-molbbbp: 0.890491 test loss: 0.648755
[Epoch 115; Iter     6/   41] train: loss: 0.0032977
[Epoch 115; Iter    36/   41] train: loss: 0.0085600
[Epoch 115] ogbg-molbbbp: 0.922547 val loss: 0.505030
[Epoch 115] ogbg-molbbbp: 0.882312 test loss: 0.696348
[Epoch 116; Iter    25/   41] train: loss: 0.0071965
[Epoch 116] ogbg-molbbbp: 0.921488 val loss: 0.470950
[Epoch 116] ogbg-molbbbp: 0.877103 test loss: 0.667163
[Epoch 117; Iter    14/   41] train: loss: 0.0049550
[Epoch 117] ogbg-molbbbp: 0.923775 val loss: 0.483773
[Epoch 117] ogbg-molbbbp: 0.875434 test loss: 0.710674
[Epoch 118; Iter     3/   41] train: loss: 0.0105931
[Epoch 118; Iter    33/   41] train: loss: 0.0683467
[Epoch 118] ogbg-molbbbp: 0.926984 val loss: 0.468689
[Epoch 118] ogbg-molbbbp: 0.883747 test loss: 0.668451
[Epoch 119; Iter    22/   41] train: loss: 0.0041041
[Epoch 119] ogbg-molbbbp: 0.922069 val loss: 0.528330
[Epoch 119] ogbg-molbbbp: 0.868690 test loss: 0.758458
[Epoch 120; Iter    11/   41] train: loss: 0.0554582
[Epoch 120; Iter    41/   41] train: loss: 0.0056000
[Epoch 120] ogbg-molbbbp: 0.920635 val loss: 0.505750
[Epoch 120] ogbg-molbbbp: 0.869858 test loss: 0.761979
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 43.
Statistics on  val_best_checkpoint
mean_pred: 2.144578695297241
std_pred: 3.5341439247131348
mean_targets: 0.7720588445663452
std_targets: 0.4200195074081421
prcauc: 0.9736789580538057
rocauc: 0.9308414405188599
ogbg-molbbbp: 0.9308414405188599
BCEWithLogitsLoss: 0.29798649517553194
Statistics on  test
mean_pred: 2.1324822902679443
std_pred: 3.4764318466186523
mean_targets: 0.7647058963775635
std_targets: 0.42470329999923706
prcauc: 0.9751018527903912
rocauc: 0.9242788461538461
ogbg-molbbbp: 0.9242788461538461
BCEWithLogitsLoss: 0.30981701985001564
Statistics on  train
mean_pred: 2.2913146018981934
std_pred: 3.4013895988464355
mean_targets: 0.7628781199455261
std_targets: 0.425491601228714
prcauc: 0.9928716340903029
rocauc: 0.9780241711941456
ogbg-molbbbp: 0.9780241711941456
BCEWithLogitsLoss: 0.16612666218382557
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 73] ogbg-molbbbp: 0.870795 test loss: 0.495249
[Epoch 74; Iter     6/   48] train: loss: 0.1899540
[Epoch 74; Iter    36/   48] train: loss: 0.0793952
[Epoch 74] ogbg-molbbbp: 0.901151 val loss: 0.508987
[Epoch 74] ogbg-molbbbp: 0.866326 test loss: 0.577624
[Epoch 75; Iter    18/   48] train: loss: 0.0754984
[Epoch 75; Iter    48/   48] train: loss: 0.0532337
[Epoch 75] ogbg-molbbbp: 0.917031 val loss: 0.394389
[Epoch 75] ogbg-molbbbp: 0.864475 test loss: 0.532461
[Epoch 76; Iter    30/   48] train: loss: 0.1030682
[Epoch 76] ogbg-molbbbp: 0.908694 val loss: 0.410203
[Epoch 76] ogbg-molbbbp: 0.853623 test loss: 0.545439
[Epoch 77; Iter    12/   48] train: loss: 0.0392125
[Epoch 77; Iter    42/   48] train: loss: 0.0960271
[Epoch 77] ogbg-molbbbp: 0.914252 val loss: 0.367405
[Epoch 77] ogbg-molbbbp: 0.877306 test loss: 0.512850
[Epoch 78; Iter    24/   48] train: loss: 0.0246562
[Epoch 78] ogbg-molbbbp: 0.919923 val loss: 0.403400
[Epoch 78] ogbg-molbbbp: 0.855793 test loss: 0.588866
[Epoch 79; Iter     6/   48] train: loss: 0.0514511
[Epoch 79; Iter    36/   48] train: loss: 0.1471196
[Epoch 79] ogbg-molbbbp: 0.935802 val loss: 0.332605
[Epoch 79] ogbg-molbbbp: 0.881711 test loss: 0.509219
[Epoch 80; Iter    18/   48] train: loss: 0.0893288
[Epoch 80; Iter    48/   48] train: loss: 0.0168711
[Epoch 80] ogbg-molbbbp: 0.918335 val loss: 0.438983
[Epoch 80] ogbg-molbbbp: 0.861602 test loss: 0.619444
[Epoch 81; Iter    30/   48] train: loss: 0.0117961
[Epoch 81] ogbg-molbbbp: 0.926898 val loss: 0.368736
[Epoch 81] ogbg-molbbbp: 0.865943 test loss: 0.570177
[Epoch 82; Iter    12/   48] train: loss: 0.0096128
[Epoch 82; Iter    42/   48] train: loss: 0.0141412
[Epoch 82] ogbg-molbbbp: 0.917598 val loss: 0.428147
[Epoch 82] ogbg-molbbbp: 0.873029 test loss: 0.635276
[Epoch 83; Iter    24/   48] train: loss: 0.0077931
[Epoch 83] ogbg-molbbbp: 0.917484 val loss: 0.407275
[Epoch 83] ogbg-molbbbp: 0.882541 test loss: 0.600148
[Epoch 84; Iter     6/   48] train: loss: 0.0304286
[Epoch 84; Iter    36/   48] train: loss: 0.0227858
[Epoch 84] ogbg-molbbbp: 0.916180 val loss: 0.434050
[Epoch 84] ogbg-molbbbp: 0.878711 test loss: 0.651758
[Epoch 85; Iter    18/   48] train: loss: 0.0180805
[Epoch 85; Iter    48/   48] train: loss: 0.0199356
[Epoch 85] ogbg-molbbbp: 0.916010 val loss: 0.443429
[Epoch 85] ogbg-molbbbp: 0.875966 test loss: 0.637420
[Epoch 86; Iter    30/   48] train: loss: 0.0073075
[Epoch 86] ogbg-molbbbp: 0.923155 val loss: 0.401873
[Epoch 86] ogbg-molbbbp: 0.870922 test loss: 0.618341
[Epoch 87; Iter    12/   48] train: loss: 0.0065731
[Epoch 87; Iter    42/   48] train: loss: 0.0839461
[Epoch 87] ogbg-molbbbp: 0.923496 val loss: 0.422718
[Epoch 87] ogbg-molbbbp: 0.869071 test loss: 0.667220
[Epoch 88; Iter    24/   48] train: loss: 0.0098327
[Epoch 88] ogbg-molbbbp: 0.920547 val loss: 0.412674
[Epoch 88] ogbg-molbbbp: 0.863071 test loss: 0.659543
[Epoch 89; Iter     6/   48] train: loss: 0.0043440
[Epoch 89; Iter    36/   48] train: loss: 0.0063523
[Epoch 89] ogbg-molbbbp: 0.927182 val loss: 0.386134
[Epoch 89] ogbg-molbbbp: 0.855474 test loss: 0.671364
[Epoch 90; Iter    18/   48] train: loss: 0.0060058
[Epoch 90; Iter    48/   48] train: loss: 0.0128669
[Epoch 90] ogbg-molbbbp: 0.909091 val loss: 0.455172
[Epoch 90] ogbg-molbbbp: 0.856495 test loss: 0.663113
[Epoch 91; Iter    30/   48] train: loss: 0.0590255
[Epoch 91] ogbg-molbbbp: 0.921057 val loss: 0.429044
[Epoch 91] ogbg-molbbbp: 0.857581 test loss: 0.674505
[Epoch 92; Iter    12/   48] train: loss: 0.1693450
[Epoch 92; Iter    42/   48] train: loss: 0.0041855
[Epoch 92] ogbg-molbbbp: 0.924573 val loss: 0.430271
[Epoch 92] ogbg-molbbbp: 0.859560 test loss: 0.673313
[Epoch 93; Iter    24/   48] train: loss: 0.0072744
[Epoch 93] ogbg-molbbbp: 0.934781 val loss: 0.413866
[Epoch 93] ogbg-molbbbp: 0.870284 test loss: 0.705088
[Epoch 94; Iter     6/   48] train: loss: 0.0694088
[Epoch 94; Iter    36/   48] train: loss: 0.0049651
[Epoch 94] ogbg-molbbbp: 0.931549 val loss: 0.415590
[Epoch 94] ogbg-molbbbp: 0.875199 test loss: 0.619620
[Epoch 95; Iter    18/   48] train: loss: 0.0084300
[Epoch 95; Iter    48/   48] train: loss: 0.0945677
[Epoch 95] ogbg-molbbbp: 0.930018 val loss: 0.401231
[Epoch 95] ogbg-molbbbp: 0.865688 test loss: 0.628368
[Epoch 96; Iter    30/   48] train: loss: 0.0064312
[Epoch 96] ogbg-molbbbp: 0.921964 val loss: 0.449492
[Epoch 96] ogbg-molbbbp: 0.851005 test loss: 0.722215
[Epoch 97; Iter    12/   48] train: loss: 0.7702793
[Epoch 97; Iter    42/   48] train: loss: 0.0085026
[Epoch 97] ogbg-molbbbp: 0.920433 val loss: 0.439589
[Epoch 97] ogbg-molbbbp: 0.856623 test loss: 0.672046
[Epoch 98; Iter    24/   48] train: loss: 0.1356559
[Epoch 98] ogbg-molbbbp: 0.921114 val loss: 0.427186
[Epoch 98] ogbg-molbbbp: 0.836323 test loss: 0.696876
[Epoch 99; Iter     6/   48] train: loss: 0.0509411
[Epoch 99; Iter    36/   48] train: loss: 0.1602782
[Epoch 99] ogbg-molbbbp: 0.918392 val loss: 0.473563
[Epoch 99] ogbg-molbbbp: 0.871114 test loss: 0.707623
[Epoch 100; Iter    18/   48] train: loss: 0.0607280
[Epoch 100; Iter    48/   48] train: loss: 0.0209513
[Epoch 100] ogbg-molbbbp: 0.914819 val loss: 0.463453
[Epoch 100] ogbg-molbbbp: 0.846728 test loss: 0.690804
[Epoch 101; Iter    30/   48] train: loss: 0.0096279
[Epoch 101] ogbg-molbbbp: 0.930755 val loss: 0.438957
[Epoch 101] ogbg-molbbbp: 0.853559 test loss: 0.708003
[Epoch 102; Iter    12/   48] train: loss: 0.0025295
[Epoch 102; Iter    42/   48] train: loss: 0.0093058
[Epoch 102] ogbg-molbbbp: 0.917825 val loss: 0.460256
[Epoch 102] ogbg-molbbbp: 0.865879 test loss: 0.705922
[Epoch 103; Iter    24/   48] train: loss: 0.0074698
[Epoch 103] ogbg-molbbbp: 0.922305 val loss: 0.437967
[Epoch 103] ogbg-molbbbp: 0.861347 test loss: 0.713626
[Epoch 104; Iter     6/   48] train: loss: 0.0018243
[Epoch 104; Iter    36/   48] train: loss: 0.0049363
[Epoch 104] ogbg-molbbbp: 0.919299 val loss: 0.452091
[Epoch 104] ogbg-molbbbp: 0.846090 test loss: 0.761962
[Epoch 105; Iter    18/   48] train: loss: 0.0021322
[Epoch 105; Iter    48/   48] train: loss: 0.0020532
[Epoch 105] ogbg-molbbbp: 0.918335 val loss: 0.469321
[Epoch 105] ogbg-molbbbp: 0.863390 test loss: 0.721293
[Epoch 106; Iter    30/   48] train: loss: 0.0039758
[Epoch 106] ogbg-molbbbp: 0.915386 val loss: 0.537451
[Epoch 106] ogbg-molbbbp: 0.850622 test loss: 0.833369
[Epoch 107; Iter    12/   48] train: loss: 0.0018704
[Epoch 107; Iter    42/   48] train: loss: 0.0029056
[Epoch 107] ogbg-molbbbp: 0.920887 val loss: 0.500011
[Epoch 107] ogbg-molbbbp: 0.852154 test loss: 0.779273
[Epoch 108; Iter    24/   48] train: loss: 0.0129835
[Epoch 108] ogbg-molbbbp: 0.918278 val loss: 0.477365
[Epoch 108] ogbg-molbbbp: 0.853559 test loss: 0.732907
[Epoch 109; Iter     6/   48] train: loss: 0.0006418
[Epoch 109; Iter    36/   48] train: loss: 0.0019449
[Epoch 109] ogbg-molbbbp: 0.915613 val loss: 0.505116
[Epoch 109] ogbg-molbbbp: 0.846090 test loss: 0.770658
[Epoch 110; Iter    18/   48] train: loss: 0.0006070
[Epoch 110; Iter    48/   48] train: loss: 0.0003480
[Epoch 110] ogbg-molbbbp: 0.917428 val loss: 0.490616
[Epoch 110] ogbg-molbbbp: 0.847622 test loss: 0.766336
[Epoch 111; Iter    30/   48] train: loss: 0.0011859
[Epoch 111] ogbg-molbbbp: 0.916917 val loss: 0.489587
[Epoch 111] ogbg-molbbbp: 0.848516 test loss: 0.743189
[Epoch 112; Iter    12/   48] train: loss: 0.0011379
[Epoch 112; Iter    42/   48] train: loss: 0.0008023
[Epoch 112] ogbg-molbbbp: 0.918505 val loss: 0.494473
[Epoch 112] ogbg-molbbbp: 0.843217 test loss: 0.761132
[Epoch 113; Iter    24/   48] train: loss: 0.0058571
[Epoch 113] ogbg-molbbbp: 0.919186 val loss: 0.492718
[Epoch 113] ogbg-molbbbp: 0.849473 test loss: 0.761297
[Epoch 114; Iter     6/   48] train: loss: 0.0020099
[Epoch 114; Iter    36/   48] train: loss: 0.0008608
[Epoch 114] ogbg-molbbbp: 0.920093 val loss: 0.498156
[Epoch 114] ogbg-molbbbp: 0.847494 test loss: 0.793133
[Epoch 115; Iter    18/   48] train: loss: 0.0033850
[Epoch 115; Iter    48/   48] train: loss: 0.0023990
[Epoch 115] ogbg-molbbbp: 0.918222 val loss: 0.507287
[Epoch 115] ogbg-molbbbp: 0.846984 test loss: 0.792689
[Epoch 69; Iter    40/   55] train: loss: 0.0254438
[Epoch 69] ogbg-molbbbp: 0.944444 val loss: 0.329300
[Epoch 69] ogbg-molbbbp: 0.905203 test loss: 0.580597
[Epoch 70; Iter    15/   55] train: loss: 0.1165015
[Epoch 70; Iter    45/   55] train: loss: 0.0047600
[Epoch 70] ogbg-molbbbp: 0.939506 val loss: 0.374258
[Epoch 70] ogbg-molbbbp: 0.887419 test loss: 0.600479
[Epoch 71; Iter    20/   55] train: loss: 0.0432211
[Epoch 71; Iter    50/   55] train: loss: 0.0232943
[Epoch 71] ogbg-molbbbp: 0.949506 val loss: 0.339842
[Epoch 71] ogbg-molbbbp: 0.896972 test loss: 0.577733
[Epoch 72; Iter    25/   55] train: loss: 0.0119030
[Epoch 72; Iter    55/   55] train: loss: 0.0883019
[Epoch 72] ogbg-molbbbp: 0.938395 val loss: 0.378157
[Epoch 72] ogbg-molbbbp: 0.894768 test loss: 0.534936
[Epoch 73; Iter    30/   55] train: loss: 0.1258416
[Epoch 73] ogbg-molbbbp: 0.946049 val loss: 0.415539
[Epoch 73] ogbg-molbbbp: 0.891240 test loss: 0.572731
[Epoch 74; Iter     5/   55] train: loss: 0.0213109
[Epoch 74; Iter    35/   55] train: loss: 0.0411156
[Epoch 74] ogbg-molbbbp: 0.936420 val loss: 0.410696
[Epoch 74] ogbg-molbbbp: 0.869489 test loss: 0.640167
[Epoch 75; Iter    10/   55] train: loss: 0.2779364
[Epoch 75; Iter    40/   55] train: loss: 0.0160055
[Epoch 75] ogbg-molbbbp: 0.928272 val loss: 0.441940
[Epoch 75] ogbg-molbbbp: 0.894621 test loss: 0.571739
[Epoch 76; Iter    15/   55] train: loss: 0.0118100
[Epoch 76; Iter    45/   55] train: loss: 0.0072336
[Epoch 76] ogbg-molbbbp: 0.932222 val loss: 0.426275
[Epoch 76] ogbg-molbbbp: 0.902116 test loss: 0.574275
[Epoch 77; Iter    20/   55] train: loss: 0.0041157
[Epoch 77; Iter    50/   55] train: loss: 0.1784151
[Epoch 77] ogbg-molbbbp: 0.932840 val loss: 0.443808
[Epoch 77] ogbg-molbbbp: 0.889477 test loss: 0.643696
[Epoch 78; Iter    25/   55] train: loss: 0.0070078
[Epoch 78; Iter    55/   55] train: loss: 0.0027497
[Epoch 78] ogbg-molbbbp: 0.935556 val loss: 0.439887
[Epoch 78] ogbg-molbbbp: 0.880658 test loss: 0.606793
[Epoch 79; Iter    30/   55] train: loss: 0.0030316
[Epoch 79] ogbg-molbbbp: 0.936790 val loss: 0.412388
[Epoch 79] ogbg-molbbbp: 0.883157 test loss: 0.600350
[Epoch 80; Iter     5/   55] train: loss: 0.0071988
[Epoch 80; Iter    35/   55] train: loss: 0.0574780
[Epoch 80] ogbg-molbbbp: 0.940494 val loss: 0.425087
[Epoch 80] ogbg-molbbbp: 0.907995 test loss: 0.607830
[Epoch 81; Iter    10/   55] train: loss: 0.0076023
[Epoch 81; Iter    40/   55] train: loss: 0.0401655
[Epoch 81] ogbg-molbbbp: 0.913704 val loss: 0.489989
[Epoch 81] ogbg-molbbbp: 0.880364 test loss: 0.616491
[Epoch 82; Iter    15/   55] train: loss: 0.0351041
[Epoch 82; Iter    45/   55] train: loss: 0.1231312
[Epoch 82] ogbg-molbbbp: 0.928765 val loss: 0.401135
[Epoch 82] ogbg-molbbbp: 0.887860 test loss: 0.501804
[Epoch 83; Iter    20/   55] train: loss: 0.0183571
[Epoch 83; Iter    50/   55] train: loss: 0.0522498
[Epoch 83] ogbg-molbbbp: 0.926420 val loss: 0.451573
[Epoch 83] ogbg-molbbbp: 0.895356 test loss: 0.624391
[Epoch 84; Iter    25/   55] train: loss: 0.0143746
[Epoch 84; Iter    55/   55] train: loss: 0.0570958
[Epoch 84] ogbg-molbbbp: 0.930988 val loss: 0.459561
[Epoch 84] ogbg-molbbbp: 0.883010 test loss: 0.733818
[Epoch 85; Iter    30/   55] train: loss: 0.0040611
[Epoch 85] ogbg-molbbbp: 0.920000 val loss: 0.548111
[Epoch 85] ogbg-molbbbp: 0.897560 test loss: 0.692582
[Epoch 86; Iter     5/   55] train: loss: 0.1814434
[Epoch 86; Iter    35/   55] train: loss: 0.0031979
[Epoch 86] ogbg-molbbbp: 0.934198 val loss: 0.426143
[Epoch 86] ogbg-molbbbp: 0.894768 test loss: 0.588839
[Epoch 87; Iter    10/   55] train: loss: 0.0186413
[Epoch 87; Iter    40/   55] train: loss: 0.0135578
[Epoch 87] ogbg-molbbbp: 0.928889 val loss: 0.479792
[Epoch 87] ogbg-molbbbp: 0.906526 test loss: 0.653168
[Epoch 88; Iter    15/   55] train: loss: 0.0065742
[Epoch 88; Iter    45/   55] train: loss: 0.0940920
[Epoch 88] ogbg-molbbbp: 0.930370 val loss: 0.482285
[Epoch 88] ogbg-molbbbp: 0.902116 test loss: 0.593771
[Epoch 89; Iter    20/   55] train: loss: 0.0402621
[Epoch 89; Iter    50/   55] train: loss: 0.0226859
[Epoch 89] ogbg-molbbbp: 0.929136 val loss: 0.500506
[Epoch 89] ogbg-molbbbp: 0.893592 test loss: 0.669227
[Epoch 90; Iter    25/   55] train: loss: 0.0376260
[Epoch 90; Iter    55/   55] train: loss: 0.0014340
[Epoch 90] ogbg-molbbbp: 0.936667 val loss: 0.495021
[Epoch 90] ogbg-molbbbp: 0.912698 test loss: 0.594531
[Epoch 91; Iter    30/   55] train: loss: 0.1139541
[Epoch 91] ogbg-molbbbp: 0.933580 val loss: 0.477566
[Epoch 91] ogbg-molbbbp: 0.901822 test loss: 0.701709
[Epoch 92; Iter     5/   55] train: loss: 0.0597588
[Epoch 92; Iter    35/   55] train: loss: 0.0044222
[Epoch 92] ogbg-molbbbp: 0.941852 val loss: 0.493746
[Epoch 92] ogbg-molbbbp: 0.909759 test loss: 0.614962
[Epoch 93; Iter    10/   55] train: loss: 0.0020875
[Epoch 93; Iter    40/   55] train: loss: 0.0210823
[Epoch 93] ogbg-molbbbp: 0.938025 val loss: 0.443911
[Epoch 93] ogbg-molbbbp: 0.905350 test loss: 0.682878
[Epoch 94; Iter    15/   55] train: loss: 0.0036231
[Epoch 94; Iter    45/   55] train: loss: 0.0194761
[Epoch 94] ogbg-molbbbp: 0.945062 val loss: 0.442244
[Epoch 94] ogbg-molbbbp: 0.915491 test loss: 0.649311
[Epoch 95; Iter    20/   55] train: loss: 0.0227229
[Epoch 95; Iter    50/   55] train: loss: 0.0168399
[Epoch 95] ogbg-molbbbp: 0.937284 val loss: 0.497420
[Epoch 95] ogbg-molbbbp: 0.909465 test loss: 0.680843
[Epoch 96; Iter    25/   55] train: loss: 0.0504332
[Epoch 96; Iter    55/   55] train: loss: 0.0005939
[Epoch 96] ogbg-molbbbp: 0.930617 val loss: 0.500626
[Epoch 96] ogbg-molbbbp: 0.910788 test loss: 0.712700
[Epoch 97; Iter    30/   55] train: loss: 0.0082206
[Epoch 97] ogbg-molbbbp: 0.937531 val loss: 0.473167
[Epoch 97] ogbg-molbbbp: 0.915785 test loss: 0.714369
[Epoch 98; Iter     5/   55] train: loss: 0.0009073
[Epoch 98; Iter    35/   55] train: loss: 0.0098384
[Epoch 98] ogbg-molbbbp: 0.930864 val loss: 0.490969
[Epoch 98] ogbg-molbbbp: 0.905497 test loss: 0.730534
[Epoch 99; Iter    10/   55] train: loss: 0.0061362
[Epoch 99; Iter    40/   55] train: loss: 0.0008972
[Epoch 99] ogbg-molbbbp: 0.922099 val loss: 0.523750
[Epoch 99] ogbg-molbbbp: 0.912551 test loss: 0.652017
[Epoch 100; Iter    15/   55] train: loss: 0.0063346
[Epoch 100; Iter    45/   55] train: loss: 0.0019155
[Epoch 100] ogbg-molbbbp: 0.927531 val loss: 0.504880
[Epoch 100] ogbg-molbbbp: 0.924456 test loss: 0.565773
[Epoch 101; Iter    20/   55] train: loss: 0.0024073
[Epoch 101; Iter    50/   55] train: loss: 0.0013663
[Epoch 101] ogbg-molbbbp: 0.931358 val loss: 0.489668
[Epoch 101] ogbg-molbbbp: 0.914903 test loss: 0.660307
[Epoch 102; Iter    25/   55] train: loss: 0.0014085
[Epoch 102; Iter    55/   55] train: loss: 0.0044802
[Epoch 102] ogbg-molbbbp: 0.928272 val loss: 0.519081
[Epoch 102] ogbg-molbbbp: 0.908436 test loss: 0.724376
[Epoch 103; Iter    30/   55] train: loss: 0.0631730
[Epoch 103] ogbg-molbbbp: 0.934074 val loss: 0.471579
[Epoch 103] ogbg-molbbbp: 0.905350 test loss: 0.726466
[Epoch 104; Iter     5/   55] train: loss: 0.0048521
[Epoch 104; Iter    35/   55] train: loss: 0.0053565
[Epoch 104] ogbg-molbbbp: 0.925309 val loss: 0.528521
[Epoch 104] ogbg-molbbbp: 0.908877 test loss: 0.671286
[Epoch 105; Iter    10/   55] train: loss: 0.0013143
[Epoch 105; Iter    40/   55] train: loss: 0.0028558
[Epoch 105] ogbg-molbbbp: 0.928889 val loss: 0.515819
[Epoch 105] ogbg-molbbbp: 0.904027 test loss: 0.811658
[Epoch 106; Iter    15/   55] train: loss: 0.0062624
[Epoch 106; Iter    45/   55] train: loss: 0.0327788
[Epoch 106] ogbg-molbbbp: 0.925062 val loss: 0.533048
[Epoch 106] ogbg-molbbbp: 0.904321 test loss: 0.709412
[Epoch 107; Iter    20/   55] train: loss: 0.0023580
[Epoch 107; Iter    50/   55] train: loss: 0.0010689
[Epoch 107] ogbg-molbbbp: 0.935309 val loss: 0.493405
[Epoch 107] ogbg-molbbbp: 0.917108 test loss: 0.622496
[Epoch 108; Iter    25/   55] train: loss: 0.0035053
[Epoch 108; Iter    55/   55] train: loss: 0.0002326
[Epoch 108] ogbg-molbbbp: 0.937654 val loss: 0.486942
[Epoch 108] ogbg-molbbbp: 0.917402 test loss: 0.648100
[Epoch 109; Iter    30/   55] train: loss: 0.0076820
[Epoch 78] ogbg-molbbbp: 0.905274 val loss: 0.416492
[Epoch 78] ogbg-molbbbp: 0.887353 test loss: 0.524859
[Epoch 79; Iter    12/   41] train: loss: 0.0160283
[Epoch 79] ogbg-molbbbp: 0.911555 val loss: 0.385433
[Epoch 79] ogbg-molbbbp: 0.883347 test loss: 0.548294
[Epoch 80; Iter     1/   41] train: loss: 0.0252108
[Epoch 80; Iter    31/   41] train: loss: 0.0909722
[Epoch 80] ogbg-molbbbp: 0.915480 val loss: 0.407680
[Epoch 80] ogbg-molbbbp: 0.878105 test loss: 0.622532
[Epoch 81; Iter    20/   41] train: loss: 0.1226516
[Epoch 81] ogbg-molbbbp: 0.919952 val loss: 0.426392
[Epoch 81] ogbg-molbbbp: 0.894231 test loss: 0.547803
[Epoch 82; Iter     9/   41] train: loss: 0.0096266
[Epoch 82; Iter    39/   41] train: loss: 0.0878975
[Epoch 82] ogbg-molbbbp: 0.923844 val loss: 0.336632
[Epoch 82] ogbg-molbbbp: 0.877370 test loss: 0.533661
[Epoch 83; Iter    28/   41] train: loss: 0.1266925
[Epoch 83] ogbg-molbbbp: 0.920908 val loss: 0.379799
[Epoch 83] ogbg-molbbbp: 0.886585 test loss: 0.548710
[Epoch 84; Iter    17/   41] train: loss: 0.0819809
[Epoch 84] ogbg-molbbbp: 0.910667 val loss: 0.384067
[Epoch 84] ogbg-molbbbp: 0.861211 test loss: 0.567574
[Epoch 85; Iter     6/   41] train: loss: 0.0116917
[Epoch 85; Iter    36/   41] train: loss: 0.1787128
[Epoch 85] ogbg-molbbbp: 0.899710 val loss: 0.735928
[Epoch 85] ogbg-molbbbp: 0.876169 test loss: 0.657976
[Epoch 86; Iter    25/   41] train: loss: 0.1441430
[Epoch 86] ogbg-molbbbp: 0.891688 val loss: 0.830237
[Epoch 86] ogbg-molbbbp: 0.867121 test loss: 0.604614
[Epoch 87; Iter    14/   41] train: loss: 0.0492306
[Epoch 87] ogbg-molbbbp: 0.897696 val loss: 0.466313
[Epoch 87] ogbg-molbbbp: 0.864483 test loss: 0.643504
[Epoch 88; Iter     3/   41] train: loss: 0.1699845
[Epoch 88; Iter    33/   41] train: loss: 0.0757808
[Epoch 88] ogbg-molbbbp: 0.908175 val loss: 0.432079
[Epoch 88] ogbg-molbbbp: 0.880776 test loss: 0.550147
[Epoch 89; Iter    22/   41] train: loss: 0.0281114
[Epoch 89] ogbg-molbbbp: 0.923093 val loss: 0.383391
[Epoch 89] ogbg-molbbbp: 0.878172 test loss: 0.547542
[Epoch 90; Iter    11/   41] train: loss: 0.0802902
[Epoch 90; Iter    41/   41] train: loss: 0.1348125
[Epoch 90] ogbg-molbbbp: 0.924731 val loss: 0.389365
[Epoch 90] ogbg-molbbbp: 0.888655 test loss: 0.562643
[Epoch 91; Iter    30/   41] train: loss: 0.0132949
[Epoch 91] ogbg-molbbbp: 0.911760 val loss: 0.397284
[Epoch 91] ogbg-molbbbp: 0.873665 test loss: 0.547237
[Epoch 92; Iter    19/   41] train: loss: 0.0076370
[Epoch 92] ogbg-molbbbp: 0.916231 val loss: 0.403132
[Epoch 92] ogbg-molbbbp: 0.881177 test loss: 0.561455
[Epoch 93; Iter     8/   41] train: loss: 0.0191770
[Epoch 93; Iter    38/   41] train: loss: 0.0282862
[Epoch 93] ogbg-molbbbp: 0.916129 val loss: 0.423237
[Epoch 93] ogbg-molbbbp: 0.883814 test loss: 0.576611
[Epoch 94; Iter    27/   41] train: loss: 0.0041283
[Epoch 94] ogbg-molbbbp: 0.913023 val loss: 0.399281
[Epoch 94] ogbg-molbbbp: 0.881978 test loss: 0.539887
[Epoch 95; Iter    16/   41] train: loss: 0.0074973
[Epoch 95] ogbg-molbbbp: 0.917938 val loss: 0.415149
[Epoch 95] ogbg-molbbbp: 0.881878 test loss: 0.597260
[Epoch 96; Iter     5/   41] train: loss: 0.0070307
[Epoch 96; Iter    35/   41] train: loss: 0.0662978
[Epoch 96] ogbg-molbbbp: 0.918177 val loss: 0.434774
[Epoch 96] ogbg-molbbbp: 0.890592 test loss: 0.572070
[Epoch 97; Iter    24/   41] train: loss: 0.2399588
[Epoch 97] ogbg-molbbbp: 0.909677 val loss: 0.429436
[Epoch 97] ogbg-molbbbp: 0.880142 test loss: 0.591232
[Epoch 98; Iter    13/   41] train: loss: 0.0131452
[Epoch 98] ogbg-molbbbp: 0.907493 val loss: 0.482065
[Epoch 98] ogbg-molbbbp: 0.875267 test loss: 0.649739
[Epoch 99; Iter     2/   41] train: loss: 0.0252889
[Epoch 99; Iter    32/   41] train: loss: 0.0043024
[Epoch 99] ogbg-molbbbp: 0.907561 val loss: 0.443103
[Epoch 99] ogbg-molbbbp: 0.877404 test loss: 0.598639
[Epoch 100; Iter    21/   41] train: loss: 0.0141417
[Epoch 100] ogbg-molbbbp: 0.919406 val loss: 0.475258
[Epoch 100] ogbg-molbbbp: 0.890358 test loss: 0.661535
[Epoch 101; Iter    10/   41] train: loss: 0.0091999
[Epoch 101; Iter    40/   41] train: loss: 0.0049947
[Epoch 101] ogbg-molbbbp: 0.905752 val loss: 0.461001
[Epoch 101] ogbg-molbbbp: 0.862480 test loss: 0.696707
[Epoch 102; Iter    29/   41] train: loss: 0.0137700
[Epoch 102] ogbg-molbbbp: 0.910838 val loss: 0.488260
[Epoch 102] ogbg-molbbbp: 0.886185 test loss: 0.633305
[Epoch 103; Iter    18/   41] train: loss: 0.0489190
[Epoch 103] ogbg-molbbbp: 0.906503 val loss: 0.482900
[Epoch 103] ogbg-molbbbp: 0.869291 test loss: 0.652559
[Epoch 104; Iter     7/   41] train: loss: 0.0594593
[Epoch 104; Iter    37/   41] train: loss: 0.0272007
[Epoch 104] ogbg-molbbbp: 0.909268 val loss: 0.463465
[Epoch 104] ogbg-molbbbp: 0.876035 test loss: 0.626051
[Epoch 105; Iter    26/   41] train: loss: 0.0498106
[Epoch 105] ogbg-molbbbp: 0.905137 val loss: 0.476634
[Epoch 105] ogbg-molbbbp: 0.872796 test loss: 0.651238
[Epoch 106; Iter    15/   41] train: loss: 0.0024320
[Epoch 106] ogbg-molbbbp: 0.907663 val loss: 0.492425
[Epoch 106] ogbg-molbbbp: 0.886285 test loss: 0.622800
[Epoch 107; Iter     4/   41] train: loss: 0.0144219
[Epoch 107; Iter    34/   41] train: loss: 0.0056100
[Epoch 107] ogbg-molbbbp: 0.916778 val loss: 0.497403
[Epoch 107] ogbg-molbbbp: 0.873564 test loss: 0.740871
[Epoch 108; Iter    23/   41] train: loss: 0.0209348
[Epoch 108] ogbg-molbbbp: 0.906298 val loss: 0.494601
[Epoch 108] ogbg-molbbbp: 0.866820 test loss: 0.683792
[Epoch 109; Iter    12/   41] train: loss: 0.0276899
[Epoch 109] ogbg-molbbbp: 0.910872 val loss: 0.478860
[Epoch 109] ogbg-molbbbp: 0.875401 test loss: 0.643842
[Epoch 110; Iter     1/   41] train: loss: 0.0022597
[Epoch 110; Iter    31/   41] train: loss: 0.0133151
[Epoch 110] ogbg-molbbbp: 0.913569 val loss: 0.456435
[Epoch 110] ogbg-molbbbp: 0.868623 test loss: 0.639159
[Epoch 111; Iter    20/   41] train: loss: 0.1246002
[Epoch 111] ogbg-molbbbp: 0.909336 val loss: 0.476686
[Epoch 111] ogbg-molbbbp: 0.868556 test loss: 0.665031
[Epoch 112; Iter     9/   41] train: loss: 0.0210863
[Epoch 112; Iter    39/   41] train: loss: 0.0046269
[Epoch 112] ogbg-molbbbp: 0.909404 val loss: 0.443268
[Epoch 112] ogbg-molbbbp: 0.869992 test loss: 0.635498
[Epoch 113; Iter    28/   41] train: loss: 0.0043452
[Epoch 113] ogbg-molbbbp: 0.917494 val loss: 0.465523
[Epoch 113] ogbg-molbbbp: 0.874967 test loss: 0.669085
[Epoch 114; Iter    17/   41] train: loss: 0.0038852
[Epoch 114] ogbg-molbbbp: 0.917358 val loss: 0.459952
[Epoch 114] ogbg-molbbbp: 0.878072 test loss: 0.677175
[Epoch 115; Iter     6/   41] train: loss: 0.0047802
[Epoch 115; Iter    36/   41] train: loss: 0.0006929
[Epoch 115] ogbg-molbbbp: 0.916675 val loss: 0.480223
[Epoch 115] ogbg-molbbbp: 0.875935 test loss: 0.702544
[Epoch 116; Iter    25/   41] train: loss: 0.0879572
[Epoch 116] ogbg-molbbbp: 0.913910 val loss: 0.483702
[Epoch 116] ogbg-molbbbp: 0.872029 test loss: 0.703786
[Epoch 117; Iter    14/   41] train: loss: 0.0028713
[Epoch 117] ogbg-molbbbp: 0.913944 val loss: 0.510412
[Epoch 117] ogbg-molbbbp: 0.871761 test loss: 0.747124
[Epoch 118; Iter     3/   41] train: loss: 0.0015052
[Epoch 118; Iter    33/   41] train: loss: 0.0810825
[Epoch 118] ogbg-molbbbp: 0.914491 val loss: 0.518523
[Epoch 118] ogbg-molbbbp: 0.874232 test loss: 0.739661
[Epoch 119; Iter    22/   41] train: loss: 0.0015046
[Epoch 119] ogbg-molbbbp: 0.913432 val loss: 0.514247
[Epoch 119] ogbg-molbbbp: 0.870560 test loss: 0.745577
[Epoch 120; Iter    11/   41] train: loss: 0.0012241
[Epoch 120; Iter    41/   41] train: loss: 0.0012991
[Epoch 120] ogbg-molbbbp: 0.911248 val loss: 0.529959
[Epoch 120] ogbg-molbbbp: 0.873731 test loss: 0.737579
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 44.
Statistics on  val_best_checkpoint
mean_pred: 3.6300525665283203
std_pred: 3.5603291988372803
mean_targets: 0.7720588445663452
std_targets: 0.4200195074081421
prcauc: 0.9762113451320102
rocauc: 0.9300221880867041
ogbg-molbbbp: 0.9300221880867041
BCEWithLogitsLoss: 0.28191492706537247
Statistics on  test
mean_pred: 3.732015609741211
std_pred: 3.3613245487213135
mean_targets: 0.7647058963775635
std_targets: 0.42470329999923706
prcauc: 0.9647201250746775
rocauc: 0.9016426282051282
ogbg-molbbbp: 0.9016426282051282
BCEWithLogitsLoss: 0.3652266849364553
Statistics on  train
mean_pred: 3.7953083515167236
std_pred: 3.4435484409332275
mean_targets: 0.7628781199455261
std_targets: 0.425491601228714
prcauc: 0.9937598097540716
rocauc: 0.9791643567283881
ogbg-molbbbp: 0.9791643567283881
BCEWithLogitsLoss: 0.19165248123974335
[Epoch 73] ogbg-molbbbp: 0.876604 test loss: 0.582400
[Epoch 74; Iter     6/   48] train: loss: 0.0131212
[Epoch 74; Iter    36/   48] train: loss: 0.1216686
[Epoch 74] ogbg-molbbbp: 0.878750 val loss: 0.500160
[Epoch 74] ogbg-molbbbp: 0.876476 test loss: 0.700716
[Epoch 75; Iter    18/   48] train: loss: 0.0769232
[Epoch 75; Iter    48/   48] train: loss: 0.0854241
[Epoch 75] ogbg-molbbbp: 0.925424 val loss: 0.371928
[Epoch 75] ogbg-molbbbp: 0.887201 test loss: 0.500446
[Epoch 76; Iter    30/   48] train: loss: 0.0524819
[Epoch 76] ogbg-molbbbp: 0.917484 val loss: 0.502332
[Epoch 76] ogbg-molbbbp: 0.891861 test loss: 0.481526
[Epoch 77; Iter    12/   48] train: loss: 0.0538277
[Epoch 77; Iter    42/   48] train: loss: 0.1140644
[Epoch 77] ogbg-molbbbp: 0.933307 val loss: 0.432940
[Epoch 77] ogbg-molbbbp: 0.898053 test loss: 0.470517
[Epoch 78; Iter    24/   48] train: loss: 0.0968219
[Epoch 78] ogbg-molbbbp: 0.920547 val loss: 0.416786
[Epoch 78] ogbg-molbbbp: 0.886945 test loss: 0.500515
[Epoch 79; Iter     6/   48] train: loss: 0.0098669
[Epoch 79; Iter    36/   48] train: loss: 0.4408323
[Epoch 79] ogbg-molbbbp: 0.923836 val loss: 0.455910
[Epoch 79] ogbg-molbbbp: 0.883307 test loss: 0.608384
[Epoch 80; Iter    18/   48] train: loss: 0.0240413
[Epoch 80; Iter    48/   48] train: loss: 0.1252865
[Epoch 80] ogbg-molbbbp: 0.919753 val loss: 0.399804
[Epoch 80] ogbg-molbbbp: 0.866773 test loss: 0.595092
[Epoch 81; Iter    30/   48] train: loss: 0.0357903
[Epoch 81] ogbg-molbbbp: 0.928543 val loss: 0.390972
[Epoch 81] ogbg-molbbbp: 0.890329 test loss: 0.526762
[Epoch 82; Iter    12/   48] train: loss: 0.0114281
[Epoch 82; Iter    42/   48] train: loss: 0.0764353
[Epoch 82] ogbg-molbbbp: 0.909488 val loss: 0.475304
[Epoch 82] ogbg-molbbbp: 0.878647 test loss: 0.513271
[Epoch 83; Iter    24/   48] train: loss: 0.0266974
[Epoch 83] ogbg-molbbbp: 0.912437 val loss: 0.439233
[Epoch 83] ogbg-molbbbp: 0.886754 test loss: 0.558171
[Epoch 84; Iter     6/   48] train: loss: 0.0184481
[Epoch 84; Iter    36/   48] train: loss: 0.0268089
[Epoch 84] ogbg-molbbbp: 0.908127 val loss: 0.464621
[Epoch 84] ogbg-molbbbp: 0.890839 test loss: 0.521718
[Epoch 85; Iter    18/   48] train: loss: 0.0508478
[Epoch 85; Iter    48/   48] train: loss: 0.0185934
[Epoch 85] ogbg-molbbbp: 0.915556 val loss: 0.445513
[Epoch 85] ogbg-molbbbp: 0.870795 test loss: 0.613250
[Epoch 86; Iter    30/   48] train: loss: 0.1690581
[Epoch 86] ogbg-molbbbp: 0.918789 val loss: 0.506798
[Epoch 86] ogbg-molbbbp: 0.898628 test loss: 0.582999
[Epoch 87; Iter    12/   48] train: loss: 0.0073946
[Epoch 87; Iter    42/   48] train: loss: 0.1015347
[Epoch 87] ogbg-molbbbp: 0.906142 val loss: 0.466369
[Epoch 87] ogbg-molbbbp: 0.865560 test loss: 0.673499
[Epoch 88; Iter    24/   48] train: loss: 0.0602251
[Epoch 88] ogbg-molbbbp: 0.912891 val loss: 0.459677
[Epoch 88] ogbg-molbbbp: 0.879540 test loss: 0.628833
[Epoch 89; Iter     6/   48] train: loss: 0.0139713
[Epoch 89; Iter    36/   48] train: loss: 0.0143968
[Epoch 89] ogbg-molbbbp: 0.921227 val loss: 0.429713
[Epoch 89] ogbg-molbbbp: 0.886882 test loss: 0.584532
[Epoch 90; Iter    18/   48] train: loss: 0.0103449
[Epoch 90; Iter    48/   48] train: loss: 0.2120041
[Epoch 90] ogbg-molbbbp: 0.922191 val loss: 0.429025
[Epoch 90] ogbg-molbbbp: 0.888031 test loss: 0.630526
[Epoch 91; Iter    30/   48] train: loss: 0.0150491
[Epoch 91] ogbg-molbbbp: 0.912947 val loss: 0.421572
[Epoch 91] ogbg-molbbbp: 0.858091 test loss: 0.576780
[Epoch 92; Iter    12/   48] train: loss: 0.1873619
[Epoch 92; Iter    42/   48] train: loss: 0.0439020
[Epoch 92] ogbg-molbbbp: 0.912664 val loss: 0.444147
[Epoch 92] ogbg-molbbbp: 0.887584 test loss: 0.601805
[Epoch 93; Iter    24/   48] train: loss: 0.0209103
[Epoch 93] ogbg-molbbbp: 0.915613 val loss: 0.454620
[Epoch 93] ogbg-molbbbp: 0.884456 test loss: 0.605919
[Epoch 94; Iter     6/   48] train: loss: 0.0153745
[Epoch 94; Iter    36/   48] train: loss: 0.0278745
[Epoch 94] ogbg-molbbbp: 0.924573 val loss: 0.432027
[Epoch 94] ogbg-molbbbp: 0.890712 test loss: 0.610602
[Epoch 95; Iter    18/   48] train: loss: 0.1502395
[Epoch 95; Iter    48/   48] train: loss: 0.0053487
[Epoch 95] ogbg-molbbbp: 0.919299 val loss: 0.425296
[Epoch 95] ogbg-molbbbp: 0.873221 test loss: 0.624042
[Epoch 96; Iter    30/   48] train: loss: 0.0342291
[Epoch 96] ogbg-molbbbp: 0.911927 val loss: 0.465850
[Epoch 96] ogbg-molbbbp: 0.885413 test loss: 0.587937
[Epoch 97; Iter    12/   48] train: loss: 0.0489577
[Epoch 97; Iter    42/   48] train: loss: 0.0451162
[Epoch 97] ogbg-molbbbp: 0.890036 val loss: 0.587598
[Epoch 97] ogbg-molbbbp: 0.855538 test loss: 0.698172
[Epoch 98; Iter    24/   48] train: loss: 0.0101554
[Epoch 98] ogbg-molbbbp: 0.913741 val loss: 0.499618
[Epoch 98] ogbg-molbbbp: 0.882094 test loss: 0.659923
[Epoch 99; Iter     6/   48] train: loss: 0.0290955
[Epoch 99; Iter    36/   48] train: loss: 0.0480679
[Epoch 99] ogbg-molbbbp: 0.909545 val loss: 0.478403
[Epoch 99] ogbg-molbbbp: 0.884137 test loss: 0.508598
[Epoch 100; Iter    18/   48] train: loss: 0.0198570
[Epoch 100; Iter    48/   48] train: loss: 0.0245779
[Epoch 100] ogbg-molbbbp: 0.911303 val loss: 0.463643
[Epoch 100] ogbg-molbbbp: 0.872455 test loss: 0.537337
[Epoch 101; Iter    30/   48] train: loss: 0.0302227
[Epoch 101] ogbg-molbbbp: 0.912040 val loss: 0.530337
[Epoch 101] ogbg-molbbbp: 0.874816 test loss: 0.662282
[Epoch 102; Iter    12/   48] train: loss: 0.0732820
[Epoch 102; Iter    42/   48] train: loss: 0.0559090
[Epoch 102] ogbg-molbbbp: 0.913401 val loss: 0.513562
[Epoch 102] ogbg-molbbbp: 0.863581 test loss: 0.708396
[Epoch 103; Iter    24/   48] train: loss: 0.0375118
[Epoch 103] ogbg-molbbbp: 0.914308 val loss: 0.525769
[Epoch 103] ogbg-molbbbp: 0.879987 test loss: 0.731356
[Epoch 104; Iter     6/   48] train: loss: 0.0021300
[Epoch 104; Iter    36/   48] train: loss: 0.0560519
[Epoch 104] ogbg-molbbbp: 0.914989 val loss: 0.499182
[Epoch 104] ogbg-molbbbp: 0.879796 test loss: 0.631892
[Epoch 105; Iter    18/   48] train: loss: 0.0125988
[Epoch 105; Iter    48/   48] train: loss: 0.0035551
[Epoch 105] ogbg-molbbbp: 0.916577 val loss: 0.513007
[Epoch 105] ogbg-molbbbp: 0.875327 test loss: 0.690165
[Epoch 106; Iter    30/   48] train: loss: 0.0300549
[Epoch 106] ogbg-molbbbp: 0.916520 val loss: 0.461761
[Epoch 106] ogbg-molbbbp: 0.867794 test loss: 0.685108
[Epoch 107; Iter    12/   48] train: loss: 0.0313301
[Epoch 107; Iter    42/   48] train: loss: 0.0111922
[Epoch 107] ogbg-molbbbp: 0.919412 val loss: 0.458046
[Epoch 107] ogbg-molbbbp: 0.873731 test loss: 0.654519
[Epoch 108; Iter    24/   48] train: loss: 0.0009466
[Epoch 108] ogbg-molbbbp: 0.914932 val loss: 0.470871
[Epoch 108] ogbg-molbbbp: 0.864092 test loss: 0.701666
[Epoch 109; Iter     6/   48] train: loss: 0.0155403
[Epoch 109; Iter    36/   48] train: loss: 0.0031229
[Epoch 109] ogbg-molbbbp: 0.916520 val loss: 0.472802
[Epoch 109] ogbg-molbbbp: 0.865241 test loss: 0.742637
[Epoch 110; Iter    18/   48] train: loss: 0.0066594
[Epoch 110; Iter    48/   48] train: loss: 0.0085123
[Epoch 110] ogbg-molbbbp: 0.919015 val loss: 0.496166
[Epoch 110] ogbg-molbbbp: 0.874370 test loss: 0.754031
[Epoch 111; Iter    30/   48] train: loss: 0.0234609
[Epoch 111] ogbg-molbbbp: 0.916123 val loss: 0.491248
[Epoch 111] ogbg-molbbbp: 0.878455 test loss: 0.707650
[Epoch 112; Iter    12/   48] train: loss: 0.1814617
[Epoch 112; Iter    42/   48] train: loss: 0.0036858
[Epoch 112] ogbg-molbbbp: 0.916804 val loss: 0.482777
[Epoch 112] ogbg-molbbbp: 0.874625 test loss: 0.692642
[Epoch 113; Iter    24/   48] train: loss: 0.0013412
[Epoch 113] ogbg-molbbbp: 0.911756 val loss: 0.502501
[Epoch 113] ogbg-molbbbp: 0.866326 test loss: 0.729936
[Epoch 114; Iter     6/   48] train: loss: 0.0091198
[Epoch 114; Iter    36/   48] train: loss: 0.0009879
[Epoch 114] ogbg-molbbbp: 0.912720 val loss: 0.569946
[Epoch 114] ogbg-molbbbp: 0.867794 test loss: 0.852605
[Epoch 115; Iter    18/   48] train: loss: 0.0014417
[Epoch 115; Iter    48/   48] train: loss: 0.0016566
[Epoch 115] ogbg-molbbbp: 0.912153 val loss: 0.521370
[Epoch 115] ogbg-molbbbp: 0.864986 test loss: 0.725142
[Epoch 73] ogbg-molbbbp: 0.872901 test loss: 0.555839
[Epoch 74; Iter     6/   48] train: loss: 0.0154658
[Epoch 74; Iter    36/   48] train: loss: 0.1564109
[Epoch 74] ogbg-molbbbp: 0.915046 val loss: 0.388097
[Epoch 74] ogbg-molbbbp: 0.890584 test loss: 0.529605
[Epoch 75; Iter    18/   48] train: loss: 0.0445731
[Epoch 75; Iter    48/   48] train: loss: 0.1071759
[Epoch 75] ogbg-molbbbp: 0.914082 val loss: 0.418874
[Epoch 75] ogbg-molbbbp: 0.882413 test loss: 0.573993
[Epoch 76; Iter    30/   48] train: loss: 0.0273995
[Epoch 76] ogbg-molbbbp: 0.907446 val loss: 0.399811
[Epoch 76] ogbg-molbbbp: 0.885413 test loss: 0.490809
[Epoch 77; Iter    12/   48] train: loss: 0.0077663
[Epoch 77; Iter    42/   48] train: loss: 0.0263368
[Epoch 77] ogbg-molbbbp: 0.909771 val loss: 0.427695
[Epoch 77] ogbg-molbbbp: 0.895755 test loss: 0.529093
[Epoch 78; Iter    24/   48] train: loss: 0.0082309
[Epoch 78] ogbg-molbbbp: 0.916123 val loss: 0.403323
[Epoch 78] ogbg-molbbbp: 0.881072 test loss: 0.558988
[Epoch 79; Iter     6/   48] train: loss: 0.0224069
[Epoch 79; Iter    36/   48] train: loss: 0.0185439
[Epoch 79] ogbg-molbbbp: 0.916010 val loss: 0.455576
[Epoch 79] ogbg-molbbbp: 0.885733 test loss: 0.630153
[Epoch 80; Iter    18/   48] train: loss: 0.0025543
[Epoch 80; Iter    48/   48] train: loss: 0.0596830
[Epoch 80] ogbg-molbbbp: 0.914365 val loss: 0.420425
[Epoch 80] ogbg-molbbbp: 0.887456 test loss: 0.544025
[Epoch 81; Iter    30/   48] train: loss: 0.0797832
[Epoch 81] ogbg-molbbbp: 0.908013 val loss: 0.488006
[Epoch 81] ogbg-molbbbp: 0.886052 test loss: 0.702440
[Epoch 82; Iter    12/   48] train: loss: 0.0054319
[Epoch 82; Iter    42/   48] train: loss: 0.1511000
[Epoch 82] ogbg-molbbbp: 0.909998 val loss: 0.443372
[Epoch 82] ogbg-molbbbp: 0.901245 test loss: 0.575177
[Epoch 83; Iter    24/   48] train: loss: 0.0325179
[Epoch 83] ogbg-molbbbp: 0.917314 val loss: 0.415948
[Epoch 83] ogbg-molbbbp: 0.894542 test loss: 0.562940
[Epoch 84; Iter     6/   48] train: loss: 0.1241286
[Epoch 84; Iter    36/   48] train: loss: 0.0236258
[Epoch 84] ogbg-molbbbp: 0.901378 val loss: 0.467809
[Epoch 84] ogbg-molbbbp: 0.882668 test loss: 0.639634
[Epoch 85; Iter    18/   48] train: loss: 0.0143342
[Epoch 85; Iter    48/   48] train: loss: 0.0151418
[Epoch 85] ogbg-molbbbp: 0.907446 val loss: 0.466066
[Epoch 85] ogbg-molbbbp: 0.876732 test loss: 0.678485
[Epoch 86; Iter    30/   48] train: loss: 0.0857726
[Epoch 86] ogbg-molbbbp: 0.918959 val loss: 0.385138
[Epoch 86] ogbg-molbbbp: 0.886371 test loss: 0.559048
[Epoch 87; Iter    12/   48] train: loss: 0.0055521
[Epoch 87; Iter    42/   48] train: loss: 0.0405347
[Epoch 87] ogbg-molbbbp: 0.912947 val loss: 0.491414
[Epoch 87] ogbg-molbbbp: 0.910373 test loss: 0.692091
[Epoch 88; Iter    24/   48] train: loss: 0.0053867
[Epoch 88] ogbg-molbbbp: 0.902569 val loss: 0.442156
[Epoch 88] ogbg-molbbbp: 0.867858 test loss: 0.608717
[Epoch 89; Iter     6/   48] train: loss: 0.0146200
[Epoch 89; Iter    36/   48] train: loss: 0.0164988
[Epoch 89] ogbg-molbbbp: 0.905745 val loss: 0.523616
[Epoch 89] ogbg-molbbbp: 0.897032 test loss: 0.605973
[Epoch 90; Iter    18/   48] train: loss: 0.0048454
[Epoch 90; Iter    48/   48] train: loss: 0.3137219
[Epoch 90] ogbg-molbbbp: 0.897862 val loss: 0.523648
[Epoch 90] ogbg-molbbbp: 0.870093 test loss: 0.751729
[Epoch 91; Iter    30/   48] train: loss: 0.0093124
[Epoch 91] ogbg-molbbbp: 0.915329 val loss: 0.430200
[Epoch 91] ogbg-molbbbp: 0.884711 test loss: 0.602175
[Epoch 92; Iter    12/   48] train: loss: 0.0032940
[Epoch 92; Iter    42/   48] train: loss: 0.0114997
[Epoch 92] ogbg-molbbbp: 0.919696 val loss: 0.420768
[Epoch 92] ogbg-molbbbp: 0.895819 test loss: 0.580111
[Epoch 93; Iter    24/   48] train: loss: 0.0092970
[Epoch 93] ogbg-molbbbp: 0.918618 val loss: 0.441229
[Epoch 93] ogbg-molbbbp: 0.894031 test loss: 0.621661
[Epoch 94; Iter     6/   48] train: loss: 0.0045658
[Epoch 94; Iter    36/   48] train: loss: 0.0024732
[Epoch 94] ogbg-molbbbp: 0.922702 val loss: 0.426940
[Epoch 94] ogbg-molbbbp: 0.891733 test loss: 0.652491
[Epoch 95; Iter    18/   48] train: loss: 0.0031992
[Epoch 95; Iter    48/   48] train: loss: 0.0055133
[Epoch 95] ogbg-molbbbp: 0.919923 val loss: 0.446364
[Epoch 95] ogbg-molbbbp: 0.889499 test loss: 0.693941
[Epoch 96; Iter    30/   48] train: loss: 0.0064911
[Epoch 96] ogbg-molbbbp: 0.924800 val loss: 0.441346
[Epoch 96] ogbg-molbbbp: 0.886626 test loss: 0.700510
[Epoch 97; Iter    12/   48] train: loss: 0.0043108
[Epoch 97; Iter    42/   48] train: loss: 0.0022469
[Epoch 97] ogbg-molbbbp: 0.920093 val loss: 0.448599
[Epoch 97] ogbg-molbbbp: 0.884392 test loss: 0.728751
[Epoch 98; Iter    24/   48] train: loss: 0.0031082
[Epoch 98] ogbg-molbbbp: 0.919299 val loss: 0.442516
[Epoch 98] ogbg-molbbbp: 0.881966 test loss: 0.692503
[Epoch 99; Iter     6/   48] train: loss: 0.0490428
[Epoch 99; Iter    36/   48] train: loss: 0.0024189
[Epoch 99] ogbg-molbbbp: 0.920036 val loss: 0.448784
[Epoch 99] ogbg-molbbbp: 0.886882 test loss: 0.737987
[Epoch 100; Iter    18/   48] train: loss: 0.0011779
[Epoch 100; Iter    48/   48] train: loss: 0.0067794
[Epoch 100] ogbg-molbbbp: 0.922532 val loss: 0.459526
[Epoch 100] ogbg-molbbbp: 0.882349 test loss: 0.751072
[Epoch 101; Iter    30/   48] train: loss: 0.0017649
[Epoch 101] ogbg-molbbbp: 0.922135 val loss: 0.443637
[Epoch 101] ogbg-molbbbp: 0.886371 test loss: 0.660904
[Epoch 102; Iter    12/   48] train: loss: 0.0011952
[Epoch 102; Iter    42/   48] train: loss: 0.0022343
[Epoch 102] ogbg-molbbbp: 0.923723 val loss: 0.406635
[Epoch 102] ogbg-molbbbp: 0.869646 test loss: 0.673968
[Epoch 103; Iter    24/   48] train: loss: 0.0156588
[Epoch 103] ogbg-molbbbp: 0.923949 val loss: 0.435715
[Epoch 103] ogbg-molbbbp: 0.895053 test loss: 0.669399
[Epoch 104; Iter     6/   48] train: loss: 0.0020974
[Epoch 104; Iter    36/   48] train: loss: 0.0070510
[Epoch 104] ogbg-molbbbp: 0.924630 val loss: 0.428602
[Epoch 104] ogbg-molbbbp: 0.889052 test loss: 0.671183
[Epoch 105; Iter    18/   48] train: loss: 0.0035606
[Epoch 105; Iter    48/   48] train: loss: 0.0008905
[Epoch 105] ogbg-molbbbp: 0.919696 val loss: 0.429472
[Epoch 105] ogbg-molbbbp: 0.884009 test loss: 0.668458
[Epoch 106; Iter    30/   48] train: loss: 0.0470588
[Epoch 106] ogbg-molbbbp: 0.921908 val loss: 0.459613
[Epoch 106] ogbg-molbbbp: 0.879860 test loss: 0.734978
[Epoch 107; Iter    12/   48] train: loss: 0.0033074
[Epoch 107; Iter    42/   48] train: loss: 0.0034511
[Epoch 107] ogbg-molbbbp: 0.922758 val loss: 0.448754
[Epoch 107] ogbg-molbbbp: 0.882094 test loss: 0.732278
[Epoch 108; Iter    24/   48] train: loss: 0.0023406
[Epoch 108] ogbg-molbbbp: 0.921908 val loss: 0.463493
[Epoch 108] ogbg-molbbbp: 0.880243 test loss: 0.739721
[Epoch 109; Iter     6/   48] train: loss: 0.0358988
[Epoch 109; Iter    36/   48] train: loss: 0.0080624
[Epoch 109] ogbg-molbbbp: 0.916860 val loss: 0.472569
[Epoch 109] ogbg-molbbbp: 0.880562 test loss: 0.760408
[Epoch 110; Iter    18/   48] train: loss: 0.0041960
[Epoch 110; Iter    48/   48] train: loss: 0.0011140
[Epoch 110] ogbg-molbbbp: 0.914535 val loss: 0.435953
[Epoch 110] ogbg-molbbbp: 0.881647 test loss: 0.655875
[Epoch 111; Iter    30/   48] train: loss: 0.0143192
[Epoch 111] ogbg-molbbbp: 0.923382 val loss: 0.435603
[Epoch 111] ogbg-molbbbp: 0.895627 test loss: 0.690620
[Epoch 112; Iter    12/   48] train: loss: 0.0013344
[Epoch 112; Iter    42/   48] train: loss: 0.0500918
[Epoch 112] ogbg-molbbbp: 0.915046 val loss: 0.455494
[Epoch 112] ogbg-molbbbp: 0.879157 test loss: 0.677149
[Epoch 113; Iter    24/   48] train: loss: 0.0151967
[Epoch 113] ogbg-molbbbp: 0.922361 val loss: 0.459852
[Epoch 113] ogbg-molbbbp: 0.880689 test loss: 0.741078
[Epoch 114; Iter     6/   48] train: loss: 0.0291717
[Epoch 114; Iter    36/   48] train: loss: 0.0051753
[Epoch 114] ogbg-molbbbp: 0.909091 val loss: 0.494638
[Epoch 114] ogbg-molbbbp: 0.877306 test loss: 0.735739
[Epoch 115; Iter    18/   48] train: loss: 0.0294894
[Epoch 115; Iter    48/   48] train: loss: 0.0021609
[Epoch 115] ogbg-molbbbp: 0.918562 val loss: 0.465083
[Epoch 115] ogbg-molbbbp: 0.888541 test loss: 0.740076
[Epoch 78] ogbg-molbbbp: 0.927087 val loss: 0.410472
[Epoch 78] ogbg-molbbbp: 0.884816 test loss: 0.595933
[Epoch 79; Iter    12/   41] train: loss: 0.0177586
[Epoch 79] ogbg-molbbbp: 0.927360 val loss: 0.466752
[Epoch 79] ogbg-molbbbp: 0.892061 test loss: 0.600349
[Epoch 80; Iter     1/   41] train: loss: 0.0030596
[Epoch 80; Iter    31/   41] train: loss: 0.0039143
[Epoch 80] ogbg-molbbbp: 0.927530 val loss: 0.405923
[Epoch 80] ogbg-molbbbp: 0.880308 test loss: 0.624715
[Epoch 81; Iter    20/   41] train: loss: 0.0052060
[Epoch 81] ogbg-molbbbp: 0.917631 val loss: 0.498930
[Epoch 81] ogbg-molbbbp: 0.876502 test loss: 0.643713
[Epoch 82; Iter     9/   41] train: loss: 0.0048669
[Epoch 82; Iter    39/   41] train: loss: 0.0038851
[Epoch 82] ogbg-molbbbp: 0.916266 val loss: 0.437342
[Epoch 82] ogbg-molbbbp: 0.876569 test loss: 0.566755
[Epoch 83; Iter    28/   41] train: loss: 0.0071595
[Epoch 83] ogbg-molbbbp: 0.924629 val loss: 0.443494
[Epoch 83] ogbg-molbbbp: 0.873197 test loss: 0.647326
[Epoch 84; Iter    17/   41] train: loss: 0.0037519
[Epoch 84] ogbg-molbbbp: 0.922000 val loss: 0.434999
[Epoch 84] ogbg-molbbbp: 0.855836 test loss: 0.676241
[Epoch 85; Iter     6/   41] train: loss: 0.0077546
[Epoch 85; Iter    36/   41] train: loss: 0.0048938
[Epoch 85] ogbg-molbbbp: 0.912477 val loss: 0.478301
[Epoch 85] ogbg-molbbbp: 0.870793 test loss: 0.644023
[Epoch 86; Iter    25/   41] train: loss: 0.0056306
[Epoch 86] ogbg-molbbbp: 0.912715 val loss: 0.532086
[Epoch 86] ogbg-molbbbp: 0.868957 test loss: 0.650049
[Epoch 87; Iter    14/   41] train: loss: 0.0058493
[Epoch 87] ogbg-molbbbp: 0.911930 val loss: 0.496142
[Epoch 87] ogbg-molbbbp: 0.850861 test loss: 0.717222
[Epoch 88; Iter     3/   41] train: loss: 0.0142843
[Epoch 88; Iter    33/   41] train: loss: 0.0116215
[Epoch 88] ogbg-molbbbp: 0.914149 val loss: 0.461290
[Epoch 88] ogbg-molbbbp: 0.882412 test loss: 0.663666
[Epoch 89; Iter    22/   41] train: loss: 0.0587725
[Epoch 89] ogbg-molbbbp: 0.923263 val loss: 0.493472
[Epoch 89] ogbg-molbbbp: 0.857873 test loss: 0.865344
[Epoch 90; Iter    11/   41] train: loss: 0.1904619
[Epoch 90; Iter    41/   41] train: loss: 0.0105858
[Epoch 90] ogbg-molbbbp: 0.923332 val loss: 0.377762
[Epoch 90] ogbg-molbbbp: 0.876870 test loss: 0.593927
[Epoch 91; Iter    30/   41] train: loss: 0.0224225
[Epoch 91] ogbg-molbbbp: 0.918826 val loss: 0.428734
[Epoch 91] ogbg-molbbbp: 0.876870 test loss: 0.638769
[Epoch 92; Iter    19/   41] train: loss: 0.0091142
[Epoch 92] ogbg-molbbbp: 0.908346 val loss: 0.489757
[Epoch 92] ogbg-molbbbp: 0.875267 test loss: 0.676165
[Epoch 93; Iter     8/   41] train: loss: 0.0359402
[Epoch 93; Iter    38/   41] train: loss: 0.0248472
[Epoch 93] ogbg-molbbbp: 0.921932 val loss: 0.429490
[Epoch 93] ogbg-molbbbp: 0.885283 test loss: 0.652313
[Epoch 94; Iter    27/   41] train: loss: 0.1037064
[Epoch 94] ogbg-molbbbp: 0.912886 val loss: 0.455834
[Epoch 94] ogbg-molbbbp: 0.877838 test loss: 0.616417
[Epoch 95; Iter    16/   41] train: loss: 0.0035462
[Epoch 95] ogbg-molbbbp: 0.918109 val loss: 0.407724
[Epoch 95] ogbg-molbbbp: 0.877037 test loss: 0.591857
[Epoch 96; Iter     5/   41] train: loss: 0.0069932
[Epoch 96; Iter    35/   41] train: loss: 0.0072722
[Epoch 96] ogbg-molbbbp: 0.912647 val loss: 0.457307
[Epoch 96] ogbg-molbbbp: 0.876436 test loss: 0.662956
[Epoch 97; Iter    24/   41] train: loss: 0.0062471
[Epoch 97] ogbg-molbbbp: 0.915856 val loss: 0.480632
[Epoch 97] ogbg-molbbbp: 0.877604 test loss: 0.717955
[Epoch 98; Iter    13/   41] train: loss: 0.0945394
[Epoch 98] ogbg-molbbbp: 0.921727 val loss: 0.438697
[Epoch 98] ogbg-molbbbp: 0.882111 test loss: 0.657340
[Epoch 99; Iter     2/   41] train: loss: 0.0025153
[Epoch 99; Iter    32/   41] train: loss: 0.0012836
[Epoch 99] ogbg-molbbbp: 0.916573 val loss: 0.455861
[Epoch 99] ogbg-molbbbp: 0.869191 test loss: 0.713676
[Epoch 100; Iter    21/   41] train: loss: 0.0054113
[Epoch 100] ogbg-molbbbp: 0.913466 val loss: 0.434320
[Epoch 100] ogbg-molbbbp: 0.863949 test loss: 0.673394
[Epoch 101; Iter    10/   41] train: loss: 0.0045484
[Epoch 101; Iter    40/   41] train: loss: 0.1091153
[Epoch 101] ogbg-molbbbp: 0.910770 val loss: 0.461909
[Epoch 101] ogbg-molbbbp: 0.862981 test loss: 0.710783
[Epoch 102; Iter    29/   41] train: loss: 0.0087784
[Epoch 102] ogbg-molbbbp: 0.919474 val loss: 0.471483
[Epoch 102] ogbg-molbbbp: 0.862780 test loss: 0.767142
[Epoch 103; Iter    18/   41] train: loss: 0.0017284
[Epoch 103] ogbg-molbbbp: 0.917187 val loss: 0.455023
[Epoch 103] ogbg-molbbbp: 0.865451 test loss: 0.710670
[Epoch 104; Iter     7/   41] train: loss: 0.0016141
[Epoch 104; Iter    37/   41] train: loss: 0.0015831
[Epoch 104] ogbg-molbbbp: 0.919850 val loss: 0.489638
[Epoch 104] ogbg-molbbbp: 0.871361 test loss: 0.728888
[Epoch 105; Iter    26/   41] train: loss: 0.0051051
[Epoch 105] ogbg-molbbbp: 0.916778 val loss: 0.480341
[Epoch 105] ogbg-molbbbp: 0.867254 test loss: 0.732442
[Epoch 106; Iter    15/   41] train: loss: 0.0251658
[Epoch 106] ogbg-molbbbp: 0.919201 val loss: 0.486546
[Epoch 106] ogbg-molbbbp: 0.878940 test loss: 0.710596
[Epoch 107; Iter     4/   41] train: loss: 0.0018788
[Epoch 107; Iter    34/   41] train: loss: 0.1409344
[Epoch 107] ogbg-molbbbp: 0.917494 val loss: 0.549732
[Epoch 107] ogbg-molbbbp: 0.873331 test loss: 0.823195
[Epoch 108; Iter    23/   41] train: loss: 0.0011152
[Epoch 108] ogbg-molbbbp: 0.922922 val loss: 0.454926
[Epoch 108] ogbg-molbbbp: 0.879841 test loss: 0.681397
[Epoch 109; Iter    12/   41] train: loss: 0.0221866
[Epoch 109] ogbg-molbbbp: 0.912272 val loss: 0.482785
[Epoch 109] ogbg-molbbbp: 0.868356 test loss: 0.703265
[Epoch 110; Iter     1/   41] train: loss: 0.0042381
[Epoch 110; Iter    31/   41] train: loss: 0.0009048
[Epoch 110] ogbg-molbbbp: 0.916641 val loss: 0.496849
[Epoch 110] ogbg-molbbbp: 0.879407 test loss: 0.732520
[Epoch 111; Iter    20/   41] train: loss: 0.0011922
[Epoch 111] ogbg-molbbbp: 0.920225 val loss: 0.443621
[Epoch 111] ogbg-molbbbp: 0.872029 test loss: 0.728650
[Epoch 112; Iter     9/   41] train: loss: 0.0015030
[Epoch 112; Iter    39/   41] train: loss: 0.0772408
[Epoch 112] ogbg-molbbbp: 0.924458 val loss: 0.447366
[Epoch 112] ogbg-molbbbp: 0.879541 test loss: 0.688398
[Epoch 113; Iter    28/   41] train: loss: 0.0012520
[Epoch 113] ogbg-molbbbp: 0.918928 val loss: 0.496989
[Epoch 113] ogbg-molbbbp: 0.873631 test loss: 0.773035
[Epoch 114; Iter    17/   41] train: loss: 0.0313946
[Epoch 114] ogbg-molbbbp: 0.921215 val loss: 0.475176
[Epoch 114] ogbg-molbbbp: 0.880475 test loss: 0.708902
[Epoch 115; Iter     6/   41] train: loss: 0.0122223
[Epoch 115; Iter    36/   41] train: loss: 0.0536044
[Epoch 115] ogbg-molbbbp: 0.919782 val loss: 0.497837
[Epoch 115] ogbg-molbbbp: 0.872630 test loss: 0.776652
[Epoch 116; Iter    25/   41] train: loss: 0.0021095
[Epoch 116] ogbg-molbbbp: 0.917733 val loss: 0.483233
[Epoch 116] ogbg-molbbbp: 0.872129 test loss: 0.727883
[Epoch 117; Iter    14/   41] train: loss: 0.0025141
[Epoch 117] ogbg-molbbbp: 0.915139 val loss: 0.516768
[Epoch 117] ogbg-molbbbp: 0.870860 test loss: 0.786960
[Epoch 118; Iter     3/   41] train: loss: 0.0015917
[Epoch 118; Iter    33/   41] train: loss: 0.0021545
[Epoch 118] ogbg-molbbbp: 0.915276 val loss: 0.516217
[Epoch 118] ogbg-molbbbp: 0.876736 test loss: 0.761530
[Epoch 119; Iter    22/   41] train: loss: 0.0034703
[Epoch 119] ogbg-molbbbp: 0.911350 val loss: 0.509140
[Epoch 119] ogbg-molbbbp: 0.866653 test loss: 0.776889
[Epoch 120; Iter    11/   41] train: loss: 0.0015840
[Epoch 120; Iter    41/   41] train: loss: 0.0375170
[Epoch 120] ogbg-molbbbp: 0.917426 val loss: 0.473738
[Epoch 120] ogbg-molbbbp: 0.867822 test loss: 0.741795
[Epoch 121; Iter    30/   41] train: loss: 0.0022904
[Epoch 121] ogbg-molbbbp: 0.913501 val loss: 0.519764
[Epoch 121] ogbg-molbbbp: 0.869191 test loss: 0.788895
[Epoch 122; Iter    19/   41] train: loss: 0.0215288
[Epoch 122] ogbg-molbbbp: 0.914764 val loss: 0.507097
[Epoch 122] ogbg-molbbbp: 0.869692 test loss: 0.770973
[Epoch 123; Iter     8/   41] train: loss: 0.0006220
[Epoch 123; Iter    38/   41] train: loss: 0.1199958
[Epoch 123] ogbg-molbbbp: 0.913978 val loss: 0.517296
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.0035281
[Epoch 116] ogbg-molbbbp: 0.909828 val loss: 0.542697
[Epoch 116] ogbg-molbbbp: 0.863007 test loss: 0.796299
[Epoch 117; Iter    12/   48] train: loss: 0.0037436
[Epoch 117; Iter    42/   48] train: loss: 0.0017206
[Epoch 117] ogbg-molbbbp: 0.911870 val loss: 0.551561
[Epoch 117] ogbg-molbbbp: 0.863900 test loss: 0.759214
[Epoch 118; Iter    24/   48] train: loss: 0.0435653
[Epoch 118] ogbg-molbbbp: 0.912267 val loss: 0.511394
[Epoch 118] ogbg-molbbbp: 0.860198 test loss: 0.756975
[Epoch 119; Iter     6/   48] train: loss: 0.0254668
[Epoch 119; Iter    36/   48] train: loss: 0.0024922
[Epoch 119] ogbg-molbbbp: 0.902172 val loss: 0.525926
[Epoch 119] ogbg-molbbbp: 0.860326 test loss: 0.789930
[Epoch 120; Iter    18/   48] train: loss: 0.0038114
[Epoch 120; Iter    48/   48] train: loss: 0.0008366
[Epoch 120] ogbg-molbbbp: 0.907843 val loss: 0.562306
[Epoch 120] ogbg-molbbbp: 0.869390 test loss: 0.798580
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 55.
Statistics on  val_best_checkpoint
mean_pred: 3.227651596069336
std_pred: 4.412909030914307
mean_targets: 0.7483659982681274
std_targets: 0.434662789106369
prcauc: 0.972241396473862
rocauc: 0.9410763908580503
ogbg-molbbbp: 0.9410763908580503
BCEWithLogitsLoss: 0.2717277509245006
Statistics on  test
mean_pred: 3.555906295776367
std_pred: 4.223631858825684
mean_targets: 0.7875816822052002
std_targets: 0.4096892774105072
prcauc: 0.979270380974115
rocauc: 0.9328439195659113
ogbg-molbbbp: 0.9328439195659113
BCEWithLogitsLoss: 0.27963216772133653
Statistics on  train
mean_pred: 3.5207347869873047
std_pred: 5.275763034820557
mean_targets: 0.7638401985168457
std_targets: 0.42487040162086487
prcauc: 0.9987118898231226
rocauc: 0.995461846296246
ogbg-molbbbp: 0.995461846296246
BCEWithLogitsLoss: 0.15438308225323757
[Epoch 123] ogbg-molbbbp: 0.869658 test loss: 0.781016
[Epoch 124; Iter    27/   41] train: loss: 0.0007021
[Epoch 124] ogbg-molbbbp: 0.915105 val loss: 0.492557
[Epoch 124] ogbg-molbbbp: 0.872429 test loss: 0.748768
[Epoch 125; Iter    16/   41] train: loss: 0.0007360
[Epoch 125] ogbg-molbbbp: 0.908551 val loss: 0.549262
[Epoch 125] ogbg-molbbbp: 0.866119 test loss: 0.780701
[Epoch 126; Iter     5/   41] train: loss: 0.0009433
[Epoch 126; Iter    35/   41] train: loss: 0.0025186
[Epoch 126] ogbg-molbbbp: 0.911214 val loss: 0.522341
[Epoch 126] ogbg-molbbbp: 0.868924 test loss: 0.762278
[Epoch 127; Iter    24/   41] train: loss: 0.0009497
[Epoch 127] ogbg-molbbbp: 0.910975 val loss: 0.522862
[Epoch 127] ogbg-molbbbp: 0.867755 test loss: 0.764646
[Epoch 128; Iter    13/   41] train: loss: 0.0007654
[Epoch 128] ogbg-molbbbp: 0.912408 val loss: 0.526880
[Epoch 128] ogbg-molbbbp: 0.867655 test loss: 0.774955
[Epoch 129; Iter     2/   41] train: loss: 0.0546234
[Epoch 129; Iter    32/   41] train: loss: 0.0050337
[Epoch 129] ogbg-molbbbp: 0.913740 val loss: 0.509438
[Epoch 129] ogbg-molbbbp: 0.863482 test loss: 0.784588
[Epoch 130; Iter    21/   41] train: loss: 0.0013646
[Epoch 130] ogbg-molbbbp: 0.916095 val loss: 0.510647
[Epoch 130] ogbg-molbbbp: 0.870793 test loss: 0.787312
[Epoch 131; Iter    10/   41] train: loss: 0.0047950
[Epoch 131; Iter    40/   41] train: loss: 0.0022013
[Epoch 131] ogbg-molbbbp: 0.917187 val loss: 0.520351
[Epoch 131] ogbg-molbbbp: 0.873965 test loss: 0.769149
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 131 epochs. Best model checkpoint was in epoch 71.
Statistics on  val_best_checkpoint
mean_pred: 3.347658157348633
std_pred: 5.331610679626465
mean_targets: 0.7720588445663452
std_targets: 0.4200195074081421
prcauc: 0.9787138105014128
rocauc: 0.9385219320703191
ogbg-molbbbp: 0.9385219320703191
BCEWithLogitsLoss: 0.36395338816302164
Statistics on  test
mean_pred: 3.166728973388672
std_pred: 5.495092391967773
mean_targets: 0.7647058963775635
std_targets: 0.42470329999923706
prcauc: 0.9557517950051782
rocauc: 0.8847823183760684
ogbg-molbbbp: 0.8847823183760684
BCEWithLogitsLoss: 0.5643262429428952
Statistics on  train
mean_pred: 3.5922765731811523
std_pred: 5.626899242401123
mean_targets: 0.7628781199455261
std_targets: 0.4254915714263916
prcauc: 0.9999179067024535
rocauc: 0.9997338951103226
ogbg-molbbbp: 0.9997338951103226
BCEWithLogitsLoss: 0.026898591620166126
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
All runs completed.
[Epoch 116; Iter    30/   48] train: loss: 0.0023215
[Epoch 116] ogbg-molbbbp: 0.916237 val loss: 0.490122
[Epoch 116] ogbg-molbbbp: 0.884839 test loss: 0.777154
[Epoch 117; Iter    12/   48] train: loss: 0.0044087
[Epoch 117; Iter    42/   48] train: loss: 0.0089177
[Epoch 117] ogbg-molbbbp: 0.923042 val loss: 0.475398
[Epoch 117] ogbg-molbbbp: 0.888222 test loss: 0.793612
[Epoch 118; Iter    24/   48] train: loss: 0.0762185
[Epoch 118] ogbg-molbbbp: 0.917825 val loss: 0.460459
[Epoch 118] ogbg-molbbbp: 0.888350 test loss: 0.750755
[Epoch 119; Iter     6/   48] train: loss: 0.0012102
[Epoch 119; Iter    36/   48] train: loss: 0.0029589
[Epoch 119] ogbg-molbbbp: 0.921851 val loss: 0.459770
[Epoch 119] ogbg-molbbbp: 0.891414 test loss: 0.742618
[Epoch 120; Iter    18/   48] train: loss: 0.0027472
[Epoch 120; Iter    48/   48] train: loss: 0.0130620
[Epoch 120] ogbg-molbbbp: 0.924460 val loss: 0.457514
[Epoch 120] ogbg-molbbbp: 0.891861 test loss: 0.788020
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 54.
Statistics on  val_best_checkpoint
mean_pred: 2.7818827629089355
std_pred: 4.9476318359375
mean_targets: 0.7483659982681274
std_targets: 0.434662789106369
prcauc: 0.968680077882091
rocauc: 0.9272387001644643
ogbg-molbbbp: 0.9272387001644643
BCEWithLogitsLoss: 0.3636702868071469
Statistics on  test
mean_pred: 3.2107977867126465
std_pred: 4.662288665771484
mean_targets: 0.7875816822052002
std_targets: 0.4096892774105072
prcauc: 0.9672837460660614
rocauc: 0.903925949569103
ogbg-molbbbp: 0.903925949569103
BCEWithLogitsLoss: 0.4539945843544873
Statistics on  train
mean_pred: 3.1404497623443604
std_pred: 4.6752238273620605
mean_targets: 0.7638401985168457
std_targets: 0.4248703718185425
prcauc: 0.9975561337246703
rocauc: 0.9918343179157706
ogbg-molbbbp: 0.9918343179157706
BCEWithLogitsLoss: 0.1088804315465192
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.923827 val loss: 0.557301
[Epoch 109] ogbg-molbbbp: 0.897413 test loss: 0.630973
[Epoch 110; Iter     5/   55] train: loss: 0.0046261
[Epoch 110; Iter    35/   55] train: loss: 0.0031543
[Epoch 110] ogbg-molbbbp: 0.906914 val loss: 0.615298
[Epoch 110] ogbg-molbbbp: 0.874045 test loss: 0.670033
[Epoch 111; Iter    10/   55] train: loss: 0.0010317
[Epoch 111; Iter    40/   55] train: loss: 0.0178141
[Epoch 111] ogbg-molbbbp: 0.914198 val loss: 0.537528
[Epoch 111] ogbg-molbbbp: 0.881540 test loss: 0.702865
[Epoch 112; Iter    15/   55] train: loss: 0.0059148
[Epoch 112; Iter    45/   55] train: loss: 0.0145805
[Epoch 112] ogbg-molbbbp: 0.922222 val loss: 0.524476
[Epoch 112] ogbg-molbbbp: 0.865961 test loss: 0.666289
[Epoch 113; Iter    20/   55] train: loss: 0.0423926
[Epoch 113; Iter    50/   55] train: loss: 0.0379006
[Epoch 113] ogbg-molbbbp: 0.924815 val loss: 0.518786
[Epoch 113] ogbg-molbbbp: 0.870076 test loss: 0.638605
[Epoch 114; Iter    25/   55] train: loss: 0.0472423
[Epoch 114; Iter    55/   55] train: loss: 0.0281828
[Epoch 114] ogbg-molbbbp: 0.922469 val loss: 0.536985
[Epoch 114] ogbg-molbbbp: 0.878160 test loss: 0.659332
[Epoch 115; Iter    30/   55] train: loss: 0.0154823
[Epoch 115] ogbg-molbbbp: 0.925309 val loss: 0.571289
[Epoch 115] ogbg-molbbbp: 0.868607 test loss: 0.751111
[Epoch 116; Iter     5/   55] train: loss: 0.0009606
[Epoch 116; Iter    35/   55] train: loss: 0.0572869
[Epoch 116] ogbg-molbbbp: 0.922963 val loss: 0.590142
[Epoch 116] ogbg-molbbbp: 0.869636 test loss: 0.720240
[Epoch 117; Iter    10/   55] train: loss: 0.0020926
[Epoch 117; Iter    40/   55] train: loss: 0.0187164
[Epoch 117] ogbg-molbbbp: 0.923086 val loss: 0.585370
[Epoch 117] ogbg-molbbbp: 0.874192 test loss: 0.702948
[Epoch 118; Iter    15/   55] train: loss: 0.1070618
[Epoch 118; Iter    45/   55] train: loss: 0.0497911
[Epoch 118] ogbg-molbbbp: 0.916173 val loss: 0.599537
[Epoch 118] ogbg-molbbbp: 0.867872 test loss: 0.715979
[Epoch 119; Iter    20/   55] train: loss: 0.0040949
[Epoch 119; Iter    50/   55] train: loss: 0.0081774
[Epoch 119] ogbg-molbbbp: 0.926173 val loss: 0.579312
[Epoch 119] ogbg-molbbbp: 0.877278 test loss: 0.718853
[Epoch 120; Iter    25/   55] train: loss: 0.0009057
[Epoch 120; Iter    55/   55] train: loss: 0.0003846
[Epoch 120] ogbg-molbbbp: 0.920988 val loss: 0.593720
[Epoch 120] ogbg-molbbbp: 0.876984 test loss: 0.730098
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 58.
Statistics on  val_best_checkpoint
mean_pred: 3.190532922744751
std_pred: 4.861615180969238
mean_targets: 0.7352941632270813
std_targets: 0.44226178526878357
prcauc: 0.9788460780875918
rocauc: 0.9455555555555556
ogbg-molbbbp: 0.9455555555555556
BCEWithLogitsLoss: 0.3039319206561361
Statistics on  test
mean_pred: 3.448183298110962
std_pred: 6.8623785972595215
mean_targets: 0.7941176891326904
std_targets: 0.40533962845802307
prcauc: 0.9634721705057161
rocauc: 0.8947677836566725
ogbg-molbbbp: 0.8947677836566725
BCEWithLogitsLoss: 0.40353941305407454
Statistics on  train
mean_pred: 3.521383285522461
std_pred: 5.621054172515869
mean_targets: 0.7651747465133667
std_targets: 0.42401954531669617
prcauc: 0.9966679238805226
rocauc: 0.991458919127
ogbg-molbbbp: 0.991458919127
BCEWithLogitsLoss: 0.0871867873875255
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.0006523
[Epoch 116] ogbg-molbbbp: 0.925140 val loss: 0.484291
[Epoch 116] ogbg-molbbbp: 0.850112 test loss: 0.806547
[Epoch 117; Iter    12/   48] train: loss: 0.0022119
[Epoch 117; Iter    42/   48] train: loss: 0.0007883
[Epoch 117] ogbg-molbbbp: 0.922361 val loss: 0.504584
[Epoch 117] ogbg-molbbbp: 0.849793 test loss: 0.819470
[Epoch 118; Iter    24/   48] train: loss: 0.0206439
[Epoch 118] ogbg-molbbbp: 0.918675 val loss: 0.484844
[Epoch 118] ogbg-molbbbp: 0.846218 test loss: 0.773548
[Epoch 119; Iter     6/   48] train: loss: 0.0015164
[Epoch 119; Iter    36/   48] train: loss: 0.0007721
[Epoch 119] ogbg-molbbbp: 0.917428 val loss: 0.489020
[Epoch 119] ogbg-molbbbp: 0.852793 test loss: 0.754036
[Epoch 120; Iter    18/   48] train: loss: 0.0010151
[Epoch 120; Iter    48/   48] train: loss: 0.0038810
[Epoch 120] ogbg-molbbbp: 0.922418 val loss: 0.491920
[Epoch 120] ogbg-molbbbp: 0.849026 test loss: 0.821787
[Epoch 121; Iter    30/   48] train: loss: 0.0076714
[Epoch 121] ogbg-molbbbp: 0.924176 val loss: 0.471625
[Epoch 121] ogbg-molbbbp: 0.854899 test loss: 0.792069
[Epoch 122; Iter    12/   48] train: loss: 0.0009658
[Epoch 122; Iter    42/   48] train: loss: 0.0019495
[Epoch 122] ogbg-molbbbp: 0.924460 val loss: 0.473054
[Epoch 122] ogbg-molbbbp: 0.853367 test loss: 0.787092
[Epoch 123; Iter    24/   48] train: loss: 0.0008652
[Epoch 123] ogbg-molbbbp: 0.924630 val loss: 0.454397
[Epoch 123] ogbg-molbbbp: 0.839132 test loss: 0.784279
[Epoch 124; Iter     6/   48] train: loss: 0.0011159
[Epoch 124; Iter    36/   48] train: loss: 0.0024030
[Epoch 124] ogbg-molbbbp: 0.921794 val loss: 0.495539
[Epoch 124] ogbg-molbbbp: 0.838366 test loss: 0.842109
[Epoch 125; Iter    18/   48] train: loss: 0.0013751
[Epoch 125; Iter    48/   48] train: loss: 0.1842145
[Epoch 125] ogbg-molbbbp: 0.920547 val loss: 0.505732
[Epoch 125] ogbg-molbbbp: 0.832110 test loss: 0.879249
[Epoch 126; Iter    30/   48] train: loss: 0.0013469
[Epoch 126] ogbg-molbbbp: 0.921738 val loss: 0.490420
[Epoch 126] ogbg-molbbbp: 0.839770 test loss: 0.819632
[Epoch 127; Iter    12/   48] train: loss: 0.0767303
[Epoch 127; Iter    42/   48] train: loss: 0.0060532
[Epoch 127] ogbg-molbbbp: 0.917484 val loss: 0.513284
[Epoch 127] ogbg-molbbbp: 0.831471 test loss: 0.879403
[Epoch 128; Iter    24/   48] train: loss: 0.0003581
[Epoch 128] ogbg-molbbbp: 0.914876 val loss: 0.518616
[Epoch 128] ogbg-molbbbp: 0.843920 test loss: 0.832756
[Epoch 129; Iter     6/   48] train: loss: 0.0026524
[Epoch 129; Iter    36/   48] train: loss: 0.0010380
[Epoch 129] ogbg-molbbbp: 0.924063 val loss: 0.510148
[Epoch 129] ogbg-molbbbp: 0.841749 test loss: 0.878732
[Epoch 130; Iter    18/   48] train: loss: 0.0018144
[Epoch 130; Iter    48/   48] train: loss: 0.0284983
[Epoch 130] ogbg-molbbbp: 0.916066 val loss: 0.514218
[Epoch 130] ogbg-molbbbp: 0.849984 test loss: 0.832701
[Epoch 131; Iter    30/   48] train: loss: 0.0030154
[Epoch 131] ogbg-molbbbp: 0.919923 val loss: 0.502265
[Epoch 131] ogbg-molbbbp: 0.854389 test loss: 0.798361
[Epoch 132; Iter    12/   48] train: loss: 0.0068173
[Epoch 132; Iter    42/   48] train: loss: 0.1009606
[Epoch 132] ogbg-molbbbp: 0.924460 val loss: 0.482915
[Epoch 132] ogbg-molbbbp: 0.856368 test loss: 0.775235
[Epoch 133; Iter    24/   48] train: loss: 0.0089073
[Epoch 133] ogbg-molbbbp: 0.919923 val loss: 0.515093
[Epoch 133] ogbg-molbbbp: 0.856176 test loss: 0.818620
[Epoch 134; Iter     6/   48] train: loss: 0.0049366
[Epoch 134; Iter    36/   48] train: loss: 0.0013076
[Epoch 134] ogbg-molbbbp: 0.920547 val loss: 0.504138
[Epoch 134] ogbg-molbbbp: 0.855091 test loss: 0.801903
[Epoch 135; Iter    18/   48] train: loss: 0.0019362
[Epoch 135; Iter    48/   48] train: loss: 0.0019321
[Epoch 135] ogbg-molbbbp: 0.920660 val loss: 0.518706
[Epoch 135] ogbg-molbbbp: 0.854708 test loss: 0.824870
[Epoch 136; Iter    30/   48] train: loss: 0.0005057
[Epoch 136] ogbg-molbbbp: 0.921568 val loss: 0.496982
[Epoch 136] ogbg-molbbbp: 0.852984 test loss: 0.790119
[Epoch 137; Iter    12/   48] train: loss: 0.0038764
[Epoch 137; Iter    42/   48] train: loss: 0.0109791
[Epoch 137] ogbg-molbbbp: 0.921681 val loss: 0.503153
[Epoch 137] ogbg-molbbbp: 0.852346 test loss: 0.802388
[Epoch 138; Iter    24/   48] train: loss: 0.0069066
[Epoch 138] ogbg-molbbbp: 0.923042 val loss: 0.506051
[Epoch 138] ogbg-molbbbp: 0.851580 test loss: 0.819642
[Epoch 139; Iter     6/   48] train: loss: 0.0053774
[Epoch 139; Iter    36/   48] train: loss: 0.0640200
[Epoch 139] ogbg-molbbbp: 0.924630 val loss: 0.497806
[Epoch 139] ogbg-molbbbp: 0.855282 test loss: 0.813661
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 139 epochs. Best model checkpoint was in epoch 79.
Statistics on  val_best_checkpoint
mean_pred: 4.291925430297852
std_pred: 5.757418632507324
mean_targets: 0.7483659982681274
std_targets: 0.434662789106369
prcauc: 0.9712632437602885
rocauc: 0.9358021890772983
ogbg-molbbbp: 0.9358021890772983
BCEWithLogitsLoss: 0.33260515196756885
Statistics on  test
mean_pred: 4.612170696258545
std_pred: 5.424672603607178
mean_targets: 0.7875816822052002
std_targets: 0.4096892774105072
prcauc: 0.958280912990403
rocauc: 0.8817108203000319
ogbg-molbbbp: 0.8817108203000319
BCEWithLogitsLoss: 0.5092185180295598
Statistics on  train
mean_pred: 4.560823917388916
std_pred: 5.757546901702881
mean_targets: 0.7638401985168457
std_targets: 0.42487040162086487
prcauc: 0.9996411945893228
rocauc: 0.9988157787275747
ogbg-molbbbp: 0.9988157787275747
BCEWithLogitsLoss: 0.041343379920969404
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.939012 val loss: 0.430624
[Epoch 109] ogbg-molbbbp: 0.887713 test loss: 0.698704
[Epoch 110; Iter     5/   55] train: loss: 0.0288490
[Epoch 110; Iter    35/   55] train: loss: 0.0059421
[Epoch 110] ogbg-molbbbp: 0.927037 val loss: 0.492516
[Epoch 110] ogbg-molbbbp: 0.870076 test loss: 0.674902
[Epoch 111; Iter    10/   55] train: loss: 0.0033413
[Epoch 111; Iter    40/   55] train: loss: 0.0037352
[Epoch 111] ogbg-molbbbp: 0.931852 val loss: 0.511230
[Epoch 111] ogbg-molbbbp: 0.894915 test loss: 0.593145
[Epoch 112; Iter    15/   55] train: loss: 0.0061327
[Epoch 112; Iter    45/   55] train: loss: 0.0684380
[Epoch 112] ogbg-molbbbp: 0.923827 val loss: 0.563085
[Epoch 112] ogbg-molbbbp: 0.903733 test loss: 0.595666
[Epoch 113; Iter    20/   55] train: loss: 0.0080971
[Epoch 113; Iter    50/   55] train: loss: 0.0560993
[Epoch 113] ogbg-molbbbp: 0.897160 val loss: 0.678518
[Epoch 113] ogbg-molbbbp: 0.886537 test loss: 0.650282
[Epoch 114; Iter    25/   55] train: loss: 0.0912478
[Epoch 114; Iter    55/   55] train: loss: 0.0307058
[Epoch 114] ogbg-molbbbp: 0.917901 val loss: 0.626194
[Epoch 114] ogbg-molbbbp: 0.894915 test loss: 0.756812
[Epoch 115; Iter    30/   55] train: loss: 0.0007410
[Epoch 115] ogbg-molbbbp: 0.920494 val loss: 0.534212
[Epoch 115] ogbg-molbbbp: 0.889918 test loss: 0.587747
[Epoch 116; Iter     5/   55] train: loss: 0.0199752
[Epoch 116; Iter    35/   55] train: loss: 0.1133470
[Epoch 116] ogbg-molbbbp: 0.922099 val loss: 0.549004
[Epoch 116] ogbg-molbbbp: 0.893886 test loss: 0.618084
[Epoch 117; Iter    10/   55] train: loss: 0.0185451
[Epoch 117; Iter    40/   55] train: loss: 0.0515976
[Epoch 117] ogbg-molbbbp: 0.924444 val loss: 0.545604
[Epoch 117] ogbg-molbbbp: 0.883157 test loss: 0.672368
[Epoch 118; Iter    15/   55] train: loss: 0.0018432
[Epoch 118; Iter    45/   55] train: loss: 0.1105662
[Epoch 118] ogbg-molbbbp: 0.925185 val loss: 0.508692
[Epoch 118] ogbg-molbbbp: 0.887566 test loss: 0.631628
[Epoch 119; Iter    20/   55] train: loss: 0.0935624
[Epoch 119; Iter    50/   55] train: loss: 0.0017461
[Epoch 119] ogbg-molbbbp: 0.918889 val loss: 0.538613
[Epoch 119] ogbg-molbbbp: 0.894474 test loss: 0.627512
[Epoch 120; Iter    25/   55] train: loss: 0.0013039
[Epoch 120; Iter    55/   55] train: loss: 0.0466916
[Epoch 120] ogbg-molbbbp: 0.918889 val loss: 0.563265
[Epoch 120] ogbg-molbbbp: 0.899618 test loss: 0.657102
[Epoch 121; Iter    30/   55] train: loss: 0.0036022
[Epoch 121] ogbg-molbbbp: 0.917160 val loss: 0.604729
[Epoch 121] ogbg-molbbbp: 0.892857 test loss: 0.691832
[Epoch 122; Iter     5/   55] train: loss: 0.0061848
[Epoch 122; Iter    35/   55] train: loss: 0.0021703
[Epoch 122] ogbg-molbbbp: 0.914568 val loss: 0.586300
[Epoch 122] ogbg-molbbbp: 0.900647 test loss: 0.669938
[Epoch 123; Iter    10/   55] train: loss: 0.0734707
[Epoch 123; Iter    40/   55] train: loss: 0.0164171
[Epoch 123] ogbg-molbbbp: 0.920000 val loss: 0.557750
[Epoch 123] ogbg-molbbbp: 0.904027 test loss: 0.667264
[Epoch 124; Iter    15/   55] train: loss: 0.0159479
[Epoch 124; Iter    45/   55] train: loss: 0.0006497
[Epoch 124] ogbg-molbbbp: 0.924444 val loss: 0.537177
[Epoch 124] ogbg-molbbbp: 0.901382 test loss: 0.700385
[Epoch 125; Iter    20/   55] train: loss: 0.0160671
[Epoch 125; Iter    50/   55] train: loss: 0.0063463
[Epoch 125] ogbg-molbbbp: 0.925556 val loss: 0.521091
[Epoch 125] ogbg-molbbbp: 0.901235 test loss: 0.641080
[Epoch 126; Iter    25/   55] train: loss: 0.0010792
[Epoch 126; Iter    55/   55] train: loss: 0.0037741
[Epoch 126] ogbg-molbbbp: 0.930864 val loss: 0.520666
[Epoch 126] ogbg-molbbbp: 0.898001 test loss: 0.666089
[Epoch 127; Iter    30/   55] train: loss: 0.0009407
[Epoch 127] ogbg-molbbbp: 0.926049 val loss: 0.526874
[Epoch 127] ogbg-molbbbp: 0.901235 test loss: 0.670685
[Epoch 128; Iter     5/   55] train: loss: 0.0008054
[Epoch 128; Iter    35/   55] train: loss: 0.0012323
[Epoch 128] ogbg-molbbbp: 0.923951 val loss: 0.507155
[Epoch 128] ogbg-molbbbp: 0.906820 test loss: 0.657545
[Epoch 129; Iter    10/   55] train: loss: 0.0409761
[Epoch 129; Iter    40/   55] train: loss: 0.0012393
[Epoch 129] ogbg-molbbbp: 0.919012 val loss: 0.562980
[Epoch 129] ogbg-molbbbp: 0.900059 test loss: 0.698474
[Epoch 130; Iter    15/   55] train: loss: 0.0356675
[Epoch 130; Iter    45/   55] train: loss: 0.0012164
[Epoch 130] ogbg-molbbbp: 0.919136 val loss: 0.521006
[Epoch 130] ogbg-molbbbp: 0.900206 test loss: 0.631747
[Epoch 131; Iter    20/   55] train: loss: 0.0009841
[Epoch 131; Iter    50/   55] train: loss: 0.0022460
[Epoch 131] ogbg-molbbbp: 0.925679 val loss: 0.602603
[Epoch 131] ogbg-molbbbp: 0.901382 test loss: 0.655662
[Epoch 132; Iter    25/   55] train: loss: 0.0232854
[Epoch 132; Iter    55/   55] train: loss: 0.1114998
[Epoch 132] ogbg-molbbbp: 0.917778 val loss: 0.574773
[Epoch 132] ogbg-molbbbp: 0.904321 test loss: 0.700384
[Epoch 133; Iter    30/   55] train: loss: 0.0170729
[Epoch 133] ogbg-molbbbp: 0.926543 val loss: 0.503850
[Epoch 133] ogbg-molbbbp: 0.899912 test loss: 0.675207
[Epoch 134; Iter     5/   55] train: loss: 0.0008872
[Epoch 134; Iter    35/   55] train: loss: 0.0008376
[Epoch 134] ogbg-molbbbp: 0.924938 val loss: 0.511640
[Epoch 134] ogbg-molbbbp: 0.901675 test loss: 0.655813
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 134 epochs. Best model checkpoint was in epoch 74.
Statistics on  val_best_checkpoint
mean_pred: 2.9961140155792236
std_pred: 5.6732497215271
mean_targets: 0.7352941632270813
std_targets: 0.44226178526878357
prcauc: 0.9789371771832671
rocauc: 0.9451851851851852
ogbg-molbbbp: 0.9451851851851852
BCEWithLogitsLoss: 0.34080869598048075
Statistics on  test
mean_pred: 2.7239835262298584
std_pred: 5.655153751373291
mean_targets: 0.7941176891326904
std_targets: 0.40533962845802307
prcauc: 0.9651748293685412
rocauc: 0.8909465020576132
ogbg-molbbbp: 0.8909465020576132
BCEWithLogitsLoss: 0.5776716598442623
Statistics on  train
mean_pred: 3.3735337257385254
std_pred: 5.628688335418701
mean_targets: 0.7651747465133667
std_targets: 0.42401960492134094
prcauc: 0.9998676940734617
rocauc: 0.9995648389904265
ogbg-molbbbp: 0.9995648389904265
BCEWithLogitsLoss: 0.03786748667162928
[Epoch 109] ogbg-molbbbp: 0.938148 val loss: 0.490368
[Epoch 109] ogbg-molbbbp: 0.912845 test loss: 0.644833
[Epoch 110; Iter     5/   55] train: loss: 0.0021262
[Epoch 110; Iter    35/   55] train: loss: 0.0025036
[Epoch 110] ogbg-molbbbp: 0.942346 val loss: 0.471611
[Epoch 110] ogbg-molbbbp: 0.907848 test loss: 0.695375
[Epoch 111; Iter    10/   55] train: loss: 0.0008587
[Epoch 111; Iter    40/   55] train: loss: 0.0091047
[Epoch 111] ogbg-molbbbp: 0.936173 val loss: 0.514523
[Epoch 111] ogbg-molbbbp: 0.906526 test loss: 0.733872
[Epoch 112; Iter    15/   55] train: loss: 0.0713712
[Epoch 112; Iter    45/   55] train: loss: 0.0011552
[Epoch 112] ogbg-molbbbp: 0.938148 val loss: 0.512871
[Epoch 112] ogbg-molbbbp: 0.911376 test loss: 0.652652
[Epoch 113; Iter    20/   55] train: loss: 0.0112653
[Epoch 113; Iter    50/   55] train: loss: 0.0049788
[Epoch 113] ogbg-molbbbp: 0.934444 val loss: 0.546537
[Epoch 113] ogbg-molbbbp: 0.903586 test loss: 0.645936
[Epoch 114; Iter    25/   55] train: loss: 0.0041932
[Epoch 114; Iter    55/   55] train: loss: 0.0110856
[Epoch 114] ogbg-molbbbp: 0.928765 val loss: 0.523161
[Epoch 114] ogbg-molbbbp: 0.892416 test loss: 0.857136
[Epoch 115; Iter    30/   55] train: loss: 0.0038397
[Epoch 115] ogbg-molbbbp: 0.941111 val loss: 0.470399
[Epoch 115] ogbg-molbbbp: 0.911376 test loss: 0.640419
[Epoch 116; Iter     5/   55] train: loss: 0.0037629
[Epoch 116; Iter    35/   55] train: loss: 0.0097797
[Epoch 116] ogbg-molbbbp: 0.935802 val loss: 0.503268
[Epoch 116] ogbg-molbbbp: 0.914756 test loss: 0.656279
[Epoch 117; Iter    10/   55] train: loss: 0.0011675
[Epoch 117; Iter    40/   55] train: loss: 0.0022582
[Epoch 117] ogbg-molbbbp: 0.935926 val loss: 0.511021
[Epoch 117] ogbg-molbbbp: 0.918577 test loss: 0.677114
[Epoch 118; Iter    15/   55] train: loss: 0.0015649
[Epoch 118; Iter    45/   55] train: loss: 0.0068036
[Epoch 118] ogbg-molbbbp: 0.935556 val loss: 0.515990
[Epoch 118] ogbg-molbbbp: 0.914609 test loss: 0.656833
[Epoch 119; Iter    20/   55] train: loss: 0.0018861
[Epoch 119; Iter    50/   55] train: loss: 0.0016939
[Epoch 119] ogbg-molbbbp: 0.931728 val loss: 0.511695
[Epoch 119] ogbg-molbbbp: 0.915050 test loss: 0.659260
[Epoch 120; Iter    25/   55] train: loss: 0.0013527
[Epoch 120; Iter    55/   55] train: loss: 0.0002576
[Epoch 120] ogbg-molbbbp: 0.938025 val loss: 0.525156
[Epoch 120] ogbg-molbbbp: 0.919165 test loss: 0.695988
[Epoch 121; Iter    30/   55] train: loss: 0.0067449
[Epoch 121] ogbg-molbbbp: 0.933210 val loss: 0.502210
[Epoch 121] ogbg-molbbbp: 0.918871 test loss: 0.642785
[Epoch 122; Iter     5/   55] train: loss: 0.0006212
[Epoch 122; Iter    35/   55] train: loss: 0.0446194
[Epoch 122] ogbg-molbbbp: 0.943333 val loss: 0.484508
[Epoch 122] ogbg-molbbbp: 0.921223 test loss: 0.685267
[Epoch 123; Iter    10/   55] train: loss: 0.0177325
[Epoch 123; Iter    40/   55] train: loss: 0.0503886
[Epoch 123] ogbg-molbbbp: 0.937901 val loss: 0.513366
[Epoch 123] ogbg-molbbbp: 0.922399 test loss: 0.657146
[Epoch 124; Iter    15/   55] train: loss: 0.0097185
[Epoch 124; Iter    45/   55] train: loss: 0.0195603
[Epoch 124] ogbg-molbbbp: 0.935432 val loss: 0.526152
[Epoch 124] ogbg-molbbbp: 0.917108 test loss: 0.713018
[Epoch 125; Iter    20/   55] train: loss: 0.0127288
[Epoch 125; Iter    50/   55] train: loss: 0.0439603
[Epoch 125] ogbg-molbbbp: 0.935185 val loss: 0.527533
[Epoch 125] ogbg-molbbbp: 0.921664 test loss: 0.704050
[Epoch 126; Iter    25/   55] train: loss: 0.0006845
[Epoch 126; Iter    55/   55] train: loss: 0.0004736
[Epoch 126] ogbg-molbbbp: 0.935802 val loss: 0.525017
[Epoch 126] ogbg-molbbbp: 0.920194 test loss: 0.660141
[Epoch 127; Iter    30/   55] train: loss: 0.0013063
[Epoch 127] ogbg-molbbbp: 0.938889 val loss: 0.518280
[Epoch 127] ogbg-molbbbp: 0.920929 test loss: 0.693373
[Epoch 128; Iter     5/   55] train: loss: 0.0022795
[Epoch 128; Iter    35/   55] train: loss: 0.0420308
[Epoch 128] ogbg-molbbbp: 0.936420 val loss: 0.519294
[Epoch 128] ogbg-molbbbp: 0.912845 test loss: 0.651373
[Epoch 129; Iter    10/   55] train: loss: 0.0049165
[Epoch 129; Iter    40/   55] train: loss: 0.0588932
[Epoch 129] ogbg-molbbbp: 0.937778 val loss: 0.523711
[Epoch 129] ogbg-molbbbp: 0.915344 test loss: 0.658285
[Epoch 130; Iter    15/   55] train: loss: 0.0004845
[Epoch 130; Iter    45/   55] train: loss: 0.0344089
[Epoch 130] ogbg-molbbbp: 0.940000 val loss: 0.503483
[Epoch 130] ogbg-molbbbp: 0.919165 test loss: 0.625896
[Epoch 131; Iter    20/   55] train: loss: 0.0003498
[Epoch 131; Iter    50/   55] train: loss: 0.0097746
[Epoch 131] ogbg-molbbbp: 0.937901 val loss: 0.526861
[Epoch 131] ogbg-molbbbp: 0.918871 test loss: 0.694362
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 131 epochs. Best model checkpoint was in epoch 71.
Statistics on  val_best_checkpoint
mean_pred: 2.8266468048095703
std_pred: 5.517617702484131
mean_targets: 0.7352941632270813
std_targets: 0.44226178526878357
prcauc: 0.9825875462305201
rocauc: 0.9495061728395063
ogbg-molbbbp: 0.9495061728395063
BCEWithLogitsLoss: 0.3398416649018015
Statistics on  test
mean_pred: 3.152724504470825
std_pred: 5.587865352630615
mean_targets: 0.7941176891326904
std_targets: 0.40533962845802307
prcauc: 0.960374391224318
rocauc: 0.8969723691945914
ogbg-molbbbp: 0.8969723691945914
BCEWithLogitsLoss: 0.5777328674282346
Statistics on  train
mean_pred: 3.5354933738708496
std_pred: 5.553525447845459
mean_targets: 0.7651747465133667
std_targets: 0.42401960492134094
prcauc: 0.9998680697095681
rocauc: 0.9995763456517373
ogbg-molbbbp: 0.9995763456517373
BCEWithLogitsLoss: 0.03426794153638184
All runs completed.
