>>> Starting run for dataset: bace
Running SCAFF configs_split_experiments/GraphCL/bace/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/bace/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/bace/scaff/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/bace/scaff/train_prop=0.7.yml --seed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.8/PNA_ogbg-molbace_GraphCL_bace_scaff=0.8_5_26-05_09-42-09
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.8
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6941631
[Epoch 1] ogbg-molbace: 0.440659 val loss: 0.692166
[Epoch 1] ogbg-molbace: 0.429491 test loss: 0.693840
[Epoch 2; Iter    19/   41] train: loss: 0.6973654
[Epoch 2] ogbg-molbace: 0.406227 val loss: 0.691402
[Epoch 2] ogbg-molbace: 0.444792 test loss: 0.694443
[Epoch 3; Iter     8/   41] train: loss: 0.6944934
[Epoch 3; Iter    38/   41] train: loss: 0.6957358
[Epoch 3] ogbg-molbace: 0.435165 val loss: 0.690996
[Epoch 3] ogbg-molbace: 0.444792 test loss: 0.694651
[Epoch 4; Iter    27/   41] train: loss: 0.6940734
[Epoch 4] ogbg-molbace: 0.442125 val loss: 0.691196
[Epoch 4] ogbg-molbace: 0.446705 test loss: 0.694732
[Epoch 5; Iter    16/   41] train: loss: 0.6944140
[Epoch 5] ogbg-molbace: 0.431868 val loss: 0.691992
[Epoch 5] ogbg-molbace: 0.459572 test loss: 0.694731
[Epoch 6; Iter     5/   41] train: loss: 0.6944993
[Epoch 6; Iter    35/   41] train: loss: 0.6972181
[Epoch 6] ogbg-molbace: 0.441392 val loss: 0.692018
[Epoch 6] ogbg-molbace: 0.459920 test loss: 0.694895
[Epoch 7; Iter    24/   41] train: loss: 0.6930083
[Epoch 7] ogbg-molbace: 0.426374 val loss: 0.693080
[Epoch 7] ogbg-molbace: 0.459920 test loss: 0.695155
[Epoch 8; Iter    13/   41] train: loss: 0.6900389
[Epoch 8] ogbg-molbace: 0.432601 val loss: 0.693074
[Epoch 8] ogbg-molbace: 0.456268 test loss: 0.695385
[Epoch 9; Iter     2/   41] train: loss: 0.6933948
[Epoch 9; Iter    32/   41] train: loss: 0.6909602
[Epoch 9] ogbg-molbace: 0.456410 val loss: 0.693950
[Epoch 9] ogbg-molbace: 0.462354 test loss: 0.695565
[Epoch 10; Iter    21/   41] train: loss: 0.6918141
[Epoch 10] ogbg-molbace: 0.475458 val loss: 0.694257
[Epoch 10] ogbg-molbace: 0.485481 test loss: 0.695550
[Epoch 11; Iter    10/   41] train: loss: 0.6945540
[Epoch 11; Iter    40/   41] train: loss: 0.6928239
[Epoch 11] ogbg-molbace: 0.454579 val loss: 0.695062
[Epoch 11] ogbg-molbace: 0.488785 test loss: 0.695739
[Epoch 12; Iter    29/   41] train: loss: 0.6920391
[Epoch 12] ogbg-molbace: 0.488278 val loss: 0.695949
[Epoch 12] ogbg-molbace: 0.516606 test loss: 0.695838
[Epoch 13; Iter    18/   41] train: loss: 0.6947280
[Epoch 13] ogbg-molbace: 0.499634 val loss: 0.696641
[Epoch 13] ogbg-molbace: 0.526865 test loss: 0.695879
[Epoch 14; Iter     7/   41] train: loss: 0.6912009
[Epoch 14; Iter    37/   41] train: loss: 0.6882623
[Epoch 14] ogbg-molbace: 0.523443 val loss: 0.698040
[Epoch 14] ogbg-molbace: 0.536602 test loss: 0.696256
[Epoch 15; Iter    26/   41] train: loss: 0.6939883
[Epoch 15] ogbg-molbace: 0.521978 val loss: 0.698858
[Epoch 15] ogbg-molbace: 0.548079 test loss: 0.696868
[Epoch 16; Iter    15/   41] train: loss: 0.6843122
[Epoch 16] ogbg-molbace: 0.539560 val loss: 0.699656
[Epoch 16] ogbg-molbace: 0.566684 test loss: 0.697214
[Epoch 17; Iter     4/   41] train: loss: 0.6893942
[Epoch 17; Iter    34/   41] train: loss: 0.6943185
[Epoch 17] ogbg-molbace: 0.622344 val loss: 0.695543
[Epoch 17] ogbg-molbace: 0.739350 test loss: 0.689008
[Epoch 18; Iter    23/   41] train: loss: 0.6675870
[Epoch 18] ogbg-molbace: 0.674359 val loss: 0.697613
[Epoch 18] ogbg-molbace: 0.774648 test loss: 0.691867
[Epoch 19; Iter    12/   41] train: loss: 0.6294688
[Epoch 19] ogbg-molbace: 0.634066 val loss: 0.736852
[Epoch 19] ogbg-molbace: 0.728047 test loss: 0.747801
[Epoch 20; Iter     1/   41] train: loss: 0.5960811
[Epoch 20; Iter    31/   41] train: loss: 0.5263762
[Epoch 20] ogbg-molbace: 0.679121 val loss: 0.658170
[Epoch 20] ogbg-molbace: 0.769084 test loss: 0.674200
[Epoch 21; Iter    20/   41] train: loss: 0.5247238
[Epoch 21] ogbg-molbace: 0.688278 val loss: 1.034972
[Epoch 21] ogbg-molbace: 0.747174 test loss: 0.965905
[Epoch 22; Iter     9/   41] train: loss: 0.3979757
[Epoch 22; Iter    39/   41] train: loss: 0.4531927
[Epoch 22] ogbg-molbace: 0.631868 val loss: 0.759481
[Epoch 22] ogbg-molbace: 0.773778 test loss: 0.742023
[Epoch 23; Iter    28/   41] train: loss: 0.5383353
[Epoch 23] ogbg-molbace: 0.708425 val loss: 0.697014
[Epoch 23] ogbg-molbace: 0.806816 test loss: 0.673656
[Epoch 24; Iter    17/   41] train: loss: 0.4872547
[Epoch 24] ogbg-molbace: 0.692308 val loss: 0.834786
[Epoch 24] ogbg-molbace: 0.777256 test loss: 0.949473
[Epoch 25; Iter     6/   41] train: loss: 0.4095693
[Epoch 25; Iter    36/   41] train: loss: 0.4600287
[Epoch 25] ogbg-molbace: 0.620513 val loss: 0.799441
[Epoch 25] ogbg-molbace: 0.770822 test loss: 0.735524
[Epoch 26; Iter    25/   41] train: loss: 0.3381007
[Epoch 26] ogbg-molbace: 0.682051 val loss: 0.747623
[Epoch 26] ogbg-molbace: 0.779343 test loss: 0.623725
[Epoch 27; Iter    14/   41] train: loss: 0.5240496
[Epoch 27] ogbg-molbace: 0.721245 val loss: 0.708536
[Epoch 27] ogbg-molbace: 0.799687 test loss: 0.667350
[Epoch 28; Iter     3/   41] train: loss: 0.3390129
[Epoch 28; Iter    33/   41] train: loss: 0.5475276
[Epoch 28] ogbg-molbace: 0.664835 val loss: 0.879600
[Epoch 28] ogbg-molbace: 0.781951 test loss: 0.837073
[Epoch 29; Iter    22/   41] train: loss: 0.3272292
[Epoch 29] ogbg-molbace: 0.712088 val loss: 0.821904
[Epoch 29] ogbg-molbace: 0.801947 test loss: 0.906377
[Epoch 30; Iter    11/   41] train: loss: 0.4589972
[Epoch 30; Iter    41/   41] train: loss: 0.7516398
[Epoch 30] ogbg-molbace: 0.724542 val loss: 0.925979
[Epoch 30] ogbg-molbace: 0.799861 test loss: 0.753726
[Epoch 31; Iter    30/   41] train: loss: 0.3966761
[Epoch 31] ogbg-molbace: 0.704029 val loss: 0.920356
[Epoch 31] ogbg-molbace: 0.801600 test loss: 0.947973
[Epoch 32; Iter    19/   41] train: loss: 0.6199533
[Epoch 32] ogbg-molbace: 0.707326 val loss: 1.011373
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.6/PNA_ogbg-molbace_GraphCL_bace_scaff=0.6_5_26-05_09-42-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.6
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6945992
[Epoch 1] ogbg-molbace: 0.457100 val loss: 0.693737
[Epoch 1] ogbg-molbace: 0.492736 test loss: 0.693011
[Epoch 2; Iter    29/   31] train: loss: 0.6958779
[Epoch 2] ogbg-molbace: 0.444191 val loss: 0.694019
[Epoch 2] ogbg-molbace: 0.497630 test loss: 0.692919
[Epoch 3; Iter    28/   31] train: loss: 0.6912740
[Epoch 3] ogbg-molbace: 0.423932 val loss: 0.695978
[Epoch 3] ogbg-molbace: 0.483721 test loss: 0.692604
[Epoch 4; Iter    27/   31] train: loss: 0.6928123
[Epoch 4] ogbg-molbace: 0.432969 val loss: 0.696446
[Epoch 4] ogbg-molbace: 0.465897 test loss: 0.692713
[Epoch 5; Iter    26/   31] train: loss: 0.6966432
[Epoch 5] ogbg-molbace: 0.436445 val loss: 0.695593
[Epoch 5] ogbg-molbace: 0.478209 test loss: 0.692795
[Epoch 6; Iter    25/   31] train: loss: 0.6941825
[Epoch 6] ogbg-molbace: 0.433069 val loss: 0.695923
[Epoch 6] ogbg-molbace: 0.466464 test loss: 0.692920
[Epoch 7; Iter    24/   31] train: loss: 0.6906971
[Epoch 7] ogbg-molbace: 0.441410 val loss: 0.695637
[Epoch 7] ogbg-molbace: 0.469194 test loss: 0.692887
[Epoch 8; Iter    23/   31] train: loss: 0.6953569
[Epoch 8] ogbg-molbace: 0.462761 val loss: 0.696149
[Epoch 8] ogbg-molbace: 0.479549 test loss: 0.692592
[Epoch 9; Iter    22/   31] train: loss: 0.6926849
[Epoch 9] ogbg-molbace: 0.455809 val loss: 0.696045
[Epoch 9] ogbg-molbace: 0.478055 test loss: 0.692561
[Epoch 10; Iter    21/   31] train: loss: 0.6951158
[Epoch 10] ogbg-molbace: 0.460377 val loss: 0.694759
[Epoch 10] ogbg-molbace: 0.490161 test loss: 0.692731
[Epoch 11; Iter    20/   31] train: loss: 0.6975365
[Epoch 11] ogbg-molbace: 0.457696 val loss: 0.694987
[Epoch 11] ogbg-molbace: 0.491294 test loss: 0.692675
[Epoch 12; Iter    19/   31] train: loss: 0.6950789
[Epoch 12] ogbg-molbace: 0.464747 val loss: 0.694698
[Epoch 12] ogbg-molbace: 0.491912 test loss: 0.692766
[Epoch 13; Iter    18/   31] train: loss: 0.6938474
[Epoch 13] ogbg-molbace: 0.459384 val loss: 0.694501
[Epoch 13] ogbg-molbace: 0.493303 test loss: 0.692695
[Epoch 14; Iter    17/   31] train: loss: 0.6918143
[Epoch 14] ogbg-molbace: 0.471400 val loss: 0.694192
[Epoch 14] ogbg-molbace: 0.504894 test loss: 0.692657
[Epoch 15; Iter    16/   31] train: loss: 0.6934381
[Epoch 15] ogbg-molbace: 0.482721 val loss: 0.693537
[Epoch 15] ogbg-molbace: 0.514166 test loss: 0.692787
[Epoch 16; Iter    15/   31] train: loss: 0.6935136
[Epoch 16] ogbg-molbace: 0.486197 val loss: 0.692839
[Epoch 16] ogbg-molbace: 0.513239 test loss: 0.692978
[Epoch 17; Iter    14/   31] train: loss: 0.6947927
[Epoch 17] ogbg-molbace: 0.490566 val loss: 0.692368
[Epoch 17] ogbg-molbace: 0.526788 test loss: 0.693009
[Epoch 18; Iter    13/   31] train: loss: 0.6920216
[Epoch 18] ogbg-molbace: 0.513009 val loss: 0.692524
[Epoch 18] ogbg-molbace: 0.536627 test loss: 0.692748
[Epoch 19; Iter    12/   31] train: loss: 0.6915992
[Epoch 19] ogbg-molbace: 0.511917 val loss: 0.692180
[Epoch 19] ogbg-molbace: 0.538378 test loss: 0.692689
[Epoch 20; Iter    11/   31] train: loss: 0.6911419
[Epoch 20] ogbg-molbace: 0.529990 val loss: 0.692068
[Epoch 20] ogbg-molbace: 0.559860 test loss: 0.692541
[Epoch 21; Iter    10/   31] train: loss: 0.6946143
[Epoch 21] ogbg-molbace: 0.547567 val loss: 0.691083
[Epoch 21] ogbg-molbace: 0.551669 test loss: 0.692760
[Epoch 22; Iter     9/   31] train: loss: 0.6939917
[Epoch 22] ogbg-molbace: 0.584310 val loss: 0.690897
[Epoch 22] ogbg-molbace: 0.581754 test loss: 0.692428
[Epoch 23; Iter     8/   31] train: loss: 0.6864694
[Epoch 23] ogbg-molbace: 0.670109 val loss: 0.707910
[Epoch 23] ogbg-molbace: 0.771894 test loss: 0.670625
[Epoch 24; Iter     7/   31] train: loss: 0.6675122
[Epoch 24] ogbg-molbace: 0.711420 val loss: 0.673523
[Epoch 24] ogbg-molbace: 0.772872 test loss: 0.653591
[Epoch 25; Iter     6/   31] train: loss: 0.6549690
[Epoch 25] ogbg-molbace: 0.701887 val loss: 0.688798
[Epoch 25] ogbg-molbace: 0.776891 test loss: 0.616638
[Epoch 26; Iter     5/   31] train: loss: 0.5788264
[Epoch 26] ogbg-molbace: 0.671797 val loss: 0.662067
[Epoch 26] ogbg-molbace: 0.743406 test loss: 0.608709
[Epoch 27; Iter     4/   31] train: loss: 0.5325214
[Epoch 27] ogbg-molbace: 0.734757 val loss: 0.429565
[Epoch 27] ogbg-molbace: 0.767463 test loss: 0.734603
[Epoch 28; Iter     3/   31] train: loss: 0.5349118
[Epoch 28] ogbg-molbace: 0.773883 val loss: 1.002333
[Epoch 28] ogbg-molbace: 0.761745 test loss: 0.608235
[Epoch 29; Iter     2/   31] train: loss: 0.5086229
[Epoch 29] ogbg-molbace: 0.658193 val loss: 0.544259
[Epoch 29] ogbg-molbace: 0.721512 test loss: 0.678538
[Epoch 30; Iter     1/   31] train: loss: 0.4446939
[Epoch 30; Iter    31/   31] train: loss: 0.6432487
[Epoch 30] ogbg-molbace: 0.712512 val loss: 0.671708
[Epoch 30] ogbg-molbace: 0.758036 test loss: 0.614709
[Epoch 31; Iter    30/   31] train: loss: 0.5822655
[Epoch 31] ogbg-molbace: 0.678848 val loss: 0.506405
[Epoch 31] ogbg-molbace: 0.734340 test loss: 0.764991
[Epoch 32; Iter    29/   31] train: loss: 0.5926057
[Epoch 32] ogbg-molbace: 0.743893 val loss: 0.591974
[Epoch 32] ogbg-molbace: 0.776839 test loss: 0.579536
[Epoch 33; Iter    28/   31] train: loss: 0.3251866
[Epoch 33] ogbg-molbace: 0.753823 val loss: 0.360941
[Epoch 33] ogbg-molbace: 0.764785 test loss: 0.828589
[Epoch 34; Iter    27/   31] train: loss: 0.5512449
[Epoch 34] ogbg-molbace: 0.754816 val loss: 0.402509
[Epoch 34] ogbg-molbace: 0.756697 test loss: 0.834631
[Epoch 35; Iter    26/   31] train: loss: 0.3542829
[Epoch 35] ogbg-molbace: 0.775670 val loss: 0.396310
[Epoch 35] ogbg-molbace: 0.758860 test loss: 0.753465
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.7/PNA_ogbg-molbace_GraphCL_bace_scaff=0.7_6_26-05_09-42-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.7
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6963778
[Epoch 1] ogbg-molbace: 0.467336 val loss: 0.693426
[Epoch 1] ogbg-molbace: 0.355273 test loss: 0.693907
[Epoch 2; Iter    24/   36] train: loss: 0.6951997
[Epoch 2] ogbg-molbace: 0.457822 val loss: 0.694547
[Epoch 2] ogbg-molbace: 0.331437 test loss: 0.697073
[Epoch 3; Iter    18/   36] train: loss: 0.6991487
[Epoch 3] ogbg-molbace: 0.456342 val loss: 0.695995
[Epoch 3] ogbg-molbace: 0.343084 test loss: 0.698083
[Epoch 4; Iter    12/   36] train: loss: 0.6988862
[Epoch 4] ogbg-molbace: 0.455180 val loss: 0.695857
[Epoch 4] ogbg-molbace: 0.340827 test loss: 0.698053
[Epoch 5; Iter     6/   36] train: loss: 0.6951724
[Epoch 5; Iter    36/   36] train: loss: 0.6913425
[Epoch 5] ogbg-molbace: 0.458879 val loss: 0.695010
[Epoch 5] ogbg-molbace: 0.343084 test loss: 0.698895
[Epoch 6; Iter    30/   36] train: loss: 0.6975300
[Epoch 6] ogbg-molbace: 0.461311 val loss: 0.695225
[Epoch 6] ogbg-molbace: 0.352654 test loss: 0.698504
[Epoch 7; Iter    24/   36] train: loss: 0.6979706
[Epoch 7] ogbg-molbace: 0.461839 val loss: 0.694860
[Epoch 7] ogbg-molbace: 0.352022 test loss: 0.698840
[Epoch 8; Iter    18/   36] train: loss: 0.6953852
[Epoch 8] ogbg-molbace: 0.460465 val loss: 0.694516
[Epoch 8] ogbg-molbace: 0.355182 test loss: 0.699257
[Epoch 9; Iter    12/   36] train: loss: 0.6955655
[Epoch 9] ogbg-molbace: 0.464588 val loss: 0.694572
[Epoch 9] ogbg-molbace: 0.364301 test loss: 0.698665
[Epoch 10; Iter     6/   36] train: loss: 0.6911646
[Epoch 10; Iter    36/   36] train: loss: 0.6960084
[Epoch 10] ogbg-molbace: 0.463848 val loss: 0.693508
[Epoch 10] ogbg-molbace: 0.358974 test loss: 0.699685
[Epoch 11; Iter    30/   36] train: loss: 0.6935264
[Epoch 11] ogbg-molbace: 0.465962 val loss: 0.692781
[Epoch 11] ogbg-molbace: 0.362134 test loss: 0.700066
[Epoch 12; Iter    24/   36] train: loss: 0.6955885
[Epoch 12] ogbg-molbace: 0.466490 val loss: 0.692178
[Epoch 12] ogbg-molbace: 0.369718 test loss: 0.699997
[Epoch 13; Iter    18/   36] train: loss: 0.6984026
[Epoch 13] ogbg-molbace: 0.467653 val loss: 0.692241
[Epoch 13] ogbg-molbace: 0.369628 test loss: 0.700074
[Epoch 14; Iter    12/   36] train: loss: 0.6917373
[Epoch 14] ogbg-molbace: 0.475053 val loss: 0.691712
[Epoch 14] ogbg-molbace: 0.381726 test loss: 0.699943
[Epoch 15; Iter     6/   36] train: loss: 0.6916987
[Epoch 15; Iter    36/   36] train: loss: 0.6964936
[Epoch 15] ogbg-molbace: 0.468710 val loss: 0.690910
[Epoch 15] ogbg-molbace: 0.381275 test loss: 0.700430
[Epoch 16; Iter    30/   36] train: loss: 0.6967926
[Epoch 16] ogbg-molbace: 0.475581 val loss: 0.690050
[Epoch 16] ogbg-molbace: 0.383803 test loss: 0.701256
[Epoch 17; Iter    24/   36] train: loss: 0.6964671
[Epoch 17] ogbg-molbace: 0.474736 val loss: 0.689521
[Epoch 17] ogbg-molbace: 0.390123 test loss: 0.701440
[Epoch 18; Iter    18/   36] train: loss: 0.6947699
[Epoch 18] ogbg-molbace: 0.475370 val loss: 0.688921
[Epoch 18] ogbg-molbace: 0.392741 test loss: 0.701509
[Epoch 19; Iter    12/   36] train: loss: 0.6931595
[Epoch 19] ogbg-molbace: 0.483087 val loss: 0.688386
[Epoch 19] ogbg-molbace: 0.421542 test loss: 0.701145
[Epoch 20; Iter     6/   36] train: loss: 0.6873524
[Epoch 20; Iter    36/   36] train: loss: 0.6633997
[Epoch 20] ogbg-molbace: 0.637526 val loss: 0.689434
[Epoch 20] ogbg-molbace: 0.791983 test loss: 0.657635
[Epoch 21; Iter    30/   36] train: loss: 0.6169981
[Epoch 21] ogbg-molbace: 0.652114 val loss: 0.703909
[Epoch 21] ogbg-molbace: 0.757584 test loss: 0.632703
[Epoch 22; Iter    24/   36] train: loss: 0.6321532
[Epoch 22] ogbg-molbace: 0.685835 val loss: 0.642096
[Epoch 22] ogbg-molbace: 0.794962 test loss: 0.611363
[Epoch 23; Iter    18/   36] train: loss: 0.5691299
[Epoch 23] ogbg-molbace: 0.654123 val loss: 0.689074
[Epoch 23] ogbg-molbace: 0.786475 test loss: 0.558451
[Epoch 24; Iter    12/   36] train: loss: 0.5267569
[Epoch 24] ogbg-molbace: 0.693658 val loss: 0.532140
[Epoch 24] ogbg-molbace: 0.810401 test loss: 0.719598
[Epoch 25; Iter     6/   36] train: loss: 0.4749818
[Epoch 25; Iter    36/   36] train: loss: 0.4740883
[Epoch 25] ogbg-molbace: 0.693129 val loss: 0.674556
[Epoch 25] ogbg-molbace: 0.791531 test loss: 0.596442
[Epoch 26; Iter    30/   36] train: loss: 0.5484571
[Epoch 26] ogbg-molbace: 0.695032 val loss: 0.580128
[Epoch 26] ogbg-molbace: 0.796407 test loss: 0.713369
[Epoch 27; Iter    24/   36] train: loss: 0.4045121
[Epoch 27] ogbg-molbace: 0.706342 val loss: 0.767786
[Epoch 27] ogbg-molbace: 0.829451 test loss: 0.505067
[Epoch 28; Iter    18/   36] train: loss: 0.3297113
[Epoch 28] ogbg-molbace: 0.713531 val loss: 0.639594
[Epoch 28] ogbg-molbace: 0.827284 test loss: 0.572737
[Epoch 29; Iter    12/   36] train: loss: 0.4323676
[Epoch 29] ogbg-molbace: 0.711522 val loss: 0.982183
[Epoch 29] ogbg-molbace: 0.806609 test loss: 0.513544
[Epoch 30; Iter     6/   36] train: loss: 0.4713905
[Epoch 30; Iter    36/   36] train: loss: 1.0840651
[Epoch 30] ogbg-molbace: 0.718499 val loss: 0.856760
[Epoch 30] ogbg-molbace: 0.807602 test loss: 0.528861
[Epoch 31; Iter    30/   36] train: loss: 0.3268903
[Epoch 31] ogbg-molbace: 0.712368 val loss: 0.604962
[Epoch 31] ogbg-molbace: 0.820874 test loss: 0.732325
[Epoch 32; Iter    24/   36] train: loss: 0.2743260
[Epoch 32] ogbg-molbace: 0.688266 val loss: 0.601944
[Epoch 32] ogbg-molbace: 0.765980 test loss: 1.052253
[Epoch 33; Iter    18/   36] train: loss: 0.4874274
[Epoch 33] ogbg-molbace: 0.703383 val loss: 0.521474
[Epoch 33] ogbg-molbace: 0.812568 test loss: 1.021912
[Epoch 34; Iter    12/   36] train: loss: 0.3631515
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.8/PNA_ogbg-molbace_GraphCL_bace_scaff=0.8_6_26-05_09-42-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.8
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6950312
[Epoch 1] ogbg-molbace: 0.390842 val loss: 0.693616
[Epoch 1] ogbg-molbace: 0.308294 test loss: 0.693569
[Epoch 2; Iter    19/   41] train: loss: 0.7015147
[Epoch 2] ogbg-molbace: 0.383150 val loss: 0.694402
[Epoch 2] ogbg-molbace: 0.298035 test loss: 0.695208
[Epoch 3; Iter     8/   41] train: loss: 0.6928485
[Epoch 3; Iter    38/   41] train: loss: 0.7026150
[Epoch 3] ogbg-molbace: 0.381685 val loss: 0.695068
[Epoch 3] ogbg-molbace: 0.304643 test loss: 0.696302
[Epoch 4; Iter    27/   41] train: loss: 0.6963117
[Epoch 4] ogbg-molbace: 0.366667 val loss: 0.695214
[Epoch 4] ogbg-molbace: 0.300122 test loss: 0.696418
[Epoch 5; Iter    16/   41] train: loss: 0.6983405
[Epoch 5] ogbg-molbace: 0.372894 val loss: 0.695499
[Epoch 5] ogbg-molbace: 0.309511 test loss: 0.696708
[Epoch 6; Iter     5/   41] train: loss: 0.6953483
[Epoch 6; Iter    35/   41] train: loss: 0.6975852
[Epoch 6] ogbg-molbace: 0.376190 val loss: 0.695585
[Epoch 6] ogbg-molbace: 0.305164 test loss: 0.696479
[Epoch 7; Iter    24/   41] train: loss: 0.6994967
[Epoch 7] ogbg-molbace: 0.379121 val loss: 0.696618
[Epoch 7] ogbg-molbace: 0.306555 test loss: 0.696888
[Epoch 8; Iter    13/   41] train: loss: 0.6998385
[Epoch 8] ogbg-molbace: 0.374725 val loss: 0.696639
[Epoch 8] ogbg-molbace: 0.305338 test loss: 0.696956
[Epoch 9; Iter     2/   41] train: loss: 0.6927477
[Epoch 9; Iter    32/   41] train: loss: 0.6953177
[Epoch 9] ogbg-molbace: 0.388278 val loss: 0.697411
[Epoch 9] ogbg-molbace: 0.313685 test loss: 0.697470
[Epoch 10; Iter    21/   41] train: loss: 0.6960940
[Epoch 10] ogbg-molbace: 0.376557 val loss: 0.698159
[Epoch 10] ogbg-molbace: 0.318206 test loss: 0.697161
[Epoch 11; Iter    10/   41] train: loss: 0.6949072
[Epoch 11; Iter    40/   41] train: loss: 0.6967743
[Epoch 11] ogbg-molbace: 0.379121 val loss: 0.698883
[Epoch 11] ogbg-molbace: 0.315597 test loss: 0.698364
[Epoch 12; Iter    29/   41] train: loss: 0.6955029
[Epoch 12] ogbg-molbace: 0.379487 val loss: 0.699180
[Epoch 12] ogbg-molbace: 0.323596 test loss: 0.698249
[Epoch 13; Iter    18/   41] train: loss: 0.6949422
[Epoch 13] ogbg-molbace: 0.395604 val loss: 0.700263
[Epoch 13] ogbg-molbace: 0.332290 test loss: 0.698399
[Epoch 14; Iter     7/   41] train: loss: 0.6895126
[Epoch 14; Iter    37/   41] train: loss: 0.6925826
[Epoch 14] ogbg-molbace: 0.394139 val loss: 0.701040
[Epoch 14] ogbg-molbace: 0.333159 test loss: 0.699072
[Epoch 15; Iter    26/   41] train: loss: 0.6897358
[Epoch 15] ogbg-molbace: 0.402930 val loss: 0.701895
[Epoch 15] ogbg-molbace: 0.342201 test loss: 0.698796
[Epoch 16; Iter    15/   41] train: loss: 0.6896077
[Epoch 16] ogbg-molbace: 0.405861 val loss: 0.703070
[Epoch 16] ogbg-molbace: 0.343592 test loss: 0.699651
[Epoch 17; Iter     4/   41] train: loss: 0.6878887
[Epoch 17; Iter    34/   41] train: loss: 0.6983106
[Epoch 17] ogbg-molbace: 0.638462 val loss: 0.695647
[Epoch 17] ogbg-molbace: 0.665623 test loss: 0.690611
[Epoch 18; Iter    23/   41] train: loss: 0.6668013
[Epoch 18] ogbg-molbace: 0.669231 val loss: 0.687927
[Epoch 18] ogbg-molbace: 0.757086 test loss: 0.674837
[Epoch 19; Iter    12/   41] train: loss: 0.6310849
[Epoch 19] ogbg-molbace: 0.655678 val loss: 0.686492
[Epoch 19] ogbg-molbace: 0.742653 test loss: 0.709502
[Epoch 20; Iter     1/   41] train: loss: 0.5587086
[Epoch 20; Iter    31/   41] train: loss: 0.5533006
[Epoch 20] ogbg-molbace: 0.633700 val loss: 0.764389
[Epoch 20] ogbg-molbace: 0.761954 test loss: 0.784464
[Epoch 21; Iter    20/   41] train: loss: 0.5678983
[Epoch 21] ogbg-molbace: 0.620879 val loss: 0.862770
[Epoch 21] ogbg-molbace: 0.727700 test loss: 0.916149
[Epoch 22; Iter     9/   41] train: loss: 0.5027606
[Epoch 22; Iter    39/   41] train: loss: 0.4658118
[Epoch 22] ogbg-molbace: 0.674725 val loss: 0.719713
[Epoch 22] ogbg-molbace: 0.764041 test loss: 0.618284
[Epoch 23; Iter    28/   41] train: loss: 0.4154097
[Epoch 23] ogbg-molbace: 0.699634 val loss: 0.745083
[Epoch 23] ogbg-molbace: 0.787167 test loss: 0.840309
[Epoch 24; Iter    17/   41] train: loss: 0.3310628
[Epoch 24] ogbg-molbace: 0.678022 val loss: 0.854424
[Epoch 24] ogbg-molbace: 0.793427 test loss: 0.910190
[Epoch 25; Iter     6/   41] train: loss: 0.3499763
[Epoch 25; Iter    36/   41] train: loss: 0.2955730
[Epoch 25] ogbg-molbace: 0.675458 val loss: 0.758316
[Epoch 25] ogbg-molbace: 0.787863 test loss: 0.853016
[Epoch 26; Iter    25/   41] train: loss: 0.4223655
[Epoch 26] ogbg-molbace: 0.694872 val loss: 0.880113
[Epoch 26] ogbg-molbace: 0.767866 test loss: 1.073811
[Epoch 27; Iter    14/   41] train: loss: 0.4835436
[Epoch 27] ogbg-molbace: 0.695604 val loss: 0.675150
[Epoch 27] ogbg-molbace: 0.794818 test loss: 0.662657
[Epoch 28; Iter     3/   41] train: loss: 0.4393473
[Epoch 28; Iter    33/   41] train: loss: 0.4775278
[Epoch 28] ogbg-molbace: 0.670330 val loss: 0.724400
[Epoch 28] ogbg-molbace: 0.757955 test loss: 0.941781
[Epoch 29; Iter    22/   41] train: loss: 0.5146201
[Epoch 29] ogbg-molbace: 0.688278 val loss: 0.625472
[Epoch 29] ogbg-molbace: 0.762128 test loss: 0.741384
[Epoch 30; Iter    11/   41] train: loss: 0.4588378
[Epoch 30; Iter    41/   41] train: loss: 0.5497862
[Epoch 30] ogbg-molbace: 0.642857 val loss: 0.782079
[Epoch 30] ogbg-molbace: 0.779343 test loss: 0.755651
[Epoch 31; Iter    30/   41] train: loss: 0.4355943
[Epoch 31] ogbg-molbace: 0.610989 val loss: 1.532749
[Epoch 31] ogbg-molbace: 0.751348 test loss: 1.682485
[Epoch 32; Iter    19/   41] train: loss: 0.3911993
[Epoch 32] ogbg-molbace: 0.716117 val loss: 0.750523
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.7/PNA_ogbg-molbace_GraphCL_bace_scaff=0.7_4_26-05_09-42-08
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.7
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6912849
[Epoch 1] ogbg-molbace: 0.482981 val loss: 0.694105
[Epoch 1] ogbg-molbace: 0.406916 test loss: 0.693352
[Epoch 2; Iter    24/   36] train: loss: 0.7012216
[Epoch 2] ogbg-molbace: 0.450740 val loss: 0.696639
[Epoch 2] ogbg-molbace: 0.377754 test loss: 0.694956
[Epoch 3; Iter    18/   36] train: loss: 0.6960407
[Epoch 3] ogbg-molbace: 0.430021 val loss: 0.697862
[Epoch 3] ogbg-molbace: 0.367190 test loss: 0.695640
[Epoch 4; Iter    12/   36] train: loss: 0.6943420
[Epoch 4] ogbg-molbace: 0.422304 val loss: 0.698650
[Epoch 4] ogbg-molbace: 0.370170 test loss: 0.694944
[Epoch 5; Iter     6/   36] train: loss: 0.7000186
[Epoch 5; Iter    36/   36] train: loss: 0.6888196
[Epoch 5] ogbg-molbace: 0.420190 val loss: 0.698232
[Epoch 5] ogbg-molbace: 0.366649 test loss: 0.695250
[Epoch 6; Iter    30/   36] train: loss: 0.6964839
[Epoch 6] ogbg-molbace: 0.427378 val loss: 0.697648
[Epoch 6] ogbg-molbace: 0.372788 test loss: 0.695616
[Epoch 7; Iter    24/   36] train: loss: 0.6972664
[Epoch 7] ogbg-molbace: 0.430444 val loss: 0.697480
[Epoch 7] ogbg-molbace: 0.374955 test loss: 0.695504
[Epoch 8; Iter    18/   36] train: loss: 0.6961545
[Epoch 8] ogbg-molbace: 0.424419 val loss: 0.697701
[Epoch 8] ogbg-molbace: 0.383893 test loss: 0.694920
[Epoch 9; Iter    12/   36] train: loss: 0.6948565
[Epoch 9] ogbg-molbace: 0.423150 val loss: 0.696584
[Epoch 9] ogbg-molbace: 0.380462 test loss: 0.696099
[Epoch 10; Iter     6/   36] train: loss: 0.6964624
[Epoch 10; Iter    36/   36] train: loss: 0.6936296
[Epoch 10] ogbg-molbace: 0.431501 val loss: 0.696912
[Epoch 10] ogbg-molbace: 0.381636 test loss: 0.695562
[Epoch 11; Iter    30/   36] train: loss: 0.6967543
[Epoch 11] ogbg-molbace: 0.428436 val loss: 0.696755
[Epoch 11] ogbg-molbace: 0.386602 test loss: 0.695772
[Epoch 12; Iter    24/   36] train: loss: 0.6938534
[Epoch 12] ogbg-molbace: 0.435307 val loss: 0.695197
[Epoch 12] ogbg-molbace: 0.387775 test loss: 0.696745
[Epoch 13; Iter    18/   36] train: loss: 0.6920393
[Epoch 13] ogbg-molbace: 0.433721 val loss: 0.694960
[Epoch 13] ogbg-molbace: 0.387053 test loss: 0.696816
[Epoch 14; Iter    12/   36] train: loss: 0.6936957
[Epoch 14] ogbg-molbace: 0.437949 val loss: 0.694766
[Epoch 14] ogbg-molbace: 0.400776 test loss: 0.696724
[Epoch 15; Iter     6/   36] train: loss: 0.6924239
[Epoch 15; Iter    36/   36] train: loss: 0.6917891
[Epoch 15] ogbg-molbace: 0.439006 val loss: 0.693398
[Epoch 15] ogbg-molbace: 0.405562 test loss: 0.697354
[Epoch 16; Iter    30/   36] train: loss: 0.6978893
[Epoch 16] ogbg-molbace: 0.443975 val loss: 0.692525
[Epoch 16] ogbg-molbace: 0.420549 test loss: 0.697601
[Epoch 17; Iter    24/   36] train: loss: 0.6895667
[Epoch 17] ogbg-molbace: 0.451797 val loss: 0.691689
[Epoch 17] ogbg-molbace: 0.426508 test loss: 0.698042
[Epoch 18; Iter    18/   36] train: loss: 0.6914502
[Epoch 18] ogbg-molbace: 0.443869 val loss: 0.691221
[Epoch 18] ogbg-molbace: 0.427140 test loss: 0.698313
[Epoch 19; Iter    12/   36] train: loss: 0.6892481
[Epoch 19] ogbg-molbace: 0.454440 val loss: 0.690069
[Epoch 19] ogbg-molbace: 0.445016 test loss: 0.698621
[Epoch 20; Iter     6/   36] train: loss: 0.6879566
[Epoch 20; Iter    36/   36] train: loss: 0.6759245
[Epoch 20] ogbg-molbace: 0.657400 val loss: 0.694458
[Epoch 20] ogbg-molbace: 0.819339 test loss: 0.661978
[Epoch 21; Iter    30/   36] train: loss: 0.6535715
[Epoch 21] ogbg-molbace: 0.652008 val loss: 0.714670
[Epoch 21] ogbg-molbace: 0.784850 test loss: 0.604171
[Epoch 22; Iter    24/   36] train: loss: 0.5942212
[Epoch 22] ogbg-molbace: 0.637949 val loss: 0.729711
[Epoch 22] ogbg-molbace: 0.790267 test loss: 0.551679
[Epoch 23; Iter    18/   36] train: loss: 0.5767520
[Epoch 23] ogbg-molbace: 0.661311 val loss: 0.736008
[Epoch 23] ogbg-molbace: 0.754966 test loss: 0.613944
[Epoch 24; Iter    12/   36] train: loss: 0.5887978
[Epoch 24] ogbg-molbace: 0.694609 val loss: 0.614456
[Epoch 24] ogbg-molbace: 0.797400 test loss: 0.658297
[Epoch 25; Iter     6/   36] train: loss: 0.4679860
[Epoch 25; Iter    36/   36] train: loss: 0.4565061
[Epoch 25] ogbg-molbace: 0.714482 val loss: 0.687855
[Epoch 25] ogbg-molbace: 0.825479 test loss: 0.542874
[Epoch 26; Iter    30/   36] train: loss: 0.3552152
[Epoch 26] ogbg-molbace: 0.713108 val loss: 0.612671
[Epoch 26] ogbg-molbace: 0.810581 test loss: 0.619118
[Epoch 27; Iter    24/   36] train: loss: 0.4366978
[Epoch 27] ogbg-molbace: 0.721142 val loss: 0.622633
[Epoch 27] ogbg-molbace: 0.811936 test loss: 0.636934
[Epoch 28; Iter    18/   36] train: loss: 0.4189813
[Epoch 28] ogbg-molbace: 0.707400 val loss: 0.514085
[Epoch 28] ogbg-molbace: 0.805796 test loss: 1.094338
[Epoch 29; Iter    12/   36] train: loss: 0.4577282
[Epoch 29] ogbg-molbace: 0.697886 val loss: 0.865474
[Epoch 29] ogbg-molbace: 0.788191 test loss: 0.528464
[Epoch 30; Iter     6/   36] train: loss: 0.5335824
[Epoch 30; Iter    36/   36] train: loss: 0.3525604
[Epoch 30] ogbg-molbace: 0.715856 val loss: 0.582075
[Epoch 30] ogbg-molbace: 0.823312 test loss: 0.716975
[Epoch 31; Iter    30/   36] train: loss: 0.5515627
[Epoch 31] ogbg-molbace: 0.710677 val loss: 0.735634
[Epoch 31] ogbg-molbace: 0.798573 test loss: 0.643494
[Epoch 32; Iter    24/   36] train: loss: 0.3185908
[Epoch 32] ogbg-molbace: 0.703277 val loss: 0.639904
[Epoch 32] ogbg-molbace: 0.810852 test loss: 0.666239
[Epoch 33; Iter    18/   36] train: loss: 0.3774056
[Epoch 33] ogbg-molbace: 0.715011 val loss: 1.096966
[Epoch 33] ogbg-molbace: 0.826923 test loss: 0.492147
[Epoch 34; Iter    12/   36] train: loss: 0.3629734
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.8/PNA_ogbg-molbace_GraphCL_bace_scaff=0.8_4_26-05_09-42-09
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.8
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6946846
[Epoch 1] ogbg-molbace: 0.402930 val loss: 0.692686
[Epoch 1] ogbg-molbace: 0.399061 test loss: 0.693830
[Epoch 2; Iter    19/   41] train: loss: 0.6947215
[Epoch 2] ogbg-molbace: 0.359341 val loss: 0.692525
[Epoch 2] ogbg-molbace: 0.390193 test loss: 0.695428
[Epoch 3; Iter     8/   41] train: loss: 0.6890867
[Epoch 3; Iter    38/   41] train: loss: 0.6967059
[Epoch 3] ogbg-molbace: 0.366300 val loss: 0.692163
[Epoch 3] ogbg-molbace: 0.382368 test loss: 0.695824
[Epoch 4; Iter    27/   41] train: loss: 0.6968171
[Epoch 4] ogbg-molbace: 0.361172 val loss: 0.692548
[Epoch 4] ogbg-molbace: 0.383585 test loss: 0.695948
[Epoch 5; Iter    16/   41] train: loss: 0.6947884
[Epoch 5] ogbg-molbace: 0.367033 val loss: 0.692152
[Epoch 5] ogbg-molbace: 0.390193 test loss: 0.695624
[Epoch 6; Iter     5/   41] train: loss: 0.7019798
[Epoch 6; Iter    35/   41] train: loss: 0.6907607
[Epoch 6] ogbg-molbace: 0.364103 val loss: 0.692974
[Epoch 6] ogbg-molbace: 0.383759 test loss: 0.696172
[Epoch 7; Iter    24/   41] train: loss: 0.6940095
[Epoch 7] ogbg-molbace: 0.372894 val loss: 0.693015
[Epoch 7] ogbg-molbace: 0.395236 test loss: 0.695967
[Epoch 8; Iter    13/   41] train: loss: 0.6914994
[Epoch 8] ogbg-molbace: 0.363736 val loss: 0.694182
[Epoch 8] ogbg-molbace: 0.387063 test loss: 0.696765
[Epoch 9; Iter     2/   41] train: loss: 0.6980399
[Epoch 9; Iter    32/   41] train: loss: 0.6966116
[Epoch 9] ogbg-molbace: 0.364469 val loss: 0.694568
[Epoch 9] ogbg-molbace: 0.386889 test loss: 0.696485
[Epoch 10; Iter    21/   41] train: loss: 0.6957610
[Epoch 10] ogbg-molbace: 0.368498 val loss: 0.694874
[Epoch 10] ogbg-molbace: 0.398366 test loss: 0.696535
[Epoch 11; Iter    10/   41] train: loss: 0.6995579
[Epoch 11; Iter    40/   41] train: loss: 0.6941286
[Epoch 11] ogbg-molbace: 0.371062 val loss: 0.696089
[Epoch 11] ogbg-molbace: 0.396279 test loss: 0.697022
[Epoch 12; Iter    29/   41] train: loss: 0.6982664
[Epoch 12] ogbg-molbace: 0.372894 val loss: 0.696794
[Epoch 12] ogbg-molbace: 0.404104 test loss: 0.697138
[Epoch 13; Iter    18/   41] train: loss: 0.6940095
[Epoch 13] ogbg-molbace: 0.380220 val loss: 0.697883
[Epoch 13] ogbg-molbace: 0.403930 test loss: 0.697782
[Epoch 14; Iter     7/   41] train: loss: 0.6958494
[Epoch 14; Iter    37/   41] train: loss: 0.6903545
[Epoch 14] ogbg-molbace: 0.383150 val loss: 0.698198
[Epoch 14] ogbg-molbace: 0.404104 test loss: 0.697857
[Epoch 15; Iter    26/   41] train: loss: 0.6947518
[Epoch 15] ogbg-molbace: 0.385348 val loss: 0.699027
[Epoch 15] ogbg-molbace: 0.411233 test loss: 0.698149
[Epoch 16; Iter    15/   41] train: loss: 0.6920192
[Epoch 16] ogbg-molbace: 0.389011 val loss: 0.700167
[Epoch 16] ogbg-molbace: 0.417319 test loss: 0.698587
[Epoch 17; Iter     4/   41] train: loss: 0.6936738
[Epoch 17; Iter    34/   41] train: loss: 0.6849753
[Epoch 17] ogbg-molbace: 0.661538 val loss: 0.680783
[Epoch 17] ogbg-molbace: 0.742653 test loss: 0.686086
[Epoch 18; Iter    23/   41] train: loss: 0.6545444
[Epoch 18] ogbg-molbace: 0.674359 val loss: 0.681607
[Epoch 18] ogbg-molbace: 0.760911 test loss: 0.683911
[Epoch 19; Iter    12/   41] train: loss: 0.6116589
[Epoch 19] ogbg-molbace: 0.669231 val loss: 0.649174
[Epoch 19] ogbg-molbace: 0.756912 test loss: 0.684262
[Epoch 20; Iter     1/   41] train: loss: 0.5737045
[Epoch 20; Iter    31/   41] train: loss: 0.5188594
[Epoch 20] ogbg-molbace: 0.677656 val loss: 0.629087
[Epoch 20] ogbg-molbace: 0.783864 test loss: 0.610843
[Epoch 21; Iter    20/   41] train: loss: 0.4748307
[Epoch 21] ogbg-molbace: 0.662637 val loss: 0.836232
[Epoch 21] ogbg-molbace: 0.783516 test loss: 0.818938
[Epoch 22; Iter     9/   41] train: loss: 0.5596997
[Epoch 22; Iter    39/   41] train: loss: 0.3438788
[Epoch 22] ogbg-molbace: 0.672527 val loss: 0.717183
[Epoch 22] ogbg-molbace: 0.769084 test loss: 0.819382
[Epoch 23; Iter    28/   41] train: loss: 0.4405135
[Epoch 23] ogbg-molbace: 0.721612 val loss: 0.648041
[Epoch 23] ogbg-molbace: 0.784907 test loss: 0.716864
[Epoch 24; Iter    17/   41] train: loss: 0.5653491
[Epoch 24] ogbg-molbace: 0.712454 val loss: 0.692394
[Epoch 24] ogbg-molbace: 0.798122 test loss: 0.684356
[Epoch 25; Iter     6/   41] train: loss: 0.4259747
[Epoch 25; Iter    36/   41] train: loss: 0.4918485
[Epoch 25] ogbg-molbace: 0.691209 val loss: 0.942795
[Epoch 25] ogbg-molbace: 0.804730 test loss: 0.947914
[Epoch 26; Iter    25/   41] train: loss: 0.4408578
[Epoch 26] ogbg-molbace: 0.645788 val loss: 0.814016
[Epoch 26] ogbg-molbace: 0.775517 test loss: 0.884437
[Epoch 27; Iter    14/   41] train: loss: 0.3231972
[Epoch 27] ogbg-molbace: 0.648718 val loss: 0.834537
[Epoch 27] ogbg-molbace: 0.774126 test loss: 0.761511
[Epoch 28; Iter     3/   41] train: loss: 0.4116565
[Epoch 28; Iter    33/   41] train: loss: 0.4338376
[Epoch 28] ogbg-molbace: 0.731136 val loss: 0.756094
[Epoch 28] ogbg-molbace: 0.798296 test loss: 0.890204
[Epoch 29; Iter    22/   41] train: loss: 0.5851269
[Epoch 29] ogbg-molbace: 0.672161 val loss: 0.702919
[Epoch 29] ogbg-molbace: 0.790471 test loss: 0.894697
[Epoch 30; Iter    11/   41] train: loss: 0.4034610
[Epoch 30; Iter    41/   41] train: loss: 0.3163770
[Epoch 30] ogbg-molbace: 0.631136 val loss: 1.224884
[Epoch 30] ogbg-molbace: 0.761085 test loss: 1.247691
[Epoch 31; Iter    30/   41] train: loss: 0.3575637
[Epoch 31] ogbg-molbace: 0.639194 val loss: 1.092287
[Epoch 31] ogbg-molbace: 0.771170 test loss: 1.021075
[Epoch 32; Iter    19/   41] train: loss: 0.3953187
[Epoch 32] ogbg-molbace: 0.728938 val loss: 0.689394
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.6/PNA_ogbg-molbace_GraphCL_bace_scaff=0.6_4_26-05_09-42-08
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.6
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6988268
[Epoch 1] ogbg-molbace: 0.425124 val loss: 0.693925
[Epoch 1] ogbg-molbace: 0.412889 test loss: 0.693207
[Epoch 2; Iter    29/   31] train: loss: 0.6952682
[Epoch 2] ogbg-molbace: 0.384211 val loss: 0.696600
[Epoch 2] ogbg-molbace: 0.387286 test loss: 0.693820
[Epoch 3; Iter    28/   31] train: loss: 0.6914089
[Epoch 3] ogbg-molbace: 0.337239 val loss: 0.699695
[Epoch 3] ogbg-molbace: 0.370750 test loss: 0.694432
[Epoch 4; Iter    27/   31] train: loss: 0.6944562
[Epoch 4] ogbg-molbace: 0.334260 val loss: 0.700448
[Epoch 4] ogbg-molbace: 0.374871 test loss: 0.694377
[Epoch 5; Iter    26/   31] train: loss: 0.6935009
[Epoch 5] ogbg-molbace: 0.334955 val loss: 0.700705
[Epoch 5] ogbg-molbace: 0.372862 test loss: 0.694255
[Epoch 6; Iter    25/   31] train: loss: 0.6959109
[Epoch 6] ogbg-molbace: 0.333267 val loss: 0.700907
[Epoch 6] ogbg-molbace: 0.370853 test loss: 0.694077
[Epoch 7; Iter    24/   31] train: loss: 0.6935876
[Epoch 7] ogbg-molbace: 0.340914 val loss: 0.700455
[Epoch 7] ogbg-molbace: 0.381156 test loss: 0.694060
[Epoch 8; Iter    23/   31] train: loss: 0.6922365
[Epoch 8] ogbg-molbace: 0.340516 val loss: 0.699291
[Epoch 8] ogbg-molbace: 0.378786 test loss: 0.694464
[Epoch 9; Iter    22/   31] train: loss: 0.6911101
[Epoch 9] ogbg-molbace: 0.340914 val loss: 0.699401
[Epoch 9] ogbg-molbace: 0.383165 test loss: 0.694194
[Epoch 10; Iter    21/   31] train: loss: 0.6966881
[Epoch 10] ogbg-molbace: 0.335948 val loss: 0.699729
[Epoch 10] ogbg-molbace: 0.375695 test loss: 0.694496
[Epoch 11; Iter    20/   31] train: loss: 0.6912777
[Epoch 11] ogbg-molbace: 0.345581 val loss: 0.699194
[Epoch 11] ogbg-molbace: 0.378735 test loss: 0.694479
[Epoch 12; Iter    19/   31] train: loss: 0.6944320
[Epoch 12] ogbg-molbace: 0.341410 val loss: 0.698519
[Epoch 12] ogbg-molbace: 0.381826 test loss: 0.694716
[Epoch 13; Iter    18/   31] train: loss: 0.6897042
[Epoch 13] ogbg-molbace: 0.338729 val loss: 0.699109
[Epoch 13] ogbg-molbace: 0.384453 test loss: 0.694527
[Epoch 14; Iter    17/   31] train: loss: 0.6927158
[Epoch 14] ogbg-molbace: 0.341807 val loss: 0.697333
[Epoch 14] ogbg-molbace: 0.398207 test loss: 0.694590
[Epoch 15; Iter    16/   31] train: loss: 0.6993529
[Epoch 15] ogbg-molbace: 0.344290 val loss: 0.698109
[Epoch 15] ogbg-molbace: 0.393210 test loss: 0.694526
[Epoch 16; Iter    15/   31] train: loss: 0.6991561
[Epoch 16] ogbg-molbace: 0.347269 val loss: 0.697400
[Epoch 16] ogbg-molbace: 0.396713 test loss: 0.694380
[Epoch 17; Iter    14/   31] train: loss: 0.6968309
[Epoch 17] ogbg-molbace: 0.346971 val loss: 0.697177
[Epoch 17] ogbg-molbace: 0.398413 test loss: 0.694625
[Epoch 18; Iter    13/   31] train: loss: 0.6941909
[Epoch 18] ogbg-molbace: 0.349255 val loss: 0.696580
[Epoch 18] ogbg-molbace: 0.402431 test loss: 0.694736
[Epoch 19; Iter    12/   31] train: loss: 0.6937691
[Epoch 19] ogbg-molbace: 0.355909 val loss: 0.696658
[Epoch 19] ogbg-molbace: 0.410107 test loss: 0.694452
[Epoch 20; Iter    11/   31] train: loss: 0.6894689
[Epoch 20] ogbg-molbace: 0.355412 val loss: 0.696718
[Epoch 20] ogbg-molbace: 0.414589 test loss: 0.694277
[Epoch 21; Iter    10/   31] train: loss: 0.6941267
[Epoch 21] ogbg-molbace: 0.352234 val loss: 0.695467
[Epoch 21] ogbg-molbace: 0.417886 test loss: 0.694585
[Epoch 22; Iter     9/   31] train: loss: 0.6935621
[Epoch 22] ogbg-molbace: 0.371301 val loss: 0.695680
[Epoch 22] ogbg-molbace: 0.455388 test loss: 0.693589
[Epoch 23; Iter     8/   31] train: loss: 0.6886827
[Epoch 23] ogbg-molbace: 0.674578 val loss: 0.710931
[Epoch 23] ogbg-molbace: 0.769627 test loss: 0.664757
[Epoch 24; Iter     7/   31] train: loss: 0.6687623
[Epoch 24] ogbg-molbace: 0.625025 val loss: 0.658861
[Epoch 24] ogbg-molbace: 0.751700 test loss: 0.665169
[Epoch 25; Iter     6/   31] train: loss: 0.6457019
[Epoch 25] ogbg-molbace: 0.684310 val loss: 0.697202
[Epoch 25] ogbg-molbace: 0.782454 test loss: 0.602415
[Epoch 26; Iter     5/   31] train: loss: 0.5968724
[Epoch 26] ogbg-molbace: 0.697120 val loss: 0.491817
[Epoch 26] ogbg-molbace: 0.774985 test loss: 0.684392
[Epoch 27; Iter     4/   31] train: loss: 0.5784897
[Epoch 27] ogbg-molbace: 0.698411 val loss: 0.682556
[Epoch 27] ogbg-molbace: 0.761642 test loss: 0.575919
[Epoch 28; Iter     3/   31] train: loss: 0.6209028
[Epoch 28] ogbg-molbace: 0.722443 val loss: 0.631365
[Epoch 28] ogbg-molbace: 0.782248 test loss: 0.564151
[Epoch 29; Iter     2/   31] train: loss: 0.3834556
[Epoch 29] ogbg-molbace: 0.718471 val loss: 0.450285
[Epoch 29] ogbg-molbace: 0.774418 test loss: 0.670773
[Epoch 30; Iter     1/   31] train: loss: 0.4669688
[Epoch 30; Iter    31/   31] train: loss: 0.4056798
[Epoch 30] ogbg-molbace: 0.682423 val loss: 0.555413
[Epoch 30] ogbg-molbace: 0.735370 test loss: 0.673707
[Epoch 31; Iter    30/   31] train: loss: 0.6347592
[Epoch 31] ogbg-molbace: 0.769315 val loss: 0.791969
[Epoch 31] ogbg-molbace: 0.768597 test loss: 0.574329
[Epoch 32; Iter    29/   31] train: loss: 0.4647623
[Epoch 32] ogbg-molbace: 0.714101 val loss: 0.571512
[Epoch 32] ogbg-molbace: 0.768494 test loss: 0.660436
[Epoch 33; Iter    28/   31] train: loss: 0.4412295
[Epoch 33] ogbg-molbace: 0.709037 val loss: 0.468468
[Epoch 33] ogbg-molbace: 0.755873 test loss: 0.700582
[Epoch 34; Iter    27/   31] train: loss: 0.5901945
[Epoch 34] ogbg-molbace: 0.713505 val loss: 0.568392
[Epoch 34] ogbg-molbace: 0.775500 test loss: 0.626832
[Epoch 35; Iter    26/   31] train: loss: 0.5223916
[Epoch 35] ogbg-molbace: 0.734260 val loss: 0.433324
[Epoch 35] ogbg-molbace: 0.758448 test loss: 0.776923
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.7/PNA_ogbg-molbace_GraphCL_bace_scaff=0.7_5_26-05_09-42-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.7
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6935421
[Epoch 1] ogbg-molbace: 0.549154 val loss: 0.693694
[Epoch 1] ogbg-molbace: 0.493680 test loss: 0.693158
[Epoch 2; Iter    24/   36] train: loss: 0.6943516
[Epoch 2] ogbg-molbace: 0.517653 val loss: 0.694307
[Epoch 2] ogbg-molbace: 0.520404 test loss: 0.693012
[Epoch 3; Iter    18/   36] train: loss: 0.6923124
[Epoch 3] ogbg-molbace: 0.483510 val loss: 0.694721
[Epoch 3] ogbg-molbace: 0.492055 test loss: 0.693113
[Epoch 4; Iter    12/   36] train: loss: 0.6965270
[Epoch 4] ogbg-molbace: 0.479810 val loss: 0.694958
[Epoch 4] ogbg-molbace: 0.485193 test loss: 0.693277
[Epoch 5; Iter     6/   36] train: loss: 0.6951624
[Epoch 5; Iter    36/   36] train: loss: 0.6987526
[Epoch 5] ogbg-molbace: 0.473996 val loss: 0.694632
[Epoch 5] ogbg-molbace: 0.488985 test loss: 0.693424
[Epoch 6; Iter    30/   36] train: loss: 0.6945643
[Epoch 6] ogbg-molbace: 0.473679 val loss: 0.694170
[Epoch 6] ogbg-molbace: 0.486728 test loss: 0.693670
[Epoch 7; Iter    24/   36] train: loss: 0.6937711
[Epoch 7] ogbg-molbace: 0.471353 val loss: 0.694160
[Epoch 7] ogbg-molbace: 0.486006 test loss: 0.693824
[Epoch 8; Iter    18/   36] train: loss: 0.6967100
[Epoch 8] ogbg-molbace: 0.469345 val loss: 0.693654
[Epoch 8] ogbg-molbace: 0.498555 test loss: 0.693897
[Epoch 9; Iter    12/   36] train: loss: 0.6931318
[Epoch 9] ogbg-molbace: 0.484672 val loss: 0.693088
[Epoch 9] ogbg-molbace: 0.499729 test loss: 0.694360
[Epoch 10; Iter     6/   36] train: loss: 0.6904548
[Epoch 10; Iter    36/   36] train: loss: 0.6927714
[Epoch 10] ogbg-molbace: 0.488161 val loss: 0.692340
[Epoch 10] ogbg-molbace: 0.512730 test loss: 0.694446
[Epoch 11; Iter    30/   36] train: loss: 0.6926363
[Epoch 11] ogbg-molbace: 0.491966 val loss: 0.691759
[Epoch 11] ogbg-molbace: 0.517335 test loss: 0.694763
[Epoch 12; Iter    24/   36] train: loss: 0.6945154
[Epoch 12] ogbg-molbace: 0.490592 val loss: 0.690961
[Epoch 12] ogbg-molbace: 0.518960 test loss: 0.695294
[Epoch 13; Iter    18/   36] train: loss: 0.6960607
[Epoch 13] ogbg-molbace: 0.499894 val loss: 0.690661
[Epoch 13] ogbg-molbace: 0.533406 test loss: 0.695285
[Epoch 14; Iter    12/   36] train: loss: 0.6909930
[Epoch 14] ogbg-molbace: 0.512579 val loss: 0.689981
[Epoch 14] ogbg-molbace: 0.548122 test loss: 0.695394
[Epoch 15; Iter     6/   36] train: loss: 0.6936768
[Epoch 15; Iter    36/   36] train: loss: 0.6921718
[Epoch 15] ogbg-molbace: 0.524947 val loss: 0.689227
[Epoch 15] ogbg-molbace: 0.558505 test loss: 0.695804
[Epoch 16; Iter    30/   36] train: loss: 0.6901014
[Epoch 16] ogbg-molbace: 0.535095 val loss: 0.688489
[Epoch 16] ogbg-molbace: 0.575840 test loss: 0.695882
[Epoch 17; Iter    24/   36] train: loss: 0.6903378
[Epoch 17] ogbg-molbace: 0.531924 val loss: 0.687988
[Epoch 17] ogbg-molbace: 0.567172 test loss: 0.696390
[Epoch 18; Iter    18/   36] train: loss: 0.6936714
[Epoch 18] ogbg-molbace: 0.538055 val loss: 0.687224
[Epoch 18] ogbg-molbace: 0.574215 test loss: 0.696803
[Epoch 19; Iter    12/   36] train: loss: 0.6929726
[Epoch 19] ogbg-molbace: 0.558034 val loss: 0.686360
[Epoch 19] ogbg-molbace: 0.601932 test loss: 0.696837
[Epoch 20; Iter     6/   36] train: loss: 0.6938881
[Epoch 20; Iter    36/   36] train: loss: 0.6676612
[Epoch 20] ogbg-molbace: 0.656554 val loss: 0.690240
[Epoch 20] ogbg-molbace: 0.815908 test loss: 0.667184
[Epoch 21; Iter    30/   36] train: loss: 0.6544023
[Epoch 21] ogbg-molbace: 0.627167 val loss: 0.669382
[Epoch 21] ogbg-molbace: 0.784760 test loss: 0.633079
[Epoch 22; Iter    24/   36] train: loss: 0.5977277
[Epoch 22] ogbg-molbace: 0.652643 val loss: 0.679776
[Epoch 22] ogbg-molbace: 0.774828 test loss: 0.605510
[Epoch 23; Iter    18/   36] train: loss: 0.5850149
[Epoch 23] ogbg-molbace: 0.647886 val loss: 0.540507
[Epoch 23] ogbg-molbace: 0.754243 test loss: 0.819669
[Epoch 24; Iter    12/   36] train: loss: 0.5826872
[Epoch 24] ogbg-molbace: 0.656660 val loss: 0.645163
[Epoch 24] ogbg-molbace: 0.770946 test loss: 0.630208
[Epoch 25; Iter     6/   36] train: loss: 0.5240339
[Epoch 25; Iter    36/   36] train: loss: 0.3328276
[Epoch 25] ogbg-molbace: 0.679915 val loss: 0.637044
[Epoch 25] ogbg-molbace: 0.808505 test loss: 0.586292
[Epoch 26; Iter    30/   36] train: loss: 0.3775009
[Epoch 26] ogbg-molbace: 0.691966 val loss: 0.651482
[Epoch 26] ogbg-molbace: 0.790267 test loss: 0.614440
[Epoch 27; Iter    24/   36] train: loss: 0.4328570
[Epoch 27] ogbg-molbace: 0.709619 val loss: 0.700245
[Epoch 27] ogbg-molbace: 0.819429 test loss: 0.548389
[Epoch 28; Iter    18/   36] train: loss: 0.3883258
[Epoch 28] ogbg-molbace: 0.709619 val loss: 1.048631
[Epoch 28] ogbg-molbace: 0.813380 test loss: 0.465499
[Epoch 29; Iter    12/   36] train: loss: 0.4337142
[Epoch 29] ogbg-molbace: 0.712262 val loss: 0.667395
[Epoch 29] ogbg-molbace: 0.802365 test loss: 0.630404
[Epoch 30; Iter     6/   36] train: loss: 0.3230042
[Epoch 30; Iter    36/   36] train: loss: 0.4206646
[Epoch 30] ogbg-molbace: 0.704228 val loss: 0.578793
[Epoch 30] ogbg-molbace: 0.786927 test loss: 0.785197
[Epoch 31; Iter    30/   36] train: loss: 0.4288807
[Epoch 31] ogbg-molbace: 0.703594 val loss: 0.709524
[Epoch 31] ogbg-molbace: 0.810672 test loss: 0.635709
[Epoch 32; Iter    24/   36] train: loss: 0.3368452
[Epoch 32] ogbg-molbace: 0.674101 val loss: 0.832673
[Epoch 32] ogbg-molbace: 0.800650 test loss: 0.607757
[Epoch 33; Iter    18/   36] train: loss: 0.4565539
[Epoch 33] ogbg-molbace: 0.699366 val loss: 0.667108
[Epoch 33] ogbg-molbace: 0.800469 test loss: 0.709459
[Epoch 34; Iter    12/   36] train: loss: 0.3961901
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/bace/scaff/train_prop=0.6/PNA_ogbg-molbace_GraphCL_bace_scaff=0.6_6_26-05_09-42-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/bace/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_bace_scaff=0.6
logdir: runs/split/GraphCL/bace/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6964946
[Epoch 1] ogbg-molbace: 0.318073 val loss: 0.693650
[Epoch 1] ogbg-molbace: 0.385792 test loss: 0.693628
[Epoch 2; Iter    29/   31] train: loss: 0.6980586
[Epoch 2] ogbg-molbace: 0.304767 val loss: 0.695635
[Epoch 2] ogbg-molbace: 0.362714 test loss: 0.695326
[Epoch 3; Iter    28/   31] train: loss: 0.6956290
[Epoch 3] ogbg-molbace: 0.302880 val loss: 0.698014
[Epoch 3] ogbg-molbace: 0.372862 test loss: 0.696446
[Epoch 4; Iter    27/   31] train: loss: 0.6953176
[Epoch 4] ogbg-molbace: 0.311122 val loss: 0.697735
[Epoch 4] ogbg-molbace: 0.371986 test loss: 0.697359
[Epoch 5; Iter    26/   31] train: loss: 0.6953260
[Epoch 5] ogbg-molbace: 0.308937 val loss: 0.697868
[Epoch 5] ogbg-molbace: 0.376468 test loss: 0.697286
[Epoch 6; Iter    25/   31] train: loss: 0.6928006
[Epoch 6] ogbg-molbace: 0.314697 val loss: 0.697949
[Epoch 6] ogbg-molbace: 0.387647 test loss: 0.697028
[Epoch 7; Iter    24/   31] train: loss: 0.6967093
[Epoch 7] ogbg-molbace: 0.314399 val loss: 0.697931
[Epoch 7] ogbg-molbace: 0.385432 test loss: 0.696934
[Epoch 8; Iter    23/   31] train: loss: 0.6974966
[Epoch 8] ogbg-molbace: 0.315889 val loss: 0.698425
[Epoch 8] ogbg-molbace: 0.390480 test loss: 0.696361
[Epoch 9; Iter    22/   31] train: loss: 0.6941335
[Epoch 9] ogbg-molbace: 0.318471 val loss: 0.696659
[Epoch 9] ogbg-molbace: 0.382907 test loss: 0.697576
[Epoch 10; Iter    21/   31] train: loss: 0.6960580
[Epoch 10] ogbg-molbace: 0.313208 val loss: 0.696752
[Epoch 10] ogbg-molbace: 0.383577 test loss: 0.697235
[Epoch 11; Iter    20/   31] train: loss: 0.6930650
[Epoch 11] ogbg-molbace: 0.311321 val loss: 0.696281
[Epoch 11] ogbg-molbace: 0.385947 test loss: 0.697145
[Epoch 12; Iter    19/   31] train: loss: 0.6971868
[Epoch 12] ogbg-molbace: 0.311321 val loss: 0.696752
[Epoch 12] ogbg-molbace: 0.392747 test loss: 0.697069
[Epoch 13; Iter    18/   31] train: loss: 0.6996695
[Epoch 13] ogbg-molbace: 0.314697 val loss: 0.696468
[Epoch 13] ogbg-molbace: 0.386204 test loss: 0.697230
[Epoch 14; Iter    17/   31] train: loss: 0.7000918
[Epoch 14] ogbg-molbace: 0.315293 val loss: 0.696062
[Epoch 14] ogbg-molbace: 0.391768 test loss: 0.697298
[Epoch 15; Iter    16/   31] train: loss: 0.6958754
[Epoch 15] ogbg-molbace: 0.311817 val loss: 0.696209
[Epoch 15] ogbg-molbace: 0.399598 test loss: 0.697139
[Epoch 16; Iter    15/   31] train: loss: 0.6940379
[Epoch 16] ogbg-molbace: 0.315591 val loss: 0.694755
[Epoch 16] ogbg-molbace: 0.404801 test loss: 0.697491
[Epoch 17; Iter    14/   31] train: loss: 0.6942222
[Epoch 17] ogbg-molbace: 0.318371 val loss: 0.694848
[Epoch 17] ogbg-molbace: 0.405419 test loss: 0.697257
[Epoch 18; Iter    13/   31] train: loss: 0.6969584
[Epoch 18] ogbg-molbace: 0.316286 val loss: 0.694492
[Epoch 18] ogbg-molbace: 0.410777 test loss: 0.697218
[Epoch 19; Iter    12/   31] train: loss: 0.6916605
[Epoch 19] ogbg-molbace: 0.319166 val loss: 0.694398
[Epoch 19] ogbg-molbace: 0.415156 test loss: 0.697073
[Epoch 20; Iter    11/   31] train: loss: 0.6947119
[Epoch 20] ogbg-molbace: 0.318570 val loss: 0.694781
[Epoch 20] ogbg-molbace: 0.416546 test loss: 0.696568
[Epoch 21; Iter    10/   31] train: loss: 0.6999276
[Epoch 21] ogbg-molbace: 0.316683 val loss: 0.693881
[Epoch 21] ogbg-molbace: 0.418865 test loss: 0.696824
[Epoch 22; Iter     9/   31] train: loss: 0.6942732
[Epoch 22] ogbg-molbace: 0.336842 val loss: 0.693015
[Epoch 22] ogbg-molbace: 0.448228 test loss: 0.696634
[Epoch 23; Iter     8/   31] train: loss: 0.6872546
[Epoch 23] ogbg-molbace: 0.643297 val loss: 0.680842
[Epoch 23] ogbg-molbace: 0.749897 test loss: 0.680126
[Epoch 24; Iter     7/   31] train: loss: 0.6782704
[Epoch 24] ogbg-molbace: 0.679940 val loss: 0.662122
[Epoch 24] ogbg-molbace: 0.770245 test loss: 0.660617
[Epoch 25; Iter     6/   31] train: loss: 0.6584694
[Epoch 25] ogbg-molbace: 0.621450 val loss: 0.582816
[Epoch 25] ogbg-molbace: 0.707810 test loss: 0.685486
[Epoch 26; Iter     5/   31] train: loss: 0.6373810
[Epoch 26] ogbg-molbace: 0.722939 val loss: 0.662837
[Epoch 26] ogbg-molbace: 0.765300 test loss: 0.594224
[Epoch 27; Iter     4/   31] train: loss: 0.5613340
[Epoch 27] ogbg-molbace: 0.765442 val loss: 0.683127
[Epoch 27] ogbg-molbace: 0.763600 test loss: 0.563020
[Epoch 28; Iter     3/   31] train: loss: 0.6645676
[Epoch 28] ogbg-molbace: 0.662363 val loss: 0.477114
[Epoch 28] ogbg-molbace: 0.701937 test loss: 0.749545
[Epoch 29; Iter     2/   31] train: loss: 0.5178949
[Epoch 29] ogbg-molbace: 0.732572 val loss: 0.497640
[Epoch 29] ogbg-molbace: 0.766227 test loss: 0.653392
[Epoch 30; Iter     1/   31] train: loss: 0.4915176
[Epoch 30; Iter    31/   31] train: loss: 0.4384023
[Epoch 30] ogbg-molbace: 0.691956 val loss: 0.709800
[Epoch 30] ogbg-molbace: 0.725479 test loss: 0.628265
[Epoch 31; Iter    30/   31] train: loss: 0.4826834
[Epoch 31] ogbg-molbace: 0.739722 val loss: 0.509779
[Epoch 31] ogbg-molbace: 0.765660 test loss: 0.614989
[Epoch 32; Iter    29/   31] train: loss: 0.4331979
[Epoch 32] ogbg-molbace: 0.725521 val loss: 0.853804
[Epoch 32] ogbg-molbace: 0.777354 test loss: 0.601633
[Epoch 33; Iter    28/   31] train: loss: 0.5197313
[Epoch 33] ogbg-molbace: 0.712314 val loss: 0.457625
[Epoch 33] ogbg-molbace: 0.741140 test loss: 0.733903
[Epoch 34; Iter    27/   31] train: loss: 0.5383333
[Epoch 34] ogbg-molbace: 0.738133 val loss: 0.912295
[Epoch 34] ogbg-molbace: 0.762673 test loss: 0.596188
[Epoch 35; Iter    26/   31] train: loss: 0.5098690
[Epoch 35] ogbg-molbace: 0.714399 val loss: 0.445565
[Epoch 35] ogbg-molbace: 0.766588 test loss: 0.702570
[Epoch 34] ogbg-molbace: 0.692178 val loss: 0.940975
[Epoch 34] ogbg-molbace: 0.818436 test loss: 0.567931
[Epoch 35; Iter     6/   36] train: loss: 0.4525035
[Epoch 35; Iter    36/   36] train: loss: 0.5829739
[Epoch 35] ogbg-molbace: 0.713108 val loss: 0.661954
[Epoch 35] ogbg-molbace: 0.815999 test loss: 0.718698
[Epoch 36; Iter    30/   36] train: loss: 0.5382062
[Epoch 36] ogbg-molbace: 0.710148 val loss: 0.524674
[Epoch 36] ogbg-molbace: 0.819339 test loss: 0.992003
[Epoch 37; Iter    24/   36] train: loss: 0.3227059
[Epoch 37] ogbg-molbace: 0.710571 val loss: 0.782031
[Epoch 37] ogbg-molbace: 0.817985 test loss: 0.556023
[Epoch 38; Iter    18/   36] train: loss: 0.6008428
[Epoch 38] ogbg-molbace: 0.704228 val loss: 0.664061
[Epoch 38] ogbg-molbace: 0.821145 test loss: 0.683706
[Epoch 39; Iter    12/   36] train: loss: 0.3252068
[Epoch 39] ogbg-molbace: 0.725370 val loss: 0.767372
[Epoch 39] ogbg-molbace: 0.823492 test loss: 0.590598
[Epoch 40; Iter     6/   36] train: loss: 0.4745654
[Epoch 40; Iter    36/   36] train: loss: 0.2682943
[Epoch 40] ogbg-molbace: 0.779493 val loss: 1.491673
[Epoch 40] ogbg-molbace: 0.858523 test loss: 0.432967
[Epoch 41; Iter    30/   36] train: loss: 0.5183024
[Epoch 41] ogbg-molbace: 0.737844 val loss: 0.854353
[Epoch 41] ogbg-molbace: 0.802365 test loss: 0.632446
[Epoch 42; Iter    24/   36] train: loss: 0.4094000
[Epoch 42] ogbg-molbace: 0.697780 val loss: 0.803196
[Epoch 42] ogbg-molbace: 0.837577 test loss: 0.556935
[Epoch 43; Iter    18/   36] train: loss: 0.4186837
[Epoch 43] ogbg-molbace: 0.776638 val loss: 0.604747
[Epoch 43] ogbg-molbace: 0.822138 test loss: 0.610104
[Epoch 44; Iter    12/   36] train: loss: 0.3242887
[Epoch 44] ogbg-molbace: 0.768182 val loss: 0.649845
[Epoch 44] ogbg-molbace: 0.838660 test loss: 0.556950
[Epoch 45; Iter     6/   36] train: loss: 0.3363334
[Epoch 45; Iter    36/   36] train: loss: 0.5487298
[Epoch 45] ogbg-molbace: 0.749471 val loss: 0.628439
[Epoch 45] ogbg-molbace: 0.846786 test loss: 0.576473
[Epoch 46; Iter    30/   36] train: loss: 0.4044304
[Epoch 46] ogbg-molbace: 0.730550 val loss: 0.828478
[Epoch 46] ogbg-molbace: 0.845702 test loss: 0.446846
[Epoch 47; Iter    24/   36] train: loss: 0.3037456
[Epoch 47] ogbg-molbace: 0.749471 val loss: 0.638603
[Epoch 47] ogbg-molbace: 0.853377 test loss: 0.566240
[Epoch 48; Iter    18/   36] train: loss: 0.3511120
[Epoch 48] ogbg-molbace: 0.754228 val loss: 0.703021
[Epoch 48] ogbg-molbace: 0.850488 test loss: 0.496002
[Epoch 49; Iter    12/   36] train: loss: 0.5346148
[Epoch 49] ogbg-molbace: 0.734989 val loss: 0.633474
[Epoch 49] ogbg-molbace: 0.816721 test loss: 0.752829
[Epoch 50; Iter     6/   36] train: loss: 0.3993577
[Epoch 50; Iter    36/   36] train: loss: 0.1183007
[Epoch 50] ogbg-molbace: 0.745983 val loss: 0.568591
[Epoch 50] ogbg-molbace: 0.795684 test loss: 0.852690
[Epoch 51; Iter    30/   36] train: loss: 0.3358347
[Epoch 51] ogbg-molbace: 0.779070 val loss: 0.586622
[Epoch 51] ogbg-molbace: 0.815367 test loss: 0.734463
[Epoch 52; Iter    24/   36] train: loss: 0.3347991
[Epoch 52] ogbg-molbace: 0.741755 val loss: 0.658378
[Epoch 52] ogbg-molbace: 0.848862 test loss: 0.621159
[Epoch 53; Iter    18/   36] train: loss: 0.2066335
[Epoch 53] ogbg-molbace: 0.764271 val loss: 0.498184
[Epoch 53] ogbg-molbace: 0.848953 test loss: 0.814512
[Epoch 54; Iter    12/   36] train: loss: 0.2237891
[Epoch 54] ogbg-molbace: 0.779387 val loss: 0.563003
[Epoch 54] ogbg-molbace: 0.851481 test loss: 0.559292
[Epoch 55; Iter     6/   36] train: loss: 0.1843874
[Epoch 55; Iter    36/   36] train: loss: 0.2022296
[Epoch 55] ogbg-molbace: 0.763108 val loss: 0.542043
[Epoch 55] ogbg-molbace: 0.842813 test loss: 0.612987
[Epoch 56; Iter    30/   36] train: loss: 0.3053442
[Epoch 56] ogbg-molbace: 0.766808 val loss: 0.577515
[Epoch 56] ogbg-molbace: 0.849585 test loss: 0.736460
[Epoch 57; Iter    24/   36] train: loss: 0.3766435
[Epoch 57] ogbg-molbace: 0.759831 val loss: 0.547020
[Epoch 57] ogbg-molbace: 0.809498 test loss: 1.194977
[Epoch 58; Iter    18/   36] train: loss: 0.3598855
[Epoch 58] ogbg-molbace: 0.790063 val loss: 0.496503
[Epoch 58] ogbg-molbace: 0.843174 test loss: 0.942976
[Epoch 59; Iter    12/   36] train: loss: 0.2340502
[Epoch 59] ogbg-molbace: 0.801903 val loss: 1.015434
[Epoch 59] ogbg-molbace: 0.834778 test loss: 0.480213
[Epoch 60; Iter     6/   36] train: loss: 0.2621304
[Epoch 60; Iter    36/   36] train: loss: 0.3627192
[Epoch 60] ogbg-molbace: 0.767759 val loss: 0.737570
[Epoch 60] ogbg-molbace: 0.839021 test loss: 0.620410
[Epoch 61; Iter    30/   36] train: loss: 0.1738045
[Epoch 61] ogbg-molbace: 0.761311 val loss: 0.597542
[Epoch 61] ogbg-molbace: 0.845973 test loss: 0.848631
[Epoch 62; Iter    24/   36] train: loss: 0.2900486
[Epoch 62] ogbg-molbace: 0.780233 val loss: 0.548486
[Epoch 62] ogbg-molbace: 0.830534 test loss: 0.686729
[Epoch 63; Iter    18/   36] train: loss: 0.2568187
[Epoch 63] ogbg-molbace: 0.801163 val loss: 1.037533
[Epoch 63] ogbg-molbace: 0.836493 test loss: 0.513523
[Epoch 64; Iter    12/   36] train: loss: 0.4566006
[Epoch 64] ogbg-molbace: 0.798943 val loss: 0.734528
[Epoch 64] ogbg-molbace: 0.854009 test loss: 0.532291
[Epoch 65; Iter     6/   36] train: loss: 0.3712183
[Epoch 65; Iter    36/   36] train: loss: 0.5189651
[Epoch 65] ogbg-molbace: 0.781395 val loss: 0.566644
[Epoch 65] ogbg-molbace: 0.817533 test loss: 1.219088
[Epoch 66; Iter    30/   36] train: loss: 0.1852440
[Epoch 66] ogbg-molbace: 0.758879 val loss: 0.641883
[Epoch 66] ogbg-molbace: 0.801463 test loss: 0.851459
[Epoch 67; Iter    24/   36] train: loss: 0.2916290
[Epoch 67] ogbg-molbace: 0.759937 val loss: 0.602072
[Epoch 67] ogbg-molbace: 0.827645 test loss: 0.947270
[Epoch 68; Iter    18/   36] train: loss: 0.4281135
[Epoch 68] ogbg-molbace: 0.784461 val loss: 0.850564
[Epoch 68] ogbg-molbace: 0.823131 test loss: 0.602660
[Epoch 69; Iter    12/   36] train: loss: 0.1628596
[Epoch 69] ogbg-molbace: 0.762896 val loss: 0.855889
[Epoch 69] ogbg-molbace: 0.830173 test loss: 0.649995
[Epoch 70; Iter     6/   36] train: loss: 0.1144054
[Epoch 70; Iter    36/   36] train: loss: 0.6507542
[Epoch 70] ogbg-molbace: 0.792072 val loss: 0.566273
[Epoch 70] ogbg-molbace: 0.806248 test loss: 0.796899
[Epoch 71; Iter    30/   36] train: loss: 0.1621921
[Epoch 71] ogbg-molbace: 0.809619 val loss: 0.642577
[Epoch 71] ogbg-molbace: 0.820242 test loss: 0.663776
[Epoch 72; Iter    24/   36] train: loss: 0.1495185
[Epoch 72] ogbg-molbace: 0.775687 val loss: 0.747482
[Epoch 72] ogbg-molbace: 0.821416 test loss: 0.679248
[Epoch 73; Iter    18/   36] train: loss: 0.3458654
[Epoch 73] ogbg-molbace: 0.763214 val loss: 0.946603
[Epoch 73] ogbg-molbace: 0.820332 test loss: 0.708012
[Epoch 74; Iter    12/   36] train: loss: 0.1207746
[Epoch 74] ogbg-molbace: 0.765645 val loss: 0.825977
[Epoch 74] ogbg-molbace: 0.809047 test loss: 0.854127
[Epoch 75; Iter     6/   36] train: loss: 0.1942855
[Epoch 75; Iter    36/   36] train: loss: 0.6126282
[Epoch 75] ogbg-molbace: 0.819873 val loss: 0.492644
[Epoch 75] ogbg-molbace: 0.819791 test loss: 0.814750
[Epoch 76; Iter    30/   36] train: loss: 0.0660101
[Epoch 76] ogbg-molbace: 0.806448 val loss: 0.654689
[Epoch 76] ogbg-molbace: 0.834056 test loss: 0.732083
[Epoch 77; Iter    24/   36] train: loss: 0.1370811
[Epoch 77] ogbg-molbace: 0.810465 val loss: 0.615712
[Epoch 77] ogbg-molbace: 0.816992 test loss: 0.813793
[Epoch 78; Iter    18/   36] train: loss: 0.0705705
[Epoch 78] ogbg-molbace: 0.800529 val loss: 0.599758
[Epoch 78] ogbg-molbace: 0.830986 test loss: 0.817417
[Epoch 79; Iter    12/   36] train: loss: 0.1450434
[Epoch 79] ogbg-molbace: 0.786364 val loss: 0.757894
[Epoch 79] ogbg-molbace: 0.829451 test loss: 0.832976
[Epoch 80; Iter     6/   36] train: loss: 0.1206742
[Epoch 80; Iter    36/   36] train: loss: 0.3343133
[Epoch 80] ogbg-molbace: 0.794820 val loss: 0.626423
[Epoch 80] ogbg-molbace: 0.824124 test loss: 0.835061
[Epoch 81; Iter    30/   36] train: loss: 0.1840372
[Epoch 81] ogbg-molbace: 0.817548 val loss: 0.648197
[Epoch 81] ogbg-molbace: 0.829993 test loss: 0.827950
[Epoch 82; Iter    24/   36] train: loss: 0.2005415
[Epoch 32] ogbg-molbace: 0.792036 test loss: 0.809749
[Epoch 33; Iter     8/   41] train: loss: 0.3620505
[Epoch 33; Iter    38/   41] train: loss: 0.3937176
[Epoch 33] ogbg-molbace: 0.719780 val loss: 1.119813
[Epoch 33] ogbg-molbace: 0.801774 test loss: 1.284502
[Epoch 34; Iter    27/   41] train: loss: 0.4436733
[Epoch 34] ogbg-molbace: 0.706960 val loss: 0.885338
[Epoch 34] ogbg-molbace: 0.800035 test loss: 1.019031
[Epoch 35; Iter    16/   41] train: loss: 0.2793368
[Epoch 35] ogbg-molbace: 0.702198 val loss: 0.856802
[Epoch 35] ogbg-molbace: 0.735350 test loss: 1.010058
[Epoch 36; Iter     5/   41] train: loss: 0.4420201
[Epoch 36; Iter    35/   41] train: loss: 0.3722818
[Epoch 36] ogbg-molbace: 0.684615 val loss: 1.420504
[Epoch 36] ogbg-molbace: 0.755695 test loss: 0.977667
[Epoch 37; Iter    24/   41] train: loss: 0.2469556
[Epoch 37] ogbg-molbace: 0.690476 val loss: 0.642670
[Epoch 37] ogbg-molbace: 0.805773 test loss: 0.547337
[Epoch 38; Iter    13/   41] train: loss: 0.3220268
[Epoch 38] ogbg-molbace: 0.741026 val loss: 1.157952
[Epoch 38] ogbg-molbace: 0.807512 test loss: 1.326463
[Epoch 39; Iter     2/   41] train: loss: 0.3832110
[Epoch 39; Iter    32/   41] train: loss: 0.4024355
[Epoch 39] ogbg-molbace: 0.711355 val loss: 1.118765
[Epoch 39] ogbg-molbace: 0.821596 test loss: 1.069943
[Epoch 40; Iter    21/   41] train: loss: 0.2790284
[Epoch 40] ogbg-molbace: 0.713553 val loss: 0.879039
[Epoch 40] ogbg-molbace: 0.855503 test loss: 0.757382
[Epoch 41; Iter    10/   41] train: loss: 0.4623299
[Epoch 41; Iter    40/   41] train: loss: 0.1909890
[Epoch 41] ogbg-molbace: 0.736264 val loss: 1.171608
[Epoch 41] ogbg-molbace: 0.810642 test loss: 0.864825
[Epoch 42; Iter    29/   41] train: loss: 0.3780653
[Epoch 42] ogbg-molbace: 0.725641 val loss: 0.782356
[Epoch 42] ogbg-molbace: 0.825074 test loss: 0.719572
[Epoch 43; Iter    18/   41] train: loss: 0.2358595
[Epoch 43] ogbg-molbace: 0.707692 val loss: 0.992124
[Epoch 43] ogbg-molbace: 0.844549 test loss: 0.677572
[Epoch 44; Iter     7/   41] train: loss: 0.3653361
[Epoch 44; Iter    37/   41] train: loss: 0.3222233
[Epoch 44] ogbg-molbace: 0.778388 val loss: 0.727126
[Epoch 44] ogbg-molbace: 0.795166 test loss: 0.651203
[Epoch 45; Iter    26/   41] train: loss: 0.2733331
[Epoch 45] ogbg-molbace: 0.753114 val loss: 0.692341
[Epoch 45] ogbg-molbace: 0.813598 test loss: 0.890538
[Epoch 46; Iter    15/   41] train: loss: 0.2719332
[Epoch 46] ogbg-molbace: 0.692308 val loss: 1.285047
[Epoch 46] ogbg-molbace: 0.825769 test loss: 1.181457
[Epoch 47; Iter     4/   41] train: loss: 0.2520310
[Epoch 47; Iter    34/   41] train: loss: 0.2722585
[Epoch 47] ogbg-molbace: 0.749084 val loss: 0.944652
[Epoch 47] ogbg-molbace: 0.815858 test loss: 0.797008
[Epoch 48; Iter    23/   41] train: loss: 0.2691644
[Epoch 48] ogbg-molbace: 0.657509 val loss: 0.972318
[Epoch 48] ogbg-molbace: 0.801947 test loss: 0.893930
[Epoch 49; Iter    12/   41] train: loss: 0.3816583
[Epoch 49] ogbg-molbace: 0.758608 val loss: 0.821694
[Epoch 49] ogbg-molbace: 0.810989 test loss: 0.779163
[Epoch 50; Iter     1/   41] train: loss: 0.3982593
[Epoch 50; Iter    31/   41] train: loss: 0.3762581
[Epoch 50] ogbg-molbace: 0.762271 val loss: 0.943228
[Epoch 50] ogbg-molbace: 0.805599 test loss: 0.977538
[Epoch 51; Iter    20/   41] train: loss: 0.4240212
[Epoch 51] ogbg-molbace: 0.712088 val loss: 1.091427
[Epoch 51] ogbg-molbace: 0.793775 test loss: 1.183946
[Epoch 52; Iter     9/   41] train: loss: 0.2549265
[Epoch 52; Iter    39/   41] train: loss: 0.3104953
[Epoch 52] ogbg-molbace: 0.763004 val loss: 1.033159
[Epoch 52] ogbg-molbace: 0.798644 test loss: 0.644496
[Epoch 53; Iter    28/   41] train: loss: 0.1309527
[Epoch 53] ogbg-molbace: 0.756410 val loss: 1.017917
[Epoch 53] ogbg-molbace: 0.842810 test loss: 0.663000
[Epoch 54; Iter    17/   41] train: loss: 0.4615006
[Epoch 54] ogbg-molbace: 0.739927 val loss: 1.194477
[Epoch 54] ogbg-molbace: 0.774648 test loss: 1.287451
[Epoch 55; Iter     6/   41] train: loss: 0.2849522
[Epoch 55; Iter    36/   41] train: loss: 0.2305767
[Epoch 55] ogbg-molbace: 0.712088 val loss: 1.196908
[Epoch 55] ogbg-molbace: 0.842288 test loss: 0.982273
[Epoch 56; Iter    25/   41] train: loss: 0.2192322
[Epoch 56] ogbg-molbace: 0.721245 val loss: 1.004792
[Epoch 56] ogbg-molbace: 0.799513 test loss: 0.905245
[Epoch 57; Iter    14/   41] train: loss: 0.2450780
[Epoch 57] ogbg-molbace: 0.710623 val loss: 1.092337
[Epoch 57] ogbg-molbace: 0.833073 test loss: 0.864782
[Epoch 58; Iter     3/   41] train: loss: 0.2616720
[Epoch 58; Iter    33/   41] train: loss: 0.3483783
[Epoch 58] ogbg-molbace: 0.716850 val loss: 1.024212
[Epoch 58] ogbg-molbace: 0.780908 test loss: 1.382300
[Epoch 59; Iter    22/   41] train: loss: 0.4448687
[Epoch 59] ogbg-molbace: 0.736264 val loss: 0.969319
[Epoch 59] ogbg-molbace: 0.829595 test loss: 1.161225
[Epoch 60; Iter    11/   41] train: loss: 0.3107994
[Epoch 60; Iter    41/   41] train: loss: 0.0658163
[Epoch 60] ogbg-molbace: 0.745788 val loss: 1.351229
[Epoch 60] ogbg-molbace: 0.832551 test loss: 1.146755
[Epoch 61; Iter    30/   41] train: loss: 0.1868380
[Epoch 61] ogbg-molbace: 0.747985 val loss: 1.132993
[Epoch 61] ogbg-molbace: 0.812380 test loss: 0.783054
[Epoch 62; Iter    19/   41] train: loss: 0.1524297
[Epoch 62] ogbg-molbace: 0.752015 val loss: 1.188851
[Epoch 62] ogbg-molbace: 0.830290 test loss: 0.847467
[Epoch 63; Iter     8/   41] train: loss: 0.2611747
[Epoch 63; Iter    38/   41] train: loss: 0.2569570
[Epoch 63] ogbg-molbace: 0.756777 val loss: 1.043574
[Epoch 63] ogbg-molbace: 0.806295 test loss: 0.960514
[Epoch 64; Iter    27/   41] train: loss: 0.4477177
[Epoch 64] ogbg-molbace: 0.756777 val loss: 0.953314
[Epoch 64] ogbg-molbace: 0.830117 test loss: 0.754816
[Epoch 65; Iter    16/   41] train: loss: 0.2248435
[Epoch 65] ogbg-molbace: 0.729304 val loss: 1.217808
[Epoch 65] ogbg-molbace: 0.810816 test loss: 1.384029
[Epoch 66; Iter     5/   41] train: loss: 0.1590987
[Epoch 66; Iter    35/   41] train: loss: 0.3583245
[Epoch 66] ogbg-molbace: 0.780220 val loss: 1.211556
[Epoch 66] ogbg-molbace: 0.799339 test loss: 0.737721
[Epoch 67; Iter    24/   41] train: loss: 0.1781667
[Epoch 67] ogbg-molbace: 0.797802 val loss: 0.890527
[Epoch 67] ogbg-molbace: 0.823857 test loss: 0.702887
[Epoch 68; Iter    13/   41] train: loss: 0.1510113
[Epoch 68] ogbg-molbace: 0.701099 val loss: 1.491028
[Epoch 68] ogbg-molbace: 0.823857 test loss: 2.354469
[Epoch 69; Iter     2/   41] train: loss: 0.2422836
[Epoch 69; Iter    32/   41] train: loss: 0.2734334
[Epoch 69] ogbg-molbace: 0.747619 val loss: 1.284808
[Epoch 69] ogbg-molbace: 0.825596 test loss: 0.897630
[Epoch 70; Iter    21/   41] train: loss: 0.3356890
[Epoch 70] ogbg-molbace: 0.748352 val loss: 1.151226
[Epoch 70] ogbg-molbace: 0.826291 test loss: 1.113953
[Epoch 71; Iter    10/   41] train: loss: 0.2409789
[Epoch 71; Iter    40/   41] train: loss: 0.1244326
[Epoch 71] ogbg-molbace: 0.799634 val loss: 0.635215
[Epoch 71] ogbg-molbace: 0.812554 test loss: 0.723136
[Epoch 72; Iter    29/   41] train: loss: 0.2170946
[Epoch 72] ogbg-molbace: 0.686813 val loss: 2.014324
[Epoch 72] ogbg-molbace: 0.828552 test loss: 1.341555
[Epoch 73; Iter    18/   41] train: loss: 0.0921099
[Epoch 73] ogbg-molbace: 0.705861 val loss: 0.920974
[Epoch 73] ogbg-molbace: 0.768910 test loss: 1.640924
[Epoch 74; Iter     7/   41] train: loss: 0.1608022
[Epoch 74; Iter    37/   41] train: loss: 0.2697340
[Epoch 74] ogbg-molbace: 0.747985 val loss: 1.362909
[Epoch 74] ogbg-molbace: 0.807686 test loss: 0.958616
[Epoch 75; Iter    26/   41] train: loss: 0.1409686
[Epoch 75] ogbg-molbace: 0.745421 val loss: 1.148197
[Epoch 75] ogbg-molbace: 0.800904 test loss: 1.277409
[Epoch 76; Iter    15/   41] train: loss: 0.0549665
[Epoch 76] ogbg-molbace: 0.766667 val loss: 1.163849
[Epoch 76] ogbg-molbace: 0.795862 test loss: 1.159340
[Epoch 77; Iter     4/   41] train: loss: 0.0364140
[Epoch 77; Iter    34/   41] train: loss: 0.0749471
[Epoch 77] ogbg-molbace: 0.756777 val loss: 0.968278
[Epoch 77] ogbg-molbace: 0.793427 test loss: 1.115385
[Epoch 78; Iter    23/   41] train: loss: 0.3533304
[Epoch 36; Iter    25/   31] train: loss: 0.6213412
[Epoch 36] ogbg-molbace: 0.777358 val loss: 0.864170
[Epoch 36] ogbg-molbace: 0.775809 test loss: 0.662824
[Epoch 37; Iter    24/   31] train: loss: 0.3406823
[Epoch 37] ogbg-molbace: 0.750745 val loss: 0.572310
[Epoch 37] ogbg-molbace: 0.765145 test loss: 0.623720
[Epoch 38; Iter    23/   31] train: loss: 0.3656206
[Epoch 38] ogbg-molbace: 0.783913 val loss: 0.752613
[Epoch 38] ogbg-molbace: 0.774212 test loss: 0.646328
[Epoch 39; Iter    22/   31] train: loss: 0.2722036
[Epoch 39] ogbg-molbace: 0.720457 val loss: 0.616453
[Epoch 39] ogbg-molbace: 0.751030 test loss: 0.675135
[Epoch 40; Iter    21/   31] train: loss: 0.4382312
[Epoch 40] ogbg-molbace: 0.735849 val loss: 0.539488
[Epoch 40] ogbg-molbace: 0.759376 test loss: 0.735587
[Epoch 41; Iter    20/   31] train: loss: 0.3638428
[Epoch 41] ogbg-molbace: 0.744290 val loss: 0.490124
[Epoch 41] ogbg-molbace: 0.764888 test loss: 0.700417
[Epoch 42; Iter    19/   31] train: loss: 0.2859842
[Epoch 42] ogbg-molbace: 0.701490 val loss: 0.948929
[Epoch 42] ogbg-molbace: 0.732691 test loss: 0.758502
[Epoch 43; Iter    18/   31] train: loss: 0.3577422
[Epoch 43] ogbg-molbace: 0.758689 val loss: 0.345603
[Epoch 43] ogbg-molbace: 0.769524 test loss: 0.931697
[Epoch 44; Iter    17/   31] train: loss: 0.7332316
[Epoch 44] ogbg-molbace: 0.757001 val loss: 0.558538
[Epoch 44] ogbg-molbace: 0.768545 test loss: 0.691617
[Epoch 45; Iter    16/   31] train: loss: 0.4421075
[Epoch 45] ogbg-molbace: 0.737637 val loss: 0.567952
[Epoch 45] ogbg-molbace: 0.779518 test loss: 0.672429
[Epoch 46; Iter    15/   31] train: loss: 0.5594433
[Epoch 46] ogbg-molbace: 0.666832 val loss: 1.250511
[Epoch 46] ogbg-molbace: 0.776736 test loss: 0.777175
[Epoch 47; Iter    14/   31] train: loss: 0.4487252
[Epoch 47] ogbg-molbace: 0.709930 val loss: 0.707430
[Epoch 47] ogbg-molbace: 0.749073 test loss: 0.660512
[Epoch 48; Iter    13/   31] train: loss: 0.6017081
[Epoch 48] ogbg-molbace: 0.801787 val loss: 0.382259
[Epoch 48] ogbg-molbace: 0.821142 test loss: 0.659480
[Epoch 49; Iter    12/   31] train: loss: 0.4077233
[Epoch 49] ogbg-molbace: 0.752334 val loss: 0.904554
[Epoch 49] ogbg-molbace: 0.782866 test loss: 0.656113
[Epoch 50; Iter    11/   31] train: loss: 0.3602866
[Epoch 50] ogbg-molbace: 0.776266 val loss: 0.353810
[Epoch 50] ogbg-molbace: 0.789100 test loss: 0.850355
[Epoch 51; Iter    10/   31] train: loss: 0.4227712
[Epoch 51] ogbg-molbace: 0.770209 val loss: 0.350448
[Epoch 51] ogbg-molbace: 0.757212 test loss: 1.012624
[Epoch 52; Iter     9/   31] train: loss: 0.3482890
[Epoch 52] ogbg-molbace: 0.824131 val loss: 0.419967
[Epoch 52] ogbg-molbace: 0.797651 test loss: 0.667908
[Epoch 53; Iter     8/   31] train: loss: 0.3971800
[Epoch 53] ogbg-molbace: 0.801887 val loss: 0.680568
[Epoch 53] ogbg-molbace: 0.792087 test loss: 0.622015
[Epoch 54; Iter     7/   31] train: loss: 0.2401491
[Epoch 54] ogbg-molbace: 0.742205 val loss: 0.398760
[Epoch 54] ogbg-molbace: 0.769576 test loss: 0.799463
[Epoch 55; Iter     6/   31] train: loss: 0.4671559
[Epoch 55] ogbg-molbace: 0.695233 val loss: 0.558820
[Epoch 55] ogbg-molbace: 0.775294 test loss: 0.659287
[Epoch 56; Iter     5/   31] train: loss: 0.3800593
[Epoch 56] ogbg-molbace: 0.798113 val loss: 0.574405
[Epoch 56] ogbg-molbace: 0.788275 test loss: 0.593996
[Epoch 57; Iter     4/   31] train: loss: 0.3158661
[Epoch 57] ogbg-molbace: 0.799901 val loss: 0.290911
[Epoch 57] ogbg-molbace: 0.808881 test loss: 0.968271
[Epoch 58; Iter     3/   31] train: loss: 0.4789267
[Epoch 58] ogbg-molbace: 0.775372 val loss: 0.483571
[Epoch 58] ogbg-molbace: 0.813672 test loss: 0.664906
[Epoch 59; Iter     2/   31] train: loss: 0.3983108
[Epoch 59] ogbg-molbace: 0.770904 val loss: 0.375497
[Epoch 59] ogbg-molbace: 0.797239 test loss: 0.831180
[Epoch 60; Iter     1/   31] train: loss: 0.2823561
[Epoch 60; Iter    31/   31] train: loss: 0.3268417
[Epoch 60] ogbg-molbace: 0.756207 val loss: 0.541066
[Epoch 60] ogbg-molbace: 0.791624 test loss: 0.713385
[Epoch 61; Iter    30/   31] train: loss: 0.2849017
[Epoch 61] ogbg-molbace: 0.830089 val loss: 0.434562
[Epoch 61] ogbg-molbace: 0.762827 test loss: 0.711151
[Epoch 62; Iter    29/   31] train: loss: 0.4141351
[Epoch 62] ogbg-molbace: 0.719762 val loss: 0.741128
[Epoch 62] ogbg-molbace: 0.788842 test loss: 0.734868
[Epoch 63; Iter    28/   31] train: loss: 0.2561566
[Epoch 63] ogbg-molbace: 0.787090 val loss: 0.457680
[Epoch 63] ogbg-molbace: 0.762776 test loss: 0.791126
[Epoch 64; Iter    27/   31] train: loss: 0.2298191
[Epoch 64] ogbg-molbace: 0.769215 val loss: 0.482085
[Epoch 64] ogbg-molbace: 0.792654 test loss: 0.786706
[Epoch 65; Iter    26/   31] train: loss: 0.2521054
[Epoch 65] ogbg-molbace: 0.792254 val loss: 0.396000
[Epoch 65] ogbg-molbace: 0.784154 test loss: 0.931458
[Epoch 66; Iter    25/   31] train: loss: 0.3458987
[Epoch 66] ogbg-molbace: 0.754618 val loss: 0.654049
[Epoch 66] ogbg-molbace: 0.774366 test loss: 0.726275
[Epoch 67; Iter    24/   31] train: loss: 0.7622104
[Epoch 67] ogbg-molbace: 0.810030 val loss: 0.483197
[Epoch 67] ogbg-molbace: 0.802545 test loss: 0.753485
[Epoch 68; Iter    23/   31] train: loss: 0.3357427
[Epoch 68] ogbg-molbace: 0.825621 val loss: 0.570918
[Epoch 68] ogbg-molbace: 0.779775 test loss: 0.857038
[Epoch 69; Iter    22/   31] train: loss: 0.1897264
[Epoch 69] ogbg-molbace: 0.779146 val loss: 0.379673
[Epoch 69] ogbg-molbace: 0.800742 test loss: 0.852049
[Epoch 70; Iter    21/   31] train: loss: 0.2008121
[Epoch 70] ogbg-molbace: 0.747368 val loss: 0.443841
[Epoch 70] ogbg-molbace: 0.805636 test loss: 0.932767
[Epoch 71; Iter    20/   31] train: loss: 0.4940422
[Epoch 71] ogbg-molbace: 0.765442 val loss: 0.803810
[Epoch 71] ogbg-molbace: 0.788430 test loss: 0.894976
[Epoch 72; Iter    19/   31] train: loss: 0.2996042
[Epoch 72] ogbg-molbace: 0.755809 val loss: 0.506319
[Epoch 72] ogbg-molbace: 0.794818 test loss: 0.834438
[Epoch 73; Iter    18/   31] train: loss: 0.1596874
[Epoch 73] ogbg-molbace: 0.761768 val loss: 0.511129
[Epoch 73] ogbg-molbace: 0.792654 test loss: 0.813364
[Epoch 74; Iter    17/   31] train: loss: 0.1789459
[Epoch 74] ogbg-molbace: 0.771797 val loss: 0.449734
[Epoch 74] ogbg-molbace: 0.775603 test loss: 0.767942
[Epoch 75; Iter    16/   31] train: loss: 0.2505576
[Epoch 75] ogbg-molbace: 0.747865 val loss: 0.561008
[Epoch 75] ogbg-molbace: 0.779775 test loss: 0.938976
[Epoch 76; Iter    15/   31] train: loss: 0.1768408
[Epoch 76] ogbg-molbace: 0.777656 val loss: 0.456998
[Epoch 76] ogbg-molbace: 0.768030 test loss: 1.021869
[Epoch 77; Iter    14/   31] train: loss: 0.1707463
[Epoch 77] ogbg-molbace: 0.804270 val loss: 0.940135
[Epoch 77] ogbg-molbace: 0.815166 test loss: 0.870637
[Epoch 78; Iter    13/   31] train: loss: 0.2936563
[Epoch 78] ogbg-molbace: 0.750844 val loss: 0.693373
[Epoch 78] ogbg-molbace: 0.810272 test loss: 0.781672
[Epoch 79; Iter    12/   31] train: loss: 0.2018049
[Epoch 79] ogbg-molbace: 0.787190 val loss: 0.380927
[Epoch 79] ogbg-molbace: 0.779003 test loss: 0.990090
[Epoch 80; Iter    11/   31] train: loss: 0.2545829
[Epoch 80] ogbg-molbace: 0.743793 val loss: 0.583040
[Epoch 80] ogbg-molbace: 0.771379 test loss: 0.866183
[Epoch 81; Iter    10/   31] train: loss: 0.2829341
[Epoch 81] ogbg-molbace: 0.785303 val loss: 0.594880
[Epoch 81] ogbg-molbace: 0.801566 test loss: 0.804714
[Epoch 82; Iter     9/   31] train: loss: 0.2578292
[Epoch 82] ogbg-molbace: 0.771996 val loss: 0.453521
[Epoch 82] ogbg-molbace: 0.800072 test loss: 0.862644
[Epoch 83; Iter     8/   31] train: loss: 0.1366820
[Epoch 83] ogbg-molbace: 0.782522 val loss: 0.650616
[Epoch 83] ogbg-molbace: 0.799866 test loss: 0.846641
[Epoch 84; Iter     7/   31] train: loss: 0.1857528
[Epoch 84] ogbg-molbace: 0.756902 val loss: 0.518741
[Epoch 84] ogbg-molbace: 0.789975 test loss: 1.011617
[Epoch 85; Iter     6/   31] train: loss: 0.1829734
[Epoch 85] ogbg-molbace: 0.763158 val loss: 0.608516
[Epoch 85] ogbg-molbace: 0.786421 test loss: 0.915074
[Epoch 86; Iter     5/   31] train: loss: 0.0681798
[Epoch 86] ogbg-molbace: 0.778947 val loss: 0.837868
[Epoch 86] ogbg-molbace: 0.804348 test loss: 0.969893
[Epoch 32] ogbg-molbace: 0.754825 test loss: 0.998809
[Epoch 33; Iter     8/   41] train: loss: 0.3509471
[Epoch 33; Iter    38/   41] train: loss: 0.4138108
[Epoch 33] ogbg-molbace: 0.731136 val loss: 0.899247
[Epoch 33] ogbg-molbace: 0.807686 test loss: 0.967965
[Epoch 34; Iter    27/   41] train: loss: 0.3385219
[Epoch 34] ogbg-molbace: 0.656044 val loss: 1.474414
[Epoch 34] ogbg-molbace: 0.773257 test loss: 1.549633
[Epoch 35; Iter    16/   41] train: loss: 0.4415939
[Epoch 35] ogbg-molbace: 0.760806 val loss: 0.704046
[Epoch 35] ogbg-molbace: 0.801078 test loss: 0.789512
[Epoch 36; Iter     5/   41] train: loss: 0.3323199
[Epoch 36; Iter    35/   41] train: loss: 0.5010393
[Epoch 36] ogbg-molbace: 0.763004 val loss: 0.778846
[Epoch 36] ogbg-molbace: 0.816554 test loss: 1.059705
[Epoch 37; Iter    24/   41] train: loss: 0.4128405
[Epoch 37] ogbg-molbace: 0.715018 val loss: 0.766823
[Epoch 37] ogbg-molbace: 0.778126 test loss: 0.826490
[Epoch 38; Iter    13/   41] train: loss: 0.3559017
[Epoch 38] ogbg-molbace: 0.727473 val loss: 0.654209
[Epoch 38] ogbg-molbace: 0.811163 test loss: 0.851752
[Epoch 39; Iter     2/   41] train: loss: 0.3419325
[Epoch 39; Iter    32/   41] train: loss: 0.2786582
[Epoch 39] ogbg-molbace: 0.761905 val loss: 0.763111
[Epoch 39] ogbg-molbace: 0.788037 test loss: 0.810640
[Epoch 40; Iter    21/   41] train: loss: 0.2558460
[Epoch 40] ogbg-molbace: 0.618681 val loss: 0.839921
[Epoch 40] ogbg-molbace: 0.814641 test loss: 0.882563
[Epoch 41; Iter    10/   41] train: loss: 0.2206735
[Epoch 41; Iter    40/   41] train: loss: 0.5278370
[Epoch 41] ogbg-molbace: 0.760806 val loss: 1.010323
[Epoch 41] ogbg-molbace: 0.787863 test loss: 1.090797
[Epoch 42; Iter    29/   41] train: loss: 0.3585219
[Epoch 42] ogbg-molbace: 0.705128 val loss: 0.988814
[Epoch 42] ogbg-molbace: 0.798818 test loss: 1.183560
[Epoch 43; Iter    18/   41] train: loss: 0.3515219
[Epoch 43] ogbg-molbace: 0.696337 val loss: 0.830178
[Epoch 43] ogbg-molbace: 0.747870 test loss: 0.802023
[Epoch 44; Iter     7/   41] train: loss: 0.3609696
[Epoch 44; Iter    37/   41] train: loss: 0.3585179
[Epoch 44] ogbg-molbace: 0.767766 val loss: 0.718972
[Epoch 44] ogbg-molbace: 0.830464 test loss: 0.817267
[Epoch 45; Iter    26/   41] train: loss: 0.2383975
[Epoch 45] ogbg-molbace: 0.728571 val loss: 1.340301
[Epoch 45] ogbg-molbace: 0.837941 test loss: 1.634761
[Epoch 46; Iter    15/   41] train: loss: 0.3601436
[Epoch 46] ogbg-molbace: 0.755678 val loss: 1.057398
[Epoch 46] ogbg-molbace: 0.805077 test loss: 1.125832
[Epoch 47; Iter     4/   41] train: loss: 0.4617167
[Epoch 47; Iter    34/   41] train: loss: 0.4142144
[Epoch 47] ogbg-molbace: 0.717216 val loss: 1.018388
[Epoch 47] ogbg-molbace: 0.836202 test loss: 0.882375
[Epoch 48; Iter    23/   41] train: loss: 0.2563684
[Epoch 48] ogbg-molbace: 0.771062 val loss: 0.823049
[Epoch 48] ogbg-molbace: 0.803165 test loss: 1.090596
[Epoch 49; Iter    12/   41] train: loss: 0.3548733
[Epoch 49] ogbg-molbace: 0.717216 val loss: 0.974915
[Epoch 49] ogbg-molbace: 0.816032 test loss: 0.795132
[Epoch 50; Iter     1/   41] train: loss: 0.3511704
[Epoch 50; Iter    31/   41] train: loss: 0.3127769
[Epoch 50] ogbg-molbace: 0.750916 val loss: 0.882291
[Epoch 50] ogbg-molbace: 0.795688 test loss: 1.414882
[Epoch 51; Iter    20/   41] train: loss: 0.3680032
[Epoch 51] ogbg-molbace: 0.725641 val loss: 0.863551
[Epoch 51] ogbg-molbace: 0.807860 test loss: 0.982341
[Epoch 52; Iter     9/   41] train: loss: 0.3818888
[Epoch 52; Iter    39/   41] train: loss: 0.3029363
[Epoch 52] ogbg-molbace: 0.721245 val loss: 1.003220
[Epoch 52] ogbg-molbace: 0.793601 test loss: 1.020138
[Epoch 53; Iter    28/   41] train: loss: 0.1612584
[Epoch 53] ogbg-molbace: 0.740659 val loss: 1.094331
[Epoch 53] ogbg-molbace: 0.814467 test loss: 0.937914
[Epoch 54; Iter    17/   41] train: loss: 0.3835732
[Epoch 54] ogbg-molbace: 0.790842 val loss: 0.866414
[Epoch 54] ogbg-molbace: 0.827334 test loss: 0.939288
[Epoch 55; Iter     6/   41] train: loss: 0.5214929
[Epoch 55; Iter    36/   41] train: loss: 0.2665672
[Epoch 55] ogbg-molbace: 0.717949 val loss: 1.222221
[Epoch 55] ogbg-molbace: 0.800730 test loss: 1.272241
[Epoch 56; Iter    25/   41] train: loss: 0.2319091
[Epoch 56] ogbg-molbace: 0.744322 val loss: 1.269027
[Epoch 56] ogbg-molbace: 0.747870 test loss: 0.877501
[Epoch 57; Iter    14/   41] train: loss: 0.3689456
[Epoch 57] ogbg-molbace: 0.775824 val loss: 1.119794
[Epoch 57] ogbg-molbace: 0.803860 test loss: 1.142858
[Epoch 58; Iter     3/   41] train: loss: 0.3101985
[Epoch 58; Iter    33/   41] train: loss: 0.2649927
[Epoch 58] ogbg-molbace: 0.739560 val loss: 1.213122
[Epoch 58] ogbg-molbace: 0.784038 test loss: 1.263226
[Epoch 59; Iter    22/   41] train: loss: 0.2972901
[Epoch 59] ogbg-molbace: 0.742125 val loss: 0.976446
[Epoch 59] ogbg-molbace: 0.778821 test loss: 0.918360
[Epoch 60; Iter    11/   41] train: loss: 0.4691603
[Epoch 60; Iter    41/   41] train: loss: 0.1862651
[Epoch 60] ogbg-molbace: 0.670330 val loss: 1.001057
[Epoch 60] ogbg-molbace: 0.806816 test loss: 0.856162
[Epoch 61; Iter    30/   41] train: loss: 0.2792954
[Epoch 61] ogbg-molbace: 0.704762 val loss: 0.960960
[Epoch 61] ogbg-molbace: 0.786646 test loss: 1.330852
[Epoch 62; Iter    19/   41] train: loss: 0.2010244
[Epoch 62] ogbg-molbace: 0.715018 val loss: 1.362956
[Epoch 62] ogbg-molbace: 0.803339 test loss: 0.893822
[Epoch 63; Iter     8/   41] train: loss: 0.2897456
[Epoch 63; Iter    38/   41] train: loss: 0.1951061
[Epoch 63] ogbg-molbace: 0.699634 val loss: 1.231885
[Epoch 63] ogbg-molbace: 0.820553 test loss: 1.038037
[Epoch 64; Iter    27/   41] train: loss: 0.2591322
[Epoch 64] ogbg-molbace: 0.750916 val loss: 0.848023
[Epoch 64] ogbg-molbace: 0.829769 test loss: 0.740387
[Epoch 65; Iter    16/   41] train: loss: 0.1753936
[Epoch 65] ogbg-molbace: 0.705128 val loss: 1.070312
[Epoch 65] ogbg-molbace: 0.807338 test loss: 1.052312
[Epoch 66; Iter     5/   41] train: loss: 0.1507813
[Epoch 66; Iter    35/   41] train: loss: 0.2683054
[Epoch 66] ogbg-molbace: 0.727106 val loss: 0.924754
[Epoch 66] ogbg-molbace: 0.808381 test loss: 1.275341
[Epoch 67; Iter    24/   41] train: loss: 0.2572788
[Epoch 67] ogbg-molbace: 0.738828 val loss: 1.053365
[Epoch 67] ogbg-molbace: 0.810816 test loss: 1.255247
[Epoch 68; Iter    13/   41] train: loss: 0.5325435
[Epoch 68] ogbg-molbace: 0.726007 val loss: 1.043962
[Epoch 68] ogbg-molbace: 0.763346 test loss: 1.506471
[Epoch 69; Iter     2/   41] train: loss: 0.2422640
[Epoch 69; Iter    32/   41] train: loss: 0.3839411
[Epoch 69] ogbg-molbace: 0.751282 val loss: 1.077334
[Epoch 69] ogbg-molbace: 0.836376 test loss: 1.059637
[Epoch 70; Iter    21/   41] train: loss: 0.1335892
[Epoch 70] ogbg-molbace: 0.685348 val loss: 1.092696
[Epoch 70] ogbg-molbace: 0.796035 test loss: 1.273912
[Epoch 71; Iter    10/   41] train: loss: 0.2272616
[Epoch 71; Iter    40/   41] train: loss: 0.1771084
[Epoch 71] ogbg-molbace: 0.726740 val loss: 0.898383
[Epoch 71] ogbg-molbace: 0.822640 test loss: 1.039852
[Epoch 72; Iter    29/   41] train: loss: 0.0997613
[Epoch 72] ogbg-molbace: 0.737729 val loss: 0.985360
[Epoch 72] ogbg-molbace: 0.793775 test loss: 1.042647
[Epoch 73; Iter    18/   41] train: loss: 0.2716444
[Epoch 73] ogbg-molbace: 0.747253 val loss: 0.857197
[Epoch 73] ogbg-molbace: 0.809077 test loss: 1.231347
[Epoch 74; Iter     7/   41] train: loss: 0.2917707
[Epoch 74; Iter    37/   41] train: loss: 0.3620298
[Epoch 74] ogbg-molbace: 0.705495 val loss: 1.377121
[Epoch 74] ogbg-molbace: 0.812728 test loss: 1.015072
[Epoch 75; Iter    26/   41] train: loss: 0.2453335
[Epoch 75] ogbg-molbace: 0.680586 val loss: 1.428141
[Epoch 75] ogbg-molbace: 0.809077 test loss: 1.368800
[Epoch 76; Iter    15/   41] train: loss: 0.1827686
[Epoch 76] ogbg-molbace: 0.712821 val loss: 1.210700
[Epoch 76] ogbg-molbace: 0.834637 test loss: 1.139766
[Epoch 77; Iter     4/   41] train: loss: 0.1168718
[Epoch 77; Iter    34/   41] train: loss: 0.0620904
[Epoch 77] ogbg-molbace: 0.692674 val loss: 1.117361
[Epoch 77] ogbg-molbace: 0.727004 test loss: 1.861236
[Epoch 78; Iter    23/   41] train: loss: 0.1817841
[Epoch 32] ogbg-molbace: 0.772909 test loss: 1.392841
[Epoch 33; Iter     8/   41] train: loss: 0.6361281
[Epoch 33; Iter    38/   41] train: loss: 0.6096950
[Epoch 33] ogbg-molbace: 0.658608 val loss: 0.846697
[Epoch 33] ogbg-molbace: 0.801426 test loss: 0.827739
[Epoch 34; Iter    27/   41] train: loss: 0.5419636
[Epoch 34] ogbg-molbace: 0.718315 val loss: 0.864469
[Epoch 34] ogbg-molbace: 0.814119 test loss: 1.007071
[Epoch 35; Iter    16/   41] train: loss: 0.3936158
[Epoch 35] ogbg-molbace: 0.680952 val loss: 0.972315
[Epoch 35] ogbg-molbace: 0.792732 test loss: 1.035091
[Epoch 36; Iter     5/   41] train: loss: 0.4430000
[Epoch 36; Iter    35/   41] train: loss: 0.3764144
[Epoch 36] ogbg-molbace: 0.705495 val loss: 0.829568
[Epoch 36] ogbg-molbace: 0.839680 test loss: 0.982195
[Epoch 37; Iter    24/   41] train: loss: 0.4959471
[Epoch 37] ogbg-molbace: 0.688278 val loss: 0.782517
[Epoch 37] ogbg-molbace: 0.822640 test loss: 0.702124
[Epoch 38; Iter    13/   41] train: loss: 0.3895800
[Epoch 38] ogbg-molbace: 0.742491 val loss: 0.809706
[Epoch 38] ogbg-molbace: 0.803860 test loss: 0.647537
[Epoch 39; Iter     2/   41] train: loss: 0.5700966
[Epoch 39; Iter    32/   41] train: loss: 0.4439177
[Epoch 39] ogbg-molbace: 0.772894 val loss: 0.681897
[Epoch 39] ogbg-molbace: 0.776213 test loss: 0.706781
[Epoch 40; Iter    21/   41] train: loss: 0.2652986
[Epoch 40] ogbg-molbace: 0.751282 val loss: 0.842334
[Epoch 40] ogbg-molbace: 0.830812 test loss: 0.867272
[Epoch 41; Iter    10/   41] train: loss: 0.5771459
[Epoch 41; Iter    40/   41] train: loss: 0.4940590
[Epoch 41] ogbg-molbace: 0.695604 val loss: 1.037421
[Epoch 41] ogbg-molbace: 0.811163 test loss: 0.874388
[Epoch 42; Iter    29/   41] train: loss: 0.3825164
[Epoch 42] ogbg-molbace: 0.743223 val loss: 0.690563
[Epoch 42] ogbg-molbace: 0.773952 test loss: 0.811899
[Epoch 43; Iter    18/   41] train: loss: 0.3576148
[Epoch 43] ogbg-molbace: 0.693040 val loss: 0.729967
[Epoch 43] ogbg-molbace: 0.734655 test loss: 1.211549
[Epoch 44; Iter     7/   41] train: loss: 0.3520736
[Epoch 44; Iter    37/   41] train: loss: 0.4242525
[Epoch 44] ogbg-molbace: 0.730403 val loss: 0.807695
[Epoch 44] ogbg-molbace: 0.796557 test loss: 0.712638
[Epoch 45; Iter    26/   41] train: loss: 0.2847494
[Epoch 45] ogbg-molbace: 0.670330 val loss: 1.256141
[Epoch 45] ogbg-molbace: 0.801426 test loss: 1.271245
[Epoch 46; Iter    15/   41] train: loss: 0.3130619
[Epoch 46] ogbg-molbace: 0.709890 val loss: 0.996351
[Epoch 46] ogbg-molbace: 0.807686 test loss: 0.820035
[Epoch 47; Iter     4/   41] train: loss: 0.2909724
[Epoch 47; Iter    34/   41] train: loss: 0.2382556
[Epoch 47] ogbg-molbace: 0.763736 val loss: 0.655124
[Epoch 47] ogbg-molbace: 0.812902 test loss: 0.646804
[Epoch 48; Iter    23/   41] train: loss: 0.2212663
[Epoch 48] ogbg-molbace: 0.752747 val loss: 1.248031
[Epoch 48] ogbg-molbace: 0.818988 test loss: 1.031132
[Epoch 49; Iter    12/   41] train: loss: 0.3964114
[Epoch 49] ogbg-molbace: 0.756044 val loss: 0.946193
[Epoch 49] ogbg-molbace: 0.833246 test loss: 0.948992
[Epoch 50; Iter     1/   41] train: loss: 0.3914080
[Epoch 50; Iter    31/   41] train: loss: 0.3126180
[Epoch 50] ogbg-molbace: 0.719414 val loss: 1.089300
[Epoch 50] ogbg-molbace: 0.825422 test loss: 0.652088
[Epoch 51; Iter    20/   41] train: loss: 0.3527275
[Epoch 51] ogbg-molbace: 0.679121 val loss: 1.264526
[Epoch 51] ogbg-molbace: 0.820901 test loss: 1.082978
[Epoch 52; Iter     9/   41] train: loss: 0.3450890
[Epoch 52; Iter    39/   41] train: loss: 0.2085010
[Epoch 52] ogbg-molbace: 0.741392 val loss: 1.107656
[Epoch 52] ogbg-molbace: 0.835855 test loss: 0.694401
[Epoch 53; Iter    28/   41] train: loss: 0.2910205
[Epoch 53] ogbg-molbace: 0.790110 val loss: 1.177997
[Epoch 53] ogbg-molbace: 0.790645 test loss: 1.141011
[Epoch 54; Iter    17/   41] train: loss: 0.2155414
[Epoch 54] ogbg-molbace: 0.752747 val loss: 0.923430
[Epoch 54] ogbg-molbace: 0.805773 test loss: 1.040678
[Epoch 55; Iter     6/   41] train: loss: 0.1358785
[Epoch 55; Iter    36/   41] train: loss: 0.2630475
[Epoch 55] ogbg-molbace: 0.678755 val loss: 1.014548
[Epoch 55] ogbg-molbace: 0.801600 test loss: 0.741145
[Epoch 56; Iter    25/   41] train: loss: 0.2470646
[Epoch 56] ogbg-molbace: 0.728938 val loss: 1.034132
[Epoch 56] ogbg-molbace: 0.816728 test loss: 0.770598
[Epoch 57; Iter    14/   41] train: loss: 0.3604282
[Epoch 57] ogbg-molbace: 0.734432 val loss: 1.191010
[Epoch 57] ogbg-molbace: 0.813772 test loss: 0.962382
[Epoch 58; Iter     3/   41] train: loss: 0.3853139
[Epoch 58; Iter    33/   41] train: loss: 0.1847769
[Epoch 58] ogbg-molbace: 0.704029 val loss: 1.189999
[Epoch 58] ogbg-molbace: 0.845070 test loss: 0.942263
[Epoch 59; Iter    22/   41] train: loss: 0.2089462
[Epoch 59] ogbg-molbace: 0.691209 val loss: 0.972432
[Epoch 59] ogbg-molbace: 0.798644 test loss: 0.937573
[Epoch 60; Iter    11/   41] train: loss: 0.4249853
[Epoch 60; Iter    41/   41] train: loss: 0.5678830
[Epoch 60] ogbg-molbace: 0.751282 val loss: 0.969474
[Epoch 60] ogbg-molbace: 0.814641 test loss: 1.043802
[Epoch 61; Iter    30/   41] train: loss: 0.3282930
[Epoch 61] ogbg-molbace: 0.708425 val loss: 1.205404
[Epoch 61] ogbg-molbace: 0.796035 test loss: 1.016296
[Epoch 62; Iter    19/   41] train: loss: 0.2726347
[Epoch 62] ogbg-molbace: 0.738462 val loss: 1.047919
[Epoch 62] ogbg-molbace: 0.815510 test loss: 1.027623
[Epoch 63; Iter     8/   41] train: loss: 0.1607568
[Epoch 63; Iter    38/   41] train: loss: 0.2383620
[Epoch 63] ogbg-molbace: 0.709890 val loss: 1.575717
[Epoch 63] ogbg-molbace: 0.790819 test loss: 1.064288
[Epoch 64; Iter    27/   41] train: loss: 0.1911372
[Epoch 64] ogbg-molbace: 0.724908 val loss: 1.075772
[Epoch 64] ogbg-molbace: 0.815684 test loss: 0.715471
[Epoch 65; Iter    16/   41] train: loss: 0.4681217
[Epoch 65] ogbg-molbace: 0.787546 val loss: 0.828038
[Epoch 65] ogbg-molbace: 0.804208 test loss: 0.752173
[Epoch 66; Iter     5/   41] train: loss: 0.1154337
[Epoch 66; Iter    35/   41] train: loss: 0.3283191
[Epoch 66] ogbg-molbace: 0.693407 val loss: 1.276124
[Epoch 66] ogbg-molbace: 0.786472 test loss: 1.213837
[Epoch 67; Iter    24/   41] train: loss: 0.3948512
[Epoch 67] ogbg-molbace: 0.687546 val loss: 1.227575
[Epoch 67] ogbg-molbace: 0.795166 test loss: 1.023200
[Epoch 68; Iter    13/   41] train: loss: 0.1863836
[Epoch 68] ogbg-molbace: 0.702930 val loss: 1.710090
[Epoch 68] ogbg-molbace: 0.748913 test loss: 1.879018
[Epoch 69; Iter     2/   41] train: loss: 0.1849210
[Epoch 69; Iter    32/   41] train: loss: 0.1907131
[Epoch 69] ogbg-molbace: 0.751648 val loss: 1.201110
[Epoch 69] ogbg-molbace: 0.797079 test loss: 0.885810
[Epoch 70; Iter    21/   41] train: loss: 0.2612190
[Epoch 70] ogbg-molbace: 0.691941 val loss: 1.354071
[Epoch 70] ogbg-molbace: 0.772909 test loss: 1.146191
[Epoch 71; Iter    10/   41] train: loss: 0.1535434
[Epoch 71; Iter    40/   41] train: loss: 0.2420447
[Epoch 71] ogbg-molbace: 0.693407 val loss: 1.541857
[Epoch 71] ogbg-molbace: 0.824378 test loss: 0.959907
[Epoch 72; Iter    29/   41] train: loss: 0.1637999
[Epoch 72] ogbg-molbace: 0.743590 val loss: 1.057524
[Epoch 72] ogbg-molbace: 0.797774 test loss: 1.049368
[Epoch 73; Iter    18/   41] train: loss: 0.1002485
[Epoch 73] ogbg-molbace: 0.769963 val loss: 1.055225
[Epoch 73] ogbg-molbace: 0.792906 test loss: 1.237872
[Epoch 74; Iter     7/   41] train: loss: 0.1321370
[Epoch 74; Iter    37/   41] train: loss: 0.4067953
[Epoch 74] ogbg-molbace: 0.712088 val loss: 1.401088
[Epoch 74] ogbg-molbace: 0.768214 test loss: 1.227190
[Epoch 75; Iter    26/   41] train: loss: 0.2224472
[Epoch 75] ogbg-molbace: 0.708425 val loss: 1.173939
[Epoch 75] ogbg-molbace: 0.783864 test loss: 1.249416
[Epoch 76; Iter    15/   41] train: loss: 0.1605660
[Epoch 76] ogbg-molbace: 0.746886 val loss: 1.089177
[Epoch 76] ogbg-molbace: 0.790645 test loss: 1.063711
[Epoch 77; Iter     4/   41] train: loss: 0.0510027
[Epoch 77; Iter    34/   41] train: loss: 0.1642260
[Epoch 77] ogbg-molbace: 0.739927 val loss: 1.330349
[Epoch 77] ogbg-molbace: 0.802295 test loss: 1.268045
[Epoch 78; Iter    23/   41] train: loss: 0.0805399
[Epoch 34] ogbg-molbace: 0.728753 val loss: 0.502939
[Epoch 34] ogbg-molbace: 0.828187 test loss: 1.026090
[Epoch 35; Iter     6/   36] train: loss: 0.5993209
[Epoch 35; Iter    36/   36] train: loss: 0.4072538
[Epoch 35] ogbg-molbace: 0.680338 val loss: 0.697102
[Epoch 35] ogbg-molbace: 0.798754 test loss: 0.721418
[Epoch 36; Iter    30/   36] train: loss: 0.4842768
[Epoch 36] ogbg-molbace: 0.707822 val loss: 0.600454
[Epoch 36] ogbg-molbace: 0.797129 test loss: 0.857211
[Epoch 37; Iter    24/   36] train: loss: 0.5224325
[Epoch 37] ogbg-molbace: 0.717548 val loss: 1.016064
[Epoch 37] ogbg-molbace: 0.810943 test loss: 0.523624
[Epoch 38; Iter    18/   36] train: loss: 0.4827017
[Epoch 38] ogbg-molbace: 0.696723 val loss: 0.623826
[Epoch 38] ogbg-molbace: 0.820242 test loss: 0.694260
[Epoch 39; Iter    12/   36] train: loss: 0.3467873
[Epoch 39] ogbg-molbace: 0.735941 val loss: 0.660176
[Epoch 39] ogbg-molbace: 0.844168 test loss: 0.614822
[Epoch 40; Iter     6/   36] train: loss: 0.3473992
[Epoch 40; Iter    36/   36] train: loss: 0.1727811
[Epoch 40] ogbg-molbace: 0.698732 val loss: 0.649927
[Epoch 40] ogbg-molbace: 0.834507 test loss: 0.604274
[Epoch 41; Iter    30/   36] train: loss: 0.4519187
[Epoch 41] ogbg-molbace: 0.689112 val loss: 0.735721
[Epoch 41] ogbg-molbace: 0.829180 test loss: 0.623972
[Epoch 42; Iter    24/   36] train: loss: 0.2976191
[Epoch 42] ogbg-molbace: 0.747146 val loss: 0.626855
[Epoch 42] ogbg-molbace: 0.835681 test loss: 0.642266
[Epoch 43; Iter    18/   36] train: loss: 0.5145783
[Epoch 43] ogbg-molbace: 0.722093 val loss: 0.540183
[Epoch 43] ogbg-molbace: 0.753431 test loss: 1.468574
[Epoch 44; Iter    12/   36] train: loss: 0.2823824
[Epoch 44] ogbg-molbace: 0.755391 val loss: 0.552362
[Epoch 44] ogbg-molbace: 0.849133 test loss: 0.718003
[Epoch 45; Iter     6/   36] train: loss: 0.3404651
[Epoch 45; Iter    36/   36] train: loss: 0.4726460
[Epoch 45] ogbg-molbace: 0.711099 val loss: 0.826140
[Epoch 45] ogbg-molbace: 0.812116 test loss: 0.596234
[Epoch 46; Iter    30/   36] train: loss: 0.3706456
[Epoch 46] ogbg-molbace: 0.736047 val loss: 0.616787
[Epoch 46] ogbg-molbace: 0.858794 test loss: 0.698897
[Epoch 47; Iter    24/   36] train: loss: 0.3861938
[Epoch 47] ogbg-molbace: 0.779070 val loss: 1.003111
[Epoch 47] ogbg-molbace: 0.838480 test loss: 0.465360
[Epoch 48; Iter    18/   36] train: loss: 0.5327071
[Epoch 48] ogbg-molbace: 0.726956 val loss: 0.542607
[Epoch 48] ogbg-molbace: 0.836674 test loss: 0.832802
[Epoch 49; Iter    12/   36] train: loss: 0.3598065
[Epoch 49] ogbg-molbace: 0.759619 val loss: 0.750933
[Epoch 49] ogbg-molbace: 0.830354 test loss: 0.541080
[Epoch 50; Iter     6/   36] train: loss: 0.2725891
[Epoch 50; Iter    36/   36] train: loss: 0.2950761
[Epoch 50] ogbg-molbace: 0.771247 val loss: 0.529284
[Epoch 50] ogbg-molbace: 0.840285 test loss: 0.666714
[Epoch 51; Iter    30/   36] train: loss: 0.3287007
[Epoch 51] ogbg-molbace: 0.723996 val loss: 0.634454
[Epoch 51] ogbg-molbace: 0.797039 test loss: 0.863557
[Epoch 52; Iter    24/   36] train: loss: 0.4407669
[Epoch 52] ogbg-molbace: 0.724419 val loss: 0.604761
[Epoch 52] ogbg-molbace: 0.806157 test loss: 1.376161
[Epoch 53; Iter    18/   36] train: loss: 0.3717287
[Epoch 53] ogbg-molbace: 0.759302 val loss: 0.695414
[Epoch 53] ogbg-molbace: 0.862044 test loss: 0.486179
[Epoch 54; Iter    12/   36] train: loss: 0.2453371
[Epoch 54] ogbg-molbace: 0.721987 val loss: 0.540849
[Epoch 54] ogbg-molbace: 0.847418 test loss: 0.969159
[Epoch 55; Iter     6/   36] train: loss: 0.4351926
[Epoch 55; Iter    36/   36] train: loss: 0.4917155
[Epoch 55] ogbg-molbace: 0.760359 val loss: 0.792241
[Epoch 55] ogbg-molbace: 0.858342 test loss: 0.528300
[Epoch 56; Iter    30/   36] train: loss: 0.3085170
[Epoch 56] ogbg-molbace: 0.774207 val loss: 0.784408
[Epoch 56] ogbg-molbace: 0.857078 test loss: 0.497414
[Epoch 57; Iter    24/   36] train: loss: 0.1870771
[Epoch 57] ogbg-molbace: 0.771564 val loss: 0.555819
[Epoch 57] ogbg-molbace: 0.842994 test loss: 0.747502
[Epoch 58; Iter    18/   36] train: loss: 0.2958010
[Epoch 58] ogbg-molbace: 0.751374 val loss: 0.627735
[Epoch 58] ogbg-molbace: 0.845612 test loss: 0.569722
[Epoch 59; Iter    12/   36] train: loss: 0.2755336
[Epoch 59] ogbg-molbace: 0.797357 val loss: 0.489099
[Epoch 59] ogbg-molbace: 0.846696 test loss: 0.714529
[Epoch 60; Iter     6/   36] train: loss: 0.1127028
[Epoch 60; Iter    36/   36] train: loss: 0.0800390
[Epoch 60] ogbg-molbace: 0.766702 val loss: 0.636704
[Epoch 60] ogbg-molbace: 0.830534 test loss: 0.739164
[Epoch 61; Iter    30/   36] train: loss: 0.2370498
[Epoch 61] ogbg-molbace: 0.728858 val loss: 0.587818
[Epoch 61] ogbg-molbace: 0.805164 test loss: 1.175586
[Epoch 62; Iter    24/   36] train: loss: 0.5126423
[Epoch 62] ogbg-molbace: 0.747674 val loss: 0.787710
[Epoch 62] ogbg-molbace: 0.857530 test loss: 0.534010
[Epoch 63; Iter    18/   36] train: loss: 0.2595212
[Epoch 63] ogbg-molbace: 0.761628 val loss: 0.648354
[Epoch 63] ogbg-molbace: 0.848230 test loss: 0.590442
[Epoch 64; Iter    12/   36] train: loss: 0.1325348
[Epoch 64] ogbg-molbace: 0.746934 val loss: 0.646962
[Epoch 64] ogbg-molbace: 0.830354 test loss: 0.867184
[Epoch 65; Iter     6/   36] train: loss: 0.3773745
[Epoch 65; Iter    36/   36] train: loss: 0.2633981
[Epoch 65] ogbg-molbace: 0.768499 val loss: 0.728887
[Epoch 65] ogbg-molbace: 0.857801 test loss: 0.571004
[Epoch 66; Iter    30/   36] train: loss: 0.2777887
[Epoch 66] ogbg-molbace: 0.777590 val loss: 0.560227
[Epoch 66] ogbg-molbace: 0.847869 test loss: 0.742383
[Epoch 67; Iter    24/   36] train: loss: 0.1790897
[Epoch 67] ogbg-molbace: 0.780127 val loss: 0.748867
[Epoch 67] ogbg-molbace: 0.859336 test loss: 0.651863
[Epoch 68; Iter    18/   36] train: loss: 0.3850377
[Epoch 68] ogbg-molbace: 0.785729 val loss: 0.626448
[Epoch 68] ogbg-molbace: 0.866739 test loss: 0.731018
[Epoch 69; Iter    12/   36] train: loss: 0.2136038
[Epoch 69] ogbg-molbace: 0.797886 val loss: 0.761036
[Epoch 69] ogbg-molbace: 0.857259 test loss: 0.632215
[Epoch 70; Iter     6/   36] train: loss: 0.4652925
[Epoch 70; Iter    36/   36] train: loss: 1.0156475
[Epoch 70] ogbg-molbace: 0.754440 val loss: 1.249955
[Epoch 70] ogbg-molbace: 0.817895 test loss: 0.540826
[Epoch 71; Iter    30/   36] train: loss: 0.4482706
[Epoch 71] ogbg-molbace: 0.783932 val loss: 0.676448
[Epoch 71] ogbg-molbace: 0.844529 test loss: 0.621095
[Epoch 72; Iter    24/   36] train: loss: 0.1304280
[Epoch 72] ogbg-molbace: 0.788795 val loss: 0.803582
[Epoch 72] ogbg-molbace: 0.867551 test loss: 0.590137
[Epoch 73; Iter    18/   36] train: loss: 0.2573226
[Epoch 73] ogbg-molbace: 0.792918 val loss: 0.666438
[Epoch 73] ogbg-molbace: 0.822409 test loss: 0.924372
[Epoch 74; Iter    12/   36] train: loss: 0.2161862
[Epoch 74] ogbg-molbace: 0.788161 val loss: 0.806333
[Epoch 74] ogbg-molbace: 0.861231 test loss: 0.568187
[Epoch 75; Iter     6/   36] train: loss: 0.3256497
[Epoch 75; Iter    36/   36] train: loss: 0.6778904
[Epoch 75] ogbg-molbace: 0.772516 val loss: 0.664503
[Epoch 75] ogbg-molbace: 0.854821 test loss: 0.743632
[Epoch 76; Iter    30/   36] train: loss: 0.2728480
[Epoch 76] ogbg-molbace: 0.777590 val loss: 0.620576
[Epoch 76] ogbg-molbace: 0.845161 test loss: 0.858382
[Epoch 77; Iter    24/   36] train: loss: 0.2014153
[Epoch 77] ogbg-molbace: 0.787949 val loss: 0.739438
[Epoch 77] ogbg-molbace: 0.857801 test loss: 0.644906
[Epoch 78; Iter    18/   36] train: loss: 0.1658293
[Epoch 78] ogbg-molbace: 0.755708 val loss: 0.737111
[Epoch 78] ogbg-molbace: 0.857349 test loss: 0.771100
[Epoch 79; Iter    12/   36] train: loss: 0.1440831
[Epoch 79] ogbg-molbace: 0.821247 val loss: 0.573584
[Epoch 79] ogbg-molbace: 0.820603 test loss: 0.982870
[Epoch 80; Iter     6/   36] train: loss: 0.2359078
[Epoch 80; Iter    36/   36] train: loss: 0.3165425
[Epoch 80] ogbg-molbace: 0.761522 val loss: 0.627162
[Epoch 80] ogbg-molbace: 0.833514 test loss: 0.850378
[Epoch 81; Iter    30/   36] train: loss: 0.0928055
[Epoch 81] ogbg-molbace: 0.797780 val loss: 0.615893
[Epoch 81] ogbg-molbace: 0.843626 test loss: 0.804311
[Epoch 82; Iter    24/   36] train: loss: 0.1008271
[Epoch 36; Iter    25/   31] train: loss: 0.2974194
[Epoch 36] ogbg-molbace: 0.751539 val loss: 0.603478
[Epoch 36] ogbg-molbace: 0.763445 test loss: 0.654506
[Epoch 37; Iter    24/   31] train: loss: 0.4011118
[Epoch 37] ogbg-molbace: 0.740616 val loss: 0.694954
[Epoch 37] ogbg-molbace: 0.768803 test loss: 0.638631
[Epoch 38; Iter    23/   31] train: loss: 0.3920840
[Epoch 38] ogbg-molbace: 0.782423 val loss: 0.798213
[Epoch 38] ogbg-molbace: 0.770606 test loss: 0.605698
[Epoch 39; Iter    22/   31] train: loss: 0.4236308
[Epoch 39] ogbg-molbace: 0.768322 val loss: 0.577860
[Epoch 39] ogbg-molbace: 0.782042 test loss: 0.652931
[Epoch 40; Iter    21/   31] train: loss: 0.4599267
[Epoch 40] ogbg-molbace: 0.741708 val loss: 0.420772
[Epoch 40] ogbg-molbace: 0.757418 test loss: 0.766825
[Epoch 41; Iter    20/   31] train: loss: 0.5582020
[Epoch 41] ogbg-molbace: 0.743297 val loss: 0.398786
[Epoch 41] ogbg-molbace: 0.760045 test loss: 0.786136
[Epoch 42; Iter    19/   31] train: loss: 0.5309665
[Epoch 42] ogbg-molbace: 0.746077 val loss: 0.620189
[Epoch 42] ogbg-molbace: 0.777869 test loss: 0.652454
[Epoch 43; Iter    18/   31] train: loss: 0.3035967
[Epoch 43] ogbg-molbace: 0.770308 val loss: 0.663687
[Epoch 43] ogbg-molbace: 0.776736 test loss: 0.662638
[Epoch 44; Iter    17/   31] train: loss: 0.4112937
[Epoch 44] ogbg-molbace: 0.749454 val loss: 0.443234
[Epoch 44] ogbg-molbace: 0.759891 test loss: 0.778563
[Epoch 45; Iter    16/   31] train: loss: 0.5135996
[Epoch 45] ogbg-molbace: 0.762363 val loss: 0.462987
[Epoch 45] ogbg-molbace: 0.785287 test loss: 0.750735
[Epoch 46; Iter    15/   31] train: loss: 0.4450800
[Epoch 46] ogbg-molbace: 0.736246 val loss: 0.414339
[Epoch 46] ogbg-molbace: 0.782506 test loss: 0.980796
[Epoch 47; Iter    14/   31] train: loss: 0.2232613
[Epoch 47] ogbg-molbace: 0.719364 val loss: 0.753684
[Epoch 47] ogbg-molbace: 0.783587 test loss: 0.660173
[Epoch 48; Iter    13/   31] train: loss: 0.4522735
[Epoch 48] ogbg-molbace: 0.729394 val loss: 0.474798
[Epoch 48] ogbg-molbace: 0.780960 test loss: 0.700146
[Epoch 49; Iter    12/   31] train: loss: 0.4093652
[Epoch 49] ogbg-molbace: 0.739424 val loss: 0.628589
[Epoch 49] ogbg-molbace: 0.809551 test loss: 0.609542
[Epoch 50; Iter    11/   31] train: loss: 0.3601680
[Epoch 50] ogbg-molbace: 0.733962 val loss: 0.385524
[Epoch 50] ogbg-molbace: 0.777509 test loss: 0.958563
[Epoch 51; Iter    10/   31] train: loss: 0.3354559
[Epoch 51] ogbg-molbace: 0.799801 val loss: 0.412446
[Epoch 51] ogbg-molbace: 0.762930 test loss: 0.808121
[Epoch 52; Iter     9/   31] train: loss: 0.4515638
[Epoch 52] ogbg-molbace: 0.799503 val loss: 0.369329
[Epoch 52] ogbg-molbace: 0.765042 test loss: 0.833760
[Epoch 53; Iter     8/   31] train: loss: 0.4050818
[Epoch 53] ogbg-molbace: 0.739722 val loss: 0.550399
[Epoch 53] ogbg-molbace: 0.773851 test loss: 0.792887
[Epoch 54; Iter     7/   31] train: loss: 0.5368055
[Epoch 54] ogbg-molbace: 0.826216 val loss: 0.414219
[Epoch 54] ogbg-molbace: 0.796569 test loss: 0.662312
[Epoch 55; Iter     6/   31] train: loss: 0.2337878
[Epoch 55] ogbg-molbace: 0.738530 val loss: 0.410383
[Epoch 55] ogbg-molbace: 0.748094 test loss: 0.928857
[Epoch 56; Iter     5/   31] train: loss: 0.3216638
[Epoch 56] ogbg-molbace: 0.755015 val loss: 0.482516
[Epoch 56] ogbg-molbace: 0.765763 test loss: 0.760092
[Epoch 57; Iter     4/   31] train: loss: 0.2172421
[Epoch 57] ogbg-molbace: 0.746177 val loss: 0.513502
[Epoch 57] ogbg-molbace: 0.799969 test loss: 0.759428
[Epoch 58; Iter     3/   31] train: loss: 0.2077632
[Epoch 58] ogbg-molbace: 0.775968 val loss: 0.602454
[Epoch 58] ogbg-molbace: 0.772254 test loss: 0.758074
[Epoch 59; Iter     2/   31] train: loss: 0.2521166
[Epoch 59] ogbg-molbace: 0.669712 val loss: 0.391476
[Epoch 59] ogbg-molbace: 0.760097 test loss: 1.137865
[Epoch 60; Iter     1/   31] train: loss: 0.3838464
[Epoch 60; Iter    31/   31] train: loss: 0.2485300
[Epoch 60] ogbg-molbace: 0.750645 val loss: 0.564039
[Epoch 60] ogbg-molbace: 0.790542 test loss: 0.872699
[Epoch 61; Iter    30/   31] train: loss: 0.2680992
[Epoch 61] ogbg-molbace: 0.798411 val loss: 0.658767
[Epoch 61] ogbg-molbace: 0.786575 test loss: 0.637157
[Epoch 62; Iter    29/   31] train: loss: 0.2517558
[Epoch 62] ogbg-molbace: 0.731480 val loss: 0.646071
[Epoch 62] ogbg-molbace: 0.776221 test loss: 0.744886
[Epoch 63; Iter    28/   31] train: loss: 0.2528524
[Epoch 63] ogbg-molbace: 0.750050 val loss: 0.439465
[Epoch 63] ogbg-molbace: 0.791881 test loss: 0.893162
[Epoch 64; Iter    27/   31] train: loss: 0.3536024
[Epoch 64] ogbg-molbace: 0.801787 val loss: 0.637706
[Epoch 64] ogbg-molbace: 0.776736 test loss: 0.751794
[Epoch 65; Iter    26/   31] train: loss: 0.3357416
[Epoch 65] ogbg-molbace: 0.750844 val loss: 0.461696
[Epoch 65] ogbg-molbace: 0.764476 test loss: 0.845581
[Epoch 66; Iter    25/   31] train: loss: 0.2565815
[Epoch 66] ogbg-molbace: 0.795531 val loss: 0.423145
[Epoch 66] ogbg-molbace: 0.751597 test loss: 1.038923
[Epoch 67; Iter    24/   31] train: loss: 0.1506520
[Epoch 67] ogbg-molbace: 0.757597 val loss: 0.593603
[Epoch 67] ogbg-molbace: 0.794921 test loss: 0.792910
[Epoch 68; Iter    23/   31] train: loss: 0.1391705
[Epoch 68] ogbg-molbace: 0.769315 val loss: 0.393120
[Epoch 68] ogbg-molbace: 0.805584 test loss: 0.890679
[Epoch 69; Iter    22/   31] train: loss: 0.2704616
[Epoch 69] ogbg-molbace: 0.749950 val loss: 0.765043
[Epoch 69] ogbg-molbace: 0.787193 test loss: 0.782262
[Epoch 70; Iter    21/   31] train: loss: 0.5403267
[Epoch 70] ogbg-molbace: 0.712711 val loss: 0.541540
[Epoch 70] ogbg-molbace: 0.770709 test loss: 0.794435
[Epoch 71; Iter    20/   31] train: loss: 0.5185484
[Epoch 71] ogbg-molbace: 0.737736 val loss: 0.424563
[Epoch 71] ogbg-molbace: 0.758912 test loss: 1.642833
[Epoch 72; Iter    19/   31] train: loss: 0.6544804
[Epoch 72] ogbg-molbace: 0.750050 val loss: 0.649633
[Epoch 72] ogbg-molbace: 0.784309 test loss: 0.675237
[Epoch 73; Iter    18/   31] train: loss: 0.4268605
[Epoch 73] ogbg-molbace: 0.708838 val loss: 0.471044
[Epoch 73] ogbg-molbace: 0.777045 test loss: 0.831154
[Epoch 74; Iter    17/   31] train: loss: 0.2680578
[Epoch 74] ogbg-molbace: 0.782324 val loss: 0.364807
[Epoch 74] ogbg-molbace: 0.786060 test loss: 1.018913
[Epoch 75; Iter    16/   31] train: loss: 0.1654830
[Epoch 75] ogbg-molbace: 0.803575 val loss: 0.887349
[Epoch 75] ogbg-molbace: 0.799402 test loss: 0.796007
[Epoch 76; Iter    15/   31] train: loss: 0.1455016
[Epoch 76] ogbg-molbace: 0.732671 val loss: 0.492368
[Epoch 76] ogbg-molbace: 0.790078 test loss: 0.914638
[Epoch 77; Iter    14/   31] train: loss: 0.1783056
[Epoch 77] ogbg-molbace: 0.749553 val loss: 0.478046
[Epoch 77] ogbg-molbace: 0.780291 test loss: 0.874043
[Epoch 78; Iter    13/   31] train: loss: 0.2348257
[Epoch 78] ogbg-molbace: 0.759285 val loss: 0.493201
[Epoch 78] ogbg-molbace: 0.797342 test loss: 0.829665
[Epoch 79; Iter    12/   31] train: loss: 0.1839135
[Epoch 79] ogbg-molbace: 0.757398 val loss: 0.742882
[Epoch 79] ogbg-molbace: 0.808417 test loss: 0.869839
[Epoch 80; Iter    11/   31] train: loss: 0.1875276
[Epoch 80] ogbg-molbace: 0.734260 val loss: 0.579103
[Epoch 80] ogbg-molbace: 0.787142 test loss: 0.996558
[Epoch 81; Iter    10/   31] train: loss: 0.1832232
[Epoch 81] ogbg-molbace: 0.759881 val loss: 0.519029
[Epoch 81] ogbg-molbace: 0.766948 test loss: 1.261333
[Epoch 82; Iter     9/   31] train: loss: 0.1705929
[Epoch 82] ogbg-molbace: 0.769414 val loss: 0.823157
[Epoch 82] ogbg-molbace: 0.795539 test loss: 0.955175
[Epoch 83; Iter     8/   31] train: loss: 0.2533587
[Epoch 83] ogbg-molbace: 0.701092 val loss: 0.500207
[Epoch 83] ogbg-molbace: 0.788842 test loss: 1.093096
[Epoch 84; Iter     7/   31] train: loss: 0.2132115
[Epoch 84] ogbg-molbace: 0.714896 val loss: 0.599734
[Epoch 84] ogbg-molbace: 0.770400 test loss: 1.077096
[Epoch 85; Iter     6/   31] train: loss: 0.1986055
[Epoch 85] ogbg-molbace: 0.719067 val loss: 0.501546
[Epoch 85] ogbg-molbace: 0.800072 test loss: 1.027690
[Epoch 86; Iter     5/   31] train: loss: 0.1330252
[Epoch 86] ogbg-molbace: 0.742602 val loss: 0.621160
[Epoch 86] ogbg-molbace: 0.802390 test loss: 0.920749
[Epoch 34] ogbg-molbace: 0.699049 val loss: 1.192881
[Epoch 34] ogbg-molbace: 0.815096 test loss: 0.491936
[Epoch 35; Iter     6/   36] train: loss: 0.2557412
[Epoch 35; Iter    36/   36] train: loss: 0.3374632
[Epoch 35] ogbg-molbace: 0.718922 val loss: 0.699498
[Epoch 35] ogbg-molbace: 0.814825 test loss: 0.613541
[Epoch 36; Iter    30/   36] train: loss: 0.4039569
[Epoch 36] ogbg-molbace: 0.709197 val loss: 0.789864
[Epoch 36] ogbg-molbace: 0.808234 test loss: 0.595233
[Epoch 37; Iter    24/   36] train: loss: 0.3484161
[Epoch 37] ogbg-molbace: 0.712262 val loss: 0.539497
[Epoch 37] ogbg-molbace: 0.813741 test loss: 0.868253
[Epoch 38; Iter    18/   36] train: loss: 0.5010783
[Epoch 38] ogbg-molbace: 0.719345 val loss: 0.678300
[Epoch 38] ogbg-molbace: 0.811484 test loss: 0.648509
[Epoch 39; Iter    12/   36] train: loss: 0.5080251
[Epoch 39] ogbg-molbace: 0.711099 val loss: 0.751075
[Epoch 39] ogbg-molbace: 0.805164 test loss: 0.709709
[Epoch 40; Iter     6/   36] train: loss: 0.4411238
[Epoch 40; Iter    36/   36] train: loss: 0.4467803
[Epoch 40] ogbg-molbace: 0.761945 val loss: 0.819004
[Epoch 40] ogbg-molbace: 0.795594 test loss: 0.576937
[Epoch 41; Iter    30/   36] train: loss: 0.3985666
[Epoch 41] ogbg-molbace: 0.702854 val loss: 0.793927
[Epoch 41] ogbg-molbace: 0.859245 test loss: 0.504811
[Epoch 42; Iter    24/   36] train: loss: 0.3073522
[Epoch 42] ogbg-molbace: 0.710571 val loss: 0.771223
[Epoch 42] ogbg-molbace: 0.832430 test loss: 0.505823
[Epoch 43; Iter    18/   36] train: loss: 0.3730266
[Epoch 43] ogbg-molbace: 0.737526 val loss: 0.538732
[Epoch 43] ogbg-molbace: 0.829000 test loss: 0.726898
[Epoch 44; Iter    12/   36] train: loss: 0.5607406
[Epoch 44] ogbg-molbace: 0.735624 val loss: 0.726358
[Epoch 44] ogbg-molbace: 0.863850 test loss: 0.528997
[Epoch 45; Iter     6/   36] train: loss: 0.2108654
[Epoch 45; Iter    36/   36] train: loss: 0.5000048
[Epoch 45] ogbg-molbace: 0.716596 val loss: 0.536888
[Epoch 45] ogbg-molbace: 0.788642 test loss: 0.888908
[Epoch 46; Iter    30/   36] train: loss: 0.3666939
[Epoch 46] ogbg-molbace: 0.740592 val loss: 0.811864
[Epoch 46] ogbg-molbace: 0.835681 test loss: 0.594255
[Epoch 47; Iter    24/   36] train: loss: 0.3469655
[Epoch 47] ogbg-molbace: 0.778118 val loss: 0.615186
[Epoch 47] ogbg-molbace: 0.863579 test loss: 0.539048
[Epoch 48; Iter    18/   36] train: loss: 0.3021402
[Epoch 48] ogbg-molbace: 0.741226 val loss: 0.533197
[Epoch 48] ogbg-molbace: 0.834326 test loss: 0.813920
[Epoch 49; Iter    12/   36] train: loss: 0.3664103
[Epoch 49] ogbg-molbace: 0.757082 val loss: 0.680248
[Epoch 49] ogbg-molbace: 0.830986 test loss: 0.559506
[Epoch 50; Iter     6/   36] train: loss: 0.4410341
[Epoch 50; Iter    36/   36] train: loss: 0.3270394
[Epoch 50] ogbg-molbace: 0.688161 val loss: 0.683155
[Epoch 50] ogbg-molbace: 0.797129 test loss: 0.709615
[Epoch 51; Iter    30/   36] train: loss: 0.3680836
[Epoch 51] ogbg-molbace: 0.718182 val loss: 0.542449
[Epoch 51] ogbg-molbace: 0.827194 test loss: 0.901651
[Epoch 52; Iter    24/   36] train: loss: 0.2707965
[Epoch 52] ogbg-molbace: 0.739958 val loss: 0.627415
[Epoch 52] ogbg-molbace: 0.849765 test loss: 0.580580
[Epoch 53; Iter    18/   36] train: loss: 0.3179642
[Epoch 53] ogbg-molbace: 0.714482 val loss: 0.691272
[Epoch 53] ogbg-molbace: 0.816540 test loss: 1.210554
[Epoch 54; Iter    12/   36] train: loss: 0.3131193
[Epoch 54] ogbg-molbace: 0.798309 val loss: 0.698052
[Epoch 54] ogbg-molbace: 0.823402 test loss: 0.533158
[Epoch 55; Iter     6/   36] train: loss: 0.3520060
[Epoch 55; Iter    36/   36] train: loss: 0.5510806
[Epoch 55] ogbg-molbace: 0.792389 val loss: 0.533886
[Epoch 55] ogbg-molbace: 0.836764 test loss: 0.733517
[Epoch 56; Iter    30/   36] train: loss: 0.2312210
[Epoch 56] ogbg-molbace: 0.766596 val loss: 0.751519
[Epoch 56] ogbg-molbace: 0.849224 test loss: 0.530993
[Epoch 57; Iter    24/   36] train: loss: 0.2034846
[Epoch 57] ogbg-molbace: 0.778118 val loss: 0.576947
[Epoch 57] ogbg-molbace: 0.848230 test loss: 0.717850
[Epoch 58; Iter    18/   36] train: loss: 0.2474354
[Epoch 58] ogbg-molbace: 0.778013 val loss: 0.699295
[Epoch 58] ogbg-molbace: 0.837216 test loss: 0.534937
[Epoch 59; Iter    12/   36] train: loss: 0.2492941
[Epoch 59] ogbg-molbace: 0.789535 val loss: 0.609788
[Epoch 59] ogbg-molbace: 0.839292 test loss: 0.541417
[Epoch 60; Iter     6/   36] train: loss: 0.3905114
[Epoch 60; Iter    36/   36] train: loss: 0.8389832
[Epoch 60] ogbg-molbace: 0.767230 val loss: 0.668269
[Epoch 60] ogbg-molbace: 0.822770 test loss: 0.676348
[Epoch 61; Iter    30/   36] train: loss: 0.1827721
[Epoch 61] ogbg-molbace: 0.767759 val loss: 0.628889
[Epoch 61] ogbg-molbace: 0.839563 test loss: 0.666979
[Epoch 62; Iter    24/   36] train: loss: 0.1015847
[Epoch 62] ogbg-molbace: 0.753383 val loss: 0.589365
[Epoch 62] ogbg-molbace: 0.816901 test loss: 0.725659
[Epoch 63; Iter    18/   36] train: loss: 0.3601573
[Epoch 63] ogbg-molbace: 0.782452 val loss: 0.523084
[Epoch 63] ogbg-molbace: 0.833062 test loss: 1.085511
[Epoch 64; Iter    12/   36] train: loss: 0.3354377
[Epoch 64] ogbg-molbace: 0.763108 val loss: 0.825418
[Epoch 64] ogbg-molbace: 0.828729 test loss: 0.587297
[Epoch 65; Iter     6/   36] train: loss: 0.2601026
[Epoch 65; Iter    36/   36] train: loss: 0.4055477
[Epoch 65] ogbg-molbace: 0.761416 val loss: 0.533702
[Epoch 65] ogbg-molbace: 0.855092 test loss: 1.070093
[Epoch 66; Iter    30/   36] train: loss: 0.2566479
[Epoch 66] ogbg-molbace: 0.786047 val loss: 0.611122
[Epoch 66] ogbg-molbace: 0.851752 test loss: 0.636375
[Epoch 67; Iter    24/   36] train: loss: 0.2797236
[Epoch 67] ogbg-molbace: 0.764482 val loss: 0.592491
[Epoch 67] ogbg-molbace: 0.829902 test loss: 0.738392
[Epoch 68; Iter    18/   36] train: loss: 0.1724294
[Epoch 68] ogbg-molbace: 0.785307 val loss: 0.715738
[Epoch 68] ogbg-molbace: 0.834326 test loss: 0.588101
[Epoch 69; Iter    12/   36] train: loss: 0.3823309
[Epoch 69] ogbg-molbace: 0.747040 val loss: 0.762989
[Epoch 69] ogbg-molbace: 0.834688 test loss: 0.789769
[Epoch 70; Iter     6/   36] train: loss: 0.2018097
[Epoch 70; Iter    36/   36] train: loss: 0.3656124
[Epoch 70] ogbg-molbace: 0.761099 val loss: 0.541855
[Epoch 70] ogbg-molbace: 0.846334 test loss: 0.842802
[Epoch 71; Iter    30/   36] train: loss: 0.3625475
[Epoch 71] ogbg-molbace: 0.806660 val loss: 0.481008
[Epoch 71] ogbg-molbace: 0.844077 test loss: 0.803509
[Epoch 72; Iter    24/   36] train: loss: 0.1395484
[Epoch 72] ogbg-molbace: 0.778118 val loss: 0.640946
[Epoch 72] ogbg-molbace: 0.858342 test loss: 0.647274
[Epoch 73; Iter    18/   36] train: loss: 0.1997216
[Epoch 73] ogbg-molbace: 0.775053 val loss: 0.698199
[Epoch 73] ogbg-molbace: 0.833694 test loss: 0.717555
[Epoch 74; Iter    12/   36] train: loss: 0.1251536
[Epoch 74] ogbg-molbace: 0.772727 val loss: 0.588227
[Epoch 74] ogbg-molbace: 0.850578 test loss: 0.859221
[Epoch 75; Iter     6/   36] train: loss: 0.1242696
[Epoch 75; Iter    36/   36] train: loss: 0.1065294
[Epoch 75] ogbg-molbace: 0.756025 val loss: 0.815551
[Epoch 75] ogbg-molbace: 0.839744 test loss: 0.669536
[Epoch 76; Iter    30/   36] train: loss: 0.1149499
[Epoch 76] ogbg-molbace: 0.777696 val loss: 0.841260
[Epoch 76] ogbg-molbace: 0.844348 test loss: 0.806141
[Epoch 77; Iter    24/   36] train: loss: 0.2542416
[Epoch 77] ogbg-molbace: 0.797992 val loss: 0.667874
[Epoch 77] ogbg-molbace: 0.846966 test loss: 0.713713
[Epoch 78; Iter    18/   36] train: loss: 0.1472608
[Epoch 78] ogbg-molbace: 0.792495 val loss: 0.673485
[Epoch 78] ogbg-molbace: 0.842181 test loss: 0.835508
[Epoch 79; Iter    12/   36] train: loss: 0.4429310
[Epoch 79] ogbg-molbace: 0.779704 val loss: 0.841680
[Epoch 79] ogbg-molbace: 0.839834 test loss: 0.701023
[Epoch 80; Iter     6/   36] train: loss: 0.1224110
[Epoch 80; Iter    36/   36] train: loss: 0.3479406
[Epoch 80] ogbg-molbace: 0.780973 val loss: 0.673124
[Epoch 80] ogbg-molbace: 0.840827 test loss: 0.864954
[Epoch 81; Iter    30/   36] train: loss: 0.0968798
[Epoch 81] ogbg-molbace: 0.799894 val loss: 0.635807
[Epoch 81] ogbg-molbace: 0.847418 test loss: 0.967943
[Epoch 82; Iter    24/   36] train: loss: 0.0330478
[Epoch 36; Iter    25/   31] train: loss: 0.5136256
[Epoch 36] ogbg-molbace: 0.742006 val loss: 0.752177
[Epoch 36] ogbg-molbace: 0.779621 test loss: 0.617062
[Epoch 37; Iter    24/   31] train: loss: 0.3972375
[Epoch 37] ogbg-molbace: 0.696425 val loss: 0.477118
[Epoch 37] ogbg-molbace: 0.779466 test loss: 0.684440
[Epoch 38; Iter    23/   31] train: loss: 0.6766025
[Epoch 38] ogbg-molbace: 0.699404 val loss: 0.603793
[Epoch 38] ogbg-molbace: 0.762673 test loss: 0.763582
[Epoch 39; Iter    22/   31] train: loss: 0.6256487
[Epoch 39] ogbg-molbace: 0.685303 val loss: 0.524387
[Epoch 39] ogbg-molbace: 0.730012 test loss: 0.808899
[Epoch 40; Iter    21/   31] train: loss: 0.5770199
[Epoch 40] ogbg-molbace: 0.759782 val loss: 0.408601
[Epoch 40] ogbg-molbace: 0.777148 test loss: 0.754530
[Epoch 41; Iter    20/   31] train: loss: 0.3992542
[Epoch 41] ogbg-molbace: 0.738530 val loss: 0.595721
[Epoch 41] ogbg-molbace: 0.771636 test loss: 0.634718
[Epoch 42; Iter    19/   31] train: loss: 0.3171318
[Epoch 42] ogbg-molbace: 0.741509 val loss: 0.544430
[Epoch 42] ogbg-molbace: 0.757109 test loss: 0.692639
[Epoch 43; Iter    18/   31] train: loss: 0.4813371
[Epoch 43] ogbg-molbace: 0.745184 val loss: 0.694238
[Epoch 43] ogbg-molbace: 0.766742 test loss: 0.751086
[Epoch 44; Iter    17/   31] train: loss: 0.3323407
[Epoch 44] ogbg-molbace: 0.740119 val loss: 0.469204
[Epoch 44] ogbg-molbace: 0.764063 test loss: 0.760592
[Epoch 45; Iter    16/   31] train: loss: 0.5687582
[Epoch 45] ogbg-molbace: 0.777954 val loss: 0.778596
[Epoch 45] ogbg-molbace: 0.778951 test loss: 0.703937
[Epoch 46; Iter    15/   31] train: loss: 0.4209657
[Epoch 46] ogbg-molbace: 0.769315 val loss: 0.592491
[Epoch 46] ogbg-molbace: 0.788430 test loss: 0.630180
[Epoch 47; Iter    14/   31] train: loss: 0.3916879
[Epoch 47] ogbg-molbace: 0.686097 val loss: 0.700170
[Epoch 47] ogbg-molbace: 0.751133 test loss: 0.762739
[Epoch 48; Iter    13/   31] train: loss: 0.4846019
[Epoch 48] ogbg-molbace: 0.806653 val loss: 0.682353
[Epoch 48] ogbg-molbace: 0.791366 test loss: 0.565767
[Epoch 49; Iter    12/   31] train: loss: 0.3759828
[Epoch 49] ogbg-molbace: 0.815988 val loss: 0.680296
[Epoch 49] ogbg-molbace: 0.809087 test loss: 0.587372
[Epoch 50; Iter    11/   31] train: loss: 0.2782840
[Epoch 50] ogbg-molbace: 0.757398 val loss: 0.605134
[Epoch 50] ogbg-molbace: 0.788430 test loss: 0.642497
[Epoch 51; Iter    10/   31] train: loss: 0.6335003
[Epoch 51] ogbg-molbace: 0.764151 val loss: 0.485748
[Epoch 51] ogbg-molbace: 0.793427 test loss: 0.654642
[Epoch 52; Iter     9/   31] train: loss: 0.3672214
[Epoch 52] ogbg-molbace: 0.708441 val loss: 0.725782
[Epoch 52] ogbg-molbace: 0.767773 test loss: 0.783558
[Epoch 53; Iter     8/   31] train: loss: 0.5106075
[Epoch 53] ogbg-molbace: 0.793644 val loss: 0.702827
[Epoch 53] ogbg-molbace: 0.786524 test loss: 0.658139
[Epoch 54; Iter     7/   31] train: loss: 0.4283973
[Epoch 54] ogbg-molbace: 0.736346 val loss: 0.396586
[Epoch 54] ogbg-molbace: 0.788275 test loss: 0.787554
[Epoch 55; Iter     6/   31] train: loss: 0.3825192
[Epoch 55] ogbg-molbace: 0.685601 val loss: 0.420189
[Epoch 55] ogbg-molbace: 0.718009 test loss: 0.924289
[Epoch 56; Iter     5/   31] train: loss: 0.4391155
[Epoch 56] ogbg-molbace: 0.763754 val loss: 0.527871
[Epoch 56] ogbg-molbace: 0.753142 test loss: 0.735900
[Epoch 57; Iter     4/   31] train: loss: 0.4854741
[Epoch 57] ogbg-molbace: 0.738530 val loss: 0.421870
[Epoch 57] ogbg-molbace: 0.780806 test loss: 0.759091
[Epoch 58; Iter     3/   31] train: loss: 0.2820008
[Epoch 58] ogbg-molbace: 0.735849 val loss: 0.981198
[Epoch 58] ogbg-molbace: 0.810942 test loss: 0.688269
[Epoch 59; Iter     2/   31] train: loss: 0.5166726
[Epoch 59] ogbg-molbace: 0.733069 val loss: 0.430200
[Epoch 59] ogbg-molbace: 0.798887 test loss: 0.778851
[Epoch 60; Iter     1/   31] train: loss: 0.3050849
[Epoch 60; Iter    31/   31] train: loss: 0.3588446
[Epoch 60] ogbg-molbace: 0.769017 val loss: 0.660129
[Epoch 60] ogbg-molbace: 0.798424 test loss: 0.642991
[Epoch 61; Iter    30/   31] train: loss: 0.2362095
[Epoch 61] ogbg-molbace: 0.770904 val loss: 0.633714
[Epoch 61] ogbg-molbace: 0.794766 test loss: 0.641226
[Epoch 62; Iter    29/   31] train: loss: 0.3336772
[Epoch 62] ogbg-molbace: 0.731778 val loss: 0.481731
[Epoch 62] ogbg-molbace: 0.793581 test loss: 0.704644
[Epoch 63; Iter    28/   31] train: loss: 0.4056762
[Epoch 63] ogbg-molbace: 0.691658 val loss: 0.478009
[Epoch 63] ogbg-molbace: 0.795436 test loss: 0.795658
[Epoch 64; Iter    27/   31] train: loss: 0.2737582
[Epoch 64] ogbg-molbace: 0.725720 val loss: 0.714324
[Epoch 64] ogbg-molbace: 0.785494 test loss: 0.661331
[Epoch 65; Iter    26/   31] train: loss: 0.4334934
[Epoch 65] ogbg-molbace: 0.670506 val loss: 0.410788
[Epoch 65] ogbg-molbace: 0.779518 test loss: 1.176561
[Epoch 66; Iter    25/   31] train: loss: 0.3310532
[Epoch 66] ogbg-molbace: 0.794836 val loss: 0.603296
[Epoch 66] ogbg-molbace: 0.793839 test loss: 0.681521
[Epoch 67; Iter    24/   31] train: loss: 0.2312767
[Epoch 67] ogbg-molbace: 0.733466 val loss: 0.537136
[Epoch 67] ogbg-molbace: 0.796312 test loss: 0.760150
[Epoch 68; Iter    23/   31] train: loss: 0.3548377
[Epoch 68] ogbg-molbace: 0.753525 val loss: 0.490465
[Epoch 68] ogbg-molbace: 0.827735 test loss: 0.711509
[Epoch 69; Iter    22/   31] train: loss: 0.2433289
[Epoch 69] ogbg-molbace: 0.700099 val loss: 0.386281
[Epoch 69] ogbg-molbace: 0.813466 test loss: 1.098002
[Epoch 70; Iter    21/   31] train: loss: 0.2891711
[Epoch 70] ogbg-molbace: 0.703774 val loss: 0.848931
[Epoch 70] ogbg-molbace: 0.803987 test loss: 0.732821
[Epoch 71; Iter    20/   31] train: loss: 0.2377833
[Epoch 71] ogbg-molbace: 0.744687 val loss: 0.457809
[Epoch 71] ogbg-molbace: 0.776839 test loss: 0.915841
[Epoch 72; Iter    19/   31] train: loss: 0.2014074
[Epoch 72] ogbg-molbace: 0.741410 val loss: 0.496310
[Epoch 72] ogbg-molbace: 0.806305 test loss: 0.696434
[Epoch 73; Iter    18/   31] train: loss: 0.3806268
[Epoch 73] ogbg-molbace: 0.730387 val loss: 0.838262
[Epoch 73] ogbg-molbace: 0.805018 test loss: 0.787810
[Epoch 74; Iter    17/   31] train: loss: 0.3320669
[Epoch 74] ogbg-molbace: 0.745680 val loss: 0.564637
[Epoch 74] ogbg-molbace: 0.822223 test loss: 0.755973
[Epoch 75; Iter    16/   31] train: loss: 0.4390043
[Epoch 75] ogbg-molbace: 0.742403 val loss: 0.550488
[Epoch 75] ogbg-molbace: 0.819545 test loss: 0.746538
[Epoch 76; Iter    15/   31] train: loss: 0.1301524
[Epoch 76] ogbg-molbace: 0.727805 val loss: 0.455600
[Epoch 76] ogbg-molbace: 0.811045 test loss: 0.963233
[Epoch 77; Iter    14/   31] train: loss: 0.3574840
[Epoch 77] ogbg-molbace: 0.702979 val loss: 0.474901
[Epoch 77] ogbg-molbace: 0.794406 test loss: 0.875863
[Epoch 78; Iter    13/   31] train: loss: 0.3111386
[Epoch 78] ogbg-molbace: 0.735948 val loss: 0.525580
[Epoch 78] ogbg-molbace: 0.811148 test loss: 0.636049
[Epoch 79; Iter    12/   31] train: loss: 0.3684947
[Epoch 79] ogbg-molbace: 0.709930 val loss: 0.541702
[Epoch 79] ogbg-molbace: 0.805945 test loss: 0.784637
[Epoch 80; Iter    11/   31] train: loss: 0.1438066
[Epoch 80] ogbg-molbace: 0.727905 val loss: 0.812467
[Epoch 80] ogbg-molbace: 0.816969 test loss: 0.813708
[Epoch 81; Iter    10/   31] train: loss: 0.2592690
[Epoch 81] ogbg-molbace: 0.694935 val loss: 0.433071
[Epoch 81] ogbg-molbace: 0.815217 test loss: 0.826196
[Epoch 82; Iter     9/   31] train: loss: 0.0935937
[Epoch 82] ogbg-molbace: 0.790566 val loss: 0.735770
[Epoch 82] ogbg-molbace: 0.821451 test loss: 0.792274
[Epoch 83; Iter     8/   31] train: loss: 0.2820387
[Epoch 83] ogbg-molbace: 0.745780 val loss: 0.498076
[Epoch 83] ogbg-molbace: 0.809139 test loss: 0.829461
[Epoch 84; Iter     7/   31] train: loss: 0.2215387
[Epoch 84] ogbg-molbace: 0.741212 val loss: 0.517747
[Epoch 84] ogbg-molbace: 0.817433 test loss: 0.921815
[Epoch 85; Iter     6/   31] train: loss: 0.1174762
[Epoch 85] ogbg-molbace: 0.757398 val loss: 0.642248
[Epoch 85] ogbg-molbace: 0.800999 test loss: 1.151897
[Epoch 86; Iter     5/   31] train: loss: 0.2369090
[Epoch 86] ogbg-molbace: 0.753029 val loss: 1.317489
[Epoch 86] ogbg-molbace: 0.802545 test loss: 1.528785
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 87; Iter     4/   31] train: loss: 0.0839068
[Epoch 87] ogbg-molbace: 0.768222 val loss: 0.556397
[Epoch 87] ogbg-molbace: 0.791160 test loss: 1.098528
[Epoch 88; Iter     3/   31] train: loss: 0.1374437
[Epoch 88] ogbg-molbace: 0.731380 val loss: 0.678510
[Epoch 88] ogbg-molbace: 0.799196 test loss: 0.983455
[Epoch 89; Iter     2/   31] train: loss: 0.1573095
[Epoch 89] ogbg-molbace: 0.793148 val loss: 0.879620
[Epoch 89] ogbg-molbace: 0.816814 test loss: 1.035034
[Epoch 90; Iter     1/   31] train: loss: 0.0848718
[Epoch 90; Iter    31/   31] train: loss: 0.0313343
[Epoch 90] ogbg-molbace: 0.763257 val loss: 0.613058
[Epoch 90] ogbg-molbace: 0.807542 test loss: 1.023972
[Epoch 91; Iter    30/   31] train: loss: 0.1055509
[Epoch 91] ogbg-molbace: 0.775074 val loss: 0.755567
[Epoch 91] ogbg-molbace: 0.798681 test loss: 1.180116
[Epoch 92; Iter    29/   31] train: loss: 0.2208888
[Epoch 92] ogbg-molbace: 0.789176 val loss: 0.650253
[Epoch 92] ogbg-molbace: 0.800278 test loss: 0.957805
[Epoch 93; Iter    28/   31] train: loss: 0.2029039
[Epoch 93] ogbg-molbace: 0.773883 val loss: 0.457800
[Epoch 93] ogbg-molbace: 0.811148 test loss: 1.060900
[Epoch 94; Iter    27/   31] train: loss: 0.1255154
[Epoch 94] ogbg-molbace: 0.776068 val loss: 1.015534
[Epoch 94] ogbg-molbace: 0.809499 test loss: 1.064789
[Epoch 95; Iter    26/   31] train: loss: 0.0510934
[Epoch 95] ogbg-molbace: 0.777557 val loss: 0.432262
[Epoch 95] ogbg-molbace: 0.802442 test loss: 1.129587
[Epoch 96; Iter    25/   31] train: loss: 0.0794031
[Epoch 96] ogbg-molbace: 0.768024 val loss: 0.668740
[Epoch 96] ogbg-molbace: 0.805172 test loss: 1.028658
[Epoch 97; Iter    24/   31] train: loss: 0.0904112
[Epoch 97] ogbg-molbace: 0.753724 val loss: 0.664745
[Epoch 97] ogbg-molbace: 0.801566 test loss: 1.003480
[Epoch 98; Iter    23/   31] train: loss: 0.1017707
[Epoch 98] ogbg-molbace: 0.780735 val loss: 0.590537
[Epoch 98] ogbg-molbace: 0.792757 test loss: 1.017254
[Epoch 99; Iter    22/   31] train: loss: 0.3920858
[Epoch 99] ogbg-molbace: 0.754916 val loss: 0.469035
[Epoch 99] ogbg-molbace: 0.808263 test loss: 1.453970
[Epoch 100; Iter    21/   31] train: loss: 0.2307621
[Epoch 100] ogbg-molbace: 0.783019 val loss: 0.816900
[Epoch 100] ogbg-molbace: 0.798218 test loss: 0.971014
[Epoch 101; Iter    20/   31] train: loss: 0.1794938
[Epoch 101] ogbg-molbace: 0.723237 val loss: 0.712600
[Epoch 101] ogbg-molbace: 0.789048 test loss: 1.253238
[Epoch 102; Iter    19/   31] train: loss: 0.1514402
[Epoch 102] ogbg-molbace: 0.763059 val loss: 0.785843
[Epoch 102] ogbg-molbace: 0.794354 test loss: 1.047010
[Epoch 103; Iter    18/   31] train: loss: 0.0790372
[Epoch 103] ogbg-molbace: 0.773188 val loss: 0.401052
[Epoch 103] ogbg-molbace: 0.792087 test loss: 1.128094
[Epoch 104; Iter    17/   31] train: loss: 0.1348540
[Epoch 104] ogbg-molbace: 0.760477 val loss: 0.945209
[Epoch 104] ogbg-molbace: 0.805790 test loss: 1.103276
[Epoch 105; Iter    16/   31] train: loss: 0.1152794
[Epoch 105] ogbg-molbace: 0.766236 val loss: 0.648700
[Epoch 105] ogbg-molbace: 0.794869 test loss: 1.099144
[Epoch 106; Iter    15/   31] train: loss: 0.0504350
[Epoch 106] ogbg-molbace: 0.780338 val loss: 0.458691
[Epoch 106] ogbg-molbace: 0.798990 test loss: 0.991469
[Epoch 107; Iter    14/   31] train: loss: 0.0802146
[Epoch 107] ogbg-molbace: 0.773684 val loss: 0.770896
[Epoch 107] ogbg-molbace: 0.806460 test loss: 1.080203
[Epoch 108; Iter    13/   31] train: loss: 0.0912463
[Epoch 108] ogbg-molbace: 0.768123 val loss: 0.530913
[Epoch 108] ogbg-molbace: 0.791624 test loss: 0.986468
[Epoch 109; Iter    12/   31] train: loss: 0.1677539
[Epoch 109] ogbg-molbace: 0.763654 val loss: 0.561317
[Epoch 109] ogbg-molbace: 0.805018 test loss: 1.020723
[Epoch 110; Iter    11/   31] train: loss: 0.0345164
[Epoch 110] ogbg-molbace: 0.773684 val loss: 0.577075
[Epoch 110] ogbg-molbace: 0.800484 test loss: 1.026920
[Epoch 111; Iter    10/   31] train: loss: 0.0342757
[Epoch 111] ogbg-molbace: 0.773386 val loss: 0.573643
[Epoch 111] ogbg-molbace: 0.802545 test loss: 1.040598
[Epoch 112; Iter     9/   31] train: loss: 0.0985513
[Epoch 112] ogbg-molbace: 0.772095 val loss: 0.584214
[Epoch 112] ogbg-molbace: 0.792499 test loss: 1.061745
[Epoch 113; Iter     8/   31] train: loss: 0.0516049
[Epoch 113] ogbg-molbace: 0.776365 val loss: 0.773751
[Epoch 113] ogbg-molbace: 0.794354 test loss: 1.072968
[Epoch 114; Iter     7/   31] train: loss: 0.0500083
[Epoch 114] ogbg-molbace: 0.777954 val loss: 0.638214
[Epoch 114] ogbg-molbace: 0.799402 test loss: 1.052886
[Epoch 115; Iter     6/   31] train: loss: 0.0218560
[Epoch 115] ogbg-molbace: 0.784906 val loss: 0.714872
[Epoch 115] ogbg-molbace: 0.806924 test loss: 1.179166
[Epoch 116; Iter     5/   31] train: loss: 0.0723589
[Epoch 116] ogbg-molbace: 0.761569 val loss: 0.857341
[Epoch 116] ogbg-molbace: 0.791160 test loss: 1.150052
[Epoch 117; Iter     4/   31] train: loss: 0.0270004
[Epoch 117] ogbg-molbace: 0.762363 val loss: 0.644115
[Epoch 117] ogbg-molbace: 0.798990 test loss: 1.185222
[Epoch 118; Iter     3/   31] train: loss: 0.0401526
[Epoch 118] ogbg-molbace: 0.756107 val loss: 0.495316
[Epoch 118] ogbg-molbace: 0.796157 test loss: 1.059782
[Epoch 119; Iter     2/   31] train: loss: 0.0274089
[Epoch 119] ogbg-molbace: 0.775074 val loss: 0.630768
[Epoch 119] ogbg-molbace: 0.790645 test loss: 1.021270
[Epoch 120; Iter     1/   31] train: loss: 0.0449591
[Epoch 120; Iter    31/   31] train: loss: 0.3296115
[Epoch 120] ogbg-molbace: 0.774578 val loss: 0.740280
[Epoch 120] ogbg-molbace: 0.797754 test loss: 1.053260
[Epoch 121; Iter    30/   31] train: loss: 0.0898911
[Epoch 121] ogbg-molbace: 0.777656 val loss: 1.070407
[Epoch 121] ogbg-molbace: 0.789409 test loss: 1.206249
Early stopping criterion based on -ogbg-molbace- that should be max reached after 121 epochs. Best model checkpoint was in epoch 61.
Statistics on  val_best_checkpoint
mean_pred: -1.5784868001937866
std_pred: 2.3114993572235107
mean_targets: 0.12541253864765167
std_targets: 0.33173397183418274
prcauc: 0.4042161221506902
rocauc: 0.8300893743793446
ogbg-molbace: 0.8300893743793446
BCEWithLogitsLoss: 0.43456223471598193
Statistics on  test
mean_pred: 0.07558473199605942
std_pred: 2.5069806575775146
mean_targets: 0.6963696479797363
std_targets: 0.4605855941772461
prcauc: 0.8566834911162851
rocauc: 0.7628271172470636
ogbg-molbace: 0.7628271172470636
BCEWithLogitsLoss: 0.7111506543376229
Statistics on  train
mean_pred: -0.3303534984588623
std_pred: 2.7537219524383545
mean_targets: 0.48732084035873413
std_targets: 0.5001149773597717
prcauc: 0.9213357157916618
rocauc: 0.9381842066851555
ogbg-molbace: 0.9381842066851555
BCEWithLogitsLoss: 0.31780872998699067
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 87; Iter     4/   31] train: loss: 0.0505183
[Epoch 87] ogbg-molbace: 0.746872 val loss: 0.476384
[Epoch 87] ogbg-molbace: 0.796363 test loss: 1.126159
[Epoch 88; Iter     3/   31] train: loss: 0.2643523
[Epoch 88] ogbg-molbace: 0.719861 val loss: 0.582151
[Epoch 88] ogbg-molbace: 0.802236 test loss: 1.001358
[Epoch 89; Iter     2/   31] train: loss: 0.2415144
[Epoch 89] ogbg-molbace: 0.744985 val loss: 0.644302
[Epoch 89] ogbg-molbace: 0.788121 test loss: 1.094119
[Epoch 90; Iter     1/   31] train: loss: 0.0462023
[Epoch 90; Iter    31/   31] train: loss: 0.0840060
[Epoch 90] ogbg-molbace: 0.710427 val loss: 0.517500
[Epoch 90] ogbg-molbace: 0.795436 test loss: 1.063105
[Epoch 91; Iter    30/   31] train: loss: 0.1063877
[Epoch 91] ogbg-molbace: 0.715392 val loss: 0.714930
[Epoch 91] ogbg-molbace: 0.797084 test loss: 1.062100
[Epoch 92; Iter    29/   31] train: loss: 0.1939645
[Epoch 92] ogbg-molbace: 0.732771 val loss: 0.602405
[Epoch 92] ogbg-molbace: 0.786575 test loss: 1.244212
[Epoch 93; Iter    28/   31] train: loss: 0.2433296
[Epoch 93] ogbg-molbace: 0.728004 val loss: 0.734410
[Epoch 93] ogbg-molbace: 0.764476 test loss: 1.155007
[Epoch 94; Iter    27/   31] train: loss: 0.1537122
[Epoch 94] ogbg-molbace: 0.757498 val loss: 0.986519
[Epoch 94] ogbg-molbace: 0.790593 test loss: 1.162168
[Epoch 95; Iter    26/   31] train: loss: 0.1686758
[Epoch 95] ogbg-molbace: 0.725422 val loss: 0.630713
[Epoch 95] ogbg-molbace: 0.797445 test loss: 1.170939
[Epoch 96; Iter    25/   31] train: loss: 0.1631851
[Epoch 96] ogbg-molbace: 0.765144 val loss: 0.458638
[Epoch 96] ogbg-molbace: 0.778333 test loss: 1.329186
[Epoch 97; Iter    24/   31] train: loss: 0.0428719
[Epoch 97] ogbg-molbace: 0.728997 val loss: 1.143057
[Epoch 97] ogbg-molbace: 0.794560 test loss: 1.181843
[Epoch 98; Iter    23/   31] train: loss: 0.1225897
[Epoch 98] ogbg-molbace: 0.728401 val loss: 0.780061
[Epoch 98] ogbg-molbace: 0.795127 test loss: 1.218494
[Epoch 99; Iter    22/   31] train: loss: 0.1130950
[Epoch 99] ogbg-molbace: 0.754916 val loss: 0.909128
[Epoch 99] ogbg-molbace: 0.772357 test loss: 1.229310
[Epoch 100; Iter    21/   31] train: loss: 0.0665856
[Epoch 100] ogbg-molbace: 0.742105 val loss: 0.637041
[Epoch 100] ogbg-molbace: 0.788533 test loss: 1.264658
[Epoch 101; Iter    20/   31] train: loss: 0.0324006
[Epoch 101] ogbg-molbace: 0.727706 val loss: 0.683231
[Epoch 101] ogbg-molbace: 0.792809 test loss: 1.209597
[Epoch 102; Iter    19/   31] train: loss: 0.0938952
[Epoch 102] ogbg-molbace: 0.751341 val loss: 0.733202
[Epoch 102] ogbg-molbace: 0.793066 test loss: 1.252458
[Epoch 103; Iter    18/   31] train: loss: 0.0963022
[Epoch 103] ogbg-molbace: 0.722642 val loss: 0.697523
[Epoch 103] ogbg-molbace: 0.791109 test loss: 1.405447
[Epoch 104; Iter    17/   31] train: loss: 0.4495139
[Epoch 104] ogbg-molbace: 0.709235 val loss: 0.769320
[Epoch 104] ogbg-molbace: 0.797909 test loss: 1.165851
[Epoch 105; Iter    16/   31] train: loss: 0.0515350
[Epoch 105] ogbg-molbace: 0.720655 val loss: 0.745813
[Epoch 105] ogbg-molbace: 0.797239 test loss: 1.226362
[Epoch 106; Iter    15/   31] train: loss: 0.2319741
[Epoch 106] ogbg-molbace: 0.731281 val loss: 0.750690
[Epoch 106] ogbg-molbace: 0.792654 test loss: 1.213093
[Epoch 107; Iter    14/   31] train: loss: 0.1130467
[Epoch 107] ogbg-molbace: 0.733764 val loss: 0.697283
[Epoch 107] ogbg-molbace: 0.798939 test loss: 1.062803
[Epoch 108; Iter    13/   31] train: loss: 0.0233309
[Epoch 108] ogbg-molbace: 0.746177 val loss: 0.595678
[Epoch 108] ogbg-molbace: 0.782248 test loss: 1.266212
[Epoch 109; Iter    12/   31] train: loss: 0.0206204
[Epoch 109] ogbg-molbace: 0.726415 val loss: 0.680853
[Epoch 109] ogbg-molbace: 0.785700 test loss: 1.183353
[Epoch 110; Iter    11/   31] train: loss: 0.2133385
[Epoch 110] ogbg-molbace: 0.766733 val loss: 1.003940
[Epoch 110] ogbg-molbace: 0.794199 test loss: 1.164288
[Epoch 111; Iter    10/   31] train: loss: 0.0392664
[Epoch 111] ogbg-molbace: 0.754121 val loss: 0.485725
[Epoch 111] ogbg-molbace: 0.782557 test loss: 1.309284
[Epoch 112; Iter     9/   31] train: loss: 0.0856862
[Epoch 112] ogbg-molbace: 0.727805 val loss: 0.691480
[Epoch 112] ogbg-molbace: 0.781063 test loss: 1.204301
[Epoch 113; Iter     8/   31] train: loss: 0.0339502
[Epoch 113] ogbg-molbace: 0.724628 val loss: 0.774036
[Epoch 113] ogbg-molbace: 0.776015 test loss: 1.215065
[Epoch 114; Iter     7/   31] train: loss: 0.0477715
[Epoch 114] ogbg-molbace: 0.725819 val loss: 0.703190
[Epoch 114] ogbg-molbace: 0.774675 test loss: 1.306687
[Epoch 115; Iter     6/   31] train: loss: 0.0289090
[Epoch 115] ogbg-molbace: 0.741311 val loss: 0.779611
[Epoch 115] ogbg-molbace: 0.777921 test loss: 1.222756
[Epoch 116; Iter     5/   31] train: loss: 0.0100541
[Epoch 116] ogbg-molbace: 0.742900 val loss: 0.804065
[Epoch 116] ogbg-molbace: 0.776118 test loss: 1.239452
[Epoch 117; Iter     4/   31] train: loss: 0.0107319
[Epoch 117] ogbg-molbace: 0.738630 val loss: 0.743588
[Epoch 117] ogbg-molbace: 0.778281 test loss: 1.285064
[Epoch 118; Iter     3/   31] train: loss: 0.0164830
[Epoch 118] ogbg-molbace: 0.738729 val loss: 0.906641
[Epoch 118] ogbg-molbace: 0.783381 test loss: 1.205824
[Epoch 119; Iter     2/   31] train: loss: 0.0092577
[Epoch 119] ogbg-molbace: 0.746375 val loss: 0.828869
[Epoch 119] ogbg-molbace: 0.765145 test loss: 1.350543
[Epoch 120; Iter     1/   31] train: loss: 0.0137213
[Epoch 120; Iter    31/   31] train: loss: 0.0115788
[Epoch 120] ogbg-molbace: 0.774280 val loss: 0.822628
[Epoch 120] ogbg-molbace: 0.765042 test loss: 1.407324
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 54.
Statistics on  val_best_checkpoint
mean_pred: -1.1373037099838257
std_pred: 1.81177818775177
mean_targets: 0.12541253864765167
std_targets: 0.33173397183418274
prcauc: 0.4707254378180983
rocauc: 0.8262164846077457
ogbg-molbace: 0.8262164846077457
BCEWithLogitsLoss: 0.41421949016776954
Statistics on  test
mean_pred: 0.22200565040111542
std_pred: 2.24078631401062
mean_targets: 0.6963696479797363
std_targets: 0.4605855941772461
prcauc: 0.8691005840117518
rocauc: 0.7965691324953638
ogbg-molbace: 0.7965691324953638
BCEWithLogitsLoss: 0.6623123735189438
Statistics on  train
mean_pred: -0.0512552410364151
std_pred: 2.3209903240203857
mean_targets: 0.48732084035873413
std_targets: 0.5001149773597717
prcauc: 0.9404061074617804
rocauc: 0.9491363791174039
ogbg-molbace: 0.9491363791174039
BCEWithLogitsLoss: 0.3039730566163217
[Epoch 87; Iter     4/   31] train: loss: 0.3378101
[Epoch 87] ogbg-molbace: 0.719662 val loss: 0.526238
[Epoch 87] ogbg-molbace: 0.811251 test loss: 0.743964
[Epoch 88; Iter     3/   31] train: loss: 0.1905102
[Epoch 88] ogbg-molbace: 0.704568 val loss: 0.439677
[Epoch 88] ogbg-molbace: 0.800742 test loss: 1.296191
[Epoch 89; Iter     2/   31] train: loss: 0.1277300
[Epoch 89] ogbg-molbace: 0.706554 val loss: 0.624961
[Epoch 89] ogbg-molbace: 0.818669 test loss: 0.916864
[Epoch 90; Iter     1/   31] train: loss: 0.1988358
[Epoch 90; Iter    31/   31] train: loss: 0.8055070
[Epoch 90] ogbg-molbace: 0.735154 val loss: 0.800209
[Epoch 90] ogbg-molbace: 0.829693 test loss: 0.822241
[Epoch 91; Iter    30/   31] train: loss: 0.2632726
[Epoch 91] ogbg-molbace: 0.750149 val loss: 0.562445
[Epoch 91] ogbg-molbace: 0.800999 test loss: 0.989733
[Epoch 92; Iter    29/   31] train: loss: 0.0958658
[Epoch 92] ogbg-molbace: 0.732969 val loss: 0.474157
[Epoch 92] ogbg-molbace: 0.806872 test loss: 0.953765
[Epoch 93; Iter    28/   31] train: loss: 0.1561357
[Epoch 93] ogbg-molbace: 0.734161 val loss: 0.492174
[Epoch 93] ogbg-molbace: 0.810530 test loss: 0.968825
[Epoch 94; Iter    27/   31] train: loss: 0.1970828
[Epoch 94] ogbg-molbace: 0.729593 val loss: 0.591429
[Epoch 94] ogbg-molbace: 0.815681 test loss: 0.896968
[Epoch 95; Iter    26/   31] train: loss: 0.0764766
[Epoch 95] ogbg-molbace: 0.726216 val loss: 0.590717
[Epoch 95] ogbg-molbace: 0.811457 test loss: 0.996895
[Epoch 96; Iter    25/   31] train: loss: 0.0549918
[Epoch 96] ogbg-molbace: 0.739523 val loss: 0.689162
[Epoch 96] ogbg-molbace: 0.804348 test loss: 0.993885
[Epoch 97; Iter    24/   31] train: loss: 0.0899440
[Epoch 97] ogbg-molbace: 0.730983 val loss: 0.639917
[Epoch 97] ogbg-molbace: 0.816454 test loss: 1.029534
[Epoch 98; Iter    23/   31] train: loss: 0.0340435
[Epoch 98] ogbg-molbace: 0.728103 val loss: 0.553257
[Epoch 98] ogbg-molbace: 0.818308 test loss: 1.061046
[Epoch 99; Iter    22/   31] train: loss: 0.1279935
[Epoch 99] ogbg-molbace: 0.713803 val loss: 0.662880
[Epoch 99] ogbg-molbace: 0.827169 test loss: 0.921547
[Epoch 100; Iter    21/   31] train: loss: 0.0979065
[Epoch 100] ogbg-molbace: 0.751936 val loss: 0.615620
[Epoch 100] ogbg-molbace: 0.809911 test loss: 0.989543
[Epoch 101; Iter    20/   31] train: loss: 0.1012484
[Epoch 101] ogbg-molbace: 0.715194 val loss: 0.624925
[Epoch 101] ogbg-molbace: 0.805739 test loss: 1.074251
[Epoch 102; Iter    19/   31] train: loss: 0.0940651
[Epoch 102] ogbg-molbace: 0.754717 val loss: 0.893371
[Epoch 102] ogbg-molbace: 0.825366 test loss: 1.018127
[Epoch 103; Iter    18/   31] train: loss: 0.0531518
[Epoch 103] ogbg-molbace: 0.736346 val loss: 0.614592
[Epoch 103] ogbg-molbace: 0.822635 test loss: 1.110469
[Epoch 104; Iter    17/   31] train: loss: 0.0628780
[Epoch 104] ogbg-molbace: 0.727210 val loss: 0.820401
[Epoch 104] ogbg-molbace: 0.813930 test loss: 1.159608
[Epoch 105; Iter    16/   31] train: loss: 0.2047363
[Epoch 105] ogbg-molbace: 0.710824 val loss: 0.647894
[Epoch 105] ogbg-molbace: 0.823511 test loss: 1.154740
[Epoch 106; Iter    15/   31] train: loss: 0.0527126
[Epoch 106] ogbg-molbace: 0.719762 val loss: 0.889061
[Epoch 106] ogbg-molbace: 0.828251 test loss: 1.146644
[Epoch 107; Iter    14/   31] train: loss: 0.1022856
[Epoch 107] ogbg-molbace: 0.723833 val loss: 0.929620
[Epoch 107] ogbg-molbace: 0.815114 test loss: 1.083779
[Epoch 108; Iter    13/   31] train: loss: 0.0464046
[Epoch 108] ogbg-molbace: 0.696326 val loss: 0.705385
[Epoch 108] ogbg-molbace: 0.805069 test loss: 1.218987
[Epoch 109; Iter    12/   31] train: loss: 0.0603019
[Epoch 109] ogbg-molbace: 0.707150 val loss: 0.695422
[Epoch 109] ogbg-molbace: 0.817175 test loss: 1.018752
[Epoch 110; Iter    11/   31] train: loss: 0.1045422
[Epoch 110] ogbg-molbace: 0.715690 val loss: 0.815563
[Epoch 110] ogbg-molbace: 0.832372 test loss: 1.016893
[Epoch 111; Iter    10/   31] train: loss: 0.0378958
[Epoch 111] ogbg-molbace: 0.699702 val loss: 0.795372
[Epoch 111] ogbg-molbace: 0.810220 test loss: 1.085527
[Epoch 112; Iter     9/   31] train: loss: 0.0423526
[Epoch 112] ogbg-molbace: 0.749553 val loss: 0.753968
[Epoch 112] ogbg-molbace: 0.822223 test loss: 1.021516
[Epoch 113; Iter     8/   31] train: loss: 0.0320080
[Epoch 113] ogbg-molbace: 0.725422 val loss: 1.033640
[Epoch 113] ogbg-molbace: 0.832835 test loss: 1.101263
[Epoch 114; Iter     7/   31] train: loss: 0.0566980
[Epoch 114] ogbg-molbace: 0.700695 val loss: 0.786871
[Epoch 114] ogbg-molbace: 0.813878 test loss: 1.091433
[Epoch 115; Iter     6/   31] train: loss: 0.0452231
[Epoch 115] ogbg-molbace: 0.709831 val loss: 0.590471
[Epoch 115] ogbg-molbace: 0.822069 test loss: 1.265330
[Epoch 116; Iter     5/   31] train: loss: 0.0454159
[Epoch 116] ogbg-molbace: 0.714399 val loss: 0.819428
[Epoch 116] ogbg-molbace: 0.811714 test loss: 1.240190
[Epoch 117; Iter     4/   31] train: loss: 0.0493287
[Epoch 117] ogbg-molbace: 0.752929 val loss: 0.615386
[Epoch 117] ogbg-molbace: 0.812745 test loss: 0.947326
[Epoch 118; Iter     3/   31] train: loss: 0.1013061
[Epoch 118] ogbg-molbace: 0.732870 val loss: 0.707868
[Epoch 118] ogbg-molbace: 0.830260 test loss: 1.043255
[Epoch 119; Iter     2/   31] train: loss: 0.0317096
[Epoch 119] ogbg-molbace: 0.730785 val loss: 0.675921
[Epoch 119] ogbg-molbace: 0.827787 test loss: 1.034619
[Epoch 120; Iter     1/   31] train: loss: 0.0185346
[Epoch 120; Iter    31/   31] train: loss: 0.0210060
[Epoch 120] ogbg-molbace: 0.729791 val loss: 0.746420
[Epoch 120] ogbg-molbace: 0.825108 test loss: 1.065468
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 49.
Statistics on  val_best_checkpoint
mean_pred: -0.24140699207782745
std_pred: 2.044818639755249
mean_targets: 0.12541253864765167
std_targets: 0.33173397183418274
prcauc: 0.4229999799670972
rocauc: 0.8159880834160874
ogbg-molbace: 0.8159880834160874
BCEWithLogitsLoss: 0.6802959487858143
Statistics on  test
mean_pred: 0.9597009420394897
std_pred: 2.2999672889709473
mean_targets: 0.6963696479797363
std_targets: 0.4605855941772461
prcauc: 0.8916299409855887
rocauc: 0.8090871625798475
ogbg-molbace: 0.8090871625798475
BCEWithLogitsLoss: 0.5873719602823257
Statistics on  train
mean_pred: 0.5959250926971436
std_pred: 2.2929792404174805
mean_targets: 0.48732084035873413
std_targets: 0.5001149773597717
prcauc: 0.9014007578034557
rocauc: 0.9237045686761057
ogbg-molbace: 0.9237045686761057
BCEWithLogitsLoss: 0.4020228361891162
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.689744 val loss: 1.581657
[Epoch 78] ogbg-molbace: 0.799339 test loss: 1.649514
[Epoch 79; Iter    12/   41] train: loss: 0.2298115
[Epoch 79] ogbg-molbace: 0.754945 val loss: 1.281910
[Epoch 79] ogbg-molbace: 0.789428 test loss: 1.679822
[Epoch 80; Iter     1/   41] train: loss: 0.3089251
[Epoch 80; Iter    31/   41] train: loss: 0.0955339
[Epoch 80] ogbg-molbace: 0.747619 val loss: 1.548649
[Epoch 80] ogbg-molbace: 0.790645 test loss: 1.563951
[Epoch 81; Iter    20/   41] train: loss: 0.0993991
[Epoch 81] ogbg-molbace: 0.756410 val loss: 1.063693
[Epoch 81] ogbg-molbace: 0.790123 test loss: 1.454416
[Epoch 82; Iter     9/   41] train: loss: 0.0937556
[Epoch 82; Iter    39/   41] train: loss: 0.1715696
[Epoch 82] ogbg-molbace: 0.740293 val loss: 1.491860
[Epoch 82] ogbg-molbace: 0.782125 test loss: 1.740168
[Epoch 83; Iter    28/   41] train: loss: 0.3427618
[Epoch 83] ogbg-molbace: 0.764835 val loss: 1.227967
[Epoch 83] ogbg-molbace: 0.796383 test loss: 1.696179
[Epoch 84; Iter    17/   41] train: loss: 0.0774818
[Epoch 84] ogbg-molbace: 0.771062 val loss: 1.279745
[Epoch 84] ogbg-molbace: 0.770649 test loss: 1.770895
[Epoch 85; Iter     6/   41] train: loss: 0.2806364
[Epoch 85; Iter    36/   41] train: loss: 0.1612932
[Epoch 85] ogbg-molbace: 0.721612 val loss: 1.330973
[Epoch 85] ogbg-molbace: 0.780560 test loss: 1.424422
[Epoch 86; Iter    25/   41] train: loss: 0.0485655
[Epoch 86] ogbg-molbace: 0.734799 val loss: 1.357224
[Epoch 86] ogbg-molbace: 0.813945 test loss: 1.647407
[Epoch 87; Iter    14/   41] train: loss: 0.0803255
[Epoch 87] ogbg-molbace: 0.736996 val loss: 1.458672
[Epoch 87] ogbg-molbace: 0.807338 test loss: 1.661911
[Epoch 88; Iter     3/   41] train: loss: 0.1522116
[Epoch 88; Iter    33/   41] train: loss: 0.0917358
[Epoch 88] ogbg-molbace: 0.709524 val loss: 1.501613
[Epoch 88] ogbg-molbace: 0.793775 test loss: 1.867737
[Epoch 89; Iter    22/   41] train: loss: 0.1191871
[Epoch 89] ogbg-molbace: 0.756410 val loss: 1.508170
[Epoch 89] ogbg-molbace: 0.783168 test loss: 1.735969
[Epoch 90; Iter    11/   41] train: loss: 0.0648132
[Epoch 90; Iter    41/   41] train: loss: 0.2099299
[Epoch 90] ogbg-molbace: 0.747985 val loss: 1.434776
[Epoch 90] ogbg-molbace: 0.765606 test loss: 1.935355
[Epoch 91; Iter    30/   41] train: loss: 0.0114235
[Epoch 91] ogbg-molbace: 0.728938 val loss: 1.611405
[Epoch 91] ogbg-molbace: 0.770301 test loss: 1.972173
[Epoch 92; Iter    19/   41] train: loss: 0.1354939
[Epoch 92] ogbg-molbace: 0.727473 val loss: 1.303184
[Epoch 92] ogbg-molbace: 0.782125 test loss: 1.660543
[Epoch 93; Iter     8/   41] train: loss: 0.1327517
[Epoch 93; Iter    38/   41] train: loss: 0.1207277
[Epoch 93] ogbg-molbace: 0.735165 val loss: 1.552937
[Epoch 93] ogbg-molbace: 0.782820 test loss: 2.325123
[Epoch 94; Iter    27/   41] train: loss: 0.1487816
[Epoch 94] ogbg-molbace: 0.744322 val loss: 1.578678
[Epoch 94] ogbg-molbace: 0.784038 test loss: 1.763909
[Epoch 95; Iter    16/   41] train: loss: 0.0756934
[Epoch 95] ogbg-molbace: 0.721612 val loss: 1.367456
[Epoch 95] ogbg-molbace: 0.795514 test loss: 2.071979
[Epoch 96; Iter     5/   41] train: loss: 0.2134907
[Epoch 96; Iter    35/   41] train: loss: 0.0467332
[Epoch 96] ogbg-molbace: 0.776557 val loss: 1.041875
[Epoch 96] ogbg-molbace: 0.771518 test loss: 1.603532
[Epoch 97; Iter    24/   41] train: loss: 0.1893810
[Epoch 97] ogbg-molbace: 0.765201 val loss: 1.690382
[Epoch 97] ogbg-molbace: 0.802643 test loss: 1.458915
[Epoch 98; Iter    13/   41] train: loss: 0.0383141
[Epoch 98] ogbg-molbace: 0.739927 val loss: 1.580210
[Epoch 98] ogbg-molbace: 0.782994 test loss: 2.093466
[Epoch 99; Iter     2/   41] train: loss: 0.0087845
[Epoch 99; Iter    32/   41] train: loss: 0.0407119
[Epoch 99] ogbg-molbace: 0.723443 val loss: 1.536442
[Epoch 99] ogbg-molbace: 0.776039 test loss: 2.224727
[Epoch 100; Iter    21/   41] train: loss: 0.0719668
[Epoch 100] ogbg-molbace: 0.736630 val loss: 1.673628
[Epoch 100] ogbg-molbace: 0.772561 test loss: 2.059714
[Epoch 101; Iter    10/   41] train: loss: 0.0278274
[Epoch 101; Iter    40/   41] train: loss: 0.0576348
[Epoch 101] ogbg-molbace: 0.745421 val loss: 1.450862
[Epoch 101] ogbg-molbace: 0.771518 test loss: 2.087665
[Epoch 102; Iter    29/   41] train: loss: 0.0213501
[Epoch 102] ogbg-molbace: 0.734799 val loss: 1.826720
[Epoch 102] ogbg-molbace: 0.767866 test loss: 2.071949
[Epoch 103; Iter    18/   41] train: loss: 0.0182019
[Epoch 103] ogbg-molbace: 0.713187 val loss: 2.001509
[Epoch 103] ogbg-molbace: 0.770996 test loss: 2.523855
[Epoch 104; Iter     7/   41] train: loss: 0.0409792
[Epoch 104; Iter    37/   41] train: loss: 0.0334111
[Epoch 104] ogbg-molbace: 0.759341 val loss: 1.358745
[Epoch 104] ogbg-molbace: 0.767693 test loss: 2.231396
[Epoch 105; Iter    26/   41] train: loss: 0.0188714
[Epoch 105] ogbg-molbace: 0.760073 val loss: 1.603169
[Epoch 105] ogbg-molbace: 0.760216 test loss: 2.538006
[Epoch 106; Iter    15/   41] train: loss: 0.2186791
[Epoch 106] ogbg-molbace: 0.723443 val loss: 1.913844
[Epoch 106] ogbg-molbace: 0.774996 test loss: 2.303982
[Epoch 107; Iter     4/   41] train: loss: 0.0590145
[Epoch 107; Iter    34/   41] train: loss: 0.0221068
[Epoch 107] ogbg-molbace: 0.732234 val loss: 1.786740
[Epoch 107] ogbg-molbace: 0.772909 test loss: 2.385272
[Epoch 108; Iter    23/   41] train: loss: 0.0747040
[Epoch 108] ogbg-molbace: 0.733700 val loss: 1.807878
[Epoch 108] ogbg-molbace: 0.775691 test loss: 2.282963
[Epoch 109; Iter    12/   41] train: loss: 0.0486397
[Epoch 109] ogbg-molbace: 0.722711 val loss: 1.719134
[Epoch 109] ogbg-molbace: 0.770649 test loss: 2.085851
[Epoch 110; Iter     1/   41] train: loss: 0.0256088
[Epoch 110; Iter    31/   41] train: loss: 0.0352244
[Epoch 110] ogbg-molbace: 0.715385 val loss: 2.040853
[Epoch 110] ogbg-molbace: 0.785081 test loss: 2.200899
[Epoch 111; Iter    20/   41] train: loss: 0.0238130
[Epoch 111] ogbg-molbace: 0.733700 val loss: 1.702300
[Epoch 111] ogbg-molbace: 0.775691 test loss: 2.700422
[Epoch 112; Iter     9/   41] train: loss: 0.0139013
[Epoch 112; Iter    39/   41] train: loss: 0.0303888
[Epoch 112] ogbg-molbace: 0.697436 val loss: 2.209105
[Epoch 112] ogbg-molbace: 0.747001 test loss: 2.523818
[Epoch 113; Iter    28/   41] train: loss: 0.0068351
[Epoch 113] ogbg-molbace: 0.720879 val loss: 2.082072
[Epoch 113] ogbg-molbace: 0.767519 test loss: 2.424954
[Epoch 114; Iter    17/   41] train: loss: 0.1072370
[Epoch 114] ogbg-molbace: 0.712454 val loss: 2.365364
[Epoch 114] ogbg-molbace: 0.764563 test loss: 2.438608
[Epoch 115; Iter     6/   41] train: loss: 0.0482884
[Epoch 115; Iter    36/   41] train: loss: 0.0530067
[Epoch 115] ogbg-molbace: 0.722344 val loss: 2.158429
[Epoch 115] ogbg-molbace: 0.758303 test loss: 2.413643
[Epoch 116; Iter    25/   41] train: loss: 0.0486713
[Epoch 116] ogbg-molbace: 0.718315 val loss: 1.807362
[Epoch 116] ogbg-molbace: 0.770301 test loss: 2.349764
[Epoch 117; Iter    14/   41] train: loss: 0.0286985
[Epoch 117] ogbg-molbace: 0.709158 val loss: 2.638823
[Epoch 117] ogbg-molbace: 0.760737 test loss: 2.496310
[Epoch 118; Iter     3/   41] train: loss: 0.0119973
[Epoch 118; Iter    33/   41] train: loss: 0.0400607
[Epoch 118] ogbg-molbace: 0.727839 val loss: 2.183184
[Epoch 118] ogbg-molbace: 0.757955 test loss: 2.236189
[Epoch 119; Iter    22/   41] train: loss: 0.1644646
[Epoch 119] ogbg-molbace: 0.706960 val loss: 2.434640
[Epoch 119] ogbg-molbace: 0.761607 test loss: 2.501258
[Epoch 120; Iter    11/   41] train: loss: 0.0606194
[Epoch 120; Iter    41/   41] train: loss: 0.0261813
[Epoch 120] ogbg-molbace: 0.698901 val loss: 2.338000
[Epoch 120] ogbg-molbace: 0.746653 test loss: 2.307286
Early stopping criterion based on -ogbg-molbace- that should be max reached after 120 epochs. Best model checkpoint was in epoch 53.
Statistics on  val_best_checkpoint
mean_pred: 0.13835254311561584
std_pred: 2.1907958984375
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9535363011607559
rocauc: 0.7901098901098901
ogbg-molbace: 0.7901098901098901
BCEWithLogitsLoss: 1.1779974748690922
Statistics on  test
mean_pred: -1.7703815698623657
std_pred: 3.354417324066162
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7567710286327205
rocauc: 0.7906451051990958
ogbg-molbace: 0.7906451051990958
BCEWithLogitsLoss: 1.1410112157464027
Statistics on  train
mean_pred: -1.1564477682113647
std_pred: 3.224738836288452
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9420859495120202
rocauc: 0.9614982876712329
ogbg-molbace: 0.9614982876712329
BCEWithLogitsLoss: 0.2467620345513995
[Epoch 78] ogbg-molbace: 0.768864 val loss: 1.303476
[Epoch 78] ogbg-molbace: 0.802469 test loss: 1.293550
[Epoch 79; Iter    12/   41] train: loss: 0.3068069
[Epoch 79] ogbg-molbace: 0.659341 val loss: 1.524201
[Epoch 79] ogbg-molbace: 0.790123 test loss: 1.292804
[Epoch 80; Iter     1/   41] train: loss: 0.1007878
[Epoch 80; Iter    31/   41] train: loss: 0.2056185
[Epoch 80] ogbg-molbace: 0.693040 val loss: 1.353844
[Epoch 80] ogbg-molbace: 0.758303 test loss: 1.567832
[Epoch 81; Iter    20/   41] train: loss: 0.1122316
[Epoch 81] ogbg-molbace: 0.730403 val loss: 1.094278
[Epoch 81] ogbg-molbace: 0.802295 test loss: 1.130297
[Epoch 82; Iter     9/   41] train: loss: 0.2544503
[Epoch 82; Iter    39/   41] train: loss: 0.1698514
[Epoch 82] ogbg-molbace: 0.742491 val loss: 1.190785
[Epoch 82] ogbg-molbace: 0.814989 test loss: 1.501656
[Epoch 83; Iter    28/   41] train: loss: 0.1579283
[Epoch 83] ogbg-molbace: 0.790476 val loss: 1.000845
[Epoch 83] ogbg-molbace: 0.777952 test loss: 1.234709
[Epoch 84; Iter    17/   41] train: loss: 0.1036754
[Epoch 84] ogbg-molbace: 0.724908 val loss: 0.979987
[Epoch 84] ogbg-molbace: 0.786646 test loss: 1.203413
[Epoch 85; Iter     6/   41] train: loss: 0.1896342
[Epoch 85; Iter    36/   41] train: loss: 0.1734254
[Epoch 85] ogbg-molbace: 0.716484 val loss: 1.195086
[Epoch 85] ogbg-molbace: 0.802469 test loss: 1.308424
[Epoch 86; Iter    25/   41] train: loss: 0.1365872
[Epoch 86] ogbg-molbace: 0.757143 val loss: 1.389410
[Epoch 86] ogbg-molbace: 0.822466 test loss: 1.301654
[Epoch 87; Iter    14/   41] train: loss: 0.1565847
[Epoch 87] ogbg-molbace: 0.777656 val loss: 1.165114
[Epoch 87] ogbg-molbace: 0.798296 test loss: 1.601433
[Epoch 88; Iter     3/   41] train: loss: 0.3906658
[Epoch 88; Iter    33/   41] train: loss: 0.2426504
[Epoch 88] ogbg-molbace: 0.773993 val loss: 1.131514
[Epoch 88] ogbg-molbace: 0.815858 test loss: 1.295986
[Epoch 89; Iter    22/   41] train: loss: 0.1545976
[Epoch 89] ogbg-molbace: 0.772161 val loss: 1.092102
[Epoch 89] ogbg-molbace: 0.817423 test loss: 1.103056
[Epoch 90; Iter    11/   41] train: loss: 0.2693748
[Epoch 90; Iter    41/   41] train: loss: 0.4998174
[Epoch 90] ogbg-molbace: 0.732601 val loss: 1.370445
[Epoch 90] ogbg-molbace: 0.786472 test loss: 1.242304
[Epoch 91; Iter    30/   41] train: loss: 0.1541754
[Epoch 91] ogbg-molbace: 0.728205 val loss: 1.145077
[Epoch 91] ogbg-molbace: 0.823857 test loss: 0.972258
[Epoch 92; Iter    19/   41] train: loss: 0.0857361
[Epoch 92] ogbg-molbace: 0.708425 val loss: 1.741912
[Epoch 92] ogbg-molbace: 0.798470 test loss: 1.510744
[Epoch 93; Iter     8/   41] train: loss: 0.0766452
[Epoch 93; Iter    38/   41] train: loss: 0.2270294
[Epoch 93] ogbg-molbace: 0.748352 val loss: 1.742335
[Epoch 93] ogbg-molbace: 0.788732 test loss: 1.772593
[Epoch 94; Iter    27/   41] train: loss: 0.0371498
[Epoch 94] ogbg-molbace: 0.734066 val loss: 1.745339
[Epoch 94] ogbg-molbace: 0.752391 test loss: 2.166459
[Epoch 95; Iter    16/   41] train: loss: 0.2513996
[Epoch 95] ogbg-molbace: 0.761172 val loss: 1.678200
[Epoch 95] ogbg-molbace: 0.770127 test loss: 1.442629
[Epoch 96; Iter     5/   41] train: loss: 0.1779168
[Epoch 96; Iter    35/   41] train: loss: 0.3229096
[Epoch 96] ogbg-molbace: 0.790842 val loss: 1.383072
[Epoch 96] ogbg-molbace: 0.788732 test loss: 1.617797
[Epoch 97; Iter    24/   41] train: loss: 0.2380256
[Epoch 97] ogbg-molbace: 0.752747 val loss: 1.628272
[Epoch 97] ogbg-molbace: 0.792906 test loss: 1.117642
[Epoch 98; Iter    13/   41] train: loss: 0.0577287
[Epoch 98] ogbg-molbace: 0.742857 val loss: 1.498348
[Epoch 98] ogbg-molbace: 0.819857 test loss: 1.843892
[Epoch 99; Iter     2/   41] train: loss: 0.0549262
[Epoch 99; Iter    32/   41] train: loss: 0.1217922
[Epoch 99] ogbg-molbace: 0.755311 val loss: 1.396112
[Epoch 99] ogbg-molbace: 0.812728 test loss: 1.939183
[Epoch 100; Iter    21/   41] train: loss: 0.2232973
[Epoch 100] ogbg-molbace: 0.733700 val loss: 1.383686
[Epoch 100] ogbg-molbace: 0.805773 test loss: 1.624942
[Epoch 101; Iter    10/   41] train: loss: 0.1301196
[Epoch 101; Iter    40/   41] train: loss: 0.0551688
[Epoch 101] ogbg-molbace: 0.768864 val loss: 1.476028
[Epoch 101] ogbg-molbace: 0.795688 test loss: 1.589837
[Epoch 102; Iter    29/   41] train: loss: 0.1440809
[Epoch 102] ogbg-molbace: 0.727473 val loss: 1.973021
[Epoch 102] ogbg-molbace: 0.788732 test loss: 1.848622
[Epoch 103; Iter    18/   41] train: loss: 0.0856367
[Epoch 103] ogbg-molbace: 0.772527 val loss: 1.401106
[Epoch 103] ogbg-molbace: 0.815163 test loss: 1.556406
[Epoch 104; Iter     7/   41] train: loss: 0.1028829
[Epoch 104; Iter    37/   41] train: loss: 0.0683338
[Epoch 104] ogbg-molbace: 0.778755 val loss: 1.251092
[Epoch 104] ogbg-molbace: 0.792558 test loss: 1.446812
[Epoch 105; Iter    26/   41] train: loss: 0.2065382
[Epoch 105] ogbg-molbace: 0.715751 val loss: 1.872696
[Epoch 105] ogbg-molbace: 0.775343 test loss: 1.921737
[Epoch 106; Iter    15/   41] train: loss: 0.0778524
[Epoch 106] ogbg-molbace: 0.711722 val loss: 1.937982
[Epoch 106] ogbg-molbace: 0.790123 test loss: 1.691082
[Epoch 107; Iter     4/   41] train: loss: 0.0742792
[Epoch 107; Iter    34/   41] train: loss: 0.1468927
[Epoch 107] ogbg-molbace: 0.751648 val loss: 1.716124
[Epoch 107] ogbg-molbace: 0.793253 test loss: 1.609776
[Epoch 108; Iter    23/   41] train: loss: 0.1929346
[Epoch 108] ogbg-molbace: 0.757143 val loss: 1.668366
[Epoch 108] ogbg-molbace: 0.792210 test loss: 1.887591
[Epoch 109; Iter    12/   41] train: loss: 0.0266605
[Epoch 109] ogbg-molbace: 0.734799 val loss: 1.876939
[Epoch 109] ogbg-molbace: 0.797253 test loss: 1.839095
[Epoch 110; Iter     1/   41] train: loss: 0.0432884
[Epoch 110; Iter    31/   41] train: loss: 0.0555287
[Epoch 110] ogbg-molbace: 0.726374 val loss: 1.657850
[Epoch 110] ogbg-molbace: 0.809077 test loss: 1.802778
[Epoch 111; Iter    20/   41] train: loss: 0.0303614
[Epoch 111] ogbg-molbace: 0.761172 val loss: 1.611287
[Epoch 111] ogbg-molbace: 0.781951 test loss: 1.907969
[Epoch 112; Iter     9/   41] train: loss: 0.1719108
[Epoch 112; Iter    39/   41] train: loss: 0.0649861
[Epoch 112] ogbg-molbace: 0.737729 val loss: 1.691455
[Epoch 112] ogbg-molbace: 0.776561 test loss: 1.779912
[Epoch 113; Iter    28/   41] train: loss: 0.0229757
[Epoch 113] ogbg-molbace: 0.761905 val loss: 1.600103
[Epoch 113] ogbg-molbace: 0.792558 test loss: 1.708178
[Epoch 114; Iter    17/   41] train: loss: 0.0849926
[Epoch 114] ogbg-molbace: 0.743956 val loss: 1.892748
[Epoch 114] ogbg-molbace: 0.802295 test loss: 1.958144
[Epoch 115; Iter     6/   41] train: loss: 0.0153130
[Epoch 115; Iter    36/   41] train: loss: 0.0348966
[Epoch 115] ogbg-molbace: 0.750916 val loss: 1.798449
[Epoch 115] ogbg-molbace: 0.798644 test loss: 1.616676
[Epoch 116; Iter    25/   41] train: loss: 0.0451157
[Epoch 116] ogbg-molbace: 0.741026 val loss: 1.630218
[Epoch 116] ogbg-molbace: 0.786820 test loss: 1.874298
[Epoch 117; Iter    14/   41] train: loss: 0.0348525
[Epoch 117] ogbg-molbace: 0.745788 val loss: 1.784454
[Epoch 117] ogbg-molbace: 0.793253 test loss: 1.751268
[Epoch 118; Iter     3/   41] train: loss: 0.0181313
[Epoch 118; Iter    33/   41] train: loss: 0.0371167
[Epoch 118] ogbg-molbace: 0.755678 val loss: 1.774710
[Epoch 118] ogbg-molbace: 0.775865 test loss: 1.787790
[Epoch 119; Iter    22/   41] train: loss: 0.0519356
[Epoch 119] ogbg-molbace: 0.743223 val loss: 1.869178
[Epoch 119] ogbg-molbace: 0.788211 test loss: 2.282164
[Epoch 120; Iter    11/   41] train: loss: 0.0191931
[Epoch 120; Iter    41/   41] train: loss: 0.0053281
[Epoch 120] ogbg-molbace: 0.732601 val loss: 1.751970
[Epoch 120] ogbg-molbace: 0.807686 test loss: 1.964129
[Epoch 121; Iter    30/   41] train: loss: 0.0058255
[Epoch 121] ogbg-molbace: 0.735897 val loss: 2.137523
[Epoch 121] ogbg-molbace: 0.796731 test loss: 2.578450
[Epoch 122; Iter    19/   41] train: loss: 0.0513835
[Epoch 122] ogbg-molbace: 0.719048 val loss: 1.399818
[Epoch 122] ogbg-molbace: 0.810989 test loss: 1.901788
[Epoch 123; Iter     8/   41] train: loss: 0.0151116
[Epoch 123; Iter    38/   41] train: loss: 0.0367800
[Epoch 123] ogbg-molbace: 0.736630 val loss: 1.625178
[Epoch 82] ogbg-molbace: 0.813742 val loss: 0.790394
[Epoch 82] ogbg-molbace: 0.816269 test loss: 0.797246
[Epoch 83; Iter    18/   36] train: loss: 0.1410948
[Epoch 83] ogbg-molbace: 0.801374 val loss: 0.628916
[Epoch 83] ogbg-molbace: 0.823763 test loss: 1.007382
[Epoch 84; Iter    12/   36] train: loss: 0.2286370
[Epoch 84] ogbg-molbace: 0.794926 val loss: 0.685351
[Epoch 84] ogbg-molbace: 0.823673 test loss: 0.908543
[Epoch 85; Iter     6/   36] train: loss: 0.1213356
[Epoch 85; Iter    36/   36] train: loss: 0.0330262
[Epoch 85] ogbg-molbace: 0.787844 val loss: 0.952715
[Epoch 85] ogbg-molbace: 0.814554 test loss: 0.882697
[Epoch 86; Iter    30/   36] train: loss: 0.1214663
[Epoch 86] ogbg-molbace: 0.796723 val loss: 0.780042
[Epoch 86] ogbg-molbace: 0.824034 test loss: 0.828536
[Epoch 87; Iter    24/   36] train: loss: 0.1618394
[Epoch 87] ogbg-molbace: 0.790592 val loss: 0.685234
[Epoch 87] ogbg-molbace: 0.810040 test loss: 0.913379
[Epoch 88; Iter    18/   36] train: loss: 0.1591756
[Epoch 88] ogbg-molbace: 0.792706 val loss: 0.793333
[Epoch 88] ogbg-molbace: 0.808866 test loss: 0.865940
[Epoch 89; Iter    12/   36] train: loss: 0.0832634
[Epoch 89] ogbg-molbace: 0.808668 val loss: 0.818847
[Epoch 89] ogbg-molbace: 0.824395 test loss: 0.882260
[Epoch 90; Iter     6/   36] train: loss: 0.2611471
[Epoch 90; Iter    36/   36] train: loss: 0.0555273
[Epoch 90] ogbg-molbace: 0.823573 val loss: 0.536911
[Epoch 90] ogbg-molbace: 0.831979 test loss: 0.887283
[Epoch 91; Iter    30/   36] train: loss: 0.0247647
[Epoch 91] ogbg-molbace: 0.800951 val loss: 0.816598
[Epoch 91] ogbg-molbace: 0.816901 test loss: 0.961145
[Epoch 92; Iter    24/   36] train: loss: 0.0587466
[Epoch 92] ogbg-molbace: 0.781501 val loss: 1.330688
[Epoch 92] ogbg-molbace: 0.804352 test loss: 0.893311
[Epoch 93; Iter    18/   36] train: loss: 0.0718026
[Epoch 93] ogbg-molbace: 0.799260 val loss: 0.739693
[Epoch 93] ogbg-molbace: 0.809047 test loss: 0.998398
[Epoch 94; Iter    12/   36] train: loss: 0.2498318
[Epoch 94] ogbg-molbace: 0.771353 val loss: 1.006477
[Epoch 94] ogbg-molbace: 0.819249 test loss: 0.907248
[Epoch 95; Iter     6/   36] train: loss: 0.0828303
[Epoch 95; Iter    36/   36] train: loss: 0.0293338
[Epoch 95] ogbg-molbace: 0.823890 val loss: 0.829075
[Epoch 95] ogbg-molbace: 0.824937 test loss: 0.847486
[Epoch 96; Iter    30/   36] train: loss: 0.1443811
[Epoch 96] ogbg-molbace: 0.804757 val loss: 0.744198
[Epoch 96] ogbg-molbace: 0.810311 test loss: 1.183682
[Epoch 97; Iter    24/   36] train: loss: 0.0328459
[Epoch 97] ogbg-molbace: 0.807822 val loss: 0.799921
[Epoch 97] ogbg-molbace: 0.836674 test loss: 0.895078
[Epoch 98; Iter    18/   36] train: loss: 0.0433518
[Epoch 98] ogbg-molbace: 0.783192 val loss: 0.806025
[Epoch 98] ogbg-molbace: 0.805796 test loss: 1.084992
[Epoch 99; Iter    12/   36] train: loss: 0.1303800
[Epoch 99] ogbg-molbace: 0.817759 val loss: 0.956807
[Epoch 99] ogbg-molbace: 0.815096 test loss: 0.887593
[Epoch 100; Iter     6/   36] train: loss: 0.2933320
[Epoch 100; Iter    36/   36] train: loss: 0.0424794
[Epoch 100] ogbg-molbace: 0.802431 val loss: 0.926464
[Epoch 100] ogbg-molbace: 0.818165 test loss: 1.050675
[Epoch 101; Iter    30/   36] train: loss: 0.0959682
[Epoch 101] ogbg-molbace: 0.810465 val loss: 0.729220
[Epoch 101] ogbg-molbace: 0.811304 test loss: 1.227486
[Epoch 102; Iter    24/   36] train: loss: 0.0510380
[Epoch 102] ogbg-molbace: 0.803171 val loss: 0.803203
[Epoch 102] ogbg-molbace: 0.814193 test loss: 1.002743
[Epoch 103; Iter    18/   36] train: loss: 0.1151412
[Epoch 103] ogbg-molbace: 0.799366 val loss: 0.741173
[Epoch 103] ogbg-molbace: 0.817985 test loss: 1.102273
[Epoch 104; Iter    12/   36] train: loss: 0.0836120
[Epoch 104] ogbg-molbace: 0.812262 val loss: 0.782228
[Epoch 104] ogbg-molbace: 0.829000 test loss: 1.035865
[Epoch 105; Iter     6/   36] train: loss: 0.2490006
[Epoch 105; Iter    36/   36] train: loss: 0.0365003
[Epoch 105] ogbg-molbace: 0.816808 val loss: 0.834727
[Epoch 105] ogbg-molbace: 0.822951 test loss: 1.079934
[Epoch 106; Iter    30/   36] train: loss: 0.0126296
[Epoch 106] ogbg-molbace: 0.804228 val loss: 0.833019
[Epoch 106] ogbg-molbace: 0.808144 test loss: 1.147209
[Epoch 107; Iter    24/   36] train: loss: 0.0212534
[Epoch 107] ogbg-molbace: 0.824524 val loss: 0.839523
[Epoch 107] ogbg-molbace: 0.812116 test loss: 1.120766
[Epoch 108; Iter    18/   36] train: loss: 0.1062527
[Epoch 108] ogbg-molbace: 0.799789 val loss: 0.886002
[Epoch 108] ogbg-molbace: 0.812929 test loss: 1.185335
[Epoch 109; Iter    12/   36] train: loss: 0.0589898
[Epoch 109] ogbg-molbace: 0.820507 val loss: 0.795908
[Epoch 109] ogbg-molbace: 0.814283 test loss: 1.183445
[Epoch 110; Iter     6/   36] train: loss: 0.0332566
[Epoch 110; Iter    36/   36] train: loss: 0.0945788
[Epoch 110] ogbg-molbace: 0.811734 val loss: 0.751573
[Epoch 110] ogbg-molbace: 0.812748 test loss: 1.321480
[Epoch 111; Iter    30/   36] train: loss: 0.0382200
[Epoch 111] ogbg-molbace: 0.818076 val loss: 0.726804
[Epoch 111] ogbg-molbace: 0.813380 test loss: 1.268136
[Epoch 112; Iter    24/   36] train: loss: 0.0513380
[Epoch 112] ogbg-molbace: 0.813002 val loss: 0.804747
[Epoch 112] ogbg-molbace: 0.803449 test loss: 1.201402
[Epoch 113; Iter    18/   36] train: loss: 0.0103985
[Epoch 113] ogbg-molbace: 0.809197 val loss: 1.024580
[Epoch 113] ogbg-molbace: 0.818707 test loss: 1.165613
[Epoch 114; Iter    12/   36] train: loss: 0.1370504
[Epoch 114] ogbg-molbace: 0.802960 val loss: 0.911977
[Epoch 114] ogbg-molbace: 0.798212 test loss: 1.295901
[Epoch 115; Iter     6/   36] train: loss: 0.0287956
[Epoch 115; Iter    36/   36] train: loss: 0.0204665
[Epoch 115] ogbg-molbace: 0.799154 val loss: 0.806647
[Epoch 115] ogbg-molbace: 0.791351 test loss: 1.268089
[Epoch 116; Iter    30/   36] train: loss: 0.0319092
[Epoch 116] ogbg-molbace: 0.793446 val loss: 0.962893
[Epoch 116] ogbg-molbace: 0.797490 test loss: 1.244879
[Epoch 117; Iter    24/   36] train: loss: 0.0577443
[Epoch 117] ogbg-molbace: 0.810994 val loss: 0.887697
[Epoch 117] ogbg-molbace: 0.812387 test loss: 1.253092
[Epoch 118; Iter    18/   36] train: loss: 0.0194484
[Epoch 118] ogbg-molbace: 0.826216 val loss: 0.874147
[Epoch 118] ogbg-molbace: 0.804442 test loss: 1.356855
[Epoch 119; Iter    12/   36] train: loss: 0.0659703
[Epoch 119] ogbg-molbace: 0.816596 val loss: 1.143842
[Epoch 119] ogbg-molbace: 0.818436 test loss: 1.052530
[Epoch 120; Iter     6/   36] train: loss: 0.0244309
[Epoch 120; Iter    36/   36] train: loss: 1.0507342
[Epoch 120] ogbg-molbace: 0.811099 val loss: 0.737022
[Epoch 120] ogbg-molbace: 0.794511 test loss: 1.572109
[Epoch 121; Iter    30/   36] train: loss: 0.1001020
[Epoch 121] ogbg-molbace: 0.787315 val loss: 1.082707
[Epoch 121] ogbg-molbace: 0.760473 test loss: 1.308550
[Epoch 122; Iter    24/   36] train: loss: 0.0502599
[Epoch 122] ogbg-molbace: 0.793340 val loss: 0.843652
[Epoch 122] ogbg-molbace: 0.795233 test loss: 1.357250
[Epoch 123; Iter    18/   36] train: loss: 0.0606668
[Epoch 123] ogbg-molbace: 0.811099 val loss: 0.860291
[Epoch 123] ogbg-molbace: 0.799928 test loss: 1.196093
[Epoch 124; Iter    12/   36] train: loss: 0.1073291
[Epoch 124] ogbg-molbace: 0.799366 val loss: 0.774595
[Epoch 124] ogbg-molbace: 0.787017 test loss: 1.299513
[Epoch 125; Iter     6/   36] train: loss: 0.0126033
[Epoch 125; Iter    36/   36] train: loss: 0.0124309
[Epoch 125] ogbg-molbace: 0.804123 val loss: 0.887264
[Epoch 125] ogbg-molbace: 0.810491 test loss: 1.243990
[Epoch 126; Iter    30/   36] train: loss: 0.1592866
[Epoch 126] ogbg-molbace: 0.801268 val loss: 0.877658
[Epoch 126] ogbg-molbace: 0.789094 test loss: 1.372762
[Epoch 127; Iter    24/   36] train: loss: 0.0146545
[Epoch 127] ogbg-molbace: 0.791438 val loss: 1.021078
[Epoch 127] ogbg-molbace: 0.804261 test loss: 1.177434
[Epoch 128; Iter    18/   36] train: loss: 0.0182109
[Epoch 128] ogbg-molbace: 0.794080 val loss: 0.932248
[Epoch 128] ogbg-molbace: 0.803359 test loss: 1.238398
[Epoch 129; Iter    12/   36] train: loss: 0.0073355
[Epoch 129] ogbg-molbace: 0.794820 val loss: 1.154191
[Epoch 129] ogbg-molbace: 0.802185 test loss: 1.181359
[Epoch 78] ogbg-molbace: 0.731136 val loss: 1.277059
[Epoch 78] ogbg-molbace: 0.798991 test loss: 1.583571
[Epoch 79; Iter    12/   41] train: loss: 0.1888224
[Epoch 79] ogbg-molbace: 0.758608 val loss: 1.197859
[Epoch 79] ogbg-molbace: 0.795340 test loss: 1.321342
[Epoch 80; Iter     1/   41] train: loss: 0.1837237
[Epoch 80; Iter    31/   41] train: loss: 0.3294458
[Epoch 80] ogbg-molbace: 0.755311 val loss: 1.159808
[Epoch 80] ogbg-molbace: 0.803339 test loss: 1.494484
[Epoch 81; Iter    20/   41] train: loss: 0.1062149
[Epoch 81] ogbg-molbace: 0.778755 val loss: 1.333810
[Epoch 81] ogbg-molbace: 0.793079 test loss: 1.635874
[Epoch 82; Iter     9/   41] train: loss: 0.0248926
[Epoch 82; Iter    39/   41] train: loss: 0.1091166
[Epoch 82] ogbg-molbace: 0.784249 val loss: 0.953307
[Epoch 82] ogbg-molbace: 0.793949 test loss: 1.076469
[Epoch 83; Iter    28/   41] train: loss: 0.1003161
[Epoch 83] ogbg-molbace: 0.788645 val loss: 0.964753
[Epoch 83] ogbg-molbace: 0.807512 test loss: 1.606729
[Epoch 84; Iter    17/   41] train: loss: 0.0675719
[Epoch 84] ogbg-molbace: 0.772527 val loss: 1.354176
[Epoch 84] ogbg-molbace: 0.779343 test loss: 1.573954
[Epoch 85; Iter     6/   41] train: loss: 0.1655110
[Epoch 85; Iter    36/   41] train: loss: 0.1802382
[Epoch 85] ogbg-molbace: 0.718315 val loss: 1.646047
[Epoch 85] ogbg-molbace: 0.787863 test loss: 1.632540
[Epoch 86; Iter    25/   41] train: loss: 0.0878490
[Epoch 86] ogbg-molbace: 0.777289 val loss: 1.050616
[Epoch 86] ogbg-molbace: 0.782299 test loss: 1.449618
[Epoch 87; Iter    14/   41] train: loss: 0.0812474
[Epoch 87] ogbg-molbace: 0.766667 val loss: 1.329799
[Epoch 87] ogbg-molbace: 0.789428 test loss: 1.256598
[Epoch 88; Iter     3/   41] train: loss: 0.1749398
[Epoch 88; Iter    33/   41] train: loss: 0.1703115
[Epoch 88] ogbg-molbace: 0.752747 val loss: 1.315495
[Epoch 88] ogbg-molbace: 0.798818 test loss: 1.232125
[Epoch 89; Iter    22/   41] train: loss: 0.0367419
[Epoch 89] ogbg-molbace: 0.767766 val loss: 1.070078
[Epoch 89] ogbg-molbace: 0.767345 test loss: 1.170666
[Epoch 90; Iter    11/   41] train: loss: 0.1072720
[Epoch 90; Iter    41/   41] train: loss: 0.3931206
[Epoch 90] ogbg-molbace: 0.753114 val loss: 1.312322
[Epoch 90] ogbg-molbace: 0.815336 test loss: 1.342217
[Epoch 91; Iter    30/   41] train: loss: 0.0782411
[Epoch 91] ogbg-molbace: 0.794505 val loss: 0.898642
[Epoch 91] ogbg-molbace: 0.779690 test loss: 1.258187
[Epoch 92; Iter    19/   41] train: loss: 0.1674617
[Epoch 92] ogbg-molbace: 0.756410 val loss: 1.659145
[Epoch 92] ogbg-molbace: 0.797948 test loss: 1.595380
[Epoch 93; Iter     8/   41] train: loss: 0.1608466
[Epoch 93; Iter    38/   41] train: loss: 0.0836053
[Epoch 93] ogbg-molbace: 0.775824 val loss: 1.172312
[Epoch 93] ogbg-molbace: 0.757607 test loss: 1.809823
[Epoch 94; Iter    27/   41] train: loss: 0.1386995
[Epoch 94] ogbg-molbace: 0.761172 val loss: 1.027422
[Epoch 94] ogbg-molbace: 0.780386 test loss: 1.232045
[Epoch 95; Iter    16/   41] train: loss: 0.0558291
[Epoch 95] ogbg-molbace: 0.797436 val loss: 0.651349
[Epoch 95] ogbg-molbace: 0.800730 test loss: 1.306056
[Epoch 96; Iter     5/   41] train: loss: 0.0397591
[Epoch 96; Iter    35/   41] train: loss: 0.0493704
[Epoch 96] ogbg-molbace: 0.712088 val loss: 1.551351
[Epoch 96] ogbg-molbace: 0.779690 test loss: 1.464423
[Epoch 97; Iter    24/   41] train: loss: 0.1953859
[Epoch 97] ogbg-molbace: 0.750916 val loss: 1.099377
[Epoch 97] ogbg-molbace: 0.794992 test loss: 1.365563
[Epoch 98; Iter    13/   41] train: loss: 0.0598023
[Epoch 98] ogbg-molbace: 0.734066 val loss: 1.697431
[Epoch 98] ogbg-molbace: 0.765084 test loss: 1.677449
[Epoch 99; Iter     2/   41] train: loss: 0.1361055
[Epoch 99; Iter    32/   41] train: loss: 0.0856260
[Epoch 99] ogbg-molbace: 0.752015 val loss: 1.556649
[Epoch 99] ogbg-molbace: 0.787515 test loss: 1.322188
[Epoch 100; Iter    21/   41] train: loss: 0.0947341
[Epoch 100] ogbg-molbace: 0.780586 val loss: 1.168713
[Epoch 100] ogbg-molbace: 0.794818 test loss: 1.715310
[Epoch 101; Iter    10/   41] train: loss: 0.0506106
[Epoch 101; Iter    40/   41] train: loss: 0.0553243
[Epoch 101] ogbg-molbace: 0.761172 val loss: 1.276361
[Epoch 101] ogbg-molbace: 0.788732 test loss: 1.719609
[Epoch 102; Iter    29/   41] train: loss: 0.0636968
[Epoch 102] ogbg-molbace: 0.769597 val loss: 1.281732
[Epoch 102] ogbg-molbace: 0.779690 test loss: 1.716531
[Epoch 103; Iter    18/   41] train: loss: 0.0429326
[Epoch 103] ogbg-molbace: 0.778022 val loss: 1.144243
[Epoch 103] ogbg-molbace: 0.786472 test loss: 1.644726
[Epoch 104; Iter     7/   41] train: loss: 0.0842869
[Epoch 104; Iter    37/   41] train: loss: 0.0129226
[Epoch 104] ogbg-molbace: 0.760073 val loss: 1.239608
[Epoch 104] ogbg-molbace: 0.784559 test loss: 1.641995
[Epoch 105; Iter    26/   41] train: loss: 0.0186326
[Epoch 105] ogbg-molbace: 0.779853 val loss: 1.240817
[Epoch 105] ogbg-molbace: 0.784211 test loss: 1.731549
[Epoch 106; Iter    15/   41] train: loss: 0.0216199
[Epoch 106] ogbg-molbace: 0.791209 val loss: 1.200908
[Epoch 106] ogbg-molbace: 0.756738 test loss: 1.870829
[Epoch 107; Iter     4/   41] train: loss: 0.0323808
[Epoch 107; Iter    34/   41] train: loss: 0.0137916
[Epoch 107] ogbg-molbace: 0.771429 val loss: 1.454705
[Epoch 107] ogbg-molbace: 0.780212 test loss: 1.804951
[Epoch 108; Iter    23/   41] train: loss: 0.0679540
[Epoch 108] ogbg-molbace: 0.770330 val loss: 1.300747
[Epoch 108] ogbg-molbace: 0.779343 test loss: 1.925555
[Epoch 109; Iter    12/   41] train: loss: 0.0365315
[Epoch 109] ogbg-molbace: 0.802930 val loss: 1.127937
[Epoch 109] ogbg-molbace: 0.769779 test loss: 2.078825
[Epoch 110; Iter     1/   41] train: loss: 0.0895016
[Epoch 110; Iter    31/   41] train: loss: 0.0367141
[Epoch 110] ogbg-molbace: 0.758242 val loss: 1.320133
[Epoch 110] ogbg-molbace: 0.777430 test loss: 1.924914
[Epoch 111; Iter    20/   41] train: loss: 0.0776355
[Epoch 111] ogbg-molbace: 0.773626 val loss: 1.267775
[Epoch 111] ogbg-molbace: 0.761954 test loss: 2.039732
[Epoch 112; Iter     9/   41] train: loss: 0.0353651
[Epoch 112; Iter    39/   41] train: loss: 0.0099506
[Epoch 112] ogbg-molbace: 0.767399 val loss: 1.355878
[Epoch 112] ogbg-molbace: 0.749783 test loss: 1.982842
[Epoch 113; Iter    28/   41] train: loss: 0.0412686
[Epoch 113] ogbg-molbace: 0.760806 val loss: 1.242190
[Epoch 113] ogbg-molbace: 0.786994 test loss: 1.849397
[Epoch 114; Iter    17/   41] train: loss: 0.0239931
[Epoch 114] ogbg-molbace: 0.772527 val loss: 1.308355
[Epoch 114] ogbg-molbace: 0.773431 test loss: 1.818791
[Epoch 115; Iter     6/   41] train: loss: 0.1026210
[Epoch 115; Iter    36/   41] train: loss: 0.0185481
[Epoch 115] ogbg-molbace: 0.763370 val loss: 1.428551
[Epoch 115] ogbg-molbace: 0.775343 test loss: 1.915031
[Epoch 116; Iter    25/   41] train: loss: 0.0149387
[Epoch 116] ogbg-molbace: 0.769963 val loss: 1.343271
[Epoch 116] ogbg-molbace: 0.760737 test loss: 1.980366
[Epoch 117; Iter    14/   41] train: loss: 0.0348907
[Epoch 117] ogbg-molbace: 0.743223 val loss: 1.500274
[Epoch 117] ogbg-molbace: 0.751174 test loss: 2.250581
[Epoch 118; Iter     3/   41] train: loss: 0.0192886
[Epoch 118; Iter    33/   41] train: loss: 0.0159892
[Epoch 118] ogbg-molbace: 0.745421 val loss: 1.267092
[Epoch 118] ogbg-molbace: 0.753434 test loss: 2.032948
[Epoch 119; Iter    22/   41] train: loss: 0.0127636
[Epoch 119] ogbg-molbace: 0.789377 val loss: 1.257353
[Epoch 119] ogbg-molbace: 0.745436 test loss: 1.855282
[Epoch 120; Iter    11/   41] train: loss: 0.0566482
[Epoch 120; Iter    41/   41] train: loss: 0.1063163
[Epoch 120] ogbg-molbace: 0.772894 val loss: 1.422987
[Epoch 120] ogbg-molbace: 0.742653 test loss: 2.533800
[Epoch 121; Iter    30/   41] train: loss: 0.0578108
[Epoch 121] ogbg-molbace: 0.737363 val loss: 1.314733
[Epoch 121] ogbg-molbace: 0.745609 test loss: 2.180472
[Epoch 122; Iter    19/   41] train: loss: 0.1318484
[Epoch 122] ogbg-molbace: 0.787912 val loss: 0.900430
[Epoch 122] ogbg-molbace: 0.747001 test loss: 2.156812
[Epoch 123; Iter     8/   41] train: loss: 0.1591693
[Epoch 123; Iter    38/   41] train: loss: 0.0385263
[Epoch 123] ogbg-molbace: 0.786813 val loss: 1.119026
[Epoch 82] ogbg-molbace: 0.803911 val loss: 0.640410
[Epoch 82] ogbg-molbace: 0.859155 test loss: 0.715944
[Epoch 83; Iter    18/   36] train: loss: 0.1301706
[Epoch 83] ogbg-molbace: 0.807822 val loss: 0.820604
[Epoch 83] ogbg-molbace: 0.859516 test loss: 0.682466
[Epoch 84; Iter    12/   36] train: loss: 0.0687984
[Epoch 84] ogbg-molbace: 0.793446 val loss: 0.736634
[Epoch 84] ogbg-molbace: 0.859155 test loss: 0.820968
[Epoch 85; Iter     6/   36] train: loss: 0.3056430
[Epoch 85; Iter    36/   36] train: loss: 0.1960530
[Epoch 85] ogbg-molbace: 0.815328 val loss: 0.713834
[Epoch 85] ogbg-molbace: 0.867281 test loss: 0.719266
[Epoch 86; Iter    30/   36] train: loss: 0.1769867
[Epoch 86] ogbg-molbace: 0.810148 val loss: 0.665717
[Epoch 86] ogbg-molbace: 0.854009 test loss: 0.843418
[Epoch 87; Iter    24/   36] train: loss: 0.1055021
[Epoch 87] ogbg-molbace: 0.802748 val loss: 0.706700
[Epoch 87] ogbg-molbace: 0.857620 test loss: 0.847397
[Epoch 88; Iter    18/   36] train: loss: 0.0955017
[Epoch 88] ogbg-molbace: 0.811839 val loss: 0.675872
[Epoch 88] ogbg-molbace: 0.845161 test loss: 0.976943
[Epoch 89; Iter    12/   36] train: loss: 0.0922750
[Epoch 89] ogbg-molbace: 0.807928 val loss: 0.965435
[Epoch 89] ogbg-molbace: 0.848050 test loss: 0.713924
[Epoch 90; Iter     6/   36] train: loss: 0.1051037
[Epoch 90; Iter    36/   36] train: loss: 0.0802977
[Epoch 90] ogbg-molbace: 0.798626 val loss: 0.771860
[Epoch 90] ogbg-molbace: 0.869177 test loss: 0.736306
[Epoch 91; Iter    30/   36] train: loss: 0.1554132
[Epoch 91] ogbg-molbace: 0.809937 val loss: 0.648410
[Epoch 91] ogbg-molbace: 0.867551 test loss: 0.922482
[Epoch 92; Iter    24/   36] train: loss: 0.1308272
[Epoch 92] ogbg-molbace: 0.795455 val loss: 1.090080
[Epoch 92] ogbg-molbace: 0.840466 test loss: 0.886665
[Epoch 93; Iter    18/   36] train: loss: 0.2217544
[Epoch 93] ogbg-molbace: 0.804123 val loss: 0.661272
[Epoch 93] ogbg-molbace: 0.852203 test loss: 1.021171
[Epoch 94; Iter    12/   36] train: loss: 0.0671112
[Epoch 94] ogbg-molbace: 0.792178 val loss: 0.808785
[Epoch 94] ogbg-molbace: 0.850758 test loss: 0.866214
[Epoch 95; Iter     6/   36] train: loss: 0.1719019
[Epoch 95; Iter    36/   36] train: loss: 0.1871948
[Epoch 95] ogbg-molbace: 0.811099 val loss: 0.772965
[Epoch 95] ogbg-molbace: 0.849043 test loss: 0.879450
[Epoch 96; Iter    30/   36] train: loss: 0.0397263
[Epoch 96] ogbg-molbace: 0.809197 val loss: 0.666002
[Epoch 96] ogbg-molbace: 0.859426 test loss: 1.008330
[Epoch 97; Iter    24/   36] train: loss: 0.0378715
[Epoch 97] ogbg-molbace: 0.802537 val loss: 0.691416
[Epoch 97] ogbg-molbace: 0.849224 test loss: 0.872035
[Epoch 98; Iter    18/   36] train: loss: 0.0388070
[Epoch 98] ogbg-molbace: 0.810148 val loss: 0.893302
[Epoch 98] ogbg-molbace: 0.838931 test loss: 0.888610
[Epoch 99; Iter    12/   36] train: loss: 0.0554096
[Epoch 99] ogbg-molbace: 0.822304 val loss: 0.885811
[Epoch 99] ogbg-molbace: 0.856808 test loss: 0.887451
[Epoch 100; Iter     6/   36] train: loss: 0.1568685
[Epoch 100; Iter    36/   36] train: loss: 0.1110924
[Epoch 100] ogbg-molbace: 0.814693 val loss: 0.663718
[Epoch 100] ogbg-molbace: 0.848411 test loss: 1.107055
[Epoch 101; Iter    30/   36] train: loss: 0.0749254
[Epoch 101] ogbg-molbace: 0.835095 val loss: 0.669374
[Epoch 101] ogbg-molbace: 0.858252 test loss: 0.927688
[Epoch 102; Iter    24/   36] train: loss: 0.0519484
[Epoch 102] ogbg-molbace: 0.810359 val loss: 0.773565
[Epoch 102] ogbg-molbace: 0.844348 test loss: 0.937348
[Epoch 103; Iter    18/   36] train: loss: 0.0937027
[Epoch 103] ogbg-molbace: 0.783932 val loss: 0.901671
[Epoch 103] ogbg-molbace: 0.839021 test loss: 0.863038
[Epoch 104; Iter    12/   36] train: loss: 0.0332311
[Epoch 104] ogbg-molbace: 0.826427 val loss: 0.963393
[Epoch 104] ogbg-molbace: 0.861593 test loss: 0.761987
[Epoch 105; Iter     6/   36] train: loss: 0.0228004
[Epoch 105; Iter    36/   36] train: loss: 0.0652672
[Epoch 105] ogbg-molbace: 0.814271 val loss: 0.866598
[Epoch 105] ogbg-molbace: 0.838480 test loss: 0.993083
[Epoch 106; Iter    30/   36] train: loss: 0.0347289
[Epoch 106] ogbg-molbace: 0.839852 val loss: 0.852029
[Epoch 106] ogbg-molbace: 0.851661 test loss: 0.898778
[Epoch 107; Iter    24/   36] train: loss: 0.0258682
[Epoch 107] ogbg-molbace: 0.810148 val loss: 0.827420
[Epoch 107] ogbg-molbace: 0.859606 test loss: 0.979108
[Epoch 108; Iter    18/   36] train: loss: 0.0522593
[Epoch 108] ogbg-molbace: 0.829493 val loss: 0.857531
[Epoch 108] ogbg-molbace: 0.849494 test loss: 0.958761
[Epoch 109; Iter    12/   36] train: loss: 0.0274069
[Epoch 109] ogbg-molbace: 0.818076 val loss: 0.928018
[Epoch 109] ogbg-molbace: 0.856627 test loss: 0.912146
[Epoch 110; Iter     6/   36] train: loss: 0.0048150
[Epoch 110; Iter    36/   36] train: loss: 0.0313514
[Epoch 110] ogbg-molbace: 0.814376 val loss: 0.914261
[Epoch 110] ogbg-molbace: 0.856446 test loss: 0.987869
[Epoch 111; Iter    30/   36] train: loss: 0.0312409
[Epoch 111] ogbg-molbace: 0.815645 val loss: 0.945473
[Epoch 111] ogbg-molbace: 0.855453 test loss: 0.941863
[Epoch 112; Iter    24/   36] train: loss: 0.0133906
[Epoch 112] ogbg-molbace: 0.815645 val loss: 0.916238
[Epoch 112] ogbg-molbace: 0.851752 test loss: 1.030417
[Epoch 113; Iter    18/   36] train: loss: 0.0577337
[Epoch 113] ogbg-molbace: 0.810254 val loss: 0.980057
[Epoch 113] ogbg-molbace: 0.848140 test loss: 1.040903
[Epoch 114; Iter    12/   36] train: loss: 0.1773119
[Epoch 114] ogbg-molbace: 0.819133 val loss: 0.890957
[Epoch 114] ogbg-molbace: 0.849043 test loss: 1.221378
[Epoch 115; Iter     6/   36] train: loss: 0.0490014
[Epoch 115; Iter    36/   36] train: loss: 0.2262371
[Epoch 115] ogbg-molbace: 0.823362 val loss: 0.872689
[Epoch 115] ogbg-molbace: 0.847689 test loss: 1.116923
[Epoch 116; Iter    30/   36] train: loss: 0.0467975
[Epoch 116] ogbg-molbace: 0.809937 val loss: 1.045450
[Epoch 116] ogbg-molbace: 0.853648 test loss: 1.042563
[Epoch 117; Iter    24/   36] train: loss: 0.0869078
[Epoch 117] ogbg-molbace: 0.803277 val loss: 0.911014
[Epoch 117] ogbg-molbace: 0.846696 test loss: 1.191121
[Epoch 118; Iter    18/   36] train: loss: 0.0131469
[Epoch 118] ogbg-molbace: 0.813953 val loss: 0.886035
[Epoch 118] ogbg-molbace: 0.847328 test loss: 1.127538
[Epoch 119; Iter    12/   36] train: loss: 0.1384835
[Epoch 119] ogbg-molbace: 0.807611 val loss: 0.762565
[Epoch 119] ogbg-molbace: 0.835320 test loss: 1.311102
[Epoch 120; Iter     6/   36] train: loss: 0.0396951
[Epoch 120; Iter    36/   36] train: loss: 0.0919488
[Epoch 120] ogbg-molbace: 0.805180 val loss: 1.038395
[Epoch 120] ogbg-molbace: 0.841008 test loss: 1.121677
[Epoch 121; Iter    30/   36] train: loss: 0.0047991
[Epoch 121] ogbg-molbace: 0.798203 val loss: 1.027861
[Epoch 121] ogbg-molbace: 0.840737 test loss: 1.160328
[Epoch 122; Iter    24/   36] train: loss: 0.0055975
[Epoch 122] ogbg-molbace: 0.796617 val loss: 1.062804
[Epoch 122] ogbg-molbace: 0.834417 test loss: 1.124554
[Epoch 123; Iter    18/   36] train: loss: 0.0114148
[Epoch 123] ogbg-molbace: 0.803066 val loss: 1.303792
[Epoch 123] ogbg-molbace: 0.844348 test loss: 1.044480
[Epoch 124; Iter    12/   36] train: loss: 0.1793623
[Epoch 124] ogbg-molbace: 0.806448 val loss: 1.066191
[Epoch 124] ogbg-molbace: 0.848592 test loss: 1.112559
[Epoch 125; Iter     6/   36] train: loss: 0.0064893
[Epoch 125; Iter    36/   36] train: loss: 0.1824561
[Epoch 125] ogbg-molbace: 0.810994 val loss: 0.928067
[Epoch 125] ogbg-molbace: 0.847508 test loss: 1.134860
[Epoch 126; Iter    30/   36] train: loss: 0.0165597
[Epoch 126] ogbg-molbace: 0.812685 val loss: 1.458717
[Epoch 126] ogbg-molbace: 0.855092 test loss: 1.018528
[Epoch 127; Iter    24/   36] train: loss: 0.0199452
[Epoch 127] ogbg-molbace: 0.811205 val loss: 1.207549
[Epoch 127] ogbg-molbace: 0.847960 test loss: 0.947720
[Epoch 128; Iter    18/   36] train: loss: 0.0919100
[Epoch 128] ogbg-molbace: 0.776850 val loss: 1.208513
[Epoch 128] ogbg-molbace: 0.836854 test loss: 1.144283
[Epoch 129; Iter    12/   36] train: loss: 0.0237025
[Epoch 129] ogbg-molbace: 0.815433 val loss: 1.000602
[Epoch 129] ogbg-molbace: 0.852022 test loss: 1.093360
[Epoch 82] ogbg-molbace: 0.774101 val loss: 0.955075
[Epoch 82] ogbg-molbace: 0.830805 test loss: 0.735058
[Epoch 83; Iter    18/   36] train: loss: 0.1511170
[Epoch 83] ogbg-molbace: 0.826004 val loss: 0.690179
[Epoch 83] ogbg-molbace: 0.839744 test loss: 0.782666
[Epoch 84; Iter    12/   36] train: loss: 0.0561244
[Epoch 84] ogbg-molbace: 0.803911 val loss: 0.814114
[Epoch 84] ogbg-molbace: 0.842452 test loss: 0.912019
[Epoch 85; Iter     6/   36] train: loss: 0.0994883
[Epoch 85; Iter    36/   36] train: loss: 0.0727962
[Epoch 85] ogbg-molbace: 0.786681 val loss: 0.727083
[Epoch 85] ogbg-molbace: 0.850397 test loss: 0.820876
[Epoch 86; Iter    30/   36] train: loss: 0.1634659
[Epoch 86] ogbg-molbace: 0.813002 val loss: 0.598297
[Epoch 86] ogbg-molbace: 0.845251 test loss: 1.128985
[Epoch 87; Iter    24/   36] train: loss: 0.1060117
[Epoch 87] ogbg-molbace: 0.795560 val loss: 0.744190
[Epoch 87] ogbg-molbace: 0.845341 test loss: 0.904428
[Epoch 88; Iter    18/   36] train: loss: 0.1010024
[Epoch 88] ogbg-molbace: 0.791332 val loss: 0.835042
[Epoch 88] ogbg-molbace: 0.843174 test loss: 0.946198
[Epoch 89; Iter    12/   36] train: loss: 0.0992347
[Epoch 89] ogbg-molbace: 0.787844 val loss: 0.738367
[Epoch 89] ogbg-molbace: 0.844619 test loss: 0.770160
[Epoch 90; Iter     6/   36] train: loss: 0.0568772
[Epoch 90; Iter    36/   36] train: loss: 0.0816099
[Epoch 90] ogbg-molbace: 0.807188 val loss: 0.725061
[Epoch 90] ogbg-molbace: 0.851390 test loss: 0.842293
[Epoch 91; Iter    30/   36] train: loss: 0.1412692
[Epoch 91] ogbg-molbace: 0.807082 val loss: 0.671612
[Epoch 91] ogbg-molbace: 0.839112 test loss: 0.920461
[Epoch 92; Iter    24/   36] train: loss: 0.0689232
[Epoch 92] ogbg-molbace: 0.771459 val loss: 0.902251
[Epoch 92] ogbg-molbace: 0.848411 test loss: 0.860918
[Epoch 93; Iter    18/   36] train: loss: 0.2674378
[Epoch 93] ogbg-molbace: 0.800740 val loss: 0.625269
[Epoch 93] ogbg-molbace: 0.849856 test loss: 0.957992
[Epoch 94; Iter    12/   36] train: loss: 0.0961210
[Epoch 94] ogbg-molbace: 0.798097 val loss: 0.712797
[Epoch 94] ogbg-molbace: 0.842542 test loss: 0.818437
[Epoch 95; Iter     6/   36] train: loss: 0.0839535
[Epoch 95; Iter    36/   36] train: loss: 0.2663907
[Epoch 95] ogbg-molbace: 0.803805 val loss: 0.825694
[Epoch 95] ogbg-molbace: 0.842994 test loss: 0.964002
[Epoch 96; Iter    30/   36] train: loss: 0.2169220
[Epoch 96] ogbg-molbace: 0.804651 val loss: 0.603682
[Epoch 96] ogbg-molbace: 0.844348 test loss: 1.174392
[Epoch 97; Iter    24/   36] train: loss: 0.1185659
[Epoch 97] ogbg-molbace: 0.803911 val loss: 0.623182
[Epoch 97] ogbg-molbace: 0.837938 test loss: 1.012881
[Epoch 98; Iter    18/   36] train: loss: 0.1209567
[Epoch 98] ogbg-molbace: 0.806448 val loss: 0.782202
[Epoch 98] ogbg-molbace: 0.838389 test loss: 0.972495
[Epoch 99; Iter    12/   36] train: loss: 0.0272369
[Epoch 99] ogbg-molbace: 0.795349 val loss: 0.666889
[Epoch 99] ogbg-molbace: 0.837306 test loss: 1.062936
[Epoch 100; Iter     6/   36] train: loss: 0.0605324
[Epoch 100; Iter    36/   36] train: loss: 0.0426645
[Epoch 100] ogbg-molbace: 0.779387 val loss: 0.814193
[Epoch 100] ogbg-molbace: 0.827104 test loss: 0.884189
[Epoch 101; Iter    30/   36] train: loss: 0.0304357
[Epoch 101] ogbg-molbace: 0.810042 val loss: 0.678598
[Epoch 101] ogbg-molbace: 0.840105 test loss: 1.007492
[Epoch 102; Iter    24/   36] train: loss: 0.0978742
[Epoch 102] ogbg-molbace: 0.797357 val loss: 1.036534
[Epoch 102] ogbg-molbace: 0.858884 test loss: 0.766612
[Epoch 103; Iter    18/   36] train: loss: 0.1312725
[Epoch 103] ogbg-molbace: 0.805814 val loss: 0.646801
[Epoch 103] ogbg-molbace: 0.846154 test loss: 0.986434
[Epoch 104; Iter    12/   36] train: loss: 0.0151421
[Epoch 104] ogbg-molbace: 0.797780 val loss: 0.878218
[Epoch 104] ogbg-molbace: 0.834417 test loss: 0.997216
[Epoch 105; Iter     6/   36] train: loss: 0.0190240
[Epoch 105; Iter    36/   36] train: loss: 0.0198261
[Epoch 105] ogbg-molbace: 0.802008 val loss: 0.752587
[Epoch 105] ogbg-molbace: 0.850849 test loss: 0.995083
[Epoch 106; Iter    30/   36] train: loss: 0.0574208
[Epoch 106] ogbg-molbace: 0.796512 val loss: 0.792524
[Epoch 106] ogbg-molbace: 0.841549 test loss: 1.013779
[Epoch 107; Iter    24/   36] train: loss: 0.0347022
[Epoch 107] ogbg-molbace: 0.797357 val loss: 0.992211
[Epoch 107] ogbg-molbace: 0.840827 test loss: 1.071382
[Epoch 108; Iter    18/   36] train: loss: 0.0487031
[Epoch 108] ogbg-molbace: 0.802431 val loss: 0.773323
[Epoch 108] ogbg-molbace: 0.837306 test loss: 1.128603
[Epoch 109; Iter    12/   36] train: loss: 0.0228081
[Epoch 109] ogbg-molbace: 0.799154 val loss: 1.000811
[Epoch 109] ogbg-molbace: 0.838750 test loss: 0.993829
[Epoch 110; Iter     6/   36] train: loss: 0.0180324
[Epoch 110; Iter    36/   36] train: loss: 0.0463122
[Epoch 110] ogbg-molbace: 0.800423 val loss: 0.776273
[Epoch 110] ogbg-molbace: 0.848862 test loss: 1.099033
[Epoch 111; Iter    30/   36] train: loss: 0.0204109
[Epoch 111] ogbg-molbace: 0.790909 val loss: 0.895370
[Epoch 111] ogbg-molbace: 0.843626 test loss: 1.077094
[Epoch 112; Iter    24/   36] train: loss: 0.0071579
[Epoch 112] ogbg-molbace: 0.803488 val loss: 0.758380
[Epoch 112] ogbg-molbace: 0.841640 test loss: 1.249506
[Epoch 113; Iter    18/   36] train: loss: 0.0250749
[Epoch 113] ogbg-molbace: 0.786047 val loss: 0.844614
[Epoch 113] ogbg-molbace: 0.848592 test loss: 1.303189
[Epoch 114; Iter    12/   36] train: loss: 0.0109084
[Epoch 114] ogbg-molbace: 0.788795 val loss: 1.066984
[Epoch 114] ogbg-molbace: 0.835681 test loss: 1.161252
[Epoch 115; Iter     6/   36] train: loss: 0.0330069
[Epoch 115; Iter    36/   36] train: loss: 0.1187158
[Epoch 115] ogbg-molbace: 0.775899 val loss: 1.059176
[Epoch 115] ogbg-molbace: 0.836042 test loss: 1.188956
[Epoch 116; Iter    30/   36] train: loss: 0.0080666
[Epoch 116] ogbg-molbace: 0.778858 val loss: 1.277734
[Epoch 116] ogbg-molbace: 0.835139 test loss: 1.181919
[Epoch 117; Iter    24/   36] train: loss: 0.0956048
[Epoch 117] ogbg-molbace: 0.780867 val loss: 1.650031
[Epoch 117] ogbg-molbace: 0.840737 test loss: 1.067604
[Epoch 118; Iter    18/   36] train: loss: 0.1731409
[Epoch 118] ogbg-molbace: 0.791543 val loss: 1.042888
[Epoch 118] ogbg-molbace: 0.834597 test loss: 1.126946
[Epoch 119; Iter    12/   36] train: loss: 0.0992145
[Epoch 119] ogbg-molbace: 0.778753 val loss: 1.071630
[Epoch 119] ogbg-molbace: 0.827013 test loss: 1.235051
[Epoch 120; Iter     6/   36] train: loss: 0.0373743
[Epoch 120; Iter    36/   36] train: loss: 0.0921317
[Epoch 120] ogbg-molbace: 0.780444 val loss: 1.245959
[Epoch 120] ogbg-molbace: 0.848772 test loss: 1.073734
[Epoch 121; Iter    30/   36] train: loss: 0.0086243
[Epoch 121] ogbg-molbace: 0.781290 val loss: 0.971546
[Epoch 121] ogbg-molbace: 0.836403 test loss: 1.241128
[Epoch 122; Iter    24/   36] train: loss: 0.0736374
[Epoch 122] ogbg-molbace: 0.785518 val loss: 1.054152
[Epoch 122] ogbg-molbace: 0.828638 test loss: 1.163588
[Epoch 123; Iter    18/   36] train: loss: 0.1391439
[Epoch 123] ogbg-molbace: 0.780233 val loss: 0.883938
[Epoch 123] ogbg-molbace: 0.831437 test loss: 1.325608
[Epoch 124; Iter    12/   36] train: loss: 0.0152400
[Epoch 124] ogbg-molbace: 0.777801 val loss: 1.044160
[Epoch 124] ogbg-molbace: 0.832430 test loss: 1.201214
[Epoch 125; Iter     6/   36] train: loss: 0.0225289
[Epoch 125; Iter    36/   36] train: loss: 0.0098040
[Epoch 125] ogbg-molbace: 0.789852 val loss: 1.053636
[Epoch 125] ogbg-molbace: 0.838841 test loss: 1.243876
[Epoch 126; Iter    30/   36] train: loss: 0.0225803
[Epoch 126] ogbg-molbace: 0.779810 val loss: 1.228202
[Epoch 126] ogbg-molbace: 0.828458 test loss: 1.220872
[Epoch 127; Iter    24/   36] train: loss: 0.0114704
[Epoch 127] ogbg-molbace: 0.786152 val loss: 1.084101
[Epoch 127] ogbg-molbace: 0.819610 test loss: 1.351232
[Epoch 128; Iter    18/   36] train: loss: 0.0038100
[Epoch 128] ogbg-molbace: 0.791966 val loss: 1.045704
[Epoch 128] ogbg-molbace: 0.831166 test loss: 1.279393
[Epoch 129; Iter    12/   36] train: loss: 0.0290320
[Epoch 129] ogbg-molbace: 0.789852 val loss: 1.106712
[Epoch 129] ogbg-molbace: 0.825388 test loss: 1.301330
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 130; Iter     6/   36] train: loss: 0.0078723
[Epoch 130; Iter    36/   36] train: loss: 0.0022016
[Epoch 130] ogbg-molbace: 0.774841 val loss: 1.219803
[Epoch 130] ogbg-molbace: 0.829180 test loss: 1.334938
[Epoch 131; Iter    30/   36] train: loss: 0.0104292
[Epoch 131] ogbg-molbace: 0.780127 val loss: 1.050864
[Epoch 131] ogbg-molbace: 0.825027 test loss: 1.400426
[Epoch 132; Iter    24/   36] train: loss: 0.0055959
[Epoch 132] ogbg-molbace: 0.783510 val loss: 1.218116
[Epoch 132] ogbg-molbace: 0.833424 test loss: 1.340882
[Epoch 133; Iter    18/   36] train: loss: 0.0134687
[Epoch 133] ogbg-molbace: 0.776956 val loss: 1.038514
[Epoch 133] ogbg-molbace: 0.823312 test loss: 1.520896
[Epoch 134; Iter    12/   36] train: loss: 0.0034862
[Epoch 134] ogbg-molbace: 0.788689 val loss: 1.063801
[Epoch 134] ogbg-molbace: 0.833153 test loss: 1.418615
[Epoch 135; Iter     6/   36] train: loss: 0.0087367
[Epoch 135; Iter    36/   36] train: loss: 0.0014993
[Epoch 135] ogbg-molbace: 0.772939 val loss: 1.086310
[Epoch 135] ogbg-molbace: 0.834146 test loss: 1.544448
[Epoch 136; Iter    30/   36] train: loss: 0.0068052
[Epoch 136] ogbg-molbace: 0.783404 val loss: 0.999590
[Epoch 136] ogbg-molbace: 0.833062 test loss: 1.413032
[Epoch 137; Iter    24/   36] train: loss: 0.0224319
[Epoch 137] ogbg-molbace: 0.794397 val loss: 0.965984
[Epoch 137] ogbg-molbace: 0.832250 test loss: 1.397449
[Epoch 138; Iter    18/   36] train: loss: 0.0066619
[Epoch 138] ogbg-molbace: 0.779598 val loss: 1.124938
[Epoch 138] ogbg-molbace: 0.825569 test loss: 1.276698
[Epoch 139; Iter    12/   36] train: loss: 0.0048518
[Epoch 139] ogbg-molbace: 0.785518 val loss: 1.113132
[Epoch 139] ogbg-molbace: 0.828368 test loss: 1.326187
[Epoch 140; Iter     6/   36] train: loss: 0.0051277
[Epoch 140; Iter    36/   36] train: loss: 0.0952332
[Epoch 140] ogbg-molbace: 0.790275 val loss: 1.245240
[Epoch 140] ogbg-molbace: 0.825117 test loss: 1.452086
[Epoch 141; Iter    30/   36] train: loss: 0.0369794
[Epoch 141] ogbg-molbace: 0.782875 val loss: 1.263446
[Epoch 141] ogbg-molbace: 0.832701 test loss: 1.300571
[Epoch 142; Iter    24/   36] train: loss: 0.0043951
[Epoch 142] ogbg-molbace: 0.785835 val loss: 1.155576
[Epoch 142] ogbg-molbace: 0.825298 test loss: 1.414071
[Epoch 143; Iter    18/   36] train: loss: 0.0020811
[Epoch 143] ogbg-molbace: 0.778436 val loss: 1.228275
[Epoch 143] ogbg-molbace: 0.827375 test loss: 1.415737
Early stopping criterion based on -ogbg-molbace- that should be max reached after 143 epochs. Best model checkpoint was in epoch 83.
Statistics on  val_best_checkpoint
mean_pred: -1.0906840562820435
std_pred: 3.6002533435821533
mean_targets: 0.24229073524475098
std_targets: 0.4294162094593048
prcauc: 0.5846969745350871
rocauc: 0.8260042283298097
ogbg-molbace: 0.8260042283298097
BCEWithLogitsLoss: 0.6901793405413628
Statistics on  test
mean_pred: 0.6708834767341614
std_pred: 8.040887832641602
mean_targets: 0.6872246265411377
std_targets: 0.4646482765674591
prcauc: 0.8627911223949323
rocauc: 0.8397435897435898
ogbg-molbace: 0.8397435897435898
BCEWithLogitsLoss: 0.7826655986718833
Statistics on  train
mean_pred: -0.510636568069458
std_pred: 4.895506858825684
mean_targets: 0.45325779914855957
std_targets: 0.49804559350013733
prcauc: 0.9941944745523572
rocauc: 0.9950273459988486
ogbg-molbace: 0.9950273459988486
BCEWithLogitsLoss: 0.10383080277178022
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.800904 test loss: 2.056716
[Epoch 124; Iter    27/   41] train: loss: 0.0379526
[Epoch 124] ogbg-molbace: 0.723810 val loss: 1.710651
[Epoch 124] ogbg-molbace: 0.772387 test loss: 2.103534
[Epoch 125; Iter    16/   41] train: loss: 0.0110698
[Epoch 125] ogbg-molbace: 0.715385 val loss: 1.844270
[Epoch 125] ogbg-molbace: 0.808381 test loss: 1.960587
[Epoch 126; Iter     5/   41] train: loss: 0.0604898
[Epoch 126; Iter    35/   41] train: loss: 0.0444101
[Epoch 126] ogbg-molbace: 0.739194 val loss: 2.113328
[Epoch 126] ogbg-molbace: 0.796905 test loss: 2.113406
[Epoch 127; Iter    24/   41] train: loss: 0.0507090
[Epoch 127] ogbg-molbace: 0.744689 val loss: 1.676450
[Epoch 127] ogbg-molbace: 0.808729 test loss: 1.657278
[Epoch 128; Iter    13/   41] train: loss: 0.0222186
[Epoch 128] ogbg-molbace: 0.735165 val loss: 2.315574
[Epoch 128] ogbg-molbace: 0.788732 test loss: 1.701055
[Epoch 129; Iter     2/   41] train: loss: 0.0097312
[Epoch 129; Iter    32/   41] train: loss: 0.0331512
[Epoch 129] ogbg-molbace: 0.745421 val loss: 1.719507
[Epoch 129] ogbg-molbace: 0.812380 test loss: 1.856239
[Epoch 130; Iter    21/   41] train: loss: 0.0238917
[Epoch 130] ogbg-molbace: 0.727473 val loss: 2.103209
[Epoch 130] ogbg-molbace: 0.794297 test loss: 1.904030
[Epoch 131; Iter    10/   41] train: loss: 0.0117917
[Epoch 131; Iter    40/   41] train: loss: 0.0534050
[Epoch 131] ogbg-molbace: 0.702564 val loss: 1.986004
[Epoch 131] ogbg-molbace: 0.770649 test loss: 2.655702
[Epoch 132; Iter    29/   41] train: loss: 0.1084497
[Epoch 132] ogbg-molbace: 0.727473 val loss: 1.978726
[Epoch 132] ogbg-molbace: 0.799339 test loss: 1.695720
[Epoch 133; Iter    18/   41] train: loss: 0.0295883
[Epoch 133] ogbg-molbace: 0.726007 val loss: 1.956436
[Epoch 133] ogbg-molbace: 0.784211 test loss: 2.174856
[Epoch 134; Iter     7/   41] train: loss: 0.0131806
[Epoch 134; Iter    37/   41] train: loss: 0.0041140
[Epoch 134] ogbg-molbace: 0.729304 val loss: 2.012278
[Epoch 134] ogbg-molbace: 0.800556 test loss: 1.853338
[Epoch 135; Iter    26/   41] train: loss: 0.0062929
[Epoch 135] ogbg-molbace: 0.728571 val loss: 1.929080
[Epoch 135] ogbg-molbace: 0.801774 test loss: 1.956214
[Epoch 136; Iter    15/   41] train: loss: 0.0140096
[Epoch 136] ogbg-molbace: 0.738095 val loss: 1.792474
[Epoch 136] ogbg-molbace: 0.796035 test loss: 1.898213
[Epoch 137; Iter     4/   41] train: loss: 0.0101290
[Epoch 137; Iter    34/   41] train: loss: 0.0060911
[Epoch 137] ogbg-molbace: 0.739560 val loss: 2.082387
[Epoch 137] ogbg-molbace: 0.796035 test loss: 1.938658
[Epoch 138; Iter    23/   41] train: loss: 0.0042712
[Epoch 138] ogbg-molbace: 0.743223 val loss: 2.029525
[Epoch 138] ogbg-molbace: 0.789950 test loss: 1.928388
[Epoch 139; Iter    12/   41] train: loss: 0.0072810
[Epoch 139] ogbg-molbace: 0.753114 val loss: 2.010033
[Epoch 139] ogbg-molbace: 0.791341 test loss: 1.999891
[Epoch 140; Iter     1/   41] train: loss: 0.1030929
[Epoch 140; Iter    31/   41] train: loss: 0.0070033
[Epoch 140] ogbg-molbace: 0.736264 val loss: 2.278268
[Epoch 140] ogbg-molbace: 0.789254 test loss: 1.710651
[Epoch 141; Iter    20/   41] train: loss: 0.0300369
[Epoch 141] ogbg-molbace: 0.715751 val loss: 1.926347
[Epoch 141] ogbg-molbace: 0.790297 test loss: 1.891323
[Epoch 142; Iter     9/   41] train: loss: 0.0024091
[Epoch 142; Iter    39/   41] train: loss: 0.0047070
[Epoch 142] ogbg-molbace: 0.742491 val loss: 2.279325
[Epoch 142] ogbg-molbace: 0.779169 test loss: 2.124871
[Epoch 143; Iter    28/   41] train: loss: 0.0383219
[Epoch 143] ogbg-molbace: 0.721612 val loss: 2.360150
[Epoch 143] ogbg-molbace: 0.775343 test loss: 2.303375
[Epoch 144; Iter    17/   41] train: loss: 0.0708297
[Epoch 144] ogbg-molbace: 0.727106 val loss: 2.306047
[Epoch 144] ogbg-molbace: 0.784907 test loss: 2.272413
[Epoch 145; Iter     6/   41] train: loss: 0.0049088
[Epoch 145; Iter    36/   41] train: loss: 0.0040272
[Epoch 145] ogbg-molbace: 0.736996 val loss: 2.187054
[Epoch 145] ogbg-molbace: 0.777604 test loss: 2.430750
[Epoch 146; Iter    25/   41] train: loss: 0.0145004
[Epoch 146] ogbg-molbace: 0.713919 val loss: 2.282579
[Epoch 146] ogbg-molbace: 0.781951 test loss: 2.330154
[Epoch 147; Iter    14/   41] train: loss: 0.0070468
[Epoch 147] ogbg-molbace: 0.730769 val loss: 2.445090
[Epoch 147] ogbg-molbace: 0.798296 test loss: 2.186191
[Epoch 148; Iter     3/   41] train: loss: 0.0125714
[Epoch 148; Iter    33/   41] train: loss: 0.0658704
[Epoch 148] ogbg-molbace: 0.743223 val loss: 2.199742
[Epoch 148] ogbg-molbace: 0.785081 test loss: 2.089352
[Epoch 149; Iter    22/   41] train: loss: 0.0064451
[Epoch 149] ogbg-molbace: 0.732601 val loss: 2.254881
[Epoch 149] ogbg-molbace: 0.788037 test loss: 2.169072
[Epoch 150; Iter    11/   41] train: loss: 0.0175022
[Epoch 150; Iter    41/   41] train: loss: 0.0050303
[Epoch 150] ogbg-molbace: 0.758242 val loss: 2.137410
[Epoch 150] ogbg-molbace: 0.786472 test loss: 2.012474
[Epoch 151; Iter    30/   41] train: loss: 0.0041008
[Epoch 151] ogbg-molbace: 0.739927 val loss: 2.175182
[Epoch 151] ogbg-molbace: 0.788732 test loss: 2.106606
[Epoch 152; Iter    19/   41] train: loss: 0.0031277
[Epoch 152] ogbg-molbace: 0.746886 val loss: 2.348395
[Epoch 152] ogbg-molbace: 0.780908 test loss: 1.968474
[Epoch 153; Iter     8/   41] train: loss: 0.0058618
[Epoch 153; Iter    38/   41] train: loss: 0.0209768
[Epoch 153] ogbg-molbace: 0.749451 val loss: 2.275397
[Epoch 153] ogbg-molbace: 0.785429 test loss: 2.227582
[Epoch 154; Iter    27/   41] train: loss: 0.0053171
[Epoch 154] ogbg-molbace: 0.734432 val loss: 2.257186
[Epoch 154] ogbg-molbace: 0.793601 test loss: 1.840437
[Epoch 155; Iter    16/   41] train: loss: 0.0792619
[Epoch 155] ogbg-molbace: 0.738828 val loss: 2.443433
[Epoch 155] ogbg-molbace: 0.781255 test loss: 2.054145
[Epoch 156; Iter     5/   41] train: loss: 0.0072845
[Epoch 156; Iter    35/   41] train: loss: 0.0030128
[Epoch 156] ogbg-molbace: 0.723077 val loss: 2.782863
[Epoch 156] ogbg-molbace: 0.775517 test loss: 2.421833
Early stopping criterion based on -ogbg-molbace- that should be max reached after 156 epochs. Best model checkpoint was in epoch 96.
Statistics on  val_best_checkpoint
mean_pred: 1.369286298751831
std_pred: 4.489266395568848
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9561738523062454
rocauc: 0.7908424908424909
ogbg-molbace: 0.7908424908424909
BCEWithLogitsLoss: 1.383072167634964
Statistics on  test
mean_pred: -1.535839557647705
std_pred: 6.010002613067627
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7736045772442706
rocauc: 0.7887323943661971
ogbg-molbace: 0.7887323943661971
BCEWithLogitsLoss: 1.6177971276144187
Statistics on  train
mean_pred: -0.7037322521209717
std_pred: 5.145493030548096
mean_targets: 0.39669421315193176
std_targets: 0.489413857460022
prcauc: 0.9788979769725283
rocauc: 0.9845348173515981
ogbg-molbace: 0.9845348173515981
BCEWithLogitsLoss: 0.16450581672351536
[Epoch 123] ogbg-molbace: 0.746827 test loss: 1.975460
[Epoch 124; Iter    27/   41] train: loss: 0.0352074
[Epoch 124] ogbg-molbace: 0.806593 val loss: 1.187153
[Epoch 124] ogbg-molbace: 0.747522 test loss: 2.098765
[Epoch 125; Iter    16/   41] train: loss: 0.0416448
[Epoch 125] ogbg-molbace: 0.789744 val loss: 1.192727
[Epoch 125] ogbg-molbace: 0.756390 test loss: 2.064565
[Epoch 126; Iter     5/   41] train: loss: 0.0063154
[Epoch 126; Iter    35/   41] train: loss: 0.0037151
[Epoch 126] ogbg-molbace: 0.773626 val loss: 1.265305
[Epoch 126] ogbg-molbace: 0.762998 test loss: 1.896155
[Epoch 127; Iter    24/   41] train: loss: 0.0150322
[Epoch 127] ogbg-molbace: 0.785714 val loss: 1.153510
[Epoch 127] ogbg-molbace: 0.747174 test loss: 2.046099
[Epoch 128; Iter    13/   41] train: loss: 0.0106661
[Epoch 128] ogbg-molbace: 0.789011 val loss: 1.291082
[Epoch 128] ogbg-molbace: 0.758825 test loss: 2.094174
[Epoch 129; Iter     2/   41] train: loss: 0.0049097
[Epoch 129; Iter    32/   41] train: loss: 0.0106655
[Epoch 129] ogbg-molbace: 0.788278 val loss: 1.228156
[Epoch 129] ogbg-molbace: 0.750304 test loss: 2.176528
[Epoch 130; Iter    21/   41] train: loss: 0.0189047
[Epoch 130] ogbg-molbace: 0.776923 val loss: 1.244928
[Epoch 130] ogbg-molbace: 0.753260 test loss: 2.065542
[Epoch 131; Iter    10/   41] train: loss: 0.0346298
[Epoch 131; Iter    40/   41] train: loss: 0.0263400
[Epoch 131] ogbg-molbace: 0.772894 val loss: 1.329081
[Epoch 131] ogbg-molbace: 0.746479 test loss: 2.022393
[Epoch 132; Iter    29/   41] train: loss: 0.0105995
[Epoch 132] ogbg-molbace: 0.784249 val loss: 1.328478
[Epoch 132] ogbg-molbace: 0.756390 test loss: 2.152624
[Epoch 133; Iter    18/   41] train: loss: 0.0197089
[Epoch 133] ogbg-molbace: 0.792308 val loss: 1.299959
[Epoch 133] ogbg-molbace: 0.760389 test loss: 2.029575
[Epoch 134; Iter     7/   41] train: loss: 0.0127030
[Epoch 134; Iter    37/   41] train: loss: 0.0076461
[Epoch 134] ogbg-molbace: 0.789011 val loss: 1.208291
[Epoch 134] ogbg-molbace: 0.751695 test loss: 2.050622
[Epoch 135; Iter    26/   41] train: loss: 0.0054638
[Epoch 135] ogbg-molbace: 0.781319 val loss: 1.397512
[Epoch 135] ogbg-molbace: 0.750652 test loss: 2.166034
[Epoch 136; Iter    15/   41] train: loss: 0.0411399
[Epoch 136] ogbg-molbace: 0.773626 val loss: 1.663394
[Epoch 136] ogbg-molbace: 0.759346 test loss: 2.040112
[Epoch 137; Iter     4/   41] train: loss: 0.0157697
[Epoch 137; Iter    34/   41] train: loss: 0.0120747
[Epoch 137] ogbg-molbace: 0.774359 val loss: 1.394571
[Epoch 137] ogbg-molbace: 0.761433 test loss: 1.976341
[Epoch 138; Iter    23/   41] train: loss: 0.0039572
[Epoch 138] ogbg-molbace: 0.790476 val loss: 1.201200
[Epoch 138] ogbg-molbace: 0.759520 test loss: 2.055768
[Epoch 139; Iter    12/   41] train: loss: 0.0029010
[Epoch 139] ogbg-molbace: 0.761172 val loss: 1.480515
[Epoch 139] ogbg-molbace: 0.768040 test loss: 1.971701
[Epoch 140; Iter     1/   41] train: loss: 0.0163184
[Epoch 140; Iter    31/   41] train: loss: 0.0971119
[Epoch 140] ogbg-molbace: 0.774359 val loss: 1.366480
[Epoch 140] ogbg-molbace: 0.768040 test loss: 1.976516
[Epoch 141; Iter    20/   41] train: loss: 0.0347992
[Epoch 141] ogbg-molbace: 0.764103 val loss: 1.588885
[Epoch 141] ogbg-molbace: 0.770475 test loss: 2.088649
[Epoch 142; Iter     9/   41] train: loss: 0.0182645
[Epoch 142; Iter    39/   41] train: loss: 0.0118867
[Epoch 142] ogbg-molbace: 0.767033 val loss: 1.557176
[Epoch 142] ogbg-molbace: 0.772387 test loss: 2.270466
[Epoch 143; Iter    28/   41] train: loss: 0.0041405
[Epoch 143] ogbg-molbace: 0.781685 val loss: 1.422046
[Epoch 143] ogbg-molbace: 0.767519 test loss: 2.212938
[Epoch 144; Iter    17/   41] train: loss: 0.0017586
[Epoch 144] ogbg-molbace: 0.774359 val loss: 1.472366
[Epoch 144] ogbg-molbace: 0.759346 test loss: 2.202670
[Epoch 145; Iter     6/   41] train: loss: 0.0073957
[Epoch 145; Iter    36/   41] train: loss: 0.0079041
[Epoch 145] ogbg-molbace: 0.783150 val loss: 1.439900
[Epoch 145] ogbg-molbace: 0.752739 test loss: 2.254752
[Epoch 146; Iter    25/   41] train: loss: 0.0024000
[Epoch 146] ogbg-molbace: 0.784615 val loss: 1.366225
[Epoch 146] ogbg-molbace: 0.760737 test loss: 2.252799
[Epoch 147; Iter    14/   41] train: loss: 0.0609788
[Epoch 147] ogbg-molbace: 0.786447 val loss: 1.433075
[Epoch 147] ogbg-molbace: 0.755521 test loss: 2.129462
[Epoch 148; Iter     3/   41] train: loss: 0.0210367
[Epoch 148; Iter    33/   41] train: loss: 0.0283290
[Epoch 148] ogbg-molbace: 0.774359 val loss: 1.608703
[Epoch 148] ogbg-molbace: 0.753608 test loss: 2.145330
[Epoch 149; Iter    22/   41] train: loss: 0.0052453
[Epoch 149] ogbg-molbace: 0.775458 val loss: 1.545909
[Epoch 149] ogbg-molbace: 0.749783 test loss: 2.281364
[Epoch 150; Iter    11/   41] train: loss: 0.0015508
[Epoch 150; Iter    41/   41] train: loss: 0.0033154
[Epoch 150] ogbg-molbace: 0.775092 val loss: 1.450070
[Epoch 150] ogbg-molbace: 0.757086 test loss: 2.199235
[Epoch 151; Iter    30/   41] train: loss: 0.0060586
[Epoch 151] ogbg-molbace: 0.772161 val loss: 1.526374
[Epoch 151] ogbg-molbace: 0.758998 test loss: 2.261145
[Epoch 152; Iter    19/   41] train: loss: 0.0351375
[Epoch 152] ogbg-molbace: 0.783150 val loss: 1.482531
[Epoch 152] ogbg-molbace: 0.752217 test loss: 2.350374
[Epoch 153; Iter     8/   41] train: loss: 0.0039324
[Epoch 153; Iter    38/   41] train: loss: 0.0012867
[Epoch 153] ogbg-molbace: 0.784982 val loss: 1.453902
[Epoch 153] ogbg-molbace: 0.749957 test loss: 2.260659
[Epoch 154; Iter    27/   41] train: loss: 0.0032385
[Epoch 154] ogbg-molbace: 0.784249 val loss: 1.503936
[Epoch 154] ogbg-molbace: 0.750652 test loss: 2.320690
[Epoch 155; Iter    16/   41] train: loss: 0.0123431
[Epoch 155] ogbg-molbace: 0.783516 val loss: 1.441912
[Epoch 155] ogbg-molbace: 0.748913 test loss: 2.189884
[Epoch 156; Iter     5/   41] train: loss: 0.0029637
[Epoch 156; Iter    35/   41] train: loss: 0.0333276
[Epoch 156] ogbg-molbace: 0.782051 val loss: 1.482743
[Epoch 156] ogbg-molbace: 0.750478 test loss: 2.226790
[Epoch 157; Iter    24/   41] train: loss: 0.0062894
[Epoch 157] ogbg-molbace: 0.782784 val loss: 1.493984
[Epoch 157] ogbg-molbace: 0.754999 test loss: 2.246201
[Epoch 158; Iter    13/   41] train: loss: 0.0088693
[Epoch 158] ogbg-molbace: 0.778388 val loss: 1.532137
[Epoch 158] ogbg-molbace: 0.755173 test loss: 2.296790
[Epoch 159; Iter     2/   41] train: loss: 0.0034788
[Epoch 159; Iter    32/   41] train: loss: 0.0229933
[Epoch 159] ogbg-molbace: 0.766300 val loss: 1.468395
[Epoch 159] ogbg-molbace: 0.773431 test loss: 1.993577
[Epoch 160; Iter    21/   41] train: loss: 0.0126691
[Epoch 160] ogbg-molbace: 0.771062 val loss: 1.445859
[Epoch 160] ogbg-molbace: 0.772040 test loss: 2.215309
[Epoch 161; Iter    10/   41] train: loss: 0.0041695
[Epoch 161; Iter    40/   41] train: loss: 0.0034520
[Epoch 161] ogbg-molbace: 0.775458 val loss: 1.536948
[Epoch 161] ogbg-molbace: 0.766475 test loss: 2.238850
[Epoch 162; Iter    29/   41] train: loss: 0.0029702
[Epoch 162] ogbg-molbace: 0.775458 val loss: 1.547613
[Epoch 162] ogbg-molbace: 0.763867 test loss: 2.248315
[Epoch 163; Iter    18/   41] train: loss: 0.0014139
[Epoch 163] ogbg-molbace: 0.776557 val loss: 1.542546
[Epoch 163] ogbg-molbace: 0.760737 test loss: 2.155747
[Epoch 164; Iter     7/   41] train: loss: 0.0040763
[Epoch 164; Iter    37/   41] train: loss: 0.0035648
[Epoch 164] ogbg-molbace: 0.777656 val loss: 1.658133
[Epoch 164] ogbg-molbace: 0.762476 test loss: 2.249472
[Epoch 165; Iter    26/   41] train: loss: 0.0033070
[Epoch 165] ogbg-molbace: 0.778755 val loss: 1.590871
[Epoch 165] ogbg-molbace: 0.755173 test loss: 2.344323
[Epoch 166; Iter    15/   41] train: loss: 0.0083676
[Epoch 166] ogbg-molbace: 0.769963 val loss: 1.654957
[Epoch 166] ogbg-molbace: 0.752565 test loss: 2.237297
[Epoch 167; Iter     4/   41] train: loss: 0.0087777
[Epoch 167; Iter    34/   41] train: loss: 0.0085984
[Epoch 167] ogbg-molbace: 0.768864 val loss: 1.602577
[Epoch 167] ogbg-molbace: 0.753608 test loss: 2.305082
[Epoch 168; Iter    23/   41] train: loss: 0.0012518
[Epoch 168] ogbg-molbace: 0.775458 val loss: 1.555703
[Epoch 168] ogbg-molbace: 0.752913 test loss: 2.310748/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 169; Iter    12/   41] train: loss: 0.0011365
[Epoch 169] ogbg-molbace: 0.773993 val loss: 1.590384
[Epoch 169] ogbg-molbace: 0.758998 test loss: 2.401204
[Epoch 170; Iter     1/   41] train: loss: 0.0078410
[Epoch 170; Iter    31/   41] train: loss: 0.0044423
[Epoch 170] ogbg-molbace: 0.775458 val loss: 1.525554
[Epoch 170] ogbg-molbace: 0.755869 test loss: 2.289711
[Epoch 171; Iter    20/   41] train: loss: 0.0019522
[Epoch 171] ogbg-molbace: 0.779487 val loss: 1.398944
[Epoch 171] ogbg-molbace: 0.762824 test loss: 2.223596
[Epoch 172; Iter     9/   41] train: loss: 0.0015076
[Epoch 172; Iter    39/   41] train: loss: 0.0015584
[Epoch 172] ogbg-molbace: 0.781685 val loss: 1.519163
[Epoch 172] ogbg-molbace: 0.752217 test loss: 2.221709
[Epoch 173; Iter    28/   41] train: loss: 0.0022328
[Epoch 173] ogbg-molbace: 0.775824 val loss: 1.580892
[Epoch 173] ogbg-molbace: 0.749783 test loss: 2.330979
[Epoch 174; Iter    17/   41] train: loss: 0.0048961
[Epoch 174] ogbg-molbace: 0.772527 val loss: 1.480356
[Epoch 174] ogbg-molbace: 0.746305 test loss: 2.315791
[Epoch 175; Iter     6/   41] train: loss: 0.0012208
[Epoch 175; Iter    36/   41] train: loss: 0.0018112
[Epoch 175] ogbg-molbace: 0.782051 val loss: 1.519817
[Epoch 175] ogbg-molbace: 0.751174 test loss: 2.302902
[Epoch 176; Iter    25/   41] train: loss: 0.0013919
[Epoch 176] ogbg-molbace: 0.780586 val loss: 1.437092
[Epoch 176] ogbg-molbace: 0.755521 test loss: 2.292688
[Epoch 177; Iter    14/   41] train: loss: 0.0008065
[Epoch 177] ogbg-molbace: 0.774725 val loss: 1.516769
[Epoch 177] ogbg-molbace: 0.747348 test loss: 2.346308
[Epoch 178; Iter     3/   41] train: loss: 0.0036596
[Epoch 178; Iter    33/   41] train: loss: 0.0164634
[Epoch 178] ogbg-molbace: 0.779853 val loss: 1.558263
[Epoch 178] ogbg-molbace: 0.749435 test loss: 2.421557
[Epoch 179; Iter    22/   41] train: loss: 0.0087189
[Epoch 179] ogbg-molbace: 0.781319 val loss: 1.477725
[Epoch 179] ogbg-molbace: 0.750130 test loss: 2.314935
[Epoch 180; Iter    11/   41] train: loss: 0.0012987
[Epoch 180; Iter    41/   41] train: loss: 0.0023609
[Epoch 180] ogbg-molbace: 0.780220 val loss: 1.552693
[Epoch 180] ogbg-molbace: 0.748913 test loss: 2.332007
[Epoch 181; Iter    30/   41] train: loss: 0.0007524
[Epoch 181] ogbg-molbace: 0.776557 val loss: 1.559237
[Epoch 181] ogbg-molbace: 0.746479 test loss: 2.337135
[Epoch 182; Iter    19/   41] train: loss: 0.0069886
[Epoch 182] ogbg-molbace: 0.771429 val loss: 1.614786
[Epoch 182] ogbg-molbace: 0.755695 test loss: 2.310204
[Epoch 183; Iter     8/   41] train: loss: 0.0205053
[Epoch 183; Iter    38/   41] train: loss: 0.0008979
[Epoch 183] ogbg-molbace: 0.770330 val loss: 1.583422
[Epoch 183] ogbg-molbace: 0.756216 test loss: 2.304506
[Epoch 184; Iter    27/   41] train: loss: 0.0240932
[Epoch 184] ogbg-molbace: 0.777289 val loss: 1.607149
[Epoch 184] ogbg-molbace: 0.749087 test loss: 2.441100
Early stopping criterion based on -ogbg-molbace- that should be max reached after 184 epochs. Best model checkpoint was in epoch 124.
Statistics on  val_best_checkpoint
mean_pred: 0.4920310080051422
std_pred: 4.608753204345703
mean_targets: 0.860927164554596
std_targets: 0.3471740186214447
prcauc: 0.9651341393707301
rocauc: 0.8065934065934066
ogbg-molbace: 0.8065934065934066
BCEWithLogitsLoss: 1.1871534585952759
Statistics on  test
mean_pred: -1.818593144416809
std_pred: 6.644918918609619
mean_targets: 0.5328947305679321
std_targets: 0.5005661249160767
prcauc: 0.7289981617319305
rocauc: 0.7475221700573813
ogbg-molbace: 0.7475221700573813
BCEWithLogitsLoss: 2.098765100042025
Statistics on  train
mean_pred: -1.7033231258392334
std_pred: 6.627303600311279
mean_targets: 0.39669421315193176
std_targets: 0.4894137978553772
prcauc: 0.9997922965155868
rocauc: 0.9998544520547945
ogbg-molbace: 0.9998544520547945
BCEWithLogitsLoss: 0.0205017720042478
All runs completed.
[Epoch 130; Iter     6/   36] train: loss: 0.0104451
[Epoch 130; Iter    36/   36] train: loss: 0.0107790
[Epoch 130] ogbg-molbace: 0.806871 val loss: 0.942366
[Epoch 130] ogbg-molbace: 0.791892 test loss: 1.314995
[Epoch 131; Iter    30/   36] train: loss: 0.0110893
[Epoch 131] ogbg-molbace: 0.799260 val loss: 1.133266
[Epoch 131] ogbg-molbace: 0.801463 test loss: 1.197126
[Epoch 132; Iter    24/   36] train: loss: 0.0571589
[Epoch 132] ogbg-molbace: 0.796406 val loss: 0.956202
[Epoch 132] ogbg-molbace: 0.806880 test loss: 1.330238
[Epoch 133; Iter    18/   36] train: loss: 0.0069024
[Epoch 133] ogbg-molbace: 0.803911 val loss: 1.026318
[Epoch 133] ogbg-molbace: 0.809047 test loss: 1.202983
[Epoch 134; Iter    12/   36] train: loss: 0.0295506
[Epoch 134] ogbg-molbace: 0.794503 val loss: 1.053068
[Epoch 134] ogbg-molbace: 0.803088 test loss: 1.224803
[Epoch 135; Iter     6/   36] train: loss: 0.0044639
[Epoch 135; Iter    36/   36] train: loss: 0.0039478
[Epoch 135] ogbg-molbace: 0.797569 val loss: 1.062727
[Epoch 135] ogbg-molbace: 0.800560 test loss: 1.272211
[Epoch 136; Iter    30/   36] train: loss: 0.1254811
[Epoch 136] ogbg-molbace: 0.796195 val loss: 1.046560
[Epoch 136] ogbg-molbace: 0.798664 test loss: 1.259488
[Epoch 137; Iter    24/   36] train: loss: 0.0062084
[Epoch 137] ogbg-molbace: 0.794292 val loss: 1.015892
[Epoch 137] ogbg-molbace: 0.802275 test loss: 1.336453
[Epoch 138; Iter    18/   36] train: loss: 0.0033940
[Epoch 138] ogbg-molbace: 0.804863 val loss: 1.028674
[Epoch 138] ogbg-molbace: 0.800199 test loss: 1.316754
[Epoch 139; Iter    12/   36] train: loss: 0.0552346
[Epoch 139] ogbg-molbace: 0.797357 val loss: 1.099822
[Epoch 139] ogbg-molbace: 0.804261 test loss: 1.310331
[Epoch 140; Iter     6/   36] train: loss: 0.0134445
[Epoch 140; Iter    36/   36] train: loss: 0.0068247
[Epoch 140] ogbg-molbace: 0.798097 val loss: 1.043920
[Epoch 140] ogbg-molbace: 0.794059 test loss: 1.339133
[Epoch 141; Iter    30/   36] train: loss: 0.0031669
[Epoch 141] ogbg-molbace: 0.801374 val loss: 1.040799
[Epoch 141] ogbg-molbace: 0.808415 test loss: 1.353066
[Epoch 142; Iter    24/   36] train: loss: 0.0181490
[Epoch 142] ogbg-molbace: 0.802431 val loss: 1.029818
[Epoch 142] ogbg-molbace: 0.807963 test loss: 1.374662
[Epoch 143; Iter    18/   36] train: loss: 0.0315285
[Epoch 143] ogbg-molbace: 0.798626 val loss: 1.235461
[Epoch 143] ogbg-molbace: 0.804352 test loss: 1.371427
[Epoch 144; Iter    12/   36] train: loss: 0.0051959
[Epoch 144] ogbg-molbace: 0.793869 val loss: 1.237914
[Epoch 144] ogbg-molbace: 0.804081 test loss: 1.349726
[Epoch 145; Iter     6/   36] train: loss: 0.0027538
[Epoch 145; Iter    36/   36] train: loss: 0.0794194
[Epoch 145] ogbg-molbace: 0.796934 val loss: 1.008450
[Epoch 145] ogbg-molbace: 0.803449 test loss: 1.319573
[Epoch 146; Iter    30/   36] train: loss: 0.0311088
[Epoch 146] ogbg-molbace: 0.790592 val loss: 1.201586
[Epoch 146] ogbg-molbace: 0.798935 test loss: 1.357611
[Epoch 147; Iter    24/   36] train: loss: 0.0115546
[Epoch 147] ogbg-molbace: 0.792495 val loss: 1.141416
[Epoch 147] ogbg-molbace: 0.806248 test loss: 1.259262
[Epoch 148; Iter    18/   36] train: loss: 0.0022719
[Epoch 148] ogbg-molbace: 0.792706 val loss: 1.067818
[Epoch 148] ogbg-molbace: 0.799928 test loss: 1.415214
[Epoch 149; Iter    12/   36] train: loss: 0.0098306
[Epoch 149] ogbg-molbace: 0.798626 val loss: 1.029680
[Epoch 149] ogbg-molbace: 0.799928 test loss: 1.417526
[Epoch 150; Iter     6/   36] train: loss: 0.0181133
[Epoch 150; Iter    36/   36] train: loss: 0.0538716
[Epoch 150] ogbg-molbace: 0.814482 val loss: 1.146063
[Epoch 150] ogbg-molbace: 0.804984 test loss: 1.283622
[Epoch 151; Iter    30/   36] train: loss: 0.0036132
[Epoch 151] ogbg-molbace: 0.790909 val loss: 1.021219
[Epoch 151] ogbg-molbace: 0.791260 test loss: 1.499995
[Epoch 152; Iter    24/   36] train: loss: 0.0070293
[Epoch 152] ogbg-molbace: 0.794503 val loss: 1.105345
[Epoch 152] ogbg-molbace: 0.802004 test loss: 1.340156
[Epoch 153; Iter    18/   36] train: loss: 0.0477137
[Epoch 153] ogbg-molbace: 0.805814 val loss: 1.120569
[Epoch 153] ogbg-molbace: 0.799928 test loss: 1.388845
[Epoch 154; Iter    12/   36] train: loss: 0.0034061
[Epoch 154] ogbg-molbace: 0.799683 val loss: 1.126719
[Epoch 154] ogbg-molbace: 0.807692 test loss: 1.303777
[Epoch 155; Iter     6/   36] train: loss: 0.0127699
[Epoch 155; Iter    36/   36] train: loss: 0.0047006
[Epoch 155] ogbg-molbace: 0.801268 val loss: 1.106175
[Epoch 155] ogbg-molbace: 0.803449 test loss: 1.406063
[Epoch 156; Iter    30/   36] train: loss: 0.0020035
[Epoch 156] ogbg-molbace: 0.802748 val loss: 1.150964
[Epoch 156] ogbg-molbace: 0.803991 test loss: 1.478374
[Epoch 157; Iter    24/   36] train: loss: 0.0031300
[Epoch 157] ogbg-molbace: 0.801268 val loss: 1.280555
[Epoch 157] ogbg-molbace: 0.797400 test loss: 1.383221
[Epoch 158; Iter    18/   36] train: loss: 0.0023560
[Epoch 158] ogbg-molbace: 0.801057 val loss: 1.049964
[Epoch 158] ogbg-molbace: 0.795414 test loss: 1.460598
[Epoch 159; Iter    12/   36] train: loss: 0.0010351
[Epoch 159] ogbg-molbace: 0.797886 val loss: 1.213784
[Epoch 159] ogbg-molbace: 0.794150 test loss: 1.461811
[Epoch 160; Iter     6/   36] train: loss: 0.0074269
[Epoch 160; Iter    36/   36] train: loss: 0.0169939
[Epoch 160] ogbg-molbace: 0.798732 val loss: 1.096603
[Epoch 160] ogbg-molbace: 0.799205 test loss: 1.474249
[Epoch 161; Iter    30/   36] train: loss: 0.0048997
[Epoch 161] ogbg-molbace: 0.800951 val loss: 1.067455
[Epoch 161] ogbg-molbace: 0.789003 test loss: 1.453366
[Epoch 162; Iter    24/   36] train: loss: 0.0008231
[Epoch 162] ogbg-molbace: 0.801163 val loss: 1.133055
[Epoch 162] ogbg-molbace: 0.796948 test loss: 1.455862
[Epoch 163; Iter    18/   36] train: loss: 0.0010593
[Epoch 163] ogbg-molbace: 0.802326 val loss: 1.114927
[Epoch 163] ogbg-molbace: 0.798032 test loss: 1.496243
[Epoch 164; Iter    12/   36] train: loss: 0.0012549
[Epoch 164] ogbg-molbace: 0.801691 val loss: 1.142272
[Epoch 164] ogbg-molbace: 0.796587 test loss: 1.466627
[Epoch 165; Iter     6/   36] train: loss: 0.0028979
[Epoch 165; Iter    36/   36] train: loss: 0.0007361
[Epoch 165] ogbg-molbace: 0.799049 val loss: 1.171284
[Epoch 165] ogbg-molbace: 0.802185 test loss: 1.413950
[Epoch 166; Iter    30/   36] train: loss: 0.0162155
[Epoch 166] ogbg-molbace: 0.799577 val loss: 1.142306
[Epoch 166] ogbg-molbace: 0.800650 test loss: 1.401443
[Epoch 167; Iter    24/   36] train: loss: 0.0016444
[Epoch 167] ogbg-molbace: 0.798203 val loss: 1.176516
[Epoch 167] ogbg-molbace: 0.797671 test loss: 1.471586
[Epoch 168; Iter    18/   36] train: loss: 0.0028882
[Epoch 168] ogbg-molbace: 0.801691 val loss: 1.087447
[Epoch 168] ogbg-molbace: 0.793969 test loss: 1.667357
[Epoch 169; Iter    12/   36] train: loss: 0.0023342
[Epoch 169] ogbg-molbace: 0.793763 val loss: 1.137337
[Epoch 169] ogbg-molbace: 0.791351 test loss: 1.563804
[Epoch 170; Iter     6/   36] train: loss: 0.0052014
[Epoch 170; Iter    36/   36] train: loss: 0.0057265
[Epoch 170] ogbg-molbace: 0.799894 val loss: 1.189047
[Epoch 170] ogbg-molbace: 0.792705 test loss: 1.433911
[Epoch 171; Iter    30/   36] train: loss: 0.0007197
[Epoch 171] ogbg-molbace: 0.796195 val loss: 1.201757
[Epoch 171] ogbg-molbace: 0.796948 test loss: 1.440222
[Epoch 172; Iter    24/   36] train: loss: 0.0011369
[Epoch 172] ogbg-molbace: 0.803911 val loss: 1.114793
[Epoch 172] ogbg-molbace: 0.787198 test loss: 1.587051
[Epoch 173; Iter    18/   36] train: loss: 0.0066433
[Epoch 173] ogbg-molbace: 0.801057 val loss: 1.216760
[Epoch 173] ogbg-molbace: 0.792886 test loss: 1.510132
[Epoch 174; Iter    12/   36] train: loss: 0.0009583
[Epoch 174] ogbg-molbace: 0.794292 val loss: 1.200187
[Epoch 174] ogbg-molbace: 0.793427 test loss: 1.510359
[Epoch 175; Iter     6/   36] train: loss: 0.0007513
[Epoch 175; Iter    36/   36] train: loss: 0.0057475
[Epoch 175] ogbg-molbace: 0.792600 val loss: 1.313560
[Epoch 175] ogbg-molbace: 0.795504 test loss: 1.456308
[Epoch 176; Iter    30/   36] train: loss: 0.0024108
[Epoch 176] ogbg-molbace: 0.795772 val loss: 1.165194
[Epoch 176] ogbg-molbace: 0.792976 test loss: 1.518902
[Epoch 130; Iter     6/   36] train: loss: 0.1513186
[Epoch 130; Iter    36/   36] train: loss: 0.0188225
[Epoch 130] ogbg-molbace: 0.814799 val loss: 1.104049
[Epoch 130] ogbg-molbace: 0.833604 test loss: 1.227228
[Epoch 131; Iter    30/   36] train: loss: 0.3809880
[Epoch 131] ogbg-molbace: 0.792389 val loss: 1.207407
[Epoch 131] ogbg-molbace: 0.838750 test loss: 1.177677
[Epoch 132; Iter    24/   36] train: loss: 0.0249059
[Epoch 132] ogbg-molbace: 0.808562 val loss: 0.983738
[Epoch 132] ogbg-molbace: 0.845522 test loss: 1.136706
[Epoch 133; Iter    18/   36] train: loss: 0.0055010
[Epoch 133] ogbg-molbace: 0.810677 val loss: 1.065671
[Epoch 133] ogbg-molbace: 0.843174 test loss: 1.126647
[Epoch 134; Iter    12/   36] train: loss: 0.0086389
[Epoch 134] ogbg-molbace: 0.808562 val loss: 1.022630
[Epoch 134] ogbg-molbace: 0.844438 test loss: 1.148420
[Epoch 135; Iter     6/   36] train: loss: 0.0210895
[Epoch 135; Iter    36/   36] train: loss: 0.0011858
[Epoch 135] ogbg-molbace: 0.816068 val loss: 1.081573
[Epoch 135] ogbg-molbace: 0.846425 test loss: 1.129785
[Epoch 136; Iter    30/   36] train: loss: 0.0231690
[Epoch 136] ogbg-molbace: 0.811099 val loss: 1.042717
[Epoch 136] ogbg-molbace: 0.838480 test loss: 1.200312
[Epoch 137; Iter    24/   36] train: loss: 0.0248143
[Epoch 137] ogbg-molbace: 0.817759 val loss: 1.057056
[Epoch 137] ogbg-molbace: 0.845702 test loss: 1.119004
[Epoch 138; Iter    18/   36] train: loss: 0.0224794
[Epoch 138] ogbg-molbace: 0.811099 val loss: 1.047250
[Epoch 138] ogbg-molbace: 0.835771 test loss: 1.212957
[Epoch 139; Iter    12/   36] train: loss: 0.0040285
[Epoch 139] ogbg-molbace: 0.809725 val loss: 1.085482
[Epoch 139] ogbg-molbace: 0.834597 test loss: 1.307971
[Epoch 140; Iter     6/   36] train: loss: 0.0059194
[Epoch 140; Iter    36/   36] train: loss: 0.0013190
[Epoch 140] ogbg-molbace: 0.803171 val loss: 1.302617
[Epoch 140] ogbg-molbace: 0.833965 test loss: 1.239736
[Epoch 141; Iter    30/   36] train: loss: 0.0028655
[Epoch 141] ogbg-molbace: 0.802220 val loss: 1.051249
[Epoch 141] ogbg-molbace: 0.837757 test loss: 1.267192
[Epoch 142; Iter    24/   36] train: loss: 0.0030060
[Epoch 142] ogbg-molbace: 0.811839 val loss: 1.102750
[Epoch 142] ogbg-molbace: 0.841549 test loss: 1.203301
[Epoch 143; Iter    18/   36] train: loss: 0.0349054
[Epoch 143] ogbg-molbace: 0.798520 val loss: 1.155347
[Epoch 143] ogbg-molbace: 0.830264 test loss: 1.310879
[Epoch 144; Iter    12/   36] train: loss: 0.0077145
[Epoch 144] ogbg-molbace: 0.799471 val loss: 1.178093
[Epoch 144] ogbg-molbace: 0.839292 test loss: 1.203150
[Epoch 145; Iter     6/   36] train: loss: 0.0056270
[Epoch 145; Iter    36/   36] train: loss: 0.0093499
[Epoch 145] ogbg-molbace: 0.802008 val loss: 1.225727
[Epoch 145] ogbg-molbace: 0.831528 test loss: 1.222745
[Epoch 146; Iter    30/   36] train: loss: 0.0028590
[Epoch 146] ogbg-molbace: 0.805497 val loss: 1.173323
[Epoch 146] ogbg-molbace: 0.834146 test loss: 1.238805
[Epoch 147; Iter    24/   36] train: loss: 0.0136455
[Epoch 147] ogbg-molbace: 0.797357 val loss: 1.143605
[Epoch 147] ogbg-molbace: 0.836042 test loss: 1.321847
[Epoch 148; Iter    18/   36] train: loss: 0.0184088
[Epoch 148] ogbg-molbace: 0.804968 val loss: 1.321454
[Epoch 148] ogbg-molbace: 0.838480 test loss: 1.261802
[Epoch 149; Iter    12/   36] train: loss: 0.0096973
[Epoch 149] ogbg-molbace: 0.806237 val loss: 1.040745
[Epoch 149] ogbg-molbace: 0.845973 test loss: 1.271044
[Epoch 150; Iter     6/   36] train: loss: 0.0100074
[Epoch 150; Iter    36/   36] train: loss: 0.0450476
[Epoch 150] ogbg-molbace: 0.799260 val loss: 1.174919
[Epoch 150] ogbg-molbace: 0.840917 test loss: 1.237137
[Epoch 151; Iter    30/   36] train: loss: 0.0030128
[Epoch 151] ogbg-molbace: 0.814693 val loss: 1.070716
[Epoch 151] ogbg-molbace: 0.841820 test loss: 1.263079
[Epoch 152; Iter    24/   36] train: loss: 0.1131666
[Epoch 152] ogbg-molbace: 0.810782 val loss: 1.192161
[Epoch 152] ogbg-molbace: 0.848321 test loss: 1.184529
[Epoch 153; Iter    18/   36] train: loss: 0.0054491
[Epoch 153] ogbg-molbace: 0.818816 val loss: 1.060186
[Epoch 153] ogbg-molbace: 0.851120 test loss: 1.159694
[Epoch 154; Iter    12/   36] train: loss: 0.0066906
[Epoch 154] ogbg-molbace: 0.804757 val loss: 1.287015
[Epoch 154] ogbg-molbace: 0.843897 test loss: 1.153114
[Epoch 155; Iter     6/   36] train: loss: 0.0027372
[Epoch 155; Iter    36/   36] train: loss: 0.0051110
[Epoch 155] ogbg-molbace: 0.805180 val loss: 1.224365
[Epoch 155] ogbg-molbace: 0.847508 test loss: 1.196059
[Epoch 156; Iter    30/   36] train: loss: 0.0130432
[Epoch 156] ogbg-molbace: 0.801480 val loss: 1.401020
[Epoch 156] ogbg-molbace: 0.846605 test loss: 1.220345
[Epoch 157; Iter    24/   36] train: loss: 0.0059226
[Epoch 157] ogbg-molbace: 0.814799 val loss: 1.087823
[Epoch 157] ogbg-molbace: 0.843987 test loss: 1.277759
[Epoch 158; Iter    18/   36] train: loss: 0.0020479
[Epoch 158] ogbg-molbace: 0.820190 val loss: 1.140377
[Epoch 158] ogbg-molbace: 0.846876 test loss: 1.251886
[Epoch 159; Iter    12/   36] train: loss: 0.0265318
[Epoch 159] ogbg-molbace: 0.842706 val loss: 1.119403
[Epoch 159] ogbg-molbace: 0.844709 test loss: 1.207441
[Epoch 160; Iter     6/   36] train: loss: 0.0061392
[Epoch 160; Iter    36/   36] train: loss: 0.0021562
[Epoch 160] ogbg-molbace: 0.817653 val loss: 1.086259
[Epoch 160] ogbg-molbace: 0.843626 test loss: 1.294663
[Epoch 161; Iter    30/   36] train: loss: 0.0099767
[Epoch 161] ogbg-molbace: 0.823996 val loss: 1.132723
[Epoch 161] ogbg-molbace: 0.844529 test loss: 1.280746
[Epoch 162; Iter    24/   36] train: loss: 0.0013950
[Epoch 162] ogbg-molbace: 0.818393 val loss: 1.190966
[Epoch 162] ogbg-molbace: 0.847147 test loss: 1.260162
[Epoch 163; Iter    18/   36] train: loss: 0.0082004
[Epoch 163] ogbg-molbace: 0.818076 val loss: 1.210180
[Epoch 163] ogbg-molbace: 0.847779 test loss: 1.229830
[Epoch 164; Iter    12/   36] train: loss: 0.0014066
[Epoch 164] ogbg-molbace: 0.813742 val loss: 1.273645
[Epoch 164] ogbg-molbace: 0.849314 test loss: 1.242145
[Epoch 165; Iter     6/   36] train: loss: 0.1764920
[Epoch 165; Iter    36/   36] train: loss: 0.0168525
[Epoch 165] ogbg-molbace: 0.819450 val loss: 1.189450
[Epoch 165] ogbg-molbace: 0.850849 test loss: 1.226295
[Epoch 166; Iter    30/   36] train: loss: 0.0092386
[Epoch 166] ogbg-molbace: 0.813848 val loss: 1.182207
[Epoch 166] ogbg-molbace: 0.845251 test loss: 1.271015
[Epoch 167; Iter    24/   36] train: loss: 0.0024601
[Epoch 167] ogbg-molbace: 0.819133 val loss: 1.286347
[Epoch 167] ogbg-molbace: 0.848140 test loss: 1.211021
[Epoch 168; Iter    18/   36] train: loss: 0.0034569
[Epoch 168] ogbg-molbace: 0.811628 val loss: 1.322236
[Epoch 168] ogbg-molbace: 0.844800 test loss: 1.244196
[Epoch 169; Iter    12/   36] train: loss: 0.0408422
[Epoch 169] ogbg-molbace: 0.812156 val loss: 1.278123
[Epoch 169] ogbg-molbace: 0.843355 test loss: 1.264976
[Epoch 170; Iter     6/   36] train: loss: 0.0009884
[Epoch 170; Iter    36/   36] train: loss: 0.0234690
[Epoch 170] ogbg-molbace: 0.808457 val loss: 1.176543
[Epoch 170] ogbg-molbace: 0.841820 test loss: 1.319165
[Epoch 171; Iter    30/   36] train: loss: 0.0043707
[Epoch 171] ogbg-molbace: 0.810888 val loss: 1.342569
[Epoch 171] ogbg-molbace: 0.843716 test loss: 1.286484
[Epoch 172; Iter    24/   36] train: loss: 0.0009338
[Epoch 172] ogbg-molbace: 0.818288 val loss: 1.195825
[Epoch 172] ogbg-molbace: 0.842633 test loss: 1.306302
[Epoch 173; Iter    18/   36] train: loss: 0.0022493
[Epoch 173] ogbg-molbace: 0.808668 val loss: 1.290777
[Epoch 173] ogbg-molbace: 0.835139 test loss: 1.282334
[Epoch 174; Iter    12/   36] train: loss: 0.0040866
[Epoch 174] ogbg-molbace: 0.810994 val loss: 1.189674
[Epoch 174] ogbg-molbace: 0.841098 test loss: 1.354565
[Epoch 175; Iter     6/   36] train: loss: 0.0022138
[Epoch 175; Iter    36/   36] train: loss: 0.0048440
[Epoch 175] ogbg-molbace: 0.802537 val loss: 1.174435
[Epoch 175] ogbg-molbace: 0.835952 test loss: 1.361906
[Epoch 176; Iter    30/   36] train: loss: 0.0013183
[Epoch 176] ogbg-molbace: 0.807822 val loss: 1.303416
[Epoch 176] ogbg-molbace: 0.841640 test loss: 1.308027
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 177; Iter    24/   36] train: loss: 0.0068825
[Epoch 177] ogbg-molbace: 0.795349 val loss: 1.184185
[Epoch 177] ogbg-molbace: 0.790809 test loss: 1.517023
[Epoch 178; Iter    18/   36] train: loss: 0.0013371
[Epoch 178] ogbg-molbace: 0.792495 val loss: 1.270461
[Epoch 178] ogbg-molbace: 0.794420 test loss: 1.545690
Early stopping criterion based on -ogbg-molbace- that should be max reached after 178 epochs. Best model checkpoint was in epoch 118.
Statistics on  val_best_checkpoint
mean_pred: -2.060109853744507
std_pred: 5.586322784423828
mean_targets: 0.24229073524475098
std_targets: 0.4294162094593048
prcauc: 0.5942822671721333
rocauc: 0.8262156448202959
ogbg-molbace: 0.8262156448202959
BCEWithLogitsLoss: 0.8741473220288754
Statistics on  test
mean_pred: 1.1692795753479004
std_pred: 7.537425994873047
mean_targets: 0.6872246265411377
std_targets: 0.4646482765674591
prcauc: 0.8515087490702116
rocauc: 0.8044420368364029
ogbg-molbace: 0.8044420368364029
BCEWithLogitsLoss: 1.3568545803427696
Statistics on  train
mean_pred: -1.1142607927322388
std_pred: 7.340978622436523
mean_targets: 0.45325779914855957
std_targets: 0.49804556369781494
prcauc: 0.9999869791289904
rocauc: 0.9999892055267703
ogbg-molbace: 0.9999892055267703
BCEWithLogitsLoss: 0.011819272389402613
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 177; Iter    24/   36] train: loss: 0.0022513
[Epoch 177] ogbg-molbace: 0.812262 val loss: 1.235279
[Epoch 177] ogbg-molbace: 0.840014 test loss: 1.350956
[Epoch 178; Iter    18/   36] train: loss: 0.0020745
[Epoch 178] ogbg-molbace: 0.802431 val loss: 1.301913
[Epoch 178] ogbg-molbace: 0.840917 test loss: 1.364820
[Epoch 179; Iter    12/   36] train: loss: 0.0014446
[Epoch 179] ogbg-molbace: 0.814588 val loss: 1.305099
[Epoch 179] ogbg-molbace: 0.845702 test loss: 1.264094
[Epoch 180; Iter     6/   36] train: loss: 0.0128344
[Epoch 180; Iter    36/   36] train: loss: 0.0108781
[Epoch 180] ogbg-molbace: 0.814588 val loss: 1.237727
[Epoch 180] ogbg-molbace: 0.842452 test loss: 1.299405
[Epoch 181; Iter    30/   36] train: loss: 0.0242656
[Epoch 181] ogbg-molbace: 0.811628 val loss: 1.298076
[Epoch 181] ogbg-molbace: 0.842272 test loss: 1.307261
[Epoch 182; Iter    24/   36] train: loss: 0.0005901
[Epoch 182] ogbg-molbace: 0.805920 val loss: 1.264406
[Epoch 182] ogbg-molbace: 0.838480 test loss: 1.356958
[Epoch 183; Iter    18/   36] train: loss: 0.0019490
[Epoch 183] ogbg-molbace: 0.809091 val loss: 1.330524
[Epoch 183] ogbg-molbace: 0.837848 test loss: 1.347558
[Epoch 184; Iter    12/   36] train: loss: 0.0028847
[Epoch 184] ogbg-molbace: 0.808245 val loss: 1.264914
[Epoch 184] ogbg-molbace: 0.838841 test loss: 1.372829
[Epoch 185; Iter     6/   36] train: loss: 0.0008384
[Epoch 185; Iter    36/   36] train: loss: 0.0255610
[Epoch 185] ogbg-molbace: 0.808985 val loss: 1.276052
[Epoch 185] ogbg-molbace: 0.837757 test loss: 1.410383
[Epoch 186; Iter    30/   36] train: loss: 0.0069491
[Epoch 186] ogbg-molbace: 0.808457 val loss: 1.368322
[Epoch 186] ogbg-molbace: 0.838389 test loss: 1.318938
[Epoch 187; Iter    24/   36] train: loss: 0.0073151
[Epoch 187] ogbg-molbace: 0.805814 val loss: 1.284623
[Epoch 187] ogbg-molbace: 0.839021 test loss: 1.345262
[Epoch 188; Iter    18/   36] train: loss: 0.0154288
[Epoch 188] ogbg-molbace: 0.809302 val loss: 1.309678
[Epoch 188] ogbg-molbace: 0.841820 test loss: 1.316356
[Epoch 189; Iter    12/   36] train: loss: 0.0014650
[Epoch 189] ogbg-molbace: 0.809514 val loss: 1.278518
[Epoch 189] ogbg-molbace: 0.841640 test loss: 1.317428
[Epoch 190; Iter     6/   36] train: loss: 0.0025746
[Epoch 190; Iter    36/   36] train: loss: 0.0014266
[Epoch 190] ogbg-molbace: 0.809619 val loss: 1.180175
[Epoch 190] ogbg-molbace: 0.840195 test loss: 1.428885
[Epoch 191; Iter    30/   36] train: loss: 0.0020171
[Epoch 191] ogbg-molbace: 0.808774 val loss: 1.211507
[Epoch 191] ogbg-molbace: 0.840105 test loss: 1.427180
[Epoch 192; Iter    24/   36] train: loss: 0.0008095
[Epoch 192] ogbg-molbace: 0.812156 val loss: 1.254375
[Epoch 192] ogbg-molbace: 0.840917 test loss: 1.375330
[Epoch 193; Iter    18/   36] train: loss: 0.0047397
[Epoch 193] ogbg-molbace: 0.805285 val loss: 1.369238
[Epoch 193] ogbg-molbace: 0.836764 test loss: 1.396140
[Epoch 194; Iter    12/   36] train: loss: 0.0111000
[Epoch 194] ogbg-molbace: 0.808985 val loss: 1.263493
[Epoch 194] ogbg-molbace: 0.836674 test loss: 1.448895
[Epoch 195; Iter     6/   36] train: loss: 0.0127764
[Epoch 195; Iter    36/   36] train: loss: 0.0089512
[Epoch 195] ogbg-molbace: 0.809091 val loss: 1.264871
[Epoch 195] ogbg-molbace: 0.840195 test loss: 1.449743
[Epoch 196; Iter    30/   36] train: loss: 0.0034798
[Epoch 196] ogbg-molbace: 0.810359 val loss: 1.306660
[Epoch 196] ogbg-molbace: 0.839202 test loss: 1.423138
[Epoch 197; Iter    24/   36] train: loss: 0.0013226
[Epoch 197] ogbg-molbace: 0.809197 val loss: 1.421054
[Epoch 197] ogbg-molbace: 0.841459 test loss: 1.350101
[Epoch 198; Iter    18/   36] train: loss: 0.0019852
[Epoch 198] ogbg-molbace: 0.811522 val loss: 1.289665
[Epoch 198] ogbg-molbace: 0.841098 test loss: 1.345191
[Epoch 199; Iter    12/   36] train: loss: 0.0006492
[Epoch 199] ogbg-molbace: 0.811416 val loss: 1.310579
[Epoch 199] ogbg-molbace: 0.839924 test loss: 1.375280
[Epoch 200; Iter     6/   36] train: loss: 0.0019300
[Epoch 200; Iter    36/   36] train: loss: 0.0018849
[Epoch 200] ogbg-molbace: 0.810465 val loss: 1.381561
[Epoch 200] ogbg-molbace: 0.841369 test loss: 1.313557
[Epoch 201; Iter    30/   36] train: loss: 0.0031435
[Epoch 201] ogbg-molbace: 0.805603 val loss: 1.255844
[Epoch 201] ogbg-molbace: 0.836854 test loss: 1.413507
[Epoch 202; Iter    24/   36] train: loss: 0.0010916
[Epoch 202] ogbg-molbace: 0.806765 val loss: 1.243108
[Epoch 202] ogbg-molbace: 0.839653 test loss: 1.454608
[Epoch 203; Iter    18/   36] train: loss: 0.0019990
[Epoch 203] ogbg-molbace: 0.809302 val loss: 1.343957
[Epoch 203] ogbg-molbace: 0.840466 test loss: 1.322232
[Epoch 204; Iter    12/   36] train: loss: 0.0016242
[Epoch 204] ogbg-molbace: 0.810888 val loss: 1.285674
[Epoch 204] ogbg-molbace: 0.838389 test loss: 1.385693
[Epoch 205; Iter     6/   36] train: loss: 0.0005800
[Epoch 205; Iter    36/   36] train: loss: 0.0013004
[Epoch 205] ogbg-molbace: 0.806871 val loss: 1.223733
[Epoch 205] ogbg-molbace: 0.832701 test loss: 1.558326
[Epoch 206; Iter    30/   36] train: loss: 0.0027850
[Epoch 206] ogbg-molbace: 0.807294 val loss: 1.238924
[Epoch 206] ogbg-molbace: 0.835049 test loss: 1.495518
[Epoch 207; Iter    24/   36] train: loss: 0.0006995
[Epoch 207] ogbg-molbace: 0.805285 val loss: 1.320784
[Epoch 207] ogbg-molbace: 0.839202 test loss: 1.452461
[Epoch 208; Iter    18/   36] train: loss: 0.0005628
[Epoch 208] ogbg-molbace: 0.809619 val loss: 1.335556
[Epoch 208] ogbg-molbace: 0.837757 test loss: 1.444833
[Epoch 209; Iter    12/   36] train: loss: 0.0065299
[Epoch 209] ogbg-molbace: 0.810254 val loss: 1.410930
[Epoch 209] ogbg-molbace: 0.839473 test loss: 1.400690
[Epoch 210; Iter     6/   36] train: loss: 0.0039511
[Epoch 210; Iter    36/   36] train: loss: 0.0068101
[Epoch 210] ogbg-molbace: 0.809514 val loss: 1.353579
[Epoch 210] ogbg-molbace: 0.837667 test loss: 1.398777
[Epoch 211; Iter    30/   36] train: loss: 0.0008240
[Epoch 211] ogbg-molbace: 0.808985 val loss: 1.247491
[Epoch 211] ogbg-molbace: 0.837486 test loss: 1.491551
[Epoch 212; Iter    24/   36] train: loss: 0.0010063
[Epoch 212] ogbg-molbace: 0.809937 val loss: 1.285922
[Epoch 212] ogbg-molbace: 0.838389 test loss: 1.386244
[Epoch 213; Iter    18/   36] train: loss: 0.0005817
[Epoch 213] ogbg-molbace: 0.807400 val loss: 1.379305
[Epoch 213] ogbg-molbace: 0.838750 test loss: 1.368326
[Epoch 214; Iter    12/   36] train: loss: 0.0010722
[Epoch 214] ogbg-molbace: 0.808140 val loss: 1.342830
[Epoch 214] ogbg-molbace: 0.835139 test loss: 1.428468
[Epoch 215; Iter     6/   36] train: loss: 0.0012424
[Epoch 215; Iter    36/   36] train: loss: 0.0134836
[Epoch 215] ogbg-molbace: 0.814905 val loss: 1.155192
[Epoch 215] ogbg-molbace: 0.830354 test loss: 1.560887
[Epoch 216; Iter    30/   36] train: loss: 0.0016509
[Epoch 216] ogbg-molbace: 0.811945 val loss: 1.349960
[Epoch 216] ogbg-molbace: 0.832792 test loss: 1.484727
[Epoch 217; Iter    24/   36] train: loss: 0.0015406
[Epoch 217] ogbg-molbace: 0.809937 val loss: 1.302567
[Epoch 217] ogbg-molbace: 0.834958 test loss: 1.461061
[Epoch 218; Iter    18/   36] train: loss: 0.0003350
[Epoch 218] ogbg-molbace: 0.809197 val loss: 1.268824
[Epoch 218] ogbg-molbace: 0.834958 test loss: 1.419898
[Epoch 219; Iter    12/   36] train: loss: 0.0005337
[Epoch 219] ogbg-molbace: 0.812156 val loss: 1.292577
[Epoch 219] ogbg-molbace: 0.833694 test loss: 1.423637
Early stopping criterion based on -ogbg-molbace- that should be max reached after 219 epochs. Best model checkpoint was in epoch 159.
Statistics on  val_best_checkpoint
mean_pred: -1.6287683248519897
std_pred: 6.3488450050354
mean_targets: 0.24229073524475098
std_targets: 0.4294162094593048
prcauc: 0.5773770278288292
rocauc: 0.8427061310782242
ogbg-molbace: 0.8427061310782242
BCEWithLogitsLoss: 1.1194032207131386
Statistics on  test
mean_pred: 1.4422509670257568
std_pred: 8.501394271850586
mean_targets: 0.6872246265411377
std_targets: 0.4646482765674591
prcauc: 0.8758731704263946
rocauc: 0.8447092813289996
ogbg-molbace: 0.8447092813289996
BCEWithLogitsLoss: 1.2074411627836525
Statistics on  train
mean_pred: -0.5261473059654236
std_pred: 8.398323059082031
mean_targets: 0.45325779914855957
std_targets: 0.49804556369781494
prcauc: 0.9998051117032278
rocauc: 0.9998452792170409
ogbg-molbace: 0.9998452792170409
BCEWithLogitsLoss: 0.008822838538132297
All runs completed.
