>>> Starting run for dataset: bbbp
Running SCAFF configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.7.yml --seed 6 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.6.yml --seed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.6/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.6_4_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.6
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregatorsTraceback (most recent call last):
  File "/workspace/train.py", line 691, in <module>
: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6927094
    train(args)
  File "/workspace/train.py", line 297, in train
    return train_ogbg(args, device, metrics_dict)
  File "/workspace/train.py", line 481, in train_ogbg
    val_metrics = trainer.train(train_loader, val_loader, test_loader)
  File "/workspace/trainer/trainer.py", line 91, in train
    val_metrics = self.predict(val_loader, epoch)
  File "/workspace/trainer/trainer.py", line 182, in predict
    total_metrics = self.evaluate_metrics(epoch_predictions, epoch_targets, val=True)
  File "/workspace/trainer/trainer.py", line 201, in evaluate_metrics
    metrics[key] = metric(predictions, targets).item()
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/trainer/metrics.py", line 184, in forward
    raise RuntimeError('No positively labeled data available. Cannot compute Average Precision.')
RuntimeError: No positively labeled data available. Cannot compute Average Precision.
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.6/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.6_6_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.6
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6916275
Traceback (most recent call last):
  File "/workspace/train.py", line 691, in <module>
    train(args)
  File "/workspace/train.py", line 297, in train
    return train_ogbg(args, device, metrics_dict)
  File "/workspace/train.py", line 481, in train_ogbg
    val_metrics = trainer.train(train_loader, val_loader, test_loader)
  File "/workspace/trainer/trainer.py", line 91, in train
    val_metrics = self.predict(val_loader, epoch)
  File "/workspace/trainer/trainer.py", line 182, in predict
    total_metrics = self.evaluate_metrics(epoch_predictions, epoch_targets, val=True)
  File "/workspace/trainer/trainer.py", line 201, in evaluate_metrics
    metrics[key] = metric(predictions, targets).item()
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/trainer/metrics.py", line 184, in forward
    raise RuntimeError('No positively labeled data available. Cannot compute Average Precision.')
RuntimeError: No positively labeled data available. Cannot compute Average Precision.
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.6/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.6_5_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.6
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregatorsTraceback (most recent call last):
  File "/workspace/train.py", line 691, in <module>
: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   41] train: loss: 0.6912166
    train(args)
  File "/workspace/train.py", line 297, in train
    return train_ogbg(args, device, metrics_dict)
  File "/workspace/train.py", line 481, in train_ogbg
    val_metrics = trainer.train(train_loader, val_loader, test_loader)
  File "/workspace/trainer/trainer.py", line 91, in train
    val_metrics = self.predict(val_loader, epoch)
  File "/workspace/trainer/trainer.py", line 182, in predict
    total_metrics = self.evaluate_metrics(epoch_predictions, epoch_targets, val=True)
  File "/workspace/trainer/trainer.py", line 201, in evaluate_metrics
    metrics[key] = metric(predictions, targets).item()
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/trainer/metrics.py", line 184, in forward
    raise RuntimeError('No positively labeled data available. Cannot compute Average Precision.')
RuntimeError: No positively labeled data available. Cannot compute Average Precision.
All runs completed.
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.8/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.8_5_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.8
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6939895
[Epoch 1] ogbg-molbbbp: 0.830827 val loss: 0.689182
[Epoch 1] ogbg-molbbbp: 0.512249 test loss: 0.692677
[Epoch 2; Iter     5/   55] train: loss: 0.6939717
[Epoch 2; Iter    35/   55] train: loss: 0.6921082
[Epoch 2] ogbg-molbbbp: 0.773374 val loss: 0.686356
[Epoch 2] ogbg-molbbbp: 0.486208 test loss: 0.692856
[Epoch 3; Iter    10/   55] train: loss: 0.6924200
[Epoch 3; Iter    40/   55] train: loss: 0.6918980
[Epoch 3] ogbg-molbbbp: 0.773673 val loss: 0.686571
[Epoch 3] ogbg-molbbbp: 0.488715 test loss: 0.692731
[Epoch 4; Iter    15/   55] train: loss: 0.6910273
[Epoch 4; Iter    45/   55] train: loss: 0.6919813
[Epoch 4] ogbg-molbbbp: 0.772478 val loss: 0.686993
[Epoch 4] ogbg-molbbbp: 0.490837 test loss: 0.692563
[Epoch 5; Iter    20/   55] train: loss: 0.6905544
[Epoch 5; Iter    50/   55] train: loss: 0.6956550
[Epoch 5] ogbg-molbbbp: 0.776860 val loss: 0.687049
[Epoch 5] ogbg-molbbbp: 0.495274 test loss: 0.692327
[Epoch 6; Iter    25/   55] train: loss: 0.6895805
[Epoch 6; Iter    55/   55] train: loss: 0.6868336
[Epoch 6] ogbg-molbbbp: 0.790202 val loss: 0.686084
[Epoch 6] ogbg-molbbbp: 0.498650 test loss: 0.692149
[Epoch 7; Iter    30/   55] train: loss: 0.6861193
[Epoch 7] ogbg-molbbbp: 0.792990 val loss: 0.686561
[Epoch 7] ogbg-molbbbp: 0.503376 test loss: 0.691911
[Epoch 8; Iter     5/   55] train: loss: 0.6861213
[Epoch 8; Iter    35/   55] train: loss: 0.6816033
[Epoch 8] ogbg-molbbbp: 0.808025 val loss: 0.686092
[Epoch 8] ogbg-molbbbp: 0.498071 test loss: 0.691909
[Epoch 9; Iter    10/   55] train: loss: 0.6904274
[Epoch 9; Iter    40/   55] train: loss: 0.6869763
[Epoch 9] ogbg-molbbbp: 0.815394 val loss: 0.685809
[Epoch 9] ogbg-molbbbp: 0.497782 test loss: 0.691648
[Epoch 10; Iter    15/   55] train: loss: 0.6823931
[Epoch 10; Iter    45/   55] train: loss: 0.6818923
[Epoch 10] ogbg-molbbbp: 0.827342 val loss: 0.685753
[Epoch 10] ogbg-molbbbp: 0.502797 test loss: 0.691316
[Epoch 11; Iter    20/   55] train: loss: 0.6808157
[Epoch 11; Iter    50/   55] train: loss: 0.6750900
[Epoch 11] ogbg-molbbbp: 0.821567 val loss: 0.685839
[Epoch 11] ogbg-molbbbp: 0.508777 test loss: 0.691265
[Epoch 12; Iter    25/   55] train: loss: 0.6761420
[Epoch 12; Iter    55/   55] train: loss: 0.6714888
[Epoch 12] ogbg-molbbbp: 0.836005 val loss: 0.685724
[Epoch 12] ogbg-molbbbp: 0.511671 test loss: 0.690679
[Epoch 13; Iter    30/   55] train: loss: 0.6795565
[Epoch 13] ogbg-molbbbp: 0.909589 val loss: 0.619825
[Epoch 13] ogbg-molbbbp: 0.597319 test loss: 0.680014
[Epoch 14; Iter     5/   55] train: loss: 0.6645089
[Epoch 14; Iter    35/   55] train: loss: 0.6055999
[Epoch 14] ogbg-molbbbp: 0.903117 val loss: 0.496318
[Epoch 14] ogbg-molbbbp: 0.621142 test loss: 0.674686
[Epoch 15; Iter    10/   55] train: loss: 0.5603158
[Epoch 15; Iter    40/   55] train: loss: 0.5461428
[Epoch 15] ogbg-molbbbp: 0.929005 val loss: 0.406838
[Epoch 15] ogbg-molbbbp: 0.614390 test loss: 0.707443
[Epoch 16; Iter    15/   55] train: loss: 0.3915764
[Epoch 16; Iter    45/   55] train: loss: 0.5014724
[Epoch 16] ogbg-molbbbp: 0.881211 val loss: 0.459789
[Epoch 16] ogbg-molbbbp: 0.622106 test loss: 0.717702
[Epoch 17; Iter    20/   55] train: loss: 0.3617046
[Epoch 17; Iter    50/   55] train: loss: 0.4312295
[Epoch 17] ogbg-molbbbp: 0.942846 val loss: 0.345251
[Epoch 17] ogbg-molbbbp: 0.640818 test loss: 0.744355
[Epoch 18; Iter    25/   55] train: loss: 0.2733384
[Epoch 18; Iter    55/   55] train: loss: 0.5353193
[Epoch 18] ogbg-molbbbp: 0.934980 val loss: 0.326853
[Epoch 18] ogbg-molbbbp: 0.636478 test loss: 0.837754
[Epoch 19; Iter    30/   55] train: loss: 0.3205720
[Epoch 19] ogbg-molbbbp: 0.937568 val loss: 0.371831
[Epoch 19] ogbg-molbbbp: 0.642168 test loss: 0.962408
[Epoch 20; Iter     5/   55] train: loss: 0.2664358
[Epoch 20; Iter    35/   55] train: loss: 0.2193703
[Epoch 20] ogbg-molbbbp: 0.926715 val loss: 0.404407
[Epoch 20] ogbg-molbbbp: 0.628954 test loss: 0.990321
[Epoch 21; Iter    10/   55] train: loss: 0.2864611
[Epoch 21; Iter    40/   55] train: loss: 0.4969093
[Epoch 21] ogbg-molbbbp: 0.912875 val loss: 0.488184
[Epoch 21] ogbg-molbbbp: 0.648823 test loss: 1.024659
[Epoch 22; Iter    15/   55] train: loss: 0.4593271
[Epoch 22; Iter    45/   55] train: loss: 0.3132461
[Epoch 22] ogbg-molbbbp: 0.906104 val loss: 0.445152
[Epoch 22] ogbg-molbbbp: 0.627025 test loss: 0.963695
[Epoch 23; Iter    20/   55] train: loss: 0.2186852
[Epoch 23; Iter    50/   55] train: loss: 0.3147523
[Epoch 23] ogbg-molbbbp: 0.914767 val loss: 0.499366
[Epoch 23] ogbg-molbbbp: 0.643808 test loss: 1.141949
[Epoch 24; Iter    25/   55] train: loss: 0.4004529
[Epoch 24; Iter    55/   55] train: loss: 0.2501616
[Epoch 24] ogbg-molbbbp: 0.936374 val loss: 0.344709
[Epoch 24] ogbg-molbbbp: 0.653935 test loss: 1.021820
[Epoch 25; Iter    30/   55] train: loss: 0.2225169
[Epoch 25] ogbg-molbbbp: 0.920741 val loss: 0.441956
[Epoch 25] ogbg-molbbbp: 0.657215 test loss: 1.212585
[Epoch 26; Iter     5/   55] train: loss: 0.2353809
[Epoch 26; Iter    35/   55] train: loss: 0.2379824
[Epoch 26] ogbg-molbbbp: 0.892164 val loss: 0.784647
[Epoch 26] ogbg-molbbbp: 0.629437 test loss: 1.363748
[Epoch 27; Iter    10/   55] train: loss: 0.3116715
[Epoch 27; Iter    40/   55] train: loss: 0.2066592
[Epoch 27] ogbg-molbbbp: 0.904013 val loss: 0.505961
[Epoch 27] ogbg-molbbbp: 0.585455 test loss: 2.499735
[Epoch 28; Iter    15/   55] train: loss: 0.2153786
[Epoch 28; Iter    45/   55] train: loss: 0.1423854
[Epoch 28] ogbg-molbbbp: 0.940456 val loss: 0.521450
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.8/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.8_6_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.8
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6933044
[Epoch 1] ogbg-molbbbp: 0.695310 val loss: 0.691252
[Epoch 1] ogbg-molbbbp: 0.616512 test loss: 0.691880
[Epoch 2; Iter     5/   55] train: loss: 0.6939372
[Epoch 2; Iter    35/   55] train: loss: 0.6957976
[Epoch 2] ogbg-molbbbp: 0.599323 val loss: 0.691247
[Epoch 2] ogbg-molbbbp: 0.601948 test loss: 0.690859
[Epoch 3; Iter    10/   55] train: loss: 0.6924673
[Epoch 3; Iter    40/   55] train: loss: 0.6919085
[Epoch 3] ogbg-molbbbp: 0.590760 val loss: 0.691530
[Epoch 3] ogbg-molbbbp: 0.597608 test loss: 0.690960
[Epoch 4; Iter    15/   55] train: loss: 0.6910120
[Epoch 4; Iter    45/   55] train: loss: 0.6928008
[Epoch 4] ogbg-molbbbp: 0.628298 val loss: 0.690943
[Epoch 4] ogbg-molbbbp: 0.594811 test loss: 0.690907
[Epoch 5; Iter    20/   55] train: loss: 0.6927332
[Epoch 5; Iter    50/   55] train: loss: 0.6870205
[Epoch 5] ogbg-molbbbp: 0.626406 val loss: 0.691112
[Epoch 5] ogbg-molbbbp: 0.597512 test loss: 0.690758
[Epoch 6; Iter    25/   55] train: loss: 0.6896964
[Epoch 6; Iter    55/   55] train: loss: 0.6876629
[Epoch 6] ogbg-molbbbp: 0.643334 val loss: 0.690796
[Epoch 6] ogbg-molbbbp: 0.596740 test loss: 0.690553
[Epoch 7; Iter    30/   55] train: loss: 0.6871856
[Epoch 7] ogbg-molbbbp: 0.637160 val loss: 0.691671
[Epoch 7] ogbg-molbbbp: 0.597029 test loss: 0.690334
[Epoch 8; Iter     5/   55] train: loss: 0.6860068
[Epoch 8; Iter    35/   55] train: loss: 0.6842486
[Epoch 8] ogbg-molbbbp: 0.636264 val loss: 0.691991
[Epoch 8] ogbg-molbbbp: 0.596547 test loss: 0.690187
[Epoch 9; Iter    10/   55] train: loss: 0.6823841
[Epoch 9; Iter    40/   55] train: loss: 0.6840454
[Epoch 9] ogbg-molbbbp: 0.678084 val loss: 0.691409
[Epoch 9] ogbg-molbbbp: 0.597512 test loss: 0.690052
[Epoch 10; Iter    15/   55] train: loss: 0.6798117
[Epoch 10; Iter    45/   55] train: loss: 0.6816750
[Epoch 10] ogbg-molbbbp: 0.695011 val loss: 0.691449
[Epoch 10] ogbg-molbbbp: 0.597319 test loss: 0.689854
[Epoch 11; Iter    20/   55] train: loss: 0.6775602
[Epoch 11; Iter    50/   55] train: loss: 0.6733643
[Epoch 11] ogbg-molbbbp: 0.703176 val loss: 0.691896
[Epoch 11] ogbg-molbbbp: 0.601177 test loss: 0.689674
[Epoch 12; Iter    25/   55] train: loss: 0.6707716
[Epoch 12; Iter    55/   55] train: loss: 0.6917852
[Epoch 12] ogbg-molbbbp: 0.723688 val loss: 0.692180
[Epoch 12] ogbg-molbbbp: 0.591242 test loss: 0.689678
[Epoch 13; Iter    30/   55] train: loss: 0.6720517
[Epoch 13] ogbg-molbbbp: 0.909788 val loss: 0.618938
[Epoch 13] ogbg-molbbbp: 0.589410 test loss: 0.681408
[Epoch 14; Iter     5/   55] train: loss: 0.6405759
[Epoch 14; Iter    35/   55] train: loss: 0.5845066
[Epoch 14] ogbg-molbbbp: 0.921836 val loss: 0.520144
[Epoch 14] ogbg-molbbbp: 0.630980 test loss: 0.673151
[Epoch 15; Iter    10/   55] train: loss: 0.5625376
[Epoch 15; Iter    40/   55] train: loss: 0.4956192
[Epoch 15] ogbg-molbbbp: 0.827641 val loss: 0.544368
[Epoch 15] ogbg-molbbbp: 0.567226 test loss: 0.715977
[Epoch 16; Iter    15/   55] train: loss: 0.4627948
[Epoch 16; Iter    45/   55] train: loss: 0.4202855
[Epoch 16] ogbg-molbbbp: 0.918550 val loss: 0.396962
[Epoch 16] ogbg-molbbbp: 0.609857 test loss: 0.770395
[Epoch 17; Iter    20/   55] train: loss: 0.3951970
[Epoch 17; Iter    50/   55] train: loss: 0.3908048
[Epoch 17] ogbg-molbbbp: 0.936672 val loss: 0.364304
[Epoch 17] ogbg-molbbbp: 0.629147 test loss: 0.807455
[Epoch 18; Iter    25/   55] train: loss: 0.2450227
[Epoch 18; Iter    55/   55] train: loss: 0.3560490
[Epoch 18] ogbg-molbbbp: 0.934382 val loss: 0.386032
[Epoch 18] ogbg-molbbbp: 0.612365 test loss: 0.858108
[Epoch 19; Iter    30/   55] train: loss: 0.2673239
[Epoch 19] ogbg-molbbbp: 0.926715 val loss: 0.395154
[Epoch 19] ogbg-molbbbp: 0.642554 test loss: 0.903718
[Epoch 20; Iter     5/   55] train: loss: 0.3572958
[Epoch 20; Iter    35/   55] train: loss: 0.4357879
[Epoch 20] ogbg-molbbbp: 0.911779 val loss: 0.447077
[Epoch 20] ogbg-molbbbp: 0.647762 test loss: 0.957493
[Epoch 21; Iter    10/   55] train: loss: 0.3412524
[Epoch 21; Iter    40/   55] train: loss: 0.2764200
[Epoch 21] ogbg-molbbbp: 0.918152 val loss: 0.441610
[Epoch 21] ogbg-molbbbp: 0.650077 test loss: 1.161492
[Epoch 22; Iter    15/   55] train: loss: 0.2309872
[Epoch 22; Iter    45/   55] train: loss: 0.2230658
[Epoch 22] ogbg-molbbbp: 0.927213 val loss: 0.415797
[Epoch 22] ogbg-molbbbp: 0.666763 test loss: 1.018156
[Epoch 23; Iter    20/   55] train: loss: 0.4258675
[Epoch 23; Iter    50/   55] train: loss: 0.3631783
[Epoch 23] ogbg-molbbbp: 0.917156 val loss: 0.512343
[Epoch 23] ogbg-molbbbp: 0.668113 test loss: 1.087129
[Epoch 24; Iter    25/   55] train: loss: 0.2344359
[Epoch 24; Iter    55/   55] train: loss: 0.0907512
[Epoch 24] ogbg-molbbbp: 0.924325 val loss: 0.419536
[Epoch 24] ogbg-molbbbp: 0.662133 test loss: 1.006609
[Epoch 25; Iter    30/   55] train: loss: 0.2783074
[Epoch 25] ogbg-molbbbp: 0.909190 val loss: 0.482252
[Epoch 25] ogbg-molbbbp: 0.657986 test loss: 1.106448
[Epoch 26; Iter     5/   55] train: loss: 0.2608490
[Epoch 26; Iter    35/   55] train: loss: 0.2423488
[Epoch 26] ogbg-molbbbp: 0.949915 val loss: 0.317056
[Epoch 26] ogbg-molbbbp: 0.642940 test loss: 1.065169
[Epoch 27; Iter    10/   55] train: loss: 0.2034440
[Epoch 27; Iter    40/   55] train: loss: 0.1193041
[Epoch 27] ogbg-molbbbp: 0.947127 val loss: 0.316486
[Epoch 27] ogbg-molbbbp: 0.662230 test loss: 1.172294
[Epoch 28; Iter    15/   55] train: loss: 0.1444989
[Epoch 28; Iter    45/   55] train: loss: 0.2597054
[Epoch 28] ogbg-molbbbp: 0.942049 val loss: 0.313065
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.8/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.8_4_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.8
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   55] train: loss: 0.6913464
[Epoch 1] ogbg-molbbbp: 0.629593 val loss: 0.692138
[Epoch 1] ogbg-molbbbp: 0.530478 test loss: 0.692881
[Epoch 2; Iter     5/   55] train: loss: 0.6924450
[Epoch 2; Iter    35/   55] train: loss: 0.6924824
[Epoch 2] ogbg-molbbbp: 0.613462 val loss: 0.691002
[Epoch 2] ogbg-molbbbp: 0.515143 test loss: 0.692894
[Epoch 3; Iter    10/   55] train: loss: 0.6921225
[Epoch 3; Iter    40/   55] train: loss: 0.6933986
[Epoch 3] ogbg-molbbbp: 0.603007 val loss: 0.691752
[Epoch 3] ogbg-molbbbp: 0.516011 test loss: 0.692909
[Epoch 4; Iter    15/   55] train: loss: 0.6924711
[Epoch 4; Iter    45/   55] train: loss: 0.6902018
[Epoch 4] ogbg-molbbbp: 0.610973 val loss: 0.691429
[Epoch 4] ogbg-molbbbp: 0.518326 test loss: 0.692703
[Epoch 5; Iter    20/   55] train: loss: 0.6925603
[Epoch 5; Iter    50/   55] train: loss: 0.6885024
[Epoch 5] ogbg-molbbbp: 0.613263 val loss: 0.691657
[Epoch 5] ogbg-molbbbp: 0.525656 test loss: 0.692495
[Epoch 6; Iter    25/   55] train: loss: 0.6895051
[Epoch 6; Iter    55/   55] train: loss: 0.6959885
[Epoch 6] ogbg-molbbbp: 0.655681 val loss: 0.690845
[Epoch 6] ogbg-molbbbp: 0.521701 test loss: 0.692290
[Epoch 7; Iter    30/   55] train: loss: 0.6887948
[Epoch 7] ogbg-molbbbp: 0.632082 val loss: 0.691679
[Epoch 7] ogbg-molbbbp: 0.521894 test loss: 0.692351
[Epoch 8; Iter     5/   55] train: loss: 0.6872915
[Epoch 8; Iter    35/   55] train: loss: 0.6889781
[Epoch 8] ogbg-molbbbp: 0.654386 val loss: 0.691745
[Epoch 8] ogbg-molbbbp: 0.517747 test loss: 0.692137
[Epoch 9; Iter    10/   55] train: loss: 0.6852095
[Epoch 9; Iter    40/   55] train: loss: 0.6817462
[Epoch 9] ogbg-molbbbp: 0.690232 val loss: 0.691016
[Epoch 9] ogbg-molbbbp: 0.521798 test loss: 0.691721
[Epoch 10; Iter    15/   55] train: loss: 0.6857153
[Epoch 10; Iter    45/   55] train: loss: 0.6750189
[Epoch 10] ogbg-molbbbp: 0.700388 val loss: 0.691413
[Epoch 10] ogbg-molbbbp: 0.527392 test loss: 0.691401
[Epoch 11; Iter    20/   55] train: loss: 0.6843321
[Epoch 11; Iter    50/   55] train: loss: 0.6771483
[Epoch 11] ogbg-molbbbp: 0.713631 val loss: 0.691309
[Epoch 11] ogbg-molbbbp: 0.525367 test loss: 0.691283
[Epoch 12; Iter    25/   55] train: loss: 0.6766505
[Epoch 12; Iter    55/   55] train: loss: 0.6733553
[Epoch 12] ogbg-molbbbp: 0.741113 val loss: 0.691088
[Epoch 12] ogbg-molbbbp: 0.535880 test loss: 0.690884
[Epoch 13; Iter    30/   55] train: loss: 0.6700225
[Epoch 13] ogbg-molbbbp: 0.910286 val loss: 0.633183
[Epoch 13] ogbg-molbbbp: 0.571181 test loss: 0.683596
[Epoch 14; Iter     5/   55] train: loss: 0.6582623
[Epoch 14; Iter    35/   55] train: loss: 0.5814320
[Epoch 14] ogbg-molbbbp: 0.932490 val loss: 0.458012
[Epoch 14] ogbg-molbbbp: 0.612365 test loss: 0.694631
[Epoch 15; Iter    10/   55] train: loss: 0.5379752
[Epoch 15; Iter    40/   55] train: loss: 0.5629935
[Epoch 15] ogbg-molbbbp: 0.883700 val loss: 0.491861
[Epoch 15] ogbg-molbbbp: 0.585166 test loss: 0.718064
[Epoch 16; Iter    15/   55] train: loss: 0.5313541
[Epoch 16; Iter    45/   55] train: loss: 0.3602386
[Epoch 16] ogbg-molbbbp: 0.905407 val loss: 0.449643
[Epoch 16] ogbg-molbbbp: 0.630401 test loss: 0.751998
[Epoch 17; Iter    20/   55] train: loss: 0.3298573
[Epoch 17; Iter    50/   55] train: loss: 0.4915681
[Epoch 17] ogbg-molbbbp: 0.921836 val loss: 0.390951
[Epoch 17] ogbg-molbbbp: 0.641011 test loss: 0.803316
[Epoch 18; Iter    25/   55] train: loss: 0.3394468
[Epoch 18; Iter    55/   55] train: loss: 0.4385184
[Epoch 18] ogbg-molbbbp: 0.928408 val loss: 0.392896
[Epoch 18] ogbg-molbbbp: 0.629919 test loss: 0.934098
[Epoch 19; Iter    30/   55] train: loss: 0.2721257
[Epoch 19] ogbg-molbbbp: 0.910883 val loss: 0.390621
[Epoch 19] ogbg-molbbbp: 0.621817 test loss: 0.865611
[Epoch 20; Iter     5/   55] train: loss: 0.2562791
[Epoch 20; Iter    35/   55] train: loss: 0.1669187
[Epoch 20] ogbg-molbbbp: 0.928707 val loss: 0.406891
[Epoch 20] ogbg-molbbbp: 0.621335 test loss: 1.017080
[Epoch 21; Iter    10/   55] train: loss: 0.3246138
[Epoch 21; Iter    40/   55] train: loss: 0.3853703
[Epoch 21] ogbg-molbbbp: 0.926715 val loss: 0.420816
[Epoch 21] ogbg-molbbbp: 0.656732 test loss: 1.010888
[Epoch 22; Iter    15/   55] train: loss: 0.2952636
[Epoch 22; Iter    45/   55] train: loss: 0.4191974
[Epoch 22] ogbg-molbbbp: 0.913870 val loss: 0.578856
[Epoch 22] ogbg-molbbbp: 0.649306 test loss: 1.109523
[Epoch 23; Iter    20/   55] train: loss: 0.4136929
[Epoch 23; Iter    50/   55] train: loss: 0.1968569
[Epoch 23] ogbg-molbbbp: 0.925919 val loss: 0.438271
[Epoch 23] ogbg-molbbbp: 0.653742 test loss: 1.007326
[Epoch 24; Iter    25/   55] train: loss: 0.2421927
[Epoch 24; Iter    55/   55] train: loss: 0.6207256
[Epoch 24] ogbg-molbbbp: 0.894255 val loss: 0.493529
[Epoch 24] ogbg-molbbbp: 0.642168 test loss: 0.947955
[Epoch 25; Iter    30/   55] train: loss: 0.1594835
[Epoch 25] ogbg-molbbbp: 0.933984 val loss: 0.405837
[Epoch 25] ogbg-molbbbp: 0.646894 test loss: 1.011268
[Epoch 26; Iter     5/   55] train: loss: 0.2715015
[Epoch 26; Iter    35/   55] train: loss: 0.1775868
[Epoch 26] ogbg-molbbbp: 0.870955 val loss: 0.630487
[Epoch 26] ogbg-molbbbp: 0.611400 test loss: 0.986745
[Epoch 27; Iter    10/   55] train: loss: 0.1797710
[Epoch 27; Iter    40/   55] train: loss: 0.1301559
[Epoch 27] ogbg-molbbbp: 0.936374 val loss: 0.443043
[Epoch 27] ogbg-molbbbp: 0.693673 test loss: 0.944695
[Epoch 28; Iter    15/   55] train: loss: 0.1630806
[Epoch 28; Iter    45/   55] train: loss: 0.2820918
[Epoch 28] ogbg-molbbbp: 0.942348 val loss: 0.388722
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.7/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.7_6_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.7
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6917359
[Epoch 1] ogbg-molbbbp: 0.718358 val loss: 0.692316
[Epoch 1] ogbg-molbbbp: 0.640905 test loss: 0.691730
[Epoch 2; Iter    12/   48] train: loss: 0.6927788
[Epoch 2; Iter    42/   48] train: loss: 0.6924949
[Epoch 2] ogbg-molbbbp: 0.617126 val loss: 0.690923
[Epoch 2] ogbg-molbbbp: 0.582079 test loss: 0.691303
[Epoch 3; Iter    24/   48] train: loss: 0.6926669
[Epoch 3] ogbg-molbbbp: 0.611730 val loss: 0.690690
[Epoch 3] ogbg-molbbbp: 0.589382 test loss: 0.691122
[Epoch 4; Iter     6/   48] train: loss: 0.6919721
[Epoch 4; Iter    36/   48] train: loss: 0.6911227
[Epoch 4] ogbg-molbbbp: 0.620997 val loss: 0.689436
[Epoch 4] ogbg-molbbbp: 0.592608 test loss: 0.691488
[Epoch 5; Iter    18/   48] train: loss: 0.6952065
[Epoch 5; Iter    48/   48] train: loss: 0.6903455
[Epoch 5] ogbg-molbbbp: 0.609267 val loss: 0.688626
[Epoch 5] ogbg-molbbbp: 0.589471 test loss: 0.691933
[Epoch 6; Iter    30/   48] train: loss: 0.6898251
[Epoch 6] ogbg-molbbbp: 0.628504 val loss: 0.687987
[Epoch 6] ogbg-molbbbp: 0.600672 test loss: 0.691703
[Epoch 7; Iter    12/   48] train: loss: 0.6893042
[Epoch 7; Iter    42/   48] train: loss: 0.6878011
[Epoch 7] ogbg-molbbbp: 0.633314 val loss: 0.686326
[Epoch 7] ogbg-molbbbp: 0.603450 test loss: 0.692162
[Epoch 8; Iter    24/   48] train: loss: 0.6870863
[Epoch 8] ogbg-molbbbp: 0.648211 val loss: 0.684258
[Epoch 8] ogbg-molbbbp: 0.605780 test loss: 0.692636
[Epoch 9; Iter     6/   48] train: loss: 0.6866311
[Epoch 9; Iter    36/   48] train: loss: 0.6819583
[Epoch 9] ogbg-molbbbp: 0.669443 val loss: 0.682584
[Epoch 9] ogbg-molbbbp: 0.615547 test loss: 0.692789
[Epoch 10; Iter    18/   48] train: loss: 0.6850948
[Epoch 10; Iter    48/   48] train: loss: 0.6790634
[Epoch 10] ogbg-molbbbp: 0.679062 val loss: 0.680310
[Epoch 10] ogbg-molbbbp: 0.626882 test loss: 0.693147
[Epoch 11; Iter    30/   48] train: loss: 0.6800774
[Epoch 11] ogbg-molbbbp: 0.699941 val loss: 0.677150
[Epoch 11] ogbg-molbbbp: 0.630869 test loss: 0.693935
[Epoch 12; Iter    12/   48] train: loss: 0.6828679
[Epoch 12; Iter    42/   48] train: loss: 0.6860401
[Epoch 12] ogbg-molbbbp: 0.703578 val loss: 0.674634
[Epoch 12] ogbg-molbbbp: 0.634498 test loss: 0.694674
[Epoch 13; Iter    24/   48] train: loss: 0.6733410
[Epoch 13] ogbg-molbbbp: 0.731378 val loss: 0.672068
[Epoch 13] ogbg-molbbbp: 0.650179 test loss: 0.694946
[Epoch 14; Iter     6/   48] train: loss: 0.6818411
[Epoch 14; Iter    36/   48] train: loss: 0.6839541
[Epoch 14] ogbg-molbbbp: 0.764575 val loss: 0.668599
[Epoch 14] ogbg-molbbbp: 0.659946 test loss: 0.695380
[Epoch 15; Iter    18/   48] train: loss: 0.6723517
[Epoch 15; Iter    48/   48] train: loss: 0.6595608
[Epoch 15] ogbg-molbbbp: 0.955191 val loss: 0.617502
[Epoch 15] ogbg-molbbbp: 0.739247 test loss: 0.642521
[Epoch 16; Iter    30/   48] train: loss: 0.6015936
[Epoch 16] ogbg-molbbbp: 0.959648 val loss: 0.593949
[Epoch 16] ogbg-molbbbp: 0.756900 test loss: 0.551272
[Epoch 17; Iter    12/   48] train: loss: 0.6336625
[Epoch 17; Iter    42/   48] train: loss: 0.5325274
[Epoch 17] ogbg-molbbbp: 0.960938 val loss: 0.478931
[Epoch 17] ogbg-molbbbp: 0.766084 test loss: 0.554981
[Epoch 18; Iter    24/   48] train: loss: 0.4228949
[Epoch 18] ogbg-molbbbp: 0.962463 val loss: 0.549142
[Epoch 18] ogbg-molbbbp: 0.765681 test loss: 0.546527
[Epoch 19; Iter     6/   48] train: loss: 0.4500837
[Epoch 19; Iter    36/   48] train: loss: 0.3709312
[Epoch 19] ogbg-molbbbp: 0.963284 val loss: 0.710195
[Epoch 19] ogbg-molbbbp: 0.746998 test loss: 0.721248
[Epoch 20; Iter    18/   48] train: loss: 0.3940195
[Epoch 20; Iter    48/   48] train: loss: 0.2931907
[Epoch 20] ogbg-molbbbp: 0.953079 val loss: 0.243309
[Epoch 20] ogbg-molbbbp: 0.755287 test loss: 0.680976
[Epoch 21; Iter    30/   48] train: loss: 0.3352357
[Epoch 21] ogbg-molbbbp: 0.963284 val loss: 0.277740
[Epoch 21] ogbg-molbbbp: 0.762276 test loss: 0.631520
[Epoch 22; Iter    12/   48] train: loss: 0.3543054
[Epoch 22; Iter    42/   48] train: loss: 0.2302177
[Epoch 22] ogbg-molbbbp: 0.916481 val loss: 0.297639
[Epoch 22] ogbg-molbbbp: 0.744713 test loss: 0.658030
[Epoch 23; Iter    24/   48] train: loss: 0.3022844
[Epoch 23] ogbg-molbbbp: 0.952258 val loss: 0.194278
[Epoch 23] ogbg-molbbbp: 0.756720 test loss: 0.775505
[Epoch 24; Iter     6/   48] train: loss: 0.2214192
[Epoch 24; Iter    36/   48] train: loss: 0.2047970
[Epoch 24] ogbg-molbbbp: 0.952610 val loss: 0.154799
[Epoch 24] ogbg-molbbbp: 0.757661 test loss: 0.922281
[Epoch 25; Iter    18/   48] train: loss: 0.2679908
[Epoch 25; Iter    48/   48] train: loss: 0.2643557
[Epoch 25] ogbg-molbbbp: 0.969853 val loss: 0.216949
[Epoch 25] ogbg-molbbbp: 0.784050 test loss: 0.726556
[Epoch 26; Iter    30/   48] train: loss: 0.4660392
[Epoch 26] ogbg-molbbbp: 0.956950 val loss: 0.172084
[Epoch 26] ogbg-molbbbp: 0.773253 test loss: 0.871474
[Epoch 27; Iter    12/   48] train: loss: 0.2230142
[Epoch 27; Iter    42/   48] train: loss: 0.4071084
[Epoch 27] ogbg-molbbbp: 0.970323 val loss: 0.261718
[Epoch 27] ogbg-molbbbp: 0.778405 test loss: 0.665637
[Epoch 28; Iter    24/   48] train: loss: 0.2204683
[Epoch 28] ogbg-molbbbp: 0.966217 val loss: 0.187543
[Epoch 28] ogbg-molbbbp: 0.788217 test loss: 0.734047
[Epoch 29; Iter     6/   48] train: loss: 0.2889435
[Epoch 29; Iter    36/   48] train: loss: 0.3006427
[Epoch 29] ogbg-molbbbp: 0.967977 val loss: 0.158830
[Epoch 29] ogbg-molbbbp: 0.787007 test loss: 0.819076
[Epoch 30; Iter    18/   48] train: loss: 0.2648484
[Epoch 30; Iter    48/   48] train: loss: 0.1492136
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.7/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.7_4_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.7
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6951299
[Epoch 1] ogbg-molbbbp: 0.582053 val loss: 0.693385
[Epoch 1] ogbg-molbbbp: 0.569176 test loss: 0.692615
[Epoch 2; Iter    12/   48] train: loss: 0.6948878
[Epoch 2; Iter    42/   48] train: loss: 0.6918886
[Epoch 2] ogbg-molbbbp: 0.538651 val loss: 0.694271
[Epoch 2] ogbg-molbbbp: 0.539203 test loss: 0.692709
[Epoch 3; Iter    24/   48] train: loss: 0.6953037
[Epoch 3] ogbg-molbbbp: 0.552610 val loss: 0.693694
[Epoch 3] ogbg-molbbbp: 0.547581 test loss: 0.692527
[Epoch 4; Iter     6/   48] train: loss: 0.6928208
[Epoch 4; Iter    36/   48] train: loss: 0.6907923
[Epoch 4] ogbg-molbbbp: 0.567273 val loss: 0.692756
[Epoch 4] ogbg-molbbbp: 0.550179 test loss: 0.692670
[Epoch 5; Iter    18/   48] train: loss: 0.6906202
[Epoch 5; Iter    48/   48] train: loss: 0.6925361
[Epoch 5] ogbg-molbbbp: 0.609501 val loss: 0.691650
[Epoch 5] ogbg-molbbbp: 0.572267 test loss: 0.692147
[Epoch 6; Iter    30/   48] train: loss: 0.6910563
[Epoch 6] ogbg-molbbbp: 0.590616 val loss: 0.690266
[Epoch 6] ogbg-molbbbp: 0.561425 test loss: 0.693127
[Epoch 7; Iter    12/   48] train: loss: 0.6932512
[Epoch 7; Iter    42/   48] train: loss: 0.6888384
[Epoch 7] ogbg-molbbbp: 0.592727 val loss: 0.688037
[Epoch 7] ogbg-molbbbp: 0.555869 test loss: 0.694004
[Epoch 8; Iter    24/   48] train: loss: 0.6862417
[Epoch 8] ogbg-molbbbp: 0.605044 val loss: 0.686842
[Epoch 8] ogbg-molbbbp: 0.563530 test loss: 0.694258
[Epoch 9; Iter     6/   48] train: loss: 0.6877778
[Epoch 9; Iter    36/   48] train: loss: 0.6842961
[Epoch 9] ogbg-molbbbp: 0.628270 val loss: 0.684522
[Epoch 9] ogbg-molbbbp: 0.575986 test loss: 0.694232
[Epoch 10; Iter    18/   48] train: loss: 0.6860620
[Epoch 10; Iter    48/   48] train: loss: 0.6829513
[Epoch 10] ogbg-molbbbp: 0.672493 val loss: 0.682683
[Epoch 10] ogbg-molbbbp: 0.604077 test loss: 0.693656
[Epoch 11; Iter    30/   48] train: loss: 0.6829165
[Epoch 11] ogbg-molbbbp: 0.667449 val loss: 0.680272
[Epoch 11] ogbg-molbbbp: 0.592921 test loss: 0.694969
[Epoch 12; Iter    12/   48] train: loss: 0.6852248
[Epoch 12; Iter    42/   48] train: loss: 0.6821986
[Epoch 12] ogbg-molbbbp: 0.707331 val loss: 0.676556
[Epoch 12] ogbg-molbbbp: 0.606407 test loss: 0.695512
[Epoch 13; Iter    24/   48] train: loss: 0.6844209
[Epoch 13] ogbg-molbbbp: 0.717067 val loss: 0.674159
[Epoch 13] ogbg-molbbbp: 0.613889 test loss: 0.695875
[Epoch 14; Iter     6/   48] train: loss: 0.6763675
[Epoch 14; Iter    36/   48] train: loss: 0.6837503
[Epoch 14] ogbg-molbbbp: 0.738886 val loss: 0.670656
[Epoch 14] ogbg-molbbbp: 0.622939 test loss: 0.696499
[Epoch 15; Iter    18/   48] train: loss: 0.6801244
[Epoch 15; Iter    48/   48] train: loss: 0.6536922
[Epoch 15] ogbg-molbbbp: 0.953783 val loss: 0.621502
[Epoch 15] ogbg-molbbbp: 0.733781 test loss: 0.648931
[Epoch 16; Iter    30/   48] train: loss: 0.6053615
[Epoch 16] ogbg-molbbbp: 0.945103 val loss: 0.514076
[Epoch 16] ogbg-molbbbp: 0.735529 test loss: 0.587150
[Epoch 17; Iter    12/   48] train: loss: 0.5490295
[Epoch 17; Iter    42/   48] train: loss: 0.5019180
[Epoch 17] ogbg-molbbbp: 0.951085 val loss: 0.482285
[Epoch 17] ogbg-molbbbp: 0.757751 test loss: 0.543681
[Epoch 18; Iter    24/   48] train: loss: 0.4081843
[Epoch 18] ogbg-molbbbp: 0.945572 val loss: 0.500151
[Epoch 18] ogbg-molbbbp: 0.759140 test loss: 0.544292
[Epoch 19; Iter     6/   48] train: loss: 0.4905020
[Epoch 19; Iter    36/   48] train: loss: 0.4512309
[Epoch 19] ogbg-molbbbp: 0.944282 val loss: 0.381350
[Epoch 19] ogbg-molbbbp: 0.744713 test loss: 0.611794
[Epoch 20; Iter    18/   48] train: loss: 0.3416210
[Epoch 20; Iter    48/   48] train: loss: 0.3376119
[Epoch 20] ogbg-molbbbp: 0.935718 val loss: 0.314288
[Epoch 20] ogbg-molbbbp: 0.753405 test loss: 0.615081
[Epoch 21; Iter    30/   48] train: loss: 0.3644781
[Epoch 21] ogbg-molbbbp: 0.959883 val loss: 0.237149
[Epoch 21] ogbg-molbbbp: 0.764516 test loss: 0.665849
[Epoch 22; Iter    12/   48] train: loss: 0.2872485
[Epoch 22; Iter    42/   48] train: loss: 0.2826513
[Epoch 22] ogbg-molbbbp: 0.964575 val loss: 0.201950
[Epoch 22] ogbg-molbbbp: 0.755466 test loss: 0.742578
[Epoch 23; Iter    24/   48] train: loss: 0.4529269
[Epoch 23] ogbg-molbbbp: 0.963988 val loss: 0.283837
[Epoch 23] ogbg-molbbbp: 0.776971 test loss: 0.617772
[Epoch 24; Iter     6/   48] train: loss: 0.2575063
[Epoch 24; Iter    36/   48] train: loss: 0.3972015
[Epoch 24] ogbg-molbbbp: 0.963754 val loss: 0.218291
[Epoch 24] ogbg-molbbbp: 0.780869 test loss: 0.703318
[Epoch 25; Iter    18/   48] train: loss: 0.2945221
[Epoch 25; Iter    48/   48] train: loss: 0.2083560
[Epoch 25] ogbg-molbbbp: 0.964457 val loss: 0.149573
[Epoch 25] ogbg-molbbbp: 0.788396 test loss: 0.875266
[Epoch 26; Iter    30/   48] train: loss: 0.3768025
[Epoch 26] ogbg-molbbbp: 0.969384 val loss: 0.176708
[Epoch 26] ogbg-molbbbp: 0.771595 test loss: 0.770331
[Epoch 27; Iter    12/   48] train: loss: 0.1010829
[Epoch 27; Iter    42/   48] train: loss: 0.1736978
[Epoch 27] ogbg-molbbbp: 0.969501 val loss: 0.155754
[Epoch 27] ogbg-molbbbp: 0.772177 test loss: 0.888945
[Epoch 28; Iter    24/   48] train: loss: 0.2881361
[Epoch 28] ogbg-molbbbp: 0.964106 val loss: 0.154159
[Epoch 28] ogbg-molbbbp: 0.776971 test loss: 0.831019
[Epoch 29; Iter     6/   48] train: loss: 0.2259895
[Epoch 29; Iter    36/   48] train: loss: 0.2470070
[Epoch 29] ogbg-molbbbp: 0.976305 val loss: 0.140937
[Epoch 29] ogbg-molbbbp: 0.784722 test loss: 0.814865
[Epoch 30; Iter    18/   48] train: loss: 0.1341005
[Epoch 30; Iter    48/   48] train: loss: 0.1263395
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bbbp/scaff/train_prop=0.7/PNA_ogbg-molbbbp_3DInfomax_bbbp_scaff=0.7_5_26-05_09-18-13
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bbbp_scaff=0.7
logdir: runs/split/3DInfomax/bbbp/scaff/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbbbp
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbbbp
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   48] train: loss: 0.6959690
[Epoch 1] ogbg-molbbbp: 0.836833 val loss: 0.691788
[Epoch 1] ogbg-molbbbp: 0.666577 test loss: 0.691405
[Epoch 2; Iter    12/   48] train: loss: 0.6891364
[Epoch 2; Iter    42/   48] train: loss: 0.6895946
[Epoch 2] ogbg-molbbbp: 0.805513 val loss: 0.690232
[Epoch 2] ogbg-molbbbp: 0.636066 test loss: 0.689386
[Epoch 3; Iter    24/   48] train: loss: 0.6937383
[Epoch 3] ogbg-molbbbp: 0.796598 val loss: 0.690187
[Epoch 3] ogbg-molbbbp: 0.626658 test loss: 0.689592
[Epoch 4; Iter     6/   48] train: loss: 0.6893086
[Epoch 4; Iter    36/   48] train: loss: 0.6925806
[Epoch 4] ogbg-molbbbp: 0.811026 val loss: 0.689212
[Epoch 4] ogbg-molbbbp: 0.635932 test loss: 0.689263
[Epoch 5; Iter    18/   48] train: loss: 0.6890383
[Epoch 5; Iter    48/   48] train: loss: 0.6907837
[Epoch 5] ogbg-molbbbp: 0.796012 val loss: 0.687871
[Epoch 5] ogbg-molbbbp: 0.628315 test loss: 0.690339
[Epoch 6; Iter    30/   48] train: loss: 0.6917218
[Epoch 6] ogbg-molbbbp: 0.799531 val loss: 0.686523
[Epoch 6] ogbg-molbbbp: 0.629749 test loss: 0.690538
[Epoch 7; Iter    12/   48] train: loss: 0.6843504
[Epoch 7; Iter    42/   48] train: loss: 0.6880929
[Epoch 7] ogbg-molbbbp: 0.818182 val loss: 0.684440
[Epoch 7] ogbg-molbbbp: 0.639651 test loss: 0.690552
[Epoch 8; Iter    24/   48] train: loss: 0.6952199
[Epoch 8] ogbg-molbbbp: 0.817126 val loss: 0.683885
[Epoch 8] ogbg-molbbbp: 0.643056 test loss: 0.690368
[Epoch 9; Iter     6/   48] train: loss: 0.6868728
[Epoch 9; Iter    36/   48] train: loss: 0.6868462
[Epoch 9] ogbg-molbbbp: 0.825572 val loss: 0.681234
[Epoch 9] ogbg-molbbbp: 0.643996 test loss: 0.691130
[Epoch 10; Iter    18/   48] train: loss: 0.6852332
[Epoch 10; Iter    48/   48] train: loss: 0.6811675
[Epoch 10] ogbg-molbbbp: 0.842229 val loss: 0.678859
[Epoch 10] ogbg-molbbbp: 0.648790 test loss: 0.691389
[Epoch 11; Iter    30/   48] train: loss: 0.6838151
[Epoch 11] ogbg-molbbbp: 0.850323 val loss: 0.676416
[Epoch 11] ogbg-molbbbp: 0.653987 test loss: 0.691700
[Epoch 12; Iter    12/   48] train: loss: 0.6798443
[Epoch 12; Iter    42/   48] train: loss: 0.6804407
[Epoch 12] ogbg-molbbbp: 0.850909 val loss: 0.674064
[Epoch 12] ogbg-molbbbp: 0.655152 test loss: 0.692131
[Epoch 13; Iter    24/   48] train: loss: 0.6799260
[Epoch 13] ogbg-molbbbp: 0.868152 val loss: 0.670726
[Epoch 13] ogbg-molbbbp: 0.663262 test loss: 0.692552
[Epoch 14; Iter     6/   48] train: loss: 0.6771432
[Epoch 14; Iter    36/   48] train: loss: 0.6766708
[Epoch 14] ogbg-molbbbp: 0.872493 val loss: 0.668463
[Epoch 14] ogbg-molbbbp: 0.665905 test loss: 0.692842
[Epoch 15; Iter    18/   48] train: loss: 0.6734305
[Epoch 15; Iter    48/   48] train: loss: 0.6272407
[Epoch 15] ogbg-molbbbp: 0.952493 val loss: 0.604705
[Epoch 15] ogbg-molbbbp: 0.740054 test loss: 0.647726
[Epoch 16; Iter    30/   48] train: loss: 0.6045327
[Epoch 16] ogbg-molbbbp: 0.941701 val loss: 0.538914
[Epoch 16] ogbg-molbbbp: 0.750134 test loss: 0.574718
[Epoch 17; Iter    12/   48] train: loss: 0.5675363
[Epoch 17; Iter    42/   48] train: loss: 0.5755717
[Epoch 17] ogbg-molbbbp: 0.958710 val loss: 0.584859
[Epoch 17] ogbg-molbbbp: 0.766532 test loss: 0.529979
[Epoch 18; Iter    24/   48] train: loss: 0.4286581
[Epoch 18] ogbg-molbbbp: 0.956950 val loss: 0.357257
[Epoch 18] ogbg-molbbbp: 0.766308 test loss: 0.585156
[Epoch 19; Iter     6/   48] train: loss: 0.5030240
[Epoch 19; Iter    36/   48] train: loss: 0.4698219
[Epoch 19] ogbg-molbbbp: 0.958827 val loss: 0.500773
[Epoch 19] ogbg-molbbbp: 0.768369 test loss: 0.557494
[Epoch 20; Iter    18/   48] train: loss: 0.3560363
[Epoch 20; Iter    48/   48] train: loss: 0.4371038
[Epoch 20] ogbg-molbbbp: 0.970088 val loss: 0.320154
[Epoch 20] ogbg-molbbbp: 0.764516 test loss: 0.616249
[Epoch 21; Iter    30/   48] train: loss: 0.3449473
[Epoch 21] ogbg-molbbbp: 0.973842 val loss: 0.284359
[Epoch 21] ogbg-molbbbp: 0.771371 test loss: 0.622174
[Epoch 22; Iter    12/   48] train: loss: 0.2958360
[Epoch 22; Iter    42/   48] train: loss: 0.3190995
[Epoch 22] ogbg-molbbbp: 0.932199 val loss: 0.232531
[Epoch 22] ogbg-molbbbp: 0.756183 test loss: 0.712387
[Epoch 23; Iter    24/   48] train: loss: 0.3201731
[Epoch 23] ogbg-molbbbp: 0.961760 val loss: 0.166950
[Epoch 23] ogbg-molbbbp: 0.767563 test loss: 0.861533
[Epoch 24; Iter     6/   48] train: loss: 0.3429037
[Epoch 24; Iter    36/   48] train: loss: 0.3447484
[Epoch 24] ogbg-molbbbp: 0.974194 val loss: 0.200323
[Epoch 24] ogbg-molbbbp: 0.763172 test loss: 0.756112
[Epoch 25; Iter    18/   48] train: loss: 0.1933399
[Epoch 25; Iter    48/   48] train: loss: 0.2738414
[Epoch 25] ogbg-molbbbp: 0.971613 val loss: 0.146117
[Epoch 25] ogbg-molbbbp: 0.762590 test loss: 0.908956
[Epoch 26; Iter    30/   48] train: loss: 0.1331401
[Epoch 26] ogbg-molbbbp: 0.974194 val loss: 0.180078
[Epoch 26] ogbg-molbbbp: 0.760932 test loss: 0.819849
[Epoch 27; Iter    12/   48] train: loss: 0.3104703
[Epoch 27; Iter    42/   48] train: loss: 0.2804973
[Epoch 27] ogbg-molbbbp: 0.967742 val loss: 0.179967
[Epoch 27] ogbg-molbbbp: 0.766084 test loss: 0.832799
[Epoch 28; Iter    24/   48] train: loss: 0.1498972
[Epoch 28] ogbg-molbbbp: 0.950264 val loss: 0.167551
[Epoch 28] ogbg-molbbbp: 0.755287 test loss: 0.914753
[Epoch 29; Iter     6/   48] train: loss: 0.3254080
[Epoch 29; Iter    36/   48] train: loss: 0.2959143
[Epoch 29] ogbg-molbbbp: 0.976188 val loss: 0.232752
[Epoch 29] ogbg-molbbbp: 0.789964 test loss: 0.704620
[Epoch 30; Iter    18/   48] train: loss: 0.0907288
[Epoch 30; Iter    48/   48] train: loss: 0.2849532
[Epoch 28] ogbg-molbbbp: 0.662326 test loss: 1.460753
[Epoch 29; Iter    20/   55] train: loss: 0.2768273
[Epoch 29; Iter    50/   55] train: loss: 0.2521923
[Epoch 29] ogbg-molbbbp: 0.953400 val loss: 0.347728
[Epoch 29] ogbg-molbbbp: 0.661844 test loss: 1.173740
[Epoch 30; Iter    25/   55] train: loss: 0.1868336
[Epoch 30; Iter    55/   55] train: loss: 0.4261582
[Epoch 30] ogbg-molbbbp: 0.953898 val loss: 0.269670
[Epoch 30] ogbg-molbbbp: 0.712384 test loss: 0.834125
[Epoch 31; Iter    30/   55] train: loss: 0.3716076
[Epoch 31] ogbg-molbbbp: 0.920343 val loss: 0.717598
[Epoch 31] ogbg-molbbbp: 0.665799 test loss: 1.593430
[Epoch 32; Iter     5/   55] train: loss: 0.1230837
[Epoch 32; Iter    35/   55] train: loss: 0.1156369
[Epoch 32] ogbg-molbbbp: 0.955691 val loss: 0.399095
[Epoch 32] ogbg-molbbbp: 0.660783 test loss: 1.308427
[Epoch 33; Iter    10/   55] train: loss: 0.1488551
[Epoch 33; Iter    40/   55] train: loss: 0.2184542
[Epoch 33] ogbg-molbbbp: 0.961067 val loss: 0.286040
[Epoch 33] ogbg-molbbbp: 0.679398 test loss: 1.177032
[Epoch 34; Iter    15/   55] train: loss: 0.3171338
[Epoch 34; Iter    45/   55] train: loss: 0.3135313
[Epoch 34] ogbg-molbbbp: 0.942447 val loss: 0.417118
[Epoch 34] ogbg-molbbbp: 0.683160 test loss: 1.838748
[Epoch 35; Iter    20/   55] train: loss: 0.2680695
[Epoch 35; Iter    50/   55] train: loss: 0.2914730
[Epoch 35] ogbg-molbbbp: 0.954794 val loss: 0.379015
[Epoch 35] ogbg-molbbbp: 0.689525 test loss: 1.162524
[Epoch 36; Iter    25/   55] train: loss: 0.2739728
[Epoch 36; Iter    55/   55] train: loss: 0.1814406
[Epoch 36] ogbg-molbbbp: 0.957582 val loss: 0.318868
[Epoch 36] ogbg-molbbbp: 0.678434 test loss: 1.189020
[Epoch 37; Iter    30/   55] train: loss: 0.1919309
[Epoch 37] ogbg-molbbbp: 0.957782 val loss: 0.311739
[Epoch 37] ogbg-molbbbp: 0.690490 test loss: 1.288032
[Epoch 38; Iter     5/   55] train: loss: 0.1527071
[Epoch 38; Iter    35/   55] train: loss: 0.1488594
[Epoch 38] ogbg-molbbbp: 0.937071 val loss: 0.512858
[Epoch 38] ogbg-molbbbp: 0.587770 test loss: 1.806436
[Epoch 39; Iter    10/   55] train: loss: 0.0433916
[Epoch 39; Iter    40/   55] train: loss: 0.1135607
[Epoch 39] ogbg-molbbbp: 0.940356 val loss: 0.359926
[Epoch 39] ogbg-molbbbp: 0.665027 test loss: 1.189434
[Epoch 40; Iter    15/   55] train: loss: 0.0550592
[Epoch 40; Iter    45/   55] train: loss: 0.3606410
[Epoch 40] ogbg-molbbbp: 0.946530 val loss: 0.468665
[Epoch 40] ogbg-molbbbp: 0.670235 test loss: 1.411962
[Epoch 41; Iter    20/   55] train: loss: 0.1126280
[Epoch 41; Iter    50/   55] train: loss: 0.1098897
[Epoch 41] ogbg-molbbbp: 0.950911 val loss: 0.405544
[Epoch 41] ogbg-molbbbp: 0.643326 test loss: 1.456634
[Epoch 42; Iter    25/   55] train: loss: 0.0859375
[Epoch 42; Iter    55/   55] train: loss: 0.0569468
[Epoch 42] ogbg-molbbbp: 0.951509 val loss: 0.477190
[Epoch 42] ogbg-molbbbp: 0.658083 test loss: 1.449110
[Epoch 43; Iter    30/   55] train: loss: 0.1041100
[Epoch 43] ogbg-molbbbp: 0.943344 val loss: 0.373493
[Epoch 43] ogbg-molbbbp: 0.629147 test loss: 1.468902
[Epoch 44; Iter     5/   55] train: loss: 0.0794619
[Epoch 44; Iter    35/   55] train: loss: 0.0461316
[Epoch 44] ogbg-molbbbp: 0.958479 val loss: 0.334896
[Epoch 44] ogbg-molbbbp: 0.703414 test loss: 1.208968
[Epoch 45; Iter    10/   55] train: loss: 0.0701701
[Epoch 45; Iter    40/   55] train: loss: 0.1294822
[Epoch 45] ogbg-molbbbp: 0.950413 val loss: 0.478536
[Epoch 45] ogbg-molbbbp: 0.660204 test loss: 1.790267
[Epoch 46; Iter    15/   55] train: loss: 0.3264340
[Epoch 46; Iter    45/   55] train: loss: 0.1815443
[Epoch 46] ogbg-molbbbp: 0.931793 val loss: 0.603256
[Epoch 46] ogbg-molbbbp: 0.615548 test loss: 1.518623
[Epoch 47; Iter    20/   55] train: loss: 0.1714602
[Epoch 47; Iter    50/   55] train: loss: 0.0964706
[Epoch 47] ogbg-molbbbp: 0.955491 val loss: 0.393504
[Epoch 47] ogbg-molbbbp: 0.675444 test loss: 1.542326
[Epoch 48; Iter    25/   55] train: loss: 0.0693112
[Epoch 48; Iter    55/   55] train: loss: 0.0237528
[Epoch 48] ogbg-molbbbp: 0.939958 val loss: 0.492199
[Epoch 48] ogbg-molbbbp: 0.640239 test loss: 1.502648
[Epoch 49; Iter    30/   55] train: loss: 0.2615927
[Epoch 49] ogbg-molbbbp: 0.958080 val loss: 0.311697
[Epoch 49] ogbg-molbbbp: 0.644579 test loss: 1.346641
[Epoch 50; Iter     5/   55] train: loss: 0.1884886
[Epoch 50; Iter    35/   55] train: loss: 0.0280187
[Epoch 50] ogbg-molbbbp: 0.944240 val loss: 0.564448
[Epoch 50] ogbg-molbbbp: 0.644772 test loss: 1.693669
[Epoch 51; Iter    10/   55] train: loss: 0.0419298
[Epoch 51; Iter    40/   55] train: loss: 0.1500777
[Epoch 51] ogbg-molbbbp: 0.928507 val loss: 0.638549
[Epoch 51] ogbg-molbbbp: 0.650463 test loss: 1.953326
[Epoch 52; Iter    15/   55] train: loss: 0.0454353
[Epoch 52; Iter    45/   55] train: loss: 0.0256071
[Epoch 52] ogbg-molbbbp: 0.939659 val loss: 0.663002
[Epoch 52] ogbg-molbbbp: 0.616512 test loss: 2.252864
[Epoch 53; Iter    20/   55] train: loss: 0.0857460
[Epoch 53; Iter    50/   55] train: loss: 0.0432008
[Epoch 53] ogbg-molbbbp: 0.945435 val loss: 0.457228
[Epoch 53] ogbg-molbbbp: 0.682774 test loss: 1.342272
[Epoch 54; Iter    25/   55] train: loss: 0.0314429
[Epoch 54; Iter    55/   55] train: loss: 0.2300527
[Epoch 54] ogbg-molbbbp: 0.939859 val loss: 0.485700
[Epoch 54] ogbg-molbbbp: 0.664931 test loss: 1.541967
[Epoch 55; Iter    30/   55] train: loss: 0.0702576
[Epoch 55] ogbg-molbbbp: 0.948820 val loss: 0.590881
[Epoch 55] ogbg-molbbbp: 0.670718 test loss: 1.962589
[Epoch 56; Iter     5/   55] train: loss: 0.0324113
[Epoch 56; Iter    35/   55] train: loss: 0.0223525
[Epoch 56] ogbg-molbbbp: 0.944339 val loss: 0.612227
[Epoch 56] ogbg-molbbbp: 0.646508 test loss: 2.063884
[Epoch 57; Iter    10/   55] train: loss: 0.0485494
[Epoch 57; Iter    40/   55] train: loss: 0.0541496
[Epoch 57] ogbg-molbbbp: 0.941551 val loss: 0.554507
[Epoch 57] ogbg-molbbbp: 0.697145 test loss: 1.558551
[Epoch 58; Iter    15/   55] train: loss: 0.0803598
[Epoch 58; Iter    45/   55] train: loss: 0.0478031
[Epoch 58] ogbg-molbbbp: 0.935179 val loss: 0.590601
[Epoch 58] ogbg-molbbbp: 0.648148 test loss: 1.856727
[Epoch 59; Iter    20/   55] train: loss: 0.0698822
[Epoch 59; Iter    50/   55] train: loss: 0.0934681
[Epoch 59] ogbg-molbbbp: 0.941153 val loss: 0.575321
[Epoch 59] ogbg-molbbbp: 0.634549 test loss: 2.036544
[Epoch 60; Iter    25/   55] train: loss: 0.1035742
[Epoch 60; Iter    55/   55] train: loss: 0.2299813
[Epoch 60] ogbg-molbbbp: 0.934681 val loss: 0.654727
[Epoch 60] ogbg-molbbbp: 0.642458 test loss: 2.115040
[Epoch 61; Iter    30/   55] train: loss: 0.0401036
[Epoch 61] ogbg-molbbbp: 0.951807 val loss: 0.462904
[Epoch 61] ogbg-molbbbp: 0.641975 test loss: 1.976124
[Epoch 62; Iter     5/   55] train: loss: 0.1108263
[Epoch 62; Iter    35/   55] train: loss: 0.1820256
[Epoch 62] ogbg-molbbbp: 0.951807 val loss: 0.472089
[Epoch 62] ogbg-molbbbp: 0.669657 test loss: 1.825904
[Epoch 63; Iter    10/   55] train: loss: 0.0383861
[Epoch 63; Iter    40/   55] train: loss: 0.0185101
[Epoch 63] ogbg-molbbbp: 0.942846 val loss: 0.578446
[Epoch 63] ogbg-molbbbp: 0.652971 test loss: 1.977288
[Epoch 64; Iter    15/   55] train: loss: 0.0092226
[Epoch 64; Iter    45/   55] train: loss: 0.0220219
[Epoch 64] ogbg-molbbbp: 0.951409 val loss: 0.538655
[Epoch 64] ogbg-molbbbp: 0.664834 test loss: 2.191948
[Epoch 65; Iter    20/   55] train: loss: 0.0514100
[Epoch 65; Iter    50/   55] train: loss: 0.0565042
[Epoch 65] ogbg-molbbbp: 0.942746 val loss: 0.670773
[Epoch 65] ogbg-molbbbp: 0.623843 test loss: 2.475327
[Epoch 66; Iter    25/   55] train: loss: 0.0087741
[Epoch 66; Iter    55/   55] train: loss: 0.1398799
[Epoch 66] ogbg-molbbbp: 0.957881 val loss: 0.412489
[Epoch 66] ogbg-molbbbp: 0.671875 test loss: 1.729329
[Epoch 67; Iter    30/   55] train: loss: 0.0327625
[Epoch 67] ogbg-molbbbp: 0.951210 val loss: 0.388749
[Epoch 67] ogbg-molbbbp: 0.638117 test loss: 1.753141
[Epoch 68; Iter     5/   55] train: loss: 0.0481476
[Epoch 68; Iter    35/   55] train: loss: 0.1329612
[Epoch 68] ogbg-molbbbp: 0.931196 val loss: 0.771165
[Epoch 68] ogbg-molbbbp: 0.649209 test loss: 2.027656
[Epoch 69; Iter    10/   55] train: loss: 0.0375313
[Epoch 28] ogbg-molbbbp: 0.680652 test loss: 0.906819
[Epoch 29; Iter    20/   55] train: loss: 0.1635112
[Epoch 29; Iter    50/   55] train: loss: 0.2605624
[Epoch 29] ogbg-molbbbp: 0.952106 val loss: 0.358593
[Epoch 29] ogbg-molbbbp: 0.679784 test loss: 1.047861
[Epoch 30; Iter    25/   55] train: loss: 0.2213949
[Epoch 30; Iter    55/   55] train: loss: 0.1191623
[Epoch 30] ogbg-molbbbp: 0.907697 val loss: 0.685432
[Epoch 30] ogbg-molbbbp: 0.615258 test loss: 3.947702
[Epoch 31; Iter    30/   55] train: loss: 0.3673165
[Epoch 31] ogbg-molbbbp: 0.956786 val loss: 0.274597
[Epoch 31] ogbg-molbbbp: 0.668017 test loss: 1.022313
[Epoch 32; Iter     5/   55] train: loss: 0.1673569
[Epoch 32; Iter    35/   55] train: loss: 0.1564163
[Epoch 32] ogbg-molbbbp: 0.925122 val loss: 0.531841
[Epoch 32] ogbg-molbbbp: 0.673322 test loss: 1.068480
[Epoch 33; Iter    10/   55] train: loss: 0.1005599
[Epoch 33; Iter    40/   55] train: loss: 0.1011002
[Epoch 33] ogbg-molbbbp: 0.940157 val loss: 0.379342
[Epoch 33] ogbg-molbbbp: 0.694155 test loss: 1.262496
[Epoch 34; Iter    15/   55] train: loss: 0.3391555
[Epoch 34; Iter    45/   55] train: loss: 0.1398440
[Epoch 34] ogbg-molbbbp: 0.939361 val loss: 0.536903
[Epoch 34] ogbg-molbbbp: 0.659336 test loss: 1.318961
[Epoch 35; Iter    20/   55] train: loss: 0.1386367
[Epoch 35; Iter    50/   55] train: loss: 0.1531029
[Epoch 35] ogbg-molbbbp: 0.952504 val loss: 0.299168
[Epoch 35] ogbg-molbbbp: 0.695988 test loss: 1.222383
[Epoch 36; Iter    25/   55] train: loss: 0.2111064
[Epoch 36; Iter    55/   55] train: loss: 0.0989173
[Epoch 36] ogbg-molbbbp: 0.955193 val loss: 0.341727
[Epoch 36] ogbg-molbbbp: 0.712481 test loss: 1.156164
[Epoch 37; Iter    30/   55] train: loss: 0.1862798
[Epoch 37] ogbg-molbbbp: 0.954396 val loss: 0.362509
[Epoch 37] ogbg-molbbbp: 0.729649 test loss: 1.161827
[Epoch 38; Iter     5/   55] train: loss: 0.0665828
[Epoch 38; Iter    35/   55] train: loss: 0.1178106
[Epoch 38] ogbg-molbbbp: 0.960868 val loss: 0.300158
[Epoch 38] ogbg-molbbbp: 0.710455 test loss: 1.188312
[Epoch 39; Iter    10/   55] train: loss: 0.0569090
[Epoch 39; Iter    40/   55] train: loss: 0.1803047
[Epoch 39] ogbg-molbbbp: 0.960669 val loss: 0.313312
[Epoch 39] ogbg-molbbbp: 0.649209 test loss: 1.324616
[Epoch 40; Iter    15/   55] train: loss: 0.1731405
[Epoch 40; Iter    45/   55] train: loss: 0.1080066
[Epoch 40] ogbg-molbbbp: 0.971921 val loss: 0.253767
[Epoch 40] ogbg-molbbbp: 0.681906 test loss: 1.358097
[Epoch 41; Iter    20/   55] train: loss: 0.3399099
[Epoch 41; Iter    50/   55] train: loss: 0.0549597
[Epoch 41] ogbg-molbbbp: 0.951807 val loss: 0.404063
[Epoch 41] ogbg-molbbbp: 0.676890 test loss: 1.487686
[Epoch 42; Iter    25/   55] train: loss: 0.0445981
[Epoch 42; Iter    55/   55] train: loss: 0.1538762
[Epoch 42] ogbg-molbbbp: 0.958379 val loss: 0.359437
[Epoch 42] ogbg-molbbbp: 0.689815 test loss: 1.412450
[Epoch 43; Iter    30/   55] train: loss: 0.0746311
[Epoch 43] ogbg-molbbbp: 0.953400 val loss: 0.489899
[Epoch 43] ogbg-molbbbp: 0.700810 test loss: 1.469603
[Epoch 44; Iter     5/   55] train: loss: 0.1454038
[Epoch 44; Iter    35/   55] train: loss: 0.2660872
[Epoch 44] ogbg-molbbbp: 0.955591 val loss: 0.375769
[Epoch 44] ogbg-molbbbp: 0.688657 test loss: 1.388255
[Epoch 45; Iter    10/   55] train: loss: 0.0704900
[Epoch 45; Iter    40/   55] train: loss: 0.0617848
[Epoch 45] ogbg-molbbbp: 0.941850 val loss: 0.453443
[Epoch 45] ogbg-molbbbp: 0.689718 test loss: 1.631110
[Epoch 46; Iter    15/   55] train: loss: 0.2431055
[Epoch 46; Iter    45/   55] train: loss: 0.0530971
[Epoch 46] ogbg-molbbbp: 0.942846 val loss: 0.622693
[Epoch 46] ogbg-molbbbp: 0.663484 test loss: 1.906337
[Epoch 47; Iter    20/   55] train: loss: 0.1724647
[Epoch 47; Iter    50/   55] train: loss: 0.1054578
[Epoch 47] ogbg-molbbbp: 0.966643 val loss: 0.306623
[Epoch 47] ogbg-molbbbp: 0.705343 test loss: 1.495603
[Epoch 48; Iter    25/   55] train: loss: 0.0630511
[Epoch 48; Iter    55/   55] train: loss: 0.4240972
[Epoch 48] ogbg-molbbbp: 0.948820 val loss: 0.410207
[Epoch 48] ogbg-molbbbp: 0.699363 test loss: 1.479107
[Epoch 49; Iter    30/   55] train: loss: 0.0597667
[Epoch 49] ogbg-molbbbp: 0.944638 val loss: 0.397009
[Epoch 49] ogbg-molbbbp: 0.634549 test loss: 1.664447
[Epoch 50; Iter     5/   55] train: loss: 0.0609136
[Epoch 50; Iter    35/   55] train: loss: 0.0405494
[Epoch 50] ogbg-molbbbp: 0.964254 val loss: 0.302625
[Epoch 50] ogbg-molbbbp: 0.679302 test loss: 1.341342
[Epoch 51; Iter    10/   55] train: loss: 0.0381987
[Epoch 51; Iter    40/   55] train: loss: 0.0695963
[Epoch 51] ogbg-molbbbp: 0.965648 val loss: 0.358796
[Epoch 51] ogbg-molbbbp: 0.651813 test loss: 1.682046
[Epoch 52; Iter    15/   55] train: loss: 0.1800359
[Epoch 52; Iter    45/   55] train: loss: 0.0252045
[Epoch 52] ogbg-molbbbp: 0.971323 val loss: 0.265441
[Epoch 52] ogbg-molbbbp: 0.689622 test loss: 1.359250
[Epoch 53; Iter    20/   55] train: loss: 0.0760674
[Epoch 53; Iter    50/   55] train: loss: 0.1128288
[Epoch 53] ogbg-molbbbp: 0.940556 val loss: 0.462082
[Epoch 53] ogbg-molbbbp: 0.612654 test loss: 1.564023
[Epoch 54; Iter    25/   55] train: loss: 0.0404074
[Epoch 54; Iter    55/   55] train: loss: 0.1332806
[Epoch 54] ogbg-molbbbp: 0.963955 val loss: 0.552286
[Epoch 54] ogbg-molbbbp: 0.685860 test loss: 2.303450
[Epoch 55; Iter    30/   55] train: loss: 0.0352468
[Epoch 55] ogbg-molbbbp: 0.919048 val loss: 1.726315
[Epoch 55] ogbg-molbbbp: 0.598573 test loss: 4.533130
[Epoch 56; Iter     5/   55] train: loss: 0.0319391
[Epoch 56; Iter    35/   55] train: loss: 0.0518112
[Epoch 56] ogbg-molbbbp: 0.944339 val loss: 0.415390
[Epoch 56] ogbg-molbbbp: 0.671779 test loss: 1.460437
[Epoch 57; Iter    10/   55] train: loss: 0.0787432
[Epoch 57; Iter    40/   55] train: loss: 0.0898047
[Epoch 57] ogbg-molbbbp: 0.947227 val loss: 0.419818
[Epoch 57] ogbg-molbbbp: 0.686535 test loss: 1.422579
[Epoch 58; Iter    15/   55] train: loss: 0.0862474
[Epoch 58; Iter    45/   55] train: loss: 0.0463484
[Epoch 58] ogbg-molbbbp: 0.959474 val loss: 0.365743
[Epoch 58] ogbg-molbbbp: 0.628086 test loss: 1.737765
[Epoch 59; Iter    20/   55] train: loss: 0.1409850
[Epoch 59; Iter    50/   55] train: loss: 0.3715048
[Epoch 59] ogbg-molbbbp: 0.939659 val loss: 0.504210
[Epoch 59] ogbg-molbbbp: 0.654417 test loss: 1.457916
[Epoch 60; Iter    25/   55] train: loss: 0.0197581
[Epoch 60; Iter    55/   55] train: loss: 0.1548738
[Epoch 60] ogbg-molbbbp: 0.957483 val loss: 0.391497
[Epoch 60] ogbg-molbbbp: 0.646701 test loss: 1.524960
[Epoch 61; Iter    30/   55] train: loss: 0.0543414
[Epoch 61] ogbg-molbbbp: 0.964951 val loss: 0.384862
[Epoch 61] ogbg-molbbbp: 0.699267 test loss: 1.678944
[Epoch 62; Iter     5/   55] train: loss: 0.0892379
[Epoch 62; Iter    35/   55] train: loss: 0.0404483
[Epoch 62] ogbg-molbbbp: 0.955292 val loss: 0.417959
[Epoch 62] ogbg-molbbbp: 0.694830 test loss: 1.669573
[Epoch 63; Iter    10/   55] train: loss: 0.0186102
[Epoch 63; Iter    40/   55] train: loss: 0.0731815
[Epoch 63] ogbg-molbbbp: 0.947924 val loss: 0.447192
[Epoch 63] ogbg-molbbbp: 0.667921 test loss: 1.726521
[Epoch 64; Iter    15/   55] train: loss: 0.0850982
[Epoch 64; Iter    45/   55] train: loss: 0.1016696
[Epoch 64] ogbg-molbbbp: 0.948920 val loss: 0.460650
[Epoch 64] ogbg-molbbbp: 0.661458 test loss: 1.776184
[Epoch 65; Iter    20/   55] train: loss: 0.0122513
[Epoch 65; Iter    50/   55] train: loss: 0.0826102
[Epoch 65] ogbg-molbbbp: 0.947326 val loss: 0.559203
[Epoch 65] ogbg-molbbbp: 0.649498 test loss: 2.044603
[Epoch 66; Iter    25/   55] train: loss: 0.0090796
[Epoch 66; Iter    55/   55] train: loss: 0.0725779
[Epoch 66] ogbg-molbbbp: 0.944538 val loss: 0.534521
[Epoch 66] ogbg-molbbbp: 0.648052 test loss: 2.184768
[Epoch 67; Iter    30/   55] train: loss: 0.1209513
[Epoch 67] ogbg-molbbbp: 0.934482 val loss: 0.561674
[Epoch 67] ogbg-molbbbp: 0.666088 test loss: 1.689866
[Epoch 68; Iter     5/   55] train: loss: 0.0150638
[Epoch 68; Iter    35/   55] train: loss: 0.0033525
[Epoch 68] ogbg-molbbbp: 0.944538 val loss: 0.564944
[Epoch 68] ogbg-molbbbp: 0.653839 test loss: 2.084568
[Epoch 69; Iter    10/   55] train: loss: 0.0935184
[Epoch 28] ogbg-molbbbp: 0.654900 test loss: 1.302014
[Epoch 29; Iter    20/   55] train: loss: 0.1446466
[Epoch 29; Iter    50/   55] train: loss: 0.1983028
[Epoch 29] ogbg-molbbbp: 0.968436 val loss: 0.236065
[Epoch 29] ogbg-molbbbp: 0.658275 test loss: 0.927058
[Epoch 30; Iter    25/   55] train: loss: 0.1966054
[Epoch 30; Iter    55/   55] train: loss: 0.2038235
[Epoch 30] ogbg-molbbbp: 0.884596 val loss: 0.741779
[Epoch 30] ogbg-molbbbp: 0.619888 test loss: 1.147000
[Epoch 31; Iter    30/   55] train: loss: 0.2635919
[Epoch 31] ogbg-molbbbp: 0.956388 val loss: 0.287272
[Epoch 31] ogbg-molbbbp: 0.664545 test loss: 1.088848
[Epoch 32; Iter     5/   55] train: loss: 0.2019038
[Epoch 32; Iter    35/   55] train: loss: 0.2709963
[Epoch 32] ogbg-molbbbp: 0.934083 val loss: 0.333872
[Epoch 32] ogbg-molbbbp: 0.682485 test loss: 0.957774
[Epoch 33; Iter    10/   55] train: loss: 0.1305458
[Epoch 33; Iter    40/   55] train: loss: 0.2842813
[Epoch 33] ogbg-molbbbp: 0.954197 val loss: 0.327395
[Epoch 33] ogbg-molbbbp: 0.683256 test loss: 1.062326
[Epoch 34; Iter    15/   55] train: loss: 0.1217569
[Epoch 34; Iter    45/   55] train: loss: 0.2770430
[Epoch 34] ogbg-molbbbp: 0.959474 val loss: 0.304442
[Epoch 34] ogbg-molbbbp: 0.707369 test loss: 1.125139
[Epoch 35; Iter    20/   55] train: loss: 0.2087261
[Epoch 35; Iter    50/   55] train: loss: 0.1955938
[Epoch 35] ogbg-molbbbp: 0.958180 val loss: 0.559668
[Epoch 35] ogbg-molbbbp: 0.690779 test loss: 1.370830
[Epoch 36; Iter    25/   55] train: loss: 0.3848635
[Epoch 36; Iter    55/   55] train: loss: 0.8887540
[Epoch 36] ogbg-molbbbp: 0.952604 val loss: 0.311321
[Epoch 36] ogbg-molbbbp: 0.672261 test loss: 1.090297
[Epoch 37; Iter    30/   55] train: loss: 0.1095858
[Epoch 37] ogbg-molbbbp: 0.925421 val loss: 0.575220
[Epoch 37] ogbg-molbbbp: 0.655575 test loss: 1.374546
[Epoch 38; Iter     5/   55] train: loss: 0.1312350
[Epoch 38; Iter    35/   55] train: loss: 0.1611690
[Epoch 38] ogbg-molbbbp: 0.967639 val loss: 0.257501
[Epoch 38] ogbg-molbbbp: 0.705343 test loss: 1.270616
[Epoch 39; Iter    10/   55] train: loss: 0.0955983
[Epoch 39; Iter    40/   55] train: loss: 0.0789426
[Epoch 39] ogbg-molbbbp: 0.951907 val loss: 0.420253
[Epoch 39] ogbg-molbbbp: 0.641782 test loss: 1.469737
[Epoch 40; Iter    15/   55] train: loss: 0.0601522
[Epoch 40; Iter    45/   55] train: loss: 0.2293773
[Epoch 40] ogbg-molbbbp: 0.948820 val loss: 0.442122
[Epoch 40] ogbg-molbbbp: 0.683256 test loss: 1.314139
[Epoch 41; Iter    20/   55] train: loss: 0.1987004
[Epoch 41; Iter    50/   55] train: loss: 0.2337875
[Epoch 41] ogbg-molbbbp: 0.952405 val loss: 0.398970
[Epoch 41] ogbg-molbbbp: 0.691647 test loss: 1.463062
[Epoch 42; Iter    25/   55] train: loss: 0.1453734
[Epoch 42; Iter    55/   55] train: loss: 0.3244591
[Epoch 42] ogbg-molbbbp: 0.948322 val loss: 0.363596
[Epoch 42] ogbg-molbbbp: 0.657697 test loss: 1.402244
[Epoch 43; Iter    30/   55] train: loss: 0.1597260
[Epoch 43] ogbg-molbbbp: 0.933088 val loss: 0.539138
[Epoch 43] ogbg-molbbbp: 0.657118 test loss: 1.250415
[Epoch 44; Iter     5/   55] train: loss: 0.1251010
[Epoch 44; Iter    35/   55] train: loss: 0.0593350
[Epoch 44] ogbg-molbbbp: 0.958777 val loss: 0.306133
[Epoch 44] ogbg-molbbbp: 0.722126 test loss: 1.103652
[Epoch 45; Iter    10/   55] train: loss: 0.0435265
[Epoch 45; Iter    40/   55] train: loss: 0.1058443
[Epoch 45] ogbg-molbbbp: 0.957383 val loss: 0.308350
[Epoch 45] ogbg-molbbbp: 0.692901 test loss: 1.183801
[Epoch 46; Iter    15/   55] train: loss: 0.1079394
[Epoch 46; Iter    45/   55] train: loss: 0.1406935
[Epoch 46] ogbg-molbbbp: 0.949716 val loss: 0.426531
[Epoch 46] ogbg-molbbbp: 0.707176 test loss: 1.221809
[Epoch 47; Iter    20/   55] train: loss: 0.2082053
[Epoch 47; Iter    50/   55] train: loss: 0.1497315
[Epoch 47] ogbg-molbbbp: 0.951608 val loss: 0.323603
[Epoch 47] ogbg-molbbbp: 0.645351 test loss: 1.385092
[Epoch 48; Iter    25/   55] train: loss: 0.1466619
[Epoch 48; Iter    55/   55] train: loss: 0.0704396
[Epoch 48] ogbg-molbbbp: 0.959574 val loss: 0.325172
[Epoch 48] ogbg-molbbbp: 0.670428 test loss: 1.401683
[Epoch 49; Iter    30/   55] train: loss: 0.0323640
[Epoch 49] ogbg-molbbbp: 0.969431 val loss: 0.312288
[Epoch 49] ogbg-molbbbp: 0.694927 test loss: 1.330754
[Epoch 50; Iter     5/   55] train: loss: 0.0330969
[Epoch 50; Iter    35/   55] train: loss: 0.0365025
[Epoch 50] ogbg-molbbbp: 0.957085 val loss: 0.304770
[Epoch 50] ogbg-molbbbp: 0.692612 test loss: 1.164952
[Epoch 51; Iter    10/   55] train: loss: 0.0994605
[Epoch 51; Iter    40/   55] train: loss: 0.0617082
[Epoch 51] ogbg-molbbbp: 0.956288 val loss: 0.379020
[Epoch 51] ogbg-molbbbp: 0.695505 test loss: 1.535804
[Epoch 52; Iter    15/   55] train: loss: 0.0172126
[Epoch 52; Iter    45/   55] train: loss: 0.1270802
[Epoch 52] ogbg-molbbbp: 0.957483 val loss: 0.308494
[Epoch 52] ogbg-molbbbp: 0.636767 test loss: 1.265660
[Epoch 53; Iter    20/   55] train: loss: 0.1615512
[Epoch 53; Iter    50/   55] train: loss: 0.1113639
[Epoch 53] ogbg-molbbbp: 0.958379 val loss: 0.334019
[Epoch 53] ogbg-molbbbp: 0.696373 test loss: 1.367445
[Epoch 54; Iter    25/   55] train: loss: 0.1197026
[Epoch 54; Iter    55/   55] train: loss: 0.0130297
[Epoch 54] ogbg-molbbbp: 0.959873 val loss: 0.355791
[Epoch 54] ogbg-molbbbp: 0.681424 test loss: 1.404416
[Epoch 55; Iter    30/   55] train: loss: 0.0757267
[Epoch 55] ogbg-molbbbp: 0.958479 val loss: 0.378403
[Epoch 55] ogbg-molbbbp: 0.686535 test loss: 1.547970
[Epoch 56; Iter     5/   55] train: loss: 0.0898003
[Epoch 56; Iter    35/   55] train: loss: 0.0393642
[Epoch 56] ogbg-molbbbp: 0.961565 val loss: 0.380853
[Epoch 56] ogbg-molbbbp: 0.681038 test loss: 1.662343
[Epoch 57; Iter    10/   55] train: loss: 0.1531141
[Epoch 57; Iter    40/   55] train: loss: 0.2233798
[Epoch 57] ogbg-molbbbp: 0.957981 val loss: 0.401785
[Epoch 57] ogbg-molbbbp: 0.668113 test loss: 1.662511
[Epoch 58; Iter    15/   55] train: loss: 0.1766652
[Epoch 58; Iter    45/   55] train: loss: 0.1468991
[Epoch 58] ogbg-molbbbp: 0.914070 val loss: 1.795905
[Epoch 58] ogbg-molbbbp: 0.644965 test loss: 1.823129
[Epoch 59; Iter    20/   55] train: loss: 0.5884742
[Epoch 59; Iter    50/   55] train: loss: 0.2497533
[Epoch 59] ogbg-molbbbp: 0.959375 val loss: 0.281504
[Epoch 59] ogbg-molbbbp: 0.661169 test loss: 1.203241
[Epoch 60; Iter    25/   55] train: loss: 0.2402237
[Epoch 60; Iter    55/   55] train: loss: 0.2264549
[Epoch 60] ogbg-molbbbp: 0.960072 val loss: 0.296678
[Epoch 60] ogbg-molbbbp: 0.665413 test loss: 1.224149
[Epoch 61; Iter    30/   55] train: loss: 0.2729545
[Epoch 61] ogbg-molbbbp: 0.952106 val loss: 0.333289
[Epoch 61] ogbg-molbbbp: 0.688754 test loss: 1.226841
[Epoch 62; Iter     5/   55] train: loss: 0.1432707
[Epoch 62; Iter    35/   55] train: loss: 0.2042551
[Epoch 62] ogbg-molbbbp: 0.962163 val loss: 0.313645
[Epoch 62] ogbg-molbbbp: 0.678627 test loss: 1.267595
[Epoch 63; Iter    10/   55] train: loss: 0.0970747
[Epoch 63; Iter    40/   55] train: loss: 0.0498994
[Epoch 63] ogbg-molbbbp: 0.959474 val loss: 0.325342
[Epoch 63] ogbg-molbbbp: 0.691744 test loss: 1.355476
[Epoch 64; Iter    15/   55] train: loss: 0.1350020
[Epoch 64; Iter    45/   55] train: loss: 0.2290002
[Epoch 64] ogbg-molbbbp: 0.958080 val loss: 0.331873
[Epoch 64] ogbg-molbbbp: 0.671489 test loss: 1.287275
[Epoch 65; Iter    20/   55] train: loss: 0.1282439
[Epoch 65; Iter    50/   55] train: loss: 0.0266965
[Epoch 65] ogbg-molbbbp: 0.930499 val loss: 0.481850
[Epoch 65] ogbg-molbbbp: 0.627218 test loss: 1.759358
[Epoch 66; Iter    25/   55] train: loss: 0.2461819
[Epoch 66; Iter    55/   55] train: loss: 0.0203933
[Epoch 66] ogbg-molbbbp: 0.954894 val loss: 0.330939
[Epoch 66] ogbg-molbbbp: 0.670910 test loss: 1.414478
[Epoch 67; Iter    30/   55] train: loss: 0.0227184
[Epoch 67] ogbg-molbbbp: 0.950712 val loss: 0.443993
[Epoch 67] ogbg-molbbbp: 0.638021 test loss: 1.550700
[Epoch 68; Iter     5/   55] train: loss: 0.1072467
[Epoch 68; Iter    35/   55] train: loss: 0.1170347
[Epoch 68] ogbg-molbbbp: 0.958777 val loss: 0.331023
[Epoch 68] ogbg-molbbbp: 0.673611 test loss: 1.410351
[Epoch 69; Iter    10/   55] train: loss: 0.0381903
[Epoch 30] ogbg-molbbbp: 0.984047 val loss: 0.270803
[Epoch 30] ogbg-molbbbp: 0.787366 test loss: 0.760949
[Epoch 31; Iter    30/   48] train: loss: 0.2292763
[Epoch 31] ogbg-molbbbp: 0.977478 val loss: 0.338567
[Epoch 31] ogbg-molbbbp: 0.758781 test loss: 0.862163
[Epoch 32; Iter    12/   48] train: loss: 0.2820687
[Epoch 32; Iter    42/   48] train: loss: 0.2957636
[Epoch 32] ogbg-molbbbp: 0.973959 val loss: 0.154110
[Epoch 32] ogbg-molbbbp: 0.763262 test loss: 1.214727
[Epoch 33; Iter    24/   48] train: loss: 0.2281816
[Epoch 33] ogbg-molbbbp: 0.984868 val loss: 0.214811
[Epoch 33] ogbg-molbbbp: 0.787186 test loss: 0.687979
[Epoch 34; Iter     6/   48] train: loss: 0.2009104
[Epoch 34; Iter    36/   48] train: loss: 0.1298922
[Epoch 34] ogbg-molbbbp: 0.982522 val loss: 0.137996
[Epoch 34] ogbg-molbbbp: 0.774642 test loss: 0.714134
[Epoch 35; Iter    18/   48] train: loss: 0.0843061
[Epoch 35; Iter    48/   48] train: loss: 0.2358752
[Epoch 35] ogbg-molbbbp: 0.965279 val loss: 0.778418
[Epoch 35] ogbg-molbbbp: 0.777554 test loss: 1.028261
[Epoch 36; Iter    30/   48] train: loss: 0.1865946
[Epoch 36] ogbg-molbbbp: 0.980762 val loss: 0.110945
[Epoch 36] ogbg-molbbbp: 0.789337 test loss: 0.882659
[Epoch 37; Iter    12/   48] train: loss: 0.2457168
[Epoch 37; Iter    42/   48] train: loss: 0.3489641
[Epoch 37] ogbg-molbbbp: 0.975367 val loss: 0.159117
[Epoch 37] ogbg-molbbbp: 0.793728 test loss: 0.769440
[Epoch 38; Iter    24/   48] train: loss: 0.2073178
[Epoch 38] ogbg-molbbbp: 0.964223 val loss: 0.191896
[Epoch 38] ogbg-molbbbp: 0.783602 test loss: 0.940105
[Epoch 39; Iter     6/   48] train: loss: 0.3116207
[Epoch 39; Iter    36/   48] train: loss: 0.3333367
[Epoch 39] ogbg-molbbbp: 0.965982 val loss: 0.229244
[Epoch 39] ogbg-molbbbp: 0.778405 test loss: 1.045239
[Epoch 40; Iter    18/   48] train: loss: 0.1900215
[Epoch 40; Iter    48/   48] train: loss: 0.2399072
[Epoch 40] ogbg-molbbbp: 0.973372 val loss: 0.098487
[Epoch 40] ogbg-molbbbp: 0.768504 test loss: 1.072482
[Epoch 41; Iter    30/   48] train: loss: 0.1501692
[Epoch 41] ogbg-molbbbp: 0.976305 val loss: 0.110341
[Epoch 41] ogbg-molbbbp: 0.775806 test loss: 1.006340
[Epoch 42; Iter    12/   48] train: loss: 0.0719630
[Epoch 42; Iter    42/   48] train: loss: 0.0967786
[Epoch 42] ogbg-molbbbp: 0.967038 val loss: 0.116881
[Epoch 42] ogbg-molbbbp: 0.764292 test loss: 1.009164
[Epoch 43; Iter    24/   48] train: loss: 0.2114040
[Epoch 43] ogbg-molbbbp: 0.987683 val loss: 0.105411
[Epoch 43] ogbg-molbbbp: 0.809588 test loss: 0.792696
[Epoch 44; Iter     6/   48] train: loss: 0.0596160
[Epoch 44; Iter    36/   48] train: loss: 0.3450637
[Epoch 44] ogbg-molbbbp: 0.972669 val loss: 0.105406
[Epoch 44] ogbg-molbbbp: 0.795833 test loss: 1.026473
[Epoch 45; Iter    18/   48] train: loss: 0.1869396
[Epoch 45; Iter    48/   48] train: loss: 0.4115933
[Epoch 45] ogbg-molbbbp: 0.911437 val loss: 0.166078
[Epoch 45] ogbg-molbbbp: 0.743638 test loss: 1.117890
[Epoch 46; Iter    30/   48] train: loss: 0.3048408
[Epoch 46] ogbg-molbbbp: 0.957302 val loss: 0.129805
[Epoch 46] ogbg-molbbbp: 0.778181 test loss: 1.044539
[Epoch 47; Iter    12/   48] train: loss: 0.1003106
[Epoch 47; Iter    42/   48] train: loss: 0.1478931
[Epoch 47] ogbg-molbbbp: 0.954721 val loss: 0.157728
[Epoch 47] ogbg-molbbbp: 0.794310 test loss: 1.029874
[Epoch 48; Iter    24/   48] train: loss: 0.2054448
[Epoch 48] ogbg-molbbbp: 0.973607 val loss: 0.184505
[Epoch 48] ogbg-molbbbp: 0.806765 test loss: 0.942970
[Epoch 49; Iter     6/   48] train: loss: 0.1533781
[Epoch 49; Iter    36/   48] train: loss: 0.0961308
[Epoch 49] ogbg-molbbbp: 0.975367 val loss: 0.140121
[Epoch 49] ogbg-molbbbp: 0.786783 test loss: 0.882353
[Epoch 50; Iter    18/   48] train: loss: 0.0452534
[Epoch 50; Iter    48/   48] train: loss: 0.3416732
[Epoch 50] ogbg-molbbbp: 0.959765 val loss: 0.115635
[Epoch 50] ogbg-molbbbp: 0.776030 test loss: 0.993748
[Epoch 51; Iter    30/   48] train: loss: 0.0706252
[Epoch 51] ogbg-molbbbp: 0.964457 val loss: 0.118390
[Epoch 51] ogbg-molbbbp: 0.797670 test loss: 1.180358
[Epoch 52; Iter    12/   48] train: loss: 0.1252395
[Epoch 52; Iter    42/   48] train: loss: 0.1184787
[Epoch 52] ogbg-molbbbp: 0.970557 val loss: 0.112734
[Epoch 52] ogbg-molbbbp: 0.780780 test loss: 1.107345
[Epoch 53; Iter    24/   48] train: loss: 0.0967296
[Epoch 53] ogbg-molbbbp: 0.970440 val loss: 0.170841
[Epoch 53] ogbg-molbbbp: 0.762724 test loss: 1.682613
[Epoch 54; Iter     6/   48] train: loss: 0.1384116
[Epoch 54; Iter    36/   48] train: loss: 0.1923824
[Epoch 54] ogbg-molbbbp: 0.963402 val loss: 0.142998
[Epoch 54] ogbg-molbbbp: 0.817518 test loss: 1.137794
[Epoch 55; Iter    18/   48] train: loss: 0.1544780
[Epoch 55; Iter    48/   48] train: loss: 0.1114262
[Epoch 55] ogbg-molbbbp: 0.963754 val loss: 0.110104
[Epoch 55] ogbg-molbbbp: 0.774462 test loss: 1.171411
[Epoch 56; Iter    30/   48] train: loss: 0.0214104
[Epoch 56] ogbg-molbbbp: 0.975015 val loss: 0.267939
[Epoch 56] ogbg-molbbbp: 0.776075 test loss: 1.173168
[Epoch 57; Iter    12/   48] train: loss: 0.1697838
[Epoch 57; Iter    42/   48] train: loss: 0.1889096
[Epoch 57] ogbg-molbbbp: 0.962346 val loss: 0.265596
[Epoch 57] ogbg-molbbbp: 0.791980 test loss: 1.080421
[Epoch 58; Iter    24/   48] train: loss: 0.0478327
[Epoch 58] ogbg-molbbbp: 0.953196 val loss: 0.155738
[Epoch 58] ogbg-molbbbp: 0.807079 test loss: 1.084231
[Epoch 59; Iter     6/   48] train: loss: 0.0311469
[Epoch 59; Iter    36/   48] train: loss: 0.0270694
[Epoch 59] ogbg-molbbbp: 0.956950 val loss: 0.204666
[Epoch 59] ogbg-molbbbp: 0.772043 test loss: 1.165465
[Epoch 60; Iter    18/   48] train: loss: 0.1002018
[Epoch 60; Iter    48/   48] train: loss: 0.0724093
[Epoch 60] ogbg-molbbbp: 0.938182 val loss: 0.197343
[Epoch 60] ogbg-molbbbp: 0.762590 test loss: 1.140560
[Epoch 61; Iter    30/   48] train: loss: 0.3500487
[Epoch 61] ogbg-molbbbp: 0.958123 val loss: 0.207456
[Epoch 61] ogbg-molbbbp: 0.790547 test loss: 1.081962
[Epoch 62; Iter    12/   48] train: loss: 0.0192028
[Epoch 62; Iter    42/   48] train: loss: 0.0263542
[Epoch 62] ogbg-molbbbp: 0.951320 val loss: 0.160507
[Epoch 62] ogbg-molbbbp: 0.770206 test loss: 1.179505
[Epoch 63; Iter    24/   48] train: loss: 0.0319334
[Epoch 63] ogbg-molbbbp: 0.940762 val loss: 0.148234
[Epoch 63] ogbg-molbbbp: 0.781452 test loss: 1.274247
[Epoch 64; Iter     6/   48] train: loss: 0.0638170
[Epoch 64; Iter    36/   48] train: loss: 0.1038595
[Epoch 64] ogbg-molbbbp: 0.932082 val loss: 0.164592
[Epoch 64] ogbg-molbbbp: 0.766891 test loss: 1.199216
[Epoch 65; Iter    18/   48] train: loss: 0.0568376
[Epoch 65; Iter    48/   48] train: loss: 0.2754177
[Epoch 65] ogbg-molbbbp: 0.944985 val loss: 0.160017
[Epoch 65] ogbg-molbbbp: 0.799238 test loss: 1.220376
[Epoch 66; Iter    30/   48] train: loss: 0.1253801
[Epoch 66] ogbg-molbbbp: 0.958006 val loss: 0.160589
[Epoch 66] ogbg-molbbbp: 0.789337 test loss: 1.219900
[Epoch 67; Iter    12/   48] train: loss: 0.0574320
[Epoch 67; Iter    42/   48] train: loss: 0.0890610
[Epoch 67] ogbg-molbbbp: 0.963167 val loss: 0.177855
[Epoch 67] ogbg-molbbbp: 0.772446 test loss: 1.097690
[Epoch 68; Iter    24/   48] train: loss: 0.0303860
[Epoch 68] ogbg-molbbbp: 0.955894 val loss: 0.181622
[Epoch 68] ogbg-molbbbp: 0.787366 test loss: 1.074877
[Epoch 69; Iter     6/   48] train: loss: 0.0135351
[Epoch 69; Iter    36/   48] train: loss: 0.0202286
[Epoch 69] ogbg-molbbbp: 0.949912 val loss: 0.183048
[Epoch 69] ogbg-molbbbp: 0.791487 test loss: 1.199269
[Epoch 70; Iter    18/   48] train: loss: 0.0424414
[Epoch 70; Iter    48/   48] train: loss: 0.1728442
[Epoch 70] ogbg-molbbbp: 0.941232 val loss: 0.136884
[Epoch 70] ogbg-molbbbp: 0.776747 test loss: 1.158892
[Epoch 71; Iter    30/   48] train: loss: 0.1106560
[Epoch 71] ogbg-molbbbp: 0.952845 val loss: 0.169125
[Epoch 71] ogbg-molbbbp: 0.752195 test loss: 1.251161
[Epoch 72; Iter    12/   48] train: loss: 0.0422078
[Epoch 72; Iter    42/   48] train: loss: 0.0175919
[Epoch 72] ogbg-molbbbp: 0.955073 val loss: 0.128286
[Epoch 72] ogbg-molbbbp: 0.772894 test loss: 1.341919
[Epoch 73; Iter    24/   48] train: loss: 0.0337891
[Epoch 73] ogbg-molbbbp: 0.960587 val loss: 0.165115
[Epoch 30] ogbg-molbbbp: 0.952023 val loss: 0.204741
[Epoch 30] ogbg-molbbbp: 0.763665 test loss: 0.801127
[Epoch 31; Iter    30/   48] train: loss: 0.2110632
[Epoch 31] ogbg-molbbbp: 0.986276 val loss: 0.235944
[Epoch 31] ogbg-molbbbp: 0.783199 test loss: 0.717662
[Epoch 32; Iter    12/   48] train: loss: 0.1859306
[Epoch 32; Iter    42/   48] train: loss: 0.2290425
[Epoch 32] ogbg-molbbbp: 0.972434 val loss: 0.239693
[Epoch 32] ogbg-molbbbp: 0.752151 test loss: 0.897960
[Epoch 33; Iter    24/   48] train: loss: 0.1511331
[Epoch 33] ogbg-molbbbp: 0.954604 val loss: 0.285253
[Epoch 33] ogbg-molbbbp: 0.762903 test loss: 1.078000
[Epoch 34; Iter     6/   48] train: loss: 0.1938217
[Epoch 34; Iter    36/   48] train: loss: 0.2336740
[Epoch 34] ogbg-molbbbp: 0.970792 val loss: 0.180455
[Epoch 34] ogbg-molbbbp: 0.779435 test loss: 0.871003
[Epoch 35; Iter    18/   48] train: loss: 0.1732157
[Epoch 35; Iter    48/   48] train: loss: 0.2352790
[Epoch 35] ogbg-molbbbp: 0.982287 val loss: 0.236499
[Epoch 35] ogbg-molbbbp: 0.791039 test loss: 0.673074
[Epoch 36; Iter    30/   48] train: loss: 0.4069858
[Epoch 36] ogbg-molbbbp: 0.961056 val loss: 0.167726
[Epoch 36] ogbg-molbbbp: 0.777285 test loss: 0.886975
[Epoch 37; Iter    12/   48] train: loss: 0.3102407
[Epoch 37; Iter    42/   48] train: loss: 0.2822465
[Epoch 37] ogbg-molbbbp: 0.915191 val loss: 0.170976
[Epoch 37] ogbg-molbbbp: 0.745699 test loss: 1.042275
[Epoch 38; Iter    24/   48] train: loss: 0.2549907
[Epoch 38] ogbg-molbbbp: 0.971261 val loss: 0.123010
[Epoch 38] ogbg-molbbbp: 0.789651 test loss: 0.813198
[Epoch 39; Iter     6/   48] train: loss: 0.3368427
[Epoch 39; Iter    36/   48] train: loss: 0.1454918
[Epoch 39] ogbg-molbbbp: 0.973607 val loss: 0.161376
[Epoch 39] ogbg-molbbbp: 0.781631 test loss: 0.961076
[Epoch 40; Iter    18/   48] train: loss: 0.2196011
[Epoch 40; Iter    48/   48] train: loss: 0.1011644
[Epoch 40] ogbg-molbbbp: 0.989795 val loss: 0.239397
[Epoch 40] ogbg-molbbbp: 0.801210 test loss: 0.753370
[Epoch 41; Iter    30/   48] train: loss: 0.0538770
[Epoch 41] ogbg-molbbbp: 0.993783 val loss: 0.071820
[Epoch 41] ogbg-molbbbp: 0.808020 test loss: 0.880366
[Epoch 42; Iter    12/   48] train: loss: 0.1722227
[Epoch 42; Iter    42/   48] train: loss: 0.1327356
[Epoch 42] ogbg-molbbbp: 0.962698 val loss: 0.134300
[Epoch 42] ogbg-molbbbp: 0.807348 test loss: 0.799425
[Epoch 43; Iter    24/   48] train: loss: 0.2887067
[Epoch 43] ogbg-molbbbp: 0.959883 val loss: 0.105975
[Epoch 43] ogbg-molbbbp: 0.778898 test loss: 0.995150
[Epoch 44; Iter     6/   48] train: loss: 0.0887760
[Epoch 44; Iter    36/   48] train: loss: 0.1400122
[Epoch 44] ogbg-molbbbp: 0.979707 val loss: 0.137760
[Epoch 44] ogbg-molbbbp: 0.777016 test loss: 0.872580
[Epoch 45; Iter    18/   48] train: loss: 0.0815256
[Epoch 45; Iter    48/   48] train: loss: 0.2819608
[Epoch 45] ogbg-molbbbp: 0.980176 val loss: 0.194838
[Epoch 45] ogbg-molbbbp: 0.809543 test loss: 0.770202
[Epoch 46; Iter    30/   48] train: loss: 0.1144589
[Epoch 46] ogbg-molbbbp: 0.971144 val loss: 0.222151
[Epoch 46] ogbg-molbbbp: 0.793100 test loss: 0.874340
[Epoch 47; Iter    12/   48] train: loss: 0.1237254
[Epoch 47; Iter    42/   48] train: loss: 0.0575825
[Epoch 47] ogbg-molbbbp: 0.977126 val loss: 0.123946
[Epoch 47] ogbg-molbbbp: 0.796774 test loss: 0.900311
[Epoch 48; Iter    24/   48] train: loss: 0.0936280
[Epoch 48] ogbg-molbbbp: 0.963871 val loss: 0.155269
[Epoch 48] ogbg-molbbbp: 0.783020 test loss: 0.878300
[Epoch 49; Iter     6/   48] train: loss: 0.0855004
[Epoch 49; Iter    36/   48] train: loss: 0.1055627
[Epoch 49] ogbg-molbbbp: 0.977830 val loss: 0.184373
[Epoch 49] ogbg-molbbbp: 0.809229 test loss: 0.746629
[Epoch 50; Iter    18/   48] train: loss: 0.0852194
[Epoch 50; Iter    48/   48] train: loss: 0.0265601
[Epoch 50] ogbg-molbbbp: 0.969736 val loss: 0.125763
[Epoch 50] ogbg-molbbbp: 0.792921 test loss: 0.969462
[Epoch 51; Iter    30/   48] train: loss: 0.1257737
[Epoch 51] ogbg-molbbbp: 0.944282 val loss: 0.167569
[Epoch 51] ogbg-molbbbp: 0.766129 test loss: 0.988902
[Epoch 52; Iter    12/   48] train: loss: 0.0811847
[Epoch 52; Iter    42/   48] train: loss: 0.1084919
[Epoch 52] ogbg-molbbbp: 0.914135 val loss: 0.171654
[Epoch 52] ogbg-molbbbp: 0.759229 test loss: 1.261611
[Epoch 53; Iter    24/   48] train: loss: 0.1614487
[Epoch 53] ogbg-molbbbp: 0.919296 val loss: 0.168086
[Epoch 53] ogbg-molbbbp: 0.774731 test loss: 1.154905
[Epoch 54; Iter     6/   48] train: loss: 0.1064002
[Epoch 54; Iter    36/   48] train: loss: 0.0706160
[Epoch 54] ogbg-molbbbp: 0.923519 val loss: 0.178789
[Epoch 54] ogbg-molbbbp: 0.807616 test loss: 1.113692
[Epoch 55; Iter    18/   48] train: loss: 0.1187727
[Epoch 55; Iter    48/   48] train: loss: 0.1843357
[Epoch 55] ogbg-molbbbp: 0.981584 val loss: 0.096689
[Epoch 55] ogbg-molbbbp: 0.788620 test loss: 1.171038
[Epoch 56; Iter    30/   48] train: loss: 0.2595619
[Epoch 56] ogbg-molbbbp: 0.967155 val loss: 0.264956
[Epoch 56] ogbg-molbbbp: 0.770430 test loss: 0.994631
[Epoch 57; Iter    12/   48] train: loss: 0.6373441
[Epoch 57; Iter    42/   48] train: loss: 0.1050487
[Epoch 57] ogbg-molbbbp: 0.979707 val loss: 0.142832
[Epoch 57] ogbg-molbbbp: 0.796416 test loss: 0.907022
[Epoch 58; Iter    24/   48] train: loss: 0.0246634
[Epoch 58] ogbg-molbbbp: 0.980997 val loss: 0.152368
[Epoch 58] ogbg-molbbbp: 0.792204 test loss: 0.972083
[Epoch 59; Iter     6/   48] train: loss: 0.0704036
[Epoch 59; Iter    36/   48] train: loss: 0.0721520
[Epoch 59] ogbg-molbbbp: 0.963636 val loss: 0.137092
[Epoch 59] ogbg-molbbbp: 0.770968 test loss: 1.099432
[Epoch 60; Iter    18/   48] train: loss: 0.1131042
[Epoch 60; Iter    48/   48] train: loss: 0.0622571
[Epoch 60] ogbg-molbbbp: 0.957771 val loss: 0.148374
[Epoch 60] ogbg-molbbbp: 0.767159 test loss: 0.991574
[Epoch 61; Iter    30/   48] train: loss: 0.1448170
[Epoch 61] ogbg-molbbbp: 0.981584 val loss: 0.204107
[Epoch 61] ogbg-molbbbp: 0.792921 test loss: 0.928149
[Epoch 62; Iter    12/   48] train: loss: 0.0170427
[Epoch 62; Iter    42/   48] train: loss: 0.0469377
[Epoch 62] ogbg-molbbbp: 0.960352 val loss: 0.235664
[Epoch 62] ogbg-molbbbp: 0.746819 test loss: 1.138259
[Epoch 63; Iter    24/   48] train: loss: 0.2206706
[Epoch 63] ogbg-molbbbp: 0.980059 val loss: 0.096152
[Epoch 63] ogbg-molbbbp: 0.754525 test loss: 1.447733
[Epoch 64; Iter     6/   48] train: loss: 0.0339050
[Epoch 64; Iter    36/   48] train: loss: 0.0532451
[Epoch 64] ogbg-molbbbp: 0.966217 val loss: 0.163511
[Epoch 64] ogbg-molbbbp: 0.769848 test loss: 0.996051
[Epoch 65; Iter    18/   48] train: loss: 0.0358644
[Epoch 65; Iter    48/   48] train: loss: 0.2248596
[Epoch 65] ogbg-molbbbp: 0.986276 val loss: 0.114160
[Epoch 65] ogbg-molbbbp: 0.785036 test loss: 1.060342
[Epoch 66; Iter    30/   48] train: loss: 0.0170564
[Epoch 66] ogbg-molbbbp: 0.960000 val loss: 0.117235
[Epoch 66] ogbg-molbbbp: 0.766084 test loss: 1.367888
[Epoch 67; Iter    12/   48] train: loss: 0.0177497
[Epoch 67; Iter    42/   48] train: loss: 0.0231184
[Epoch 67] ogbg-molbbbp: 0.955425 val loss: 0.206028
[Epoch 67] ogbg-molbbbp: 0.782527 test loss: 0.985078
[Epoch 68; Iter    24/   48] train: loss: 0.2894562
[Epoch 68] ogbg-molbbbp: 0.976422 val loss: 0.110133
[Epoch 68] ogbg-molbbbp: 0.765457 test loss: 1.140456
[Epoch 69; Iter     6/   48] train: loss: 0.1258768
[Epoch 69; Iter    36/   48] train: loss: 0.1964599
[Epoch 69] ogbg-molbbbp: 0.986628 val loss: 0.095140
[Epoch 69] ogbg-molbbbp: 0.769713 test loss: 1.241225
[Epoch 70; Iter    18/   48] train: loss: 0.3491047
[Epoch 70; Iter    48/   48] train: loss: 0.2952440
[Epoch 70] ogbg-molbbbp: 0.971261 val loss: 0.156841
[Epoch 70] ogbg-molbbbp: 0.766980 test loss: 1.093765
[Epoch 71; Iter    30/   48] train: loss: 0.0204951
[Epoch 71] ogbg-molbbbp: 0.990147 val loss: 0.147054
[Epoch 71] ogbg-molbbbp: 0.786962 test loss: 1.305981
[Epoch 72; Iter    12/   48] train: loss: 0.0141092
[Epoch 72; Iter    42/   48] train: loss: 0.1057736
[Epoch 72] ogbg-molbbbp: 0.968446 val loss: 0.150146
[Epoch 72] ogbg-molbbbp: 0.764068 test loss: 1.002017
[Epoch 73; Iter    24/   48] train: loss: 0.0119699
[Epoch 73] ogbg-molbbbp: 0.980411 val loss: 0.166144
[Epoch 30] ogbg-molbbbp: 0.976188 val loss: 0.152365
[Epoch 30] ogbg-molbbbp: 0.756452 test loss: 0.969330
[Epoch 31; Iter    30/   48] train: loss: 0.1599849
[Epoch 31] ogbg-molbbbp: 0.976774 val loss: 0.200171
[Epoch 31] ogbg-molbbbp: 0.785394 test loss: 0.733342
[Epoch 32; Iter    12/   48] train: loss: 0.2168832
[Epoch 32; Iter    42/   48] train: loss: 0.2598172
[Epoch 32] ogbg-molbbbp: 0.982991 val loss: 0.177697
[Epoch 32] ogbg-molbbbp: 0.796192 test loss: 0.778143
[Epoch 33; Iter    24/   48] train: loss: 0.2670355
[Epoch 33] ogbg-molbbbp: 0.974780 val loss: 0.289841
[Epoch 33] ogbg-molbbbp: 0.770609 test loss: 0.880961
[Epoch 34; Iter     6/   48] train: loss: 0.2429105
[Epoch 34; Iter    36/   48] train: loss: 0.1841601
[Epoch 34] ogbg-molbbbp: 0.984047 val loss: 0.156143
[Epoch 34] ogbg-molbbbp: 0.787858 test loss: 0.931941
[Epoch 35; Iter    18/   48] train: loss: 0.1082108
[Epoch 35; Iter    48/   48] train: loss: 0.1463143
[Epoch 35] ogbg-molbbbp: 0.984633 val loss: 0.224057
[Epoch 35] ogbg-molbbbp: 0.784185 test loss: 1.158864
[Epoch 36; Iter    30/   48] train: loss: 0.1899906
[Epoch 36] ogbg-molbbbp: 0.978534 val loss: 0.321622
[Epoch 36] ogbg-molbbbp: 0.795027 test loss: 0.666759
[Epoch 37; Iter    12/   48] train: loss: 0.1528516
[Epoch 37; Iter    42/   48] train: loss: 0.4122832
[Epoch 37] ogbg-molbbbp: 0.975249 val loss: 0.141664
[Epoch 37] ogbg-molbbbp: 0.790412 test loss: 0.802659
[Epoch 38; Iter    24/   48] train: loss: 0.3846309
[Epoch 38] ogbg-molbbbp: 0.969853 val loss: 0.116468
[Epoch 38] ogbg-molbbbp: 0.770430 test loss: 0.934922
[Epoch 39; Iter     6/   48] train: loss: 0.2059047
[Epoch 39; Iter    36/   48] train: loss: 0.0658370
[Epoch 39] ogbg-molbbbp: 0.979120 val loss: 0.148046
[Epoch 39] ogbg-molbbbp: 0.803136 test loss: 0.778989
[Epoch 40; Iter    18/   48] train: loss: 0.2006120
[Epoch 40; Iter    48/   48] train: loss: 0.0922249
[Epoch 40] ogbg-molbbbp: 0.974428 val loss: 0.121770
[Epoch 40] ogbg-molbbbp: 0.815054 test loss: 0.696087
[Epoch 41; Iter    30/   48] train: loss: 0.1523433
[Epoch 41] ogbg-molbbbp: 0.928446 val loss: 0.148339
[Epoch 41] ogbg-molbbbp: 0.788575 test loss: 0.883899
[Epoch 42; Iter    12/   48] train: loss: 0.1744099
[Epoch 42; Iter    42/   48] train: loss: 0.2607467
[Epoch 42] ogbg-molbbbp: 0.981584 val loss: 0.096684
[Epoch 42] ogbg-molbbbp: 0.781496 test loss: 1.083100
[Epoch 43; Iter    24/   48] train: loss: 0.0900134
[Epoch 43] ogbg-molbbbp: 0.973607 val loss: 0.142940
[Epoch 43] ogbg-molbbbp: 0.756496 test loss: 0.976860
[Epoch 44; Iter     6/   48] train: loss: 0.0791535
[Epoch 44; Iter    36/   48] train: loss: 0.0618460
[Epoch 44] ogbg-molbbbp: 0.990616 val loss: 0.153201
[Epoch 44] ogbg-molbbbp: 0.769803 test loss: 0.720877
[Epoch 45; Iter    18/   48] train: loss: 0.1433465
[Epoch 45; Iter    48/   48] train: loss: 0.0542638
[Epoch 45] ogbg-molbbbp: 0.978299 val loss: 0.144970
[Epoch 45] ogbg-molbbbp: 0.788978 test loss: 0.920130
[Epoch 46; Iter    30/   48] train: loss: 0.0917745
[Epoch 46] ogbg-molbbbp: 0.966452 val loss: 0.120121
[Epoch 46] ogbg-molbbbp: 0.769489 test loss: 1.057156
[Epoch 47; Iter    12/   48] train: loss: 0.0327915
[Epoch 47; Iter    42/   48] train: loss: 0.1331065
[Epoch 47] ogbg-molbbbp: 0.967742 val loss: 0.129297
[Epoch 47] ogbg-molbbbp: 0.797760 test loss: 0.931245
[Epoch 48; Iter    24/   48] train: loss: 0.1566047
[Epoch 48] ogbg-molbbbp: 0.972317 val loss: 0.108730
[Epoch 48] ogbg-molbbbp: 0.728763 test loss: 1.079073
[Epoch 49; Iter     6/   48] train: loss: 0.1461497
[Epoch 49; Iter    36/   48] train: loss: 0.0855822
[Epoch 49] ogbg-molbbbp: 0.967977 val loss: 0.205622
[Epoch 49] ogbg-molbbbp: 0.763665 test loss: 0.986086
[Epoch 50; Iter    18/   48] train: loss: 0.1056581
[Epoch 50; Iter    48/   48] train: loss: 0.1430179
[Epoch 50] ogbg-molbbbp: 0.951085 val loss: 0.146596
[Epoch 50] ogbg-molbbbp: 0.806586 test loss: 1.036132
[Epoch 51; Iter    30/   48] train: loss: 0.1594143
[Epoch 51] ogbg-molbbbp: 0.979472 val loss: 0.133840
[Epoch 51] ogbg-molbbbp: 0.793548 test loss: 0.983576
[Epoch 52; Iter    12/   48] train: loss: 0.0541082
[Epoch 52; Iter    42/   48] train: loss: 0.1118608
[Epoch 52] ogbg-molbbbp: 0.941818 val loss: 0.137090
[Epoch 52] ogbg-molbbbp: 0.760887 test loss: 1.271570
[Epoch 53; Iter    24/   48] train: loss: 0.3186068
[Epoch 53] ogbg-molbbbp: 0.980293 val loss: 0.098480
[Epoch 53] ogbg-molbbbp: 0.790726 test loss: 1.012330
[Epoch 54; Iter     6/   48] train: loss: 0.1547200
[Epoch 54; Iter    36/   48] train: loss: 0.1824426
[Epoch 54] ogbg-molbbbp: 0.986510 val loss: 0.141568
[Epoch 54] ogbg-molbbbp: 0.799104 test loss: 0.819332
[Epoch 55; Iter    18/   48] train: loss: 0.0906946
[Epoch 55; Iter    48/   48] train: loss: 0.2069541
[Epoch 55] ogbg-molbbbp: 0.982991 val loss: 0.108839
[Epoch 55] ogbg-molbbbp: 0.776210 test loss: 1.167242
[Epoch 56; Iter    30/   48] train: loss: 0.0946274
[Epoch 56] ogbg-molbbbp: 0.964692 val loss: 0.135413
[Epoch 56] ogbg-molbbbp: 0.777151 test loss: 1.040608
[Epoch 57; Iter    12/   48] train: loss: 0.0219494
[Epoch 57; Iter    42/   48] train: loss: 0.1483912
[Epoch 57] ogbg-molbbbp: 0.977009 val loss: 0.130365
[Epoch 57] ogbg-molbbbp: 0.771057 test loss: 0.991272
[Epoch 58; Iter    24/   48] train: loss: 0.0762332
[Epoch 58] ogbg-molbbbp: 0.977595 val loss: 0.098847
[Epoch 58] ogbg-molbbbp: 0.757034 test loss: 1.286269
[Epoch 59; Iter     6/   48] train: loss: 0.0377021
[Epoch 59; Iter    36/   48] train: loss: 0.3913053
[Epoch 59] ogbg-molbbbp: 0.965161 val loss: 0.119941
[Epoch 59] ogbg-molbbbp: 0.789606 test loss: 1.172156
[Epoch 60; Iter    18/   48] train: loss: 0.0662968
[Epoch 60; Iter    48/   48] train: loss: 0.0990873
[Epoch 60] ogbg-molbbbp: 0.964457 val loss: 0.126799
[Epoch 60] ogbg-molbbbp: 0.766398 test loss: 1.125454
[Epoch 61; Iter    30/   48] train: loss: 0.0725370
[Epoch 61] ogbg-molbbbp: 0.954252 val loss: 0.235853
[Epoch 61] ogbg-molbbbp: 0.770923 test loss: 1.136882
[Epoch 62; Iter    12/   48] train: loss: 0.1289142
[Epoch 62; Iter    42/   48] train: loss: 0.0668777
[Epoch 62] ogbg-molbbbp: 0.968211 val loss: 0.140864
[Epoch 62] ogbg-molbbbp: 0.785797 test loss: 1.117976
[Epoch 63; Iter    24/   48] train: loss: 0.2766947
[Epoch 63] ogbg-molbbbp: 0.954252 val loss: 0.154541
[Epoch 63] ogbg-molbbbp: 0.765143 test loss: 1.114536
[Epoch 64; Iter     6/   48] train: loss: 0.0283668
[Epoch 64; Iter    36/   48] train: loss: 0.0650737
[Epoch 64] ogbg-molbbbp: 0.923871 val loss: 0.194506
[Epoch 64] ogbg-molbbbp: 0.754928 test loss: 1.316164
[Epoch 65; Iter    18/   48] train: loss: 0.0083195
[Epoch 65; Iter    48/   48] train: loss: 0.0823716
[Epoch 65] ogbg-molbbbp: 0.973138 val loss: 0.163153
[Epoch 65] ogbg-molbbbp: 0.803629 test loss: 0.907310
[Epoch 66; Iter    30/   48] train: loss: 0.0880158
[Epoch 66] ogbg-molbbbp: 0.911320 val loss: 0.192911
[Epoch 66] ogbg-molbbbp: 0.753853 test loss: 1.502820
[Epoch 67; Iter    12/   48] train: loss: 0.1182223
[Epoch 67; Iter    42/   48] train: loss: 0.0324296
[Epoch 67] ogbg-molbbbp: 0.935249 val loss: 0.437855
[Epoch 67] ogbg-molbbbp: 0.737231 test loss: 1.878958
[Epoch 68; Iter    24/   48] train: loss: 0.2157975
[Epoch 68] ogbg-molbbbp: 0.960704 val loss: 0.137514
[Epoch 68] ogbg-molbbbp: 0.772894 test loss: 1.153920
[Epoch 69; Iter     6/   48] train: loss: 0.0250520
[Epoch 69; Iter    36/   48] train: loss: 0.0655579
[Epoch 69] ogbg-molbbbp: 0.973255 val loss: 0.126640
[Epoch 69] ogbg-molbbbp: 0.778360 test loss: 1.217636
[Epoch 70; Iter    18/   48] train: loss: 0.0995291
[Epoch 70; Iter    48/   48] train: loss: 0.1042949
[Epoch 70] ogbg-molbbbp: 0.972903 val loss: 0.104505
[Epoch 70] ogbg-molbbbp: 0.766532 test loss: 1.114902
[Epoch 71; Iter    30/   48] train: loss: 0.0505393
[Epoch 71] ogbg-molbbbp: 0.944282 val loss: 0.187498
[Epoch 71] ogbg-molbbbp: 0.800851 test loss: 1.139345
[Epoch 72; Iter    12/   48] train: loss: 0.1602653
[Epoch 72; Iter    42/   48] train: loss: 0.0551023
[Epoch 72] ogbg-molbbbp: 0.918710 val loss: 0.206607
[Epoch 72] ogbg-molbbbp: 0.769444 test loss: 1.242063
[Epoch 73; Iter    24/   48] train: loss: 0.0209156
[Epoch 73] ogbg-molbbbp: 0.975836 val loss: 0.109644
[Epoch 69; Iter    40/   55] train: loss: 0.0080285
[Epoch 69] ogbg-molbbbp: 0.949816 val loss: 0.426931
[Epoch 69] ogbg-molbbbp: 0.650077 test loss: 1.898928
[Epoch 70; Iter    15/   55] train: loss: 0.0633974
[Epoch 70; Iter    45/   55] train: loss: 0.0646269
[Epoch 70] ogbg-molbbbp: 0.929802 val loss: 0.639114
[Epoch 70] ogbg-molbbbp: 0.629630 test loss: 2.059343
[Epoch 71; Iter    20/   55] train: loss: 0.0240673
[Epoch 71; Iter    50/   55] train: loss: 0.1325531
[Epoch 71] ogbg-molbbbp: 0.958877 val loss: 0.501498
[Epoch 71] ogbg-molbbbp: 0.676022 test loss: 2.250377
[Epoch 72; Iter    25/   55] train: loss: 0.1467570
[Epoch 72; Iter    55/   55] train: loss: 0.0509727
[Epoch 72] ogbg-molbbbp: 0.924027 val loss: 0.714149
[Epoch 72] ogbg-molbbbp: 0.657504 test loss: 2.277847
[Epoch 73; Iter    30/   55] train: loss: 0.1099052
[Epoch 73] ogbg-molbbbp: 0.946829 val loss: 0.571309
[Epoch 73] ogbg-molbbbp: 0.660687 test loss: 2.030083
[Epoch 74; Iter     5/   55] train: loss: 0.0336408
[Epoch 74; Iter    35/   55] train: loss: 0.0122267
[Epoch 74] ogbg-molbbbp: 0.952106 val loss: 0.441356
[Epoch 74] ogbg-molbbbp: 0.684992 test loss: 1.678973
[Epoch 75; Iter    10/   55] train: loss: 0.0338536
[Epoch 75; Iter    40/   55] train: loss: 0.0517125
[Epoch 75] ogbg-molbbbp: 0.943344 val loss: 0.814027
[Epoch 75] ogbg-molbbbp: 0.638407 test loss: 2.237512
[Epoch 76; Iter    15/   55] train: loss: 0.0728647
[Epoch 76; Iter    45/   55] train: loss: 0.0822321
[Epoch 76] ogbg-molbbbp: 0.953998 val loss: 0.564944
[Epoch 76] ogbg-molbbbp: 0.639178 test loss: 2.176914
[Epoch 77; Iter    20/   55] train: loss: 0.0183157
[Epoch 77; Iter    50/   55] train: loss: 0.0067167
[Epoch 77] ogbg-molbbbp: 0.935577 val loss: 0.643723
[Epoch 77] ogbg-molbbbp: 0.680941 test loss: 2.020704
[Epoch 78; Iter    25/   55] train: loss: 0.0535957
[Epoch 78; Iter    55/   55] train: loss: 0.4032736
[Epoch 78] ogbg-molbbbp: 0.950911 val loss: 0.386896
[Epoch 78] ogbg-molbbbp: 0.652199 test loss: 1.681373
[Epoch 79; Iter    30/   55] train: loss: 0.0214465
[Epoch 79] ogbg-molbbbp: 0.953301 val loss: 0.492800
[Epoch 79] ogbg-molbbbp: 0.651138 test loss: 2.081966
[Epoch 80; Iter     5/   55] train: loss: 0.1475234
[Epoch 80; Iter    35/   55] train: loss: 0.0246394
[Epoch 80] ogbg-molbbbp: 0.951110 val loss: 0.590436
[Epoch 80] ogbg-molbbbp: 0.626350 test loss: 2.174757
[Epoch 81; Iter    10/   55] train: loss: 0.0223811
[Epoch 81; Iter    40/   55] train: loss: 0.0149781
[Epoch 81] ogbg-molbbbp: 0.945534 val loss: 0.624730
[Epoch 81] ogbg-molbbbp: 0.653453 test loss: 2.143121
[Epoch 82; Iter    15/   55] train: loss: 0.0140349
[Epoch 82; Iter    45/   55] train: loss: 0.0519478
[Epoch 82] ogbg-molbbbp: 0.938166 val loss: 0.679989
[Epoch 82] ogbg-molbbbp: 0.637731 test loss: 2.466254
[Epoch 83; Iter    20/   55] train: loss: 0.0193724
[Epoch 83; Iter    50/   55] train: loss: 0.0764371
[Epoch 83] ogbg-molbbbp: 0.941352 val loss: 0.714362
[Epoch 83] ogbg-molbbbp: 0.652874 test loss: 2.254732
[Epoch 84; Iter    25/   55] train: loss: 0.0614220
[Epoch 84; Iter    55/   55] train: loss: 0.0783608
[Epoch 84] ogbg-molbbbp: 0.933586 val loss: 0.720953
[Epoch 84] ogbg-molbbbp: 0.641397 test loss: 2.300795
[Epoch 85; Iter    30/   55] train: loss: 0.0042999
[Epoch 85] ogbg-molbbbp: 0.955491 val loss: 0.462661
[Epoch 85] ogbg-molbbbp: 0.647859 test loss: 1.897895
[Epoch 86; Iter     5/   55] train: loss: 0.0075892
[Epoch 86; Iter    35/   55] train: loss: 0.0054275
[Epoch 86] ogbg-molbbbp: 0.941352 val loss: 0.568051
[Epoch 86] ogbg-molbbbp: 0.637442 test loss: 2.248379
[Epoch 87; Iter    10/   55] train: loss: 0.0284381
[Epoch 87; Iter    40/   55] train: loss: 0.1246279
[Epoch 87] ogbg-molbbbp: 0.952903 val loss: 0.487339
[Epoch 87] ogbg-molbbbp: 0.654900 test loss: 2.015095
[Epoch 88; Iter    15/   55] train: loss: 0.0091956
[Epoch 88; Iter    45/   55] train: loss: 0.0419162
[Epoch 88] ogbg-molbbbp: 0.950115 val loss: 0.623001
[Epoch 88] ogbg-molbbbp: 0.669560 test loss: 2.342856
[Epoch 89; Iter    20/   55] train: loss: 0.1035229
[Epoch 89; Iter    50/   55] train: loss: 0.1505503
[Epoch 89] ogbg-molbbbp: 0.933685 val loss: 0.799353
[Epoch 89] ogbg-molbbbp: 0.627315 test loss: 2.170659
[Epoch 90; Iter    25/   55] train: loss: 0.0153477
[Epoch 90; Iter    55/   55] train: loss: 0.2142667
[Epoch 90] ogbg-molbbbp: 0.914169 val loss: 1.227731
[Epoch 90] ogbg-molbbbp: 0.577739 test loss: 2.884050
[Epoch 91; Iter    30/   55] train: loss: 0.0277115
[Epoch 91] ogbg-molbbbp: 0.939759 val loss: 0.570594
[Epoch 91] ogbg-molbbbp: 0.627604 test loss: 2.037723
[Epoch 92; Iter     5/   55] train: loss: 0.0564882
[Epoch 92; Iter    35/   55] train: loss: 0.0138527
[Epoch 92] ogbg-molbbbp: 0.947028 val loss: 0.612852
[Epoch 92] ogbg-molbbbp: 0.611979 test loss: 2.232009
[Epoch 93; Iter    10/   55] train: loss: 0.0089847
[Epoch 93; Iter    40/   55] train: loss: 0.0161031
[Epoch 93] ogbg-molbbbp: 0.934780 val loss: 0.538702
[Epoch 93] ogbg-molbbbp: 0.670718 test loss: 1.780377
[Epoch 94; Iter    15/   55] train: loss: 0.0158835
[Epoch 94; Iter    45/   55] train: loss: 0.0604997
[Epoch 94] ogbg-molbbbp: 0.939162 val loss: 0.628100
[Epoch 94] ogbg-molbbbp: 0.626543 test loss: 2.453229
[Epoch 95; Iter    20/   55] train: loss: 0.0918387
[Epoch 95; Iter    50/   55] train: loss: 0.0043954
[Epoch 95] ogbg-molbbbp: 0.950612 val loss: 0.470350
[Epoch 95] ogbg-molbbbp: 0.672550 test loss: 1.795329
[Epoch 96; Iter    25/   55] train: loss: 0.1033124
[Epoch 96; Iter    55/   55] train: loss: 0.0315697
[Epoch 96] ogbg-molbbbp: 0.942647 val loss: 0.687872
[Epoch 96] ogbg-molbbbp: 0.634163 test loss: 2.352293
[Epoch 97; Iter    30/   55] train: loss: 0.0522105
[Epoch 97] ogbg-molbbbp: 0.949418 val loss: 0.500083
[Epoch 97] ogbg-molbbbp: 0.652006 test loss: 1.936298
[Epoch 98; Iter     5/   55] train: loss: 0.0043210
[Epoch 98; Iter    35/   55] train: loss: 0.0266890
[Epoch 98] ogbg-molbbbp: 0.947924 val loss: 0.541797
[Epoch 98] ogbg-molbbbp: 0.666570 test loss: 2.148652
[Epoch 99; Iter    10/   55] train: loss: 0.1248360
[Epoch 99; Iter    40/   55] train: loss: 0.0051759
[Epoch 99] ogbg-molbbbp: 0.939162 val loss: 0.724909
[Epoch 99] ogbg-molbbbp: 0.643519 test loss: 2.556205
[Epoch 100; Iter    15/   55] train: loss: 0.0044922
[Epoch 100; Iter    45/   55] train: loss: 0.0093953
[Epoch 100] ogbg-molbbbp: 0.945235 val loss: 0.620731
[Epoch 100] ogbg-molbbbp: 0.662230 test loss: 2.376385
[Epoch 101; Iter    20/   55] train: loss: 0.0027728
[Epoch 101; Iter    50/   55] train: loss: 0.1318536
[Epoch 101] ogbg-molbbbp: 0.935876 val loss: 0.695576
[Epoch 101] ogbg-molbbbp: 0.636863 test loss: 2.278637
[Epoch 102; Iter    25/   55] train: loss: 0.0318157
[Epoch 102; Iter    55/   55] train: loss: 0.0074520
[Epoch 102] ogbg-molbbbp: 0.944638 val loss: 0.565717
[Epoch 102] ogbg-molbbbp: 0.642361 test loss: 2.584232
[Epoch 103; Iter    30/   55] train: loss: 0.0197771
[Epoch 103] ogbg-molbbbp: 0.945932 val loss: 0.653461
[Epoch 103] ogbg-molbbbp: 0.655864 test loss: 2.566121
[Epoch 104; Iter     5/   55] train: loss: 0.0110439
[Epoch 104; Iter    35/   55] train: loss: 0.0163578
[Epoch 104] ogbg-molbbbp: 0.945235 val loss: 0.611461
[Epoch 104] ogbg-molbbbp: 0.643519 test loss: 2.447320
[Epoch 105; Iter    10/   55] train: loss: 0.0597473
[Epoch 105; Iter    40/   55] train: loss: 0.0989891
[Epoch 105] ogbg-molbbbp: 0.932192 val loss: 0.822846
[Epoch 105] ogbg-molbbbp: 0.626543 test loss: 2.649779
[Epoch 106; Iter    15/   55] train: loss: 0.2918175
[Epoch 106; Iter    45/   55] train: loss: 0.0184937
[Epoch 106] ogbg-molbbbp: 0.949716 val loss: 0.559372
[Epoch 106] ogbg-molbbbp: 0.613233 test loss: 2.569385
[Epoch 107; Iter    20/   55] train: loss: 0.0071854
[Epoch 107; Iter    50/   55] train: loss: 0.0053771
[Epoch 107] ogbg-molbbbp: 0.942049 val loss: 0.556660
[Epoch 107] ogbg-molbbbp: 0.665220 test loss: 1.981317
[Epoch 108; Iter    25/   55] train: loss: 0.0086652
[Epoch 108; Iter    55/   55] train: loss: 0.0996813
[Epoch 108] ogbg-molbbbp: 0.926715 val loss: 0.760360
[Epoch 108] ogbg-molbbbp: 0.615741 test loss: 2.398760
[Epoch 109; Iter    30/   55] train: loss: 0.0240476
[Epoch 69; Iter    40/   55] train: loss: 0.0413583
[Epoch 69] ogbg-molbbbp: 0.948721 val loss: 0.423636
[Epoch 69] ogbg-molbbbp: 0.662326 test loss: 1.701772
[Epoch 70; Iter    15/   55] train: loss: 0.1544138
[Epoch 70; Iter    45/   55] train: loss: 0.0633121
[Epoch 70] ogbg-molbbbp: 0.951409 val loss: 0.566889
[Epoch 70] ogbg-molbbbp: 0.692805 test loss: 1.981816
[Epoch 71; Iter    20/   55] train: loss: 0.0147621
[Epoch 71; Iter    50/   55] train: loss: 0.0094963
[Epoch 71] ogbg-molbbbp: 0.959076 val loss: 0.440347
[Epoch 71] ogbg-molbbbp: 0.646701 test loss: 1.812810
[Epoch 72; Iter    25/   55] train: loss: 0.0431703
[Epoch 72; Iter    55/   55] train: loss: 0.5906502
[Epoch 72] ogbg-molbbbp: 0.949119 val loss: 0.627293
[Epoch 72] ogbg-molbbbp: 0.640818 test loss: 1.976257
[Epoch 73; Iter    30/   55] train: loss: 0.0261278
[Epoch 73] ogbg-molbbbp: 0.953898 val loss: 0.535809
[Epoch 73] ogbg-molbbbp: 0.629726 test loss: 2.086327
[Epoch 74; Iter     5/   55] train: loss: 0.0237918
[Epoch 74; Iter    35/   55] train: loss: 0.0156517
[Epoch 74] ogbg-molbbbp: 0.948223 val loss: 0.529386
[Epoch 74] ogbg-molbbbp: 0.661844 test loss: 1.884723
[Epoch 75; Iter    10/   55] train: loss: 0.0585395
[Epoch 75; Iter    40/   55] train: loss: 0.0657585
[Epoch 75] ogbg-molbbbp: 0.954695 val loss: 0.543990
[Epoch 75] ogbg-molbbbp: 0.656539 test loss: 2.004140
[Epoch 76; Iter    15/   55] train: loss: 0.1462236
[Epoch 76; Iter    45/   55] train: loss: 0.0106434
[Epoch 76] ogbg-molbbbp: 0.955093 val loss: 0.438766
[Epoch 76] ogbg-molbbbp: 0.676890 test loss: 1.706638
[Epoch 77; Iter    20/   55] train: loss: 0.0297339
[Epoch 77; Iter    50/   55] train: loss: 0.0340284
[Epoch 77] ogbg-molbbbp: 0.946231 val loss: 0.632992
[Epoch 77] ogbg-molbbbp: 0.676987 test loss: 2.076371
[Epoch 78; Iter    25/   55] train: loss: 0.0898645
[Epoch 78; Iter    55/   55] train: loss: 0.0711930
[Epoch 78] ogbg-molbbbp: 0.952504 val loss: 0.504802
[Epoch 78] ogbg-molbbbp: 0.683160 test loss: 2.030776
[Epoch 79; Iter    30/   55] train: loss: 0.0421533
[Epoch 79] ogbg-molbbbp: 0.954097 val loss: 0.506657
[Epoch 79] ogbg-molbbbp: 0.662809 test loss: 1.877920
[Epoch 80; Iter     5/   55] train: loss: 0.0372895
[Epoch 80; Iter    35/   55] train: loss: 0.0243015
[Epoch 80] ogbg-molbbbp: 0.939659 val loss: 0.702474
[Epoch 80] ogbg-molbbbp: 0.665220 test loss: 2.304802
[Epoch 81; Iter    10/   55] train: loss: 0.0183720
[Epoch 81; Iter    40/   55] train: loss: 0.0208733
[Epoch 81] ogbg-molbbbp: 0.949617 val loss: 0.474059
[Epoch 81] ogbg-molbbbp: 0.659529 test loss: 1.714946
[Epoch 82; Iter    15/   55] train: loss: 0.1214375
[Epoch 82; Iter    45/   55] train: loss: 0.0100338
[Epoch 82] ogbg-molbbbp: 0.953301 val loss: 0.472809
[Epoch 82] ogbg-molbbbp: 0.670235 test loss: 1.848219
[Epoch 83; Iter    20/   55] train: loss: 0.1003826
[Epoch 83; Iter    50/   55] train: loss: 0.0124473
[Epoch 83] ogbg-molbbbp: 0.952903 val loss: 0.530348
[Epoch 83] ogbg-molbbbp: 0.653549 test loss: 2.092538
[Epoch 84; Iter    25/   55] train: loss: 0.1107397
[Epoch 84; Iter    55/   55] train: loss: 0.0180778
[Epoch 84] ogbg-molbbbp: 0.948621 val loss: 0.601391
[Epoch 84] ogbg-molbbbp: 0.673129 test loss: 2.050532
[Epoch 85; Iter    30/   55] train: loss: 0.0046117
[Epoch 85] ogbg-molbbbp: 0.950812 val loss: 0.551591
[Epoch 85] ogbg-molbbbp: 0.673515 test loss: 2.060011
[Epoch 86; Iter     5/   55] train: loss: 0.0882525
[Epoch 86; Iter    35/   55] train: loss: 0.0094466
[Epoch 86] ogbg-molbbbp: 0.946430 val loss: 0.559205
[Epoch 86] ogbg-molbbbp: 0.676215 test loss: 1.945005
[Epoch 87; Iter    10/   55] train: loss: 0.0133280
[Epoch 87; Iter    40/   55] train: loss: 0.0060403
[Epoch 87] ogbg-molbbbp: 0.951907 val loss: 0.579786
[Epoch 87] ogbg-molbbbp: 0.680941 test loss: 2.109192
[Epoch 88; Iter    15/   55] train: loss: 0.0256756
[Epoch 88; Iter    45/   55] train: loss: 0.0062201
[Epoch 88] ogbg-molbbbp: 0.954396 val loss: 0.495489
[Epoch 88] ogbg-molbbbp: 0.680266 test loss: 1.968778
[Epoch 89; Iter    20/   55] train: loss: 0.0034773
[Epoch 89; Iter    50/   55] train: loss: 0.0636992
[Epoch 89] ogbg-molbbbp: 0.949119 val loss: 0.618514
[Epoch 89] ogbg-molbbbp: 0.682099 test loss: 2.243668
[Epoch 90; Iter    25/   55] train: loss: 0.0039273
[Epoch 90; Iter    55/   55] train: loss: 0.2384054
[Epoch 90] ogbg-molbbbp: 0.946829 val loss: 0.636163
[Epoch 90] ogbg-molbbbp: 0.663966 test loss: 2.313138
[Epoch 91; Iter    30/   55] train: loss: 0.0067689
[Epoch 91] ogbg-molbbbp: 0.954297 val loss: 0.465937
[Epoch 91] ogbg-molbbbp: 0.659336 test loss: 2.107414
[Epoch 92; Iter     5/   55] train: loss: 0.0065769
[Epoch 92; Iter    35/   55] train: loss: 0.0055423
[Epoch 92] ogbg-molbbbp: 0.956786 val loss: 0.461075
[Epoch 92] ogbg-molbbbp: 0.656057 test loss: 2.078648
[Epoch 93; Iter    10/   55] train: loss: 0.0039450
[Epoch 93; Iter    40/   55] train: loss: 0.0057860
[Epoch 93] ogbg-molbbbp: 0.954097 val loss: 0.565754
[Epoch 93] ogbg-molbbbp: 0.679012 test loss: 2.190441
[Epoch 94; Iter    15/   55] train: loss: 0.0230771
[Epoch 94; Iter    45/   55] train: loss: 0.0013193
[Epoch 94] ogbg-molbbbp: 0.944638 val loss: 0.616381
[Epoch 94] ogbg-molbbbp: 0.656250 test loss: 2.182368
[Epoch 95; Iter    20/   55] train: loss: 0.3076943
[Epoch 95; Iter    50/   55] train: loss: 0.0072515
[Epoch 95] ogbg-molbbbp: 0.956587 val loss: 0.542939
[Epoch 95] ogbg-molbbbp: 0.638503 test loss: 2.479555
[Epoch 96; Iter    25/   55] train: loss: 0.0093921
[Epoch 96; Iter    55/   55] train: loss: 0.0053282
[Epoch 96] ogbg-molbbbp: 0.949418 val loss: 0.543959
[Epoch 96] ogbg-molbbbp: 0.682774 test loss: 2.233292
[Epoch 97; Iter    30/   55] train: loss: 0.0055419
[Epoch 97] ogbg-molbbbp: 0.948422 val loss: 0.586215
[Epoch 97] ogbg-molbbbp: 0.648727 test loss: 2.489655
[Epoch 98; Iter     5/   55] train: loss: 0.0372031
[Epoch 98; Iter    35/   55] train: loss: 0.0075170
[Epoch 98] ogbg-molbbbp: 0.945932 val loss: 0.566232
[Epoch 98] ogbg-molbbbp: 0.672840 test loss: 2.175673
[Epoch 99; Iter    10/   55] train: loss: 0.0026587
[Epoch 99; Iter    40/   55] train: loss: 0.0016880
[Epoch 99] ogbg-molbbbp: 0.949915 val loss: 0.542947
[Epoch 99] ogbg-molbbbp: 0.668403 test loss: 2.228630
[Epoch 100; Iter    15/   55] train: loss: 0.0083391
[Epoch 100; Iter    45/   55] train: loss: 0.0189792
[Epoch 100] ogbg-molbbbp: 0.949318 val loss: 0.590477
[Epoch 100] ogbg-molbbbp: 0.661265 test loss: 2.372774
[Epoch 101; Iter    20/   55] train: loss: 0.0232177
[Epoch 101; Iter    50/   55] train: loss: 0.0064908
[Epoch 101] ogbg-molbbbp: 0.936672 val loss: 0.766938
[Epoch 101] ogbg-molbbbp: 0.644869 test loss: 2.665240
[Epoch 102; Iter    25/   55] train: loss: 0.1032348
[Epoch 102; Iter    55/   55] train: loss: 0.0009752
[Epoch 102] ogbg-molbbbp: 0.931295 val loss: 0.836500
[Epoch 102] ogbg-molbbbp: 0.651427 test loss: 2.762303
[Epoch 103; Iter    30/   55] train: loss: 0.0196083
[Epoch 103] ogbg-molbbbp: 0.947426 val loss: 0.517275
[Epoch 103] ogbg-molbbbp: 0.659336 test loss: 2.069903
[Epoch 104; Iter     5/   55] train: loss: 0.0516325
[Epoch 104; Iter    35/   55] train: loss: 0.0288559
[Epoch 104] ogbg-molbbbp: 0.945833 val loss: 0.547909
[Epoch 104] ogbg-molbbbp: 0.652778 test loss: 2.363136
[Epoch 105; Iter    10/   55] train: loss: 0.1413815
[Epoch 105; Iter    40/   55] train: loss: 0.0029780
[Epoch 105] ogbg-molbbbp: 0.939560 val loss: 0.678915
[Epoch 105] ogbg-molbbbp: 0.665123 test loss: 2.487509
[Epoch 106; Iter    15/   55] train: loss: 0.0355009
[Epoch 106; Iter    45/   55] train: loss: 0.0029007
[Epoch 106] ogbg-molbbbp: 0.934681 val loss: 0.654561
[Epoch 106] ogbg-molbbbp: 0.642458 test loss: 2.304483
[Epoch 107; Iter    20/   55] train: loss: 0.0265898
[Epoch 107; Iter    50/   55] train: loss: 0.0032839
[Epoch 107] ogbg-molbbbp: 0.950712 val loss: 0.450958
[Epoch 107] ogbg-molbbbp: 0.682099 test loss: 1.755814
[Epoch 108; Iter    25/   55] train: loss: 0.0029066
[Epoch 108; Iter    55/   55] train: loss: 0.0010042
[Epoch 108] ogbg-molbbbp: 0.944937 val loss: 0.607756
[Epoch 108] ogbg-molbbbp: 0.673611 test loss: 2.172452
[Epoch 109; Iter    30/   55] train: loss: 0.0274234
[Epoch 69; Iter    40/   55] train: loss: 0.1157965
[Epoch 69] ogbg-molbbbp: 0.954595 val loss: 0.408087
[Epoch 69] ogbg-molbbbp: 0.661748 test loss: 1.491764
[Epoch 70; Iter    15/   55] train: loss: 0.0356486
[Epoch 70; Iter    45/   55] train: loss: 0.0473602
[Epoch 70] ogbg-molbbbp: 0.944538 val loss: 0.488138
[Epoch 70] ogbg-molbbbp: 0.649402 test loss: 1.681928
[Epoch 71; Iter    20/   55] train: loss: 0.0127842
[Epoch 71; Iter    50/   55] train: loss: 0.0466113
[Epoch 71] ogbg-molbbbp: 0.942945 val loss: 0.475717
[Epoch 71] ogbg-molbbbp: 0.666860 test loss: 1.614166
[Epoch 72; Iter    25/   55] train: loss: 0.0144881
[Epoch 72; Iter    55/   55] train: loss: 0.0091474
[Epoch 72] ogbg-molbbbp: 0.951110 val loss: 0.445799
[Epoch 72] ogbg-molbbbp: 0.650849 test loss: 1.694611
[Epoch 73; Iter    30/   55] train: loss: 0.0571119
[Epoch 73] ogbg-molbbbp: 0.934581 val loss: 0.533320
[Epoch 73] ogbg-molbbbp: 0.639950 test loss: 1.865185
[Epoch 74; Iter     5/   55] train: loss: 0.0146487
[Epoch 74; Iter    35/   55] train: loss: 0.0252190
[Epoch 74] ogbg-molbbbp: 0.931694 val loss: 0.547328
[Epoch 74] ogbg-molbbbp: 0.638985 test loss: 1.751467
[Epoch 75; Iter    10/   55] train: loss: 0.0304929
[Epoch 75; Iter    40/   55] train: loss: 0.0523976
[Epoch 75] ogbg-molbbbp: 0.939560 val loss: 0.629483
[Epoch 75] ogbg-molbbbp: 0.669174 test loss: 2.479738
[Epoch 76; Iter    15/   55] train: loss: 0.1750793
[Epoch 76; Iter    45/   55] train: loss: 0.0498835
[Epoch 76] ogbg-molbbbp: 0.949019 val loss: 0.484793
[Epoch 76] ogbg-molbbbp: 0.655864 test loss: 1.762592
[Epoch 77; Iter    20/   55] train: loss: 0.0735437
[Epoch 77; Iter    50/   55] train: loss: 0.1373983
[Epoch 77] ogbg-molbbbp: 0.932291 val loss: 0.615317
[Epoch 77] ogbg-molbbbp: 0.627990 test loss: 2.006440
[Epoch 78; Iter    25/   55] train: loss: 0.0101749
[Epoch 78; Iter    55/   55] train: loss: 0.0057085
[Epoch 78] ogbg-molbbbp: 0.959773 val loss: 0.401169
[Epoch 78] ogbg-molbbbp: 0.637828 test loss: 1.966576
[Epoch 79; Iter    30/   55] train: loss: 0.0602339
[Epoch 79] ogbg-molbbbp: 0.955292 val loss: 0.456461
[Epoch 79] ogbg-molbbbp: 0.660880 test loss: 1.915424
[Epoch 80; Iter     5/   55] train: loss: 0.0157310
[Epoch 80; Iter    35/   55] train: loss: 0.0652172
[Epoch 80] ogbg-molbbbp: 0.952703 val loss: 0.371442
[Epoch 80] ogbg-molbbbp: 0.620563 test loss: 1.602555
[Epoch 81; Iter    10/   55] train: loss: 0.1499484
[Epoch 81; Iter    40/   55] train: loss: 0.0544232
[Epoch 81] ogbg-molbbbp: 0.952206 val loss: 0.351471
[Epoch 81] ogbg-molbbbp: 0.651138 test loss: 1.522816
[Epoch 82; Iter    15/   55] train: loss: 0.0150457
[Epoch 82; Iter    45/   55] train: loss: 0.0441339
[Epoch 82] ogbg-molbbbp: 0.931992 val loss: 0.625114
[Epoch 82] ogbg-molbbbp: 0.631269 test loss: 1.975246
[Epoch 83; Iter    20/   55] train: loss: 0.0767686
[Epoch 83; Iter    50/   55] train: loss: 0.0261496
[Epoch 83] ogbg-molbbbp: 0.941352 val loss: 0.408302
[Epoch 83] ogbg-molbbbp: 0.592978 test loss: 1.835697
[Epoch 84; Iter    25/   55] train: loss: 0.1489407
[Epoch 84; Iter    55/   55] train: loss: 0.0061610
[Epoch 84] ogbg-molbbbp: 0.952006 val loss: 0.488472
[Epoch 84] ogbg-molbbbp: 0.638214 test loss: 2.266447
[Epoch 85; Iter    30/   55] train: loss: 0.1613584
[Epoch 85] ogbg-molbbbp: 0.949915 val loss: 0.502445
[Epoch 85] ogbg-molbbbp: 0.639660 test loss: 2.019004
[Epoch 86; Iter     5/   55] train: loss: 0.0079130
[Epoch 86; Iter    35/   55] train: loss: 0.1439564
[Epoch 86] ogbg-molbbbp: 0.950413 val loss: 0.505268
[Epoch 86] ogbg-molbbbp: 0.630208 test loss: 2.045012
[Epoch 87; Iter    10/   55] train: loss: 0.0417993
[Epoch 87; Iter    40/   55] train: loss: 0.0078963
[Epoch 87] ogbg-molbbbp: 0.947028 val loss: 0.510869
[Epoch 87] ogbg-molbbbp: 0.626061 test loss: 2.183392
[Epoch 88; Iter    15/   55] train: loss: 0.0435113
[Epoch 88; Iter    45/   55] train: loss: 0.0063941
[Epoch 88] ogbg-molbbbp: 0.942746 val loss: 0.569682
[Epoch 88] ogbg-molbbbp: 0.629823 test loss: 2.165981
[Epoch 89; Iter    20/   55] train: loss: 0.1397833
[Epoch 89; Iter    50/   55] train: loss: 0.0074003
[Epoch 89] ogbg-molbbbp: 0.943045 val loss: 0.557913
[Epoch 89] ogbg-molbbbp: 0.632523 test loss: 2.105812
[Epoch 90; Iter    25/   55] train: loss: 0.0208082
[Epoch 90; Iter    55/   55] train: loss: 0.0025455
[Epoch 90] ogbg-molbbbp: 0.948820 val loss: 0.471742
[Epoch 90] ogbg-molbbbp: 0.628762 test loss: 2.125656
[Epoch 91; Iter    30/   55] train: loss: 0.0029760
[Epoch 91] ogbg-molbbbp: 0.937867 val loss: 0.555919
[Epoch 91] ogbg-molbbbp: 0.614776 test loss: 2.105407
[Epoch 92; Iter     5/   55] train: loss: 0.0061711
[Epoch 92; Iter    35/   55] train: loss: 0.0083709
[Epoch 92] ogbg-molbbbp: 0.947426 val loss: 0.501596
[Epoch 92] ogbg-molbbbp: 0.615258 test loss: 2.224165
[Epoch 93; Iter    10/   55] train: loss: 0.0039615
[Epoch 93; Iter    40/   55] train: loss: 0.0036822
[Epoch 93] ogbg-molbbbp: 0.947426 val loss: 0.555045
[Epoch 93] ogbg-molbbbp: 0.625096 test loss: 2.288871
[Epoch 94; Iter    15/   55] train: loss: 0.0213724
[Epoch 94; Iter    45/   55] train: loss: 0.0166682
[Epoch 94] ogbg-molbbbp: 0.947824 val loss: 0.489333
[Epoch 94] ogbg-molbbbp: 0.634066 test loss: 2.093600
[Epoch 95; Iter    20/   55] train: loss: 0.0166886
[Epoch 95; Iter    50/   55] train: loss: 0.0071281
[Epoch 95] ogbg-molbbbp: 0.945235 val loss: 0.536387
[Epoch 95] ogbg-molbbbp: 0.629919 test loss: 2.253603
[Epoch 96; Iter    25/   55] train: loss: 0.0271157
[Epoch 96; Iter    55/   55] train: loss: 0.0045042
[Epoch 96] ogbg-molbbbp: 0.942547 val loss: 0.496009
[Epoch 96] ogbg-molbbbp: 0.644097 test loss: 2.027751
[Epoch 97; Iter    30/   55] train: loss: 0.0031494
[Epoch 97] ogbg-molbbbp: 0.941352 val loss: 0.679492
[Epoch 97] ogbg-molbbbp: 0.648534 test loss: 2.471188
[Epoch 98; Iter     5/   55] train: loss: 0.0076451
[Epoch 98; Iter    35/   55] train: loss: 0.0098510
[Epoch 98] ogbg-molbbbp: 0.950015 val loss: 0.484526
[Epoch 98] ogbg-molbbbp: 0.619695 test loss: 2.138109
[Epoch 99; Iter    10/   55] train: loss: 0.0949242
[Epoch 99; Iter    40/   55] train: loss: 0.0288756
[Epoch 99] ogbg-molbbbp: 0.945036 val loss: 0.586204
[Epoch 99] ogbg-molbbbp: 0.646798 test loss: 2.345728
[Epoch 100; Iter    15/   55] train: loss: 0.0085145
[Epoch 100; Iter    45/   55] train: loss: 0.0062348
[Epoch 100] ogbg-molbbbp: 0.954197 val loss: 0.470293
[Epoch 100] ogbg-molbbbp: 0.638021 test loss: 2.064904
[Epoch 101; Iter    20/   55] train: loss: 0.1004284
[Epoch 101; Iter    50/   55] train: loss: 0.0163074
[Epoch 101] ogbg-molbbbp: 0.945235 val loss: 0.603638
[Epoch 101] ogbg-molbbbp: 0.648341 test loss: 2.320944
[Epoch 102; Iter    25/   55] train: loss: 0.0517829
[Epoch 102; Iter    55/   55] train: loss: 0.0107095
[Epoch 102] ogbg-molbbbp: 0.951210 val loss: 0.453980
[Epoch 102] ogbg-molbbbp: 0.629726 test loss: 2.059039
[Epoch 103; Iter    30/   55] train: loss: 0.0022328
[Epoch 103] ogbg-molbbbp: 0.948621 val loss: 0.518763
[Epoch 103] ogbg-molbbbp: 0.619888 test loss: 2.274752
[Epoch 104; Iter     5/   55] train: loss: 0.0116825
[Epoch 104; Iter    35/   55] train: loss: 0.0172919
[Epoch 104] ogbg-molbbbp: 0.948322 val loss: 0.516533
[Epoch 104] ogbg-molbbbp: 0.625482 test loss: 2.261301
[Epoch 105; Iter    10/   55] train: loss: 0.0032352
[Epoch 105; Iter    40/   55] train: loss: 0.0625710
[Epoch 105] ogbg-molbbbp: 0.949617 val loss: 0.526731
[Epoch 105] ogbg-molbbbp: 0.636671 test loss: 2.383368
[Epoch 106; Iter    15/   55] train: loss: 0.0086009
[Epoch 106; Iter    45/   55] train: loss: 0.0375505
[Epoch 106] ogbg-molbbbp: 0.945733 val loss: 0.560726
[Epoch 106] ogbg-molbbbp: 0.628086 test loss: 2.260062
[Epoch 107; Iter    20/   55] train: loss: 0.0030890
[Epoch 107; Iter    50/   55] train: loss: 0.0536966
[Epoch 107] ogbg-molbbbp: 0.951807 val loss: 0.517039
[Epoch 107] ogbg-molbbbp: 0.635224 test loss: 2.321077
[Epoch 108; Iter    25/   55] train: loss: 0.0039434
[Epoch 108; Iter    55/   55] train: loss: 0.1202603
[Epoch 108] ogbg-molbbbp: 0.947227 val loss: 0.578203
[Epoch 108] ogbg-molbbbp: 0.634549 test loss: 2.391416
[Epoch 109; Iter    30/   55] train: loss: 0.0026413
[Epoch 73] ogbg-molbbbp: 0.775314 test loss: 1.174964
[Epoch 74; Iter     6/   48] train: loss: 0.0843890
[Epoch 74; Iter    36/   48] train: loss: 0.1942810
[Epoch 74] ogbg-molbbbp: 0.970909 val loss: 0.155953
[Epoch 74] ogbg-molbbbp: 0.805287 test loss: 0.951871
[Epoch 75; Iter    18/   48] train: loss: 0.0181570
[Epoch 75; Iter    48/   48] train: loss: 0.0631468
[Epoch 75] ogbg-molbbbp: 0.961173 val loss: 0.136757
[Epoch 75] ogbg-molbbbp: 0.772670 test loss: 1.006756
[Epoch 76; Iter    30/   48] train: loss: 0.0133944
[Epoch 76] ogbg-molbbbp: 0.968798 val loss: 0.109114
[Epoch 76] ogbg-molbbbp: 0.776075 test loss: 1.507795
[Epoch 77; Iter    12/   48] train: loss: 0.0527299
[Epoch 77; Iter    42/   48] train: loss: 0.0181165
[Epoch 77] ogbg-molbbbp: 0.976774 val loss: 0.157220
[Epoch 77] ogbg-molbbbp: 0.770789 test loss: 1.351800
[Epoch 78; Iter    24/   48] train: loss: 0.0528645
[Epoch 78] ogbg-molbbbp: 0.964106 val loss: 0.133983
[Epoch 78] ogbg-molbbbp: 0.773746 test loss: 1.243757
[Epoch 79; Iter     6/   48] train: loss: 0.2147184
[Epoch 79; Iter    36/   48] train: loss: 0.1858615
[Epoch 79] ogbg-molbbbp: 0.976891 val loss: 0.140427
[Epoch 79] ogbg-molbbbp: 0.786066 test loss: 1.099090
[Epoch 80; Iter    18/   48] train: loss: 0.0155126
[Epoch 80; Iter    48/   48] train: loss: 0.0103648
[Epoch 80] ogbg-molbbbp: 0.980293 val loss: 0.148001
[Epoch 80] ogbg-molbbbp: 0.788799 test loss: 1.205418
[Epoch 81; Iter    30/   48] train: loss: 0.0323333
[Epoch 81] ogbg-molbbbp: 0.971496 val loss: 0.156376
[Epoch 81] ogbg-molbbbp: 0.785170 test loss: 1.354210
[Epoch 82; Iter    12/   48] train: loss: 0.2094119
[Epoch 82; Iter    42/   48] train: loss: 0.0093122
[Epoch 82] ogbg-molbbbp: 0.979707 val loss: 0.147056
[Epoch 82] ogbg-molbbbp: 0.796281 test loss: 1.182882
[Epoch 83; Iter    24/   48] train: loss: 0.0121959
[Epoch 83] ogbg-molbbbp: 0.970088 val loss: 0.216370
[Epoch 83] ogbg-molbbbp: 0.776120 test loss: 1.343661
[Epoch 84; Iter     6/   48] train: loss: 0.0081495
[Epoch 84; Iter    36/   48] train: loss: 0.0332370
[Epoch 84] ogbg-molbbbp: 0.957771 val loss: 0.201405
[Epoch 84] ogbg-molbbbp: 0.773297 test loss: 1.405784
[Epoch 85; Iter    18/   48] train: loss: 0.3392051
[Epoch 85; Iter    48/   48] train: loss: 0.0467428
[Epoch 85] ogbg-molbbbp: 0.981584 val loss: 0.131279
[Epoch 85] ogbg-molbbbp: 0.779480 test loss: 1.117808
[Epoch 86; Iter    30/   48] train: loss: 0.0106330
[Epoch 86] ogbg-molbbbp: 0.941114 val loss: 0.160555
[Epoch 86] ogbg-molbbbp: 0.768593 test loss: 1.276420
[Epoch 87; Iter    12/   48] train: loss: 0.0484252
[Epoch 87; Iter    42/   48] train: loss: 0.1580214
[Epoch 87] ogbg-molbbbp: 0.974311 val loss: 0.171303
[Epoch 87] ogbg-molbbbp: 0.777106 test loss: 1.261804
[Epoch 88; Iter    24/   48] train: loss: 0.0324244
[Epoch 88] ogbg-molbbbp: 0.978299 val loss: 0.160681
[Epoch 88] ogbg-molbbbp: 0.778271 test loss: 1.294714
[Epoch 89; Iter     6/   48] train: loss: 0.0412724
[Epoch 89; Iter    36/   48] train: loss: 0.0518376
[Epoch 89] ogbg-molbbbp: 0.969619 val loss: 0.177808
[Epoch 89] ogbg-molbbbp: 0.772312 test loss: 1.111870
[Epoch 90; Iter    18/   48] train: loss: 0.0176281
[Epoch 90; Iter    48/   48] train: loss: 0.0226510
[Epoch 90] ogbg-molbbbp: 0.966217 val loss: 0.157297
[Epoch 90] ogbg-molbbbp: 0.769265 test loss: 1.404963
[Epoch 91; Iter    30/   48] train: loss: 0.0538124
[Epoch 91] ogbg-molbbbp: 0.952727 val loss: 0.173508
[Epoch 91] ogbg-molbbbp: 0.767966 test loss: 1.391318
[Epoch 92; Iter    12/   48] train: loss: 0.0915868
[Epoch 92; Iter    42/   48] train: loss: 0.0917540
[Epoch 92] ogbg-molbbbp: 0.956246 val loss: 0.160288
[Epoch 92] ogbg-molbbbp: 0.730152 test loss: 1.717730
[Epoch 93; Iter    24/   48] train: loss: 0.0065721
[Epoch 93] ogbg-molbbbp: 0.971613 val loss: 0.137335
[Epoch 93] ogbg-molbbbp: 0.759364 test loss: 1.456115
[Epoch 94; Iter     6/   48] train: loss: 0.0372296
[Epoch 94; Iter    36/   48] train: loss: 0.0348766
[Epoch 94] ogbg-molbbbp: 0.977361 val loss: 0.137636
[Epoch 94] ogbg-molbbbp: 0.771953 test loss: 1.330018
[Epoch 95; Iter    18/   48] train: loss: 0.0769957
[Epoch 95; Iter    48/   48] train: loss: 0.0081371
[Epoch 95] ogbg-molbbbp: 0.978182 val loss: 0.141235
[Epoch 95] ogbg-molbbbp: 0.782348 test loss: 1.413989
[Epoch 96; Iter    30/   48] train: loss: 0.0026254
[Epoch 96] ogbg-molbbbp: 0.972551 val loss: 0.156755
[Epoch 96] ogbg-molbbbp: 0.761022 test loss: 1.472425
[Epoch 97; Iter    12/   48] train: loss: 0.0575731
[Epoch 97; Iter    42/   48] train: loss: 0.0380785
[Epoch 97] ogbg-molbbbp: 0.965630 val loss: 0.175065
[Epoch 97] ogbg-molbbbp: 0.759901 test loss: 1.558575
[Epoch 98; Iter    24/   48] train: loss: 0.0201336
[Epoch 98] ogbg-molbbbp: 0.969501 val loss: 0.149126
[Epoch 98] ogbg-molbbbp: 0.777464 test loss: 1.478081
[Epoch 99; Iter     6/   48] train: loss: 0.0161340
[Epoch 99; Iter    36/   48] train: loss: 0.0662374
[Epoch 99] ogbg-molbbbp: 0.969736 val loss: 0.161561
[Epoch 99] ogbg-molbbbp: 0.777195 test loss: 1.434295
[Epoch 100; Iter    18/   48] train: loss: 0.0069473
[Epoch 100; Iter    48/   48] train: loss: 0.0028987
[Epoch 100] ogbg-molbbbp: 0.974663 val loss: 0.158676
[Epoch 100] ogbg-molbbbp: 0.772491 test loss: 1.474102
[Epoch 101; Iter    30/   48] train: loss: 0.0464174
[Epoch 101] ogbg-molbbbp: 0.966921 val loss: 0.169904
[Epoch 101] ogbg-molbbbp: 0.763306 test loss: 1.554182
[Epoch 102; Iter    12/   48] train: loss: 0.0309697
[Epoch 102; Iter    42/   48] train: loss: 0.0311737
[Epoch 102] ogbg-molbbbp: 0.965630 val loss: 0.159694
[Epoch 102] ogbg-molbbbp: 0.769668 test loss: 1.333708
[Epoch 103; Iter    24/   48] train: loss: 0.0353625
[Epoch 103] ogbg-molbbbp: 0.969853 val loss: 0.147551
[Epoch 103] ogbg-molbbbp: 0.772536 test loss: 1.669617
[Epoch 104; Iter     6/   48] train: loss: 0.0007556
[Epoch 104; Iter    36/   48] train: loss: 0.0018572
[Epoch 104] ogbg-molbbbp: 0.971848 val loss: 0.144583
[Epoch 104] ogbg-molbbbp: 0.760305 test loss: 1.516119
[Epoch 105; Iter    18/   48] train: loss: 0.0128969
[Epoch 105; Iter    48/   48] train: loss: 0.0009459
[Epoch 105] ogbg-molbbbp: 0.975601 val loss: 0.155705
[Epoch 105] ogbg-molbbbp: 0.771147 test loss: 1.557374
[Epoch 106; Iter    30/   48] train: loss: 0.0017748
[Epoch 106] ogbg-molbbbp: 0.966452 val loss: 0.169459
[Epoch 106] ogbg-molbbbp: 0.763530 test loss: 1.673588
[Epoch 107; Iter    12/   48] train: loss: 0.0340049
[Epoch 107; Iter    42/   48] train: loss: 0.1178785
[Epoch 107] ogbg-molbbbp: 0.969032 val loss: 0.176428
[Epoch 107] ogbg-molbbbp: 0.779167 test loss: 1.402359
[Epoch 108; Iter    24/   48] train: loss: 0.0015547
[Epoch 108] ogbg-molbbbp: 0.950029 val loss: 0.196373
[Epoch 108] ogbg-molbbbp: 0.766398 test loss: 1.431526
[Epoch 109; Iter     6/   48] train: loss: 0.0183806
[Epoch 109; Iter    36/   48] train: loss: 0.0997564
[Epoch 109] ogbg-molbbbp: 0.949912 val loss: 0.165360
[Epoch 109] ogbg-molbbbp: 0.746819 test loss: 1.759455
[Epoch 110; Iter    18/   48] train: loss: 0.0479911
[Epoch 110; Iter    48/   48] train: loss: 0.1988523
[Epoch 110] ogbg-molbbbp: 0.970088 val loss: 0.170962
[Epoch 110] ogbg-molbbbp: 0.761111 test loss: 1.585761
[Epoch 111; Iter    30/   48] train: loss: 0.0023765
[Epoch 111] ogbg-molbbbp: 0.964457 val loss: 0.158094
[Epoch 111] ogbg-molbbbp: 0.757796 test loss: 1.551593
[Epoch 112; Iter    12/   48] train: loss: 0.0020684
[Epoch 112; Iter    42/   48] train: loss: 0.0016903
[Epoch 112] ogbg-molbbbp: 0.966921 val loss: 0.177075
[Epoch 112] ogbg-molbbbp: 0.744220 test loss: 1.687123
[Epoch 113; Iter    24/   48] train: loss: 0.0026447
[Epoch 113] ogbg-molbbbp: 0.966804 val loss: 0.177372
[Epoch 113] ogbg-molbbbp: 0.762769 test loss: 1.589327
[Epoch 114; Iter     6/   48] train: loss: 0.0028184
[Epoch 114; Iter    36/   48] train: loss: 0.0519449
[Epoch 114] ogbg-molbbbp: 0.966569 val loss: 0.160075
[Epoch 114] ogbg-molbbbp: 0.760663 test loss: 1.635208
[Epoch 115; Iter    18/   48] train: loss: 0.0351751
[Epoch 115; Iter    48/   48] train: loss: 0.0017522
[Epoch 115] ogbg-molbbbp: 0.946041 val loss: 0.148182
[Epoch 115] ogbg-molbbbp: 0.754973 test loss: 1.481754
[Epoch 73] ogbg-molbbbp: 0.776478 test loss: 1.167508
[Epoch 74; Iter     6/   48] train: loss: 0.0024548
[Epoch 74; Iter    36/   48] train: loss: 0.0227919
[Epoch 74] ogbg-molbbbp: 0.956598 val loss: 0.152745
[Epoch 74] ogbg-molbbbp: 0.770654 test loss: 1.318517
[Epoch 75; Iter    18/   48] train: loss: 0.1508605
[Epoch 75; Iter    48/   48] train: loss: 0.0111343
[Epoch 75] ogbg-molbbbp: 0.953431 val loss: 0.162031
[Epoch 75] ogbg-molbbbp: 0.766129 test loss: 1.272281
[Epoch 76; Iter    30/   48] train: loss: 0.0154305
[Epoch 76] ogbg-molbbbp: 0.948856 val loss: 0.182012
[Epoch 76] ogbg-molbbbp: 0.766935 test loss: 1.415293
[Epoch 77; Iter    12/   48] train: loss: 0.0256259
[Epoch 77; Iter    42/   48] train: loss: 0.0109442
[Epoch 77] ogbg-molbbbp: 0.953196 val loss: 0.199865
[Epoch 77] ogbg-molbbbp: 0.773029 test loss: 1.231455
[Epoch 78; Iter    24/   48] train: loss: 0.0192545
[Epoch 78] ogbg-molbbbp: 0.953900 val loss: 0.153218
[Epoch 78] ogbg-molbbbp: 0.782079 test loss: 1.451921
[Epoch 79; Iter     6/   48] train: loss: 0.1337365
[Epoch 79; Iter    36/   48] train: loss: 0.0071835
[Epoch 79] ogbg-molbbbp: 0.944868 val loss: 0.168575
[Epoch 79] ogbg-molbbbp: 0.771461 test loss: 1.545617
[Epoch 80; Iter    18/   48] train: loss: 0.0058523
[Epoch 80; Iter    48/   48] train: loss: 0.0057964
[Epoch 80] ogbg-molbbbp: 0.945337 val loss: 0.194591
[Epoch 80] ogbg-molbbbp: 0.781810 test loss: 1.317517
[Epoch 81; Iter    30/   48] train: loss: 0.0433155
[Epoch 81] ogbg-molbbbp: 0.950968 val loss: 0.156217
[Epoch 81] ogbg-molbbbp: 0.773970 test loss: 1.437041
[Epoch 82; Iter    12/   48] train: loss: 0.0333395
[Epoch 82; Iter    42/   48] train: loss: 0.0095086
[Epoch 82] ogbg-molbbbp: 0.954956 val loss: 0.172281
[Epoch 82] ogbg-molbbbp: 0.770923 test loss: 1.395127
[Epoch 83; Iter    24/   48] train: loss: 0.0076091
[Epoch 83] ogbg-molbbbp: 0.952727 val loss: 0.146319
[Epoch 83] ogbg-molbbbp: 0.773790 test loss: 1.484444
[Epoch 84; Iter     6/   48] train: loss: 0.1156188
[Epoch 84; Iter    36/   48] train: loss: 0.0132692
[Epoch 84] ogbg-molbbbp: 0.945455 val loss: 0.183796
[Epoch 84] ogbg-molbbbp: 0.777375 test loss: 1.391322
[Epoch 85; Iter    18/   48] train: loss: 0.0172820
[Epoch 85; Iter    48/   48] train: loss: 0.0033414
[Epoch 85] ogbg-molbbbp: 0.947918 val loss: 0.186608
[Epoch 85] ogbg-molbbbp: 0.778898 test loss: 1.304795
[Epoch 86; Iter    30/   48] train: loss: 0.0095937
[Epoch 86] ogbg-molbbbp: 0.951554 val loss: 0.173256
[Epoch 86] ogbg-molbbbp: 0.782392 test loss: 1.299829
[Epoch 87; Iter    12/   48] train: loss: 0.0966361
[Epoch 87; Iter    42/   48] train: loss: 0.0031852
[Epoch 87] ogbg-molbbbp: 0.949795 val loss: 0.174326
[Epoch 87] ogbg-molbbbp: 0.775358 test loss: 1.320877
[Epoch 88; Iter    24/   48] train: loss: 0.0119331
[Epoch 88] ogbg-molbbbp: 0.940762 val loss: 0.217457
[Epoch 88] ogbg-molbbbp: 0.764247 test loss: 1.417303
[Epoch 89; Iter     6/   48] train: loss: 0.1528781
[Epoch 89; Iter    36/   48] train: loss: 0.0879644
[Epoch 89] ogbg-molbbbp: 0.941349 val loss: 0.177837
[Epoch 89] ogbg-molbbbp: 0.761604 test loss: 1.470679
[Epoch 90; Iter    18/   48] train: loss: 0.0142159
[Epoch 90; Iter    48/   48] train: loss: 0.0025068
[Epoch 90] ogbg-molbbbp: 0.942053 val loss: 0.170418
[Epoch 90] ogbg-molbbbp: 0.772894 test loss: 1.351886
[Epoch 91; Iter    30/   48] train: loss: 0.0174734
[Epoch 91] ogbg-molbbbp: 0.960469 val loss: 0.151560
[Epoch 91] ogbg-molbbbp: 0.782930 test loss: 1.407586
[Epoch 92; Iter    12/   48] train: loss: 0.0024063
[Epoch 92; Iter    42/   48] train: loss: 0.0094275
[Epoch 92] ogbg-molbbbp: 0.938651 val loss: 0.218550
[Epoch 92] ogbg-molbbbp: 0.780018 test loss: 1.262345
[Epoch 93; Iter    24/   48] train: loss: 0.0170467
[Epoch 93] ogbg-molbbbp: 0.953079 val loss: 0.187151
[Epoch 93] ogbg-molbbbp: 0.771237 test loss: 1.536493
[Epoch 94; Iter     6/   48] train: loss: 0.0408219
[Epoch 94; Iter    36/   48] train: loss: 0.0045421
[Epoch 94] ogbg-molbbbp: 0.956364 val loss: 0.178422
[Epoch 94] ogbg-molbbbp: 0.779884 test loss: 1.407258
[Epoch 95; Iter    18/   48] train: loss: 0.0033136
[Epoch 95; Iter    48/   48] train: loss: 0.1900871
[Epoch 95] ogbg-molbbbp: 0.953079 val loss: 0.163276
[Epoch 95] ogbg-molbbbp: 0.772222 test loss: 1.666254
[Epoch 96; Iter    30/   48] train: loss: 0.0078757
[Epoch 96] ogbg-molbbbp: 0.925865 val loss: 0.312208
[Epoch 96] ogbg-molbbbp: 0.745251 test loss: 2.225864
[Epoch 97; Iter    12/   48] train: loss: 0.0225401
[Epoch 97; Iter    42/   48] train: loss: 0.0050617
[Epoch 97] ogbg-molbbbp: 0.934780 val loss: 0.203170
[Epoch 97] ogbg-molbbbp: 0.756631 test loss: 1.337056
[Epoch 98; Iter    24/   48] train: loss: 0.0044537
[Epoch 98] ogbg-molbbbp: 0.945806 val loss: 0.201901
[Epoch 98] ogbg-molbbbp: 0.761918 test loss: 1.477755
[Epoch 99; Iter     6/   48] train: loss: 0.0306447
[Epoch 99; Iter    36/   48] train: loss: 0.0540319
[Epoch 99] ogbg-molbbbp: 0.951202 val loss: 0.190599
[Epoch 99] ogbg-molbbbp: 0.757975 test loss: 1.438754
[Epoch 100; Iter    18/   48] train: loss: 0.0174634
[Epoch 100; Iter    48/   48] train: loss: 0.0776941
[Epoch 100] ogbg-molbbbp: 0.950616 val loss: 0.204424
[Epoch 100] ogbg-molbbbp: 0.758423 test loss: 1.392188
[Epoch 101; Iter    30/   48] train: loss: 0.0070713
[Epoch 101] ogbg-molbbbp: 0.948504 val loss: 0.212744
[Epoch 101] ogbg-molbbbp: 0.762769 test loss: 1.420899
[Epoch 102; Iter    12/   48] train: loss: 0.0036578
[Epoch 102; Iter    42/   48] train: loss: 0.0044230
[Epoch 102] ogbg-molbbbp: 0.959531 val loss: 0.183779
[Epoch 102] ogbg-molbbbp: 0.768100 test loss: 1.480784
[Epoch 103; Iter    24/   48] train: loss: 0.0457384
[Epoch 103] ogbg-molbbbp: 0.950616 val loss: 0.187806
[Epoch 103] ogbg-molbbbp: 0.767876 test loss: 1.431630
[Epoch 104; Iter     6/   48] train: loss: 0.0055296
[Epoch 104; Iter    36/   48] train: loss: 0.0055464
[Epoch 104] ogbg-molbbbp: 0.951437 val loss: 0.180244
[Epoch 104] ogbg-molbbbp: 0.763082 test loss: 1.453423
[Epoch 105; Iter    18/   48] train: loss: 0.0040894
[Epoch 105; Iter    48/   48] train: loss: 0.0013788
[Epoch 105] ogbg-molbbbp: 0.953079 val loss: 0.203487
[Epoch 105] ogbg-molbbbp: 0.765860 test loss: 1.568648
[Epoch 106; Iter    30/   48] train: loss: 0.0078844
[Epoch 106] ogbg-molbbbp: 0.950733 val loss: 0.188334
[Epoch 106] ogbg-molbbbp: 0.763575 test loss: 1.451663
[Epoch 107; Iter    12/   48] train: loss: 0.0011709
[Epoch 107; Iter    42/   48] train: loss: 0.0070239
[Epoch 107] ogbg-molbbbp: 0.954252 val loss: 0.198745
[Epoch 107] ogbg-molbbbp: 0.765054 test loss: 1.480302
[Epoch 108; Iter    24/   48] train: loss: 0.0017518
[Epoch 108] ogbg-molbbbp: 0.949795 val loss: 0.162466
[Epoch 108] ogbg-molbbbp: 0.762366 test loss: 1.672696
[Epoch 109; Iter     6/   48] train: loss: 0.0023648
[Epoch 109; Iter    36/   48] train: loss: 0.0820383
[Epoch 109] ogbg-molbbbp: 0.937595 val loss: 0.204368
[Epoch 109] ogbg-molbbbp: 0.756989 test loss: 1.438688
[Epoch 110; Iter    18/   48] train: loss: 0.0050264
[Epoch 110; Iter    48/   48] train: loss: 0.0080281
[Epoch 110] ogbg-molbbbp: 0.949795 val loss: 0.167654
[Epoch 110] ogbg-molbbbp: 0.767115 test loss: 1.615514
[Epoch 111; Iter    30/   48] train: loss: 0.0069504
[Epoch 111] ogbg-molbbbp: 0.948270 val loss: 0.181098
[Epoch 111] ogbg-molbbbp: 0.759588 test loss: 1.683527
[Epoch 112; Iter    12/   48] train: loss: 0.0165690
[Epoch 112; Iter    42/   48] train: loss: 0.0232597
[Epoch 112] ogbg-molbbbp: 0.945924 val loss: 0.190300
[Epoch 112] ogbg-molbbbp: 0.755376 test loss: 1.603217
[Epoch 113; Iter    24/   48] train: loss: 0.0370179
[Epoch 113] ogbg-molbbbp: 0.948856 val loss: 0.184819
[Epoch 113] ogbg-molbbbp: 0.764292 test loss: 1.559858
[Epoch 114; Iter     6/   48] train: loss: 0.0035781
[Epoch 114; Iter    36/   48] train: loss: 0.0781188
[Epoch 114] ogbg-molbbbp: 0.942874 val loss: 0.184476
[Epoch 114] ogbg-molbbbp: 0.762410 test loss: 1.695773
[Epoch 115; Iter    18/   48] train: loss: 0.0029895
[Epoch 115; Iter    48/   48] train: loss: 0.0096819
[Epoch 115] ogbg-molbbbp: 0.949091 val loss: 0.168871
[Epoch 115] ogbg-molbbbp: 0.763217 test loss: 1.565415
[Epoch 73] ogbg-molbbbp: 0.795520 test loss: 1.022680
[Epoch 74; Iter     6/   48] train: loss: 0.0821949
[Epoch 74; Iter    36/   48] train: loss: 0.0961180
[Epoch 74] ogbg-molbbbp: 0.982639 val loss: 0.090236
[Epoch 74] ogbg-molbbbp: 0.752599 test loss: 1.328703
[Epoch 75; Iter    18/   48] train: loss: 0.0439785
[Epoch 75; Iter    48/   48] train: loss: 0.0365570
[Epoch 75] ogbg-molbbbp: 0.976188 val loss: 0.160924
[Epoch 75] ogbg-molbbbp: 0.773163 test loss: 1.157890
[Epoch 76; Iter    30/   48] train: loss: 0.0593198
[Epoch 76] ogbg-molbbbp: 0.983578 val loss: 0.111378
[Epoch 76] ogbg-molbbbp: 0.753047 test loss: 1.339914
[Epoch 77; Iter    12/   48] train: loss: 0.2029658
[Epoch 77; Iter    42/   48] train: loss: 0.0566865
[Epoch 77] ogbg-molbbbp: 0.989560 val loss: 0.098568
[Epoch 77] ogbg-molbbbp: 0.777599 test loss: 1.218228
[Epoch 78; Iter    24/   48] train: loss: 0.0615400
[Epoch 78] ogbg-molbbbp: 0.988152 val loss: 0.097685
[Epoch 78] ogbg-molbbbp: 0.763620 test loss: 1.259947
[Epoch 79; Iter     6/   48] train: loss: 0.0077714
[Epoch 79; Iter    36/   48] train: loss: 0.0041343
[Epoch 79] ogbg-molbbbp: 0.986276 val loss: 0.107374
[Epoch 79] ogbg-molbbbp: 0.773746 test loss: 1.248111
[Epoch 80; Iter    18/   48] train: loss: 0.0924365
[Epoch 80; Iter    48/   48] train: loss: 0.1148270
[Epoch 80] ogbg-molbbbp: 0.989443 val loss: 0.097842
[Epoch 80] ogbg-molbbbp: 0.776882 test loss: 1.263314
[Epoch 81; Iter    30/   48] train: loss: 0.0421551
[Epoch 81] ogbg-molbbbp: 0.986158 val loss: 0.084681
[Epoch 81] ogbg-molbbbp: 0.757482 test loss: 1.407425
[Epoch 82; Iter    12/   48] train: loss: 0.0173601
[Epoch 82; Iter    42/   48] train: loss: 0.0066075
[Epoch 82] ogbg-molbbbp: 0.990264 val loss: 0.094930
[Epoch 82] ogbg-molbbbp: 0.770251 test loss: 1.279942
[Epoch 83; Iter    24/   48] train: loss: 0.0585535
[Epoch 83] ogbg-molbbbp: 0.989091 val loss: 0.108648
[Epoch 83] ogbg-molbbbp: 0.769489 test loss: 1.254766
[Epoch 84; Iter     6/   48] train: loss: 0.0087892
[Epoch 84; Iter    36/   48] train: loss: 0.0053921
[Epoch 84] ogbg-molbbbp: 0.983343 val loss: 0.099693
[Epoch 84] ogbg-molbbbp: 0.768145 test loss: 1.326901
[Epoch 85; Iter    18/   48] train: loss: 0.0924215
[Epoch 85; Iter    48/   48] train: loss: 0.0081903
[Epoch 85] ogbg-molbbbp: 0.985806 val loss: 0.089772
[Epoch 85] ogbg-molbbbp: 0.763799 test loss: 1.382693
[Epoch 86; Iter    30/   48] train: loss: 0.0081343
[Epoch 86] ogbg-molbbbp: 0.988974 val loss: 0.083367
[Epoch 86] ogbg-molbbbp: 0.768414 test loss: 1.416483
[Epoch 87; Iter    12/   48] train: loss: 0.0033587
[Epoch 87; Iter    42/   48] train: loss: 0.0649606
[Epoch 87] ogbg-molbbbp: 0.985337 val loss: 0.118426
[Epoch 87] ogbg-molbbbp: 0.757124 test loss: 1.304354
[Epoch 88; Iter    24/   48] train: loss: 0.0154953
[Epoch 88] ogbg-molbbbp: 0.990381 val loss: 0.099710
[Epoch 88] ogbg-molbbbp: 0.758065 test loss: 1.368590
[Epoch 89; Iter     6/   48] train: loss: 0.0134470
[Epoch 89; Iter    36/   48] train: loss: 0.0119890
[Epoch 89] ogbg-molbbbp: 0.991554 val loss: 0.102749
[Epoch 89] ogbg-molbbbp: 0.769624 test loss: 1.424539
[Epoch 90; Iter    18/   48] train: loss: 0.0019413
[Epoch 90; Iter    48/   48] train: loss: 0.0027607
[Epoch 90] ogbg-molbbbp: 0.977478 val loss: 0.124577
[Epoch 90] ogbg-molbbbp: 0.746057 test loss: 1.473642
[Epoch 91; Iter    30/   48] train: loss: 0.0509835
[Epoch 91] ogbg-molbbbp: 0.985220 val loss: 0.120239
[Epoch 91] ogbg-molbbbp: 0.775896 test loss: 1.262187
[Epoch 92; Iter    12/   48] train: loss: 0.0045506
[Epoch 92; Iter    42/   48] train: loss: 0.0217385
[Epoch 92] ogbg-molbbbp: 0.987097 val loss: 0.112833
[Epoch 92] ogbg-molbbbp: 0.774866 test loss: 1.184214
[Epoch 93; Iter    24/   48] train: loss: 0.0659593
[Epoch 93] ogbg-molbbbp: 0.977361 val loss: 0.116236
[Epoch 93] ogbg-molbbbp: 0.759498 test loss: 1.447584
[Epoch 94; Iter     6/   48] train: loss: 0.0060437
[Epoch 94; Iter    36/   48] train: loss: 0.0087644
[Epoch 94] ogbg-molbbbp: 0.980411 val loss: 0.121760
[Epoch 94] ogbg-molbbbp: 0.763799 test loss: 1.372299
[Epoch 95; Iter    18/   48] train: loss: 0.0201531
[Epoch 95; Iter    48/   48] train: loss: 0.1118199
[Epoch 95] ogbg-molbbbp: 0.982170 val loss: 0.112644
[Epoch 95] ogbg-molbbbp: 0.764740 test loss: 1.450765
[Epoch 96; Iter    30/   48] train: loss: 0.0684949
[Epoch 96] ogbg-molbbbp: 0.978534 val loss: 0.105985
[Epoch 96] ogbg-molbbbp: 0.762186 test loss: 1.500419
[Epoch 97; Iter    12/   48] train: loss: 0.0033002
[Epoch 97; Iter    42/   48] train: loss: 0.0036927
[Epoch 97] ogbg-molbbbp: 0.981232 val loss: 0.109432
[Epoch 97] ogbg-molbbbp: 0.757392 test loss: 1.494885
[Epoch 98; Iter    24/   48] train: loss: 0.0078274
[Epoch 98] ogbg-molbbbp: 0.983343 val loss: 0.087877
[Epoch 98] ogbg-molbbbp: 0.762321 test loss: 1.756384
[Epoch 99; Iter     6/   48] train: loss: 0.0052406
[Epoch 99; Iter    36/   48] train: loss: 0.0031845
[Epoch 99] ogbg-molbbbp: 0.988270 val loss: 0.089919
[Epoch 99] ogbg-molbbbp: 0.758826 test loss: 1.518210
[Epoch 100; Iter    18/   48] train: loss: 0.0150210
[Epoch 100; Iter    48/   48] train: loss: 0.0015178
[Epoch 100] ogbg-molbbbp: 0.987801 val loss: 0.101848
[Epoch 100] ogbg-molbbbp: 0.762142 test loss: 1.471202
[Epoch 101; Iter    30/   48] train: loss: 0.0033867
[Epoch 101] ogbg-molbbbp: 0.984985 val loss: 0.130498
[Epoch 101] ogbg-molbbbp: 0.742832 test loss: 1.456370
[Epoch 102; Iter    12/   48] train: loss: 0.0241519
[Epoch 102; Iter    42/   48] train: loss: 0.0057361
[Epoch 102] ogbg-molbbbp: 0.987683 val loss: 0.102809
[Epoch 102] ogbg-molbbbp: 0.763530 test loss: 1.482685
[Epoch 103; Iter    24/   48] train: loss: 0.0157610
[Epoch 103] ogbg-molbbbp: 0.986628 val loss: 0.098489
[Epoch 103] ogbg-molbbbp: 0.766980 test loss: 1.540312
[Epoch 104; Iter     6/   48] train: loss: 0.1844391
[Epoch 104; Iter    36/   48] train: loss: 0.0082706
[Epoch 104] ogbg-molbbbp: 0.980880 val loss: 0.114948
[Epoch 104] ogbg-molbbbp: 0.768190 test loss: 1.482261
[Epoch 105; Iter    18/   48] train: loss: 0.0090933
[Epoch 105; Iter    48/   48] train: loss: 0.0150816
[Epoch 105] ogbg-molbbbp: 0.984047 val loss: 0.089413
[Epoch 105] ogbg-molbbbp: 0.770968 test loss: 1.466618
[Epoch 106; Iter    30/   48] train: loss: 0.0326861
[Epoch 106] ogbg-molbbbp: 0.984047 val loss: 0.106679
[Epoch 106] ogbg-molbbbp: 0.770789 test loss: 1.510722
[Epoch 107; Iter    12/   48] train: loss: 0.0059412
[Epoch 107; Iter    42/   48] train: loss: 0.0833224
[Epoch 107] ogbg-molbbbp: 0.984985 val loss: 0.087499
[Epoch 107] ogbg-molbbbp: 0.775986 test loss: 1.467875
[Epoch 108; Iter    24/   48] train: loss: 0.0077198
[Epoch 108] ogbg-molbbbp: 0.985220 val loss: 0.086705
[Epoch 108] ogbg-molbbbp: 0.769848 test loss: 1.516479
[Epoch 109; Iter     6/   48] train: loss: 0.0215595
[Epoch 109; Iter    36/   48] train: loss: 0.0344070
[Epoch 109] ogbg-molbbbp: 0.984516 val loss: 0.089052
[Epoch 109] ogbg-molbbbp: 0.776792 test loss: 1.530749
[Epoch 110; Iter    18/   48] train: loss: 0.0021122
[Epoch 110; Iter    48/   48] train: loss: 0.0926666
[Epoch 110] ogbg-molbbbp: 0.980997 val loss: 0.098159
[Epoch 110] ogbg-molbbbp: 0.763441 test loss: 1.474540
[Epoch 111; Iter    30/   48] train: loss: 0.0086785
[Epoch 111] ogbg-molbbbp: 0.981232 val loss: 0.091916
[Epoch 111] ogbg-molbbbp: 0.761380 test loss: 1.631278
[Epoch 112; Iter    12/   48] train: loss: 0.0116511
[Epoch 112; Iter    42/   48] train: loss: 0.0037996
[Epoch 112] ogbg-molbbbp: 0.986041 val loss: 0.104453
[Epoch 112] ogbg-molbbbp: 0.765009 test loss: 1.462093
[Epoch 113; Iter    24/   48] train: loss: 0.0018571
[Epoch 113] ogbg-molbbbp: 0.984282 val loss: 0.105533
[Epoch 113] ogbg-molbbbp: 0.761783 test loss: 1.535236
[Epoch 114; Iter     6/   48] train: loss: 0.0021530
[Epoch 114; Iter    36/   48] train: loss: 0.0033154
[Epoch 114] ogbg-molbbbp: 0.985689 val loss: 0.106361
[Epoch 114] ogbg-molbbbp: 0.774866 test loss: 1.432390
[Epoch 115; Iter    18/   48] train: loss: 0.0040416
[Epoch 115; Iter    48/   48] train: loss: 0.1876587
[Epoch 115] ogbg-molbbbp: 0.986628 val loss: 0.087631
[Epoch 115] ogbg-molbbbp: 0.769937 test loss: 1.582706
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.1137700
[Epoch 116] ogbg-molbbbp: 0.954018 val loss: 0.189815
[Epoch 116] ogbg-molbbbp: 0.769803 test loss: 1.563435
[Epoch 117; Iter    12/   48] train: loss: 0.0006060
[Epoch 117; Iter    42/   48] train: loss: 0.0545999
[Epoch 117] ogbg-molbbbp: 0.950968 val loss: 0.172062
[Epoch 117] ogbg-molbbbp: 0.758109 test loss: 1.556221
[Epoch 118; Iter    24/   48] train: loss: 0.0094969
[Epoch 118] ogbg-molbbbp: 0.948974 val loss: 0.174436
[Epoch 118] ogbg-molbbbp: 0.765233 test loss: 1.475803
[Epoch 119; Iter     6/   48] train: loss: 0.0445760
[Epoch 119; Iter    36/   48] train: loss: 0.0096362
[Epoch 119] ogbg-molbbbp: 0.947918 val loss: 0.195646
[Epoch 119] ogbg-molbbbp: 0.769713 test loss: 1.419564
[Epoch 120; Iter    18/   48] train: loss: 0.0106450
[Epoch 120; Iter    48/   48] train: loss: 0.0272104
[Epoch 120] ogbg-molbbbp: 0.949795 val loss: 0.147335
[Epoch 120] ogbg-molbbbp: 0.765143 test loss: 1.760967
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 43.
Statistics on  val_best_checkpoint
mean_pred: 3.0404250621795654
std_pred: 9.668976783752441
mean_targets: 0.8986928462982178
std_targets: 0.30222928524017334
prcauc: 0.9984914271889667
rocauc: 0.987683284457478
ogbg-molbbbp: 0.987683284457478
BCEWithLogitsLoss: 0.10541071336377751
Statistics on  test
mean_pred: -3.622145652770996
std_pred: 30.070858001708984
mean_targets: 0.3921568691730499
std_targets: 0.4890310764312744
prcauc: 0.7390809234169653
rocauc: 0.8095878136200716
ogbg-molbbbp: 0.8095878136200716
BCEWithLogitsLoss: 0.7926955579949374
Statistics on  train
mean_pred: 3.34363055229187
std_pred: 4.103304862976074
mean_targets: 0.816398024559021
std_targets: 0.3872949779033661
prcauc: 0.9962311219368912
rocauc: 0.9848163679847983
ogbg-molbbbp: 0.9848163679847983
BCEWithLogitsLoss: 0.11860108569574852
[Epoch 116; Iter    30/   48] train: loss: 0.0315568
[Epoch 116] ogbg-molbbbp: 0.962346 val loss: 0.180995
[Epoch 116] ogbg-molbbbp: 0.766398 test loss: 1.509546
[Epoch 117; Iter    12/   48] train: loss: 0.0442149
[Epoch 117; Iter    42/   48] train: loss: 0.1192649
[Epoch 117] ogbg-molbbbp: 0.954487 val loss: 0.182238
[Epoch 117] ogbg-molbbbp: 0.764785 test loss: 1.641885
[Epoch 118; Iter    24/   48] train: loss: 0.0012387
[Epoch 118] ogbg-molbbbp: 0.935484 val loss: 0.186180
[Epoch 118] ogbg-molbbbp: 0.738262 test loss: 1.777668
[Epoch 119; Iter     6/   48] train: loss: 0.0021601
[Epoch 119; Iter    36/   48] train: loss: 0.0222153
[Epoch 119] ogbg-molbbbp: 0.964340 val loss: 0.163657
[Epoch 119] ogbg-molbbbp: 0.753808 test loss: 1.726363
[Epoch 120; Iter    18/   48] train: loss: 0.0025018
[Epoch 120; Iter    48/   48] train: loss: 0.0057432
[Epoch 120] ogbg-molbbbp: 0.962933 val loss: 0.179414
[Epoch 120] ogbg-molbbbp: 0.759185 test loss: 1.627586
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 44.
Statistics on  val_best_checkpoint
mean_pred: 2.5305752754211426
std_pred: 2.843219041824341
mean_targets: 0.8986928462982178
std_targets: 0.30222928524017334
prcauc: 0.9989396179908863
rocauc: 0.9906158357771262
ogbg-molbbbp: 0.9906158357771262
BCEWithLogitsLoss: 0.1532005647366697
Statistics on  test
mean_pred: -0.19324079155921936
std_pred: 3.4044559001922607
mean_targets: 0.3921568691730499
std_targets: 0.4890310764312744
prcauc: 0.6350438848261418
rocauc: 0.7698028673835126
ogbg-molbbbp: 0.7698028673835126
BCEWithLogitsLoss: 0.7208771547725932
Statistics on  train
mean_pred: 2.5230345726013184
std_pred: 2.919048547744751
mean_targets: 0.816398024559021
std_targets: 0.3872950077056885
prcauc: 0.9971102305957047
rocauc: 0.9877240114012384
ogbg-molbbbp: 0.9877240114012384
BCEWithLogitsLoss: 0.12763190610955158
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 116; Iter    30/   48] train: loss: 0.0060926
[Epoch 116] ogbg-molbbbp: 0.986158 val loss: 0.090893
[Epoch 116] ogbg-molbbbp: 0.774104 test loss: 1.497919
[Epoch 117; Iter    12/   48] train: loss: 0.0007037
[Epoch 117; Iter    42/   48] train: loss: 0.0022477
[Epoch 117] ogbg-molbbbp: 0.983812 val loss: 0.105266
[Epoch 117] ogbg-molbbbp: 0.766801 test loss: 1.512240
[Epoch 118; Iter    24/   48] train: loss: 0.0029938
[Epoch 118] ogbg-molbbbp: 0.985337 val loss: 0.102451
[Epoch 118] ogbg-molbbbp: 0.766084 test loss: 1.517611
[Epoch 119; Iter     6/   48] train: loss: 0.0015549
[Epoch 119; Iter    36/   48] train: loss: 0.0026148
[Epoch 119] ogbg-molbbbp: 0.985103 val loss: 0.103833
[Epoch 119] ogbg-molbbbp: 0.760305 test loss: 1.497897
[Epoch 120; Iter    18/   48] train: loss: 0.0501142
[Epoch 120; Iter    48/   48] train: loss: 0.0017601
[Epoch 120] ogbg-molbbbp: 0.982874 val loss: 0.100882
[Epoch 120] ogbg-molbbbp: 0.763262 test loss: 1.548741
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 41.
Statistics on  val_best_checkpoint
mean_pred: 3.84993052482605
std_pred: 3.6491308212280273
mean_targets: 0.8986928462982178
std_targets: 0.30222928524017334
prcauc: 0.9992807570566443
rocauc: 0.9937829912023461
ogbg-molbbbp: 0.9937829912023461
BCEWithLogitsLoss: 0.07181999832391739
Statistics on  test
mean_pred: 0.3461778461933136
std_pred: 4.399850368499756
mean_targets: 0.3921568691730499
std_targets: 0.4890310764312744
prcauc: 0.6924371705304363
rocauc: 0.8080197132616487
ogbg-molbbbp: 0.8080197132616487
BCEWithLogitsLoss: 0.8803664716807279
Statistics on  train
mean_pred: 3.537658452987671
std_pred: 3.5127007961273193
mean_targets: 0.816398024559021
std_targets: 0.3872950077056885
prcauc: 0.9959655970956298
rocauc: 0.9823215280280445
ogbg-molbbbp: 0.9823215280280445
BCEWithLogitsLoss: 0.15310574365624538
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.944240 val loss: 0.624432
[Epoch 109] ogbg-molbbbp: 0.675251 test loss: 2.297485
[Epoch 110; Iter     5/   55] train: loss: 0.0069638
[Epoch 110; Iter    35/   55] train: loss: 0.0016277
[Epoch 110] ogbg-molbbbp: 0.944538 val loss: 0.543650
[Epoch 110] ogbg-molbbbp: 0.669753 test loss: 2.119464
[Epoch 111; Iter    10/   55] train: loss: 0.0010011
[Epoch 111; Iter    40/   55] train: loss: 0.0009905
[Epoch 111] ogbg-molbbbp: 0.942149 val loss: 0.582186
[Epoch 111] ogbg-molbbbp: 0.668210 test loss: 2.152203
[Epoch 112; Iter    15/   55] train: loss: 0.0034036
[Epoch 112; Iter    45/   55] train: loss: 0.0018590
[Epoch 112] ogbg-molbbbp: 0.941750 val loss: 0.656456
[Epoch 112] ogbg-molbbbp: 0.678723 test loss: 2.371150
[Epoch 113; Iter    20/   55] train: loss: 0.0233495
[Epoch 113; Iter    50/   55] train: loss: 0.0180538
[Epoch 113] ogbg-molbbbp: 0.938863 val loss: 0.575714
[Epoch 113] ogbg-molbbbp: 0.673997 test loss: 2.103981
[Epoch 114; Iter    25/   55] train: loss: 0.0015838
[Epoch 114; Iter    55/   55] train: loss: 0.0080639
[Epoch 114] ogbg-molbbbp: 0.940755 val loss: 0.640693
[Epoch 114] ogbg-molbbbp: 0.665799 test loss: 2.361652
[Epoch 115; Iter    30/   55] train: loss: 0.0165870
[Epoch 115] ogbg-molbbbp: 0.941750 val loss: 0.617290
[Epoch 115] ogbg-molbbbp: 0.668499 test loss: 2.365674
[Epoch 116; Iter     5/   55] train: loss: 0.0022984
[Epoch 116; Iter    35/   55] train: loss: 0.0158194
[Epoch 116] ogbg-molbbbp: 0.934382 val loss: 0.686076
[Epoch 116] ogbg-molbbbp: 0.658854 test loss: 2.383077
[Epoch 117; Iter    10/   55] train: loss: 0.0646324
[Epoch 117; Iter    40/   55] train: loss: 0.0020701
[Epoch 117] ogbg-molbbbp: 0.940655 val loss: 0.647534
[Epoch 117] ogbg-molbbbp: 0.672550 test loss: 2.349280
[Epoch 118; Iter    15/   55] train: loss: 0.0018004
[Epoch 118; Iter    45/   55] train: loss: 0.0024019
[Epoch 118] ogbg-molbbbp: 0.946231 val loss: 0.562752
[Epoch 118] ogbg-molbbbp: 0.669464 test loss: 2.286236
[Epoch 119; Iter    20/   55] train: loss: 0.0672669
[Epoch 119; Iter    50/   55] train: loss: 0.0018879
[Epoch 119] ogbg-molbbbp: 0.941153 val loss: 0.703835
[Epoch 119] ogbg-molbbbp: 0.657118 test loss: 2.493542
[Epoch 120; Iter    25/   55] train: loss: 0.0301185
[Epoch 120; Iter    55/   55] train: loss: 0.0020584
[Epoch 120] ogbg-molbbbp: 0.943244 val loss: 0.671477
[Epoch 120] ogbg-molbbbp: 0.667245 test loss: 2.442385
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 40.
Statistics on  val_best_checkpoint
mean_pred: -0.8149829506874084
std_pred: 6.078700065612793
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9565923547000457
rocauc: 0.9719207408144976
ogbg-molbbbp: 0.9719207408144976
BCEWithLogitsLoss: 0.2537672274879047
Statistics on  test
mean_pred: 1.8421497344970703
std_pred: 4.32622766494751
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.692652354315798
rocauc: 0.6819058641975309
ogbg-molbbbp: 0.6819058641975309
BCEWithLogitsLoss: 1.3580974084990365
Statistics on  train
mean_pred: 3.29396653175354
std_pred: 3.2721714973449707
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9974875837780266
rocauc: 0.9876658172511277
ogbg-molbbbp: 0.9876658172511277
BCEWithLogitsLoss: 0.1076399032365192
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 109] ogbg-molbbbp: 0.935975 val loss: 0.638227
[Epoch 109] ogbg-molbbbp: 0.646123 test loss: 2.202115
[Epoch 110; Iter     5/   55] train: loss: 0.0038292
[Epoch 110; Iter    35/   55] train: loss: 0.0082900
[Epoch 110] ogbg-molbbbp: 0.931395 val loss: 0.624984
[Epoch 110] ogbg-molbbbp: 0.635802 test loss: 2.113864
[Epoch 111; Iter    10/   55] train: loss: 0.0039010
[Epoch 111; Iter    40/   55] train: loss: 0.0018606
[Epoch 111] ogbg-molbbbp: 0.942647 val loss: 0.591177
[Epoch 111] ogbg-molbbbp: 0.660687 test loss: 2.214737
[Epoch 112; Iter    15/   55] train: loss: 0.0046519
[Epoch 112; Iter    45/   55] train: loss: 0.0050192
[Epoch 112] ogbg-molbbbp: 0.942049 val loss: 0.601359
[Epoch 112] ogbg-molbbbp: 0.646026 test loss: 2.277916
[Epoch 113; Iter    20/   55] train: loss: 0.0107073
[Epoch 113; Iter    50/   55] train: loss: 0.0041698
[Epoch 113] ogbg-molbbbp: 0.938962 val loss: 0.659529
[Epoch 113] ogbg-molbbbp: 0.638214 test loss: 2.308250
[Epoch 114; Iter    25/   55] train: loss: 0.0300775
[Epoch 114; Iter    55/   55] train: loss: 0.2590482
[Epoch 114] ogbg-molbbbp: 0.939460 val loss: 0.644640
[Epoch 114] ogbg-molbbbp: 0.653646 test loss: 2.313752
[Epoch 115; Iter    30/   55] train: loss: 0.0038916
[Epoch 115] ogbg-molbbbp: 0.946032 val loss: 0.523798
[Epoch 115] ogbg-molbbbp: 0.652585 test loss: 2.111871
[Epoch 116; Iter     5/   55] train: loss: 0.0014116
[Epoch 116; Iter    35/   55] train: loss: 0.0038831
[Epoch 116] ogbg-molbbbp: 0.938465 val loss: 0.644700
[Epoch 116] ogbg-molbbbp: 0.629726 test loss: 2.417217
[Epoch 117; Iter    10/   55] train: loss: 0.0978563
[Epoch 117; Iter    40/   55] train: loss: 0.0050907
[Epoch 117] ogbg-molbbbp: 0.941352 val loss: 0.643185
[Epoch 117] ogbg-molbbbp: 0.649016 test loss: 2.367752
[Epoch 118; Iter    15/   55] train: loss: 0.0027645
[Epoch 118; Iter    45/   55] train: loss: 0.0019276
[Epoch 118] ogbg-molbbbp: 0.938066 val loss: 0.666123
[Epoch 118] ogbg-molbbbp: 0.631462 test loss: 2.527893
[Epoch 119; Iter    20/   55] train: loss: 0.0550652
[Epoch 119; Iter    50/   55] train: loss: 0.0319576
[Epoch 119] ogbg-molbbbp: 0.946231 val loss: 0.588460
[Epoch 119] ogbg-molbbbp: 0.650656 test loss: 2.326057
[Epoch 120; Iter    25/   55] train: loss: 0.0090764
[Epoch 120; Iter    55/   55] train: loss: 0.0010857
[Epoch 120] ogbg-molbbbp: 0.944937 val loss: 0.550639
[Epoch 120] ogbg-molbbbp: 0.637731 test loss: 2.270240
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 120 epochs. Best model checkpoint was in epoch 49.
Statistics on  val_best_checkpoint
mean_pred: -0.04833042249083519
std_pred: 6.151402950286865
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9534505164637735
rocauc: 0.9694314447874142
ogbg-molbbbp: 0.9694314447874142
BCEWithLogitsLoss: 0.31228756265980856
Statistics on  test
mean_pred: 1.7550634145736694
std_pred: 4.465105056762695
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.7164285834433872
rocauc: 0.6949266975308641
ogbg-molbbbp: 0.6949266975308641
BCEWithLogitsLoss: 1.3307540927614485
Statistics on  train
mean_pred: 4.553627967834473
std_pred: 4.289630889892578
mean_targets: 0.839362382888794
std_targets: 0.367309033870697
prcauc: 0.9985762473152502
rocauc: 0.9927204902447321
ogbg-molbbbp: 0.9927204902447321
BCEWithLogitsLoss: 0.08813102313063362
[Epoch 109] ogbg-molbbbp: 0.936473 val loss: 0.480766
[Epoch 109] ogbg-molbbbp: 0.581019 test loss: 2.193384
[Epoch 110; Iter     5/   55] train: loss: 0.0408779
[Epoch 110; Iter    35/   55] train: loss: 0.1236812
[Epoch 110] ogbg-molbbbp: 0.954097 val loss: 0.561767
[Epoch 110] ogbg-molbbbp: 0.608893 test loss: 2.599226
[Epoch 111; Iter    10/   55] train: loss: 0.0057282
[Epoch 111; Iter    40/   55] train: loss: 0.0031820
[Epoch 111] ogbg-molbbbp: 0.951708 val loss: 0.557045
[Epoch 111] ogbg-molbbbp: 0.640721 test loss: 2.334418
[Epoch 112; Iter    15/   55] train: loss: 0.0510457
[Epoch 112; Iter    45/   55] train: loss: 0.0033237
[Epoch 112] ogbg-molbbbp: 0.942945 val loss: 0.681376
[Epoch 112] ogbg-molbbbp: 0.634934 test loss: 2.667924
[Epoch 113; Iter    20/   55] train: loss: 0.0064737
[Epoch 113; Iter    50/   55] train: loss: 0.0967859
[Epoch 113] ogbg-molbbbp: 0.962860 val loss: 0.576388
[Epoch 113] ogbg-molbbbp: 0.617188 test loss: 2.980194
[Epoch 114; Iter    25/   55] train: loss: 0.0021778
[Epoch 114; Iter    55/   55] train: loss: 0.0047339
[Epoch 114] ogbg-molbbbp: 0.939759 val loss: 0.847054
[Epoch 114] ogbg-molbbbp: 0.581694 test loss: 3.315471
[Epoch 115; Iter    30/   55] train: loss: 0.0074123
[Epoch 115] ogbg-molbbbp: 0.952703 val loss: 0.530597
[Epoch 115] ogbg-molbbbp: 0.628472 test loss: 2.355106
[Epoch 116; Iter     5/   55] train: loss: 0.0055279
[Epoch 116; Iter    35/   55] train: loss: 0.0435787
[Epoch 116] ogbg-molbbbp: 0.948621 val loss: 0.626499
[Epoch 116] ogbg-molbbbp: 0.640432 test loss: 2.556476
[Epoch 117; Iter    10/   55] train: loss: 0.0279010
[Epoch 117; Iter    40/   55] train: loss: 0.0154675
[Epoch 117] ogbg-molbbbp: 0.952106 val loss: 0.603449
[Epoch 117] ogbg-molbbbp: 0.641397 test loss: 2.445776
[Epoch 118; Iter    15/   55] train: loss: 0.1460751
[Epoch 118; Iter    45/   55] train: loss: 0.0047399
[Epoch 118] ogbg-molbbbp: 0.953002 val loss: 0.553699
[Epoch 118] ogbg-molbbbp: 0.649595 test loss: 2.169356
[Epoch 119; Iter    20/   55] train: loss: 0.0031611
[Epoch 119; Iter    50/   55] train: loss: 0.1013861
[Epoch 119] ogbg-molbbbp: 0.957682 val loss: 0.482767
[Epoch 119] ogbg-molbbbp: 0.652296 test loss: 2.234039
[Epoch 120; Iter    25/   55] train: loss: 0.0822513
[Epoch 120; Iter    55/   55] train: loss: 0.1040576
[Epoch 120] ogbg-molbbbp: 0.949318 val loss: 0.576373
[Epoch 120] ogbg-molbbbp: 0.639660 test loss: 2.317684
[Epoch 121; Iter    30/   55] train: loss: 0.0531603
[Epoch 121] ogbg-molbbbp: 0.946231 val loss: 0.509712
[Epoch 121] ogbg-molbbbp: 0.654996 test loss: 2.051103
[Epoch 122; Iter     5/   55] train: loss: 0.0796894
[Epoch 122; Iter    35/   55] train: loss: 0.0019792
[Epoch 122] ogbg-molbbbp: 0.946132 val loss: 0.622612
[Epoch 122] ogbg-molbbbp: 0.651235 test loss: 2.528623
[Epoch 123; Iter    10/   55] train: loss: 0.0125971
[Epoch 123; Iter    40/   55] train: loss: 0.0051307
[Epoch 123] ogbg-molbbbp: 0.947824 val loss: 0.532392
[Epoch 123] ogbg-molbbbp: 0.648148 test loss: 2.225197
[Epoch 124; Iter    15/   55] train: loss: 0.0085299
[Epoch 124; Iter    45/   55] train: loss: 0.0015761
[Epoch 124] ogbg-molbbbp: 0.940755 val loss: 0.642352
[Epoch 124] ogbg-molbbbp: 0.654417 test loss: 2.306476
[Epoch 125; Iter    20/   55] train: loss: 0.1118523
[Epoch 125; Iter    50/   55] train: loss: 0.0347266
[Epoch 125] ogbg-molbbbp: 0.947924 val loss: 0.570210
[Epoch 125] ogbg-molbbbp: 0.646991 test loss: 2.298112
[Epoch 126; Iter    25/   55] train: loss: 0.0986483
[Epoch 126; Iter    55/   55] train: loss: 0.0015954
[Epoch 126] ogbg-molbbbp: 0.946331 val loss: 0.617383
[Epoch 126] ogbg-molbbbp: 0.660108 test loss: 2.345233
[Epoch 127; Iter    30/   55] train: loss: 0.0016056
[Epoch 127] ogbg-molbbbp: 0.948820 val loss: 0.589793
[Epoch 127] ogbg-molbbbp: 0.662133 test loss: 2.382343
[Epoch 128; Iter     5/   55] train: loss: 0.0016427
[Epoch 128; Iter    35/   55] train: loss: 0.0010147
[Epoch 128] ogbg-molbbbp: 0.946132 val loss: 0.566122
[Epoch 128] ogbg-molbbbp: 0.650463 test loss: 2.287645
[Epoch 129; Iter    10/   55] train: loss: 0.0039864
[Epoch 129; Iter    40/   55] train: loss: 0.0064567
[Epoch 129] ogbg-molbbbp: 0.947625 val loss: 0.569212
[Epoch 129] ogbg-molbbbp: 0.654321 test loss: 2.336884
[Epoch 130; Iter    15/   55] train: loss: 0.0430136
[Epoch 130; Iter    45/   55] train: loss: 0.0031360
[Epoch 130] ogbg-molbbbp: 0.940755 val loss: 0.706718
[Epoch 130] ogbg-molbbbp: 0.643711 test loss: 2.515561
[Epoch 131; Iter    20/   55] train: loss: 0.0209089
[Epoch 131; Iter    50/   55] train: loss: 0.0018052
[Epoch 131] ogbg-molbbbp: 0.940157 val loss: 0.692737
[Epoch 131] ogbg-molbbbp: 0.622878 test loss: 2.637695
[Epoch 132; Iter    25/   55] train: loss: 0.0045457
[Epoch 132; Iter    55/   55] train: loss: 0.0015928
[Epoch 132] ogbg-molbbbp: 0.933287 val loss: 0.857656
[Epoch 132] ogbg-molbbbp: 0.633777 test loss: 2.674340
[Epoch 133; Iter    30/   55] train: loss: 0.0009900
[Epoch 133] ogbg-molbbbp: 0.946530 val loss: 0.728891
[Epoch 133] ogbg-molbbbp: 0.625000 test loss: 3.003992
[Epoch 134; Iter     5/   55] train: loss: 0.0005813
[Epoch 134; Iter    35/   55] train: loss: 0.0963425
[Epoch 134] ogbg-molbbbp: 0.955193 val loss: 0.553661
[Epoch 134] ogbg-molbbbp: 0.630787 test loss: 2.576223
[Epoch 135; Iter    10/   55] train: loss: 0.0035737
[Epoch 135; Iter    40/   55] train: loss: 0.0030857
[Epoch 135] ogbg-molbbbp: 0.951309 val loss: 0.671377
[Epoch 135] ogbg-molbbbp: 0.631944 test loss: 2.813553
[Epoch 136; Iter    15/   55] train: loss: 0.0060794
[Epoch 136; Iter    45/   55] train: loss: 0.1753115
[Epoch 136] ogbg-molbbbp: 0.932590 val loss: 0.705398
[Epoch 136] ogbg-molbbbp: 0.640721 test loss: 2.584365
[Epoch 137; Iter    20/   55] train: loss: 0.0063063
[Epoch 137; Iter    50/   55] train: loss: 0.0021394
[Epoch 137] ogbg-molbbbp: 0.939460 val loss: 0.687281
[Epoch 137] ogbg-molbbbp: 0.628376 test loss: 2.686844
[Epoch 138; Iter    25/   55] train: loss: 0.0682306
[Epoch 138; Iter    55/   55] train: loss: 0.0113158
[Epoch 138] ogbg-molbbbp: 0.938465 val loss: 0.708474
[Epoch 138] ogbg-molbbbp: 0.616127 test loss: 2.574957
[Epoch 139; Iter    30/   55] train: loss: 0.0017619
[Epoch 139] ogbg-molbbbp: 0.944240 val loss: 0.651012
[Epoch 139] ogbg-molbbbp: 0.627990 test loss: 2.688269
[Epoch 140; Iter     5/   55] train: loss: 0.0104305
[Epoch 140; Iter    35/   55] train: loss: 0.0556998
[Epoch 140] ogbg-molbbbp: 0.948422 val loss: 0.595128
[Epoch 140] ogbg-molbbbp: 0.624904 test loss: 2.652763
[Epoch 141; Iter    10/   55] train: loss: 0.0051936
[Epoch 141; Iter    40/   55] train: loss: 0.0239783
[Epoch 141] ogbg-molbbbp: 0.938763 val loss: 0.698859
[Epoch 141] ogbg-molbbbp: 0.620853 test loss: 2.739287
[Epoch 142; Iter    15/   55] train: loss: 0.0011885
[Epoch 142; Iter    45/   55] train: loss: 0.0055910
[Epoch 142] ogbg-molbbbp: 0.933287 val loss: 0.676133
[Epoch 142] ogbg-molbbbp: 0.632812 test loss: 2.638025
[Epoch 143; Iter    20/   55] train: loss: 0.0013160
[Epoch 143; Iter    50/   55] train: loss: 0.0132353
[Epoch 143] ogbg-molbbbp: 0.932391 val loss: 0.688109
[Epoch 143] ogbg-molbbbp: 0.624711 test loss: 2.779395
[Epoch 144; Iter    25/   55] train: loss: 0.0029106
[Epoch 144; Iter    55/   55] train: loss: 0.0526411
[Epoch 144] ogbg-molbbbp: 0.940058 val loss: 0.687518
[Epoch 144] ogbg-molbbbp: 0.626254 test loss: 2.912852
[Epoch 145; Iter    30/   55] train: loss: 0.0088002
[Epoch 145] ogbg-molbbbp: 0.933685 val loss: 0.654587
[Epoch 145] ogbg-molbbbp: 0.627025 test loss: 2.658102
[Epoch 146; Iter     5/   55] train: loss: 0.0008394
[Epoch 146; Iter    35/   55] train: loss: 0.0025963
[Epoch 146] ogbg-molbbbp: 0.929404 val loss: 0.874947
[Epoch 146] ogbg-molbbbp: 0.625000 test loss: 2.698499
[Epoch 147; Iter    10/   55] train: loss: 0.0273033
[Epoch 147; Iter    40/   55] train: loss: 0.0521288
[Epoch 147] ogbg-molbbbp: 0.933586 val loss: 0.756598
[Epoch 147] ogbg-molbbbp: 0.627218 test loss: 2.870330
[Epoch 148; Iter    15/   55] train: loss: 0.0007219
[Epoch 148; Iter    45/   55] train: loss: 0.0012562
[Epoch 148] ogbg-molbbbp: 0.935378 val loss: 0.682953
[Epoch 148] ogbg-molbbbp: 0.629533 test loss: 2.687365
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 149; Iter    20/   55] train: loss: 0.0006192
[Epoch 149; Iter    50/   55] train: loss: 0.0012720
[Epoch 149] ogbg-molbbbp: 0.936672 val loss: 0.622244
[Epoch 149] ogbg-molbbbp: 0.631559 test loss: 2.477952
[Epoch 150; Iter    25/   55] train: loss: 0.0102301
[Epoch 150; Iter    55/   55] train: loss: 0.0005105
[Epoch 150] ogbg-molbbbp: 0.934482 val loss: 0.595375
[Epoch 150] ogbg-molbbbp: 0.634452 test loss: 2.409189
[Epoch 151; Iter    30/   55] train: loss: 0.0220997
[Epoch 151] ogbg-molbbbp: 0.931893 val loss: 0.765551
[Epoch 151] ogbg-molbbbp: 0.620756 test loss: 2.885192
[Epoch 152; Iter     5/   55] train: loss: 0.0007853
[Epoch 152; Iter    35/   55] train: loss: 0.0026508
[Epoch 152] ogbg-molbbbp: 0.937967 val loss: 0.709187
[Epoch 152] ogbg-molbbbp: 0.622878 test loss: 2.806766
[Epoch 153; Iter    10/   55] train: loss: 0.0221465
[Epoch 153; Iter    40/   55] train: loss: 0.0017896
[Epoch 153] ogbg-molbbbp: 0.938465 val loss: 0.671189
[Epoch 153] ogbg-molbbbp: 0.627894 test loss: 2.688144
[Epoch 154; Iter    15/   55] train: loss: 0.0209504
[Epoch 154; Iter    45/   55] train: loss: 0.0020423
[Epoch 154] ogbg-molbbbp: 0.937369 val loss: 0.702366
[Epoch 154] ogbg-molbbbp: 0.630208 test loss: 2.654565
[Epoch 155; Iter    20/   55] train: loss: 0.0007924
[Epoch 155; Iter    50/   55] train: loss: 0.0047919
[Epoch 155] ogbg-molbbbp: 0.938265 val loss: 0.755881
[Epoch 155] ogbg-molbbbp: 0.613619 test loss: 2.919545
[Epoch 156; Iter    25/   55] train: loss: 0.0016287
[Epoch 156; Iter    55/   55] train: loss: 0.0357713
[Epoch 156] ogbg-molbbbp: 0.938166 val loss: 0.670137
[Epoch 156] ogbg-molbbbp: 0.632330 test loss: 2.596612
[Epoch 157; Iter    30/   55] train: loss: 0.0259114
[Epoch 157] ogbg-molbbbp: 0.941850 val loss: 0.686775
[Epoch 157] ogbg-molbbbp: 0.620949 test loss: 2.790323
[Epoch 158; Iter     5/   55] train: loss: 0.0358474
[Epoch 158; Iter    35/   55] train: loss: 0.0248626
[Epoch 158] ogbg-molbbbp: 0.941950 val loss: 0.727114
[Epoch 158] ogbg-molbbbp: 0.629823 test loss: 2.902069
[Epoch 159; Iter    10/   55] train: loss: 0.0792764
[Epoch 159; Iter    40/   55] train: loss: 0.0360330
[Epoch 159] ogbg-molbbbp: 0.941153 val loss: 0.680033
[Epoch 159] ogbg-molbbbp: 0.645062 test loss: 2.568329
[Epoch 160; Iter    15/   55] train: loss: 0.0243400
[Epoch 160; Iter    45/   55] train: loss: 0.0006600
[Epoch 160] ogbg-molbbbp: 0.942149 val loss: 0.652016
[Epoch 160] ogbg-molbbbp: 0.632234 test loss: 2.612943
[Epoch 161; Iter    20/   55] train: loss: 0.0017091
[Epoch 161; Iter    50/   55] train: loss: 0.0191888
[Epoch 161] ogbg-molbbbp: 0.940655 val loss: 0.690435
[Epoch 161] ogbg-molbbbp: 0.634549 test loss: 2.767904
[Epoch 162; Iter    25/   55] train: loss: 0.0755069
[Epoch 162; Iter    55/   55] train: loss: 0.0003095
[Epoch 162] ogbg-molbbbp: 0.940157 val loss: 0.695775
[Epoch 162] ogbg-molbbbp: 0.634645 test loss: 2.862653
[Epoch 163; Iter    30/   55] train: loss: 0.0008057
[Epoch 163] ogbg-molbbbp: 0.942149 val loss: 0.700084
[Epoch 163] ogbg-molbbbp: 0.627894 test loss: 2.912926
[Epoch 164; Iter     5/   55] train: loss: 0.0022796
[Epoch 164; Iter    35/   55] train: loss: 0.0017837
[Epoch 164] ogbg-molbbbp: 0.939859 val loss: 0.673947
[Epoch 164] ogbg-molbbbp: 0.631076 test loss: 2.736344
[Epoch 165; Iter    10/   55] train: loss: 0.0005990
[Epoch 165; Iter    40/   55] train: loss: 0.0010974
[Epoch 165] ogbg-molbbbp: 0.942149 val loss: 0.672933
[Epoch 165] ogbg-molbbbp: 0.630305 test loss: 2.760354
[Epoch 166; Iter    15/   55] train: loss: 0.0205208
[Epoch 166; Iter    45/   55] train: loss: 0.0021992
[Epoch 166] ogbg-molbbbp: 0.938962 val loss: 0.752438
[Epoch 166] ogbg-molbbbp: 0.635995 test loss: 2.797546
[Epoch 167; Iter    20/   55] train: loss: 0.0013038
[Epoch 167; Iter    50/   55] train: loss: 0.0373222
[Epoch 167] ogbg-molbbbp: 0.941153 val loss: 0.637064
[Epoch 167] ogbg-molbbbp: 0.639660 test loss: 2.652976
[Epoch 168; Iter    25/   55] train: loss: 0.0005969
[Epoch 168; Iter    55/   55] train: loss: 0.0011775
[Epoch 168] ogbg-molbbbp: 0.940058 val loss: 0.676315
[Epoch 168] ogbg-molbbbp: 0.635995 test loss: 2.803928
[Epoch 169; Iter    30/   55] train: loss: 0.0201670
[Epoch 169] ogbg-molbbbp: 0.941750 val loss: 0.709489
[Epoch 169] ogbg-molbbbp: 0.633777 test loss: 2.911282
[Epoch 170; Iter     5/   55] train: loss: 0.0126248
[Epoch 170; Iter    35/   55] train: loss: 0.0010173
[Epoch 170] ogbg-molbbbp: 0.943344 val loss: 0.662253
[Epoch 170] ogbg-molbbbp: 0.636478 test loss: 2.744073
[Epoch 171; Iter    10/   55] train: loss: 0.0009213
[Epoch 171; Iter    40/   55] train: loss: 0.0563374
[Epoch 171] ogbg-molbbbp: 0.941153 val loss: 0.815119
[Epoch 171] ogbg-molbbbp: 0.633584 test loss: 3.109026
[Epoch 172; Iter    15/   55] train: loss: 0.0077189
[Epoch 172; Iter    45/   55] train: loss: 0.0156904
[Epoch 172] ogbg-molbbbp: 0.943841 val loss: 0.696275
[Epoch 172] ogbg-molbbbp: 0.634452 test loss: 2.823444
[Epoch 173; Iter    20/   55] train: loss: 0.0005857
[Epoch 173; Iter    50/   55] train: loss: 0.0065135
[Epoch 173] ogbg-molbbbp: 0.939859 val loss: 0.754219
[Epoch 173] ogbg-molbbbp: 0.635610 test loss: 2.886591
Early stopping criterion based on -ogbg-molbbbp- that should be max reached after 173 epochs. Best model checkpoint was in epoch 113.
Statistics on  val_best_checkpoint
mean_pred: -8.422119140625
std_pred: 107.56065368652344
mean_targets: 0.4068627655506134
std_targets: 0.4924573600292206
prcauc: 0.9357138828009662
rocauc: 0.9628597032759136
ogbg-molbbbp: 0.9628597032759136
BCEWithLogitsLoss: 0.5763876502002988
Statistics on  test
mean_pred: 0.9540039300918579
std_pred: 67.38677978515625
mean_targets: 0.529411792755127
std_targets: 0.5003620982170105
prcauc: 0.654264781094765
rocauc: 0.6171875
ogbg-molbbbp: 0.6171875
BCEWithLogitsLoss: 2.980193563870021
Statistics on  train
mean_pred: 7.545321464538574
std_pred: 6.710911750793457
mean_targets: 0.839362382888794
std_targets: 0.36730900406837463
prcauc: 0.9997918406682801
rocauc: 0.9988973396751404
ogbg-molbbbp: 0.9988973396751404
BCEWithLogitsLoss: 0.032380743707868864
All runs completed.
