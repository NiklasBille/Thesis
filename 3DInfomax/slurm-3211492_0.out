>>> Starting run for dataset: bace
Running RANDOM configs_split_experiments/3DInfomax/bace/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/3DInfomax/bace/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/3DInfomax/bace/random/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/bace/random/train_prop=0.7.yml --seed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.8/PNA_ogbg-molbace_3DInfomax_bace_random=0.8_5_26-05_09-18-11
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.8
logdir: runs/split/3DInfomax/bace/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6940570
[Epoch 1] ogbg-molbace: 0.407876 val loss: 0.694085
[Epoch 1] ogbg-molbace: 0.358033 test loss: 0.693912
[Epoch 2; Iter    19/   41] train: loss: 0.6972287
[Epoch 2] ogbg-molbace: 0.433544 val loss: 0.696635
[Epoch 2] ogbg-molbace: 0.371203 test loss: 0.695595
[Epoch 3; Iter     8/   41] train: loss: 0.6962361
[Epoch 3; Iter    38/   41] train: loss: 0.6912050
[Epoch 3] ogbg-molbace: 0.455872 val loss: 0.696163
[Epoch 3] ogbg-molbace: 0.395435 test loss: 0.696117
[Epoch 4; Iter    27/   41] train: loss: 0.6932663
[Epoch 4] ogbg-molbace: 0.446378 val loss: 0.696523
[Epoch 4] ogbg-molbace: 0.398595 test loss: 0.695563
[Epoch 5; Iter    16/   41] train: loss: 0.6967342
[Epoch 5] ogbg-molbace: 0.455872 val loss: 0.696516
[Epoch 5] ogbg-molbace: 0.401580 test loss: 0.695363
[Epoch 6; Iter     5/   41] train: loss: 0.6949556
[Epoch 6; Iter    35/   41] train: loss: 0.6958028
[Epoch 6] ogbg-molbace: 0.452532 val loss: 0.696588
[Epoch 6] ogbg-molbace: 0.402809 test loss: 0.695167
[Epoch 7; Iter    24/   41] train: loss: 0.6953294
[Epoch 7] ogbg-molbace: 0.452532 val loss: 0.696743
[Epoch 7] ogbg-molbace: 0.397366 test loss: 0.694901
[Epoch 8; Iter    13/   41] train: loss: 0.6930363
[Epoch 8] ogbg-molbace: 0.455169 val loss: 0.696735
[Epoch 8] ogbg-molbace: 0.405795 test loss: 0.694871
[Epoch 9; Iter     2/   41] train: loss: 0.6923611
[Epoch 9; Iter    32/   41] train: loss: 0.6938530
[Epoch 9] ogbg-molbace: 0.462553 val loss: 0.696583
[Epoch 9] ogbg-molbace: 0.408077 test loss: 0.694597
[Epoch 10; Iter    21/   41] train: loss: 0.6935959
[Epoch 10] ogbg-molbace: 0.461850 val loss: 0.696703
[Epoch 10] ogbg-molbace: 0.416155 test loss: 0.694042
[Epoch 11; Iter    10/   41] train: loss: 0.6975090
[Epoch 11; Iter    40/   41] train: loss: 0.6965709
[Epoch 11] ogbg-molbace: 0.463432 val loss: 0.697041
[Epoch 11] ogbg-molbace: 0.414574 test loss: 0.693794
[Epoch 12; Iter    29/   41] train: loss: 0.6928406
[Epoch 12] ogbg-molbace: 0.463783 val loss: 0.697076
[Epoch 12] ogbg-molbace: 0.419140 test loss: 0.693397
[Epoch 13; Iter    18/   41] train: loss: 0.6924687
[Epoch 13] ogbg-molbace: 0.459740 val loss: 0.696916
[Epoch 13] ogbg-molbace: 0.420193 test loss: 0.693174
[Epoch 14; Iter     7/   41] train: loss: 0.6939366
[Epoch 14; Iter    37/   41] train: loss: 0.6907464
[Epoch 14] ogbg-molbace: 0.467475 val loss: 0.697171
[Epoch 14] ogbg-molbace: 0.435119 test loss: 0.692507
[Epoch 15; Iter    26/   41] train: loss: 0.6910095
[Epoch 15] ogbg-molbace: 0.472222 val loss: 0.697272
[Epoch 15] ogbg-molbace: 0.432660 test loss: 0.692455
[Epoch 16; Iter    15/   41] train: loss: 0.6974158
[Epoch 16] ogbg-molbace: 0.465190 val loss: 0.697396
[Epoch 16] ogbg-molbace: 0.448112 test loss: 0.691735
[Epoch 17; Iter     4/   41] train: loss: 0.6895474
[Epoch 17; Iter    34/   41] train: loss: 0.6951162
[Epoch 17] ogbg-molbace: 0.617440 val loss: 0.690904
[Epoch 17] ogbg-molbace: 0.653556 test loss: 0.687425
[Epoch 18; Iter    23/   41] train: loss: 0.6610839
[Epoch 18] ogbg-molbace: 0.685478 val loss: 0.662933
[Epoch 18] ogbg-molbace: 0.750658 test loss: 0.653772
[Epoch 19; Iter    12/   41] train: loss: 0.6451595
[Epoch 19] ogbg-molbace: 0.800105 val loss: 0.599635
[Epoch 19] ogbg-molbace: 0.818086 test loss: 0.601095
[Epoch 20; Iter     1/   41] train: loss: 0.6062480
[Epoch 20; Iter    31/   41] train: loss: 0.6998763
[Epoch 20] ogbg-molbace: 0.779536 val loss: 0.532955
[Epoch 20] ogbg-molbace: 0.813169 test loss: 0.578406
[Epoch 21; Iter    20/   41] train: loss: 0.5569317
[Epoch 21] ogbg-molbace: 0.736639 val loss: 0.557508
[Epoch 21] ogbg-molbace: 0.784372 test loss: 0.577190
[Epoch 22; Iter     9/   41] train: loss: 0.4945386
[Epoch 22; Iter    39/   41] train: loss: 0.5244419
[Epoch 22] ogbg-molbace: 0.797820 val loss: 0.530909
[Epoch 22] ogbg-molbace: 0.784197 test loss: 0.549311
[Epoch 23; Iter    28/   41] train: loss: 0.6048598
[Epoch 23] ogbg-molbace: 0.760021 val loss: 0.572625
[Epoch 23] ogbg-molbace: 0.747147 test loss: 0.509253
[Epoch 24; Iter    17/   41] train: loss: 0.3474822
[Epoch 24] ogbg-molbace: 0.813819 val loss: 0.489356
[Epoch 24] ogbg-molbace: 0.833538 test loss: 0.452917
[Epoch 25; Iter     6/   41] train: loss: 0.5478536
[Epoch 25; Iter    36/   41] train: loss: 0.5641541
[Epoch 25] ogbg-molbace: 0.827532 val loss: 0.493326
[Epoch 25] ogbg-molbace: 0.831431 test loss: 0.451664
[Epoch 26; Iter    25/   41] train: loss: 0.4184179
[Epoch 26] ogbg-molbace: 0.806962 val loss: 0.606089
[Epoch 26] ogbg-molbace: 0.757331 test loss: 0.523988
[Epoch 27; Iter    14/   41] train: loss: 0.5877339
[Epoch 27] ogbg-molbace: 0.812060 val loss: 0.513220
[Epoch 27] ogbg-molbace: 0.799473 test loss: 0.539553
[Epoch 28; Iter     3/   41] train: loss: 0.4023864
[Epoch 28; Iter    33/   41] train: loss: 0.5069756
[Epoch 28] ogbg-molbace: 0.796765 val loss: 0.490227
[Epoch 28] ogbg-molbace: 0.825110 test loss: 0.461590
[Epoch 29; Iter    22/   41] train: loss: 0.4797353
[Epoch 29] ogbg-molbace: 0.828059 val loss: 0.547786
[Epoch 29] ogbg-molbace: 0.819666 test loss: 0.466898
[Epoch 30; Iter    11/   41] train: loss: 0.5540595
[Epoch 30; Iter    41/   41] train: loss: 0.4934928
[Epoch 30] ogbg-molbace: 0.820675 val loss: 0.513978
[Epoch 30] ogbg-molbace: 0.837752 test loss: 0.596431
[Epoch 31; Iter    30/   41] train: loss: 0.5312343
[Epoch 31] ogbg-molbace: 0.826653 val loss: 0.463044
[Epoch 31] ogbg-molbace: 0.849693 test loss: 0.436511
[Epoch 32; Iter    19/   41] train: loss: 0.4776548
[Epoch 32] ogbg-molbace: 0.815049 val loss: 0.493526
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.8/PNA_ogbg-molbace_3DInfomax_bace_random=0.8_4_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.8
logdir: runs/split/3DInfomax/bace/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6946718
[Epoch 1] ogbg-molbace: 0.411217 val loss: 0.693690
[Epoch 1] ogbg-molbace: 0.366462 test loss: 0.693440
[Epoch 2; Iter    19/   41] train: loss: 0.6953160
[Epoch 2] ogbg-molbace: 0.413326 val loss: 0.695585
[Epoch 2] ogbg-molbace: 0.367340 test loss: 0.694474
[Epoch 3; Iter     8/   41] train: loss: 0.6937386
[Epoch 3; Iter    38/   41] train: loss: 0.6968386
[Epoch 3] ogbg-molbace: 0.413854 val loss: 0.696409
[Epoch 3] ogbg-molbace: 0.365935 test loss: 0.694703
[Epoch 4; Iter    27/   41] train: loss: 0.6927726
[Epoch 4] ogbg-molbace: 0.420359 val loss: 0.696315
[Epoch 4] ogbg-molbace: 0.374715 test loss: 0.694577
[Epoch 5; Iter    16/   41] train: loss: 0.6952392
[Epoch 5] ogbg-molbace: 0.426160 val loss: 0.696250
[Epoch 5] ogbg-molbace: 0.381914 test loss: 0.694406
[Epoch 6; Iter     5/   41] train: loss: 0.6940311
[Epoch 6; Iter    35/   41] train: loss: 0.6929135
[Epoch 6] ogbg-molbace: 0.417370 val loss: 0.696533
[Epoch 6] ogbg-molbace: 0.373837 test loss: 0.694611
[Epoch 7; Iter    24/   41] train: loss: 0.6916677
[Epoch 7] ogbg-molbace: 0.423172 val loss: 0.696485
[Epoch 7] ogbg-molbace: 0.375944 test loss: 0.694254
[Epoch 8; Iter    13/   41] train: loss: 0.6891870
[Epoch 8] ogbg-molbace: 0.420007 val loss: 0.696355
[Epoch 8] ogbg-molbace: 0.383319 test loss: 0.694150
[Epoch 9; Iter     2/   41] train: loss: 0.6927548
[Epoch 9; Iter    32/   41] train: loss: 0.6920067
[Epoch 9] ogbg-molbace: 0.436181 val loss: 0.696468
[Epoch 9] ogbg-molbace: 0.402985 test loss: 0.693238
[Epoch 10; Iter    21/   41] train: loss: 0.6928846
[Epoch 10] ogbg-molbace: 0.419304 val loss: 0.696985
[Epoch 10] ogbg-molbace: 0.384899 test loss: 0.693494
[Epoch 11; Iter    10/   41] train: loss: 0.6915683
[Epoch 11; Iter    40/   41] train: loss: 0.6932377
[Epoch 11] ogbg-molbace: 0.435302 val loss: 0.697081
[Epoch 11] ogbg-molbace: 0.394908 test loss: 0.692732
[Epoch 12; Iter    29/   41] train: loss: 0.6911545
[Epoch 12] ogbg-molbace: 0.428797 val loss: 0.697493
[Epoch 12] ogbg-molbace: 0.395083 test loss: 0.692545
[Epoch 13; Iter    18/   41] train: loss: 0.6908304
[Epoch 13] ogbg-molbace: 0.430731 val loss: 0.697530
[Epoch 13] ogbg-molbace: 0.396313 test loss: 0.692573
[Epoch 14; Iter     7/   41] train: loss: 0.6908981
[Epoch 14; Iter    37/   41] train: loss: 0.6924698
[Epoch 14] ogbg-molbace: 0.441807 val loss: 0.697544
[Epoch 14] ogbg-molbace: 0.414047 test loss: 0.691816
[Epoch 15; Iter    26/   41] train: loss: 0.6928121
[Epoch 15] ogbg-molbace: 0.439873 val loss: 0.698000
[Epoch 15] ogbg-molbace: 0.412643 test loss: 0.691190
[Epoch 16; Iter    15/   41] train: loss: 0.6910982
[Epoch 16] ogbg-molbace: 0.449191 val loss: 0.697737
[Epoch 16] ogbg-molbace: 0.428622 test loss: 0.690686
[Epoch 17; Iter     4/   41] train: loss: 0.6957929
[Epoch 17; Iter    34/   41] train: loss: 0.6860292
[Epoch 17] ogbg-molbace: 0.576301 val loss: 0.687799
[Epoch 17] ogbg-molbace: 0.625988 test loss: 0.686516
[Epoch 18; Iter    23/   41] train: loss: 0.6615779
[Epoch 18] ogbg-molbace: 0.682841 val loss: 0.657901
[Epoch 18] ogbg-molbace: 0.738718 test loss: 0.662013
[Epoch 19; Iter    12/   41] train: loss: 0.6424654
[Epoch 19] ogbg-molbace: 0.750176 val loss: 0.606149
[Epoch 19] ogbg-molbace: 0.783319 test loss: 0.604456
[Epoch 20; Iter     1/   41] train: loss: 0.5924438
[Epoch 20; Iter    31/   41] train: loss: 0.6265283
[Epoch 20] ogbg-molbace: 0.786217 val loss: 0.563027
[Epoch 20] ogbg-molbace: 0.807902 test loss: 0.592527
[Epoch 21; Iter    20/   41] train: loss: 0.5265501
[Epoch 21] ogbg-molbace: 0.760197 val loss: 0.562168
[Epoch 21] ogbg-molbace: 0.770149 test loss: 0.529891
[Epoch 22; Iter     9/   41] train: loss: 0.6131675
[Epoch 22; Iter    39/   41] train: loss: 0.5239373
[Epoch 22] ogbg-molbace: 0.787271 val loss: 0.551522
[Epoch 22] ogbg-molbace: 0.766286 test loss: 0.509911
[Epoch 23; Iter    28/   41] train: loss: 0.3850918
[Epoch 23] ogbg-molbace: 0.822082 val loss: 0.494411
[Epoch 23] ogbg-molbace: 0.829148 test loss: 0.476181
[Epoch 24; Iter    17/   41] train: loss: 0.5901408
[Epoch 24] ogbg-molbace: 0.810654 val loss: 0.485167
[Epoch 24] ogbg-molbace: 0.835996 test loss: 0.454048
[Epoch 25; Iter     6/   41] train: loss: 0.5320941
[Epoch 25; Iter    36/   41] train: loss: 0.5232563
[Epoch 25] ogbg-molbace: 0.831224 val loss: 0.484658
[Epoch 25] ogbg-molbace: 0.824407 test loss: 0.469180
[Epoch 26; Iter    25/   41] train: loss: 0.5994038
[Epoch 26] ogbg-molbace: 0.848805 val loss: 0.441304
[Epoch 26] ogbg-molbace: 0.837577 test loss: 0.486845
[Epoch 27; Iter    14/   41] train: loss: 0.4032886
[Epoch 27] ogbg-molbace: 0.854255 val loss: 0.435653
[Epoch 27] ogbg-molbace: 0.843547 test loss: 0.458390
[Epoch 28; Iter     3/   41] train: loss: 0.4083200
[Epoch 28; Iter    33/   41] train: loss: 0.4060771
[Epoch 28] ogbg-molbace: 0.755098 val loss: 0.575293
[Epoch 28] ogbg-molbace: 0.784372 test loss: 0.577558
[Epoch 29; Iter    22/   41] train: loss: 0.4512711
[Epoch 29] ogbg-molbace: 0.819796 val loss: 0.470445
[Epoch 29] ogbg-molbace: 0.847234 test loss: 0.519943
[Epoch 30; Iter    11/   41] train: loss: 0.4568735
[Epoch 30; Iter    41/   41] train: loss: 0.4245512
[Epoch 30] ogbg-molbace: 0.834916 val loss: 0.454950
[Epoch 30] ogbg-molbace: 0.840562 test loss: 0.472049
[Epoch 31; Iter    30/   41] train: loss: 0.4582805
[Epoch 31] ogbg-molbace: 0.809599 val loss: 0.494984
[Epoch 31] ogbg-molbace: 0.815803 test loss: 0.469136
[Epoch 32; Iter    19/   41] train: loss: 0.4467485
[Epoch 32] ogbg-molbace: 0.837377 val loss: 0.433868
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.7/PNA_ogbg-molbace_3DInfomax_bace_random=0.7_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.7
logdir: runs/split/3DInfomax/bace/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6928620
[Epoch 1] ogbg-molbace: 0.537169 val loss: 0.693016
[Epoch 1] ogbg-molbace: 0.464236 test loss: 0.693279
[Epoch 2; Iter    24/   36] train: loss: 0.6957008
[Epoch 2] ogbg-molbace: 0.545455 val loss: 0.692653
[Epoch 2] ogbg-molbace: 0.459104 test loss: 0.693813
[Epoch 3; Iter    18/   36] train: loss: 0.6921154
[Epoch 3] ogbg-molbace: 0.551847 val loss: 0.692635
[Epoch 3] ogbg-molbace: 0.460504 test loss: 0.694153
[Epoch 4; Iter    12/   36] train: loss: 0.6934323
[Epoch 4] ogbg-molbace: 0.554056 val loss: 0.692447
[Epoch 4] ogbg-molbace: 0.465246 test loss: 0.694112
[Epoch 5; Iter     6/   36] train: loss: 0.6919726
[Epoch 5; Iter    36/   36] train: loss: 0.6915364
[Epoch 5] ogbg-molbace: 0.562421 val loss: 0.692366
[Epoch 5] ogbg-molbace: 0.467579 test loss: 0.694027
[Epoch 6; Iter    30/   36] train: loss: 0.6945598
[Epoch 6] ogbg-molbace: 0.558712 val loss: 0.692386
[Epoch 6] ogbg-molbace: 0.471311 test loss: 0.693883
[Epoch 7; Iter    24/   36] train: loss: 0.6940879
[Epoch 7] ogbg-molbace: 0.555792 val loss: 0.692310
[Epoch 7] ogbg-molbace: 0.471311 test loss: 0.693903
[Epoch 8; Iter    18/   36] train: loss: 0.6875798
[Epoch 8] ogbg-molbace: 0.550110 val loss: 0.692296
[Epoch 8] ogbg-molbace: 0.473799 test loss: 0.693840
[Epoch 9; Iter    12/   36] train: loss: 0.6892272
[Epoch 9] ogbg-molbace: 0.563052 val loss: 0.692033
[Epoch 9] ogbg-molbace: 0.479863 test loss: 0.693652
[Epoch 10; Iter     6/   36] train: loss: 0.6896291
[Epoch 10; Iter    36/   36] train: loss: 0.6942704
[Epoch 10] ogbg-molbace: 0.565025 val loss: 0.692003
[Epoch 10] ogbg-molbace: 0.479474 test loss: 0.693664
[Epoch 11; Iter    30/   36] train: loss: 0.6926369
[Epoch 11] ogbg-molbace: 0.557686 val loss: 0.691950
[Epoch 11] ogbg-molbace: 0.471466 test loss: 0.693682
[Epoch 12; Iter    24/   36] train: loss: 0.6911784
[Epoch 12] ogbg-molbace: 0.566998 val loss: 0.691722
[Epoch 12] ogbg-molbace: 0.481029 test loss: 0.693456
[Epoch 13; Iter    18/   36] train: loss: 0.6893206
[Epoch 13] ogbg-molbace: 0.572285 val loss: 0.691435
[Epoch 13] ogbg-molbace: 0.482040 test loss: 0.693333
[Epoch 14; Iter    12/   36] train: loss: 0.6913424
[Epoch 14] ogbg-molbace: 0.574653 val loss: 0.691319
[Epoch 14] ogbg-molbace: 0.490515 test loss: 0.693200
[Epoch 15; Iter     6/   36] train: loss: 0.6954684
[Epoch 15; Iter    36/   36] train: loss: 0.6977866
[Epoch 15] ogbg-molbace: 0.576626 val loss: 0.691170
[Epoch 15] ogbg-molbace: 0.490281 test loss: 0.693104
[Epoch 16; Iter    30/   36] train: loss: 0.6888028
[Epoch 16] ogbg-molbace: 0.577573 val loss: 0.691119
[Epoch 16] ogbg-molbace: 0.494946 test loss: 0.693000
[Epoch 17; Iter    24/   36] train: loss: 0.6935626
[Epoch 17] ogbg-molbace: 0.583254 val loss: 0.690955
[Epoch 17] ogbg-molbace: 0.490748 test loss: 0.693009
[Epoch 18; Iter    18/   36] train: loss: 0.6892875
[Epoch 18] ogbg-molbace: 0.591146 val loss: 0.690533
[Epoch 18] ogbg-molbace: 0.505520 test loss: 0.692615
[Epoch 19; Iter    12/   36] train: loss: 0.6927192
[Epoch 19] ogbg-molbace: 0.586253 val loss: 0.690517
[Epoch 19] ogbg-molbace: 0.498523 test loss: 0.692699
[Epoch 20; Iter     6/   36] train: loss: 0.6885192
[Epoch 20; Iter    36/   36] train: loss: 0.6633742
[Epoch 20] ogbg-molbace: 0.704703 val loss: 0.671679
[Epoch 20] ogbg-molbace: 0.674312 test loss: 0.674889
[Epoch 21; Iter    30/   36] train: loss: 0.6624771
[Epoch 21] ogbg-molbace: 0.770518 val loss: 0.644442
[Epoch 21] ogbg-molbace: 0.772275 test loss: 0.637222
[Epoch 22; Iter    24/   36] train: loss: 0.6287004
[Epoch 22] ogbg-molbace: 0.775331 val loss: 0.614530
[Epoch 22] ogbg-molbace: 0.775074 test loss: 0.604393
[Epoch 23; Iter    18/   36] train: loss: 0.6275016
[Epoch 23] ogbg-molbace: 0.800426 val loss: 0.567703
[Epoch 23] ogbg-molbace: 0.802908 test loss: 0.563155
[Epoch 24; Iter    12/   36] train: loss: 0.6738326
[Epoch 24] ogbg-molbace: 0.817393 val loss: 0.575237
[Epoch 24] ogbg-molbace: 0.813171 test loss: 0.552901
[Epoch 25; Iter     6/   36] train: loss: 0.5727535
[Epoch 25; Iter    36/   36] train: loss: 0.4419145
[Epoch 25] ogbg-molbace: 0.809107 val loss: 0.540375
[Epoch 25] ogbg-molbace: 0.791945 test loss: 0.547854
[Epoch 26; Iter    30/   36] train: loss: 0.3999378
[Epoch 26] ogbg-molbace: 0.791903 val loss: 0.545045
[Epoch 26] ogbg-molbace: 0.797543 test loss: 0.556230
[Epoch 27; Iter    24/   36] train: loss: 0.5172750
[Epoch 27] ogbg-molbace: 0.738005 val loss: 0.612294
[Epoch 27] ogbg-molbace: 0.734567 test loss: 0.611495
[Epoch 28; Iter    18/   36] train: loss: 0.6057587
[Epoch 28] ogbg-molbace: 0.816919 val loss: 0.535391
[Epoch 28] ogbg-molbace: 0.821412 test loss: 0.521031
[Epoch 29; Iter    12/   36] train: loss: 0.4878520
[Epoch 29] ogbg-molbace: 0.829467 val loss: 0.521580
[Epoch 29] ogbg-molbace: 0.833541 test loss: 0.512027
[Epoch 30; Iter     6/   36] train: loss: 0.5196277
[Epoch 30; Iter    36/   36] train: loss: 0.5473460
[Epoch 30] ogbg-molbace: 0.826705 val loss: 0.519201
[Epoch 30] ogbg-molbace: 0.819390 test loss: 0.538575
[Epoch 31; Iter    30/   36] train: loss: 0.4790402
[Epoch 31] ogbg-molbace: 0.807686 val loss: 0.537876
[Epoch 31] ogbg-molbace: 0.801431 test loss: 0.554045
[Epoch 32; Iter    24/   36] train: loss: 0.6642836
[Epoch 32] ogbg-molbace: 0.804845 val loss: 0.546608
[Epoch 32] ogbg-molbace: 0.795133 test loss: 0.540435
[Epoch 33; Iter    18/   36] train: loss: 0.5357860
[Epoch 33] ogbg-molbace: 0.834359 val loss: 0.509339
[Epoch 33] ogbg-molbace: 0.820557 test loss: 0.522309
[Epoch 34; Iter    12/   36] train: loss: 0.4594014
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.6/PNA_ogbg-molbace_3DInfomax_bace_random=0.6_4_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.6
logdir: runs/split/3DInfomax/bace/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6946676
[Epoch 1] ogbg-molbace: 0.456237 val loss: 0.693346
[Epoch 1] ogbg-molbace: 0.379810 test loss: 0.693372
[Epoch 2; Iter    29/   31] train: loss: 0.6933601
[Epoch 2] ogbg-molbace: 0.450229 val loss: 0.693870
[Epoch 2] ogbg-molbace: 0.382820 test loss: 0.694024
[Epoch 3; Iter    28/   31] train: loss: 0.6926856
[Epoch 3] ogbg-molbace: 0.452426 val loss: 0.694540
[Epoch 3] ogbg-molbace: 0.390455 test loss: 0.694662
[Epoch 4; Iter    27/   31] train: loss: 0.6909276
[Epoch 4] ogbg-molbace: 0.452291 val loss: 0.694661
[Epoch 4] ogbg-molbace: 0.390716 test loss: 0.694865
[Epoch 5; Iter    26/   31] train: loss: 0.6955655
[Epoch 5] ogbg-molbace: 0.450184 val loss: 0.694586
[Epoch 5] ogbg-molbace: 0.390673 test loss: 0.694742
[Epoch 6; Iter    25/   31] train: loss: 0.6948657
[Epoch 6] ogbg-molbace: 0.428706 val loss: 0.695010
[Epoch 6] ogbg-molbace: 0.382340 test loss: 0.694985
[Epoch 7; Iter    24/   31] train: loss: 0.6918097
[Epoch 7] ogbg-molbace: 0.454713 val loss: 0.694436
[Epoch 7] ogbg-molbace: 0.395515 test loss: 0.694578
[Epoch 8; Iter    23/   31] train: loss: 0.6896997
[Epoch 8] ogbg-molbace: 0.458883 val loss: 0.694530
[Epoch 8] ogbg-molbace: 0.392505 test loss: 0.694691
[Epoch 9; Iter    22/   31] train: loss: 0.6923033
[Epoch 9] ogbg-molbace: 0.469151 val loss: 0.694222
[Epoch 9] ogbg-molbace: 0.398525 test loss: 0.694495
[Epoch 10; Iter    21/   31] train: loss: 0.6923867
[Epoch 10] ogbg-molbace: 0.469151 val loss: 0.694197
[Epoch 10] ogbg-molbace: 0.399267 test loss: 0.694381
[Epoch 11; Iter    20/   31] train: loss: 0.6927996
[Epoch 11] ogbg-molbace: 0.472379 val loss: 0.694217
[Epoch 11] ogbg-molbace: 0.399529 test loss: 0.694426
[Epoch 12; Iter    19/   31] train: loss: 0.6938165
[Epoch 12] ogbg-molbace: 0.475921 val loss: 0.693966
[Epoch 12] ogbg-molbace: 0.401536 test loss: 0.694170
[Epoch 13; Iter    18/   31] train: loss: 0.6924518
[Epoch 13] ogbg-molbace: 0.489463 val loss: 0.693786
[Epoch 13] ogbg-molbace: 0.411003 test loss: 0.693964
[Epoch 14; Iter    17/   31] train: loss: 0.6897945
[Epoch 14] ogbg-molbace: 0.483141 val loss: 0.693837
[Epoch 14] ogbg-molbace: 0.404502 test loss: 0.694092
[Epoch 15; Iter    16/   31] train: loss: 0.6938449
[Epoch 15] ogbg-molbace: 0.477805 val loss: 0.693999
[Epoch 15] ogbg-molbace: 0.397260 test loss: 0.694303
[Epoch 16; Iter    15/   31] train: loss: 0.6905838
[Epoch 16] ogbg-molbace: 0.492333 val loss: 0.693516
[Epoch 16] ogbg-molbace: 0.406291 test loss: 0.693893
[Epoch 17; Iter    14/   31] train: loss: 0.6912600
[Epoch 17] ogbg-molbace: 0.500224 val loss: 0.693231
[Epoch 17] ogbg-molbace: 0.413882 test loss: 0.693523
[Epoch 18; Iter    13/   31] train: loss: 0.6891760
[Epoch 18] ogbg-molbace: 0.500135 val loss: 0.693302
[Epoch 18] ogbg-molbace: 0.415191 test loss: 0.693607
[Epoch 19; Iter    12/   31] train: loss: 0.6901060
[Epoch 19] ogbg-molbace: 0.508385 val loss: 0.692933
[Epoch 19] ogbg-molbace: 0.419379 test loss: 0.693202
[Epoch 20; Iter    11/   31] train: loss: 0.6942205
[Epoch 20] ogbg-molbace: 0.513003 val loss: 0.692704
[Epoch 20] ogbg-molbace: 0.425661 test loss: 0.693149
[Epoch 21; Iter    10/   31] train: loss: 0.6950413
[Epoch 21] ogbg-molbace: 0.532105 val loss: 0.692275
[Epoch 21] ogbg-molbace: 0.438967 test loss: 0.692674
[Epoch 22; Iter     9/   31] train: loss: 0.6921791
[Epoch 22] ogbg-molbace: 0.546319 val loss: 0.691995
[Epoch 22] ogbg-molbace: 0.447998 test loss: 0.692395
[Epoch 23; Iter     8/   31] train: loss: 0.6895558
[Epoch 23] ogbg-molbace: 0.744597 val loss: 0.671822
[Epoch 23] ogbg-molbace: 0.655615 test loss: 0.677097
[Epoch 24; Iter     7/   31] train: loss: 0.6587531
[Epoch 24] ogbg-molbace: 0.795086 val loss: 0.645803
[Epoch 24] ogbg-molbace: 0.729168 test loss: 0.655597
[Epoch 25; Iter     6/   31] train: loss: 0.6184751
[Epoch 25] ogbg-molbace: 0.801677 val loss: 0.601867
[Epoch 25] ogbg-molbace: 0.789024 test loss: 0.611812
[Epoch 26; Iter     5/   31] train: loss: 0.6717529
[Epoch 26] ogbg-molbace: 0.830822 val loss: 0.548412
[Epoch 26] ogbg-molbace: 0.789285 test loss: 0.563769
[Epoch 27; Iter     4/   31] train: loss: 0.6170605
[Epoch 27] ogbg-molbace: 0.813021 val loss: 0.544145
[Epoch 27] ogbg-molbace: 0.746706 test loss: 0.585107
[Epoch 28; Iter     3/   31] train: loss: 0.6418103
[Epoch 28] ogbg-molbace: 0.820106 val loss: 0.538958
[Epoch 28] ogbg-molbace: 0.778422 test loss: 0.559642
[Epoch 29; Iter     2/   31] train: loss: 0.6735833
[Epoch 29] ogbg-molbace: 0.850641 val loss: 0.500475
[Epoch 29] ogbg-molbace: 0.817337 test loss: 0.519265
[Epoch 30; Iter     1/   31] train: loss: 0.5137854
[Epoch 30; Iter    31/   31] train: loss: 0.5193424
[Epoch 30] ogbg-molbace: 0.792485 val loss: 0.563316
[Epoch 30] ogbg-molbace: 0.736236 test loss: 0.587354
[Epoch 31; Iter    30/   31] train: loss: 0.5943053
[Epoch 31] ogbg-molbace: 0.837862 val loss: 0.495068
[Epoch 31] ogbg-molbace: 0.812931 test loss: 0.511957
[Epoch 32; Iter    29/   31] train: loss: 0.6724675
[Epoch 32] ogbg-molbace: 0.873240 val loss: 0.453078
[Epoch 32] ogbg-molbace: 0.824928 test loss: 0.490460
[Epoch 33; Iter    28/   31] train: loss: 0.5126346
[Epoch 33] ogbg-molbace: 0.842705 val loss: 0.491175
[Epoch 33] ogbg-molbace: 0.800846 test loss: 0.515190
[Epoch 34; Iter    27/   31] train: loss: 0.5967410
[Epoch 34] ogbg-molbace: 0.855439 val loss: 0.499705
[Epoch 34] ogbg-molbace: 0.810968 test loss: 0.518944
[Epoch 35; Iter    26/   31] train: loss: 0.4553564
[Epoch 35] ogbg-molbace: 0.855708 val loss: 0.482569
[Epoch 35] ogbg-molbace: 0.799014 test loss: 0.514795
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.7/PNA_ogbg-molbace_3DInfomax_bace_random=0.7_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.7
logdir: runs/split/3DInfomax/bace/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6953378
[Epoch 1] ogbg-molbace: 0.377762 val loss: 0.693535
[Epoch 1] ogbg-molbace: 0.390375 test loss: 0.693622
[Epoch 2; Iter    24/   36] train: loss: 0.6976807
[Epoch 2] ogbg-molbace: 0.388494 val loss: 0.694343
[Epoch 2] ogbg-molbace: 0.379179 test loss: 0.695204
[Epoch 3; Iter    18/   36] train: loss: 0.6950878
[Epoch 3] ogbg-molbace: 0.421638 val loss: 0.694182
[Epoch 3] ogbg-molbace: 0.412766 test loss: 0.695151
[Epoch 4; Iter    12/   36] train: loss: 0.6902587
[Epoch 4] ogbg-molbace: 0.417850 val loss: 0.694076
[Epoch 4] ogbg-molbace: 0.415721 test loss: 0.695017
[Epoch 5; Iter     6/   36] train: loss: 0.6937093
[Epoch 5; Iter    36/   36] train: loss: 0.6963519
[Epoch 5] ogbg-molbace: 0.426610 val loss: 0.693959
[Epoch 5] ogbg-molbace: 0.425206 test loss: 0.694779
[Epoch 6; Iter    30/   36] train: loss: 0.6924372
[Epoch 6] ogbg-molbace: 0.419586 val loss: 0.693894
[Epoch 6] ogbg-molbace: 0.419919 test loss: 0.694897
[Epoch 7; Iter    24/   36] train: loss: 0.6939777
[Epoch 7] ogbg-molbace: 0.414931 val loss: 0.693922
[Epoch 7] ogbg-molbace: 0.412067 test loss: 0.695001
[Epoch 8; Iter    18/   36] train: loss: 0.6971642
[Epoch 8] ogbg-molbace: 0.419586 val loss: 0.693758
[Epoch 8] ogbg-molbace: 0.421319 test loss: 0.694811
[Epoch 9; Iter    12/   36] train: loss: 0.6902761
[Epoch 9] ogbg-molbace: 0.427715 val loss: 0.693683
[Epoch 9] ogbg-molbace: 0.424817 test loss: 0.694797
[Epoch 10; Iter     6/   36] train: loss: 0.6974826
[Epoch 10; Iter    36/   36] train: loss: 0.6875153
[Epoch 10] ogbg-molbace: 0.433554 val loss: 0.693444
[Epoch 10] ogbg-molbace: 0.426606 test loss: 0.694532
[Epoch 11; Iter    30/   36] train: loss: 0.6954526
[Epoch 11] ogbg-molbace: 0.439710 val loss: 0.693393
[Epoch 11] ogbg-molbace: 0.423029 test loss: 0.694725
[Epoch 12; Iter    24/   36] train: loss: 0.6960137
[Epoch 12] ogbg-molbace: 0.429845 val loss: 0.693275
[Epoch 12] ogbg-molbace: 0.425284 test loss: 0.694535
[Epoch 13; Iter    18/   36] train: loss: 0.6983246
[Epoch 13] ogbg-molbace: 0.432055 val loss: 0.693149
[Epoch 13] ogbg-molbace: 0.424118 test loss: 0.694572
[Epoch 14; Iter    12/   36] train: loss: 0.6913257
[Epoch 14] ogbg-molbace: 0.450363 val loss: 0.692821
[Epoch 14] ogbg-molbace: 0.433836 test loss: 0.694183
[Epoch 15; Iter     6/   36] train: loss: 0.6921072
[Epoch 15; Iter    36/   36] train: loss: 0.7002514
[Epoch 15] ogbg-molbace: 0.444050 val loss: 0.692790
[Epoch 15] ogbg-molbace: 0.433836 test loss: 0.694195
[Epoch 16; Iter    30/   36] train: loss: 0.6909269
[Epoch 16] ogbg-molbace: 0.447364 val loss: 0.692644
[Epoch 16] ogbg-molbace: 0.443399 test loss: 0.694022
[Epoch 17; Iter    24/   36] train: loss: 0.6937744
[Epoch 17] ogbg-molbace: 0.458176 val loss: 0.692375
[Epoch 17] ogbg-molbace: 0.443477 test loss: 0.693969
[Epoch 18; Iter    18/   36] train: loss: 0.6889220
[Epoch 18] ogbg-molbace: 0.466935 val loss: 0.692205
[Epoch 18] ogbg-molbace: 0.453040 test loss: 0.693786
[Epoch 19; Iter    12/   36] train: loss: 0.6911281
[Epoch 19] ogbg-molbace: 0.475142 val loss: 0.691904
[Epoch 19] ogbg-molbace: 0.459571 test loss: 0.693548
[Epoch 20; Iter     6/   36] train: loss: 0.6924753
[Epoch 20; Iter    36/   36] train: loss: 0.6624063
[Epoch 20] ogbg-molbace: 0.705019 val loss: 0.672953
[Epoch 20] ogbg-molbace: 0.671124 test loss: 0.675953
[Epoch 21; Iter    30/   36] train: loss: 0.6726836
[Epoch 21] ogbg-molbace: 0.761285 val loss: 0.637175
[Epoch 21] ogbg-molbace: 0.746618 test loss: 0.635952
[Epoch 22; Iter    24/   36] train: loss: 0.6385568
[Epoch 22] ogbg-molbace: 0.783381 val loss: 0.591722
[Epoch 22] ogbg-molbace: 0.771497 test loss: 0.587080
[Epoch 23; Iter    18/   36] train: loss: 0.6153367
[Epoch 23] ogbg-molbace: 0.788747 val loss: 0.594481
[Epoch 23] ogbg-molbace: 0.775385 test loss: 0.587685
[Epoch 24; Iter    12/   36] train: loss: 0.5570808
[Epoch 24] ogbg-molbace: 0.798690 val loss: 0.557774
[Epoch 24] ogbg-molbace: 0.808583 test loss: 0.544434
[Epoch 25; Iter     6/   36] train: loss: 0.6821444
[Epoch 25; Iter    36/   36] train: loss: 0.6409997
[Epoch 25] ogbg-molbace: 0.826310 val loss: 0.553405
[Epoch 25] ogbg-molbace: 0.829342 test loss: 0.540136
[Epoch 26; Iter    30/   36] train: loss: 0.6144685
[Epoch 26] ogbg-molbace: 0.831755 val loss: 0.535465
[Epoch 26] ogbg-molbace: 0.815970 test loss: 0.535012
[Epoch 27; Iter    24/   36] train: loss: 0.4909600
[Epoch 27] ogbg-molbace: 0.814631 val loss: 0.534120
[Epoch 27] ogbg-molbace: 0.806717 test loss: 0.533396
[Epoch 28; Iter    18/   36] train: loss: 0.4688919
[Epoch 28] ogbg-molbace: 0.807213 val loss: 0.551616
[Epoch 28] ogbg-molbace: 0.784015 test loss: 0.553705
[Epoch 29; Iter    12/   36] train: loss: 0.4397211
[Epoch 29] ogbg-molbace: 0.803504 val loss: 0.551356
[Epoch 29] ogbg-molbace: 0.790935 test loss: 0.558517
[Epoch 30; Iter     6/   36] train: loss: 0.5204069
[Epoch 30; Iter    36/   36] train: loss: 0.5499576
[Epoch 30] ogbg-molbace: 0.852667 val loss: 0.507329
[Epoch 30] ogbg-molbace: 0.843492 test loss: 0.501602
[Epoch 31; Iter    30/   36] train: loss: 0.5935814
[Epoch 31] ogbg-molbace: 0.837437 val loss: 0.517853
[Epoch 31] ogbg-molbace: 0.826543 test loss: 0.517169
[Epoch 32; Iter    24/   36] train: loss: 0.4657338
[Epoch 32] ogbg-molbace: 0.831676 val loss: 0.533309
[Epoch 32] ogbg-molbace: 0.832608 test loss: 0.527411
[Epoch 33; Iter    18/   36] train: loss: 0.4959424
[Epoch 33] ogbg-molbace: 0.835227 val loss: 0.519637
[Epoch 33] ogbg-molbace: 0.826777 test loss: 0.518902
[Epoch 34; Iter    12/   36] train: loss: 0.5465745
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.7/PNA_ogbg-molbace_3DInfomax_bace_random=0.7_4_26-05_09-18-11
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.7
logdir: runs/split/3DInfomax/bace/random/train_prop=0.7
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   36] train: loss: 0.6945141
[Epoch 1] ogbg-molbace: 0.410827 val loss: 0.693433
[Epoch 1] ogbg-molbace: 0.396517 test loss: 0.693449
[Epoch 2; Iter    24/   36] train: loss: 0.6936370
[Epoch 2] ogbg-molbace: 0.427004 val loss: 0.694259
[Epoch 2] ogbg-molbace: 0.388276 test loss: 0.694519
[Epoch 3; Iter    18/   36] train: loss: 0.6947005
[Epoch 3] ogbg-molbace: 0.438289 val loss: 0.694599
[Epoch 3] ogbg-molbace: 0.387887 test loss: 0.695192
[Epoch 4; Iter    12/   36] train: loss: 0.6948225
[Epoch 4] ogbg-molbace: 0.424321 val loss: 0.694946
[Epoch 4] ogbg-molbace: 0.384233 test loss: 0.695440
[Epoch 5; Iter     6/   36] train: loss: 0.6919589
[Epoch 5; Iter    36/   36] train: loss: 0.6906372
[Epoch 5] ogbg-molbace: 0.440893 val loss: 0.694643
[Epoch 5] ogbg-molbace: 0.390530 test loss: 0.695248
[Epoch 6; Iter    30/   36] train: loss: 0.6938589
[Epoch 6] ogbg-molbace: 0.455177 val loss: 0.694268
[Epoch 6] ogbg-molbace: 0.405924 test loss: 0.694817
[Epoch 7; Iter    24/   36] train: loss: 0.6950762
[Epoch 7] ogbg-molbace: 0.445549 val loss: 0.694594
[Epoch 7] ogbg-molbace: 0.395195 test loss: 0.695177
[Epoch 8; Iter    18/   36] train: loss: 0.6914889
[Epoch 8] ogbg-molbace: 0.448864 val loss: 0.694363
[Epoch 8] ogbg-molbace: 0.397294 test loss: 0.694979
[Epoch 9; Iter    12/   36] train: loss: 0.6916929
[Epoch 9] ogbg-molbace: 0.447364 val loss: 0.694390
[Epoch 9] ogbg-molbace: 0.398227 test loss: 0.695096
[Epoch 10; Iter     6/   36] train: loss: 0.6949318
[Epoch 10; Iter    36/   36] train: loss: 0.6903351
[Epoch 10] ogbg-molbace: 0.445865 val loss: 0.694413
[Epoch 10] ogbg-molbace: 0.398538 test loss: 0.695091
[Epoch 11; Iter    30/   36] train: loss: 0.6919467
[Epoch 11] ogbg-molbace: 0.464962 val loss: 0.693850
[Epoch 11] ogbg-molbace: 0.405769 test loss: 0.694690
[Epoch 12; Iter    24/   36] train: loss: 0.6912440
[Epoch 12] ogbg-molbace: 0.468119 val loss: 0.693764
[Epoch 12] ogbg-molbace: 0.408646 test loss: 0.694647
[Epoch 13; Iter    18/   36] train: loss: 0.6922330
[Epoch 13] ogbg-molbace: 0.470723 val loss: 0.693565
[Epoch 13] ogbg-molbace: 0.411056 test loss: 0.694517
[Epoch 14; Iter    12/   36] train: loss: 0.6948389
[Epoch 14] ogbg-molbace: 0.458649 val loss: 0.693852
[Epoch 14] ogbg-molbace: 0.409579 test loss: 0.694786
[Epoch 15; Iter     6/   36] train: loss: 0.6939122
[Epoch 15; Iter    36/   36] train: loss: 0.6921555
[Epoch 15] ogbg-molbace: 0.482008 val loss: 0.693205
[Epoch 15] ogbg-molbace: 0.420852 test loss: 0.694231
[Epoch 16; Iter    30/   36] train: loss: 0.6902735
[Epoch 16] ogbg-molbace: 0.480666 val loss: 0.693179
[Epoch 16] ogbg-molbace: 0.421941 test loss: 0.694260
[Epoch 17; Iter    24/   36] train: loss: 0.6932222
[Epoch 17] ogbg-molbace: 0.482481 val loss: 0.692997
[Epoch 17] ogbg-molbace: 0.421319 test loss: 0.694146
[Epoch 18; Iter    18/   36] train: loss: 0.6937600
[Epoch 18] ogbg-molbace: 0.489662 val loss: 0.692749
[Epoch 18] ogbg-molbace: 0.430571 test loss: 0.693978
[Epoch 19; Iter    12/   36] train: loss: 0.6923026
[Epoch 19] ogbg-molbace: 0.494003 val loss: 0.692566
[Epoch 19] ogbg-molbace: 0.436713 test loss: 0.693956
[Epoch 20; Iter     6/   36] train: loss: 0.6937105
[Epoch 20; Iter    36/   36] train: loss: 0.6890845
[Epoch 20] ogbg-molbace: 0.696654 val loss: 0.670089
[Epoch 20] ogbg-molbace: 0.683875 test loss: 0.670952
[Epoch 21; Iter    30/   36] train: loss: 0.6604888
[Epoch 21] ogbg-molbace: 0.770044 val loss: 0.636833
[Epoch 21] ogbg-molbace: 0.762556 test loss: 0.634731
[Epoch 22; Iter    24/   36] train: loss: 0.6166386
[Epoch 22] ogbg-molbace: 0.770518 val loss: 0.600892
[Epoch 22] ogbg-molbace: 0.749572 test loss: 0.603418
[Epoch 23; Iter    18/   36] train: loss: 0.6419883
[Epoch 23] ogbg-molbace: 0.789852 val loss: 0.570043
[Epoch 23] ogbg-molbace: 0.767377 test loss: 0.573661
[Epoch 24; Iter    12/   36] train: loss: 0.4587778
[Epoch 24] ogbg-molbace: 0.805713 val loss: 0.560139
[Epoch 24] ogbg-molbace: 0.808272 test loss: 0.551590
[Epoch 25; Iter     6/   36] train: loss: 0.5331967
[Epoch 25; Iter    36/   36] train: loss: 0.4349228
[Epoch 25] ogbg-molbace: 0.798059 val loss: 0.555039
[Epoch 25] ogbg-molbace: 0.781760 test loss: 0.555960
[Epoch 26; Iter    30/   36] train: loss: 0.5204819
[Epoch 26] ogbg-molbace: 0.819287 val loss: 0.544840
[Epoch 26] ogbg-molbace: 0.808817 test loss: 0.532429
[Epoch 27; Iter    24/   36] train: loss: 0.4904785
[Epoch 27] ogbg-molbace: 0.800032 val loss: 0.555159
[Epoch 27] ogbg-molbace: 0.786425 test loss: 0.548375
[Epoch 28; Iter    18/   36] train: loss: 0.5569829
[Epoch 28] ogbg-molbace: 0.793403 val loss: 0.569419
[Epoch 28] ogbg-molbace: 0.774063 test loss: 0.573577
[Epoch 29; Iter    12/   36] train: loss: 0.4214595
[Epoch 29] ogbg-molbace: 0.810133 val loss: 0.541515
[Epoch 29] ogbg-molbace: 0.792023 test loss: 0.542041
[Epoch 30; Iter     6/   36] train: loss: 0.3795057
[Epoch 30; Iter    36/   36] train: loss: 0.5273806
[Epoch 30] ogbg-molbace: 0.825994 val loss: 0.521512
[Epoch 30] ogbg-molbace: 0.838128 test loss: 0.519269
[Epoch 31; Iter    30/   36] train: loss: 0.4792834
[Epoch 31] ogbg-molbace: 0.821575 val loss: 0.525494
[Epoch 31] ogbg-molbace: 0.821490 test loss: 0.521686
[Epoch 32; Iter    24/   36] train: loss: 0.4669949
[Epoch 32] ogbg-molbace: 0.811237 val loss: 0.590220
[Epoch 32] ogbg-molbace: 0.802053 test loss: 0.588442
[Epoch 33; Iter    18/   36] train: loss: 0.5269538
[Epoch 33] ogbg-molbace: 0.839094 val loss: 0.505262
[Epoch 33] ogbg-molbace: 0.837039 test loss: 0.498295
[Epoch 34; Iter    12/   36] train: loss: 0.4680770
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.6/PNA_ogbg-molbace_3DInfomax_bace_random=0.6_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.6
logdir: runs/split/3DInfomax/bace/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6934183
[Epoch 1] ogbg-molbace: 0.363510 val loss: 0.693540
[Epoch 1] ogbg-molbace: 0.392287 test loss: 0.693552
[Epoch 2; Iter    29/   31] train: loss: 0.6951079
[Epoch 2] ogbg-molbace: 0.320599 val loss: 0.694778
[Epoch 2] ogbg-molbace: 0.374095 test loss: 0.694860
[Epoch 3; Iter    28/   31] train: loss: 0.6948296
[Epoch 3] ogbg-molbace: 0.345799 val loss: 0.695456
[Epoch 3] ogbg-molbace: 0.403673 test loss: 0.695682
[Epoch 4; Iter    27/   31] train: loss: 0.6960214
[Epoch 4] ogbg-molbace: 0.353556 val loss: 0.695560
[Epoch 4] ogbg-molbace: 0.417503 test loss: 0.695603
[Epoch 5; Iter    26/   31] train: loss: 0.6944478
[Epoch 5] ogbg-molbace: 0.359519 val loss: 0.695464
[Epoch 5] ogbg-molbace: 0.414449 test loss: 0.695672
[Epoch 6; Iter    25/   31] train: loss: 0.6973116
[Epoch 6] ogbg-molbace: 0.357098 val loss: 0.695322
[Epoch 6] ogbg-molbace: 0.414929 test loss: 0.695481
[Epoch 7; Iter    24/   31] train: loss: 0.6924589
[Epoch 7] ogbg-molbace: 0.370908 val loss: 0.695241
[Epoch 7] ogbg-molbace: 0.429631 test loss: 0.695502
[Epoch 8; Iter    23/   31] train: loss: 0.6906277
[Epoch 8] ogbg-molbace: 0.371760 val loss: 0.695166
[Epoch 8] ogbg-molbace: 0.428540 test loss: 0.695499
[Epoch 9; Iter    22/   31] train: loss: 0.6939718
[Epoch 9] ogbg-molbace: 0.368039 val loss: 0.695066
[Epoch 9] ogbg-molbace: 0.430416 test loss: 0.695146
[Epoch 10; Iter    21/   31] train: loss: 0.6959572
[Epoch 10] ogbg-molbace: 0.358443 val loss: 0.694922
[Epoch 10] ogbg-molbace: 0.421778 test loss: 0.695169
[Epoch 11; Iter    20/   31] train: loss: 0.6959040
[Epoch 11] ogbg-molbace: 0.372837 val loss: 0.694660
[Epoch 11] ogbg-molbace: 0.432423 test loss: 0.694926
[Epoch 12; Iter    19/   31] train: loss: 0.6891743
[Epoch 12] ogbg-molbace: 0.368532 val loss: 0.694731
[Epoch 12] ogbg-molbace: 0.428060 test loss: 0.695041
[Epoch 13; Iter    18/   31] train: loss: 0.6962815
[Epoch 13] ogbg-molbace: 0.377096 val loss: 0.694563
[Epoch 13] ogbg-molbace: 0.437178 test loss: 0.694843
[Epoch 14; Iter    17/   31] train: loss: 0.6941516
[Epoch 14] ogbg-molbace: 0.375303 val loss: 0.694473
[Epoch 14] ogbg-molbace: 0.429151 test loss: 0.694839
[Epoch 15; Iter    16/   31] train: loss: 0.6928040
[Epoch 15] ogbg-molbace: 0.385571 val loss: 0.694379
[Epoch 15] ogbg-molbace: 0.437353 test loss: 0.694813
[Epoch 16; Iter    15/   31] train: loss: 0.6965752
[Epoch 16] ogbg-molbace: 0.383329 val loss: 0.694288
[Epoch 16] ogbg-molbace: 0.433557 test loss: 0.694625
[Epoch 17; Iter    14/   31] train: loss: 0.6920233
[Epoch 17] ogbg-molbace: 0.394135 val loss: 0.693912
[Epoch 17] ogbg-molbace: 0.453713 test loss: 0.694225
[Epoch 18; Iter    13/   31] train: loss: 0.6920571
[Epoch 18] ogbg-molbace: 0.380908 val loss: 0.693853
[Epoch 18] ogbg-molbace: 0.435869 test loss: 0.694041
[Epoch 19; Iter    12/   31] train: loss: 0.6923666
[Epoch 19] ogbg-molbace: 0.392476 val loss: 0.693641
[Epoch 19] ogbg-molbace: 0.448739 test loss: 0.694000
[Epoch 20; Iter    11/   31] train: loss: 0.6938408
[Epoch 20] ogbg-molbace: 0.398081 val loss: 0.693636
[Epoch 20] ogbg-molbace: 0.445075 test loss: 0.694023
[Epoch 21; Iter    10/   31] train: loss: 0.6926888
[Epoch 21] ogbg-molbace: 0.412519 val loss: 0.693345
[Epoch 21] ogbg-molbace: 0.454280 test loss: 0.693684
[Epoch 22; Iter     9/   31] train: loss: 0.6920928
[Epoch 22] ogbg-molbace: 0.421890 val loss: 0.693112
[Epoch 22] ogbg-molbace: 0.463921 test loss: 0.693532
[Epoch 23; Iter     8/   31] train: loss: 0.6931569
[Epoch 23] ogbg-molbace: 0.729307 val loss: 0.672597
[Epoch 23] ogbg-molbace: 0.656269 test loss: 0.676875
[Epoch 24; Iter     7/   31] train: loss: 0.6918983
[Epoch 24] ogbg-molbace: 0.759708 val loss: 0.639296
[Epoch 24] ogbg-molbace: 0.732528 test loss: 0.647242
[Epoch 25; Iter     6/   31] train: loss: 0.6101965
[Epoch 25] ogbg-molbace: 0.795355 val loss: 0.595647
[Epoch 25] ogbg-molbace: 0.729474 test loss: 0.615181
[Epoch 26; Iter     5/   31] train: loss: 0.6492374
[Epoch 26] ogbg-molbace: 0.810331 val loss: 0.580917
[Epoch 26] ogbg-molbace: 0.751941 test loss: 0.601428
[Epoch 27; Iter     4/   31] train: loss: 0.6622365
[Epoch 27] ogbg-molbace: 0.832885 val loss: 0.557674
[Epoch 27] ogbg-molbace: 0.770483 test loss: 0.576166
[Epoch 28; Iter     3/   31] train: loss: 0.6260018
[Epoch 28] ogbg-molbace: 0.838086 val loss: 0.527342
[Epoch 28] ogbg-molbace: 0.798534 test loss: 0.538565
[Epoch 29; Iter     2/   31] train: loss: 0.5358752
[Epoch 29] ogbg-molbace: 0.818492 val loss: 0.520676
[Epoch 29] ogbg-molbace: 0.775718 test loss: 0.539487
[Epoch 30; Iter     1/   31] train: loss: 0.5547698
[Epoch 30; Iter    31/   31] train: loss: 0.7142943
[Epoch 30] ogbg-molbace: 0.824814 val loss: 0.532025
[Epoch 30] ogbg-molbace: 0.784356 test loss: 0.543934
[Epoch 31; Iter    30/   31] train: loss: 0.5789452
[Epoch 31] ogbg-molbace: 0.849117 val loss: 0.501185
[Epoch 31] ogbg-molbace: 0.795786 test loss: 0.523656
[Epoch 32; Iter    29/   31] train: loss: 0.6186112
[Epoch 32] ogbg-molbace: 0.840463 val loss: 0.508364
[Epoch 32] ogbg-molbace: 0.782392 test loss: 0.545241
[Epoch 33; Iter    28/   31] train: loss: 0.5763885
[Epoch 33] ogbg-molbace: 0.872523 val loss: 0.475302
[Epoch 33] ogbg-molbace: 0.828418 test loss: 0.501383
[Epoch 34; Iter    27/   31] train: loss: 0.6482146
[Epoch 34] ogbg-molbace: 0.838893 val loss: 0.515696
[Epoch 34] ogbg-molbace: 0.801064 test loss: 0.527006
[Epoch 35; Iter    26/   31] train: loss: 0.4336644
[Epoch 35] ogbg-molbace: 0.870908 val loss: 0.458396
[Epoch 35] ogbg-molbace: 0.824186 test loss: 0.487269
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.6/PNA_ogbg-molbace_3DInfomax_bace_random=0.6_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.6
logdir: runs/split/3DInfomax/bace/random/train_prop=0.6
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1; Iter    30/   31] train: loss: 0.6917930
[Epoch 1] ogbg-molbace: 0.522868 val loss: 0.693133
[Epoch 1] ogbg-molbace: 0.484076 test loss: 0.693226
[Epoch 2; Iter    29/   31] train: loss: 0.6900944
[Epoch 2] ogbg-molbace: 0.529235 val loss: 0.693124
[Epoch 2] ogbg-molbace: 0.479888 test loss: 0.693546
[Epoch 3; Iter    28/   31] train: loss: 0.6913572
[Epoch 3] ogbg-molbace: 0.545781 val loss: 0.693050
[Epoch 3] ogbg-molbace: 0.483422 test loss: 0.694091
[Epoch 4; Iter    27/   31] train: loss: 0.6901704
[Epoch 4] ogbg-molbace: 0.539996 val loss: 0.693015
[Epoch 4] ogbg-molbace: 0.482244 test loss: 0.694160
[Epoch 5; Iter    26/   31] train: loss: 0.6947266
[Epoch 5] ogbg-molbace: 0.533315 val loss: 0.693045
[Epoch 5] ogbg-molbace: 0.481153 test loss: 0.694163
[Epoch 6; Iter    25/   31] train: loss: 0.6894541
[Epoch 6] ogbg-molbace: 0.540445 val loss: 0.692923
[Epoch 6] ogbg-molbace: 0.484687 test loss: 0.693997
[Epoch 7; Iter    24/   31] train: loss: 0.6947336
[Epoch 7] ogbg-molbace: 0.545377 val loss: 0.692804
[Epoch 7] ogbg-molbace: 0.489661 test loss: 0.693938
[Epoch 8; Iter    23/   31] train: loss: 0.6946726
[Epoch 8] ogbg-molbace: 0.554031 val loss: 0.692623
[Epoch 8] ogbg-molbace: 0.493369 test loss: 0.693738
[Epoch 9; Iter    22/   31] train: loss: 0.6937594
[Epoch 9] ogbg-molbace: 0.553269 val loss: 0.692656
[Epoch 9] ogbg-molbace: 0.493936 test loss: 0.693822
[Epoch 10; Iter    21/   31] train: loss: 0.6905018
[Epoch 10] ogbg-molbace: 0.552462 val loss: 0.692572
[Epoch 10] ogbg-molbace: 0.494808 test loss: 0.693687
[Epoch 11; Iter    20/   31] train: loss: 0.6909320
[Epoch 11] ogbg-molbace: 0.554614 val loss: 0.692477
[Epoch 11] ogbg-molbace: 0.493282 test loss: 0.693709
[Epoch 12; Iter    19/   31] train: loss: 0.6905872
[Epoch 12] ogbg-molbace: 0.557259 val loss: 0.692535
[Epoch 12] ogbg-molbace: 0.493718 test loss: 0.693699
[Epoch 13; Iter    18/   31] train: loss: 0.6930938
[Epoch 13] ogbg-molbace: 0.557035 val loss: 0.692358
[Epoch 13] ogbg-molbace: 0.495681 test loss: 0.693622
[Epoch 14; Iter    17/   31] train: loss: 0.6920676
[Epoch 14] ogbg-molbace: 0.556766 val loss: 0.692248
[Epoch 14] ogbg-molbace: 0.496161 test loss: 0.693361
[Epoch 15; Iter    16/   31] train: loss: 0.6898359
[Epoch 15] ogbg-molbace: 0.562775 val loss: 0.691961
[Epoch 15] ogbg-molbace: 0.501003 test loss: 0.693140
[Epoch 16; Iter    15/   31] train: loss: 0.6905748
[Epoch 16] ogbg-molbace: 0.569276 val loss: 0.691795
[Epoch 16] ogbg-molbace: 0.507067 test loss: 0.692943
[Epoch 17; Iter    14/   31] train: loss: 0.6914065
[Epoch 17] ogbg-molbace: 0.578020 val loss: 0.691502
[Epoch 17] ogbg-molbace: 0.510252 test loss: 0.692731
[Epoch 18; Iter    13/   31] train: loss: 0.6908424
[Epoch 18] ogbg-molbace: 0.574702 val loss: 0.691629
[Epoch 18] ogbg-molbace: 0.503752 test loss: 0.692860
[Epoch 19; Iter    12/   31] train: loss: 0.6925952
[Epoch 19] ogbg-molbace: 0.579813 val loss: 0.691390
[Epoch 19] ogbg-molbace: 0.508333 test loss: 0.692750
[Epoch 20; Iter    11/   31] train: loss: 0.6932471
[Epoch 20] ogbg-molbace: 0.581383 val loss: 0.691214
[Epoch 20] ogbg-molbace: 0.515575 test loss: 0.692527
[Epoch 21; Iter    10/   31] train: loss: 0.6911255
[Epoch 21] ogbg-molbace: 0.589768 val loss: 0.691080
[Epoch 21] ogbg-molbace: 0.520024 test loss: 0.692402
[Epoch 22; Iter     9/   31] train: loss: 0.6857343
[Epoch 22] ogbg-molbace: 0.599229 val loss: 0.690731
[Epoch 22] ogbg-molbace: 0.528270 test loss: 0.692048
[Epoch 23; Iter     8/   31] train: loss: 0.6810462
[Epoch 23] ogbg-molbace: 0.728993 val loss: 0.673131
[Epoch 23] ogbg-molbace: 0.636288 test loss: 0.678931
[Epoch 24; Iter     7/   31] train: loss: 0.6663010
[Epoch 24] ogbg-molbace: 0.778540 val loss: 0.639003
[Epoch 24] ogbg-molbace: 0.751243 test loss: 0.647583
[Epoch 25; Iter     6/   31] train: loss: 0.6372777
[Epoch 25] ogbg-molbace: 0.784907 val loss: 0.619638
[Epoch 25] ogbg-molbace: 0.743347 test loss: 0.634163
[Epoch 26; Iter     5/   31] train: loss: 0.6997526
[Epoch 26] ogbg-molbace: 0.812080 val loss: 0.590478
[Epoch 26] ogbg-molbace: 0.771704 test loss: 0.607260
[Epoch 27; Iter     4/   31] train: loss: 0.5877258
[Epoch 27] ogbg-molbace: 0.837683 val loss: 0.537200
[Epoch 27] ogbg-molbace: 0.792775 test loss: 0.560668
[Epoch 28; Iter     3/   31] train: loss: 0.5843917
[Epoch 28] ogbg-molbace: 0.820465 val loss: 0.534796
[Epoch 28] ogbg-molbace: 0.742474 test loss: 0.565722
[Epoch 29; Iter     2/   31] train: loss: 0.5786965
[Epoch 29] ogbg-molbace: 0.827415 val loss: 0.531564
[Epoch 29] ogbg-molbace: 0.774540 test loss: 0.564239
[Epoch 30; Iter     1/   31] train: loss: 0.5096611
[Epoch 30; Iter    31/   31] train: loss: 0.7493090
[Epoch 30] ogbg-molbace: 0.853601 val loss: 0.512215
[Epoch 30] ogbg-molbace: 0.809266 test loss: 0.532729
[Epoch 31; Iter    30/   31] train: loss: 0.6774890
[Epoch 31] ogbg-molbace: 0.849924 val loss: 0.481289
[Epoch 31] ogbg-molbace: 0.818820 test loss: 0.507679
[Epoch 32; Iter    29/   31] train: loss: 0.4883325
[Epoch 32] ogbg-molbace: 0.852166 val loss: 0.492611
[Epoch 32] ogbg-molbace: 0.794521 test loss: 0.527830
[Epoch 33; Iter    28/   31] train: loss: 0.4452272
[Epoch 33] ogbg-molbace: 0.865079 val loss: 0.466829
[Epoch 33] ogbg-molbace: 0.835267 test loss: 0.491000
[Epoch 34; Iter    27/   31] train: loss: 0.5553646
[Epoch 34] ogbg-molbace: 0.862837 val loss: 0.465621
[Epoch 34] ogbg-molbace: 0.819518 test loss: 0.497596
[Epoch 35; Iter    26/   31] train: loss: 0.4763247
[Epoch 35] ogbg-molbace: 0.851359 val loss: 0.484836
[Epoch 35] ogbg-molbace: 0.786450 test loss: 0.522125
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/bace/random/train_prop=0.8/PNA_ogbg-molbace_3DInfomax_bace_random=0.8_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/bace/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_bace_random=0.8
logdir: runs/split/3DInfomax/bace/random/train_prop=0.8
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molbace
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: BCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molbace
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 1
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   41] train: loss: 0.6923079
[Epoch 1] ogbg-molbace: 0.474156 val loss: 0.693386
[Epoch 1] ogbg-molbace: 0.479543 test loss: 0.693300
[Epoch 2; Iter    19/   41] train: loss: 0.6947217
[Epoch 2] ogbg-molbace: 0.475387 val loss: 0.693923
[Epoch 2] ogbg-molbace: 0.477436 test loss: 0.693826
[Epoch 3; Iter     8/   41] train: loss: 0.6938754
[Epoch 3; Iter    38/   41] train: loss: 0.6930652
[Epoch 3] ogbg-molbace: 0.466596 val loss: 0.694231
[Epoch 3] ogbg-molbace: 0.485865 test loss: 0.694223
[Epoch 4; Iter    27/   41] train: loss: 0.6958211
[Epoch 4] ogbg-molbace: 0.471343 val loss: 0.694065
[Epoch 4] ogbg-molbace: 0.491484 test loss: 0.694356
[Epoch 5; Iter    16/   41] train: loss: 0.6918484
[Epoch 5] ogbg-molbace: 0.470288 val loss: 0.694213
[Epoch 5] ogbg-molbace: 0.484811 test loss: 0.694265
[Epoch 6; Iter     5/   41] train: loss: 0.6912805
[Epoch 6; Iter    35/   41] train: loss: 0.6928752
[Epoch 6] ogbg-molbace: 0.479958 val loss: 0.694185
[Epoch 6] ogbg-molbace: 0.494820 test loss: 0.693746
[Epoch 7; Iter    24/   41] train: loss: 0.6917515
[Epoch 7] ogbg-molbace: 0.479255 val loss: 0.694345
[Epoch 7] ogbg-molbace: 0.497629 test loss: 0.693414
[Epoch 8; Iter    13/   41] train: loss: 0.6909747
[Epoch 8] ogbg-molbace: 0.486814 val loss: 0.694342
[Epoch 8] ogbg-molbace: 0.496927 test loss: 0.693499
[Epoch 9; Iter     2/   41] train: loss: 0.6931493
[Epoch 9; Iter    32/   41] train: loss: 0.6902594
[Epoch 9] ogbg-molbace: 0.479606 val loss: 0.694494
[Epoch 9] ogbg-molbace: 0.500615 test loss: 0.693198
[Epoch 10; Iter    21/   41] train: loss: 0.6966635
[Epoch 10] ogbg-molbace: 0.492264 val loss: 0.694342
[Epoch 10] ogbg-molbace: 0.507287 test loss: 0.693045
[Epoch 11; Iter    10/   41] train: loss: 0.6895070
[Epoch 11; Iter    40/   41] train: loss: 0.6935184
[Epoch 11] ogbg-molbace: 0.480309 val loss: 0.694943
[Epoch 11] ogbg-molbace: 0.503073 test loss: 0.692548
[Epoch 12; Iter    29/   41] train: loss: 0.6964400
[Epoch 12] ogbg-molbace: 0.486463 val loss: 0.695084
[Epoch 12] ogbg-molbace: 0.512028 test loss: 0.691691
[Epoch 13; Iter    18/   41] train: loss: 0.6885472
[Epoch 13] ogbg-molbace: 0.494726 val loss: 0.694707
[Epoch 13] ogbg-molbace: 0.510975 test loss: 0.692291
[Epoch 14; Iter     7/   41] train: loss: 0.6902449
[Epoch 14; Iter    37/   41] train: loss: 0.6879544
[Epoch 14] ogbg-molbace: 0.495605 val loss: 0.695020
[Epoch 14] ogbg-molbace: 0.519579 test loss: 0.691403
[Epoch 15; Iter    26/   41] train: loss: 0.6938890
[Epoch 15] ogbg-molbace: 0.507384 val loss: 0.695214
[Epoch 15] ogbg-molbace: 0.529061 test loss: 0.690655
[Epoch 16; Iter    15/   41] train: loss: 0.6905712
[Epoch 16] ogbg-molbace: 0.506505 val loss: 0.695364
[Epoch 16] ogbg-molbace: 0.533275 test loss: 0.690074
[Epoch 17; Iter     4/   41] train: loss: 0.6971625
[Epoch 17; Iter    34/   41] train: loss: 0.6901670
[Epoch 17] ogbg-molbace: 0.607947 val loss: 0.687668
[Epoch 17] ogbg-molbace: 0.647410 test loss: 0.688302
[Epoch 18; Iter    23/   41] train: loss: 0.6642399
[Epoch 18] ogbg-molbace: 0.707454 val loss: 0.664799
[Epoch 18] ogbg-molbace: 0.723968 test loss: 0.653529
[Epoch 19; Iter    12/   41] train: loss: 0.6774355
[Epoch 19] ogbg-molbace: 0.717475 val loss: 0.688286
[Epoch 19] ogbg-molbace: 0.700615 test loss: 0.602158
[Epoch 20; Iter     1/   41] train: loss: 0.6063762
[Epoch 20; Iter    31/   41] train: loss: 0.6067041
[Epoch 20] ogbg-molbace: 0.748242 val loss: 0.583537
[Epoch 20] ogbg-molbace: 0.809482 test loss: 0.524308
[Epoch 21; Iter    20/   41] train: loss: 0.5312271
[Epoch 21] ogbg-molbace: 0.723277 val loss: 0.635068
[Epoch 21] ogbg-molbace: 0.705356 test loss: 0.564582
[Epoch 22; Iter     9/   41] train: loss: 0.5709001
[Epoch 22; Iter    39/   41] train: loss: 0.4660309
[Epoch 22] ogbg-molbace: 0.785865 val loss: 0.550082
[Epoch 22] ogbg-molbace: 0.776822 test loss: 0.521708
[Epoch 23; Iter    28/   41] train: loss: 0.5120158
[Epoch 23] ogbg-molbace: 0.817686 val loss: 0.464240
[Epoch 23] ogbg-molbace: 0.842493 test loss: 0.452609
[Epoch 24; Iter    17/   41] train: loss: 0.3510942
[Epoch 24] ogbg-molbace: 0.782173 val loss: 0.524868
[Epoch 24] ogbg-molbace: 0.791747 test loss: 0.522201
[Epoch 25; Iter     6/   41] train: loss: 0.5198312
[Epoch 25; Iter    36/   41] train: loss: 0.4670210
[Epoch 25] ogbg-molbace: 0.822433 val loss: 0.458265
[Epoch 25] ogbg-molbace: 0.828973 test loss: 0.474686
[Epoch 26; Iter    25/   41] train: loss: 0.3854426
[Epoch 26] ogbg-molbace: 0.842300 val loss: 0.460239
[Epoch 26] ogbg-molbace: 0.830378 test loss: 0.546537
[Epoch 27; Iter    14/   41] train: loss: 0.4831850
[Epoch 27] ogbg-molbace: 0.836850 val loss: 0.498181
[Epoch 27] ogbg-molbace: 0.796664 test loss: 0.480439
[Epoch 28; Iter     3/   41] train: loss: 0.5164336
[Epoch 28; Iter    33/   41] train: loss: 0.5205436
[Epoch 28] ogbg-molbace: 0.817335 val loss: 0.614666
[Epoch 28] ogbg-molbace: 0.791923 test loss: 0.498751
[Epoch 29; Iter    22/   41] train: loss: 0.4948862
[Epoch 29] ogbg-molbace: 0.804677 val loss: 0.575351
[Epoch 29] ogbg-molbace: 0.744337 test loss: 0.522257
[Epoch 30; Iter    11/   41] train: loss: 0.5056568
[Epoch 30; Iter    41/   41] train: loss: 0.3956536
[Epoch 30] ogbg-molbace: 0.842475 val loss: 0.478986
[Epoch 30] ogbg-molbace: 0.845127 test loss: 0.435275
[Epoch 31; Iter    30/   41] train: loss: 0.4511978
[Epoch 31] ogbg-molbace: 0.843530 val loss: 0.576794
[Epoch 31] ogbg-molbace: 0.800527 test loss: 0.503632
[Epoch 32; Iter    19/   41] train: loss: 0.4910993
[Epoch 32] ogbg-molbace: 0.833509 val loss: 0.460223
[Epoch 32] ogbg-molbace: 0.840386 test loss: 0.445987
[Epoch 33; Iter     8/   41] train: loss: 0.5998791
[Epoch 33; Iter    38/   41] train: loss: 0.5153997
[Epoch 33] ogbg-molbace: 0.839838 val loss: 0.432194
[Epoch 33] ogbg-molbace: 0.848990 test loss: 0.430435
[Epoch 34; Iter    27/   41] train: loss: 0.4819965
[Epoch 34] ogbg-molbace: 0.808720 val loss: 0.474998
[Epoch 34] ogbg-molbace: 0.821773 test loss: 0.482041
[Epoch 35; Iter    16/   41] train: loss: 0.4742581
[Epoch 35] ogbg-molbace: 0.894515 val loss: 0.378652
[Epoch 35] ogbg-molbace: 0.863389 test loss: 0.406656
[Epoch 36; Iter     5/   41] train: loss: 0.4564548
[Epoch 36; Iter    35/   41] train: loss: 0.6504689
[Epoch 36] ogbg-molbace: 0.892757 val loss: 0.420382
[Epoch 36] ogbg-molbace: 0.850746 test loss: 0.483258
[Epoch 37; Iter    24/   41] train: loss: 0.5712487
[Epoch 37] ogbg-molbace: 0.887131 val loss: 0.366020
[Epoch 37] ogbg-molbace: 0.856365 test loss: 0.403839
[Epoch 38; Iter    13/   41] train: loss: 0.4538776
[Epoch 38] ogbg-molbace: 0.861639 val loss: 0.419824
[Epoch 38] ogbg-molbace: 0.884109 test loss: 0.365115
[Epoch 39; Iter     2/   41] train: loss: 0.3618162
[Epoch 39; Iter    32/   41] train: loss: 0.6918508
[Epoch 39] ogbg-molbace: 0.862166 val loss: 0.493261
[Epoch 39] ogbg-molbace: 0.859526 test loss: 0.583119
[Epoch 40; Iter    21/   41] train: loss: 0.5274035
[Epoch 40] ogbg-molbace: 0.847222 val loss: 0.550887
[Epoch 40] ogbg-molbace: 0.842845 test loss: 0.445663
[Epoch 41; Iter    10/   41] train: loss: 0.4768467
[Epoch 41; Iter    40/   41] train: loss: 0.5303017
[Epoch 41] ogbg-molbace: 0.875879 val loss: 0.407027
[Epoch 41] ogbg-molbace: 0.860228 test loss: 0.397534
[Epoch 42; Iter    29/   41] train: loss: 0.5471382
[Epoch 42] ogbg-molbace: 0.885373 val loss: 0.368647
[Epoch 42] ogbg-molbace: 0.839333 test loss: 0.430519
[Epoch 43; Iter    18/   41] train: loss: 0.2184487
[Epoch 43] ogbg-molbace: 0.871660 val loss: 0.448549
[Epoch 43] ogbg-molbace: 0.834943 test loss: 0.446273
[Epoch 44; Iter     7/   41] train: loss: 0.4219924
[Epoch 44; Iter    37/   41] train: loss: 0.3554794
[Epoch 44] ogbg-molbace: 0.905063 val loss: 0.400171
[Epoch 44] ogbg-molbace: 0.858472 test loss: 0.415582
[Epoch 45; Iter    26/   41] train: loss: 0.4125268
[Epoch 45] ogbg-molbace: 0.890471 val loss: 0.443402
[Epoch 45] ogbg-molbace: 0.872695 test loss: 0.496523
[Epoch 46; Iter    15/   41] train: loss: 0.4816369
[Epoch 46] ogbg-molbace: 0.861463 val loss: 0.435818
[Epoch 46] ogbg-molbace: 0.839860 test loss: 0.437372
[Epoch 47; Iter     4/   41] train: loss: 0.4180279
[Epoch 47; Iter    34/   41] train: loss: 0.2268870
[Epoch 47] ogbg-molbace: 0.900844 val loss: 0.409310
[Epoch 47] ogbg-molbace: 0.860579 test loss: 0.392091
[Epoch 48; Iter    23/   41] train: loss: 0.5670306
[Epoch 48] ogbg-molbace: 0.882208 val loss: 0.404359
[Epoch 48] ogbg-molbace: 0.859701 test loss: 0.415157
[Epoch 49; Iter    12/   41] train: loss: 0.2468259
[Epoch 49] ogbg-molbace: 0.892757 val loss: 0.359382
[Epoch 49] ogbg-molbace: 0.865847 test loss: 0.407502
[Epoch 50; Iter     1/   41] train: loss: 0.3638542
[Epoch 50; Iter    31/   41] train: loss: 0.3610478
[Epoch 50] ogbg-molbace: 0.859001 val loss: 0.593735
[Epoch 50] ogbg-molbace: 0.846005 test loss: 0.432916
[Epoch 51; Iter    20/   41] train: loss: 0.3875076
[Epoch 51] ogbg-molbace: 0.871308 val loss: 0.445393
[Epoch 51] ogbg-molbace: 0.866901 test loss: 0.387161
[Epoch 52; Iter     9/   41] train: loss: 0.3419579
[Epoch 52; Iter    39/   41] train: loss: 0.3208004
[Epoch 52] ogbg-molbace: 0.904887 val loss: 0.383819
[Epoch 52] ogbg-molbace: 0.864969 test loss: 0.412418
[Epoch 53; Iter    28/   41] train: loss: 0.4835551
[Epoch 53] ogbg-molbace: 0.869374 val loss: 0.462154
[Epoch 53] ogbg-molbace: 0.854785 test loss: 0.425468
[Epoch 54; Iter    17/   41] train: loss: 0.2895502
[Epoch 54] ogbg-molbace: 0.878692 val loss: 0.403715
[Epoch 54] ogbg-molbace: 0.873573 test loss: 0.443205
[Epoch 55; Iter     6/   41] train: loss: 0.3105201
[Epoch 55; Iter    36/   41] train: loss: 0.3680441
[Epoch 55] ogbg-molbace: 0.901195 val loss: 0.517861
[Epoch 55] ogbg-molbace: 0.874276 test loss: 0.445623
[Epoch 56; Iter    25/   41] train: loss: 0.2646534
[Epoch 56] ogbg-molbace: 0.902250 val loss: 0.370904
[Epoch 56] ogbg-molbace: 0.841264 test loss: 0.431490
[Epoch 57; Iter    14/   41] train: loss: 0.4308286
[Epoch 57] ogbg-molbace: 0.870605 val loss: 0.516533
[Epoch 57] ogbg-molbace: 0.837928 test loss: 0.539081
[Epoch 58; Iter     3/   41] train: loss: 0.2228208
[Epoch 58; Iter    33/   41] train: loss: 0.2114390
[Epoch 58] ogbg-molbace: 0.912799 val loss: 0.364275
[Epoch 58] ogbg-molbace: 0.861282 test loss: 0.401156
[Epoch 59; Iter    22/   41] train: loss: 0.2287857
[Epoch 59] ogbg-molbace: 0.882736 val loss: 0.421303
[Epoch 59] ogbg-molbace: 0.844249 test loss: 0.548969
[Epoch 60; Iter    11/   41] train: loss: 0.1548393
[Epoch 60; Iter    41/   41] train: loss: 0.4831575
[Epoch 60] ogbg-molbace: 0.897152 val loss: 0.414479
[Epoch 60] ogbg-molbace: 0.850395 test loss: 0.479186
[Epoch 61; Iter    30/   41] train: loss: 0.1792155
[Epoch 61] ogbg-molbace: 0.895042 val loss: 0.417093
[Epoch 61] ogbg-molbace: 0.866023 test loss: 0.393786
[Epoch 62; Iter    19/   41] train: loss: 0.1923924
[Epoch 62] ogbg-molbace: 0.908228 val loss: 0.364312
[Epoch 62] ogbg-molbace: 0.867954 test loss: 0.467667
[Epoch 63; Iter     8/   41] train: loss: 0.1602854
[Epoch 63; Iter    38/   41] train: loss: 0.3074399
[Epoch 63] ogbg-molbace: 0.878165 val loss: 0.490380
[Epoch 63] ogbg-molbace: 0.857068 test loss: 0.532884
[Epoch 64; Iter    27/   41] train: loss: 0.6094753
[Epoch 64] ogbg-molbace: 0.882384 val loss: 0.488078
[Epoch 64] ogbg-molbace: 0.863916 test loss: 0.445379
[Epoch 65; Iter    16/   41] train: loss: 0.4079022
[Epoch 65] ogbg-molbace: 0.877286 val loss: 0.543441
[Epoch 65] ogbg-molbace: 0.858824 test loss: 0.469318
[Epoch 66; Iter     5/   41] train: loss: 0.1538391
[Epoch 66; Iter    35/   41] train: loss: 0.2378952
[Epoch 66] ogbg-molbace: 0.903657 val loss: 0.364121
[Epoch 66] ogbg-molbace: 0.897981 test loss: 0.420188
[Epoch 67; Iter    24/   41] train: loss: 0.5362977
[Epoch 67] ogbg-molbace: 0.902954 val loss: 0.467172
[Epoch 67] ogbg-molbace: 0.874100 test loss: 0.443075
[Epoch 68; Iter    13/   41] train: loss: 0.2622544
[Epoch 68] ogbg-molbace: 0.880977 val loss: 0.484273
[Epoch 68] ogbg-molbace: 0.865496 test loss: 0.450184
[Epoch 69; Iter     2/   41] train: loss: 0.1003530
[Epoch 69; Iter    32/   41] train: loss: 0.1801704
[Epoch 69] ogbg-molbace: 0.895570 val loss: 0.388556
[Epoch 69] ogbg-molbace: 0.886392 test loss: 0.442279
[Epoch 70; Iter    21/   41] train: loss: 0.3571155
[Epoch 70] ogbg-molbace: 0.905591 val loss: 0.366202
[Epoch 70] ogbg-molbace: 0.883582 test loss: 0.412908
[Epoch 71; Iter    10/   41] train: loss: 0.2324654
[Epoch 71; Iter    40/   41] train: loss: 0.2283098
[Epoch 71] ogbg-molbace: 0.894691 val loss: 0.373656
[Epoch 71] ogbg-molbace: 0.887445 test loss: 0.428086
[Epoch 72; Iter    29/   41] train: loss: 0.4691100
[Epoch 72] ogbg-molbace: 0.889065 val loss: 0.515775
[Epoch 72] ogbg-molbace: 0.870588 test loss: 0.544141
[Epoch 73; Iter    18/   41] train: loss: 0.1520923
[Epoch 73] ogbg-molbace: 0.910513 val loss: 0.363245
[Epoch 73] ogbg-molbace: 0.866198 test loss: 0.509382
[Epoch 74; Iter     7/   41] train: loss: 0.1760719
[Epoch 74; Iter    37/   41] train: loss: 0.2898211
[Epoch 74] ogbg-molbace: 0.883263 val loss: 0.443041
[Epoch 74] ogbg-molbace: 0.882880 test loss: 0.416518
[Epoch 75; Iter    26/   41] train: loss: 0.1163821
[Epoch 75] ogbg-molbace: 0.880626 val loss: 0.484156
[Epoch 75] ogbg-molbace: 0.845127 test loss: 0.486824
[Epoch 76; Iter    15/   41] train: loss: 0.1707380
[Epoch 76] ogbg-molbace: 0.920183 val loss: 0.362170
[Epoch 76] ogbg-molbace: 0.820896 test loss: 0.611717
[Epoch 77; Iter     4/   41] train: loss: 0.1192585
[Epoch 77; Iter    34/   41] train: loss: 0.1791650
[Epoch 77] ogbg-molbace: 0.918249 val loss: 0.411016
[Epoch 77] ogbg-molbace: 0.864794 test loss: 0.486732
[Epoch 78; Iter    23/   41] train: loss: 0.1427917
[Epoch 34] ogbg-molbace: 0.785985 val loss: 0.563119
[Epoch 34] ogbg-molbace: 0.773830 test loss: 0.566254
[Epoch 35; Iter     6/   36] train: loss: 0.6017578
[Epoch 35; Iter    36/   36] train: loss: 0.4376571
[Epoch 35] ogbg-molbace: 0.832860 val loss: 0.543938
[Epoch 35] ogbg-molbace: 0.804541 test loss: 0.553049
[Epoch 36; Iter    30/   36] train: loss: 0.6073583
[Epoch 36] ogbg-molbace: 0.836806 val loss: 0.518673
[Epoch 36] ogbg-molbace: 0.834551 test loss: 0.521897
[Epoch 37; Iter    24/   36] train: loss: 0.6457772
[Epoch 37] ogbg-molbace: 0.853614 val loss: 0.541094
[Epoch 37] ogbg-molbace: 0.800264 test loss: 0.695599
[Epoch 38; Iter    18/   36] train: loss: 0.3653196
[Epoch 38] ogbg-molbace: 0.855271 val loss: 0.481982
[Epoch 38] ogbg-molbace: 0.848080 test loss: 0.485482
[Epoch 39; Iter    12/   36] train: loss: 0.5635175
[Epoch 39] ogbg-molbace: 0.871922 val loss: 0.462335
[Epoch 39] ogbg-molbace: 0.862930 test loss: 0.472133
[Epoch 40; Iter     6/   36] train: loss: 0.4052869
[Epoch 40; Iter    36/   36] train: loss: 0.2428723
[Epoch 40] ogbg-molbace: 0.843671 val loss: 0.498810
[Epoch 40] ogbg-molbace: 0.820323 test loss: 0.536380
[Epoch 41; Iter    30/   36] train: loss: 0.6963068
[Epoch 41] ogbg-molbace: 0.819366 val loss: 0.590655
[Epoch 41] ogbg-molbace: 0.818846 test loss: 0.575331
[Epoch 42; Iter    24/   36] train: loss: 0.6659973
[Epoch 42] ogbg-molbace: 0.853772 val loss: 0.487370
[Epoch 42] ogbg-molbace: 0.853833 test loss: 0.490350
[Epoch 43; Iter    18/   36] train: loss: 0.3407181
[Epoch 43] ogbg-molbace: 0.867503 val loss: 0.444694
[Epoch 43] ogbg-molbace: 0.837739 test loss: 0.505817
[Epoch 44; Iter    12/   36] train: loss: 0.5263003
[Epoch 44] ogbg-molbace: 0.830256 val loss: 0.505366
[Epoch 44] ogbg-molbace: 0.853911 test loss: 0.485190
[Epoch 45; Iter     6/   36] train: loss: 0.2591892
[Epoch 45; Iter    36/   36] train: loss: 0.2087537
[Epoch 45] ogbg-molbace: 0.827888 val loss: 0.783354
[Epoch 45] ogbg-molbace: 0.796066 test loss: 0.986559
[Epoch 46; Iter    30/   36] train: loss: 0.6633575
[Epoch 46] ogbg-molbace: 0.858823 val loss: 0.454585
[Epoch 46] ogbg-molbace: 0.869305 test loss: 0.460656
[Epoch 47; Iter    24/   36] train: loss: 0.4798056
[Epoch 47] ogbg-molbace: 0.852036 val loss: 0.597682
[Epoch 47] ogbg-molbace: 0.862463 test loss: 0.568786
[Epoch 48; Iter    18/   36] train: loss: 0.2971559
[Epoch 48] ogbg-molbace: 0.840357 val loss: 0.573982
[Epoch 48] ogbg-molbace: 0.843492 test loss: 0.641011
[Epoch 49; Iter    12/   36] train: loss: 0.4341303
[Epoch 49] ogbg-molbace: 0.839568 val loss: 0.557103
[Epoch 49] ogbg-molbace: 0.882833 test loss: 0.475580
[Epoch 50; Iter     6/   36] train: loss: 0.4768393
[Epoch 50; Iter    36/   36] train: loss: 0.6103598
[Epoch 50] ogbg-molbace: 0.863952 val loss: 0.506440
[Epoch 50] ogbg-molbace: 0.836651 test loss: 0.558653
[Epoch 51; Iter    30/   36] train: loss: 0.3456046
[Epoch 51] ogbg-molbace: 0.847380 val loss: 0.698868
[Epoch 51] ogbg-molbace: 0.864718 test loss: 0.652202
[Epoch 52; Iter    24/   36] train: loss: 0.4513792
[Epoch 52] ogbg-molbace: 0.857718 val loss: 0.528800
[Epoch 52] ogbg-molbace: 0.842715 test loss: 0.579091
[Epoch 53; Iter    18/   36] train: loss: 0.2191515
[Epoch 53] ogbg-molbace: 0.896070 val loss: 0.404060
[Epoch 53] ogbg-molbace: 0.889520 test loss: 0.438714
[Epoch 54; Iter    12/   36] train: loss: 0.3884599
[Epoch 54] ogbg-molbace: 0.881234 val loss: 0.448424
[Epoch 54] ogbg-molbace: 0.872493 test loss: 0.477333
[Epoch 55; Iter     6/   36] train: loss: 0.2774847
[Epoch 55; Iter    36/   36] train: loss: 0.2855116
[Epoch 55] ogbg-molbace: 0.869792 val loss: 0.481340
[Epoch 55] ogbg-molbace: 0.866584 test loss: 0.478203
[Epoch 56; Iter    30/   36] train: loss: 0.5738325
[Epoch 56] ogbg-molbace: 0.858033 val loss: 0.490142
[Epoch 56] ogbg-molbace: 0.875603 test loss: 0.484471
[Epoch 57; Iter    24/   36] train: loss: 0.3415505
[Epoch 57] ogbg-molbace: 0.872001 val loss: 0.504359
[Epoch 57] ogbg-molbace: 0.876769 test loss: 0.503178
[Epoch 58; Iter    18/   36] train: loss: 0.2236731
[Epoch 58] ogbg-molbace: 0.841540 val loss: 0.585994
[Epoch 58] ogbg-molbace: 0.838905 test loss: 0.603746
[Epoch 59; Iter    12/   36] train: loss: 0.2753352
[Epoch 59] ogbg-molbace: 0.875868 val loss: 0.459166
[Epoch 59] ogbg-molbace: 0.901182 test loss: 0.420996
[Epoch 60; Iter     6/   36] train: loss: 0.1955590
[Epoch 60; Iter    36/   36] train: loss: 0.3668267
[Epoch 60] ogbg-molbace: 0.854956 val loss: 0.517405
[Epoch 60] ogbg-molbace: 0.846525 test loss: 0.547095
[Epoch 61; Iter    30/   36] train: loss: 0.2752784
[Epoch 61] ogbg-molbace: 0.847617 val loss: 0.499367
[Epoch 61] ogbg-molbace: 0.874281 test loss: 0.497644
[Epoch 62; Iter    24/   36] train: loss: 0.1248628
[Epoch 62] ogbg-molbace: 0.880919 val loss: 0.494110
[Epoch 62] ogbg-molbace: 0.876847 test loss: 0.501868
[Epoch 63; Iter    18/   36] train: loss: 0.3049237
[Epoch 63] ogbg-molbace: 0.843040 val loss: 0.843736
[Epoch 63] ogbg-molbace: 0.850645 test loss: 0.871585
[Epoch 64; Iter    12/   36] train: loss: 0.2241084
[Epoch 64] ogbg-molbace: 0.864978 val loss: 0.513832
[Epoch 64] ogbg-molbace: 0.871249 test loss: 0.502209
[Epoch 65; Iter     6/   36] train: loss: 0.2206120
[Epoch 65; Iter    36/   36] train: loss: 0.2045951
[Epoch 65] ogbg-molbace: 0.848406 val loss: 0.610888
[Epoch 65] ogbg-molbace: 0.871015 test loss: 0.533923
[Epoch 66; Iter    30/   36] train: loss: 0.4230877
[Epoch 66] ogbg-molbace: 0.853851 val loss: 0.576620
[Epoch 66] ogbg-molbace: 0.844192 test loss: 0.687021
[Epoch 67; Iter    24/   36] train: loss: 0.3106597
[Epoch 67] ogbg-molbace: 0.866162 val loss: 0.528064
[Epoch 67] ogbg-molbace: 0.835407 test loss: 0.653003
[Epoch 68; Iter    18/   36] train: loss: 0.1280544
[Epoch 68] ogbg-molbace: 0.865057 val loss: 0.521773
[Epoch 68] ogbg-molbace: 0.896284 test loss: 0.492761
[Epoch 69; Iter    12/   36] train: loss: 0.3897234
[Epoch 69] ogbg-molbace: 0.869397 val loss: 0.557402
[Epoch 69] ogbg-molbace: 0.851190 test loss: 0.623168
[Epoch 70; Iter     6/   36] train: loss: 0.2789828
[Epoch 70; Iter    36/   36] train: loss: 0.5118668
[Epoch 70] ogbg-molbace: 0.874921 val loss: 0.524119
[Epoch 70] ogbg-molbace: 0.890919 test loss: 0.506827
[Epoch 71; Iter    30/   36] train: loss: 0.2797302
[Epoch 71] ogbg-molbace: 0.865846 val loss: 0.585115
[Epoch 71] ogbg-molbace: 0.873115 test loss: 0.556051
[Epoch 72; Iter    24/   36] train: loss: 0.2657736
[Epoch 72] ogbg-molbace: 0.879972 val loss: 0.518445
[Epoch 72] ogbg-molbace: 0.857720 test loss: 0.651624
[Epoch 73; Iter    18/   36] train: loss: 0.2168660
[Epoch 73] ogbg-molbace: 0.880445 val loss: 0.503863
[Epoch 73] ogbg-molbace: 0.862385 test loss: 0.581048
[Epoch 74; Iter    12/   36] train: loss: 0.3795033
[Epoch 74] ogbg-molbace: 0.878788 val loss: 0.521884
[Epoch 74] ogbg-molbace: 0.853289 test loss: 0.607562
[Epoch 75; Iter     6/   36] train: loss: 0.1422128
[Epoch 75; Iter    36/   36] train: loss: 0.1682815
[Epoch 75] ogbg-molbace: 0.878393 val loss: 0.597732
[Epoch 75] ogbg-molbace: 0.886643 test loss: 0.585337
[Epoch 76; Iter    30/   36] train: loss: 0.1913758
[Epoch 76] ogbg-molbace: 0.880366 val loss: 0.498320
[Epoch 76] ogbg-molbace: 0.880423 test loss: 0.564620
[Epoch 77; Iter    24/   36] train: loss: 0.1094207
[Epoch 77] ogbg-molbace: 0.880129 val loss: 0.527076
[Epoch 77] ogbg-molbace: 0.876225 test loss: 0.593637
[Epoch 78; Iter    18/   36] train: loss: 0.0747661
[Epoch 78] ogbg-molbace: 0.877920 val loss: 0.553893
[Epoch 78] ogbg-molbace: 0.881434 test loss: 0.583376
[Epoch 79; Iter    12/   36] train: loss: 0.0310851
[Epoch 79] ogbg-molbace: 0.893071 val loss: 0.530650
[Epoch 79] ogbg-molbace: 0.882600 test loss: 0.596398
[Epoch 80; Iter     6/   36] train: loss: 0.0715619
[Epoch 80; Iter    36/   36] train: loss: 0.4388725
[Epoch 80] ogbg-molbace: 0.882734 val loss: 0.567078
[Epoch 80] ogbg-molbace: 0.875991 test loss: 0.628608
[Epoch 81; Iter    30/   36] train: loss: 0.1935643
[Epoch 81] ogbg-molbace: 0.894492 val loss: 0.517255
[Epoch 81] ogbg-molbace: 0.873970 test loss: 0.646863
[Epoch 82; Iter    24/   36] train: loss: 0.1142619
[Epoch 34] ogbg-molbace: 0.825363 val loss: 0.511846
[Epoch 34] ogbg-molbace: 0.829731 test loss: 0.501714
[Epoch 35; Iter     6/   36] train: loss: 0.5636761
[Epoch 35; Iter    36/   36] train: loss: 0.7773646
[Epoch 35] ogbg-molbace: 0.851484 val loss: 0.500666
[Epoch 35] ogbg-molbace: 0.853677 test loss: 0.491624
[Epoch 36; Iter    30/   36] train: loss: 0.4854172
[Epoch 36] ogbg-molbace: 0.811711 val loss: 0.528597
[Epoch 36] ogbg-molbace: 0.811305 test loss: 0.527381
[Epoch 37; Iter    24/   36] train: loss: 0.4300006
[Epoch 37] ogbg-molbace: 0.841461 val loss: 0.537412
[Epoch 37] ogbg-molbace: 0.844581 test loss: 0.511265
[Epoch 38; Iter    18/   36] train: loss: 0.4153556
[Epoch 38] ogbg-molbace: 0.835701 val loss: 0.519723
[Epoch 38] ogbg-molbace: 0.824677 test loss: 0.515630
[Epoch 39; Iter    12/   36] train: loss: 0.3633692
[Epoch 39] ogbg-molbace: 0.863715 val loss: 0.468902
[Epoch 39] ogbg-molbace: 0.867983 test loss: 0.459968
[Epoch 40; Iter     6/   36] train: loss: 0.3844682
[Epoch 40; Iter    36/   36] train: loss: 0.3173938
[Epoch 40] ogbg-molbace: 0.785906 val loss: 0.795782
[Epoch 40] ogbg-molbace: 0.825533 test loss: 0.718142
[Epoch 41; Iter    30/   36] train: loss: 0.5240225
[Epoch 41] ogbg-molbace: 0.852036 val loss: 0.480009
[Epoch 41] ogbg-molbace: 0.854222 test loss: 0.485387
[Epoch 42; Iter    24/   36] train: loss: 0.3564534
[Epoch 42] ogbg-molbace: 0.876815 val loss: 0.447523
[Epoch 42] ogbg-molbace: 0.892474 test loss: 0.424787
[Epoch 43; Iter    18/   36] train: loss: 0.4380980
[Epoch 43] ogbg-molbace: 0.860480 val loss: 0.479105
[Epoch 43] ogbg-molbace: 0.877935 test loss: 0.451842
[Epoch 44; Iter    12/   36] train: loss: 0.5485005
[Epoch 44] ogbg-molbace: 0.864347 val loss: 0.493530
[Epoch 44] ogbg-molbace: 0.867050 test loss: 0.490571
[Epoch 45; Iter     6/   36] train: loss: 0.4161547
[Epoch 45; Iter    36/   36] train: loss: 0.1936044
[Epoch 45] ogbg-molbace: 0.872948 val loss: 0.481110
[Epoch 45] ogbg-molbace: 0.867905 test loss: 0.478664
[Epoch 46; Iter    30/   36] train: loss: 0.3655232
[Epoch 46] ogbg-molbace: 0.862453 val loss: 0.477128
[Epoch 46] ogbg-molbace: 0.880578 test loss: 0.466341
[Epoch 47; Iter    24/   36] train: loss: 0.3543122
[Epoch 47] ogbg-molbace: 0.885496 val loss: 0.424440
[Epoch 47] ogbg-molbace: 0.896517 test loss: 0.407818
[Epoch 48; Iter    18/   36] train: loss: 0.4390539
[Epoch 48] ogbg-molbace: 0.874842 val loss: 0.528777
[Epoch 48] ogbg-molbace: 0.849790 test loss: 0.590758
[Epoch 49; Iter    12/   36] train: loss: 0.2959215
[Epoch 49] ogbg-molbace: 0.874605 val loss: 0.501939
[Epoch 49] ogbg-molbace: 0.873814 test loss: 0.501317
[Epoch 50; Iter     6/   36] train: loss: 0.3852885
[Epoch 50; Iter    36/   36] train: loss: 0.3264214
[Epoch 50] ogbg-molbace: 0.865530 val loss: 0.502921
[Epoch 50] ogbg-molbace: 0.868605 test loss: 0.520148
[Epoch 51; Iter    30/   36] train: loss: 0.4972953
[Epoch 51] ogbg-molbace: 0.877052 val loss: 0.441485
[Epoch 51] ogbg-molbace: 0.882522 test loss: 0.456713
[Epoch 52; Iter    24/   36] train: loss: 0.2788858
[Epoch 52] ogbg-molbace: 0.867109 val loss: 0.482973
[Epoch 52] ogbg-molbace: 0.878712 test loss: 0.459541
[Epoch 53; Iter    18/   36] train: loss: 0.2751661
[Epoch 53] ogbg-molbace: 0.865767 val loss: 0.474140
[Epoch 53] ogbg-molbace: 0.892863 test loss: 0.438724
[Epoch 54; Iter    12/   36] train: loss: 0.3856024
[Epoch 54] ogbg-molbace: 0.874053 val loss: 0.456130
[Epoch 54] ogbg-molbace: 0.862385 test loss: 0.534413
[Epoch 55; Iter     6/   36] train: loss: 0.3130694
[Epoch 55; Iter    36/   36] train: loss: 0.5805064
[Epoch 55] ogbg-molbace: 0.872159 val loss: 0.505312
[Epoch 55] ogbg-molbace: 0.899394 test loss: 0.506482
[Epoch 56; Iter    30/   36] train: loss: 0.2330544
[Epoch 56] ogbg-molbace: 0.874527 val loss: 0.534188
[Epoch 56] ogbg-molbace: 0.879334 test loss: 0.577318
[Epoch 57; Iter    24/   36] train: loss: 0.2083449
[Epoch 57] ogbg-molbace: 0.877525 val loss: 0.458516
[Epoch 57] ogbg-molbace: 0.885399 test loss: 0.457360
[Epoch 58; Iter    18/   36] train: loss: 0.5911444
[Epoch 58] ogbg-molbace: 0.880603 val loss: 0.472668
[Epoch 58] ogbg-molbace: 0.887032 test loss: 0.450879
[Epoch 59; Iter    12/   36] train: loss: 0.3146026
[Epoch 59] ogbg-molbace: 0.868056 val loss: 0.482998
[Epoch 59] ogbg-molbace: 0.873581 test loss: 0.507565
[Epoch 60; Iter     6/   36] train: loss: 0.1941633
[Epoch 60; Iter    36/   36] train: loss: 0.1794840
[Epoch 60] ogbg-molbace: 0.872317 val loss: 0.503480
[Epoch 60] ogbg-molbace: 0.861763 test loss: 0.544253
[Epoch 61; Iter    30/   36] train: loss: 0.3447334
[Epoch 61] ogbg-molbace: 0.801057 val loss: 1.023362
[Epoch 61] ogbg-molbace: 0.836029 test loss: 1.079173
[Epoch 62; Iter    24/   36] train: loss: 0.3133959
[Epoch 62] ogbg-molbace: 0.860401 val loss: 0.478714
[Epoch 62] ogbg-molbace: 0.859975 test loss: 0.482863
[Epoch 63; Iter    18/   36] train: loss: 0.2148802
[Epoch 63] ogbg-molbace: 0.851799 val loss: 0.541599
[Epoch 63] ogbg-molbace: 0.878090 test loss: 0.488017
[Epoch 64; Iter    12/   36] train: loss: 0.4050346
[Epoch 64] ogbg-molbace: 0.888415 val loss: 0.428734
[Epoch 64] ogbg-molbace: 0.887965 test loss: 0.432476
[Epoch 65; Iter     6/   36] train: loss: 0.2833989
[Epoch 65; Iter    36/   36] train: loss: 0.1734052
[Epoch 65] ogbg-molbace: 0.889915 val loss: 0.450143
[Epoch 65] ogbg-molbace: 0.902037 test loss: 0.415538
[Epoch 66; Iter    30/   36] train: loss: 0.4398663
[Epoch 66] ogbg-molbace: 0.881392 val loss: 0.445166
[Epoch 66] ogbg-molbace: 0.890375 test loss: 0.433914
[Epoch 67; Iter    24/   36] train: loss: 0.2747429
[Epoch 67] ogbg-molbace: 0.861664 val loss: 0.531305
[Epoch 67] ogbg-molbace: 0.875914 test loss: 0.519187
[Epoch 68; Iter    18/   36] train: loss: 0.2179593
[Epoch 68] ogbg-molbace: 0.850458 val loss: 0.762649
[Epoch 68] ogbg-molbace: 0.863474 test loss: 0.741956
[Epoch 69; Iter    12/   36] train: loss: 0.2388160
[Epoch 69] ogbg-molbace: 0.882970 val loss: 0.480067
[Epoch 69] ogbg-molbace: 0.892163 test loss: 0.481073
[Epoch 70; Iter     6/   36] train: loss: 0.2377878
[Epoch 70; Iter    36/   36] train: loss: 0.6429523
[Epoch 70] ogbg-molbace: 0.865372 val loss: 0.564681
[Epoch 70] ogbg-molbace: 0.864018 test loss: 0.613827
[Epoch 71; Iter    30/   36] train: loss: 0.4550083
[Epoch 71] ogbg-molbace: 0.879182 val loss: 0.495611
[Epoch 71] ogbg-molbace: 0.884855 test loss: 0.501895
[Epoch 72; Iter    24/   36] train: loss: 0.5083913
[Epoch 72] ogbg-molbace: 0.847932 val loss: 0.906875
[Epoch 72] ogbg-molbace: 0.870238 test loss: 0.819486
[Epoch 73; Iter    18/   36] train: loss: 0.1328376
[Epoch 73] ogbg-molbace: 0.868371 val loss: 0.542769
[Epoch 73] ogbg-molbace: 0.866195 test loss: 0.545676
[Epoch 74; Iter    12/   36] train: loss: 0.1721494
[Epoch 74] ogbg-molbace: 0.873106 val loss: 0.509166
[Epoch 74] ogbg-molbace: 0.891696 test loss: 0.477486
[Epoch 75; Iter     6/   36] train: loss: 0.1459584
[Epoch 75; Iter    36/   36] train: loss: 0.9555749
[Epoch 75] ogbg-molbace: 0.884785 val loss: 0.456423
[Epoch 75] ogbg-molbace: 0.841626 test loss: 0.616585
[Epoch 76; Iter    30/   36] train: loss: 0.2886250
[Epoch 76] ogbg-molbace: 0.879182 val loss: 0.477770
[Epoch 76] ogbg-molbace: 0.863552 test loss: 0.518840
[Epoch 77; Iter    24/   36] train: loss: 0.1572450
[Epoch 77] ogbg-molbace: 0.869476 val loss: 0.550574
[Epoch 77] ogbg-molbace: 0.838905 test loss: 0.653211
[Epoch 78; Iter    18/   36] train: loss: 0.1639982
[Epoch 78] ogbg-molbace: 0.864426 val loss: 0.573512
[Epoch 78] ogbg-molbace: 0.868061 test loss: 0.579414
[Epoch 79; Iter    12/   36] train: loss: 0.1673276
[Epoch 79] ogbg-molbace: 0.881550 val loss: 0.518326
[Epoch 79] ogbg-molbace: 0.868838 test loss: 0.594505
[Epoch 80; Iter     6/   36] train: loss: 0.2163737
[Epoch 80; Iter    36/   36] train: loss: 0.2122506
[Epoch 80] ogbg-molbace: 0.891098 val loss: 0.461739
[Epoch 80] ogbg-molbace: 0.899238 test loss: 0.442007
[Epoch 81; Iter    30/   36] train: loss: 0.2720818
[Epoch 81] ogbg-molbace: 0.832623 val loss: 0.643296
[Epoch 81] ogbg-molbace: 0.842637 test loss: 0.673875
[Epoch 82; Iter    24/   36] train: loss: 0.0510514
[Epoch 32] ogbg-molbace: 0.850219 test loss: 0.431502
[Epoch 33; Iter     8/   41] train: loss: 0.3643949
[Epoch 33; Iter    38/   41] train: loss: 0.3657082
[Epoch 33] ogbg-molbace: 0.821730 val loss: 0.582064
[Epoch 33] ogbg-molbace: 0.784899 test loss: 0.494529
[Epoch 34; Iter    27/   41] train: loss: 0.4708491
[Epoch 34] ogbg-molbace: 0.825598 val loss: 0.504576
[Epoch 34] ogbg-molbace: 0.846181 test loss: 0.501770
[Epoch 35; Iter    16/   41] train: loss: 0.3624770
[Epoch 35] ogbg-molbace: 0.876758 val loss: 0.393946
[Epoch 35] ogbg-molbace: 0.887972 test loss: 0.415614
[Epoch 36; Iter     5/   41] train: loss: 0.4614317
[Epoch 36; Iter    35/   41] train: loss: 0.4624788
[Epoch 36] ogbg-molbace: 0.917722 val loss: 0.397471
[Epoch 36] ogbg-molbace: 0.854785 test loss: 0.421551
[Epoch 37; Iter    24/   41] train: loss: 0.4375516
[Epoch 37] ogbg-molbace: 0.852848 val loss: 1.006011
[Epoch 37] ogbg-molbace: 0.855838 test loss: 0.804263
[Epoch 38; Iter    13/   41] train: loss: 0.4273068
[Epoch 38] ogbg-molbace: 0.876758 val loss: 0.401944
[Epoch 38] ogbg-molbace: 0.858824 test loss: 0.430325
[Epoch 39; Iter     2/   41] train: loss: 0.2756585
[Epoch 39; Iter    32/   41] train: loss: 0.5883489
[Epoch 39] ogbg-molbace: 0.856716 val loss: 0.479579
[Epoch 39] ogbg-molbace: 0.845127 test loss: 0.407633
[Epoch 40; Iter    21/   41] train: loss: 0.3743837
[Epoch 40] ogbg-molbace: 0.872890 val loss: 0.412664
[Epoch 40] ogbg-molbace: 0.873222 test loss: 0.383941
[Epoch 41; Iter    10/   41] train: loss: 0.4020004
[Epoch 41; Iter    40/   41] train: loss: 0.4450282
[Epoch 41] ogbg-molbace: 0.832630 val loss: 0.473422
[Epoch 41] ogbg-molbace: 0.826690 test loss: 0.447086
[Epoch 42; Iter    29/   41] train: loss: 0.3341309
[Epoch 42] ogbg-molbace: 0.854430 val loss: 0.475723
[Epoch 42] ogbg-molbace: 0.869183 test loss: 0.467792
[Epoch 43; Iter    18/   41] train: loss: 0.3496739
[Epoch 43] ogbg-molbace: 0.899437 val loss: 0.354331
[Epoch 43] ogbg-molbace: 0.873222 test loss: 0.430660
[Epoch 44; Iter     7/   41] train: loss: 0.4956313
[Epoch 44; Iter    37/   41] train: loss: 0.3386332
[Epoch 44] ogbg-molbace: 0.909283 val loss: 0.460169
[Epoch 44] ogbg-molbace: 0.870061 test loss: 0.449489
[Epoch 45; Iter    26/   41] train: loss: 0.3553832
[Epoch 45] ogbg-molbace: 0.930028 val loss: 0.306778
[Epoch 45] ogbg-molbace: 0.855838 test loss: 0.445463
[Epoch 46; Iter    15/   41] train: loss: 0.3457828
[Epoch 46] ogbg-molbace: 0.878516 val loss: 0.426020
[Epoch 46] ogbg-molbace: 0.860931 test loss: 0.393977
[Epoch 47; Iter     4/   41] train: loss: 0.3799990
[Epoch 47; Iter    34/   41] train: loss: 0.6607378
[Epoch 47] ogbg-molbace: 0.888713 val loss: 0.395000
[Epoch 47] ogbg-molbace: 0.875856 test loss: 0.396693
[Epoch 48; Iter    23/   41] train: loss: 0.2635025
[Epoch 48] ogbg-molbace: 0.905415 val loss: 0.332410
[Epoch 48] ogbg-molbace: 0.882353 test loss: 0.353100
[Epoch 49; Iter    12/   41] train: loss: 0.3880636
[Epoch 49] ogbg-molbace: 0.878165 val loss: 0.400352
[Epoch 49] ogbg-molbace: 0.874451 test loss: 0.388425
[Epoch 50; Iter     1/   41] train: loss: 0.3766364
[Epoch 50; Iter    31/   41] train: loss: 0.2098205
[Epoch 50] ogbg-molbace: 0.898558 val loss: 0.366621
[Epoch 50] ogbg-molbace: 0.882353 test loss: 0.426044
[Epoch 51; Iter    20/   41] train: loss: 0.2609493
[Epoch 51] ogbg-molbace: 0.885724 val loss: 0.371265
[Epoch 51] ogbg-molbace: 0.857594 test loss: 0.424735
[Epoch 52; Iter     9/   41] train: loss: 0.1843844
[Epoch 52; Iter    39/   41] train: loss: 0.1955338
[Epoch 52] ogbg-molbace: 0.898207 val loss: 0.424495
[Epoch 52] ogbg-molbace: 0.868306 test loss: 0.425208
[Epoch 53; Iter    28/   41] train: loss: 0.2845669
[Epoch 53] ogbg-molbace: 0.893987 val loss: 0.402080
[Epoch 53] ogbg-molbace: 0.880948 test loss: 0.387917
[Epoch 54; Iter    17/   41] train: loss: 0.3010835
[Epoch 54] ogbg-molbace: 0.891526 val loss: 0.395928
[Epoch 54] ogbg-molbace: 0.872871 test loss: 0.384130
[Epoch 55; Iter     6/   41] train: loss: 0.2345859
[Epoch 55; Iter    36/   41] train: loss: 0.4679438
[Epoch 55] ogbg-molbace: 0.870781 val loss: 0.428116
[Epoch 55] ogbg-molbace: 0.861282 test loss: 0.467826
[Epoch 56; Iter    25/   41] train: loss: 0.4963823
[Epoch 56] ogbg-molbace: 0.904008 val loss: 0.377880
[Epoch 56] ogbg-molbace: 0.894118 test loss: 0.433127
[Epoch 57; Iter    14/   41] train: loss: 0.3056899
[Epoch 57] ogbg-molbace: 0.918601 val loss: 0.308169
[Epoch 57] ogbg-molbace: 0.890606 test loss: 0.362990
[Epoch 58; Iter     3/   41] train: loss: 0.2927501
[Epoch 58; Iter    33/   41] train: loss: 0.3325738
[Epoch 58] ogbg-molbace: 0.911392 val loss: 0.440203
[Epoch 58] ogbg-molbace: 0.859350 test loss: 0.446872
[Epoch 59; Iter    22/   41] train: loss: 0.2868420
[Epoch 59] ogbg-molbace: 0.881329 val loss: 0.577540
[Epoch 59] ogbg-molbace: 0.883758 test loss: 0.381480
[Epoch 60; Iter    11/   41] train: loss: 0.2463408
[Epoch 60; Iter    41/   41] train: loss: 0.1372638
[Epoch 60] ogbg-molbace: 0.897152 val loss: 0.437822
[Epoch 60] ogbg-molbace: 0.872520 test loss: 0.399426
[Epoch 61; Iter    30/   41] train: loss: 0.4887757
[Epoch 61] ogbg-molbace: 0.911920 val loss: 0.348433
[Epoch 61] ogbg-molbace: 0.876734 test loss: 0.401436
[Epoch 62; Iter    19/   41] train: loss: 0.1792764
[Epoch 62] ogbg-molbace: 0.923347 val loss: 0.311045
[Epoch 62] ogbg-molbace: 0.876207 test loss: 0.427065
[Epoch 63; Iter     8/   41] train: loss: 0.2335005
[Epoch 63; Iter    38/   41] train: loss: 0.1958847
[Epoch 63] ogbg-molbace: 0.883439 val loss: 0.583860
[Epoch 63] ogbg-molbace: 0.859877 test loss: 0.476191
[Epoch 64; Iter    27/   41] train: loss: 0.1891069
[Epoch 64] ogbg-molbace: 0.921238 val loss: 0.322219
[Epoch 64] ogbg-molbace: 0.895698 test loss: 0.370138
[Epoch 65; Iter    16/   41] train: loss: 0.0968837
[Epoch 65] ogbg-molbace: 0.895921 val loss: 0.417960
[Epoch 65] ogbg-molbace: 0.871115 test loss: 0.461068
[Epoch 66; Iter     5/   41] train: loss: 0.1165722
[Epoch 66; Iter    35/   41] train: loss: 0.1587439
[Epoch 66] ogbg-molbace: 0.909459 val loss: 0.364858
[Epoch 66] ogbg-molbace: 0.875680 test loss: 0.448876
[Epoch 67; Iter    24/   41] train: loss: 0.2717135
[Epoch 67] ogbg-molbace: 0.932489 val loss: 0.298843
[Epoch 67] ogbg-molbace: 0.910097 test loss: 0.347464
[Epoch 68; Iter    13/   41] train: loss: 0.1631311
[Epoch 68] ogbg-molbace: 0.925457 val loss: 0.310520
[Epoch 68] ogbg-molbace: 0.904302 test loss: 0.366866
[Epoch 69; Iter     2/   41] train: loss: 0.1920003
[Epoch 69; Iter    32/   41] train: loss: 0.1641785
[Epoch 69] ogbg-molbace: 0.915436 val loss: 0.360832
[Epoch 69] ogbg-molbace: 0.881826 test loss: 0.418056
[Epoch 70; Iter    21/   41] train: loss: 0.2388997
[Epoch 70] ogbg-molbace: 0.927567 val loss: 0.311700
[Epoch 70] ogbg-molbace: 0.890079 test loss: 0.392966
[Epoch 71; Iter    10/   41] train: loss: 0.1090387
[Epoch 71; Iter    40/   41] train: loss: 0.1086253
[Epoch 71] ogbg-molbace: 0.887131 val loss: 0.553051
[Epoch 71] ogbg-molbace: 0.871817 test loss: 0.457719
[Epoch 72; Iter    29/   41] train: loss: 0.2155780
[Epoch 72] ogbg-molbace: 0.910513 val loss: 0.404217
[Epoch 72] ogbg-molbace: 0.906234 test loss: 0.389847
[Epoch 73; Iter    18/   41] train: loss: 0.1685988
[Epoch 73] ogbg-molbace: 0.906118 val loss: 0.368294
[Epoch 73] ogbg-molbace: 0.887972 test loss: 0.415265
[Epoch 74; Iter     7/   41] train: loss: 0.1020255
[Epoch 74; Iter    37/   41] train: loss: 0.1156765
[Epoch 74] ogbg-molbace: 0.891350 val loss: 0.458417
[Epoch 74] ogbg-molbace: 0.898332 test loss: 0.405487
[Epoch 75; Iter    26/   41] train: loss: 0.2330451
[Epoch 75] ogbg-molbace: 0.902426 val loss: 0.475387
[Epoch 75] ogbg-molbace: 0.873749 test loss: 0.500285
[Epoch 76; Iter    15/   41] train: loss: 0.0511124
[Epoch 76] ogbg-molbace: 0.899086 val loss: 0.435721
[Epoch 76] ogbg-molbace: 0.867076 test loss: 0.492180
[Epoch 77; Iter     4/   41] train: loss: 0.2251767
[Epoch 77; Iter    34/   41] train: loss: 0.2986716
[Epoch 77] ogbg-molbace: 0.890823 val loss: 0.453926
[Epoch 77] ogbg-molbace: 0.874978 test loss: 0.442550
[Epoch 78; Iter    23/   41] train: loss: 0.1402407
[Epoch 36; Iter    25/   31] train: loss: 0.4116265
[Epoch 36] ogbg-molbace: 0.867187 val loss: 0.460918
[Epoch 36] ogbg-molbace: 0.818515 test loss: 0.493843
[Epoch 37; Iter    24/   31] train: loss: 0.5531465
[Epoch 37] ogbg-molbace: 0.881177 val loss: 0.450894
[Epoch 37] ogbg-molbace: 0.834264 test loss: 0.480924
[Epoch 38; Iter    23/   31] train: loss: 0.4435232
[Epoch 38] ogbg-molbace: 0.861044 val loss: 0.477844
[Epoch 38] ogbg-molbace: 0.795393 test loss: 0.522338
[Epoch 39; Iter    22/   31] train: loss: 0.5743099
[Epoch 39] ogbg-molbace: 0.881177 val loss: 0.431245
[Epoch 39] ogbg-molbace: 0.835704 test loss: 0.470041
[Epoch 40; Iter    21/   31] train: loss: 0.4746006
[Epoch 40] ogbg-molbace: 0.867949 val loss: 0.468029
[Epoch 40] ogbg-molbace: 0.818166 test loss: 0.503640
[Epoch 41; Iter    20/   31] train: loss: 0.4311505
[Epoch 41] ogbg-molbace: 0.878710 val loss: 0.469771
[Epoch 41] ogbg-molbace: 0.839019 test loss: 0.493962
[Epoch 42; Iter    19/   31] train: loss: 0.5084162
[Epoch 42] ogbg-molbace: 0.876065 val loss: 0.441992
[Epoch 42] ogbg-molbace: 0.844385 test loss: 0.470365
[Epoch 43; Iter    18/   31] train: loss: 0.5899318
[Epoch 43] ogbg-molbace: 0.882522 val loss: 0.455577
[Epoch 43] ogbg-molbace: 0.845083 test loss: 0.474999
[Epoch 44; Iter    17/   31] train: loss: 0.4715512
[Epoch 44] ogbg-molbace: 0.874002 val loss: 0.457446
[Epoch 44] ogbg-molbace: 0.820827 test loss: 0.486345
[Epoch 45; Iter    16/   31] train: loss: 0.5100315
[Epoch 45] ogbg-molbace: 0.872119 val loss: 0.468074
[Epoch 45] ogbg-molbace: 0.855466 test loss: 0.463638
[Epoch 46; Iter    15/   31] train: loss: 0.4812731
[Epoch 46] ogbg-molbace: 0.829298 val loss: 0.490434
[Epoch 46] ogbg-molbace: 0.798272 test loss: 0.514042
[Epoch 47; Iter    14/   31] train: loss: 0.5657961
[Epoch 47] ogbg-molbace: 0.864945 val loss: 0.430147
[Epoch 47] ogbg-molbace: 0.856339 test loss: 0.441195
[Epoch 48; Iter    13/   31] train: loss: 0.3723240
[Epoch 48] ogbg-molbace: 0.855035 val loss: 0.457719
[Epoch 48] ogbg-molbace: 0.821351 test loss: 0.510406
[Epoch 49; Iter    12/   31] train: loss: 0.4331347
[Epoch 49] ogbg-molbace: 0.810196 val loss: 0.618458
[Epoch 49] ogbg-molbace: 0.761583 test loss: 0.677840
[Epoch 50; Iter    11/   31] train: loss: 0.2892031
[Epoch 50] ogbg-molbace: 0.867501 val loss: 0.464649
[Epoch 50] ogbg-molbace: 0.827415 test loss: 0.507807
[Epoch 51; Iter    10/   31] train: loss: 0.3013019
[Epoch 51] ogbg-molbace: 0.867904 val loss: 0.444950
[Epoch 51] ogbg-molbace: 0.829552 test loss: 0.490721
[Epoch 52; Iter     9/   31] train: loss: 0.4083776
[Epoch 52] ogbg-molbace: 0.806206 val loss: 0.632174
[Epoch 52] ogbg-molbace: 0.757046 test loss: 0.652715
[Epoch 53; Iter     8/   31] train: loss: 0.5985914
[Epoch 53] ogbg-molbace: 0.876244 val loss: 0.446562
[Epoch 53] ogbg-molbace: 0.867594 test loss: 0.445914
[Epoch 54; Iter     7/   31] train: loss: 0.5866606
[Epoch 54] ogbg-molbace: 0.883598 val loss: 0.479993
[Epoch 54] ogbg-molbace: 0.862359 test loss: 0.497777
[Epoch 55; Iter     6/   31] train: loss: 0.3325894
[Epoch 55] ogbg-molbace: 0.859116 val loss: 0.489201
[Epoch 55] ogbg-molbace: 0.830643 test loss: 0.517674
[Epoch 56; Iter     5/   31] train: loss: 0.4056828
[Epoch 56] ogbg-molbace: 0.907587 val loss: 0.375452
[Epoch 56] ogbg-molbace: 0.873702 test loss: 0.426927
[Epoch 57; Iter     4/   31] train: loss: 0.3399787
[Epoch 57] ogbg-molbace: 0.876155 val loss: 0.621795
[Epoch 57] ogbg-molbace: 0.854681 test loss: 0.668052
[Epoch 58; Iter     3/   31] train: loss: 0.3281233
[Epoch 58] ogbg-molbace: 0.893821 val loss: 0.436517
[Epoch 58] ogbg-molbace: 0.884958 test loss: 0.427979
[Epoch 59; Iter     2/   31] train: loss: 0.3542591
[Epoch 59] ogbg-molbace: 0.892072 val loss: 0.415151
[Epoch 59] ogbg-molbace: 0.850188 test loss: 0.474718
[Epoch 60; Iter     1/   31] train: loss: 0.4244000
[Epoch 60; Iter    31/   31] train: loss: 0.2458390
[Epoch 60] ogbg-molbace: 0.872837 val loss: 0.438180
[Epoch 60] ogbg-molbace: 0.843513 test loss: 0.476664
[Epoch 61; Iter    30/   31] train: loss: 0.2615803
[Epoch 61] ogbg-molbace: 0.891400 val loss: 0.400397
[Epoch 61] ogbg-molbace: 0.872262 test loss: 0.435806
[Epoch 62; Iter    29/   31] train: loss: 0.3612577
[Epoch 62] ogbg-molbace: 0.902789 val loss: 0.396345
[Epoch 62] ogbg-molbace: 0.880421 test loss: 0.411785
[Epoch 63; Iter    28/   31] train: loss: 0.1783224
[Epoch 63] ogbg-molbace: 0.878710 val loss: 0.457368
[Epoch 63] ogbg-molbace: 0.850755 test loss: 0.470538
[Epoch 64; Iter    27/   31] train: loss: 0.2513302
[Epoch 64] ogbg-molbace: 0.901399 val loss: 0.389428
[Epoch 64] ogbg-molbace: 0.869121 test loss: 0.429641
[Epoch 65; Iter    26/   31] train: loss: 0.2093845
[Epoch 65] ogbg-molbace: 0.904583 val loss: 0.392781
[Epoch 65] ogbg-molbace: 0.879112 test loss: 0.441622
[Epoch 66; Iter    25/   31] train: loss: 0.2349897
[Epoch 66] ogbg-molbace: 0.876379 val loss: 0.445706
[Epoch 66] ogbg-molbace: 0.833697 test loss: 0.519676
[Epoch 67; Iter    24/   31] train: loss: 0.2650156
[Epoch 67] ogbg-molbace: 0.886737 val loss: 0.457481
[Epoch 67] ogbg-molbace: 0.834700 test loss: 0.546783
[Epoch 68; Iter    23/   31] train: loss: 0.4117050
[Epoch 68] ogbg-molbace: 0.881715 val loss: 0.460892
[Epoch 68] ogbg-molbace: 0.869732 test loss: 0.455687
[Epoch 69; Iter    22/   31] train: loss: 0.3457304
[Epoch 69] ogbg-molbace: 0.889203 val loss: 0.425681
[Epoch 69] ogbg-molbace: 0.859349 test loss: 0.478388
[Epoch 70; Iter    21/   31] train: loss: 0.3085152
[Epoch 70] ogbg-molbace: 0.884360 val loss: 0.438649
[Epoch 70] ogbg-molbace: 0.867333 test loss: 0.458289
[Epoch 71; Iter    20/   31] train: loss: 0.3717725
[Epoch 71] ogbg-molbace: 0.894180 val loss: 0.426649
[Epoch 71] ogbg-molbace: 0.884434 test loss: 0.452439
[Epoch 72; Iter    19/   31] train: loss: 0.2627496
[Epoch 72] ogbg-molbace: 0.899157 val loss: 0.408070
[Epoch 72] ogbg-molbace: 0.867813 test loss: 0.448916
[Epoch 73; Iter    18/   31] train: loss: 0.5207816
[Epoch 73] ogbg-molbace: 0.886423 val loss: 0.458896
[Epoch 73] ogbg-molbace: 0.866286 test loss: 0.495750
[Epoch 74; Iter    17/   31] train: loss: 0.4076872
[Epoch 74] ogbg-molbace: 0.886871 val loss: 0.429823
[Epoch 74] ogbg-molbace: 0.847177 test loss: 0.544613
[Epoch 75; Iter    16/   31] train: loss: 0.3322789
[Epoch 75] ogbg-molbace: 0.898933 val loss: 0.400262
[Epoch 75] ogbg-molbace: 0.863188 test loss: 0.496726
[Epoch 76; Iter    15/   31] train: loss: 0.2403093
[Epoch 76] ogbg-molbace: 0.905121 val loss: 0.453897
[Epoch 76] ogbg-molbace: 0.881031 test loss: 0.482953
[Epoch 77; Iter    14/   31] train: loss: 0.3523069
[Epoch 77] ogbg-molbace: 0.894718 val loss: 0.435365
[Epoch 77] ogbg-molbace: 0.881293 test loss: 0.495629
[Epoch 78; Iter    13/   31] train: loss: 0.2132927
[Epoch 78] ogbg-molbace: 0.888127 val loss: 0.435935
[Epoch 78] ogbg-molbace: 0.867158 test loss: 0.489341
[Epoch 79; Iter    12/   31] train: loss: 0.1537022
[Epoch 79] ogbg-molbace: 0.863914 val loss: 0.498853
[Epoch 79] ogbg-molbace: 0.863581 test loss: 0.547449
[Epoch 80; Iter    11/   31] train: loss: 0.2394922
[Epoch 80] ogbg-molbace: 0.888710 val loss: 0.420079
[Epoch 80] ogbg-molbace: 0.856644 test loss: 0.492957
[Epoch 81; Iter    10/   31] train: loss: 0.2718564
[Epoch 81] ogbg-molbace: 0.888037 val loss: 0.448020
[Epoch 81] ogbg-molbace: 0.861138 test loss: 0.482634
[Epoch 82; Iter     9/   31] train: loss: 0.2699023
[Epoch 82] ogbg-molbace: 0.904538 val loss: 0.392576
[Epoch 82] ogbg-molbace: 0.880421 test loss: 0.437931
[Epoch 83; Iter     8/   31] train: loss: 0.1872571
[Epoch 83] ogbg-molbace: 0.909380 val loss: 0.392688
[Epoch 83] ogbg-molbace: 0.878283 test loss: 0.469394
[Epoch 84; Iter     7/   31] train: loss: 0.0666515
[Epoch 84] ogbg-molbace: 0.896198 val loss: 0.443637
[Epoch 84] ogbg-molbace: 0.869034 test loss: 0.508637
[Epoch 85; Iter     6/   31] train: loss: 0.1131585
[Epoch 85] ogbg-molbace: 0.901175 val loss: 0.410786
[Epoch 85] ogbg-molbace: 0.869732 test loss: 0.508385
[Epoch 86; Iter     5/   31] train: loss: 0.1239629
[Epoch 86] ogbg-molbace: 0.908259 val loss: 0.470485
[Epoch 86] ogbg-molbace: 0.882515 test loss: 0.525500
[Epoch 32] ogbg-molbace: 0.805092 test loss: 0.475553
[Epoch 33; Iter     8/   41] train: loss: 0.4865102
[Epoch 33; Iter    38/   41] train: loss: 0.4539919
[Epoch 33] ogbg-molbace: 0.840717 val loss: 0.480985
[Epoch 33] ogbg-molbace: 0.821949 test loss: 0.456306
[Epoch 34; Iter    27/   41] train: loss: 0.4526490
[Epoch 34] ogbg-molbace: 0.841421 val loss: 0.424877
[Epoch 34] ogbg-molbace: 0.832836 test loss: 0.440292
[Epoch 35; Iter    16/   41] train: loss: 0.5153417
[Epoch 35] ogbg-molbace: 0.893987 val loss: 0.371283
[Epoch 35] ogbg-molbace: 0.851975 test loss: 0.426343
[Epoch 36; Iter     5/   41] train: loss: 0.4669448
[Epoch 36; Iter    35/   41] train: loss: 0.5080486
[Epoch 36] ogbg-molbace: 0.848101 val loss: 0.443204
[Epoch 36] ogbg-molbace: 0.802809 test loss: 0.454325
[Epoch 37; Iter    24/   41] train: loss: 0.3310916
[Epoch 37] ogbg-molbace: 0.878340 val loss: 0.387364
[Epoch 37] ogbg-molbace: 0.839333 test loss: 0.435738
[Epoch 38; Iter    13/   41] train: loss: 0.4988645
[Epoch 38] ogbg-molbace: 0.862693 val loss: 0.444269
[Epoch 38] ogbg-molbace: 0.873398 test loss: 0.381303
[Epoch 39; Iter     2/   41] train: loss: 0.5040306
[Epoch 39; Iter    32/   41] train: loss: 0.4788651
[Epoch 39] ogbg-molbace: 0.889416 val loss: 0.364912
[Epoch 39] ogbg-molbace: 0.893942 test loss: 0.382834
[Epoch 40; Iter    21/   41] train: loss: 0.3528484
[Epoch 40] ogbg-molbace: 0.853551 val loss: 0.486559
[Epoch 40] ogbg-molbace: 0.857419 test loss: 0.430701
[Epoch 41; Iter    10/   41] train: loss: 0.4493119
[Epoch 41; Iter    40/   41] train: loss: 0.2737047
[Epoch 41] ogbg-molbace: 0.917546 val loss: 0.333203
[Epoch 41] ogbg-molbace: 0.842493 test loss: 0.470154
[Epoch 42; Iter    29/   41] train: loss: 0.4878139
[Epoch 42] ogbg-molbace: 0.901371 val loss: 0.401396
[Epoch 42] ogbg-molbace: 0.818086 test loss: 0.476563
[Epoch 43; Iter    18/   41] train: loss: 0.3428016
[Epoch 43] ogbg-molbace: 0.899789 val loss: 0.365323
[Epoch 43] ogbg-molbace: 0.850395 test loss: 0.479059
[Epoch 44; Iter     7/   41] train: loss: 0.3870002
[Epoch 44; Iter    37/   41] train: loss: 0.4340356
[Epoch 44] ogbg-molbace: 0.889241 val loss: 0.365696
[Epoch 44] ogbg-molbace: 0.853029 test loss: 0.429927
[Epoch 45; Iter    26/   41] train: loss: 0.3022391
[Epoch 45] ogbg-molbace: 0.902075 val loss: 0.379873
[Epoch 45] ogbg-molbace: 0.882529 test loss: 0.369573
[Epoch 46; Iter    15/   41] train: loss: 0.4685600
[Epoch 46] ogbg-molbace: 0.905063 val loss: 0.342669
[Epoch 46] ogbg-molbace: 0.882880 test loss: 0.364334
[Epoch 47; Iter     4/   41] train: loss: 0.2907274
[Epoch 47; Iter    34/   41] train: loss: 0.2713819
[Epoch 47] ogbg-molbace: 0.908404 val loss: 0.370156
[Epoch 47] ogbg-molbace: 0.878665 test loss: 0.368383
[Epoch 48; Iter    23/   41] train: loss: 0.3153712
[Epoch 48] ogbg-molbace: 0.908579 val loss: 0.353206
[Epoch 48] ogbg-molbace: 0.892186 test loss: 0.357095
[Epoch 49; Iter    12/   41] train: loss: 0.2324173
[Epoch 49] ogbg-molbace: 0.893987 val loss: 0.383180
[Epoch 49] ogbg-molbace: 0.870413 test loss: 0.402813
[Epoch 50; Iter     1/   41] train: loss: 0.3491670
[Epoch 50; Iter    31/   41] train: loss: 0.5149021
[Epoch 50] ogbg-molbace: 0.875879 val loss: 0.518799
[Epoch 50] ogbg-molbace: 0.841440 test loss: 0.422775
[Epoch 51; Iter    20/   41] train: loss: 0.4442161
[Epoch 51] ogbg-molbace: 0.896976 val loss: 0.399060
[Epoch 51] ogbg-molbace: 0.858648 test loss: 0.410327
[Epoch 52; Iter     9/   41] train: loss: 0.2126756
[Epoch 52; Iter    39/   41] train: loss: 0.4110476
[Epoch 52] ogbg-molbace: 0.905591 val loss: 0.334228
[Epoch 52] ogbg-molbace: 0.844074 test loss: 0.437714
[Epoch 53; Iter    28/   41] train: loss: 0.2500544
[Epoch 53] ogbg-molbace: 0.921589 val loss: 0.315766
[Epoch 53] ogbg-molbace: 0.884460 test loss: 0.408729
[Epoch 54; Iter    17/   41] train: loss: 0.3102638
[Epoch 54] ogbg-molbace: 0.884669 val loss: 0.446824
[Epoch 54] ogbg-molbace: 0.847937 test loss: 0.588538
[Epoch 55; Iter     6/   41] train: loss: 0.1690230
[Epoch 55; Iter    36/   41] train: loss: 0.4514089
[Epoch 55] ogbg-molbace: 0.867968 val loss: 0.435259
[Epoch 55] ogbg-molbace: 0.879192 test loss: 0.374694
[Epoch 56; Iter    25/   41] train: loss: 0.3514342
[Epoch 56] ogbg-molbace: 0.901371 val loss: 0.402652
[Epoch 56] ogbg-molbace: 0.855136 test loss: 0.431394
[Epoch 57; Iter    14/   41] train: loss: 0.1938860
[Epoch 57] ogbg-molbace: 0.906646 val loss: 0.351075
[Epoch 57] ogbg-molbace: 0.848990 test loss: 0.533245
[Epoch 58; Iter     3/   41] train: loss: 0.3784348
[Epoch 58; Iter    33/   41] train: loss: 0.3216036
[Epoch 58] ogbg-molbace: 0.911744 val loss: 0.359131
[Epoch 58] ogbg-molbace: 0.869359 test loss: 0.610916
[Epoch 59; Iter    22/   41] train: loss: 0.2952197
[Epoch 59] ogbg-molbace: 0.905063 val loss: 0.334693
[Epoch 59] ogbg-molbace: 0.858121 test loss: 0.441424
[Epoch 60; Iter    11/   41] train: loss: 0.2883817
[Epoch 60; Iter    41/   41] train: loss: 0.2883339
[Epoch 60] ogbg-molbace: 0.928622 val loss: 0.285553
[Epoch 60] ogbg-molbace: 0.874802 test loss: 0.395833
[Epoch 61; Iter    30/   41] train: loss: 0.3312079
[Epoch 61] ogbg-molbace: 0.911041 val loss: 0.351176
[Epoch 61] ogbg-molbace: 0.888850 test loss: 0.388272
[Epoch 62; Iter    19/   41] train: loss: 0.1615757
[Epoch 62] ogbg-molbace: 0.929325 val loss: 0.290892
[Epoch 62] ogbg-molbace: 0.887270 test loss: 0.408542
[Epoch 63; Iter     8/   41] train: loss: 0.3101824
[Epoch 63; Iter    38/   41] train: loss: 0.2926385
[Epoch 63] ogbg-molbace: 0.914557 val loss: 0.407550
[Epoch 63] ogbg-molbace: 0.872520 test loss: 0.416890
[Epoch 64; Iter    27/   41] train: loss: 0.2246725
[Epoch 64] ogbg-molbace: 0.912096 val loss: 0.353049
[Epoch 64] ogbg-molbace: 0.869710 test loss: 0.516413
[Epoch 65; Iter    16/   41] train: loss: 0.2223614
[Epoch 65] ogbg-molbace: 0.893987 val loss: 0.386417
[Epoch 65] ogbg-molbace: 0.865145 test loss: 0.421514
[Epoch 66; Iter     5/   41] train: loss: 0.1592559
[Epoch 66; Iter    35/   41] train: loss: 0.1703745
[Epoch 66] ogbg-molbace: 0.897679 val loss: 0.508560
[Epoch 66] ogbg-molbace: 0.878665 test loss: 0.415497
[Epoch 67; Iter    24/   41] train: loss: 0.3191651
[Epoch 67] ogbg-molbace: 0.882208 val loss: 0.484709
[Epoch 67] ogbg-molbace: 0.840737 test loss: 0.516959
[Epoch 68; Iter    13/   41] train: loss: 0.6401398
[Epoch 68] ogbg-molbace: 0.907700 val loss: 0.345006
[Epoch 68] ogbg-molbace: 0.874276 test loss: 0.436135
[Epoch 69; Iter     2/   41] train: loss: 0.1600977
[Epoch 69; Iter    32/   41] train: loss: 0.2911015
[Epoch 69] ogbg-molbace: 0.883790 val loss: 0.525777
[Epoch 69] ogbg-molbace: 0.876734 test loss: 0.435376
[Epoch 70; Iter    21/   41] train: loss: 0.2586054
[Epoch 70] ogbg-molbace: 0.848805 val loss: 0.537229
[Epoch 70] ogbg-molbace: 0.852678 test loss: 0.511195
[Epoch 71; Iter    10/   41] train: loss: 0.2191928
[Epoch 71; Iter    40/   41] train: loss: 0.2575508
[Epoch 71] ogbg-molbace: 0.889944 val loss: 0.514507
[Epoch 71] ogbg-molbace: 0.862862 test loss: 0.737943
[Epoch 72; Iter    29/   41] train: loss: 0.1831648
[Epoch 72] ogbg-molbace: 0.897855 val loss: 0.377950
[Epoch 72] ogbg-molbace: 0.903600 test loss: 0.345231
[Epoch 73; Iter    18/   41] train: loss: 0.1517669
[Epoch 73] ogbg-molbace: 0.901723 val loss: 0.504807
[Epoch 73] ogbg-molbace: 0.863038 test loss: 0.523855
[Epoch 74; Iter     7/   41] train: loss: 0.2907608
[Epoch 74; Iter    37/   41] train: loss: 0.1615199
[Epoch 74] ogbg-molbace: 0.893460 val loss: 0.448256
[Epoch 74] ogbg-molbace: 0.865145 test loss: 0.538989
[Epoch 75; Iter    26/   41] train: loss: 0.1533170
[Epoch 75] ogbg-molbace: 0.921062 val loss: 0.402932
[Epoch 75] ogbg-molbace: 0.895698 test loss: 0.381208
[Epoch 76; Iter    15/   41] train: loss: 0.1133691
[Epoch 76] ogbg-molbace: 0.900668 val loss: 0.546545
[Epoch 76] ogbg-molbace: 0.898859 test loss: 0.383404
[Epoch 77; Iter     4/   41] train: loss: 0.2327936
[Epoch 77; Iter    34/   41] train: loss: 0.1650822
[Epoch 77] ogbg-molbace: 0.886428 val loss: 0.407807
[Epoch 77] ogbg-molbace: 0.853380 test loss: 0.481796
[Epoch 78; Iter    23/   41] train: loss: 0.2503054
[Epoch 34] ogbg-molbace: 0.798532 val loss: 0.554225
[Epoch 34] ogbg-molbace: 0.799254 test loss: 0.541184
[Epoch 35; Iter     6/   36] train: loss: 0.5276458
[Epoch 35; Iter    36/   36] train: loss: 0.7212344
[Epoch 35] ogbg-molbace: 0.812579 val loss: 0.530800
[Epoch 35] ogbg-molbace: 0.813248 test loss: 0.527938
[Epoch 36; Iter    30/   36] train: loss: 0.5552096
[Epoch 36] ogbg-molbace: 0.831439 val loss: 0.507846
[Epoch 36] ogbg-molbace: 0.844114 test loss: 0.488829
[Epoch 37; Iter    24/   36] train: loss: 0.5260703
[Epoch 37] ogbg-molbace: 0.833570 val loss: 0.529467
[Epoch 37] ogbg-molbace: 0.817213 test loss: 0.541372
[Epoch 38; Iter    18/   36] train: loss: 0.5011304
[Epoch 38] ogbg-molbace: 0.826073 val loss: 0.527502
[Epoch 38] ogbg-molbace: 0.836340 test loss: 0.515874
[Epoch 39; Iter    12/   36] train: loss: 0.4779937
[Epoch 39] ogbg-molbace: 0.856297 val loss: 0.493442
[Epoch 39] ogbg-molbace: 0.839527 test loss: 0.489423
[Epoch 40; Iter     6/   36] train: loss: 0.3992153
[Epoch 40; Iter    36/   36] train: loss: 0.2727561
[Epoch 40] ogbg-molbace: 0.809107 val loss: 0.536875
[Epoch 40] ogbg-molbace: 0.834629 test loss: 0.544895
[Epoch 41; Iter    30/   36] train: loss: 0.3366317
[Epoch 41] ogbg-molbace: 0.859533 val loss: 0.555738
[Epoch 41] ogbg-molbace: 0.852200 test loss: 0.563763
[Epoch 42; Iter    24/   36] train: loss: 0.3153050
[Epoch 42] ogbg-molbace: 0.671954 val loss: 1.488592
[Epoch 42] ogbg-molbace: 0.735033 test loss: 0.967041
[Epoch 43; Iter    18/   36] train: loss: 0.5547239
[Epoch 43] ogbg-molbace: 0.806345 val loss: 0.600366
[Epoch 43] ogbg-molbace: 0.799642 test loss: 0.564986
[Epoch 44; Iter    12/   36] train: loss: 0.4448559
[Epoch 44] ogbg-molbace: 0.836648 val loss: 0.638816
[Epoch 44] ogbg-molbace: 0.863474 test loss: 0.612154
[Epoch 45; Iter     6/   36] train: loss: 0.4149758
[Epoch 45; Iter    36/   36] train: loss: 0.3390607
[Epoch 45] ogbg-molbace: 0.881550 val loss: 0.455456
[Epoch 45] ogbg-molbace: 0.891852 test loss: 0.433321
[Epoch 46; Iter    30/   36] train: loss: 0.3615477
[Epoch 46] ogbg-molbace: 0.848801 val loss: 0.587981
[Epoch 46] ogbg-molbace: 0.848080 test loss: 0.587455
[Epoch 47; Iter    24/   36] train: loss: 0.5930292
[Epoch 47] ogbg-molbace: 0.835306 val loss: 0.559371
[Epoch 47] ogbg-molbace: 0.828254 test loss: 0.518459
[Epoch 48; Iter    18/   36] train: loss: 0.4097090
[Epoch 48] ogbg-molbace: 0.884233 val loss: 0.430969
[Epoch 48] ogbg-molbace: 0.863318 test loss: 0.467445
[Epoch 49; Iter    12/   36] train: loss: 0.4519992
[Epoch 49] ogbg-molbace: 0.864662 val loss: 0.470225
[Epoch 49] ogbg-molbace: 0.860675 test loss: 0.493404
[Epoch 50; Iter     6/   36] train: loss: 0.3548775
[Epoch 50; Iter    36/   36] train: loss: 0.5678552
[Epoch 50] ogbg-molbace: 0.834596 val loss: 0.616538
[Epoch 50] ogbg-molbace: 0.857643 test loss: 0.636475
[Epoch 51; Iter    30/   36] train: loss: 0.4495595
[Epoch 51] ogbg-molbace: 0.843908 val loss: 0.484326
[Epoch 51] ogbg-molbace: 0.863785 test loss: 0.470334
[Epoch 52; Iter    24/   36] train: loss: 0.3401994
[Epoch 52] ogbg-molbace: 0.847932 val loss: 0.592301
[Epoch 52] ogbg-molbace: 0.866506 test loss: 0.557753
[Epoch 53; Iter    18/   36] train: loss: 0.3160614
[Epoch 53] ogbg-molbace: 0.860480 val loss: 0.474834
[Epoch 53] ogbg-molbace: 0.865107 test loss: 0.481378
[Epoch 54; Iter    12/   36] train: loss: 0.2604278
[Epoch 54] ogbg-molbace: 0.888968 val loss: 0.426833
[Epoch 54] ogbg-molbace: 0.861686 test loss: 0.482519
[Epoch 55; Iter     6/   36] train: loss: 0.2294158
[Epoch 55; Iter    36/   36] train: loss: 0.8348526
[Epoch 55] ogbg-molbace: 0.857165 val loss: 0.492566
[Epoch 55] ogbg-molbace: 0.860364 test loss: 0.491109
[Epoch 56; Iter    30/   36] train: loss: 0.4129144
[Epoch 56] ogbg-molbace: 0.855035 val loss: 0.542757
[Epoch 56] ogbg-molbace: 0.863085 test loss: 0.530379
[Epoch 57; Iter    24/   36] train: loss: 0.3604999
[Epoch 57] ogbg-molbace: 0.867977 val loss: 0.466003
[Epoch 57] ogbg-molbace: 0.857409 test loss: 0.500120
[Epoch 58; Iter    18/   36] train: loss: 0.2917186
[Epoch 58] ogbg-molbace: 0.864189 val loss: 0.489384
[Epoch 58] ogbg-molbace: 0.857021 test loss: 0.495976
[Epoch 59; Iter    12/   36] train: loss: 0.3389177
[Epoch 59] ogbg-molbace: 0.879104 val loss: 0.459605
[Epoch 59] ogbg-molbace: 0.890686 test loss: 0.466989
[Epoch 60; Iter     6/   36] train: loss: 0.2213778
[Epoch 60; Iter    36/   36] train: loss: 0.3670094
[Epoch 60] ogbg-molbace: 0.894413 val loss: 0.435292
[Epoch 60] ogbg-molbace: 0.875369 test loss: 0.492398
[Epoch 61; Iter    30/   36] train: loss: 0.5508735
[Epoch 61] ogbg-molbace: 0.878157 val loss: 0.451949
[Epoch 61] ogbg-molbace: 0.885243 test loss: 0.448731
[Epoch 62; Iter    24/   36] train: loss: 0.3125767
[Epoch 62] ogbg-molbace: 0.881076 val loss: 0.471261
[Epoch 62] ogbg-molbace: 0.900638 test loss: 0.423566
[Epoch 63; Iter    18/   36] train: loss: 0.3425470
[Epoch 63] ogbg-molbace: 0.860401 val loss: 0.537721
[Epoch 63] ogbg-molbace: 0.870549 test loss: 0.524198
[Epoch 64; Iter    12/   36] train: loss: 0.4219835
[Epoch 64] ogbg-molbace: 0.869634 val loss: 0.522500
[Epoch 64] ogbg-molbace: 0.865107 test loss: 0.563113
[Epoch 65; Iter     6/   36] train: loss: 0.4306320
[Epoch 65; Iter    36/   36] train: loss: 0.1203618
[Epoch 65] ogbg-molbace: 0.856850 val loss: 0.574861
[Epoch 65] ogbg-molbace: 0.861064 test loss: 0.582901
[Epoch 66; Iter    30/   36] train: loss: 0.4165122
[Epoch 66] ogbg-molbace: 0.861190 val loss: 0.521517
[Epoch 66] ogbg-molbace: 0.863474 test loss: 0.543281
[Epoch 67; Iter    24/   36] train: loss: 0.3721239
[Epoch 67] ogbg-molbace: 0.866635 val loss: 0.595471
[Epoch 67] ogbg-molbace: 0.859198 test loss: 0.580159
[Epoch 68; Iter    18/   36] train: loss: 0.1660706
[Epoch 68] ogbg-molbace: 0.876973 val loss: 0.512417
[Epoch 68] ogbg-molbace: 0.872182 test loss: 0.507096
[Epoch 69; Iter    12/   36] train: loss: 0.2636407
[Epoch 69] ogbg-molbace: 0.860795 val loss: 0.602826
[Epoch 69] ogbg-molbace: 0.857176 test loss: 0.603602
[Epoch 70; Iter     6/   36] train: loss: 0.1643548
[Epoch 70; Iter    36/   36] train: loss: 0.8370624
[Epoch 70] ogbg-molbace: 0.866714 val loss: 0.511304
[Epoch 70] ogbg-molbace: 0.854921 test loss: 0.543665
[Epoch 71; Iter    30/   36] train: loss: 0.1597376
[Epoch 71] ogbg-molbace: 0.878472 val loss: 0.464148
[Epoch 71] ogbg-molbace: 0.868216 test loss: 0.532004
[Epoch 72; Iter    24/   36] train: loss: 0.1693345
[Epoch 72] ogbg-molbace: 0.894097 val loss: 0.534670
[Epoch 72] ogbg-molbace: 0.877624 test loss: 0.545784
[Epoch 73; Iter    18/   36] train: loss: 0.2433201
[Epoch 73] ogbg-molbace: 0.865136 val loss: 0.529689
[Epoch 73] ogbg-molbace: 0.859198 test loss: 0.539650
[Epoch 74; Iter    12/   36] train: loss: 0.3684692
[Epoch 74] ogbg-molbace: 0.878788 val loss: 0.572701
[Epoch 74] ogbg-molbace: 0.875136 test loss: 0.532209
[Epoch 75; Iter     6/   36] train: loss: 0.2685134
[Epoch 75; Iter    36/   36] train: loss: 0.6930370
[Epoch 75] ogbg-molbace: 0.869871 val loss: 0.749611
[Epoch 75] ogbg-molbace: 0.859975 test loss: 0.720122
[Epoch 76; Iter    30/   36] train: loss: 0.4292569
[Epoch 76] ogbg-molbace: 0.886206 val loss: 0.471256
[Epoch 76] ogbg-molbace: 0.899083 test loss: 0.425166
[Epoch 77; Iter    24/   36] train: loss: 0.1088582
[Epoch 77] ogbg-molbace: 0.880524 val loss: 0.507561
[Epoch 77] ogbg-molbace: 0.890064 test loss: 0.487583
[Epoch 78; Iter    18/   36] train: loss: 0.0853462
[Epoch 78] ogbg-molbace: 0.878867 val loss: 0.545709
[Epoch 78] ogbg-molbace: 0.885710 test loss: 0.527570
[Epoch 79; Iter    12/   36] train: loss: 0.1022218
[Epoch 79] ogbg-molbace: 0.876184 val loss: 0.574824
[Epoch 79] ogbg-molbace: 0.891619 test loss: 0.502312
[Epoch 80; Iter     6/   36] train: loss: 0.0919547
[Epoch 80; Iter    36/   36] train: loss: 0.2044980
[Epoch 80] ogbg-molbace: 0.875789 val loss: 0.567168
[Epoch 80] ogbg-molbace: 0.879723 test loss: 0.579445
[Epoch 81; Iter    30/   36] train: loss: 0.0476302
[Epoch 81] ogbg-molbace: 0.882260 val loss: 0.570651
[Epoch 81] ogbg-molbace: 0.885477 test loss: 0.551151
[Epoch 82; Iter    24/   36] train: loss: 0.1182367
[Epoch 36; Iter    25/   31] train: loss: 0.5958213
[Epoch 36] ogbg-molbace: 0.864676 val loss: 0.474923
[Epoch 36] ogbg-molbace: 0.813803 test loss: 0.497973
[Epoch 37; Iter    24/   31] train: loss: 0.6250199
[Epoch 37] ogbg-molbace: 0.862479 val loss: 0.463535
[Epoch 37] ogbg-molbace: 0.821133 test loss: 0.490134
[Epoch 38; Iter    23/   31] train: loss: 0.5123113
[Epoch 38] ogbg-molbace: 0.878352 val loss: 0.452772
[Epoch 38] ogbg-molbace: 0.832650 test loss: 0.477643
[Epoch 39; Iter    22/   31] train: loss: 0.5052865
[Epoch 39] ogbg-molbace: 0.867097 val loss: 0.467696
[Epoch 39] ogbg-molbace: 0.815112 test loss: 0.501131
[Epoch 40; Iter    21/   31] train: loss: 0.4620247
[Epoch 40] ogbg-molbace: 0.864586 val loss: 0.465270
[Epoch 40] ogbg-molbace: 0.810008 test loss: 0.493238
[Epoch 41; Iter    20/   31] train: loss: 0.4721726
[Epoch 41] ogbg-molbace: 0.881042 val loss: 0.438420
[Epoch 41] ogbg-molbace: 0.838888 test loss: 0.470060
[Epoch 42; Iter    19/   31] train: loss: 0.4234989
[Epoch 42] ogbg-molbace: 0.886557 val loss: 0.427710
[Epoch 42] ogbg-molbace: 0.838452 test loss: 0.469254
[Epoch 43; Iter    18/   31] train: loss: 0.5666478
[Epoch 43] ogbg-molbace: 0.852390 val loss: 0.453701
[Epoch 43] ogbg-molbace: 0.827109 test loss: 0.474763
[Epoch 44; Iter    17/   31] train: loss: 0.3488364
[Epoch 44] ogbg-molbace: 0.883194 val loss: 0.424455
[Epoch 44] ogbg-molbace: 0.853198 test loss: 0.452244
[Epoch 45; Iter    16/   31] train: loss: 0.4849959
[Epoch 45] ogbg-molbace: 0.885257 val loss: 0.424238
[Epoch 45] ogbg-molbace: 0.859218 test loss: 0.439254
[Epoch 46; Iter    15/   31] train: loss: 0.4306209
[Epoch 46] ogbg-molbace: 0.872792 val loss: 0.432396
[Epoch 46] ogbg-molbace: 0.880900 test loss: 0.409529
[Epoch 47; Iter    14/   31] train: loss: 0.4102745
[Epoch 47] ogbg-molbace: 0.857771 val loss: 0.529102
[Epoch 47] ogbg-molbace: 0.851976 test loss: 0.457640
[Epoch 48; Iter    13/   31] train: loss: 0.4342208
[Epoch 48] ogbg-molbace: 0.868442 val loss: 0.479660
[Epoch 48] ogbg-molbace: 0.838060 test loss: 0.509296
[Epoch 49; Iter    12/   31] train: loss: 0.3919972
[Epoch 49] ogbg-molbace: 0.841673 val loss: 0.734853
[Epoch 49] ogbg-molbace: 0.841855 test loss: 0.737201
[Epoch 50; Iter    11/   31] train: loss: 0.3368503
[Epoch 50] ogbg-molbace: 0.858847 val loss: 0.470047
[Epoch 50] ogbg-molbace: 0.812582 test loss: 0.504903
[Epoch 51; Iter    10/   31] train: loss: 0.5299537
[Epoch 51] ogbg-molbace: 0.887678 val loss: 0.409515
[Epoch 51] ogbg-molbace: 0.882864 test loss: 0.430393
[Epoch 52; Iter     9/   31] train: loss: 0.3608868
[Epoch 52] ogbg-molbace: 0.868039 val loss: 1.054884
[Epoch 52] ogbg-molbace: 0.870561 test loss: 1.034324
[Epoch 53; Iter     8/   31] train: loss: 0.4014724
[Epoch 53] ogbg-molbace: 0.848668 val loss: 0.481575
[Epoch 53] ogbg-molbace: 0.840459 test loss: 0.475237
[Epoch 54; Iter     7/   31] train: loss: 0.2802996
[Epoch 54] ogbg-molbace: 0.847458 val loss: 0.500115
[Epoch 54] ogbg-molbace: 0.817468 test loss: 0.506557
[Epoch 55; Iter     6/   31] train: loss: 0.4677095
[Epoch 55] ogbg-molbace: 0.860326 val loss: 0.449496
[Epoch 55] ogbg-molbace: 0.868380 test loss: 0.418842
[Epoch 56; Iter     5/   31] train: loss: 0.3366136
[Epoch 56] ogbg-molbace: 0.852569 val loss: 0.599620
[Epoch 56] ogbg-molbace: 0.813454 test loss: 0.707882
[Epoch 57; Iter     4/   31] train: loss: 0.4735825
[Epoch 57] ogbg-molbace: 0.831764 val loss: 0.566372
[Epoch 57] ogbg-molbace: 0.781651 test loss: 0.605437
[Epoch 58; Iter     3/   31] train: loss: 0.4033847
[Epoch 58] ogbg-molbace: 0.845305 val loss: 0.550980
[Epoch 58] ogbg-molbace: 0.856252 test loss: 0.457181
[Epoch 59; Iter     2/   31] train: loss: 0.4082647
[Epoch 59] ogbg-molbace: 0.876424 val loss: 0.431499
[Epoch 59] ogbg-molbace: 0.877105 test loss: 0.413173
[Epoch 60; Iter     1/   31] train: loss: 0.2707137
[Epoch 60; Iter    31/   31] train: loss: 0.8260260
[Epoch 60] ogbg-molbace: 0.894942 val loss: 0.463547
[Epoch 60] ogbg-molbace: 0.867202 test loss: 0.485727
[Epoch 61; Iter    30/   31] train: loss: 0.2812525
[Epoch 61] ogbg-molbace: 0.882791 val loss: 0.443650
[Epoch 61] ogbg-molbace: 0.869078 test loss: 0.468229
[Epoch 62; Iter    29/   31] train: loss: 0.2573444
[Epoch 62] ogbg-molbace: 0.876962 val loss: 0.417337
[Epoch 62] ogbg-molbace: 0.869383 test loss: 0.427711
[Epoch 63; Iter    28/   31] train: loss: 0.5665268
[Epoch 63] ogbg-molbace: 0.899830 val loss: 0.387589
[Epoch 63] ogbg-molbace: 0.878763 test loss: 0.409971
[Epoch 64; Iter    27/   31] train: loss: 0.4451079
[Epoch 64] ogbg-molbace: 0.880683 val loss: 0.445820
[Epoch 64] ogbg-molbace: 0.877890 test loss: 0.454923
[Epoch 65; Iter    26/   31] train: loss: 0.2029031
[Epoch 65] ogbg-molbace: 0.881894 val loss: 0.507559
[Epoch 65] ogbg-molbace: 0.865151 test loss: 0.492505
[Epoch 66; Iter    25/   31] train: loss: 0.4098231
[Epoch 66] ogbg-molbace: 0.890907 val loss: 0.395396
[Epoch 66] ogbg-molbace: 0.874706 test loss: 0.410437
[Epoch 67; Iter    24/   31] train: loss: 0.2561731
[Epoch 67] ogbg-molbace: 0.907901 val loss: 0.370351
[Epoch 67] ogbg-molbace: 0.872873 test loss: 0.422069
[Epoch 68; Iter    23/   31] train: loss: 0.1980111
[Epoch 68] ogbg-molbace: 0.896556 val loss: 0.378677
[Epoch 68] ogbg-molbace: 0.856819 test loss: 0.450700
[Epoch 69; Iter    22/   31] train: loss: 0.1956799
[Epoch 69] ogbg-molbace: 0.897408 val loss: 0.388700
[Epoch 69] ogbg-molbace: 0.870997 test loss: 0.438446
[Epoch 70; Iter    21/   31] train: loss: 0.1955210
[Epoch 70] ogbg-molbace: 0.884540 val loss: 0.437368
[Epoch 70] ogbg-molbace: 0.855554 test loss: 0.494746
[Epoch 71; Iter    20/   31] train: loss: 0.4404978
[Epoch 71] ogbg-molbace: 0.893776 val loss: 0.398208
[Epoch 71] ogbg-molbace: 0.846828 test loss: 0.489064
[Epoch 72; Iter    19/   31] train: loss: 0.4082823
[Epoch 72] ogbg-molbace: 0.898126 val loss: 0.428474
[Epoch 72] ogbg-molbace: 0.867071 test loss: 0.483552
[Epoch 73; Iter    18/   31] train: loss: 0.2642249
[Epoch 73] ogbg-molbace: 0.878845 val loss: 0.442747
[Epoch 73] ogbg-molbace: 0.871477 test loss: 0.477247
[Epoch 74; Iter    17/   31] train: loss: 0.2597086
[Epoch 74] ogbg-molbace: 0.901354 val loss: 0.422692
[Epoch 74] ogbg-molbace: 0.892461 test loss: 0.419819
[Epoch 75; Iter    16/   31] train: loss: 0.2619523
[Epoch 75] ogbg-molbace: 0.899830 val loss: 0.429137
[Epoch 75] ogbg-molbace: 0.865195 test loss: 0.538711
[Epoch 76; Iter    15/   31] train: loss: 0.2534347
[Epoch 76] ogbg-molbace: 0.890234 val loss: 0.491804
[Epoch 76] ogbg-molbace: 0.873004 test loss: 0.507584
[Epoch 77; Iter    14/   31] train: loss: 0.1730512
[Epoch 77] ogbg-molbace: 0.899381 val loss: 0.471323
[Epoch 77] ogbg-molbace: 0.880813 test loss: 0.477685
[Epoch 78; Iter    13/   31] train: loss: 0.2550427
[Epoch 78] ogbg-molbace: 0.892521 val loss: 0.441231
[Epoch 78] ogbg-molbace: 0.868467 test loss: 0.494836
[Epoch 79; Iter    12/   31] train: loss: 0.1718916
[Epoch 79] ogbg-molbace: 0.889203 val loss: 0.460051
[Epoch 79] ogbg-molbace: 0.854376 test loss: 0.547860
[Epoch 80; Iter    11/   31] train: loss: 0.3421354
[Epoch 80] ogbg-molbace: 0.889472 val loss: 0.574956
[Epoch 80] ogbg-molbace: 0.886354 test loss: 0.504791
[Epoch 81; Iter    10/   31] train: loss: 0.2134486
[Epoch 81] ogbg-molbace: 0.883508 val loss: 0.483501
[Epoch 81] ogbg-molbace: 0.852718 test loss: 0.550482
[Epoch 82; Iter     9/   31] train: loss: 0.1810663
[Epoch 82] ogbg-molbace: 0.886288 val loss: 0.460623
[Epoch 82] ogbg-molbace: 0.858477 test loss: 0.514866
[Epoch 83; Iter     8/   31] train: loss: 0.1172806
[Epoch 83] ogbg-molbace: 0.897319 val loss: 0.409391
[Epoch 83] ogbg-molbace: 0.888055 test loss: 0.422060
[Epoch 84; Iter     7/   31] train: loss: 0.2044225
[Epoch 84] ogbg-molbace: 0.892611 val loss: 0.443784
[Epoch 84] ogbg-molbace: 0.879504 test loss: 0.463123
[Epoch 85; Iter     6/   31] train: loss: 0.0848698
[Epoch 85] ogbg-molbace: 0.910456 val loss: 0.416780
[Epoch 85] ogbg-molbace: 0.873048 test loss: 0.520701
[Epoch 86; Iter     5/   31] train: loss: 0.1045909
[Epoch 86] ogbg-molbace: 0.905165 val loss: 0.429093
[Epoch 86] ogbg-molbace: 0.872830 test loss: 0.504510
[Epoch 36; Iter    25/   31] train: loss: 0.4677038
[Epoch 36] ogbg-molbace: 0.858667 val loss: 0.474807
[Epoch 36] ogbg-molbace: 0.825670 test loss: 0.488019
[Epoch 37; Iter    24/   31] train: loss: 0.5187252
[Epoch 37] ogbg-molbace: 0.868442 val loss: 0.491560
[Epoch 37] ogbg-molbace: 0.814894 test loss: 0.518382
[Epoch 38; Iter    23/   31] train: loss: 0.4953133
[Epoch 38] ogbg-molbace: 0.869160 val loss: 0.452083
[Epoch 38] ogbg-molbace: 0.833522 test loss: 0.472010
[Epoch 39; Iter    22/   31] train: loss: 0.5152953
[Epoch 39] ogbg-molbace: 0.865707 val loss: 0.489926
[Epoch 39] ogbg-molbace: 0.826062 test loss: 0.513270
[Epoch 40; Iter    21/   31] train: loss: 0.7381371
[Epoch 40] ogbg-molbace: 0.885436 val loss: 0.472278
[Epoch 40] ogbg-molbace: 0.843426 test loss: 0.503864
[Epoch 41; Iter    20/   31] train: loss: 0.5021486
[Epoch 41] ogbg-molbace: 0.888216 val loss: 0.451826
[Epoch 41] ogbg-molbace: 0.838801 test loss: 0.485294
[Epoch 42; Iter    19/   31] train: loss: 0.5063280
[Epoch 42] ogbg-molbace: 0.869967 val loss: 0.433580
[Epoch 42] ogbg-molbace: 0.827109 test loss: 0.473285
[Epoch 43; Iter    18/   31] train: loss: 0.4141813
[Epoch 43] ogbg-molbace: 0.878710 val loss: 0.460104
[Epoch 43] ogbg-molbace: 0.838409 test loss: 0.479306
[Epoch 44; Iter    17/   31] train: loss: 0.5433688
[Epoch 44] ogbg-molbace: 0.875706 val loss: 0.447645
[Epoch 44] ogbg-molbace: 0.849184 test loss: 0.458608
[Epoch 45; Iter    16/   31] train: loss: 0.4653271
[Epoch 45] ogbg-molbace: 0.897184 val loss: 0.400554
[Epoch 45] ogbg-molbace: 0.873179 test loss: 0.420263
[Epoch 46; Iter    15/   31] train: loss: 0.4635741
[Epoch 46] ogbg-molbace: 0.817191 val loss: 0.513585
[Epoch 46] ogbg-molbace: 0.823401 test loss: 0.492113
[Epoch 47; Iter    14/   31] train: loss: 0.5319731
[Epoch 47] ogbg-molbace: 0.846426 val loss: 0.501688
[Epoch 47] ogbg-molbace: 0.816159 test loss: 0.518316
[Epoch 48; Iter    13/   31] train: loss: 0.6451052
[Epoch 48] ogbg-molbace: 0.884898 val loss: 0.428771
[Epoch 48] ogbg-molbace: 0.867638 test loss: 0.436817
[Epoch 49; Iter    12/   31] train: loss: 0.2918087
[Epoch 49] ogbg-molbace: 0.848489 val loss: 0.642603
[Epoch 49] ogbg-molbace: 0.803420 test loss: 0.731038
[Epoch 50; Iter    11/   31] train: loss: 0.5109022
[Epoch 50] ogbg-molbace: 0.852569 val loss: 0.494000
[Epoch 50] ogbg-molbace: 0.849969 test loss: 0.483454
[Epoch 51; Iter    10/   31] train: loss: 0.2921130
[Epoch 51] ogbg-molbace: 0.878262 val loss: 0.500809
[Epoch 51] ogbg-molbace: 0.849228 test loss: 0.511235
[Epoch 52; Iter     9/   31] train: loss: 0.5312368
[Epoch 52] ogbg-molbace: 0.873330 val loss: 0.472542
[Epoch 52] ogbg-molbace: 0.850755 test loss: 0.491663
[Epoch 53; Iter     8/   31] train: loss: 0.3545685
[Epoch 53] ogbg-molbace: 0.857502 val loss: 0.534958
[Epoch 53] ogbg-molbace: 0.847832 test loss: 0.537522
[Epoch 54; Iter     7/   31] train: loss: 0.4633497
[Epoch 54] ogbg-molbace: 0.863420 val loss: 0.511594
[Epoch 54] ogbg-molbace: 0.816072 test loss: 0.527908
[Epoch 55; Iter     6/   31] train: loss: 0.3044924
[Epoch 55] ogbg-molbace: 0.871850 val loss: 0.444590
[Epoch 55] ogbg-molbace: 0.835311 test loss: 0.505594
[Epoch 56; Iter     5/   31] train: loss: 0.2965070
[Epoch 56] ogbg-molbace: 0.875034 val loss: 0.419843
[Epoch 56] ogbg-molbace: 0.869558 test loss: 0.435364
[Epoch 57; Iter     4/   31] train: loss: 0.2568192
[Epoch 57] ogbg-molbace: 0.890593 val loss: 0.451283
[Epoch 57] ogbg-molbace: 0.880988 test loss: 0.460543
[Epoch 58; Iter     3/   31] train: loss: 0.3486835
[Epoch 58] ogbg-molbace: 0.886557 val loss: 0.398748
[Epoch 58] ogbg-molbace: 0.888273 test loss: 0.410753
[Epoch 59; Iter     2/   31] train: loss: 0.2178079
[Epoch 59] ogbg-molbace: 0.896063 val loss: 0.381438
[Epoch 59] ogbg-molbace: 0.882515 test loss: 0.438829
[Epoch 60; Iter     1/   31] train: loss: 0.3537933
[Epoch 60; Iter    31/   31] train: loss: 0.4060054
[Epoch 60] ogbg-molbace: 0.892566 val loss: 0.393399
[Epoch 60] ogbg-molbace: 0.880464 test loss: 0.403517
[Epoch 61; Iter    30/   31] train: loss: 0.5580251
[Epoch 61] ogbg-molbace: 0.878710 val loss: 0.425856
[Epoch 61] ogbg-molbace: 0.861356 test loss: 0.443647
[Epoch 62; Iter    29/   31] train: loss: 0.3804487
[Epoch 62] ogbg-molbace: 0.883419 val loss: 0.421123
[Epoch 62] ogbg-molbace: 0.875229 test loss: 0.432775
[Epoch 63; Iter    28/   31] train: loss: 0.3605054
[Epoch 63] ogbg-molbace: 0.897005 val loss: 0.433082
[Epoch 63] ogbg-molbace: 0.890236 test loss: 0.425551
[Epoch 64; Iter    27/   31] train: loss: 0.3439859
[Epoch 64] ogbg-molbace: 0.874765 val loss: 0.430319
[Epoch 64] ogbg-molbace: 0.854419 test loss: 0.486261
[Epoch 65; Iter    26/   31] train: loss: 0.2571029
[Epoch 65] ogbg-molbace: 0.888844 val loss: 0.415089
[Epoch 65] ogbg-molbace: 0.865631 test loss: 0.445933
[Epoch 66; Iter    25/   31] train: loss: 0.3659417
[Epoch 66] ogbg-molbace: 0.886512 val loss: 0.460514
[Epoch 66] ogbg-molbace: 0.875796 test loss: 0.460813
[Epoch 67; Iter    24/   31] train: loss: 0.3615153
[Epoch 67] ogbg-molbace: 0.856560 val loss: 0.553460
[Epoch 67] ogbg-molbace: 0.845432 test loss: 0.613240
[Epoch 68; Iter    23/   31] train: loss: 0.4313684
[Epoch 68] ogbg-molbace: 0.883105 val loss: 0.593914
[Epoch 68] ogbg-molbace: 0.877585 test loss: 0.574454
[Epoch 69; Iter    22/   31] train: loss: 0.1849128
[Epoch 69] ogbg-molbace: 0.859788 val loss: 0.587018
[Epoch 69] ogbg-molbace: 0.825321 test loss: 0.728855
[Epoch 70; Iter    21/   31] train: loss: 0.6164511
[Epoch 70] ogbg-molbace: 0.890907 val loss: 0.407796
[Epoch 70] ogbg-molbace: 0.885787 test loss: 0.405419
[Epoch 71; Iter    20/   31] train: loss: 0.2641293
[Epoch 71] ogbg-molbace: 0.883508 val loss: 0.440320
[Epoch 71] ogbg-molbace: 0.855772 test loss: 0.460995
[Epoch 72; Iter    19/   31] train: loss: 0.2754067
[Epoch 72] ogbg-molbace: 0.894449 val loss: 0.407586
[Epoch 72] ogbg-molbace: 0.885612 test loss: 0.412301
[Epoch 73; Iter    18/   31] train: loss: 0.1397380
[Epoch 73] ogbg-molbace: 0.886916 val loss: 0.406826
[Epoch 73] ogbg-molbace: 0.872524 test loss: 0.433002
[Epoch 74; Iter    17/   31] train: loss: 0.3244530
[Epoch 74] ogbg-molbace: 0.897453 val loss: 0.405465
[Epoch 74] ogbg-molbace: 0.885132 test loss: 0.414698
[Epoch 75; Iter    16/   31] train: loss: 0.1457247
[Epoch 75] ogbg-molbace: 0.879697 val loss: 0.432693
[Epoch 75] ogbg-molbace: 0.854158 test loss: 0.462394
[Epoch 76; Iter    15/   31] train: loss: 0.3948492
[Epoch 76] ogbg-molbace: 0.896422 val loss: 0.407550
[Epoch 76] ogbg-molbace: 0.882907 test loss: 0.457514
[Epoch 77; Iter    14/   31] train: loss: 0.4469431
[Epoch 77] ogbg-molbace: 0.882029 val loss: 0.482529
[Epoch 77] ogbg-molbace: 0.868860 test loss: 0.473825
[Epoch 78; Iter    13/   31] train: loss: 0.2767056
[Epoch 78] ogbg-molbace: 0.911981 val loss: 0.357848
[Epoch 78] ogbg-molbace: 0.881817 test loss: 0.424823
[Epoch 79; Iter    12/   31] train: loss: 0.2082146
[Epoch 79] ogbg-molbace: 0.899785 val loss: 0.397274
[Epoch 79] ogbg-molbace: 0.864977 test loss: 0.476494
[Epoch 80; Iter    11/   31] train: loss: 0.2237843
[Epoch 80] ogbg-molbace: 0.900547 val loss: 0.450783
[Epoch 80] ogbg-molbace: 0.871259 test loss: 0.513208
[Epoch 81; Iter    10/   31] train: loss: 0.1676145
[Epoch 81] ogbg-molbace: 0.891759 val loss: 0.473043
[Epoch 81] ogbg-molbace: 0.858302 test loss: 0.572812
[Epoch 82; Iter     9/   31] train: loss: 0.2131141
[Epoch 82] ogbg-molbace: 0.877410 val loss: 0.444831
[Epoch 82] ogbg-molbace: 0.888579 test loss: 0.417887
[Epoch 83; Iter     8/   31] train: loss: 0.2804608
[Epoch 83] ogbg-molbace: 0.878755 val loss: 0.413871
[Epoch 83] ogbg-molbace: 0.865893 test loss: 0.441633
[Epoch 84; Iter     7/   31] train: loss: 0.2454896
[Epoch 84] ogbg-molbace: 0.894987 val loss: 0.426086
[Epoch 84] ogbg-molbace: 0.883649 test loss: 0.432911
[Epoch 85; Iter     6/   31] train: loss: 0.0816697
[Epoch 85] ogbg-molbace: 0.870460 val loss: 0.454023
[Epoch 85] ogbg-molbace: 0.868118 test loss: 0.479969
[Epoch 86; Iter     5/   31] train: loss: 0.3163905
[Epoch 86] ogbg-molbace: 0.907407 val loss: 0.451806
[Epoch 86] ogbg-molbace: 0.883823 test loss: 0.499613
[Epoch 78] ogbg-molbace: 0.894691 val loss: 0.466382
[Epoch 78] ogbg-molbace: 0.872520 test loss: 0.485037
[Epoch 79; Iter    12/   41] train: loss: 0.1106087
[Epoch 79] ogbg-molbace: 0.905767 val loss: 0.407922
[Epoch 79] ogbg-molbace: 0.886743 test loss: 0.451273
[Epoch 80; Iter     1/   41] train: loss: 0.1683964
[Epoch 80; Iter    31/   41] train: loss: 0.1240162
[Epoch 80] ogbg-molbace: 0.884669 val loss: 0.541919
[Epoch 80] ogbg-molbace: 0.857594 test loss: 0.544326
[Epoch 81; Iter    20/   41] train: loss: 0.0335451
[Epoch 81] ogbg-molbace: 0.902778 val loss: 0.438648
[Epoch 81] ogbg-molbace: 0.880246 test loss: 0.474864
[Epoch 82; Iter     9/   41] train: loss: 0.0595741
[Epoch 82; Iter    39/   41] train: loss: 0.1041984
[Epoch 82] ogbg-molbace: 0.911744 val loss: 0.410873
[Epoch 82] ogbg-molbace: 0.880421 test loss: 0.498555
[Epoch 83; Iter    28/   41] train: loss: 0.3031672
[Epoch 83] ogbg-molbace: 0.909283 val loss: 0.438774
[Epoch 83] ogbg-molbace: 0.874276 test loss: 0.529446
[Epoch 84; Iter    17/   41] train: loss: 0.1305307
[Epoch 84] ogbg-molbace: 0.904536 val loss: 0.460912
[Epoch 84] ogbg-molbace: 0.880948 test loss: 0.492370
[Epoch 85; Iter     6/   41] train: loss: 0.0981879
[Epoch 85; Iter    36/   41] train: loss: 0.1196632
[Epoch 85] ogbg-molbace: 0.907349 val loss: 0.411813
[Epoch 85] ogbg-molbace: 0.889025 test loss: 0.452512
[Epoch 86; Iter    25/   41] train: loss: 0.1154213
[Epoch 86] ogbg-molbace: 0.906118 val loss: 0.442825
[Epoch 86] ogbg-molbace: 0.871115 test loss: 0.511783
[Epoch 87; Iter    14/   41] train: loss: 0.0413338
[Epoch 87] ogbg-molbace: 0.885900 val loss: 0.516656
[Epoch 87] ogbg-molbace: 0.882002 test loss: 0.547029
[Epoch 88; Iter     3/   41] train: loss: 0.1323705
[Epoch 88; Iter    33/   41] train: loss: 0.1548735
[Epoch 88] ogbg-molbace: 0.893460 val loss: 0.510285
[Epoch 88] ogbg-molbace: 0.878314 test loss: 0.515288
[Epoch 89; Iter    22/   41] train: loss: 0.1444031
[Epoch 89] ogbg-molbace: 0.871308 val loss: 0.503267
[Epoch 89] ogbg-molbace: 0.870061 test loss: 0.558707
[Epoch 90; Iter    11/   41] train: loss: 0.1222253
[Epoch 90; Iter    41/   41] train: loss: 0.0442341
[Epoch 90] ogbg-molbace: 0.902250 val loss: 0.454300
[Epoch 90] ogbg-molbace: 0.861282 test loss: 0.547692
[Epoch 91; Iter    30/   41] train: loss: 0.0880763
[Epoch 91] ogbg-molbace: 0.910162 val loss: 0.473545
[Epoch 91] ogbg-molbace: 0.872520 test loss: 0.527545
[Epoch 92; Iter    19/   41] train: loss: 0.1527339
[Epoch 92] ogbg-molbace: 0.889768 val loss: 0.482605
[Epoch 92] ogbg-molbace: 0.884109 test loss: 0.492856
[Epoch 93; Iter     8/   41] train: loss: 0.0345548
[Epoch 93; Iter    38/   41] train: loss: 0.1417380
[Epoch 93] ogbg-molbace: 0.917546 val loss: 0.427565
[Epoch 93] ogbg-molbace: 0.880070 test loss: 0.526082
[Epoch 94; Iter    27/   41] train: loss: 0.0311241
[Epoch 94] ogbg-molbace: 0.899965 val loss: 0.475809
[Epoch 94] ogbg-molbace: 0.885689 test loss: 0.572646
[Epoch 95; Iter    16/   41] train: loss: 0.0838854
[Epoch 95] ogbg-molbace: 0.894866 val loss: 0.524891
[Epoch 95] ogbg-molbace: 0.875154 test loss: 0.632569
[Epoch 96; Iter     5/   41] train: loss: 0.4149219
[Epoch 96; Iter    35/   41] train: loss: 0.4499862
[Epoch 96] ogbg-molbace: 0.895921 val loss: 0.511630
[Epoch 96] ogbg-molbace: 0.880948 test loss: 0.669555
[Epoch 97; Iter    24/   41] train: loss: 0.3355184
[Epoch 97] ogbg-molbace: 0.910338 val loss: 0.466950
[Epoch 97] ogbg-molbace: 0.900439 test loss: 0.528283
[Epoch 98; Iter    13/   41] train: loss: 0.0208012
[Epoch 98] ogbg-molbace: 0.901723 val loss: 0.484709
[Epoch 98] ogbg-molbace: 0.861633 test loss: 0.579202
[Epoch 99; Iter     2/   41] train: loss: 0.0446874
[Epoch 99; Iter    32/   41] train: loss: 0.1269804
[Epoch 99] ogbg-molbace: 0.908052 val loss: 0.472638
[Epoch 99] ogbg-molbace: 0.863213 test loss: 0.621842
[Epoch 100; Iter    21/   41] train: loss: 0.1260665
[Epoch 100] ogbg-molbace: 0.924051 val loss: 0.353647
[Epoch 100] ogbg-molbace: 0.861633 test loss: 0.583241
[Epoch 101; Iter    10/   41] train: loss: 0.0283403
[Epoch 101; Iter    40/   41] train: loss: 0.0689572
[Epoch 101] ogbg-molbace: 0.898558 val loss: 0.924994
[Epoch 101] ogbg-molbace: 0.850746 test loss: 0.648214
[Epoch 102; Iter    29/   41] train: loss: 0.0573935
[Epoch 102] ogbg-molbace: 0.891702 val loss: 0.579024
[Epoch 102] ogbg-molbace: 0.855487 test loss: 0.649332
[Epoch 103; Iter    18/   41] train: loss: 0.0959723
[Epoch 103] ogbg-molbace: 0.918601 val loss: 0.452926
[Epoch 103] ogbg-molbace: 0.876032 test loss: 0.585730
[Epoch 104; Iter     7/   41] train: loss: 0.0472169
[Epoch 104; Iter    37/   41] train: loss: 0.0335963
[Epoch 104] ogbg-molbace: 0.908755 val loss: 0.459358
[Epoch 104] ogbg-molbace: 0.872520 test loss: 0.600261
[Epoch 105; Iter    26/   41] train: loss: 0.1604026
[Epoch 105] ogbg-molbace: 0.907173 val loss: 0.487625
[Epoch 105] ogbg-molbace: 0.878490 test loss: 0.557563
[Epoch 106; Iter    15/   41] train: loss: 0.0184628
[Epoch 106] ogbg-molbace: 0.902602 val loss: 0.521590
[Epoch 106] ogbg-molbace: 0.879543 test loss: 0.578216
[Epoch 107; Iter     4/   41] train: loss: 0.0318643
[Epoch 107; Iter    34/   41] train: loss: 0.0370080
[Epoch 107] ogbg-molbace: 0.901195 val loss: 0.550354
[Epoch 107] ogbg-molbace: 0.867076 test loss: 0.625762
[Epoch 108; Iter    23/   41] train: loss: 0.1004053
[Epoch 108] ogbg-molbace: 0.912799 val loss: 0.472706
[Epoch 108] ogbg-molbace: 0.879017 test loss: 0.597769
[Epoch 109; Iter    12/   41] train: loss: 0.0094347
[Epoch 109] ogbg-molbace: 0.904536 val loss: 0.556752
[Epoch 109] ogbg-molbace: 0.866550 test loss: 0.649422
[Epoch 110; Iter     1/   41] train: loss: 0.0383066
[Epoch 110; Iter    31/   41] train: loss: 0.0745280
[Epoch 110] ogbg-molbace: 0.904536 val loss: 0.508254
[Epoch 110] ogbg-molbace: 0.867076 test loss: 0.650836
[Epoch 111; Iter    20/   41] train: loss: 0.0492340
[Epoch 111] ogbg-molbace: 0.886076 val loss: 0.655856
[Epoch 111] ogbg-molbace: 0.853380 test loss: 0.657079
[Epoch 112; Iter     9/   41] train: loss: 0.0092925
[Epoch 112; Iter    39/   41] train: loss: 0.0696383
[Epoch 112] ogbg-molbace: 0.908228 val loss: 0.577234
[Epoch 112] ogbg-molbace: 0.864267 test loss: 0.671035
[Epoch 113; Iter    28/   41] train: loss: 0.0385758
[Epoch 113] ogbg-molbace: 0.916842 val loss: 0.490056
[Epoch 113] ogbg-molbace: 0.866725 test loss: 0.660298
[Epoch 114; Iter    17/   41] train: loss: 0.0074645
[Epoch 114] ogbg-molbace: 0.906118 val loss: 0.527193
[Epoch 114] ogbg-molbace: 0.859350 test loss: 0.738626
[Epoch 115; Iter     6/   41] train: loss: 0.0784392
[Epoch 115; Iter    36/   41] train: loss: 0.0816533
[Epoch 115] ogbg-molbace: 0.897328 val loss: 0.550960
[Epoch 115] ogbg-molbace: 0.884109 test loss: 0.637402
[Epoch 116; Iter    25/   41] train: loss: 0.0176010
[Epoch 116] ogbg-molbace: 0.915963 val loss: 0.444846
[Epoch 116] ogbg-molbace: 0.866901 test loss: 0.635978
[Epoch 117; Iter    14/   41] train: loss: 0.0358689
[Epoch 117] ogbg-molbace: 0.919128 val loss: 0.458931
[Epoch 117] ogbg-molbace: 0.871466 test loss: 0.700445
[Epoch 118; Iter     3/   41] train: loss: 0.0116568
[Epoch 118; Iter    33/   41] train: loss: 0.0113670
[Epoch 118] ogbg-molbace: 0.913678 val loss: 0.497001
[Epoch 118] ogbg-molbace: 0.852151 test loss: 0.721926
[Epoch 119; Iter    22/   41] train: loss: 0.0669976
[Epoch 119] ogbg-molbace: 0.903481 val loss: 0.527017
[Epoch 119] ogbg-molbace: 0.856190 test loss: 0.712692
[Epoch 120; Iter    11/   41] train: loss: 0.0293140
[Epoch 120; Iter    41/   41] train: loss: 0.1531743
[Epoch 120] ogbg-molbace: 0.898558 val loss: 0.580945
[Epoch 120] ogbg-molbace: 0.855663 test loss: 0.741705
[Epoch 121; Iter    30/   41] train: loss: 0.0192047
[Epoch 121] ogbg-molbace: 0.905239 val loss: 0.564210
[Epoch 121] ogbg-molbace: 0.875329 test loss: 0.724948
[Epoch 122; Iter    19/   41] train: loss: 0.0080881
[Epoch 122] ogbg-molbace: 0.907349 val loss: 0.647979
[Epoch 122] ogbg-molbace: 0.851449 test loss: 0.722285
[Epoch 123; Iter     8/   41] train: loss: 0.0090378
[Epoch 123; Iter    38/   41] train: loss: 0.0213045
[Epoch 123] ogbg-molbace: 0.901547 val loss: 0.603047
[Epoch 82] ogbg-molbace: 0.868845 val loss: 0.682482
[Epoch 82] ogbg-molbace: 0.869305 test loss: 0.721826
[Epoch 83; Iter    18/   36] train: loss: 0.2090018
[Epoch 83] ogbg-molbace: 0.885890 val loss: 0.614422
[Epoch 83] ogbg-molbace: 0.874670 test loss: 0.676483
[Epoch 84; Iter    12/   36] train: loss: 0.0542568
[Epoch 84] ogbg-molbace: 0.884864 val loss: 0.669001
[Epoch 84] ogbg-molbace: 0.883922 test loss: 0.697192
[Epoch 85; Iter     6/   36] train: loss: 0.1719674
[Epoch 85; Iter    36/   36] train: loss: 0.0577722
[Epoch 85] ogbg-molbace: 0.885496 val loss: 0.589088
[Epoch 85] ogbg-molbace: 0.868061 test loss: 0.697502
[Epoch 86; Iter    30/   36] train: loss: 0.1271881
[Epoch 86] ogbg-molbace: 0.874132 val loss: 0.727604
[Epoch 86] ogbg-molbace: 0.859586 test loss: 0.783142
[Epoch 87; Iter    24/   36] train: loss: 0.0782636
[Epoch 87] ogbg-molbace: 0.867503 val loss: 0.789895
[Epoch 87] ogbg-molbace: 0.858653 test loss: 0.876690
[Epoch 88; Iter    18/   36] train: loss: 0.0602408
[Epoch 88] ogbg-molbace: 0.899858 val loss: 0.501071
[Epoch 88] ogbg-molbace: 0.874048 test loss: 0.650672
[Epoch 89; Iter    12/   36] train: loss: 0.0608609
[Epoch 89] ogbg-molbace: 0.867582 val loss: 0.647852
[Epoch 89] ogbg-molbace: 0.866972 test loss: 0.746527
[Epoch 90; Iter     6/   36] train: loss: 0.0269941
[Epoch 90; Iter    36/   36] train: loss: 0.0787666
[Epoch 90] ogbg-molbace: 0.887311 val loss: 0.567469
[Epoch 90] ogbg-molbace: 0.869383 test loss: 0.690861
[Epoch 91; Iter    30/   36] train: loss: 0.1124438
[Epoch 91] ogbg-molbace: 0.860164 val loss: 0.722112
[Epoch 91] ogbg-molbace: 0.854455 test loss: 0.805488
[Epoch 92; Iter    24/   36] train: loss: 0.1026046
[Epoch 92] ogbg-molbace: 0.892519 val loss: 0.588355
[Epoch 92] ogbg-molbace: 0.875914 test loss: 0.675154
[Epoch 93; Iter    18/   36] train: loss: 0.0215567
[Epoch 93] ogbg-molbace: 0.892045 val loss: 0.690323
[Epoch 93] ogbg-molbace: 0.887187 test loss: 0.710004
[Epoch 94; Iter    12/   36] train: loss: 0.0580334
[Epoch 94] ogbg-molbace: 0.882812 val loss: 0.659503
[Epoch 94] ogbg-molbace: 0.890530 test loss: 0.719385
[Epoch 95; Iter     6/   36] train: loss: 0.0350380
[Epoch 95; Iter    36/   36] train: loss: 0.0553243
[Epoch 95] ogbg-molbace: 0.899384 val loss: 0.585546
[Epoch 95] ogbg-molbace: 0.877391 test loss: 0.718392
[Epoch 96; Iter    30/   36] train: loss: 0.2796911
[Epoch 96] ogbg-molbace: 0.881550 val loss: 0.762326
[Epoch 96] ogbg-molbace: 0.881045 test loss: 0.803922
[Epoch 97; Iter    24/   36] train: loss: 0.2119358
[Epoch 97] ogbg-molbace: 0.879893 val loss: 0.859297
[Epoch 97] ogbg-molbace: 0.870316 test loss: 0.884837
[Epoch 98; Iter    18/   36] train: loss: 0.1185583
[Epoch 98] ogbg-molbace: 0.868608 val loss: 0.749677
[Epoch 98] ogbg-molbace: 0.855777 test loss: 0.964192
[Epoch 99; Iter    12/   36] train: loss: 0.1305228
[Epoch 99] ogbg-molbace: 0.870423 val loss: 0.762826
[Epoch 99] ogbg-molbace: 0.854299 test loss: 0.891667
[Epoch 100; Iter     6/   36] train: loss: 0.1747300
[Epoch 100; Iter    36/   36] train: loss: 0.0258370
[Epoch 100] ogbg-molbace: 0.879972 val loss: 0.672421
[Epoch 100] ogbg-molbace: 0.875369 test loss: 0.705338
[Epoch 101; Iter    30/   36] train: loss: 0.0587843
[Epoch 101] ogbg-molbace: 0.877210 val loss: 0.712343
[Epoch 101] ogbg-molbace: 0.863707 test loss: 0.766517
[Epoch 102; Iter    24/   36] train: loss: 0.0137538
[Epoch 102] ogbg-molbace: 0.882181 val loss: 0.695319
[Epoch 102] ogbg-molbace: 0.875603 test loss: 0.740435
[Epoch 103; Iter    18/   36] train: loss: 0.0398226
[Epoch 103] ogbg-molbace: 0.881866 val loss: 0.723624
[Epoch 103] ogbg-molbace: 0.873659 test loss: 0.783708
[Epoch 104; Iter    12/   36] train: loss: 0.1938425
[Epoch 104] ogbg-molbace: 0.876815 val loss: 0.738894
[Epoch 104] ogbg-molbace: 0.872026 test loss: 0.776924
[Epoch 105; Iter     6/   36] train: loss: 0.0071217
[Epoch 105; Iter    36/   36] train: loss: 0.0546027
[Epoch 105] ogbg-molbace: 0.876736 val loss: 0.749570
[Epoch 105] ogbg-molbace: 0.872881 test loss: 0.804433
[Epoch 106; Iter    30/   36] train: loss: 0.0243172
[Epoch 106] ogbg-molbace: 0.880129 val loss: 0.758387
[Epoch 106] ogbg-molbace: 0.859509 test loss: 0.902291
[Epoch 107; Iter    24/   36] train: loss: 0.0094273
[Epoch 107] ogbg-molbace: 0.880840 val loss: 0.775744
[Epoch 107] ogbg-molbace: 0.870860 test loss: 0.834322
[Epoch 108; Iter    18/   36] train: loss: 0.0775174
[Epoch 108] ogbg-molbace: 0.882812 val loss: 0.724373
[Epoch 108] ogbg-molbace: 0.858342 test loss: 0.875889
[Epoch 109; Iter    12/   36] train: loss: 0.0258251
[Epoch 109] ogbg-molbace: 0.868924 val loss: 0.842102
[Epoch 109] ogbg-molbace: 0.851423 test loss: 0.970420
[Epoch 110; Iter     6/   36] train: loss: 0.0158026
[Epoch 110; Iter    36/   36] train: loss: 0.0228164
[Epoch 110] ogbg-molbace: 0.869397 val loss: 0.801964
[Epoch 110] ogbg-molbace: 0.857720 test loss: 0.905650
[Epoch 111; Iter    30/   36] train: loss: 0.0072729
[Epoch 111] ogbg-molbace: 0.876736 val loss: 0.832269
[Epoch 111] ogbg-molbace: 0.867828 test loss: 0.912185
[Epoch 112; Iter    24/   36] train: loss: 0.0472939
[Epoch 112] ogbg-molbace: 0.872317 val loss: 0.862201
[Epoch 112] ogbg-molbace: 0.863940 test loss: 0.937668
[Epoch 113; Iter    18/   36] train: loss: 0.0101278
[Epoch 113] ogbg-molbace: 0.883759 val loss: 0.800416
[Epoch 113] ogbg-molbace: 0.861530 test loss: 0.956281
[Epoch 114; Iter    12/   36] train: loss: 0.0030175
[Epoch 114] ogbg-molbace: 0.882497 val loss: 0.763208
[Epoch 114] ogbg-molbace: 0.863785 test loss: 0.900471
[Epoch 115; Iter     6/   36] train: loss: 0.0079489
[Epoch 115; Iter    36/   36] train: loss: 0.0512710
[Epoch 115] ogbg-molbace: 0.877210 val loss: 0.801250
[Epoch 115] ogbg-molbace: 0.858342 test loss: 0.980866
[Epoch 116; Iter    30/   36] train: loss: 0.0066328
[Epoch 116] ogbg-molbace: 0.879972 val loss: 0.793870
[Epoch 116] ogbg-molbace: 0.863707 test loss: 0.930339
[Epoch 117; Iter    24/   36] train: loss: 0.0137638
[Epoch 117] ogbg-molbace: 0.873422 val loss: 0.858669
[Epoch 117] ogbg-molbace: 0.860675 test loss: 1.011316
[Epoch 118; Iter    18/   36] train: loss: 0.0370470
[Epoch 118] ogbg-molbace: 0.876736 val loss: 0.825046
[Epoch 118] ogbg-molbace: 0.864951 test loss: 0.923241
[Epoch 119; Iter    12/   36] train: loss: 0.0040800
[Epoch 119] ogbg-molbace: 0.871607 val loss: 0.854908
[Epoch 119] ogbg-molbace: 0.864018 test loss: 0.938007
[Epoch 120; Iter     6/   36] train: loss: 0.0016336
[Epoch 120; Iter    36/   36] train: loss: 0.0007432
[Epoch 120] ogbg-molbace: 0.877841 val loss: 0.845171
[Epoch 120] ogbg-molbace: 0.867594 test loss: 0.962288
[Epoch 121; Iter    30/   36] train: loss: 0.0488488
[Epoch 121] ogbg-molbace: 0.877841 val loss: 0.879008
[Epoch 121] ogbg-molbace: 0.856476 test loss: 1.059415
[Epoch 122; Iter    24/   36] train: loss: 0.0044958
[Epoch 122] ogbg-molbace: 0.880129 val loss: 0.830731
[Epoch 122] ogbg-molbace: 0.851656 test loss: 1.029783
[Epoch 123; Iter    18/   36] train: loss: 0.0059643
[Epoch 123] ogbg-molbace: 0.883207 val loss: 0.844198
[Epoch 123] ogbg-molbace: 0.857565 test loss: 1.029695
[Epoch 124; Iter    12/   36] train: loss: 0.0192072
[Epoch 124] ogbg-molbace: 0.874132 val loss: 0.917688
[Epoch 124] ogbg-molbace: 0.853600 test loss: 1.109175
[Epoch 125; Iter     6/   36] train: loss: 0.0556354
[Epoch 125; Iter    36/   36] train: loss: 0.0171449
[Epoch 125] ogbg-molbace: 0.888494 val loss: 0.771183
[Epoch 125] ogbg-molbace: 0.858964 test loss: 0.946472
[Epoch 126; Iter    30/   36] train: loss: 0.0103416
[Epoch 126] ogbg-molbace: 0.888889 val loss: 0.866241
[Epoch 126] ogbg-molbace: 0.863007 test loss: 1.053601
[Epoch 127; Iter    24/   36] train: loss: 0.1182722
[Epoch 127] ogbg-molbace: 0.877131 val loss: 0.804046
[Epoch 127] ogbg-molbace: 0.839605 test loss: 1.143933
[Epoch 128; Iter    18/   36] train: loss: 0.0227382
[Epoch 128] ogbg-molbace: 0.882970 val loss: 0.865057
[Epoch 128] ogbg-molbace: 0.868061 test loss: 1.007197
[Epoch 129; Iter    12/   36] train: loss: 0.0066404
[Epoch 129] ogbg-molbace: 0.882891 val loss: 0.828424
[Epoch 129] ogbg-molbace: 0.865806 test loss: 0.984131
[Epoch 78] ogbg-molbace: 0.902426 val loss: 0.452182
[Epoch 78] ogbg-molbace: 0.857946 test loss: 0.534816
[Epoch 79; Iter    12/   41] train: loss: 0.2300882
[Epoch 79] ogbg-molbace: 0.921941 val loss: 0.361909
[Epoch 79] ogbg-molbace: 0.882002 test loss: 0.477970
[Epoch 80; Iter     1/   41] train: loss: 0.1009102
[Epoch 80; Iter    31/   41] train: loss: 0.1250366
[Epoch 80] ogbg-molbace: 0.898383 val loss: 0.409008
[Epoch 80] ogbg-molbace: 0.877963 test loss: 0.492611
[Epoch 81; Iter    20/   41] train: loss: 0.0748536
[Epoch 81] ogbg-molbace: 0.904184 val loss: 0.457127
[Epoch 81] ogbg-molbace: 0.895171 test loss: 0.470246
[Epoch 82; Iter     9/   41] train: loss: 0.1591305
[Epoch 82; Iter    39/   41] train: loss: 0.1904346
[Epoch 82] ogbg-molbace: 0.897679 val loss: 0.428362
[Epoch 82] ogbg-molbace: 0.880948 test loss: 0.483686
[Epoch 83; Iter    28/   41] train: loss: 0.0879646
[Epoch 83] ogbg-molbace: 0.908404 val loss: 0.445406
[Epoch 83] ogbg-molbace: 0.905004 test loss: 0.474546
[Epoch 84; Iter    17/   41] train: loss: 0.3765885
[Epoch 84] ogbg-molbace: 0.914381 val loss: 0.434939
[Epoch 84] ogbg-molbace: 0.893942 test loss: 0.527258
[Epoch 85; Iter     6/   41] train: loss: 0.1253029
[Epoch 85; Iter    36/   41] train: loss: 0.0448700
[Epoch 85] ogbg-molbace: 0.919655 val loss: 0.366042
[Epoch 85] ogbg-molbace: 0.913433 test loss: 0.388814
[Epoch 86; Iter    25/   41] train: loss: 0.1896802
[Epoch 86] ogbg-molbace: 0.906470 val loss: 0.465616
[Epoch 86] ogbg-molbace: 0.907638 test loss: 0.415103
[Epoch 87; Iter    14/   41] train: loss: 0.1065700
[Epoch 87] ogbg-molbace: 0.909107 val loss: 0.445346
[Epoch 87] ogbg-molbace: 0.881651 test loss: 0.515651
[Epoch 88; Iter     3/   41] train: loss: 0.0776506
[Epoch 88; Iter    33/   41] train: loss: 0.1583498
[Epoch 88] ogbg-molbace: 0.891878 val loss: 0.589781
[Epoch 88] ogbg-molbace: 0.881124 test loss: 0.516459
[Epoch 89; Iter    22/   41] train: loss: 0.1992620
[Epoch 89] ogbg-molbace: 0.900844 val loss: 0.539509
[Epoch 89] ogbg-molbace: 0.884460 test loss: 0.527641
[Epoch 90; Iter    11/   41] train: loss: 0.1321320
[Epoch 90; Iter    41/   41] train: loss: 0.1025021
[Epoch 90] ogbg-molbace: 0.893987 val loss: 0.600378
[Epoch 90] ogbg-molbace: 0.895171 test loss: 0.499215
[Epoch 91; Iter    30/   41] train: loss: 0.1101805
[Epoch 91] ogbg-molbace: 0.896624 val loss: 0.515201
[Epoch 91] ogbg-molbace: 0.880246 test loss: 0.501893
[Epoch 92; Iter    19/   41] train: loss: 0.2474489
[Epoch 92] ogbg-molbace: 0.921062 val loss: 0.410265
[Epoch 92] ogbg-molbace: 0.887972 test loss: 0.480279
[Epoch 93; Iter     8/   41] train: loss: 0.0774489
[Epoch 93; Iter    38/   41] train: loss: 0.1824659
[Epoch 93] ogbg-molbace: 0.897152 val loss: 0.583374
[Epoch 93] ogbg-molbace: 0.899210 test loss: 0.409813
[Epoch 94; Iter    27/   41] train: loss: 0.0614031
[Epoch 94] ogbg-molbace: 0.903305 val loss: 0.688301
[Epoch 94] ogbg-molbace: 0.872695 test loss: 0.576093
[Epoch 95; Iter    16/   41] train: loss: 0.0989374
[Epoch 95] ogbg-molbace: 0.914557 val loss: 0.441601
[Epoch 95] ogbg-molbace: 0.900615 test loss: 0.450122
[Epoch 96; Iter     5/   41] train: loss: 0.0312357
[Epoch 96; Iter    35/   41] train: loss: 0.0146937
[Epoch 96] ogbg-molbace: 0.910338 val loss: 0.452791
[Epoch 96] ogbg-molbace: 0.895522 test loss: 0.465139
[Epoch 97; Iter    24/   41] train: loss: 0.0329350
[Epoch 97] ogbg-molbace: 0.912799 val loss: 0.463416
[Epoch 97] ogbg-molbace: 0.893064 test loss: 0.464296
[Epoch 98; Iter    13/   41] train: loss: 0.0246313
[Epoch 98] ogbg-molbace: 0.904184 val loss: 0.498225
[Epoch 98] ogbg-molbace: 0.896927 test loss: 0.462102
[Epoch 99; Iter     2/   41] train: loss: 0.0209687
[Epoch 99; Iter    32/   41] train: loss: 0.0170626
[Epoch 99] ogbg-molbace: 0.907700 val loss: 0.517216
[Epoch 99] ogbg-molbace: 0.893766 test loss: 0.489437
[Epoch 100; Iter    21/   41] train: loss: 0.0381829
[Epoch 100] ogbg-molbace: 0.904887 val loss: 0.575828
[Epoch 100] ogbg-molbace: 0.894293 test loss: 0.522621
[Epoch 101; Iter    10/   41] train: loss: 0.0204077
[Epoch 101; Iter    40/   41] train: loss: 0.0566086
[Epoch 101] ogbg-molbace: 0.913678 val loss: 0.470420
[Epoch 101] ogbg-molbace: 0.892713 test loss: 0.531617
[Epoch 102; Iter    29/   41] train: loss: 0.0917573
[Epoch 102] ogbg-molbace: 0.913326 val loss: 0.484861
[Epoch 102] ogbg-molbace: 0.897629 test loss: 0.503675
[Epoch 103; Iter    18/   41] train: loss: 0.0202853
[Epoch 103] ogbg-molbace: 0.912271 val loss: 0.532950
[Epoch 103] ogbg-molbace: 0.893942 test loss: 0.545199
[Epoch 104; Iter     7/   41] train: loss: 0.0359454
[Epoch 104; Iter    37/   41] train: loss: 0.0171932
[Epoch 104] ogbg-molbace: 0.908579 val loss: 0.526330
[Epoch 104] ogbg-molbace: 0.902546 test loss: 0.525403
[Epoch 105; Iter    26/   41] train: loss: 0.0628202
[Epoch 105] ogbg-molbace: 0.909986 val loss: 0.527251
[Epoch 105] ogbg-molbace: 0.904302 test loss: 0.471687
[Epoch 106; Iter    15/   41] train: loss: 0.0352061
[Epoch 106] ogbg-molbace: 0.909986 val loss: 0.542276
[Epoch 106] ogbg-molbace: 0.886743 test loss: 0.581033
[Epoch 107; Iter     4/   41] train: loss: 0.0315448
[Epoch 107; Iter    34/   41] train: loss: 0.0141635
[Epoch 107] ogbg-molbace: 0.918425 val loss: 0.461165
[Epoch 107] ogbg-molbace: 0.892362 test loss: 0.533835
[Epoch 108; Iter    23/   41] train: loss: 0.0258940
[Epoch 108] ogbg-molbace: 0.894515 val loss: 0.547307
[Epoch 108] ogbg-molbace: 0.885162 test loss: 0.557408
[Epoch 109; Iter    12/   41] train: loss: 0.0109300
[Epoch 109] ogbg-molbace: 0.913502 val loss: 0.522690
[Epoch 109] ogbg-molbace: 0.898156 test loss: 0.520697
[Epoch 110; Iter     1/   41] train: loss: 0.0553581
[Epoch 110; Iter    31/   41] train: loss: 0.1887144
[Epoch 110] ogbg-molbace: 0.918601 val loss: 0.475355
[Epoch 110] ogbg-molbace: 0.893591 test loss: 0.562956
[Epoch 111; Iter    20/   41] train: loss: 0.0159225
[Epoch 111] ogbg-molbace: 0.924578 val loss: 0.477527
[Epoch 111] ogbg-molbace: 0.880070 test loss: 0.607191
[Epoch 112; Iter     9/   41] train: loss: 0.0096599
[Epoch 112; Iter    39/   41] train: loss: 0.0186237
[Epoch 112] ogbg-molbace: 0.917546 val loss: 0.505624
[Epoch 112] ogbg-molbace: 0.890430 test loss: 0.567794
[Epoch 113; Iter    28/   41] train: loss: 0.0176078
[Epoch 113] ogbg-molbace: 0.911392 val loss: 0.617739
[Epoch 113] ogbg-molbace: 0.875680 test loss: 0.633143
[Epoch 114; Iter    17/   41] train: loss: 0.0069966
[Epoch 114] ogbg-molbace: 0.920007 val loss: 0.494621
[Epoch 114] ogbg-molbace: 0.887972 test loss: 0.585474
[Epoch 115; Iter     6/   41] train: loss: 0.1076152
[Epoch 115; Iter    36/   41] train: loss: 0.0226814
[Epoch 115] ogbg-molbace: 0.903305 val loss: 0.608276
[Epoch 115] ogbg-molbace: 0.886567 test loss: 0.655072
[Epoch 116; Iter    25/   41] train: loss: 0.0052102
[Epoch 116] ogbg-molbace: 0.903305 val loss: 0.702254
[Epoch 116] ogbg-molbace: 0.871817 test loss: 0.719977
[Epoch 117; Iter    14/   41] train: loss: 0.0121925
[Epoch 117] ogbg-molbace: 0.898558 val loss: 0.662831
[Epoch 117] ogbg-molbace: 0.860579 test loss: 0.755871
[Epoch 118; Iter     3/   41] train: loss: 0.0138133
[Epoch 118; Iter    33/   41] train: loss: 0.0743160
[Epoch 118] ogbg-molbace: 0.904360 val loss: 0.675619
[Epoch 118] ogbg-molbace: 0.880070 test loss: 0.682094
[Epoch 119; Iter    22/   41] train: loss: 0.0814639
[Epoch 119] ogbg-molbace: 0.909634 val loss: 0.546893
[Epoch 119] ogbg-molbace: 0.884636 test loss: 0.657357
[Epoch 120; Iter    11/   41] train: loss: 0.1609303
[Epoch 120; Iter    41/   41] train: loss: 0.0016300
[Epoch 120] ogbg-molbace: 0.912975 val loss: 0.551994
[Epoch 120] ogbg-molbace: 0.891133 test loss: 0.660364
[Epoch 121; Iter    30/   41] train: loss: 0.0060593
[Epoch 121] ogbg-molbace: 0.910162 val loss: 0.569836
[Epoch 121] ogbg-molbace: 0.892362 test loss: 0.594270
[Epoch 122; Iter    19/   41] train: loss: 0.0074327
[Epoch 122] ogbg-molbace: 0.912623 val loss: 0.575397
[Epoch 122] ogbg-molbace: 0.898156 test loss: 0.571896
[Epoch 123; Iter     8/   41] train: loss: 0.0055597
[Epoch 123; Iter    38/   41] train: loss: 0.0036741
[Epoch 123] ogbg-molbace: 0.909986 val loss: 0.614130
[Epoch 82] ogbg-molbace: 0.873737 val loss: 0.589966
[Epoch 82] ogbg-molbace: 0.888042 test loss: 0.521180
[Epoch 83; Iter    18/   36] train: loss: 0.1539240
[Epoch 83] ogbg-molbace: 0.875789 val loss: 0.599521
[Epoch 83] ogbg-molbace: 0.878013 test loss: 0.586646
[Epoch 84; Iter    12/   36] train: loss: 0.1185917
[Epoch 84] ogbg-molbace: 0.867266 val loss: 0.647157
[Epoch 84] ogbg-molbace: 0.874125 test loss: 0.602082
[Epoch 85; Iter     6/   36] train: loss: 0.0504521
[Epoch 85; Iter    36/   36] train: loss: 0.2017204
[Epoch 85] ogbg-molbace: 0.887074 val loss: 0.625297
[Epoch 85] ogbg-molbace: 0.876380 test loss: 0.628858
[Epoch 86; Iter    30/   36] train: loss: 0.2235759
[Epoch 86] ogbg-molbace: 0.872317 val loss: 0.604776
[Epoch 86] ogbg-molbace: 0.872104 test loss: 0.596587
[Epoch 87; Iter    24/   36] train: loss: 0.0651864
[Epoch 87] ogbg-molbace: 0.876105 val loss: 0.608994
[Epoch 87] ogbg-molbace: 0.866350 test loss: 0.652567
[Epoch 88; Iter    18/   36] train: loss: 0.0407460
[Epoch 88] ogbg-molbace: 0.884785 val loss: 0.610875
[Epoch 88] ogbg-molbace: 0.878246 test loss: 0.620847
[Epoch 89; Iter    12/   36] train: loss: 0.0567450
[Epoch 89] ogbg-molbace: 0.873422 val loss: 0.764110
[Epoch 89] ogbg-molbace: 0.876302 test loss: 0.678590
[Epoch 90; Iter     6/   36] train: loss: 0.0772906
[Epoch 90; Iter    36/   36] train: loss: 0.0457736
[Epoch 90] ogbg-molbace: 0.863794 val loss: 0.714620
[Epoch 90] ogbg-molbace: 0.860286 test loss: 0.700876
[Epoch 91; Iter    30/   36] train: loss: 0.0291404
[Epoch 91] ogbg-molbace: 0.868292 val loss: 0.788537
[Epoch 91] ogbg-molbace: 0.867828 test loss: 0.742467
[Epoch 92; Iter    24/   36] train: loss: 0.1337798
[Epoch 92] ogbg-molbace: 0.865372 val loss: 0.749734
[Epoch 92] ogbg-molbace: 0.872415 test loss: 0.680020
[Epoch 93; Iter    18/   36] train: loss: 0.1289333
[Epoch 93] ogbg-molbace: 0.876815 val loss: 0.656498
[Epoch 93] ogbg-molbace: 0.872570 test loss: 0.639682
[Epoch 94; Iter    12/   36] train: loss: 0.1316837
[Epoch 94] ogbg-molbace: 0.897175 val loss: 0.558779
[Epoch 94] ogbg-molbace: 0.881356 test loss: 0.580019
[Epoch 95; Iter     6/   36] train: loss: 0.0364542
[Epoch 95; Iter    36/   36] train: loss: 0.0319932
[Epoch 95] ogbg-molbace: 0.862847 val loss: 0.765818
[Epoch 95] ogbg-molbace: 0.868838 test loss: 0.742167
[Epoch 96; Iter    30/   36] train: loss: 0.1342011
[Epoch 96] ogbg-molbace: 0.862847 val loss: 0.775241
[Epoch 96] ogbg-molbace: 0.860986 test loss: 0.791055
[Epoch 97; Iter    24/   36] train: loss: 0.1823726
[Epoch 97] ogbg-molbace: 0.879893 val loss: 0.757153
[Epoch 97] ogbg-molbace: 0.867594 test loss: 0.747306
[Epoch 98; Iter    18/   36] train: loss: 0.0272610
[Epoch 98] ogbg-molbace: 0.878867 val loss: 0.668014
[Epoch 98] ogbg-molbace: 0.852511 test loss: 0.793537
[Epoch 99; Iter    12/   36] train: loss: 0.0466814
[Epoch 99] ogbg-molbace: 0.859454 val loss: 0.793653
[Epoch 99] ogbg-molbace: 0.871560 test loss: 0.716834
[Epoch 100; Iter     6/   36] train: loss: 0.0207767
[Epoch 100; Iter    36/   36] train: loss: 0.0531866
[Epoch 100] ogbg-molbace: 0.865846 val loss: 0.855929
[Epoch 100] ogbg-molbace: 0.861919 test loss: 0.815896
[Epoch 101; Iter    30/   36] train: loss: 0.1000028
[Epoch 101] ogbg-molbace: 0.872317 val loss: 0.795874
[Epoch 101] ogbg-molbace: 0.867517 test loss: 0.806760
[Epoch 102; Iter    24/   36] train: loss: 0.0330924
[Epoch 102] ogbg-molbace: 0.856534 val loss: 0.778556
[Epoch 102] ogbg-molbace: 0.863629 test loss: 0.680989
[Epoch 103; Iter    18/   36] train: loss: 0.0109483
[Epoch 103] ogbg-molbace: 0.870423 val loss: 0.760166
[Epoch 103] ogbg-molbace: 0.859120 test loss: 0.809756
[Epoch 104; Iter    12/   36] train: loss: 0.0449205
[Epoch 104] ogbg-molbace: 0.860874 val loss: 0.784708
[Epoch 104] ogbg-molbace: 0.861375 test loss: 0.803198
[Epoch 105; Iter     6/   36] train: loss: 0.0636194
[Epoch 105; Iter    36/   36] train: loss: 0.1210483
[Epoch 105] ogbg-molbace: 0.878867 val loss: 0.736622
[Epoch 105] ogbg-molbace: 0.878790 test loss: 0.690939
[Epoch 106; Iter    30/   36] train: loss: 0.0584814
[Epoch 106] ogbg-molbace: 0.881313 val loss: 0.719028
[Epoch 106] ogbg-molbace: 0.880578 test loss: 0.705462
[Epoch 107; Iter    24/   36] train: loss: 0.0069302
[Epoch 107] ogbg-molbace: 0.882655 val loss: 0.729757
[Epoch 107] ogbg-molbace: 0.875058 test loss: 0.745044
[Epoch 108; Iter    18/   36] train: loss: 0.0177384
[Epoch 108] ogbg-molbace: 0.872080 val loss: 0.804906
[Epoch 108] ogbg-molbace: 0.878479 test loss: 0.711081
[Epoch 109; Iter    12/   36] train: loss: 0.0032197
[Epoch 109] ogbg-molbace: 0.866162 val loss: 0.885309
[Epoch 109] ogbg-molbace: 0.866117 test loss: 0.845099
[Epoch 110; Iter     6/   36] train: loss: 0.0223176
[Epoch 110; Iter    36/   36] train: loss: 0.0768282
[Epoch 110] ogbg-molbace: 0.872869 val loss: 0.826765
[Epoch 110] ogbg-molbace: 0.868294 test loss: 0.802659
[Epoch 111; Iter    30/   36] train: loss: 0.0164734
[Epoch 111] ogbg-molbace: 0.876184 val loss: 0.792935
[Epoch 111] ogbg-molbace: 0.862385 test loss: 0.809109
[Epoch 112; Iter    24/   36] train: loss: 0.0064701
[Epoch 112] ogbg-molbace: 0.873106 val loss: 0.800073
[Epoch 112] ogbg-molbace: 0.862385 test loss: 0.824045
[Epoch 113; Iter    18/   36] train: loss: 0.0319367
[Epoch 113] ogbg-molbace: 0.876342 val loss: 0.794115
[Epoch 113] ogbg-molbace: 0.858342 test loss: 0.870448
[Epoch 114; Iter    12/   36] train: loss: 0.0699313
[Epoch 114] ogbg-molbace: 0.870423 val loss: 0.819535
[Epoch 114] ogbg-molbace: 0.857565 test loss: 0.876201
[Epoch 115; Iter     6/   36] train: loss: 0.0067589
[Epoch 115; Iter    36/   36] train: loss: 0.2684565
[Epoch 115] ogbg-molbace: 0.879893 val loss: 0.810072
[Epoch 115] ogbg-molbace: 0.868838 test loss: 0.809481
[Epoch 116; Iter    30/   36] train: loss: 0.0138069
[Epoch 116] ogbg-molbace: 0.884628 val loss: 0.860122
[Epoch 116] ogbg-molbace: 0.871482 test loss: 0.915059
[Epoch 117; Iter    24/   36] train: loss: 0.0126682
[Epoch 117] ogbg-molbace: 0.869871 val loss: 0.880681
[Epoch 117] ogbg-molbace: 0.870782 test loss: 0.894506
[Epoch 118; Iter    18/   36] train: loss: 0.0056675
[Epoch 118] ogbg-molbace: 0.875710 val loss: 0.833462
[Epoch 118] ogbg-molbace: 0.864562 test loss: 0.909913
[Epoch 119; Iter    12/   36] train: loss: 0.0129865
[Epoch 119] ogbg-molbace: 0.872633 val loss: 0.894975
[Epoch 119] ogbg-molbace: 0.876225 test loss: 0.835815
[Epoch 120; Iter     6/   36] train: loss: 0.0085579
[Epoch 120; Iter    36/   36] train: loss: 0.0785606
[Epoch 120] ogbg-molbace: 0.865451 val loss: 0.886385
[Epoch 120] ogbg-molbace: 0.869383 test loss: 0.843561
[Epoch 121; Iter    30/   36] train: loss: 0.0350362
[Epoch 121] ogbg-molbace: 0.863952 val loss: 0.997459
[Epoch 121] ogbg-molbace: 0.877002 test loss: 0.947146
[Epoch 122; Iter    24/   36] train: loss: 0.1361338
[Epoch 122] ogbg-molbace: 0.863557 val loss: 0.981879
[Epoch 122] ogbg-molbace: 0.880501 test loss: 0.805984
[Epoch 123; Iter    18/   36] train: loss: 0.0264648
[Epoch 123] ogbg-molbace: 0.860874 val loss: 0.917951
[Epoch 123] ogbg-molbace: 0.869305 test loss: 0.853287
[Epoch 124; Iter    12/   36] train: loss: 0.0113041
[Epoch 124] ogbg-molbace: 0.868924 val loss: 0.883316
[Epoch 124] ogbg-molbace: 0.862308 test loss: 0.883395
[Epoch 125; Iter     6/   36] train: loss: 0.0252168
[Epoch 125; Iter    36/   36] train: loss: 0.0791572
[Epoch 125] ogbg-molbace: 0.865136 val loss: 0.922702
[Epoch 125] ogbg-molbace: 0.855388 test loss: 0.981543
[Epoch 126; Iter    30/   36] train: loss: 0.0124352
[Epoch 126] ogbg-molbace: 0.863952 val loss: 0.969479
[Epoch 126] ogbg-molbace: 0.870704 test loss: 0.963726
[Epoch 127; Iter    24/   36] train: loss: 0.0140638
[Epoch 127] ogbg-molbace: 0.863636 val loss: 0.916043
[Epoch 127] ogbg-molbace: 0.864951 test loss: 0.942857
[Epoch 128; Iter    18/   36] train: loss: 0.0055948
[Epoch 128] ogbg-molbace: 0.871370 val loss: 0.923777
[Epoch 128] ogbg-molbace: 0.877313 test loss: 0.930068
[Epoch 129; Iter    12/   36] train: loss: 0.0044656
[Epoch 129] ogbg-molbace: 0.870028 val loss: 0.919367
[Epoch 129] ogbg-molbace: 0.874981 test loss: 0.933441
[Epoch 82] ogbg-molbace: 0.866556 val loss: 0.563101
[Epoch 82] ogbg-molbace: 0.855699 test loss: 0.601652
[Epoch 83; Iter    18/   36] train: loss: 0.1712612
[Epoch 83] ogbg-molbace: 0.868687 val loss: 0.593249
[Epoch 83] ogbg-molbace: 0.863007 test loss: 0.682343
[Epoch 84; Iter    12/   36] train: loss: 0.0608381
[Epoch 84] ogbg-molbace: 0.867740 val loss: 0.659511
[Epoch 84] ogbg-molbace: 0.880578 test loss: 0.647218
[Epoch 85; Iter     6/   36] train: loss: 0.4239863
[Epoch 85; Iter    36/   36] train: loss: 0.2815012
[Epoch 85] ogbg-molbace: 0.863163 val loss: 0.568728
[Epoch 85] ogbg-molbace: 0.861452 test loss: 0.585149
[Epoch 86; Iter    30/   36] train: loss: 0.3579283
[Epoch 86] ogbg-molbace: 0.872790 val loss: 0.587355
[Epoch 86] ogbg-molbace: 0.852356 test loss: 0.665441
[Epoch 87; Iter    24/   36] train: loss: 0.1860113
[Epoch 87] ogbg-molbace: 0.855666 val loss: 0.589028
[Epoch 87] ogbg-molbace: 0.868294 test loss: 0.588170
[Epoch 88; Iter    18/   36] train: loss: 0.0887654
[Epoch 88] ogbg-molbace: 0.868845 val loss: 0.576850
[Epoch 88] ogbg-molbace: 0.879490 test loss: 0.575737
[Epoch 89; Iter    12/   36] train: loss: 0.1412488
[Epoch 89] ogbg-molbace: 0.878472 val loss: 0.559983
[Epoch 89] ogbg-molbace: 0.881200 test loss: 0.572924
[Epoch 90; Iter     6/   36] train: loss: 0.0677407
[Epoch 90; Iter    36/   36] train: loss: 0.1316659
[Epoch 90] ogbg-molbace: 0.878867 val loss: 0.572253
[Epoch 90] ogbg-molbace: 0.885554 test loss: 0.580015
[Epoch 91; Iter    30/   36] train: loss: 0.0560347
[Epoch 91] ogbg-molbace: 0.875947 val loss: 0.596215
[Epoch 91] ogbg-molbace: 0.863241 test loss: 1.177631
[Epoch 92; Iter    24/   36] train: loss: 0.0686633
[Epoch 92] ogbg-molbace: 0.875237 val loss: 0.590958
[Epoch 92] ogbg-molbace: 0.869694 test loss: 0.683453
[Epoch 93; Iter    18/   36] train: loss: 0.1217403
[Epoch 93] ogbg-molbace: 0.868687 val loss: 0.658253
[Epoch 93] ogbg-molbace: 0.865729 test loss: 0.763937
[Epoch 94; Iter    12/   36] train: loss: 0.1690196
[Epoch 94] ogbg-molbace: 0.864978 val loss: 0.724441
[Epoch 94] ogbg-molbace: 0.872415 test loss: 0.793876
[Epoch 95; Iter     6/   36] train: loss: 0.0860264
[Epoch 95; Iter    36/   36] train: loss: 0.0118861
[Epoch 95] ogbg-molbace: 0.864741 val loss: 0.678590
[Epoch 95] ogbg-molbace: 0.878013 test loss: 0.700692
[Epoch 96; Iter    30/   36] train: loss: 0.1127736
[Epoch 96] ogbg-molbace: 0.854877 val loss: 0.733194
[Epoch 96] ogbg-molbace: 0.849246 test loss: 0.863115
[Epoch 97; Iter    24/   36] train: loss: 0.0357193
[Epoch 97] ogbg-molbace: 0.873737 val loss: 0.688917
[Epoch 97] ogbg-molbace: 0.879334 test loss: 0.723959
[Epoch 98; Iter    18/   36] train: loss: 0.0497756
[Epoch 98] ogbg-molbace: 0.878788 val loss: 0.655390
[Epoch 98] ogbg-molbace: 0.869460 test loss: 0.772831
[Epoch 99; Iter    12/   36] train: loss: 0.0157309
[Epoch 99] ogbg-molbace: 0.869555 val loss: 0.772215
[Epoch 99] ogbg-molbace: 0.852045 test loss: 0.946328
[Epoch 100; Iter     6/   36] train: loss: 0.0626246
[Epoch 100; Iter    36/   36] train: loss: 0.2504755
[Epoch 100] ogbg-molbace: 0.864504 val loss: 0.664943
[Epoch 100] ogbg-molbace: 0.848702 test loss: 0.818376
[Epoch 101; Iter    30/   36] train: loss: 0.2096855
[Epoch 101] ogbg-molbace: 0.878946 val loss: 0.688558
[Epoch 101] ogbg-molbace: 0.874981 test loss: 0.740376
[Epoch 102; Iter    24/   36] train: loss: 0.0182378
[Epoch 102] ogbg-molbace: 0.862137 val loss: 0.731341
[Epoch 102] ogbg-molbace: 0.862930 test loss: 0.824292
[Epoch 103; Iter    18/   36] train: loss: 0.0307838
[Epoch 103] ogbg-molbace: 0.865294 val loss: 0.777297
[Epoch 103] ogbg-molbace: 0.866584 test loss: 0.818813
[Epoch 104; Iter    12/   36] train: loss: 0.0166017
[Epoch 104] ogbg-molbace: 0.861111 val loss: 0.800504
[Epoch 104] ogbg-molbace: 0.860908 test loss: 0.982116
[Epoch 105; Iter     6/   36] train: loss: 0.1091209
[Epoch 105; Iter    36/   36] train: loss: 0.1917163
[Epoch 105] ogbg-molbace: 0.858902 val loss: 0.763033
[Epoch 105] ogbg-molbace: 0.867750 test loss: 0.785205
[Epoch 106; Iter    30/   36] train: loss: 0.0456668
[Epoch 106] ogbg-molbace: 0.871370 val loss: 0.722499
[Epoch 106] ogbg-molbace: 0.854766 test loss: 0.872200
[Epoch 107; Iter    24/   36] train: loss: 0.0312761
[Epoch 107] ogbg-molbace: 0.862295 val loss: 0.803151
[Epoch 107] ogbg-molbace: 0.870316 test loss: 0.841667
[Epoch 108; Iter    18/   36] train: loss: 0.0239434
[Epoch 108] ogbg-molbace: 0.867740 val loss: 0.751548
[Epoch 108] ogbg-molbace: 0.868527 test loss: 0.884794
[Epoch 109; Iter    12/   36] train: loss: 0.0471724
[Epoch 109] ogbg-molbace: 0.868608 val loss: 0.749322
[Epoch 109] ogbg-molbace: 0.863552 test loss: 0.852542
[Epoch 110; Iter     6/   36] train: loss: 0.1048564
[Epoch 110; Iter    36/   36] train: loss: 0.3819821
[Epoch 110] ogbg-molbace: 0.860401 val loss: 0.932731
[Epoch 110] ogbg-molbace: 0.858576 test loss: 1.024140
[Epoch 111; Iter    30/   36] train: loss: 0.1017588
[Epoch 111] ogbg-molbace: 0.871528 val loss: 0.714332
[Epoch 111] ogbg-molbace: 0.850490 test loss: 0.922758
[Epoch 112; Iter    24/   36] train: loss: 0.0508028
[Epoch 112] ogbg-molbace: 0.876263 val loss: 0.725427
[Epoch 112] ogbg-molbace: 0.880034 test loss: 0.807366
[Epoch 113; Iter    18/   36] train: loss: 0.1043424
[Epoch 113] ogbg-molbace: 0.869239 val loss: 0.771856
[Epoch 113] ogbg-molbace: 0.864251 test loss: 0.876372
[Epoch 114; Iter    12/   36] train: loss: 0.0354126
[Epoch 114] ogbg-molbace: 0.878630 val loss: 0.798291
[Epoch 114] ogbg-molbace: 0.874592 test loss: 0.905536
[Epoch 115; Iter     6/   36] train: loss: 0.0734281
[Epoch 115; Iter    36/   36] train: loss: 0.0258084
[Epoch 115] ogbg-molbace: 0.873185 val loss: 0.770187
[Epoch 115] ogbg-molbace: 0.864018 test loss: 0.947811
[Epoch 116; Iter    30/   36] train: loss: 0.0289984
[Epoch 116] ogbg-molbace: 0.872790 val loss: 0.768269
[Epoch 116] ogbg-molbace: 0.873970 test loss: 0.865481
[Epoch 117; Iter    24/   36] train: loss: 0.0050984
[Epoch 117] ogbg-molbace: 0.889126 val loss: 0.711595
[Epoch 117] ogbg-molbace: 0.867905 test loss: 0.897154
[Epoch 118; Iter    18/   36] train: loss: 0.0203077
[Epoch 118] ogbg-molbace: 0.881234 val loss: 0.752828
[Epoch 118] ogbg-molbace: 0.859664 test loss: 0.951877
[Epoch 119; Iter    12/   36] train: loss: 0.0195109
[Epoch 119] ogbg-molbace: 0.883681 val loss: 0.764100
[Epoch 119] ogbg-molbace: 0.870782 test loss: 0.926621
[Epoch 120; Iter     6/   36] train: loss: 0.0063495
[Epoch 120; Iter    36/   36] train: loss: 0.3599285
[Epoch 120] ogbg-molbace: 0.880445 val loss: 0.805411
[Epoch 120] ogbg-molbace: 0.879412 test loss: 0.879517
[Epoch 121; Iter    30/   36] train: loss: 0.0213358
[Epoch 121] ogbg-molbace: 0.878946 val loss: 0.809344
[Epoch 121] ogbg-molbace: 0.877080 test loss: 0.942073
[Epoch 122; Iter    24/   36] train: loss: 0.0047612
[Epoch 122] ogbg-molbace: 0.872001 val loss: 0.791032
[Epoch 122] ogbg-molbace: 0.858031 test loss: 1.028690
[Epoch 123; Iter    18/   36] train: loss: 0.0368180
[Epoch 123] ogbg-molbace: 0.869318 val loss: 0.838700
[Epoch 123] ogbg-molbace: 0.866506 test loss: 1.007845
[Epoch 124; Iter    12/   36] train: loss: 0.0622365
[Epoch 124] ogbg-molbace: 0.871607 val loss: 0.844836
[Epoch 124] ogbg-molbace: 0.861375 test loss: 1.032164
[Epoch 125; Iter     6/   36] train: loss: 0.0058185
[Epoch 125; Iter    36/   36] train: loss: 0.0044612
[Epoch 125] ogbg-molbace: 0.870896 val loss: 0.858202
[Epoch 125] ogbg-molbace: 0.864873 test loss: 1.073905
[Epoch 126; Iter    30/   36] train: loss: 0.0070822
[Epoch 126] ogbg-molbace: 0.867109 val loss: 0.843408
[Epoch 126] ogbg-molbace: 0.868294 test loss: 0.949043
[Epoch 127; Iter    24/   36] train: loss: 0.0128495
[Epoch 127] ogbg-molbace: 0.871765 val loss: 0.823294
[Epoch 127] ogbg-molbace: 0.868061 test loss: 0.970715
[Epoch 128; Iter    18/   36] train: loss: 0.0405131
[Epoch 128] ogbg-molbace: 0.862689 val loss: 0.880107
[Epoch 128] ogbg-molbace: 0.858031 test loss: 1.100889
[Epoch 129; Iter    12/   36] train: loss: 0.0139432
[Epoch 129] ogbg-molbace: 0.861585 val loss: 0.987584
[Epoch 129] ogbg-molbace: 0.858731 test loss: 1.145180
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 78] ogbg-molbace: 0.880274 val loss: 0.457946
[Epoch 78] ogbg-molbace: 0.843371 test loss: 2.155734
[Epoch 79; Iter    12/   41] train: loss: 0.1486048
[Epoch 79] ogbg-molbace: 0.898383 val loss: 0.405992
[Epoch 79] ogbg-molbace: 0.881826 test loss: 0.422368
[Epoch 80; Iter     1/   41] train: loss: 0.1023610
[Epoch 80; Iter    31/   41] train: loss: 0.3473668
[Epoch 80] ogbg-molbace: 0.918601 val loss: 0.364087
[Epoch 80] ogbg-molbace: 0.885865 test loss: 0.466349
[Epoch 81; Iter    20/   41] train: loss: 0.1723717
[Epoch 81] ogbg-molbace: 0.905591 val loss: 0.360842
[Epoch 81] ogbg-molbace: 0.862862 test loss: 0.829362
[Epoch 82; Iter     9/   41] train: loss: 0.0468816
[Epoch 82; Iter    39/   41] train: loss: 0.2387491
[Epoch 82] ogbg-molbace: 0.906118 val loss: 0.388054
[Epoch 82] ogbg-molbace: 0.872344 test loss: 0.502698
[Epoch 83; Iter    28/   41] train: loss: 0.1097254
[Epoch 83] ogbg-molbace: 0.895745 val loss: 0.420551
[Epoch 83] ogbg-molbace: 0.884284 test loss: 0.629387
[Epoch 84; Iter    17/   41] train: loss: 0.1589671
[Epoch 84] ogbg-molbace: 0.893108 val loss: 0.567093
[Epoch 84] ogbg-molbace: 0.838455 test loss: 0.665478
[Epoch 85; Iter     6/   41] train: loss: 0.2249057
[Epoch 85; Iter    36/   41] train: loss: 0.2514650
[Epoch 85] ogbg-molbace: 0.883439 val loss: 0.666653
[Epoch 85] ogbg-molbace: 0.847586 test loss: 0.631714
[Epoch 86; Iter    25/   41] train: loss: 0.2168472
[Epoch 86] ogbg-molbace: 0.897855 val loss: 0.421013
[Epoch 86] ogbg-molbace: 0.867954 test loss: 1.034762
[Epoch 87; Iter    14/   41] train: loss: 0.3134209
[Epoch 87] ogbg-molbace: 0.898031 val loss: 0.429830
[Epoch 87] ogbg-molbace: 0.846883 test loss: 0.543936
[Epoch 88; Iter     3/   41] train: loss: 0.4046195
[Epoch 88; Iter    33/   41] train: loss: 0.0452177
[Epoch 88] ogbg-molbace: 0.914557 val loss: 0.375849
[Epoch 88] ogbg-molbace: 0.869886 test loss: 0.483019
[Epoch 89; Iter    22/   41] train: loss: 0.2970932
[Epoch 89] ogbg-molbace: 0.912271 val loss: 0.445512
[Epoch 89] ogbg-molbace: 0.893415 test loss: 0.470853
[Epoch 90; Iter    11/   41] train: loss: 0.1573528
[Epoch 90; Iter    41/   41] train: loss: 0.9127882
[Epoch 90] ogbg-molbace: 0.908228 val loss: 0.356449
[Epoch 90] ogbg-molbace: 0.878490 test loss: 0.417517
[Epoch 91; Iter    30/   41] train: loss: 0.4280120
[Epoch 91] ogbg-molbace: 0.887131 val loss: 0.447309
[Epoch 91] ogbg-molbace: 0.871993 test loss: 0.517734
[Epoch 92; Iter    19/   41] train: loss: 0.2214160
[Epoch 92] ogbg-molbace: 0.907700 val loss: 0.437665
[Epoch 92] ogbg-molbace: 0.877085 test loss: 0.528078
[Epoch 93; Iter     8/   41] train: loss: 0.2176024
[Epoch 93; Iter    38/   41] train: loss: 0.0422330
[Epoch 93] ogbg-molbace: 0.903481 val loss: 0.404663
[Epoch 93] ogbg-molbace: 0.876383 test loss: 0.467870
[Epoch 94; Iter    27/   41] train: loss: 0.2017918
[Epoch 94] ogbg-molbace: 0.924754 val loss: 0.355802
[Epoch 94] ogbg-molbace: 0.890606 test loss: 0.433635
[Epoch 95; Iter    16/   41] train: loss: 0.1085130
[Epoch 95] ogbg-molbace: 0.900668 val loss: 0.444515
[Epoch 95] ogbg-molbace: 0.893415 test loss: 0.433368
[Epoch 96; Iter     5/   41] train: loss: 0.1044199
[Epoch 96; Iter    35/   41] train: loss: 0.0759795
[Epoch 96] ogbg-molbace: 0.894515 val loss: 0.486327
[Epoch 96] ogbg-molbace: 0.890255 test loss: 0.450800
[Epoch 97; Iter    24/   41] train: loss: 0.0299502
[Epoch 97] ogbg-molbace: 0.908931 val loss: 0.404700
[Epoch 97] ogbg-molbace: 0.904126 test loss: 0.407633
[Epoch 98; Iter    13/   41] train: loss: 0.0249708
[Epoch 98] ogbg-molbace: 0.911920 val loss: 0.415329
[Epoch 98] ogbg-molbace: 0.897103 test loss: 0.450616
[Epoch 99; Iter     2/   41] train: loss: 0.0218302
[Epoch 99; Iter    32/   41] train: loss: 0.0618604
[Epoch 99] ogbg-molbace: 0.906118 val loss: 0.455597
[Epoch 99] ogbg-molbace: 0.897629 test loss: 0.438885
[Epoch 100; Iter    21/   41] train: loss: 0.0211812
[Epoch 100] ogbg-molbace: 0.912447 val loss: 0.436323
[Epoch 100] ogbg-molbace: 0.899385 test loss: 0.458996
[Epoch 101; Iter    10/   41] train: loss: 0.0603523
[Epoch 101; Iter    40/   41] train: loss: 0.0282220
[Epoch 101] ogbg-molbace: 0.902250 val loss: 0.474831
[Epoch 101] ogbg-molbace: 0.896576 test loss: 0.514985
[Epoch 102; Iter    29/   41] train: loss: 0.0516998
[Epoch 102] ogbg-molbace: 0.907349 val loss: 0.462436
[Epoch 102] ogbg-molbace: 0.880246 test loss: 0.575550
[Epoch 103; Iter    18/   41] train: loss: 0.0613508
[Epoch 103] ogbg-molbace: 0.904712 val loss: 0.471975
[Epoch 103] ogbg-molbace: 0.895874 test loss: 0.494542
[Epoch 104; Iter     7/   41] train: loss: 0.0402768
[Epoch 104; Iter    37/   41] train: loss: 0.0104290
[Epoch 104] ogbg-molbace: 0.912623 val loss: 0.464569
[Epoch 104] ogbg-molbace: 0.906058 test loss: 0.443125
[Epoch 105; Iter    26/   41] train: loss: 0.0591897
[Epoch 105] ogbg-molbace: 0.909283 val loss: 0.522655
[Epoch 105] ogbg-molbace: 0.864267 test loss: 0.666115
[Epoch 106; Iter    15/   41] train: loss: 0.0275088
[Epoch 106] ogbg-molbace: 0.891702 val loss: 0.532636
[Epoch 106] ogbg-molbace: 0.881124 test loss: 0.578816
[Epoch 107; Iter     4/   41] train: loss: 0.1446220
[Epoch 107; Iter    34/   41] train: loss: 0.0218548
[Epoch 107] ogbg-molbace: 0.915084 val loss: 0.479221
[Epoch 107] ogbg-molbace: 0.895171 test loss: 0.494910
[Epoch 108; Iter    23/   41] train: loss: 0.0221351
[Epoch 108] ogbg-molbace: 0.905767 val loss: 0.503709
[Epoch 108] ogbg-molbace: 0.886392 test loss: 0.621713
[Epoch 109; Iter    12/   41] train: loss: 0.0740708
[Epoch 109] ogbg-molbace: 0.905063 val loss: 0.514416
[Epoch 109] ogbg-molbace: 0.881475 test loss: 0.566417
[Epoch 110; Iter     1/   41] train: loss: 0.2929395
[Epoch 110; Iter    31/   41] train: loss: 0.0079358
[Epoch 110] ogbg-molbace: 0.919480 val loss: 0.458747
[Epoch 110] ogbg-molbace: 0.867779 test loss: 0.663509
[Epoch 111; Iter    20/   41] train: loss: 0.0608073
[Epoch 111] ogbg-molbace: 0.909810 val loss: 0.488178
[Epoch 111] ogbg-molbace: 0.883055 test loss: 0.609116
[Epoch 112; Iter     9/   41] train: loss: 0.0579596
[Epoch 112; Iter    39/   41] train: loss: 0.0323760
[Epoch 112] ogbg-molbace: 0.892053 val loss: 0.628129
[Epoch 112] ogbg-molbace: 0.888323 test loss: 0.603293
[Epoch 113; Iter    28/   41] train: loss: 0.0166064
[Epoch 113] ogbg-molbace: 0.876582 val loss: 0.906181
[Epoch 113] ogbg-molbace: 0.893064 test loss: 0.559489
[Epoch 114; Iter    17/   41] train: loss: 0.0764112
[Epoch 114] ogbg-molbace: 0.880098 val loss: 0.656792
[Epoch 114] ogbg-molbace: 0.876910 test loss: 0.640857
[Epoch 115; Iter     6/   41] train: loss: 0.0297481
[Epoch 115; Iter    36/   41] train: loss: 0.1967971
[Epoch 115] ogbg-molbace: 0.898383 val loss: 0.532620
[Epoch 115] ogbg-molbace: 0.878841 test loss: 0.588231
[Epoch 116; Iter    25/   41] train: loss: 0.0185480
[Epoch 116] ogbg-molbace: 0.909459 val loss: 0.513395
[Epoch 116] ogbg-molbace: 0.889025 test loss: 0.584380
[Epoch 117; Iter    14/   41] train: loss: 0.0163086
[Epoch 117] ogbg-molbace: 0.922996 val loss: 0.442265
[Epoch 117] ogbg-molbace: 0.883582 test loss: 0.556741
[Epoch 118; Iter     3/   41] train: loss: 0.0055934
[Epoch 118; Iter    33/   41] train: loss: 0.0720067
[Epoch 118] ogbg-molbace: 0.908404 val loss: 0.505503
[Epoch 118] ogbg-molbace: 0.880070 test loss: 0.600665
[Epoch 119; Iter    22/   41] train: loss: 0.0140197
[Epoch 119] ogbg-molbace: 0.905591 val loss: 0.516120
[Epoch 119] ogbg-molbace: 0.874978 test loss: 0.692359
[Epoch 120; Iter    11/   41] train: loss: 0.0390188
[Epoch 120; Iter    41/   41] train: loss: 0.0701838
[Epoch 120] ogbg-molbace: 0.897504 val loss: 0.577924
[Epoch 120] ogbg-molbace: 0.879543 test loss: 0.545630
[Epoch 121; Iter    30/   41] train: loss: 0.1294298
[Epoch 121] ogbg-molbace: 0.907349 val loss: 0.566338
[Epoch 121] ogbg-molbace: 0.889377 test loss: 0.574643
[Epoch 122; Iter    19/   41] train: loss: 0.0266900
[Epoch 122] ogbg-molbace: 0.906294 val loss: 0.519729
[Epoch 122] ogbg-molbace: 0.881475 test loss: 0.580534
Early stopping criterion based on -ogbg-molbace- that should be max reached after 122 epochs. Best model checkpoint was in epoch 62.
Statistics on  val_best_checkpoint
mean_pred: 0.11895731836557388
std_pred: 3.6584131717681885
mean_targets: 0.5231788158416748
std_targets: 0.5011245608329773
prcauc: 0.9292270072143455
rocauc: 0.9293248945147679
ogbg-molbace: 0.9293248945147679
BCEWithLogitsLoss: 0.2908921704317133
Statistics on  test
mean_pred: -0.10458095371723175
std_pred: 4.86198616027832
mean_targets: 0.44078949093818665
std_targets: 0.4981229901313782
prcauc: 0.8331991071961606
rocauc: 0.8872695346795434
ogbg-molbace: 0.8872695346795434
BCEWithLogitsLoss: 0.4085416942834854
Statistics on  train
mean_pred: -0.45202744007110596
std_pred: 4.256838798522949
mean_targets: 0.45041322708129883
std_targets: 0.497740775346756
prcauc: 0.9718190289211897
rocauc: 0.9769966199903428
ogbg-molbace: 0.9769966199903428
BCEWithLogitsLoss: 0.19623219648875842
[Epoch 87; Iter     4/   31] train: loss: 0.1901402
[Epoch 87] ogbg-molbace: 0.903775 val loss: 0.432174
[Epoch 87] ogbg-molbace: 0.866547 test loss: 0.543480
[Epoch 88; Iter     3/   31] train: loss: 0.2219070
[Epoch 88] ogbg-molbace: 0.903820 val loss: 0.448545
[Epoch 88] ogbg-molbace: 0.866329 test loss: 0.575009
[Epoch 89; Iter     2/   31] train: loss: 0.1109111
[Epoch 89] ogbg-molbace: 0.904314 val loss: 0.436262
[Epoch 89] ogbg-molbace: 0.875055 test loss: 0.533034
[Epoch 90; Iter     1/   31] train: loss: 0.0627480
[Epoch 90; Iter    31/   31] train: loss: 0.9106069
[Epoch 90] ogbg-molbace: 0.889023 val loss: 0.475318
[Epoch 90] ogbg-molbace: 0.865326 test loss: 0.582434
[Epoch 91; Iter    30/   31] train: loss: 0.1325400
[Epoch 91] ogbg-molbace: 0.907273 val loss: 0.464991
[Epoch 91] ogbg-molbace: 0.861356 test loss: 0.633123
[Epoch 92; Iter    29/   31] train: loss: 0.4356840
[Epoch 92] ogbg-molbace: 0.905345 val loss: 0.434422
[Epoch 92] ogbg-molbace: 0.870648 test loss: 0.554956
[Epoch 93; Iter    28/   31] train: loss: 0.2783200
[Epoch 93] ogbg-molbace: 0.905031 val loss: 0.467185
[Epoch 93] ogbg-molbace: 0.869689 test loss: 0.592426
[Epoch 94; Iter    27/   31] train: loss: 0.0343709
[Epoch 94] ogbg-molbace: 0.900188 val loss: 0.487104
[Epoch 94] ogbg-molbace: 0.862098 test loss: 0.597650
[Epoch 95; Iter    26/   31] train: loss: 0.0512487
[Epoch 95] ogbg-molbace: 0.902565 val loss: 0.485001
[Epoch 95] ogbg-molbace: 0.879286 test loss: 0.584897
[Epoch 96; Iter    25/   31] train: loss: 0.0829665
[Epoch 96] ogbg-molbace: 0.898753 val loss: 0.480819
[Epoch 96] ogbg-molbace: 0.863712 test loss: 0.599907
[Epoch 97; Iter    24/   31] train: loss: 0.2129787
[Epoch 97] ogbg-molbace: 0.896108 val loss: 0.493202
[Epoch 97] ogbg-molbace: 0.867420 test loss: 0.614939
[Epoch 98; Iter    23/   31] train: loss: 0.1107350
[Epoch 98] ogbg-molbace: 0.902161 val loss: 0.545857
[Epoch 98] ogbg-molbace: 0.883562 test loss: 0.621615
[Epoch 99; Iter    22/   31] train: loss: 0.1663121
[Epoch 99] ogbg-molbace: 0.907856 val loss: 0.459798
[Epoch 99] ogbg-molbace: 0.877585 test loss: 0.576380
[Epoch 100; Iter    21/   31] train: loss: 0.1299234
[Epoch 100] ogbg-molbace: 0.902072 val loss: 0.581538
[Epoch 100] ogbg-molbace: 0.882776 test loss: 0.622291
[Epoch 101; Iter    20/   31] train: loss: 0.0987818
[Epoch 101] ogbg-molbace: 0.902475 val loss: 0.513889
[Epoch 101] ogbg-molbace: 0.873048 test loss: 0.605604
[Epoch 102; Iter    19/   31] train: loss: 0.0400373
[Epoch 102] ogbg-molbace: 0.892431 val loss: 0.556298
[Epoch 102] ogbg-molbace: 0.853547 test loss: 0.671011
[Epoch 103; Iter    18/   31] train: loss: 0.0174836
[Epoch 103] ogbg-molbace: 0.899650 val loss: 0.631347
[Epoch 103] ogbg-molbace: 0.872437 test loss: 0.760865
[Epoch 104; Iter    17/   31] train: loss: 0.0974910
[Epoch 104] ogbg-molbace: 0.892969 val loss: 0.552109
[Epoch 104] ogbg-molbace: 0.874487 test loss: 0.635526
[Epoch 105; Iter    16/   31] train: loss: 0.2181075
[Epoch 105] ogbg-molbace: 0.878666 val loss: 0.692724
[Epoch 105] ogbg-molbace: 0.849359 test loss: 0.782695
[Epoch 106; Iter    15/   31] train: loss: 0.1655217
[Epoch 106] ogbg-molbace: 0.895570 val loss: 0.503838
[Epoch 106] ogbg-molbace: 0.879461 test loss: 0.567839
[Epoch 107; Iter    14/   31] train: loss: 0.0647094
[Epoch 107] ogbg-molbace: 0.900547 val loss: 0.498928
[Epoch 107] ogbg-molbace: 0.869558 test loss: 0.631972
[Epoch 108; Iter    13/   31] train: loss: 0.0500202
[Epoch 108] ogbg-molbace: 0.896063 val loss: 0.543376
[Epoch 108] ogbg-molbace: 0.875360 test loss: 0.617318
[Epoch 109; Iter    12/   31] train: loss: 0.0991004
[Epoch 109] ogbg-molbace: 0.899516 val loss: 0.550758
[Epoch 109] ogbg-molbace: 0.874095 test loss: 0.662235
[Epoch 110; Iter    11/   31] train: loss: 0.0324952
[Epoch 110] ogbg-molbace: 0.895884 val loss: 0.587264
[Epoch 110] ogbg-molbace: 0.878632 test loss: 0.641229
[Epoch 111; Iter    10/   31] train: loss: 0.0158617
[Epoch 111] ogbg-molbace: 0.892028 val loss: 0.639371
[Epoch 111] ogbg-molbace: 0.881642 test loss: 0.663357
[Epoch 112; Iter     9/   31] train: loss: 0.1009399
[Epoch 112] ogbg-molbace: 0.895435 val loss: 0.575101
[Epoch 112] ogbg-molbace: 0.886572 test loss: 0.618218
[Epoch 113; Iter     8/   31] train: loss: 0.0134201
[Epoch 113] ogbg-molbace: 0.891938 val loss: 0.579414
[Epoch 113] ogbg-molbace: 0.881206 test loss: 0.624286
[Epoch 114; Iter     7/   31] train: loss: 0.0277897
[Epoch 114] ogbg-molbace: 0.897050 val loss: 0.589640
[Epoch 114] ogbg-molbace: 0.878850 test loss: 0.659272
[Epoch 115; Iter     6/   31] train: loss: 0.0184491
[Epoch 115] ogbg-molbace: 0.890100 val loss: 0.603191
[Epoch 115] ogbg-molbace: 0.879984 test loss: 0.642633
[Epoch 116; Iter     5/   31] train: loss: 0.0637259
[Epoch 116] ogbg-molbace: 0.889113 val loss: 0.590132
[Epoch 116] ogbg-molbace: 0.873179 test loss: 0.663659
[Epoch 117; Iter     4/   31] train: loss: 0.0217416
[Epoch 117] ogbg-molbace: 0.895480 val loss: 0.596930
[Epoch 117] ogbg-molbace: 0.876538 test loss: 0.672830
[Epoch 118; Iter     3/   31] train: loss: 0.0206034
[Epoch 118] ogbg-molbace: 0.899067 val loss: 0.628159
[Epoch 118] ogbg-molbace: 0.881119 test loss: 0.693212
[Epoch 119; Iter     2/   31] train: loss: 0.0243719
[Epoch 119] ogbg-molbace: 0.891355 val loss: 0.651289
[Epoch 119] ogbg-molbace: 0.888404 test loss: 0.622566
[Epoch 120; Iter     1/   31] train: loss: 0.1226650
[Epoch 120; Iter    31/   31] train: loss: 0.0154238
[Epoch 120] ogbg-molbace: 0.893283 val loss: 0.585404
[Epoch 120] ogbg-molbace: 0.877192 test loss: 0.648053
[Epoch 121; Iter    30/   31] train: loss: 0.0574764
[Epoch 121] ogbg-molbace: 0.894942 val loss: 0.611434
[Epoch 121] ogbg-molbace: 0.881075 test loss: 0.669572
[Epoch 122; Iter    29/   31] train: loss: 0.0421025
[Epoch 122] ogbg-molbace: 0.897184 val loss: 0.592914
[Epoch 122] ogbg-molbace: 0.880551 test loss: 0.655687
[Epoch 123; Iter    28/   31] train: loss: 0.0225106
[Epoch 123] ogbg-molbace: 0.893283 val loss: 0.642877
[Epoch 123] ogbg-molbace: 0.882602 test loss: 0.657017
[Epoch 124; Iter    27/   31] train: loss: 0.0088598
[Epoch 124] ogbg-molbace: 0.895346 val loss: 0.637496
[Epoch 124] ogbg-molbace: 0.877279 test loss: 0.682639
[Epoch 125; Iter    26/   31] train: loss: 0.0123475
[Epoch 125] ogbg-molbace: 0.896646 val loss: 0.625188
[Epoch 125] ogbg-molbace: 0.876581 test loss: 0.712204
[Epoch 126; Iter    25/   31] train: loss: 0.0141086
[Epoch 126] ogbg-molbace: 0.895570 val loss: 0.668008
[Epoch 126] ogbg-molbace: 0.881991 test loss: 0.694151
[Epoch 127; Iter    24/   31] train: loss: 0.0091868
[Epoch 127] ogbg-molbace: 0.892028 val loss: 0.694449
[Epoch 127] ogbg-molbace: 0.883780 test loss: 0.707030
[Epoch 128; Iter    23/   31] train: loss: 0.0173048
[Epoch 128] ogbg-molbace: 0.887364 val loss: 0.681712
[Epoch 128] ogbg-molbace: 0.869994 test loss: 0.757656
[Epoch 129; Iter    22/   31] train: loss: 0.0303980
[Epoch 129] ogbg-molbace: 0.889965 val loss: 0.714589
[Epoch 129] ogbg-molbace: 0.879679 test loss: 0.724764
[Epoch 130; Iter    21/   31] train: loss: 0.0130722
[Epoch 130] ogbg-molbace: 0.882791 val loss: 0.706035
[Epoch 130] ogbg-molbace: 0.868772 test loss: 0.755681
[Epoch 131; Iter    20/   31] train: loss: 0.0175562
[Epoch 131] ogbg-molbace: 0.892162 val loss: 0.680257
[Epoch 131] ogbg-molbace: 0.879766 test loss: 0.718051
[Epoch 132; Iter    19/   31] train: loss: 0.0325136
[Epoch 132] ogbg-molbace: 0.884584 val loss: 0.749301
[Epoch 132] ogbg-molbace: 0.878850 test loss: 0.776973
[Epoch 133; Iter    18/   31] train: loss: 0.0193803
[Epoch 133] ogbg-molbace: 0.886512 val loss: 0.717369
[Epoch 133] ogbg-molbace: 0.878021 test loss: 0.767483
[Epoch 134; Iter    17/   31] train: loss: 0.0258919
[Epoch 134] ogbg-molbace: 0.885391 val loss: 0.712447
[Epoch 134] ogbg-molbace: 0.868249 test loss: 0.783425
[Epoch 135; Iter    16/   31] train: loss: 0.0036119
[Epoch 135] ogbg-molbace: 0.890593 val loss: 0.700550
[Epoch 135] ogbg-molbace: 0.875273 test loss: 0.753564
[Epoch 136; Iter    15/   31] train: loss: 0.0133046
[Epoch 136] ogbg-molbace: 0.891893 val loss: 0.694499
[Epoch 136] ogbg-molbace: 0.876887 test loss: 0.751621
[Epoch 87; Iter     4/   31] train: loss: 0.1692661
[Epoch 87] ogbg-molbace: 0.902251 val loss: 0.511863
[Epoch 87] ogbg-molbace: 0.876276 test loss: 0.550960
[Epoch 88; Iter     3/   31] train: loss: 0.0257193
[Epoch 88] ogbg-molbace: 0.905973 val loss: 0.445161
[Epoch 88] ogbg-molbace: 0.878937 test loss: 0.511148
[Epoch 89; Iter     2/   31] train: loss: 0.0348467
[Epoch 89] ogbg-molbace: 0.903506 val loss: 0.478032
[Epoch 89] ogbg-molbace: 0.877541 test loss: 0.494649
[Epoch 90; Iter     1/   31] train: loss: 0.0743048
[Epoch 90; Iter    31/   31] train: loss: 0.6751959
[Epoch 90] ogbg-molbace: 0.899830 val loss: 0.478005
[Epoch 90] ogbg-molbace: 0.866504 test loss: 0.562574
[Epoch 91; Iter    30/   31] train: loss: 0.0702297
[Epoch 91] ogbg-molbace: 0.889696 val loss: 0.506895
[Epoch 91] ogbg-molbace: 0.873135 test loss: 0.565611
[Epoch 92; Iter    29/   31] train: loss: 0.1462043
[Epoch 92] ogbg-molbace: 0.888037 val loss: 0.519650
[Epoch 92] ogbg-molbace: 0.862665 test loss: 0.574021
[Epoch 93; Iter    28/   31] train: loss: 0.0869177
[Epoch 93] ogbg-molbace: 0.897319 val loss: 0.449546
[Epoch 93] ogbg-molbace: 0.855030 test loss: 0.551460
[Epoch 94; Iter    27/   31] train: loss: 0.1332325
[Epoch 94] ogbg-molbace: 0.889337 val loss: 0.534258
[Epoch 94] ogbg-molbace: 0.882602 test loss: 0.527233
[Epoch 95; Iter    26/   31] train: loss: 0.0957517
[Epoch 95] ogbg-molbace: 0.900278 val loss: 0.463630
[Epoch 95] ogbg-molbace: 0.877018 test loss: 0.545760
[Epoch 96; Iter    25/   31] train: loss: 0.0626251
[Epoch 96] ogbg-molbace: 0.900682 val loss: 0.482963
[Epoch 96] ogbg-molbace: 0.874138 test loss: 0.567589
[Epoch 97; Iter    24/   31] train: loss: 0.1317992
[Epoch 97] ogbg-molbace: 0.904089 val loss: 0.486591
[Epoch 97] ogbg-molbace: 0.903935 test loss: 0.485293
[Epoch 98; Iter    23/   31] train: loss: 0.1033365
[Epoch 98] ogbg-molbace: 0.913281 val loss: 0.445090
[Epoch 98] ogbg-molbace: 0.876320 test loss: 0.604329
[Epoch 99; Iter    22/   31] train: loss: 0.0639952
[Epoch 99] ogbg-molbace: 0.891041 val loss: 0.586782
[Epoch 99] ogbg-molbace: 0.853111 test loss: 0.652173
[Epoch 100; Iter    21/   31] train: loss: 0.1651841
[Epoch 100] ogbg-molbace: 0.908528 val loss: 0.475761
[Epoch 100] ogbg-molbace: 0.873571 test loss: 0.561909
[Epoch 101; Iter    20/   31] train: loss: 0.2633216
[Epoch 101] ogbg-molbace: 0.895525 val loss: 0.545322
[Epoch 101] ogbg-molbace: 0.883823 test loss: 0.567030
[Epoch 102; Iter    19/   31] train: loss: 0.0352349
[Epoch 102] ogbg-molbace: 0.887633 val loss: 0.576689
[Epoch 102] ogbg-molbace: 0.862839 test loss: 0.644670
[Epoch 103; Iter    18/   31] train: loss: 0.0548879
[Epoch 103] ogbg-molbace: 0.899740 val loss: 0.488971
[Epoch 103] ogbg-molbace: 0.883736 test loss: 0.526415
[Epoch 104; Iter    17/   31] train: loss: 0.0307911
[Epoch 104] ogbg-molbace: 0.899650 val loss: 0.484351
[Epoch 104] ogbg-molbace: 0.863319 test loss: 0.631367
[Epoch 105; Iter    16/   31] train: loss: 0.0626541
[Epoch 105] ogbg-molbace: 0.901399 val loss: 0.514217
[Epoch 105] ogbg-molbace: 0.874487 test loss: 0.598809
[Epoch 106; Iter    15/   31] train: loss: 0.0498163
[Epoch 106] ogbg-molbace: 0.897498 val loss: 0.493603
[Epoch 106] ogbg-molbace: 0.886005 test loss: 0.486730
[Epoch 107; Iter    14/   31] train: loss: 0.0456405
[Epoch 107] ogbg-molbace: 0.905031 val loss: 0.490063
[Epoch 107] ogbg-molbace: 0.880770 test loss: 0.555544
[Epoch 108; Iter    13/   31] train: loss: 0.1166047
[Epoch 108] ogbg-molbace: 0.887768 val loss: 0.628413
[Epoch 108] ogbg-molbace: 0.872786 test loss: 0.647526
[Epoch 109; Iter    12/   31] train: loss: 0.0613476
[Epoch 109] ogbg-molbace: 0.898664 val loss: 0.514461
[Epoch 109] ogbg-molbace: 0.880726 test loss: 0.557796
[Epoch 110; Iter    11/   31] train: loss: 0.0197184
[Epoch 110] ogbg-molbace: 0.900771 val loss: 0.492021
[Epoch 110] ogbg-molbace: 0.884870 test loss: 0.542077
[Epoch 111; Iter    10/   31] train: loss: 0.0417565
[Epoch 111] ogbg-molbace: 0.903775 val loss: 0.510103
[Epoch 111] ogbg-molbace: 0.884129 test loss: 0.575782
[Epoch 112; Iter     9/   31] train: loss: 0.0227719
[Epoch 112] ogbg-molbace: 0.896467 val loss: 0.534916
[Epoch 112] ogbg-molbace: 0.879810 test loss: 0.564171
[Epoch 113; Iter     8/   31] train: loss: 0.0286479
[Epoch 113] ogbg-molbace: 0.906645 val loss: 0.491675
[Epoch 113] ogbg-molbace: 0.882296 test loss: 0.588779
[Epoch 114; Iter     7/   31] train: loss: 0.0964005
[Epoch 114] ogbg-molbace: 0.902968 val loss: 0.547999
[Epoch 114] ogbg-molbace: 0.894119 test loss: 0.550534
[Epoch 115; Iter     6/   31] train: loss: 0.0917744
[Epoch 115] ogbg-molbace: 0.901085 val loss: 0.535445
[Epoch 115] ogbg-molbace: 0.877759 test loss: 0.631288
[Epoch 116; Iter     5/   31] train: loss: 0.0327403
[Epoch 116] ogbg-molbace: 0.910681 val loss: 0.537888
[Epoch 116] ogbg-molbace: 0.882645 test loss: 0.626147
[Epoch 117; Iter     4/   31] train: loss: 0.0320863
[Epoch 117] ogbg-molbace: 0.898126 val loss: 0.584706
[Epoch 117] ogbg-molbace: 0.876494 test loss: 0.665450
[Epoch 118; Iter     3/   31] train: loss: 0.0558919
[Epoch 118] ogbg-molbace: 0.906197 val loss: 0.558442
[Epoch 118] ogbg-molbace: 0.884216 test loss: 0.620769
[Epoch 119; Iter     2/   31] train: loss: 0.0565326
[Epoch 119] ogbg-molbace: 0.903327 val loss: 0.542133
[Epoch 119] ogbg-molbace: 0.885830 test loss: 0.600040
[Epoch 120; Iter     1/   31] train: loss: 0.0092719
[Epoch 120; Iter    31/   31] train: loss: 0.1262902
[Epoch 120] ogbg-molbace: 0.889068 val loss: 0.622712
[Epoch 120] ogbg-molbace: 0.878545 test loss: 0.670686
[Epoch 121; Iter    30/   31] train: loss: 0.0034820
[Epoch 121] ogbg-molbace: 0.904941 val loss: 0.584150
[Epoch 121] ogbg-molbace: 0.883780 test loss: 0.659692
[Epoch 122; Iter    29/   31] train: loss: 0.0091837
[Epoch 122] ogbg-molbace: 0.892835 val loss: 0.639335
[Epoch 122] ogbg-molbace: 0.884478 test loss: 0.653027
[Epoch 123; Iter    28/   31] train: loss: 0.0136670
[Epoch 123] ogbg-molbace: 0.900637 val loss: 0.602020
[Epoch 123] ogbg-molbace: 0.883780 test loss: 0.646695
[Epoch 124; Iter    27/   31] train: loss: 0.0393784
[Epoch 124] ogbg-molbace: 0.902475 val loss: 0.605875
[Epoch 124] ogbg-molbace: 0.885438 test loss: 0.654093
[Epoch 125; Iter    26/   31] train: loss: 0.0064563
[Epoch 125] ogbg-molbace: 0.899561 val loss: 0.588420
[Epoch 125] ogbg-molbace: 0.876363 test loss: 0.676939
[Epoch 126; Iter    25/   31] train: loss: 0.0279132
[Epoch 126] ogbg-molbace: 0.904896 val loss: 0.591815
[Epoch 126] ogbg-molbace: 0.887357 test loss: 0.650848
[Epoch 127; Iter    24/   31] train: loss: 0.1013231
[Epoch 127] ogbg-molbace: 0.897767 val loss: 0.632405
[Epoch 127] ogbg-molbace: 0.880333 test loss: 0.679256
[Epoch 128; Iter    23/   31] train: loss: 0.0986084
[Epoch 128] ogbg-molbace: 0.896242 val loss: 0.645313
[Epoch 128] ogbg-molbace: 0.874138 test loss: 0.720140
[Epoch 129; Iter    22/   31] train: loss: 0.0089462
[Epoch 129] ogbg-molbace: 0.901175 val loss: 0.590197
[Epoch 129] ogbg-molbace: 0.879504 test loss: 0.667044
[Epoch 130; Iter    21/   31] train: loss: 0.0191216
[Epoch 130] ogbg-molbace: 0.900413 val loss: 0.665906
[Epoch 130] ogbg-molbace: 0.873397 test loss: 0.791059
[Epoch 131; Iter    20/   31] train: loss: 0.0514556
[Epoch 131] ogbg-molbace: 0.903462 val loss: 0.579851
[Epoch 131] ogbg-molbace: 0.874226 test loss: 0.679422
[Epoch 132; Iter    19/   31] train: loss: 0.0456852
[Epoch 132] ogbg-molbace: 0.899067 val loss: 0.548728
[Epoch 132] ogbg-molbace: 0.881380 test loss: 0.645605
[Epoch 133; Iter    18/   31] train: loss: 0.2000714
[Epoch 133] ogbg-molbace: 0.911577 val loss: 0.522966
[Epoch 133] ogbg-molbace: 0.885307 test loss: 0.598997
[Epoch 134; Iter    17/   31] train: loss: 0.0350541
[Epoch 134] ogbg-molbace: 0.913685 val loss: 0.461340
[Epoch 134] ogbg-molbace: 0.883736 test loss: 0.580438
[Epoch 135; Iter    16/   31] train: loss: 0.0069366
[Epoch 135] ogbg-molbace: 0.909605 val loss: 0.511206
[Epoch 135] ogbg-molbace: 0.881773 test loss: 0.618895
[Epoch 136; Iter    15/   31] train: loss: 0.0268551
[Epoch 136] ogbg-molbace: 0.913192 val loss: 0.508375
[Epoch 136] ogbg-molbace: 0.888622 test loss: 0.609137
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.890606 test loss: 0.637236
[Epoch 124; Iter    27/   41] train: loss: 0.0057164
[Epoch 124] ogbg-molbace: 0.911920 val loss: 0.604514
[Epoch 124] ogbg-molbace: 0.890781 test loss: 0.632646
[Epoch 125; Iter    16/   41] train: loss: 0.0414203
[Epoch 125] ogbg-molbace: 0.907525 val loss: 0.682696
[Epoch 125] ogbg-molbace: 0.882002 test loss: 0.655876
[Epoch 126; Iter     5/   41] train: loss: 0.0036738
[Epoch 126; Iter    35/   41] train: loss: 0.0027730
[Epoch 126] ogbg-molbace: 0.906646 val loss: 0.668848
[Epoch 126] ogbg-molbace: 0.887094 test loss: 0.686220
[Epoch 127; Iter    24/   41] train: loss: 0.0183019
[Epoch 127] ogbg-molbace: 0.912096 val loss: 0.638555
[Epoch 127] ogbg-molbace: 0.888499 test loss: 0.641935
Early stopping criterion based on -ogbg-molbace- that should be max reached after 127 epochs. Best model checkpoint was in epoch 67.
Statistics on  val_best_checkpoint
mean_pred: -0.4814211130142212
std_pred: 3.586212158203125
mean_targets: 0.5231788158416748
std_targets: 0.5011245608329773
prcauc: 0.933305387918223
rocauc: 0.9324894514767933
ogbg-molbace: 0.9324894514767933
BCEWithLogitsLoss: 0.29884338937699795
Statistics on  test
mean_pred: -0.6560410261154175
std_pred: 3.828336715698242
mean_targets: 0.44078949093818665
std_targets: 0.4981229901313782
prcauc: 0.8532038238579174
rocauc: 0.9100965759438103
ogbg-molbace: 0.9100965759438103
BCEWithLogitsLoss: 0.34746358295281726
Statistics on  train
mean_pred: -0.878452479839325
std_pred: 3.962467670440674
mean_targets: 0.45041322708129883
std_targets: 0.497740775346756
prcauc: 0.9750086003878562
rocauc: 0.980249706835897
ogbg-molbace: 0.980249706835897
BCEWithLogitsLoss: 0.18535880744457245
[Epoch 87; Iter     4/   31] train: loss: 0.2958078
[Epoch 87] ogbg-molbace: 0.893507 val loss: 0.421056
[Epoch 87] ogbg-molbace: 0.874618 test loss: 0.492596
[Epoch 88; Iter     3/   31] train: loss: 0.2219940
[Epoch 88] ogbg-molbace: 0.905883 val loss: 0.418530
[Epoch 88] ogbg-molbace: 0.871521 test loss: 0.506015
[Epoch 89; Iter     2/   31] train: loss: 0.1048234
[Epoch 89] ogbg-molbace: 0.880100 val loss: 0.477510
[Epoch 89] ogbg-molbace: 0.853503 test loss: 0.554996
[Epoch 90; Iter     1/   31] train: loss: 0.2211513
[Epoch 90; Iter    31/   31] train: loss: 1.5651590
[Epoch 90] ogbg-molbace: 0.897050 val loss: 0.399650
[Epoch 90] ogbg-molbace: 0.879504 test loss: 0.443097
[Epoch 91; Iter    30/   31] train: loss: 0.1190759
[Epoch 91] ogbg-molbace: 0.886154 val loss: 0.437343
[Epoch 91] ogbg-molbace: 0.866591 test loss: 0.483596
[Epoch 92; Iter    29/   31] train: loss: 0.1750094
[Epoch 92] ogbg-molbace: 0.904044 val loss: 0.420680
[Epoch 92] ogbg-molbace: 0.876189 test loss: 0.488730
[Epoch 93; Iter    28/   31] train: loss: 0.3411791
[Epoch 93] ogbg-molbace: 0.895794 val loss: 0.476548
[Epoch 93] ogbg-molbace: 0.863319 test loss: 0.593494
[Epoch 94; Iter    27/   31] train: loss: 0.2065500
[Epoch 94] ogbg-molbace: 0.905569 val loss: 0.423775
[Epoch 94] ogbg-molbace: 0.865937 test loss: 0.560067
[Epoch 95; Iter    26/   31] train: loss: 0.1293333
[Epoch 95] ogbg-molbace: 0.911353 val loss: 0.394993
[Epoch 95] ogbg-molbace: 0.879810 test loss: 0.494229
[Epoch 96; Iter    25/   31] train: loss: 0.1987198
[Epoch 96] ogbg-molbace: 0.916375 val loss: 0.403207
[Epoch 96] ogbg-molbace: 0.868685 test loss: 0.534602
[Epoch 97; Iter    24/   31] train: loss: 0.1313836
[Epoch 97] ogbg-molbace: 0.916196 val loss: 0.408080
[Epoch 97] ogbg-molbace: 0.873091 test loss: 0.550797
[Epoch 98; Iter    23/   31] train: loss: 0.2666021
[Epoch 98] ogbg-molbace: 0.902879 val loss: 0.466258
[Epoch 98] ogbg-molbace: 0.869427 test loss: 0.550731
[Epoch 99; Iter    22/   31] train: loss: 0.1996003
[Epoch 99] ogbg-molbace: 0.912698 val loss: 0.417617
[Epoch 99] ogbg-molbace: 0.869121 test loss: 0.569467
[Epoch 100; Iter    21/   31] train: loss: 0.1221054
[Epoch 100] ogbg-molbace: 0.906645 val loss: 0.437260
[Epoch 100] ogbg-molbace: 0.864410 test loss: 0.612127
[Epoch 101; Iter    20/   31] train: loss: 0.0654522
[Epoch 101] ogbg-molbace: 0.907676 val loss: 0.438000
[Epoch 101] ogbg-molbace: 0.865544 test loss: 0.586290
[Epoch 102; Iter    19/   31] train: loss: 0.1279534
[Epoch 102] ogbg-molbace: 0.911174 val loss: 0.425158
[Epoch 102] ogbg-molbace: 0.864933 test loss: 0.616277
[Epoch 103; Iter    18/   31] train: loss: 0.0946046
[Epoch 103] ogbg-molbace: 0.915478 val loss: 0.402595
[Epoch 103] ogbg-molbace: 0.865195 test loss: 0.582412
[Epoch 104; Iter    17/   31] train: loss: 0.0537347
[Epoch 104] ogbg-molbace: 0.900457 val loss: 0.474391
[Epoch 104] ogbg-molbace: 0.855205 test loss: 0.641794
[Epoch 105; Iter    16/   31] train: loss: 0.2319798
[Epoch 105] ogbg-molbace: 0.897319 val loss: 0.490181
[Epoch 105] ogbg-molbace: 0.865588 test loss: 0.565907
[Epoch 106; Iter    15/   31] train: loss: 0.1679783
[Epoch 106] ogbg-molbace: 0.907497 val loss: 0.422453
[Epoch 106] ogbg-molbace: 0.853765 test loss: 0.581630
[Epoch 107; Iter    14/   31] train: loss: 0.2713675
[Epoch 107] ogbg-molbace: 0.908394 val loss: 0.439218
[Epoch 107] ogbg-molbace: 0.865980 test loss: 0.619984
[Epoch 108; Iter    13/   31] train: loss: 0.0929289
[Epoch 108] ogbg-molbace: 0.914896 val loss: 0.440652
[Epoch 108] ogbg-molbace: 0.868249 test loss: 0.602960
[Epoch 109; Iter    12/   31] train: loss: 0.0773911
[Epoch 109] ogbg-molbace: 0.909963 val loss: 0.443352
[Epoch 109] ogbg-molbace: 0.868816 test loss: 0.631473
[Epoch 110; Iter    11/   31] train: loss: 0.0237116
[Epoch 110] ogbg-molbace: 0.905614 val loss: 0.526376
[Epoch 110] ogbg-molbace: 0.879723 test loss: 0.588956
[Epoch 111; Iter    10/   31] train: loss: 0.2108139
[Epoch 111] ogbg-molbace: 0.900413 val loss: 0.513783
[Epoch 111] ogbg-molbace: 0.867725 test loss: 0.635898
[Epoch 112; Iter     9/   31] train: loss: 0.0928999
[Epoch 112] ogbg-molbace: 0.916375 val loss: 0.440168
[Epoch 112] ogbg-molbace: 0.854376 test loss: 0.695689
[Epoch 113; Iter     8/   31] train: loss: 0.0607563
[Epoch 113] ogbg-molbace: 0.918438 val loss: 0.454223
[Epoch 113] ogbg-molbace: 0.853547 test loss: 0.702041
[Epoch 114; Iter     7/   31] train: loss: 0.0403078
[Epoch 114] ogbg-molbace: 0.909649 val loss: 0.491088
[Epoch 114] ogbg-molbace: 0.859829 test loss: 0.658679
[Epoch 115; Iter     6/   31] train: loss: 0.0537893
[Epoch 115] ogbg-molbace: 0.909605 val loss: 0.506836
[Epoch 115] ogbg-molbace: 0.865064 test loss: 0.706799
[Epoch 116; Iter     5/   31] train: loss: 0.0777404
[Epoch 116] ogbg-molbace: 0.919021 val loss: 0.467274
[Epoch 116] ogbg-molbace: 0.888491 test loss: 0.596276
[Epoch 117; Iter     4/   31] train: loss: 0.0400671
[Epoch 117] ogbg-molbace: 0.906511 val loss: 0.497182
[Epoch 117] ogbg-molbace: 0.872786 test loss: 0.606648
[Epoch 118; Iter     3/   31] train: loss: 0.0595389
[Epoch 118] ogbg-molbace: 0.907407 val loss: 0.553582
[Epoch 118] ogbg-molbace: 0.845738 test loss: 0.804325
[Epoch 119; Iter     2/   31] train: loss: 0.0334341
[Epoch 119] ogbg-molbace: 0.917272 val loss: 0.449129
[Epoch 119] ogbg-molbace: 0.862447 test loss: 0.685694
[Epoch 120; Iter     1/   31] train: loss: 0.0365524
[Epoch 120; Iter    31/   31] train: loss: 0.1553847
[Epoch 120] ogbg-molbace: 0.918797 val loss: 0.485085
[Epoch 120] ogbg-molbace: 0.871783 test loss: 0.647649
[Epoch 121; Iter    30/   31] train: loss: 0.0338986
[Epoch 121] ogbg-molbace: 0.920097 val loss: 0.456307
[Epoch 121] ogbg-molbace: 0.864584 test loss: 0.692702
[Epoch 122; Iter    29/   31] train: loss: 0.0328947
[Epoch 122] ogbg-molbace: 0.923774 val loss: 0.448036
[Epoch 122] ogbg-molbace: 0.873135 test loss: 0.656357
[Epoch 123; Iter    28/   31] train: loss: 0.0158529
[Epoch 123] ogbg-molbace: 0.924536 val loss: 0.433308
[Epoch 123] ogbg-molbace: 0.873309 test loss: 0.676138
[Epoch 124; Iter    27/   31] train: loss: 0.0471617
[Epoch 124] ogbg-molbace: 0.925209 val loss: 0.474061
[Epoch 124] ogbg-molbace: 0.870866 test loss: 0.712022
[Epoch 125; Iter    26/   31] train: loss: 0.0108172
[Epoch 125] ogbg-molbace: 0.923639 val loss: 0.441262
[Epoch 125] ogbg-molbace: 0.861923 test loss: 0.736817
[Epoch 126; Iter    25/   31] train: loss: 0.0235893
[Epoch 126] ogbg-molbace: 0.924401 val loss: 0.493082
[Epoch 126] ogbg-molbace: 0.865239 test loss: 0.762029
[Epoch 127; Iter    24/   31] train: loss: 0.0648785
[Epoch 127] ogbg-molbace: 0.924222 val loss: 0.464005
[Epoch 127] ogbg-molbace: 0.864541 test loss: 0.721925
[Epoch 128; Iter    23/   31] train: loss: 0.0219311
[Epoch 128] ogbg-molbace: 0.923370 val loss: 0.474281
[Epoch 128] ogbg-molbace: 0.864104 test loss: 0.748958
[Epoch 129; Iter    22/   31] train: loss: 0.3141193
[Epoch 129] ogbg-molbace: 0.921263 val loss: 0.495566
[Epoch 129] ogbg-molbace: 0.870343 test loss: 0.722311
[Epoch 130; Iter    21/   31] train: loss: 0.0293486
[Epoch 130] ogbg-molbace: 0.919335 val loss: 0.508663
[Epoch 130] ogbg-molbace: 0.863581 test loss: 0.765909
[Epoch 131; Iter    20/   31] train: loss: 0.0067522
[Epoch 131] ogbg-molbace: 0.919738 val loss: 0.505365
[Epoch 131] ogbg-molbace: 0.863275 test loss: 0.779287
[Epoch 132; Iter    19/   31] train: loss: 0.0121691
[Epoch 132] ogbg-molbace: 0.920949 val loss: 0.552001
[Epoch 132] ogbg-molbace: 0.871346 test loss: 0.782541
[Epoch 133; Iter    18/   31] train: loss: 0.0338192
[Epoch 133] ogbg-molbace: 0.920859 val loss: 0.514828
[Epoch 133] ogbg-molbace: 0.870605 test loss: 0.747957
[Epoch 134; Iter    17/   31] train: loss: 0.0119717
[Epoch 134] ogbg-molbace: 0.921666 val loss: 0.505385
[Epoch 134] ogbg-molbace: 0.865500 test loss: 0.785494
[Epoch 135; Iter    16/   31] train: loss: 0.0063928
[Epoch 135] ogbg-molbace: 0.926823 val loss: 0.468754
[Epoch 135] ogbg-molbace: 0.862490 test loss: 0.776108
[Epoch 136; Iter    15/   31] train: loss: 0.0740813
[Epoch 136] ogbg-molbace: 0.921756 val loss: 0.496765
[Epoch 136] ogbg-molbace: 0.858433 test loss: 0.818716
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 137; Iter    14/   31] train: loss: 0.0033224
[Epoch 137] ogbg-molbace: 0.887723 val loss: 0.747845
[Epoch 137] ogbg-molbace: 0.876974 test loss: 0.785488
[Epoch 138; Iter    13/   31] train: loss: 0.0040401
[Epoch 138] ogbg-molbace: 0.886781 val loss: 0.719335
[Epoch 138] ogbg-molbace: 0.877541 test loss: 0.768497
[Epoch 139; Iter    12/   31] train: loss: 0.0111595
[Epoch 139] ogbg-molbace: 0.889472 val loss: 0.717446
[Epoch 139] ogbg-molbace: 0.873571 test loss: 0.781125
[Epoch 140; Iter    11/   31] train: loss: 0.0043660
[Epoch 140] ogbg-molbace: 0.889741 val loss: 0.735479
[Epoch 140] ogbg-molbace: 0.879112 test loss: 0.761132
[Epoch 141; Iter    10/   31] train: loss: 0.0042489
[Epoch 141] ogbg-molbace: 0.889427 val loss: 0.701013
[Epoch 141] ogbg-molbace: 0.878239 test loss: 0.753444
[Epoch 142; Iter     9/   31] train: loss: 0.0039853
[Epoch 142] ogbg-molbace: 0.887902 val loss: 0.720021
[Epoch 142] ogbg-molbace: 0.874793 test loss: 0.771342
[Epoch 143; Iter     8/   31] train: loss: 0.0029424
[Epoch 143] ogbg-molbace: 0.889337 val loss: 0.735650
[Epoch 143] ogbg-molbace: 0.877192 test loss: 0.772335
Early stopping criterion based on -ogbg-molbace- that should be max reached after 143 epochs. Best model checkpoint was in epoch 83.
Statistics on  val_best_checkpoint
mean_pred: -0.5059196949005127
std_pred: 3.8680803775787354
mean_targets: 0.41584160923957825
std_targets: 0.49368178844451904
prcauc: 0.8464554749032248
rocauc: 0.909380324634562
ogbg-molbace: 0.909380324634562
BCEWithLogitsLoss: 0.3926878828894008
Statistics on  test
mean_pred: 0.029250722378492355
std_pred: 3.818995952606201
mean_targets: 0.48184821009635925
std_targets: 0.5004969835281372
prcauc: 0.8492601278994946
rocauc: 0.8782828723497078
ogbg-molbace: 0.8782828723497078
BCEWithLogitsLoss: 0.469394207932055
Statistics on  train
mean_pred: -0.2603659927845001
std_pred: 4.045557975769043
mean_targets: 0.4619625210762024
std_targets: 0.49882611632347107
prcauc: 0.9953988871109057
rocauc: 0.9959456551508276
ogbg-molbace: 0.9959456551508276
BCEWithLogitsLoss: 0.11456411205712826
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 130; Iter     6/   36] train: loss: 0.0065070
[Epoch 130; Iter    36/   36] train: loss: 0.1614649
[Epoch 130] ogbg-molbace: 0.864899 val loss: 0.931202
[Epoch 130] ogbg-molbace: 0.864640 test loss: 0.982416
[Epoch 131; Iter    30/   36] train: loss: 0.0121331
[Epoch 131] ogbg-molbace: 0.860795 val loss: 0.914633
[Epoch 131] ogbg-molbace: 0.868683 test loss: 0.960059
[Epoch 132; Iter    24/   36] train: loss: 0.0189544
[Epoch 132] ogbg-molbace: 0.863321 val loss: 0.880103
[Epoch 132] ogbg-molbace: 0.867361 test loss: 0.990008
[Epoch 133; Iter    18/   36] train: loss: 0.0072608
[Epoch 133] ogbg-molbace: 0.868766 val loss: 0.885419
[Epoch 133] ogbg-molbace: 0.859820 test loss: 1.099799
[Epoch 134; Iter    12/   36] train: loss: 0.0152503
[Epoch 134] ogbg-molbace: 0.864741 val loss: 0.932364
[Epoch 134] ogbg-molbace: 0.864873 test loss: 1.057818
[Epoch 135; Iter     6/   36] train: loss: 0.0575705
[Epoch 135; Iter    36/   36] train: loss: 0.0069706
[Epoch 135] ogbg-molbace: 0.861506 val loss: 0.983102
[Epoch 135] ogbg-molbace: 0.853677 test loss: 1.147634
[Epoch 136; Iter    30/   36] train: loss: 0.0018368
[Epoch 136] ogbg-molbace: 0.866083 val loss: 0.915190
[Epoch 136] ogbg-molbace: 0.861919 test loss: 1.071817
[Epoch 137; Iter    24/   36] train: loss: 0.0380112
[Epoch 137] ogbg-molbace: 0.861979 val loss: 1.036679
[Epoch 137] ogbg-molbace: 0.855621 test loss: 1.135293
[Epoch 138; Iter    18/   36] train: loss: 0.1276787
[Epoch 138] ogbg-molbace: 0.867030 val loss: 0.890870
[Epoch 138] ogbg-molbace: 0.854299 test loss: 1.041325
[Epoch 139; Iter    12/   36] train: loss: 0.0253847
[Epoch 139] ogbg-molbace: 0.863557 val loss: 1.035653
[Epoch 139] ogbg-molbace: 0.852978 test loss: 1.249082
[Epoch 140; Iter     6/   36] train: loss: 0.0346251
[Epoch 140; Iter    36/   36] train: loss: 0.0040069
[Epoch 140] ogbg-molbace: 0.876657 val loss: 0.863056
[Epoch 140] ogbg-molbace: 0.859198 test loss: 1.073389
Early stopping criterion based on -ogbg-molbace- that should be max reached after 140 epochs. Best model checkpoint was in epoch 80.
Statistics on  val_best_checkpoint
mean_pred: -0.9341256022453308
std_pred: 3.65506649017334
mean_targets: 0.4361233413219452
std_targets: 0.49699893593788147
prcauc: 0.8484373395296173
rocauc: 0.8910984848484849
ogbg-molbace: 0.8910984848484849
BCEWithLogitsLoss: 0.4617389254271984
Statistics on  test
mean_pred: -0.636526882648468
std_pred: 3.689831256866455
mean_targets: 0.48017618060112
std_targets: 0.5007109642028809
prcauc: 0.8840445529542997
rocauc: 0.8992380656196548
ogbg-molbace: 0.8992380656196548
BCEWithLogitsLoss: 0.4420072678476572
Statistics on  train
mean_pred: -0.9228372573852539
std_pred: 3.9769036769866943
mean_targets: 0.45609065890312195
std_targets: 0.4983035624027252
prcauc: 0.983986037040143
rocauc: 0.9871391189325972
ogbg-molbace: 0.9871391189325972
BCEWithLogitsLoss: 0.16341779474169016
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 130; Iter     6/   36] train: loss: 0.0075799
[Epoch 130; Iter    36/   36] train: loss: 0.0085024
[Epoch 130] ogbg-molbace: 0.881550 val loss: 0.846513
[Epoch 130] ogbg-molbace: 0.864951 test loss: 1.007983
[Epoch 131; Iter    30/   36] train: loss: 0.0041238
[Epoch 131] ogbg-molbace: 0.881629 val loss: 0.872634
[Epoch 131] ogbg-molbace: 0.867905 test loss: 0.984536
[Epoch 132; Iter    24/   36] train: loss: 0.0041257
[Epoch 132] ogbg-molbace: 0.878867 val loss: 0.883697
[Epoch 132] ogbg-molbace: 0.860286 test loss: 1.098888
[Epoch 133; Iter    18/   36] train: loss: 0.0278921
[Epoch 133] ogbg-molbace: 0.880919 val loss: 0.868762
[Epoch 133] ogbg-molbace: 0.868761 test loss: 0.947754
[Epoch 134; Iter    12/   36] train: loss: 0.0107735
[Epoch 134] ogbg-molbace: 0.886127 val loss: 0.815063
[Epoch 134] ogbg-molbace: 0.868916 test loss: 0.980087
[Epoch 135; Iter     6/   36] train: loss: 0.0141025
[Epoch 135; Iter    36/   36] train: loss: 0.0027543
[Epoch 135] ogbg-molbace: 0.881787 val loss: 0.860009
[Epoch 135] ogbg-molbace: 0.868061 test loss: 1.037219
[Epoch 136; Iter    30/   36] train: loss: 0.0017398
[Epoch 136] ogbg-molbace: 0.882418 val loss: 0.849303
[Epoch 136] ogbg-molbace: 0.868139 test loss: 1.017976
[Epoch 137; Iter    24/   36] train: loss: 0.0118706
[Epoch 137] ogbg-molbace: 0.886995 val loss: 0.844687
[Epoch 137] ogbg-molbace: 0.871249 test loss: 1.006924
[Epoch 138; Iter    18/   36] train: loss: 0.0067622
[Epoch 138] ogbg-molbace: 0.883286 val loss: 0.864875
[Epoch 138] ogbg-molbace: 0.866350 test loss: 1.041454
[Epoch 139; Iter    12/   36] train: loss: 0.0204725
[Epoch 139] ogbg-molbace: 0.861979 val loss: 1.041255
[Epoch 139] ogbg-molbace: 0.852200 test loss: 1.171671
[Epoch 140; Iter     6/   36] train: loss: 0.0099456
[Epoch 140; Iter    36/   36] train: loss: 0.0440430
[Epoch 140] ogbg-molbace: 0.870581 val loss: 0.979444
[Epoch 140] ogbg-molbace: 0.858731 test loss: 1.100977
[Epoch 141; Iter    30/   36] train: loss: 0.0086468
[Epoch 141] ogbg-molbace: 0.868845 val loss: 0.995169
[Epoch 141] ogbg-molbace: 0.861997 test loss: 1.076111
[Epoch 142; Iter    24/   36] train: loss: 0.0038313
[Epoch 142] ogbg-molbace: 0.870344 val loss: 1.021825
[Epoch 142] ogbg-molbace: 0.862696 test loss: 1.106796
[Epoch 143; Iter    18/   36] train: loss: 0.0068581
[Epoch 143] ogbg-molbace: 0.869634 val loss: 0.987499
[Epoch 143] ogbg-molbace: 0.859042 test loss: 1.136450
[Epoch 144; Iter    12/   36] train: loss: 0.0051621
[Epoch 144] ogbg-molbace: 0.871686 val loss: 1.002682
[Epoch 144] ogbg-molbace: 0.860286 test loss: 1.148702
[Epoch 145; Iter     6/   36] train: loss: 0.0094854
[Epoch 145; Iter    36/   36] train: loss: 0.0038043
[Epoch 145] ogbg-molbace: 0.872869 val loss: 0.968080
[Epoch 145] ogbg-molbace: 0.858809 test loss: 1.164492
[Epoch 146; Iter    30/   36] train: loss: 0.0052645
[Epoch 146] ogbg-molbace: 0.872159 val loss: 0.961534
[Epoch 146] ogbg-molbace: 0.859897 test loss: 1.127962
[Epoch 147; Iter    24/   36] train: loss: 0.0061429
[Epoch 147] ogbg-molbace: 0.863715 val loss: 1.029239
[Epoch 147] ogbg-molbace: 0.851112 test loss: 1.217815
[Epoch 148; Iter    18/   36] train: loss: 0.0124528
[Epoch 148] ogbg-molbace: 0.874369 val loss: 0.956868
[Epoch 148] ogbg-molbace: 0.864407 test loss: 1.106526
Early stopping criterion based on -ogbg-molbace- that should be max reached after 148 epochs. Best model checkpoint was in epoch 88.
Statistics on  val_best_checkpoint
mean_pred: -1.310520052909851
std_pred: 4.706217288970947
mean_targets: 0.4361233413219452
std_targets: 0.49699893593788147
prcauc: 0.8626778263532406
rocauc: 0.8998579545454546
ogbg-molbace: 0.8998579545454546
BCEWithLogitsLoss: 0.5010709669440985
Statistics on  test
mean_pred: -1.0963472127914429
std_pred: 5.028928756713867
mean_targets: 0.48017618060112
std_targets: 0.5007109642028809
prcauc: 0.8511271419777747
rocauc: 0.8740475820245684
ogbg-molbace: 0.8740475820245684
BCEWithLogitsLoss: 0.6506724059581757
Statistics on  train
mean_pred: -1.3328405618667603
std_pred: 5.072157382965088
mean_targets: 0.45609065890312195
std_targets: 0.4983035624027252
prcauc: 0.9998778076224174
rocauc: 0.9998957614446745
ogbg-molbace: 0.9998957614446745
BCEWithLogitsLoss: 0.05080009251832962
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 123] ogbg-molbace: 0.865320 test loss: 0.817858
[Epoch 124; Iter    27/   41] train: loss: 0.0129188
[Epoch 124] ogbg-molbace: 0.898031 val loss: 0.645174
[Epoch 124] ogbg-molbace: 0.858297 test loss: 0.774772
[Epoch 125; Iter    16/   41] train: loss: 0.0269833
[Epoch 125] ogbg-molbace: 0.905415 val loss: 0.631056
[Epoch 125] ogbg-molbace: 0.855312 test loss: 0.841317
[Epoch 126; Iter     5/   41] train: loss: 0.0955146
[Epoch 126; Iter    35/   41] train: loss: 0.0152502
[Epoch 126] ogbg-molbace: 0.911568 val loss: 0.784649
[Epoch 126] ogbg-molbace: 0.871993 test loss: 0.677175
[Epoch 127; Iter    24/   41] train: loss: 0.0228199
[Epoch 127] ogbg-molbace: 0.909459 val loss: 0.584601
[Epoch 127] ogbg-molbace: 0.865145 test loss: 0.784730
[Epoch 128; Iter    13/   41] train: loss: 0.0639307
[Epoch 128] ogbg-molbace: 0.915084 val loss: 0.506156
[Epoch 128] ogbg-molbace: 0.872695 test loss: 0.733940
[Epoch 129; Iter     2/   41] train: loss: 0.0309298
[Epoch 129; Iter    32/   41] train: loss: 0.0146102
[Epoch 129] ogbg-molbace: 0.906294 val loss: 0.546202
[Epoch 129] ogbg-molbace: 0.871466 test loss: 0.726660
[Epoch 130; Iter    21/   41] train: loss: 0.0092206
[Epoch 130] ogbg-molbace: 0.920007 val loss: 0.497010
[Epoch 130] ogbg-molbace: 0.870588 test loss: 0.733288
[Epoch 131; Iter    10/   41] train: loss: 0.0133431
[Epoch 131; Iter    40/   41] train: loss: 0.0097653
[Epoch 131] ogbg-molbace: 0.911217 val loss: 0.526371
[Epoch 131] ogbg-molbace: 0.870413 test loss: 0.728860
[Epoch 132; Iter    29/   41] train: loss: 0.0120076
[Epoch 132] ogbg-molbace: 0.916139 val loss: 0.547466
[Epoch 132] ogbg-molbace: 0.876032 test loss: 0.728677
[Epoch 133; Iter    18/   41] train: loss: 0.0316174
[Epoch 133] ogbg-molbace: 0.909283 val loss: 0.545349
[Epoch 133] ogbg-molbace: 0.869535 test loss: 0.839754
[Epoch 134; Iter     7/   41] train: loss: 0.0032000
[Epoch 134; Iter    37/   41] train: loss: 0.0432207
[Epoch 134] ogbg-molbace: 0.911041 val loss: 0.559276
[Epoch 134] ogbg-molbace: 0.870061 test loss: 0.770024
[Epoch 135; Iter    26/   41] train: loss: 0.0028458
[Epoch 135] ogbg-molbace: 0.912447 val loss: 0.563128
[Epoch 135] ogbg-molbace: 0.877436 test loss: 0.730588
[Epoch 136; Iter    15/   41] train: loss: 0.0047079
[Epoch 136] ogbg-molbace: 0.909634 val loss: 0.573462
[Epoch 136] ogbg-molbace: 0.865847 test loss: 0.773975
[Epoch 137; Iter     4/   41] train: loss: 0.0019936
[Epoch 137; Iter    34/   41] train: loss: 0.0155959
[Epoch 137] ogbg-molbace: 0.910513 val loss: 0.599924
[Epoch 137] ogbg-molbace: 0.869535 test loss: 0.778313
[Epoch 138; Iter    23/   41] train: loss: 0.0162902
[Epoch 138] ogbg-molbace: 0.913150 val loss: 0.536832
[Epoch 138] ogbg-molbace: 0.876383 test loss: 0.759647
[Epoch 139; Iter    12/   41] train: loss: 0.0071744
[Epoch 139] ogbg-molbace: 0.920534 val loss: 0.510860
[Epoch 139] ogbg-molbace: 0.868481 test loss: 0.738715
[Epoch 140; Iter     1/   41] train: loss: 0.0068833
[Epoch 140; Iter    31/   41] train: loss: 0.0196557
[Epoch 140] ogbg-molbace: 0.912096 val loss: 0.553879
[Epoch 140] ogbg-molbace: 0.868832 test loss: 0.747646
[Epoch 141; Iter    20/   41] train: loss: 0.0023480
[Epoch 141] ogbg-molbace: 0.913854 val loss: 0.569978
[Epoch 141] ogbg-molbace: 0.873047 test loss: 0.779211
[Epoch 142; Iter     9/   41] train: loss: 0.0012316
[Epoch 142; Iter    39/   41] train: loss: 0.0037907
[Epoch 142] ogbg-molbace: 0.909810 val loss: 0.595684
[Epoch 142] ogbg-molbace: 0.873047 test loss: 0.728365
[Epoch 143; Iter    28/   41] train: loss: 0.0221508
[Epoch 143] ogbg-molbace: 0.906821 val loss: 0.616393
[Epoch 143] ogbg-molbace: 0.873573 test loss: 0.797531
[Epoch 144; Iter    17/   41] train: loss: 0.0015187
[Epoch 144] ogbg-molbace: 0.916667 val loss: 0.614216
[Epoch 144] ogbg-molbace: 0.877085 test loss: 0.711270
[Epoch 145; Iter     6/   41] train: loss: 0.0018318
[Epoch 145; Iter    36/   41] train: loss: 0.0021270
[Epoch 145] ogbg-molbace: 0.912799 val loss: 0.604044
[Epoch 145] ogbg-molbace: 0.866198 test loss: 0.810178
[Epoch 146; Iter    25/   41] train: loss: 0.0129784
[Epoch 146] ogbg-molbace: 0.911920 val loss: 0.573859
[Epoch 146] ogbg-molbace: 0.870764 test loss: 0.776770
[Epoch 147; Iter    14/   41] train: loss: 0.0133304
[Epoch 147] ogbg-molbace: 0.912096 val loss: 0.621758
[Epoch 147] ogbg-molbace: 0.864969 test loss: 0.788010
[Epoch 148; Iter     3/   41] train: loss: 0.0024081
[Epoch 148; Iter    33/   41] train: loss: 0.0114573
[Epoch 148] ogbg-molbace: 0.903481 val loss: 0.599045
[Epoch 148] ogbg-molbace: 0.867252 test loss: 0.807453
[Epoch 149; Iter    22/   41] train: loss: 0.0014306
[Epoch 149] ogbg-molbace: 0.912447 val loss: 0.578109
[Epoch 149] ogbg-molbace: 0.870413 test loss: 0.805972
[Epoch 150; Iter    11/   41] train: loss: 0.0011095
[Epoch 150; Iter    41/   41] train: loss: 0.0127697
[Epoch 150] ogbg-molbace: 0.908755 val loss: 0.597034
[Epoch 150] ogbg-molbace: 0.869359 test loss: 0.807213
[Epoch 151; Iter    30/   41] train: loss: 0.0887446
[Epoch 151] ogbg-molbace: 0.914733 val loss: 0.615617
[Epoch 151] ogbg-molbace: 0.864091 test loss: 0.824033
[Epoch 152; Iter    19/   41] train: loss: 0.0028582
[Epoch 152] ogbg-molbace: 0.910513 val loss: 0.629405
[Epoch 152] ogbg-molbace: 0.873924 test loss: 0.740800
[Epoch 153; Iter     8/   41] train: loss: 0.0042457
[Epoch 153; Iter    38/   41] train: loss: 0.0127118
[Epoch 153] ogbg-molbace: 0.909986 val loss: 0.577713
[Epoch 153] ogbg-molbace: 0.875505 test loss: 0.756700
[Epoch 154; Iter    27/   41] train: loss: 0.0037740
[Epoch 154] ogbg-molbace: 0.905942 val loss: 0.656796
[Epoch 154] ogbg-molbace: 0.873749 test loss: 0.797548
[Epoch 155; Iter    16/   41] train: loss: 0.0022044
[Epoch 155] ogbg-molbace: 0.906294 val loss: 0.625462
[Epoch 155] ogbg-molbace: 0.873222 test loss: 0.773193
[Epoch 156; Iter     5/   41] train: loss: 0.0004598
[Epoch 156; Iter    35/   41] train: loss: 0.0010647
[Epoch 156] ogbg-molbace: 0.902954 val loss: 0.629049
[Epoch 156] ogbg-molbace: 0.873222 test loss: 0.751314
[Epoch 157; Iter    24/   41] train: loss: 0.0020839
[Epoch 157] ogbg-molbace: 0.898910 val loss: 0.627026
[Epoch 157] ogbg-molbace: 0.874276 test loss: 0.744988
[Epoch 158; Iter    13/   41] train: loss: 0.0031314
[Epoch 158] ogbg-molbace: 0.911217 val loss: 0.613440
[Epoch 158] ogbg-molbace: 0.871993 test loss: 0.782923
[Epoch 159; Iter     2/   41] train: loss: 0.0010165
[Epoch 159; Iter    32/   41] train: loss: 0.0015811
[Epoch 159] ogbg-molbace: 0.915260 val loss: 0.590767
[Epoch 159] ogbg-molbace: 0.871115 test loss: 0.780880
[Epoch 160; Iter    21/   41] train: loss: 0.0015981
[Epoch 160] ogbg-molbace: 0.911041 val loss: 0.607000
[Epoch 160] ogbg-molbace: 0.873398 test loss: 0.800052
Early stopping criterion based on -ogbg-molbace- that should be max reached after 160 epochs. Best model checkpoint was in epoch 100.
Statistics on  val_best_checkpoint
mean_pred: 0.13969242572784424
std_pred: 5.554039478302002
mean_targets: 0.5231788158416748
std_targets: 0.5011245608329773
prcauc: 0.891987695358288
rocauc: 0.9240506329113924
ogbg-molbace: 0.9240506329113924
BCEWithLogitsLoss: 0.3536471116046111
Statistics on  test
mean_pred: -0.20882099866867065
std_pred: 5.313292026519775
mean_targets: 0.44078949093818665
std_targets: 0.4981229901313782
prcauc: 0.8238182695535752
rocauc: 0.8616330114135206
ogbg-molbace: 0.8616330114135206
BCEWithLogitsLoss: 0.5832405487696329
Statistics on  train
mean_pred: -0.31304478645324707
std_pred: 5.9711713790893555
mean_targets: 0.45041322708129883
std_targets: 0.49774080514907837
prcauc: 0.9990944541126852
rocauc: 0.9992467407049734
ogbg-molbace: 0.9992467407049734
BCEWithLogitsLoss: 0.05177098255967948
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 130; Iter     6/   36] train: loss: 0.0049679
[Epoch 130; Iter    36/   36] train: loss: 0.0288797
[Epoch 130] ogbg-molbace: 0.871054 val loss: 0.949660
[Epoch 130] ogbg-molbace: 0.870938 test loss: 0.968517
[Epoch 131; Iter    30/   36] train: loss: 0.0078203
[Epoch 131] ogbg-molbace: 0.873264 val loss: 0.919136
[Epoch 131] ogbg-molbace: 0.875058 test loss: 0.944467
[Epoch 132; Iter    24/   36] train: loss: 0.1085495
[Epoch 132] ogbg-molbace: 0.872159 val loss: 0.902574
[Epoch 132] ogbg-molbace: 0.854144 test loss: 1.045636
[Epoch 133; Iter    18/   36] train: loss: 0.0090409
[Epoch 133] ogbg-molbace: 0.878472 val loss: 0.877292
[Epoch 133] ogbg-molbace: 0.854533 test loss: 1.012851
[Epoch 134; Iter    12/   36] train: loss: 0.0086137
[Epoch 134] ogbg-molbace: 0.877604 val loss: 0.923002
[Epoch 134] ogbg-molbace: 0.857720 test loss: 1.022637
[Epoch 135; Iter     6/   36] train: loss: 0.0048234
[Epoch 135; Iter    36/   36] train: loss: 0.0031791
[Epoch 135] ogbg-molbace: 0.869081 val loss: 0.917294
[Epoch 135] ogbg-molbace: 0.863940 test loss: 0.981077
[Epoch 136; Iter    30/   36] train: loss: 0.0081334
[Epoch 136] ogbg-molbace: 0.870028 val loss: 0.963877
[Epoch 136] ogbg-molbace: 0.865806 test loss: 0.992658
[Epoch 137; Iter    24/   36] train: loss: 0.0174777
[Epoch 137] ogbg-molbace: 0.866556 val loss: 0.976929
[Epoch 137] ogbg-molbace: 0.861919 test loss: 1.018674
[Epoch 138; Iter    18/   36] train: loss: 0.0458586
[Epoch 138] ogbg-molbace: 0.864189 val loss: 0.943235
[Epoch 138] ogbg-molbace: 0.857254 test loss: 1.044522
[Epoch 139; Iter    12/   36] train: loss: 0.0020729
[Epoch 139] ogbg-molbace: 0.862374 val loss: 0.979137
[Epoch 139] ogbg-molbace: 0.859586 test loss: 1.065911
[Epoch 140; Iter     6/   36] train: loss: 0.0026989
[Epoch 140; Iter    36/   36] train: loss: 0.1150167
[Epoch 140] ogbg-molbace: 0.862610 val loss: 1.002346
[Epoch 140] ogbg-molbace: 0.863241 test loss: 0.985340
[Epoch 141; Iter    30/   36] train: loss: 0.0396753
[Epoch 141] ogbg-molbace: 0.867898 val loss: 0.991370
[Epoch 141] ogbg-molbace: 0.870938 test loss: 0.961429
[Epoch 142; Iter    24/   36] train: loss: 0.0071841
[Epoch 142] ogbg-molbace: 0.869081 val loss: 0.969246
[Epoch 142] ogbg-molbace: 0.866584 test loss: 0.992133
[Epoch 143; Iter    18/   36] train: loss: 0.0029967
[Epoch 143] ogbg-molbace: 0.872554 val loss: 0.948016
[Epoch 143] ogbg-molbace: 0.869072 test loss: 0.987065
[Epoch 144; Iter    12/   36] train: loss: 0.0196543
[Epoch 144] ogbg-molbace: 0.871765 val loss: 0.946946
[Epoch 144] ogbg-molbace: 0.864562 test loss: 1.039493
[Epoch 145; Iter     6/   36] train: loss: 0.0707473
[Epoch 145; Iter    36/   36] train: loss: 0.0434553
[Epoch 145] ogbg-molbace: 0.861190 val loss: 1.019187
[Epoch 145] ogbg-molbace: 0.869927 test loss: 1.001921
[Epoch 146; Iter    30/   36] train: loss: 0.0033301
[Epoch 146] ogbg-molbace: 0.860243 val loss: 0.991073
[Epoch 146] ogbg-molbace: 0.870471 test loss: 1.000891
[Epoch 147; Iter    24/   36] train: loss: 0.0029917
[Epoch 147] ogbg-molbace: 0.864820 val loss: 0.983142
[Epoch 147] ogbg-molbace: 0.871482 test loss: 0.964925
[Epoch 148; Iter    18/   36] train: loss: 0.0038164
[Epoch 148] ogbg-molbace: 0.860480 val loss: 1.022532
[Epoch 148] ogbg-molbace: 0.871015 test loss: 1.013184
[Epoch 149; Iter    12/   36] train: loss: 0.0176813
[Epoch 149] ogbg-molbace: 0.860401 val loss: 1.039291
[Epoch 149] ogbg-molbace: 0.867905 test loss: 1.085582
[Epoch 150; Iter     6/   36] train: loss: 0.0030433
[Epoch 150; Iter    36/   36] train: loss: 0.0338210
[Epoch 150] ogbg-molbace: 0.851641 val loss: 1.053075
[Epoch 150] ogbg-molbace: 0.864407 test loss: 0.994123
[Epoch 151; Iter    30/   36] train: loss: 0.0083022
[Epoch 151] ogbg-molbace: 0.856613 val loss: 1.048432
[Epoch 151] ogbg-molbace: 0.871093 test loss: 1.006810
[Epoch 152; Iter    24/   36] train: loss: 0.0125768
[Epoch 152] ogbg-molbace: 0.851720 val loss: 1.083747
[Epoch 152] ogbg-molbace: 0.870471 test loss: 1.020826
[Epoch 153; Iter    18/   36] train: loss: 0.0062422
[Epoch 153] ogbg-molbace: 0.858586 val loss: 1.061059
[Epoch 153] ogbg-molbace: 0.865806 test loss: 1.047056
[Epoch 154; Iter    12/   36] train: loss: 0.0052273
[Epoch 154] ogbg-molbace: 0.860243 val loss: 1.043957
[Epoch 154] ogbg-molbace: 0.864329 test loss: 1.067873
Early stopping criterion based on -ogbg-molbace- that should be max reached after 154 epochs. Best model checkpoint was in epoch 94.
Statistics on  val_best_checkpoint
mean_pred: -0.5259891152381897
std_pred: 5.720271110534668
mean_targets: 0.4361233413219452
std_targets: 0.49699893593788147
prcauc: 0.8293491463148517
rocauc: 0.8971748737373738
ogbg-molbace: 0.8971748737373738
BCEWithLogitsLoss: 0.5587785858660936
Statistics on  test
mean_pred: 0.05004759877920151
std_pred: 4.871495723724365
mean_targets: 0.48017618060112
std_targets: 0.5007109642028809
prcauc: 0.8556728262708467
rocauc: 0.8813559322033898
ogbg-molbace: 0.8813559322033898
BCEWithLogitsLoss: 0.5800190381705761
Statistics on  train
mean_pred: -0.39394378662109375
std_pred: 5.612476348876953
mean_targets: 0.45609065890312195
std_targets: 0.49830353260040283
prcauc: 0.9990786960352244
rocauc: 0.9992164136185875
ogbg-molbace: 0.9992164136185875
BCEWithLogitsLoss: 0.05899992947363191
All runs completed.
[Epoch 137; Iter    14/   31] train: loss: 0.0088530
[Epoch 137] ogbg-molbace: 0.911219 val loss: 0.546900
[Epoch 137] ogbg-molbace: 0.893596 test loss: 0.605600
[Epoch 138; Iter    13/   31] train: loss: 0.0101688
[Epoch 138] ogbg-molbace: 0.905524 val loss: 0.558430
[Epoch 138] ogbg-molbace: 0.891065 test loss: 0.596722
[Epoch 139; Iter    12/   31] train: loss: 0.0268273
[Epoch 139] ogbg-molbace: 0.908663 val loss: 0.561834
[Epoch 139] ogbg-molbace: 0.886092 test loss: 0.634376
[Epoch 140; Iter    11/   31] train: loss: 0.0521681
[Epoch 140] ogbg-molbace: 0.908977 val loss: 0.556750
[Epoch 140] ogbg-molbace: 0.886005 test loss: 0.643298
[Epoch 141; Iter    10/   31] train: loss: 0.0334554
[Epoch 141] ogbg-molbace: 0.909515 val loss: 0.564983
[Epoch 141] ogbg-molbace: 0.892898 test loss: 0.633914
[Epoch 142; Iter     9/   31] train: loss: 0.0182586
[Epoch 142] ogbg-molbace: 0.909022 val loss: 0.562071
[Epoch 142] ogbg-molbace: 0.890018 test loss: 0.641243
[Epoch 143; Iter     8/   31] train: loss: 0.0125121
[Epoch 143] ogbg-molbace: 0.909111 val loss: 0.565115
[Epoch 143] ogbg-molbace: 0.888142 test loss: 0.666055
[Epoch 144; Iter     7/   31] train: loss: 0.0287956
[Epoch 144] ogbg-molbace: 0.906914 val loss: 0.597961
[Epoch 144] ogbg-molbace: 0.889102 test loss: 0.676446
[Epoch 145; Iter     6/   31] train: loss: 0.0066163
[Epoch 145] ogbg-molbace: 0.906242 val loss: 0.591981
[Epoch 145] ogbg-molbace: 0.887684 test loss: 0.653985
[Epoch 146; Iter     5/   31] train: loss: 0.0781306
[Epoch 146] ogbg-molbace: 0.907497 val loss: 0.586170
[Epoch 146] ogbg-molbace: 0.887357 test loss: 0.676382
[Epoch 147; Iter     4/   31] train: loss: 0.0076711
[Epoch 147] ogbg-molbace: 0.904941 val loss: 0.596866
[Epoch 147] ogbg-molbace: 0.885525 test loss: 0.678050
[Epoch 148; Iter     3/   31] train: loss: 0.0119391
[Epoch 148] ogbg-molbace: 0.907273 val loss: 0.566705
[Epoch 148] ogbg-molbace: 0.880726 test loss: 0.713947
[Epoch 149; Iter     2/   31] train: loss: 0.0054372
[Epoch 149] ogbg-molbace: 0.906511 val loss: 0.606237
[Epoch 149] ogbg-molbace: 0.885481 test loss: 0.700197
[Epoch 150; Iter     1/   31] train: loss: 0.0069733
[Epoch 150; Iter    31/   31] train: loss: 0.0021026
[Epoch 150] ogbg-molbace: 0.907901 val loss: 0.588602
[Epoch 150] ogbg-molbace: 0.882689 test loss: 0.707944
[Epoch 151; Iter    30/   31] train: loss: 0.0578340
[Epoch 151] ogbg-molbace: 0.907094 val loss: 0.608040
[Epoch 151] ogbg-molbace: 0.883518 test loss: 0.704361
[Epoch 152; Iter    29/   31] train: loss: 0.0148132
[Epoch 152] ogbg-molbace: 0.905973 val loss: 0.592145
[Epoch 152] ogbg-molbace: 0.880421 test loss: 0.720919
[Epoch 153; Iter    28/   31] train: loss: 0.0073082
[Epoch 153] ogbg-molbace: 0.904448 val loss: 0.626830
[Epoch 153] ogbg-molbace: 0.879897 test loss: 0.731930
[Epoch 154; Iter    27/   31] train: loss: 0.0043679
[Epoch 154] ogbg-molbace: 0.904269 val loss: 0.671842
[Epoch 154] ogbg-molbace: 0.878894 test loss: 0.770050
[Epoch 155; Iter    26/   31] train: loss: 0.0060332
[Epoch 155] ogbg-molbace: 0.898798 val loss: 0.632507
[Epoch 155] ogbg-molbace: 0.872655 test loss: 0.778239
[Epoch 156; Iter    25/   31] train: loss: 0.0052377
[Epoch 156] ogbg-molbace: 0.901309 val loss: 0.622079
[Epoch 156] ogbg-molbace: 0.886397 test loss: 0.686406
[Epoch 157; Iter    24/   31] train: loss: 0.0349374
[Epoch 157] ogbg-molbace: 0.903910 val loss: 0.643705
[Epoch 157] ogbg-molbace: 0.882296 test loss: 0.739805
[Epoch 158; Iter    23/   31] train: loss: 0.0831946
[Epoch 158] ogbg-molbace: 0.890279 val loss: 0.705458
[Epoch 158] ogbg-molbace: 0.875316 test loss: 0.775758
[Epoch 159; Iter    22/   31] train: loss: 0.0530310
[Epoch 159] ogbg-molbace: 0.905390 val loss: 0.647944
[Epoch 159] ogbg-molbace: 0.885830 test loss: 0.735494
[Epoch 160; Iter    21/   31] train: loss: 0.0077373
[Epoch 160] ogbg-molbace: 0.898350 val loss: 0.627267
[Epoch 160] ogbg-molbace: 0.875447 test loss: 0.751525
[Epoch 161; Iter    20/   31] train: loss: 0.0110344
[Epoch 161] ogbg-molbace: 0.895615 val loss: 0.684644
[Epoch 161] ogbg-molbace: 0.875273 test loss: 0.803172
[Epoch 162; Iter    19/   31] train: loss: 0.0045827
[Epoch 162] ogbg-molbace: 0.895032 val loss: 0.630499
[Epoch 162] ogbg-molbace: 0.875055 test loss: 0.760970
[Epoch 163; Iter    18/   31] train: loss: 0.0064096
[Epoch 163] ogbg-molbace: 0.900637 val loss: 0.624405
[Epoch 163] ogbg-molbace: 0.878501 test loss: 0.751127
[Epoch 164; Iter    17/   31] train: loss: 0.0019315
[Epoch 164] ogbg-molbace: 0.901220 val loss: 0.612825
[Epoch 164] ogbg-molbace: 0.880988 test loss: 0.729002
[Epoch 165; Iter    16/   31] train: loss: 0.0048080
[Epoch 165] ogbg-molbace: 0.899561 val loss: 0.619746
[Epoch 165] ogbg-molbace: 0.882253 test loss: 0.730856
[Epoch 166; Iter    15/   31] train: loss: 0.0076160
[Epoch 166] ogbg-molbace: 0.899740 val loss: 0.638734
[Epoch 166] ogbg-molbace: 0.882776 test loss: 0.736903
[Epoch 167; Iter    14/   31] train: loss: 0.0025531
[Epoch 167] ogbg-molbace: 0.899605 val loss: 0.641580
[Epoch 167] ogbg-molbace: 0.881380 test loss: 0.752721
[Epoch 168; Iter    13/   31] train: loss: 0.0106786
[Epoch 168] ogbg-molbace: 0.897857 val loss: 0.626414
[Epoch 168] ogbg-molbace: 0.879853 test loss: 0.746785
[Epoch 169; Iter    12/   31] train: loss: 0.0042106
[Epoch 169] ogbg-molbace: 0.899067 val loss: 0.686986
[Epoch 169] ogbg-molbace: 0.880333 test loss: 0.776674
[Epoch 170; Iter    11/   31] train: loss: 0.0084180
[Epoch 170] ogbg-molbace: 0.900951 val loss: 0.667640
[Epoch 170] ogbg-molbace: 0.882689 test loss: 0.770977
[Epoch 171; Iter    10/   31] train: loss: 0.0062611
[Epoch 171] ogbg-molbace: 0.901399 val loss: 0.633612
[Epoch 171] ogbg-molbace: 0.881555 test loss: 0.757419
[Epoch 172; Iter     9/   31] train: loss: 0.0153478
[Epoch 172] ogbg-molbace: 0.900614 val loss: 0.658118
[Epoch 172] ogbg-molbace: 0.882253 test loss: 0.770734
[Epoch 173; Iter     8/   31] train: loss: 0.0120408
[Epoch 173] ogbg-molbace: 0.900682 val loss: 0.658391
[Epoch 173] ogbg-molbace: 0.880944 test loss: 0.768519
[Epoch 174; Iter     7/   31] train: loss: 0.0068125
[Epoch 174] ogbg-molbace: 0.901668 val loss: 0.665533
[Epoch 174] ogbg-molbace: 0.882733 test loss: 0.779184
[Epoch 175; Iter     6/   31] train: loss: 0.0012691
[Epoch 175] ogbg-molbace: 0.900951 val loss: 0.657737
[Epoch 175] ogbg-molbace: 0.880377 test loss: 0.784930
[Epoch 176; Iter     5/   31] train: loss: 0.0029582
[Epoch 176] ogbg-molbace: 0.900637 val loss: 0.668044
[Epoch 176] ogbg-molbace: 0.882296 test loss: 0.777092
[Epoch 177; Iter     4/   31] train: loss: 0.0035111
[Epoch 177] ogbg-molbace: 0.901040 val loss: 0.664615
[Epoch 177] ogbg-molbace: 0.884172 test loss: 0.768882
[Epoch 178; Iter     3/   31] train: loss: 0.0018076
[Epoch 178] ogbg-molbace: 0.899381 val loss: 0.655530
[Epoch 178] ogbg-molbace: 0.879679 test loss: 0.786274
[Epoch 179; Iter     2/   31] train: loss: 0.0172731
[Epoch 179] ogbg-molbace: 0.899561 val loss: 0.657118
[Epoch 179] ogbg-molbace: 0.880464 test loss: 0.775193
[Epoch 180; Iter     1/   31] train: loss: 0.0136318
[Epoch 180; Iter    31/   31] train: loss: 0.4742656
[Epoch 180] ogbg-molbace: 0.899023 val loss: 0.704216
[Epoch 180] ogbg-molbace: 0.881119 test loss: 0.810889
[Epoch 181; Iter    30/   31] train: loss: 0.0060397
[Epoch 181] ogbg-molbace: 0.896915 val loss: 0.718484
[Epoch 181] ogbg-molbace: 0.875840 test loss: 0.820843
[Epoch 182; Iter    29/   31] train: loss: 0.0260549
[Epoch 182] ogbg-molbace: 0.902430 val loss: 0.674972
[Epoch 182] ogbg-molbace: 0.875665 test loss: 0.809555
[Epoch 183; Iter    28/   31] train: loss: 0.0067544
[Epoch 183] ogbg-molbace: 0.899919 val loss: 0.686446
[Epoch 183] ogbg-molbace: 0.876581 test loss: 0.814326
[Epoch 184; Iter    27/   31] train: loss: 0.0048982
[Epoch 184] ogbg-molbace: 0.892341 val loss: 0.743514
[Epoch 184] ogbg-molbace: 0.874051 test loss: 0.828734
[Epoch 185; Iter    26/   31] train: loss: 0.0022301
[Epoch 185] ogbg-molbace: 0.896781 val loss: 0.726667
[Epoch 185] ogbg-molbace: 0.873571 test loss: 0.840007
[Epoch 186; Iter    25/   31] train: loss: 0.0093331
[Epoch 186] ogbg-molbace: 0.898798 val loss: 0.698020
[Epoch 137; Iter    14/   31] train: loss: 0.0180313
[Epoch 137] ogbg-molbace: 0.922922 val loss: 0.501980
[Epoch 137] ogbg-molbace: 0.858956 test loss: 0.848634
[Epoch 138; Iter    13/   31] train: loss: 0.0082477
[Epoch 138] ogbg-molbace: 0.923101 val loss: 0.540360
[Epoch 138] ogbg-molbace: 0.859436 test loss: 0.846896
[Epoch 139; Iter    12/   31] train: loss: 0.0036107
[Epoch 139] ogbg-molbace: 0.921397 val loss: 0.512660
[Epoch 139] ogbg-molbace: 0.861138 test loss: 0.807351
[Epoch 140; Iter    11/   31] train: loss: 0.0112975
[Epoch 140] ogbg-molbace: 0.923280 val loss: 0.511090
[Epoch 140] ogbg-molbace: 0.868249 test loss: 0.839938
[Epoch 141; Iter    10/   31] train: loss: 0.0404832
[Epoch 141] ogbg-molbace: 0.908394 val loss: 0.602823
[Epoch 141] ogbg-molbace: 0.851104 test loss: 0.861738
[Epoch 142; Iter     9/   31] train: loss: 0.0313692
[Epoch 142] ogbg-molbace: 0.903910 val loss: 0.634739
[Epoch 142] ogbg-molbace: 0.847614 test loss: 0.900912
[Epoch 143; Iter     8/   31] train: loss: 0.0359774
[Epoch 143] ogbg-molbace: 0.908977 val loss: 0.595353
[Epoch 143] ogbg-molbace: 0.857779 test loss: 0.893060
[Epoch 144; Iter     7/   31] train: loss: 0.0096987
[Epoch 144] ogbg-molbace: 0.914716 val loss: 0.608965
[Epoch 144] ogbg-molbace: 0.859218 test loss: 0.882089
[Epoch 145; Iter     6/   31] train: loss: 0.0335771
[Epoch 145] ogbg-molbace: 0.906511 val loss: 0.679236
[Epoch 145] ogbg-molbace: 0.858695 test loss: 0.870177
[Epoch 146; Iter     5/   31] train: loss: 0.0206470
[Epoch 146] ogbg-molbace: 0.916106 val loss: 0.502248
[Epoch 146] ogbg-molbace: 0.866984 test loss: 0.744275
[Epoch 147; Iter     4/   31] train: loss: 0.0209248
[Epoch 147] ogbg-molbace: 0.912160 val loss: 0.551299
[Epoch 147] ogbg-molbace: 0.873571 test loss: 0.731675
[Epoch 148; Iter     3/   31] train: loss: 0.0359652
[Epoch 148] ogbg-molbace: 0.915568 val loss: 0.545390
[Epoch 148] ogbg-molbace: 0.877018 test loss: 0.737762
[Epoch 149; Iter     2/   31] train: loss: 0.0114471
[Epoch 149] ogbg-molbace: 0.914492 val loss: 0.538166
[Epoch 149] ogbg-molbace: 0.872568 test loss: 0.762966
[Epoch 150; Iter     1/   31] train: loss: 0.0062185
[Epoch 150; Iter    31/   31] train: loss: 0.0024009
[Epoch 150] ogbg-molbace: 0.916017 val loss: 0.533096
[Epoch 150] ogbg-molbace: 0.872481 test loss: 0.772063
[Epoch 151; Iter    30/   31] train: loss: 0.0082810
[Epoch 151] ogbg-molbace: 0.914761 val loss: 0.571246
[Epoch 151] ogbg-molbace: 0.870168 test loss: 0.814600
[Epoch 152; Iter    29/   31] train: loss: 0.0436979
[Epoch 152] ogbg-molbace: 0.914671 val loss: 0.563073
[Epoch 152] ogbg-molbace: 0.869994 test loss: 0.808186
[Epoch 153; Iter    28/   31] train: loss: 0.0131008
[Epoch 153] ogbg-molbace: 0.916868 val loss: 0.554388
[Epoch 153] ogbg-molbace: 0.868685 test loss: 0.860699
[Epoch 154; Iter    27/   31] train: loss: 0.0074295
[Epoch 154] ogbg-molbace: 0.913595 val loss: 0.597033
[Epoch 154] ogbg-molbace: 0.872655 test loss: 0.830269
[Epoch 155; Iter    26/   31] train: loss: 0.0148837
[Epoch 155] ogbg-molbace: 0.914806 val loss: 0.571862
[Epoch 155] ogbg-molbace: 0.864497 test loss: 0.866240
[Epoch 156; Iter    25/   31] train: loss: 0.0086040
[Epoch 156] ogbg-molbace: 0.914940 val loss: 0.564834
[Epoch 156] ogbg-molbace: 0.861138 test loss: 0.911920
[Epoch 157; Iter    24/   31] train: loss: 0.0055993
[Epoch 157] ogbg-molbace: 0.916106 val loss: 0.573708
[Epoch 157] ogbg-molbace: 0.871172 test loss: 0.849736
[Epoch 158; Iter    23/   31] train: loss: 0.0053500
[Epoch 158] ogbg-molbace: 0.915927 val loss: 0.562958
[Epoch 158] ogbg-molbace: 0.864497 test loss: 0.881874
[Epoch 159; Iter    22/   31] train: loss: 0.0198075
[Epoch 159] ogbg-molbace: 0.918303 val loss: 0.543650
[Epoch 159] ogbg-molbace: 0.870387 test loss: 0.831846
[Epoch 160; Iter    21/   31] train: loss: 0.0107024
[Epoch 160] ogbg-molbace: 0.918572 val loss: 0.538954
[Epoch 160] ogbg-molbace: 0.869339 test loss: 0.834640
[Epoch 161; Iter    20/   31] train: loss: 0.1695943
[Epoch 161] ogbg-molbace: 0.918303 val loss: 0.545226
[Epoch 161] ogbg-molbace: 0.866024 test loss: 0.841605
[Epoch 162; Iter    19/   31] train: loss: 0.0135129
[Epoch 162] ogbg-molbace: 0.913640 val loss: 0.588630
[Epoch 162] ogbg-molbace: 0.864061 test loss: 0.845343
[Epoch 163; Iter    18/   31] train: loss: 0.0819545
[Epoch 163] ogbg-molbace: 0.911084 val loss: 0.616230
[Epoch 163] ogbg-molbace: 0.879592 test loss: 0.751437
[Epoch 164; Iter    17/   31] train: loss: 0.0073517
[Epoch 164] ogbg-molbace: 0.918348 val loss: 0.534415
[Epoch 164] ogbg-molbace: 0.867027 test loss: 0.866900
[Epoch 165; Iter    16/   31] train: loss: 0.0097148
[Epoch 165] ogbg-molbace: 0.916017 val loss: 0.576942
[Epoch 165] ogbg-molbace: 0.871739 test loss: 0.854198
[Epoch 166; Iter    15/   31] train: loss: 0.0028620
[Epoch 166] ogbg-molbace: 0.916330 val loss: 0.600790
[Epoch 166] ogbg-molbace: 0.868554 test loss: 0.889221
[Epoch 167; Iter    14/   31] train: loss: 0.0069565
[Epoch 167] ogbg-molbace: 0.917720 val loss: 0.574072
[Epoch 167] ogbg-molbace: 0.870125 test loss: 0.875649
[Epoch 168; Iter    13/   31] train: loss: 0.0080018
[Epoch 168] ogbg-molbace: 0.917989 val loss: 0.573153
[Epoch 168] ogbg-molbace: 0.871783 test loss: 0.863494
[Epoch 169; Iter    12/   31] train: loss: 0.0019953
[Epoch 169] ogbg-molbace: 0.918886 val loss: 0.552792
[Epoch 169] ogbg-molbace: 0.874226 test loss: 0.856444
[Epoch 170; Iter    11/   31] train: loss: 0.4242602
[Epoch 170] ogbg-molbace: 0.915568 val loss: 0.614720
[Epoch 170] ogbg-molbace: 0.867769 test loss: 0.926592
[Epoch 171; Iter    10/   31] train: loss: 0.0193340
[Epoch 171] ogbg-molbace: 0.919155 val loss: 0.579147
[Epoch 171] ogbg-molbace: 0.876887 test loss: 0.771172
[Epoch 172; Iter     9/   31] train: loss: 0.0259133
[Epoch 172] ogbg-molbace: 0.922608 val loss: 0.518020
[Epoch 172] ogbg-molbace: 0.874880 test loss: 0.793510
[Epoch 173; Iter     8/   31] train: loss: 0.0305549
[Epoch 173] ogbg-molbace: 0.920859 val loss: 0.565085
[Epoch 173] ogbg-molbace: 0.870474 test loss: 0.848203
[Epoch 174; Iter     7/   31] train: loss: 0.0065731
[Epoch 174] ogbg-molbace: 0.914985 val loss: 0.561869
[Epoch 174] ogbg-molbace: 0.862141 test loss: 0.867667
[Epoch 175; Iter     6/   31] train: loss: 0.0139850
[Epoch 175] ogbg-molbace: 0.918124 val loss: 0.571449
[Epoch 175] ogbg-molbace: 0.861312 test loss: 0.899505
[Epoch 176; Iter     5/   31] train: loss: 0.0102075
[Epoch 176] ogbg-molbace: 0.919514 val loss: 0.554061
[Epoch 176] ogbg-molbace: 0.859480 test loss: 0.933190
[Epoch 177; Iter     4/   31] train: loss: 0.0027265
[Epoch 177] ogbg-molbace: 0.918976 val loss: 0.547961
[Epoch 177] ogbg-molbace: 0.858520 test loss: 0.933152
[Epoch 178; Iter     3/   31] train: loss: 0.0026608
[Epoch 178] ogbg-molbace: 0.919335 val loss: 0.560364
[Epoch 178] ogbg-molbace: 0.861181 test loss: 0.947024
[Epoch 179; Iter     2/   31] train: loss: 0.0119960
[Epoch 179] ogbg-molbace: 0.919917 val loss: 0.552730
[Epoch 179] ogbg-molbace: 0.862970 test loss: 0.938016
[Epoch 180; Iter     1/   31] train: loss: 0.0085736
[Epoch 180; Iter    31/   31] train: loss: 0.0205483
[Epoch 180] ogbg-molbace: 0.919379 val loss: 0.544605
[Epoch 180] ogbg-molbace: 0.862883 test loss: 0.909663
[Epoch 181; Iter    30/   31] train: loss: 0.0050125
[Epoch 181] ogbg-molbace: 0.920411 val loss: 0.548881
[Epoch 181] ogbg-molbace: 0.864192 test loss: 0.935493
[Epoch 182; Iter    29/   31] train: loss: 0.0037504
[Epoch 182] ogbg-molbace: 0.921532 val loss: 0.569511
[Epoch 182] ogbg-molbace: 0.867245 test loss: 0.924282
[Epoch 183; Iter    28/   31] train: loss: 0.0036195
[Epoch 183] ogbg-molbace: 0.921756 val loss: 0.572797
[Epoch 183] ogbg-molbace: 0.867769 test loss: 0.924757
[Epoch 184; Iter    27/   31] train: loss: 0.0115643
[Epoch 184] ogbg-molbace: 0.921218 val loss: 0.572643
[Epoch 184] ogbg-molbace: 0.864541 test loss: 0.961750
[Epoch 185; Iter    26/   31] train: loss: 0.0024635
[Epoch 185] ogbg-molbace: 0.923370 val loss: 0.558639
[Epoch 185] ogbg-molbace: 0.863973 test loss: 0.986179
[Epoch 186; Iter    25/   31] train: loss: 0.0325164
[Epoch 186] ogbg-molbace: 0.923594 val loss: 0.537706
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 186] ogbg-molbace: 0.877105 test loss: 0.800839
[Epoch 187; Iter    24/   31] train: loss: 0.0038971
[Epoch 187] ogbg-molbace: 0.897722 val loss: 0.699872
[Epoch 187] ogbg-molbace: 0.873702 test loss: 0.843923
[Epoch 188; Iter    23/   31] train: loss: 0.0013193
[Epoch 188] ogbg-molbace: 0.897543 val loss: 0.683377
[Epoch 188] ogbg-molbace: 0.871172 test loss: 0.821214
[Epoch 189; Iter    22/   31] train: loss: 0.0016785
[Epoch 189] ogbg-molbace: 0.900099 val loss: 0.724419
[Epoch 189] ogbg-molbace: 0.876276 test loss: 0.828703
[Epoch 190; Iter    21/   31] train: loss: 0.0020443
[Epoch 190] ogbg-molbace: 0.901982 val loss: 0.692239
[Epoch 190] ogbg-molbace: 0.878152 test loss: 0.801123
[Epoch 191; Iter    20/   31] train: loss: 0.0062476
[Epoch 191] ogbg-molbace: 0.903058 val loss: 0.676346
[Epoch 191] ogbg-molbace: 0.877847 test loss: 0.800607
[Epoch 192; Iter    19/   31] train: loss: 0.0019726
[Epoch 192] ogbg-molbace: 0.902834 val loss: 0.687108
[Epoch 192] ogbg-molbace: 0.876232 test loss: 0.844741
[Epoch 193; Iter    18/   31] train: loss: 0.0037058
[Epoch 193] ogbg-molbace: 0.903596 val loss: 0.671809
[Epoch 193] ogbg-molbace: 0.878545 test loss: 0.817306
[Epoch 194; Iter    17/   31] train: loss: 0.0012599
[Epoch 194] ogbg-molbace: 0.903462 val loss: 0.718076
[Epoch 194] ogbg-molbace: 0.879417 test loss: 0.835560
Early stopping criterion based on -ogbg-molbace- that should be max reached after 194 epochs. Best model checkpoint was in epoch 134.
Statistics on  val_best_checkpoint
mean_pred: -1.7565557956695557
std_pred: 6.235694408416748
mean_targets: 0.41584160923957825
std_targets: 0.49368178844451904
prcauc: 0.8729745725719342
rocauc: 0.9136848713119899
ogbg-molbace: 0.9136848713119899
BCEWithLogitsLoss: 0.46134028109637176
Statistics on  test
mean_pred: -1.0345267057418823
std_pred: 6.058401584625244
mean_targets: 0.48184821009635925
std_targets: 0.5004969835281372
prcauc: 0.8557263157019073
rocauc: 0.8837361486781258
ogbg-molbace: 0.8837361486781258
BCEWithLogitsLoss: 0.580437646670775
Statistics on  train
mean_pred: -1.4316586256027222
std_pred: 6.2868218421936035
mean_targets: 0.4619625210762024
std_targets: 0.49882611632347107
prcauc: 1.0
rocauc: 1.0
ogbg-molbace: 1.0
BCEWithLogitsLoss: 0.037822895382921544
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 186] ogbg-molbace: 0.863319 test loss: 0.965284
[Epoch 187; Iter    24/   31] train: loss: 0.0034299
[Epoch 187] ogbg-molbace: 0.923280 val loss: 0.540026
[Epoch 187] ogbg-molbace: 0.863930 test loss: 0.957726
[Epoch 188; Iter    23/   31] train: loss: 0.0043210
[Epoch 188] ogbg-molbace: 0.923549 val loss: 0.549749
[Epoch 188] ogbg-molbace: 0.866329 test loss: 0.972596
[Epoch 189; Iter    22/   31] train: loss: 0.0134286
[Epoch 189] ogbg-molbace: 0.916734 val loss: 0.590311
[Epoch 189] ogbg-molbace: 0.859698 test loss: 0.956874
[Epoch 190; Iter    21/   31] train: loss: 0.0015794
[Epoch 190] ogbg-molbace: 0.916196 val loss: 0.582660
[Epoch 190] ogbg-molbace: 0.861749 test loss: 0.950623
[Epoch 191; Iter    20/   31] train: loss: 0.0027897
[Epoch 191] ogbg-molbace: 0.918124 val loss: 0.597374
[Epoch 191] ogbg-molbace: 0.862534 test loss: 0.976059
[Epoch 192; Iter    19/   31] train: loss: 0.0068747
[Epoch 192] ogbg-molbace: 0.915747 val loss: 0.603578
[Epoch 192] ogbg-molbace: 0.860702 test loss: 0.973549
[Epoch 193; Iter    18/   31] train: loss: 0.0008798
[Epoch 193] ogbg-molbace: 0.917182 val loss: 0.588966
[Epoch 193] ogbg-molbace: 0.864933 test loss: 0.958273
[Epoch 194; Iter    17/   31] train: loss: 0.0054395
[Epoch 194] ogbg-molbace: 0.917093 val loss: 0.580814
[Epoch 194] ogbg-molbace: 0.864977 test loss: 0.910911
[Epoch 195; Iter    16/   31] train: loss: 0.0008968
[Epoch 195] ogbg-molbace: 0.917272 val loss: 0.600415
[Epoch 195] ogbg-molbace: 0.866722 test loss: 0.916738
Early stopping criterion based on -ogbg-molbace- that should be max reached after 195 epochs. Best model checkpoint was in epoch 135.
Statistics on  val_best_checkpoint
mean_pred: -0.9432545304298401
std_pred: 6.793394565582275
mean_targets: 0.41584160923957825
std_targets: 0.49368178844451904
prcauc: 0.8654348383135009
rocauc: 0.9268227064837234
ogbg-molbace: 0.9268227064837234
BCEWithLogitsLoss: 0.46875402839346364
Statistics on  test
mean_pred: -0.37571460008621216
std_pred: 6.596466064453125
mean_targets: 0.48184821009635925
std_targets: 0.5004969835281372
prcauc: 0.8335588502553122
rocauc: 0.8624901841026088
ogbg-molbace: 0.8624901841026088
BCEWithLogitsLoss: 0.7761078549162697
Statistics on  train
mean_pred: -0.6235153079032898
std_pred: 6.886890411376953
mean_targets: 0.4619625210762024
std_targets: 0.49882614612579346
prcauc: 1.0
rocauc: 1.0
ogbg-molbace: 1.0
BCEWithLogitsLoss: 0.0076118961216941955
All runs completed.
