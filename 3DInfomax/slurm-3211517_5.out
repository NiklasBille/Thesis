>>> Starting run for dataset: sider
Running RANDOM configs_split_experiments/GraphCL/sider/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/sider/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/sider/random/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 4: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.7.yml --seed 6 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/GraphCL/sider/random/train_prop=0.6.yml --seed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.8/PNA_ogbg-molsider_GraphCL_sider_random=0.8_5_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.8
logdir: runs/split/GraphCL/sider/random/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6932235
[Epoch 1] ogbg-molsider: 0.510630 val loss: 0.693179
[Epoch 1] ogbg-molsider: 0.495423 test loss: 0.693000
[Epoch 2; Iter    24/   36] train: loss: 0.6927986
[Epoch 2] ogbg-molsider: 0.491367 val loss: 0.693220
[Epoch 2] ogbg-molsider: 0.502244 test loss: 0.693024
[Epoch 3; Iter    18/   36] train: loss: 0.6933159
[Epoch 3] ogbg-molsider: 0.485981 val loss: 0.693181
[Epoch 3] ogbg-molsider: 0.505851 test loss: 0.693139
[Epoch 4; Iter    12/   36] train: loss: 0.6928485
[Epoch 4] ogbg-molsider: 0.486581 val loss: 0.693234
[Epoch 4] ogbg-molsider: 0.503806 test loss: 0.693075
[Epoch 5; Iter     6/   36] train: loss: 0.6933491
[Epoch 5; Iter    36/   36] train: loss: 0.6927848
[Epoch 5] ogbg-molsider: 0.484455 val loss: 0.693199
[Epoch 5] ogbg-molsider: 0.503125 test loss: 0.693092
[Epoch 6; Iter    30/   36] train: loss: 0.6925712
[Epoch 6] ogbg-molsider: 0.485330 val loss: 0.692971
[Epoch 6] ogbg-molsider: 0.508754 test loss: 0.692859
[Epoch 7; Iter    24/   36] train: loss: 0.6933777
[Epoch 7] ogbg-molsider: 0.485980 val loss: 0.692885
[Epoch 7] ogbg-molsider: 0.506068 test loss: 0.692785
[Epoch 8; Iter    18/   36] train: loss: 0.6925715
[Epoch 8] ogbg-molsider: 0.487035 val loss: 0.692846
[Epoch 8] ogbg-molsider: 0.507189 test loss: 0.692764
[Epoch 9; Iter    12/   36] train: loss: 0.6927859
[Epoch 9] ogbg-molsider: 0.485109 val loss: 0.692839
[Epoch 9] ogbg-molsider: 0.503929 test loss: 0.692697
[Epoch 10; Iter     6/   36] train: loss: 0.6930721
[Epoch 10; Iter    36/   36] train: loss: 0.6928892
[Epoch 10] ogbg-molsider: 0.488540 val loss: 0.692631
[Epoch 10] ogbg-molsider: 0.505227 test loss: 0.692453
[Epoch 11; Iter    30/   36] train: loss: 0.6921880
[Epoch 11] ogbg-molsider: 0.484107 val loss: 0.692557
[Epoch 11] ogbg-molsider: 0.506273 test loss: 0.692525
[Epoch 12; Iter    24/   36] train: loss: 0.6927437
[Epoch 12] ogbg-molsider: 0.487753 val loss: 0.692482
[Epoch 12] ogbg-molsider: 0.502762 test loss: 0.692382
[Epoch 13; Iter    18/   36] train: loss: 0.6922337
[Epoch 13] ogbg-molsider: 0.485960 val loss: 0.692274
[Epoch 13] ogbg-molsider: 0.504526 test loss: 0.692257
[Epoch 14; Iter    12/   36] train: loss: 0.6919163
[Epoch 14] ogbg-molsider: 0.487906 val loss: 0.692154
[Epoch 14] ogbg-molsider: 0.504939 test loss: 0.692104
[Epoch 15; Iter     6/   36] train: loss: 0.6922705
[Epoch 15; Iter    36/   36] train: loss: 0.6922332
[Epoch 15] ogbg-molsider: 0.488473 val loss: 0.691963
[Epoch 15] ogbg-molsider: 0.506094 test loss: 0.691831
[Epoch 16; Iter    30/   36] train: loss: 0.6920884
[Epoch 16] ogbg-molsider: 0.489213 val loss: 0.691694
[Epoch 16] ogbg-molsider: 0.504515 test loss: 0.691643
[Epoch 17; Iter    24/   36] train: loss: 0.6912226
[Epoch 17] ogbg-molsider: 0.490435 val loss: 0.691553
[Epoch 17] ogbg-molsider: 0.506458 test loss: 0.691414
[Epoch 18; Iter    18/   36] train: loss: 0.6918544
[Epoch 18] ogbg-molsider: 0.488318 val loss: 0.691307
[Epoch 18] ogbg-molsider: 0.509202 test loss: 0.691265
[Epoch 19; Iter    12/   36] train: loss: 0.6915988
[Epoch 19] ogbg-molsider: 0.489855 val loss: 0.691257
[Epoch 19] ogbg-molsider: 0.505555 test loss: 0.691150
[Epoch 20; Iter     6/   36] train: loss: 0.6906312
[Epoch 20; Iter    36/   36] train: loss: 0.6846114
[Epoch 20] ogbg-molsider: 0.570686 val loss: 0.682050
[Epoch 20] ogbg-molsider: 0.583022 test loss: 0.683426
[Epoch 21; Iter    30/   36] train: loss: 0.6741669
[Epoch 21] ogbg-molsider: 0.595009 val loss: 0.662762
[Epoch 21] ogbg-molsider: 0.593939 test loss: 0.667556
[Epoch 22; Iter    24/   36] train: loss: 0.6527660
[Epoch 22] ogbg-molsider: 0.592202 val loss: 0.641944
[Epoch 22] ogbg-molsider: 0.578804 test loss: 0.650964
[Epoch 23; Iter    18/   36] train: loss: 0.6418217
[Epoch 23] ogbg-molsider: 0.597545 val loss: 0.626092
[Epoch 23] ogbg-molsider: 0.562908 test loss: 0.640339
[Epoch 24; Iter    12/   36] train: loss: 0.6060770
[Epoch 24] ogbg-molsider: 0.583787 val loss: 0.617184
[Epoch 24] ogbg-molsider: 0.553986 test loss: 0.617101
[Epoch 25; Iter     6/   36] train: loss: 0.5769709
[Epoch 25; Iter    36/   36] train: loss: 0.5698438
[Epoch 25] ogbg-molsider: 0.572193 val loss: 0.543201
[Epoch 25] ogbg-molsider: 0.567657 test loss: 0.565442
[Epoch 26; Iter    30/   36] train: loss: 0.5393446
[Epoch 26] ogbg-molsider: 0.539380 val loss: 0.535226
[Epoch 26] ogbg-molsider: 0.556891 test loss: 0.548395
[Epoch 27; Iter    24/   36] train: loss: 0.5286056
[Epoch 27] ogbg-molsider: 0.605935 val loss: 0.519646
[Epoch 27] ogbg-molsider: 0.570358 test loss: 0.547038
[Epoch 28; Iter    18/   36] train: loss: 0.5368709
[Epoch 28] ogbg-molsider: 0.598022 val loss: 0.499209
[Epoch 28] ogbg-molsider: 0.571717 test loss: 0.528993
[Epoch 29; Iter    12/   36] train: loss: 0.4896192
[Epoch 29] ogbg-molsider: 0.590650 val loss: 0.488788
[Epoch 29] ogbg-molsider: 0.586124 test loss: 0.538643
[Epoch 30; Iter     6/   36] train: loss: 0.5370049
[Epoch 30; Iter    36/   36] train: loss: 0.5011341
[Epoch 30] ogbg-molsider: 0.591159 val loss: 0.488644
[Epoch 30] ogbg-molsider: 0.596411 test loss: 0.527174
[Epoch 31; Iter    30/   36] train: loss: 0.4926641
[Epoch 31] ogbg-molsider: 0.615262 val loss: 0.489692
[Epoch 31] ogbg-molsider: 0.583577 test loss: 0.521674
[Epoch 32; Iter    24/   36] train: loss: 0.4738682
[Epoch 32] ogbg-molsider: 0.614184 val loss: 0.493391
[Epoch 32] ogbg-molsider: 0.599607 test loss: 0.519443
[Epoch 33; Iter    18/   36] train: loss: 0.5233123
[Epoch 33] ogbg-molsider: 0.599494 val loss: 0.495544
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.8/PNA_ogbg-molsider_GraphCL_sider_random=0.8_4_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.8
logdir: runs/split/GraphCL/sider/random/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6933045
[Epoch 1] ogbg-molsider: 0.499121 val loss: 0.693277
[Epoch 1] ogbg-molsider: 0.478789 test loss: 0.693281
[Epoch 2; Iter    24/   36] train: loss: 0.6932240
[Epoch 2] ogbg-molsider: 0.498672 val loss: 0.693439
[Epoch 2] ogbg-molsider: 0.476491 test loss: 0.693476
[Epoch 3; Iter    18/   36] train: loss: 0.6929834
[Epoch 3] ogbg-molsider: 0.500567 val loss: 0.693395
[Epoch 3] ogbg-molsider: 0.476189 test loss: 0.693475
[Epoch 4; Iter    12/   36] train: loss: 0.6934607
[Epoch 4] ogbg-molsider: 0.502179 val loss: 0.693347
[Epoch 4] ogbg-molsider: 0.474857 test loss: 0.693441
[Epoch 5; Iter     6/   36] train: loss: 0.6934330
[Epoch 5; Iter    36/   36] train: loss: 0.6930247
[Epoch 5] ogbg-molsider: 0.504058 val loss: 0.693381
[Epoch 5] ogbg-molsider: 0.474763 test loss: 0.693502
[Epoch 6; Iter    30/   36] train: loss: 0.6931607
[Epoch 6] ogbg-molsider: 0.501486 val loss: 0.693254
[Epoch 6] ogbg-molsider: 0.474941 test loss: 0.693366
[Epoch 7; Iter    24/   36] train: loss: 0.6932320
[Epoch 7] ogbg-molsider: 0.502369 val loss: 0.693156
[Epoch 7] ogbg-molsider: 0.474587 test loss: 0.693283
[Epoch 8; Iter    18/   36] train: loss: 0.6926628
[Epoch 8] ogbg-molsider: 0.504783 val loss: 0.692974
[Epoch 8] ogbg-molsider: 0.473678 test loss: 0.693097
[Epoch 9; Iter    12/   36] train: loss: 0.6925351
[Epoch 9] ogbg-molsider: 0.501960 val loss: 0.692849
[Epoch 9] ogbg-molsider: 0.476086 test loss: 0.692963
[Epoch 10; Iter     6/   36] train: loss: 0.6926242
[Epoch 10; Iter    36/   36] train: loss: 0.6923990
[Epoch 10] ogbg-molsider: 0.504943 val loss: 0.692829
[Epoch 10] ogbg-molsider: 0.478052 test loss: 0.692943
[Epoch 11; Iter    30/   36] train: loss: 0.6928376
[Epoch 11] ogbg-molsider: 0.502329 val loss: 0.692565
[Epoch 11] ogbg-molsider: 0.483002 test loss: 0.692702
[Epoch 12; Iter    24/   36] train: loss: 0.6927315
[Epoch 12] ogbg-molsider: 0.504219 val loss: 0.692569
[Epoch 12] ogbg-molsider: 0.479403 test loss: 0.692743
[Epoch 13; Iter    18/   36] train: loss: 0.6920915
[Epoch 13] ogbg-molsider: 0.503228 val loss: 0.692339
[Epoch 13] ogbg-molsider: 0.473432 test loss: 0.692449
[Epoch 14; Iter    12/   36] train: loss: 0.6921979
[Epoch 14] ogbg-molsider: 0.506584 val loss: 0.692281
[Epoch 14] ogbg-molsider: 0.475637 test loss: 0.692450
[Epoch 15; Iter     6/   36] train: loss: 0.6918967
[Epoch 15; Iter    36/   36] train: loss: 0.6920685
[Epoch 15] ogbg-molsider: 0.504976 val loss: 0.691991
[Epoch 15] ogbg-molsider: 0.477404 test loss: 0.692124
[Epoch 16; Iter    30/   36] train: loss: 0.6917840
[Epoch 16] ogbg-molsider: 0.504616 val loss: 0.691799
[Epoch 16] ogbg-molsider: 0.476630 test loss: 0.691939
[Epoch 17; Iter    24/   36] train: loss: 0.6914260
[Epoch 17] ogbg-molsider: 0.506299 val loss: 0.691648
[Epoch 17] ogbg-molsider: 0.477592 test loss: 0.691795
[Epoch 18; Iter    18/   36] train: loss: 0.6918639
[Epoch 18] ogbg-molsider: 0.507409 val loss: 0.691329
[Epoch 18] ogbg-molsider: 0.477641 test loss: 0.691482
[Epoch 19; Iter    12/   36] train: loss: 0.6912261
[Epoch 19] ogbg-molsider: 0.507693 val loss: 0.691130
[Epoch 19] ogbg-molsider: 0.481104 test loss: 0.691306
[Epoch 20; Iter     6/   36] train: loss: 0.6906980
[Epoch 20; Iter    36/   36] train: loss: 0.6845238
[Epoch 20] ogbg-molsider: 0.585944 val loss: 0.681880
[Epoch 20] ogbg-molsider: 0.571535 test loss: 0.683293
[Epoch 21; Iter    30/   36] train: loss: 0.6693282
[Epoch 21] ogbg-molsider: 0.587738 val loss: 0.657110
[Epoch 21] ogbg-molsider: 0.583922 test loss: 0.662133
[Epoch 22; Iter    24/   36] train: loss: 0.6556234
[Epoch 22] ogbg-molsider: 0.589162 val loss: 0.630863
[Epoch 22] ogbg-molsider: 0.563447 test loss: 0.640654
[Epoch 23; Iter    18/   36] train: loss: 0.6290783
[Epoch 23] ogbg-molsider: 0.593150 val loss: 0.618511
[Epoch 23] ogbg-molsider: 0.566008 test loss: 0.629684
[Epoch 24; Iter    12/   36] train: loss: 0.6026036
[Epoch 24] ogbg-molsider: 0.580882 val loss: 0.574333
[Epoch 24] ogbg-molsider: 0.584172 test loss: 0.594016
[Epoch 25; Iter     6/   36] train: loss: 0.5818577
[Epoch 25; Iter    36/   36] train: loss: 0.5855239
[Epoch 25] ogbg-molsider: 0.570630 val loss: 0.554897
[Epoch 25] ogbg-molsider: 0.588958 test loss: 0.577010
[Epoch 26; Iter    30/   36] train: loss: 0.5190133
[Epoch 26] ogbg-molsider: 0.609781 val loss: 0.512727
[Epoch 26] ogbg-molsider: 0.571371 test loss: 0.545865
[Epoch 27; Iter    24/   36] train: loss: 0.5296628
[Epoch 27] ogbg-molsider: 0.610678 val loss: 0.500912
[Epoch 27] ogbg-molsider: 0.566183 test loss: 0.538777
[Epoch 28; Iter    18/   36] train: loss: 0.5342269
[Epoch 28] ogbg-molsider: 0.622395 val loss: 0.493756
[Epoch 28] ogbg-molsider: 0.586454 test loss: 0.530891
[Epoch 29; Iter    12/   36] train: loss: 0.5160374
[Epoch 29] ogbg-molsider: 0.631333 val loss: 0.486886
[Epoch 29] ogbg-molsider: 0.568678 test loss: 0.525810
[Epoch 30; Iter     6/   36] train: loss: 0.5048824
[Epoch 30; Iter    36/   36] train: loss: 0.4996399
[Epoch 30] ogbg-molsider: 0.633153 val loss: 0.485781
[Epoch 30] ogbg-molsider: 0.560793 test loss: 0.529148
[Epoch 31; Iter    30/   36] train: loss: 0.4744323
[Epoch 31] ogbg-molsider: 0.639892 val loss: 0.480570
[Epoch 31] ogbg-molsider: 0.577796 test loss: 0.530001
[Epoch 32; Iter    24/   36] train: loss: 0.4632846
[Epoch 32] ogbg-molsider: 0.643528 val loss: 0.485911
[Epoch 32] ogbg-molsider: 0.585391 test loss: 0.524814
[Epoch 33; Iter    18/   36] train: loss: 0.5028703
[Epoch 33] ogbg-molsider: 0.624515 val loss: 0.485493
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.8/PNA_ogbg-molsider_GraphCL_sider_random=0.8_6_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.8
logdir: runs/split/GraphCL/sider/random/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6933326
[Epoch 1] ogbg-molsider: 0.489949 val loss: 0.693158
[Epoch 1] ogbg-molsider: 0.498530 test loss: 0.693077
[Epoch 2; Iter    24/   36] train: loss: 0.6934422
[Epoch 2] ogbg-molsider: 0.489366 val loss: 0.693237
[Epoch 2] ogbg-molsider: 0.496726 test loss: 0.693133
[Epoch 3; Iter    18/   36] train: loss: 0.6934002
[Epoch 3] ogbg-molsider: 0.487723 val loss: 0.693301
[Epoch 3] ogbg-molsider: 0.499912 test loss: 0.693210
[Epoch 4; Iter    12/   36] train: loss: 0.6937167
[Epoch 4] ogbg-molsider: 0.489816 val loss: 0.693255
[Epoch 4] ogbg-molsider: 0.499091 test loss: 0.693160
[Epoch 5; Iter     6/   36] train: loss: 0.6933987
[Epoch 5; Iter    36/   36] train: loss: 0.6929815
[Epoch 5] ogbg-molsider: 0.489083 val loss: 0.693220
[Epoch 5] ogbg-molsider: 0.502205 test loss: 0.693113
[Epoch 6; Iter    30/   36] train: loss: 0.6938947
[Epoch 6] ogbg-molsider: 0.490856 val loss: 0.693053
[Epoch 6] ogbg-molsider: 0.501605 test loss: 0.692945
[Epoch 7; Iter    24/   36] train: loss: 0.6929366
[Epoch 7] ogbg-molsider: 0.490398 val loss: 0.693001
[Epoch 7] ogbg-molsider: 0.504819 test loss: 0.692887
[Epoch 8; Iter    18/   36] train: loss: 0.6926597
[Epoch 8] ogbg-molsider: 0.489237 val loss: 0.692911
[Epoch 8] ogbg-molsider: 0.502555 test loss: 0.692838
[Epoch 9; Iter    12/   36] train: loss: 0.6927779
[Epoch 9] ogbg-molsider: 0.490719 val loss: 0.692910
[Epoch 9] ogbg-molsider: 0.500955 test loss: 0.692843
[Epoch 10; Iter     6/   36] train: loss: 0.6927046
[Epoch 10; Iter    36/   36] train: loss: 0.6932264
[Epoch 10] ogbg-molsider: 0.490483 val loss: 0.692671
[Epoch 10] ogbg-molsider: 0.502077 test loss: 0.692620
[Epoch 11; Iter    30/   36] train: loss: 0.6930258
[Epoch 11] ogbg-molsider: 0.489721 val loss: 0.692597
[Epoch 11] ogbg-molsider: 0.504111 test loss: 0.692562
[Epoch 12; Iter    24/   36] train: loss: 0.6926711
[Epoch 12] ogbg-molsider: 0.493638 val loss: 0.692439
[Epoch 12] ogbg-molsider: 0.506952 test loss: 0.692393
[Epoch 13; Iter    18/   36] train: loss: 0.6922120
[Epoch 13] ogbg-molsider: 0.492007 val loss: 0.692384
[Epoch 13] ogbg-molsider: 0.506599 test loss: 0.692314
[Epoch 14; Iter    12/   36] train: loss: 0.6921616
[Epoch 14] ogbg-molsider: 0.489744 val loss: 0.692204
[Epoch 14] ogbg-molsider: 0.504840 test loss: 0.692219
[Epoch 15; Iter     6/   36] train: loss: 0.6925595
[Epoch 15; Iter    36/   36] train: loss: 0.6916064
[Epoch 15] ogbg-molsider: 0.491071 val loss: 0.692068
[Epoch 15] ogbg-molsider: 0.505728 test loss: 0.692038
[Epoch 16; Iter    30/   36] train: loss: 0.6920581
[Epoch 16] ogbg-molsider: 0.493631 val loss: 0.691906
[Epoch 16] ogbg-molsider: 0.501451 test loss: 0.691970
[Epoch 17; Iter    24/   36] train: loss: 0.6922426
[Epoch 17] ogbg-molsider: 0.492640 val loss: 0.691679
[Epoch 17] ogbg-molsider: 0.503530 test loss: 0.691783
[Epoch 18; Iter    18/   36] train: loss: 0.6913917
[Epoch 18] ogbg-molsider: 0.493397 val loss: 0.691470
[Epoch 18] ogbg-molsider: 0.508094 test loss: 0.691476
[Epoch 19; Iter    12/   36] train: loss: 0.6915672
[Epoch 19] ogbg-molsider: 0.490920 val loss: 0.691198
[Epoch 19] ogbg-molsider: 0.507301 test loss: 0.691281
[Epoch 20; Iter     6/   36] train: loss: 0.6911506
[Epoch 20; Iter    36/   36] train: loss: 0.6859680
[Epoch 20] ogbg-molsider: 0.579674 val loss: 0.681666
[Epoch 20] ogbg-molsider: 0.592149 test loss: 0.682931
[Epoch 21; Iter    30/   36] train: loss: 0.6704463
[Epoch 21] ogbg-molsider: 0.602733 val loss: 0.671677
[Epoch 21] ogbg-molsider: 0.587264 test loss: 0.673239
[Epoch 22; Iter    24/   36] train: loss: 0.6630588
[Epoch 22] ogbg-molsider: 0.580062 val loss: 0.635348
[Epoch 22] ogbg-molsider: 0.593519 test loss: 0.642659
[Epoch 23; Iter    18/   36] train: loss: 0.6317730
[Epoch 23] ogbg-molsider: 0.592746 val loss: 0.615802
[Epoch 23] ogbg-molsider: 0.571664 test loss: 0.633957
[Epoch 24; Iter    12/   36] train: loss: 0.6048260
[Epoch 24] ogbg-molsider: 0.588357 val loss: 0.612694
[Epoch 24] ogbg-molsider: 0.588409 test loss: 0.628311
[Epoch 25; Iter     6/   36] train: loss: 0.5894622
[Epoch 25; Iter    36/   36] train: loss: 0.5800932
[Epoch 25] ogbg-molsider: 0.589505 val loss: 0.548851
[Epoch 25] ogbg-molsider: 0.590186 test loss: 0.572691
[Epoch 26; Iter    30/   36] train: loss: 0.5324407
[Epoch 26] ogbg-molsider: 0.594061 val loss: 0.516944
[Epoch 26] ogbg-molsider: 0.572373 test loss: 0.547646
[Epoch 27; Iter    24/   36] train: loss: 0.5490146
[Epoch 27] ogbg-molsider: 0.618766 val loss: 0.506888
[Epoch 27] ogbg-molsider: 0.573385 test loss: 0.533829
[Epoch 28; Iter    18/   36] train: loss: 0.5015985
[Epoch 28] ogbg-molsider: 0.614052 val loss: 0.500633
[Epoch 28] ogbg-molsider: 0.594415 test loss: 0.527590
[Epoch 29; Iter    12/   36] train: loss: 0.5350840
[Epoch 29] ogbg-molsider: 0.614348 val loss: 0.492476
[Epoch 29] ogbg-molsider: 0.566938 test loss: 0.527066
[Epoch 30; Iter     6/   36] train: loss: 0.4826112
[Epoch 30; Iter    36/   36] train: loss: 0.5124735
[Epoch 30] ogbg-molsider: 0.581059 val loss: 0.496253
[Epoch 30] ogbg-molsider: 0.578947 test loss: 0.532507
[Epoch 31; Iter    30/   36] train: loss: 0.5158318
[Epoch 31] ogbg-molsider: 0.637128 val loss: 0.481715
[Epoch 31] ogbg-molsider: 0.577847 test loss: 0.527125
[Epoch 32; Iter    24/   36] train: loss: 0.4908207
[Epoch 32] ogbg-molsider: 0.647503 val loss: 0.479139
[Epoch 32] ogbg-molsider: 0.594862 test loss: 0.516762
[Epoch 33; Iter    18/   36] train: loss: 0.4706150
[Epoch 33] ogbg-molsider: 0.635191 val loss: 0.481365
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.7/PNA_ogbg-molsider_GraphCL_sider_random=0.7_6_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.7
logdir: runs/split/GraphCL/sider/random/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6933089
[Epoch 1] ogbg-molsider: 0.490997 val loss: 0.693217
[Epoch 1] ogbg-molsider: 0.497359 test loss: 0.693146
[Epoch 2; Iter    28/   32] train: loss: 0.6940007
[Epoch 2] ogbg-molsider: 0.493676 val loss: 0.693263
[Epoch 2] ogbg-molsider: 0.497754 test loss: 0.693219
[Epoch 3; Iter    26/   32] train: loss: 0.6926769
[Epoch 3] ogbg-molsider: 0.491773 val loss: 0.693163
[Epoch 3] ogbg-molsider: 0.496297 test loss: 0.693242
[Epoch 4; Iter    24/   32] train: loss: 0.6931484
[Epoch 4] ogbg-molsider: 0.497026 val loss: 0.693242
[Epoch 4] ogbg-molsider: 0.500636 test loss: 0.693246
[Epoch 5; Iter    22/   32] train: loss: 0.6933822
[Epoch 5] ogbg-molsider: 0.492176 val loss: 0.693260
[Epoch 5] ogbg-molsider: 0.501142 test loss: 0.693248
[Epoch 6; Iter    20/   32] train: loss: 0.6930149
[Epoch 6] ogbg-molsider: 0.493002 val loss: 0.693111
[Epoch 6] ogbg-molsider: 0.499730 test loss: 0.693104
[Epoch 7; Iter    18/   32] train: loss: 0.6929116
[Epoch 7] ogbg-molsider: 0.493132 val loss: 0.693005
[Epoch 7] ogbg-molsider: 0.497563 test loss: 0.693076
[Epoch 8; Iter    16/   32] train: loss: 0.6929359
[Epoch 8] ogbg-molsider: 0.492603 val loss: 0.693029
[Epoch 8] ogbg-molsider: 0.500666 test loss: 0.693002
[Epoch 9; Iter    14/   32] train: loss: 0.6926223
[Epoch 9] ogbg-molsider: 0.494340 val loss: 0.693049
[Epoch 9] ogbg-molsider: 0.502451 test loss: 0.693038
[Epoch 10; Iter    12/   32] train: loss: 0.6927270
[Epoch 10] ogbg-molsider: 0.490650 val loss: 0.692850
[Epoch 10] ogbg-molsider: 0.496889 test loss: 0.692890
[Epoch 11; Iter    10/   32] train: loss: 0.6920782
[Epoch 11] ogbg-molsider: 0.501265 val loss: 0.692777
[Epoch 11] ogbg-molsider: 0.502546 test loss: 0.692816
[Epoch 12; Iter     8/   32] train: loss: 0.6929797
[Epoch 12] ogbg-molsider: 0.492905 val loss: 0.692613
[Epoch 12] ogbg-molsider: 0.500285 test loss: 0.692622
[Epoch 13; Iter     6/   32] train: loss: 0.6923187
[Epoch 13] ogbg-molsider: 0.496484 val loss: 0.692511
[Epoch 13] ogbg-molsider: 0.502091 test loss: 0.692486
[Epoch 14; Iter     4/   32] train: loss: 0.6923977
[Epoch 14] ogbg-molsider: 0.494588 val loss: 0.692410
[Epoch 14] ogbg-molsider: 0.501394 test loss: 0.692443
[Epoch 15; Iter     2/   32] train: loss: 0.6921107
[Epoch 15; Iter    32/   32] train: loss: 0.6937235
[Epoch 15] ogbg-molsider: 0.493989 val loss: 0.692322
[Epoch 15] ogbg-molsider: 0.502517 test loss: 0.692323
[Epoch 16; Iter    30/   32] train: loss: 0.6918391
[Epoch 16] ogbg-molsider: 0.497807 val loss: 0.692237
[Epoch 16] ogbg-molsider: 0.504012 test loss: 0.692207
[Epoch 17; Iter    28/   32] train: loss: 0.6921398
[Epoch 17] ogbg-molsider: 0.497472 val loss: 0.692028
[Epoch 17] ogbg-molsider: 0.502508 test loss: 0.692029
[Epoch 18; Iter    26/   32] train: loss: 0.6918374
[Epoch 18] ogbg-molsider: 0.497274 val loss: 0.691781
[Epoch 18] ogbg-molsider: 0.504160 test loss: 0.691760
[Epoch 19; Iter    24/   32] train: loss: 0.6917708
[Epoch 19] ogbg-molsider: 0.496865 val loss: 0.691698
[Epoch 19] ogbg-molsider: 0.503375 test loss: 0.691689
[Epoch 20; Iter    22/   32] train: loss: 0.6908486
[Epoch 20] ogbg-molsider: 0.503208 val loss: 0.691502
[Epoch 20] ogbg-molsider: 0.506382 test loss: 0.691476
[Epoch 21; Iter    20/   32] train: loss: 0.6916376
[Epoch 21] ogbg-molsider: 0.495085 val loss: 0.691291
[Epoch 21] ogbg-molsider: 0.500299 test loss: 0.691342
[Epoch 22; Iter    18/   32] train: loss: 0.6909790
[Epoch 22] ogbg-molsider: 0.555156 val loss: 0.687449
[Epoch 22] ogbg-molsider: 0.562737 test loss: 0.687301
[Epoch 23; Iter    16/   32] train: loss: 0.6854496
[Epoch 23] ogbg-molsider: 0.567673 val loss: 0.678055
[Epoch 23] ogbg-molsider: 0.591813 test loss: 0.678653
[Epoch 24; Iter    14/   32] train: loss: 0.6759583
[Epoch 24] ogbg-molsider: 0.568950 val loss: 0.654141
[Epoch 24] ogbg-molsider: 0.587255 test loss: 0.657794
[Epoch 25; Iter    12/   32] train: loss: 0.6573465
[Epoch 25] ogbg-molsider: 0.582933 val loss: 0.646141
[Epoch 25] ogbg-molsider: 0.590928 test loss: 0.649633
[Epoch 26; Iter    10/   32] train: loss: 0.6421987
[Epoch 26] ogbg-molsider: 0.577477 val loss: 0.617463
[Epoch 26] ogbg-molsider: 0.595083 test loss: 0.622750
[Epoch 27; Iter     8/   32] train: loss: 0.6034306
[Epoch 27] ogbg-molsider: 0.576644 val loss: 0.613100
[Epoch 27] ogbg-molsider: 0.553985 test loss: 0.612391
[Epoch 28; Iter     6/   32] train: loss: 0.5725091
[Epoch 28] ogbg-molsider: 0.587783 val loss: 0.589077
[Epoch 28] ogbg-molsider: 0.548522 test loss: 0.593117
[Epoch 29; Iter     4/   32] train: loss: 0.5674275
[Epoch 29] ogbg-molsider: 0.595337 val loss: 0.541200
[Epoch 29] ogbg-molsider: 0.590184 test loss: 0.546610
[Epoch 30; Iter     2/   32] train: loss: 0.5496803
[Epoch 30; Iter    32/   32] train: loss: 0.4880912
[Epoch 30] ogbg-molsider: 0.599285 val loss: 0.544832
[Epoch 30] ogbg-molsider: 0.563667 test loss: 0.552349
[Epoch 31; Iter    30/   32] train: loss: 0.5268369
[Epoch 31] ogbg-molsider: 0.584421 val loss: 0.514023
[Epoch 31] ogbg-molsider: 0.584938 test loss: 0.520722
[Epoch 32; Iter    28/   32] train: loss: 0.5280970
[Epoch 32] ogbg-molsider: 0.611170 val loss: 0.509986
[Epoch 32] ogbg-molsider: 0.570750 test loss: 0.517568
[Epoch 33; Iter    26/   32] train: loss: 0.4691333
[Epoch 33] ogbg-molsider: 0.612826 val loss: 0.500390
[Epoch 33] ogbg-molsider: 0.594775 test loss: 0.511289
[Epoch 34; Iter    24/   32] train: loss: 0.4861140
[Epoch 34] ogbg-molsider: 0.597692 val loss: 0.498334
[Epoch 34] ogbg-molsider: 0.599596 test loss: 0.508459
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.6/PNA_ogbg-molsider_GraphCL_sider_random=0.6_5_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.6
logdir: runs/split/GraphCL/sider/random/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.487814 val loss: 0.693192
[Epoch 1] ogbg-molsider: 0.504106 test loss: 0.693126
[Epoch 2; Iter     3/   27] train: loss: 0.6933532
[Epoch 2] ogbg-molsider: 0.481400 val loss: 0.693208
[Epoch 2] ogbg-molsider: 0.496796 test loss: 0.693154
[Epoch 3; Iter     6/   27] train: loss: 0.6928376
[Epoch 3] ogbg-molsider: 0.477138 val loss: 0.693195
[Epoch 3] ogbg-molsider: 0.494748 test loss: 0.693176
[Epoch 4; Iter     9/   27] train: loss: 0.6935980
[Epoch 4] ogbg-molsider: 0.481649 val loss: 0.693142
[Epoch 4] ogbg-molsider: 0.494679 test loss: 0.693135
[Epoch 5; Iter    12/   27] train: loss: 0.6932205
[Epoch 5] ogbg-molsider: 0.483476 val loss: 0.693090
[Epoch 5] ogbg-molsider: 0.495513 test loss: 0.693137
[Epoch 6; Iter    15/   27] train: loss: 0.6930293
[Epoch 6] ogbg-molsider: 0.483354 val loss: 0.693075
[Epoch 6] ogbg-molsider: 0.494521 test loss: 0.693146
[Epoch 7; Iter    18/   27] train: loss: 0.6924961
[Epoch 7] ogbg-molsider: 0.483390 val loss: 0.693030
[Epoch 7] ogbg-molsider: 0.495429 test loss: 0.693059
[Epoch 8; Iter    21/   27] train: loss: 0.6926944
[Epoch 8] ogbg-molsider: 0.482580 val loss: 0.692928
[Epoch 8] ogbg-molsider: 0.496013 test loss: 0.692953
[Epoch 9; Iter    24/   27] train: loss: 0.6930497
[Epoch 9] ogbg-molsider: 0.483073 val loss: 0.692925
[Epoch 9] ogbg-molsider: 0.496214 test loss: 0.692957
[Epoch 10; Iter    27/   27] train: loss: 0.6931697
[Epoch 10] ogbg-molsider: 0.486504 val loss: 0.692804
[Epoch 10] ogbg-molsider: 0.497663 test loss: 0.692826
[Epoch 11] ogbg-molsider: 0.482643 val loss: 0.692840
[Epoch 11] ogbg-molsider: 0.495545 test loss: 0.692870
[Epoch 12; Iter     3/   27] train: loss: 0.6928820
[Epoch 12] ogbg-molsider: 0.484191 val loss: 0.692673
[Epoch 12] ogbg-molsider: 0.497139 test loss: 0.692671
[Epoch 13; Iter     6/   27] train: loss: 0.6930649
[Epoch 13] ogbg-molsider: 0.485345 val loss: 0.692597
[Epoch 13] ogbg-molsider: 0.495651 test loss: 0.692648
[Epoch 14; Iter     9/   27] train: loss: 0.6926376
[Epoch 14] ogbg-molsider: 0.485192 val loss: 0.692552
[Epoch 14] ogbg-molsider: 0.496411 test loss: 0.692590
[Epoch 15; Iter    12/   27] train: loss: 0.6923481
[Epoch 15] ogbg-molsider: 0.482518 val loss: 0.692481
[Epoch 15] ogbg-molsider: 0.496081 test loss: 0.692491
[Epoch 16; Iter    15/   27] train: loss: 0.6918677
[Epoch 16] ogbg-molsider: 0.484052 val loss: 0.692327
[Epoch 16] ogbg-molsider: 0.497519 test loss: 0.692327
[Epoch 17; Iter    18/   27] train: loss: 0.6923930
[Epoch 17] ogbg-molsider: 0.484053 val loss: 0.692230
[Epoch 17] ogbg-molsider: 0.495211 test loss: 0.692251
[Epoch 18; Iter    21/   27] train: loss: 0.6921901
[Epoch 18] ogbg-molsider: 0.483181 val loss: 0.692165
[Epoch 18] ogbg-molsider: 0.496046 test loss: 0.692192
[Epoch 19; Iter    24/   27] train: loss: 0.6922355
[Epoch 19] ogbg-molsider: 0.486493 val loss: 0.691977
[Epoch 19] ogbg-molsider: 0.496465 test loss: 0.691963
[Epoch 20; Iter    27/   27] train: loss: 0.6918321
[Epoch 20] ogbg-molsider: 0.485136 val loss: 0.691862
[Epoch 20] ogbg-molsider: 0.496850 test loss: 0.691837
[Epoch 21] ogbg-molsider: 0.485798 val loss: 0.691787
[Epoch 21] ogbg-molsider: 0.496002 test loss: 0.691794
[Epoch 22; Iter     3/   27] train: loss: 0.6917163
[Epoch 22] ogbg-molsider: 0.486013 val loss: 0.691697
[Epoch 22] ogbg-molsider: 0.496918 test loss: 0.691681
[Epoch 23; Iter     6/   27] train: loss: 0.6915448
[Epoch 23] ogbg-molsider: 0.485647 val loss: 0.691504
[Epoch 23] ogbg-molsider: 0.498871 test loss: 0.691442
[Epoch 24; Iter     9/   27] train: loss: 0.6924701
[Epoch 24] ogbg-molsider: 0.484173 val loss: 0.691407
[Epoch 24] ogbg-molsider: 0.496548 test loss: 0.691397
[Epoch 25; Iter    12/   27] train: loss: 0.6909572
[Epoch 25] ogbg-molsider: 0.484292 val loss: 0.691262
[Epoch 25] ogbg-molsider: 0.497191 test loss: 0.691242
[Epoch 26; Iter    15/   27] train: loss: 0.6905430
[Epoch 26] ogbg-molsider: 0.547065 val loss: 0.687322
[Epoch 26] ogbg-molsider: 0.558780 test loss: 0.687163
[Epoch 27; Iter    18/   27] train: loss: 0.6835167
[Epoch 27] ogbg-molsider: 0.569330 val loss: 0.680683
[Epoch 27] ogbg-molsider: 0.579482 test loss: 0.680627
[Epoch 28; Iter    21/   27] train: loss: 0.6752052
[Epoch 28] ogbg-molsider: 0.568145 val loss: 0.672187
[Epoch 28] ogbg-molsider: 0.582759 test loss: 0.672229
[Epoch 29; Iter    24/   27] train: loss: 0.6529466
[Epoch 29] ogbg-molsider: 0.588620 val loss: 0.641259
[Epoch 29] ogbg-molsider: 0.592568 test loss: 0.636647
[Epoch 30; Iter    27/   27] train: loss: 0.6395984
[Epoch 30] ogbg-molsider: 0.582184 val loss: 0.642132
[Epoch 30] ogbg-molsider: 0.586584 test loss: 0.638285
[Epoch 31] ogbg-molsider: 0.598683 val loss: 0.611213
[Epoch 31] ogbg-molsider: 0.582885 test loss: 0.605978
[Epoch 32; Iter     3/   27] train: loss: 0.6084038
[Epoch 32] ogbg-molsider: 0.587936 val loss: 0.592873
[Epoch 32] ogbg-molsider: 0.576604 test loss: 0.582410
[Epoch 33; Iter     6/   27] train: loss: 0.6002982
[Epoch 33] ogbg-molsider: 0.584307 val loss: 0.572565
[Epoch 33] ogbg-molsider: 0.594272 test loss: 0.551734
[Epoch 34; Iter     9/   27] train: loss: 0.5287777
[Epoch 34] ogbg-molsider: 0.587974 val loss: 0.570428
[Epoch 34] ogbg-molsider: 0.598592 test loss: 0.539596
[Epoch 35; Iter    12/   27] train: loss: 0.5190574
[Epoch 35] ogbg-molsider: 0.595119 val loss: 0.591607
[Epoch 35] ogbg-molsider: 0.573207 test loss: 0.547734
[Epoch 36; Iter    15/   27] train: loss: 0.5102920
[Epoch 36] ogbg-molsider: 0.624436 val loss: 0.551737
[Epoch 36] ogbg-molsider: 0.575830 test loss: 0.522582
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.7/PNA_ogbg-molsider_GraphCL_sider_random=0.7_4_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.7
logdir: runs/split/GraphCL/sider/random/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6934084
[Epoch 1] ogbg-molsider: 0.481213 val loss: 0.693205
[Epoch 1] ogbg-molsider: 0.486072 test loss: 0.693299
[Epoch 2; Iter    28/   32] train: loss: 0.6935810
[Epoch 2] ogbg-molsider: 0.481742 val loss: 0.693322
[Epoch 2] ogbg-molsider: 0.485393 test loss: 0.693470
[Epoch 3; Iter    26/   32] train: loss: 0.6935315
[Epoch 3] ogbg-molsider: 0.482161 val loss: 0.693259
[Epoch 3] ogbg-molsider: 0.486360 test loss: 0.693462
[Epoch 4; Iter    24/   32] train: loss: 0.6931578
[Epoch 4] ogbg-molsider: 0.485214 val loss: 0.693255
[Epoch 4] ogbg-molsider: 0.488220 test loss: 0.693507
[Epoch 5; Iter    22/   32] train: loss: 0.6931039
[Epoch 5] ogbg-molsider: 0.485974 val loss: 0.693200
[Epoch 5] ogbg-molsider: 0.488496 test loss: 0.693447
[Epoch 6; Iter    20/   32] train: loss: 0.6931123
[Epoch 6] ogbg-molsider: 0.484428 val loss: 0.693107
[Epoch 6] ogbg-molsider: 0.486494 test loss: 0.693262
[Epoch 7; Iter    18/   32] train: loss: 0.6929088
[Epoch 7] ogbg-molsider: 0.484018 val loss: 0.693070
[Epoch 7] ogbg-molsider: 0.487153 test loss: 0.693250
[Epoch 8; Iter    16/   32] train: loss: 0.6931224
[Epoch 8] ogbg-molsider: 0.483669 val loss: 0.693008
[Epoch 8] ogbg-molsider: 0.487272 test loss: 0.693205
[Epoch 9; Iter    14/   32] train: loss: 0.6933382
[Epoch 9] ogbg-molsider: 0.484041 val loss: 0.692903
[Epoch 9] ogbg-molsider: 0.485058 test loss: 0.693114
[Epoch 10; Iter    12/   32] train: loss: 0.6929008
[Epoch 10] ogbg-molsider: 0.484756 val loss: 0.692858
[Epoch 10] ogbg-molsider: 0.485307 test loss: 0.693084
[Epoch 11; Iter    10/   32] train: loss: 0.6924266
[Epoch 11] ogbg-molsider: 0.483731 val loss: 0.692784
[Epoch 11] ogbg-molsider: 0.488662 test loss: 0.693002
[Epoch 12; Iter     8/   32] train: loss: 0.6925849
[Epoch 12] ogbg-molsider: 0.486591 val loss: 0.692619
[Epoch 12] ogbg-molsider: 0.486652 test loss: 0.692848
[Epoch 13; Iter     6/   32] train: loss: 0.6927609
[Epoch 13] ogbg-molsider: 0.487181 val loss: 0.692434
[Epoch 13] ogbg-molsider: 0.489443 test loss: 0.692687
[Epoch 14; Iter     4/   32] train: loss: 0.6924065
[Epoch 14] ogbg-molsider: 0.485556 val loss: 0.692392
[Epoch 14] ogbg-molsider: 0.488141 test loss: 0.692600
[Epoch 15; Iter     2/   32] train: loss: 0.6926516
[Epoch 15; Iter    32/   32] train: loss: 0.6915051
[Epoch 15] ogbg-molsider: 0.485513 val loss: 0.692210
[Epoch 15] ogbg-molsider: 0.490776 test loss: 0.692397
[Epoch 16; Iter    30/   32] train: loss: 0.6919330
[Epoch 16] ogbg-molsider: 0.486174 val loss: 0.692087
[Epoch 16] ogbg-molsider: 0.488299 test loss: 0.692335
[Epoch 17; Iter    28/   32] train: loss: 0.6917668
[Epoch 17] ogbg-molsider: 0.486549 val loss: 0.691967
[Epoch 17] ogbg-molsider: 0.485907 test loss: 0.692163
[Epoch 18; Iter    26/   32] train: loss: 0.6918506
[Epoch 18] ogbg-molsider: 0.489668 val loss: 0.691763
[Epoch 18] ogbg-molsider: 0.486330 test loss: 0.691960
[Epoch 19; Iter    24/   32] train: loss: 0.6921570
[Epoch 19] ogbg-molsider: 0.488004 val loss: 0.691534
[Epoch 19] ogbg-molsider: 0.486643 test loss: 0.691714
[Epoch 20; Iter    22/   32] train: loss: 0.6909046
[Epoch 20] ogbg-molsider: 0.488840 val loss: 0.691391
[Epoch 20] ogbg-molsider: 0.488589 test loss: 0.691582
[Epoch 21; Iter    20/   32] train: loss: 0.6914265
[Epoch 21] ogbg-molsider: 0.487666 val loss: 0.691200
[Epoch 21] ogbg-molsider: 0.490950 test loss: 0.691342
[Epoch 22; Iter    18/   32] train: loss: 0.6909581
[Epoch 22] ogbg-molsider: 0.535472 val loss: 0.687301
[Epoch 22] ogbg-molsider: 0.535059 test loss: 0.688052
[Epoch 23; Iter    16/   32] train: loss: 0.6844006
[Epoch 23] ogbg-molsider: 0.561398 val loss: 0.674787
[Epoch 23] ogbg-molsider: 0.590647 test loss: 0.675963
[Epoch 24; Iter    14/   32] train: loss: 0.6693581
[Epoch 24] ogbg-molsider: 0.569053 val loss: 0.657655
[Epoch 24] ogbg-molsider: 0.589779 test loss: 0.660939
[Epoch 25; Iter    12/   32] train: loss: 0.6523548
[Epoch 25] ogbg-molsider: 0.566836 val loss: 0.636911
[Epoch 25] ogbg-molsider: 0.584781 test loss: 0.641935
[Epoch 26; Iter    10/   32] train: loss: 0.6384832
[Epoch 26] ogbg-molsider: 0.580553 val loss: 0.624204
[Epoch 26] ogbg-molsider: 0.600715 test loss: 0.627116
[Epoch 27; Iter     8/   32] train: loss: 0.6028106
[Epoch 27] ogbg-molsider: 0.574630 val loss: 0.596329
[Epoch 27] ogbg-molsider: 0.596292 test loss: 0.602382
[Epoch 28; Iter     6/   32] train: loss: 0.5811032
[Epoch 28] ogbg-molsider: 0.594085 val loss: 0.548544
[Epoch 28] ogbg-molsider: 0.589480 test loss: 0.551854
[Epoch 29; Iter     4/   32] train: loss: 0.5660445
[Epoch 29] ogbg-molsider: 0.581214 val loss: 0.543265
[Epoch 29] ogbg-molsider: 0.590692 test loss: 0.543744
[Epoch 30; Iter     2/   32] train: loss: 0.5250892
[Epoch 30; Iter    32/   32] train: loss: 0.5389971
[Epoch 30] ogbg-molsider: 0.584949 val loss: 0.527514
[Epoch 30] ogbg-molsider: 0.573841 test loss: 0.533877
[Epoch 31; Iter    30/   32] train: loss: 0.4906265
[Epoch 31] ogbg-molsider: 0.580799 val loss: 0.514558
[Epoch 31] ogbg-molsider: 0.593340 test loss: 0.519641
[Epoch 32; Iter    28/   32] train: loss: 0.5053428
[Epoch 32] ogbg-molsider: 0.592840 val loss: 0.507875
[Epoch 32] ogbg-molsider: 0.579000 test loss: 0.514093
[Epoch 33; Iter    26/   32] train: loss: 0.5223364
[Epoch 33] ogbg-molsider: 0.593023 val loss: 0.504827
[Epoch 33] ogbg-molsider: 0.597021 test loss: 0.512018
[Epoch 34; Iter    24/   32] train: loss: 0.5169495
[Epoch 34] ogbg-molsider: 0.595633 val loss: 0.505660
[Epoch 34] ogbg-molsider: 0.585243 test loss: 0.511116
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.7/PNA_ogbg-molsider_GraphCL_sider_random=0.7_5_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.7
logdir: runs/split/GraphCL/sider/random/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6929799
[Epoch 1] ogbg-molsider: 0.514006 val loss: 0.693054
[Epoch 1] ogbg-molsider: 0.501278 test loss: 0.693059
[Epoch 2; Iter    28/   32] train: loss: 0.6930915
[Epoch 2] ogbg-molsider: 0.508479 val loss: 0.692992
[Epoch 2] ogbg-molsider: 0.494107 test loss: 0.693166
[Epoch 3; Iter    26/   32] train: loss: 0.6931445
[Epoch 3] ogbg-molsider: 0.506487 val loss: 0.692942
[Epoch 3] ogbg-molsider: 0.495134 test loss: 0.693254
[Epoch 4; Iter    24/   32] train: loss: 0.6934555
[Epoch 4] ogbg-molsider: 0.507863 val loss: 0.692805
[Epoch 4] ogbg-molsider: 0.497116 test loss: 0.693059
[Epoch 5; Iter    22/   32] train: loss: 0.6931221
[Epoch 5] ogbg-molsider: 0.505147 val loss: 0.692796
[Epoch 5] ogbg-molsider: 0.495828 test loss: 0.693097
[Epoch 6; Iter    20/   32] train: loss: 0.6927489
[Epoch 6] ogbg-molsider: 0.506589 val loss: 0.692781
[Epoch 6] ogbg-molsider: 0.496192 test loss: 0.693075
[Epoch 7; Iter    18/   32] train: loss: 0.6925793
[Epoch 7] ogbg-molsider: 0.507270 val loss: 0.692811
[Epoch 7] ogbg-molsider: 0.495636 test loss: 0.693141
[Epoch 8; Iter    16/   32] train: loss: 0.6929368
[Epoch 8] ogbg-molsider: 0.505209 val loss: 0.692703
[Epoch 8] ogbg-molsider: 0.496690 test loss: 0.692991
[Epoch 9; Iter    14/   32] train: loss: 0.6925015
[Epoch 9] ogbg-molsider: 0.508466 val loss: 0.692593
[Epoch 9] ogbg-molsider: 0.494372 test loss: 0.692965
[Epoch 10; Iter    12/   32] train: loss: 0.6927018
[Epoch 10] ogbg-molsider: 0.506696 val loss: 0.692579
[Epoch 10] ogbg-molsider: 0.493562 test loss: 0.692925
[Epoch 11; Iter    10/   32] train: loss: 0.6923087
[Epoch 11] ogbg-molsider: 0.506401 val loss: 0.692449
[Epoch 11] ogbg-molsider: 0.494035 test loss: 0.692785
[Epoch 12; Iter     8/   32] train: loss: 0.6929659
[Epoch 12] ogbg-molsider: 0.505959 val loss: 0.692314
[Epoch 12] ogbg-molsider: 0.497014 test loss: 0.692568
[Epoch 13; Iter     6/   32] train: loss: 0.6926985
[Epoch 13] ogbg-molsider: 0.507440 val loss: 0.692163
[Epoch 13] ogbg-molsider: 0.494939 test loss: 0.692457
[Epoch 14; Iter     4/   32] train: loss: 0.6924179
[Epoch 14] ogbg-molsider: 0.505993 val loss: 0.692153
[Epoch 14] ogbg-molsider: 0.494662 test loss: 0.692439
[Epoch 15; Iter     2/   32] train: loss: 0.6922843
[Epoch 15; Iter    32/   32] train: loss: 0.6921531
[Epoch 15] ogbg-molsider: 0.508917 val loss: 0.691934
[Epoch 15] ogbg-molsider: 0.495505 test loss: 0.692218
[Epoch 16; Iter    30/   32] train: loss: 0.6920383
[Epoch 16] ogbg-molsider: 0.507753 val loss: 0.691788
[Epoch 16] ogbg-molsider: 0.496509 test loss: 0.692075
[Epoch 17; Iter    28/   32] train: loss: 0.6920521
[Epoch 17] ogbg-molsider: 0.507343 val loss: 0.691748
[Epoch 17] ogbg-molsider: 0.495366 test loss: 0.692022
[Epoch 18; Iter    26/   32] train: loss: 0.6917058
[Epoch 18] ogbg-molsider: 0.508824 val loss: 0.691465
[Epoch 18] ogbg-molsider: 0.494078 test loss: 0.691749
[Epoch 19; Iter    24/   32] train: loss: 0.6916916
[Epoch 19] ogbg-molsider: 0.507772 val loss: 0.691361
[Epoch 19] ogbg-molsider: 0.496098 test loss: 0.691619
[Epoch 20; Iter    22/   32] train: loss: 0.6916363
[Epoch 20] ogbg-molsider: 0.507837 val loss: 0.691115
[Epoch 20] ogbg-molsider: 0.497498 test loss: 0.691320
[Epoch 21; Iter    20/   32] train: loss: 0.6909592
[Epoch 21] ogbg-molsider: 0.509167 val loss: 0.690937
[Epoch 21] ogbg-molsider: 0.500063 test loss: 0.691146
[Epoch 22; Iter    18/   32] train: loss: 0.6911001
[Epoch 22] ogbg-molsider: 0.560134 val loss: 0.687429
[Epoch 22] ogbg-molsider: 0.571597 test loss: 0.687579
[Epoch 23; Iter    16/   32] train: loss: 0.6857795
[Epoch 23] ogbg-molsider: 0.568416 val loss: 0.678762
[Epoch 23] ogbg-molsider: 0.593277 test loss: 0.679715
[Epoch 24; Iter    14/   32] train: loss: 0.6766632
[Epoch 24] ogbg-molsider: 0.577489 val loss: 0.657410
[Epoch 24] ogbg-molsider: 0.603402 test loss: 0.656954
[Epoch 25; Iter    12/   32] train: loss: 0.6667249
[Epoch 25] ogbg-molsider: 0.575452 val loss: 0.643840
[Epoch 25] ogbg-molsider: 0.584434 test loss: 0.648920
[Epoch 26; Iter    10/   32] train: loss: 0.6442279
[Epoch 26] ogbg-molsider: 0.571640 val loss: 0.629319
[Epoch 26] ogbg-molsider: 0.580954 test loss: 0.636654
[Epoch 27; Iter     8/   32] train: loss: 0.6223132
[Epoch 27] ogbg-molsider: 0.580667 val loss: 0.596968
[Epoch 27] ogbg-molsider: 0.581965 test loss: 0.601146
[Epoch 28; Iter     6/   32] train: loss: 0.6016189
[Epoch 28] ogbg-molsider: 0.573961 val loss: 0.570109
[Epoch 28] ogbg-molsider: 0.570246 test loss: 0.575045
[Epoch 29; Iter     4/   32] train: loss: 0.5732993
[Epoch 29] ogbg-molsider: 0.586914 val loss: 0.538654
[Epoch 29] ogbg-molsider: 0.596096 test loss: 0.542607
[Epoch 30; Iter     2/   32] train: loss: 0.5561904
[Epoch 30; Iter    32/   32] train: loss: 0.5196326
[Epoch 30] ogbg-molsider: 0.584494 val loss: 0.527931
[Epoch 30] ogbg-molsider: 0.577662 test loss: 0.537131
[Epoch 31; Iter    30/   32] train: loss: 0.5314552
[Epoch 31] ogbg-molsider: 0.586206 val loss: 0.517078
[Epoch 31] ogbg-molsider: 0.591165 test loss: 0.523340
[Epoch 32; Iter    28/   32] train: loss: 0.5152388
[Epoch 32] ogbg-molsider: 0.590729 val loss: 0.510569
[Epoch 32] ogbg-molsider: 0.596506 test loss: 0.514914
[Epoch 33; Iter    26/   32] train: loss: 0.4925070
[Epoch 33] ogbg-molsider: 0.599623 val loss: 0.502771
[Epoch 33] ogbg-molsider: 0.586946 test loss: 0.513133
[Epoch 34; Iter    24/   32] train: loss: 0.4800686
[Epoch 34] ogbg-molsider: 0.598880 val loss: 0.502989
[Epoch 34] ogbg-molsider: 0.598233 test loss: 0.509518
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.6/PNA_ogbg-molsider_GraphCL_sider_random=0.6_4_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.6
logdir: runs/split/GraphCL/sider/random/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.475175 val loss: 0.693163
[Epoch 1] ogbg-molsider: 0.488398 test loss: 0.693219
[Epoch 2; Iter     3/   27] train: loss: 0.6931921
[Epoch 2] ogbg-molsider: 0.475762 val loss: 0.693230
[Epoch 2] ogbg-molsider: 0.487551 test loss: 0.693334
[Epoch 3; Iter     6/   27] train: loss: 0.6935788
[Epoch 3] ogbg-molsider: 0.484191 val loss: 0.693174
[Epoch 3] ogbg-molsider: 0.489131 test loss: 0.693327
[Epoch 4; Iter     9/   27] train: loss: 0.6933898
[Epoch 4] ogbg-molsider: 0.485205 val loss: 0.693149
[Epoch 4] ogbg-molsider: 0.488036 test loss: 0.693315
[Epoch 5; Iter    12/   27] train: loss: 0.6931911
[Epoch 5] ogbg-molsider: 0.486311 val loss: 0.693074
[Epoch 5] ogbg-molsider: 0.490770 test loss: 0.693249
[Epoch 6; Iter    15/   27] train: loss: 0.6929023
[Epoch 6] ogbg-molsider: 0.486718 val loss: 0.693122
[Epoch 6] ogbg-molsider: 0.487746 test loss: 0.693293
[Epoch 7; Iter    18/   27] train: loss: 0.6928962
[Epoch 7] ogbg-molsider: 0.486498 val loss: 0.693102
[Epoch 7] ogbg-molsider: 0.487147 test loss: 0.693267
[Epoch 8; Iter    21/   27] train: loss: 0.6932210
[Epoch 8] ogbg-molsider: 0.489310 val loss: 0.692990
[Epoch 8] ogbg-molsider: 0.491059 test loss: 0.693133
[Epoch 9; Iter    24/   27] train: loss: 0.6934680
[Epoch 9] ogbg-molsider: 0.486532 val loss: 0.692940
[Epoch 9] ogbg-molsider: 0.490309 test loss: 0.693099
[Epoch 10; Iter    27/   27] train: loss: 0.6927310
[Epoch 10] ogbg-molsider: 0.485915 val loss: 0.692879
[Epoch 10] ogbg-molsider: 0.490736 test loss: 0.693059
[Epoch 11] ogbg-molsider: 0.488952 val loss: 0.692793
[Epoch 11] ogbg-molsider: 0.491424 test loss: 0.692988
[Epoch 12; Iter     3/   27] train: loss: 0.6928353
[Epoch 12] ogbg-molsider: 0.485768 val loss: 0.692712
[Epoch 12] ogbg-molsider: 0.492067 test loss: 0.692863
[Epoch 13; Iter     6/   27] train: loss: 0.6924424
[Epoch 13] ogbg-molsider: 0.488835 val loss: 0.692626
[Epoch 13] ogbg-molsider: 0.490460 test loss: 0.692762
[Epoch 14; Iter     9/   27] train: loss: 0.6927264
[Epoch 14] ogbg-molsider: 0.489491 val loss: 0.692543
[Epoch 14] ogbg-molsider: 0.490249 test loss: 0.692664
[Epoch 15; Iter    12/   27] train: loss: 0.6925563
[Epoch 15] ogbg-molsider: 0.486999 val loss: 0.692456
[Epoch 15] ogbg-molsider: 0.490827 test loss: 0.692608
[Epoch 16; Iter    15/   27] train: loss: 0.6925333
[Epoch 16] ogbg-molsider: 0.489630 val loss: 0.692344
[Epoch 16] ogbg-molsider: 0.491466 test loss: 0.692471
[Epoch 17; Iter    18/   27] train: loss: 0.6919764
[Epoch 17] ogbg-molsider: 0.490915 val loss: 0.692238
[Epoch 17] ogbg-molsider: 0.490217 test loss: 0.692368
[Epoch 18; Iter    21/   27] train: loss: 0.6921543
[Epoch 18] ogbg-molsider: 0.490011 val loss: 0.692124
[Epoch 18] ogbg-molsider: 0.491782 test loss: 0.692237
[Epoch 19; Iter    24/   27] train: loss: 0.6919703
[Epoch 19] ogbg-molsider: 0.490998 val loss: 0.692000
[Epoch 19] ogbg-molsider: 0.491281 test loss: 0.692107
[Epoch 20; Iter    27/   27] train: loss: 0.6916519
[Epoch 20] ogbg-molsider: 0.488784 val loss: 0.691838
[Epoch 20] ogbg-molsider: 0.491484 test loss: 0.691948
[Epoch 21] ogbg-molsider: 0.488997 val loss: 0.691724
[Epoch 21] ogbg-molsider: 0.492724 test loss: 0.691840
[Epoch 22; Iter     3/   27] train: loss: 0.6915418
[Epoch 22] ogbg-molsider: 0.490852 val loss: 0.691577
[Epoch 22] ogbg-molsider: 0.493753 test loss: 0.691666
[Epoch 23; Iter     6/   27] train: loss: 0.6918284
[Epoch 23] ogbg-molsider: 0.489777 val loss: 0.691452
[Epoch 23] ogbg-molsider: 0.492029 test loss: 0.691548
[Epoch 24; Iter     9/   27] train: loss: 0.6917049
[Epoch 24] ogbg-molsider: 0.491722 val loss: 0.691287
[Epoch 24] ogbg-molsider: 0.493706 test loss: 0.691390
[Epoch 25; Iter    12/   27] train: loss: 0.6907276
[Epoch 25] ogbg-molsider: 0.493992 val loss: 0.691130
[Epoch 25] ogbg-molsider: 0.494592 test loss: 0.691194
[Epoch 26; Iter    15/   27] train: loss: 0.6901845
[Epoch 26] ogbg-molsider: 0.540775 val loss: 0.686848
[Epoch 26] ogbg-molsider: 0.562676 test loss: 0.686420
[Epoch 27; Iter    18/   27] train: loss: 0.6832596
[Epoch 27] ogbg-molsider: 0.566122 val loss: 0.678756
[Epoch 27] ogbg-molsider: 0.583569 test loss: 0.678604
[Epoch 28; Iter    21/   27] train: loss: 0.6744000
[Epoch 28] ogbg-molsider: 0.578128 val loss: 0.670763
[Epoch 28] ogbg-molsider: 0.588826 test loss: 0.669645
[Epoch 29; Iter    24/   27] train: loss: 0.6550177
[Epoch 29] ogbg-molsider: 0.561853 val loss: 0.664459
[Epoch 29] ogbg-molsider: 0.575574 test loss: 0.664426
[Epoch 30; Iter    27/   27] train: loss: 0.6254586
[Epoch 30] ogbg-molsider: 0.585386 val loss: 0.623596
[Epoch 30] ogbg-molsider: 0.571676 test loss: 0.619220
[Epoch 31] ogbg-molsider: 0.575376 val loss: 0.612427
[Epoch 31] ogbg-molsider: 0.577775 test loss: 0.608981
[Epoch 32; Iter     3/   27] train: loss: 0.6137360
[Epoch 32] ogbg-molsider: 0.566534 val loss: 0.592066
[Epoch 32] ogbg-molsider: 0.561483 test loss: 0.581676
[Epoch 33; Iter     6/   27] train: loss: 0.5698439
[Epoch 33] ogbg-molsider: 0.572769 val loss: 0.578145
[Epoch 33] ogbg-molsider: 0.564595 test loss: 0.568338
[Epoch 34; Iter     9/   27] train: loss: 0.5519854
[Epoch 34] ogbg-molsider: 0.589496 val loss: 0.547978
[Epoch 34] ogbg-molsider: 0.595374 test loss: 0.534736
[Epoch 35; Iter    12/   27] train: loss: 0.5124997
[Epoch 35] ogbg-molsider: 0.591717 val loss: 0.532361
[Epoch 35] ogbg-molsider: 0.588594 test loss: 0.520879
[Epoch 36; Iter    15/   27] train: loss: 0.5181553
[Epoch 36] ogbg-molsider: 0.599235 val loss: 0.528107
[Epoch 36] ogbg-molsider: 0.592417 test loss: 0.515309
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/GraphCL/sider/random/train_prop=0.6/PNA_ogbg-molsider_GraphCL_sider_random=0.6_6_26-05_11-22-10
config: <_io.TextIOWrapper name='configs_split_experiments/GraphCL/sider/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_sider_random=0.6
logdir: runs/split/GraphCL/sider/random/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.496123 val loss: 0.693214
[Epoch 1] ogbg-molsider: 0.493747 test loss: 0.693162
[Epoch 2; Iter     3/   27] train: loss: 0.6931455
[Epoch 2] ogbg-molsider: 0.492123 val loss: 0.693298
[Epoch 2] ogbg-molsider: 0.494864 test loss: 0.693240
[Epoch 3; Iter     6/   27] train: loss: 0.6933346
[Epoch 3] ogbg-molsider: 0.492360 val loss: 0.693328
[Epoch 3] ogbg-molsider: 0.493412 test loss: 0.693314
[Epoch 4; Iter     9/   27] train: loss: 0.6933651
[Epoch 4] ogbg-molsider: 0.496553 val loss: 0.693227
[Epoch 4] ogbg-molsider: 0.498056 test loss: 0.693207
[Epoch 5; Iter    12/   27] train: loss: 0.6932848
[Epoch 5] ogbg-molsider: 0.495991 val loss: 0.693211
[Epoch 5] ogbg-molsider: 0.496118 test loss: 0.693211
[Epoch 6; Iter    15/   27] train: loss: 0.6930576
[Epoch 6] ogbg-molsider: 0.494053 val loss: 0.693170
[Epoch 6] ogbg-molsider: 0.493912 test loss: 0.693175
[Epoch 7; Iter    18/   27] train: loss: 0.6931525
[Epoch 7] ogbg-molsider: 0.497038 val loss: 0.693167
[Epoch 7] ogbg-molsider: 0.495826 test loss: 0.693202
[Epoch 8; Iter    21/   27] train: loss: 0.6927385
[Epoch 8] ogbg-molsider: 0.494157 val loss: 0.693083
[Epoch 8] ogbg-molsider: 0.496838 test loss: 0.693078
[Epoch 9; Iter    24/   27] train: loss: 0.6933026
[Epoch 9] ogbg-molsider: 0.495471 val loss: 0.693048
[Epoch 9] ogbg-molsider: 0.497243 test loss: 0.693043
[Epoch 10; Iter    27/   27] train: loss: 0.6927446
[Epoch 10] ogbg-molsider: 0.496398 val loss: 0.693037
[Epoch 10] ogbg-molsider: 0.495551 test loss: 0.693042
[Epoch 11] ogbg-molsider: 0.497270 val loss: 0.692947
[Epoch 11] ogbg-molsider: 0.497366 test loss: 0.692936
[Epoch 12; Iter     3/   27] train: loss: 0.6936072
[Epoch 12] ogbg-molsider: 0.493764 val loss: 0.692858
[Epoch 12] ogbg-molsider: 0.495481 test loss: 0.692842
[Epoch 13; Iter     6/   27] train: loss: 0.6930894
[Epoch 13] ogbg-molsider: 0.495752 val loss: 0.692822
[Epoch 13] ogbg-molsider: 0.497824 test loss: 0.692783
[Epoch 14; Iter     9/   27] train: loss: 0.6924720
[Epoch 14] ogbg-molsider: 0.495549 val loss: 0.692717
[Epoch 14] ogbg-molsider: 0.496605 test loss: 0.692693
[Epoch 15; Iter    12/   27] train: loss: 0.6923652
[Epoch 15] ogbg-molsider: 0.495453 val loss: 0.692577
[Epoch 15] ogbg-molsider: 0.497353 test loss: 0.692518
[Epoch 16; Iter    15/   27] train: loss: 0.6922709
[Epoch 16] ogbg-molsider: 0.496374 val loss: 0.692592
[Epoch 16] ogbg-molsider: 0.497353 test loss: 0.692558
[Epoch 17; Iter    18/   27] train: loss: 0.6924841
[Epoch 17] ogbg-molsider: 0.495552 val loss: 0.692456
[Epoch 17] ogbg-molsider: 0.498244 test loss: 0.692389
[Epoch 18; Iter    21/   27] train: loss: 0.6925156
[Epoch 18] ogbg-molsider: 0.497043 val loss: 0.692280
[Epoch 18] ogbg-molsider: 0.501112 test loss: 0.692185
[Epoch 19; Iter    24/   27] train: loss: 0.6925659
[Epoch 19] ogbg-molsider: 0.494924 val loss: 0.692242
[Epoch 19] ogbg-molsider: 0.499047 test loss: 0.692189
[Epoch 20; Iter    27/   27] train: loss: 0.6916391
[Epoch 20] ogbg-molsider: 0.496111 val loss: 0.692161
[Epoch 20] ogbg-molsider: 0.500060 test loss: 0.692074
[Epoch 21] ogbg-molsider: 0.493999 val loss: 0.692010
[Epoch 21] ogbg-molsider: 0.497226 test loss: 0.691928
[Epoch 22; Iter     3/   27] train: loss: 0.6916477
[Epoch 22] ogbg-molsider: 0.493119 val loss: 0.691887
[Epoch 22] ogbg-molsider: 0.497888 test loss: 0.691805
[Epoch 23; Iter     6/   27] train: loss: 0.6914577
[Epoch 23] ogbg-molsider: 0.493209 val loss: 0.691756
[Epoch 23] ogbg-molsider: 0.498719 test loss: 0.691655
[Epoch 24; Iter     9/   27] train: loss: 0.6912927
[Epoch 24] ogbg-molsider: 0.494889 val loss: 0.691601
[Epoch 24] ogbg-molsider: 0.498663 test loss: 0.691509
[Epoch 25; Iter    12/   27] train: loss: 0.6918653
[Epoch 25] ogbg-molsider: 0.496343 val loss: 0.691462
[Epoch 25] ogbg-molsider: 0.500516 test loss: 0.691350
[Epoch 26; Iter    15/   27] train: loss: 0.6900140
[Epoch 26] ogbg-molsider: 0.553804 val loss: 0.687230
[Epoch 26] ogbg-molsider: 0.566162 test loss: 0.686470
[Epoch 27; Iter    18/   27] train: loss: 0.6844216
[Epoch 27] ogbg-molsider: 0.561062 val loss: 0.679372
[Epoch 27] ogbg-molsider: 0.593787 test loss: 0.678590
[Epoch 28; Iter    21/   27] train: loss: 0.6746058
[Epoch 28] ogbg-molsider: 0.577957 val loss: 0.665647
[Epoch 28] ogbg-molsider: 0.586688 test loss: 0.664727
[Epoch 29; Iter    24/   27] train: loss: 0.6463977
[Epoch 29] ogbg-molsider: 0.589933 val loss: 0.660287
[Epoch 29] ogbg-molsider: 0.571225 test loss: 0.656778
[Epoch 30; Iter    27/   27] train: loss: 0.6313705
[Epoch 30] ogbg-molsider: 0.581308 val loss: 0.661297
[Epoch 30] ogbg-molsider: 0.573950 test loss: 0.658142
[Epoch 31] ogbg-molsider: 0.571724 val loss: 0.607937
[Epoch 31] ogbg-molsider: 0.579852 test loss: 0.601764
[Epoch 32; Iter     3/   27] train: loss: 0.6083416
[Epoch 32] ogbg-molsider: 0.585128 val loss: 0.618056
[Epoch 32] ogbg-molsider: 0.584522 test loss: 0.612518
[Epoch 33; Iter     6/   27] train: loss: 0.5881411
[Epoch 33] ogbg-molsider: 0.574671 val loss: 0.571456
[Epoch 33] ogbg-molsider: 0.585512 test loss: 0.564816
[Epoch 34; Iter     9/   27] train: loss: 0.5253353
[Epoch 34] ogbg-molsider: 0.589601 val loss: 0.549073
[Epoch 34] ogbg-molsider: 0.580121 test loss: 0.540494
[Epoch 35; Iter    12/   27] train: loss: 0.5276650
[Epoch 35] ogbg-molsider: 0.594494 val loss: 0.550227
[Epoch 35] ogbg-molsider: 0.592351 test loss: 0.538417
[Epoch 36; Iter    15/   27] train: loss: 0.5095620
[Epoch 36] ogbg-molsider: 0.595917 val loss: 0.533333
[Epoch 36] ogbg-molsider: 0.588236 test loss: 0.521605
[Epoch 33] ogbg-molsider: 0.576926 test loss: 0.526273
[Epoch 34; Iter    12/   36] train: loss: 0.4470528
[Epoch 34] ogbg-molsider: 0.634400 val loss: 0.479051
[Epoch 34] ogbg-molsider: 0.582704 test loss: 0.524289
[Epoch 35; Iter     6/   36] train: loss: 0.4754146
[Epoch 35; Iter    36/   36] train: loss: 0.5232723
[Epoch 35] ogbg-molsider: 0.600832 val loss: 0.593668
[Epoch 35] ogbg-molsider: 0.586978 test loss: 0.535315
[Epoch 36; Iter    30/   36] train: loss: 0.4735411
[Epoch 36] ogbg-molsider: 0.632386 val loss: 0.490093
[Epoch 36] ogbg-molsider: 0.603877 test loss: 0.517328
[Epoch 37; Iter    24/   36] train: loss: 0.4484124
[Epoch 37] ogbg-molsider: 0.640646 val loss: 0.484995
[Epoch 37] ogbg-molsider: 0.595554 test loss: 0.515343
[Epoch 38; Iter    18/   36] train: loss: 0.4812901
[Epoch 38] ogbg-molsider: 0.615792 val loss: 0.488229
[Epoch 38] ogbg-molsider: 0.602144 test loss: 0.525263
[Epoch 39; Iter    12/   36] train: loss: 0.4681949
[Epoch 39] ogbg-molsider: 0.609794 val loss: 0.497670
[Epoch 39] ogbg-molsider: 0.597108 test loss: 0.519705
[Epoch 40; Iter     6/   36] train: loss: 0.5368952
[Epoch 40; Iter    36/   36] train: loss: 0.5343586
[Epoch 40] ogbg-molsider: 0.635233 val loss: 0.490850
[Epoch 40] ogbg-molsider: 0.591615 test loss: 0.524785
[Epoch 41; Iter    30/   36] train: loss: 0.4611015
[Epoch 41] ogbg-molsider: 0.533279 val loss: 0.522774
[Epoch 41] ogbg-molsider: 0.583156 test loss: 0.563545
[Epoch 42; Iter    24/   36] train: loss: 0.4220500
[Epoch 42] ogbg-molsider: 0.620859 val loss: 0.668584
[Epoch 42] ogbg-molsider: 0.592675 test loss: 0.719992
[Epoch 43; Iter    18/   36] train: loss: 0.4720726
[Epoch 43] ogbg-molsider: 0.609149 val loss: 0.486534
[Epoch 43] ogbg-molsider: 0.597162 test loss: 0.515909
[Epoch 44; Iter    12/   36] train: loss: 0.4828502
[Epoch 44] ogbg-molsider: 0.608399 val loss: 0.506182
[Epoch 44] ogbg-molsider: 0.582434 test loss: 0.523775
[Epoch 45; Iter     6/   36] train: loss: 0.4398554
[Epoch 45; Iter    36/   36] train: loss: 0.5217967
[Epoch 45] ogbg-molsider: 0.642712 val loss: 0.478901
[Epoch 45] ogbg-molsider: 0.611142 test loss: 0.545659
[Epoch 46; Iter    30/   36] train: loss: 0.4972293
[Epoch 46] ogbg-molsider: 0.640310 val loss: 0.508344
[Epoch 46] ogbg-molsider: 0.608799 test loss: 0.523252
[Epoch 47; Iter    24/   36] train: loss: 0.4567450
[Epoch 47] ogbg-molsider: 0.620463 val loss: 0.489868
[Epoch 47] ogbg-molsider: 0.594138 test loss: 0.541390
[Epoch 48; Iter    18/   36] train: loss: 0.4513167
[Epoch 48] ogbg-molsider: 0.644125 val loss: 0.481453
[Epoch 48] ogbg-molsider: 0.584309 test loss: 0.534476
[Epoch 49; Iter    12/   36] train: loss: 0.4891481
[Epoch 49] ogbg-molsider: 0.622831 val loss: 0.498656
[Epoch 49] ogbg-molsider: 0.590749 test loss: 0.533864
[Epoch 50; Iter     6/   36] train: loss: 0.4605409
[Epoch 50; Iter    36/   36] train: loss: 0.4612574
[Epoch 50] ogbg-molsider: 0.664796 val loss: 0.477486
[Epoch 50] ogbg-molsider: 0.610325 test loss: 0.515219
[Epoch 51; Iter    30/   36] train: loss: 0.5120205
[Epoch 51] ogbg-molsider: 0.617881 val loss: 0.493401
[Epoch 51] ogbg-molsider: 0.624489 test loss: 0.546172
[Epoch 52; Iter    24/   36] train: loss: 0.5301130
[Epoch 52] ogbg-molsider: 0.636304 val loss: 0.484901
[Epoch 52] ogbg-molsider: 0.621687 test loss: 0.509912
[Epoch 53; Iter    18/   36] train: loss: 0.4365501
[Epoch 53] ogbg-molsider: 0.655397 val loss: 0.471122
[Epoch 53] ogbg-molsider: 0.628529 test loss: 0.516032
[Epoch 54; Iter    12/   36] train: loss: 0.4780386
[Epoch 54] ogbg-molsider: 0.668368 val loss: 0.467347
[Epoch 54] ogbg-molsider: 0.625602 test loss: 0.512420
[Epoch 55; Iter     6/   36] train: loss: 0.4380982
[Epoch 55; Iter    36/   36] train: loss: 0.4796672
[Epoch 55] ogbg-molsider: 0.648769 val loss: 0.474685
[Epoch 55] ogbg-molsider: 0.620462 test loss: 0.519343
[Epoch 56; Iter    30/   36] train: loss: 0.4482772
[Epoch 56] ogbg-molsider: 0.670586 val loss: 0.536776
[Epoch 56] ogbg-molsider: 0.616332 test loss: 0.552765
[Epoch 57; Iter    24/   36] train: loss: 0.4499965
[Epoch 57] ogbg-molsider: 0.668960 val loss: 0.469358
[Epoch 57] ogbg-molsider: 0.634905 test loss: 0.517994
[Epoch 58; Iter    18/   36] train: loss: 0.4333464
[Epoch 58] ogbg-molsider: 0.655625 val loss: 0.473321
[Epoch 58] ogbg-molsider: 0.607750 test loss: 0.530659
[Epoch 59; Iter    12/   36] train: loss: 0.4333665
[Epoch 59] ogbg-molsider: 0.652286 val loss: 0.504467
[Epoch 59] ogbg-molsider: 0.581713 test loss: 0.536561
[Epoch 60; Iter     6/   36] train: loss: 0.4773935
[Epoch 60; Iter    36/   36] train: loss: 0.3889296
[Epoch 60] ogbg-molsider: 0.674443 val loss: 0.467331
[Epoch 60] ogbg-molsider: 0.654417 test loss: 0.510894
[Epoch 61; Iter    30/   36] train: loss: 0.4305912
[Epoch 61] ogbg-molsider: 0.654053 val loss: 0.473307
[Epoch 61] ogbg-molsider: 0.636544 test loss: 0.508497
[Epoch 62; Iter    24/   36] train: loss: 0.4030325
[Epoch 62] ogbg-molsider: 0.679233 val loss: 0.468180
[Epoch 62] ogbg-molsider: 0.647628 test loss: 0.499846
[Epoch 63; Iter    18/   36] train: loss: 0.4134899
[Epoch 63] ogbg-molsider: 0.645556 val loss: 0.479193
[Epoch 63] ogbg-molsider: 0.612448 test loss: 0.541018
[Epoch 64; Iter    12/   36] train: loss: 0.3876172
[Epoch 64] ogbg-molsider: 0.681249 val loss: 0.478780
[Epoch 64] ogbg-molsider: 0.646211 test loss: 0.507560
[Epoch 65; Iter     6/   36] train: loss: 0.4091571
[Epoch 65; Iter    36/   36] train: loss: 0.4519058
[Epoch 65] ogbg-molsider: 0.678954 val loss: 0.479868
[Epoch 65] ogbg-molsider: 0.639137 test loss: 0.524972
[Epoch 66; Iter    30/   36] train: loss: 0.4022638
[Epoch 66] ogbg-molsider: 0.655715 val loss: 0.488029
[Epoch 66] ogbg-molsider: 0.647887 test loss: 0.508556
[Epoch 67; Iter    24/   36] train: loss: 0.4550748
[Epoch 67] ogbg-molsider: 0.662719 val loss: 0.491671
[Epoch 67] ogbg-molsider: 0.611850 test loss: 0.527389
[Epoch 68; Iter    18/   36] train: loss: 0.3423403
[Epoch 68] ogbg-molsider: 0.672725 val loss: 0.489410
[Epoch 68] ogbg-molsider: 0.644390 test loss: 0.558919
[Epoch 69; Iter    12/   36] train: loss: 0.3868543
[Epoch 69] ogbg-molsider: 0.653396 val loss: 0.498943
[Epoch 69] ogbg-molsider: 0.641939 test loss: 0.556356
[Epoch 70; Iter     6/   36] train: loss: 0.3654060
[Epoch 70; Iter    36/   36] train: loss: 0.4557759
[Epoch 70] ogbg-molsider: 0.670951 val loss: 0.475433
[Epoch 70] ogbg-molsider: 0.638844 test loss: 0.519083
[Epoch 71; Iter    30/   36] train: loss: 0.4205652
[Epoch 71] ogbg-molsider: 0.665792 val loss: 0.523674
[Epoch 71] ogbg-molsider: 0.646330 test loss: 0.541169
[Epoch 72; Iter    24/   36] train: loss: 0.3836664
[Epoch 72] ogbg-molsider: 0.682628 val loss: 0.479735
[Epoch 72] ogbg-molsider: 0.625812 test loss: 0.531599
[Epoch 73; Iter    18/   36] train: loss: 0.3700645
[Epoch 73] ogbg-molsider: 0.654594 val loss: 0.497094
[Epoch 73] ogbg-molsider: 0.637798 test loss: 0.559051
[Epoch 74; Iter    12/   36] train: loss: 0.4008662
[Epoch 74] ogbg-molsider: 0.650821 val loss: 0.514781
[Epoch 74] ogbg-molsider: 0.627783 test loss: 0.542458
[Epoch 75; Iter     6/   36] train: loss: 0.3474576
[Epoch 75; Iter    36/   36] train: loss: 0.4452456
[Epoch 75] ogbg-molsider: 0.651370 val loss: 0.514276
[Epoch 75] ogbg-molsider: 0.632332 test loss: 0.553492
[Epoch 76; Iter    30/   36] train: loss: 0.3844762
[Epoch 76] ogbg-molsider: 0.659667 val loss: 0.499197
[Epoch 76] ogbg-molsider: 0.647817 test loss: 0.522323
[Epoch 77; Iter    24/   36] train: loss: 0.4070087
[Epoch 77] ogbg-molsider: 0.663782 val loss: 0.507684
[Epoch 77] ogbg-molsider: 0.657219 test loss: 0.548520
[Epoch 78; Iter    18/   36] train: loss: 0.3382365
[Epoch 78] ogbg-molsider: 0.679775 val loss: 0.479645
[Epoch 78] ogbg-molsider: 0.652500 test loss: 0.528837
[Epoch 79; Iter    12/   36] train: loss: 0.2965571
[Epoch 79] ogbg-molsider: 0.668655 val loss: 0.499585
[Epoch 79] ogbg-molsider: 0.654556 test loss: 0.547874
[Epoch 80; Iter     6/   36] train: loss: 0.3381984
[Epoch 80; Iter    36/   36] train: loss: 0.3419658
[Epoch 80] ogbg-molsider: 0.671915 val loss: 0.508268
[Epoch 80] ogbg-molsider: 0.663201 test loss: 0.532396
[Epoch 37; Iter    18/   27] train: loss: 0.5112705
[Epoch 37] ogbg-molsider: 0.602879 val loss: 0.571891
[Epoch 37] ogbg-molsider: 0.594682 test loss: 0.520521
[Epoch 38; Iter    21/   27] train: loss: 0.4891385
[Epoch 38] ogbg-molsider: 0.595317 val loss: 0.568966
[Epoch 38] ogbg-molsider: 0.603452 test loss: 0.510475
[Epoch 39; Iter    24/   27] train: loss: 0.5035430
[Epoch 39] ogbg-molsider: 0.598778 val loss: 0.544645
[Epoch 39] ogbg-molsider: 0.602023 test loss: 0.505899
[Epoch 40; Iter    27/   27] train: loss: 0.5113089
[Epoch 40] ogbg-molsider: 0.592627 val loss: 0.583150
[Epoch 40] ogbg-molsider: 0.592617 test loss: 0.505657
[Epoch 41] ogbg-molsider: 0.601318 val loss: 0.556944
[Epoch 41] ogbg-molsider: 0.574111 test loss: 0.511524
[Epoch 42; Iter     3/   27] train: loss: 0.5000601
[Epoch 42] ogbg-molsider: 0.598483 val loss: 0.528651
[Epoch 42] ogbg-molsider: 0.582984 test loss: 0.511350
[Epoch 43; Iter     6/   27] train: loss: 0.4652231
[Epoch 43] ogbg-molsider: 0.610938 val loss: 0.519164
[Epoch 43] ogbg-molsider: 0.600827 test loss: 0.504880
[Epoch 44; Iter     9/   27] train: loss: 0.4187969
[Epoch 44] ogbg-molsider: 0.625325 val loss: 0.522362
[Epoch 44] ogbg-molsider: 0.605795 test loss: 0.503030
[Epoch 45; Iter    12/   27] train: loss: 0.5258636
[Epoch 45] ogbg-molsider: 0.619630 val loss: 0.528458
[Epoch 45] ogbg-molsider: 0.600666 test loss: 0.508749
[Epoch 46; Iter    15/   27] train: loss: 0.5252457
[Epoch 46] ogbg-molsider: 0.627745 val loss: 0.521768
[Epoch 46] ogbg-molsider: 0.606791 test loss: 0.504957
[Epoch 47; Iter    18/   27] train: loss: 0.4889952
[Epoch 47] ogbg-molsider: 0.606859 val loss: 0.525925
[Epoch 47] ogbg-molsider: 0.595717 test loss: 0.504529
[Epoch 48; Iter    21/   27] train: loss: 0.4818016
[Epoch 48] ogbg-molsider: 0.614870 val loss: 0.526271
[Epoch 48] ogbg-molsider: 0.597761 test loss: 0.502831
[Epoch 49; Iter    24/   27] train: loss: 0.5079802
[Epoch 49] ogbg-molsider: 0.594551 val loss: 0.547362
[Epoch 49] ogbg-molsider: 0.608582 test loss: 0.520701
[Epoch 50; Iter    27/   27] train: loss: 0.4789184
[Epoch 50] ogbg-molsider: 0.614233 val loss: 0.520937
[Epoch 50] ogbg-molsider: 0.604053 test loss: 0.503486
[Epoch 51] ogbg-molsider: 0.622069 val loss: 0.518866
[Epoch 51] ogbg-molsider: 0.594899 test loss: 0.505623
[Epoch 52; Iter     3/   27] train: loss: 0.4537181
[Epoch 52] ogbg-molsider: 0.616828 val loss: 0.514770
[Epoch 52] ogbg-molsider: 0.601918 test loss: 0.505909
[Epoch 53; Iter     6/   27] train: loss: 0.4905663
[Epoch 53] ogbg-molsider: 0.617357 val loss: 0.517850
[Epoch 53] ogbg-molsider: 0.609361 test loss: 0.503312
[Epoch 54; Iter     9/   27] train: loss: 0.4758078
[Epoch 54] ogbg-molsider: 0.606375 val loss: 0.529014
[Epoch 54] ogbg-molsider: 0.610171 test loss: 0.520869
[Epoch 55; Iter    12/   27] train: loss: 0.5103580
[Epoch 55] ogbg-molsider: 0.622922 val loss: 0.538677
[Epoch 55] ogbg-molsider: 0.631817 test loss: 0.508120
[Epoch 56; Iter    15/   27] train: loss: 0.4729820
[Epoch 56] ogbg-molsider: 0.634936 val loss: 0.533964
[Epoch 56] ogbg-molsider: 0.638206 test loss: 0.498611
[Epoch 57; Iter    18/   27] train: loss: 0.4750669
[Epoch 57] ogbg-molsider: 0.541435 val loss: 0.564398
[Epoch 57] ogbg-molsider: 0.517952 test loss: 0.590477
[Epoch 58; Iter    21/   27] train: loss: 0.5423005
[Epoch 58] ogbg-molsider: 0.613157 val loss: 0.531016
[Epoch 58] ogbg-molsider: 0.605070 test loss: 0.546167
[Epoch 59; Iter    24/   27] train: loss: 0.4574407
[Epoch 59] ogbg-molsider: 0.592643 val loss: 0.619403
[Epoch 59] ogbg-molsider: 0.596099 test loss: 0.646146
[Epoch 60; Iter    27/   27] train: loss: 0.5009236
[Epoch 60] ogbg-molsider: 0.608782 val loss: 0.520112
[Epoch 60] ogbg-molsider: 0.593989 test loss: 0.515294
[Epoch 61] ogbg-molsider: 0.612511 val loss: 0.519622
[Epoch 61] ogbg-molsider: 0.622591 test loss: 0.502694
[Epoch 62; Iter     3/   27] train: loss: 0.4957061
[Epoch 62] ogbg-molsider: 0.556466 val loss: 1.483614
[Epoch 62] ogbg-molsider: 0.566284 test loss: 3.476927
[Epoch 63; Iter     6/   27] train: loss: 0.4585031
[Epoch 63] ogbg-molsider: 0.608422 val loss: 0.560166
[Epoch 63] ogbg-molsider: 0.580103 test loss: 0.609429
[Epoch 64; Iter     9/   27] train: loss: 0.5312112
[Epoch 64] ogbg-molsider: 0.604281 val loss: 0.530258
[Epoch 64] ogbg-molsider: 0.607873 test loss: 0.511095
[Epoch 65; Iter    12/   27] train: loss: 0.4626526
[Epoch 65] ogbg-molsider: 0.622967 val loss: 0.540873
[Epoch 65] ogbg-molsider: 0.619772 test loss: 0.499968
[Epoch 66; Iter    15/   27] train: loss: 0.4419239
[Epoch 66] ogbg-molsider: 0.607562 val loss: 0.539529
[Epoch 66] ogbg-molsider: 0.605243 test loss: 0.506806
[Epoch 67; Iter    18/   27] train: loss: 0.4911688
[Epoch 67] ogbg-molsider: 0.621573 val loss: 0.541501
[Epoch 67] ogbg-molsider: 0.595494 test loss: 0.596617
[Epoch 68; Iter    21/   27] train: loss: 0.4732651
[Epoch 68] ogbg-molsider: 0.622879 val loss: 0.514557
[Epoch 68] ogbg-molsider: 0.617985 test loss: 0.499444
[Epoch 69; Iter    24/   27] train: loss: 0.4690182
[Epoch 69] ogbg-molsider: 0.592326 val loss: 1.598661
[Epoch 69] ogbg-molsider: 0.614682 test loss: 2.014802
[Epoch 70; Iter    27/   27] train: loss: 0.4734436
[Epoch 70] ogbg-molsider: 0.591892 val loss: 0.521797
[Epoch 70] ogbg-molsider: 0.611507 test loss: 0.515067
[Epoch 71] ogbg-molsider: 0.589184 val loss: 0.525492
[Epoch 71] ogbg-molsider: 0.608606 test loss: 0.511456
[Epoch 72; Iter     3/   27] train: loss: 0.4328862
[Epoch 72] ogbg-molsider: 0.604762 val loss: 0.528979
[Epoch 72] ogbg-molsider: 0.631973 test loss: 0.501935
[Epoch 73; Iter     6/   27] train: loss: 0.4614383
[Epoch 73] ogbg-molsider: 0.614186 val loss: 0.516662
[Epoch 73] ogbg-molsider: 0.628948 test loss: 0.499581
[Epoch 74; Iter     9/   27] train: loss: 0.4881072
[Epoch 74] ogbg-molsider: 0.624338 val loss: 0.510970
[Epoch 74] ogbg-molsider: 0.627633 test loss: 0.499469
[Epoch 75; Iter    12/   27] train: loss: 0.4412241
[Epoch 75] ogbg-molsider: 0.614047 val loss: 0.523687
[Epoch 75] ogbg-molsider: 0.631718 test loss: 0.499455
[Epoch 76; Iter    15/   27] train: loss: 0.4316474
[Epoch 76] ogbg-molsider: 0.616200 val loss: 0.767467
[Epoch 76] ogbg-molsider: 0.625565 test loss: 0.503068
[Epoch 77; Iter    18/   27] train: loss: 0.4705293
[Epoch 77] ogbg-molsider: 0.626150 val loss: 0.837913
[Epoch 77] ogbg-molsider: 0.630198 test loss: 0.512569
[Epoch 78; Iter    21/   27] train: loss: 0.4290520
[Epoch 78] ogbg-molsider: 0.619690 val loss: 0.519319
[Epoch 78] ogbg-molsider: 0.623407 test loss: 0.510281
[Epoch 79; Iter    24/   27] train: loss: 0.4849154
[Epoch 79] ogbg-molsider: 0.602539 val loss: 0.525941
[Epoch 79] ogbg-molsider: 0.633611 test loss: 0.509438
[Epoch 80; Iter    27/   27] train: loss: 0.4102649
[Epoch 80] ogbg-molsider: 0.602383 val loss: 0.567819
[Epoch 80] ogbg-molsider: 0.631813 test loss: 0.524728
[Epoch 81] ogbg-molsider: 0.603708 val loss: 0.532756
[Epoch 81] ogbg-molsider: 0.630622 test loss: 0.521654
[Epoch 82; Iter     3/   27] train: loss: 0.4784420
[Epoch 82] ogbg-molsider: 0.612895 val loss: 0.552803
[Epoch 82] ogbg-molsider: 0.633922 test loss: 0.509175
[Epoch 83; Iter     6/   27] train: loss: 0.4353804
[Epoch 83] ogbg-molsider: 0.589449 val loss: 0.694913
[Epoch 83] ogbg-molsider: 0.640132 test loss: 0.535309
[Epoch 84; Iter     9/   27] train: loss: 0.4505380
[Epoch 84] ogbg-molsider: 0.610137 val loss: 0.627724
[Epoch 84] ogbg-molsider: 0.617816 test loss: 0.525695
[Epoch 85; Iter    12/   27] train: loss: 0.4330264
[Epoch 85] ogbg-molsider: 0.599835 val loss: 0.533312
[Epoch 85] ogbg-molsider: 0.633405 test loss: 0.552109
[Epoch 86; Iter    15/   27] train: loss: 0.4670467
[Epoch 86] ogbg-molsider: 0.602596 val loss: 0.562290
[Epoch 86] ogbg-molsider: 0.614688 test loss: 0.519671
[Epoch 87; Iter    18/   27] train: loss: 0.4541169
[Epoch 87] ogbg-molsider: 0.605897 val loss: 0.551324
[Epoch 87] ogbg-molsider: 0.611094 test loss: 0.527338
[Epoch 88; Iter    21/   27] train: loss: 0.4422591
[Epoch 88] ogbg-molsider: 0.609220 val loss: 0.542435
[Epoch 88] ogbg-molsider: 0.634380 test loss: 0.512634
[Epoch 89; Iter    24/   27] train: loss: 0.4509109
[Epoch 33] ogbg-molsider: 0.595280 test loss: 0.521157
[Epoch 34; Iter    12/   36] train: loss: 0.4930539
[Epoch 34] ogbg-molsider: 0.647655 val loss: 0.479673
[Epoch 34] ogbg-molsider: 0.597110 test loss: 0.518073
[Epoch 35; Iter     6/   36] train: loss: 0.4897633
[Epoch 35; Iter    36/   36] train: loss: 0.4527815
[Epoch 35] ogbg-molsider: 0.614691 val loss: 0.485351
[Epoch 35] ogbg-molsider: 0.601079 test loss: 0.526122
[Epoch 36; Iter    30/   36] train: loss: 0.5020882
[Epoch 36] ogbg-molsider: 0.650922 val loss: 0.480692
[Epoch 36] ogbg-molsider: 0.586158 test loss: 0.532136
[Epoch 37; Iter    24/   36] train: loss: 0.4744132
[Epoch 37] ogbg-molsider: 0.651942 val loss: 0.483985
[Epoch 37] ogbg-molsider: 0.596999 test loss: 0.530318
[Epoch 38; Iter    18/   36] train: loss: 0.4776566
[Epoch 38] ogbg-molsider: 0.647724 val loss: 0.484140
[Epoch 38] ogbg-molsider: 0.604879 test loss: 0.523835
[Epoch 39; Iter    12/   36] train: loss: 0.4856293
[Epoch 39] ogbg-molsider: 0.661702 val loss: 0.478476
[Epoch 39] ogbg-molsider: 0.616386 test loss: 0.512984
[Epoch 40; Iter     6/   36] train: loss: 0.4487106
[Epoch 40; Iter    36/   36] train: loss: 0.4780790
[Epoch 40] ogbg-molsider: 0.662477 val loss: 0.475390
[Epoch 40] ogbg-molsider: 0.608393 test loss: 0.519294
[Epoch 41; Iter    30/   36] train: loss: 0.4671154
[Epoch 41] ogbg-molsider: 0.658770 val loss: 0.478059
[Epoch 41] ogbg-molsider: 0.584742 test loss: 0.520473
[Epoch 42; Iter    24/   36] train: loss: 0.4757756
[Epoch 42] ogbg-molsider: 0.636659 val loss: 0.492816
[Epoch 42] ogbg-molsider: 0.606284 test loss: 0.517782
[Epoch 43; Iter    18/   36] train: loss: 0.5101489
[Epoch 43] ogbg-molsider: 0.636433 val loss: 0.481881
[Epoch 43] ogbg-molsider: 0.610426 test loss: 0.513821
[Epoch 44; Iter    12/   36] train: loss: 0.4767149
[Epoch 44] ogbg-molsider: 0.629197 val loss: 0.488264
[Epoch 44] ogbg-molsider: 0.611231 test loss: 0.519754
[Epoch 45; Iter     6/   36] train: loss: 0.4776583
[Epoch 45; Iter    36/   36] train: loss: 0.5221762
[Epoch 45] ogbg-molsider: 0.654448 val loss: 0.479836
[Epoch 45] ogbg-molsider: 0.602160 test loss: 0.529053
[Epoch 46; Iter    30/   36] train: loss: 0.5347946
[Epoch 46] ogbg-molsider: 0.612095 val loss: 0.503130
[Epoch 46] ogbg-molsider: 0.602987 test loss: 0.530548
[Epoch 47; Iter    24/   36] train: loss: 0.4719938
[Epoch 47] ogbg-molsider: 0.629916 val loss: 0.494619
[Epoch 47] ogbg-molsider: 0.614861 test loss: 0.507210
[Epoch 48; Iter    18/   36] train: loss: 0.4740122
[Epoch 48] ogbg-molsider: 0.611846 val loss: 0.502749
[Epoch 48] ogbg-molsider: 0.582445 test loss: 0.556930
[Epoch 49; Iter    12/   36] train: loss: 0.4692687
[Epoch 49] ogbg-molsider: 0.626753 val loss: 0.498756
[Epoch 49] ogbg-molsider: 0.584730 test loss: 0.608178
[Epoch 50; Iter     6/   36] train: loss: 0.4744543
[Epoch 50; Iter    36/   36] train: loss: 0.5493477
[Epoch 50] ogbg-molsider: 0.637360 val loss: 0.485127
[Epoch 50] ogbg-molsider: 0.585725 test loss: 0.525124
[Epoch 51; Iter    30/   36] train: loss: 0.4447272
[Epoch 51] ogbg-molsider: 0.629032 val loss: 0.498868
[Epoch 51] ogbg-molsider: 0.584683 test loss: 0.522968
[Epoch 52; Iter    24/   36] train: loss: 0.4232650
[Epoch 52] ogbg-molsider: 0.650839 val loss: 0.479137
[Epoch 52] ogbg-molsider: 0.623954 test loss: 0.512493
[Epoch 53; Iter    18/   36] train: loss: 0.4754573
[Epoch 53] ogbg-molsider: 0.670923 val loss: 0.479127
[Epoch 53] ogbg-molsider: 0.620117 test loss: 0.524390
[Epoch 54; Iter    12/   36] train: loss: 0.4491314
[Epoch 54] ogbg-molsider: 0.647162 val loss: 0.500382
[Epoch 54] ogbg-molsider: 0.631063 test loss: 0.516321
[Epoch 55; Iter     6/   36] train: loss: 0.4454098
[Epoch 55; Iter    36/   36] train: loss: 0.4903237
[Epoch 55] ogbg-molsider: 0.675903 val loss: 0.473300
[Epoch 55] ogbg-molsider: 0.598115 test loss: 0.542917
[Epoch 56; Iter    30/   36] train: loss: 0.4989573
[Epoch 56] ogbg-molsider: 0.612751 val loss: 0.500909
[Epoch 56] ogbg-molsider: 0.609975 test loss: 0.520104
[Epoch 57; Iter    24/   36] train: loss: 0.4176343
[Epoch 57] ogbg-molsider: 0.643556 val loss: 0.479631
[Epoch 57] ogbg-molsider: 0.630294 test loss: 0.526357
[Epoch 58; Iter    18/   36] train: loss: 0.4735548
[Epoch 58] ogbg-molsider: 0.630954 val loss: 0.487424
[Epoch 58] ogbg-molsider: 0.630424 test loss: 0.517427
[Epoch 59; Iter    12/   36] train: loss: 0.4411393
[Epoch 59] ogbg-molsider: 0.672188 val loss: 0.475924
[Epoch 59] ogbg-molsider: 0.628960 test loss: 0.531258
[Epoch 60; Iter     6/   36] train: loss: 0.4309793
[Epoch 60; Iter    36/   36] train: loss: 0.4298555
[Epoch 60] ogbg-molsider: 0.635390 val loss: 0.481523
[Epoch 60] ogbg-molsider: 0.629158 test loss: 0.522005
[Epoch 61; Iter    30/   36] train: loss: 0.4516573
[Epoch 61] ogbg-molsider: 0.610164 val loss: 0.544553
[Epoch 61] ogbg-molsider: 0.640884 test loss: 0.533040
[Epoch 62; Iter    24/   36] train: loss: 0.3828964
[Epoch 62] ogbg-molsider: 0.645200 val loss: 0.488441
[Epoch 62] ogbg-molsider: 0.604523 test loss: 0.519972
[Epoch 63; Iter    18/   36] train: loss: 0.4359419
[Epoch 63] ogbg-molsider: 0.648346 val loss: 0.513748
[Epoch 63] ogbg-molsider: 0.645964 test loss: 0.561026
[Epoch 64; Iter    12/   36] train: loss: 0.4230046
[Epoch 64] ogbg-molsider: 0.660297 val loss: 0.491782
[Epoch 64] ogbg-molsider: 0.638688 test loss: 0.525339
[Epoch 65; Iter     6/   36] train: loss: 0.3997353
[Epoch 65; Iter    36/   36] train: loss: 0.4336965
[Epoch 65] ogbg-molsider: 0.656016 val loss: 0.479960
[Epoch 65] ogbg-molsider: 0.645592 test loss: 0.505820
[Epoch 66; Iter    30/   36] train: loss: 0.4075330
[Epoch 66] ogbg-molsider: 0.634051 val loss: 0.512487
[Epoch 66] ogbg-molsider: 0.651434 test loss: 0.512997
[Epoch 67; Iter    24/   36] train: loss: 0.3751611
[Epoch 67] ogbg-molsider: 0.637568 val loss: 0.501498
[Epoch 67] ogbg-molsider: 0.665688 test loss: 0.510919
[Epoch 68; Iter    18/   36] train: loss: 0.4310118
[Epoch 68] ogbg-molsider: 0.680065 val loss: 0.489655
[Epoch 68] ogbg-molsider: 0.627885 test loss: 0.509998
[Epoch 69; Iter    12/   36] train: loss: 0.4088997
[Epoch 69] ogbg-molsider: 0.664533 val loss: 0.495814
[Epoch 69] ogbg-molsider: 0.644769 test loss: 0.551631
[Epoch 70; Iter     6/   36] train: loss: 0.4167487
[Epoch 70; Iter    36/   36] train: loss: 0.3931706
[Epoch 70] ogbg-molsider: 0.613471 val loss: 0.508917
[Epoch 70] ogbg-molsider: 0.646007 test loss: 0.541960
[Epoch 71; Iter    30/   36] train: loss: 0.4007260
[Epoch 71] ogbg-molsider: 0.674575 val loss: 0.473325
[Epoch 71] ogbg-molsider: 0.654780 test loss: 0.542003
[Epoch 72; Iter    24/   36] train: loss: 0.4011825
[Epoch 72] ogbg-molsider: 0.649451 val loss: 0.530201
[Epoch 72] ogbg-molsider: 0.630811 test loss: 0.599004
[Epoch 73; Iter    18/   36] train: loss: 0.3827690
[Epoch 73] ogbg-molsider: 0.659944 val loss: 0.497807
[Epoch 73] ogbg-molsider: 0.640872 test loss: 0.571458
[Epoch 74; Iter    12/   36] train: loss: 0.3813531
[Epoch 74] ogbg-molsider: 0.648874 val loss: 0.504150
[Epoch 74] ogbg-molsider: 0.652780 test loss: 0.549166
[Epoch 75; Iter     6/   36] train: loss: 0.3523935
[Epoch 75; Iter    36/   36] train: loss: 0.3532482
[Epoch 75] ogbg-molsider: 0.642388 val loss: 0.526135
[Epoch 75] ogbg-molsider: 0.651762 test loss: 0.573040
[Epoch 76; Iter    30/   36] train: loss: 0.4019363
[Epoch 76] ogbg-molsider: 0.662477 val loss: 0.500757
[Epoch 76] ogbg-molsider: 0.662623 test loss: 0.532415
[Epoch 77; Iter    24/   36] train: loss: 0.3655270
[Epoch 77] ogbg-molsider: 0.645560 val loss: 0.527567
[Epoch 77] ogbg-molsider: 0.653070 test loss: 0.537811
[Epoch 78; Iter    18/   36] train: loss: 0.3793970
[Epoch 78] ogbg-molsider: 0.668645 val loss: 0.527405
[Epoch 78] ogbg-molsider: 0.630168 test loss: 0.616603
[Epoch 79; Iter    12/   36] train: loss: 0.3359590
[Epoch 79] ogbg-molsider: 0.639817 val loss: 0.531899
[Epoch 79] ogbg-molsider: 0.650169 test loss: 0.558277
[Epoch 80; Iter     6/   36] train: loss: 0.3263101
[Epoch 80; Iter    36/   36] train: loss: 0.3325029
[Epoch 80] ogbg-molsider: 0.644226 val loss: 0.514780
[Epoch 80] ogbg-molsider: 0.648306 test loss: 0.536910
[Epoch 35; Iter    22/   32] train: loss: 0.4975848
[Epoch 35] ogbg-molsider: 0.612358 val loss: 0.497731
[Epoch 35] ogbg-molsider: 0.569937 test loss: 0.513383
[Epoch 36; Iter    20/   32] train: loss: 0.4661582
[Epoch 36] ogbg-molsider: 0.630458 val loss: 0.496300
[Epoch 36] ogbg-molsider: 0.587326 test loss: 0.515455
[Epoch 37; Iter    18/   32] train: loss: 0.4681889
[Epoch 37] ogbg-molsider: 0.609558 val loss: 0.502315
[Epoch 37] ogbg-molsider: 0.594797 test loss: 0.508178
[Epoch 38; Iter    16/   32] train: loss: 0.5252164
[Epoch 38] ogbg-molsider: 0.613470 val loss: 0.500797
[Epoch 38] ogbg-molsider: 0.602910 test loss: 0.504375
[Epoch 39; Iter    14/   32] train: loss: 0.4944268
[Epoch 39] ogbg-molsider: 0.617572 val loss: 0.500053
[Epoch 39] ogbg-molsider: 0.599710 test loss: 0.506487
[Epoch 40; Iter    12/   32] train: loss: 0.5254103
[Epoch 40] ogbg-molsider: 0.621082 val loss: 0.495291
[Epoch 40] ogbg-molsider: 0.583366 test loss: 0.510486
[Epoch 41; Iter    10/   32] train: loss: 0.4715935
[Epoch 41] ogbg-molsider: 0.620285 val loss: 0.496939
[Epoch 41] ogbg-molsider: 0.588262 test loss: 0.514295
[Epoch 42; Iter     8/   32] train: loss: 0.4642899
[Epoch 42] ogbg-molsider: 0.622503 val loss: 0.497969
[Epoch 42] ogbg-molsider: 0.579094 test loss: 0.519540
[Epoch 43; Iter     6/   32] train: loss: 0.5182498
[Epoch 43] ogbg-molsider: 0.625942 val loss: 0.511701
[Epoch 43] ogbg-molsider: 0.593873 test loss: 0.515064
[Epoch 44; Iter     4/   32] train: loss: 0.4648797
[Epoch 44] ogbg-molsider: 0.636620 val loss: 0.498777
[Epoch 44] ogbg-molsider: 0.618854 test loss: 0.502230
[Epoch 45; Iter     2/   32] train: loss: 0.4699982
[Epoch 45; Iter    32/   32] train: loss: 0.4633617
[Epoch 45] ogbg-molsider: 0.626954 val loss: 0.502421
[Epoch 45] ogbg-molsider: 0.606076 test loss: 0.517286
[Epoch 46; Iter    30/   32] train: loss: 0.4954385
[Epoch 46] ogbg-molsider: 0.633829 val loss: 0.506134
[Epoch 46] ogbg-molsider: 0.593619 test loss: 0.525717
[Epoch 47; Iter    28/   32] train: loss: 0.5227081
[Epoch 47] ogbg-molsider: 0.631218 val loss: 0.503113
[Epoch 47] ogbg-molsider: 0.607375 test loss: 0.526669
[Epoch 48; Iter    26/   32] train: loss: 0.4509035
[Epoch 48] ogbg-molsider: 0.633928 val loss: 0.525216
[Epoch 48] ogbg-molsider: 0.618487 test loss: 0.542966
[Epoch 49; Iter    24/   32] train: loss: 0.4591264
[Epoch 49] ogbg-molsider: 0.628826 val loss: 0.502474
[Epoch 49] ogbg-molsider: 0.633662 test loss: 0.544114
[Epoch 50; Iter    22/   32] train: loss: 0.4618046
[Epoch 50] ogbg-molsider: 0.624065 val loss: 0.544491
[Epoch 50] ogbg-molsider: 0.631626 test loss: 0.509970
[Epoch 51; Iter    20/   32] train: loss: 0.5090940
[Epoch 51] ogbg-molsider: 0.609121 val loss: 1.269358
[Epoch 51] ogbg-molsider: 0.607418 test loss: 0.513582
[Epoch 52; Iter    18/   32] train: loss: 0.5081741
[Epoch 52] ogbg-molsider: 0.619579 val loss: 0.771096
[Epoch 52] ogbg-molsider: 0.624569 test loss: 0.506907
[Epoch 53; Iter    16/   32] train: loss: 0.4539736
[Epoch 53] ogbg-molsider: 0.632966 val loss: 0.499359
[Epoch 53] ogbg-molsider: 0.639731 test loss: 0.500630
[Epoch 54; Iter    14/   32] train: loss: 0.4627914
[Epoch 54] ogbg-molsider: 0.626881 val loss: 0.504970
[Epoch 54] ogbg-molsider: 0.632984 test loss: 0.503071
[Epoch 55; Iter    12/   32] train: loss: 0.4536436
[Epoch 55] ogbg-molsider: 0.624996 val loss: 0.503107
[Epoch 55] ogbg-molsider: 0.615417 test loss: 0.517409
[Epoch 56; Iter    10/   32] train: loss: 0.4181785
[Epoch 56] ogbg-molsider: 0.636848 val loss: 0.496358
[Epoch 56] ogbg-molsider: 0.638370 test loss: 0.503305
[Epoch 57; Iter     8/   32] train: loss: 0.4435516
[Epoch 57] ogbg-molsider: 0.651892 val loss: 0.500367
[Epoch 57] ogbg-molsider: 0.638756 test loss: 0.499512
[Epoch 58; Iter     6/   32] train: loss: 0.4226955
[Epoch 58] ogbg-molsider: 0.636366 val loss: 0.509731
[Epoch 58] ogbg-molsider: 0.615182 test loss: 0.529752
[Epoch 59; Iter     4/   32] train: loss: 0.4849320
[Epoch 59] ogbg-molsider: 0.623715 val loss: 0.509826
[Epoch 59] ogbg-molsider: 0.626871 test loss: 0.504342
[Epoch 60; Iter     2/   32] train: loss: 0.4479742
[Epoch 60; Iter    32/   32] train: loss: 0.3989558
[Epoch 60] ogbg-molsider: 0.644018 val loss: 0.505425
[Epoch 60] ogbg-molsider: 0.650542 test loss: 0.496337
[Epoch 61; Iter    30/   32] train: loss: 0.4352831
[Epoch 61] ogbg-molsider: 0.638524 val loss: 0.516418
[Epoch 61] ogbg-molsider: 0.647500 test loss: 0.508140
[Epoch 62; Iter    28/   32] train: loss: 0.4399135
[Epoch 62] ogbg-molsider: 0.629884 val loss: 0.527217
[Epoch 62] ogbg-molsider: 0.651611 test loss: 0.503969
[Epoch 63; Iter    26/   32] train: loss: 0.4265231
[Epoch 63] ogbg-molsider: 0.645145 val loss: 0.505257
[Epoch 63] ogbg-molsider: 0.634662 test loss: 0.507792
[Epoch 64; Iter    24/   32] train: loss: 0.4530078
[Epoch 64] ogbg-molsider: 0.645167 val loss: 0.516870
[Epoch 64] ogbg-molsider: 0.615984 test loss: 0.521358
[Epoch 65; Iter    22/   32] train: loss: 0.4236604
[Epoch 65] ogbg-molsider: 0.626785 val loss: 0.519227
[Epoch 65] ogbg-molsider: 0.625976 test loss: 0.508146
[Epoch 66; Iter    20/   32] train: loss: 0.3887672
[Epoch 66] ogbg-molsider: 0.629486 val loss: 0.524787
[Epoch 66] ogbg-molsider: 0.624736 test loss: 0.526459
[Epoch 67; Iter    18/   32] train: loss: 0.4664033
[Epoch 67] ogbg-molsider: 0.653373 val loss: 0.509051
[Epoch 67] ogbg-molsider: 0.640243 test loss: 0.514204
[Epoch 68; Iter    16/   32] train: loss: 0.3845671
[Epoch 68] ogbg-molsider: 0.636290 val loss: 0.523892
[Epoch 68] ogbg-molsider: 0.640824 test loss: 0.521833
[Epoch 69; Iter    14/   32] train: loss: 0.4319440
[Epoch 69] ogbg-molsider: 0.620114 val loss: 0.541642
[Epoch 69] ogbg-molsider: 0.620092 test loss: 0.557132
[Epoch 70; Iter    12/   32] train: loss: 0.3964738
[Epoch 70] ogbg-molsider: 0.630755 val loss: 0.518335
[Epoch 70] ogbg-molsider: 0.648979 test loss: 0.515878
[Epoch 71; Iter    10/   32] train: loss: 0.4547507
[Epoch 71] ogbg-molsider: 0.614336 val loss: 0.547144
[Epoch 71] ogbg-molsider: 0.611081 test loss: 0.542892
[Epoch 72; Iter     8/   32] train: loss: 0.4887310
[Epoch 72] ogbg-molsider: 0.634622 val loss: 0.547202
[Epoch 72] ogbg-molsider: 0.641027 test loss: 0.517590
[Epoch 73; Iter     6/   32] train: loss: 0.4154985
[Epoch 73] ogbg-molsider: 0.620837 val loss: 0.532130
[Epoch 73] ogbg-molsider: 0.635672 test loss: 0.520125
[Epoch 74; Iter     4/   32] train: loss: 0.3809042
[Epoch 74] ogbg-molsider: 0.626190 val loss: 0.538943
[Epoch 74] ogbg-molsider: 0.640794 test loss: 0.532826
[Epoch 75; Iter     2/   32] train: loss: 0.4066899
[Epoch 75; Iter    32/   32] train: loss: 0.3835587
[Epoch 75] ogbg-molsider: 0.618419 val loss: 0.540237
[Epoch 75] ogbg-molsider: 0.635531 test loss: 0.522340
[Epoch 76; Iter    30/   32] train: loss: 0.3998664
[Epoch 76] ogbg-molsider: 0.611320 val loss: 0.553068
[Epoch 76] ogbg-molsider: 0.648508 test loss: 0.520156
[Epoch 77; Iter    28/   32] train: loss: 0.4200653
[Epoch 77] ogbg-molsider: 0.635234 val loss: 0.538912
[Epoch 77] ogbg-molsider: 0.646207 test loss: 0.530456
[Epoch 78; Iter    26/   32] train: loss: 0.4057039
[Epoch 78] ogbg-molsider: 0.631844 val loss: 0.538207
[Epoch 78] ogbg-molsider: 0.649984 test loss: 0.528482
[Epoch 79; Iter    24/   32] train: loss: 0.4005958
[Epoch 79] ogbg-molsider: 0.640232 val loss: 0.550023
[Epoch 79] ogbg-molsider: 0.632831 test loss: 0.545922
[Epoch 80; Iter    22/   32] train: loss: 0.4079641
[Epoch 80] ogbg-molsider: 0.635627 val loss: 0.535507
[Epoch 80] ogbg-molsider: 0.639754 test loss: 0.532151
[Epoch 81; Iter    20/   32] train: loss: 0.3830875
[Epoch 81] ogbg-molsider: 0.636117 val loss: 0.547600
[Epoch 81] ogbg-molsider: 0.622815 test loss: 0.567458
[Epoch 82; Iter    18/   32] train: loss: 0.3956846
[Epoch 82] ogbg-molsider: 0.631703 val loss: 0.539674
[Epoch 82] ogbg-molsider: 0.648390 test loss: 0.533365
[Epoch 83; Iter    16/   32] train: loss: 0.3624262
[Epoch 83] ogbg-molsider: 0.643027 val loss: 0.561729
[Epoch 83] ogbg-molsider: 0.664638 test loss: 0.527430
[Epoch 84; Iter    14/   32] train: loss: 0.3668445
[Epoch 84] ogbg-molsider: 0.634199 val loss: 0.787320
[Epoch 33] ogbg-molsider: 0.560825 test loss: 0.547451
[Epoch 34; Iter    12/   36] train: loss: 0.5120952
[Epoch 34] ogbg-molsider: 0.639901 val loss: 0.495160
[Epoch 34] ogbg-molsider: 0.583635 test loss: 0.533433
[Epoch 35; Iter     6/   36] train: loss: 0.4452474
[Epoch 35; Iter    36/   36] train: loss: 0.5097839
[Epoch 35] ogbg-molsider: 0.649966 val loss: 0.491768
[Epoch 35] ogbg-molsider: 0.569013 test loss: 0.523041
[Epoch 36; Iter    30/   36] train: loss: 0.4640772
[Epoch 36] ogbg-molsider: 0.633202 val loss: 0.487795
[Epoch 36] ogbg-molsider: 0.574973 test loss: 0.535167
[Epoch 37; Iter    24/   36] train: loss: 0.4774041
[Epoch 37] ogbg-molsider: 0.652946 val loss: 0.482123
[Epoch 37] ogbg-molsider: 0.586748 test loss: 0.520585
[Epoch 38; Iter    18/   36] train: loss: 0.4876407
[Epoch 38] ogbg-molsider: 0.643309 val loss: 0.482757
[Epoch 38] ogbg-molsider: 0.575381 test loss: 0.530443
[Epoch 39; Iter    12/   36] train: loss: 0.5192908
[Epoch 39] ogbg-molsider: 0.647954 val loss: 0.481131
[Epoch 39] ogbg-molsider: 0.584219 test loss: 0.597998
[Epoch 40; Iter     6/   36] train: loss: 0.5117942
[Epoch 40; Iter    36/   36] train: loss: 0.5049446
[Epoch 40] ogbg-molsider: 0.596501 val loss: 0.488389
[Epoch 40] ogbg-molsider: 0.570233 test loss: 0.546471
[Epoch 41; Iter    30/   36] train: loss: 0.5179547
[Epoch 41] ogbg-molsider: 0.609675 val loss: 0.529602
[Epoch 41] ogbg-molsider: 0.548681 test loss: 0.556172
[Epoch 42; Iter    24/   36] train: loss: 0.5466952
[Epoch 42] ogbg-molsider: 0.608498 val loss: 0.489095
[Epoch 42] ogbg-molsider: 0.570047 test loss: 0.533871
[Epoch 43; Iter    18/   36] train: loss: 0.4785482
[Epoch 43] ogbg-molsider: 0.637663 val loss: 0.504504
[Epoch 43] ogbg-molsider: 0.575413 test loss: 0.570827
[Epoch 44; Iter    12/   36] train: loss: 0.4847643
[Epoch 44] ogbg-molsider: 0.625305 val loss: 0.494558
[Epoch 44] ogbg-molsider: 0.599644 test loss: 0.519651
[Epoch 45; Iter     6/   36] train: loss: 0.4583625
[Epoch 45; Iter    36/   36] train: loss: 0.4868982
[Epoch 45] ogbg-molsider: 0.652353 val loss: 0.478138
[Epoch 45] ogbg-molsider: 0.579719 test loss: 0.532624
[Epoch 46; Iter    30/   36] train: loss: 0.4884239
[Epoch 46] ogbg-molsider: 0.654925 val loss: 0.480815
[Epoch 46] ogbg-molsider: 0.599829 test loss: 0.527482
[Epoch 47; Iter    24/   36] train: loss: 0.4686544
[Epoch 47] ogbg-molsider: 0.642459 val loss: 0.478291
[Epoch 47] ogbg-molsider: 0.572564 test loss: 0.549755
[Epoch 48; Iter    18/   36] train: loss: 0.4948749
[Epoch 48] ogbg-molsider: 0.624395 val loss: 0.726019
[Epoch 48] ogbg-molsider: 0.583478 test loss: 0.648458
[Epoch 49; Iter    12/   36] train: loss: 0.5166126
[Epoch 49] ogbg-molsider: 0.666086 val loss: 0.489766
[Epoch 49] ogbg-molsider: 0.587434 test loss: 0.538525
[Epoch 50; Iter     6/   36] train: loss: 0.4687878
[Epoch 50; Iter    36/   36] train: loss: 0.3980567
[Epoch 50] ogbg-molsider: 0.650390 val loss: 0.489323
[Epoch 50] ogbg-molsider: 0.607521 test loss: 0.559943
[Epoch 51; Iter    30/   36] train: loss: 0.5081888
[Epoch 51] ogbg-molsider: 0.645666 val loss: 0.481481
[Epoch 51] ogbg-molsider: 0.606936 test loss: 0.533691
[Epoch 52; Iter    24/   36] train: loss: 0.4362710
[Epoch 52] ogbg-molsider: 0.622083 val loss: 0.514042
[Epoch 52] ogbg-molsider: 0.617889 test loss: 3.393214
[Epoch 53; Iter    18/   36] train: loss: 0.5018169
[Epoch 53] ogbg-molsider: 0.657887 val loss: 0.476766
[Epoch 53] ogbg-molsider: 0.605452 test loss: 0.514589
[Epoch 54; Iter    12/   36] train: loss: 0.4569137
[Epoch 54] ogbg-molsider: 0.658160 val loss: 0.483306
[Epoch 54] ogbg-molsider: 0.623577 test loss: 0.517666
[Epoch 55; Iter     6/   36] train: loss: 0.4057380
[Epoch 55; Iter    36/   36] train: loss: 0.4529707
[Epoch 55] ogbg-molsider: 0.641558 val loss: 0.492881
[Epoch 55] ogbg-molsider: 0.611417 test loss: 0.526675
[Epoch 56; Iter    30/   36] train: loss: 0.4298223
[Epoch 56] ogbg-molsider: 0.666693 val loss: 0.497711
[Epoch 56] ogbg-molsider: 0.628089 test loss: 0.898980
[Epoch 57; Iter    24/   36] train: loss: 0.5109034
[Epoch 57] ogbg-molsider: 0.639122 val loss: 0.484703
[Epoch 57] ogbg-molsider: 0.630853 test loss: 0.524248
[Epoch 58; Iter    18/   36] train: loss: 0.4442950
[Epoch 58] ogbg-molsider: 0.640168 val loss: 0.522554
[Epoch 58] ogbg-molsider: 0.643638 test loss: 0.521311
[Epoch 59; Iter    12/   36] train: loss: 0.4053612
[Epoch 59] ogbg-molsider: 0.661571 val loss: 0.479569
[Epoch 59] ogbg-molsider: 0.648825 test loss: 0.519813
[Epoch 60; Iter     6/   36] train: loss: 0.4500894
[Epoch 60; Iter    36/   36] train: loss: 0.5028433
[Epoch 60] ogbg-molsider: 0.652389 val loss: 0.484361
[Epoch 60] ogbg-molsider: 0.659091 test loss: 0.495236
[Epoch 61; Iter    30/   36] train: loss: 0.3976853
[Epoch 61] ogbg-molsider: 0.627920 val loss: 0.494845
[Epoch 61] ogbg-molsider: 0.626712 test loss: 0.525846
[Epoch 62; Iter    24/   36] train: loss: 0.4328492
[Epoch 62] ogbg-molsider: 0.645160 val loss: 0.491313
[Epoch 62] ogbg-molsider: 0.643591 test loss: 0.507439
[Epoch 63; Iter    18/   36] train: loss: 0.4374147
[Epoch 63] ogbg-molsider: 0.636528 val loss: 0.514526
[Epoch 63] ogbg-molsider: 0.634280 test loss: 0.515904
[Epoch 64; Iter    12/   36] train: loss: 0.3876137
[Epoch 64] ogbg-molsider: 0.658000 val loss: 0.471419
[Epoch 64] ogbg-molsider: 0.645003 test loss: 0.519387
[Epoch 65; Iter     6/   36] train: loss: 0.4190528
[Epoch 65; Iter    36/   36] train: loss: 0.4262446
[Epoch 65] ogbg-molsider: 0.666266 val loss: 0.491812
[Epoch 65] ogbg-molsider: 0.604106 test loss: 0.569290
[Epoch 66; Iter    30/   36] train: loss: 0.4391783
[Epoch 66] ogbg-molsider: 0.611562 val loss: 0.518352
[Epoch 66] ogbg-molsider: 0.633842 test loss: 0.556950
[Epoch 67; Iter    24/   36] train: loss: 0.4654340
[Epoch 67] ogbg-molsider: 0.667340 val loss: 0.489462
[Epoch 67] ogbg-molsider: 0.630465 test loss: 0.535589
[Epoch 68; Iter    18/   36] train: loss: 0.4110135
[Epoch 68] ogbg-molsider: 0.650019 val loss: 0.502628
[Epoch 68] ogbg-molsider: 0.629557 test loss: 0.528288
[Epoch 69; Iter    12/   36] train: loss: 0.4153709
[Epoch 69] ogbg-molsider: 0.649127 val loss: 0.529213
[Epoch 69] ogbg-molsider: 0.668272 test loss: 0.522274
[Epoch 70; Iter     6/   36] train: loss: 0.4038607
[Epoch 70; Iter    36/   36] train: loss: 0.4069381
[Epoch 70] ogbg-molsider: 0.658180 val loss: 0.513796
[Epoch 70] ogbg-molsider: 0.643906 test loss: 0.545138
[Epoch 71; Iter    30/   36] train: loss: 0.3789125
[Epoch 71] ogbg-molsider: 0.628759 val loss: 0.544848
[Epoch 71] ogbg-molsider: 0.636739 test loss: 0.571728
[Epoch 72; Iter    24/   36] train: loss: 0.4300867
[Epoch 72] ogbg-molsider: 0.640066 val loss: 0.515816
[Epoch 72] ogbg-molsider: 0.643743 test loss: 0.537251
[Epoch 73; Iter    18/   36] train: loss: 0.4372270
[Epoch 73] ogbg-molsider: 0.646850 val loss: 0.510454
[Epoch 73] ogbg-molsider: 0.637551 test loss: 0.542927
[Epoch 74; Iter    12/   36] train: loss: 0.4135339
[Epoch 74] ogbg-molsider: 0.664040 val loss: 0.499649
[Epoch 74] ogbg-molsider: 0.643846 test loss: 0.524628
[Epoch 75; Iter     6/   36] train: loss: 0.4196028
[Epoch 75; Iter    36/   36] train: loss: 0.3769513
[Epoch 75] ogbg-molsider: 0.633671 val loss: 0.555078
[Epoch 75] ogbg-molsider: 0.654251 test loss: 0.514737
[Epoch 76; Iter    30/   36] train: loss: 0.3631999
[Epoch 76] ogbg-molsider: 0.663201 val loss: 0.519587
[Epoch 76] ogbg-molsider: 0.665026 test loss: 0.556233
[Epoch 77; Iter    24/   36] train: loss: 0.3821707
[Epoch 77] ogbg-molsider: 0.644137 val loss: 0.517467
[Epoch 77] ogbg-molsider: 0.629171 test loss: 0.555959
[Epoch 78; Iter    18/   36] train: loss: 0.4259768
[Epoch 78] ogbg-molsider: 0.639942 val loss: 0.534675
[Epoch 78] ogbg-molsider: 0.668533 test loss: 0.523219
[Epoch 79; Iter    12/   36] train: loss: 0.3479621
[Epoch 79] ogbg-molsider: 0.636027 val loss: 0.510215
[Epoch 79] ogbg-molsider: 0.648193 test loss: 0.529836
[Epoch 80; Iter     6/   36] train: loss: 0.3661143
[Epoch 80; Iter    36/   36] train: loss: 0.4296753
[Epoch 80] ogbg-molsider: 0.643535 val loss: 0.537672
[Epoch 80] ogbg-molsider: 0.634774 test loss: 0.535045
[Epoch 35; Iter    22/   32] train: loss: 0.5091957
[Epoch 35] ogbg-molsider: 0.613266 val loss: 0.500131
[Epoch 35] ogbg-molsider: 0.598485 test loss: 0.513563
[Epoch 36; Iter    20/   32] train: loss: 0.5152873
[Epoch 36] ogbg-molsider: 0.615303 val loss: 0.499615
[Epoch 36] ogbg-molsider: 0.605209 test loss: 0.510868
[Epoch 37; Iter    18/   32] train: loss: 0.5081276
[Epoch 37] ogbg-molsider: 0.612201 val loss: 0.502739
[Epoch 37] ogbg-molsider: 0.568875 test loss: 0.517296
[Epoch 38; Iter    16/   32] train: loss: 0.4773872
[Epoch 38] ogbg-molsider: 0.613538 val loss: 0.500880
[Epoch 38] ogbg-molsider: 0.593251 test loss: 0.512398
[Epoch 39; Iter    14/   32] train: loss: 0.4502698
[Epoch 39] ogbg-molsider: 0.625177 val loss: 0.498328
[Epoch 39] ogbg-molsider: 0.595293 test loss: 0.512562
[Epoch 40; Iter    12/   32] train: loss: 0.5105075
[Epoch 40] ogbg-molsider: 0.598615 val loss: 0.503244
[Epoch 40] ogbg-molsider: 0.609367 test loss: 0.507848
[Epoch 41; Iter    10/   32] train: loss: 0.4835899
[Epoch 41] ogbg-molsider: 0.615790 val loss: 0.502483
[Epoch 41] ogbg-molsider: 0.596234 test loss: 0.527932
[Epoch 42; Iter     8/   32] train: loss: 0.4660261
[Epoch 42] ogbg-molsider: 0.623267 val loss: 0.496949
[Epoch 42] ogbg-molsider: 0.586475 test loss: 0.516736
[Epoch 43; Iter     6/   32] train: loss: 0.4899654
[Epoch 43] ogbg-molsider: 0.622375 val loss: 0.495400
[Epoch 43] ogbg-molsider: 0.596737 test loss: 0.509791
[Epoch 44; Iter     4/   32] train: loss: 0.4786727
[Epoch 44] ogbg-molsider: 0.632021 val loss: 0.495838
[Epoch 44] ogbg-molsider: 0.631283 test loss: 0.497964
[Epoch 45; Iter     2/   32] train: loss: 0.4782583
[Epoch 45; Iter    32/   32] train: loss: 0.5392305
[Epoch 45] ogbg-molsider: 0.623932 val loss: 0.500937
[Epoch 45] ogbg-molsider: 0.609439 test loss: 0.517652
[Epoch 46; Iter    30/   32] train: loss: 0.4877407
[Epoch 46] ogbg-molsider: 0.636597 val loss: 0.492558
[Epoch 46] ogbg-molsider: 0.640038 test loss: 0.497103
[Epoch 47; Iter    28/   32] train: loss: 0.4528093
[Epoch 47] ogbg-molsider: 0.616380 val loss: 0.520090
[Epoch 47] ogbg-molsider: 0.611619 test loss: 0.519503
[Epoch 48; Iter    26/   32] train: loss: 0.4775741
[Epoch 48] ogbg-molsider: 0.614930 val loss: 0.522435
[Epoch 48] ogbg-molsider: 0.608891 test loss: 0.575527
[Epoch 49; Iter    24/   32] train: loss: 0.5012717
[Epoch 49] ogbg-molsider: 0.622580 val loss: 0.495003
[Epoch 49] ogbg-molsider: 0.606332 test loss: 0.518261
[Epoch 50; Iter    22/   32] train: loss: 0.4637946
[Epoch 50] ogbg-molsider: 0.621922 val loss: 0.517281
[Epoch 50] ogbg-molsider: 0.624557 test loss: 0.512448
[Epoch 51; Iter    20/   32] train: loss: 0.4935409
[Epoch 51] ogbg-molsider: 0.621810 val loss: 0.510902
[Epoch 51] ogbg-molsider: 0.602605 test loss: 0.519607
[Epoch 52; Iter    18/   32] train: loss: 0.4343485
[Epoch 52] ogbg-molsider: 0.641245 val loss: 0.492313
[Epoch 52] ogbg-molsider: 0.616228 test loss: 0.506558
[Epoch 53; Iter    16/   32] train: loss: 0.4642649
[Epoch 53] ogbg-molsider: 0.631427 val loss: 0.500235
[Epoch 53] ogbg-molsider: 0.591411 test loss: 0.520480
[Epoch 54; Iter    14/   32] train: loss: 0.4779398
[Epoch 54] ogbg-molsider: 0.610926 val loss: 0.551127
[Epoch 54] ogbg-molsider: 0.628457 test loss: 0.555009
[Epoch 55; Iter    12/   32] train: loss: 0.5191944
[Epoch 55] ogbg-molsider: 0.644870 val loss: 0.496569
[Epoch 55] ogbg-molsider: 0.636134 test loss: 0.510777
[Epoch 56; Iter    10/   32] train: loss: 0.4736529
[Epoch 56] ogbg-molsider: 0.640982 val loss: 0.500370
[Epoch 56] ogbg-molsider: 0.608424 test loss: 0.524209
[Epoch 57; Iter     8/   32] train: loss: 0.4803529
[Epoch 57] ogbg-molsider: 0.620530 val loss: 0.505024
[Epoch 57] ogbg-molsider: 0.621741 test loss: 0.508116
[Epoch 58; Iter     6/   32] train: loss: 0.4413256
[Epoch 58] ogbg-molsider: 0.595717 val loss: 0.533349
[Epoch 58] ogbg-molsider: 0.608826 test loss: 0.534557
[Epoch 59; Iter     4/   32] train: loss: 0.4728784
[Epoch 59] ogbg-molsider: 0.626566 val loss: 0.602165
[Epoch 59] ogbg-molsider: 0.608248 test loss: 0.521695
[Epoch 60; Iter     2/   32] train: loss: 0.4989814
[Epoch 60; Iter    32/   32] train: loss: 0.4848091
[Epoch 60] ogbg-molsider: 0.637281 val loss: 0.514971
[Epoch 60] ogbg-molsider: 0.617704 test loss: 0.515278
[Epoch 61; Iter    30/   32] train: loss: 0.5003353
[Epoch 61] ogbg-molsider: 0.611374 val loss: 0.511154
[Epoch 61] ogbg-molsider: 0.624586 test loss: 0.513665
[Epoch 62; Iter    28/   32] train: loss: 0.4732330
[Epoch 62] ogbg-molsider: 0.619487 val loss: 0.517479
[Epoch 62] ogbg-molsider: 0.618256 test loss: 0.519164
[Epoch 63; Iter    26/   32] train: loss: 0.4909277
[Epoch 63] ogbg-molsider: 0.616098 val loss: 0.552625
[Epoch 63] ogbg-molsider: 0.630357 test loss: 0.506581
[Epoch 64; Iter    24/   32] train: loss: 0.4271362
[Epoch 64] ogbg-molsider: 0.634195 val loss: 1.699977
[Epoch 64] ogbg-molsider: 0.634795 test loss: 0.514122
[Epoch 65; Iter    22/   32] train: loss: 0.4154756
[Epoch 65] ogbg-molsider: 0.611857 val loss: 0.514271
[Epoch 65] ogbg-molsider: 0.630128 test loss: 0.505286
[Epoch 66; Iter    20/   32] train: loss: 0.4812091
[Epoch 66] ogbg-molsider: 0.601903 val loss: 2.082370
[Epoch 66] ogbg-molsider: 0.627635 test loss: 1.220152
[Epoch 67; Iter    18/   32] train: loss: 0.4153750
[Epoch 67] ogbg-molsider: 0.632952 val loss: 0.510159
[Epoch 67] ogbg-molsider: 0.633561 test loss: 0.516636
[Epoch 68; Iter    16/   32] train: loss: 0.4388027
[Epoch 68] ogbg-molsider: 0.615180 val loss: 0.516005
[Epoch 68] ogbg-molsider: 0.625180 test loss: 0.512207
[Epoch 69; Iter    14/   32] train: loss: 0.4713612
[Epoch 69] ogbg-molsider: 0.608820 val loss: 0.535079
[Epoch 69] ogbg-molsider: 0.621201 test loss: 0.525902
[Epoch 70; Iter    12/   32] train: loss: 0.4265343
[Epoch 70] ogbg-molsider: 0.625982 val loss: 0.528836
[Epoch 70] ogbg-molsider: 0.651308 test loss: 0.515082
[Epoch 71; Iter    10/   32] train: loss: 0.4075847
[Epoch 71] ogbg-molsider: 0.627249 val loss: 0.529475
[Epoch 71] ogbg-molsider: 0.625576 test loss: 0.529353
[Epoch 72; Iter     8/   32] train: loss: 0.3976120
[Epoch 72] ogbg-molsider: 0.627056 val loss: 0.517817
[Epoch 72] ogbg-molsider: 0.638813 test loss: 0.509334
[Epoch 73; Iter     6/   32] train: loss: 0.4192068
[Epoch 73] ogbg-molsider: 0.617579 val loss: 0.535171
[Epoch 73] ogbg-molsider: 0.617590 test loss: 0.543720
[Epoch 74; Iter     4/   32] train: loss: 0.4153496
[Epoch 74] ogbg-molsider: 0.609562 val loss: 0.552324
[Epoch 74] ogbg-molsider: 0.614861 test loss: 0.546426
[Epoch 75; Iter     2/   32] train: loss: 0.4378307
[Epoch 75; Iter    32/   32] train: loss: 0.3707128
[Epoch 75] ogbg-molsider: 0.630490 val loss: 0.526067
[Epoch 75] ogbg-molsider: 0.636316 test loss: 0.527130
[Epoch 76; Iter    30/   32] train: loss: 0.3841221
[Epoch 76] ogbg-molsider: 0.625138 val loss: 0.553830
[Epoch 76] ogbg-molsider: 0.642899 test loss: 0.528443
[Epoch 77; Iter    28/   32] train: loss: 0.4277478
[Epoch 77] ogbg-molsider: 0.618004 val loss: 0.555093
[Epoch 77] ogbg-molsider: 0.642991 test loss: 0.529471
[Epoch 78; Iter    26/   32] train: loss: 0.4139149
[Epoch 78] ogbg-molsider: 0.621976 val loss: 0.560353
[Epoch 78] ogbg-molsider: 0.615122 test loss: 0.554584
[Epoch 79; Iter    24/   32] train: loss: 0.4073398
[Epoch 79] ogbg-molsider: 0.640155 val loss: 0.537112
[Epoch 79] ogbg-molsider: 0.640540 test loss: 0.533353
[Epoch 80; Iter    22/   32] train: loss: 0.4300789
[Epoch 80] ogbg-molsider: 0.617455 val loss: 0.541008
[Epoch 80] ogbg-molsider: 0.635868 test loss: 0.532279
[Epoch 81; Iter    20/   32] train: loss: 0.4501225
[Epoch 81] ogbg-molsider: 0.644545 val loss: 0.552896
[Epoch 81] ogbg-molsider: 0.650209 test loss: 0.531634
[Epoch 82; Iter    18/   32] train: loss: 0.4017578
[Epoch 82] ogbg-molsider: 0.638302 val loss: 0.552161
[Epoch 82] ogbg-molsider: 0.641518 test loss: 0.540747
[Epoch 83; Iter    16/   32] train: loss: 0.3922500
[Epoch 83] ogbg-molsider: 0.620339 val loss: 0.552920
[Epoch 83] ogbg-molsider: 0.636595 test loss: 0.544470
[Epoch 84; Iter    14/   32] train: loss: 0.3818498
[Epoch 84] ogbg-molsider: 0.639573 val loss: 0.535964
[Epoch 35; Iter    22/   32] train: loss: 0.5505661
[Epoch 35] ogbg-molsider: 0.600333 val loss: 0.501852
[Epoch 35] ogbg-molsider: 0.598537 test loss: 0.508333
[Epoch 36; Iter    20/   32] train: loss: 0.4973754
[Epoch 36] ogbg-molsider: 0.597941 val loss: 0.504550
[Epoch 36] ogbg-molsider: 0.599126 test loss: 0.516774
[Epoch 37; Iter    18/   32] train: loss: 0.4985468
[Epoch 37] ogbg-molsider: 0.600369 val loss: 0.502365
[Epoch 37] ogbg-molsider: 0.595312 test loss: 0.507677
[Epoch 38; Iter    16/   32] train: loss: 0.4443422
[Epoch 38] ogbg-molsider: 0.613312 val loss: 0.501479
[Epoch 38] ogbg-molsider: 0.609862 test loss: 0.508129
[Epoch 39; Iter    14/   32] train: loss: 0.5106463
[Epoch 39] ogbg-molsider: 0.628144 val loss: 0.501468
[Epoch 39] ogbg-molsider: 0.616198 test loss: 0.501516
[Epoch 40; Iter    12/   32] train: loss: 0.4964967
[Epoch 40] ogbg-molsider: 0.618926 val loss: 0.500911
[Epoch 40] ogbg-molsider: 0.579173 test loss: 0.518123
[Epoch 41; Iter    10/   32] train: loss: 0.5182034
[Epoch 41] ogbg-molsider: 0.614619 val loss: 0.501136
[Epoch 41] ogbg-molsider: 0.614430 test loss: 0.513327
[Epoch 42; Iter     8/   32] train: loss: 0.5783074
[Epoch 42] ogbg-molsider: 0.616634 val loss: 0.499288
[Epoch 42] ogbg-molsider: 0.614034 test loss: 0.508716
[Epoch 43; Iter     6/   32] train: loss: 0.4957034
[Epoch 43] ogbg-molsider: 0.625816 val loss: 0.497511
[Epoch 43] ogbg-molsider: 0.602925 test loss: 0.511603
[Epoch 44; Iter     4/   32] train: loss: 0.4714310
[Epoch 44] ogbg-molsider: 0.640379 val loss: 0.492637
[Epoch 44] ogbg-molsider: 0.635945 test loss: 0.499349
[Epoch 45; Iter     2/   32] train: loss: 0.5276282
[Epoch 45; Iter    32/   32] train: loss: 0.4258528
[Epoch 45] ogbg-molsider: 0.620318 val loss: 0.759923
[Epoch 45] ogbg-molsider: 0.616471 test loss: 0.516905
[Epoch 46; Iter    30/   32] train: loss: 0.4975387
[Epoch 46] ogbg-molsider: 0.595945 val loss: 0.516908
[Epoch 46] ogbg-molsider: 0.625244 test loss: 0.506598
[Epoch 47; Iter    28/   32] train: loss: 0.4944313
[Epoch 47] ogbg-molsider: 0.631464 val loss: 0.497708
[Epoch 47] ogbg-molsider: 0.626572 test loss: 0.518344
[Epoch 48; Iter    26/   32] train: loss: 0.4446147
[Epoch 48] ogbg-molsider: 0.596402 val loss: 0.570153
[Epoch 48] ogbg-molsider: 0.591240 test loss: 0.578640
[Epoch 49; Iter    24/   32] train: loss: 0.4539912
[Epoch 49] ogbg-molsider: 0.608289 val loss: 0.512555
[Epoch 49] ogbg-molsider: 0.628320 test loss: 0.506687
[Epoch 50; Iter    22/   32] train: loss: 0.4880581
[Epoch 50] ogbg-molsider: 0.636192 val loss: 0.498382
[Epoch 50] ogbg-molsider: 0.601467 test loss: 0.517256
[Epoch 51; Iter    20/   32] train: loss: 0.4967764
[Epoch 51] ogbg-molsider: 0.629750 val loss: 0.513085
[Epoch 51] ogbg-molsider: 0.628230 test loss: 0.501187
[Epoch 52; Iter    18/   32] train: loss: 0.4434270
[Epoch 52] ogbg-molsider: 0.643438 val loss: 0.494957
[Epoch 52] ogbg-molsider: 0.619787 test loss: 0.505015
[Epoch 53; Iter    16/   32] train: loss: 0.4466924
[Epoch 53] ogbg-molsider: 0.621807 val loss: 0.513300
[Epoch 53] ogbg-molsider: 0.615817 test loss: 0.523995
[Epoch 54; Iter    14/   32] train: loss: 0.4879658
[Epoch 54] ogbg-molsider: 0.620333 val loss: 0.740141
[Epoch 54] ogbg-molsider: 0.625589 test loss: 0.505848
[Epoch 55; Iter    12/   32] train: loss: 0.5071436
[Epoch 55] ogbg-molsider: 0.622820 val loss: 0.508989
[Epoch 55] ogbg-molsider: 0.628453 test loss: 0.503883
[Epoch 56; Iter    10/   32] train: loss: 0.4637672
[Epoch 56] ogbg-molsider: 0.623338 val loss: 0.499945
[Epoch 56] ogbg-molsider: 0.622981 test loss: 0.513400
[Epoch 57; Iter     8/   32] train: loss: 0.4658488
[Epoch 57] ogbg-molsider: 0.650916 val loss: 0.496220
[Epoch 57] ogbg-molsider: 0.623550 test loss: 0.512302
[Epoch 58; Iter     6/   32] train: loss: 0.4931871
[Epoch 58] ogbg-molsider: 0.638416 val loss: 0.491713
[Epoch 58] ogbg-molsider: 0.621669 test loss: 0.509867
[Epoch 59; Iter     4/   32] train: loss: 0.4714283
[Epoch 59] ogbg-molsider: 0.628475 val loss: 0.551743
[Epoch 59] ogbg-molsider: 0.642200 test loss: 0.518823
[Epoch 60; Iter     2/   32] train: loss: 0.4361438
[Epoch 60; Iter    32/   32] train: loss: 0.4633180
[Epoch 60] ogbg-molsider: 0.643230 val loss: 0.502379
[Epoch 60] ogbg-molsider: 0.613872 test loss: 0.523429
[Epoch 61; Iter    30/   32] train: loss: 0.4594812
[Epoch 61] ogbg-molsider: 0.630225 val loss: 0.498265
[Epoch 61] ogbg-molsider: 0.613718 test loss: 0.514512
[Epoch 62; Iter    28/   32] train: loss: 0.4731134
[Epoch 62] ogbg-molsider: 0.625065 val loss: 0.510302
[Epoch 62] ogbg-molsider: 0.621386 test loss: 0.517760
[Epoch 63; Iter    26/   32] train: loss: 0.4948960
[Epoch 63] ogbg-molsider: 0.630646 val loss: 0.508641
[Epoch 63] ogbg-molsider: 0.615031 test loss: 0.532243
[Epoch 64; Iter    24/   32] train: loss: 0.4229233
[Epoch 64] ogbg-molsider: 0.621606 val loss: 0.525795
[Epoch 64] ogbg-molsider: 0.635996 test loss: 0.526217
[Epoch 65; Iter    22/   32] train: loss: 0.3900793
[Epoch 65] ogbg-molsider: 0.591077 val loss: 0.717528
[Epoch 65] ogbg-molsider: 0.587382 test loss: 0.675448
[Epoch 66; Iter    20/   32] train: loss: 0.5028011
[Epoch 66] ogbg-molsider: 0.619928 val loss: 0.536565
[Epoch 66] ogbg-molsider: 0.601162 test loss: 0.545134
[Epoch 67; Iter    18/   32] train: loss: 0.4213927
[Epoch 67] ogbg-molsider: 0.629451 val loss: 0.528922
[Epoch 67] ogbg-molsider: 0.645312 test loss: 0.514708
[Epoch 68; Iter    16/   32] train: loss: 0.4847158
[Epoch 68] ogbg-molsider: 0.623544 val loss: 0.518532
[Epoch 68] ogbg-molsider: 0.635557 test loss: 0.508483
[Epoch 69; Iter    14/   32] train: loss: 0.4388969
[Epoch 69] ogbg-molsider: 0.611337 val loss: 0.540723
[Epoch 69] ogbg-molsider: 0.641392 test loss: 0.516206
[Epoch 70; Iter    12/   32] train: loss: 0.4430563
[Epoch 70] ogbg-molsider: 0.639532 val loss: 0.579674
[Epoch 70] ogbg-molsider: 0.623541 test loss: 0.527978
[Epoch 71; Iter    10/   32] train: loss: 0.4448211
[Epoch 71] ogbg-molsider: 0.635512 val loss: 0.566900
[Epoch 71] ogbg-molsider: 0.616626 test loss: 0.585071
[Epoch 72; Iter     8/   32] train: loss: 0.4601067
[Epoch 72] ogbg-molsider: 0.609798 val loss: 0.540637
[Epoch 72] ogbg-molsider: 0.590031 test loss: 0.569627
[Epoch 73; Iter     6/   32] train: loss: 0.3901061
[Epoch 73] ogbg-molsider: 0.616226 val loss: 0.550536
[Epoch 73] ogbg-molsider: 0.649221 test loss: 0.525122
[Epoch 74; Iter     4/   32] train: loss: 0.4453194
[Epoch 74] ogbg-molsider: 0.622743 val loss: 0.555116
[Epoch 74] ogbg-molsider: 0.630933 test loss: 0.536103
[Epoch 75; Iter     2/   32] train: loss: 0.4053377
[Epoch 75; Iter    32/   32] train: loss: 0.4851868
[Epoch 75] ogbg-molsider: 0.598023 val loss: 0.594736
[Epoch 75] ogbg-molsider: 0.615170 test loss: 0.543904
[Epoch 76; Iter    30/   32] train: loss: 0.4996437
[Epoch 76] ogbg-molsider: 0.639971 val loss: 0.533944
[Epoch 76] ogbg-molsider: 0.622853 test loss: 0.533485
[Epoch 77; Iter    28/   32] train: loss: 0.4040634
[Epoch 77] ogbg-molsider: 0.590735 val loss: 0.778542
[Epoch 77] ogbg-molsider: 0.619775 test loss: 0.746116
[Epoch 78; Iter    26/   32] train: loss: 0.4028227
[Epoch 78] ogbg-molsider: 0.642304 val loss: 0.540383
[Epoch 78] ogbg-molsider: 0.650064 test loss: 0.530902
[Epoch 79; Iter    24/   32] train: loss: 0.3898572
[Epoch 79] ogbg-molsider: 0.606774 val loss: 0.543149
[Epoch 79] ogbg-molsider: 0.638159 test loss: 0.534739
[Epoch 80; Iter    22/   32] train: loss: 0.4410235
[Epoch 80] ogbg-molsider: 0.639016 val loss: 0.520747
[Epoch 80] ogbg-molsider: 0.626172 test loss: 0.528844
[Epoch 81; Iter    20/   32] train: loss: 0.3937497
[Epoch 81] ogbg-molsider: 0.628735 val loss: 0.564477
[Epoch 81] ogbg-molsider: 0.624317 test loss: 0.574090
[Epoch 82; Iter    18/   32] train: loss: 0.3695656
[Epoch 82] ogbg-molsider: 0.613931 val loss: 0.643685
[Epoch 82] ogbg-molsider: 0.630759 test loss: 0.546042
[Epoch 83; Iter    16/   32] train: loss: 0.3917808
[Epoch 83] ogbg-molsider: 0.641456 val loss: 0.534018
[Epoch 83] ogbg-molsider: 0.635572 test loss: 0.538785
[Epoch 84; Iter    14/   32] train: loss: 0.4193153
[Epoch 84] ogbg-molsider: 0.618671 val loss: 0.539374
[Epoch 37; Iter    18/   27] train: loss: 0.4881462
[Epoch 37] ogbg-molsider: 0.602364 val loss: 0.537232
[Epoch 37] ogbg-molsider: 0.581535 test loss: 0.532679
[Epoch 38; Iter    21/   27] train: loss: 0.5356588
[Epoch 38] ogbg-molsider: 0.597115 val loss: 0.523114
[Epoch 38] ogbg-molsider: 0.596197 test loss: 0.509784
[Epoch 39; Iter    24/   27] train: loss: 0.5203463
[Epoch 39] ogbg-molsider: 0.588381 val loss: 0.534277
[Epoch 39] ogbg-molsider: 0.571797 test loss: 0.525006
[Epoch 40; Iter    27/   27] train: loss: 0.4831726
[Epoch 40] ogbg-molsider: 0.594483 val loss: 0.520183
[Epoch 40] ogbg-molsider: 0.594321 test loss: 0.509129
[Epoch 41] ogbg-molsider: 0.579953 val loss: 0.525404
[Epoch 41] ogbg-molsider: 0.597247 test loss: 0.524342
[Epoch 42; Iter     3/   27] train: loss: 0.4653162
[Epoch 42] ogbg-molsider: 0.600476 val loss: 0.520661
[Epoch 42] ogbg-molsider: 0.586130 test loss: 0.506504
[Epoch 43; Iter     6/   27] train: loss: 0.4855311
[Epoch 43] ogbg-molsider: 0.584141 val loss: 0.516995
[Epoch 43] ogbg-molsider: 0.601404 test loss: 0.509908
[Epoch 44; Iter     9/   27] train: loss: 0.4738638
[Epoch 44] ogbg-molsider: 0.601610 val loss: 0.520502
[Epoch 44] ogbg-molsider: 0.603200 test loss: 0.511338
[Epoch 45; Iter    12/   27] train: loss: 0.4312735
[Epoch 45] ogbg-molsider: 0.612423 val loss: 0.513530
[Epoch 45] ogbg-molsider: 0.615189 test loss: 0.504346
[Epoch 46; Iter    15/   27] train: loss: 0.4474811
[Epoch 46] ogbg-molsider: 0.604165 val loss: 0.520270
[Epoch 46] ogbg-molsider: 0.592826 test loss: 0.508985
[Epoch 47; Iter    18/   27] train: loss: 0.4849531
[Epoch 47] ogbg-molsider: 0.603344 val loss: 0.517741
[Epoch 47] ogbg-molsider: 0.590039 test loss: 0.514934
[Epoch 48; Iter    21/   27] train: loss: 0.4627453
[Epoch 48] ogbg-molsider: 0.605445 val loss: 0.515530
[Epoch 48] ogbg-molsider: 0.600462 test loss: 0.509486
[Epoch 49; Iter    24/   27] train: loss: 0.4385165
[Epoch 49] ogbg-molsider: 0.598070 val loss: 0.517348
[Epoch 49] ogbg-molsider: 0.604244 test loss: 0.507699
[Epoch 50; Iter    27/   27] train: loss: 0.4943843
[Epoch 50] ogbg-molsider: 0.615547 val loss: 0.513224
[Epoch 50] ogbg-molsider: 0.605923 test loss: 0.506040
[Epoch 51] ogbg-molsider: 0.609579 val loss: 0.519544
[Epoch 51] ogbg-molsider: 0.613324 test loss: 0.509894
[Epoch 52; Iter     3/   27] train: loss: 0.4662719
[Epoch 52] ogbg-molsider: 0.611005 val loss: 0.541443
[Epoch 52] ogbg-molsider: 0.597470 test loss: 0.532454
[Epoch 53; Iter     6/   27] train: loss: 0.4494902
[Epoch 53] ogbg-molsider: 0.604092 val loss: 0.518825
[Epoch 53] ogbg-molsider: 0.607334 test loss: 0.516956
[Epoch 54; Iter     9/   27] train: loss: 0.4785096
[Epoch 54] ogbg-molsider: 0.611958 val loss: 0.546744
[Epoch 54] ogbg-molsider: 0.626587 test loss: 0.508957
[Epoch 55; Iter    12/   27] train: loss: 0.5188392
[Epoch 55] ogbg-molsider: 0.621117 val loss: 0.510239
[Epoch 55] ogbg-molsider: 0.618259 test loss: 0.507242
[Epoch 56; Iter    15/   27] train: loss: 0.4860765
[Epoch 56] ogbg-molsider: 0.611569 val loss: 0.512509
[Epoch 56] ogbg-molsider: 0.626418 test loss: 0.502046
[Epoch 57; Iter    18/   27] train: loss: 0.4505489
[Epoch 57] ogbg-molsider: 0.623316 val loss: 0.515379
[Epoch 57] ogbg-molsider: 0.629077 test loss: 0.499154
[Epoch 58; Iter    21/   27] train: loss: 0.4366015
[Epoch 58] ogbg-molsider: 0.627269 val loss: 0.512894
[Epoch 58] ogbg-molsider: 0.628127 test loss: 0.500830
[Epoch 59; Iter    24/   27] train: loss: 0.4842086
[Epoch 59] ogbg-molsider: 0.611270 val loss: 0.516966
[Epoch 59] ogbg-molsider: 0.570541 test loss: 0.519274
[Epoch 60; Iter    27/   27] train: loss: 0.5328625
[Epoch 60] ogbg-molsider: 0.640464 val loss: 0.516893
[Epoch 60] ogbg-molsider: 0.629406 test loss: 0.509590
[Epoch 61] ogbg-molsider: 0.607705 val loss: 0.598270
[Epoch 61] ogbg-molsider: 0.627485 test loss: 0.619261
[Epoch 62; Iter     3/   27] train: loss: 0.4382704
[Epoch 62] ogbg-molsider: 0.632516 val loss: 0.509826
[Epoch 62] ogbg-molsider: 0.603194 test loss: 0.505653
[Epoch 63; Iter     6/   27] train: loss: 0.4169127
[Epoch 63] ogbg-molsider: 0.633761 val loss: 0.514385
[Epoch 63] ogbg-molsider: 0.604193 test loss: 0.505081
[Epoch 64; Iter     9/   27] train: loss: 0.4359652
[Epoch 64] ogbg-molsider: 0.605784 val loss: 0.523213
[Epoch 64] ogbg-molsider: 0.621097 test loss: 0.522891
[Epoch 65; Iter    12/   27] train: loss: 0.4863466
[Epoch 65] ogbg-molsider: 0.633258 val loss: 0.526451
[Epoch 65] ogbg-molsider: 0.641303 test loss: 0.517345
[Epoch 66; Iter    15/   27] train: loss: 0.4609430
[Epoch 66] ogbg-molsider: 0.620813 val loss: 0.516984
[Epoch 66] ogbg-molsider: 0.611946 test loss: 0.513703
[Epoch 67; Iter    18/   27] train: loss: 0.4196646
[Epoch 67] ogbg-molsider: 0.635105 val loss: 0.520762
[Epoch 67] ogbg-molsider: 0.622038 test loss: 0.509507
[Epoch 68; Iter    21/   27] train: loss: 0.4474956
[Epoch 68] ogbg-molsider: 0.645241 val loss: 0.515477
[Epoch 68] ogbg-molsider: 0.613486 test loss: 0.513306
[Epoch 69; Iter    24/   27] train: loss: 0.4375032
[Epoch 69] ogbg-molsider: 0.613501 val loss: 0.534496
[Epoch 69] ogbg-molsider: 0.630656 test loss: 0.513565
[Epoch 70; Iter    27/   27] train: loss: 0.4022186
[Epoch 70] ogbg-molsider: 0.625924 val loss: 0.535528
[Epoch 70] ogbg-molsider: 0.598272 test loss: 0.531923
[Epoch 71] ogbg-molsider: 0.628050 val loss: 0.530786
[Epoch 71] ogbg-molsider: 0.641250 test loss: 0.514018
[Epoch 72; Iter     3/   27] train: loss: 0.4166921
[Epoch 72] ogbg-molsider: 0.613721 val loss: 0.530884
[Epoch 72] ogbg-molsider: 0.621723 test loss: 0.510638
[Epoch 73; Iter     6/   27] train: loss: 0.4241099
[Epoch 73] ogbg-molsider: 0.613484 val loss: 0.534387
[Epoch 73] ogbg-molsider: 0.590660 test loss: 0.548966
[Epoch 74; Iter     9/   27] train: loss: 0.4493162
[Epoch 74] ogbg-molsider: 0.637017 val loss: 0.529267
[Epoch 74] ogbg-molsider: 0.616723 test loss: 0.529570
[Epoch 75; Iter    12/   27] train: loss: 0.3947177
[Epoch 75] ogbg-molsider: 0.635010 val loss: 0.554901
[Epoch 75] ogbg-molsider: 0.611655 test loss: 0.553871
[Epoch 76; Iter    15/   27] train: loss: 0.3892971
[Epoch 76] ogbg-molsider: 0.625936 val loss: 0.617101
[Epoch 76] ogbg-molsider: 0.660314 test loss: 0.548206
[Epoch 77; Iter    18/   27] train: loss: 0.4533787
[Epoch 77] ogbg-molsider: 0.609813 val loss: 0.559965
[Epoch 77] ogbg-molsider: 0.614205 test loss: 0.562467
[Epoch 78; Iter    21/   27] train: loss: 0.4470605
[Epoch 78] ogbg-molsider: 0.615761 val loss: 0.650692
[Epoch 78] ogbg-molsider: 0.627900 test loss: 0.541753
[Epoch 79; Iter    24/   27] train: loss: 0.3812152
[Epoch 79] ogbg-molsider: 0.645673 val loss: 0.547915
[Epoch 79] ogbg-molsider: 0.622622 test loss: 0.538258
[Epoch 80; Iter    27/   27] train: loss: 0.3667035
[Epoch 80] ogbg-molsider: 0.626092 val loss: 0.547587
[Epoch 80] ogbg-molsider: 0.645337 test loss: 0.517331
[Epoch 81] ogbg-molsider: 0.594784 val loss: 0.651587
[Epoch 81] ogbg-molsider: 0.602608 test loss: 0.626970
[Epoch 82; Iter     3/   27] train: loss: 0.3399603
[Epoch 82] ogbg-molsider: 0.611468 val loss: 0.565899
[Epoch 82] ogbg-molsider: 0.595812 test loss: 0.558722
[Epoch 83; Iter     6/   27] train: loss: 0.3998024
[Epoch 83] ogbg-molsider: 0.621477 val loss: 0.545866
[Epoch 83] ogbg-molsider: 0.615783 test loss: 0.535118
[Epoch 84; Iter     9/   27] train: loss: 0.3871144
[Epoch 84] ogbg-molsider: 0.607677 val loss: 0.578603
[Epoch 84] ogbg-molsider: 0.598432 test loss: 0.676028
[Epoch 85; Iter    12/   27] train: loss: 0.3864656
[Epoch 85] ogbg-molsider: 0.632117 val loss: 0.549441
[Epoch 85] ogbg-molsider: 0.617427 test loss: 0.545798
[Epoch 86; Iter    15/   27] train: loss: 0.4247751
[Epoch 86] ogbg-molsider: 0.612840 val loss: 0.551758
[Epoch 86] ogbg-molsider: 0.642127 test loss: 0.520155
[Epoch 87; Iter    18/   27] train: loss: 0.3824469
[Epoch 87] ogbg-molsider: 0.599273 val loss: 0.565856
[Epoch 87] ogbg-molsider: 0.624263 test loss: 0.532736
[Epoch 88; Iter    21/   27] train: loss: 0.3589344
[Epoch 88] ogbg-molsider: 0.604920 val loss: 0.571674
[Epoch 88] ogbg-molsider: 0.613383 test loss: 0.544464
[Epoch 89; Iter    24/   27] train: loss: 0.4308852
[Epoch 37; Iter    18/   27] train: loss: 0.5448158
[Epoch 37] ogbg-molsider: 0.581808 val loss: 0.533232
[Epoch 37] ogbg-molsider: 0.584473 test loss: 0.537503
[Epoch 38; Iter    21/   27] train: loss: 0.4889880
[Epoch 38] ogbg-molsider: 0.603013 val loss: 0.528030
[Epoch 38] ogbg-molsider: 0.584951 test loss: 0.513642
[Epoch 39; Iter    24/   27] train: loss: 0.4736625
[Epoch 39] ogbg-molsider: 0.597585 val loss: 0.520805
[Epoch 39] ogbg-molsider: 0.600228 test loss: 0.505010
[Epoch 40; Iter    27/   27] train: loss: 0.5429754
[Epoch 40] ogbg-molsider: 0.609067 val loss: 0.518490
[Epoch 40] ogbg-molsider: 0.599900 test loss: 0.505046
[Epoch 41] ogbg-molsider: 0.609765 val loss: 0.518000
[Epoch 41] ogbg-molsider: 0.574711 test loss: 0.510442
[Epoch 42; Iter     3/   27] train: loss: 0.4894934
[Epoch 42] ogbg-molsider: 0.617738 val loss: 0.520142
[Epoch 42] ogbg-molsider: 0.589679 test loss: 0.512037
[Epoch 43; Iter     6/   27] train: loss: 0.5003703
[Epoch 43] ogbg-molsider: 0.615653 val loss: 0.520114
[Epoch 43] ogbg-molsider: 0.586390 test loss: 0.514415
[Epoch 44; Iter     9/   27] train: loss: 0.4607946
[Epoch 44] ogbg-molsider: 0.584480 val loss: 0.526376
[Epoch 44] ogbg-molsider: 0.588138 test loss: 0.511555
[Epoch 45; Iter    12/   27] train: loss: 0.4456079
[Epoch 45] ogbg-molsider: 0.594782 val loss: 0.519778
[Epoch 45] ogbg-molsider: 0.582126 test loss: 0.517373
[Epoch 46; Iter    15/   27] train: loss: 0.4716800
[Epoch 46] ogbg-molsider: 0.609508 val loss: 0.520274
[Epoch 46] ogbg-molsider: 0.596183 test loss: 0.509649
[Epoch 47; Iter    18/   27] train: loss: 0.5181096
[Epoch 47] ogbg-molsider: 0.631547 val loss: 0.514207
[Epoch 47] ogbg-molsider: 0.588188 test loss: 0.512885
[Epoch 48; Iter    21/   27] train: loss: 0.5235041
[Epoch 48] ogbg-molsider: 0.624915 val loss: 0.514744
[Epoch 48] ogbg-molsider: 0.604093 test loss: 0.507727
[Epoch 49; Iter    24/   27] train: loss: 0.4512855
[Epoch 49] ogbg-molsider: 0.619486 val loss: 0.515004
[Epoch 49] ogbg-molsider: 0.605437 test loss: 0.512141
[Epoch 50; Iter    27/   27] train: loss: 0.5042861
[Epoch 50] ogbg-molsider: 0.627960 val loss: 0.510532
[Epoch 50] ogbg-molsider: 0.606116 test loss: 0.513942
[Epoch 51] ogbg-molsider: 0.604527 val loss: 0.516124
[Epoch 51] ogbg-molsider: 0.588456 test loss: 0.516892
[Epoch 52; Iter     3/   27] train: loss: 0.4505981
[Epoch 52] ogbg-molsider: 0.613966 val loss: 0.513536
[Epoch 52] ogbg-molsider: 0.612719 test loss: 0.506298
[Epoch 53; Iter     6/   27] train: loss: 0.4422584
[Epoch 53] ogbg-molsider: 0.628227 val loss: 0.513374
[Epoch 53] ogbg-molsider: 0.623023 test loss: 0.499006
[Epoch 54; Iter     9/   27] train: loss: 0.4236084
[Epoch 54] ogbg-molsider: 0.609719 val loss: 0.515022
[Epoch 54] ogbg-molsider: 0.630597 test loss: 0.498901
[Epoch 55; Iter    12/   27] train: loss: 0.4808531
[Epoch 55] ogbg-molsider: 0.605005 val loss: 0.688694
[Epoch 55] ogbg-molsider: 0.625186 test loss: 0.517654
[Epoch 56; Iter    15/   27] train: loss: 0.4286926
[Epoch 56] ogbg-molsider: 0.613661 val loss: 0.525235
[Epoch 56] ogbg-molsider: 0.613124 test loss: 0.512225
[Epoch 57; Iter    18/   27] train: loss: 0.4304136
[Epoch 57] ogbg-molsider: 0.614517 val loss: 0.537288
[Epoch 57] ogbg-molsider: 0.614516 test loss: 0.521211
[Epoch 58; Iter    21/   27] train: loss: 0.4596098
[Epoch 58] ogbg-molsider: 0.639106 val loss: 0.514054
[Epoch 58] ogbg-molsider: 0.624707 test loss: 0.505804
[Epoch 59; Iter    24/   27] train: loss: 0.4608539
[Epoch 59] ogbg-molsider: 0.625618 val loss: 0.527098
[Epoch 59] ogbg-molsider: 0.622557 test loss: 0.512895
[Epoch 60; Iter    27/   27] train: loss: 0.4279453
[Epoch 60] ogbg-molsider: 0.603866 val loss: 0.655507
[Epoch 60] ogbg-molsider: 0.594067 test loss: 1.186489
[Epoch 61] ogbg-molsider: 0.623384 val loss: 0.526385
[Epoch 61] ogbg-molsider: 0.609942 test loss: 0.528618
[Epoch 62; Iter     3/   27] train: loss: 0.4750639
[Epoch 62] ogbg-molsider: 0.604088 val loss: 0.534288
[Epoch 62] ogbg-molsider: 0.624144 test loss: 0.515257
[Epoch 63; Iter     6/   27] train: loss: 0.4875231
[Epoch 63] ogbg-molsider: 0.605480 val loss: 0.518100
[Epoch 63] ogbg-molsider: 0.583056 test loss: 0.521072
[Epoch 64; Iter     9/   27] train: loss: 0.4321122
[Epoch 64] ogbg-molsider: 0.633074 val loss: 0.521642
[Epoch 64] ogbg-molsider: 0.634000 test loss: 0.501223
[Epoch 65; Iter    12/   27] train: loss: 0.4695558
[Epoch 65] ogbg-molsider: 0.632322 val loss: 0.516819
[Epoch 65] ogbg-molsider: 0.615462 test loss: 0.513366
[Epoch 66; Iter    15/   27] train: loss: 0.4295166
[Epoch 66] ogbg-molsider: 0.613410 val loss: 0.521940
[Epoch 66] ogbg-molsider: 0.618185 test loss: 0.507723
[Epoch 67; Iter    18/   27] train: loss: 0.4668140
[Epoch 67] ogbg-molsider: 0.599263 val loss: 0.539510
[Epoch 67] ogbg-molsider: 0.595271 test loss: 0.534011
[Epoch 68; Iter    21/   27] train: loss: 0.4419359
[Epoch 68] ogbg-molsider: 0.624154 val loss: 0.548974
[Epoch 68] ogbg-molsider: 0.613958 test loss: 0.542786
[Epoch 69; Iter    24/   27] train: loss: 0.4977799
[Epoch 69] ogbg-molsider: 0.586386 val loss: 0.574082
[Epoch 69] ogbg-molsider: 0.618287 test loss: 0.622584
[Epoch 70; Iter    27/   27] train: loss: 0.4521348
[Epoch 70] ogbg-molsider: 0.583365 val loss: 0.574614
[Epoch 70] ogbg-molsider: 0.612777 test loss: 0.538419
[Epoch 71] ogbg-molsider: 0.635804 val loss: 0.532342
[Epoch 71] ogbg-molsider: 0.641543 test loss: 0.514933
[Epoch 72; Iter     3/   27] train: loss: 0.3809048
[Epoch 72] ogbg-molsider: 0.619773 val loss: 0.540872
[Epoch 72] ogbg-molsider: 0.630149 test loss: 0.538615
[Epoch 73; Iter     6/   27] train: loss: 0.3711063
[Epoch 73] ogbg-molsider: 0.609518 val loss: 0.533457
[Epoch 73] ogbg-molsider: 0.624040 test loss: 0.516176
[Epoch 74; Iter     9/   27] train: loss: 0.4059598
[Epoch 74] ogbg-molsider: 0.597013 val loss: 0.544684
[Epoch 74] ogbg-molsider: 0.614237 test loss: 0.513957
[Epoch 75; Iter    12/   27] train: loss: 0.3748589
[Epoch 75] ogbg-molsider: 0.596938 val loss: 0.554302
[Epoch 75] ogbg-molsider: 0.608198 test loss: 0.531545
[Epoch 76; Iter    15/   27] train: loss: 0.4159899
[Epoch 76] ogbg-molsider: 0.614621 val loss: 0.558925
[Epoch 76] ogbg-molsider: 0.650412 test loss: 0.519646
[Epoch 77; Iter    18/   27] train: loss: 0.4082524
[Epoch 77] ogbg-molsider: 0.604356 val loss: 0.569977
[Epoch 77] ogbg-molsider: 0.644303 test loss: 0.529387
[Epoch 78; Iter    21/   27] train: loss: 0.3901366
[Epoch 78] ogbg-molsider: 0.583380 val loss: 0.586733
[Epoch 78] ogbg-molsider: 0.619122 test loss: 0.563752
[Epoch 79; Iter    24/   27] train: loss: 0.4393177
[Epoch 79] ogbg-molsider: 0.602884 val loss: 0.570122
[Epoch 79] ogbg-molsider: 0.598117 test loss: 0.588167
[Epoch 80; Iter    27/   27] train: loss: 0.3809076
[Epoch 80] ogbg-molsider: 0.605802 val loss: 0.569782
[Epoch 80] ogbg-molsider: 0.634110 test loss: 0.530613
[Epoch 81] ogbg-molsider: 0.583625 val loss: 0.588216
[Epoch 81] ogbg-molsider: 0.629166 test loss: 0.544125
[Epoch 82; Iter     3/   27] train: loss: 0.4474061
[Epoch 82] ogbg-molsider: 0.591792 val loss: 0.603987
[Epoch 82] ogbg-molsider: 0.610964 test loss: 0.572238
[Epoch 83; Iter     6/   27] train: loss: 0.3874819
[Epoch 83] ogbg-molsider: 0.583421 val loss: 0.583062
[Epoch 83] ogbg-molsider: 0.634081 test loss: 0.532388
[Epoch 84; Iter     9/   27] train: loss: 0.3802490
[Epoch 84] ogbg-molsider: 0.610454 val loss: 0.569587
[Epoch 84] ogbg-molsider: 0.622231 test loss: 0.547153
[Epoch 85; Iter    12/   27] train: loss: 0.3848987
[Epoch 85] ogbg-molsider: 0.602402 val loss: 0.569776
[Epoch 85] ogbg-molsider: 0.632626 test loss: 0.537327
[Epoch 86; Iter    15/   27] train: loss: 0.3801670
[Epoch 86] ogbg-molsider: 0.590265 val loss: 0.585648
[Epoch 86] ogbg-molsider: 0.625458 test loss: 0.546899
[Epoch 87; Iter    18/   27] train: loss: 0.3852491
[Epoch 87] ogbg-molsider: 0.612045 val loss: 0.578413
[Epoch 87] ogbg-molsider: 0.641939 test loss: 0.551220
[Epoch 88; Iter    21/   27] train: loss: 0.3554187
[Epoch 88] ogbg-molsider: 0.613115 val loss: 0.585998
[Epoch 88] ogbg-molsider: 0.622106 test loss: 0.558120
[Epoch 89; Iter    24/   27] train: loss: 0.3809381
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 89] ogbg-molsider: 0.606661 val loss: 0.542529
[Epoch 89] ogbg-molsider: 0.627155 test loss: 0.521369
[Epoch 90; Iter    27/   27] train: loss: 0.4002824
[Epoch 90] ogbg-molsider: 0.609103 val loss: 0.546908
[Epoch 90] ogbg-molsider: 0.622835 test loss: 0.520799
[Epoch 91] ogbg-molsider: 0.599453 val loss: 0.539001
[Epoch 91] ogbg-molsider: 0.606288 test loss: 0.522902
[Epoch 92; Iter     3/   27] train: loss: 0.4174671
[Epoch 92] ogbg-molsider: 0.614691 val loss: 0.566150
[Epoch 92] ogbg-molsider: 0.628395 test loss: 0.533279
[Epoch 93; Iter     6/   27] train: loss: 0.3818857
[Epoch 93] ogbg-molsider: 0.609406 val loss: 0.544736
[Epoch 93] ogbg-molsider: 0.629314 test loss: 0.506492
[Epoch 94; Iter     9/   27] train: loss: 0.4150366
[Epoch 94] ogbg-molsider: 0.600461 val loss: 0.601017
[Epoch 94] ogbg-molsider: 0.636256 test loss: 0.545559
[Epoch 95; Iter    12/   27] train: loss: 0.4474912
[Epoch 95] ogbg-molsider: 0.614784 val loss: 0.582160
[Epoch 95] ogbg-molsider: 0.635318 test loss: 0.540576
[Epoch 96; Iter    15/   27] train: loss: 0.4037236
[Epoch 96] ogbg-molsider: 0.616381 val loss: 0.560255
[Epoch 96] ogbg-molsider: 0.643446 test loss: 0.553043
[Epoch 97; Iter    18/   27] train: loss: 0.5357147
[Epoch 97] ogbg-molsider: 0.583174 val loss: 0.594591
[Epoch 97] ogbg-molsider: 0.606264 test loss: 0.569555
[Epoch 98; Iter    21/   27] train: loss: 0.4424000
[Epoch 98] ogbg-molsider: 0.586692 val loss: 0.551871
[Epoch 98] ogbg-molsider: 0.632760 test loss: 0.518136
[Epoch 99; Iter    24/   27] train: loss: 0.4301968
[Epoch 99] ogbg-molsider: 0.575326 val loss: 0.573112
[Epoch 99] ogbg-molsider: 0.610159 test loss: 0.531904
[Epoch 100; Iter    27/   27] train: loss: 0.3818124
[Epoch 100] ogbg-molsider: 0.597922 val loss: 0.634888
[Epoch 100] ogbg-molsider: 0.617065 test loss: 0.529290
[Epoch 101] ogbg-molsider: 0.597310 val loss: 0.588965
[Epoch 101] ogbg-molsider: 0.596957 test loss: 0.572696
[Epoch 102; Iter     3/   27] train: loss: 0.4725096
[Epoch 102] ogbg-molsider: 0.593588 val loss: 0.552027
[Epoch 102] ogbg-molsider: 0.598072 test loss: 0.541236
[Epoch 103; Iter     6/   27] train: loss: 0.4717407
[Epoch 103] ogbg-molsider: 0.599726 val loss: 0.538859
[Epoch 103] ogbg-molsider: 0.625716 test loss: 0.517864
[Epoch 104; Iter     9/   27] train: loss: 0.5074903
[Epoch 104] ogbg-molsider: 0.599157 val loss: 0.530144
[Epoch 104] ogbg-molsider: 0.627923 test loss: 0.508728
[Epoch 105; Iter    12/   27] train: loss: 0.4387601
[Epoch 105] ogbg-molsider: 0.595382 val loss: 0.568131
[Epoch 105] ogbg-molsider: 0.616656 test loss: 0.541160
[Epoch 106; Iter    15/   27] train: loss: 0.4327438
[Epoch 106] ogbg-molsider: 0.593966 val loss: 0.547975
[Epoch 106] ogbg-molsider: 0.632954 test loss: 0.514461
[Epoch 107; Iter    18/   27] train: loss: 0.4196580
[Epoch 107] ogbg-molsider: 0.592528 val loss: 0.554806
[Epoch 107] ogbg-molsider: 0.628074 test loss: 0.516285
[Epoch 108; Iter    21/   27] train: loss: 0.3885774
[Epoch 108] ogbg-molsider: 0.584221 val loss: 0.562293
[Epoch 108] ogbg-molsider: 0.631029 test loss: 0.517577
[Epoch 109; Iter    24/   27] train: loss: 0.4138485
[Epoch 109] ogbg-molsider: 0.610166 val loss: 0.558870
[Epoch 109] ogbg-molsider: 0.611330 test loss: 0.536046
[Epoch 110; Iter    27/   27] train: loss: 0.3721264
[Epoch 110] ogbg-molsider: 0.593207 val loss: 0.560729
[Epoch 110] ogbg-molsider: 0.619856 test loss: 0.538171
[Epoch 111] ogbg-molsider: 0.593362 val loss: 0.577526
[Epoch 111] ogbg-molsider: 0.624972 test loss: 0.535143
[Epoch 112; Iter     3/   27] train: loss: 0.3868862
[Epoch 112] ogbg-molsider: 0.578594 val loss: 0.583645
[Epoch 112] ogbg-molsider: 0.598012 test loss: 0.560313
[Epoch 113; Iter     6/   27] train: loss: 0.4286833
[Epoch 113] ogbg-molsider: 0.575044 val loss: 0.586936
[Epoch 113] ogbg-molsider: 0.615917 test loss: 0.536327
[Epoch 114; Iter     9/   27] train: loss: 0.4419974
[Epoch 114] ogbg-molsider: 0.600924 val loss: 0.586636
[Epoch 114] ogbg-molsider: 0.619916 test loss: 0.551072
[Epoch 115; Iter    12/   27] train: loss: 0.3761449
[Epoch 115] ogbg-molsider: 0.601068 val loss: 0.602897
[Epoch 115] ogbg-molsider: 0.621983 test loss: 0.556469
[Epoch 116; Iter    15/   27] train: loss: 0.3650301
[Epoch 116] ogbg-molsider: 0.599343 val loss: 0.582157
[Epoch 116] ogbg-molsider: 0.595488 test loss: 0.556741
[Epoch 117; Iter    18/   27] train: loss: 0.3607686
[Epoch 117] ogbg-molsider: 0.590460 val loss: 0.663151
[Epoch 117] ogbg-molsider: 0.599967 test loss: 0.587947
[Epoch 118; Iter    21/   27] train: loss: 0.3729415
[Epoch 118] ogbg-molsider: 0.591295 val loss: 0.594563
[Epoch 118] ogbg-molsider: 0.596641 test loss: 0.577729
[Epoch 119; Iter    24/   27] train: loss: 0.3777677
[Epoch 119] ogbg-molsider: 0.577984 val loss: 0.592221
[Epoch 119] ogbg-molsider: 0.598566 test loss: 0.563215
[Epoch 120; Iter    27/   27] train: loss: 0.3541653
[Epoch 120] ogbg-molsider: 0.597265 val loss: 0.589644
[Epoch 120] ogbg-molsider: 0.602932 test loss: 0.561480
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 56.
Statistics on  val_best_checkpoint
mean_pred: 0.47835400700569153
std_pred: 2.7622323036193848
mean_targets: 0.5445094108581543
std_targets: 0.4980473220348358
prcauc: 0.6330243438439372
rocauc: 0.6349364291159482
ogbg-molsider: 0.6349364291159482
OGBNanLabelBCEWithLogitsLoss: 0.5339640511406792
Statistics on  test
mean_pred: 0.3732583224773407
std_pred: 1.8414993286132812
mean_targets: 0.5734265446662903
std_targets: 0.4946112036705017
prcauc: 0.6596679259231877
rocauc: 0.6382056396194108
ogbg-molsider: 0.6382056396194108
OGBNanLabelBCEWithLogitsLoss: 0.4986107283168369
Statistics on  train
mean_pred: 0.5506758689880371
std_pred: 3.8839876651763916
mean_targets: 0.5732952356338501
std_targets: 0.4946092963218689
prcauc: 0.6947339421565779
rocauc: 0.7050962539781884
ogbg-molsider: 0.7050962539781884
OGBNanLabelBCEWithLogitsLoss: 0.4811950789557563
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 89] ogbg-molsider: 0.596937 val loss: 0.612818
[Epoch 89] ogbg-molsider: 0.633893 test loss: 0.560436
[Epoch 90; Iter    27/   27] train: loss: 0.3691526
[Epoch 90] ogbg-molsider: 0.610745 val loss: 0.604327
[Epoch 90] ogbg-molsider: 0.628242 test loss: 0.564462
[Epoch 91] ogbg-molsider: 0.621320 val loss: 0.611855
[Epoch 91] ogbg-molsider: 0.638681 test loss: 0.566457
[Epoch 92; Iter     3/   27] train: loss: 0.3073080
[Epoch 92] ogbg-molsider: 0.604443 val loss: 0.586553
[Epoch 92] ogbg-molsider: 0.615327 test loss: 0.553520
[Epoch 93; Iter     6/   27] train: loss: 0.3446415
[Epoch 93] ogbg-molsider: 0.609246 val loss: 0.597101
[Epoch 93] ogbg-molsider: 0.630981 test loss: 0.559409
[Epoch 94; Iter     9/   27] train: loss: 0.3333660
[Epoch 94] ogbg-molsider: 0.599508 val loss: 0.623993
[Epoch 94] ogbg-molsider: 0.624461 test loss: 0.584761
[Epoch 95; Iter    12/   27] train: loss: 0.3240541
[Epoch 95] ogbg-molsider: 0.616123 val loss: 0.610600
[Epoch 95] ogbg-molsider: 0.626492 test loss: 0.577870
[Epoch 96; Iter    15/   27] train: loss: 0.3207142
[Epoch 96] ogbg-molsider: 0.595188 val loss: 0.639083
[Epoch 96] ogbg-molsider: 0.630422 test loss: 0.589647
[Epoch 97; Iter    18/   27] train: loss: 0.2852215
[Epoch 97] ogbg-molsider: 0.617846 val loss: 0.607175
[Epoch 97] ogbg-molsider: 0.645046 test loss: 0.553096
[Epoch 98; Iter    21/   27] train: loss: 0.3167407
[Epoch 98] ogbg-molsider: 0.607298 val loss: 0.630228
[Epoch 98] ogbg-molsider: 0.641125 test loss: 0.561052
[Epoch 99; Iter    24/   27] train: loss: 0.2743211
[Epoch 99] ogbg-molsider: 0.609703 val loss: 0.615371
[Epoch 99] ogbg-molsider: 0.639149 test loss: 0.563025
[Epoch 100; Iter    27/   27] train: loss: 0.3186800
[Epoch 100] ogbg-molsider: 0.615703 val loss: 0.616714
[Epoch 100] ogbg-molsider: 0.635934 test loss: 0.570490
[Epoch 101] ogbg-molsider: 0.623938 val loss: 0.624073
[Epoch 101] ogbg-molsider: 0.638694 test loss: 0.578661
[Epoch 102; Iter     3/   27] train: loss: 0.2796831
[Epoch 102] ogbg-molsider: 0.608272 val loss: 0.635151
[Epoch 102] ogbg-molsider: 0.642766 test loss: 0.574540
[Epoch 103; Iter     6/   27] train: loss: 0.2796033
[Epoch 103] ogbg-molsider: 0.615416 val loss: 0.637044
[Epoch 103] ogbg-molsider: 0.636914 test loss: 0.600611
[Epoch 104; Iter     9/   27] train: loss: 0.2850899
[Epoch 104] ogbg-molsider: 0.618905 val loss: 0.625333
[Epoch 104] ogbg-molsider: 0.637449 test loss: 0.577012
[Epoch 105; Iter    12/   27] train: loss: 0.2961770
[Epoch 105] ogbg-molsider: 0.616157 val loss: 0.647760
[Epoch 105] ogbg-molsider: 0.641091 test loss: 0.595315
[Epoch 106; Iter    15/   27] train: loss: 0.2895677
[Epoch 106] ogbg-molsider: 0.620699 val loss: 0.640661
[Epoch 106] ogbg-molsider: 0.634000 test loss: 0.599765
[Epoch 107; Iter    18/   27] train: loss: 0.2705054
[Epoch 107] ogbg-molsider: 0.625392 val loss: 0.628108
[Epoch 107] ogbg-molsider: 0.635620 test loss: 0.597385
[Epoch 108; Iter    21/   27] train: loss: 0.2871310
[Epoch 108] ogbg-molsider: 0.618580 val loss: 0.640529
[Epoch 108] ogbg-molsider: 0.636717 test loss: 0.598775
[Epoch 109; Iter    24/   27] train: loss: 0.2888381
[Epoch 109] ogbg-molsider: 0.616474 val loss: 0.646775
[Epoch 109] ogbg-molsider: 0.637472 test loss: 0.607995
[Epoch 110; Iter    27/   27] train: loss: 0.2325093
[Epoch 110] ogbg-molsider: 0.625183 val loss: 0.664373
[Epoch 110] ogbg-molsider: 0.641796 test loss: 0.620258
[Epoch 111] ogbg-molsider: 0.616384 val loss: 0.670329
[Epoch 111] ogbg-molsider: 0.629029 test loss: 0.635005
[Epoch 112; Iter     3/   27] train: loss: 0.2664315
[Epoch 112] ogbg-molsider: 0.620945 val loss: 0.661455
[Epoch 112] ogbg-molsider: 0.633348 test loss: 0.628642
[Epoch 113; Iter     6/   27] train: loss: 0.2622210
[Epoch 113] ogbg-molsider: 0.613060 val loss: 0.673641
[Epoch 113] ogbg-molsider: 0.636217 test loss: 0.617892
[Epoch 114; Iter     9/   27] train: loss: 0.2572958
[Epoch 114] ogbg-molsider: 0.621555 val loss: 0.676650
[Epoch 114] ogbg-molsider: 0.639214 test loss: 0.618941
[Epoch 115; Iter    12/   27] train: loss: 0.2674689
[Epoch 115] ogbg-molsider: 0.615833 val loss: 0.658338
[Epoch 115] ogbg-molsider: 0.643289 test loss: 0.618801
[Epoch 116; Iter    15/   27] train: loss: 0.2595367
[Epoch 116] ogbg-molsider: 0.625257 val loss: 0.667350
[Epoch 116] ogbg-molsider: 0.633889 test loss: 0.628917
[Epoch 117; Iter    18/   27] train: loss: 0.2664215
[Epoch 117] ogbg-molsider: 0.605533 val loss: 0.663076
[Epoch 117] ogbg-molsider: 0.629430 test loss: 0.613344
[Epoch 118; Iter    21/   27] train: loss: 0.2836264
[Epoch 118] ogbg-molsider: 0.622496 val loss: 0.663519
[Epoch 118] ogbg-molsider: 0.627651 test loss: 0.640300
[Epoch 119; Iter    24/   27] train: loss: 0.2532475
[Epoch 119] ogbg-molsider: 0.614109 val loss: 0.677581
[Epoch 119] ogbg-molsider: 0.627011 test loss: 0.637915
[Epoch 120; Iter    27/   27] train: loss: 0.2613683
[Epoch 120] ogbg-molsider: 0.605271 val loss: 0.676625
[Epoch 120] ogbg-molsider: 0.634908 test loss: 0.624129
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 58.
Statistics on  val_best_checkpoint
mean_pred: 0.40823543071746826
std_pred: 1.8774889707565308
mean_targets: 0.5445094108581543
std_targets: 0.4980473220348358
prcauc: 0.6335842399496135
rocauc: 0.6391057305262876
ogbg-molsider: 0.6391057305262876
OGBNanLabelBCEWithLogitsLoss: 0.5140535202291276
Statistics on  test
mean_pred: 0.46571922302246094
std_pred: 1.937063455581665
mean_targets: 0.5734265446662903
std_targets: 0.4946112036705017
prcauc: 0.6473591855143007
rocauc: 0.6247071060733334
ogbg-molsider: 0.6247071060733334
OGBNanLabelBCEWithLogitsLoss: 0.5058043897151947
Statistics on  train
mean_pred: 0.4651941657066345
std_pred: 1.9440141916275024
mean_targets: 0.5732952356338501
std_targets: 0.4946093261241913
prcauc: 0.6967919823985009
rocauc: 0.7078452263723796
ogbg-molsider: 0.7078452263723796
OGBNanLabelBCEWithLogitsLoss: 0.46272755993737114
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 84] ogbg-molsider: 0.648675 test loss: 0.523751
[Epoch 85; Iter    12/   32] train: loss: 0.3855911
[Epoch 85] ogbg-molsider: 0.609990 val loss: 0.689643
[Epoch 85] ogbg-molsider: 0.634010 test loss: 0.563041
[Epoch 86; Iter    10/   32] train: loss: 0.3822325
[Epoch 86] ogbg-molsider: 0.633617 val loss: 0.540147
[Epoch 86] ogbg-molsider: 0.630075 test loss: 0.538546
[Epoch 87; Iter     8/   32] train: loss: 0.3756799
[Epoch 87] ogbg-molsider: 0.625274 val loss: 0.618915
[Epoch 87] ogbg-molsider: 0.630964 test loss: 0.590571
[Epoch 88; Iter     6/   32] train: loss: 0.3591903
[Epoch 88] ogbg-molsider: 0.615500 val loss: 0.560857
[Epoch 88] ogbg-molsider: 0.631253 test loss: 0.550705
[Epoch 89; Iter     4/   32] train: loss: 0.3437558
[Epoch 89] ogbg-molsider: 0.647719 val loss: 0.549529
[Epoch 89] ogbg-molsider: 0.640149 test loss: 0.559648
[Epoch 90; Iter     2/   32] train: loss: 0.3334902
[Epoch 90; Iter    32/   32] train: loss: 0.4815565
[Epoch 90] ogbg-molsider: 0.625643 val loss: 0.600400
[Epoch 90] ogbg-molsider: 0.638782 test loss: 0.543314
[Epoch 91; Iter    30/   32] train: loss: 0.3623979
[Epoch 91] ogbg-molsider: 0.622694 val loss: 0.596326
[Epoch 91] ogbg-molsider: 0.630884 test loss: 0.572445
[Epoch 92; Iter    28/   32] train: loss: 0.3679540
[Epoch 92] ogbg-molsider: 0.633622 val loss: 0.560558
[Epoch 92] ogbg-molsider: 0.628498 test loss: 0.585397
[Epoch 93; Iter    26/   32] train: loss: 0.3505129
[Epoch 93] ogbg-molsider: 0.632756 val loss: 0.617466
[Epoch 93] ogbg-molsider: 0.650837 test loss: 0.552927
[Epoch 94; Iter    24/   32] train: loss: 0.3441688
[Epoch 94] ogbg-molsider: 0.627629 val loss: 0.615810
[Epoch 94] ogbg-molsider: 0.645208 test loss: 0.555930
[Epoch 95; Iter    22/   32] train: loss: 0.3686343
[Epoch 95] ogbg-molsider: 0.641001 val loss: 0.565739
[Epoch 95] ogbg-molsider: 0.652380 test loss: 0.562780
[Epoch 96; Iter    20/   32] train: loss: 0.3318077
[Epoch 96] ogbg-molsider: 0.626475 val loss: 0.584282
[Epoch 96] ogbg-molsider: 0.642517 test loss: 0.607670
[Epoch 97; Iter    18/   32] train: loss: 0.3993174
[Epoch 97] ogbg-molsider: 0.638071 val loss: 0.585460
[Epoch 97] ogbg-molsider: 0.643008 test loss: 0.581061
[Epoch 98; Iter    16/   32] train: loss: 0.3719259
[Epoch 98] ogbg-molsider: 0.634760 val loss: 0.630823
[Epoch 98] ogbg-molsider: 0.645261 test loss: 0.578775
[Epoch 99; Iter    14/   32] train: loss: 0.3476759
[Epoch 99] ogbg-molsider: 0.617806 val loss: 0.607016
[Epoch 99] ogbg-molsider: 0.639206 test loss: 0.588217
[Epoch 100; Iter    12/   32] train: loss: 0.3698580
[Epoch 100] ogbg-molsider: 0.639045 val loss: 0.558750
[Epoch 100] ogbg-molsider: 0.642901 test loss: 0.579002
[Epoch 101; Iter    10/   32] train: loss: 0.3124374
[Epoch 101] ogbg-molsider: 0.635179 val loss: 0.610831
[Epoch 101] ogbg-molsider: 0.640668 test loss: 0.584536
[Epoch 102; Iter     8/   32] train: loss: 0.3287678
[Epoch 102] ogbg-molsider: 0.635064 val loss: 0.569237
[Epoch 102] ogbg-molsider: 0.643469 test loss: 0.572853
[Epoch 103; Iter     6/   32] train: loss: 0.3727952
[Epoch 103] ogbg-molsider: 0.623120 val loss: 0.611842
[Epoch 103] ogbg-molsider: 0.636482 test loss: 0.645514
[Epoch 104; Iter     4/   32] train: loss: 0.3006281
[Epoch 104] ogbg-molsider: 0.631019 val loss: 0.575785
[Epoch 104] ogbg-molsider: 0.642021 test loss: 0.584393
[Epoch 105; Iter     2/   32] train: loss: 0.3349019
[Epoch 105; Iter    32/   32] train: loss: 0.4082361
[Epoch 105] ogbg-molsider: 0.633253 val loss: 0.594517
[Epoch 105] ogbg-molsider: 0.651571 test loss: 0.572241
[Epoch 106; Iter    30/   32] train: loss: 0.2724249
[Epoch 106] ogbg-molsider: 0.625082 val loss: 0.616084
[Epoch 106] ogbg-molsider: 0.647673 test loss: 0.577068
[Epoch 107; Iter    28/   32] train: loss: 0.2703220
[Epoch 107] ogbg-molsider: 0.636603 val loss: 0.590684
[Epoch 107] ogbg-molsider: 0.644398 test loss: 0.584395
[Epoch 108; Iter    26/   32] train: loss: 0.2823223
[Epoch 108] ogbg-molsider: 0.635604 val loss: 0.582947
[Epoch 108] ogbg-molsider: 0.648292 test loss: 0.587575
[Epoch 109; Iter    24/   32] train: loss: 0.2994075
[Epoch 109] ogbg-molsider: 0.631201 val loss: 0.589959
[Epoch 109] ogbg-molsider: 0.650712 test loss: 0.582278
[Epoch 110; Iter    22/   32] train: loss: 0.2900818
[Epoch 110] ogbg-molsider: 0.643306 val loss: 0.601593
[Epoch 110] ogbg-molsider: 0.642526 test loss: 0.621771
[Epoch 111; Iter    20/   32] train: loss: 0.2958590
[Epoch 111] ogbg-molsider: 0.631216 val loss: 0.601752
[Epoch 111] ogbg-molsider: 0.645554 test loss: 0.597100
[Epoch 112; Iter    18/   32] train: loss: 0.3108127
[Epoch 112] ogbg-molsider: 0.642972 val loss: 0.598188
[Epoch 112] ogbg-molsider: 0.651418 test loss: 0.602962
[Epoch 113; Iter    16/   32] train: loss: 0.3345038
[Epoch 113] ogbg-molsider: 0.635652 val loss: 0.590281
[Epoch 113] ogbg-molsider: 0.641706 test loss: 0.599994
[Epoch 114; Iter    14/   32] train: loss: 0.2922721
[Epoch 114] ogbg-molsider: 0.633956 val loss: 0.587111
[Epoch 114] ogbg-molsider: 0.650038 test loss: 0.607777
[Epoch 115; Iter    12/   32] train: loss: 0.2730782
[Epoch 115] ogbg-molsider: 0.637415 val loss: 0.602352
[Epoch 115] ogbg-molsider: 0.660554 test loss: 0.596138
[Epoch 116; Iter    10/   32] train: loss: 0.3206454
[Epoch 116] ogbg-molsider: 0.647094 val loss: 0.597560
[Epoch 116] ogbg-molsider: 0.657512 test loss: 0.598055
[Epoch 117; Iter     8/   32] train: loss: 0.2965358
[Epoch 117] ogbg-molsider: 0.622605 val loss: 0.617819
[Epoch 117] ogbg-molsider: 0.648341 test loss: 0.611270
[Epoch 118; Iter     6/   32] train: loss: 0.2788910
[Epoch 118] ogbg-molsider: 0.633061 val loss: 0.608966
[Epoch 118] ogbg-molsider: 0.652709 test loss: 0.612445
[Epoch 119; Iter     4/   32] train: loss: 0.3071419
[Epoch 119] ogbg-molsider: 0.630530 val loss: 0.626502
[Epoch 119] ogbg-molsider: 0.648347 test loss: 0.612436
[Epoch 120; Iter     2/   32] train: loss: 0.2722312
[Epoch 120; Iter    32/   32] train: loss: 0.7121012
[Epoch 120] ogbg-molsider: 0.636896 val loss: 0.608246
[Epoch 120] ogbg-molsider: 0.660765 test loss: 0.597593
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 57.
Statistics on  val_best_checkpoint
mean_pred: 0.36004453897476196
std_pred: 1.789843201637268
mean_targets: 0.5732086896896362
std_targets: 0.49465426802635193
prcauc: 0.6576630546766273
rocauc: 0.6509155219198663
ogbg-molsider: 0.6509155219198663
OGBNanLabelBCEWithLogitsLoss: 0.4962197073868343
Statistics on  test
mean_pred: 0.4062805473804474
std_pred: 1.8249174356460571
mean_targets: 0.5714039206504822
std_targets: 0.4949178397655487
prcauc: 0.6452631882899681
rocauc: 0.6235501649980345
ogbg-molsider: 0.6235501649980345
OGBNanLabelBCEWithLogitsLoss: 0.5123015940189362
Statistics on  train
mean_pred: 0.3911895155906677
std_pred: 1.831117868423462
mean_targets: 0.5655384659767151
std_targets: 0.4956952929496765
prcauc: 0.7103106424105995
rocauc: 0.7320116397938479
ogbg-molsider: 0.7320116397938479
OGBNanLabelBCEWithLogitsLoss: 0.46113203279674053
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 89] ogbg-molsider: 0.607209 val loss: 0.584409
[Epoch 89] ogbg-molsider: 0.634988 test loss: 0.542216
[Epoch 90; Iter    27/   27] train: loss: 0.3678616
[Epoch 90] ogbg-molsider: 0.622433 val loss: 0.571651
[Epoch 90] ogbg-molsider: 0.641077 test loss: 0.528849
[Epoch 91] ogbg-molsider: 0.620652 val loss: 0.582244
[Epoch 91] ogbg-molsider: 0.615276 test loss: 0.590535
[Epoch 92; Iter     3/   27] train: loss: 0.3488062
[Epoch 92] ogbg-molsider: 0.615953 val loss: 0.570119
[Epoch 92] ogbg-molsider: 0.639327 test loss: 0.531645
[Epoch 93; Iter     6/   27] train: loss: 0.3161896
[Epoch 93] ogbg-molsider: 0.615987 val loss: 0.584430
[Epoch 93] ogbg-molsider: 0.644817 test loss: 0.555306
[Epoch 94; Iter     9/   27] train: loss: 0.3463306
[Epoch 94] ogbg-molsider: 0.612729 val loss: 0.590711
[Epoch 94] ogbg-molsider: 0.624022 test loss: 0.566959
[Epoch 95; Iter    12/   27] train: loss: 0.3945336
[Epoch 95] ogbg-molsider: 0.621828 val loss: 0.608937
[Epoch 95] ogbg-molsider: 0.625224 test loss: 0.567660
[Epoch 96; Iter    15/   27] train: loss: 0.3229538
[Epoch 96] ogbg-molsider: 0.618665 val loss: 0.599889
[Epoch 96] ogbg-molsider: 0.622527 test loss: 0.573864
[Epoch 97; Iter    18/   27] train: loss: 0.3530418
[Epoch 97] ogbg-molsider: 0.622691 val loss: 0.592695
[Epoch 97] ogbg-molsider: 0.636927 test loss: 0.553224
[Epoch 98; Iter    21/   27] train: loss: 0.3037646
[Epoch 98] ogbg-molsider: 0.625604 val loss: 0.589338
[Epoch 98] ogbg-molsider: 0.610951 test loss: 0.586773
[Epoch 99; Iter    24/   27] train: loss: 0.2983670
[Epoch 99] ogbg-molsider: 0.608707 val loss: 0.609663
[Epoch 99] ogbg-molsider: 0.616547 test loss: 0.580758
[Epoch 100; Iter    27/   27] train: loss: 0.2842810
[Epoch 100] ogbg-molsider: 0.610136 val loss: 0.602609
[Epoch 100] ogbg-molsider: 0.632956 test loss: 0.567102
[Epoch 101] ogbg-molsider: 0.617965 val loss: 0.617526
[Epoch 101] ogbg-molsider: 0.621061 test loss: 0.600332
[Epoch 102; Iter     3/   27] train: loss: 0.3387983
[Epoch 102] ogbg-molsider: 0.620335 val loss: 0.620584
[Epoch 102] ogbg-molsider: 0.628866 test loss: 0.585479
[Epoch 103; Iter     6/   27] train: loss: 0.3276595
[Epoch 103] ogbg-molsider: 0.616951 val loss: 0.612146
[Epoch 103] ogbg-molsider: 0.618197 test loss: 0.574954
[Epoch 104; Iter     9/   27] train: loss: 0.3298763
[Epoch 104] ogbg-molsider: 0.615462 val loss: 0.637452
[Epoch 104] ogbg-molsider: 0.625341 test loss: 0.595845
[Epoch 105; Iter    12/   27] train: loss: 0.3337910
[Epoch 105] ogbg-molsider: 0.613905 val loss: 0.620126
[Epoch 105] ogbg-molsider: 0.629774 test loss: 0.586324
[Epoch 106; Iter    15/   27] train: loss: 0.2950157
[Epoch 106] ogbg-molsider: 0.623349 val loss: 0.606532
[Epoch 106] ogbg-molsider: 0.625459 test loss: 0.584302
[Epoch 107; Iter    18/   27] train: loss: 0.3489528
[Epoch 107] ogbg-molsider: 0.616606 val loss: 0.633847
[Epoch 107] ogbg-molsider: 0.622662 test loss: 0.599668
[Epoch 108; Iter    21/   27] train: loss: 0.3201483
[Epoch 108] ogbg-molsider: 0.628331 val loss: 0.613332
[Epoch 108] ogbg-molsider: 0.628760 test loss: 0.582850
[Epoch 109; Iter    24/   27] train: loss: 0.2938059
[Epoch 109] ogbg-molsider: 0.623407 val loss: 0.624418
[Epoch 109] ogbg-molsider: 0.636417 test loss: 0.594571
[Epoch 110; Iter    27/   27] train: loss: 0.3386159
[Epoch 110] ogbg-molsider: 0.619790 val loss: 0.621503
[Epoch 110] ogbg-molsider: 0.627861 test loss: 0.588477
[Epoch 111] ogbg-molsider: 0.621444 val loss: 0.643191
[Epoch 111] ogbg-molsider: 0.626599 test loss: 0.602647
[Epoch 112; Iter     3/   27] train: loss: 0.2625881
[Epoch 112] ogbg-molsider: 0.629632 val loss: 0.636512
[Epoch 112] ogbg-molsider: 0.629509 test loss: 0.605497
[Epoch 113; Iter     6/   27] train: loss: 0.2957235
[Epoch 113] ogbg-molsider: 0.622951 val loss: 0.635772
[Epoch 113] ogbg-molsider: 0.632988 test loss: 0.591830
[Epoch 114; Iter     9/   27] train: loss: 0.2813752
[Epoch 114] ogbg-molsider: 0.617139 val loss: 0.640122
[Epoch 114] ogbg-molsider: 0.620673 test loss: 0.605232
[Epoch 115; Iter    12/   27] train: loss: 0.2436438
[Epoch 115] ogbg-molsider: 0.621651 val loss: 0.639138
[Epoch 115] ogbg-molsider: 0.628434 test loss: 0.599188
[Epoch 116; Iter    15/   27] train: loss: 0.3056343
[Epoch 116] ogbg-molsider: 0.615250 val loss: 0.694827
[Epoch 116] ogbg-molsider: 0.623843 test loss: 0.610364
[Epoch 117; Iter    18/   27] train: loss: 0.2878513
[Epoch 117] ogbg-molsider: 0.627986 val loss: 0.658409
[Epoch 117] ogbg-molsider: 0.629219 test loss: 0.605605
[Epoch 118; Iter    21/   27] train: loss: 0.2713661
[Epoch 118] ogbg-molsider: 0.616448 val loss: 0.671172
[Epoch 118] ogbg-molsider: 0.621914 test loss: 0.618021
[Epoch 119; Iter    24/   27] train: loss: 0.2332877
[Epoch 119] ogbg-molsider: 0.630371 val loss: 0.649440
[Epoch 119] ogbg-molsider: 0.635225 test loss: 0.608997
[Epoch 120; Iter    27/   27] train: loss: 0.2420140
[Epoch 120] ogbg-molsider: 0.621898 val loss: 0.675068
[Epoch 120] ogbg-molsider: 0.623014 test loss: 0.617812
[Epoch 121] ogbg-molsider: 0.620350 val loss: 0.669985
[Epoch 121] ogbg-molsider: 0.638009 test loss: 0.596970
[Epoch 122; Iter     3/   27] train: loss: 0.2986776
[Epoch 122] ogbg-molsider: 0.622156 val loss: 0.729569
[Epoch 122] ogbg-molsider: 0.631553 test loss: 0.609091
[Epoch 123; Iter     6/   27] train: loss: 0.2551365
[Epoch 123] ogbg-molsider: 0.614787 val loss: 0.798512
[Epoch 123] ogbg-molsider: 0.639612 test loss: 0.623177
[Epoch 124; Iter     9/   27] train: loss: 0.2544600
[Epoch 124] ogbg-molsider: 0.620022 val loss: 0.676923
[Epoch 124] ogbg-molsider: 0.630736 test loss: 0.644379
[Epoch 125; Iter    12/   27] train: loss: 0.2185525
[Epoch 125] ogbg-molsider: 0.623737 val loss: 0.670027
[Epoch 125] ogbg-molsider: 0.624740 test loss: 0.647294
[Epoch 126; Iter    15/   27] train: loss: 0.2355197
[Epoch 126] ogbg-molsider: 0.615593 val loss: 0.699414
[Epoch 126] ogbg-molsider: 0.636796 test loss: 0.633977
[Epoch 127; Iter    18/   27] train: loss: 0.2450429
[Epoch 127] ogbg-molsider: 0.624998 val loss: 0.658784
[Epoch 127] ogbg-molsider: 0.624419 test loss: 0.628751
[Epoch 128; Iter    21/   27] train: loss: 0.2605267
[Epoch 128] ogbg-molsider: 0.616208 val loss: 0.713656
[Epoch 128] ogbg-molsider: 0.631914 test loss: 0.657382
[Epoch 129; Iter    24/   27] train: loss: 0.2583209
[Epoch 129] ogbg-molsider: 0.618228 val loss: 0.690523
[Epoch 129] ogbg-molsider: 0.625407 test loss: 0.656663
[Epoch 130; Iter    27/   27] train: loss: 0.3022429
[Epoch 130] ogbg-molsider: 0.626569 val loss: 0.751866
[Epoch 130] ogbg-molsider: 0.634418 test loss: 0.638192
[Epoch 131] ogbg-molsider: 0.618932 val loss: 0.762862
[Epoch 131] ogbg-molsider: 0.628377 test loss: 0.659324
[Epoch 132; Iter     3/   27] train: loss: 0.2184933
[Epoch 132] ogbg-molsider: 0.616422 val loss: 0.686535
[Epoch 132] ogbg-molsider: 0.620499 test loss: 0.638202
[Epoch 133; Iter     6/   27] train: loss: 0.3023193
[Epoch 133] ogbg-molsider: 0.618698 val loss: 0.707201
[Epoch 133] ogbg-molsider: 0.629946 test loss: 0.660079
[Epoch 134; Iter     9/   27] train: loss: 0.2659856
[Epoch 134] ogbg-molsider: 0.625000 val loss: 0.679502
[Epoch 134] ogbg-molsider: 0.627293 test loss: 0.642991
[Epoch 135; Iter    12/   27] train: loss: 0.2377647
[Epoch 135] ogbg-molsider: 0.625942 val loss: 0.689198
[Epoch 135] ogbg-molsider: 0.633126 test loss: 0.644367
[Epoch 136; Iter    15/   27] train: loss: 0.2434170
[Epoch 136] ogbg-molsider: 0.624912 val loss: 0.731850
[Epoch 136] ogbg-molsider: 0.628133 test loss: 0.641440
[Epoch 137; Iter    18/   27] train: loss: 0.2435370
[Epoch 137] ogbg-molsider: 0.623546 val loss: 0.765917
[Epoch 137] ogbg-molsider: 0.634071 test loss: 0.650297
[Epoch 138; Iter    21/   27] train: loss: 0.1980985
[Epoch 138] ogbg-molsider: 0.625960 val loss: 0.685982
[Epoch 138] ogbg-molsider: 0.632088 test loss: 0.649690
[Epoch 139; Iter    24/   27] train: loss: 0.2019713
[Epoch 139] ogbg-molsider: 0.623415 val loss: 0.722965
[Epoch 139] ogbg-molsider: 0.628286 test loss: 0.651139
Early stopping criterion based on -ogbg-molsider- that should be max reached after 139 epochs. Best model checkpoint was in epoch 79.
Statistics on  val_best_checkpoint
mean_pred: 0.6423612236976624
std_pred: 2.1274585723876953
mean_targets: 0.5445094108581543
std_targets: 0.4980473220348358
prcauc: 0.6267529207500415
rocauc: 0.6456730675688128
ogbg-molsider: 0.6456730675688128
OGBNanLabelBCEWithLogitsLoss: 0.5479146473937564
Statistics on  test
mean_pred: 0.7534805536270142
std_pred: 2.163472890853882
mean_targets: 0.5734265446662903
std_targets: 0.4946112036705017
prcauc: 0.6562591915565752
rocauc: 0.6226224218860155
ogbg-molsider: 0.6226224218860155
OGBNanLabelBCEWithLogitsLoss: 0.5382575160927243
Statistics on  train
mean_pred: 0.7784611582756042
std_pred: 2.2720463275909424
mean_targets: 0.5732952356338501
std_targets: 0.4946093261241913
prcauc: 0.7922809292040108
rocauc: 0.8357282395606057
ogbg-molsider: 0.8357282395606057
OGBNanLabelBCEWithLogitsLoss: 0.3871092564529843
All runs completed.
[Epoch 81; Iter    30/   36] train: loss: 0.3308287
[Epoch 81] ogbg-molsider: 0.629066 val loss: 0.527813
[Epoch 81] ogbg-molsider: 0.647186 test loss: 0.534981
[Epoch 82; Iter    24/   36] train: loss: 0.3237747
[Epoch 82] ogbg-molsider: 0.631823 val loss: 0.538608
[Epoch 82] ogbg-molsider: 0.645678 test loss: 0.550406
[Epoch 83; Iter    18/   36] train: loss: 0.3537985
[Epoch 83] ogbg-molsider: 0.633236 val loss: 0.546201
[Epoch 83] ogbg-molsider: 0.658465 test loss: 0.577780
[Epoch 84; Iter    12/   36] train: loss: 0.3628338
[Epoch 84] ogbg-molsider: 0.636495 val loss: 0.547822
[Epoch 84] ogbg-molsider: 0.657536 test loss: 0.532055
[Epoch 85; Iter     6/   36] train: loss: 0.3191852
[Epoch 85; Iter    36/   36] train: loss: 0.3627456
[Epoch 85] ogbg-molsider: 0.644793 val loss: 0.537569
[Epoch 85] ogbg-molsider: 0.648819 test loss: 0.576498
[Epoch 86; Iter    30/   36] train: loss: 0.3818074
[Epoch 86] ogbg-molsider: 0.622712 val loss: 0.536756
[Epoch 86] ogbg-molsider: 0.676237 test loss: 0.520296
[Epoch 87; Iter    24/   36] train: loss: 0.3486378
[Epoch 87] ogbg-molsider: 0.631803 val loss: 0.560175
[Epoch 87] ogbg-molsider: 0.647319 test loss: 0.605706
[Epoch 88; Iter    18/   36] train: loss: 0.3582586
[Epoch 88] ogbg-molsider: 0.629438 val loss: 0.548046
[Epoch 88] ogbg-molsider: 0.665360 test loss: 0.520010
[Epoch 89; Iter    12/   36] train: loss: 0.3226519
[Epoch 89] ogbg-molsider: 0.642864 val loss: 0.550536
[Epoch 89] ogbg-molsider: 0.660837 test loss: 0.530174
[Epoch 90; Iter     6/   36] train: loss: 0.3278070
[Epoch 90; Iter    36/   36] train: loss: 0.3666352
[Epoch 90] ogbg-molsider: 0.645126 val loss: 0.539189
[Epoch 90] ogbg-molsider: 0.671417 test loss: 0.542227
[Epoch 91; Iter    30/   36] train: loss: 0.3477657
[Epoch 91] ogbg-molsider: 0.636556 val loss: 0.544604
[Epoch 91] ogbg-molsider: 0.648758 test loss: 0.550812
[Epoch 92; Iter    24/   36] train: loss: 0.3168248
[Epoch 92] ogbg-molsider: 0.650351 val loss: 0.538714
[Epoch 92] ogbg-molsider: 0.666607 test loss: 0.572708
[Epoch 93; Iter    18/   36] train: loss: 0.3027129
[Epoch 93] ogbg-molsider: 0.652408 val loss: 0.536477
[Epoch 93] ogbg-molsider: 0.663274 test loss: 0.558349
[Epoch 94; Iter    12/   36] train: loss: 0.3214366
[Epoch 94] ogbg-molsider: 0.642961 val loss: 0.550305
[Epoch 94] ogbg-molsider: 0.663028 test loss: 0.559943
[Epoch 95; Iter     6/   36] train: loss: 0.2977837
[Epoch 95; Iter    36/   36] train: loss: 0.3736268
[Epoch 95] ogbg-molsider: 0.645207 val loss: 0.541851
[Epoch 95] ogbg-molsider: 0.675090 test loss: 0.541630
[Epoch 96; Iter    30/   36] train: loss: 0.2993281
[Epoch 96] ogbg-molsider: 0.650343 val loss: 0.547885
[Epoch 96] ogbg-molsider: 0.670692 test loss: 0.570673
[Epoch 97; Iter    24/   36] train: loss: 0.3198541
[Epoch 97] ogbg-molsider: 0.657082 val loss: 0.533928
[Epoch 97] ogbg-molsider: 0.671775 test loss: 0.554693
[Epoch 98; Iter    18/   36] train: loss: 0.2859876
[Epoch 98] ogbg-molsider: 0.657820 val loss: 0.540580
[Epoch 98] ogbg-molsider: 0.668772 test loss: 0.553271
[Epoch 99; Iter    12/   36] train: loss: 0.2623288
[Epoch 99] ogbg-molsider: 0.651441 val loss: 0.547009
[Epoch 99] ogbg-molsider: 0.663335 test loss: 0.563791
[Epoch 100; Iter     6/   36] train: loss: 0.3052434
[Epoch 100; Iter    36/   36] train: loss: 0.3067860
[Epoch 100] ogbg-molsider: 0.655804 val loss: 0.548952
[Epoch 100] ogbg-molsider: 0.669905 test loss: 0.563677
[Epoch 101; Iter    30/   36] train: loss: 0.2906364
[Epoch 101] ogbg-molsider: 0.647679 val loss: 0.568551
[Epoch 101] ogbg-molsider: 0.668327 test loss: 0.590152
[Epoch 102; Iter    24/   36] train: loss: 0.2902724
[Epoch 102] ogbg-molsider: 0.646351 val loss: 0.554756
[Epoch 102] ogbg-molsider: 0.665738 test loss: 0.580873
[Epoch 103; Iter    18/   36] train: loss: 0.2525533
[Epoch 103] ogbg-molsider: 0.647334 val loss: 0.572064
[Epoch 103] ogbg-molsider: 0.666461 test loss: 0.596185
[Epoch 104; Iter    12/   36] train: loss: 0.2699102
[Epoch 104] ogbg-molsider: 0.645407 val loss: 0.580312
[Epoch 104] ogbg-molsider: 0.647286 test loss: 0.606069
[Epoch 105; Iter     6/   36] train: loss: 0.2924470
[Epoch 105; Iter    36/   36] train: loss: 0.2632833
[Epoch 105] ogbg-molsider: 0.646754 val loss: 0.577589
[Epoch 105] ogbg-molsider: 0.663810 test loss: 0.595825
[Epoch 106; Iter    30/   36] train: loss: 0.2674254
[Epoch 106] ogbg-molsider: 0.651188 val loss: 0.556330
[Epoch 106] ogbg-molsider: 0.663665 test loss: 0.592570
[Epoch 107; Iter    24/   36] train: loss: 0.2982729
[Epoch 107] ogbg-molsider: 0.641763 val loss: 0.585983
[Epoch 107] ogbg-molsider: 0.647381 test loss: 0.618513
[Epoch 108; Iter    18/   36] train: loss: 0.2738268
[Epoch 108] ogbg-molsider: 0.644717 val loss: 0.590549
[Epoch 108] ogbg-molsider: 0.662236 test loss: 0.609232
[Epoch 109; Iter    12/   36] train: loss: 0.2677299
[Epoch 109] ogbg-molsider: 0.647772 val loss: 0.591043
[Epoch 109] ogbg-molsider: 0.662896 test loss: 0.593656
[Epoch 110; Iter     6/   36] train: loss: 0.2871639
[Epoch 110; Iter    36/   36] train: loss: 0.2897550
[Epoch 110] ogbg-molsider: 0.637326 val loss: 0.598339
[Epoch 110] ogbg-molsider: 0.665999 test loss: 0.602460
[Epoch 111; Iter    30/   36] train: loss: 0.2970422
[Epoch 111] ogbg-molsider: 0.636777 val loss: 0.615127
[Epoch 111] ogbg-molsider: 0.652185 test loss: 0.624592
[Epoch 112; Iter    24/   36] train: loss: 0.3190419
[Epoch 112] ogbg-molsider: 0.624877 val loss: 0.616755
[Epoch 112] ogbg-molsider: 0.655636 test loss: 0.625358
[Epoch 113; Iter    18/   36] train: loss: 0.2892855
[Epoch 113] ogbg-molsider: 0.644012 val loss: 0.615207
[Epoch 113] ogbg-molsider: 0.655402 test loss: 0.645678
[Epoch 114; Iter    12/   36] train: loss: 0.2678361
[Epoch 114] ogbg-molsider: 0.638616 val loss: 0.609885
[Epoch 114] ogbg-molsider: 0.659460 test loss: 0.610040
[Epoch 115; Iter     6/   36] train: loss: 0.2585110
[Epoch 115; Iter    36/   36] train: loss: 0.2548868
[Epoch 115] ogbg-molsider: 0.635755 val loss: 0.610674
[Epoch 115] ogbg-molsider: 0.668068 test loss: 0.594528
[Epoch 116; Iter    30/   36] train: loss: 0.2464712
[Epoch 116] ogbg-molsider: 0.638792 val loss: 0.626660
[Epoch 116] ogbg-molsider: 0.654315 test loss: 0.632797
[Epoch 117; Iter    24/   36] train: loss: 0.2751476
[Epoch 117] ogbg-molsider: 0.634746 val loss: 0.621367
[Epoch 117] ogbg-molsider: 0.661000 test loss: 0.629058
[Epoch 118; Iter    18/   36] train: loss: 0.2634886
[Epoch 118] ogbg-molsider: 0.624587 val loss: 0.634636
[Epoch 118] ogbg-molsider: 0.666165 test loss: 0.634819
[Epoch 119; Iter    12/   36] train: loss: 0.2273525
[Epoch 119] ogbg-molsider: 0.640949 val loss: 0.616256
[Epoch 119] ogbg-molsider: 0.655050 test loss: 0.638431
[Epoch 120; Iter     6/   36] train: loss: 0.3106754
[Epoch 120; Iter    36/   36] train: loss: 0.2736498
[Epoch 120] ogbg-molsider: 0.645022 val loss: 0.604121
[Epoch 120] ogbg-molsider: 0.665620 test loss: 0.628197
[Epoch 121; Iter    30/   36] train: loss: 0.2178382
[Epoch 121] ogbg-molsider: 0.632317 val loss: 0.621088
[Epoch 121] ogbg-molsider: 0.665249 test loss: 0.625271
[Epoch 122; Iter    24/   36] train: loss: 0.2304536
[Epoch 122] ogbg-molsider: 0.638293 val loss: 0.609993
[Epoch 122] ogbg-molsider: 0.655921 test loss: 0.634225
[Epoch 123; Iter    18/   36] train: loss: 0.2221658
[Epoch 123] ogbg-molsider: 0.636085 val loss: 0.620433
[Epoch 123] ogbg-molsider: 0.652870 test loss: 0.633672
[Epoch 124; Iter    12/   36] train: loss: 0.2163478
[Epoch 124] ogbg-molsider: 0.635805 val loss: 0.614725
[Epoch 124] ogbg-molsider: 0.663643 test loss: 0.642703
[Epoch 125; Iter     6/   36] train: loss: 0.2225968
[Epoch 125; Iter    36/   36] train: loss: 0.2113879
[Epoch 125] ogbg-molsider: 0.631864 val loss: 0.643229
[Epoch 125] ogbg-molsider: 0.663832 test loss: 0.635268
[Epoch 126; Iter    30/   36] train: loss: 0.2391637
[Epoch 126] ogbg-molsider: 0.630337 val loss: 0.651561
[Epoch 126] ogbg-molsider: 0.670056 test loss: 0.639453
[Epoch 127; Iter    24/   36] train: loss: 0.2043145
[Epoch 127] ogbg-molsider: 0.633608 val loss: 0.639706
[Epoch 127] ogbg-molsider: 0.657831 test loss: 0.630893
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 84] ogbg-molsider: 0.638721 test loss: 0.747285
[Epoch 85; Iter    12/   32] train: loss: 0.3784580
[Epoch 85] ogbg-molsider: 0.639111 val loss: 0.547166
[Epoch 85] ogbg-molsider: 0.647496 test loss: 0.544153
[Epoch 86; Iter    10/   32] train: loss: 0.3768605
[Epoch 86] ogbg-molsider: 0.649121 val loss: 0.547869
[Epoch 86] ogbg-molsider: 0.652024 test loss: 0.538978
[Epoch 87; Iter     8/   32] train: loss: 0.4188295
[Epoch 87] ogbg-molsider: 0.642894 val loss: 0.544321
[Epoch 87] ogbg-molsider: 0.652733 test loss: 0.528866
[Epoch 88; Iter     6/   32] train: loss: 0.3735357
[Epoch 88] ogbg-molsider: 0.651344 val loss: 0.539036
[Epoch 88] ogbg-molsider: 0.654620 test loss: 0.539691
[Epoch 89; Iter     4/   32] train: loss: 0.3766329
[Epoch 89] ogbg-molsider: 0.642318 val loss: 0.545345
[Epoch 89] ogbg-molsider: 0.646249 test loss: 0.536294
[Epoch 90; Iter     2/   32] train: loss: 0.3689187
[Epoch 90; Iter    32/   32] train: loss: 0.5840105
[Epoch 90] ogbg-molsider: 0.652987 val loss: 0.551939
[Epoch 90] ogbg-molsider: 0.652051 test loss: 0.544894
[Epoch 91; Iter    30/   32] train: loss: 0.3646625
[Epoch 91] ogbg-molsider: 0.637369 val loss: 0.564358
[Epoch 91] ogbg-molsider: 0.646677 test loss: 0.545913
[Epoch 92; Iter    28/   32] train: loss: 0.3584769
[Epoch 92] ogbg-molsider: 0.630511 val loss: 0.563286
[Epoch 92] ogbg-molsider: 0.636979 test loss: 0.546871
[Epoch 93; Iter    26/   32] train: loss: 0.3579613
[Epoch 93] ogbg-molsider: 0.645809 val loss: 0.549347
[Epoch 93] ogbg-molsider: 0.649216 test loss: 0.542654
[Epoch 94; Iter    24/   32] train: loss: 0.3506362
[Epoch 94] ogbg-molsider: 0.643704 val loss: 0.587594
[Epoch 94] ogbg-molsider: 0.649012 test loss: 0.555602
[Epoch 95; Iter    22/   32] train: loss: 0.3708081
[Epoch 95] ogbg-molsider: 0.655708 val loss: 0.577480
[Epoch 95] ogbg-molsider: 0.644791 test loss: 0.593247
[Epoch 96; Iter    20/   32] train: loss: 0.3920149
[Epoch 96] ogbg-molsider: 0.632783 val loss: 0.579266
[Epoch 96] ogbg-molsider: 0.636659 test loss: 0.580219
[Epoch 97; Iter    18/   32] train: loss: 0.3403910
[Epoch 97] ogbg-molsider: 0.649220 val loss: 0.610660
[Epoch 97] ogbg-molsider: 0.645930 test loss: 0.585920
[Epoch 98; Iter    16/   32] train: loss: 0.3436325
[Epoch 98] ogbg-molsider: 0.656675 val loss: 0.567868
[Epoch 98] ogbg-molsider: 0.645073 test loss: 0.647434
[Epoch 99; Iter    14/   32] train: loss: 0.3928720
[Epoch 99] ogbg-molsider: 0.636095 val loss: 0.578298
[Epoch 99] ogbg-molsider: 0.653426 test loss: 0.550792
[Epoch 100; Iter    12/   32] train: loss: 0.3226912
[Epoch 100] ogbg-molsider: 0.641426 val loss: 0.575949
[Epoch 100] ogbg-molsider: 0.651343 test loss: 0.558062
[Epoch 101; Iter    10/   32] train: loss: 0.3639823
[Epoch 101] ogbg-molsider: 0.635716 val loss: 0.587305
[Epoch 101] ogbg-molsider: 0.643057 test loss: 0.600892
[Epoch 102; Iter     8/   32] train: loss: 0.3231032
[Epoch 102] ogbg-molsider: 0.652044 val loss: 0.566549
[Epoch 102] ogbg-molsider: 0.643314 test loss: 0.562206
[Epoch 103; Iter     6/   32] train: loss: 0.3033533
[Epoch 103] ogbg-molsider: 0.651974 val loss: 0.572921
[Epoch 103] ogbg-molsider: 0.646829 test loss: 0.573351
[Epoch 104; Iter     4/   32] train: loss: 0.3056343
[Epoch 104] ogbg-molsider: 0.656805 val loss: 0.574039
[Epoch 104] ogbg-molsider: 0.649208 test loss: 0.570191
[Epoch 105; Iter     2/   32] train: loss: 0.3257823
[Epoch 105; Iter    32/   32] train: loss: 0.2953861
[Epoch 105] ogbg-molsider: 0.659637 val loss: 0.553177
[Epoch 105] ogbg-molsider: 0.647205 test loss: 0.551404
[Epoch 106; Iter    30/   32] train: loss: 0.3207378
[Epoch 106] ogbg-molsider: 0.646981 val loss: 0.569301
[Epoch 106] ogbg-molsider: 0.639535 test loss: 0.579304
[Epoch 107; Iter    28/   32] train: loss: 0.2943245
[Epoch 107] ogbg-molsider: 0.655321 val loss: 0.579833
[Epoch 107] ogbg-molsider: 0.653639 test loss: 0.568568
[Epoch 108; Iter    26/   32] train: loss: 0.3179851
[Epoch 108] ogbg-molsider: 0.659328 val loss: 0.578571
[Epoch 108] ogbg-molsider: 0.655365 test loss: 0.583371
[Epoch 109; Iter    24/   32] train: loss: 0.2878147
[Epoch 109] ogbg-molsider: 0.652484 val loss: 0.592527
[Epoch 109] ogbg-molsider: 0.660936 test loss: 0.577804
[Epoch 110; Iter    22/   32] train: loss: 0.3039065
[Epoch 110] ogbg-molsider: 0.657310 val loss: 0.601378
[Epoch 110] ogbg-molsider: 0.655267 test loss: 0.588536
[Epoch 111; Iter    20/   32] train: loss: 0.2828614
[Epoch 111] ogbg-molsider: 0.652838 val loss: 0.595012
[Epoch 111] ogbg-molsider: 0.662117 test loss: 0.572823
[Epoch 112; Iter    18/   32] train: loss: 0.2852921
[Epoch 112] ogbg-molsider: 0.656153 val loss: 0.595501
[Epoch 112] ogbg-molsider: 0.646856 test loss: 0.595240
[Epoch 113; Iter    16/   32] train: loss: 0.2952522
[Epoch 113] ogbg-molsider: 0.655671 val loss: 0.600240
[Epoch 113] ogbg-molsider: 0.649705 test loss: 0.591757
[Epoch 114; Iter    14/   32] train: loss: 0.2651369
[Epoch 114] ogbg-molsider: 0.660061 val loss: 0.586575
[Epoch 114] ogbg-molsider: 0.646802 test loss: 0.599621
[Epoch 115; Iter    12/   32] train: loss: 0.2871328
[Epoch 115] ogbg-molsider: 0.649927 val loss: 0.599513
[Epoch 115] ogbg-molsider: 0.652101 test loss: 0.588471
[Epoch 116; Iter    10/   32] train: loss: 0.2874308
[Epoch 116] ogbg-molsider: 0.666631 val loss: 0.595784
[Epoch 116] ogbg-molsider: 0.644878 test loss: 0.605248
[Epoch 117; Iter     8/   32] train: loss: 0.2769676
[Epoch 117] ogbg-molsider: 0.647853 val loss: 0.616239
[Epoch 117] ogbg-molsider: 0.658339 test loss: 0.587157
[Epoch 118; Iter     6/   32] train: loss: 0.2930120
[Epoch 118] ogbg-molsider: 0.664965 val loss: 0.616225
[Epoch 118] ogbg-molsider: 0.663018 test loss: 0.599988
[Epoch 119; Iter     4/   32] train: loss: 0.2537284
[Epoch 119] ogbg-molsider: 0.646125 val loss: 0.627033
[Epoch 119] ogbg-molsider: 0.658666 test loss: 0.599976
[Epoch 120; Iter     2/   32] train: loss: 0.2872147
[Epoch 120; Iter    32/   32] train: loss: 0.4035707
[Epoch 120] ogbg-molsider: 0.654554 val loss: 0.613890
[Epoch 120] ogbg-molsider: 0.669047 test loss: 0.588123
[Epoch 121; Iter    30/   32] train: loss: 0.2618682
[Epoch 121] ogbg-molsider: 0.648809 val loss: 0.600626
[Epoch 121] ogbg-molsider: 0.643577 test loss: 0.618276
[Epoch 122; Iter    28/   32] train: loss: 0.2862742
[Epoch 122] ogbg-molsider: 0.652469 val loss: 0.620369
[Epoch 122] ogbg-molsider: 0.661378 test loss: 0.607470
[Epoch 123; Iter    26/   32] train: loss: 0.2415397
[Epoch 123] ogbg-molsider: 0.637946 val loss: 0.629394
[Epoch 123] ogbg-molsider: 0.650990 test loss: 0.604267
[Epoch 124; Iter    24/   32] train: loss: 0.2540199
[Epoch 124] ogbg-molsider: 0.650344 val loss: 0.621744
[Epoch 124] ogbg-molsider: 0.659989 test loss: 0.607348
[Epoch 125; Iter    22/   32] train: loss: 0.2562515
[Epoch 125] ogbg-molsider: 0.648861 val loss: 0.617665
[Epoch 125] ogbg-molsider: 0.660423 test loss: 0.595493
[Epoch 126; Iter    20/   32] train: loss: 0.3027601
[Epoch 126] ogbg-molsider: 0.650116 val loss: 0.633122
[Epoch 126] ogbg-molsider: 0.655185 test loss: 0.615808
[Epoch 127; Iter    18/   32] train: loss: 0.2630507
[Epoch 127] ogbg-molsider: 0.658781 val loss: 0.624437
[Epoch 127] ogbg-molsider: 0.663331 test loss: 0.607878
[Epoch 128; Iter    16/   32] train: loss: 0.2605551
[Epoch 128] ogbg-molsider: 0.650381 val loss: 0.633717
[Epoch 128] ogbg-molsider: 0.653757 test loss: 0.631264
[Epoch 129; Iter    14/   32] train: loss: 0.2757344
[Epoch 129] ogbg-molsider: 0.650751 val loss: 0.637032
[Epoch 129] ogbg-molsider: 0.660257 test loss: 0.623659
[Epoch 130; Iter    12/   32] train: loss: 0.2607798
[Epoch 130] ogbg-molsider: 0.652888 val loss: 0.637453
[Epoch 130] ogbg-molsider: 0.663880 test loss: 0.621219
[Epoch 131; Iter    10/   32] train: loss: 0.2840614
[Epoch 131] ogbg-molsider: 0.649656 val loss: 0.652294
[Epoch 131] ogbg-molsider: 0.655761 test loss: 0.627002
[Epoch 132; Iter     8/   32] train: loss: 0.2616782
[Epoch 132] ogbg-molsider: 0.653422 val loss: 0.635533
[Epoch 132] ogbg-molsider: 0.662757 test loss: 0.616871
[Epoch 133; Iter     6/   32] train: loss: 0.2974262
[Epoch 133] ogbg-molsider: 0.662162 val loss: 0.622775
Early stopping criterion based on -ogbg-molsider- that should be max reached after 127 epochs. Best model checkpoint was in epoch 67.
Statistics on  val_best_checkpoint
mean_pred: 0.5173551440238953
std_pred: 2.1595656871795654
mean_targets: 0.6021755933761597
std_targets: 0.48951220512390137
prcauc: 0.7064606300806114
rocauc: 0.6673399839460039
ogbg-molsider: 0.6673399839460039
OGBNanLabelBCEWithLogitsLoss: 0.4894617676734924
Statistics on  test
mean_pred: 0.472848117351532
std_pred: 2.1249427795410156
mean_targets: 0.5446775555610657
std_targets: 0.49806439876556396
prcauc: 0.641850824367775
rocauc: 0.6304651989173725
ogbg-molsider: 0.6304651989173725
OGBNanLabelBCEWithLogitsLoss: 0.5355888247489929
Statistics on  train
mean_pred: 0.4744637608528137
std_pred: 2.4038662910461426
mean_targets: 0.5661051273345947
std_targets: 0.49561887979507446
prcauc: 0.759859201849237
rocauc: 0.8067419976258734
ogbg-molsider: 0.8067419976258734
OGBNanLabelBCEWithLogitsLoss: 0.4252566264735328
[Epoch 84] ogbg-molsider: 0.646653 test loss: 0.526582
[Epoch 85; Iter    12/   32] train: loss: 0.3824027
[Epoch 85] ogbg-molsider: 0.635593 val loss: 0.544527
[Epoch 85] ogbg-molsider: 0.651508 test loss: 0.530538
[Epoch 86; Iter    10/   32] train: loss: 0.3593515
[Epoch 86] ogbg-molsider: 0.635962 val loss: 0.540614
[Epoch 86] ogbg-molsider: 0.655872 test loss: 0.528464
[Epoch 87; Iter     8/   32] train: loss: 0.3285531
[Epoch 87] ogbg-molsider: 0.636434 val loss: 0.546759
[Epoch 87] ogbg-molsider: 0.652618 test loss: 0.534647
[Epoch 88; Iter     6/   32] train: loss: 0.3254702
[Epoch 88] ogbg-molsider: 0.632635 val loss: 0.549745
[Epoch 88] ogbg-molsider: 0.644690 test loss: 0.544343
[Epoch 89; Iter     4/   32] train: loss: 0.3452515
[Epoch 89] ogbg-molsider: 0.644247 val loss: 0.545283
[Epoch 89] ogbg-molsider: 0.651196 test loss: 0.542311
[Epoch 90; Iter     2/   32] train: loss: 0.3030100
[Epoch 90; Iter    32/   32] train: loss: 0.4022366
[Epoch 90] ogbg-molsider: 0.640350 val loss: 0.561623
[Epoch 90] ogbg-molsider: 0.649333 test loss: 0.549854
[Epoch 91; Iter    30/   32] train: loss: 0.3921601
[Epoch 91] ogbg-molsider: 0.644034 val loss: 0.562966
[Epoch 91] ogbg-molsider: 0.650552 test loss: 0.553773
[Epoch 92; Iter    28/   32] train: loss: 0.3489102
[Epoch 92] ogbg-molsider: 0.646352 val loss: 0.563581
[Epoch 92] ogbg-molsider: 0.646426 test loss: 0.558343
[Epoch 93; Iter    26/   32] train: loss: 0.3191394
[Epoch 93] ogbg-molsider: 0.647322 val loss: 0.565308
[Epoch 93] ogbg-molsider: 0.643973 test loss: 0.560045
[Epoch 94; Iter    24/   32] train: loss: 0.3158182
[Epoch 94] ogbg-molsider: 0.648338 val loss: 0.576784
[Epoch 94] ogbg-molsider: 0.644811 test loss: 0.560346
[Epoch 95; Iter    22/   32] train: loss: 0.3023987
[Epoch 95] ogbg-molsider: 0.641475 val loss: 0.570325
[Epoch 95] ogbg-molsider: 0.650848 test loss: 0.559557
[Epoch 96; Iter    20/   32] train: loss: 0.4006174
[Epoch 96] ogbg-molsider: 0.654300 val loss: 0.554255
[Epoch 96] ogbg-molsider: 0.650946 test loss: 0.562320
[Epoch 97; Iter    18/   32] train: loss: 0.3377561
[Epoch 97] ogbg-molsider: 0.638872 val loss: 0.576580
[Epoch 97] ogbg-molsider: 0.640095 test loss: 0.567428
[Epoch 98; Iter    16/   32] train: loss: 0.2975400
[Epoch 98] ogbg-molsider: 0.641315 val loss: 0.565757
[Epoch 98] ogbg-molsider: 0.649530 test loss: 0.561082
[Epoch 99; Iter    14/   32] train: loss: 0.3175160
[Epoch 99] ogbg-molsider: 0.646168 val loss: 0.577786
[Epoch 99] ogbg-molsider: 0.652234 test loss: 0.564271
[Epoch 100; Iter    12/   32] train: loss: 0.3230424
[Epoch 100] ogbg-molsider: 0.641795 val loss: 0.576714
[Epoch 100] ogbg-molsider: 0.642122 test loss: 0.574687
[Epoch 101; Iter    10/   32] train: loss: 0.2759902
[Epoch 101] ogbg-molsider: 0.648733 val loss: 0.584979
[Epoch 101] ogbg-molsider: 0.648122 test loss: 0.584638
[Epoch 102; Iter     8/   32] train: loss: 0.3428637
[Epoch 102] ogbg-molsider: 0.641460 val loss: 0.597581
[Epoch 102] ogbg-molsider: 0.639204 test loss: 0.597249
[Epoch 103; Iter     6/   32] train: loss: 0.2979006
[Epoch 103] ogbg-molsider: 0.640163 val loss: 0.600910
[Epoch 103] ogbg-molsider: 0.645861 test loss: 0.584724
[Epoch 104; Iter     4/   32] train: loss: 0.3003008
[Epoch 104] ogbg-molsider: 0.648926 val loss: 0.578968
[Epoch 104] ogbg-molsider: 0.635388 test loss: 0.590476
[Epoch 105; Iter     2/   32] train: loss: 0.2869986
[Epoch 105; Iter    32/   32] train: loss: 0.3043920
[Epoch 105] ogbg-molsider: 0.636427 val loss: 0.586281
[Epoch 105] ogbg-molsider: 0.644298 test loss: 0.582442
[Epoch 106; Iter    30/   32] train: loss: 0.3402765
[Epoch 106] ogbg-molsider: 0.653299 val loss: 0.574339
[Epoch 106] ogbg-molsider: 0.636449 test loss: 0.590623
[Epoch 107; Iter    28/   32] train: loss: 0.3239230
[Epoch 107] ogbg-molsider: 0.638694 val loss: 0.591150
[Epoch 107] ogbg-molsider: 0.633582 test loss: 0.588076
[Epoch 108; Iter    26/   32] train: loss: 0.3341735
[Epoch 108] ogbg-molsider: 0.641222 val loss: 0.599506
[Epoch 108] ogbg-molsider: 0.637214 test loss: 0.598196
[Epoch 109; Iter    24/   32] train: loss: 0.3067308
[Epoch 109] ogbg-molsider: 0.635239 val loss: 0.589430
[Epoch 109] ogbg-molsider: 0.637121 test loss: 0.589908
[Epoch 110; Iter    22/   32] train: loss: 0.3847278
[Epoch 110] ogbg-molsider: 0.630804 val loss: 0.594181
[Epoch 110] ogbg-molsider: 0.641281 test loss: 0.589892
[Epoch 111; Iter    20/   32] train: loss: 0.2934337
[Epoch 111] ogbg-molsider: 0.647142 val loss: 0.607916
[Epoch 111] ogbg-molsider: 0.643823 test loss: 0.598031
[Epoch 112; Iter    18/   32] train: loss: 0.2828106
[Epoch 112] ogbg-molsider: 0.649232 val loss: 0.586370
[Epoch 112] ogbg-molsider: 0.638171 test loss: 0.594153
[Epoch 113; Iter    16/   32] train: loss: 0.3016368
[Epoch 113] ogbg-molsider: 0.646683 val loss: 0.592056
[Epoch 113] ogbg-molsider: 0.632192 test loss: 0.605227
[Epoch 114; Iter    14/   32] train: loss: 0.2628985
[Epoch 114] ogbg-molsider: 0.643764 val loss: 0.603697
[Epoch 114] ogbg-molsider: 0.637411 test loss: 0.599277
[Epoch 115; Iter    12/   32] train: loss: 0.2623951
[Epoch 115] ogbg-molsider: 0.648706 val loss: 0.596521
[Epoch 115] ogbg-molsider: 0.650157 test loss: 0.588737
[Epoch 116; Iter    10/   32] train: loss: 0.3068386
[Epoch 116] ogbg-molsider: 0.639789 val loss: 0.607817
[Epoch 116] ogbg-molsider: 0.633748 test loss: 0.607713
[Epoch 117; Iter     8/   32] train: loss: 0.3098789
[Epoch 117] ogbg-molsider: 0.650144 val loss: 0.603508
[Epoch 117] ogbg-molsider: 0.646741 test loss: 0.596885
[Epoch 118; Iter     6/   32] train: loss: 0.2952979
[Epoch 118] ogbg-molsider: 0.643075 val loss: 0.605433
[Epoch 118] ogbg-molsider: 0.644530 test loss: 0.592017
[Epoch 119; Iter     4/   32] train: loss: 0.2892973
[Epoch 119] ogbg-molsider: 0.646097 val loss: 0.613844
[Epoch 119] ogbg-molsider: 0.645383 test loss: 0.604933
[Epoch 120; Iter     2/   32] train: loss: 0.2540367
[Epoch 120; Iter    32/   32] train: loss: 0.3853391
[Epoch 120] ogbg-molsider: 0.638428 val loss: 0.610463
[Epoch 120] ogbg-molsider: 0.641838 test loss: 0.600759
[Epoch 121; Iter    30/   32] train: loss: 0.2892364
[Epoch 121] ogbg-molsider: 0.642474 val loss: 0.626967
[Epoch 121] ogbg-molsider: 0.641776 test loss: 0.615164
[Epoch 122; Iter    28/   32] train: loss: 0.2687406
[Epoch 122] ogbg-molsider: 0.638893 val loss: 0.620265
[Epoch 122] ogbg-molsider: 0.631855 test loss: 0.618776
[Epoch 123; Iter    26/   32] train: loss: 0.2465590
[Epoch 123] ogbg-molsider: 0.647011 val loss: 0.609452
[Epoch 123] ogbg-molsider: 0.645922 test loss: 0.597076
[Epoch 124; Iter    24/   32] train: loss: 0.2657948
[Epoch 124] ogbg-molsider: 0.644733 val loss: 0.634969
[Epoch 124] ogbg-molsider: 0.640630 test loss: 0.617813
[Epoch 125; Iter    22/   32] train: loss: 0.2658677
[Epoch 125] ogbg-molsider: 0.641272 val loss: 0.606029
[Epoch 125] ogbg-molsider: 0.639176 test loss: 0.601173
[Epoch 126; Iter    20/   32] train: loss: 0.2606749
[Epoch 126] ogbg-molsider: 0.644347 val loss: 0.631620
[Epoch 126] ogbg-molsider: 0.641152 test loss: 0.624314
[Epoch 127; Iter    18/   32] train: loss: 0.2685217
[Epoch 127] ogbg-molsider: 0.640811 val loss: 0.632225
[Epoch 127] ogbg-molsider: 0.644013 test loss: 0.617284
[Epoch 128; Iter    16/   32] train: loss: 0.2556076
[Epoch 128] ogbg-molsider: 0.639088 val loss: 0.619197
[Epoch 128] ogbg-molsider: 0.641255 test loss: 0.607129
[Epoch 129; Iter    14/   32] train: loss: 0.2125250
[Epoch 129] ogbg-molsider: 0.645501 val loss: 0.617063
[Epoch 129] ogbg-molsider: 0.644996 test loss: 0.601407
[Epoch 130; Iter    12/   32] train: loss: 0.2746203
[Epoch 130] ogbg-molsider: 0.641094 val loss: 0.630319
[Epoch 130] ogbg-molsider: 0.638421 test loss: 0.624312
[Epoch 131; Iter    10/   32] train: loss: 0.2335111
[Epoch 131] ogbg-molsider: 0.637455 val loss: 0.624008
[Epoch 131] ogbg-molsider: 0.633222 test loss: 0.617346
[Epoch 132; Iter     8/   32] train: loss: 0.3123999
[Epoch 132] ogbg-molsider: 0.647632 val loss: 0.631961
[Epoch 132] ogbg-molsider: 0.638599 test loss: 0.629201
[Epoch 133; Iter     6/   32] train: loss: 0.2479779
[Epoch 133] ogbg-molsider: 0.644679 val loss: 0.635821
[Epoch 81; Iter    30/   36] train: loss: 0.4161048
[Epoch 81] ogbg-molsider: 0.647143 val loss: 0.541147
[Epoch 81] ogbg-molsider: 0.653785 test loss: 0.571227
[Epoch 82; Iter    24/   36] train: loss: 0.3626823
[Epoch 82] ogbg-molsider: 0.650009 val loss: 0.537782
[Epoch 82] ogbg-molsider: 0.640928 test loss: 0.559779
[Epoch 83; Iter    18/   36] train: loss: 0.3567697
[Epoch 83] ogbg-molsider: 0.655967 val loss: 0.527319
[Epoch 83] ogbg-molsider: 0.663823 test loss: 0.526662
[Epoch 84; Iter    12/   36] train: loss: 0.3536072
[Epoch 84] ogbg-molsider: 0.649754 val loss: 0.540982
[Epoch 84] ogbg-molsider: 0.664956 test loss: 0.565425
[Epoch 85; Iter     6/   36] train: loss: 0.3060541
[Epoch 85; Iter    36/   36] train: loss: 0.3792887
[Epoch 85] ogbg-molsider: 0.641141 val loss: 0.532857
[Epoch 85] ogbg-molsider: 0.661562 test loss: 0.558627
[Epoch 86; Iter    30/   36] train: loss: 0.3607647
[Epoch 86] ogbg-molsider: 0.645355 val loss: 0.541109
[Epoch 86] ogbg-molsider: 0.666871 test loss: 0.562731
[Epoch 87; Iter    24/   36] train: loss: 0.3578310
[Epoch 87] ogbg-molsider: 0.653353 val loss: 0.620432
[Epoch 87] ogbg-molsider: 0.664504 test loss: 0.618292
[Epoch 88; Iter    18/   36] train: loss: 0.3337807
[Epoch 88] ogbg-molsider: 0.657982 val loss: 0.534750
[Epoch 88] ogbg-molsider: 0.665507 test loss: 0.547056
[Epoch 89; Iter    12/   36] train: loss: 0.3041972
[Epoch 89] ogbg-molsider: 0.654607 val loss: 0.534963
[Epoch 89] ogbg-molsider: 0.672219 test loss: 0.562282
[Epoch 90; Iter     6/   36] train: loss: 0.3325722
[Epoch 90; Iter    36/   36] train: loss: 0.3351110
[Epoch 90] ogbg-molsider: 0.651653 val loss: 0.539023
[Epoch 90] ogbg-molsider: 0.672208 test loss: 0.554303
[Epoch 91; Iter    30/   36] train: loss: 0.2697738
[Epoch 91] ogbg-molsider: 0.654618 val loss: 0.545073
[Epoch 91] ogbg-molsider: 0.663859 test loss: 0.552575
[Epoch 92; Iter    24/   36] train: loss: 0.3656990
[Epoch 92] ogbg-molsider: 0.654021 val loss: 0.549114
[Epoch 92] ogbg-molsider: 0.655519 test loss: 0.575077
[Epoch 93; Iter    18/   36] train: loss: 0.2889584
[Epoch 93] ogbg-molsider: 0.647592 val loss: 0.542583
[Epoch 93] ogbg-molsider: 0.671490 test loss: 0.556764
[Epoch 94; Iter    12/   36] train: loss: 0.2805256
[Epoch 94] ogbg-molsider: 0.646021 val loss: 0.559994
[Epoch 94] ogbg-molsider: 0.660249 test loss: 0.569203
[Epoch 95; Iter     6/   36] train: loss: 0.2663174
[Epoch 95; Iter    36/   36] train: loss: 0.3226581
[Epoch 95] ogbg-molsider: 0.656124 val loss: 0.547335
[Epoch 95] ogbg-molsider: 0.657415 test loss: 0.580231
[Epoch 96; Iter    30/   36] train: loss: 0.3217617
[Epoch 96] ogbg-molsider: 0.653959 val loss: 0.556442
[Epoch 96] ogbg-molsider: 0.655056 test loss: 0.585438
[Epoch 97; Iter    24/   36] train: loss: 0.2730446
[Epoch 97] ogbg-molsider: 0.653547 val loss: 0.560094
[Epoch 97] ogbg-molsider: 0.655237 test loss: 0.599442
[Epoch 98; Iter    18/   36] train: loss: 0.2700768
[Epoch 98] ogbg-molsider: 0.654744 val loss: 0.555641
[Epoch 98] ogbg-molsider: 0.648560 test loss: 0.607885
[Epoch 99; Iter    12/   36] train: loss: 0.2989530
[Epoch 99] ogbg-molsider: 0.640602 val loss: 0.581468
[Epoch 99] ogbg-molsider: 0.649212 test loss: 0.579952
[Epoch 100; Iter     6/   36] train: loss: 0.2621361
[Epoch 100; Iter    36/   36] train: loss: 0.3017190
[Epoch 100] ogbg-molsider: 0.640340 val loss: 0.573767
[Epoch 100] ogbg-molsider: 0.652549 test loss: 0.597601
[Epoch 101; Iter    30/   36] train: loss: 0.2926781
[Epoch 101] ogbg-molsider: 0.651088 val loss: 0.574087
[Epoch 101] ogbg-molsider: 0.653429 test loss: 0.587836
[Epoch 102; Iter    24/   36] train: loss: 0.2756095
[Epoch 102] ogbg-molsider: 0.634683 val loss: 0.579784
[Epoch 102] ogbg-molsider: 0.661930 test loss: 0.600798
[Epoch 103; Iter    18/   36] train: loss: 0.3499588
[Epoch 103] ogbg-molsider: 0.645730 val loss: 0.564252
[Epoch 103] ogbg-molsider: 0.657599 test loss: 0.580400
[Epoch 104; Iter    12/   36] train: loss: 0.2698334
[Epoch 104] ogbg-molsider: 0.630289 val loss: 0.589990
[Epoch 104] ogbg-molsider: 0.667771 test loss: 0.591146
[Epoch 105; Iter     6/   36] train: loss: 0.2771487
[Epoch 105; Iter    36/   36] train: loss: 0.3206216
[Epoch 105] ogbg-molsider: 0.636116 val loss: 0.584657
[Epoch 105] ogbg-molsider: 0.658872 test loss: 0.586359
[Epoch 106; Iter    30/   36] train: loss: 0.2442970
[Epoch 106] ogbg-molsider: 0.642288 val loss: 0.617394
[Epoch 106] ogbg-molsider: 0.646070 test loss: 0.609756
[Epoch 107; Iter    24/   36] train: loss: 0.2594730
[Epoch 107] ogbg-molsider: 0.635384 val loss: 0.586009
[Epoch 107] ogbg-molsider: 0.652989 test loss: 0.604386
[Epoch 108; Iter    18/   36] train: loss: 0.3171758
[Epoch 108] ogbg-molsider: 0.626832 val loss: 0.682782
[Epoch 108] ogbg-molsider: 0.653356 test loss: 0.609927
[Epoch 109; Iter    12/   36] train: loss: 0.2593710
[Epoch 109] ogbg-molsider: 0.646737 val loss: 0.576969
[Epoch 109] ogbg-molsider: 0.654032 test loss: 0.604674
[Epoch 110; Iter     6/   36] train: loss: 0.3174853
[Epoch 110; Iter    36/   36] train: loss: 0.3042052
[Epoch 110] ogbg-molsider: 0.651852 val loss: 0.609223
[Epoch 110] ogbg-molsider: 0.653781 test loss: 0.590183
[Epoch 111; Iter    30/   36] train: loss: 0.3074680
[Epoch 111] ogbg-molsider: 0.651773 val loss: 0.594471
[Epoch 111] ogbg-molsider: 0.644711 test loss: 0.633799
[Epoch 112; Iter    24/   36] train: loss: 0.2489932
[Epoch 112] ogbg-molsider: 0.654554 val loss: 0.583506
[Epoch 112] ogbg-molsider: 0.659731 test loss: 0.592520
[Epoch 113; Iter    18/   36] train: loss: 0.2930118
[Epoch 113] ogbg-molsider: 0.641906 val loss: 0.600329
[Epoch 113] ogbg-molsider: 0.652147 test loss: 0.631649
[Epoch 114; Iter    12/   36] train: loss: 0.2756551
[Epoch 114] ogbg-molsider: 0.644189 val loss: 0.595155
[Epoch 114] ogbg-molsider: 0.650192 test loss: 0.612627
[Epoch 115; Iter     6/   36] train: loss: 0.2635907
[Epoch 115; Iter    36/   36] train: loss: 0.2394811
[Epoch 115] ogbg-molsider: 0.637249 val loss: 0.602323
[Epoch 115] ogbg-molsider: 0.652831 test loss: 0.604258
[Epoch 116; Iter    30/   36] train: loss: 0.2480447
[Epoch 116] ogbg-molsider: 0.638676 val loss: 0.612879
[Epoch 116] ogbg-molsider: 0.651815 test loss: 0.633957
[Epoch 117; Iter    24/   36] train: loss: 0.2572245
[Epoch 117] ogbg-molsider: 0.644345 val loss: 0.591482
[Epoch 117] ogbg-molsider: 0.649248 test loss: 0.630956
[Epoch 118; Iter    18/   36] train: loss: 0.2494088
[Epoch 118] ogbg-molsider: 0.634643 val loss: 0.611978
[Epoch 118] ogbg-molsider: 0.645775 test loss: 0.637582
[Epoch 119; Iter    12/   36] train: loss: 0.2538707
[Epoch 119] ogbg-molsider: 0.627623 val loss: 0.629990
[Epoch 119] ogbg-molsider: 0.652041 test loss: 0.620067
[Epoch 120; Iter     6/   36] train: loss: 0.2467538
[Epoch 120; Iter    36/   36] train: loss: 0.2498087
[Epoch 120] ogbg-molsider: 0.641429 val loss: 0.623952
[Epoch 120] ogbg-molsider: 0.644921 test loss: 0.645589
[Epoch 121; Iter    30/   36] train: loss: 0.2267692
[Epoch 121] ogbg-molsider: 0.639146 val loss: 0.623594
[Epoch 121] ogbg-molsider: 0.657576 test loss: 0.638406
[Epoch 122; Iter    24/   36] train: loss: 0.2257050
[Epoch 122] ogbg-molsider: 0.638843 val loss: 0.626335
[Epoch 122] ogbg-molsider: 0.648824 test loss: 0.630207
[Epoch 123; Iter    18/   36] train: loss: 0.2370023
[Epoch 123] ogbg-molsider: 0.638388 val loss: 0.682311
[Epoch 123] ogbg-molsider: 0.656908 test loss: 0.628921
[Epoch 124; Iter    12/   36] train: loss: 0.2575230
[Epoch 124] ogbg-molsider: 0.642269 val loss: 0.681348
[Epoch 124] ogbg-molsider: 0.648607 test loss: 0.643976
[Epoch 125; Iter     6/   36] train: loss: 0.2848517
[Epoch 125; Iter    36/   36] train: loss: 0.2385704
[Epoch 125] ogbg-molsider: 0.635856 val loss: 0.689896
[Epoch 125] ogbg-molsider: 0.649568 test loss: 0.654668
[Epoch 126; Iter    30/   36] train: loss: 0.2287066
[Epoch 126] ogbg-molsider: 0.633935 val loss: 0.627759
[Epoch 126] ogbg-molsider: 0.643512 test loss: 0.639811
[Epoch 127; Iter    24/   36] train: loss: 0.2160276
[Epoch 127] ogbg-molsider: 0.630063 val loss: 0.666295
[Epoch 127] ogbg-molsider: 0.637670 test loss: 0.659137
[Epoch 128; Iter    18/   36] train: loss: 0.2122348
[Epoch 81; Iter    30/   36] train: loss: 0.3615417
[Epoch 81] ogbg-molsider: 0.675737 val loss: 0.500483
[Epoch 81] ogbg-molsider: 0.663736 test loss: 0.535662
[Epoch 82; Iter    24/   36] train: loss: 0.3521224
[Epoch 82] ogbg-molsider: 0.671777 val loss: 0.501689
[Epoch 82] ogbg-molsider: 0.667658 test loss: 0.540413
[Epoch 83; Iter    18/   36] train: loss: 0.2950581
[Epoch 83] ogbg-molsider: 0.668141 val loss: 0.503149
[Epoch 83] ogbg-molsider: 0.663201 test loss: 0.547964
[Epoch 84; Iter    12/   36] train: loss: 0.3155491
[Epoch 84] ogbg-molsider: 0.679563 val loss: 0.508938
[Epoch 84] ogbg-molsider: 0.662879 test loss: 0.545781
[Epoch 85; Iter     6/   36] train: loss: 0.3269210
[Epoch 85; Iter    36/   36] train: loss: 0.3124679
[Epoch 85] ogbg-molsider: 0.660449 val loss: 0.533746
[Epoch 85] ogbg-molsider: 0.665782 test loss: 0.562388
[Epoch 86; Iter    30/   36] train: loss: 0.3377125
[Epoch 86] ogbg-molsider: 0.659217 val loss: 0.525949
[Epoch 86] ogbg-molsider: 0.661591 test loss: 0.551463
[Epoch 87; Iter    24/   36] train: loss: 0.3068602
[Epoch 87] ogbg-molsider: 0.664274 val loss: 0.519791
[Epoch 87] ogbg-molsider: 0.667976 test loss: 0.547963
[Epoch 88; Iter    18/   36] train: loss: 0.3192558
[Epoch 88] ogbg-molsider: 0.661813 val loss: 0.543143
[Epoch 88] ogbg-molsider: 0.669643 test loss: 0.593683
[Epoch 89; Iter    12/   36] train: loss: 0.3131223
[Epoch 89] ogbg-molsider: 0.678586 val loss: 0.519261
[Epoch 89] ogbg-molsider: 0.659097 test loss: 0.567248
[Epoch 90; Iter     6/   36] train: loss: 0.3036144
[Epoch 90; Iter    36/   36] train: loss: 0.3247840
[Epoch 90] ogbg-molsider: 0.669144 val loss: 0.527849
[Epoch 90] ogbg-molsider: 0.665966 test loss: 0.575459
[Epoch 91; Iter    30/   36] train: loss: 0.3019005
[Epoch 91] ogbg-molsider: 0.668970 val loss: 0.541789
[Epoch 91] ogbg-molsider: 0.670113 test loss: 0.573430
[Epoch 92; Iter    24/   36] train: loss: 0.2591645
[Epoch 92] ogbg-molsider: 0.680598 val loss: 0.516518
[Epoch 92] ogbg-molsider: 0.661450 test loss: 0.558492
[Epoch 93; Iter    18/   36] train: loss: 0.2756031
[Epoch 93] ogbg-molsider: 0.672359 val loss: 0.527817
[Epoch 93] ogbg-molsider: 0.665932 test loss: 0.564220
[Epoch 94; Iter    12/   36] train: loss: 0.3180207
[Epoch 94] ogbg-molsider: 0.660121 val loss: 0.534299
[Epoch 94] ogbg-molsider: 0.650722 test loss: 0.578952
[Epoch 95; Iter     6/   36] train: loss: 0.3202484
[Epoch 95; Iter    36/   36] train: loss: 0.3041648
[Epoch 95] ogbg-molsider: 0.639488 val loss: 0.562261
[Epoch 95] ogbg-molsider: 0.664457 test loss: 0.569931
[Epoch 96; Iter    30/   36] train: loss: 0.2718092
[Epoch 96] ogbg-molsider: 0.668770 val loss: 0.538821
[Epoch 96] ogbg-molsider: 0.660072 test loss: 0.578420
[Epoch 97; Iter    24/   36] train: loss: 0.2993103
[Epoch 97] ogbg-molsider: 0.672861 val loss: 0.534054
[Epoch 97] ogbg-molsider: 0.664582 test loss: 0.581212
[Epoch 98; Iter    18/   36] train: loss: 0.3021894
[Epoch 98] ogbg-molsider: 0.653262 val loss: 0.565483
[Epoch 98] ogbg-molsider: 0.653537 test loss: 0.598837
[Epoch 99; Iter    12/   36] train: loss: 0.3293966
[Epoch 99] ogbg-molsider: 0.665462 val loss: 0.548961
[Epoch 99] ogbg-molsider: 0.663556 test loss: 0.597523
[Epoch 100; Iter     6/   36] train: loss: 0.2861607
[Epoch 100; Iter    36/   36] train: loss: 0.2682285
[Epoch 100] ogbg-molsider: 0.655549 val loss: 0.544675
[Epoch 100] ogbg-molsider: 0.672541 test loss: 0.557107
[Epoch 101; Iter    30/   36] train: loss: 0.3095944
[Epoch 101] ogbg-molsider: 0.656834 val loss: 0.564884
[Epoch 101] ogbg-molsider: 0.671537 test loss: 0.568304
[Epoch 102; Iter    24/   36] train: loss: 0.2942741
[Epoch 102] ogbg-molsider: 0.664516 val loss: 0.559895
[Epoch 102] ogbg-molsider: 0.664323 test loss: 0.570927
[Epoch 103; Iter    18/   36] train: loss: 0.2669178
[Epoch 103] ogbg-molsider: 0.656649 val loss: 0.588770
[Epoch 103] ogbg-molsider: 0.652439 test loss: 0.603616
[Epoch 104; Iter    12/   36] train: loss: 0.2476295
[Epoch 104] ogbg-molsider: 0.654611 val loss: 0.563719
[Epoch 104] ogbg-molsider: 0.668519 test loss: 0.579041
[Epoch 105; Iter     6/   36] train: loss: 0.2779832
[Epoch 105; Iter    36/   36] train: loss: 0.2547461
[Epoch 105] ogbg-molsider: 0.662413 val loss: 0.559907
[Epoch 105] ogbg-molsider: 0.665952 test loss: 0.585867
[Epoch 106; Iter    30/   36] train: loss: 0.2762422
[Epoch 106] ogbg-molsider: 0.656493 val loss: 0.573754
[Epoch 106] ogbg-molsider: 0.654100 test loss: 0.606752
[Epoch 107; Iter    24/   36] train: loss: 0.2711147
[Epoch 107] ogbg-molsider: 0.658184 val loss: 0.571344
[Epoch 107] ogbg-molsider: 0.667373 test loss: 0.587018
[Epoch 108; Iter    18/   36] train: loss: 0.2778684
[Epoch 108] ogbg-molsider: 0.656964 val loss: 0.573513
[Epoch 108] ogbg-molsider: 0.667921 test loss: 0.588830
[Epoch 109; Iter    12/   36] train: loss: 0.2537592
[Epoch 109] ogbg-molsider: 0.662001 val loss: 0.570853
[Epoch 109] ogbg-molsider: 0.668993 test loss: 0.592368
[Epoch 110; Iter     6/   36] train: loss: 0.2488636
[Epoch 110; Iter    36/   36] train: loss: 0.3319314
[Epoch 110] ogbg-molsider: 0.661070 val loss: 0.575704
[Epoch 110] ogbg-molsider: 0.667401 test loss: 0.605501
[Epoch 111; Iter    30/   36] train: loss: 0.2698774
[Epoch 111] ogbg-molsider: 0.657842 val loss: 0.586473
[Epoch 111] ogbg-molsider: 0.661578 test loss: 0.602192
[Epoch 112; Iter    24/   36] train: loss: 0.2470766
[Epoch 112] ogbg-molsider: 0.654264 val loss: 0.589189
[Epoch 112] ogbg-molsider: 0.662358 test loss: 0.608711
[Epoch 113; Iter    18/   36] train: loss: 0.2554290
[Epoch 113] ogbg-molsider: 0.660491 val loss: 0.591755
[Epoch 113] ogbg-molsider: 0.663261 test loss: 0.609036
[Epoch 114; Iter    12/   36] train: loss: 0.2461219
[Epoch 114] ogbg-molsider: 0.650464 val loss: 0.596647
[Epoch 114] ogbg-molsider: 0.661076 test loss: 0.603626
[Epoch 115; Iter     6/   36] train: loss: 0.2028308
[Epoch 115; Iter    36/   36] train: loss: 0.2879578
[Epoch 115] ogbg-molsider: 0.661729 val loss: 0.586806
[Epoch 115] ogbg-molsider: 0.660226 test loss: 0.620349
[Epoch 116; Iter    30/   36] train: loss: 0.2537941
[Epoch 116] ogbg-molsider: 0.659540 val loss: 0.601087
[Epoch 116] ogbg-molsider: 0.664868 test loss: 0.607001
[Epoch 117; Iter    24/   36] train: loss: 0.3158287
[Epoch 117] ogbg-molsider: 0.661147 val loss: 0.581362
[Epoch 117] ogbg-molsider: 0.664643 test loss: 0.609268
[Epoch 118; Iter    18/   36] train: loss: 0.2522916
[Epoch 118] ogbg-molsider: 0.659716 val loss: 0.597113
[Epoch 118] ogbg-molsider: 0.658375 test loss: 0.620695
[Epoch 119; Iter    12/   36] train: loss: 0.2753121
[Epoch 119] ogbg-molsider: 0.661109 val loss: 0.588531
[Epoch 119] ogbg-molsider: 0.657841 test loss: 0.609378
[Epoch 120; Iter     6/   36] train: loss: 0.2175962
[Epoch 120; Iter    36/   36] train: loss: 0.2669321
[Epoch 120] ogbg-molsider: 0.650934 val loss: 0.602095
[Epoch 120] ogbg-molsider: 0.666439 test loss: 0.607173
[Epoch 121; Iter    30/   36] train: loss: 0.2652468
[Epoch 121] ogbg-molsider: 0.658889 val loss: 0.597360
[Epoch 121] ogbg-molsider: 0.663147 test loss: 0.601727
[Epoch 122; Iter    24/   36] train: loss: 0.2680448
[Epoch 122] ogbg-molsider: 0.655544 val loss: 0.599709
[Epoch 122] ogbg-molsider: 0.657075 test loss: 0.618335
[Epoch 123; Iter    18/   36] train: loss: 0.2929924
[Epoch 123] ogbg-molsider: 0.657448 val loss: 0.608133
[Epoch 123] ogbg-molsider: 0.663963 test loss: 0.613768
[Epoch 124; Iter    12/   36] train: loss: 0.2695787
[Epoch 124] ogbg-molsider: 0.656967 val loss: 0.608179
[Epoch 124] ogbg-molsider: 0.660677 test loss: 0.630383
[Epoch 125; Iter     6/   36] train: loss: 0.2580795
[Epoch 125; Iter    36/   36] train: loss: 0.2209800
[Epoch 125] ogbg-molsider: 0.664050 val loss: 0.606528
[Epoch 125] ogbg-molsider: 0.662532 test loss: 0.640475
[Epoch 126; Iter    30/   36] train: loss: 0.2391244
[Epoch 126] ogbg-molsider: 0.662224 val loss: 0.602439
[Epoch 126] ogbg-molsider: 0.662280 test loss: 0.625684
[Epoch 127; Iter    24/   36] train: loss: 0.2074879
[Epoch 127] ogbg-molsider: 0.662127 val loss: 0.624674
[Epoch 127] ogbg-molsider: 0.652646 test loss: 0.645158
[Epoch 128; Iter    18/   36] train: loss: 0.2320168
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.636010 val loss: 0.693091
[Epoch 128] ogbg-molsider: 0.638570 test loss: 0.670303
Early stopping criterion based on -ogbg-molsider- that should be max reached after 128 epochs. Best model checkpoint was in epoch 68.
Statistics on  val_best_checkpoint
mean_pred: 0.4072369337081909
std_pred: 2.1537368297576904
mean_targets: 0.6021755933761597
std_targets: 0.48951220512390137
prcauc: 0.6993864352415055
rocauc: 0.6800645020659026
ogbg-molsider: 0.6800645020659026
OGBNanLabelBCEWithLogitsLoss: 0.489654666185379
Statistics on  test
mean_pred: 0.14371219277381897
std_pred: 1.9953279495239258
mean_targets: 0.5446775555610657
std_targets: 0.49806439876556396
prcauc: 0.6588616396620236
rocauc: 0.6278852041184574
ogbg-molsider: 0.6278852041184574
OGBNanLabelBCEWithLogitsLoss: 0.5099981307983399
Statistics on  train
mean_pred: 0.2028212547302246
std_pred: 2.25626802444458
mean_targets: 0.5661051273345947
std_targets: 0.49561887979507446
prcauc: 0.7613752081304268
rocauc: 0.8008110785909008
ogbg-molsider: 0.8008110785909008
OGBNanLabelBCEWithLogitsLoss: 0.4194715801212523
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.648910 val loss: 0.629752
[Epoch 128] ogbg-molsider: 0.666915 test loss: 0.624035
[Epoch 129; Iter    12/   36] train: loss: 0.2090891
[Epoch 129] ogbg-molsider: 0.649679 val loss: 0.647523
[Epoch 129] ogbg-molsider: 0.663816 test loss: 0.629147
[Epoch 130; Iter     6/   36] train: loss: 0.2287482
[Epoch 130; Iter    36/   36] train: loss: 0.2324073
[Epoch 130] ogbg-molsider: 0.657027 val loss: 0.623405
[Epoch 130] ogbg-molsider: 0.663818 test loss: 0.631116
[Epoch 131; Iter    30/   36] train: loss: 0.2158889
[Epoch 131] ogbg-molsider: 0.659532 val loss: 0.612568
[Epoch 131] ogbg-molsider: 0.664890 test loss: 0.630787
[Epoch 132; Iter    24/   36] train: loss: 0.1993761
[Epoch 132] ogbg-molsider: 0.665512 val loss: 0.612544
[Epoch 132] ogbg-molsider: 0.660017 test loss: 0.637462
Early stopping criterion based on -ogbg-molsider- that should be max reached after 132 epochs. Best model checkpoint was in epoch 72.
Statistics on  val_best_checkpoint
mean_pred: 0.44185078144073486
std_pred: 2.2649924755096436
mean_targets: 0.6021755933761597
std_targets: 0.48951220512390137
prcauc: 0.7150566110602307
rocauc: 0.6826283356375051
ogbg-molsider: 0.6826283356375051
OGBNanLabelBCEWithLogitsLoss: 0.47973465323448183
Statistics on  test
mean_pred: 0.2100210040807724
std_pred: 2.087007999420166
mean_targets: 0.5446775555610657
std_targets: 0.49806439876556396
prcauc: 0.6531838974264247
rocauc: 0.6258116799971972
ogbg-molsider: 0.6258116799971972
OGBNanLabelBCEWithLogitsLoss: 0.5315988957881927
Statistics on  train
mean_pred: 0.32936882972717285
std_pred: 2.320399522781372
mean_targets: 0.5661051273345947
std_targets: 0.49561887979507446
prcauc: 0.7920117168965716
rocauc: 0.841938802755019
ogbg-molsider: 0.841938802755019
OGBNanLabelBCEWithLogitsLoss: 0.3813490072886149
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 133] ogbg-molsider: 0.648390 test loss: 0.622200
[Epoch 134; Iter     4/   32] train: loss: 0.2797781
[Epoch 134] ogbg-molsider: 0.641857 val loss: 0.628200
[Epoch 134] ogbg-molsider: 0.638938 test loss: 0.627008
[Epoch 135; Iter     2/   32] train: loss: 0.2279804
[Epoch 135; Iter    32/   32] train: loss: 0.4081388
[Epoch 135] ogbg-molsider: 0.638019 val loss: 0.644507
[Epoch 135] ogbg-molsider: 0.647615 test loss: 0.624779
[Epoch 136; Iter    30/   32] train: loss: 0.2769266
[Epoch 136] ogbg-molsider: 0.646676 val loss: 0.641252
[Epoch 136] ogbg-molsider: 0.638715 test loss: 0.633789
[Epoch 137; Iter    28/   32] train: loss: 0.2525567
[Epoch 137] ogbg-molsider: 0.640871 val loss: 0.638154
[Epoch 137] ogbg-molsider: 0.636694 test loss: 0.631873
[Epoch 138; Iter    26/   32] train: loss: 0.2645053
[Epoch 138] ogbg-molsider: 0.645292 val loss: 0.628260
[Epoch 138] ogbg-molsider: 0.641853 test loss: 0.624420
[Epoch 139; Iter    24/   32] train: loss: 0.2323664
[Epoch 139] ogbg-molsider: 0.642141 val loss: 0.632001
[Epoch 139] ogbg-molsider: 0.641854 test loss: 0.621793
[Epoch 140; Iter    22/   32] train: loss: 0.2430142
[Epoch 140] ogbg-molsider: 0.641893 val loss: 0.630423
[Epoch 140] ogbg-molsider: 0.640921 test loss: 0.622544
[Epoch 141; Iter    20/   32] train: loss: 0.2549884
[Epoch 141] ogbg-molsider: 0.643475 val loss: 0.643276
[Epoch 141] ogbg-molsider: 0.643527 test loss: 0.627217
[Epoch 142; Iter    18/   32] train: loss: 0.2304509
[Epoch 142] ogbg-molsider: 0.636962 val loss: 0.650968
[Epoch 142] ogbg-molsider: 0.637579 test loss: 0.632314
[Epoch 143; Iter    16/   32] train: loss: 0.2631160
[Epoch 143] ogbg-molsider: 0.644475 val loss: 0.645416
[Epoch 143] ogbg-molsider: 0.641196 test loss: 0.633701
[Epoch 144; Iter    14/   32] train: loss: 0.2421667
[Epoch 144] ogbg-molsider: 0.640916 val loss: 0.642644
[Epoch 144] ogbg-molsider: 0.642886 test loss: 0.623273
[Epoch 145; Iter    12/   32] train: loss: 0.2632918
[Epoch 145] ogbg-molsider: 0.647121 val loss: 0.649313
[Epoch 145] ogbg-molsider: 0.640282 test loss: 0.639854
[Epoch 146; Iter    10/   32] train: loss: 0.2588222
[Epoch 146] ogbg-molsider: 0.636908 val loss: 0.657224
[Epoch 146] ogbg-molsider: 0.644264 test loss: 0.631372
[Epoch 147; Iter     8/   32] train: loss: 0.2792524
[Epoch 147] ogbg-molsider: 0.643688 val loss: 0.649126
[Epoch 147] ogbg-molsider: 0.641470 test loss: 0.628911
[Epoch 148; Iter     6/   32] train: loss: 0.2469462
[Epoch 148] ogbg-molsider: 0.638157 val loss: 0.664182
[Epoch 148] ogbg-molsider: 0.637354 test loss: 0.650665
[Epoch 149; Iter     4/   32] train: loss: 0.2525305
[Epoch 149] ogbg-molsider: 0.640815 val loss: 0.659812
[Epoch 149] ogbg-molsider: 0.642374 test loss: 0.641861
[Epoch 150; Iter     2/   32] train: loss: 0.2978145
[Epoch 150; Iter    32/   32] train: loss: 0.1835127
[Epoch 150] ogbg-molsider: 0.640652 val loss: 0.655464
[Epoch 150] ogbg-molsider: 0.639510 test loss: 0.638744
[Epoch 151; Iter    30/   32] train: loss: 0.2757008
[Epoch 151] ogbg-molsider: 0.641417 val loss: 0.661181
[Epoch 151] ogbg-molsider: 0.634710 test loss: 0.646074
[Epoch 152; Iter    28/   32] train: loss: 0.2431144
[Epoch 152] ogbg-molsider: 0.640363 val loss: 0.661471
[Epoch 152] ogbg-molsider: 0.645033 test loss: 0.634079
[Epoch 153; Iter    26/   32] train: loss: 0.2240369
[Epoch 153] ogbg-molsider: 0.643041 val loss: 0.658952
[Epoch 153] ogbg-molsider: 0.642801 test loss: 0.643199
[Epoch 154; Iter    24/   32] train: loss: 0.2641388
[Epoch 154] ogbg-molsider: 0.639275 val loss: 0.663509
[Epoch 154] ogbg-molsider: 0.642378 test loss: 0.646979
[Epoch 155; Iter    22/   32] train: loss: 0.2170844
[Epoch 155] ogbg-molsider: 0.642097 val loss: 0.657259
[Epoch 155] ogbg-molsider: 0.645475 test loss: 0.638117
[Epoch 156; Iter    20/   32] train: loss: 0.2695829
[Epoch 156] ogbg-molsider: 0.637710 val loss: 0.658409
[Epoch 156] ogbg-molsider: 0.643960 test loss: 0.633951
Early stopping criterion based on -ogbg-molsider- that should be max reached after 156 epochs. Best model checkpoint was in epoch 96.
Statistics on  val_best_checkpoint
mean_pred: 0.4753541350364685
std_pred: 2.788266658782959
mean_targets: 0.5732086896896362
std_targets: 0.49465426802635193
prcauc: 0.671303174142193
rocauc: 0.6543004018015405
ogbg-molsider: 0.6543004018015405
OGBNanLabelBCEWithLogitsLoss: 0.5542554557323456
Statistics on  test
mean_pred: 0.49492979049682617
std_pred: 2.7303357124328613
mean_targets: 0.5714039206504822
std_targets: 0.4949178397655487
prcauc: 0.6706283905422343
rocauc: 0.6509459931583398
ogbg-molsider: 0.6509459931583398
OGBNanLabelBCEWithLogitsLoss: 0.562319781099047
Statistics on  train
mean_pred: 0.5081921815872192
std_pred: 2.8623275756835938
mean_targets: 0.5655384659767151
std_targets: 0.4956952929496765
prcauc: 0.8606141222122674
rocauc: 0.9019895381842069
ogbg-molsider: 0.9019895381842069
OGBNanLabelBCEWithLogitsLoss: 0.3085824027657509
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 133] ogbg-molsider: 0.654349 test loss: 0.616256
[Epoch 134; Iter     4/   32] train: loss: 0.2569950
[Epoch 134] ogbg-molsider: 0.653810 val loss: 0.638507
[Epoch 134] ogbg-molsider: 0.650870 test loss: 0.640157
[Epoch 135; Iter     2/   32] train: loss: 0.2560859
[Epoch 135; Iter    32/   32] train: loss: 0.2449687
[Epoch 135] ogbg-molsider: 0.660078 val loss: 0.626726
[Epoch 135] ogbg-molsider: 0.658347 test loss: 0.625600
[Epoch 136; Iter    30/   32] train: loss: 0.2543830
[Epoch 136] ogbg-molsider: 0.655320 val loss: 0.644226
[Epoch 136] ogbg-molsider: 0.660220 test loss: 0.635388
[Epoch 137; Iter    28/   32] train: loss: 0.2360525
[Epoch 137] ogbg-molsider: 0.652271 val loss: 0.634289
[Epoch 137] ogbg-molsider: 0.654537 test loss: 0.632880
[Epoch 138; Iter    26/   32] train: loss: 0.2330402
[Epoch 138] ogbg-molsider: 0.655298 val loss: 0.646932
[Epoch 138] ogbg-molsider: 0.661940 test loss: 0.632432
[Epoch 139; Iter    24/   32] train: loss: 0.2190019
[Epoch 139] ogbg-molsider: 0.652778 val loss: 0.639230
[Epoch 139] ogbg-molsider: 0.664783 test loss: 0.622362
[Epoch 140; Iter    22/   32] train: loss: 0.2504276
[Epoch 140] ogbg-molsider: 0.655001 val loss: 0.654816
[Epoch 140] ogbg-molsider: 0.656989 test loss: 0.656356
[Epoch 141; Iter    20/   32] train: loss: 0.2376180
[Epoch 141] ogbg-molsider: 0.655126 val loss: 0.642472
[Epoch 141] ogbg-molsider: 0.658869 test loss: 0.648182
[Epoch 142; Iter    18/   32] train: loss: 0.2695295
[Epoch 142] ogbg-molsider: 0.649940 val loss: 0.660607
[Epoch 142] ogbg-molsider: 0.658036 test loss: 0.645057
[Epoch 143; Iter    16/   32] train: loss: 0.2572555
[Epoch 143] ogbg-molsider: 0.657885 val loss: 0.652588
[Epoch 143] ogbg-molsider: 0.657751 test loss: 0.648556
[Epoch 144; Iter    14/   32] train: loss: 0.2236493
[Epoch 144] ogbg-molsider: 0.657878 val loss: 0.659255
[Epoch 144] ogbg-molsider: 0.659055 test loss: 0.658448
[Epoch 145; Iter    12/   32] train: loss: 0.2114121
[Epoch 145] ogbg-molsider: 0.658118 val loss: 0.651511
[Epoch 145] ogbg-molsider: 0.664029 test loss: 0.646213
[Epoch 146; Iter    10/   32] train: loss: 0.2689095
[Epoch 146] ogbg-molsider: 0.661038 val loss: 0.660218
[Epoch 146] ogbg-molsider: 0.662088 test loss: 0.651426
[Epoch 147; Iter     8/   32] train: loss: 0.2412233
[Epoch 147] ogbg-molsider: 0.657750 val loss: 0.664497
[Epoch 147] ogbg-molsider: 0.647437 test loss: 0.695048
[Epoch 148; Iter     6/   32] train: loss: 0.2196463
[Epoch 148] ogbg-molsider: 0.657777 val loss: 0.649399
[Epoch 148] ogbg-molsider: 0.656130 test loss: 0.657022
[Epoch 149; Iter     4/   32] train: loss: 0.2416028
[Epoch 149] ogbg-molsider: 0.655710 val loss: 0.666871
[Epoch 149] ogbg-molsider: 0.657937 test loss: 0.663964
[Epoch 150; Iter     2/   32] train: loss: 0.1956749
[Epoch 150; Iter    32/   32] train: loss: 0.3529742
[Epoch 150] ogbg-molsider: 0.661214 val loss: 0.667761
[Epoch 150] ogbg-molsider: 0.666515 test loss: 0.659158
[Epoch 151; Iter    30/   32] train: loss: 0.2883115
[Epoch 151] ogbg-molsider: 0.649942 val loss: 0.662120
[Epoch 151] ogbg-molsider: 0.663217 test loss: 0.661827
[Epoch 152; Iter    28/   32] train: loss: 0.2191632
[Epoch 152] ogbg-molsider: 0.650302 val loss: 0.676469
[Epoch 152] ogbg-molsider: 0.657836 test loss: 0.670411
[Epoch 153; Iter    26/   32] train: loss: 0.1949754
[Epoch 153] ogbg-molsider: 0.653619 val loss: 0.651828
[Epoch 153] ogbg-molsider: 0.658855 test loss: 0.657912
[Epoch 154; Iter    24/   32] train: loss: 0.2299098
[Epoch 154] ogbg-molsider: 0.661656 val loss: 0.649555
[Epoch 154] ogbg-molsider: 0.659325 test loss: 0.656085
[Epoch 155; Iter    22/   32] train: loss: 0.1808730
[Epoch 155] ogbg-molsider: 0.656668 val loss: 0.664472
[Epoch 155] ogbg-molsider: 0.658840 test loss: 0.661467
[Epoch 156; Iter    20/   32] train: loss: 0.2188908
[Epoch 156] ogbg-molsider: 0.652597 val loss: 0.660269
[Epoch 156] ogbg-molsider: 0.658572 test loss: 0.657720
[Epoch 157; Iter    18/   32] train: loss: 0.2017979
[Epoch 157] ogbg-molsider: 0.654689 val loss: 0.673534
[Epoch 157] ogbg-molsider: 0.656151 test loss: 0.668702
[Epoch 158; Iter    16/   32] train: loss: 0.2347755
[Epoch 158] ogbg-molsider: 0.659210 val loss: 0.673119
[Epoch 158] ogbg-molsider: 0.657862 test loss: 0.673246
[Epoch 159; Iter    14/   32] train: loss: 0.2241382
[Epoch 159] ogbg-molsider: 0.656954 val loss: 0.661026
[Epoch 159] ogbg-molsider: 0.655414 test loss: 0.665109
[Epoch 160; Iter    12/   32] train: loss: 0.1945763
[Epoch 160] ogbg-molsider: 0.657223 val loss: 0.663141
[Epoch 160] ogbg-molsider: 0.654915 test loss: 0.674354
[Epoch 161; Iter    10/   32] train: loss: 0.2496528
[Epoch 161] ogbg-molsider: 0.658477 val loss: 0.683821
[Epoch 161] ogbg-molsider: 0.655663 test loss: 0.686363
[Epoch 162; Iter     8/   32] train: loss: 0.2203152
[Epoch 162] ogbg-molsider: 0.659868 val loss: 0.671835
[Epoch 162] ogbg-molsider: 0.655087 test loss: 0.673977
[Epoch 163; Iter     6/   32] train: loss: 0.2171949
[Epoch 163] ogbg-molsider: 0.659250 val loss: 0.669284
[Epoch 163] ogbg-molsider: 0.655754 test loss: 0.679336
[Epoch 164; Iter     4/   32] train: loss: 0.2195868
[Epoch 164] ogbg-molsider: 0.657396 val loss: 0.671676
[Epoch 164] ogbg-molsider: 0.653622 test loss: 0.676016
[Epoch 165; Iter     2/   32] train: loss: 0.1782189
[Epoch 165; Iter    32/   32] train: loss: 0.3415195
[Epoch 165] ogbg-molsider: 0.656390 val loss: 0.677448
[Epoch 165] ogbg-molsider: 0.656098 test loss: 0.672861
[Epoch 166; Iter    30/   32] train: loss: 0.1952078
[Epoch 166] ogbg-molsider: 0.656040 val loss: 0.677569
[Epoch 166] ogbg-molsider: 0.652598 test loss: 0.680130
[Epoch 167; Iter    28/   32] train: loss: 0.2247364
[Epoch 167] ogbg-molsider: 0.654420 val loss: 0.691224
[Epoch 167] ogbg-molsider: 0.657359 test loss: 0.686720
[Epoch 168; Iter    26/   32] train: loss: 0.2296723
[Epoch 168] ogbg-molsider: 0.657804 val loss: 0.670753
[Epoch 168] ogbg-molsider: 0.656059 test loss: 0.669804
[Epoch 169; Iter    24/   32] train: loss: 0.2212639
[Epoch 169] ogbg-molsider: 0.659963 val loss: 0.663897
[Epoch 169] ogbg-molsider: 0.655929 test loss: 0.675038
[Epoch 170; Iter    22/   32] train: loss: 0.2526894
[Epoch 170] ogbg-molsider: 0.658166 val loss: 0.684405
[Epoch 170] ogbg-molsider: 0.656039 test loss: 0.682347
[Epoch 171; Iter    20/   32] train: loss: 0.2518448
[Epoch 171] ogbg-molsider: 0.652235 val loss: 0.703290
[Epoch 171] ogbg-molsider: 0.651165 test loss: 0.681943
[Epoch 172; Iter    18/   32] train: loss: 0.2243091
[Epoch 172] ogbg-molsider: 0.655063 val loss: 0.700191
[Epoch 172] ogbg-molsider: 0.654738 test loss: 0.678516
[Epoch 173; Iter    16/   32] train: loss: 0.2840368
[Epoch 173] ogbg-molsider: 0.655176 val loss: 0.684209
[Epoch 173] ogbg-molsider: 0.650271 test loss: 0.686059
[Epoch 174; Iter    14/   32] train: loss: 0.1870416
[Epoch 174] ogbg-molsider: 0.654403 val loss: 0.688259
[Epoch 174] ogbg-molsider: 0.646525 test loss: 0.699722
[Epoch 175; Iter    12/   32] train: loss: 0.1845929
[Epoch 175] ogbg-molsider: 0.658389 val loss: 0.695148
[Epoch 175] ogbg-molsider: 0.650225 test loss: 0.701997
[Epoch 176; Iter    10/   32] train: loss: 0.1739049
[Epoch 176] ogbg-molsider: 0.658501 val loss: 0.690864
[Epoch 176] ogbg-molsider: 0.655266 test loss: 0.685182
Early stopping criterion based on -ogbg-molsider- that should be max reached after 176 epochs. Best model checkpoint was in epoch 116.
Statistics on  val_best_checkpoint
mean_pred: 0.6452515721321106
std_pred: 3.153632402420044
mean_targets: 0.5732086896896362
std_targets: 0.49465426802635193
prcauc: 0.671988871515999
rocauc: 0.6666308790033313
ogbg-molsider: 0.6666308790033313
OGBNanLabelBCEWithLogitsLoss: 0.5957836764199393
Statistics on  test
mean_pred: 0.5992348194122314
std_pred: 3.1294898986816406
mean_targets: 0.5714039206504822
std_targets: 0.4949178397655487
prcauc: 0.6632116715166524
rocauc: 0.644878030859296
ogbg-molsider: 0.644878030859296
OGBNanLabelBCEWithLogitsLoss: 0.6052475316183907
Statistics on  train
mean_pred: 0.6494316458702087
std_pred: 3.37845778465271
mean_targets: 0.5655384659767151
std_targets: 0.4956952929496765
prcauc: 0.8875608258897769
rocauc: 0.9320496912330133
ogbg-molsider: 0.9320496912330133
OGBNanLabelBCEWithLogitsLoss: 0.26994444616138935
All runs completed.
