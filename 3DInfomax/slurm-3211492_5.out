>>> Starting run for dataset: sider
Running RANDOM configs_split_experiments/3DInfomax/sider/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/3DInfomax/sider/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/3DInfomax/sider/random/train_prop=0.8.yml on cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.6.yml --seed 4 --device cuda:0
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.8.yml --seed 4 --device cuda:2
Starting process for seed 4: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.7.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.6.yml --seed 5 --device cuda:0
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.8.yml --seed 5 --device cuda:2
Starting process for seed 5: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.7.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.6.yml --seed 6 --device cuda:0
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.8.yml --seed 6 --device cuda:2
Starting process for seed 6: python train.py --config configs_split_experiments/3DInfomax/sider/random/train_prop=0.7.yml --seed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.8/PNA_ogbg-molsider_3DInfomax_sider_random=0.8_4_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.8
logdir: runs/split/3DInfomax/sider/random/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6930208
[Epoch 1] ogbg-molsider: 0.486001 val loss: 0.693164
[Epoch 1] ogbg-molsider: 0.507358 test loss: 0.693241
[Epoch 2; Iter    24/   36] train: loss: 0.6930966
[Epoch 2] ogbg-molsider: 0.492652 val loss: 0.693112
[Epoch 2] ogbg-molsider: 0.508386 test loss: 0.693327
[Epoch 3; Iter    18/   36] train: loss: 0.6932514
[Epoch 3] ogbg-molsider: 0.501026 val loss: 0.693066
[Epoch 3] ogbg-molsider: 0.508804 test loss: 0.693313
[Epoch 4; Iter    12/   36] train: loss: 0.6929450
[Epoch 4] ogbg-molsider: 0.501615 val loss: 0.692988
[Epoch 4] ogbg-molsider: 0.507017 test loss: 0.693247
[Epoch 5; Iter     6/   36] train: loss: 0.6934069
[Epoch 5; Iter    36/   36] train: loss: 0.6934799
[Epoch 5] ogbg-molsider: 0.501168 val loss: 0.692958
[Epoch 5] ogbg-molsider: 0.505393 test loss: 0.693311
[Epoch 6; Iter    30/   36] train: loss: 0.6933945
[Epoch 6] ogbg-molsider: 0.504340 val loss: 0.692840
[Epoch 6] ogbg-molsider: 0.507755 test loss: 0.693142
[Epoch 7; Iter    24/   36] train: loss: 0.6931282
[Epoch 7] ogbg-molsider: 0.503129 val loss: 0.692821
[Epoch 7] ogbg-molsider: 0.508840 test loss: 0.693082
[Epoch 8; Iter    18/   36] train: loss: 0.6926250
[Epoch 8] ogbg-molsider: 0.499266 val loss: 0.692689
[Epoch 8] ogbg-molsider: 0.507897 test loss: 0.692965
[Epoch 9; Iter    12/   36] train: loss: 0.6925031
[Epoch 9] ogbg-molsider: 0.503395 val loss: 0.692630
[Epoch 9] ogbg-molsider: 0.507985 test loss: 0.692871
[Epoch 10; Iter     6/   36] train: loss: 0.6928710
[Epoch 10; Iter    36/   36] train: loss: 0.6930958
[Epoch 10] ogbg-molsider: 0.502700 val loss: 0.692489
[Epoch 10] ogbg-molsider: 0.508270 test loss: 0.692791
[Epoch 11; Iter    30/   36] train: loss: 0.6925014
[Epoch 11] ogbg-molsider: 0.500212 val loss: 0.692319
[Epoch 11] ogbg-molsider: 0.510528 test loss: 0.692544
[Epoch 12; Iter    24/   36] train: loss: 0.6920196
[Epoch 12] ogbg-molsider: 0.502886 val loss: 0.692039
[Epoch 12] ogbg-molsider: 0.512950 test loss: 0.692373
[Epoch 13; Iter    18/   36] train: loss: 0.6911362
[Epoch 13] ogbg-molsider: 0.502209 val loss: 0.692054
[Epoch 13] ogbg-molsider: 0.511035 test loss: 0.692286
[Epoch 14; Iter    12/   36] train: loss: 0.6918622
[Epoch 14] ogbg-molsider: 0.502359 val loss: 0.691853
[Epoch 14] ogbg-molsider: 0.508521 test loss: 0.692198
[Epoch 15; Iter     6/   36] train: loss: 0.6918828
[Epoch 15; Iter    36/   36] train: loss: 0.6915424
[Epoch 15] ogbg-molsider: 0.504345 val loss: 0.691692
[Epoch 15] ogbg-molsider: 0.510354 test loss: 0.691956
[Epoch 16; Iter    30/   36] train: loss: 0.6916025
[Epoch 16] ogbg-molsider: 0.503261 val loss: 0.691517
[Epoch 16] ogbg-molsider: 0.511488 test loss: 0.691790
[Epoch 17; Iter    24/   36] train: loss: 0.6913917
[Epoch 17] ogbg-molsider: 0.503795 val loss: 0.691319
[Epoch 17] ogbg-molsider: 0.510062 test loss: 0.691657
[Epoch 18; Iter    18/   36] train: loss: 0.6915513
[Epoch 18] ogbg-molsider: 0.503140 val loss: 0.691123
[Epoch 18] ogbg-molsider: 0.511427 test loss: 0.691418
[Epoch 19; Iter    12/   36] train: loss: 0.6913264
[Epoch 19] ogbg-molsider: 0.504433 val loss: 0.690856
[Epoch 19] ogbg-molsider: 0.511150 test loss: 0.691173
[Epoch 20; Iter     6/   36] train: loss: 0.6904879
[Epoch 20; Iter    36/   36] train: loss: 0.6857210
[Epoch 20] ogbg-molsider: 0.564337 val loss: 0.681843
[Epoch 20] ogbg-molsider: 0.546328 test loss: 0.683951
[Epoch 21; Iter    30/   36] train: loss: 0.6724072
[Epoch 21] ogbg-molsider: 0.571634 val loss: 0.660913
[Epoch 21] ogbg-molsider: 0.587247 test loss: 0.663273
[Epoch 22; Iter    24/   36] train: loss: 0.6573940
[Epoch 22] ogbg-molsider: 0.568306 val loss: 0.636017
[Epoch 22] ogbg-molsider: 0.576641 test loss: 0.643498
[Epoch 23; Iter    18/   36] train: loss: 0.6340834
[Epoch 23] ogbg-molsider: 0.578225 val loss: 0.622533
[Epoch 23] ogbg-molsider: 0.570266 test loss: 0.632449
[Epoch 24; Iter    12/   36] train: loss: 0.5994716
[Epoch 24] ogbg-molsider: 0.559870 val loss: 0.576184
[Epoch 24] ogbg-molsider: 0.581310 test loss: 0.584620
[Epoch 25; Iter     6/   36] train: loss: 0.5844265
[Epoch 25; Iter    36/   36] train: loss: 0.5827281
[Epoch 25] ogbg-molsider: 0.546434 val loss: 0.540900
[Epoch 25] ogbg-molsider: 0.603739 test loss: 0.556584
[Epoch 26; Iter    30/   36] train: loss: 0.5185744
[Epoch 26] ogbg-molsider: 0.616441 val loss: 0.519605
[Epoch 26] ogbg-molsider: 0.611393 test loss: 0.540150
[Epoch 27; Iter    24/   36] train: loss: 0.5337811
[Epoch 27] ogbg-molsider: 0.586242 val loss: 0.518524
[Epoch 27] ogbg-molsider: 0.587819 test loss: 0.553762
[Epoch 28; Iter    18/   36] train: loss: 0.5297802
[Epoch 28] ogbg-molsider: 0.602540 val loss: 0.542387
[Epoch 28] ogbg-molsider: 0.603123 test loss: 0.522086
[Epoch 29; Iter    12/   36] train: loss: 0.5167823
[Epoch 29] ogbg-molsider: 0.612704 val loss: 0.502018
[Epoch 29] ogbg-molsider: 0.602598 test loss: 0.522853
[Epoch 30; Iter     6/   36] train: loss: 0.5130545
[Epoch 30; Iter    36/   36] train: loss: 0.4961147
[Epoch 30] ogbg-molsider: 0.603769 val loss: 0.498137
[Epoch 30] ogbg-molsider: 0.593459 test loss: 0.523160
[Epoch 31; Iter    30/   36] train: loss: 0.4720329
[Epoch 31] ogbg-molsider: 0.607057 val loss: 0.486034
[Epoch 31] ogbg-molsider: 0.608990 test loss: 0.519306
[Epoch 32; Iter    24/   36] train: loss: 0.4520999
[Epoch 32] ogbg-molsider: 0.614476 val loss: 0.482143
[Epoch 32] ogbg-molsider: 0.619711 test loss: 0.516000
[Epoch 33; Iter    18/   36] train: loss: 0.4917741
[Epoch 33] ogbg-molsider: 0.624171 val loss: 0.483947
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.8/PNA_ogbg-molsider_3DInfomax_sider_random=0.8_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.8
logdir: runs/split/3DInfomax/sider/random/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6933280
[Epoch 1] ogbg-molsider: 0.496984 val loss: 0.693178
[Epoch 1] ogbg-molsider: 0.492564 test loss: 0.693190
[Epoch 2; Iter    24/   36] train: loss: 0.6935623
[Epoch 2] ogbg-molsider: 0.496112 val loss: 0.693130
[Epoch 2] ogbg-molsider: 0.489141 test loss: 0.693280
[Epoch 3; Iter    18/   36] train: loss: 0.6934867
[Epoch 3] ogbg-molsider: 0.498445 val loss: 0.693059
[Epoch 3] ogbg-molsider: 0.489724 test loss: 0.693301
[Epoch 4; Iter    12/   36] train: loss: 0.6934903
[Epoch 4] ogbg-molsider: 0.496762 val loss: 0.693038
[Epoch 4] ogbg-molsider: 0.487644 test loss: 0.693250
[Epoch 5; Iter     6/   36] train: loss: 0.6931102
[Epoch 5; Iter    36/   36] train: loss: 0.6932327
[Epoch 5] ogbg-molsider: 0.496081 val loss: 0.692919
[Epoch 5] ogbg-molsider: 0.488422 test loss: 0.693243
[Epoch 6; Iter    30/   36] train: loss: 0.6926671
[Epoch 6] ogbg-molsider: 0.495871 val loss: 0.692900
[Epoch 6] ogbg-molsider: 0.489043 test loss: 0.693172
[Epoch 7; Iter    24/   36] train: loss: 0.6935528
[Epoch 7] ogbg-molsider: 0.496280 val loss: 0.692861
[Epoch 7] ogbg-molsider: 0.487951 test loss: 0.693137
[Epoch 8; Iter    18/   36] train: loss: 0.6930735
[Epoch 8] ogbg-molsider: 0.495316 val loss: 0.692736
[Epoch 8] ogbg-molsider: 0.487485 test loss: 0.693014
[Epoch 9; Iter    12/   36] train: loss: 0.6929527
[Epoch 9] ogbg-molsider: 0.495352 val loss: 0.692680
[Epoch 9] ogbg-molsider: 0.489942 test loss: 0.692957
[Epoch 10; Iter     6/   36] train: loss: 0.6924822
[Epoch 10; Iter    36/   36] train: loss: 0.6929013
[Epoch 10] ogbg-molsider: 0.493995 val loss: 0.692611
[Epoch 10] ogbg-molsider: 0.488337 test loss: 0.692917
[Epoch 11; Iter    30/   36] train: loss: 0.6927693
[Epoch 11] ogbg-molsider: 0.498792 val loss: 0.692358
[Epoch 11] ogbg-molsider: 0.490297 test loss: 0.692619
[Epoch 12; Iter    24/   36] train: loss: 0.6925795
[Epoch 12] ogbg-molsider: 0.497335 val loss: 0.692301
[Epoch 12] ogbg-molsider: 0.489353 test loss: 0.692556
[Epoch 13; Iter    18/   36] train: loss: 0.6923808
[Epoch 13] ogbg-molsider: 0.499995 val loss: 0.692158
[Epoch 13] ogbg-molsider: 0.489770 test loss: 0.692443
[Epoch 14; Iter    12/   36] train: loss: 0.6920055
[Epoch 14] ogbg-molsider: 0.497370 val loss: 0.691920
[Epoch 14] ogbg-molsider: 0.488560 test loss: 0.692188
[Epoch 15; Iter     6/   36] train: loss: 0.6921909
[Epoch 15; Iter    36/   36] train: loss: 0.6919677
[Epoch 15] ogbg-molsider: 0.497054 val loss: 0.691744
[Epoch 15] ogbg-molsider: 0.488913 test loss: 0.692064
[Epoch 16; Iter    30/   36] train: loss: 0.6915518
[Epoch 16] ogbg-molsider: 0.499122 val loss: 0.691667
[Epoch 16] ogbg-molsider: 0.491783 test loss: 0.691901
[Epoch 17; Iter    24/   36] train: loss: 0.6914076
[Epoch 17] ogbg-molsider: 0.498561 val loss: 0.691504
[Epoch 17] ogbg-molsider: 0.490140 test loss: 0.691803
[Epoch 18; Iter    18/   36] train: loss: 0.6916190
[Epoch 18] ogbg-molsider: 0.501613 val loss: 0.691209
[Epoch 18] ogbg-molsider: 0.488377 test loss: 0.691503
[Epoch 19; Iter    12/   36] train: loss: 0.6912522
[Epoch 19] ogbg-molsider: 0.500075 val loss: 0.690985
[Epoch 19] ogbg-molsider: 0.490824 test loss: 0.691311
[Epoch 20; Iter     6/   36] train: loss: 0.6915649
[Epoch 20; Iter    36/   36] train: loss: 0.6858793
[Epoch 20] ogbg-molsider: 0.560531 val loss: 0.684006
[Epoch 20] ogbg-molsider: 0.594845 test loss: 0.684899
[Epoch 21; Iter    30/   36] train: loss: 0.6746951
[Epoch 21] ogbg-molsider: 0.554963 val loss: 0.664427
[Epoch 21] ogbg-molsider: 0.583614 test loss: 0.665619
[Epoch 22; Iter    24/   36] train: loss: 0.6552681
[Epoch 22] ogbg-molsider: 0.571584 val loss: 0.653549
[Epoch 22] ogbg-molsider: 0.590730 test loss: 0.653501
[Epoch 23; Iter    18/   36] train: loss: 0.6425269
[Epoch 23] ogbg-molsider: 0.580545 val loss: 0.630385
[Epoch 23] ogbg-molsider: 0.602126 test loss: 0.636059
[Epoch 24; Iter    12/   36] train: loss: 0.6060277
[Epoch 24] ogbg-molsider: 0.567557 val loss: 0.594484
[Epoch 24] ogbg-molsider: 0.611428 test loss: 0.596482
[Epoch 25; Iter     6/   36] train: loss: 0.5811074
[Epoch 25; Iter    36/   36] train: loss: 0.5732467
[Epoch 25] ogbg-molsider: 0.587661 val loss: 0.558787
[Epoch 25] ogbg-molsider: 0.600900 test loss: 0.567824
[Epoch 26; Iter    30/   36] train: loss: 0.5380169
[Epoch 26] ogbg-molsider: 0.530278 val loss: 0.535383
[Epoch 26] ogbg-molsider: 0.586042 test loss: 0.542485
[Epoch 27; Iter    24/   36] train: loss: 0.5234057
[Epoch 27] ogbg-molsider: 0.615458 val loss: 0.526632
[Epoch 27] ogbg-molsider: 0.611297 test loss: 0.545683
[Epoch 28; Iter    18/   36] train: loss: 0.5404624
[Epoch 28] ogbg-molsider: 0.590574 val loss: 0.511793
[Epoch 28] ogbg-molsider: 0.594129 test loss: 0.522022
[Epoch 29; Iter    12/   36] train: loss: 0.4954587
[Epoch 29] ogbg-molsider: 0.614082 val loss: 0.492903
[Epoch 29] ogbg-molsider: 0.618296 test loss: 0.522400
[Epoch 30; Iter     6/   36] train: loss: 0.5399861
[Epoch 30; Iter    36/   36] train: loss: 0.5058278
[Epoch 30] ogbg-molsider: 0.584831 val loss: 0.495212
[Epoch 30] ogbg-molsider: 0.602462 test loss: 0.519430
[Epoch 31; Iter    30/   36] train: loss: 0.4921495
[Epoch 31] ogbg-molsider: 0.604710 val loss: 0.487106
[Epoch 31] ogbg-molsider: 0.613187 test loss: 0.515813
[Epoch 32; Iter    24/   36] train: loss: 0.4694809
[Epoch 32] ogbg-molsider: 0.601774 val loss: 0.491511
[Epoch 32] ogbg-molsider: 0.607403 test loss: 0.512954
[Epoch 33; Iter    18/   36] train: loss: 0.5010054
[Epoch 33] ogbg-molsider: 0.615775 val loss: 0.488232
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.7/PNA_ogbg-molsider_3DInfomax_sider_random=0.7_4_26-05_09-18-11
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.7
logdir: runs/split/3DInfomax/sider/random/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6931839
[Epoch 1] ogbg-molsider: 0.500312 val loss: 0.693187
[Epoch 1] ogbg-molsider: 0.500830 test loss: 0.693241
[Epoch 2; Iter    28/   32] train: loss: 0.6935229
[Epoch 2] ogbg-molsider: 0.501234 val loss: 0.693024
[Epoch 2] ogbg-molsider: 0.505274 test loss: 0.693193
[Epoch 3; Iter    26/   32] train: loss: 0.6934010
[Epoch 3] ogbg-molsider: 0.503193 val loss: 0.692845
[Epoch 3] ogbg-molsider: 0.507959 test loss: 0.693139
[Epoch 4; Iter    24/   32] train: loss: 0.6934340
[Epoch 4] ogbg-molsider: 0.500842 val loss: 0.692964
[Epoch 4] ogbg-molsider: 0.509422 test loss: 0.693224
[Epoch 5; Iter    22/   32] train: loss: 0.6933907
[Epoch 5] ogbg-molsider: 0.501940 val loss: 0.692842
[Epoch 5] ogbg-molsider: 0.508213 test loss: 0.693058
[Epoch 6; Iter    20/   32] train: loss: 0.6929523
[Epoch 6] ogbg-molsider: 0.501756 val loss: 0.692790
[Epoch 6] ogbg-molsider: 0.506076 test loss: 0.693086
[Epoch 7; Iter    18/   32] train: loss: 0.6930544
[Epoch 7] ogbg-molsider: 0.503543 val loss: 0.692715
[Epoch 7] ogbg-molsider: 0.508444 test loss: 0.693024
[Epoch 8; Iter    16/   32] train: loss: 0.6930507
[Epoch 8] ogbg-molsider: 0.504250 val loss: 0.692606
[Epoch 8] ogbg-molsider: 0.508777 test loss: 0.692906
[Epoch 9; Iter    14/   32] train: loss: 0.6926420
[Epoch 9] ogbg-molsider: 0.503655 val loss: 0.692604
[Epoch 9] ogbg-molsider: 0.506363 test loss: 0.692919
[Epoch 10; Iter    12/   32] train: loss: 0.6925496
[Epoch 10] ogbg-molsider: 0.501617 val loss: 0.692552
[Epoch 10] ogbg-molsider: 0.508115 test loss: 0.692803
[Epoch 11; Iter    10/   32] train: loss: 0.6928454
[Epoch 11] ogbg-molsider: 0.504163 val loss: 0.692389
[Epoch 11] ogbg-molsider: 0.508669 test loss: 0.692653
[Epoch 12; Iter     8/   32] train: loss: 0.6921802
[Epoch 12] ogbg-molsider: 0.503574 val loss: 0.692269
[Epoch 12] ogbg-molsider: 0.507347 test loss: 0.692601
[Epoch 13; Iter     6/   32] train: loss: 0.6929982
[Epoch 13] ogbg-molsider: 0.503012 val loss: 0.692180
[Epoch 13] ogbg-molsider: 0.509855 test loss: 0.692420
[Epoch 14; Iter     4/   32] train: loss: 0.6918465
[Epoch 14] ogbg-molsider: 0.504015 val loss: 0.692084
[Epoch 14] ogbg-molsider: 0.507875 test loss: 0.692375
[Epoch 15; Iter     2/   32] train: loss: 0.6924220
[Epoch 15; Iter    32/   32] train: loss: 0.6922031
[Epoch 15] ogbg-molsider: 0.504114 val loss: 0.691905
[Epoch 15] ogbg-molsider: 0.509725 test loss: 0.692164
[Epoch 16; Iter    30/   32] train: loss: 0.6916189
[Epoch 16] ogbg-molsider: 0.503490 val loss: 0.691707
[Epoch 16] ogbg-molsider: 0.510476 test loss: 0.691944
[Epoch 17; Iter    28/   32] train: loss: 0.6914611
[Epoch 17] ogbg-molsider: 0.504951 val loss: 0.691663
[Epoch 17] ogbg-molsider: 0.509326 test loss: 0.691933
[Epoch 18; Iter    26/   32] train: loss: 0.6918640
[Epoch 18] ogbg-molsider: 0.502836 val loss: 0.691400
[Epoch 18] ogbg-molsider: 0.510234 test loss: 0.691664
[Epoch 19; Iter    24/   32] train: loss: 0.6919365
[Epoch 19] ogbg-molsider: 0.504930 val loss: 0.691260
[Epoch 19] ogbg-molsider: 0.511223 test loss: 0.691510
[Epoch 20; Iter    22/   32] train: loss: 0.6915173
[Epoch 20] ogbg-molsider: 0.503923 val loss: 0.691058
[Epoch 20] ogbg-molsider: 0.507597 test loss: 0.691314
[Epoch 21; Iter    20/   32] train: loss: 0.6911824
[Epoch 21] ogbg-molsider: 0.506465 val loss: 0.690853
[Epoch 21] ogbg-molsider: 0.511687 test loss: 0.691079
[Epoch 22; Iter    18/   32] train: loss: 0.6901156
[Epoch 22] ogbg-molsider: 0.537159 val loss: 0.688127
[Epoch 22] ogbg-molsider: 0.538490 test loss: 0.688454
[Epoch 23; Iter    16/   32] train: loss: 0.6850019
[Epoch 23] ogbg-molsider: 0.564651 val loss: 0.677163
[Epoch 23] ogbg-molsider: 0.590219 test loss: 0.677605
[Epoch 24; Iter    14/   32] train: loss: 0.6722580
[Epoch 24] ogbg-molsider: 0.574498 val loss: 0.662839
[Epoch 24] ogbg-molsider: 0.599186 test loss: 0.662191
[Epoch 25; Iter    12/   32] train: loss: 0.6558610
[Epoch 25] ogbg-molsider: 0.582370 val loss: 0.632421
[Epoch 25] ogbg-molsider: 0.597224 test loss: 0.635136
[Epoch 26; Iter    10/   32] train: loss: 0.6397892
[Epoch 26] ogbg-molsider: 0.578954 val loss: 0.609375
[Epoch 26] ogbg-molsider: 0.602464 test loss: 0.614095
[Epoch 27; Iter     8/   32] train: loss: 0.6076050
[Epoch 27] ogbg-molsider: 0.587610 val loss: 0.609798
[Epoch 27] ogbg-molsider: 0.607914 test loss: 0.608831
[Epoch 28; Iter     6/   32] train: loss: 0.5777698
[Epoch 28] ogbg-molsider: 0.570083 val loss: 0.557195
[Epoch 28] ogbg-molsider: 0.572611 test loss: 0.556839
[Epoch 29; Iter     4/   32] train: loss: 0.5641239
[Epoch 29] ogbg-molsider: 0.576555 val loss: 0.542781
[Epoch 29] ogbg-molsider: 0.604733 test loss: 0.539192
[Epoch 30; Iter     2/   32] train: loss: 0.5337220
[Epoch 30; Iter    32/   32] train: loss: 0.5509754
[Epoch 30] ogbg-molsider: 0.576331 val loss: 0.523981
[Epoch 30] ogbg-molsider: 0.588479 test loss: 0.524068
[Epoch 31; Iter    30/   32] train: loss: 0.5050824
[Epoch 31] ogbg-molsider: 0.580598 val loss: 0.518966
[Epoch 31] ogbg-molsider: 0.602772 test loss: 0.520028
[Epoch 32; Iter    28/   32] train: loss: 0.5034911
[Epoch 32] ogbg-molsider: 0.581510 val loss: 0.514208
[Epoch 32] ogbg-molsider: 0.601523 test loss: 0.510833
[Epoch 33; Iter    26/   32] train: loss: 0.5258592
[Epoch 33] ogbg-molsider: 0.593359 val loss: 0.507055
[Epoch 33] ogbg-molsider: 0.599850 test loss: 0.507790
[Epoch 34; Iter    24/   32] train: loss: 0.5244280
[Epoch 34] ogbg-molsider: 0.603634 val loss: 0.502966
[Epoch 34] ogbg-molsider: 0.604516 test loss: 0.505996
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.6/PNA_ogbg-molsider_3DInfomax_sider_random=0.6_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.6
logdir: runs/split/3DInfomax/sider/random/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.523997 val loss: 0.693150
[Epoch 1] ogbg-molsider: 0.499088 test loss: 0.693146
[Epoch 2; Iter     3/   27] train: loss: 0.6936138
[Epoch 2] ogbg-molsider: 0.514265 val loss: 0.693260
[Epoch 2] ogbg-molsider: 0.497539 test loss: 0.693194
[Epoch 3; Iter     6/   27] train: loss: 0.6934341
[Epoch 3] ogbg-molsider: 0.505677 val loss: 0.693347
[Epoch 3] ogbg-molsider: 0.496372 test loss: 0.693282
[Epoch 4; Iter     9/   27] train: loss: 0.6931412
[Epoch 4] ogbg-molsider: 0.504955 val loss: 0.693226
[Epoch 4] ogbg-molsider: 0.496333 test loss: 0.693132
[Epoch 5; Iter    12/   27] train: loss: 0.6935258
[Epoch 5] ogbg-molsider: 0.507555 val loss: 0.693219
[Epoch 5] ogbg-molsider: 0.497327 test loss: 0.693121
[Epoch 6; Iter    15/   27] train: loss: 0.6932063
[Epoch 6] ogbg-molsider: 0.509347 val loss: 0.693202
[Epoch 6] ogbg-molsider: 0.497083 test loss: 0.693125
[Epoch 7; Iter    18/   27] train: loss: 0.6927048
[Epoch 7] ogbg-molsider: 0.505435 val loss: 0.693134
[Epoch 7] ogbg-molsider: 0.496671 test loss: 0.693043
[Epoch 8; Iter    21/   27] train: loss: 0.6925780
[Epoch 8] ogbg-molsider: 0.506209 val loss: 0.693113
[Epoch 8] ogbg-molsider: 0.498124 test loss: 0.693003
[Epoch 9; Iter    24/   27] train: loss: 0.6927812
[Epoch 9] ogbg-molsider: 0.509210 val loss: 0.693051
[Epoch 9] ogbg-molsider: 0.496654 test loss: 0.692963
[Epoch 10; Iter    27/   27] train: loss: 0.6925890
[Epoch 10] ogbg-molsider: 0.506901 val loss: 0.693030
[Epoch 10] ogbg-molsider: 0.497746 test loss: 0.692923
[Epoch 11] ogbg-molsider: 0.506709 val loss: 0.692944
[Epoch 11] ogbg-molsider: 0.497617 test loss: 0.692834
[Epoch 12; Iter     3/   27] train: loss: 0.6930963
[Epoch 12] ogbg-molsider: 0.505566 val loss: 0.692843
[Epoch 12] ogbg-molsider: 0.496579 test loss: 0.692758
[Epoch 13; Iter     6/   27] train: loss: 0.6935505
[Epoch 13] ogbg-molsider: 0.506125 val loss: 0.692840
[Epoch 13] ogbg-molsider: 0.496707 test loss: 0.692711
[Epoch 14; Iter     9/   27] train: loss: 0.6929936
[Epoch 14] ogbg-molsider: 0.507469 val loss: 0.692759
[Epoch 14] ogbg-molsider: 0.496526 test loss: 0.692666
[Epoch 15; Iter    12/   27] train: loss: 0.6925593
[Epoch 15] ogbg-molsider: 0.507847 val loss: 0.692620
[Epoch 15] ogbg-molsider: 0.497554 test loss: 0.692482
[Epoch 16; Iter    15/   27] train: loss: 0.6919782
[Epoch 16] ogbg-molsider: 0.510418 val loss: 0.692573
[Epoch 16] ogbg-molsider: 0.496716 test loss: 0.692454
[Epoch 17; Iter    18/   27] train: loss: 0.6930970
[Epoch 17] ogbg-molsider: 0.509394 val loss: 0.692470
[Epoch 17] ogbg-molsider: 0.497688 test loss: 0.692349
[Epoch 18; Iter    21/   27] train: loss: 0.6923100
[Epoch 18] ogbg-molsider: 0.509133 val loss: 0.692357
[Epoch 18] ogbg-molsider: 0.497701 test loss: 0.692210
[Epoch 19; Iter    24/   27] train: loss: 0.6921620
[Epoch 19] ogbg-molsider: 0.509528 val loss: 0.692198
[Epoch 19] ogbg-molsider: 0.497607 test loss: 0.692053
[Epoch 20; Iter    27/   27] train: loss: 0.6917545
[Epoch 20] ogbg-molsider: 0.508270 val loss: 0.692124
[Epoch 20] ogbg-molsider: 0.498205 test loss: 0.691983
[Epoch 21] ogbg-molsider: 0.511457 val loss: 0.692045
[Epoch 21] ogbg-molsider: 0.498286 test loss: 0.691913
[Epoch 22; Iter     3/   27] train: loss: 0.6916490
[Epoch 22] ogbg-molsider: 0.509664 val loss: 0.691871
[Epoch 22] ogbg-molsider: 0.497234 test loss: 0.691750
[Epoch 23; Iter     6/   27] train: loss: 0.6921347
[Epoch 23] ogbg-molsider: 0.510653 val loss: 0.691788
[Epoch 23] ogbg-molsider: 0.498460 test loss: 0.691594
[Epoch 24; Iter     9/   27] train: loss: 0.6917414
[Epoch 24] ogbg-molsider: 0.509030 val loss: 0.691630
[Epoch 24] ogbg-molsider: 0.497561 test loss: 0.691473
[Epoch 25; Iter    12/   27] train: loss: 0.6917757
[Epoch 25] ogbg-molsider: 0.508158 val loss: 0.691480
[Epoch 25] ogbg-molsider: 0.497536 test loss: 0.691282
[Epoch 26; Iter    15/   27] train: loss: 0.6906163
[Epoch 26] ogbg-molsider: 0.532585 val loss: 0.688644
[Epoch 26] ogbg-molsider: 0.550410 test loss: 0.687979
[Epoch 27; Iter    18/   27] train: loss: 0.6843360
[Epoch 27] ogbg-molsider: 0.568515 val loss: 0.678825
[Epoch 27] ogbg-molsider: 0.585635 test loss: 0.678095
[Epoch 28; Iter    21/   27] train: loss: 0.6735450
[Epoch 28] ogbg-molsider: 0.576937 val loss: 0.668396
[Epoch 28] ogbg-molsider: 0.584158 test loss: 0.665568
[Epoch 29; Iter    24/   27] train: loss: 0.6499039
[Epoch 29] ogbg-molsider: 0.589247 val loss: 0.655380
[Epoch 29] ogbg-molsider: 0.581964 test loss: 0.650265
[Epoch 30; Iter    27/   27] train: loss: 0.6319655
[Epoch 30] ogbg-molsider: 0.582391 val loss: 0.641536
[Epoch 30] ogbg-molsider: 0.576902 test loss: 0.634901
[Epoch 31] ogbg-molsider: 0.567752 val loss: 0.595278
[Epoch 31] ogbg-molsider: 0.585164 test loss: 0.585689
[Epoch 32; Iter     3/   27] train: loss: 0.6097406
[Epoch 32] ogbg-molsider: 0.577313 val loss: 0.616124
[Epoch 32] ogbg-molsider: 0.598657 test loss: 0.606876
[Epoch 33; Iter     6/   27] train: loss: 0.5832813
[Epoch 33] ogbg-molsider: 0.584034 val loss: 0.589023
[Epoch 33] ogbg-molsider: 0.598085 test loss: 0.576955
[Epoch 34; Iter     9/   27] train: loss: 0.5307336
[Epoch 34] ogbg-molsider: 0.576022 val loss: 0.548441
[Epoch 34] ogbg-molsider: 0.591925 test loss: 0.534416
[Epoch 35; Iter    12/   27] train: loss: 0.5216898
[Epoch 35] ogbg-molsider: 0.599401 val loss: 0.545493
[Epoch 35] ogbg-molsider: 0.591238 test loss: 0.529048
[Epoch 36; Iter    15/   27] train: loss: 0.5089982
[Epoch 36] ogbg-molsider: 0.587578 val loss: 0.537857
[Epoch 36] ogbg-molsider: 0.605102 test loss: 0.517916
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.7/PNA_ogbg-molsider_3DInfomax_sider_random=0.7_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.7
logdir: runs/split/3DInfomax/sider/random/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6934180
[Epoch 1] ogbg-molsider: 0.511828 val loss: 0.693116
[Epoch 1] ogbg-molsider: 0.487603 test loss: 0.693164
[Epoch 2; Iter    28/   32] train: loss: 0.6926842
[Epoch 2] ogbg-molsider: 0.511388 val loss: 0.693078
[Epoch 2] ogbg-molsider: 0.483641 test loss: 0.693260
[Epoch 3; Iter    26/   32] train: loss: 0.6934590
[Epoch 3] ogbg-molsider: 0.504085 val loss: 0.693070
[Epoch 3] ogbg-molsider: 0.482891 test loss: 0.693209
[Epoch 4; Iter    24/   32] train: loss: 0.6933965
[Epoch 4] ogbg-molsider: 0.502919 val loss: 0.693066
[Epoch 4] ogbg-molsider: 0.485678 test loss: 0.693143
[Epoch 5; Iter    22/   32] train: loss: 0.6929313
[Epoch 5] ogbg-molsider: 0.501590 val loss: 0.693084
[Epoch 5] ogbg-molsider: 0.485118 test loss: 0.693251
[Epoch 6; Iter    20/   32] train: loss: 0.6934173
[Epoch 6] ogbg-molsider: 0.507891 val loss: 0.692888
[Epoch 6] ogbg-molsider: 0.484312 test loss: 0.692989
[Epoch 7; Iter    18/   32] train: loss: 0.6926727
[Epoch 7] ogbg-molsider: 0.506464 val loss: 0.692941
[Epoch 7] ogbg-molsider: 0.485243 test loss: 0.693111
[Epoch 8; Iter    16/   32] train: loss: 0.6928297
[Epoch 8] ogbg-molsider: 0.502902 val loss: 0.692866
[Epoch 8] ogbg-molsider: 0.485054 test loss: 0.693037
[Epoch 9; Iter    14/   32] train: loss: 0.6932816
[Epoch 9] ogbg-molsider: 0.507056 val loss: 0.692696
[Epoch 9] ogbg-molsider: 0.483553 test loss: 0.692841
[Epoch 10; Iter    12/   32] train: loss: 0.6930526
[Epoch 10] ogbg-molsider: 0.505825 val loss: 0.692664
[Epoch 10] ogbg-molsider: 0.484715 test loss: 0.692792
[Epoch 11; Iter    10/   32] train: loss: 0.6931553
[Epoch 11] ogbg-molsider: 0.506625 val loss: 0.692529
[Epoch 11] ogbg-molsider: 0.486821 test loss: 0.692645
[Epoch 12; Iter     8/   32] train: loss: 0.6927400
[Epoch 12] ogbg-molsider: 0.500902 val loss: 0.692541
[Epoch 12] ogbg-molsider: 0.485131 test loss: 0.692634
[Epoch 13; Iter     6/   32] train: loss: 0.6925788
[Epoch 13] ogbg-molsider: 0.505505 val loss: 0.692428
[Epoch 13] ogbg-molsider: 0.490244 test loss: 0.692437
[Epoch 14; Iter     4/   32] train: loss: 0.6920608
[Epoch 14] ogbg-molsider: 0.502191 val loss: 0.692283
[Epoch 14] ogbg-molsider: 0.485631 test loss: 0.692338
[Epoch 15; Iter     2/   32] train: loss: 0.6928749
[Epoch 15; Iter    32/   32] train: loss: 0.6918054
[Epoch 15] ogbg-molsider: 0.507549 val loss: 0.692066
[Epoch 15] ogbg-molsider: 0.485746 test loss: 0.692219
[Epoch 16; Iter    30/   32] train: loss: 0.6921429
[Epoch 16] ogbg-molsider: 0.504596 val loss: 0.691951
[Epoch 16] ogbg-molsider: 0.487403 test loss: 0.692028
[Epoch 17; Iter    28/   32] train: loss: 0.6916860
[Epoch 17] ogbg-molsider: 0.503021 val loss: 0.691882
[Epoch 17] ogbg-molsider: 0.485823 test loss: 0.691865
[Epoch 18; Iter    26/   32] train: loss: 0.6914964
[Epoch 18] ogbg-molsider: 0.504582 val loss: 0.691720
[Epoch 18] ogbg-molsider: 0.486577 test loss: 0.691833
[Epoch 19; Iter    24/   32] train: loss: 0.6916431
[Epoch 19] ogbg-molsider: 0.503311 val loss: 0.691545
[Epoch 19] ogbg-molsider: 0.486087 test loss: 0.691585
[Epoch 20; Iter    22/   32] train: loss: 0.6913276
[Epoch 20] ogbg-molsider: 0.501841 val loss: 0.691385
[Epoch 20] ogbg-molsider: 0.484349 test loss: 0.691452
[Epoch 21; Iter    20/   32] train: loss: 0.6915382
[Epoch 21] ogbg-molsider: 0.503445 val loss: 0.691200
[Epoch 21] ogbg-molsider: 0.486694 test loss: 0.691317
[Epoch 22; Iter    18/   32] train: loss: 0.6907791
[Epoch 22] ogbg-molsider: 0.553756 val loss: 0.688617
[Epoch 22] ogbg-molsider: 0.554452 test loss: 0.688641
[Epoch 23; Iter    16/   32] train: loss: 0.6859904
[Epoch 23] ogbg-molsider: 0.574587 val loss: 0.677159
[Epoch 23] ogbg-molsider: 0.597791 test loss: 0.677692
[Epoch 24; Iter    14/   32] train: loss: 0.6767310
[Epoch 24] ogbg-molsider: 0.574178 val loss: 0.657251
[Epoch 24] ogbg-molsider: 0.597659 test loss: 0.654943
[Epoch 25; Iter    12/   32] train: loss: 0.6613979
[Epoch 25] ogbg-molsider: 0.583473 val loss: 0.629966
[Epoch 25] ogbg-molsider: 0.599830 test loss: 0.628050
[Epoch 26; Iter    10/   32] train: loss: 0.6436206
[Epoch 26] ogbg-molsider: 0.578248 val loss: 0.627353
[Epoch 26] ogbg-molsider: 0.595212 test loss: 0.624476
[Epoch 27; Iter     8/   32] train: loss: 0.6268424
[Epoch 27] ogbg-molsider: 0.594935 val loss: 0.595915
[Epoch 27] ogbg-molsider: 0.587170 test loss: 0.598429
[Epoch 28; Iter     6/   32] train: loss: 0.6010513
[Epoch 28] ogbg-molsider: 0.580500 val loss: 0.634281
[Epoch 28] ogbg-molsider: 0.571820 test loss: 0.613856
[Epoch 29; Iter     4/   32] train: loss: 0.5791893
[Epoch 29] ogbg-molsider: 0.587103 val loss: 0.531634
[Epoch 29] ogbg-molsider: 0.616921 test loss: 0.528785
[Epoch 30; Iter     2/   32] train: loss: 0.5599984
[Epoch 30; Iter    32/   32] train: loss: 0.5207247
[Epoch 30] ogbg-molsider: 0.576097 val loss: 0.541009
[Epoch 30] ogbg-molsider: 0.581992 test loss: 0.541791
[Epoch 31; Iter    30/   32] train: loss: 0.5354003
[Epoch 31] ogbg-molsider: 0.589992 val loss: 0.515243
[Epoch 31] ogbg-molsider: 0.620657 test loss: 0.511024
[Epoch 32; Iter    28/   32] train: loss: 0.5265922
[Epoch 32] ogbg-molsider: 0.592410 val loss: 0.516056
[Epoch 32] ogbg-molsider: 0.626715 test loss: 0.511125
[Epoch 33; Iter    26/   32] train: loss: 0.4969771
[Epoch 33] ogbg-molsider: 0.596750 val loss: 0.511278
[Epoch 33] ogbg-molsider: 0.628326 test loss: 0.507522
[Epoch 34; Iter    24/   32] train: loss: 0.4816220
[Epoch 34] ogbg-molsider: 0.591069 val loss: 0.520509
[Epoch 34] ogbg-molsider: 0.605024 test loss: 0.515218
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.6/PNA_ogbg-molsider_3DInfomax_sider_random=0.6_5_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.6
logdir: runs/split/3DInfomax/sider/random/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.486134 val loss: 0.693171
[Epoch 1] ogbg-molsider: 0.499275 test loss: 0.693160
[Epoch 2; Iter     3/   27] train: loss: 0.6935107
[Epoch 2] ogbg-molsider: 0.483923 val loss: 0.693226
[Epoch 2] ogbg-molsider: 0.496432 test loss: 0.693182
[Epoch 3; Iter     6/   27] train: loss: 0.6934532
[Epoch 3] ogbg-molsider: 0.484095 val loss: 0.693260
[Epoch 3] ogbg-molsider: 0.493979 test loss: 0.693158
[Epoch 4; Iter     9/   27] train: loss: 0.6936073
[Epoch 4] ogbg-molsider: 0.484700 val loss: 0.693293
[Epoch 4] ogbg-molsider: 0.492913 test loss: 0.693172
[Epoch 5; Iter    12/   27] train: loss: 0.6934829
[Epoch 5] ogbg-molsider: 0.485552 val loss: 0.693278
[Epoch 5] ogbg-molsider: 0.494303 test loss: 0.693111
[Epoch 6; Iter    15/   27] train: loss: 0.6932060
[Epoch 6] ogbg-molsider: 0.486046 val loss: 0.693168
[Epoch 6] ogbg-molsider: 0.494359 test loss: 0.693006
[Epoch 7; Iter    18/   27] train: loss: 0.6935660
[Epoch 7] ogbg-molsider: 0.486134 val loss: 0.693169
[Epoch 7] ogbg-molsider: 0.492686 test loss: 0.692980
[Epoch 8; Iter    21/   27] train: loss: 0.6934003
[Epoch 8] ogbg-molsider: 0.487144 val loss: 0.693103
[Epoch 8] ogbg-molsider: 0.494590 test loss: 0.692946
[Epoch 9; Iter    24/   27] train: loss: 0.6934562
[Epoch 9] ogbg-molsider: 0.487168 val loss: 0.693071
[Epoch 9] ogbg-molsider: 0.494637 test loss: 0.692912
[Epoch 10; Iter    27/   27] train: loss: 0.6925524
[Epoch 10] ogbg-molsider: 0.486431 val loss: 0.693031
[Epoch 10] ogbg-molsider: 0.495530 test loss: 0.692853
[Epoch 11] ogbg-molsider: 0.487593 val loss: 0.692946
[Epoch 11] ogbg-molsider: 0.494350 test loss: 0.692764
[Epoch 12; Iter     3/   27] train: loss: 0.6929913
[Epoch 12] ogbg-molsider: 0.486311 val loss: 0.692900
[Epoch 12] ogbg-molsider: 0.496709 test loss: 0.692728
[Epoch 13; Iter     6/   27] train: loss: 0.6928370
[Epoch 13] ogbg-molsider: 0.485715 val loss: 0.692842
[Epoch 13] ogbg-molsider: 0.494290 test loss: 0.692657
[Epoch 14; Iter     9/   27] train: loss: 0.6935570
[Epoch 14] ogbg-molsider: 0.486533 val loss: 0.692779
[Epoch 14] ogbg-molsider: 0.495672 test loss: 0.692591
[Epoch 15; Iter    12/   27] train: loss: 0.6925992
[Epoch 15] ogbg-molsider: 0.487431 val loss: 0.692639
[Epoch 15] ogbg-molsider: 0.495924 test loss: 0.692431
[Epoch 16; Iter    15/   27] train: loss: 0.6919600
[Epoch 16] ogbg-molsider: 0.486289 val loss: 0.692510
[Epoch 16] ogbg-molsider: 0.496699 test loss: 0.692327
[Epoch 17; Iter    18/   27] train: loss: 0.6916282
[Epoch 17] ogbg-molsider: 0.485213 val loss: 0.692426
[Epoch 17] ogbg-molsider: 0.495012 test loss: 0.692220
[Epoch 18; Iter    21/   27] train: loss: 0.6922569
[Epoch 18] ogbg-molsider: 0.486553 val loss: 0.692252
[Epoch 18] ogbg-molsider: 0.494775 test loss: 0.692099
[Epoch 19; Iter    24/   27] train: loss: 0.6916257
[Epoch 19] ogbg-molsider: 0.484768 val loss: 0.692235
[Epoch 19] ogbg-molsider: 0.495979 test loss: 0.692040
[Epoch 20; Iter    27/   27] train: loss: 0.6922068
[Epoch 20] ogbg-molsider: 0.486817 val loss: 0.692045
[Epoch 20] ogbg-molsider: 0.495852 test loss: 0.691853
[Epoch 21] ogbg-molsider: 0.485365 val loss: 0.691932
[Epoch 21] ogbg-molsider: 0.495358 test loss: 0.691745
[Epoch 22; Iter     3/   27] train: loss: 0.6917306
[Epoch 22] ogbg-molsider: 0.486875 val loss: 0.691819
[Epoch 22] ogbg-molsider: 0.497000 test loss: 0.691607
[Epoch 23; Iter     6/   27] train: loss: 0.6915712
[Epoch 23] ogbg-molsider: 0.487110 val loss: 0.691677
[Epoch 23] ogbg-molsider: 0.495546 test loss: 0.691487
[Epoch 24; Iter     9/   27] train: loss: 0.6921046
[Epoch 24] ogbg-molsider: 0.486093 val loss: 0.691512
[Epoch 24] ogbg-molsider: 0.496997 test loss: 0.691318
[Epoch 25; Iter    12/   27] train: loss: 0.6910039
[Epoch 25] ogbg-molsider: 0.486406 val loss: 0.691358
[Epoch 25] ogbg-molsider: 0.496256 test loss: 0.691086
[Epoch 26; Iter    15/   27] train: loss: 0.6904112
[Epoch 26] ogbg-molsider: 0.547675 val loss: 0.688218
[Epoch 26] ogbg-molsider: 0.557014 test loss: 0.687701
[Epoch 27; Iter    18/   27] train: loss: 0.6838135
[Epoch 27] ogbg-molsider: 0.576244 val loss: 0.681001
[Epoch 27] ogbg-molsider: 0.572827 test loss: 0.680338
[Epoch 28; Iter    21/   27] train: loss: 0.6746712
[Epoch 28] ogbg-molsider: 0.578293 val loss: 0.670740
[Epoch 28] ogbg-molsider: 0.586356 test loss: 0.668307
[Epoch 29; Iter    24/   27] train: loss: 0.6574190
[Epoch 29] ogbg-molsider: 0.588932 val loss: 0.639154
[Epoch 29] ogbg-molsider: 0.582999 test loss: 0.634153
[Epoch 30; Iter    27/   27] train: loss: 0.6392086
[Epoch 30] ogbg-molsider: 0.587827 val loss: 0.629975
[Epoch 30] ogbg-molsider: 0.585849 test loss: 0.622301
[Epoch 31] ogbg-molsider: 0.582981 val loss: 0.616408
[Epoch 31] ogbg-molsider: 0.596632 test loss: 0.607502
[Epoch 32; Iter     3/   27] train: loss: 0.6075495
[Epoch 32] ogbg-molsider: 0.573520 val loss: 0.577065
[Epoch 32] ogbg-molsider: 0.580502 test loss: 0.567480
[Epoch 33; Iter     6/   27] train: loss: 0.6037740
[Epoch 33] ogbg-molsider: 0.581485 val loss: 0.584875
[Epoch 33] ogbg-molsider: 0.579401 test loss: 0.576379
[Epoch 34; Iter     9/   27] train: loss: 0.5296150
[Epoch 34] ogbg-molsider: 0.571780 val loss: 0.553201
[Epoch 34] ogbg-molsider: 0.595672 test loss: 0.538051
[Epoch 35; Iter    12/   27] train: loss: 0.5255781
[Epoch 35] ogbg-molsider: 0.584029 val loss: 0.610203
[Epoch 35] ogbg-molsider: 0.590027 test loss: 0.539804
[Epoch 36; Iter    15/   27] train: loss: 0.5040335
[Epoch 36] ogbg-molsider: 0.612506 val loss: 0.526412
[Epoch 36] ogbg-molsider: 0.605699 test loss: 0.514495
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.6/PNA_ogbg-molsider_3DInfomax_sider_random=0.6_4_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.6
logdir: runs/split/3DInfomax/sider/random/train_prop=0.6
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.6
[Epoch 1] ogbg-molsider: 0.521727 val loss: 0.693211
[Epoch 1] ogbg-molsider: 0.494435 test loss: 0.693243
[Epoch 2; Iter     3/   27] train: loss: 0.6930032
[Epoch 2] ogbg-molsider: 0.519778 val loss: 0.693132
[Epoch 2] ogbg-molsider: 0.494762 test loss: 0.693212
[Epoch 3; Iter     6/   27] train: loss: 0.6932088
[Epoch 3] ogbg-molsider: 0.518302 val loss: 0.693079
[Epoch 3] ogbg-molsider: 0.498333 test loss: 0.693174
[Epoch 4; Iter     9/   27] train: loss: 0.6931924
[Epoch 4] ogbg-molsider: 0.516599 val loss: 0.693079
[Epoch 4] ogbg-molsider: 0.500275 test loss: 0.693147
[Epoch 5; Iter    12/   27] train: loss: 0.6932216
[Epoch 5] ogbg-molsider: 0.517072 val loss: 0.693073
[Epoch 5] ogbg-molsider: 0.503120 test loss: 0.693105
[Epoch 6; Iter    15/   27] train: loss: 0.6930925
[Epoch 6] ogbg-molsider: 0.515602 val loss: 0.692986
[Epoch 6] ogbg-molsider: 0.501748 test loss: 0.693040
[Epoch 7; Iter    18/   27] train: loss: 0.6926525
[Epoch 7] ogbg-molsider: 0.516498 val loss: 0.693057
[Epoch 7] ogbg-molsider: 0.500262 test loss: 0.693151
[Epoch 8; Iter    21/   27] train: loss: 0.6924386
[Epoch 8] ogbg-molsider: 0.515106 val loss: 0.692933
[Epoch 8] ogbg-molsider: 0.501086 test loss: 0.692997
[Epoch 9; Iter    24/   27] train: loss: 0.6928833
[Epoch 9] ogbg-molsider: 0.515883 val loss: 0.692873
[Epoch 9] ogbg-molsider: 0.500978 test loss: 0.692926
[Epoch 10; Iter    27/   27] train: loss: 0.6918858
[Epoch 10] ogbg-molsider: 0.514474 val loss: 0.692837
[Epoch 10] ogbg-molsider: 0.502469 test loss: 0.692858
[Epoch 11] ogbg-molsider: 0.515382 val loss: 0.692765
[Epoch 11] ogbg-molsider: 0.502204 test loss: 0.692760
[Epoch 12; Iter     3/   27] train: loss: 0.6928242
[Epoch 12] ogbg-molsider: 0.514757 val loss: 0.692667
[Epoch 12] ogbg-molsider: 0.502089 test loss: 0.692707
[Epoch 13; Iter     6/   27] train: loss: 0.6923491
[Epoch 13] ogbg-molsider: 0.517028 val loss: 0.692600
[Epoch 13] ogbg-molsider: 0.502979 test loss: 0.692612
[Epoch 14; Iter     9/   27] train: loss: 0.6927452
[Epoch 14] ogbg-molsider: 0.517853 val loss: 0.692495
[Epoch 14] ogbg-molsider: 0.502868 test loss: 0.692521
[Epoch 15; Iter    12/   27] train: loss: 0.6923218
[Epoch 15] ogbg-molsider: 0.516681 val loss: 0.692440
[Epoch 15] ogbg-molsider: 0.502853 test loss: 0.692451
[Epoch 16; Iter    15/   27] train: loss: 0.6922195
[Epoch 16] ogbg-molsider: 0.518409 val loss: 0.692327
[Epoch 16] ogbg-molsider: 0.502447 test loss: 0.692357
[Epoch 17; Iter    18/   27] train: loss: 0.6920108
[Epoch 17] ogbg-molsider: 0.518231 val loss: 0.692196
[Epoch 17] ogbg-molsider: 0.502833 test loss: 0.692236
[Epoch 18; Iter    21/   27] train: loss: 0.6919426
[Epoch 18] ogbg-molsider: 0.516012 val loss: 0.692085
[Epoch 18] ogbg-molsider: 0.502889 test loss: 0.692102
[Epoch 19; Iter    24/   27] train: loss: 0.6922066
[Epoch 19] ogbg-molsider: 0.517343 val loss: 0.691936
[Epoch 19] ogbg-molsider: 0.502311 test loss: 0.691973
[Epoch 20; Iter    27/   27] train: loss: 0.6917057
[Epoch 20] ogbg-molsider: 0.517391 val loss: 0.691788
[Epoch 20] ogbg-molsider: 0.503142 test loss: 0.691805
[Epoch 21] ogbg-molsider: 0.516626 val loss: 0.691759
[Epoch 21] ogbg-molsider: 0.502702 test loss: 0.691747
[Epoch 22; Iter     3/   27] train: loss: 0.6912109
[Epoch 22] ogbg-molsider: 0.517987 val loss: 0.691563
[Epoch 22] ogbg-molsider: 0.503499 test loss: 0.691546
[Epoch 23; Iter     6/   27] train: loss: 0.6915807
[Epoch 23] ogbg-molsider: 0.517306 val loss: 0.691418
[Epoch 23] ogbg-molsider: 0.504116 test loss: 0.691400
[Epoch 24; Iter     9/   27] train: loss: 0.6910915
[Epoch 24] ogbg-molsider: 0.517072 val loss: 0.691259
[Epoch 24] ogbg-molsider: 0.504882 test loss: 0.691224
[Epoch 25; Iter    12/   27] train: loss: 0.6906518
[Epoch 25] ogbg-molsider: 0.518140 val loss: 0.691061
[Epoch 25] ogbg-molsider: 0.505378 test loss: 0.691019
[Epoch 26; Iter    15/   27] train: loss: 0.6901662
[Epoch 26] ogbg-molsider: 0.540295 val loss: 0.687376
[Epoch 26] ogbg-molsider: 0.543238 test loss: 0.686915
[Epoch 27; Iter    18/   27] train: loss: 0.6832764
[Epoch 27] ogbg-molsider: 0.561347 val loss: 0.678970
[Epoch 27] ogbg-molsider: 0.573987 test loss: 0.678269
[Epoch 28; Iter    21/   27] train: loss: 0.6713295
[Epoch 28] ogbg-molsider: 0.576176 val loss: 0.662598
[Epoch 28] ogbg-molsider: 0.576482 test loss: 0.659019
[Epoch 29; Iter    24/   27] train: loss: 0.6548596
[Epoch 29] ogbg-molsider: 0.553626 val loss: 0.659486
[Epoch 29] ogbg-molsider: 0.575736 test loss: 0.659035
[Epoch 30; Iter    27/   27] train: loss: 0.6338624
[Epoch 30] ogbg-molsider: 0.569693 val loss: 0.636249
[Epoch 30] ogbg-molsider: 0.573535 test loss: 0.634574
[Epoch 31] ogbg-molsider: 0.571285 val loss: 0.612024
[Epoch 31] ogbg-molsider: 0.586163 test loss: 0.608250
[Epoch 32; Iter     3/   27] train: loss: 0.6082854
[Epoch 32] ogbg-molsider: 0.588011 val loss: 0.585432
[Epoch 32] ogbg-molsider: 0.593803 test loss: 0.574399
[Epoch 33; Iter     6/   27] train: loss: 0.5702043
[Epoch 33] ogbg-molsider: 0.597068 val loss: 0.576479
[Epoch 33] ogbg-molsider: 0.585673 test loss: 0.564972
[Epoch 34; Iter     9/   27] train: loss: 0.5532013
[Epoch 34] ogbg-molsider: 0.568922 val loss: 0.566852
[Epoch 34] ogbg-molsider: 0.580873 test loss: 0.556897
[Epoch 35; Iter    12/   27] train: loss: 0.5188922
[Epoch 35] ogbg-molsider: 0.582914 val loss: 0.538096
[Epoch 35] ogbg-molsider: 0.592621 test loss: 0.524353
[Epoch 36; Iter    15/   27] train: loss: 0.5246345
[Epoch 36] ogbg-molsider: 0.594639 val loss: 0.528930
[Epoch 36] ogbg-molsider: 0.608472 test loss: 0.512399
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.7/PNA_ogbg-molsider_3DInfomax_sider_random=0.7_6_26-05_09-18-11
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.7
logdir: runs/split/3DInfomax/sider/random/train_prop=0.7
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.7
[Epoch 1; Iter    30/   32] train: loss: 0.6930413
[Epoch 1] ogbg-molsider: 0.502644 val loss: 0.693164
[Epoch 1] ogbg-molsider: 0.498540 test loss: 0.693159
[Epoch 2; Iter    28/   32] train: loss: 0.6934702
[Epoch 2] ogbg-molsider: 0.500250 val loss: 0.693247
[Epoch 2] ogbg-molsider: 0.492782 test loss: 0.693248
[Epoch 3; Iter    26/   32] train: loss: 0.6925754
[Epoch 3] ogbg-molsider: 0.501824 val loss: 0.693195
[Epoch 3] ogbg-molsider: 0.493425 test loss: 0.693178
[Epoch 4; Iter    24/   32] train: loss: 0.6935599
[Epoch 4] ogbg-molsider: 0.503937 val loss: 0.693290
[Epoch 4] ogbg-molsider: 0.490472 test loss: 0.693169
[Epoch 5; Iter    22/   32] train: loss: 0.6936342
[Epoch 5] ogbg-molsider: 0.503275 val loss: 0.693334
[Epoch 5] ogbg-molsider: 0.493262 test loss: 0.693259
[Epoch 6; Iter    20/   32] train: loss: 0.6932157
[Epoch 6] ogbg-molsider: 0.503573 val loss: 0.693087
[Epoch 6] ogbg-molsider: 0.494150 test loss: 0.693020
[Epoch 7; Iter    18/   32] train: loss: 0.6932127
[Epoch 7] ogbg-molsider: 0.502600 val loss: 0.693044
[Epoch 7] ogbg-molsider: 0.493653 test loss: 0.693005
[Epoch 8; Iter    16/   32] train: loss: 0.6928018
[Epoch 8] ogbg-molsider: 0.502533 val loss: 0.693086
[Epoch 8] ogbg-molsider: 0.492070 test loss: 0.692976
[Epoch 9; Iter    14/   32] train: loss: 0.6932636
[Epoch 9] ogbg-molsider: 0.507334 val loss: 0.693113
[Epoch 9] ogbg-molsider: 0.490094 test loss: 0.693007
[Epoch 10; Iter    12/   32] train: loss: 0.6924845
[Epoch 10] ogbg-molsider: 0.503462 val loss: 0.692894
[Epoch 10] ogbg-molsider: 0.493413 test loss: 0.692852
[Epoch 11; Iter    10/   32] train: loss: 0.6926920
[Epoch 11] ogbg-molsider: 0.508846 val loss: 0.692831
[Epoch 11] ogbg-molsider: 0.490432 test loss: 0.692732
[Epoch 12; Iter     8/   32] train: loss: 0.6930901
[Epoch 12] ogbg-molsider: 0.505878 val loss: 0.692607
[Epoch 12] ogbg-molsider: 0.493971 test loss: 0.692505
[Epoch 13; Iter     6/   32] train: loss: 0.6926781
[Epoch 13] ogbg-molsider: 0.505618 val loss: 0.692604
[Epoch 13] ogbg-molsider: 0.493911 test loss: 0.692520
[Epoch 14; Iter     4/   32] train: loss: 0.6921240
[Epoch 14] ogbg-molsider: 0.505217 val loss: 0.692444
[Epoch 14] ogbg-molsider: 0.494789 test loss: 0.692336
[Epoch 15; Iter     2/   32] train: loss: 0.6924999
[Epoch 15; Iter    32/   32] train: loss: 0.6931727
[Epoch 15] ogbg-molsider: 0.505208 val loss: 0.692426
[Epoch 15] ogbg-molsider: 0.491819 test loss: 0.692376
[Epoch 16; Iter    30/   32] train: loss: 0.6921852
[Epoch 16] ogbg-molsider: 0.504992 val loss: 0.692250
[Epoch 16] ogbg-molsider: 0.493185 test loss: 0.692118
[Epoch 17; Iter    28/   32] train: loss: 0.6930045
[Epoch 17] ogbg-molsider: 0.504099 val loss: 0.692161
[Epoch 17] ogbg-molsider: 0.493747 test loss: 0.692048
[Epoch 18; Iter    26/   32] train: loss: 0.6925045
[Epoch 18] ogbg-molsider: 0.502489 val loss: 0.691948
[Epoch 18] ogbg-molsider: 0.494897 test loss: 0.691910
[Epoch 19; Iter    24/   32] train: loss: 0.6916236
[Epoch 19] ogbg-molsider: 0.505597 val loss: 0.691801
[Epoch 19] ogbg-molsider: 0.492908 test loss: 0.691716
[Epoch 20; Iter    22/   32] train: loss: 0.6912243
[Epoch 20] ogbg-molsider: 0.505045 val loss: 0.691659
[Epoch 20] ogbg-molsider: 0.493450 test loss: 0.691518
[Epoch 21; Iter    20/   32] train: loss: 0.6917278
[Epoch 21] ogbg-molsider: 0.504938 val loss: 0.691396
[Epoch 21] ogbg-molsider: 0.495211 test loss: 0.691333
[Epoch 22; Iter    18/   32] train: loss: 0.6909157
[Epoch 22] ogbg-molsider: 0.541869 val loss: 0.688574
[Epoch 22] ogbg-molsider: 0.532376 test loss: 0.688396
[Epoch 23; Iter    16/   32] train: loss: 0.6849363
[Epoch 23] ogbg-molsider: 0.579333 val loss: 0.677807
[Epoch 23] ogbg-molsider: 0.589279 test loss: 0.677644
[Epoch 24; Iter    14/   32] train: loss: 0.6741001
[Epoch 24] ogbg-molsider: 0.576845 val loss: 0.648466
[Epoch 24] ogbg-molsider: 0.590006 test loss: 0.650549
[Epoch 25; Iter    12/   32] train: loss: 0.6603343
[Epoch 25] ogbg-molsider: 0.574263 val loss: 0.657095
[Epoch 25] ogbg-molsider: 0.593534 test loss: 0.657649
[Epoch 26; Iter    10/   32] train: loss: 0.6398084
[Epoch 26] ogbg-molsider: 0.580199 val loss: 0.607709
[Epoch 26] ogbg-molsider: 0.585814 test loss: 0.609297
[Epoch 27; Iter     8/   32] train: loss: 0.6080372
[Epoch 27] ogbg-molsider: 0.600896 val loss: 0.590137
[Epoch 27] ogbg-molsider: 0.595536 test loss: 0.589927
[Epoch 28; Iter     6/   32] train: loss: 0.5764342
[Epoch 28] ogbg-molsider: 0.564438 val loss: 0.576409
[Epoch 28] ogbg-molsider: 0.590347 test loss: 0.574256
[Epoch 29; Iter     4/   32] train: loss: 0.5647517
[Epoch 29] ogbg-molsider: 0.580322 val loss: 0.534783
[Epoch 29] ogbg-molsider: 0.603466 test loss: 0.536052
[Epoch 30; Iter     2/   32] train: loss: 0.5513214
[Epoch 30; Iter    32/   32] train: loss: 0.4846966
[Epoch 30] ogbg-molsider: 0.593239 val loss: 0.536077
[Epoch 30] ogbg-molsider: 0.602999 test loss: 0.534558
[Epoch 31; Iter    30/   32] train: loss: 0.5182509
[Epoch 31] ogbg-molsider: 0.604158 val loss: 0.514937
[Epoch 31] ogbg-molsider: 0.615518 test loss: 0.515144
[Epoch 32; Iter    28/   32] train: loss: 0.5259781
[Epoch 32] ogbg-molsider: 0.597965 val loss: 0.507760
[Epoch 32] ogbg-molsider: 0.613054 test loss: 0.508031
[Epoch 33; Iter    26/   32] train: loss: 0.4598686
[Epoch 33] ogbg-molsider: 0.608873 val loss: 0.505450
[Epoch 33] ogbg-molsider: 0.612558 test loss: 0.519121
[Epoch 34; Iter    24/   32] train: loss: 0.4855641
[Epoch 34] ogbg-molsider: 0.591870 val loss: 0.504212
[Epoch 34] ogbg-molsider: 0.599818 test loss: 0.558941
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/split/3DInfomax/sider/random/train_prop=0.8/PNA_ogbg-molsider_3DInfomax_sider_random=0.8_6_26-05_09-18-12
config: <_io.TextIOWrapper name='configs_split_experiments/3DInfomax/sider/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
experiment_name: 3DInfomax_sider_random=0.8
logdir: runs/split/3DInfomax/sider/random/train_prop=0.8
num_epochs: 1000
batch_size: 32
patience: 60
minimum_epochs: 120
dataset: ogbg-molsider
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molsider
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_3DInfomax_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 27
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: True
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: True
train_prop: 0.8
[Epoch 1; Iter    30/   36] train: loss: 0.6929152
[Epoch 1] ogbg-molsider: 0.495448 val loss: 0.693095
[Epoch 1] ogbg-molsider: 0.501045 test loss: 0.693116
[Epoch 2; Iter    24/   36] train: loss: 0.6931813
[Epoch 2] ogbg-molsider: 0.496969 val loss: 0.693241
[Epoch 2] ogbg-molsider: 0.497015 test loss: 0.693184
[Epoch 3; Iter    18/   36] train: loss: 0.6937016
[Epoch 3] ogbg-molsider: 0.493394 val loss: 0.693274
[Epoch 3] ogbg-molsider: 0.496465 test loss: 0.693135
[Epoch 4; Iter    12/   36] train: loss: 0.6940231
[Epoch 4] ogbg-molsider: 0.492461 val loss: 0.693228
[Epoch 4] ogbg-molsider: 0.499651 test loss: 0.693114
[Epoch 5; Iter     6/   36] train: loss: 0.6932724
[Epoch 5; Iter    36/   36] train: loss: 0.6932138
[Epoch 5] ogbg-molsider: 0.491322 val loss: 0.693204
[Epoch 5] ogbg-molsider: 0.500055 test loss: 0.693042
[Epoch 6; Iter    30/   36] train: loss: 0.6938044
[Epoch 6] ogbg-molsider: 0.494365 val loss: 0.693024
[Epoch 6] ogbg-molsider: 0.500789 test loss: 0.692922
[Epoch 7; Iter    24/   36] train: loss: 0.6930036
[Epoch 7] ogbg-molsider: 0.495944 val loss: 0.692977
[Epoch 7] ogbg-molsider: 0.499071 test loss: 0.692866
[Epoch 8; Iter    18/   36] train: loss: 0.6931069
[Epoch 8] ogbg-molsider: 0.491877 val loss: 0.692927
[Epoch 8] ogbg-molsider: 0.500576 test loss: 0.692801
[Epoch 9; Iter    12/   36] train: loss: 0.6924251
[Epoch 9] ogbg-molsider: 0.495053 val loss: 0.692880
[Epoch 9] ogbg-molsider: 0.498590 test loss: 0.692836
[Epoch 10; Iter     6/   36] train: loss: 0.6928104
[Epoch 10; Iter    36/   36] train: loss: 0.6933575
[Epoch 10] ogbg-molsider: 0.492797 val loss: 0.692659
[Epoch 10] ogbg-molsider: 0.501754 test loss: 0.692525
[Epoch 11; Iter    30/   36] train: loss: 0.6927308
[Epoch 11] ogbg-molsider: 0.496655 val loss: 0.692607
[Epoch 11] ogbg-molsider: 0.498991 test loss: 0.692573
[Epoch 12; Iter    24/   36] train: loss: 0.6923656
[Epoch 12] ogbg-molsider: 0.495703 val loss: 0.692456
[Epoch 12] ogbg-molsider: 0.499102 test loss: 0.692394
[Epoch 13; Iter    18/   36] train: loss: 0.6925353
[Epoch 13] ogbg-molsider: 0.496285 val loss: 0.692380
[Epoch 13] ogbg-molsider: 0.496101 test loss: 0.692357
[Epoch 14; Iter    12/   36] train: loss: 0.6924168
[Epoch 14] ogbg-molsider: 0.496572 val loss: 0.692208
[Epoch 14] ogbg-molsider: 0.497287 test loss: 0.692212
[Epoch 15; Iter     6/   36] train: loss: 0.6923648
[Epoch 15; Iter    36/   36] train: loss: 0.6922622
[Epoch 15] ogbg-molsider: 0.495027 val loss: 0.692021
[Epoch 15] ogbg-molsider: 0.499608 test loss: 0.691977
[Epoch 16; Iter    30/   36] train: loss: 0.6924388
[Epoch 16] ogbg-molsider: 0.495440 val loss: 0.691901
[Epoch 16] ogbg-molsider: 0.498050 test loss: 0.691936
[Epoch 17; Iter    24/   36] train: loss: 0.6918198
[Epoch 17] ogbg-molsider: 0.495296 val loss: 0.691656
[Epoch 17] ogbg-molsider: 0.498082 test loss: 0.691718
[Epoch 18; Iter    18/   36] train: loss: 0.6916385
[Epoch 18] ogbg-molsider: 0.494295 val loss: 0.691454
[Epoch 18] ogbg-molsider: 0.501157 test loss: 0.691495
[Epoch 19; Iter    12/   36] train: loss: 0.6915752
[Epoch 19] ogbg-molsider: 0.495038 val loss: 0.691245
[Epoch 19] ogbg-molsider: 0.500086 test loss: 0.691277
[Epoch 20; Iter     6/   36] train: loss: 0.6910390
[Epoch 20; Iter    36/   36] train: loss: 0.6860039
[Epoch 20] ogbg-molsider: 0.548714 val loss: 0.682546
[Epoch 20] ogbg-molsider: 0.559144 test loss: 0.683102
[Epoch 21; Iter    30/   36] train: loss: 0.6719348
[Epoch 21] ogbg-molsider: 0.567214 val loss: 0.675507
[Epoch 21] ogbg-molsider: 0.584026 test loss: 0.673973
[Epoch 22; Iter    24/   36] train: loss: 0.6644042
[Epoch 22] ogbg-molsider: 0.574605 val loss: 0.629210
[Epoch 22] ogbg-molsider: 0.599638 test loss: 0.633565
[Epoch 23; Iter    18/   36] train: loss: 0.6327379
[Epoch 23] ogbg-molsider: 0.582582 val loss: 0.616659
[Epoch 23] ogbg-molsider: 0.582881 test loss: 0.635615
[Epoch 24; Iter    12/   36] train: loss: 0.6047373
[Epoch 24] ogbg-molsider: 0.581950 val loss: 0.601782
[Epoch 24] ogbg-molsider: 0.592682 test loss: 0.615936
[Epoch 25; Iter     6/   36] train: loss: 0.5856062
[Epoch 25; Iter    36/   36] train: loss: 0.5780958
[Epoch 25] ogbg-molsider: 0.589402 val loss: 0.552971
[Epoch 25] ogbg-molsider: 0.609410 test loss: 0.571313
[Epoch 26; Iter    30/   36] train: loss: 0.5303419
[Epoch 26] ogbg-molsider: 0.614877 val loss: 0.528348
[Epoch 26] ogbg-molsider: 0.617651 test loss: 0.539954
[Epoch 27; Iter    24/   36] train: loss: 0.5519477
[Epoch 27] ogbg-molsider: 0.582726 val loss: 0.503629
[Epoch 27] ogbg-molsider: 0.598816 test loss: 0.529591
[Epoch 28; Iter    18/   36] train: loss: 0.5209541
[Epoch 28] ogbg-molsider: 0.603838 val loss: 0.505752
[Epoch 28] ogbg-molsider: 0.600462 test loss: 0.526805
[Epoch 29; Iter    12/   36] train: loss: 0.5236310
[Epoch 29] ogbg-molsider: 0.591752 val loss: 0.489027
[Epoch 29] ogbg-molsider: 0.618138 test loss: 0.515877
[Epoch 30; Iter     6/   36] train: loss: 0.4886379
[Epoch 30; Iter    36/   36] train: loss: 0.5108312
[Epoch 30] ogbg-molsider: 0.611389 val loss: 0.493727
[Epoch 30] ogbg-molsider: 0.601789 test loss: 0.525433
[Epoch 31; Iter    30/   36] train: loss: 0.5255016
[Epoch 31] ogbg-molsider: 0.628049 val loss: 0.482430
[Epoch 31] ogbg-molsider: 0.623656 test loss: 0.518370
[Epoch 32; Iter    24/   36] train: loss: 0.4969444
[Epoch 32] ogbg-molsider: 0.619197 val loss: 0.483434
[Epoch 32] ogbg-molsider: 0.627148 test loss: 0.520468
[Epoch 33; Iter    18/   36] train: loss: 0.4685612
[Epoch 33] ogbg-molsider: 0.590987 val loss: 0.493032
[Epoch 33] ogbg-molsider: 0.618360 test loss: 0.518068
[Epoch 34; Iter    12/   36] train: loss: 0.4482723
[Epoch 34] ogbg-molsider: 0.623781 val loss: 0.489815
[Epoch 34] ogbg-molsider: 0.605744 test loss: 0.512163
[Epoch 35; Iter     6/   36] train: loss: 0.4710655
[Epoch 35; Iter    36/   36] train: loss: 0.5175349
[Epoch 35] ogbg-molsider: 0.625802 val loss: 0.487021
[Epoch 35] ogbg-molsider: 0.617812 test loss: 0.512754
[Epoch 36; Iter    30/   36] train: loss: 0.4796726
[Epoch 36] ogbg-molsider: 0.629805 val loss: 0.483707
[Epoch 36] ogbg-molsider: 0.616042 test loss: 0.516104
[Epoch 37; Iter    24/   36] train: loss: 0.4358745
[Epoch 37] ogbg-molsider: 0.637350 val loss: 0.501359
[Epoch 37] ogbg-molsider: 0.625107 test loss: 0.529000
[Epoch 38; Iter    18/   36] train: loss: 0.4766003
[Epoch 38] ogbg-molsider: 0.613880 val loss: 0.486827
[Epoch 38] ogbg-molsider: 0.626288 test loss: 0.511599
[Epoch 39; Iter    12/   36] train: loss: 0.4799365
[Epoch 39] ogbg-molsider: 0.613561 val loss: 0.493218
[Epoch 39] ogbg-molsider: 0.613653 test loss: 0.531676
[Epoch 40; Iter     6/   36] train: loss: 0.5208200
[Epoch 40; Iter    36/   36] train: loss: 0.5307758
[Epoch 40] ogbg-molsider: 0.641507 val loss: 0.536920
[Epoch 40] ogbg-molsider: 0.586996 test loss: 0.554787
[Epoch 41; Iter    30/   36] train: loss: 0.4515329
[Epoch 41] ogbg-molsider: 0.579471 val loss: 0.502543
[Epoch 41] ogbg-molsider: 0.605108 test loss: 0.526833
[Epoch 42; Iter    24/   36] train: loss: 0.4214414
[Epoch 42] ogbg-molsider: 0.616884 val loss: 0.502163
[Epoch 42] ogbg-molsider: 0.602037 test loss: 0.538842
[Epoch 43; Iter    18/   36] train: loss: 0.4623606
[Epoch 43] ogbg-molsider: 0.639530 val loss: 0.943702
[Epoch 43] ogbg-molsider: 0.603472 test loss: 0.536693
[Epoch 44; Iter    12/   36] train: loss: 0.4742836
[Epoch 44] ogbg-molsider: 0.623503 val loss: 0.504014
[Epoch 44] ogbg-molsider: 0.610192 test loss: 0.527825
[Epoch 45; Iter     6/   36] train: loss: 0.4342494
[Epoch 45; Iter    36/   36] train: loss: 0.5124631
[Epoch 45] ogbg-molsider: 0.652904 val loss: 0.484874
[Epoch 45] ogbg-molsider: 0.613969 test loss: 0.522264
[Epoch 46; Iter    30/   36] train: loss: 0.4770629
[Epoch 46] ogbg-molsider: 0.614941 val loss: 0.501829
[Epoch 46] ogbg-molsider: 0.621920 test loss: 0.516307
[Epoch 47; Iter    24/   36] train: loss: 0.4429494
[Epoch 47] ogbg-molsider: 0.615027 val loss: 0.491199
[Epoch 47] ogbg-molsider: 0.626624 test loss: 0.540602
[Epoch 48; Iter    18/   36] train: loss: 0.4427776
[Epoch 48] ogbg-molsider: 0.622539 val loss: 0.488326
[Epoch 48] ogbg-molsider: 0.638931 test loss: 0.506240
[Epoch 49; Iter    12/   36] train: loss: 0.4859602
[Epoch 49] ogbg-molsider: 0.637641 val loss: 0.493614
[Epoch 49] ogbg-molsider: 0.613952 test loss: 0.520199
[Epoch 50; Iter     6/   36] train: loss: 0.4642263
[Epoch 50; Iter    36/   36] train: loss: 0.4350519
[Epoch 50] ogbg-molsider: 0.650550 val loss: 0.480680
[Epoch 50] ogbg-molsider: 0.635263 test loss: 0.526991
[Epoch 51; Iter    30/   36] train: loss: 0.4898344
[Epoch 51] ogbg-molsider: 0.631493 val loss: 0.492375
[Epoch 51] ogbg-molsider: 0.633654 test loss: 0.514946
[Epoch 52; Iter    24/   36] train: loss: 0.5049967
[Epoch 52] ogbg-molsider: 0.632787 val loss: 0.531216
[Epoch 52] ogbg-molsider: 0.624526 test loss: 0.535833
[Epoch 53; Iter    18/   36] train: loss: 0.4141264
[Epoch 53] ogbg-molsider: 0.653837 val loss: 1.472009
[Epoch 53] ogbg-molsider: 0.625881 test loss: 1.163256
[Epoch 54; Iter    12/   36] train: loss: 0.4587047
[Epoch 54] ogbg-molsider: 0.639247 val loss: 0.488808
[Epoch 54] ogbg-molsider: 0.633674 test loss: 0.518954
[Epoch 55; Iter     6/   36] train: loss: 0.4182264
[Epoch 55; Iter    36/   36] train: loss: 0.4468229
[Epoch 55] ogbg-molsider: 0.664137 val loss: 0.512146
[Epoch 55] ogbg-molsider: 0.610991 test loss: 0.541355
[Epoch 56; Iter    30/   36] train: loss: 0.4454296
[Epoch 56] ogbg-molsider: 0.678731 val loss: 0.496363
[Epoch 56] ogbg-molsider: 0.629310 test loss: 0.516503
[Epoch 57; Iter    24/   36] train: loss: 0.4543129
[Epoch 57] ogbg-molsider: 0.661022 val loss: 0.552458
[Epoch 57] ogbg-molsider: 0.624052 test loss: 0.536201
[Epoch 58; Iter    18/   36] train: loss: 0.4022487
[Epoch 58] ogbg-molsider: 0.633019 val loss: 0.542705
[Epoch 58] ogbg-molsider: 0.618405 test loss: 0.522816
[Epoch 59; Iter    12/   36] train: loss: 0.3895195
[Epoch 59] ogbg-molsider: 0.673637 val loss: 0.534552
[Epoch 59] ogbg-molsider: 0.624282 test loss: 0.527362
[Epoch 60; Iter     6/   36] train: loss: 0.4861000
[Epoch 60; Iter    36/   36] train: loss: 0.3559945
[Epoch 60] ogbg-molsider: 0.675025 val loss: 0.508083
[Epoch 60] ogbg-molsider: 0.631245 test loss: 0.580474
[Epoch 61; Iter    30/   36] train: loss: 0.4424261
[Epoch 61] ogbg-molsider: 0.633422 val loss: 0.514662
[Epoch 61] ogbg-molsider: 0.619667 test loss: 0.530676
[Epoch 62; Iter    24/   36] train: loss: 0.4029421
[Epoch 62] ogbg-molsider: 0.658981 val loss: 0.525164
[Epoch 62] ogbg-molsider: 0.647073 test loss: 0.542764
[Epoch 63; Iter    18/   36] train: loss: 0.3935148
[Epoch 63] ogbg-molsider: 0.609877 val loss: 0.534119
[Epoch 63] ogbg-molsider: 0.619492 test loss: 0.566236
[Epoch 64; Iter    12/   36] train: loss: 0.3873996
[Epoch 64] ogbg-molsider: 0.623359 val loss: 0.524010
[Epoch 64] ogbg-molsider: 0.634729 test loss: 0.544927
[Epoch 65; Iter     6/   36] train: loss: 0.3808199
[Epoch 65; Iter    36/   36] train: loss: 0.4473511
[Epoch 65] ogbg-molsider: 0.625395 val loss: 0.525098
[Epoch 65] ogbg-molsider: 0.632264 test loss: 0.528446
[Epoch 66; Iter    30/   36] train: loss: 0.4144778
[Epoch 66] ogbg-molsider: 0.672357 val loss: 0.643993
[Epoch 66] ogbg-molsider: 0.625628 test loss: 0.531336
[Epoch 67; Iter    24/   36] train: loss: 0.4487090
[Epoch 67] ogbg-molsider: 0.629156 val loss: 0.543441
[Epoch 67] ogbg-molsider: 0.626500 test loss: 0.554139
[Epoch 68; Iter    18/   36] train: loss: 0.3467986
[Epoch 68] ogbg-molsider: 0.626624 val loss: 0.511822
[Epoch 68] ogbg-molsider: 0.653013 test loss: 0.513261
[Epoch 69; Iter    12/   36] train: loss: 0.3482459
[Epoch 69] ogbg-molsider: 0.633371 val loss: 0.526338
[Epoch 69] ogbg-molsider: 0.653275 test loss: 0.536960
[Epoch 70; Iter     6/   36] train: loss: 0.3535076
[Epoch 70; Iter    36/   36] train: loss: 0.4203882
[Epoch 70] ogbg-molsider: 0.632825 val loss: 0.543189
[Epoch 70] ogbg-molsider: 0.651653 test loss: 0.539646
[Epoch 71; Iter    30/   36] train: loss: 0.4031054
[Epoch 71] ogbg-molsider: 0.620190 val loss: 0.574623
[Epoch 71] ogbg-molsider: 0.630638 test loss: 0.577134
[Epoch 72; Iter    24/   36] train: loss: 0.3812190
[Epoch 72] ogbg-molsider: 0.629332 val loss: 0.531523
[Epoch 72] ogbg-molsider: 0.640721 test loss: 0.517834
[Epoch 73; Iter    18/   36] train: loss: 0.3495159
[Epoch 73] ogbg-molsider: 0.628826 val loss: 0.548145
[Epoch 73] ogbg-molsider: 0.647818 test loss: 0.559770
[Epoch 74; Iter    12/   36] train: loss: 0.3589782
[Epoch 74] ogbg-molsider: 0.643954 val loss: 0.535109
[Epoch 74] ogbg-molsider: 0.638333 test loss: 0.556028
[Epoch 75; Iter     6/   36] train: loss: 0.3354125
[Epoch 75; Iter    36/   36] train: loss: 0.3492768
[Epoch 75] ogbg-molsider: 0.654953 val loss: 0.555231
[Epoch 75] ogbg-molsider: 0.638479 test loss: 0.620256
[Epoch 76; Iter    30/   36] train: loss: 0.3908651
[Epoch 76] ogbg-molsider: 0.627882 val loss: 0.558192
[Epoch 76] ogbg-molsider: 0.659749 test loss: 0.549592
[Epoch 77; Iter    24/   36] train: loss: 0.3973894
[Epoch 77] ogbg-molsider: 0.628548 val loss: 0.569662
[Epoch 77] ogbg-molsider: 0.637305 test loss: 0.572813
[Epoch 78; Iter    18/   36] train: loss: 0.3489481
[Epoch 78] ogbg-molsider: 0.633590 val loss: 0.586087
[Epoch 78] ogbg-molsider: 0.639861 test loss: 0.582651
[Epoch 79; Iter    12/   36] train: loss: 0.2978974
[Epoch 79] ogbg-molsider: 0.644180 val loss: 0.559228
[Epoch 79] ogbg-molsider: 0.620881 test loss: 0.676095
[Epoch 80; Iter     6/   36] train: loss: 0.4425444
[Epoch 80; Iter    36/   36] train: loss: 0.4014482
[Epoch 80] ogbg-molsider: 0.655176 val loss: 0.560783
[Epoch 80] ogbg-molsider: 0.661483 test loss: 0.574548
[Epoch 33] ogbg-molsider: 0.612326 test loss: 0.512440
[Epoch 34; Iter    12/   36] train: loss: 0.5059058
[Epoch 34] ogbg-molsider: 0.605813 val loss: 0.494258
[Epoch 34] ogbg-molsider: 0.614019 test loss: 0.514335
[Epoch 35; Iter     6/   36] train: loss: 0.4483734
[Epoch 35; Iter    36/   36] train: loss: 0.5115312
[Epoch 35] ogbg-molsider: 0.646001 val loss: 0.484956
[Epoch 35] ogbg-molsider: 0.613119 test loss: 0.519576
[Epoch 36; Iter    30/   36] train: loss: 0.4732373
[Epoch 36] ogbg-molsider: 0.653799 val loss: 0.480203
[Epoch 36] ogbg-molsider: 0.607547 test loss: 0.532003
[Epoch 37; Iter    24/   36] train: loss: 0.4996353
[Epoch 37] ogbg-molsider: 0.642403 val loss: 0.484992
[Epoch 37] ogbg-molsider: 0.614221 test loss: 0.518737
[Epoch 38; Iter    18/   36] train: loss: 0.4831907
[Epoch 38] ogbg-molsider: 0.641372 val loss: 0.479184
[Epoch 38] ogbg-molsider: 0.620109 test loss: 0.512334
[Epoch 39; Iter    12/   36] train: loss: 0.5099349
[Epoch 39] ogbg-molsider: 0.645694 val loss: 0.488237
[Epoch 39] ogbg-molsider: 0.600050 test loss: 0.521000
[Epoch 40; Iter     6/   36] train: loss: 0.5038659
[Epoch 40; Iter    36/   36] train: loss: 0.4912188
[Epoch 40] ogbg-molsider: 0.609334 val loss: 0.543564
[Epoch 40] ogbg-molsider: 0.605079 test loss: 0.599161
[Epoch 41; Iter    30/   36] train: loss: 0.5104578
[Epoch 41] ogbg-molsider: 0.655866 val loss: 0.480744
[Epoch 41] ogbg-molsider: 0.601528 test loss: 0.536443
[Epoch 42; Iter    24/   36] train: loss: 0.4667104
[Epoch 42] ogbg-molsider: 0.609207 val loss: 0.491499
[Epoch 42] ogbg-molsider: 0.616771 test loss: 0.522628
[Epoch 43; Iter    18/   36] train: loss: 0.4808435
[Epoch 43] ogbg-molsider: 0.618126 val loss: 0.488429
[Epoch 43] ogbg-molsider: 0.601146 test loss: 0.513096
[Epoch 44; Iter    12/   36] train: loss: 0.4581687
[Epoch 44] ogbg-molsider: 0.637450 val loss: 0.493482
[Epoch 44] ogbg-molsider: 0.600758 test loss: 0.527974
[Epoch 45; Iter     6/   36] train: loss: 0.4358043
[Epoch 45; Iter    36/   36] train: loss: 0.4615510
[Epoch 45] ogbg-molsider: 0.602451 val loss: 0.498173
[Epoch 45] ogbg-molsider: 0.607267 test loss: 0.515840
[Epoch 46; Iter    30/   36] train: loss: 0.4699067
[Epoch 46] ogbg-molsider: 0.646613 val loss: 0.499184
[Epoch 46] ogbg-molsider: 0.606393 test loss: 0.556592
[Epoch 47; Iter    24/   36] train: loss: 0.4511603
[Epoch 47] ogbg-molsider: 0.619309 val loss: 0.489651
[Epoch 47] ogbg-molsider: 0.611388 test loss: 0.519411
[Epoch 48; Iter    18/   36] train: loss: 0.4598542
[Epoch 48] ogbg-molsider: 0.629266 val loss: 0.489674
[Epoch 48] ogbg-molsider: 0.610754 test loss: 0.540919
[Epoch 49; Iter    12/   36] train: loss: 0.5066889
[Epoch 49] ogbg-molsider: 0.633606 val loss: 0.518979
[Epoch 49] ogbg-molsider: 0.585380 test loss: 0.585063
[Epoch 50; Iter     6/   36] train: loss: 0.4664005
[Epoch 50; Iter    36/   36] train: loss: 0.3980019
[Epoch 50] ogbg-molsider: 0.659230 val loss: 0.682399
[Epoch 50] ogbg-molsider: 0.631345 test loss: 0.940714
[Epoch 51; Iter    30/   36] train: loss: 0.5084785
[Epoch 51] ogbg-molsider: 0.641207 val loss: 0.715385
[Epoch 51] ogbg-molsider: 0.582185 test loss: 1.320400
[Epoch 52; Iter    24/   36] train: loss: 0.4892152
[Epoch 52] ogbg-molsider: 0.548328 val loss: 1.047579
[Epoch 52] ogbg-molsider: 0.560030 test loss: 1.200645
[Epoch 53; Iter    18/   36] train: loss: 0.5235978
[Epoch 53] ogbg-molsider: 0.617217 val loss: 0.503243
[Epoch 53] ogbg-molsider: 0.634605 test loss: 0.584762
[Epoch 54; Iter    12/   36] train: loss: 0.4587827
[Epoch 54] ogbg-molsider: 0.644455 val loss: 0.497190
[Epoch 54] ogbg-molsider: 0.598916 test loss: 0.554970
[Epoch 55; Iter     6/   36] train: loss: 0.4090301
[Epoch 55; Iter    36/   36] train: loss: 0.4473901
[Epoch 55] ogbg-molsider: 0.659367 val loss: 0.479575
[Epoch 55] ogbg-molsider: 0.614773 test loss: 0.508585
[Epoch 56; Iter    30/   36] train: loss: 0.4198159
[Epoch 56] ogbg-molsider: 0.646310 val loss: 0.489463
[Epoch 56] ogbg-molsider: 0.635421 test loss: 0.512782
[Epoch 57; Iter    24/   36] train: loss: 0.4829416
[Epoch 57] ogbg-molsider: 0.670397 val loss: 0.475559
[Epoch 57] ogbg-molsider: 0.640446 test loss: 0.522705
[Epoch 58; Iter    18/   36] train: loss: 0.4512120
[Epoch 58] ogbg-molsider: 0.653952 val loss: 0.487125
[Epoch 58] ogbg-molsider: 0.637409 test loss: 0.518200
[Epoch 59; Iter    12/   36] train: loss: 0.4147102
[Epoch 59] ogbg-molsider: 0.669073 val loss: 0.493092
[Epoch 59] ogbg-molsider: 0.620764 test loss: 0.523312
[Epoch 60; Iter     6/   36] train: loss: 0.4488185
[Epoch 60; Iter    36/   36] train: loss: 0.4616200
[Epoch 60] ogbg-molsider: 0.696381 val loss: 0.471306
[Epoch 60] ogbg-molsider: 0.630290 test loss: 0.511137
[Epoch 61; Iter    30/   36] train: loss: 0.3930458
[Epoch 61] ogbg-molsider: 0.668263 val loss: 0.478113
[Epoch 61] ogbg-molsider: 0.634725 test loss: 0.516853
[Epoch 62; Iter    24/   36] train: loss: 0.4408666
[Epoch 62] ogbg-molsider: 0.649350 val loss: 0.498381
[Epoch 62] ogbg-molsider: 0.662351 test loss: 0.529493
[Epoch 63; Iter    18/   36] train: loss: 0.4661267
[Epoch 63] ogbg-molsider: 0.645393 val loss: 0.507411
[Epoch 63] ogbg-molsider: 0.627510 test loss: 0.535021
[Epoch 64; Iter    12/   36] train: loss: 0.3554651
[Epoch 64] ogbg-molsider: 0.645811 val loss: 0.844570
[Epoch 64] ogbg-molsider: 0.629039 test loss: 0.561397
[Epoch 65; Iter     6/   36] train: loss: 0.3781638
[Epoch 65; Iter    36/   36] train: loss: 0.3975351
[Epoch 65] ogbg-molsider: 0.656010 val loss: 0.503174
[Epoch 65] ogbg-molsider: 0.655226 test loss: 0.522831
[Epoch 66; Iter    30/   36] train: loss: 0.4345272
[Epoch 66] ogbg-molsider: 0.630986 val loss: 0.607119
[Epoch 66] ogbg-molsider: 0.618309 test loss: 0.565860
[Epoch 67; Iter    24/   36] train: loss: 0.4729624
[Epoch 67] ogbg-molsider: 0.669248 val loss: 0.480611
[Epoch 67] ogbg-molsider: 0.641052 test loss: 0.539415
[Epoch 68; Iter    18/   36] train: loss: 0.4315552
[Epoch 68] ogbg-molsider: 0.647256 val loss: 0.495523
[Epoch 68] ogbg-molsider: 0.620222 test loss: 0.544233
[Epoch 69; Iter    12/   36] train: loss: 0.4191534
[Epoch 69] ogbg-molsider: 0.648171 val loss: 0.516896
[Epoch 69] ogbg-molsider: 0.639875 test loss: 0.528408
[Epoch 70; Iter     6/   36] train: loss: 0.4060640
[Epoch 70; Iter    36/   36] train: loss: 0.4406623
[Epoch 70] ogbg-molsider: 0.663575 val loss: 0.495839
[Epoch 70] ogbg-molsider: 0.643641 test loss: 0.534676
[Epoch 71; Iter    30/   36] train: loss: 0.4002656
[Epoch 71] ogbg-molsider: 0.658521 val loss: 0.493969
[Epoch 71] ogbg-molsider: 0.641186 test loss: 0.537448
[Epoch 72; Iter    24/   36] train: loss: 0.3925006
[Epoch 72] ogbg-molsider: 0.627565 val loss: 0.527499
[Epoch 72] ogbg-molsider: 0.664776 test loss: 0.535460
[Epoch 73; Iter    18/   36] train: loss: 0.3972532
[Epoch 73] ogbg-molsider: 0.640772 val loss: 0.509427
[Epoch 73] ogbg-molsider: 0.647015 test loss: 0.547974
[Epoch 74; Iter    12/   36] train: loss: 0.4336967
[Epoch 74] ogbg-molsider: 0.637080 val loss: 0.482938
[Epoch 74] ogbg-molsider: 0.659158 test loss: 0.521603
[Epoch 75; Iter     6/   36] train: loss: 0.3825957
[Epoch 75; Iter    36/   36] train: loss: 0.3772761
[Epoch 75] ogbg-molsider: 0.644901 val loss: 0.511663
[Epoch 75] ogbg-molsider: 0.664283 test loss: 0.533553
[Epoch 76; Iter    30/   36] train: loss: 0.3600471
[Epoch 76] ogbg-molsider: 0.669981 val loss: 0.499618
[Epoch 76] ogbg-molsider: 0.656147 test loss: 0.542641
[Epoch 77; Iter    24/   36] train: loss: 0.3785976
[Epoch 77] ogbg-molsider: 0.644877 val loss: 0.505255
[Epoch 77] ogbg-molsider: 0.661571 test loss: 0.542825
[Epoch 78; Iter    18/   36] train: loss: 0.4187065
[Epoch 78] ogbg-molsider: 0.647058 val loss: 0.523085
[Epoch 78] ogbg-molsider: 0.657920 test loss: 0.545495
[Epoch 79; Iter    12/   36] train: loss: 0.3234460
[Epoch 79] ogbg-molsider: 0.650815 val loss: 0.519401
[Epoch 79] ogbg-molsider: 0.665797 test loss: 0.531663
[Epoch 80; Iter     6/   36] train: loss: 0.3421420
[Epoch 80; Iter    36/   36] train: loss: 0.4061784
[Epoch 80] ogbg-molsider: 0.656568 val loss: 0.523273
[Epoch 80] ogbg-molsider: 0.669115 test loss: 0.538707
[Epoch 35; Iter    22/   32] train: loss: 0.4974774
[Epoch 35] ogbg-molsider: 0.591199 val loss: 0.505527
[Epoch 35] ogbg-molsider: 0.618315 test loss: 0.504061
[Epoch 36; Iter    20/   32] train: loss: 0.5119609
[Epoch 36] ogbg-molsider: 0.596209 val loss: 0.504534
[Epoch 36] ogbg-molsider: 0.625431 test loss: 0.506219
[Epoch 37; Iter    18/   32] train: loss: 0.5184250
[Epoch 37] ogbg-molsider: 0.595494 val loss: 0.509770
[Epoch 37] ogbg-molsider: 0.612039 test loss: 0.503840
[Epoch 38; Iter    16/   32] train: loss: 0.4771243
[Epoch 38] ogbg-molsider: 0.593085 val loss: 0.509386
[Epoch 38] ogbg-molsider: 0.619104 test loss: 0.502780
[Epoch 39; Iter    14/   32] train: loss: 0.4508857
[Epoch 39] ogbg-molsider: 0.604380 val loss: 0.506769
[Epoch 39] ogbg-molsider: 0.604977 test loss: 0.525937
[Epoch 40; Iter    12/   32] train: loss: 0.5126397
[Epoch 40] ogbg-molsider: 0.598622 val loss: 0.511725
[Epoch 40] ogbg-molsider: 0.618637 test loss: 0.502076
[Epoch 41; Iter    10/   32] train: loss: 0.4819797
[Epoch 41] ogbg-molsider: 0.604967 val loss: 0.528135
[Epoch 41] ogbg-molsider: 0.610016 test loss: 0.525220
[Epoch 42; Iter     8/   32] train: loss: 0.4462112
[Epoch 42] ogbg-molsider: 0.588017 val loss: 0.516517
[Epoch 42] ogbg-molsider: 0.601071 test loss: 0.510696
[Epoch 43; Iter     6/   32] train: loss: 0.4883635
[Epoch 43] ogbg-molsider: 0.600245 val loss: 0.519685
[Epoch 43] ogbg-molsider: 0.640505 test loss: 0.502351
[Epoch 44; Iter     4/   32] train: loss: 0.4717095
[Epoch 44] ogbg-molsider: 0.607276 val loss: 0.514498
[Epoch 44] ogbg-molsider: 0.627573 test loss: 0.505730
[Epoch 45; Iter     2/   32] train: loss: 0.4596647
[Epoch 45; Iter    32/   32] train: loss: 0.5735686
[Epoch 45] ogbg-molsider: 0.604013 val loss: 0.509778
[Epoch 45] ogbg-molsider: 0.632023 test loss: 0.504976
[Epoch 46; Iter    30/   32] train: loss: 0.4975298
[Epoch 46] ogbg-molsider: 0.638516 val loss: 0.495319
[Epoch 46] ogbg-molsider: 0.626811 test loss: 0.517569
[Epoch 47; Iter    28/   32] train: loss: 0.4627537
[Epoch 47] ogbg-molsider: 0.598111 val loss: 0.516271
[Epoch 47] ogbg-molsider: 0.615986 test loss: 0.509654
[Epoch 48; Iter    26/   32] train: loss: 0.4823010
[Epoch 48] ogbg-molsider: 0.602168 val loss: 0.512613
[Epoch 48] ogbg-molsider: 0.613353 test loss: 0.511750
[Epoch 49; Iter    24/   32] train: loss: 0.5217253
[Epoch 49] ogbg-molsider: 0.627589 val loss: 0.521374
[Epoch 49] ogbg-molsider: 0.608855 test loss: 0.573379
[Epoch 50; Iter    22/   32] train: loss: 0.4790685
[Epoch 50] ogbg-molsider: 0.549540 val loss: 0.877210
[Epoch 50] ogbg-molsider: 0.602042 test loss: 0.574917
[Epoch 51; Iter    20/   32] train: loss: 0.5164321
[Epoch 51] ogbg-molsider: 0.612533 val loss: 0.519694
[Epoch 51] ogbg-molsider: 0.597761 test loss: 0.548461
[Epoch 52; Iter    18/   32] train: loss: 0.4220516
[Epoch 52] ogbg-molsider: 0.610352 val loss: 1.755103
[Epoch 52] ogbg-molsider: 0.546879 test loss: 0.673305
[Epoch 53; Iter    16/   32] train: loss: 0.4948030
[Epoch 53] ogbg-molsider: 0.589450 val loss: 0.521771
[Epoch 53] ogbg-molsider: 0.569396 test loss: 0.547101
[Epoch 54; Iter    14/   32] train: loss: 0.4925175
[Epoch 54] ogbg-molsider: 0.593740 val loss: 0.512632
[Epoch 54] ogbg-molsider: 0.616368 test loss: 0.506558
[Epoch 55; Iter    12/   32] train: loss: 0.5222598
[Epoch 55] ogbg-molsider: 0.616191 val loss: 0.497256
[Epoch 55] ogbg-molsider: 0.597243 test loss: 0.514068
[Epoch 56; Iter    10/   32] train: loss: 0.4825469
[Epoch 56] ogbg-molsider: 0.605424 val loss: 0.508270
[Epoch 56] ogbg-molsider: 0.605829 test loss: 0.528603
[Epoch 57; Iter     8/   32] train: loss: 0.5079212
[Epoch 57] ogbg-molsider: 0.599158 val loss: 0.501074
[Epoch 57] ogbg-molsider: 0.591422 test loss: 0.641579
[Epoch 58; Iter     6/   32] train: loss: 0.4637038
[Epoch 58] ogbg-molsider: 0.592374 val loss: 0.503750
[Epoch 58] ogbg-molsider: 0.587213 test loss: 0.517253
[Epoch 59; Iter     4/   32] train: loss: 0.5144126
[Epoch 59] ogbg-molsider: 0.596319 val loss: 0.507598
[Epoch 59] ogbg-molsider: 0.615335 test loss: 0.507865
[Epoch 60; Iter     2/   32] train: loss: 0.5232089
[Epoch 60; Iter    32/   32] train: loss: 0.4865012
[Epoch 60] ogbg-molsider: 0.618844 val loss: 0.504924
[Epoch 60] ogbg-molsider: 0.594956 test loss: 0.513034
[Epoch 61; Iter    30/   32] train: loss: 0.5380490
[Epoch 61] ogbg-molsider: 0.603756 val loss: 0.501908
[Epoch 61] ogbg-molsider: 0.578570 test loss: 0.523328
[Epoch 62; Iter    28/   32] train: loss: 0.5169371
[Epoch 62] ogbg-molsider: 0.629911 val loss: 0.505779
[Epoch 62] ogbg-molsider: 0.601341 test loss: 0.521396
[Epoch 63; Iter    26/   32] train: loss: 0.4999915
[Epoch 63] ogbg-molsider: 0.617687 val loss: 0.499837
[Epoch 63] ogbg-molsider: 0.593216 test loss: 0.509048
[Epoch 64; Iter    24/   32] train: loss: 0.4524794
[Epoch 64] ogbg-molsider: 0.634888 val loss: 0.498727
[Epoch 64] ogbg-molsider: 0.590785 test loss: 0.518422
[Epoch 65; Iter    22/   32] train: loss: 0.5039136
[Epoch 65] ogbg-molsider: 0.636624 val loss: 0.496983
[Epoch 65] ogbg-molsider: 0.618665 test loss: 0.503819
[Epoch 66; Iter    20/   32] train: loss: 0.5039755
[Epoch 66] ogbg-molsider: 0.633730 val loss: 0.501192
[Epoch 66] ogbg-molsider: 0.614704 test loss: 0.510105
[Epoch 67; Iter    18/   32] train: loss: 0.4677095
[Epoch 67] ogbg-molsider: 0.629381 val loss: 0.512907
[Epoch 67] ogbg-molsider: 0.610460 test loss: 0.523780
[Epoch 68; Iter    16/   32] train: loss: 0.4599676
[Epoch 68] ogbg-molsider: 0.625446 val loss: 0.498933
[Epoch 68] ogbg-molsider: 0.575144 test loss: 0.527426
[Epoch 69; Iter    14/   32] train: loss: 0.5312080
[Epoch 69] ogbg-molsider: 0.640711 val loss: 0.495501
[Epoch 69] ogbg-molsider: 0.605947 test loss: 0.516009
[Epoch 70; Iter    12/   32] train: loss: 0.4983169
[Epoch 70] ogbg-molsider: 0.644269 val loss: 0.496754
[Epoch 70] ogbg-molsider: 0.619405 test loss: 0.508309
[Epoch 71; Iter    10/   32] train: loss: 0.4816721
[Epoch 71] ogbg-molsider: 0.626788 val loss: 0.497536
[Epoch 71] ogbg-molsider: 0.612446 test loss: 0.507274
[Epoch 72; Iter     8/   32] train: loss: 0.4656988
[Epoch 72] ogbg-molsider: 0.624206 val loss: 0.497989
[Epoch 72] ogbg-molsider: 0.622639 test loss: 0.502008
[Epoch 73; Iter     6/   32] train: loss: 0.4828911
[Epoch 73] ogbg-molsider: 0.612677 val loss: 0.508905
[Epoch 73] ogbg-molsider: 0.610157 test loss: 0.514991
[Epoch 74; Iter     4/   32] train: loss: 0.4706348
[Epoch 74] ogbg-molsider: 0.625726 val loss: 0.526026
[Epoch 74] ogbg-molsider: 0.620959 test loss: 0.539369
[Epoch 75; Iter     2/   32] train: loss: 0.5066808
[Epoch 75; Iter    32/   32] train: loss: 0.4314106
[Epoch 75] ogbg-molsider: 0.632407 val loss: 0.497796
[Epoch 75] ogbg-molsider: 0.613589 test loss: 0.506318
[Epoch 76; Iter    30/   32] train: loss: 0.4628974
[Epoch 76] ogbg-molsider: 0.633690 val loss: 0.508113
[Epoch 76] ogbg-molsider: 0.617785 test loss: 0.510384
[Epoch 77; Iter    28/   32] train: loss: 0.4860252
[Epoch 77] ogbg-molsider: 0.625934 val loss: 0.514986
[Epoch 77] ogbg-molsider: 0.629778 test loss: 0.506785
[Epoch 78; Iter    26/   32] train: loss: 0.4755938
[Epoch 78] ogbg-molsider: 0.635461 val loss: 0.499120
[Epoch 78] ogbg-molsider: 0.611066 test loss: 0.508251
[Epoch 79; Iter    24/   32] train: loss: 0.4437779
[Epoch 79] ogbg-molsider: 0.642154 val loss: 0.502639
[Epoch 79] ogbg-molsider: 0.606843 test loss: 0.514310
[Epoch 80; Iter    22/   32] train: loss: 0.4725576
[Epoch 80] ogbg-molsider: 0.631714 val loss: 0.498588
[Epoch 80] ogbg-molsider: 0.606565 test loss: 0.514333
[Epoch 81; Iter    20/   32] train: loss: 0.5223454
[Epoch 81] ogbg-molsider: 0.645753 val loss: 0.509440
[Epoch 81] ogbg-molsider: 0.614574 test loss: 0.510673
[Epoch 82; Iter    18/   32] train: loss: 0.4682995
[Epoch 82] ogbg-molsider: 0.628229 val loss: 0.508579
[Epoch 82] ogbg-molsider: 0.631251 test loss: 0.500259
[Epoch 83; Iter    16/   32] train: loss: 0.4700145
[Epoch 83] ogbg-molsider: 0.594083 val loss: 0.550041
[Epoch 83] ogbg-molsider: 0.583940 test loss: 0.531694
[Epoch 84; Iter    14/   32] train: loss: 0.4990090
[Epoch 84] ogbg-molsider: 0.608969 val loss: 0.518270
[Epoch 37; Iter    18/   27] train: loss: 0.4977265
[Epoch 37] ogbg-molsider: 0.587138 val loss: 0.530704
[Epoch 37] ogbg-molsider: 0.608968 test loss: 0.513780
[Epoch 38; Iter    21/   27] train: loss: 0.5222170
[Epoch 38] ogbg-molsider: 0.582395 val loss: 0.527374
[Epoch 38] ogbg-molsider: 0.608568 test loss: 0.512523
[Epoch 39; Iter    24/   27] train: loss: 0.5200869
[Epoch 39] ogbg-molsider: 0.616843 val loss: 0.523400
[Epoch 39] ogbg-molsider: 0.599050 test loss: 0.509993
[Epoch 40; Iter    27/   27] train: loss: 0.4938766
[Epoch 40] ogbg-molsider: 0.599466 val loss: 0.519656
[Epoch 40] ogbg-molsider: 0.601253 test loss: 0.506872
[Epoch 41] ogbg-molsider: 0.604266 val loss: 0.523601
[Epoch 41] ogbg-molsider: 0.614518 test loss: 0.505240
[Epoch 42; Iter     3/   27] train: loss: 0.4603112
[Epoch 42] ogbg-molsider: 0.613327 val loss: 0.520694
[Epoch 42] ogbg-molsider: 0.608660 test loss: 0.504413
[Epoch 43; Iter     6/   27] train: loss: 0.4760039
[Epoch 43] ogbg-molsider: 0.601171 val loss: 0.519097
[Epoch 43] ogbg-molsider: 0.622365 test loss: 0.500383
[Epoch 44; Iter     9/   27] train: loss: 0.4613100
[Epoch 44] ogbg-molsider: 0.597463 val loss: 0.522735
[Epoch 44] ogbg-molsider: 0.610012 test loss: 0.505259
[Epoch 45; Iter    12/   27] train: loss: 0.4409917
[Epoch 45] ogbg-molsider: 0.616230 val loss: 0.517518
[Epoch 45] ogbg-molsider: 0.617309 test loss: 0.503047
[Epoch 46; Iter    15/   27] train: loss: 0.4579645
[Epoch 46] ogbg-molsider: 0.616392 val loss: 0.519319
[Epoch 46] ogbg-molsider: 0.604208 test loss: 0.505916
[Epoch 47; Iter    18/   27] train: loss: 0.4939239
[Epoch 47] ogbg-molsider: 0.595982 val loss: 0.519488
[Epoch 47] ogbg-molsider: 0.611588 test loss: 0.508565
[Epoch 48; Iter    21/   27] train: loss: 0.4356109
[Epoch 48] ogbg-molsider: 0.619770 val loss: 0.512258
[Epoch 48] ogbg-molsider: 0.617877 test loss: 0.503523
[Epoch 49; Iter    24/   27] train: loss: 0.4384708
[Epoch 49] ogbg-molsider: 0.606131 val loss: 0.521228
[Epoch 49] ogbg-molsider: 0.622722 test loss: 0.506625
[Epoch 50; Iter    27/   27] train: loss: 0.4754198
[Epoch 50] ogbg-molsider: 0.601091 val loss: 0.527537
[Epoch 50] ogbg-molsider: 0.615562 test loss: 0.507939
[Epoch 51] ogbg-molsider: 0.599808 val loss: 0.524140
[Epoch 51] ogbg-molsider: 0.620881 test loss: 0.507689
[Epoch 52; Iter     3/   27] train: loss: 0.4470618
[Epoch 52] ogbg-molsider: 0.598405 val loss: 0.540438
[Epoch 52] ogbg-molsider: 0.617679 test loss: 0.514155
[Epoch 53; Iter     6/   27] train: loss: 0.4313052
[Epoch 53] ogbg-molsider: 0.608731 val loss: 0.538688
[Epoch 53] ogbg-molsider: 0.619300 test loss: 0.518056
[Epoch 54; Iter     9/   27] train: loss: 0.4644528
[Epoch 54] ogbg-molsider: 0.617638 val loss: 0.520077
[Epoch 54] ogbg-molsider: 0.646892 test loss: 0.493969
[Epoch 55; Iter    12/   27] train: loss: 0.5164766
[Epoch 55] ogbg-molsider: 0.637480 val loss: 0.577840
[Epoch 55] ogbg-molsider: 0.634772 test loss: 0.508821
[Epoch 56; Iter    15/   27] train: loss: 0.4707547
[Epoch 56] ogbg-molsider: 0.621663 val loss: 0.608013
[Epoch 56] ogbg-molsider: 0.637985 test loss: 0.496723
[Epoch 57; Iter    18/   27] train: loss: 0.4540826
[Epoch 57] ogbg-molsider: 0.612377 val loss: 0.541370
[Epoch 57] ogbg-molsider: 0.598374 test loss: 0.521960
[Epoch 58; Iter    21/   27] train: loss: 0.4069918
[Epoch 58] ogbg-molsider: 0.554552 val loss: 2.844348
[Epoch 58] ogbg-molsider: 0.579926 test loss: 2.349123
[Epoch 59; Iter    24/   27] train: loss: 0.5168968
[Epoch 59] ogbg-molsider: 0.579751 val loss: 0.540468
[Epoch 59] ogbg-molsider: 0.586294 test loss: 0.534911
[Epoch 60; Iter    27/   27] train: loss: 0.5485994
[Epoch 60] ogbg-molsider: 0.598797 val loss: 0.524823
[Epoch 60] ogbg-molsider: 0.619743 test loss: 0.499937
[Epoch 61] ogbg-molsider: 0.612985 val loss: 0.534126
[Epoch 61] ogbg-molsider: 0.637412 test loss: 0.502871
[Epoch 62; Iter     3/   27] train: loss: 0.4473899
[Epoch 62] ogbg-molsider: 0.621432 val loss: 0.521385
[Epoch 62] ogbg-molsider: 0.610487 test loss: 0.508426
[Epoch 63; Iter     6/   27] train: loss: 0.4179654
[Epoch 63] ogbg-molsider: 0.627495 val loss: 0.521449
[Epoch 63] ogbg-molsider: 0.600715 test loss: 0.514980
[Epoch 64; Iter     9/   27] train: loss: 0.4314977
[Epoch 64] ogbg-molsider: 0.622771 val loss: 0.529020
[Epoch 64] ogbg-molsider: 0.626289 test loss: 0.510368
[Epoch 65; Iter    12/   27] train: loss: 0.4819207
[Epoch 65] ogbg-molsider: 0.633297 val loss: 0.530241
[Epoch 65] ogbg-molsider: 0.633361 test loss: 0.517447
[Epoch 66; Iter    15/   27] train: loss: 0.4534919
[Epoch 66] ogbg-molsider: 0.590174 val loss: 0.557577
[Epoch 66] ogbg-molsider: 0.578805 test loss: 0.585455
[Epoch 67; Iter    18/   27] train: loss: 0.5197670
[Epoch 67] ogbg-molsider: 0.618561 val loss: 0.527143
[Epoch 67] ogbg-molsider: 0.611627 test loss: 0.535211
[Epoch 68; Iter    21/   27] train: loss: 0.4706809
[Epoch 68] ogbg-molsider: 0.626846 val loss: 0.521903
[Epoch 68] ogbg-molsider: 0.630927 test loss: 0.503532
[Epoch 69; Iter    24/   27] train: loss: 0.4850180
[Epoch 69] ogbg-molsider: 0.640835 val loss: 0.516523
[Epoch 69] ogbg-molsider: 0.609954 test loss: 0.516409
[Epoch 70; Iter    27/   27] train: loss: 0.4279263
[Epoch 70] ogbg-molsider: 0.657191 val loss: 0.503521
[Epoch 70] ogbg-molsider: 0.624474 test loss: 0.503910
[Epoch 71] ogbg-molsider: 0.646284 val loss: 0.539615
[Epoch 71] ogbg-molsider: 0.628925 test loss: 0.819876
[Epoch 72; Iter     3/   27] train: loss: 0.4701303
[Epoch 72] ogbg-molsider: 0.620738 val loss: 0.519214
[Epoch 72] ogbg-molsider: 0.615519 test loss: 0.517299
[Epoch 73; Iter     6/   27] train: loss: 0.4785944
[Epoch 73] ogbg-molsider: 0.633957 val loss: 0.519575
[Epoch 73] ogbg-molsider: 0.622898 test loss: 0.533203
[Epoch 74; Iter     9/   27] train: loss: 0.4764771
[Epoch 74] ogbg-molsider: 0.644630 val loss: 0.515277
[Epoch 74] ogbg-molsider: 0.619433 test loss: 0.518826
[Epoch 75; Iter    12/   27] train: loss: 0.4140084
[Epoch 75] ogbg-molsider: 0.638984 val loss: 0.525073
[Epoch 75] ogbg-molsider: 0.619740 test loss: 0.514936
[Epoch 76; Iter    15/   27] train: loss: 0.4120193
[Epoch 76] ogbg-molsider: 0.653445 val loss: 0.513989
[Epoch 76] ogbg-molsider: 0.628873 test loss: 0.528734
[Epoch 77; Iter    18/   27] train: loss: 0.4562576
[Epoch 77] ogbg-molsider: 0.601950 val loss: 0.536554
[Epoch 77] ogbg-molsider: 0.591514 test loss: 0.537371
[Epoch 78; Iter    21/   27] train: loss: 0.4498673
[Epoch 78] ogbg-molsider: 0.653030 val loss: 0.536251
[Epoch 78] ogbg-molsider: 0.613980 test loss: 0.539839
[Epoch 79; Iter    24/   27] train: loss: 0.4007751
[Epoch 79] ogbg-molsider: 0.643392 val loss: 0.538786
[Epoch 79] ogbg-molsider: 0.624792 test loss: 0.542183
[Epoch 80; Iter    27/   27] train: loss: 0.4324392
[Epoch 80] ogbg-molsider: 0.630439 val loss: 0.535137
[Epoch 80] ogbg-molsider: 0.616478 test loss: 0.541591
[Epoch 81] ogbg-molsider: 0.616465 val loss: 0.540812
[Epoch 81] ogbg-molsider: 0.613301 test loss: 0.535671
[Epoch 82; Iter     3/   27] train: loss: 0.3757438
[Epoch 82] ogbg-molsider: 0.620306 val loss: 0.537910
[Epoch 82] ogbg-molsider: 0.618440 test loss: 0.523931
[Epoch 83; Iter     6/   27] train: loss: 0.3946126
[Epoch 83] ogbg-molsider: 0.625428 val loss: 0.546002
[Epoch 83] ogbg-molsider: 0.619878 test loss: 0.543602
[Epoch 84; Iter     9/   27] train: loss: 0.3420765
[Epoch 84] ogbg-molsider: 0.625472 val loss: 0.536642
[Epoch 84] ogbg-molsider: 0.601259 test loss: 0.550734
[Epoch 85; Iter    12/   27] train: loss: 0.3930626
[Epoch 85] ogbg-molsider: 0.622552 val loss: 0.551509
[Epoch 85] ogbg-molsider: 0.625084 test loss: 0.551586
[Epoch 86; Iter    15/   27] train: loss: 0.4056381
[Epoch 86] ogbg-molsider: 0.623534 val loss: 0.547354
[Epoch 86] ogbg-molsider: 0.627385 test loss: 0.542954
[Epoch 87; Iter    18/   27] train: loss: 0.3759013
[Epoch 87] ogbg-molsider: 0.633228 val loss: 0.539775
[Epoch 87] ogbg-molsider: 0.624423 test loss: 0.542157
[Epoch 88; Iter    21/   27] train: loss: 0.3789224
[Epoch 88] ogbg-molsider: 0.626485 val loss: 0.549734
[Epoch 88] ogbg-molsider: 0.624620 test loss: 0.545058
[Epoch 89; Iter    24/   27] train: loss: 0.4169349
[Epoch 35; Iter    22/   32] train: loss: 0.5555909
[Epoch 35] ogbg-molsider: 0.594250 val loss: 0.508928
[Epoch 35] ogbg-molsider: 0.621428 test loss: 0.535584
[Epoch 36; Iter    20/   32] train: loss: 0.4871325
[Epoch 36] ogbg-molsider: 0.588803 val loss: 0.508520
[Epoch 36] ogbg-molsider: 0.618842 test loss: 0.545509
[Epoch 37; Iter    18/   32] train: loss: 0.5082396
[Epoch 37] ogbg-molsider: 0.605326 val loss: 0.509297
[Epoch 37] ogbg-molsider: 0.618960 test loss: 0.535649
[Epoch 38; Iter    16/   32] train: loss: 0.4471563
[Epoch 38] ogbg-molsider: 0.614826 val loss: 0.504095
[Epoch 38] ogbg-molsider: 0.634270 test loss: 0.545535
[Epoch 39; Iter    14/   32] train: loss: 0.5160483
[Epoch 39] ogbg-molsider: 0.607067 val loss: 0.521407
[Epoch 39] ogbg-molsider: 0.635440 test loss: 0.497114
[Epoch 40; Iter    12/   32] train: loss: 0.5053201
[Epoch 40] ogbg-molsider: 0.591656 val loss: 0.508677
[Epoch 40] ogbg-molsider: 0.606095 test loss: 0.516610
[Epoch 41; Iter    10/   32] train: loss: 0.5214056
[Epoch 41] ogbg-molsider: 0.601201 val loss: 0.527628
[Epoch 41] ogbg-molsider: 0.623098 test loss: 0.503830
[Epoch 42; Iter     8/   32] train: loss: 0.5467352
[Epoch 42] ogbg-molsider: 0.608483 val loss: 0.512523
[Epoch 42] ogbg-molsider: 0.628669 test loss: 0.501236
[Epoch 43; Iter     6/   32] train: loss: 0.4767807
[Epoch 43] ogbg-molsider: 0.611638 val loss: 0.502069
[Epoch 43] ogbg-molsider: 0.632539 test loss: 0.498388
[Epoch 44; Iter     4/   32] train: loss: 0.4730189
[Epoch 44] ogbg-molsider: 0.603168 val loss: 0.506021
[Epoch 44] ogbg-molsider: 0.631976 test loss: 0.500521
[Epoch 45; Iter     2/   32] train: loss: 0.4995356
[Epoch 45; Iter    32/   32] train: loss: 0.4548598
[Epoch 45] ogbg-molsider: 0.607980 val loss: 0.500333
[Epoch 45] ogbg-molsider: 0.610867 test loss: 0.505391
[Epoch 46; Iter    30/   32] train: loss: 0.5016926
[Epoch 46] ogbg-molsider: 0.620255 val loss: 0.511580
[Epoch 46] ogbg-molsider: 0.630219 test loss: 0.505257
[Epoch 47; Iter    28/   32] train: loss: 0.5218409
[Epoch 47] ogbg-molsider: 0.607243 val loss: 0.720559
[Epoch 47] ogbg-molsider: 0.618540 test loss: 0.609542
[Epoch 48; Iter    26/   32] train: loss: 0.4571186
[Epoch 48] ogbg-molsider: 0.614857 val loss: 0.520618
[Epoch 48] ogbg-molsider: 0.603572 test loss: 0.524421
[Epoch 49; Iter    24/   32] train: loss: 0.4697483
[Epoch 49] ogbg-molsider: 0.588489 val loss: 0.789587
[Epoch 49] ogbg-molsider: 0.580142 test loss: 0.885701
[Epoch 50; Iter    22/   32] train: loss: 0.4936069
[Epoch 50] ogbg-molsider: 0.622629 val loss: 0.512774
[Epoch 50] ogbg-molsider: 0.621805 test loss: 0.510270
[Epoch 51; Iter    20/   32] train: loss: 0.4886677
[Epoch 51] ogbg-molsider: 0.628880 val loss: 0.524092
[Epoch 51] ogbg-molsider: 0.635982 test loss: 0.538460
[Epoch 52; Iter    18/   32] train: loss: 0.4456391
[Epoch 52] ogbg-molsider: 0.614043 val loss: 0.992349
[Epoch 52] ogbg-molsider: 0.640171 test loss: 0.497747
[Epoch 53; Iter    16/   32] train: loss: 0.4452429
[Epoch 53] ogbg-molsider: 0.618322 val loss: 0.665050
[Epoch 53] ogbg-molsider: 0.622826 test loss: 0.562407
[Epoch 54; Iter    14/   32] train: loss: 0.4967551
[Epoch 54] ogbg-molsider: 0.614303 val loss: 0.521978
[Epoch 54] ogbg-molsider: 0.634869 test loss: 0.517799
[Epoch 55; Iter    12/   32] train: loss: 0.5001332
[Epoch 55] ogbg-molsider: 0.635829 val loss: 0.510389
[Epoch 55] ogbg-molsider: 0.616679 test loss: 0.509601
[Epoch 56; Iter    10/   32] train: loss: 0.4610496
[Epoch 56] ogbg-molsider: 0.629331 val loss: 0.521278
[Epoch 56] ogbg-molsider: 0.639733 test loss: 0.498489
[Epoch 57; Iter     8/   32] train: loss: 0.4479347
[Epoch 57] ogbg-molsider: 0.630617 val loss: 0.517622
[Epoch 57] ogbg-molsider: 0.619949 test loss: 0.516409
[Epoch 58; Iter     6/   32] train: loss: 0.4635713
[Epoch 58] ogbg-molsider: 0.643052 val loss: 0.699234
[Epoch 58] ogbg-molsider: 0.633744 test loss: 0.504090
[Epoch 59; Iter     4/   32] train: loss: 0.4486398
[Epoch 59] ogbg-molsider: 0.643179 val loss: 0.577505
[Epoch 59] ogbg-molsider: 0.635321 test loss: 0.542124
[Epoch 60; Iter     2/   32] train: loss: 0.4149805
[Epoch 60; Iter    32/   32] train: loss: 0.4740741
[Epoch 60] ogbg-molsider: 0.628560 val loss: 0.555612
[Epoch 60] ogbg-molsider: 0.640333 test loss: 0.531090
[Epoch 61; Iter    30/   32] train: loss: 0.4407048
[Epoch 61] ogbg-molsider: 0.632160 val loss: 0.570438
[Epoch 61] ogbg-molsider: 0.618397 test loss: 0.527668
[Epoch 62; Iter    28/   32] train: loss: 0.4748239
[Epoch 62] ogbg-molsider: 0.630296 val loss: 0.602236
[Epoch 62] ogbg-molsider: 0.634102 test loss: 0.510703
[Epoch 63; Iter    26/   32] train: loss: 0.4734134
[Epoch 63] ogbg-molsider: 0.619873 val loss: 0.660471
[Epoch 63] ogbg-molsider: 0.605313 test loss: 0.578063
[Epoch 64; Iter    24/   32] train: loss: 0.4033822
[Epoch 64] ogbg-molsider: 0.617464 val loss: 0.546462
[Epoch 64] ogbg-molsider: 0.616644 test loss: 0.560894
[Epoch 65; Iter    22/   32] train: loss: 0.3816739
[Epoch 65] ogbg-molsider: 0.642206 val loss: 0.692793
[Epoch 65] ogbg-molsider: 0.634195 test loss: 0.514951
[Epoch 66; Iter    20/   32] train: loss: 0.4729939
[Epoch 66] ogbg-molsider: 0.643259 val loss: 0.681104
[Epoch 66] ogbg-molsider: 0.632764 test loss: 0.521695
[Epoch 67; Iter    18/   32] train: loss: 0.4076822
[Epoch 67] ogbg-molsider: 0.622177 val loss: 0.970911
[Epoch 67] ogbg-molsider: 0.631982 test loss: 0.570263
[Epoch 68; Iter    16/   32] train: loss: 0.4702317
[Epoch 68] ogbg-molsider: 0.643330 val loss: 1.151146
[Epoch 68] ogbg-molsider: 0.632558 test loss: 0.514244
[Epoch 69; Iter    14/   32] train: loss: 0.4355122
[Epoch 69] ogbg-molsider: 0.647731 val loss: 0.548464
[Epoch 69] ogbg-molsider: 0.652921 test loss: 0.506832
[Epoch 70; Iter    12/   32] train: loss: 0.4009493
[Epoch 70] ogbg-molsider: 0.636217 val loss: 0.536359
[Epoch 70] ogbg-molsider: 0.628315 test loss: 0.523885
[Epoch 71; Iter    10/   32] train: loss: 0.4006081
[Epoch 71] ogbg-molsider: 0.614434 val loss: 0.568769
[Epoch 71] ogbg-molsider: 0.641594 test loss: 0.568701
[Epoch 72; Iter     8/   32] train: loss: 0.4468294
[Epoch 72] ogbg-molsider: 0.631509 val loss: 1.073606
[Epoch 72] ogbg-molsider: 0.598240 test loss: 0.882711
[Epoch 73; Iter     6/   32] train: loss: 0.4235860
[Epoch 73] ogbg-molsider: 0.608373 val loss: 0.551631
[Epoch 73] ogbg-molsider: 0.647424 test loss: 0.540647
[Epoch 74; Iter     4/   32] train: loss: 0.4996946
[Epoch 74] ogbg-molsider: 0.634600 val loss: 0.538088
[Epoch 74] ogbg-molsider: 0.619136 test loss: 0.553054
[Epoch 75; Iter     2/   32] train: loss: 0.4294414
[Epoch 75; Iter    32/   32] train: loss: 0.4211293
[Epoch 75] ogbg-molsider: 0.621624 val loss: 0.541149
[Epoch 75] ogbg-molsider: 0.590098 test loss: 0.559927
[Epoch 76; Iter    30/   32] train: loss: 0.4712324
[Epoch 76] ogbg-molsider: 0.639293 val loss: 0.537696
[Epoch 76] ogbg-molsider: 0.632554 test loss: 0.519850
[Epoch 77; Iter    28/   32] train: loss: 0.4125001
[Epoch 77] ogbg-molsider: 0.637246 val loss: 0.542969
[Epoch 77] ogbg-molsider: 0.627542 test loss: 0.533596
[Epoch 78; Iter    26/   32] train: loss: 0.4260207
[Epoch 78] ogbg-molsider: 0.650896 val loss: 0.543212
[Epoch 78] ogbg-molsider: 0.662509 test loss: 0.510251
[Epoch 79; Iter    24/   32] train: loss: 0.3998352
[Epoch 79] ogbg-molsider: 0.631933 val loss: 0.547598
[Epoch 79] ogbg-molsider: 0.644300 test loss: 0.538682
[Epoch 80; Iter    22/   32] train: loss: 0.4230803
[Epoch 80] ogbg-molsider: 0.642417 val loss: 0.538476
[Epoch 80] ogbg-molsider: 0.621231 test loss: 0.537253
[Epoch 81; Iter    20/   32] train: loss: 0.3953304
[Epoch 81] ogbg-molsider: 0.634078 val loss: 0.531513
[Epoch 81] ogbg-molsider: 0.651014 test loss: 0.519525
[Epoch 82; Iter    18/   32] train: loss: 0.3443305
[Epoch 82] ogbg-molsider: 0.627477 val loss: 0.552852
[Epoch 82] ogbg-molsider: 0.640625 test loss: 0.551604
[Epoch 83; Iter    16/   32] train: loss: 0.3645222
[Epoch 83] ogbg-molsider: 0.618949 val loss: 0.560328
[Epoch 83] ogbg-molsider: 0.621457 test loss: 0.545758
[Epoch 84; Iter    14/   32] train: loss: 0.3825738
[Epoch 84] ogbg-molsider: 0.635578 val loss: 0.550803
[Epoch 37; Iter    18/   27] train: loss: 0.5300048
[Epoch 37] ogbg-molsider: 0.585983 val loss: 0.530020
[Epoch 37] ogbg-molsider: 0.611141 test loss: 0.515412
[Epoch 38; Iter    21/   27] train: loss: 0.4928768
[Epoch 38] ogbg-molsider: 0.596112 val loss: 0.522190
[Epoch 38] ogbg-molsider: 0.603728 test loss: 0.505978
[Epoch 39; Iter    24/   27] train: loss: 0.4725305
[Epoch 39] ogbg-molsider: 0.599325 val loss: 0.520265
[Epoch 39] ogbg-molsider: 0.606973 test loss: 0.504838
[Epoch 40; Iter    27/   27] train: loss: 0.5255033
[Epoch 40] ogbg-molsider: 0.603031 val loss: 0.518979
[Epoch 40] ogbg-molsider: 0.607618 test loss: 0.502928
[Epoch 41] ogbg-molsider: 0.606267 val loss: 0.516857
[Epoch 41] ogbg-molsider: 0.607727 test loss: 0.503125
[Epoch 42; Iter     3/   27] train: loss: 0.4709946
[Epoch 42] ogbg-molsider: 0.614286 val loss: 0.518374
[Epoch 42] ogbg-molsider: 0.613984 test loss: 0.506853
[Epoch 43; Iter     6/   27] train: loss: 0.5045406
[Epoch 43] ogbg-molsider: 0.623995 val loss: 0.514876
[Epoch 43] ogbg-molsider: 0.612437 test loss: 0.501222
[Epoch 44; Iter     9/   27] train: loss: 0.4447336
[Epoch 44] ogbg-molsider: 0.624148 val loss: 0.517783
[Epoch 44] ogbg-molsider: 0.615093 test loss: 0.504393
[Epoch 45; Iter    12/   27] train: loss: 0.4471782
[Epoch 45] ogbg-molsider: 0.614249 val loss: 0.516452
[Epoch 45] ogbg-molsider: 0.626012 test loss: 0.500104
[Epoch 46; Iter    15/   27] train: loss: 0.4777555
[Epoch 46] ogbg-molsider: 0.599586 val loss: 0.515937
[Epoch 46] ogbg-molsider: 0.611902 test loss: 0.506011
[Epoch 47; Iter    18/   27] train: loss: 0.5016298
[Epoch 47] ogbg-molsider: 0.627901 val loss: 0.513907
[Epoch 47] ogbg-molsider: 0.628322 test loss: 0.502745
[Epoch 48; Iter    21/   27] train: loss: 0.5294695
[Epoch 48] ogbg-molsider: 0.610593 val loss: 0.516336
[Epoch 48] ogbg-molsider: 0.625554 test loss: 0.501528
[Epoch 49; Iter    24/   27] train: loss: 0.4628660
[Epoch 49] ogbg-molsider: 0.603150 val loss: 0.518553
[Epoch 49] ogbg-molsider: 0.628746 test loss: 0.501840
[Epoch 50; Iter    27/   27] train: loss: 0.4999174
[Epoch 50] ogbg-molsider: 0.606104 val loss: 0.523251
[Epoch 50] ogbg-molsider: 0.634156 test loss: 0.497980
[Epoch 51] ogbg-molsider: 0.605605 val loss: 0.522869
[Epoch 51] ogbg-molsider: 0.607818 test loss: 0.512984
[Epoch 52; Iter     3/   27] train: loss: 0.4367175
[Epoch 52] ogbg-molsider: 0.605388 val loss: 0.522423
[Epoch 52] ogbg-molsider: 0.638008 test loss: 0.495098
[Epoch 53; Iter     6/   27] train: loss: 0.4203427
[Epoch 53] ogbg-molsider: 0.529521 val loss: 0.590506
[Epoch 53] ogbg-molsider: 0.580956 test loss: 0.583653
[Epoch 54; Iter     9/   27] train: loss: 0.4243817
[Epoch 54] ogbg-molsider: 0.588465 val loss: 0.525445
[Epoch 54] ogbg-molsider: 0.622429 test loss: 0.500046
[Epoch 55; Iter    12/   27] train: loss: 0.4673617
[Epoch 55] ogbg-molsider: 0.590812 val loss: 0.543696
[Epoch 55] ogbg-molsider: 0.613919 test loss: 0.533350
[Epoch 56; Iter    15/   27] train: loss: 0.4213991
[Epoch 56] ogbg-molsider: 0.609080 val loss: 0.520775
[Epoch 56] ogbg-molsider: 0.626814 test loss: 0.501102
[Epoch 57; Iter    18/   27] train: loss: 0.4459452
[Epoch 57] ogbg-molsider: 0.615807 val loss: 0.528876
[Epoch 57] ogbg-molsider: 0.625432 test loss: 0.502140
[Epoch 58; Iter    21/   27] train: loss: 0.4711826
[Epoch 58] ogbg-molsider: 0.637605 val loss: 0.531098
[Epoch 58] ogbg-molsider: 0.609186 test loss: 0.541881
[Epoch 59; Iter    24/   27] train: loss: 0.4472990
[Epoch 59] ogbg-molsider: 0.616289 val loss: 0.517397
[Epoch 59] ogbg-molsider: 0.631334 test loss: 0.499769
[Epoch 60; Iter    27/   27] train: loss: 0.3972456
[Epoch 60] ogbg-molsider: 0.623320 val loss: 0.527561
[Epoch 60] ogbg-molsider: 0.601871 test loss: 0.533579
[Epoch 61] ogbg-molsider: 0.579209 val loss: 0.557827
[Epoch 61] ogbg-molsider: 0.613692 test loss: 0.575671
[Epoch 62; Iter     3/   27] train: loss: 0.4573984
[Epoch 62] ogbg-molsider: 0.628259 val loss: 0.513893
[Epoch 62] ogbg-molsider: 0.635573 test loss: 0.501081
[Epoch 63; Iter     6/   27] train: loss: 0.4717773
[Epoch 63] ogbg-molsider: 0.608478 val loss: 0.531009
[Epoch 63] ogbg-molsider: 0.616916 test loss: 0.523416
[Epoch 64; Iter     9/   27] train: loss: 0.4443716
[Epoch 64] ogbg-molsider: 0.600215 val loss: 0.551974
[Epoch 64] ogbg-molsider: 0.624579 test loss: 0.522074
[Epoch 65; Iter    12/   27] train: loss: 0.4693025
[Epoch 65] ogbg-molsider: 0.618813 val loss: 0.516413
[Epoch 65] ogbg-molsider: 0.637274 test loss: 0.495235
[Epoch 66; Iter    15/   27] train: loss: 0.4482265
[Epoch 66] ogbg-molsider: 0.626053 val loss: 0.512470
[Epoch 66] ogbg-molsider: 0.638713 test loss: 0.503417
[Epoch 67; Iter    18/   27] train: loss: 0.4836459
[Epoch 67] ogbg-molsider: 0.632934 val loss: 0.514216
[Epoch 67] ogbg-molsider: 0.647560 test loss: 0.501280
[Epoch 68; Iter    21/   27] train: loss: 0.4485398
[Epoch 68] ogbg-molsider: 0.651584 val loss: 0.516844
[Epoch 68] ogbg-molsider: 0.655741 test loss: 0.497210
[Epoch 69; Iter    24/   27] train: loss: 0.4652941
[Epoch 69] ogbg-molsider: 0.606703 val loss: 0.533222
[Epoch 69] ogbg-molsider: 0.651131 test loss: 0.510595
[Epoch 70; Iter    27/   27] train: loss: 0.4241213
[Epoch 70] ogbg-molsider: 0.589861 val loss: 0.556980
[Epoch 70] ogbg-molsider: 0.636002 test loss: 0.509238
[Epoch 71] ogbg-molsider: 0.646572 val loss: 0.528844
[Epoch 71] ogbg-molsider: 0.620157 test loss: 0.532157
[Epoch 72; Iter     3/   27] train: loss: 0.4168604
[Epoch 72] ogbg-molsider: 0.631442 val loss: 0.548375
[Epoch 72] ogbg-molsider: 0.641482 test loss: 0.522923
[Epoch 73; Iter     6/   27] train: loss: 0.3734275
[Epoch 73] ogbg-molsider: 0.584347 val loss: 0.556231
[Epoch 73] ogbg-molsider: 0.620037 test loss: 0.525457
[Epoch 74; Iter     9/   27] train: loss: 0.4312008
[Epoch 74] ogbg-molsider: 0.615980 val loss: 0.538560
[Epoch 74] ogbg-molsider: 0.648243 test loss: 0.521212
[Epoch 75; Iter    12/   27] train: loss: 0.4526806
[Epoch 75] ogbg-molsider: 0.603720 val loss: 0.552725
[Epoch 75] ogbg-molsider: 0.623055 test loss: 0.562762
[Epoch 76; Iter    15/   27] train: loss: 0.4545475
[Epoch 76] ogbg-molsider: 0.600841 val loss: 0.529755
[Epoch 76] ogbg-molsider: 0.640524 test loss: 0.499806
[Epoch 77; Iter    18/   27] train: loss: 0.4458744
[Epoch 77] ogbg-molsider: 0.613066 val loss: 0.527362
[Epoch 77] ogbg-molsider: 0.637783 test loss: 0.518755
[Epoch 78; Iter    21/   27] train: loss: 0.4473100
[Epoch 78] ogbg-molsider: 0.616480 val loss: 0.523598
[Epoch 78] ogbg-molsider: 0.632140 test loss: 0.508978
[Epoch 79; Iter    24/   27] train: loss: 0.4588935
[Epoch 79] ogbg-molsider: 0.630127 val loss: 0.550678
[Epoch 79] ogbg-molsider: 0.606674 test loss: 0.569553
[Epoch 80; Iter    27/   27] train: loss: 0.4224628
[Epoch 80] ogbg-molsider: 0.629445 val loss: 0.535247
[Epoch 80] ogbg-molsider: 0.655019 test loss: 0.502668
[Epoch 81] ogbg-molsider: 0.615269 val loss: 0.546723
[Epoch 81] ogbg-molsider: 0.633060 test loss: 0.521480
[Epoch 82; Iter     3/   27] train: loss: 0.4581116
[Epoch 82] ogbg-molsider: 0.655404 val loss: 0.579070
[Epoch 82] ogbg-molsider: 0.637487 test loss: 0.567539
[Epoch 83; Iter     6/   27] train: loss: 0.4094298
[Epoch 83] ogbg-molsider: 0.640534 val loss: 0.530193
[Epoch 83] ogbg-molsider: 0.642403 test loss: 0.513608
[Epoch 84; Iter     9/   27] train: loss: 0.3869182
[Epoch 84] ogbg-molsider: 0.637450 val loss: 0.545102
[Epoch 84] ogbg-molsider: 0.634312 test loss: 0.528790
[Epoch 85; Iter    12/   27] train: loss: 0.3771145
[Epoch 85] ogbg-molsider: 0.623729 val loss: 0.562678
[Epoch 85] ogbg-molsider: 0.642113 test loss: 0.525810
[Epoch 86; Iter    15/   27] train: loss: 0.3975571
[Epoch 86] ogbg-molsider: 0.642974 val loss: 0.533270
[Epoch 86] ogbg-molsider: 0.632675 test loss: 0.525410
[Epoch 87; Iter    18/   27] train: loss: 0.3610717
[Epoch 87] ogbg-molsider: 0.619447 val loss: 0.563693
[Epoch 87] ogbg-molsider: 0.628920 test loss: 0.556519
[Epoch 88; Iter    21/   27] train: loss: 0.4155086
[Epoch 88] ogbg-molsider: 0.618457 val loss: 0.619825
[Epoch 88] ogbg-molsider: 0.632558 test loss: 0.566585
[Epoch 89; Iter    24/   27] train: loss: 0.3964916
[Epoch 35; Iter    22/   32] train: loss: 0.5113299
[Epoch 35] ogbg-molsider: 0.599987 val loss: 0.504692
[Epoch 35] ogbg-molsider: 0.629525 test loss: 0.522294
[Epoch 36; Iter    20/   32] train: loss: 0.4893905
[Epoch 36] ogbg-molsider: 0.602409 val loss: 0.506839
[Epoch 36] ogbg-molsider: 0.617514 test loss: 0.501974
[Epoch 37; Iter    18/   32] train: loss: 0.4555389
[Epoch 37] ogbg-molsider: 0.603544 val loss: 0.506916
[Epoch 37] ogbg-molsider: 0.630210 test loss: 0.503221
[Epoch 38; Iter    16/   32] train: loss: 0.5328920
[Epoch 38] ogbg-molsider: 0.603374 val loss: 0.503991
[Epoch 38] ogbg-molsider: 0.618368 test loss: 0.499439
[Epoch 39; Iter    14/   32] train: loss: 0.4982463
[Epoch 39] ogbg-molsider: 0.609352 val loss: 0.502302
[Epoch 39] ogbg-molsider: 0.622519 test loss: 0.502408
[Epoch 40; Iter    12/   32] train: loss: 0.5132716
[Epoch 40] ogbg-molsider: 0.619285 val loss: 0.501645
[Epoch 40] ogbg-molsider: 0.623841 test loss: 0.499550
[Epoch 41; Iter    10/   32] train: loss: 0.4735931
[Epoch 41] ogbg-molsider: 0.612550 val loss: 0.508812
[Epoch 41] ogbg-molsider: 0.631731 test loss: 0.502404
[Epoch 42; Iter     8/   32] train: loss: 0.4564846
[Epoch 42] ogbg-molsider: 0.605284 val loss: 0.507156
[Epoch 42] ogbg-molsider: 0.630663 test loss: 0.499916
[Epoch 43; Iter     6/   32] train: loss: 0.5135697
[Epoch 43] ogbg-molsider: 0.602437 val loss: 0.510814
[Epoch 43] ogbg-molsider: 0.622665 test loss: 0.505495
[Epoch 44; Iter     4/   32] train: loss: 0.4654116
[Epoch 44] ogbg-molsider: 0.590994 val loss: 0.507157
[Epoch 44] ogbg-molsider: 0.597246 test loss: 0.510973
[Epoch 45; Iter     2/   32] train: loss: 0.4713463
[Epoch 45; Iter    32/   32] train: loss: 0.4388137
[Epoch 45] ogbg-molsider: 0.601311 val loss: 0.544900
[Epoch 45] ogbg-molsider: 0.574500 test loss: 0.925633
[Epoch 46; Iter    30/   32] train: loss: 0.4894435
[Epoch 46] ogbg-molsider: 0.616716 val loss: 0.500726
[Epoch 46] ogbg-molsider: 0.620735 test loss: 0.505672
[Epoch 47; Iter    28/   32] train: loss: 0.5151302
[Epoch 47] ogbg-molsider: 0.602333 val loss: 0.518229
[Epoch 47] ogbg-molsider: 0.598545 test loss: 0.518289
[Epoch 48; Iter    26/   32] train: loss: 0.4655595
[Epoch 48] ogbg-molsider: 0.597195 val loss: 0.618940
[Epoch 48] ogbg-molsider: 0.612495 test loss: 0.584029
[Epoch 49; Iter    24/   32] train: loss: 0.4628105
[Epoch 49] ogbg-molsider: 0.614677 val loss: 0.508570
[Epoch 49] ogbg-molsider: 0.608329 test loss: 0.521541
[Epoch 50; Iter    22/   32] train: loss: 0.4482187
[Epoch 50] ogbg-molsider: 0.626286 val loss: 0.508362
[Epoch 50] ogbg-molsider: 0.588516 test loss: 0.519518
[Epoch 51; Iter    20/   32] train: loss: 0.5144914
[Epoch 51] ogbg-molsider: 0.631981 val loss: 0.513553
[Epoch 51] ogbg-molsider: 0.615450 test loss: 0.519256
[Epoch 52; Iter    18/   32] train: loss: 0.5018269
[Epoch 52] ogbg-molsider: 0.638918 val loss: 0.495113
[Epoch 52] ogbg-molsider: 0.623391 test loss: 0.509998
[Epoch 53; Iter    16/   32] train: loss: 0.4517375
[Epoch 53] ogbg-molsider: 0.623921 val loss: 0.503089
[Epoch 53] ogbg-molsider: 0.637120 test loss: 0.501378
[Epoch 54; Iter    14/   32] train: loss: 0.4557586
[Epoch 54] ogbg-molsider: 0.648328 val loss: 0.492075
[Epoch 54] ogbg-molsider: 0.628067 test loss: 0.513500
[Epoch 55; Iter    12/   32] train: loss: 0.4559398
[Epoch 55] ogbg-molsider: 0.614006 val loss: 0.520111
[Epoch 55] ogbg-molsider: 0.615597 test loss: 0.526642
[Epoch 56; Iter    10/   32] train: loss: 0.4197672
[Epoch 56] ogbg-molsider: 0.636107 val loss: 0.498305
[Epoch 56] ogbg-molsider: 0.630780 test loss: 0.503638
[Epoch 57; Iter     8/   32] train: loss: 0.4349408
[Epoch 57] ogbg-molsider: 0.629695 val loss: 0.528257
[Epoch 57] ogbg-molsider: 0.623845 test loss: 0.501091
[Epoch 58; Iter     6/   32] train: loss: 0.4283040
[Epoch 58] ogbg-molsider: 0.647587 val loss: 0.541719
[Epoch 58] ogbg-molsider: 0.603843 test loss: 0.559354
[Epoch 59; Iter     4/   32] train: loss: 0.5048961
[Epoch 59] ogbg-molsider: 0.656106 val loss: 0.529035
[Epoch 59] ogbg-molsider: 0.603846 test loss: 0.529016
[Epoch 60; Iter     2/   32] train: loss: 0.4659544
[Epoch 60; Iter    32/   32] train: loss: 0.3711304
[Epoch 60] ogbg-molsider: 0.661673 val loss: 0.504485
[Epoch 60] ogbg-molsider: 0.624685 test loss: 0.505692
[Epoch 61; Iter    30/   32] train: loss: 0.4355846
[Epoch 61] ogbg-molsider: 0.622869 val loss: 0.548136
[Epoch 61] ogbg-molsider: 0.624556 test loss: 0.538271
[Epoch 62; Iter    28/   32] train: loss: 0.4220822
[Epoch 62] ogbg-molsider: 0.667233 val loss: 0.487882
[Epoch 62] ogbg-molsider: 0.618042 test loss: 0.512488
[Epoch 63; Iter    26/   32] train: loss: 0.4199277
[Epoch 63] ogbg-molsider: 0.646453 val loss: 0.527123
[Epoch 63] ogbg-molsider: 0.612094 test loss: 0.529484
[Epoch 64; Iter    24/   32] train: loss: 0.4164134
[Epoch 64] ogbg-molsider: 0.630740 val loss: 0.518887
[Epoch 64] ogbg-molsider: 0.625721 test loss: 0.519426
[Epoch 65; Iter    22/   32] train: loss: 0.4676673
[Epoch 65] ogbg-molsider: 0.666737 val loss: 0.504230
[Epoch 65] ogbg-molsider: 0.611321 test loss: 0.526193
[Epoch 66; Iter    20/   32] train: loss: 0.4002743
[Epoch 66] ogbg-molsider: 0.655493 val loss: 0.681100
[Epoch 66] ogbg-molsider: 0.599143 test loss: 1.555927
[Epoch 67; Iter    18/   32] train: loss: 0.4771053
[Epoch 67] ogbg-molsider: 0.635061 val loss: 0.552530
[Epoch 67] ogbg-molsider: 0.635236 test loss: 0.563661
[Epoch 68; Iter    16/   32] train: loss: 0.4203583
[Epoch 68] ogbg-molsider: 0.654247 val loss: 0.504316
[Epoch 68] ogbg-molsider: 0.629785 test loss: 0.522651
[Epoch 69; Iter    14/   32] train: loss: 0.4036171
[Epoch 69] ogbg-molsider: 0.641613 val loss: 0.537798
[Epoch 69] ogbg-molsider: 0.636393 test loss: 0.522129
[Epoch 70; Iter    12/   32] train: loss: 0.3594037
[Epoch 70] ogbg-molsider: 0.623859 val loss: 0.578514
[Epoch 70] ogbg-molsider: 0.630197 test loss: 0.520861
[Epoch 71; Iter    10/   32] train: loss: 0.4312361
[Epoch 71] ogbg-molsider: 0.642985 val loss: 0.540779
[Epoch 71] ogbg-molsider: 0.625686 test loss: 0.547513
[Epoch 72; Iter     8/   32] train: loss: 0.4649317
[Epoch 72] ogbg-molsider: 0.619861 val loss: 0.566335
[Epoch 72] ogbg-molsider: 0.650153 test loss: 0.537096
[Epoch 73; Iter     6/   32] train: loss: 0.4167850
[Epoch 73] ogbg-molsider: 0.629572 val loss: 0.532232
[Epoch 73] ogbg-molsider: 0.648447 test loss: 0.530258
[Epoch 74; Iter     4/   32] train: loss: 0.3798905
[Epoch 74] ogbg-molsider: 0.645648 val loss: 0.537524
[Epoch 74] ogbg-molsider: 0.629022 test loss: 0.536318
[Epoch 75; Iter     2/   32] train: loss: 0.4007495
[Epoch 75; Iter    32/   32] train: loss: 0.3913641
[Epoch 75] ogbg-molsider: 0.624936 val loss: 0.855901
[Epoch 75] ogbg-molsider: 0.633552 test loss: 0.551053
[Epoch 76; Iter    30/   32] train: loss: 0.3876659
[Epoch 76] ogbg-molsider: 0.630339 val loss: 0.567778
[Epoch 76] ogbg-molsider: 0.613028 test loss: 0.554970
[Epoch 77; Iter    28/   32] train: loss: 0.3836431
[Epoch 77] ogbg-molsider: 0.621093 val loss: 0.638250
[Epoch 77] ogbg-molsider: 0.624881 test loss: 0.558786
[Epoch 78; Iter    26/   32] train: loss: 0.4091375
[Epoch 78] ogbg-molsider: 0.625328 val loss: 0.568740
[Epoch 78] ogbg-molsider: 0.651703 test loss: 0.541693
[Epoch 79; Iter    24/   32] train: loss: 0.3770683
[Epoch 79] ogbg-molsider: 0.632832 val loss: 0.579585
[Epoch 79] ogbg-molsider: 0.637565 test loss: 0.559711
[Epoch 80; Iter    22/   32] train: loss: 0.4234123
[Epoch 80] ogbg-molsider: 0.623788 val loss: 0.569982
[Epoch 80] ogbg-molsider: 0.619199 test loss: 0.550180
[Epoch 81; Iter    20/   32] train: loss: 0.3550245
[Epoch 81] ogbg-molsider: 0.638269 val loss: 0.535506
[Epoch 81] ogbg-molsider: 0.624393 test loss: 0.558602
[Epoch 82; Iter    18/   32] train: loss: 0.3452522
[Epoch 82] ogbg-molsider: 0.637060 val loss: 0.539148
[Epoch 82] ogbg-molsider: 0.639346 test loss: 0.549386
[Epoch 83; Iter    16/   32] train: loss: 0.3181683
[Epoch 83] ogbg-molsider: 0.642983 val loss: 0.548942
[Epoch 83] ogbg-molsider: 0.644487 test loss: 0.553595
[Epoch 84; Iter    14/   32] train: loss: 0.3360964
[Epoch 84] ogbg-molsider: 0.638358 val loss: 0.600980
[Epoch 37; Iter    18/   27] train: loss: 0.5120597
[Epoch 37] ogbg-molsider: 0.581489 val loss: 0.545252
[Epoch 37] ogbg-molsider: 0.606725 test loss: 0.528355
[Epoch 38; Iter    21/   27] train: loss: 0.4974311
[Epoch 38] ogbg-molsider: 0.588884 val loss: 0.526001
[Epoch 38] ogbg-molsider: 0.605102 test loss: 0.520550
[Epoch 39; Iter    24/   27] train: loss: 0.5002475
[Epoch 39] ogbg-molsider: 0.618373 val loss: 0.522114
[Epoch 39] ogbg-molsider: 0.611554 test loss: 0.528018
[Epoch 40; Iter    27/   27] train: loss: 0.5115333
[Epoch 40] ogbg-molsider: 0.618332 val loss: 0.518014
[Epoch 40] ogbg-molsider: 0.605641 test loss: 0.533604
[Epoch 41] ogbg-molsider: 0.629548 val loss: 0.515199
[Epoch 41] ogbg-molsider: 0.601349 test loss: 0.532917
[Epoch 42; Iter     3/   27] train: loss: 0.4997275
[Epoch 42] ogbg-molsider: 0.599472 val loss: 0.517711
[Epoch 42] ogbg-molsider: 0.616839 test loss: 0.532785
[Epoch 43; Iter     6/   27] train: loss: 0.4689165
[Epoch 43] ogbg-molsider: 0.596982 val loss: 0.517141
[Epoch 43] ogbg-molsider: 0.607553 test loss: 0.533621
[Epoch 44; Iter     9/   27] train: loss: 0.4234312
[Epoch 44] ogbg-molsider: 0.610712 val loss: 0.515609
[Epoch 44] ogbg-molsider: 0.622443 test loss: 0.531263
[Epoch 45; Iter    12/   27] train: loss: 0.5176812
[Epoch 45] ogbg-molsider: 0.594150 val loss: 0.518508
[Epoch 45] ogbg-molsider: 0.618884 test loss: 0.532150
[Epoch 46; Iter    15/   27] train: loss: 0.5410215
[Epoch 46] ogbg-molsider: 0.620867 val loss: 0.512611
[Epoch 46] ogbg-molsider: 0.623790 test loss: 0.528717
[Epoch 47; Iter    18/   27] train: loss: 0.4582028
[Epoch 47] ogbg-molsider: 0.595177 val loss: 0.517878
[Epoch 47] ogbg-molsider: 0.610895 test loss: 0.534325
[Epoch 48; Iter    21/   27] train: loss: 0.4749570
[Epoch 48] ogbg-molsider: 0.601548 val loss: 0.519532
[Epoch 48] ogbg-molsider: 0.618174 test loss: 0.527576
[Epoch 49; Iter    24/   27] train: loss: 0.5028018
[Epoch 49] ogbg-molsider: 0.607698 val loss: 0.515492
[Epoch 49] ogbg-molsider: 0.631681 test loss: 0.517643
[Epoch 50; Iter    27/   27] train: loss: 0.4872474
[Epoch 50] ogbg-molsider: 0.612388 val loss: 0.516906
[Epoch 50] ogbg-molsider: 0.616788 test loss: 0.538483
[Epoch 51] ogbg-molsider: 0.615158 val loss: 0.529577
[Epoch 51] ogbg-molsider: 0.595737 test loss: 0.563280
[Epoch 52; Iter     3/   27] train: loss: 0.4467303
[Epoch 52] ogbg-molsider: 0.592043 val loss: 0.728631
[Epoch 52] ogbg-molsider: 0.557492 test loss: 0.723020
[Epoch 53; Iter     6/   27] train: loss: 0.5150419
[Epoch 53] ogbg-molsider: 0.594406 val loss: 0.526714
[Epoch 53] ogbg-molsider: 0.594251 test loss: 0.522399
[Epoch 54; Iter     9/   27] train: loss: 0.4641043
[Epoch 54] ogbg-molsider: 0.604259 val loss: 0.524692
[Epoch 54] ogbg-molsider: 0.622440 test loss: 0.505274
[Epoch 55; Iter    12/   27] train: loss: 0.5105658
[Epoch 55] ogbg-molsider: 0.620652 val loss: 0.598852
[Epoch 55] ogbg-molsider: 0.614098 test loss: 0.517559
[Epoch 56; Iter    15/   27] train: loss: 0.4666383
[Epoch 56] ogbg-molsider: 0.592718 val loss: 0.525754
[Epoch 56] ogbg-molsider: 0.627514 test loss: 0.507251
[Epoch 57; Iter    18/   27] train: loss: 0.4666803
[Epoch 57] ogbg-molsider: 0.610044 val loss: 0.541848
[Epoch 57] ogbg-molsider: 0.629451 test loss: 0.518673
[Epoch 58; Iter    21/   27] train: loss: 0.4713299
[Epoch 58] ogbg-molsider: 0.625350 val loss: 0.513810
[Epoch 58] ogbg-molsider: 0.614814 test loss: 0.512564
[Epoch 59; Iter    24/   27] train: loss: 0.4551799
[Epoch 59] ogbg-molsider: 0.614397 val loss: 0.541476
[Epoch 59] ogbg-molsider: 0.622093 test loss: 0.506017
[Epoch 60; Iter    27/   27] train: loss: 0.5200850
[Epoch 60] ogbg-molsider: 0.624482 val loss: 0.569597
[Epoch 60] ogbg-molsider: 0.594336 test loss: 0.578900
[Epoch 61] ogbg-molsider: 0.603591 val loss: 0.650891
[Epoch 61] ogbg-molsider: 0.574433 test loss: 0.930172
[Epoch 62; Iter     3/   27] train: loss: 0.4824068
[Epoch 62] ogbg-molsider: 0.600603 val loss: 0.579620
[Epoch 62] ogbg-molsider: 0.618164 test loss: 0.553063
[Epoch 63; Iter     6/   27] train: loss: 0.4406773
[Epoch 63] ogbg-molsider: 0.624551 val loss: 0.522636
[Epoch 63] ogbg-molsider: 0.617674 test loss: 0.509760
[Epoch 64; Iter     9/   27] train: loss: 0.4723940
[Epoch 64] ogbg-molsider: 0.569486 val loss: 0.539002
[Epoch 64] ogbg-molsider: 0.562555 test loss: 0.535748
[Epoch 65; Iter    12/   27] train: loss: 0.4831590
[Epoch 65] ogbg-molsider: 0.620625 val loss: 0.518688
[Epoch 65] ogbg-molsider: 0.623449 test loss: 0.504185
[Epoch 66; Iter    15/   27] train: loss: 0.4071722
[Epoch 66] ogbg-molsider: 0.604509 val loss: 0.538082
[Epoch 66] ogbg-molsider: 0.634847 test loss: 0.505723
[Epoch 67; Iter    18/   27] train: loss: 0.4534035
[Epoch 67] ogbg-molsider: 0.605923 val loss: 0.529319
[Epoch 67] ogbg-molsider: 0.611570 test loss: 0.554273
[Epoch 68; Iter    21/   27] train: loss: 0.4463019
[Epoch 68] ogbg-molsider: 0.635441 val loss: 0.506570
[Epoch 68] ogbg-molsider: 0.643993 test loss: 0.503687
[Epoch 69; Iter    24/   27] train: loss: 0.4757179
[Epoch 69] ogbg-molsider: 0.651265 val loss: 0.534915
[Epoch 69] ogbg-molsider: 0.644914 test loss: 0.503935
[Epoch 70; Iter    27/   27] train: loss: 0.4672927
[Epoch 70] ogbg-molsider: 0.621192 val loss: 0.599607
[Epoch 70] ogbg-molsider: 0.634584 test loss: 0.501909
[Epoch 71] ogbg-molsider: 0.626574 val loss: 0.534314
[Epoch 71] ogbg-molsider: 0.609826 test loss: 0.556248
[Epoch 72; Iter     3/   27] train: loss: 0.4219051
[Epoch 72] ogbg-molsider: 0.631160 val loss: 0.515112
[Epoch 72] ogbg-molsider: 0.634639 test loss: 0.514883
[Epoch 73; Iter     6/   27] train: loss: 0.4506103
[Epoch 73] ogbg-molsider: 0.640732 val loss: 0.517673
[Epoch 73] ogbg-molsider: 0.644131 test loss: 0.501475
[Epoch 74; Iter     9/   27] train: loss: 0.4406202
[Epoch 74] ogbg-molsider: 0.636124 val loss: 0.516081
[Epoch 74] ogbg-molsider: 0.627988 test loss: 0.508181
[Epoch 75; Iter    12/   27] train: loss: 0.4106176
[Epoch 75] ogbg-molsider: 0.644217 val loss: 0.510927
[Epoch 75] ogbg-molsider: 0.649967 test loss: 0.513300
[Epoch 76; Iter    15/   27] train: loss: 0.4273986
[Epoch 76] ogbg-molsider: 0.622941 val loss: 0.524870
[Epoch 76] ogbg-molsider: 0.637217 test loss: 0.529895
[Epoch 77; Iter    18/   27] train: loss: 0.4486754
[Epoch 77] ogbg-molsider: 0.652540 val loss: 0.512839
[Epoch 77] ogbg-molsider: 0.649011 test loss: 0.517160
[Epoch 78; Iter    21/   27] train: loss: 0.3959285
[Epoch 78] ogbg-molsider: 0.625213 val loss: 0.533528
[Epoch 78] ogbg-molsider: 0.632223 test loss: 0.541979
[Epoch 79; Iter    24/   27] train: loss: 0.4161003
[Epoch 79] ogbg-molsider: 0.621749 val loss: 0.537938
[Epoch 79] ogbg-molsider: 0.643133 test loss: 0.526867
[Epoch 80; Iter    27/   27] train: loss: 0.3679008
[Epoch 80] ogbg-molsider: 0.626706 val loss: 0.532703
[Epoch 80] ogbg-molsider: 0.627355 test loss: 0.543898
[Epoch 81] ogbg-molsider: 0.637674 val loss: 0.563473
[Epoch 81] ogbg-molsider: 0.660811 test loss: 0.533455
[Epoch 82; Iter     3/   27] train: loss: 0.3873126
[Epoch 82] ogbg-molsider: 0.631435 val loss: 0.534916
[Epoch 82] ogbg-molsider: 0.655182 test loss: 0.510347
[Epoch 83; Iter     6/   27] train: loss: 0.4078430
[Epoch 83] ogbg-molsider: 0.653271 val loss: 0.527424
[Epoch 83] ogbg-molsider: 0.673132 test loss: 0.507658
[Epoch 84; Iter     9/   27] train: loss: 0.4209627
[Epoch 84] ogbg-molsider: 0.645202 val loss: 0.568609
[Epoch 84] ogbg-molsider: 0.656730 test loss: 0.538333
[Epoch 85; Iter    12/   27] train: loss: 0.4306791
[Epoch 85] ogbg-molsider: 0.669892 val loss: 0.532789
[Epoch 85] ogbg-molsider: 0.647661 test loss: 0.530255
[Epoch 86; Iter    15/   27] train: loss: 0.4369408
[Epoch 86] ogbg-molsider: 0.616767 val loss: 0.560860
[Epoch 86] ogbg-molsider: 0.634014 test loss: 0.548607
[Epoch 87; Iter    18/   27] train: loss: 0.3920686
[Epoch 87] ogbg-molsider: 0.668484 val loss: 0.527639
[Epoch 87] ogbg-molsider: 0.639137 test loss: 0.542186
[Epoch 88; Iter    21/   27] train: loss: 0.4186128
[Epoch 88] ogbg-molsider: 0.644423 val loss: 0.539475
[Epoch 88] ogbg-molsider: 0.636306 test loss: 0.538304
[Epoch 89; Iter    24/   27] train: loss: 0.4042522
[Epoch 33] ogbg-molsider: 0.597060 test loss: 0.522886
[Epoch 34; Iter    12/   36] train: loss: 0.4883605
[Epoch 34] ogbg-molsider: 0.619128 val loss: 0.484841
[Epoch 34] ogbg-molsider: 0.617746 test loss: 0.512577
[Epoch 35; Iter     6/   36] train: loss: 0.4901879
[Epoch 35; Iter    36/   36] train: loss: 0.4511364
[Epoch 35] ogbg-molsider: 0.638307 val loss: 0.490667
[Epoch 35] ogbg-molsider: 0.618857 test loss: 0.517250
[Epoch 36; Iter    30/   36] train: loss: 0.4855799
[Epoch 36] ogbg-molsider: 0.636316 val loss: 0.491969
[Epoch 36] ogbg-molsider: 0.613810 test loss: 0.508738
[Epoch 37; Iter    24/   36] train: loss: 0.4833012
[Epoch 37] ogbg-molsider: 0.634484 val loss: 0.494250
[Epoch 37] ogbg-molsider: 0.623791 test loss: 0.514009
[Epoch 38; Iter    18/   36] train: loss: 0.5023777
[Epoch 38] ogbg-molsider: 0.642173 val loss: 0.477524
[Epoch 38] ogbg-molsider: 0.640005 test loss: 0.506412
[Epoch 39; Iter    12/   36] train: loss: 0.4822807
[Epoch 39] ogbg-molsider: 0.656031 val loss: 0.485366
[Epoch 39] ogbg-molsider: 0.625654 test loss: 0.501821
[Epoch 40; Iter     6/   36] train: loss: 0.4473064
[Epoch 40; Iter    36/   36] train: loss: 0.4872337
[Epoch 40] ogbg-molsider: 0.626846 val loss: 0.491116
[Epoch 40] ogbg-molsider: 0.612627 test loss: 0.526932
[Epoch 41; Iter    30/   36] train: loss: 0.4704401
[Epoch 41] ogbg-molsider: 0.607141 val loss: 0.493943
[Epoch 41] ogbg-molsider: 0.633527 test loss: 0.514004
[Epoch 42; Iter    24/   36] train: loss: 0.4748984
[Epoch 42] ogbg-molsider: 0.634745 val loss: 0.504305
[Epoch 42] ogbg-molsider: 0.601975 test loss: 0.508126
[Epoch 43; Iter    18/   36] train: loss: 0.4817001
[Epoch 43] ogbg-molsider: 0.658887 val loss: 0.751853
[Epoch 43] ogbg-molsider: 0.616792 test loss: 0.877456
[Epoch 44; Iter    12/   36] train: loss: 0.4652832
[Epoch 44] ogbg-molsider: 0.619514 val loss: 0.486832
[Epoch 44] ogbg-molsider: 0.630492 test loss: 0.505762
[Epoch 45; Iter     6/   36] train: loss: 0.4928707
[Epoch 45; Iter    36/   36] train: loss: 0.5062899
[Epoch 45] ogbg-molsider: 0.575607 val loss: 0.600773
[Epoch 45] ogbg-molsider: 0.558569 test loss: 0.587651
[Epoch 46; Iter    30/   36] train: loss: 0.5773208
[Epoch 46] ogbg-molsider: 0.627422 val loss: 0.486089
[Epoch 46] ogbg-molsider: 0.582801 test loss: 0.534821
[Epoch 47; Iter    24/   36] train: loss: 0.4964918
[Epoch 47] ogbg-molsider: 0.644393 val loss: 0.485306
[Epoch 47] ogbg-molsider: 0.596862 test loss: 0.521247
[Epoch 48; Iter    18/   36] train: loss: 0.4807436
[Epoch 48] ogbg-molsider: 0.630340 val loss: 0.488360
[Epoch 48] ogbg-molsider: 0.604834 test loss: 0.555282
[Epoch 49; Iter    12/   36] train: loss: 0.4738203
[Epoch 49] ogbg-molsider: 0.628257 val loss: 0.487211
[Epoch 49] ogbg-molsider: 0.595771 test loss: 0.521673
[Epoch 50; Iter     6/   36] train: loss: 0.4758695
[Epoch 50; Iter    36/   36] train: loss: 0.5450279
[Epoch 50] ogbg-molsider: 0.652748 val loss: 0.487023
[Epoch 50] ogbg-molsider: 0.588500 test loss: 0.518292
[Epoch 51; Iter    30/   36] train: loss: 0.4704979
[Epoch 51] ogbg-molsider: 0.598548 val loss: 0.540226
[Epoch 51] ogbg-molsider: 0.558077 test loss: 0.570870
[Epoch 52; Iter    24/   36] train: loss: 0.4566435
[Epoch 52] ogbg-molsider: 0.632817 val loss: 0.488957
[Epoch 52] ogbg-molsider: 0.608288 test loss: 0.517319
[Epoch 53; Iter    18/   36] train: loss: 0.4854075
[Epoch 53] ogbg-molsider: 0.636162 val loss: 1.126556
[Epoch 53] ogbg-molsider: 0.609884 test loss: 5.144376
[Epoch 54; Iter    12/   36] train: loss: 0.4701504
[Epoch 54] ogbg-molsider: 0.623039 val loss: 0.497273
[Epoch 54] ogbg-molsider: 0.645267 test loss: 0.500490
[Epoch 55; Iter     6/   36] train: loss: 0.4723921
[Epoch 55; Iter    36/   36] train: loss: 0.5149383
[Epoch 55] ogbg-molsider: 0.610375 val loss: 0.926518
[Epoch 55] ogbg-molsider: 0.571419 test loss: 1.279231
[Epoch 56; Iter    30/   36] train: loss: 0.5539550
[Epoch 56] ogbg-molsider: 0.611778 val loss: 0.629225
[Epoch 56] ogbg-molsider: 0.616649 test loss: 0.604929
[Epoch 57; Iter    24/   36] train: loss: 0.4524490
[Epoch 57] ogbg-molsider: 0.640114 val loss: 0.505567
[Epoch 57] ogbg-molsider: 0.617540 test loss: 0.509368
[Epoch 58; Iter    18/   36] train: loss: 0.4876437
[Epoch 58] ogbg-molsider: 0.633994 val loss: 0.489167
[Epoch 58] ogbg-molsider: 0.637183 test loss: 0.509535
[Epoch 59; Iter    12/   36] train: loss: 0.4609396
[Epoch 59] ogbg-molsider: 0.635469 val loss: 1.311141
[Epoch 59] ogbg-molsider: 0.623676 test loss: 1.774689
[Epoch 60; Iter     6/   36] train: loss: 0.4641800
[Epoch 60; Iter    36/   36] train: loss: 0.4682977
[Epoch 60] ogbg-molsider: 0.648665 val loss: 0.513548
[Epoch 60] ogbg-molsider: 0.631818 test loss: 0.516865
[Epoch 61; Iter    30/   36] train: loss: 0.4431126
[Epoch 61] ogbg-molsider: 0.614679 val loss: 0.497278
[Epoch 61] ogbg-molsider: 0.624582 test loss: 0.575171
[Epoch 62; Iter    24/   36] train: loss: 0.4189783
[Epoch 62] ogbg-molsider: 0.643460 val loss: 0.568131
[Epoch 62] ogbg-molsider: 0.617223 test loss: 1.753079
[Epoch 63; Iter    18/   36] train: loss: 0.4407688
[Epoch 63] ogbg-molsider: 0.610606 val loss: 0.497383
[Epoch 63] ogbg-molsider: 0.633521 test loss: 0.525131
[Epoch 64; Iter    12/   36] train: loss: 0.4389105
[Epoch 64] ogbg-molsider: 0.658641 val loss: 0.502794
[Epoch 64] ogbg-molsider: 0.611042 test loss: 0.520965
[Epoch 65; Iter     6/   36] train: loss: 0.4215711
[Epoch 65; Iter    36/   36] train: loss: 0.4323829
[Epoch 65] ogbg-molsider: 0.597659 val loss: 0.512007
[Epoch 65] ogbg-molsider: 0.597914 test loss: 0.559641
[Epoch 66; Iter    30/   36] train: loss: 0.4440498
[Epoch 66] ogbg-molsider: 0.634193 val loss: 0.514449
[Epoch 66] ogbg-molsider: 0.622114 test loss: 0.540753
[Epoch 67; Iter    24/   36] train: loss: 0.4206425
[Epoch 67] ogbg-molsider: 0.640445 val loss: 0.501435
[Epoch 67] ogbg-molsider: 0.646210 test loss: 0.507757
[Epoch 68; Iter    18/   36] train: loss: 0.4253245
[Epoch 68] ogbg-molsider: 0.631817 val loss: 0.502117
[Epoch 68] ogbg-molsider: 0.634164 test loss: 0.519873
[Epoch 69; Iter    12/   36] train: loss: 0.4209325
[Epoch 69] ogbg-molsider: 0.612199 val loss: 0.498723
[Epoch 69] ogbg-molsider: 0.612741 test loss: 0.540987
[Epoch 70; Iter     6/   36] train: loss: 0.4265481
[Epoch 70; Iter    36/   36] train: loss: 0.4452998
[Epoch 70] ogbg-molsider: 0.626715 val loss: 0.506486
[Epoch 70] ogbg-molsider: 0.636347 test loss: 0.521525
[Epoch 71; Iter    30/   36] train: loss: 0.3700838
[Epoch 71] ogbg-molsider: 0.641371 val loss: 0.494520
[Epoch 71] ogbg-molsider: 0.627455 test loss: 0.529250
[Epoch 72; Iter    24/   36] train: loss: 0.4393373
[Epoch 72] ogbg-molsider: 0.605122 val loss: 0.545681
[Epoch 72] ogbg-molsider: 0.604989 test loss: 0.642871
[Epoch 73; Iter    18/   36] train: loss: 0.4367065
[Epoch 73] ogbg-molsider: 0.630777 val loss: 0.515231
[Epoch 73] ogbg-molsider: 0.640289 test loss: 0.547709
[Epoch 74; Iter    12/   36] train: loss: 0.4180231
[Epoch 74] ogbg-molsider: 0.618828 val loss: 0.505822
[Epoch 74] ogbg-molsider: 0.650327 test loss: 0.571201
[Epoch 75; Iter     6/   36] train: loss: 0.3744946
[Epoch 75; Iter    36/   36] train: loss: 0.4335673
[Epoch 75] ogbg-molsider: 0.629125 val loss: 0.502911
[Epoch 75] ogbg-molsider: 0.618441 test loss: 0.530689
[Epoch 76; Iter    30/   36] train: loss: 0.4755436
[Epoch 76] ogbg-molsider: 0.651697 val loss: 0.484960
[Epoch 76] ogbg-molsider: 0.645637 test loss: 0.515371
[Epoch 77; Iter    24/   36] train: loss: 0.4426371
[Epoch 77] ogbg-molsider: 0.632568 val loss: 0.516229
[Epoch 77] ogbg-molsider: 0.597592 test loss: 0.569019
[Epoch 78; Iter    18/   36] train: loss: 0.4917106
[Epoch 78] ogbg-molsider: 0.603951 val loss: 0.503758
[Epoch 78] ogbg-molsider: 0.651835 test loss: 0.518793
[Epoch 79; Iter    12/   36] train: loss: 0.4161460
[Epoch 79] ogbg-molsider: 0.635501 val loss: 0.499033
[Epoch 79] ogbg-molsider: 0.601707 test loss: 0.545023
[Epoch 80; Iter     6/   36] train: loss: 0.3978149
[Epoch 80; Iter    36/   36] train: loss: 0.3869933
[Epoch 80] ogbg-molsider: 0.643543 val loss: 0.510838
[Epoch 80] ogbg-molsider: 0.649166 test loss: 0.535221
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 81; Iter    30/   36] train: loss: 0.4012473
[Epoch 81] ogbg-molsider: 0.624909 val loss: 0.554691
[Epoch 81] ogbg-molsider: 0.649735 test loss: 0.566743
[Epoch 82; Iter    24/   36] train: loss: 0.3867365
[Epoch 82] ogbg-molsider: 0.642045 val loss: 0.562764
[Epoch 82] ogbg-molsider: 0.657404 test loss: 0.574063
[Epoch 83; Iter    18/   36] train: loss: 0.3081104
[Epoch 83] ogbg-molsider: 0.633956 val loss: 0.570984
[Epoch 83] ogbg-molsider: 0.635345 test loss: 0.596455
[Epoch 84; Iter    12/   36] train: loss: 0.3271132
[Epoch 84] ogbg-molsider: 0.644415 val loss: 0.529065
[Epoch 84] ogbg-molsider: 0.659051 test loss: 0.567615
[Epoch 85; Iter     6/   36] train: loss: 0.3397643
[Epoch 85; Iter    36/   36] train: loss: 0.3262186
[Epoch 85] ogbg-molsider: 0.649521 val loss: 0.565467
[Epoch 85] ogbg-molsider: 0.651154 test loss: 0.613674
[Epoch 86; Iter    30/   36] train: loss: 0.3667333
[Epoch 86] ogbg-molsider: 0.635997 val loss: 0.572838
[Epoch 86] ogbg-molsider: 0.634837 test loss: 0.600820
[Epoch 87; Iter    24/   36] train: loss: 0.3307698
[Epoch 87] ogbg-molsider: 0.651057 val loss: 0.549673
[Epoch 87] ogbg-molsider: 0.644490 test loss: 0.602141
[Epoch 88; Iter    18/   36] train: loss: 0.3272891
[Epoch 88] ogbg-molsider: 0.637390 val loss: 0.593861
[Epoch 88] ogbg-molsider: 0.636359 test loss: 0.658243
[Epoch 89; Iter    12/   36] train: loss: 0.3237507
[Epoch 89] ogbg-molsider: 0.654980 val loss: 0.584852
[Epoch 89] ogbg-molsider: 0.656346 test loss: 0.654908
[Epoch 90; Iter     6/   36] train: loss: 0.3165395
[Epoch 90; Iter    36/   36] train: loss: 0.3201601
[Epoch 90] ogbg-molsider: 0.642814 val loss: 0.579215
[Epoch 90] ogbg-molsider: 0.648996 test loss: 0.600726
[Epoch 91; Iter    30/   36] train: loss: 0.2795999
[Epoch 91] ogbg-molsider: 0.655874 val loss: 0.572269
[Epoch 91] ogbg-molsider: 0.649177 test loss: 0.610510
[Epoch 92; Iter    24/   36] train: loss: 0.2576593
[Epoch 92] ogbg-molsider: 0.638497 val loss: 0.587416
[Epoch 92] ogbg-molsider: 0.649152 test loss: 0.620184
[Epoch 93; Iter    18/   36] train: loss: 0.2523412
[Epoch 93] ogbg-molsider: 0.641238 val loss: 0.592193
[Epoch 93] ogbg-molsider: 0.642714 test loss: 0.606939
[Epoch 94; Iter    12/   36] train: loss: 0.3070511
[Epoch 94] ogbg-molsider: 0.645970 val loss: 0.590658
[Epoch 94] ogbg-molsider: 0.658254 test loss: 0.607885
[Epoch 95; Iter     6/   36] train: loss: 0.2900463
[Epoch 95; Iter    36/   36] train: loss: 0.2720915
[Epoch 95] ogbg-molsider: 0.644631 val loss: 0.587865
[Epoch 95] ogbg-molsider: 0.658128 test loss: 0.608527
[Epoch 96; Iter    30/   36] train: loss: 0.2667677
[Epoch 96] ogbg-molsider: 0.629995 val loss: 0.673759
[Epoch 96] ogbg-molsider: 0.651285 test loss: 0.605028
[Epoch 97; Iter    24/   36] train: loss: 0.2919853
[Epoch 97] ogbg-molsider: 0.645093 val loss: 0.597503
[Epoch 97] ogbg-molsider: 0.665256 test loss: 0.608326
[Epoch 98; Iter    18/   36] train: loss: 0.2859897
[Epoch 98] ogbg-molsider: 0.639545 val loss: 0.601365
[Epoch 98] ogbg-molsider: 0.660698 test loss: 0.616108
[Epoch 99; Iter    12/   36] train: loss: 0.3020301
[Epoch 99] ogbg-molsider: 0.633311 val loss: 0.657071
[Epoch 99] ogbg-molsider: 0.655569 test loss: 0.642917
[Epoch 100; Iter     6/   36] train: loss: 0.2688462
[Epoch 100; Iter    36/   36] train: loss: 0.2472071
[Epoch 100] ogbg-molsider: 0.641176 val loss: 0.598395
[Epoch 100] ogbg-molsider: 0.648317 test loss: 0.623073
[Epoch 101; Iter    30/   36] train: loss: 0.2768586
[Epoch 101] ogbg-molsider: 0.639287 val loss: 0.606228
[Epoch 101] ogbg-molsider: 0.652699 test loss: 0.628300
[Epoch 102; Iter    24/   36] train: loss: 0.2736222
[Epoch 102] ogbg-molsider: 0.627214 val loss: 0.626503
[Epoch 102] ogbg-molsider: 0.638278 test loss: 0.632115
[Epoch 103; Iter    18/   36] train: loss: 0.2430132
[Epoch 103] ogbg-molsider: 0.632875 val loss: 0.615532
[Epoch 103] ogbg-molsider: 0.648242 test loss: 0.651066
[Epoch 104; Iter    12/   36] train: loss: 0.2637794
[Epoch 104] ogbg-molsider: 0.634572 val loss: 0.634392
[Epoch 104] ogbg-molsider: 0.660758 test loss: 0.639086
[Epoch 105; Iter     6/   36] train: loss: 0.2757137
[Epoch 105; Iter    36/   36] train: loss: 0.2478049
[Epoch 105] ogbg-molsider: 0.632271 val loss: 0.612610
[Epoch 105] ogbg-molsider: 0.649853 test loss: 0.630199
[Epoch 106; Iter    30/   36] train: loss: 0.2699623
[Epoch 106] ogbg-molsider: 0.642188 val loss: 0.618606
[Epoch 106] ogbg-molsider: 0.645094 test loss: 0.630893
[Epoch 107; Iter    24/   36] train: loss: 0.2730418
[Epoch 107] ogbg-molsider: 0.641911 val loss: 0.627797
[Epoch 107] ogbg-molsider: 0.649914 test loss: 0.640447
[Epoch 108; Iter    18/   36] train: loss: 0.2709109
[Epoch 108] ogbg-molsider: 0.624025 val loss: 0.656288
[Epoch 108] ogbg-molsider: 0.654138 test loss: 0.673068
[Epoch 109; Iter    12/   36] train: loss: 0.2554829
[Epoch 109] ogbg-molsider: 0.659896 val loss: 0.601695
[Epoch 109] ogbg-molsider: 0.636962 test loss: 0.689986
[Epoch 110; Iter     6/   36] train: loss: 0.2398330
[Epoch 110; Iter    36/   36] train: loss: 0.3278914
[Epoch 110] ogbg-molsider: 0.645373 val loss: 0.667948
[Epoch 110] ogbg-molsider: 0.653777 test loss: 0.660885
[Epoch 111; Iter    30/   36] train: loss: 0.2733767
[Epoch 111] ogbg-molsider: 0.654902 val loss: 0.626127
[Epoch 111] ogbg-molsider: 0.644326 test loss: 0.677771
[Epoch 112; Iter    24/   36] train: loss: 0.2443805
[Epoch 112] ogbg-molsider: 0.617983 val loss: 0.657157
[Epoch 112] ogbg-molsider: 0.647912 test loss: 0.679257
[Epoch 113; Iter    18/   36] train: loss: 0.2388785
[Epoch 113] ogbg-molsider: 0.641833 val loss: 0.660951
[Epoch 113] ogbg-molsider: 0.642184 test loss: 0.685063
[Epoch 114; Iter    12/   36] train: loss: 0.2573900
[Epoch 114] ogbg-molsider: 0.636401 val loss: 0.708775
[Epoch 114] ogbg-molsider: 0.639613 test loss: 0.663227
[Epoch 115; Iter     6/   36] train: loss: 0.2078404
[Epoch 115; Iter    36/   36] train: loss: 0.2938392
[Epoch 115] ogbg-molsider: 0.641348 val loss: 0.668056
[Epoch 115] ogbg-molsider: 0.633767 test loss: 0.676943
[Epoch 116; Iter    30/   36] train: loss: 0.2562092
[Epoch 116] ogbg-molsider: 0.637959 val loss: 0.675722
[Epoch 116] ogbg-molsider: 0.650150 test loss: 0.683614
[Epoch 117; Iter    24/   36] train: loss: 0.2843231
[Epoch 117] ogbg-molsider: 0.643515 val loss: 0.643165
[Epoch 117] ogbg-molsider: 0.646422 test loss: 0.668637
[Epoch 118; Iter    18/   36] train: loss: 0.2341585
[Epoch 118] ogbg-molsider: 0.639138 val loss: 0.648648
[Epoch 118] ogbg-molsider: 0.646102 test loss: 0.661111
[Epoch 119; Iter    12/   36] train: loss: 0.2572191
[Epoch 119] ogbg-molsider: 0.642469 val loss: 0.650323
[Epoch 119] ogbg-molsider: 0.648608 test loss: 0.661870
[Epoch 120; Iter     6/   36] train: loss: 0.2063223
[Epoch 120; Iter    36/   36] train: loss: 0.2511430
[Epoch 120] ogbg-molsider: 0.627865 val loss: 0.662115
[Epoch 120] ogbg-molsider: 0.641398 test loss: 0.704550
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 56.
Statistics on  val_best_checkpoint
mean_pred: 0.2852294147014618
std_pred: 1.9744465351104736
mean_targets: 0.6021755933761597
std_targets: 0.48951220512390137
prcauc: 0.7029307925018199
rocauc: 0.6787308375506957
ogbg-molsider: 0.6787308375506957
OGBNanLabelBCEWithLogitsLoss: 0.49636316299438477
Statistics on  test
mean_pred: 0.2830348014831543
std_pred: 1.9146093130111694
mean_targets: 0.5446775555610657
std_targets: 0.49806439876556396
prcauc: 0.6493866828104581
rocauc: 0.6293095357918926
ogbg-molsider: 0.6293095357918926
OGBNanLabelBCEWithLogitsLoss: 0.5165032684803009
Statistics on  train
mean_pred: 0.261989951133728
std_pred: 2.124558210372925
mean_targets: 0.5661051273345947
std_targets: 0.49561887979507446
prcauc: 0.7457887938799663
rocauc: 0.7771258094882664
ogbg-molsider: 0.7771258094882664
OGBNanLabelBCEWithLogitsLoss: 0.4372663117117352
[Epoch 81; Iter    30/   36] train: loss: 0.3142878
[Epoch 81] ogbg-molsider: 0.658619 val loss: 0.529666
[Epoch 81] ogbg-molsider: 0.659374 test loss: 0.546988
[Epoch 82; Iter    24/   36] train: loss: 0.2755836
[Epoch 82] ogbg-molsider: 0.652001 val loss: 0.529782
[Epoch 82] ogbg-molsider: 0.666273 test loss: 0.554019
[Epoch 83; Iter    18/   36] train: loss: 0.3191850
[Epoch 83] ogbg-molsider: 0.648148 val loss: 0.535099
[Epoch 83] ogbg-molsider: 0.655185 test loss: 0.581403
[Epoch 84; Iter    12/   36] train: loss: 0.3460780
[Epoch 84] ogbg-molsider: 0.656902 val loss: 0.530164
[Epoch 84] ogbg-molsider: 0.661147 test loss: 0.552193
[Epoch 85; Iter     6/   36] train: loss: 0.2931538
[Epoch 85; Iter    36/   36] train: loss: 0.3470490
[Epoch 85] ogbg-molsider: 0.647595 val loss: 0.549236
[Epoch 85] ogbg-molsider: 0.663840 test loss: 0.571160
[Epoch 86; Iter    30/   36] train: loss: 0.3307761
[Epoch 86] ogbg-molsider: 0.650635 val loss: 0.539774
[Epoch 86] ogbg-molsider: 0.671678 test loss: 0.553132
[Epoch 87; Iter    24/   36] train: loss: 0.3065082
[Epoch 87] ogbg-molsider: 0.659682 val loss: 0.540634
[Epoch 87] ogbg-molsider: 0.655130 test loss: 0.581324
[Epoch 88; Iter    18/   36] train: loss: 0.3308033
[Epoch 88] ogbg-molsider: 0.651176 val loss: 0.557550
[Epoch 88] ogbg-molsider: 0.667795 test loss: 0.562504
[Epoch 89; Iter    12/   36] train: loss: 0.3080015
[Epoch 89] ogbg-molsider: 0.663001 val loss: 0.530735
[Epoch 89] ogbg-molsider: 0.664551 test loss: 0.548305
[Epoch 90; Iter     6/   36] train: loss: 0.3221435
[Epoch 90; Iter    36/   36] train: loss: 0.3237455
[Epoch 90] ogbg-molsider: 0.639263 val loss: 0.551320
[Epoch 90] ogbg-molsider: 0.663556 test loss: 0.566752
[Epoch 91; Iter    30/   36] train: loss: 0.3212089
[Epoch 91] ogbg-molsider: 0.653682 val loss: 0.559498
[Epoch 91] ogbg-molsider: 0.661419 test loss: 0.584502
[Epoch 92; Iter    24/   36] train: loss: 0.3120221
[Epoch 92] ogbg-molsider: 0.655084 val loss: 0.555777
[Epoch 92] ogbg-molsider: 0.655236 test loss: 0.572664
[Epoch 93; Iter    18/   36] train: loss: 0.2868211
[Epoch 93] ogbg-molsider: 0.653506 val loss: 0.550106
[Epoch 93] ogbg-molsider: 0.650679 test loss: 0.599795
[Epoch 94; Iter    12/   36] train: loss: 0.3210523
[Epoch 94] ogbg-molsider: 0.627670 val loss: 0.576660
[Epoch 94] ogbg-molsider: 0.651556 test loss: 0.595039
[Epoch 95; Iter     6/   36] train: loss: 0.2987200
[Epoch 95; Iter    36/   36] train: loss: 0.4037832
[Epoch 95] ogbg-molsider: 0.647174 val loss: 0.552591
[Epoch 95] ogbg-molsider: 0.651806 test loss: 0.576412
[Epoch 96; Iter    30/   36] train: loss: 0.3119372
[Epoch 96] ogbg-molsider: 0.648831 val loss: 0.588769
[Epoch 96] ogbg-molsider: 0.654662 test loss: 0.623808
[Epoch 97; Iter    24/   36] train: loss: 0.3070229
[Epoch 97] ogbg-molsider: 0.658352 val loss: 0.550233
[Epoch 97] ogbg-molsider: 0.652275 test loss: 0.579321
[Epoch 98; Iter    18/   36] train: loss: 0.2895303
[Epoch 98] ogbg-molsider: 0.640681 val loss: 0.584303
[Epoch 98] ogbg-molsider: 0.654822 test loss: 0.583123
[Epoch 99; Iter    12/   36] train: loss: 0.2850269
[Epoch 99] ogbg-molsider: 0.638727 val loss: 0.576461
[Epoch 99] ogbg-molsider: 0.667270 test loss: 0.596694
[Epoch 100; Iter     6/   36] train: loss: 0.2934537
[Epoch 100; Iter    36/   36] train: loss: 0.3095734
[Epoch 100] ogbg-molsider: 0.648036 val loss: 0.562932
[Epoch 100] ogbg-molsider: 0.656043 test loss: 0.598454
[Epoch 101; Iter    30/   36] train: loss: 0.2934352
[Epoch 101] ogbg-molsider: 0.645398 val loss: 0.579656
[Epoch 101] ogbg-molsider: 0.653881 test loss: 0.593706
[Epoch 102; Iter    24/   36] train: loss: 0.3053512
[Epoch 102] ogbg-molsider: 0.660598 val loss: 0.547347
[Epoch 102] ogbg-molsider: 0.662737 test loss: 0.602575
[Epoch 103; Iter    18/   36] train: loss: 0.2678186
[Epoch 103] ogbg-molsider: 0.631857 val loss: 0.605834
[Epoch 103] ogbg-molsider: 0.665044 test loss: 0.604768
[Epoch 104; Iter    12/   36] train: loss: 0.2816402
[Epoch 104] ogbg-molsider: 0.650860 val loss: 0.570425
[Epoch 104] ogbg-molsider: 0.659678 test loss: 0.617730
[Epoch 105; Iter     6/   36] train: loss: 0.3081245
[Epoch 105; Iter    36/   36] train: loss: 0.2691446
[Epoch 105] ogbg-molsider: 0.648133 val loss: 0.575484
[Epoch 105] ogbg-molsider: 0.663349 test loss: 0.592093
[Epoch 106; Iter    30/   36] train: loss: 0.2687162
[Epoch 106] ogbg-molsider: 0.648203 val loss: 0.579219
[Epoch 106] ogbg-molsider: 0.662130 test loss: 0.601743
[Epoch 107; Iter    24/   36] train: loss: 0.2924660
[Epoch 107] ogbg-molsider: 0.644940 val loss: 0.589989
[Epoch 107] ogbg-molsider: 0.659242 test loss: 0.608159
[Epoch 108; Iter    18/   36] train: loss: 0.2713250
[Epoch 108] ogbg-molsider: 0.648324 val loss: 0.583897
[Epoch 108] ogbg-molsider: 0.659036 test loss: 0.602015
[Epoch 109; Iter    12/   36] train: loss: 0.2587370
[Epoch 109] ogbg-molsider: 0.646840 val loss: 0.586005
[Epoch 109] ogbg-molsider: 0.657396 test loss: 0.628010
[Epoch 110; Iter     6/   36] train: loss: 0.2673342
[Epoch 110; Iter    36/   36] train: loss: 0.2553393
[Epoch 110] ogbg-molsider: 0.647967 val loss: 0.589683
[Epoch 110] ogbg-molsider: 0.664597 test loss: 0.612809
[Epoch 111; Iter    30/   36] train: loss: 0.2834292
[Epoch 111] ogbg-molsider: 0.641049 val loss: 0.595142
[Epoch 111] ogbg-molsider: 0.657594 test loss: 0.622787
[Epoch 112; Iter    24/   36] train: loss: 0.3078332
[Epoch 112] ogbg-molsider: 0.648791 val loss: 0.589940
[Epoch 112] ogbg-molsider: 0.656079 test loss: 0.616989
[Epoch 113; Iter    18/   36] train: loss: 0.2841069
[Epoch 113] ogbg-molsider: 0.653508 val loss: 0.581543
[Epoch 113] ogbg-molsider: 0.652476 test loss: 0.607173
[Epoch 114; Iter    12/   36] train: loss: 0.2567988
[Epoch 114] ogbg-molsider: 0.650964 val loss: 0.581299
[Epoch 114] ogbg-molsider: 0.648154 test loss: 0.611559
[Epoch 115; Iter     6/   36] train: loss: 0.2481679
[Epoch 115; Iter    36/   36] train: loss: 0.2489464
[Epoch 115] ogbg-molsider: 0.650262 val loss: 0.597991
[Epoch 115] ogbg-molsider: 0.655035 test loss: 0.632266
[Epoch 116; Iter    30/   36] train: loss: 0.2591573
[Epoch 116] ogbg-molsider: 0.642904 val loss: 0.602926
[Epoch 116] ogbg-molsider: 0.656561 test loss: 0.625293
[Epoch 117; Iter    24/   36] train: loss: 0.2463913
[Epoch 117] ogbg-molsider: 0.643421 val loss: 0.632086
[Epoch 117] ogbg-molsider: 0.648178 test loss: 0.651119
[Epoch 118; Iter    18/   36] train: loss: 0.2530403
[Epoch 118] ogbg-molsider: 0.647846 val loss: 0.603750
[Epoch 118] ogbg-molsider: 0.646847 test loss: 0.637437
[Epoch 119; Iter    12/   36] train: loss: 0.2288972
[Epoch 119] ogbg-molsider: 0.647051 val loss: 0.605447
[Epoch 119] ogbg-molsider: 0.655961 test loss: 0.636353
[Epoch 120; Iter     6/   36] train: loss: 0.3138721
[Epoch 120; Iter    36/   36] train: loss: 0.2726921
[Epoch 120] ogbg-molsider: 0.645314 val loss: 0.619978
[Epoch 120] ogbg-molsider: 0.648268 test loss: 0.658587
Early stopping criterion based on -ogbg-molsider- that should be max reached after 120 epochs. Best model checkpoint was in epoch 60.
Statistics on  val_best_checkpoint
mean_pred: 0.48570841550827026
std_pred: 2.142786741256714
mean_targets: 0.6021755933761597
std_targets: 0.48951220512390137
prcauc: 0.7109895980619549
rocauc: 0.696381118185683
ogbg-molsider: 0.696381118185683
OGBNanLabelBCEWithLogitsLoss: 0.4713055670261383
Statistics on  test
mean_pred: 0.37740471959114075
std_pred: 2.096595287322998
mean_targets: 0.5446775555610657
std_targets: 0.49806439876556396
prcauc: 0.6513357040399883
rocauc: 0.6302898853939379
ogbg-molsider: 0.6302898853939379
OGBNanLabelBCEWithLogitsLoss: 0.5111374735832215
Statistics on  train
mean_pred: 0.3860333561897278
std_pred: 2.1754486560821533
mean_targets: 0.5661051273345947
std_targets: 0.49561887979507446
prcauc: 0.7485327773187166
rocauc: 0.7879602136259519
ogbg-molsider: 0.7879602136259519
OGBNanLabelBCEWithLogitsLoss: 0.42077503436141545
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 84] ogbg-molsider: 0.645056 test loss: 0.555541
[Epoch 85; Iter    12/   32] train: loss: 0.3530869
[Epoch 85] ogbg-molsider: 0.633418 val loss: 0.554908
[Epoch 85] ogbg-molsider: 0.636607 test loss: 0.564537
[Epoch 86; Iter    10/   32] train: loss: 0.3632164
[Epoch 86] ogbg-molsider: 0.651546 val loss: 0.584515
[Epoch 86] ogbg-molsider: 0.634814 test loss: 0.604084
[Epoch 87; Iter     8/   32] train: loss: 0.3616341
[Epoch 87] ogbg-molsider: 0.631302 val loss: 0.552789
[Epoch 87] ogbg-molsider: 0.634911 test loss: 0.572103
[Epoch 88; Iter     6/   32] train: loss: 0.3325417
[Epoch 88] ogbg-molsider: 0.644590 val loss: 0.562861
[Epoch 88] ogbg-molsider: 0.642717 test loss: 0.570038
[Epoch 89; Iter     4/   32] train: loss: 0.3682873
[Epoch 89] ogbg-molsider: 0.636647 val loss: 0.561482
[Epoch 89] ogbg-molsider: 0.633862 test loss: 0.580021
[Epoch 90; Iter     2/   32] train: loss: 0.3472418
[Epoch 90; Iter    32/   32] train: loss: 0.5783037
[Epoch 90] ogbg-molsider: 0.639964 val loss: 0.566909
[Epoch 90] ogbg-molsider: 0.642357 test loss: 0.587404
[Epoch 91; Iter    30/   32] train: loss: 0.3070329
[Epoch 91] ogbg-molsider: 0.630188 val loss: 0.593643
[Epoch 91] ogbg-molsider: 0.635544 test loss: 0.592053
[Epoch 92; Iter    28/   32] train: loss: 0.3333815
[Epoch 92] ogbg-molsider: 0.647422 val loss: 0.558966
[Epoch 92] ogbg-molsider: 0.648120 test loss: 0.581065
[Epoch 93; Iter    26/   32] train: loss: 0.3194620
[Epoch 93] ogbg-molsider: 0.640004 val loss: 0.567102
[Epoch 93] ogbg-molsider: 0.642139 test loss: 0.564988
[Epoch 94; Iter    24/   32] train: loss: 0.3393690
[Epoch 94] ogbg-molsider: 0.644934 val loss: 0.563950
[Epoch 94] ogbg-molsider: 0.633955 test loss: 0.590157
[Epoch 95; Iter    22/   32] train: loss: 0.3288944
[Epoch 95] ogbg-molsider: 0.638459 val loss: 0.590938
[Epoch 95] ogbg-molsider: 0.631288 test loss: 0.632608
[Epoch 96; Iter    20/   32] train: loss: 0.3385639
[Epoch 96] ogbg-molsider: 0.656243 val loss: 0.564653
[Epoch 96] ogbg-molsider: 0.644028 test loss: 0.588466
[Epoch 97; Iter    18/   32] train: loss: 0.3203735
[Epoch 97] ogbg-molsider: 0.641818 val loss: 0.593678
[Epoch 97] ogbg-molsider: 0.650411 test loss: 0.570607
[Epoch 98; Iter    16/   32] train: loss: 0.2944680
[Epoch 98] ogbg-molsider: 0.651829 val loss: 0.571609
[Epoch 98] ogbg-molsider: 0.647357 test loss: 0.591328
[Epoch 99; Iter    14/   32] train: loss: 0.3515719
[Epoch 99] ogbg-molsider: 0.651011 val loss: 0.569021
[Epoch 99] ogbg-molsider: 0.654746 test loss: 0.575697
[Epoch 100; Iter    12/   32] train: loss: 0.2918798
[Epoch 100] ogbg-molsider: 0.648571 val loss: 0.575175
[Epoch 100] ogbg-molsider: 0.639118 test loss: 0.584182
[Epoch 101; Iter    10/   32] train: loss: 0.3263803
[Epoch 101] ogbg-molsider: 0.659015 val loss: 0.587822
[Epoch 101] ogbg-molsider: 0.655042 test loss: 0.603247
[Epoch 102; Iter     8/   32] train: loss: 0.3023612
[Epoch 102] ogbg-molsider: 0.653063 val loss: 0.583209
[Epoch 102] ogbg-molsider: 0.648765 test loss: 0.600574
[Epoch 103; Iter     6/   32] train: loss: 0.2876963
[Epoch 103] ogbg-molsider: 0.649672 val loss: 0.576754
[Epoch 103] ogbg-molsider: 0.641223 test loss: 0.617274
[Epoch 104; Iter     4/   32] train: loss: 0.3014946
[Epoch 104] ogbg-molsider: 0.653956 val loss: 0.568909
[Epoch 104] ogbg-molsider: 0.633387 test loss: 0.599324
[Epoch 105; Iter     2/   32] train: loss: 0.3132382
[Epoch 105; Iter    32/   32] train: loss: 0.3473110
[Epoch 105] ogbg-molsider: 0.663332 val loss: 0.548723
[Epoch 105] ogbg-molsider: 0.648735 test loss: 0.573184
[Epoch 106; Iter    30/   32] train: loss: 0.3268382
[Epoch 106] ogbg-molsider: 0.648109 val loss: 0.569420
[Epoch 106] ogbg-molsider: 0.637615 test loss: 0.595710
[Epoch 107; Iter    28/   32] train: loss: 0.2810628
[Epoch 107] ogbg-molsider: 0.654778 val loss: 0.573744
[Epoch 107] ogbg-molsider: 0.654174 test loss: 0.583444
[Epoch 108; Iter    26/   32] train: loss: 0.3082049
[Epoch 108] ogbg-molsider: 0.656616 val loss: 0.573832
[Epoch 108] ogbg-molsider: 0.642989 test loss: 0.608075
[Epoch 109; Iter    24/   32] train: loss: 0.2860110
[Epoch 109] ogbg-molsider: 0.657105 val loss: 0.569531
[Epoch 109] ogbg-molsider: 0.638786 test loss: 0.598480
[Epoch 110; Iter    22/   32] train: loss: 0.2846914
[Epoch 110] ogbg-molsider: 0.656245 val loss: 0.574268
[Epoch 110] ogbg-molsider: 0.648964 test loss: 0.585058
[Epoch 111; Iter    20/   32] train: loss: 0.2805479
[Epoch 111] ogbg-molsider: 0.657595 val loss: 0.579711
[Epoch 111] ogbg-molsider: 0.649036 test loss: 0.607916
[Epoch 112; Iter    18/   32] train: loss: 0.2586315
[Epoch 112] ogbg-molsider: 0.662169 val loss: 0.584855
[Epoch 112] ogbg-molsider: 0.642949 test loss: 0.612015
[Epoch 113; Iter    16/   32] train: loss: 0.2701486
[Epoch 113] ogbg-molsider: 0.661035 val loss: 0.578443
[Epoch 113] ogbg-molsider: 0.649259 test loss: 0.604934
[Epoch 114; Iter    14/   32] train: loss: 0.2496658
[Epoch 114] ogbg-molsider: 0.659212 val loss: 0.575450
[Epoch 114] ogbg-molsider: 0.644431 test loss: 0.600814
[Epoch 115; Iter    12/   32] train: loss: 0.2725117
[Epoch 115] ogbg-molsider: 0.658645 val loss: 0.589412
[Epoch 115] ogbg-molsider: 0.639640 test loss: 0.623050
[Epoch 116; Iter    10/   32] train: loss: 0.2755010
[Epoch 116] ogbg-molsider: 0.658818 val loss: 0.585736
[Epoch 116] ogbg-molsider: 0.647411 test loss: 0.613408
[Epoch 117; Iter     8/   32] train: loss: 0.2740405
[Epoch 117] ogbg-molsider: 0.660074 val loss: 0.586098
[Epoch 117] ogbg-molsider: 0.651065 test loss: 0.596324
[Epoch 118; Iter     6/   32] train: loss: 0.2721979
[Epoch 118] ogbg-molsider: 0.656795 val loss: 0.599504
[Epoch 118] ogbg-molsider: 0.648501 test loss: 0.625792
[Epoch 119; Iter     4/   32] train: loss: 0.2485947
[Epoch 119] ogbg-molsider: 0.653396 val loss: 0.605320
[Epoch 119] ogbg-molsider: 0.647558 test loss: 0.618871
[Epoch 120; Iter     2/   32] train: loss: 0.2731226
[Epoch 120; Iter    32/   32] train: loss: 0.3607578
[Epoch 120] ogbg-molsider: 0.659952 val loss: 0.596620
[Epoch 120] ogbg-molsider: 0.649739 test loss: 0.614694
[Epoch 121; Iter    30/   32] train: loss: 0.2579470
[Epoch 121] ogbg-molsider: 0.647458 val loss: 0.604711
[Epoch 121] ogbg-molsider: 0.644163 test loss: 0.611150
[Epoch 122; Iter    28/   32] train: loss: 0.2658392
[Epoch 122] ogbg-molsider: 0.656033 val loss: 0.588314
[Epoch 122] ogbg-molsider: 0.645389 test loss: 0.600823
Early stopping criterion based on -ogbg-molsider- that should be max reached after 122 epochs. Best model checkpoint was in epoch 62.
Statistics on  val_best_checkpoint
mean_pred: 0.41310060024261475
std_pred: 1.8904720544815063
mean_targets: 0.5732086896896362
std_targets: 0.49465426802635193
prcauc: 0.6699494004920924
rocauc: 0.667233055335828
ogbg-molsider: 0.667233055335828
OGBNanLabelBCEWithLogitsLoss: 0.4878819116524288
Statistics on  test
mean_pred: 0.5285083055496216
std_pred: 1.9662435054779053
mean_targets: 0.5714039206504822
std_targets: 0.4949178397655487
prcauc: 0.6524057378276003
rocauc: 0.618041767738327
ogbg-molsider: 0.618041767738327
OGBNanLabelBCEWithLogitsLoss: 0.5124875051634652
Statistics on  train
mean_pred: 0.4866218864917755
std_pred: 1.9591799974441528
mean_targets: 0.5655384659767151
std_targets: 0.4956952929496765
prcauc: 0.7549032964502523
rocauc: 0.7865909137092089
ogbg-molsider: 0.7865909137092089
OGBNanLabelBCEWithLogitsLoss: 0.4266449948772788
[Epoch 84] ogbg-molsider: 0.583512 test loss: 0.532481
[Epoch 85; Iter    12/   32] train: loss: 0.5076190
[Epoch 85] ogbg-molsider: 0.628868 val loss: 0.502617
[Epoch 85] ogbg-molsider: 0.589999 test loss: 0.520732
[Epoch 86; Iter    10/   32] train: loss: 0.4630791
[Epoch 86] ogbg-molsider: 0.632617 val loss: 0.502157
[Epoch 86] ogbg-molsider: 0.604970 test loss: 0.510340
[Epoch 87; Iter     8/   32] train: loss: 0.4708306
[Epoch 87] ogbg-molsider: 0.630920 val loss: 0.503016
[Epoch 87] ogbg-molsider: 0.613769 test loss: 0.506879
[Epoch 88; Iter     6/   32] train: loss: 0.4627176
[Epoch 88] ogbg-molsider: 0.635529 val loss: 0.501134
[Epoch 88] ogbg-molsider: 0.607488 test loss: 0.515120
[Epoch 89; Iter     4/   32] train: loss: 0.4552247
[Epoch 89] ogbg-molsider: 0.632086 val loss: 0.501067
[Epoch 89] ogbg-molsider: 0.601700 test loss: 0.513697
[Epoch 90; Iter     2/   32] train: loss: 0.4640972
[Epoch 90; Iter    32/   32] train: loss: 0.4911616
[Epoch 90] ogbg-molsider: 0.622118 val loss: 0.514788
[Epoch 90] ogbg-molsider: 0.621966 test loss: 0.511449
[Epoch 91; Iter    30/   32] train: loss: 0.4768721
[Epoch 91] ogbg-molsider: 0.639245 val loss: 0.509304
[Epoch 91] ogbg-molsider: 0.594849 test loss: 0.522285
[Epoch 92; Iter    28/   32] train: loss: 0.4943735
[Epoch 92] ogbg-molsider: 0.630316 val loss: 0.524449
[Epoch 92] ogbg-molsider: 0.606698 test loss: 0.522405
[Epoch 93; Iter    26/   32] train: loss: 0.4883958
[Epoch 93] ogbg-molsider: 0.616114 val loss: 0.596688
[Epoch 93] ogbg-molsider: 0.572773 test loss: 0.575194
[Epoch 94; Iter    24/   32] train: loss: 0.4993614
[Epoch 94] ogbg-molsider: 0.638103 val loss: 0.502593
[Epoch 94] ogbg-molsider: 0.595576 test loss: 0.518652
[Epoch 95; Iter    22/   32] train: loss: 0.4393217
[Epoch 95] ogbg-molsider: 0.627398 val loss: 0.521995
[Epoch 95] ogbg-molsider: 0.615917 test loss: 0.517050
[Epoch 96; Iter    20/   32] train: loss: 0.5231326
[Epoch 96] ogbg-molsider: 0.648536 val loss: 0.506657
[Epoch 96] ogbg-molsider: 0.631504 test loss: 0.508840
[Epoch 97; Iter    18/   32] train: loss: 0.4526974
[Epoch 97] ogbg-molsider: 0.621855 val loss: 0.523013
[Epoch 97] ogbg-molsider: 0.642220 test loss: 0.509541
[Epoch 98; Iter    16/   32] train: loss: 0.4406732
[Epoch 98] ogbg-molsider: 0.641818 val loss: 0.507366
[Epoch 98] ogbg-molsider: 0.625444 test loss: 0.508252
[Epoch 99; Iter    14/   32] train: loss: 0.4779710
[Epoch 99] ogbg-molsider: 0.644563 val loss: 0.507604
[Epoch 99] ogbg-molsider: 0.632315 test loss: 0.512773
[Epoch 100; Iter    12/   32] train: loss: 0.4315135
[Epoch 100] ogbg-molsider: 0.631829 val loss: 0.522352
[Epoch 100] ogbg-molsider: 0.634686 test loss: 0.510963
[Epoch 101; Iter    10/   32] train: loss: 0.4425122
[Epoch 101] ogbg-molsider: 0.643190 val loss: 0.513474
[Epoch 101] ogbg-molsider: 0.627649 test loss: 0.525492
[Epoch 102; Iter     8/   32] train: loss: 0.4838830
[Epoch 102] ogbg-molsider: 0.634614 val loss: 0.515415
[Epoch 102] ogbg-molsider: 0.611513 test loss: 0.547459
[Epoch 103; Iter     6/   32] train: loss: 0.4310472
[Epoch 103] ogbg-molsider: 0.636688 val loss: 0.520592
[Epoch 103] ogbg-molsider: 0.634378 test loss: 0.518812
[Epoch 104; Iter     4/   32] train: loss: 0.4159294
[Epoch 104] ogbg-molsider: 0.628272 val loss: 0.518857
[Epoch 104] ogbg-molsider: 0.598698 test loss: 0.533545
[Epoch 105; Iter     2/   32] train: loss: 0.4210906
[Epoch 105; Iter    32/   32] train: loss: 0.4500786
[Epoch 105] ogbg-molsider: 0.626225 val loss: 0.530461
[Epoch 105] ogbg-molsider: 0.621525 test loss: 0.521136
[Epoch 106; Iter    30/   32] train: loss: 0.4733891
[Epoch 106] ogbg-molsider: 0.621781 val loss: 0.599631
[Epoch 106] ogbg-molsider: 0.624751 test loss: 0.567688
[Epoch 107; Iter    28/   32] train: loss: 0.4303831
[Epoch 107] ogbg-molsider: 0.624821 val loss: 0.518477
[Epoch 107] ogbg-molsider: 0.622635 test loss: 0.521832
[Epoch 108; Iter    26/   32] train: loss: 0.4339173
[Epoch 108] ogbg-molsider: 0.647487 val loss: 0.533002
[Epoch 108] ogbg-molsider: 0.642893 test loss: 0.521695
[Epoch 109; Iter    24/   32] train: loss: 0.4446743
[Epoch 109] ogbg-molsider: 0.633019 val loss: 0.529484
[Epoch 109] ogbg-molsider: 0.617658 test loss: 0.543423
[Epoch 110; Iter    22/   32] train: loss: 0.4858089
[Epoch 110] ogbg-molsider: 0.630550 val loss: 0.526725
[Epoch 110] ogbg-molsider: 0.622347 test loss: 0.532065
[Epoch 111; Iter    20/   32] train: loss: 0.4067432
[Epoch 111] ogbg-molsider: 0.643568 val loss: 0.525931
[Epoch 111] ogbg-molsider: 0.626380 test loss: 0.551623
[Epoch 112; Iter    18/   32] train: loss: 0.4464392
[Epoch 112] ogbg-molsider: 0.632019 val loss: 0.523290
[Epoch 112] ogbg-molsider: 0.631119 test loss: 0.573655
[Epoch 113; Iter    16/   32] train: loss: 0.4270145
[Epoch 113] ogbg-molsider: 0.631802 val loss: 0.517605
[Epoch 113] ogbg-molsider: 0.641807 test loss: 0.513651
[Epoch 114; Iter    14/   32] train: loss: 0.3861755
[Epoch 114] ogbg-molsider: 0.633579 val loss: 0.541935
[Epoch 114] ogbg-molsider: 0.620602 test loss: 0.544449
[Epoch 115; Iter    12/   32] train: loss: 0.3870892
[Epoch 115] ogbg-molsider: 0.634382 val loss: 0.524934
[Epoch 115] ogbg-molsider: 0.629807 test loss: 0.525944
[Epoch 116; Iter    10/   32] train: loss: 0.3869833
[Epoch 116] ogbg-molsider: 0.637423 val loss: 0.528007
[Epoch 116] ogbg-molsider: 0.635087 test loss: 0.526090
[Epoch 117; Iter     8/   32] train: loss: 0.4054610
[Epoch 117] ogbg-molsider: 0.642156 val loss: 0.527019
[Epoch 117] ogbg-molsider: 0.619192 test loss: 0.653125
[Epoch 118; Iter     6/   32] train: loss: 0.4196603
[Epoch 118] ogbg-molsider: 0.628270 val loss: 0.542751
[Epoch 118] ogbg-molsider: 0.635516 test loss: 0.574688
[Epoch 119; Iter     4/   32] train: loss: 0.4154568
[Epoch 119] ogbg-molsider: 0.632429 val loss: 0.529126
[Epoch 119] ogbg-molsider: 0.628048 test loss: 0.536980
[Epoch 120; Iter     2/   32] train: loss: 0.3527389
[Epoch 120; Iter    32/   32] train: loss: 0.4099907
[Epoch 120] ogbg-molsider: 0.637375 val loss: 0.536878
[Epoch 120] ogbg-molsider: 0.608633 test loss: 0.905304
[Epoch 121; Iter    30/   32] train: loss: 0.3851258
[Epoch 121] ogbg-molsider: 0.632876 val loss: 0.528571
[Epoch 121] ogbg-molsider: 0.637589 test loss: 0.534688
[Epoch 122; Iter    28/   32] train: loss: 0.3630366
[Epoch 122] ogbg-molsider: 0.632471 val loss: 0.546411
[Epoch 122] ogbg-molsider: 0.637333 test loss: 0.565526
[Epoch 123; Iter    26/   32] train: loss: 0.3346145
[Epoch 123] ogbg-molsider: 0.626609 val loss: 0.540961
[Epoch 123] ogbg-molsider: 0.630830 test loss: 0.554365
[Epoch 124; Iter    24/   32] train: loss: 0.4026777
[Epoch 124] ogbg-molsider: 0.634700 val loss: 0.536147
[Epoch 124] ogbg-molsider: 0.640621 test loss: 0.790443
[Epoch 125; Iter    22/   32] train: loss: 0.3888932
[Epoch 125] ogbg-molsider: 0.618755 val loss: 0.547220
[Epoch 125] ogbg-molsider: 0.640001 test loss: 0.536353
[Epoch 126; Iter    20/   32] train: loss: 0.3651015
[Epoch 126] ogbg-molsider: 0.629497 val loss: 0.541137
[Epoch 126] ogbg-molsider: 0.634770 test loss: 0.543900
[Epoch 127; Iter    18/   32] train: loss: 0.3725868
[Epoch 127] ogbg-molsider: 0.626322 val loss: 0.555339
[Epoch 127] ogbg-molsider: 0.642704 test loss: 0.549164
[Epoch 128; Iter    16/   32] train: loss: 0.3680953
[Epoch 128] ogbg-molsider: 0.634745 val loss: 0.549527
[Epoch 128] ogbg-molsider: 0.646288 test loss: 0.536879
[Epoch 129; Iter    14/   32] train: loss: 0.3631577
[Epoch 129] ogbg-molsider: 0.633727 val loss: 0.537094
[Epoch 129] ogbg-molsider: 0.619499 test loss: 0.547727
[Epoch 130; Iter    12/   32] train: loss: 0.4167602
[Epoch 130] ogbg-molsider: 0.640453 val loss: 0.547723
[Epoch 130] ogbg-molsider: 0.646493 test loss: 0.532094
[Epoch 131; Iter    10/   32] train: loss: 0.3893117
[Epoch 131] ogbg-molsider: 0.641849 val loss: 0.536121
[Epoch 131] ogbg-molsider: 0.642783 test loss: 0.525652
[Epoch 132; Iter     8/   32] train: loss: 0.3997989
[Epoch 132] ogbg-molsider: 0.630159 val loss: 0.544545
[Epoch 132] ogbg-molsider: 0.633622 test loss: 0.643776
[Epoch 133; Iter     6/   32] train: loss: 0.3657098
[Epoch 133] ogbg-molsider: 0.640833 val loss: 0.565690
[Epoch 81; Iter    30/   36] train: loss: 0.4622156
[Epoch 81] ogbg-molsider: 0.614230 val loss: 0.531824
[Epoch 81] ogbg-molsider: 0.651317 test loss: 0.530019
[Epoch 82; Iter    24/   36] train: loss: 0.4113887
[Epoch 82] ogbg-molsider: 0.643153 val loss: 0.523007
[Epoch 82] ogbg-molsider: 0.637054 test loss: 0.573739
[Epoch 83; Iter    18/   36] train: loss: 0.4230398
[Epoch 83] ogbg-molsider: 0.646887 val loss: 0.499184
[Epoch 83] ogbg-molsider: 0.640601 test loss: 0.538013
[Epoch 84; Iter    12/   36] train: loss: 0.3784636
[Epoch 84] ogbg-molsider: 0.644616 val loss: 0.501177
[Epoch 84] ogbg-molsider: 0.656546 test loss: 0.532265
[Epoch 85; Iter     6/   36] train: loss: 0.3563778
[Epoch 85; Iter    36/   36] train: loss: 0.3958684
[Epoch 85] ogbg-molsider: 0.632860 val loss: 0.516977
[Epoch 85] ogbg-molsider: 0.653583 test loss: 0.535769
[Epoch 86; Iter    30/   36] train: loss: 0.4150921
[Epoch 86] ogbg-molsider: 0.635856 val loss: 0.527004
[Epoch 86] ogbg-molsider: 0.653095 test loss: 0.554551
[Epoch 87; Iter    24/   36] train: loss: 0.4036314
[Epoch 87] ogbg-molsider: 0.641634 val loss: 0.510071
[Epoch 87] ogbg-molsider: 0.636687 test loss: 0.597419
[Epoch 88; Iter    18/   36] train: loss: 0.3852476
[Epoch 88] ogbg-molsider: 0.654356 val loss: 0.524866
[Epoch 88] ogbg-molsider: 0.639802 test loss: 0.543324
[Epoch 89; Iter    12/   36] train: loss: 0.3844610
[Epoch 89] ogbg-molsider: 0.638971 val loss: 0.532638
[Epoch 89] ogbg-molsider: 0.665319 test loss: 0.549050
[Epoch 90; Iter     6/   36] train: loss: 0.3882219
[Epoch 90; Iter    36/   36] train: loss: 0.4000262
[Epoch 90] ogbg-molsider: 0.657679 val loss: 0.512760
[Epoch 90] ogbg-molsider: 0.656393 test loss: 0.548145
[Epoch 91; Iter    30/   36] train: loss: 0.3373488
[Epoch 91] ogbg-molsider: 0.654979 val loss: 0.532111
[Epoch 91] ogbg-molsider: 0.644788 test loss: 0.603093
[Epoch 92; Iter    24/   36] train: loss: 0.4113976
[Epoch 92] ogbg-molsider: 0.641307 val loss: 0.537405
[Epoch 92] ogbg-molsider: 0.667980 test loss: 0.532431
[Epoch 93; Iter    18/   36] train: loss: 0.3365321
[Epoch 93] ogbg-molsider: 0.644731 val loss: 0.532063
[Epoch 93] ogbg-molsider: 0.658624 test loss: 0.543009
[Epoch 94; Iter    12/   36] train: loss: 0.3401852
[Epoch 94] ogbg-molsider: 0.647346 val loss: 0.532618
[Epoch 94] ogbg-molsider: 0.659838 test loss: 0.553521
[Epoch 95; Iter     6/   36] train: loss: 0.3148662
[Epoch 95; Iter    36/   36] train: loss: 0.3693199
[Epoch 95] ogbg-molsider: 0.650270 val loss: 0.532344
[Epoch 95] ogbg-molsider: 0.662020 test loss: 0.550576
[Epoch 96; Iter    30/   36] train: loss: 0.3346219
[Epoch 96] ogbg-molsider: 0.653379 val loss: 0.530903
[Epoch 96] ogbg-molsider: 0.660233 test loss: 0.558257
[Epoch 97; Iter    24/   36] train: loss: 0.3156264
[Epoch 97] ogbg-molsider: 0.646970 val loss: 0.539753
[Epoch 97] ogbg-molsider: 0.648942 test loss: 0.574831
[Epoch 98; Iter    18/   36] train: loss: 0.2963413
[Epoch 98] ogbg-molsider: 0.650144 val loss: 0.543993
[Epoch 98] ogbg-molsider: 0.660753 test loss: 0.575960
[Epoch 99; Iter    12/   36] train: loss: 0.3277089
[Epoch 99] ogbg-molsider: 0.657979 val loss: 0.551279
[Epoch 99] ogbg-molsider: 0.663172 test loss: 0.567224
[Epoch 100; Iter     6/   36] train: loss: 0.2854657
[Epoch 100; Iter    36/   36] train: loss: 0.3313254
[Epoch 100] ogbg-molsider: 0.649431 val loss: 0.549646
[Epoch 100] ogbg-molsider: 0.657284 test loss: 0.591110
[Epoch 101; Iter    30/   36] train: loss: 0.3310508
[Epoch 101] ogbg-molsider: 0.639800 val loss: 0.557108
[Epoch 101] ogbg-molsider: 0.651438 test loss: 0.596511
[Epoch 102; Iter    24/   36] train: loss: 0.2949778
[Epoch 102] ogbg-molsider: 0.640450 val loss: 0.563045
[Epoch 102] ogbg-molsider: 0.648644 test loss: 0.572736
[Epoch 103; Iter    18/   36] train: loss: 0.3801676
[Epoch 103] ogbg-molsider: 0.647492 val loss: 0.544370
[Epoch 103] ogbg-molsider: 0.671981 test loss: 0.545066
[Epoch 104; Iter    12/   36] train: loss: 0.3016018
[Epoch 104] ogbg-molsider: 0.639485 val loss: 0.568376
[Epoch 104] ogbg-molsider: 0.644653 test loss: 0.598445
[Epoch 105; Iter     6/   36] train: loss: 0.3388419
[Epoch 105; Iter    36/   36] train: loss: 0.3470339
[Epoch 105] ogbg-molsider: 0.645895 val loss: 0.542401
[Epoch 105] ogbg-molsider: 0.650108 test loss: 0.593201
[Epoch 106; Iter    30/   36] train: loss: 0.2802673
[Epoch 106] ogbg-molsider: 0.659140 val loss: 0.549559
[Epoch 106] ogbg-molsider: 0.655743 test loss: 0.598243
[Epoch 107; Iter    24/   36] train: loss: 0.2909347
[Epoch 107] ogbg-molsider: 0.647631 val loss: 0.544517
[Epoch 107] ogbg-molsider: 0.669582 test loss: 0.565272
[Epoch 108; Iter    18/   36] train: loss: 0.3528068
[Epoch 108] ogbg-molsider: 0.613633 val loss: 0.653060
[Epoch 108] ogbg-molsider: 0.658557 test loss: 0.590787
[Epoch 109; Iter    12/   36] train: loss: 0.2781776
[Epoch 109] ogbg-molsider: 0.638231 val loss: 0.581872
[Epoch 109] ogbg-molsider: 0.651767 test loss: 0.624582
[Epoch 110; Iter     6/   36] train: loss: 0.3308008
[Epoch 110; Iter    36/   36] train: loss: 0.3188877
[Epoch 110] ogbg-molsider: 0.630632 val loss: 0.604106
[Epoch 110] ogbg-molsider: 0.656042 test loss: 0.630362
[Epoch 111; Iter    30/   36] train: loss: 0.3428614
[Epoch 111] ogbg-molsider: 0.644953 val loss: 0.568860
[Epoch 111] ogbg-molsider: 0.671552 test loss: 0.584519
[Epoch 112; Iter    24/   36] train: loss: 0.2814086
[Epoch 112] ogbg-molsider: 0.643925 val loss: 0.560543
[Epoch 112] ogbg-molsider: 0.665225 test loss: 0.581305
[Epoch 113; Iter    18/   36] train: loss: 0.2956525
[Epoch 113] ogbg-molsider: 0.628575 val loss: 0.612003
[Epoch 113] ogbg-molsider: 0.647906 test loss: 0.647477
[Epoch 114; Iter    12/   36] train: loss: 0.3086517
[Epoch 114] ogbg-molsider: 0.629376 val loss: 0.593792
[Epoch 114] ogbg-molsider: 0.660444 test loss: 0.598548
[Epoch 115; Iter     6/   36] train: loss: 0.2789659
[Epoch 115; Iter    36/   36] train: loss: 0.2918583
[Epoch 115] ogbg-molsider: 0.638516 val loss: 0.579447
[Epoch 115] ogbg-molsider: 0.654798 test loss: 0.628212
[Epoch 116; Iter    30/   36] train: loss: 0.2869697
[Epoch 116] ogbg-molsider: 0.636686 val loss: 0.580879
[Epoch 116] ogbg-molsider: 0.660287 test loss: 0.615952
[Epoch 117; Iter    24/   36] train: loss: 0.3000971
[Epoch 117] ogbg-molsider: 0.624778 val loss: 0.611395
[Epoch 117] ogbg-molsider: 0.669362 test loss: 0.614858
[Epoch 118; Iter    18/   36] train: loss: 0.2810432
[Epoch 118] ogbg-molsider: 0.638144 val loss: 0.590015
[Epoch 118] ogbg-molsider: 0.666119 test loss: 0.601938
[Epoch 119; Iter    12/   36] train: loss: 0.2883445
[Epoch 119] ogbg-molsider: 0.643124 val loss: 0.582825
[Epoch 119] ogbg-molsider: 0.658354 test loss: 0.613701
[Epoch 120; Iter     6/   36] train: loss: 0.2724946
[Epoch 120; Iter    36/   36] train: loss: 0.3028736
[Epoch 120] ogbg-molsider: 0.637106 val loss: 0.600184
[Epoch 120] ogbg-molsider: 0.661091 test loss: 0.618759
[Epoch 121; Iter    30/   36] train: loss: 0.2399551
[Epoch 121] ogbg-molsider: 0.636342 val loss: 0.605234
[Epoch 121] ogbg-molsider: 0.656440 test loss: 0.622558
[Epoch 122; Iter    24/   36] train: loss: 0.2690480
[Epoch 122] ogbg-molsider: 0.637053 val loss: 0.603097
[Epoch 122] ogbg-molsider: 0.659406 test loss: 0.614184
[Epoch 123; Iter    18/   36] train: loss: 0.2629520
[Epoch 123] ogbg-molsider: 0.640530 val loss: 0.634257
[Epoch 123] ogbg-molsider: 0.664938 test loss: 0.633762
[Epoch 124; Iter    12/   36] train: loss: 0.2749296
[Epoch 124] ogbg-molsider: 0.631419 val loss: 0.610960
[Epoch 124] ogbg-molsider: 0.656926 test loss: 0.619867
[Epoch 125; Iter     6/   36] train: loss: 0.3035040
[Epoch 125; Iter    36/   36] train: loss: 0.2846136
[Epoch 125] ogbg-molsider: 0.644031 val loss: 0.619815
[Epoch 125] ogbg-molsider: 0.654337 test loss: 0.679234
[Epoch 126; Iter    30/   36] train: loss: 0.2501715
[Epoch 126] ogbg-molsider: 0.633723 val loss: 0.610949
[Epoch 126] ogbg-molsider: 0.656698 test loss: 0.631570
[Epoch 127; Iter    24/   36] train: loss: 0.2613648
[Epoch 127] ogbg-molsider: 0.637604 val loss: 0.601227
[Epoch 127] ogbg-molsider: 0.652644 test loss: 0.634367
[Epoch 128; Iter    18/   36] train: loss: 0.2344510
[Epoch 84] ogbg-molsider: 0.649688 test loss: 0.538240
[Epoch 85; Iter    12/   32] train: loss: 0.3713159
[Epoch 85] ogbg-molsider: 0.626570 val loss: 0.590790
[Epoch 85] ogbg-molsider: 0.647667 test loss: 0.574941
[Epoch 86; Iter    10/   32] train: loss: 0.3660449
[Epoch 86] ogbg-molsider: 0.644756 val loss: 0.532071
[Epoch 86] ogbg-molsider: 0.654374 test loss: 0.524037
[Epoch 87; Iter     8/   32] train: loss: 0.3699572
[Epoch 87] ogbg-molsider: 0.645603 val loss: 0.554281
[Epoch 87] ogbg-molsider: 0.649777 test loss: 0.552578
[Epoch 88; Iter     6/   32] train: loss: 0.3477368
[Epoch 88] ogbg-molsider: 0.648462 val loss: 0.551355
[Epoch 88] ogbg-molsider: 0.650843 test loss: 0.541818
[Epoch 89; Iter     4/   32] train: loss: 0.3156447
[Epoch 89] ogbg-molsider: 0.640217 val loss: 0.633943
[Epoch 89] ogbg-molsider: 0.640972 test loss: 0.570816
[Epoch 90; Iter     2/   32] train: loss: 0.3374211
[Epoch 90; Iter    32/   32] train: loss: 0.4076802
[Epoch 90] ogbg-molsider: 0.631885 val loss: 0.577604
[Epoch 90] ogbg-molsider: 0.651602 test loss: 0.552176
[Epoch 91; Iter    30/   32] train: loss: 0.3814235
[Epoch 91] ogbg-molsider: 0.636278 val loss: 0.572270
[Epoch 91] ogbg-molsider: 0.656888 test loss: 0.529886
[Epoch 92; Iter    28/   32] train: loss: 0.3541727
[Epoch 92] ogbg-molsider: 0.642522 val loss: 0.569655
[Epoch 92] ogbg-molsider: 0.655390 test loss: 0.564857
[Epoch 93; Iter    26/   32] train: loss: 0.3391564
[Epoch 93] ogbg-molsider: 0.640365 val loss: 0.550303
[Epoch 93] ogbg-molsider: 0.653303 test loss: 0.544748
[Epoch 94; Iter    24/   32] train: loss: 0.3382396
[Epoch 94] ogbg-molsider: 0.616965 val loss: 0.574865
[Epoch 94] ogbg-molsider: 0.643220 test loss: 0.577458
[Epoch 95; Iter    22/   32] train: loss: 0.3924000
[Epoch 95] ogbg-molsider: 0.641030 val loss: 0.568278
[Epoch 95] ogbg-molsider: 0.648857 test loss: 0.555839
[Epoch 96; Iter    20/   32] train: loss: 0.3070124
[Epoch 96] ogbg-molsider: 0.629379 val loss: 0.614113
[Epoch 96] ogbg-molsider: 0.643620 test loss: 0.586401
[Epoch 97; Iter    18/   32] train: loss: 0.3916387
[Epoch 97] ogbg-molsider: 0.652619 val loss: 0.605760
[Epoch 97] ogbg-molsider: 0.658346 test loss: 0.564584
[Epoch 98; Iter    16/   32] train: loss: 0.3616987
[Epoch 98] ogbg-molsider: 0.637151 val loss: 0.596948
[Epoch 98] ogbg-molsider: 0.649565 test loss: 0.566486
[Epoch 99; Iter    14/   32] train: loss: 0.3294716
[Epoch 99] ogbg-molsider: 0.631937 val loss: 0.582942
[Epoch 99] ogbg-molsider: 0.666365 test loss: 0.552640
[Epoch 100; Iter    12/   32] train: loss: 0.3637008
[Epoch 100] ogbg-molsider: 0.644775 val loss: 0.588346
[Epoch 100] ogbg-molsider: 0.656653 test loss: 0.566302
[Epoch 101; Iter    10/   32] train: loss: 0.3044084
[Epoch 101] ogbg-molsider: 0.641763 val loss: 0.588591
[Epoch 101] ogbg-molsider: 0.657193 test loss: 0.563954
[Epoch 102; Iter     8/   32] train: loss: 0.2970769
[Epoch 102] ogbg-molsider: 0.648710 val loss: 0.588003
[Epoch 102] ogbg-molsider: 0.671418 test loss: 0.550796
[Epoch 103; Iter     6/   32] train: loss: 0.3305671
[Epoch 103] ogbg-molsider: 0.650275 val loss: 0.577879
[Epoch 103] ogbg-molsider: 0.663886 test loss: 0.559728
[Epoch 104; Iter     4/   32] train: loss: 0.2781036
[Epoch 104] ogbg-molsider: 0.650548 val loss: 0.577259
[Epoch 104] ogbg-molsider: 0.664753 test loss: 0.565710
[Epoch 105; Iter     2/   32] train: loss: 0.3158461
[Epoch 105; Iter    32/   32] train: loss: 0.4753862
[Epoch 105] ogbg-molsider: 0.646986 val loss: 0.578041
[Epoch 105] ogbg-molsider: 0.663179 test loss: 0.559751
[Epoch 106; Iter    30/   32] train: loss: 0.3070768
[Epoch 106] ogbg-molsider: 0.646867 val loss: 0.596624
[Epoch 106] ogbg-molsider: 0.658238 test loss: 0.568137
[Epoch 107; Iter    28/   32] train: loss: 0.2686779
[Epoch 107] ogbg-molsider: 0.644265 val loss: 0.628683
[Epoch 107] ogbg-molsider: 0.661624 test loss: 0.571981
[Epoch 108; Iter    26/   32] train: loss: 0.2820641
[Epoch 108] ogbg-molsider: 0.636051 val loss: 0.591355
[Epoch 108] ogbg-molsider: 0.650260 test loss: 0.588819
[Epoch 109; Iter    24/   32] train: loss: 0.2786715
[Epoch 109] ogbg-molsider: 0.641770 val loss: 0.595180
[Epoch 109] ogbg-molsider: 0.659961 test loss: 0.574259
[Epoch 110; Iter    22/   32] train: loss: 0.2706663
[Epoch 110] ogbg-molsider: 0.638991 val loss: 0.613042
[Epoch 110] ogbg-molsider: 0.650377 test loss: 0.609557
[Epoch 111; Iter    20/   32] train: loss: 0.2843802
[Epoch 111] ogbg-molsider: 0.638770 val loss: 0.597606
[Epoch 111] ogbg-molsider: 0.655854 test loss: 0.589058
[Epoch 112; Iter    18/   32] train: loss: 0.2994418
[Epoch 112] ogbg-molsider: 0.652327 val loss: 0.602915
[Epoch 112] ogbg-molsider: 0.673445 test loss: 0.581354
[Epoch 113; Iter    16/   32] train: loss: 0.3286427
[Epoch 113] ogbg-molsider: 0.648899 val loss: 0.611910
[Epoch 113] ogbg-molsider: 0.651225 test loss: 0.611259
[Epoch 114; Iter    14/   32] train: loss: 0.2845248
[Epoch 114] ogbg-molsider: 0.644391 val loss: 0.593617
[Epoch 114] ogbg-molsider: 0.657312 test loss: 0.586841
[Epoch 115; Iter    12/   32] train: loss: 0.2814020
[Epoch 115] ogbg-molsider: 0.646712 val loss: 0.624506
[Epoch 115] ogbg-molsider: 0.661061 test loss: 0.597766
[Epoch 116; Iter    10/   32] train: loss: 0.3254778
[Epoch 116] ogbg-molsider: 0.648861 val loss: 0.619461
[Epoch 116] ogbg-molsider: 0.664263 test loss: 0.603816
[Epoch 117; Iter     8/   32] train: loss: 0.2720743
[Epoch 117] ogbg-molsider: 0.648484 val loss: 0.715966
[Epoch 117] ogbg-molsider: 0.657216 test loss: 0.642185
[Epoch 118; Iter     6/   32] train: loss: 0.2660360
[Epoch 118] ogbg-molsider: 0.641640 val loss: 0.617386
[Epoch 118] ogbg-molsider: 0.656176 test loss: 0.594005
[Epoch 119; Iter     4/   32] train: loss: 0.3354206
[Epoch 119] ogbg-molsider: 0.652927 val loss: 0.617804
[Epoch 119] ogbg-molsider: 0.668801 test loss: 0.588833
[Epoch 120; Iter     2/   32] train: loss: 0.2648478
[Epoch 120; Iter    32/   32] train: loss: 0.6068462
[Epoch 120] ogbg-molsider: 0.645565 val loss: 0.623621
[Epoch 120] ogbg-molsider: 0.658847 test loss: 0.616165
[Epoch 121; Iter    30/   32] train: loss: 0.2647660
[Epoch 121] ogbg-molsider: 0.645408 val loss: 0.686207
[Epoch 121] ogbg-molsider: 0.658034 test loss: 0.609028
[Epoch 122; Iter    28/   32] train: loss: 0.2918787
[Epoch 122] ogbg-molsider: 0.642632 val loss: 0.702751
[Epoch 122] ogbg-molsider: 0.654443 test loss: 0.597539
[Epoch 123; Iter    26/   32] train: loss: 0.2525238
[Epoch 123] ogbg-molsider: 0.653345 val loss: 0.624017
[Epoch 123] ogbg-molsider: 0.664118 test loss: 0.601176
[Epoch 124; Iter    24/   32] train: loss: 0.2950618
[Epoch 124] ogbg-molsider: 0.648634 val loss: 0.792881
[Epoch 124] ogbg-molsider: 0.654389 test loss: 0.943285
[Epoch 125; Iter    22/   32] train: loss: 0.3010484
[Epoch 125] ogbg-molsider: 0.652018 val loss: 0.639289
[Epoch 125] ogbg-molsider: 0.657887 test loss: 0.613753
[Epoch 126; Iter    20/   32] train: loss: 0.2766379
[Epoch 126] ogbg-molsider: 0.649186 val loss: 0.705918
[Epoch 126] ogbg-molsider: 0.650628 test loss: 0.608537
[Epoch 127; Iter    18/   32] train: loss: 0.2774659
[Epoch 127] ogbg-molsider: 0.648511 val loss: 0.716807
[Epoch 127] ogbg-molsider: 0.663084 test loss: 0.603298
[Epoch 128; Iter    16/   32] train: loss: 0.2411866
[Epoch 128] ogbg-molsider: 0.650905 val loss: 0.781763
[Epoch 128] ogbg-molsider: 0.652852 test loss: 0.609845
[Epoch 129; Iter    14/   32] train: loss: 0.2608179
[Epoch 129] ogbg-molsider: 0.648005 val loss: 0.787382
[Epoch 129] ogbg-molsider: 0.649971 test loss: 0.614196
[Epoch 130; Iter    12/   32] train: loss: 0.2477273
[Epoch 130] ogbg-molsider: 0.648003 val loss: 0.644505
[Epoch 130] ogbg-molsider: 0.657620 test loss: 0.628685
[Epoch 131; Iter    10/   32] train: loss: 0.2402679
[Epoch 131] ogbg-molsider: 0.650422 val loss: 0.652458
[Epoch 131] ogbg-molsider: 0.654779 test loss: 0.621744
[Epoch 132; Iter     8/   32] train: loss: 0.2651961
[Epoch 132] ogbg-molsider: 0.648789 val loss: 0.786584
[Epoch 132] ogbg-molsider: 0.646581 test loss: 0.637213
[Epoch 133; Iter     6/   32] train: loss: 0.2607118
[Epoch 133] ogbg-molsider: 0.650445 val loss: 0.741338
[Epoch 89] ogbg-molsider: 0.609887 val loss: 0.551575
[Epoch 89] ogbg-molsider: 0.619006 test loss: 0.538221
[Epoch 90; Iter    27/   27] train: loss: 0.4170935
[Epoch 90] ogbg-molsider: 0.621466 val loss: 0.592421
[Epoch 90] ogbg-molsider: 0.611711 test loss: 0.625504
[Epoch 91] ogbg-molsider: 0.643747 val loss: 0.555624
[Epoch 91] ogbg-molsider: 0.627008 test loss: 0.601294
[Epoch 92; Iter     3/   27] train: loss: 0.3533564
[Epoch 92] ogbg-molsider: 0.615692 val loss: 0.548771
[Epoch 92] ogbg-molsider: 0.625121 test loss: 0.533616
[Epoch 93; Iter     6/   27] train: loss: 0.3604605
[Epoch 93] ogbg-molsider: 0.638234 val loss: 0.541368
[Epoch 93] ogbg-molsider: 0.628711 test loss: 0.552506
[Epoch 94; Iter     9/   27] train: loss: 0.3635664
[Epoch 94] ogbg-molsider: 0.621224 val loss: 0.555276
[Epoch 94] ogbg-molsider: 0.623240 test loss: 0.556590
[Epoch 95; Iter    12/   27] train: loss: 0.3974959
[Epoch 95] ogbg-molsider: 0.626941 val loss: 0.558952
[Epoch 95] ogbg-molsider: 0.633645 test loss: 0.544821
[Epoch 96; Iter    15/   27] train: loss: 0.3340027
[Epoch 96] ogbg-molsider: 0.638006 val loss: 0.554001
[Epoch 96] ogbg-molsider: 0.625358 test loss: 0.569232
[Epoch 97; Iter    18/   27] train: loss: 0.3349478
[Epoch 97] ogbg-molsider: 0.647201 val loss: 0.553905
[Epoch 97] ogbg-molsider: 0.631048 test loss: 0.554283
[Epoch 98; Iter    21/   27] train: loss: 0.3271471
[Epoch 98] ogbg-molsider: 0.640927 val loss: 0.568560
[Epoch 98] ogbg-molsider: 0.633508 test loss: 0.566251
[Epoch 99; Iter    24/   27] train: loss: 0.3330348
[Epoch 99] ogbg-molsider: 0.634571 val loss: 0.563057
[Epoch 99] ogbg-molsider: 0.619786 test loss: 0.574656
[Epoch 100; Iter    27/   27] train: loss: 0.2977816
[Epoch 100] ogbg-molsider: 0.634840 val loss: 0.573220
[Epoch 100] ogbg-molsider: 0.627289 test loss: 0.580230
[Epoch 101] ogbg-molsider: 0.631620 val loss: 0.572482
[Epoch 101] ogbg-molsider: 0.625079 test loss: 0.577968
[Epoch 102; Iter     3/   27] train: loss: 0.3322702
[Epoch 102] ogbg-molsider: 0.625578 val loss: 0.576397
[Epoch 102] ogbg-molsider: 0.628518 test loss: 0.559849
[Epoch 103; Iter     6/   27] train: loss: 0.3213069
[Epoch 103] ogbg-molsider: 0.635022 val loss: 0.566355
[Epoch 103] ogbg-molsider: 0.628956 test loss: 0.563743
[Epoch 104; Iter     9/   27] train: loss: 0.3256557
[Epoch 104] ogbg-molsider: 0.622668 val loss: 0.574406
[Epoch 104] ogbg-molsider: 0.634357 test loss: 0.572706
[Epoch 105; Iter    12/   27] train: loss: 0.3257236
[Epoch 105] ogbg-molsider: 0.621018 val loss: 0.577080
[Epoch 105] ogbg-molsider: 0.619651 test loss: 0.576670
[Epoch 106; Iter    15/   27] train: loss: 0.3098949
[Epoch 106] ogbg-molsider: 0.622846 val loss: 0.574698
[Epoch 106] ogbg-molsider: 0.625408 test loss: 0.573373
[Epoch 107; Iter    18/   27] train: loss: 0.3651275
[Epoch 107] ogbg-molsider: 0.620553 val loss: 0.577860
[Epoch 107] ogbg-molsider: 0.627170 test loss: 0.564208
[Epoch 108; Iter    21/   27] train: loss: 0.3320464
[Epoch 108] ogbg-molsider: 0.620277 val loss: 0.576106
[Epoch 108] ogbg-molsider: 0.628518 test loss: 0.564895
[Epoch 109; Iter    24/   27] train: loss: 0.3052277
[Epoch 109] ogbg-molsider: 0.633324 val loss: 0.592627
[Epoch 109] ogbg-molsider: 0.629388 test loss: 0.588414
[Epoch 110; Iter    27/   27] train: loss: 0.3641967
[Epoch 110] ogbg-molsider: 0.632757 val loss: 0.610198
[Epoch 110] ogbg-molsider: 0.622210 test loss: 0.621768
[Epoch 111] ogbg-molsider: 0.653243 val loss: 0.604685
[Epoch 111] ogbg-molsider: 0.629678 test loss: 0.636206
[Epoch 112; Iter     3/   27] train: loss: 0.3017419
[Epoch 112] ogbg-molsider: 0.639252 val loss: 0.604166
[Epoch 112] ogbg-molsider: 0.617720 test loss: 0.620287
[Epoch 113; Iter     6/   27] train: loss: 0.3034294
[Epoch 113] ogbg-molsider: 0.644460 val loss: 0.586072
[Epoch 113] ogbg-molsider: 0.632858 test loss: 0.580504
[Epoch 114; Iter     9/   27] train: loss: 0.3054510
[Epoch 114] ogbg-molsider: 0.622978 val loss: 0.604859
[Epoch 114] ogbg-molsider: 0.630272 test loss: 0.588466
[Epoch 115; Iter    12/   27] train: loss: 0.2696200
[Epoch 115] ogbg-molsider: 0.636212 val loss: 0.608133
[Epoch 115] ogbg-molsider: 0.625209 test loss: 0.594168
[Epoch 116; Iter    15/   27] train: loss: 0.3310796
[Epoch 116] ogbg-molsider: 0.636148 val loss: 0.589449
[Epoch 116] ogbg-molsider: 0.621239 test loss: 0.594913
[Epoch 117; Iter    18/   27] train: loss: 0.3200763
[Epoch 117] ogbg-molsider: 0.638201 val loss: 0.607082
[Epoch 117] ogbg-molsider: 0.624857 test loss: 0.610805
[Epoch 118; Iter    21/   27] train: loss: 0.3047972
[Epoch 118] ogbg-molsider: 0.629231 val loss: 0.594324
[Epoch 118] ogbg-molsider: 0.626664 test loss: 0.616055
[Epoch 119; Iter    24/   27] train: loss: 0.2511089
[Epoch 119] ogbg-molsider: 0.657351 val loss: 0.585936
[Epoch 119] ogbg-molsider: 0.640529 test loss: 0.590326
[Epoch 120; Iter    27/   27] train: loss: 0.2577823
[Epoch 120] ogbg-molsider: 0.644110 val loss: 0.588471
[Epoch 120] ogbg-molsider: 0.639470 test loss: 0.574521
[Epoch 121] ogbg-molsider: 0.647623 val loss: 0.593042
[Epoch 121] ogbg-molsider: 0.635672 test loss: 0.588394
[Epoch 122; Iter     3/   27] train: loss: 0.3107572
[Epoch 122] ogbg-molsider: 0.640845 val loss: 0.604831
[Epoch 122] ogbg-molsider: 0.633037 test loss: 0.600358
[Epoch 123; Iter     6/   27] train: loss: 0.2597835
[Epoch 123] ogbg-molsider: 0.643739 val loss: 0.612274
[Epoch 123] ogbg-molsider: 0.633362 test loss: 0.608255
[Epoch 124; Iter     9/   27] train: loss: 0.2545968
[Epoch 124] ogbg-molsider: 0.644561 val loss: 0.610695
[Epoch 124] ogbg-molsider: 0.638675 test loss: 0.605871
[Epoch 125; Iter    12/   27] train: loss: 0.2388649
[Epoch 125] ogbg-molsider: 0.643671 val loss: 0.615489
[Epoch 125] ogbg-molsider: 0.629053 test loss: 0.621030
[Epoch 126; Iter    15/   27] train: loss: 0.2712493
[Epoch 126] ogbg-molsider: 0.644213 val loss: 0.614988
[Epoch 126] ogbg-molsider: 0.643726 test loss: 0.605548
[Epoch 127; Iter    18/   27] train: loss: 0.2683938
[Epoch 127] ogbg-molsider: 0.647232 val loss: 0.618592
[Epoch 127] ogbg-molsider: 0.633900 test loss: 0.620271
[Epoch 128; Iter    21/   27] train: loss: 0.2616998
[Epoch 128] ogbg-molsider: 0.641613 val loss: 0.618350
[Epoch 128] ogbg-molsider: 0.629043 test loss: 0.620477
[Epoch 129; Iter    24/   27] train: loss: 0.2749901
[Epoch 129] ogbg-molsider: 0.644448 val loss: 0.625608
[Epoch 129] ogbg-molsider: 0.636487 test loss: 0.615152
[Epoch 130; Iter    27/   27] train: loss: 0.2993509
[Epoch 130] ogbg-molsider: 0.637656 val loss: 0.624995
[Epoch 130] ogbg-molsider: 0.627488 test loss: 0.627687
[Epoch 131] ogbg-molsider: 0.638763 val loss: 0.642555
[Epoch 131] ogbg-molsider: 0.631193 test loss: 0.639944
[Epoch 132; Iter     3/   27] train: loss: 0.2482088
[Epoch 132] ogbg-molsider: 0.643516 val loss: 0.615145
[Epoch 132] ogbg-molsider: 0.633773 test loss: 0.608306
[Epoch 133; Iter     6/   27] train: loss: 0.3079935
[Epoch 133] ogbg-molsider: 0.626591 val loss: 0.635793
[Epoch 133] ogbg-molsider: 0.632250 test loss: 0.622004
[Epoch 134; Iter     9/   27] train: loss: 0.2828718
[Epoch 134] ogbg-molsider: 0.644981 val loss: 0.628725
[Epoch 134] ogbg-molsider: 0.636906 test loss: 0.631005
[Epoch 135; Iter    12/   27] train: loss: 0.2527439
[Epoch 135] ogbg-molsider: 0.641044 val loss: 0.629959
[Epoch 135] ogbg-molsider: 0.633580 test loss: 0.625288
[Epoch 136; Iter    15/   27] train: loss: 0.2546416
[Epoch 136] ogbg-molsider: 0.640687 val loss: 0.636327
[Epoch 136] ogbg-molsider: 0.637213 test loss: 0.617310
[Epoch 137; Iter    18/   27] train: loss: 0.2713126
[Epoch 137] ogbg-molsider: 0.644317 val loss: 0.628790
[Epoch 137] ogbg-molsider: 0.628628 test loss: 0.627646
[Epoch 138; Iter    21/   27] train: loss: 0.2269728
[Epoch 138] ogbg-molsider: 0.634888 val loss: 0.645654
[Epoch 138] ogbg-molsider: 0.630829 test loss: 0.632519
[Epoch 139; Iter    24/   27] train: loss: 0.2220937
[Epoch 139] ogbg-molsider: 0.638286 val loss: 0.633682
[Epoch 139] ogbg-molsider: 0.622901 test loss: 0.636609
[Epoch 140; Iter    27/   27] train: loss: 0.2713335
[Epoch 140] ogbg-molsider: 0.642737 val loss: 0.632160
[Epoch 140] ogbg-molsider: 0.630069 test loss: 0.639910
[Epoch 89] ogbg-molsider: 0.633649 val loss: 0.546815
[Epoch 89] ogbg-molsider: 0.632234 test loss: 0.525678
[Epoch 90; Iter    27/   27] train: loss: 0.4308964
[Epoch 90] ogbg-molsider: 0.642394 val loss: 0.572096
[Epoch 90] ogbg-molsider: 0.629630 test loss: 0.544161
[Epoch 91] ogbg-molsider: 0.631342 val loss: 0.570338
[Epoch 91] ogbg-molsider: 0.647530 test loss: 0.543989
[Epoch 92; Iter     3/   27] train: loss: 0.3375065
[Epoch 92] ogbg-molsider: 0.645607 val loss: 0.538605
[Epoch 92] ogbg-molsider: 0.642324 test loss: 0.515512
[Epoch 93; Iter     6/   27] train: loss: 0.3627286
[Epoch 93] ogbg-molsider: 0.619738 val loss: 0.579472
[Epoch 93] ogbg-molsider: 0.620713 test loss: 0.566119
[Epoch 94; Iter     9/   27] train: loss: 0.3683989
[Epoch 94] ogbg-molsider: 0.626308 val loss: 0.564636
[Epoch 94] ogbg-molsider: 0.641259 test loss: 0.530764
[Epoch 95; Iter    12/   27] train: loss: 0.3436812
[Epoch 95] ogbg-molsider: 0.628210 val loss: 0.557479
[Epoch 95] ogbg-molsider: 0.649800 test loss: 0.524723
[Epoch 96; Iter    15/   27] train: loss: 0.3826004
[Epoch 96] ogbg-molsider: 0.637694 val loss: 0.567266
[Epoch 96] ogbg-molsider: 0.650887 test loss: 0.531216
[Epoch 97; Iter    18/   27] train: loss: 0.3226653
[Epoch 97] ogbg-molsider: 0.649183 val loss: 0.576565
[Epoch 97] ogbg-molsider: 0.636278 test loss: 0.576225
[Epoch 98; Iter    21/   27] train: loss: 0.3537294
[Epoch 98] ogbg-molsider: 0.629173 val loss: 0.579625
[Epoch 98] ogbg-molsider: 0.623369 test loss: 0.563279
[Epoch 99; Iter    24/   27] train: loss: 0.3213623
[Epoch 99] ogbg-molsider: 0.617558 val loss: 0.570764
[Epoch 99] ogbg-molsider: 0.626888 test loss: 0.559438
[Epoch 100; Iter    27/   27] train: loss: 0.3712624
[Epoch 100] ogbg-molsider: 0.628542 val loss: 0.554773
[Epoch 100] ogbg-molsider: 0.626832 test loss: 0.534377
[Epoch 101] ogbg-molsider: 0.641610 val loss: 0.564618
[Epoch 101] ogbg-molsider: 0.634612 test loss: 0.548694
[Epoch 102; Iter     3/   27] train: loss: 0.3110000
[Epoch 102] ogbg-molsider: 0.624067 val loss: 0.578358
[Epoch 102] ogbg-molsider: 0.633530 test loss: 0.552661
[Epoch 103; Iter     6/   27] train: loss: 0.3058667
[Epoch 103] ogbg-molsider: 0.640815 val loss: 0.565545
[Epoch 103] ogbg-molsider: 0.634242 test loss: 0.554623
[Epoch 104; Iter     9/   27] train: loss: 0.3046038
[Epoch 104] ogbg-molsider: 0.640698 val loss: 0.576093
[Epoch 104] ogbg-molsider: 0.644595 test loss: 0.545009
[Epoch 105; Iter    12/   27] train: loss: 0.3337452
[Epoch 105] ogbg-molsider: 0.629990 val loss: 0.579111
[Epoch 105] ogbg-molsider: 0.636678 test loss: 0.555311
[Epoch 106; Iter    15/   27] train: loss: 0.3267431
[Epoch 106] ogbg-molsider: 0.640878 val loss: 0.578273
[Epoch 106] ogbg-molsider: 0.634368 test loss: 0.563790
[Epoch 107; Iter    18/   27] train: loss: 0.2972040
[Epoch 107] ogbg-molsider: 0.635717 val loss: 0.580435
[Epoch 107] ogbg-molsider: 0.639930 test loss: 0.562071
[Epoch 108; Iter    21/   27] train: loss: 0.3348931
[Epoch 108] ogbg-molsider: 0.633264 val loss: 0.583029
[Epoch 108] ogbg-molsider: 0.646086 test loss: 0.544989
[Epoch 109; Iter    24/   27] train: loss: 0.2919410
[Epoch 109] ogbg-molsider: 0.630075 val loss: 0.588589
[Epoch 109] ogbg-molsider: 0.637752 test loss: 0.562594
[Epoch 110; Iter    27/   27] train: loss: 0.2630917
[Epoch 110] ogbg-molsider: 0.644792 val loss: 0.567919
[Epoch 110] ogbg-molsider: 0.642054 test loss: 0.564296
[Epoch 111] ogbg-molsider: 0.644248 val loss: 0.579001
[Epoch 111] ogbg-molsider: 0.623159 test loss: 0.584221
[Epoch 112; Iter     3/   27] train: loss: 0.2832810
[Epoch 112] ogbg-molsider: 0.635724 val loss: 0.600103
[Epoch 112] ogbg-molsider: 0.640836 test loss: 0.579073
[Epoch 113; Iter     6/   27] train: loss: 0.2997290
[Epoch 113] ogbg-molsider: 0.633268 val loss: 0.606988
[Epoch 113] ogbg-molsider: 0.636577 test loss: 0.578912
[Epoch 114; Iter     9/   27] train: loss: 0.2830124
[Epoch 114] ogbg-molsider: 0.656597 val loss: 0.585413
[Epoch 114] ogbg-molsider: 0.632858 test loss: 0.580808
[Epoch 115; Iter    12/   27] train: loss: 0.3075513
[Epoch 115] ogbg-molsider: 0.647258 val loss: 0.600218
[Epoch 115] ogbg-molsider: 0.635077 test loss: 0.596760
[Epoch 116; Iter    15/   27] train: loss: 0.3024642
[Epoch 116] ogbg-molsider: 0.646030 val loss: 0.591566
[Epoch 116] ogbg-molsider: 0.641315 test loss: 0.588205
[Epoch 117; Iter    18/   27] train: loss: 0.2756663
[Epoch 117] ogbg-molsider: 0.635694 val loss: 0.596606
[Epoch 117] ogbg-molsider: 0.639152 test loss: 0.579832
[Epoch 118; Iter    21/   27] train: loss: 0.2977048
[Epoch 118] ogbg-molsider: 0.649397 val loss: 0.591522
[Epoch 118] ogbg-molsider: 0.648948 test loss: 0.578491
[Epoch 119; Iter    24/   27] train: loss: 0.2823061
[Epoch 119] ogbg-molsider: 0.646403 val loss: 0.599244
[Epoch 119] ogbg-molsider: 0.644891 test loss: 0.601060
[Epoch 120; Iter    27/   27] train: loss: 0.2912224
[Epoch 120] ogbg-molsider: 0.644425 val loss: 0.588061
[Epoch 120] ogbg-molsider: 0.632632 test loss: 0.593071
[Epoch 121] ogbg-molsider: 0.644146 val loss: 0.602527
[Epoch 121] ogbg-molsider: 0.650600 test loss: 0.582205
[Epoch 122; Iter     3/   27] train: loss: 0.2716637
[Epoch 122] ogbg-molsider: 0.647197 val loss: 0.594106
[Epoch 122] ogbg-molsider: 0.643695 test loss: 0.580905
[Epoch 123; Iter     6/   27] train: loss: 0.2286240
[Epoch 123] ogbg-molsider: 0.631799 val loss: 0.633893
[Epoch 123] ogbg-molsider: 0.640977 test loss: 0.606453
[Epoch 124; Iter     9/   27] train: loss: 0.2701124
[Epoch 124] ogbg-molsider: 0.644299 val loss: 0.609104
[Epoch 124] ogbg-molsider: 0.635504 test loss: 0.616155
[Epoch 125; Iter    12/   27] train: loss: 0.2421300
[Epoch 125] ogbg-molsider: 0.652045 val loss: 0.603388
[Epoch 125] ogbg-molsider: 0.634149 test loss: 0.614282
[Epoch 126; Iter    15/   27] train: loss: 0.2678673
[Epoch 126] ogbg-molsider: 0.653657 val loss: 0.586832
[Epoch 126] ogbg-molsider: 0.641506 test loss: 0.588342
[Epoch 127; Iter    18/   27] train: loss: 0.2672822
[Epoch 127] ogbg-molsider: 0.648322 val loss: 0.593968
[Epoch 127] ogbg-molsider: 0.641791 test loss: 0.599901
[Epoch 128; Iter    21/   27] train: loss: 0.2365274
[Epoch 128] ogbg-molsider: 0.647098 val loss: 0.600368
[Epoch 128] ogbg-molsider: 0.639156 test loss: 0.608132
[Epoch 129; Iter    24/   27] train: loss: 0.2182878
[Epoch 129] ogbg-molsider: 0.654652 val loss: 0.600643
[Epoch 129] ogbg-molsider: 0.636376 test loss: 0.609471
[Epoch 130; Iter    27/   27] train: loss: 0.2424784
[Epoch 130] ogbg-molsider: 0.644871 val loss: 0.600233
[Epoch 130] ogbg-molsider: 0.637239 test loss: 0.607911
[Epoch 131] ogbg-molsider: 0.650506 val loss: 0.596093
[Epoch 131] ogbg-molsider: 0.638623 test loss: 0.606644
[Epoch 132; Iter     3/   27] train: loss: 0.2628184
[Epoch 132] ogbg-molsider: 0.639383 val loss: 0.610144
[Epoch 132] ogbg-molsider: 0.634560 test loss: 0.613818
[Epoch 133; Iter     6/   27] train: loss: 0.2354425
[Epoch 133] ogbg-molsider: 0.646606 val loss: 0.613169
[Epoch 133] ogbg-molsider: 0.637802 test loss: 0.620489
[Epoch 134; Iter     9/   27] train: loss: 0.2325564
[Epoch 134] ogbg-molsider: 0.648690 val loss: 0.617507
[Epoch 134] ogbg-molsider: 0.641019 test loss: 0.624335
[Epoch 135; Iter    12/   27] train: loss: 0.2390008
[Epoch 135] ogbg-molsider: 0.638845 val loss: 0.625410
[Epoch 135] ogbg-molsider: 0.634772 test loss: 0.640645
[Epoch 136; Iter    15/   27] train: loss: 0.2552253
[Epoch 136] ogbg-molsider: 0.648868 val loss: 0.618492
[Epoch 136] ogbg-molsider: 0.637189 test loss: 0.625919
[Epoch 137; Iter    18/   27] train: loss: 0.2428482
[Epoch 137] ogbg-molsider: 0.648643 val loss: 0.625729
[Epoch 137] ogbg-molsider: 0.639273 test loss: 0.638538
[Epoch 138; Iter    21/   27] train: loss: 0.2520299
[Epoch 138] ogbg-molsider: 0.648767 val loss: 0.621087
[Epoch 138] ogbg-molsider: 0.631826 test loss: 0.631688
[Epoch 139; Iter    24/   27] train: loss: 0.2369214
[Epoch 139] ogbg-molsider: 0.633632 val loss: 0.637797
[Epoch 139] ogbg-molsider: 0.635739 test loss: 0.648356
[Epoch 140; Iter    27/   27] train: loss: 0.2248569
[Epoch 140] ogbg-molsider: 0.638139 val loss: 0.626202
[Epoch 140] ogbg-molsider: 0.639778 test loss: 0.630461
[Epoch 89] ogbg-molsider: 0.641301 val loss: 0.545413
[Epoch 89] ogbg-molsider: 0.631370 test loss: 0.541321
[Epoch 90; Iter    27/   27] train: loss: 0.3737272
[Epoch 90] ogbg-molsider: 0.624479 val loss: 0.554476
[Epoch 90] ogbg-molsider: 0.644753 test loss: 0.526305
[Epoch 91] ogbg-molsider: 0.654321 val loss: 0.544642
[Epoch 91] ogbg-molsider: 0.641638 test loss: 0.542269
[Epoch 92; Iter     3/   27] train: loss: 0.3540379
[Epoch 92] ogbg-molsider: 0.652475 val loss: 0.540977
[Epoch 92] ogbg-molsider: 0.649611 test loss: 0.542124
[Epoch 93; Iter     6/   27] train: loss: 0.3184328
[Epoch 93] ogbg-molsider: 0.646765 val loss: 0.547093
[Epoch 93] ogbg-molsider: 0.642374 test loss: 0.549606
[Epoch 94; Iter     9/   27] train: loss: 0.3342436
[Epoch 94] ogbg-molsider: 0.648045 val loss: 0.560576
[Epoch 94] ogbg-molsider: 0.658030 test loss: 0.548635
[Epoch 95; Iter    12/   27] train: loss: 0.3261267
[Epoch 95] ogbg-molsider: 0.639668 val loss: 0.558254
[Epoch 95] ogbg-molsider: 0.642656 test loss: 0.552804
[Epoch 96; Iter    15/   27] train: loss: 0.3108616
[Epoch 96] ogbg-molsider: 0.635405 val loss: 0.558526
[Epoch 96] ogbg-molsider: 0.653899 test loss: 0.543531
[Epoch 97; Iter    18/   27] train: loss: 0.3624594
[Epoch 97] ogbg-molsider: 0.641676 val loss: 0.570203
[Epoch 97] ogbg-molsider: 0.650256 test loss: 0.548727
[Epoch 98; Iter    21/   27] train: loss: 0.3088337
[Epoch 98] ogbg-molsider: 0.656762 val loss: 0.562522
[Epoch 98] ogbg-molsider: 0.648507 test loss: 0.554919
[Epoch 99; Iter    24/   27] train: loss: 0.3000411
[Epoch 99] ogbg-molsider: 0.640311 val loss: 0.575567
[Epoch 99] ogbg-molsider: 0.647354 test loss: 0.560328
[Epoch 100; Iter    27/   27] train: loss: 0.2914878
[Epoch 100] ogbg-molsider: 0.632458 val loss: 0.574320
[Epoch 100] ogbg-molsider: 0.639315 test loss: 0.568617
[Epoch 101] ogbg-molsider: 0.640481 val loss: 0.583340
[Epoch 101] ogbg-molsider: 0.646313 test loss: 0.567759
[Epoch 102; Iter     3/   27] train: loss: 0.2849954
[Epoch 102] ogbg-molsider: 0.636767 val loss: 0.595953
[Epoch 102] ogbg-molsider: 0.640800 test loss: 0.576675
[Epoch 103; Iter     6/   27] train: loss: 0.2971578
[Epoch 103] ogbg-molsider: 0.639812 val loss: 0.587215
[Epoch 103] ogbg-molsider: 0.649113 test loss: 0.575609
[Epoch 104; Iter     9/   27] train: loss: 0.3229858
[Epoch 104] ogbg-molsider: 0.630098 val loss: 0.579446
[Epoch 104] ogbg-molsider: 0.636579 test loss: 0.572870
[Epoch 105; Iter    12/   27] train: loss: 0.2823221
[Epoch 105] ogbg-molsider: 0.638688 val loss: 0.592550
[Epoch 105] ogbg-molsider: 0.640831 test loss: 0.593940
[Epoch 106; Iter    15/   27] train: loss: 0.3132254
[Epoch 106] ogbg-molsider: 0.629063 val loss: 0.599245
[Epoch 106] ogbg-molsider: 0.649295 test loss: 0.583765
[Epoch 107; Iter    18/   27] train: loss: 0.2988819
[Epoch 107] ogbg-molsider: 0.638792 val loss: 0.593969
[Epoch 107] ogbg-molsider: 0.648524 test loss: 0.600227
[Epoch 108; Iter    21/   27] train: loss: 0.2895649
[Epoch 108] ogbg-molsider: 0.640147 val loss: 0.574034
[Epoch 108] ogbg-molsider: 0.644554 test loss: 0.580361
[Epoch 109; Iter    24/   27] train: loss: 0.3243777
[Epoch 109] ogbg-molsider: 0.638464 val loss: 0.602465
[Epoch 109] ogbg-molsider: 0.654914 test loss: 0.586935
[Epoch 110; Iter    27/   27] train: loss: 0.3068529
[Epoch 110] ogbg-molsider: 0.632116 val loss: 0.596001
[Epoch 110] ogbg-molsider: 0.643237 test loss: 0.589223
[Epoch 111] ogbg-molsider: 0.651422 val loss: 0.608785
[Epoch 111] ogbg-molsider: 0.655082 test loss: 0.608680
[Epoch 112; Iter     3/   27] train: loss: 0.3068222
[Epoch 112] ogbg-molsider: 0.623337 val loss: 0.617763
[Epoch 112] ogbg-molsider: 0.642132 test loss: 0.604514
[Epoch 113; Iter     6/   27] train: loss: 0.3365619
[Epoch 113] ogbg-molsider: 0.630489 val loss: 0.596045
[Epoch 113] ogbg-molsider: 0.645344 test loss: 0.572472
[Epoch 114; Iter     9/   27] train: loss: 0.3668315
[Epoch 114] ogbg-molsider: 0.639474 val loss: 0.621027
[Epoch 114] ogbg-molsider: 0.655715 test loss: 0.609855
[Epoch 115; Iter    12/   27] train: loss: 0.2916348
[Epoch 115] ogbg-molsider: 0.636983 val loss: 0.622745
[Epoch 115] ogbg-molsider: 0.628798 test loss: 0.620268
[Epoch 116; Iter    15/   27] train: loss: 0.2790197
[Epoch 116] ogbg-molsider: 0.636638 val loss: 0.594146
[Epoch 116] ogbg-molsider: 0.640951 test loss: 0.598890
[Epoch 117; Iter    18/   27] train: loss: 0.2972369
[Epoch 117] ogbg-molsider: 0.634834 val loss: 0.607673
[Epoch 117] ogbg-molsider: 0.651737 test loss: 0.601046
[Epoch 118; Iter    21/   27] train: loss: 0.2572570
[Epoch 118] ogbg-molsider: 0.639705 val loss: 0.610919
[Epoch 118] ogbg-molsider: 0.641095 test loss: 0.609246
[Epoch 119; Iter    24/   27] train: loss: 0.2472871
[Epoch 119] ogbg-molsider: 0.641881 val loss: 0.610620
[Epoch 119] ogbg-molsider: 0.652647 test loss: 0.603734
[Epoch 120; Iter    27/   27] train: loss: 0.2576328
[Epoch 120] ogbg-molsider: 0.638105 val loss: 0.632628
[Epoch 120] ogbg-molsider: 0.653509 test loss: 0.613714
[Epoch 121] ogbg-molsider: 0.638690 val loss: 0.620854
[Epoch 121] ogbg-molsider: 0.651588 test loss: 0.612457
[Epoch 122; Iter     3/   27] train: loss: 0.2672211
[Epoch 122] ogbg-molsider: 0.638949 val loss: 0.630699
[Epoch 122] ogbg-molsider: 0.652449 test loss: 0.619371
[Epoch 123; Iter     6/   27] train: loss: 0.2430819
[Epoch 123] ogbg-molsider: 0.645278 val loss: 0.622902
[Epoch 123] ogbg-molsider: 0.646947 test loss: 0.610491
[Epoch 124; Iter     9/   27] train: loss: 0.2401159
[Epoch 124] ogbg-molsider: 0.637793 val loss: 0.651283
[Epoch 124] ogbg-molsider: 0.648178 test loss: 0.643184
[Epoch 125; Iter    12/   27] train: loss: 0.2367733
[Epoch 125] ogbg-molsider: 0.633331 val loss: 0.640799
[Epoch 125] ogbg-molsider: 0.645319 test loss: 0.630198
[Epoch 126; Iter    15/   27] train: loss: 0.2429970
[Epoch 126] ogbg-molsider: 0.631721 val loss: 0.632873
[Epoch 126] ogbg-molsider: 0.642013 test loss: 0.616609
[Epoch 127; Iter    18/   27] train: loss: 0.2266479
[Epoch 127] ogbg-molsider: 0.634748 val loss: 0.641854
[Epoch 127] ogbg-molsider: 0.635537 test loss: 0.647551
[Epoch 128; Iter    21/   27] train: loss: 0.2673668
[Epoch 128] ogbg-molsider: 0.641367 val loss: 0.630436
[Epoch 128] ogbg-molsider: 0.643380 test loss: 0.625426
[Epoch 129; Iter    24/   27] train: loss: 0.2116530
[Epoch 129] ogbg-molsider: 0.630065 val loss: 0.644270
[Epoch 129] ogbg-molsider: 0.644303 test loss: 0.630720
[Epoch 130; Iter    27/   27] train: loss: 0.2553726
[Epoch 130] ogbg-molsider: 0.632973 val loss: 0.653833
[Epoch 130] ogbg-molsider: 0.637998 test loss: 0.651766
[Epoch 131] ogbg-molsider: 0.629609 val loss: 0.646249
[Epoch 131] ogbg-molsider: 0.645338 test loss: 0.632018
[Epoch 132; Iter     3/   27] train: loss: 0.2631225
[Epoch 132] ogbg-molsider: 0.645703 val loss: 0.633865
[Epoch 132] ogbg-molsider: 0.642650 test loss: 0.640219
[Epoch 133; Iter     6/   27] train: loss: 0.2451752
[Epoch 133] ogbg-molsider: 0.631603 val loss: 0.646547
[Epoch 133] ogbg-molsider: 0.645102 test loss: 0.629396
[Epoch 134; Iter     9/   27] train: loss: 0.2181512
[Epoch 134] ogbg-molsider: 0.625605 val loss: 0.664602
[Epoch 134] ogbg-molsider: 0.644492 test loss: 0.649988
[Epoch 135; Iter    12/   27] train: loss: 0.3009826
[Epoch 135] ogbg-molsider: 0.624860 val loss: 0.668583
[Epoch 135] ogbg-molsider: 0.653221 test loss: 0.645777
[Epoch 136; Iter    15/   27] train: loss: 0.2271711
[Epoch 136] ogbg-molsider: 0.633018 val loss: 0.652933
[Epoch 136] ogbg-molsider: 0.644325 test loss: 0.639084
[Epoch 137; Iter    18/   27] train: loss: 0.2129889
[Epoch 137] ogbg-molsider: 0.624372 val loss: 0.648733
[Epoch 137] ogbg-molsider: 0.643010 test loss: 0.640103
[Epoch 138; Iter    21/   27] train: loss: 0.2580782
[Epoch 138] ogbg-molsider: 0.640911 val loss: 0.657666
[Epoch 138] ogbg-molsider: 0.645572 test loss: 0.645781
[Epoch 139; Iter    24/   27] train: loss: 0.2463488
[Epoch 139] ogbg-molsider: 0.631555 val loss: 0.663986
[Epoch 139] ogbg-molsider: 0.638792 test loss: 0.658567
[Epoch 140; Iter    27/   27] train: loss: 0.2423034
[Epoch 140] ogbg-molsider: 0.638218 val loss: 0.648776
[Epoch 140] ogbg-molsider: 0.635657 test loss: 0.645003
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 141] ogbg-molsider: 0.621387 val loss: 0.680498
[Epoch 141] ogbg-molsider: 0.637030 test loss: 0.677220
[Epoch 142; Iter     3/   27] train: loss: 0.2280488
[Epoch 142] ogbg-molsider: 0.632787 val loss: 0.666091
[Epoch 142] ogbg-molsider: 0.643241 test loss: 0.653257
[Epoch 143; Iter     6/   27] train: loss: 0.2218628
[Epoch 143] ogbg-molsider: 0.628439 val loss: 0.662385
[Epoch 143] ogbg-molsider: 0.638445 test loss: 0.660373
[Epoch 144; Iter     9/   27] train: loss: 0.1966576
[Epoch 144] ogbg-molsider: 0.634325 val loss: 0.661475
[Epoch 144] ogbg-molsider: 0.646374 test loss: 0.651199
[Epoch 145; Iter    12/   27] train: loss: 0.2853423
[Epoch 145] ogbg-molsider: 0.636967 val loss: 0.652918
[Epoch 145] ogbg-molsider: 0.639713 test loss: 0.655873
Early stopping criterion based on -ogbg-molsider- that should be max reached after 145 epochs. Best model checkpoint was in epoch 85.
Statistics on  val_best_checkpoint
mean_pred: 0.5238438844680786
std_pred: 2.2118642330169678
mean_targets: 0.5445094108581543
std_targets: 0.4980473220348358
prcauc: 0.6396800189687155
rocauc: 0.6698916209358385
ogbg-molsider: 0.6698916209358385
OGBNanLabelBCEWithLogitsLoss: 0.5327891409397125
Statistics on  test
mean_pred: 0.6287795305252075
std_pred: 2.552602767944336
mean_targets: 0.5734265446662903
std_targets: 0.4946112036705017
prcauc: 0.6632069992194455
rocauc: 0.6476614692829052
ogbg-molsider: 0.6476614692829052
OGBNanLabelBCEWithLogitsLoss: 0.5302547978030311
Statistics on  train
mean_pred: 0.6274011731147766
std_pred: 2.439401149749756
mean_targets: 0.5732952356338501
std_targets: 0.4946093261241913
prcauc: 0.7883435668542037
rocauc: 0.8268323348822945
ogbg-molsider: 0.8268323348822945
OGBNanLabelBCEWithLogitsLoss: 0.39765897503605596
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 133] ogbg-molsider: 0.647416 test loss: 0.542192
[Epoch 134; Iter     4/   32] train: loss: 0.3957353
[Epoch 134] ogbg-molsider: 0.637790 val loss: 0.558380
[Epoch 134] ogbg-molsider: 0.636669 test loss: 0.546179
[Epoch 135; Iter     2/   32] train: loss: 0.3336044
[Epoch 135; Iter    32/   32] train: loss: 0.4062071
[Epoch 135] ogbg-molsider: 0.641684 val loss: 0.564213
[Epoch 135] ogbg-molsider: 0.655948 test loss: 0.541301
[Epoch 136; Iter    30/   32] train: loss: 0.3964573
[Epoch 136] ogbg-molsider: 0.628141 val loss: 0.560388
[Epoch 136] ogbg-molsider: 0.632660 test loss: 0.574023
[Epoch 137; Iter    28/   32] train: loss: 0.3533317
[Epoch 137] ogbg-molsider: 0.631627 val loss: 0.562044
[Epoch 137] ogbg-molsider: 0.640607 test loss: 0.561792
[Epoch 138; Iter    26/   32] train: loss: 0.3670197
[Epoch 138] ogbg-molsider: 0.631268 val loss: 0.561856
[Epoch 138] ogbg-molsider: 0.619371 test loss: 0.566267
[Epoch 139; Iter    24/   32] train: loss: 0.3412070
[Epoch 139] ogbg-molsider: 0.631572 val loss: 0.548691
[Epoch 139] ogbg-molsider: 0.626200 test loss: 0.559590
[Epoch 140; Iter    22/   32] train: loss: 0.3418752
[Epoch 140] ogbg-molsider: 0.634056 val loss: 0.554460
[Epoch 140] ogbg-molsider: 0.629825 test loss: 0.555263
[Epoch 141; Iter    20/   32] train: loss: 0.3493933
[Epoch 141] ogbg-molsider: 0.633845 val loss: 0.556978
[Epoch 141] ogbg-molsider: 0.648522 test loss: 0.541449
[Epoch 142; Iter    18/   32] train: loss: 0.3464191
[Epoch 142] ogbg-molsider: 0.635031 val loss: 0.567281
[Epoch 142] ogbg-molsider: 0.642246 test loss: 0.553546
[Epoch 143; Iter    16/   32] train: loss: 0.3604292
[Epoch 143] ogbg-molsider: 0.629734 val loss: 0.563863
[Epoch 143] ogbg-molsider: 0.646005 test loss: 0.549357
[Epoch 144; Iter    14/   32] train: loss: 0.3394297
[Epoch 144] ogbg-molsider: 0.634633 val loss: 0.556404
[Epoch 144] ogbg-molsider: 0.643292 test loss: 0.542272
[Epoch 145; Iter    12/   32] train: loss: 0.3577337
[Epoch 145] ogbg-molsider: 0.628771 val loss: 0.569024
[Epoch 145] ogbg-molsider: 0.645308 test loss: 0.552590
[Epoch 146; Iter    10/   32] train: loss: 0.3487116
[Epoch 146] ogbg-molsider: 0.630698 val loss: 0.562968
[Epoch 146] ogbg-molsider: 0.642736 test loss: 0.546662
[Epoch 147; Iter     8/   32] train: loss: 0.3854371
[Epoch 147] ogbg-molsider: 0.639402 val loss: 0.566693
[Epoch 147] ogbg-molsider: 0.646026 test loss: 0.545536
[Epoch 148; Iter     6/   32] train: loss: 0.3316480
[Epoch 148] ogbg-molsider: 0.637095 val loss: 0.577923
[Epoch 148] ogbg-molsider: 0.641864 test loss: 0.566783
[Epoch 149; Iter     4/   32] train: loss: 0.3830268
[Epoch 149] ogbg-molsider: 0.636852 val loss: 0.563270
[Epoch 149] ogbg-molsider: 0.648951 test loss: 0.554834
[Epoch 150; Iter     2/   32] train: loss: 0.4095008
[Epoch 150; Iter    32/   32] train: loss: 0.2738282
[Epoch 150] ogbg-molsider: 0.630183 val loss: 0.569179
[Epoch 150] ogbg-molsider: 0.634556 test loss: 0.563094
[Epoch 151; Iter    30/   32] train: loss: 0.3811085
[Epoch 151] ogbg-molsider: 0.631118 val loss: 0.570767
[Epoch 151] ogbg-molsider: 0.647311 test loss: 0.549955
[Epoch 152; Iter    28/   32] train: loss: 0.3369486
[Epoch 152] ogbg-molsider: 0.624375 val loss: 0.565881
[Epoch 152] ogbg-molsider: 0.627789 test loss: 0.571091
[Epoch 153; Iter    26/   32] train: loss: 0.3032037
[Epoch 153] ogbg-molsider: 0.629571 val loss: 0.575611
[Epoch 153] ogbg-molsider: 0.636679 test loss: 0.561273
[Epoch 154; Iter    24/   32] train: loss: 0.3605621
[Epoch 154] ogbg-molsider: 0.623509 val loss: 0.587448
[Epoch 154] ogbg-molsider: 0.646403 test loss: 0.558931
[Epoch 155; Iter    22/   32] train: loss: 0.3140021
[Epoch 155] ogbg-molsider: 0.629471 val loss: 0.568317
[Epoch 155] ogbg-molsider: 0.638115 test loss: 0.559440
[Epoch 156; Iter    20/   32] train: loss: 0.3316534
[Epoch 156] ogbg-molsider: 0.633634 val loss: 0.570545
[Epoch 156] ogbg-molsider: 0.646446 test loss: 0.557123
Early stopping criterion based on -ogbg-molsider- that should be max reached after 156 epochs. Best model checkpoint was in epoch 96.
Statistics on  val_best_checkpoint
mean_pred: 0.18069840967655182
std_pred: 1.9301186800003052
mean_targets: 0.5732086896896362
std_targets: 0.49465426802635193
prcauc: 0.6514074948974311
rocauc: 0.64853623821859
ogbg-molsider: 0.64853623821859
OGBNanLabelBCEWithLogitsLoss: 0.5066566595009395
Statistics on  test
mean_pred: 0.2908703088760376
std_pred: 1.8896270990371704
mean_targets: 0.5714039206504822
std_targets: 0.4949178397655487
prcauc: 0.6533116582819075
rocauc: 0.6315043490734524
ogbg-molsider: 0.6315043490734524
OGBNanLabelBCEWithLogitsLoss: 0.5088399222918919
Statistics on  train
mean_pred: 0.2621147930622101
std_pred: 2.2386534214019775
mean_targets: 0.5655384659767151
std_targets: 0.4956952929496765
prcauc: 0.7243530954689011
rocauc: 0.756196499375491
ogbg-molsider: 0.756196499375491
OGBNanLabelBCEWithLogitsLoss: 0.4499246794730425
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 128] ogbg-molsider: 0.631041 val loss: 0.632473
[Epoch 128] ogbg-molsider: 0.650447 test loss: 0.651873
[Epoch 129; Iter    12/   36] train: loss: 0.2844899
[Epoch 129] ogbg-molsider: 0.625898 val loss: 0.629308
[Epoch 129] ogbg-molsider: 0.664395 test loss: 0.629605
[Epoch 130; Iter     6/   36] train: loss: 0.2911851
[Epoch 130; Iter    36/   36] train: loss: 0.2837574
[Epoch 130] ogbg-molsider: 0.634254 val loss: 0.641369
[Epoch 130] ogbg-molsider: 0.646757 test loss: 0.693174
[Epoch 131; Iter    30/   36] train: loss: 0.2702290
[Epoch 131] ogbg-molsider: 0.639535 val loss: 0.625110
[Epoch 131] ogbg-molsider: 0.640905 test loss: 0.673078
[Epoch 132; Iter    24/   36] train: loss: 0.2760891
[Epoch 132] ogbg-molsider: 0.646522 val loss: 0.627054
[Epoch 132] ogbg-molsider: 0.654225 test loss: 0.679250
[Epoch 133; Iter    18/   36] train: loss: 0.2385170
[Epoch 133] ogbg-molsider: 0.636800 val loss: 0.629643
[Epoch 133] ogbg-molsider: 0.656737 test loss: 0.667075
[Epoch 134; Iter    12/   36] train: loss: 0.2574647
[Epoch 134] ogbg-molsider: 0.625502 val loss: 0.645396
[Epoch 134] ogbg-molsider: 0.650007 test loss: 0.655293
[Epoch 135; Iter     6/   36] train: loss: 0.2246528
[Epoch 135; Iter    36/   36] train: loss: 0.2698899
[Epoch 135] ogbg-molsider: 0.644818 val loss: 0.629065
[Epoch 135] ogbg-molsider: 0.657748 test loss: 0.661462
[Epoch 136; Iter    30/   36] train: loss: 0.2771959
[Epoch 136] ogbg-molsider: 0.631477 val loss: 0.636085
[Epoch 136] ogbg-molsider: 0.643842 test loss: 0.678381
[Epoch 137; Iter    24/   36] train: loss: 0.2571606
[Epoch 137] ogbg-molsider: 0.627947 val loss: 0.656634
[Epoch 137] ogbg-molsider: 0.652788 test loss: 0.667234
[Epoch 138; Iter    18/   36] train: loss: 0.2463969
[Epoch 138] ogbg-molsider: 0.644019 val loss: 0.617712
[Epoch 138] ogbg-molsider: 0.654142 test loss: 0.631380
[Epoch 139; Iter    12/   36] train: loss: 0.2804359
[Epoch 139] ogbg-molsider: 0.630801 val loss: 0.664252
[Epoch 139] ogbg-molsider: 0.653089 test loss: 0.680986
[Epoch 140; Iter     6/   36] train: loss: 0.2570232
[Epoch 140; Iter    36/   36] train: loss: 0.2927125
[Epoch 140] ogbg-molsider: 0.644787 val loss: 0.617202
[Epoch 140] ogbg-molsider: 0.647703 test loss: 0.664544
[Epoch 141; Iter    30/   36] train: loss: 0.2472644
[Epoch 141] ogbg-molsider: 0.627158 val loss: 0.652272
[Epoch 141] ogbg-molsider: 0.642044 test loss: 0.689960
[Epoch 142; Iter    24/   36] train: loss: 0.2378474
[Epoch 142] ogbg-molsider: 0.636083 val loss: 0.655243
[Epoch 142] ogbg-molsider: 0.652173 test loss: 0.681887
[Epoch 143; Iter    18/   36] train: loss: 0.2474036
[Epoch 143] ogbg-molsider: 0.638224 val loss: 0.700154
[Epoch 143] ogbg-molsider: 0.638442 test loss: 0.735639
[Epoch 144; Iter    12/   36] train: loss: 0.2297435
[Epoch 144] ogbg-molsider: 0.644393 val loss: 0.643805
[Epoch 144] ogbg-molsider: 0.654928 test loss: 0.670692
[Epoch 145; Iter     6/   36] train: loss: 0.2206888
[Epoch 145; Iter    36/   36] train: loss: 0.2060636
[Epoch 145] ogbg-molsider: 0.635801 val loss: 0.654620
[Epoch 145] ogbg-molsider: 0.640597 test loss: 0.690506
[Epoch 146; Iter    30/   36] train: loss: 0.2320505
[Epoch 146] ogbg-molsider: 0.639833 val loss: 0.647883
[Epoch 146] ogbg-molsider: 0.647327 test loss: 0.681855
[Epoch 147; Iter    24/   36] train: loss: 0.2190231
[Epoch 147] ogbg-molsider: 0.642716 val loss: 0.645094
[Epoch 147] ogbg-molsider: 0.643557 test loss: 0.690391
[Epoch 148; Iter    18/   36] train: loss: 0.2629220
[Epoch 148] ogbg-molsider: 0.640512 val loss: 0.645472
[Epoch 148] ogbg-molsider: 0.645102 test loss: 0.705196
[Epoch 149; Iter    12/   36] train: loss: 0.2483258
[Epoch 149] ogbg-molsider: 0.641561 val loss: 0.636699
[Epoch 149] ogbg-molsider: 0.649237 test loss: 0.665361
[Epoch 150; Iter     6/   36] train: loss: 0.2129556
[Epoch 150; Iter    36/   36] train: loss: 0.2075217
[Epoch 150] ogbg-molsider: 0.638343 val loss: 0.652133
[Epoch 150] ogbg-molsider: 0.644958 test loss: 0.697571
[Epoch 151; Iter    30/   36] train: loss: 0.2272147
[Epoch 151] ogbg-molsider: 0.638422 val loss: 0.669180
[Epoch 151] ogbg-molsider: 0.647096 test loss: 0.676416
[Epoch 152; Iter    24/   36] train: loss: 0.2321832
[Epoch 152] ogbg-molsider: 0.644749 val loss: 0.651180
[Epoch 152] ogbg-molsider: 0.645325 test loss: 0.698819
[Epoch 153; Iter    18/   36] train: loss: 0.2179142
[Epoch 153] ogbg-molsider: 0.646652 val loss: 0.650852
[Epoch 153] ogbg-molsider: 0.646524 test loss: 0.699291
[Epoch 154; Iter    12/   36] train: loss: 0.2344296
[Epoch 154] ogbg-molsider: 0.645412 val loss: 0.642702
[Epoch 154] ogbg-molsider: 0.644711 test loss: 0.697094
[Epoch 155; Iter     6/   36] train: loss: 0.2084054
[Epoch 155; Iter    36/   36] train: loss: 0.2179849
[Epoch 155] ogbg-molsider: 0.642605 val loss: 0.653017
[Epoch 155] ogbg-molsider: 0.647526 test loss: 0.688377
[Epoch 156; Iter    30/   36] train: loss: 0.2317063
[Epoch 156] ogbg-molsider: 0.642650 val loss: 0.673779
[Epoch 156] ogbg-molsider: 0.636371 test loss: 0.722168
[Epoch 157; Iter    24/   36] train: loss: 0.2118443
[Epoch 157] ogbg-molsider: 0.643113 val loss: 0.644801
[Epoch 157] ogbg-molsider: 0.641537 test loss: 0.695404
[Epoch 158; Iter    18/   36] train: loss: 0.1976506
[Epoch 158] ogbg-molsider: 0.641228 val loss: 0.659586
[Epoch 158] ogbg-molsider: 0.647532 test loss: 0.691533
[Epoch 159; Iter    12/   36] train: loss: 0.2193431
[Epoch 159] ogbg-molsider: 0.643868 val loss: 0.695838
[Epoch 159] ogbg-molsider: 0.645822 test loss: 0.702773
[Epoch 160; Iter     6/   36] train: loss: 0.2325118
[Epoch 160; Iter    36/   36] train: loss: 0.2444802
[Epoch 160] ogbg-molsider: 0.645943 val loss: 0.691921
[Epoch 160] ogbg-molsider: 0.646547 test loss: 0.696674
[Epoch 161; Iter    30/   36] train: loss: 0.2367572
[Epoch 161] ogbg-molsider: 0.640138 val loss: 0.677703
[Epoch 161] ogbg-molsider: 0.645127 test loss: 0.715146
[Epoch 162; Iter    24/   36] train: loss: 0.2099663
[Epoch 162] ogbg-molsider: 0.638194 val loss: 0.672514
[Epoch 162] ogbg-molsider: 0.640991 test loss: 0.712905
[Epoch 163; Iter    18/   36] train: loss: 0.2234676
[Epoch 163] ogbg-molsider: 0.633311 val loss: 0.688237
[Epoch 163] ogbg-molsider: 0.646595 test loss: 0.707479
[Epoch 164; Iter    12/   36] train: loss: 0.1988840
[Epoch 164] ogbg-molsider: 0.638660 val loss: 0.718815
[Epoch 164] ogbg-molsider: 0.641833 test loss: 0.716883
[Epoch 165; Iter     6/   36] train: loss: 0.1840674
[Epoch 165; Iter    36/   36] train: loss: 0.2412656
[Epoch 165] ogbg-molsider: 0.640528 val loss: 0.668059
[Epoch 165] ogbg-molsider: 0.643525 test loss: 0.706141
[Epoch 166; Iter    30/   36] train: loss: 0.2415432
[Epoch 166] ogbg-molsider: 0.637814 val loss: 0.676992
[Epoch 166] ogbg-molsider: 0.642737 test loss: 0.715994
Early stopping criterion based on -ogbg-molsider- that should be max reached after 166 epochs. Best model checkpoint was in epoch 106.
Statistics on  val_best_checkpoint
mean_pred: 0.6887349486351013
std_pred: 3.113436460494995
mean_targets: 0.6021755933761597
std_targets: 0.48951220512390137
prcauc: 0.6968967719395444
rocauc: 0.6591400932427449
ogbg-molsider: 0.6591400932427449
OGBNanLabelBCEWithLogitsLoss: 0.5495593309402466
Statistics on  test
mean_pred: 0.4942830801010132
std_pred: 2.9791018962860107
mean_targets: 0.5446775555610657
std_targets: 0.49806439876556396
prcauc: 0.6388437941381079
rocauc: 0.6557426516243517
ogbg-molsider: 0.6557426516243517
OGBNanLabelBCEWithLogitsLoss: 0.5982433676719665
Statistics on  train
mean_pred: 0.5173360705375671
std_pred: 3.222933769226074
mean_targets: 0.5661051273345947
std_targets: 0.49561890959739685
prcauc: 0.8642068247088349
rocauc: 0.9106420221319965
ogbg-molsider: 0.9106420221319965
OGBNanLabelBCEWithLogitsLoss: 0.29693783902459675
All runs completed.
[Epoch 133] ogbg-molsider: 0.655221 test loss: 0.627029
[Epoch 134; Iter     4/   32] train: loss: 0.2465191
[Epoch 134] ogbg-molsider: 0.643606 val loss: 0.636641
[Epoch 134] ogbg-molsider: 0.652426 test loss: 0.630109
[Epoch 135; Iter     2/   32] train: loss: 0.2443228
[Epoch 135; Iter    32/   32] train: loss: 0.3680979
[Epoch 135] ogbg-molsider: 0.651725 val loss: 0.677786
[Epoch 135] ogbg-molsider: 0.657575 test loss: 0.626948
[Epoch 136; Iter    30/   32] train: loss: 0.2306193
[Epoch 136] ogbg-molsider: 0.656783 val loss: 0.658651
[Epoch 136] ogbg-molsider: 0.657954 test loss: 0.621461
[Epoch 137; Iter    28/   32] train: loss: 0.2294560
[Epoch 137] ogbg-molsider: 0.650507 val loss: 0.676150
[Epoch 137] ogbg-molsider: 0.649932 test loss: 0.630670
[Epoch 138; Iter    26/   32] train: loss: 0.2144290
[Epoch 138] ogbg-molsider: 0.653394 val loss: 0.652394
[Epoch 138] ogbg-molsider: 0.653489 test loss: 0.639676
[Epoch 139; Iter    24/   32] train: loss: 0.2755838
[Epoch 139] ogbg-molsider: 0.650161 val loss: 0.697817
[Epoch 139] ogbg-molsider: 0.659426 test loss: 0.634759
[Epoch 140; Iter    22/   32] train: loss: 0.2636615
[Epoch 140] ogbg-molsider: 0.659660 val loss: 0.699091
[Epoch 140] ogbg-molsider: 0.650492 test loss: 0.645078
[Epoch 141; Iter    20/   32] train: loss: 0.2422825
[Epoch 141] ogbg-molsider: 0.650848 val loss: 0.777441
[Epoch 141] ogbg-molsider: 0.650955 test loss: 0.635921
[Epoch 142; Iter    18/   32] train: loss: 0.2338983
[Epoch 142] ogbg-molsider: 0.652962 val loss: 0.715316
[Epoch 142] ogbg-molsider: 0.659732 test loss: 0.623902
[Epoch 143; Iter    16/   32] train: loss: 0.2494437
[Epoch 143] ogbg-molsider: 0.659007 val loss: 0.640965
[Epoch 143] ogbg-molsider: 0.659707 test loss: 0.651962
[Epoch 144; Iter    14/   32] train: loss: 0.2385237
[Epoch 144] ogbg-molsider: 0.653566 val loss: 0.657659
[Epoch 144] ogbg-molsider: 0.661637 test loss: 0.637992
[Epoch 145; Iter    12/   32] train: loss: 0.2432647
[Epoch 145] ogbg-molsider: 0.656318 val loss: 0.661536
[Epoch 145] ogbg-molsider: 0.657655 test loss: 0.650617
[Epoch 146; Iter    10/   32] train: loss: 0.1940518
[Epoch 146] ogbg-molsider: 0.653581 val loss: 0.716924
[Epoch 146] ogbg-molsider: 0.648107 test loss: 0.682045
[Epoch 147; Iter     8/   32] train: loss: 0.2564816
[Epoch 147] ogbg-molsider: 0.649360 val loss: 0.646802
[Epoch 147] ogbg-molsider: 0.646157 test loss: 0.650144
[Epoch 148; Iter     6/   32] train: loss: 0.2066663
[Epoch 148] ogbg-molsider: 0.650756 val loss: 0.747433
[Epoch 148] ogbg-molsider: 0.648780 test loss: 0.663097
[Epoch 149; Iter     4/   32] train: loss: 0.2309661
[Epoch 149] ogbg-molsider: 0.650523 val loss: 1.050780
[Epoch 149] ogbg-molsider: 0.660386 test loss: 0.661908
[Epoch 150; Iter     2/   32] train: loss: 0.2432681
[Epoch 150; Iter    32/   32] train: loss: 0.4074063
[Epoch 150] ogbg-molsider: 0.663964 val loss: 0.680575
[Epoch 150] ogbg-molsider: 0.657748 test loss: 0.656664
[Epoch 151; Iter    30/   32] train: loss: 0.2751814
[Epoch 151] ogbg-molsider: 0.654846 val loss: 0.761167
[Epoch 151] ogbg-molsider: 0.657589 test loss: 0.653735
[Epoch 152; Iter    28/   32] train: loss: 0.2242155
[Epoch 152] ogbg-molsider: 0.657457 val loss: 0.903176
[Epoch 152] ogbg-molsider: 0.657627 test loss: 0.654774
[Epoch 153; Iter    26/   32] train: loss: 0.1944973
[Epoch 153] ogbg-molsider: 0.654979 val loss: 0.756336
[Epoch 153] ogbg-molsider: 0.654810 test loss: 0.648954
[Epoch 154; Iter    24/   32] train: loss: 0.2124750
[Epoch 154] ogbg-molsider: 0.655870 val loss: 0.878767
[Epoch 154] ogbg-molsider: 0.657616 test loss: 0.646898
[Epoch 155; Iter    22/   32] train: loss: 0.2275461
[Epoch 155] ogbg-molsider: 0.655299 val loss: 0.685178
[Epoch 155] ogbg-molsider: 0.653487 test loss: 0.652091
[Epoch 156; Iter    20/   32] train: loss: 0.2503994
[Epoch 156] ogbg-molsider: 0.651567 val loss: 0.790332
[Epoch 156] ogbg-molsider: 0.656405 test loss: 0.656723
[Epoch 157; Iter    18/   32] train: loss: 0.2333306
[Epoch 157] ogbg-molsider: 0.654213 val loss: 0.830872
[Epoch 157] ogbg-molsider: 0.656699 test loss: 0.651694
[Epoch 158; Iter    16/   32] train: loss: 0.2220826
[Epoch 158] ogbg-molsider: 0.653530 val loss: 0.869033
[Epoch 158] ogbg-molsider: 0.647088 test loss: 0.661943
[Epoch 159; Iter    14/   32] train: loss: 0.2368329
[Epoch 159] ogbg-molsider: 0.653756 val loss: 0.642135
[Epoch 159] ogbg-molsider: 0.650226 test loss: 0.661830
[Epoch 160; Iter    12/   32] train: loss: 0.2426911
[Epoch 160] ogbg-molsider: 0.656682 val loss: 0.660188
[Epoch 160] ogbg-molsider: 0.654561 test loss: 0.677413
[Epoch 161; Iter    10/   32] train: loss: 0.1883577
[Epoch 161] ogbg-molsider: 0.653926 val loss: 0.729561
[Epoch 161] ogbg-molsider: 0.654569 test loss: 0.653051
[Epoch 162; Iter     8/   32] train: loss: 0.2069389
[Epoch 162] ogbg-molsider: 0.654999 val loss: 0.835339
[Epoch 162] ogbg-molsider: 0.661817 test loss: 0.665832
[Epoch 163; Iter     6/   32] train: loss: 0.2082039
[Epoch 163] ogbg-molsider: 0.652056 val loss: 0.888082
[Epoch 163] ogbg-molsider: 0.658133 test loss: 0.657727
[Epoch 164; Iter     4/   32] train: loss: 0.2372172
[Epoch 164] ogbg-molsider: 0.652832 val loss: 0.873619
[Epoch 164] ogbg-molsider: 0.652286 test loss: 0.673993
[Epoch 165; Iter     2/   32] train: loss: 0.2090261
[Epoch 165; Iter    32/   32] train: loss: 0.3416654
[Epoch 165] ogbg-molsider: 0.657197 val loss: 0.863627
[Epoch 165] ogbg-molsider: 0.659398 test loss: 0.662827
[Epoch 166; Iter    30/   32] train: loss: 0.2356010
[Epoch 166] ogbg-molsider: 0.649483 val loss: 0.705788
[Epoch 166] ogbg-molsider: 0.656098 test loss: 0.668528
[Epoch 167; Iter    28/   32] train: loss: 0.2365526
[Epoch 167] ogbg-molsider: 0.647478 val loss: 0.851197
[Epoch 167] ogbg-molsider: 0.658996 test loss: 0.672021
[Epoch 168; Iter    26/   32] train: loss: 0.1943023
[Epoch 168] ogbg-molsider: 0.653468 val loss: 0.859657
[Epoch 168] ogbg-molsider: 0.656184 test loss: 0.671904
[Epoch 169; Iter    24/   32] train: loss: 0.2312495
[Epoch 169] ogbg-molsider: 0.653850 val loss: 0.855704
[Epoch 169] ogbg-molsider: 0.650116 test loss: 0.699884
[Epoch 170; Iter    22/   32] train: loss: 0.2024262
[Epoch 170] ogbg-molsider: 0.653173 val loss: 0.697488
[Epoch 170] ogbg-molsider: 0.654702 test loss: 0.683142
[Epoch 171; Iter    20/   32] train: loss: 0.1956016
[Epoch 171] ogbg-molsider: 0.654197 val loss: 0.824235
[Epoch 171] ogbg-molsider: 0.655932 test loss: 0.693734
[Epoch 172; Iter    18/   32] train: loss: 0.2089757
[Epoch 172] ogbg-molsider: 0.655183 val loss: 0.787240
[Epoch 172] ogbg-molsider: 0.660483 test loss: 0.660309
[Epoch 173; Iter    16/   32] train: loss: 0.1794952
[Epoch 173] ogbg-molsider: 0.656380 val loss: 0.760268
[Epoch 173] ogbg-molsider: 0.657890 test loss: 0.679493
[Epoch 174; Iter    14/   32] train: loss: 0.1972765
[Epoch 174] ogbg-molsider: 0.649915 val loss: 0.805710
[Epoch 174] ogbg-molsider: 0.657945 test loss: 0.680029
[Epoch 175; Iter    12/   32] train: loss: 0.1703670
[Epoch 175] ogbg-molsider: 0.650266 val loss: 0.787746
[Epoch 175] ogbg-molsider: 0.657235 test loss: 0.675222
[Epoch 176; Iter    10/   32] train: loss: 0.2083096
[Epoch 176] ogbg-molsider: 0.647411 val loss: 0.804812
[Epoch 176] ogbg-molsider: 0.653220 test loss: 0.676199
[Epoch 177; Iter     8/   32] train: loss: 0.2021383
[Epoch 177] ogbg-molsider: 0.651684 val loss: 0.744540
[Epoch 177] ogbg-molsider: 0.654174 test loss: 0.679400
[Epoch 178; Iter     6/   32] train: loss: 0.2029074
[Epoch 178] ogbg-molsider: 0.649826 val loss: 0.735467
[Epoch 178] ogbg-molsider: 0.654986 test loss: 0.681465
[Epoch 179; Iter     4/   32] train: loss: 0.2217716
[Epoch 179] ogbg-molsider: 0.651409 val loss: 0.810372
[Epoch 179] ogbg-molsider: 0.653330 test loss: 0.683342
[Epoch 180; Iter     2/   32] train: loss: 0.2188356
[Epoch 180; Iter    32/   32] train: loss: 0.2417689
[Epoch 180] ogbg-molsider: 0.651020 val loss: 0.779409
[Epoch 180] ogbg-molsider: 0.650524 test loss: 0.688768
[Epoch 181; Iter    30/   32] train: loss: 0.1953959
[Epoch 181] ogbg-molsider: 0.651049 val loss: 0.777199
[Epoch 181] ogbg-molsider: 0.650429 test loss: 0.692714
[Epoch 182; Iter    28/   32] train: loss: 0.2147005/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 141] ogbg-molsider: 0.640430 val loss: 0.640796
[Epoch 141] ogbg-molsider: 0.632325 test loss: 0.647150
[Epoch 142; Iter     3/   27] train: loss: 0.2185705
[Epoch 142] ogbg-molsider: 0.630261 val loss: 0.659060
[Epoch 142] ogbg-molsider: 0.623651 test loss: 0.650233
[Epoch 143; Iter     6/   27] train: loss: 0.2733623
[Epoch 143] ogbg-molsider: 0.631907 val loss: 0.648060
[Epoch 143] ogbg-molsider: 0.630062 test loss: 0.627501
[Epoch 144; Iter     9/   27] train: loss: 0.2389771
[Epoch 144] ogbg-molsider: 0.640574 val loss: 0.638533
[Epoch 144] ogbg-molsider: 0.630411 test loss: 0.642179
[Epoch 145; Iter    12/   27] train: loss: 0.2549280
[Epoch 145] ogbg-molsider: 0.643589 val loss: 0.633067
[Epoch 145] ogbg-molsider: 0.633799 test loss: 0.632588
[Epoch 146; Iter    15/   27] train: loss: 0.2275539
[Epoch 146] ogbg-molsider: 0.632187 val loss: 0.645417
[Epoch 146] ogbg-molsider: 0.629516 test loss: 0.645940
[Epoch 147; Iter    18/   27] train: loss: 0.2408433
[Epoch 147] ogbg-molsider: 0.638896 val loss: 0.640658
[Epoch 147] ogbg-molsider: 0.641072 test loss: 0.625524
[Epoch 148; Iter    21/   27] train: loss: 0.2396183
[Epoch 148] ogbg-molsider: 0.637667 val loss: 0.653059
[Epoch 148] ogbg-molsider: 0.635470 test loss: 0.642839
[Epoch 149; Iter    24/   27] train: loss: 0.2654871
[Epoch 149] ogbg-molsider: 0.636167 val loss: 0.644533
[Epoch 149] ogbg-molsider: 0.634408 test loss: 0.636285
[Epoch 150; Iter    27/   27] train: loss: 0.2439592
[Epoch 150] ogbg-molsider: 0.631609 val loss: 0.655056
[Epoch 150] ogbg-molsider: 0.635370 test loss: 0.642444
[Epoch 151] ogbg-molsider: 0.636587 val loss: 0.651878
[Epoch 151] ogbg-molsider: 0.635944 test loss: 0.641489
[Epoch 152; Iter     3/   27] train: loss: 0.2337986
[Epoch 152] ogbg-molsider: 0.640905 val loss: 0.652045
[Epoch 152] ogbg-molsider: 0.633347 test loss: 0.650091
[Epoch 153; Iter     6/   27] train: loss: 0.2246352
[Epoch 153] ogbg-molsider: 0.638250 val loss: 0.653568
[Epoch 153] ogbg-molsider: 0.635495 test loss: 0.639412
[Epoch 154; Iter     9/   27] train: loss: 0.2483091
[Epoch 154] ogbg-molsider: 0.639864 val loss: 0.656031
[Epoch 154] ogbg-molsider: 0.635336 test loss: 0.643648
[Epoch 155; Iter    12/   27] train: loss: 0.2393281
[Epoch 155] ogbg-molsider: 0.642513 val loss: 0.660861
[Epoch 155] ogbg-molsider: 0.632025 test loss: 0.654785
[Epoch 156; Iter    15/   27] train: loss: 0.2127528
[Epoch 156] ogbg-molsider: 0.642389 val loss: 0.650046
[Epoch 156] ogbg-molsider: 0.631680 test loss: 0.645782
[Epoch 157; Iter    18/   27] train: loss: 0.2182469
[Epoch 157] ogbg-molsider: 0.641482 val loss: 0.662705
[Epoch 157] ogbg-molsider: 0.631777 test loss: 0.658392
[Epoch 158; Iter    21/   27] train: loss: 0.2101244
[Epoch 158] ogbg-molsider: 0.637714 val loss: 0.663021
[Epoch 158] ogbg-molsider: 0.633251 test loss: 0.649505
[Epoch 159; Iter    24/   27] train: loss: 0.2231407
[Epoch 159] ogbg-molsider: 0.640502 val loss: 0.664750
[Epoch 159] ogbg-molsider: 0.626241 test loss: 0.666710
[Epoch 160; Iter    27/   27] train: loss: 0.2570772
[Epoch 160] ogbg-molsider: 0.641060 val loss: 0.659565
[Epoch 160] ogbg-molsider: 0.636172 test loss: 0.648719
[Epoch 161] ogbg-molsider: 0.637524 val loss: 0.670637
[Epoch 161] ogbg-molsider: 0.629738 test loss: 0.656618
[Epoch 162; Iter     3/   27] train: loss: 0.2056888
[Epoch 162] ogbg-molsider: 0.634221 val loss: 0.678871
[Epoch 162] ogbg-molsider: 0.627816 test loss: 0.667644
[Epoch 163; Iter     6/   27] train: loss: 0.2181276
[Epoch 163] ogbg-molsider: 0.640818 val loss: 0.682912
[Epoch 163] ogbg-molsider: 0.626779 test loss: 0.679385
[Epoch 164; Iter     9/   27] train: loss: 0.2419676
[Epoch 164] ogbg-molsider: 0.639985 val loss: 0.669937
[Epoch 164] ogbg-molsider: 0.635094 test loss: 0.648603
[Epoch 165; Iter    12/   27] train: loss: 0.1919621
[Epoch 165] ogbg-molsider: 0.637753 val loss: 0.676627
[Epoch 165] ogbg-molsider: 0.626194 test loss: 0.683707
[Epoch 166; Iter    15/   27] train: loss: 0.2601717
[Epoch 166] ogbg-molsider: 0.640705 val loss: 0.670166
[Epoch 166] ogbg-molsider: 0.629336 test loss: 0.660811
[Epoch 167; Iter    18/   27] train: loss: 0.2161276
[Epoch 167] ogbg-molsider: 0.638831 val loss: 0.676791
[Epoch 167] ogbg-molsider: 0.626907 test loss: 0.670172
[Epoch 168; Iter    21/   27] train: loss: 0.2179502
[Epoch 168] ogbg-molsider: 0.636731 val loss: 0.681226
[Epoch 168] ogbg-molsider: 0.623429 test loss: 0.678418
[Epoch 169; Iter    24/   27] train: loss: 0.1903012
[Epoch 169] ogbg-molsider: 0.643506 val loss: 0.673896
[Epoch 169] ogbg-molsider: 0.625245 test loss: 0.681117
[Epoch 170; Iter    27/   27] train: loss: 0.2528152
[Epoch 170] ogbg-molsider: 0.635987 val loss: 0.677783
[Epoch 170] ogbg-molsider: 0.628915 test loss: 0.664747
[Epoch 171] ogbg-molsider: 0.632180 val loss: 0.682955
[Epoch 171] ogbg-molsider: 0.626409 test loss: 0.674773
[Epoch 172; Iter     3/   27] train: loss: 0.2094730
[Epoch 172] ogbg-molsider: 0.633646 val loss: 0.673865
[Epoch 172] ogbg-molsider: 0.631953 test loss: 0.655077
[Epoch 173; Iter     6/   27] train: loss: 0.2408088
[Epoch 173] ogbg-molsider: 0.636929 val loss: 0.681276
[Epoch 173] ogbg-molsider: 0.629213 test loss: 0.676450
[Epoch 174; Iter     9/   27] train: loss: 0.2192404
[Epoch 174] ogbg-molsider: 0.641884 val loss: 0.670926
[Epoch 174] ogbg-molsider: 0.631389 test loss: 0.659101
[Epoch 175; Iter    12/   27] train: loss: 0.2146883
[Epoch 175] ogbg-molsider: 0.640904 val loss: 0.676230
[Epoch 175] ogbg-molsider: 0.628091 test loss: 0.667063
[Epoch 176; Iter    15/   27] train: loss: 0.2515512
[Epoch 176] ogbg-molsider: 0.641113 val loss: 0.669756
[Epoch 176] ogbg-molsider: 0.627433 test loss: 0.671395
[Epoch 177; Iter    18/   27] train: loss: 0.2323638
[Epoch 177] ogbg-molsider: 0.637403 val loss: 0.680702
[Epoch 177] ogbg-molsider: 0.626743 test loss: 0.680408
[Epoch 178; Iter    21/   27] train: loss: 0.2014294
[Epoch 178] ogbg-molsider: 0.639723 val loss: 0.679180
[Epoch 178] ogbg-molsider: 0.626749 test loss: 0.672684
[Epoch 179; Iter    24/   27] train: loss: 0.2471608
[Epoch 179] ogbg-molsider: 0.640675 val loss: 0.683986
[Epoch 179] ogbg-molsider: 0.628588 test loss: 0.680508
Early stopping criterion based on -ogbg-molsider- that should be max reached after 179 epochs. Best model checkpoint was in epoch 119.
Statistics on  val_best_checkpoint
mean_pred: 0.4141182005405426
std_pred: 2.985290288925171
mean_targets: 0.5445094108581543
std_targets: 0.4980473220348358
prcauc: 0.6543704360607369
rocauc: 0.657350576777334
ogbg-molsider: 0.657350576777334
OGBNanLabelBCEWithLogitsLoss: 0.5859358112017313
Statistics on  test
mean_pred: 0.7544996738433838
std_pred: 2.920987129211426
mean_targets: 0.5734265446662903
std_targets: 0.4946112036705017
prcauc: 0.6620670763068586
rocauc: 0.6405285692950008
ogbg-molsider: 0.6405285692950008
OGBNanLabelBCEWithLogitsLoss: 0.5903255277209811
Statistics on  train
mean_pred: 0.6730620861053467
std_pred: 3.2235405445098877
mean_targets: 0.5732952356338501
std_targets: 0.49460935592651367
prcauc: 0.8985166951819907
rocauc: 0.9326843322743831
ogbg-molsider: 0.9326843322743831
OGBNanLabelBCEWithLogitsLoss: 0.2661790720842503
[Epoch 141] ogbg-molsider: 0.647726 val loss: 0.624579
[Epoch 141] ogbg-molsider: 0.640539 test loss: 0.630984
[Epoch 142; Iter     3/   27] train: loss: 0.2180274
[Epoch 142] ogbg-molsider: 0.640055 val loss: 0.626559
[Epoch 142] ogbg-molsider: 0.634952 test loss: 0.638571
[Epoch 143; Iter     6/   27] train: loss: 0.2359665
[Epoch 143] ogbg-molsider: 0.644749 val loss: 0.634010
[Epoch 143] ogbg-molsider: 0.642799 test loss: 0.630080
[Epoch 144; Iter     9/   27] train: loss: 0.2222633
[Epoch 144] ogbg-molsider: 0.648551 val loss: 0.641083
[Epoch 144] ogbg-molsider: 0.633371 test loss: 0.662322
[Epoch 145; Iter    12/   27] train: loss: 0.2305816
[Epoch 145] ogbg-molsider: 0.642546 val loss: 0.635096
[Epoch 145] ogbg-molsider: 0.637216 test loss: 0.645757
[Epoch 146; Iter    15/   27] train: loss: 0.2491395
[Epoch 146] ogbg-molsider: 0.657297 val loss: 0.636231
[Epoch 146] ogbg-molsider: 0.637367 test loss: 0.654446
[Epoch 147; Iter    18/   27] train: loss: 0.2060475
[Epoch 147] ogbg-molsider: 0.653438 val loss: 0.648087
[Epoch 147] ogbg-molsider: 0.637824 test loss: 0.657704
[Epoch 148; Iter    21/   27] train: loss: 0.2147028
[Epoch 148] ogbg-molsider: 0.647792 val loss: 0.638037
[Epoch 148] ogbg-molsider: 0.638822 test loss: 0.645427
[Epoch 149; Iter    24/   27] train: loss: 0.2611416
[Epoch 149] ogbg-molsider: 0.644349 val loss: 0.661699
[Epoch 149] ogbg-molsider: 0.638675 test loss: 0.656884
[Epoch 150; Iter    27/   27] train: loss: 0.2431468
[Epoch 150] ogbg-molsider: 0.647264 val loss: 0.651996
[Epoch 150] ogbg-molsider: 0.641246 test loss: 0.643501
[Epoch 151] ogbg-molsider: 0.657420 val loss: 0.622154
[Epoch 151] ogbg-molsider: 0.643088 test loss: 0.630038
[Epoch 152; Iter     3/   27] train: loss: 0.2358363
[Epoch 152] ogbg-molsider: 0.653951 val loss: 0.637274
[Epoch 152] ogbg-molsider: 0.640504 test loss: 0.652340
[Epoch 153; Iter     6/   27] train: loss: 0.2367735
[Epoch 153] ogbg-molsider: 0.657068 val loss: 0.630718
[Epoch 153] ogbg-molsider: 0.641740 test loss: 0.652141
[Epoch 154; Iter     9/   27] train: loss: 0.2191223
[Epoch 154] ogbg-molsider: 0.655635 val loss: 0.631140
[Epoch 154] ogbg-molsider: 0.639816 test loss: 0.659422
[Epoch 155; Iter    12/   27] train: loss: 0.2099583
[Epoch 155] ogbg-molsider: 0.655561 val loss: 0.639489
[Epoch 155] ogbg-molsider: 0.640890 test loss: 0.662310
[Epoch 156; Iter    15/   27] train: loss: 0.2329537
[Epoch 156] ogbg-molsider: 0.652429 val loss: 0.640672
[Epoch 156] ogbg-molsider: 0.638596 test loss: 0.656386
[Epoch 157; Iter    18/   27] train: loss: 0.2428758
[Epoch 157] ogbg-molsider: 0.653925 val loss: 0.642567
[Epoch 157] ogbg-molsider: 0.639136 test loss: 0.664942
[Epoch 158; Iter    21/   27] train: loss: 0.1902336
[Epoch 158] ogbg-molsider: 0.657305 val loss: 0.641158
[Epoch 158] ogbg-molsider: 0.640260 test loss: 0.663525
[Epoch 159; Iter    24/   27] train: loss: 0.2042065
[Epoch 159] ogbg-molsider: 0.650551 val loss: 0.642643
[Epoch 159] ogbg-molsider: 0.642344 test loss: 0.654195
[Epoch 160; Iter    27/   27] train: loss: 0.2034940
[Epoch 160] ogbg-molsider: 0.649868 val loss: 0.650763
[Epoch 160] ogbg-molsider: 0.639841 test loss: 0.665596
[Epoch 161] ogbg-molsider: 0.652668 val loss: 0.645449
[Epoch 161] ogbg-molsider: 0.639047 test loss: 0.665836
[Epoch 162; Iter     3/   27] train: loss: 0.1983722
[Epoch 162] ogbg-molsider: 0.651865 val loss: 0.651305
[Epoch 162] ogbg-molsider: 0.643538 test loss: 0.665054
[Epoch 163; Iter     6/   27] train: loss: 0.1705250
[Epoch 163] ogbg-molsider: 0.654885 val loss: 0.647779
[Epoch 163] ogbg-molsider: 0.644819 test loss: 0.658486
[Epoch 164; Iter     9/   27] train: loss: 0.1740106
[Epoch 164] ogbg-molsider: 0.652509 val loss: 0.654056
[Epoch 164] ogbg-molsider: 0.641039 test loss: 0.676756
[Epoch 165; Iter    12/   27] train: loss: 0.2071880
[Epoch 165] ogbg-molsider: 0.651770 val loss: 0.647637
[Epoch 165] ogbg-molsider: 0.645758 test loss: 0.657497
[Epoch 166; Iter    15/   27] train: loss: 0.2098375
[Epoch 166] ogbg-molsider: 0.650096 val loss: 0.655038
[Epoch 166] ogbg-molsider: 0.635629 test loss: 0.681346
[Epoch 167; Iter    18/   27] train: loss: 0.1912082
[Epoch 167] ogbg-molsider: 0.654854 val loss: 0.654551
[Epoch 167] ogbg-molsider: 0.640618 test loss: 0.672898
[Epoch 168; Iter    21/   27] train: loss: 0.2200599
[Epoch 168] ogbg-molsider: 0.650321 val loss: 0.665345
[Epoch 168] ogbg-molsider: 0.641127 test loss: 0.677503
[Epoch 169; Iter    24/   27] train: loss: 0.2154242
[Epoch 169] ogbg-molsider: 0.647773 val loss: 0.667887
[Epoch 169] ogbg-molsider: 0.639031 test loss: 0.681071
[Epoch 170; Iter    27/   27] train: loss: 0.1868955
[Epoch 170] ogbg-molsider: 0.648165 val loss: 0.658343
[Epoch 170] ogbg-molsider: 0.633923 test loss: 0.678567
[Epoch 171] ogbg-molsider: 0.656448 val loss: 0.663992
[Epoch 171] ogbg-molsider: 0.640845 test loss: 0.683705
[Epoch 172; Iter     3/   27] train: loss: 0.2058695
[Epoch 172] ogbg-molsider: 0.651893 val loss: 0.664458
[Epoch 172] ogbg-molsider: 0.637269 test loss: 0.683411
[Epoch 173; Iter     6/   27] train: loss: 0.1714624
[Epoch 173] ogbg-molsider: 0.646864 val loss: 0.675424
[Epoch 173] ogbg-molsider: 0.638107 test loss: 0.690409
[Epoch 174; Iter     9/   27] train: loss: 0.2075629
[Epoch 174] ogbg-molsider: 0.647910 val loss: 0.671390
[Epoch 174] ogbg-molsider: 0.636700 test loss: 0.689520
[Epoch 175; Iter    12/   27] train: loss: 0.1803976
[Epoch 175] ogbg-molsider: 0.651451 val loss: 0.674255
[Epoch 175] ogbg-molsider: 0.638674 test loss: 0.694095
[Epoch 176; Iter    15/   27] train: loss: 0.1882213
[Epoch 176] ogbg-molsider: 0.651895 val loss: 0.668273
[Epoch 176] ogbg-molsider: 0.634408 test loss: 0.688512
[Epoch 177; Iter    18/   27] train: loss: 0.1764857
[Epoch 177] ogbg-molsider: 0.652193 val loss: 0.679945
[Epoch 177] ogbg-molsider: 0.637760 test loss: 0.696422
[Epoch 178; Iter    21/   27] train: loss: 0.1982837
[Epoch 178] ogbg-molsider: 0.650609 val loss: 0.666632
[Epoch 178] ogbg-molsider: 0.637393 test loss: 0.684893
[Epoch 179; Iter    24/   27] train: loss: 0.1824611
[Epoch 179] ogbg-molsider: 0.648339 val loss: 0.669938
[Epoch 179] ogbg-molsider: 0.634937 test loss: 0.690066
[Epoch 180; Iter    27/   27] train: loss: 0.2060564
[Epoch 180] ogbg-molsider: 0.652081 val loss: 0.666776
[Epoch 180] ogbg-molsider: 0.634681 test loss: 0.690967
[Epoch 181] ogbg-molsider: 0.653621 val loss: 0.669167
[Epoch 181] ogbg-molsider: 0.636514 test loss: 0.689908
[Epoch 182; Iter     3/   27] train: loss: 0.1680309
[Epoch 182] ogbg-molsider: 0.650899 val loss: 0.673187
[Epoch 182] ogbg-molsider: 0.636309 test loss: 0.691850
[Epoch 183; Iter     6/   27] train: loss: 0.1657457
[Epoch 183] ogbg-molsider: 0.648069 val loss: 0.673581
[Epoch 183] ogbg-molsider: 0.636272 test loss: 0.690105
[Epoch 184; Iter     9/   27] train: loss: 0.1703120
[Epoch 184] ogbg-molsider: 0.651065 val loss: 0.677804
[Epoch 184] ogbg-molsider: 0.637145 test loss: 0.695462
[Epoch 185; Iter    12/   27] train: loss: 0.1546377
[Epoch 185] ogbg-molsider: 0.648779 val loss: 0.677228
[Epoch 185] ogbg-molsider: 0.636987 test loss: 0.690096
[Epoch 186; Iter    15/   27] train: loss: 0.1762278
[Epoch 186] ogbg-molsider: 0.647765 val loss: 0.683269
[Epoch 186] ogbg-molsider: 0.640469 test loss: 0.690739
[Epoch 187; Iter    18/   27] train: loss: 0.1901311
[Epoch 187] ogbg-molsider: 0.651146 val loss: 0.674219
[Epoch 187] ogbg-molsider: 0.637173 test loss: 0.691746
[Epoch 188; Iter    21/   27] train: loss: 0.1693032
[Epoch 188] ogbg-molsider: 0.649808 val loss: 0.677765
[Epoch 188] ogbg-molsider: 0.636894 test loss: 0.690798
[Epoch 189; Iter    24/   27] train: loss: 0.2018231
[Epoch 189] ogbg-molsider: 0.649989 val loss: 0.679252
[Epoch 189] ogbg-molsider: 0.637749 test loss: 0.695195
[Epoch 190; Iter    27/   27] train: loss: 0.1891011
[Epoch 190] ogbg-molsider: 0.646847 val loss: 0.680711
[Epoch 190] ogbg-molsider: 0.638299 test loss: 0.686434
[Epoch 191] ogbg-molsider: 0.647613 val loss: 0.681906
[Epoch 191] ogbg-molsider: 0.634432 test loss: 0.701555
[Epoch 192; Iter     3/   27] train: loss: 0.1608466
[Epoch 192] ogbg-molsider: 0.648621 val loss: 0.691114
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)

[Epoch 182] ogbg-molsider: 0.653382 val loss: 0.776376
[Epoch 182] ogbg-molsider: 0.652655 test loss: 0.683070
[Epoch 183; Iter    26/   32] train: loss: 0.2665758
[Epoch 183] ogbg-molsider: 0.651503 val loss: 0.781853
[Epoch 183] ogbg-molsider: 0.654693 test loss: 0.682297
[Epoch 184; Iter    24/   32] train: loss: 0.1945383
[Epoch 184] ogbg-molsider: 0.650437 val loss: 0.787540
[Epoch 184] ogbg-molsider: 0.655336 test loss: 0.692970
[Epoch 185; Iter    22/   32] train: loss: 0.1839748
[Epoch 185] ogbg-molsider: 0.650197 val loss: 0.802016
[Epoch 185] ogbg-molsider: 0.654429 test loss: 0.687502
[Epoch 186; Iter    20/   32] train: loss: 0.2193599
[Epoch 186] ogbg-molsider: 0.650196 val loss: 0.808072
[Epoch 186] ogbg-molsider: 0.657534 test loss: 0.694270
[Epoch 187; Iter    18/   32] train: loss: 0.2627716
[Epoch 187] ogbg-molsider: 0.649797 val loss: 0.778890
[Epoch 187] ogbg-molsider: 0.654578 test loss: 0.697463
[Epoch 188; Iter    16/   32] train: loss: 0.2034443
[Epoch 188] ogbg-molsider: 0.649044 val loss: 0.770309
[Epoch 188] ogbg-molsider: 0.654974 test loss: 0.684319
[Epoch 189; Iter    14/   32] train: loss: 0.1841913
[Epoch 189] ogbg-molsider: 0.650557 val loss: 0.779904
[Epoch 189] ogbg-molsider: 0.654576 test loss: 0.699844
[Epoch 190; Iter    12/   32] train: loss: 0.1827353
[Epoch 190] ogbg-molsider: 0.649718 val loss: 0.780064
[Epoch 190] ogbg-molsider: 0.654953 test loss: 0.704474
[Epoch 191; Iter    10/   32] train: loss: 0.1974728
[Epoch 191] ogbg-molsider: 0.647878 val loss: 0.729928
[Epoch 191] ogbg-molsider: 0.656514 test loss: 0.691423
[Epoch 192; Iter     8/   32] train: loss: 0.2147649
[Epoch 192] ogbg-molsider: 0.647937 val loss: 0.741860
[Epoch 192] ogbg-molsider: 0.652884 test loss: 0.697833
[Epoch 193; Iter     6/   32] train: loss: 0.1928675
[Epoch 193] ogbg-molsider: 0.651336 val loss: 0.735542
[Epoch 193] ogbg-molsider: 0.652595 test loss: 0.699859
[Epoch 194; Iter     4/   32] train: loss: 0.1670473
[Epoch 194] ogbg-molsider: 0.650680 val loss: 0.752894
[Epoch 194] ogbg-molsider: 0.651702 test loss: 0.693696
[Epoch 195; Iter     2/   32] train: loss: 0.1988619
[Epoch 195; Iter    32/   32] train: loss: 0.3884471
[Epoch 195] ogbg-molsider: 0.647795 val loss: 0.762878
[Epoch 195] ogbg-molsider: 0.648712 test loss: 0.710259
[Epoch 196; Iter    30/   32] train: loss: 0.1858013
[Epoch 196] ogbg-molsider: 0.648027 val loss: 0.769574
[Epoch 196] ogbg-molsider: 0.651847 test loss: 0.703361
[Epoch 197; Iter    28/   32] train: loss: 0.2046643
[Epoch 197] ogbg-molsider: 0.646213 val loss: 0.765181
[Epoch 197] ogbg-molsider: 0.651480 test loss: 0.699989
[Epoch 198; Iter    26/   32] train: loss: 0.1850410
[Epoch 198] ogbg-molsider: 0.648149 val loss: 0.726517
[Epoch 198] ogbg-molsider: 0.655222 test loss: 0.691304
[Epoch 199; Iter    24/   32] train: loss: 0.1633129
[Epoch 199] ogbg-molsider: 0.647721 val loss: 0.685519
[Epoch 199] ogbg-molsider: 0.651036 test loss: 0.699657
[Epoch 200; Iter    22/   32] train: loss: 0.2149553
[Epoch 200] ogbg-molsider: 0.649160 val loss: 0.704039
[Epoch 200] ogbg-molsider: 0.650030 test loss: 0.699923
[Epoch 201; Iter    20/   32] train: loss: 0.1977156
[Epoch 201] ogbg-molsider: 0.647685 val loss: 0.698121
[Epoch 201] ogbg-molsider: 0.648623 test loss: 0.707047
[Epoch 202; Iter    18/   32] train: loss: 0.1831214
[Epoch 202] ogbg-molsider: 0.647764 val loss: 0.707452
[Epoch 202] ogbg-molsider: 0.650184 test loss: 0.702758
[Epoch 203; Iter    16/   32] train: loss: 0.1636280
[Epoch 203] ogbg-molsider: 0.652953 val loss: 0.763701
[Epoch 203] ogbg-molsider: 0.652480 test loss: 0.716572
[Epoch 204; Iter    14/   32] train: loss: 0.1705659
[Epoch 204] ogbg-molsider: 0.650282 val loss: 0.713877
[Epoch 204] ogbg-molsider: 0.651071 test loss: 0.701211
[Epoch 205; Iter    12/   32] train: loss: 0.1839014
[Epoch 205] ogbg-molsider: 0.650705 val loss: 0.720327
[Epoch 205] ogbg-molsider: 0.647560 test loss: 0.706044
[Epoch 206; Iter    10/   32] train: loss: 0.1533005
[Epoch 206] ogbg-molsider: 0.651910 val loss: 0.746792
[Epoch 206] ogbg-molsider: 0.649854 test loss: 0.710384
[Epoch 207; Iter     8/   32] train: loss: 0.2118011
[Epoch 207] ogbg-molsider: 0.649716 val loss: 0.775066
[Epoch 207] ogbg-molsider: 0.648640 test loss: 0.719589
[Epoch 208; Iter     6/   32] train: loss: 0.2033140
[Epoch 208] ogbg-molsider: 0.649329 val loss: 0.777720
[Epoch 208] ogbg-molsider: 0.648021 test loss: 0.717234
[Epoch 209; Iter     4/   32] train: loss: 0.1929547
[Epoch 209] ogbg-molsider: 0.650962 val loss: 0.744957
[Epoch 209] ogbg-molsider: 0.648916 test loss: 0.706497
[Epoch 210; Iter     2/   32] train: loss: 0.1706241
[Epoch 210; Iter    32/   32] train: loss: 0.3783983
[Epoch 210] ogbg-molsider: 0.653055 val loss: 0.721227
[Epoch 210] ogbg-molsider: 0.648678 test loss: 0.712979
Early stopping criterion based on -ogbg-molsider- that should be max reached after 210 epochs. Best model checkpoint was in epoch 150.
Statistics on  val_best_checkpoint
mean_pred: 0.6032513380050659
std_pred: 3.855616569519043
mean_targets: 0.5732086896896362
std_targets: 0.49465426802635193
prcauc: 0.6775586234195351
rocauc: 0.6639638849021158
ogbg-molsider: 0.6639638849021158
OGBNanLabelBCEWithLogitsLoss: 0.6805750897952488
Statistics on  test
mean_pred: 0.8104814291000366
std_pred: 3.812418222427368
mean_targets: 0.5714039206504822
std_targets: 0.4949178397655487
prcauc: 0.6728852786545502
rocauc: 0.657748364943386
ogbg-molsider: 0.657748364943386
OGBNanLabelBCEWithLogitsLoss: 0.6566639457430158
Statistics on  train
mean_pred: 0.7978661060333252
std_pred: 4.028038501739502
mean_targets: 0.5655384659767151
std_targets: 0.4956952929496765
prcauc: 0.9119573044806634
rocauc: 0.9493987160491174
ogbg-molsider: 0.9493987160491174
OGBNanLabelBCEWithLogitsLoss: 0.2297458783723414
All runs completed.
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 192] ogbg-molsider: 0.635013 test loss: 0.701571
[Epoch 193; Iter     6/   27] train: loss: 0.1877980
[Epoch 193] ogbg-molsider: 0.648089 val loss: 0.681958
[Epoch 193] ogbg-molsider: 0.634849 test loss: 0.698153
[Epoch 194; Iter     9/   27] train: loss: 0.1769409
[Epoch 194] ogbg-molsider: 0.650534 val loss: 0.686013
[Epoch 194] ogbg-molsider: 0.639121 test loss: 0.695342
[Epoch 195; Iter    12/   27] train: loss: 0.1726820
[Epoch 195] ogbg-molsider: 0.649424 val loss: 0.681752
[Epoch 195] ogbg-molsider: 0.633914 test loss: 0.705369
[Epoch 196; Iter    15/   27] train: loss: 0.1630740
[Epoch 196] ogbg-molsider: 0.652895 val loss: 0.687523
[Epoch 196] ogbg-molsider: 0.634881 test loss: 0.709150
[Epoch 197; Iter    18/   27] train: loss: 0.1806575
[Epoch 197] ogbg-molsider: 0.655422 val loss: 0.679741
[Epoch 197] ogbg-molsider: 0.637952 test loss: 0.696780
[Epoch 198; Iter    21/   27] train: loss: 0.1830575
[Epoch 198] ogbg-molsider: 0.650800 val loss: 0.685589
[Epoch 198] ogbg-molsider: 0.637276 test loss: 0.699261
[Epoch 199; Iter    24/   27] train: loss: 0.1798258
[Epoch 199] ogbg-molsider: 0.648434 val loss: 0.690441
[Epoch 199] ogbg-molsider: 0.635591 test loss: 0.706551
[Epoch 200; Iter    27/   27] train: loss: 0.1699498
[Epoch 200] ogbg-molsider: 0.650614 val loss: 0.686875
[Epoch 200] ogbg-molsider: 0.637010 test loss: 0.708472
[Epoch 201] ogbg-molsider: 0.650222 val loss: 0.683899
[Epoch 201] ogbg-molsider: 0.638073 test loss: 0.700721
[Epoch 202; Iter     3/   27] train: loss: 0.1658008
[Epoch 202] ogbg-molsider: 0.648108 val loss: 0.693235
[Epoch 202] ogbg-molsider: 0.636296 test loss: 0.710349
[Epoch 203; Iter     6/   27] train: loss: 0.1715453
[Epoch 203] ogbg-molsider: 0.648572 val loss: 0.687895
[Epoch 203] ogbg-molsider: 0.634794 test loss: 0.707722
[Epoch 204; Iter     9/   27] train: loss: 0.1501899
[Epoch 204] ogbg-molsider: 0.648336 val loss: 0.692010
[Epoch 204] ogbg-molsider: 0.636993 test loss: 0.705435
[Epoch 205; Iter    12/   27] train: loss: 0.1504302
[Epoch 205] ogbg-molsider: 0.651027 val loss: 0.692501
[Epoch 205] ogbg-molsider: 0.635106 test loss: 0.715932
[Epoch 206; Iter    15/   27] train: loss: 0.1655330
[Epoch 206] ogbg-molsider: 0.645971 val loss: 0.700906
[Epoch 206] ogbg-molsider: 0.636073 test loss: 0.717892
[Epoch 207; Iter    18/   27] train: loss: 0.1682344
[Epoch 207] ogbg-molsider: 0.646933 val loss: 0.691594
[Epoch 207] ogbg-molsider: 0.635209 test loss: 0.708982
[Epoch 208; Iter    21/   27] train: loss: 0.1256147
[Epoch 208] ogbg-molsider: 0.648655 val loss: 0.697294
[Epoch 208] ogbg-molsider: 0.635480 test loss: 0.716068
[Epoch 209; Iter    24/   27] train: loss: 0.1703904
[Epoch 209] ogbg-molsider: 0.648928 val loss: 0.690313
[Epoch 209] ogbg-molsider: 0.634276 test loss: 0.708083
[Epoch 210; Iter    27/   27] train: loss: 0.1563971
[Epoch 210] ogbg-molsider: 0.648908 val loss: 0.689241
[Epoch 210] ogbg-molsider: 0.636485 test loss: 0.709348
[Epoch 211] ogbg-molsider: 0.648299 val loss: 0.694889
[Epoch 211] ogbg-molsider: 0.636828 test loss: 0.705623
Early stopping criterion based on -ogbg-molsider- that should be max reached after 211 epochs. Best model checkpoint was in epoch 151.
Statistics on  val_best_checkpoint
mean_pred: 0.4877103269100189
std_pred: 3.3186557292938232
mean_targets: 0.5445094108581543
std_targets: 0.4980473220348358
prcauc: 0.6605160957869866
rocauc: 0.6574195490044636
ogbg-molsider: 0.6574195490044636
OGBNanLabelBCEWithLogitsLoss: 0.6221543086899651
Statistics on  test
mean_pred: 0.7017504572868347
std_pred: 3.295548915863037
mean_targets: 0.5734265446662903
std_targets: 0.4946112036705017
prcauc: 0.6593230256316654
rocauc: 0.6430878807895343
ogbg-molsider: 0.6430878807895343
OGBNanLabelBCEWithLogitsLoss: 0.6300382481680976
Statistics on  train
mean_pred: 0.8702400326728821
std_pred: 3.7618298530578613
mean_targets: 0.5732952356338501
std_targets: 0.4946092963218689
prcauc: 0.9311300524519699
rocauc: 0.9635648528168508
ogbg-molsider: 0.9635648528168508
OGBNanLabelBCEWithLogitsLoss: 0.20623305605517495
All runs completed.
