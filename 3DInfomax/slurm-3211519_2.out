>>> Starting run for dataset: clintox
Running configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml on cuda:0
Running configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml on cuda:1
Running configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml on cuda:2
Running configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml on cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/home/users/u102845/.local/lib/python3.9/site-packages/ogb/graphproppred/dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_dict = torch.load(pre_processed_file_path, 'rb')
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/workspace/train.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrain_checkpoint, map_location=device)
/opt/conda/envs/3DInfomax/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[ Using Seed :  5  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.2/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.2_5_26-05_10-31-04
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.2
logdir: runs/static_noise/GraphCL/clintox/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6933196
[Epoch 1] ogbg-molclintox: 0.556269 val loss: 0.694330
[Epoch 1] ogbg-molclintox: 0.609382 test loss: 0.695139
[Epoch 2; Iter    20/   40] train: loss: 0.6922445
[Epoch 2] ogbg-molclintox: 0.580056 val loss: 0.689287
[Epoch 2] ogbg-molclintox: 0.563599 test loss: 0.698795
[Epoch 3; Iter    10/   40] train: loss: 0.6929162
[Epoch 3; Iter    40/   40] train: loss: 0.6926656
[Epoch 3] ogbg-molclintox: 0.576060 val loss: 0.675179
[Epoch 3] ogbg-molclintox: 0.512452 test loss: 0.696500
[Epoch 4; Iter    30/   40] train: loss: 0.6922835
[Epoch 4] ogbg-molclintox: 0.561976 val loss: 0.672761
[Epoch 4] ogbg-molclintox: 0.508104 test loss: 0.694441
[Epoch 5; Iter    20/   40] train: loss: 0.6925136
[Epoch 5] ogbg-molclintox: 0.581230 val loss: 0.677223
[Epoch 5] ogbg-molclintox: 0.504544 test loss: 0.697259
[Epoch 6; Iter    10/   40] train: loss: 0.6910574
[Epoch 6; Iter    40/   40] train: loss: 0.6910799
[Epoch 6] ogbg-molclintox: 0.573038 val loss: 0.674125
[Epoch 6] ogbg-molclintox: 0.502647 test loss: 0.695210
[Epoch 7; Iter    30/   40] train: loss: 0.6917971
[Epoch 7] ogbg-molclintox: 0.581342 val loss: 0.675390
[Epoch 7] ogbg-molclintox: 0.505594 test loss: 0.695947
[Epoch 8; Iter    20/   40] train: loss: 0.6891789
[Epoch 8] ogbg-molclintox: 0.580692 val loss: 0.673213
[Epoch 8] ogbg-molclintox: 0.509068 test loss: 0.694722
[Epoch 9; Iter    10/   40] train: loss: 0.6882832
[Epoch 9; Iter    40/   40] train: loss: 0.6869046
[Epoch 9] ogbg-molclintox: 0.575361 val loss: 0.672456
[Epoch 9] ogbg-molclintox: 0.507868 test loss: 0.692310
[Epoch 10; Iter    30/   40] train: loss: 0.6890491
[Epoch 10] ogbg-molclintox: 0.582403 val loss: 0.669616
[Epoch 10] ogbg-molclintox: 0.508829 test loss: 0.692296
[Epoch 11; Iter    20/   40] train: loss: 0.6872428
[Epoch 11] ogbg-molclintox: 0.577933 val loss: 0.668741
[Epoch 11] ogbg-molclintox: 0.511339 test loss: 0.691041
[Epoch 12; Iter    10/   40] train: loss: 0.6868281
[Epoch 12; Iter    40/   40] train: loss: 0.6859182
[Epoch 12] ogbg-molclintox: 0.567282 val loss: 0.668670
[Epoch 12] ogbg-molclintox: 0.515137 test loss: 0.690155
[Epoch 13; Iter    30/   40] train: loss: 0.6866763
[Epoch 13] ogbg-molclintox: 0.584638 val loss: 0.663056
[Epoch 13] ogbg-molclintox: 0.511077 test loss: 0.686705
[Epoch 14; Iter    20/   40] train: loss: 0.6826138
[Epoch 14] ogbg-molclintox: 0.574662 val loss: 0.658901
[Epoch 14] ogbg-molclintox: 0.515985 test loss: 0.684095
[Epoch 15; Iter    10/   40] train: loss: 0.6828419
[Epoch 15; Iter    40/   40] train: loss: 0.6814272
[Epoch 15] ogbg-molclintox: 0.580980 val loss: 0.663659
[Epoch 15] ogbg-molclintox: 0.517786 test loss: 0.686483
[Epoch 16; Iter    30/   40] train: loss: 0.6790709
[Epoch 16] ogbg-molclintox: 0.574662 val loss: 0.660297
[Epoch 16] ogbg-molclintox: 0.521857 test loss: 0.683814
[Epoch 17; Iter    20/   40] train: loss: 0.6809219
[Epoch 17] ogbg-molclintox: 0.575273 val loss: 0.658711
[Epoch 17] ogbg-molclintox: 0.516635 test loss: 0.681738
[Epoch 18; Iter    10/   40] train: loss: 0.6767781
[Epoch 18; Iter    40/   40] train: loss: 0.6837223
[Epoch 18] ogbg-molclintox: 0.627054 val loss: 0.614096
[Epoch 18] ogbg-molclintox: 0.623595 test loss: 0.689999
[Epoch 19; Iter    30/   40] train: loss: 0.6425541
[Epoch 19] ogbg-molclintox: 0.631826 val loss: 0.583440
[Epoch 19] ogbg-molclintox: 0.687408 test loss: 0.650776
[Epoch 20; Iter    20/   40] train: loss: 0.6100507
[Epoch 20] ogbg-molclintox: 0.606753 val loss: 0.526159
[Epoch 20] ogbg-molclintox: 0.665807 test loss: 0.632479
[Epoch 21; Iter    10/   40] train: loss: 0.5775610
[Epoch 21; Iter    40/   40] train: loss: 0.4819593
[Epoch 21] ogbg-molclintox: 0.707572 val loss: 0.506558
[Epoch 21] ogbg-molclintox: 0.693527 test loss: 0.616125
[Epoch 22; Iter    30/   40] train: loss: 0.4943980
[Epoch 22] ogbg-molclintox: 0.730947 val loss: 0.414501
[Epoch 22] ogbg-molclintox: 0.650493 test loss: 0.495276
[Epoch 23; Iter    20/   40] train: loss: 0.3445851
[Epoch 23] ogbg-molclintox: 0.652580 val loss: 0.285810
[Epoch 23] ogbg-molclintox: 0.631230 test loss: 0.379097
[Epoch 24; Iter    10/   40] train: loss: 0.3041680
[Epoch 24; Iter    40/   40] train: loss: 0.5547674
[Epoch 24] ogbg-molclintox: 0.718536 val loss: 0.282564
[Epoch 24] ogbg-molclintox: 0.661560 test loss: 0.347396
[Epoch 25; Iter    30/   40] train: loss: 0.2771109
[Epoch 25] ogbg-molclintox: 0.669648 val loss: 0.243036
[Epoch 25] ogbg-molclintox: 0.631443 test loss: 0.411623
[Epoch 26; Iter    20/   40] train: loss: 0.2489429
[Epoch 26] ogbg-molclintox: 0.800101 val loss: 0.369690
[Epoch 26] ogbg-molclintox: 0.639223 test loss: 0.610658
[Epoch 27; Iter    10/   40] train: loss: 0.2090701
[Epoch 27; Iter    40/   40] train: loss: 0.1012079
[Epoch 27] ogbg-molclintox: 0.731285 val loss: 0.357723
[Epoch 27] ogbg-molclintox: 0.599513 test loss: 0.579054
[Epoch 28; Iter    30/   40] train: loss: 0.3047849
[Epoch 28] ogbg-molclintox: 0.815482 val loss: 0.340737
[Epoch 28] ogbg-molclintox: 0.640011 test loss: 0.594688
[Epoch 29; Iter    20/   40] train: loss: 0.2285023
[Epoch 29] ogbg-molclintox: 0.822015 val loss: 0.309710
[Epoch 29] ogbg-molclintox: 0.641461 test loss: 0.538783
[Epoch 30; Iter    10/   40] train: loss: 0.1740038
[Epoch 30; Iter    40/   40] train: loss: 0.3553416
[Epoch 30] ogbg-molclintox: 0.831742 val loss: 0.192343
[Epoch 30] ogbg-molclintox: 0.654740 test loss: 0.389031
[Epoch 31; Iter    30/   40] train: loss: 0.2602244
[Epoch 31] ogbg-molclintox: 0.828196 val loss: 0.177790
[ Using Seed :  6  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.1/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.1_6_26-05_10-31-01
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.1
logdir: runs/static_noise/GraphCL/clintox/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6930810
[Epoch 1] ogbg-molclintox: 0.455060 val loss: 0.692192
[Epoch 1] ogbg-molclintox: 0.610426 test loss: 0.694893
[Epoch 2; Iter    20/   40] train: loss: 0.6934286
[Epoch 2] ogbg-molclintox: 0.418036 val loss: 0.688602
[Epoch 2] ogbg-molclintox: 0.566297 test loss: 0.695446
[Epoch 3; Iter    10/   40] train: loss: 0.6921488
[Epoch 3; Iter    40/   40] train: loss: 0.6950648
[Epoch 3] ogbg-molclintox: 0.397232 val loss: 0.687848
[Epoch 3] ogbg-molclintox: 0.539415 test loss: 0.695945
[Epoch 4; Iter    30/   40] train: loss: 0.6941494
[Epoch 4] ogbg-molclintox: 0.414901 val loss: 0.685651
[Epoch 4] ogbg-molclintox: 0.530995 test loss: 0.694075
[Epoch 5; Iter    20/   40] train: loss: 0.6918802
[Epoch 5] ogbg-molclintox: 0.423704 val loss: 0.686088
[Epoch 5] ogbg-molclintox: 0.545174 test loss: 0.694367
[Epoch 6; Iter    10/   40] train: loss: 0.6903689
[Epoch 6; Iter    40/   40] train: loss: 0.6904860
[Epoch 6] ogbg-molclintox: 0.417973 val loss: 0.685531
[Epoch 6] ogbg-molclintox: 0.536217 test loss: 0.693589
[Epoch 7; Iter    30/   40] train: loss: 0.6895095
[Epoch 7] ogbg-molclintox: 0.413977 val loss: 0.684389
[Epoch 7] ogbg-molclintox: 0.542164 test loss: 0.692761
[Epoch 8; Iter    20/   40] train: loss: 0.6885493
[Epoch 8] ogbg-molclintox: 0.420907 val loss: 0.684740
[Epoch 8] ogbg-molclintox: 0.544898 test loss: 0.692325
[Epoch 9; Iter    10/   40] train: loss: 0.6896067
[Epoch 9; Iter    40/   40] train: loss: 0.6859230
[Epoch 9] ogbg-molclintox: 0.437089 val loss: 0.681536
[Epoch 9] ogbg-molclintox: 0.535817 test loss: 0.689973
[Epoch 10; Iter    30/   40] train: loss: 0.6884815
[Epoch 10] ogbg-molclintox: 0.428961 val loss: 0.679436
[Epoch 10] ogbg-molclintox: 0.547711 test loss: 0.688548
[Epoch 11; Iter    20/   40] train: loss: 0.6842557
[Epoch 11] ogbg-molclintox: 0.432595 val loss: 0.678541
[Epoch 11] ogbg-molclintox: 0.550359 test loss: 0.687552
[Epoch 12; Iter    10/   40] train: loss: 0.6852477
[Epoch 12; Iter    40/   40] train: loss: 0.6820330
[Epoch 12] ogbg-molclintox: 0.432981 val loss: 0.676723
[Epoch 12] ogbg-molclintox: 0.555069 test loss: 0.686392
[Epoch 13; Iter    30/   40] train: loss: 0.6841285
[Epoch 13] ogbg-molclintox: 0.434130 val loss: 0.675793
[Epoch 13] ogbg-molclintox: 0.554830 test loss: 0.684599
[Epoch 14; Iter    20/   40] train: loss: 0.6791465
[Epoch 14] ogbg-molclintox: 0.441085 val loss: 0.673819
[Epoch 14] ogbg-molclintox: 0.549459 test loss: 0.682796
[Epoch 15; Iter    10/   40] train: loss: 0.6790445
[Epoch 15; Iter    40/   40] train: loss: 0.6758121
[Epoch 15] ogbg-molclintox: 0.449589 val loss: 0.670947
[Epoch 15] ogbg-molclintox: 0.564736 test loss: 0.681059
[Epoch 16; Iter    30/   40] train: loss: 0.6752627
[Epoch 16] ogbg-molclintox: 0.440786 val loss: 0.667722
[Epoch 16] ogbg-molclintox: 0.561288 test loss: 0.678462
[Epoch 17; Iter    20/   40] train: loss: 0.6740177
[Epoch 17] ogbg-molclintox: 0.450425 val loss: 0.665337
[Epoch 17] ogbg-molclintox: 0.565311 test loss: 0.676313
[Epoch 18; Iter    10/   40] train: loss: 0.6706129
[Epoch 18; Iter    40/   40] train: loss: 0.6567793
[Epoch 18] ogbg-molclintox: 0.665076 val loss: 0.661959
[Epoch 18] ogbg-molclintox: 0.670865 test loss: 0.697416
[Epoch 19; Iter    30/   40] train: loss: 0.6397479
[Epoch 19] ogbg-molclintox: 0.672392 val loss: 0.653005
[Epoch 19] ogbg-molclintox: 0.679886 test loss: 0.691243
[Epoch 20; Iter    20/   40] train: loss: 0.6402827
[Epoch 20] ogbg-molclintox: 0.707196 val loss: 0.593932
[Epoch 20] ogbg-molclintox: 0.685530 test loss: 0.644447
[Epoch 21; Iter    10/   40] train: loss: 0.5471702
[Epoch 21; Iter    40/   40] train: loss: 0.4715966
[Epoch 21] ogbg-molclintox: 0.856801 val loss: 0.507514
[Epoch 21] ogbg-molclintox: 0.727055 test loss: 0.550675
[Epoch 22; Iter    30/   40] train: loss: 0.4365469
[Epoch 22] ogbg-molclintox: 0.857289 val loss: 0.560944
[Epoch 22] ogbg-molclintox: 0.596951 test loss: 0.538566
[Epoch 23; Iter    20/   40] train: loss: 0.4167748
[Epoch 23] ogbg-molclintox: 0.772164 val loss: 0.474111
[Epoch 23] ogbg-molclintox: 0.636732 test loss: 0.517873
[Epoch 24; Iter    10/   40] train: loss: 0.4024986
[Epoch 24; Iter    40/   40] train: loss: 0.3160728
[Epoch 24] ogbg-molclintox: 0.888642 val loss: 0.385489
[Epoch 24] ogbg-molclintox: 0.618623 test loss: 0.406677
[Epoch 25; Iter    30/   40] train: loss: 0.2568397
[Epoch 25] ogbg-molclintox: 0.861496 val loss: 0.455056
[Epoch 25] ogbg-molclintox: 0.667417 test loss: 0.394743
[Epoch 26; Iter    20/   40] train: loss: 0.3057718
[Epoch 26] ogbg-molclintox: 0.875756 val loss: 0.444963
[Epoch 26] ogbg-molclintox: 0.699010 test loss: 0.419026
[Epoch 27; Iter    10/   40] train: loss: 0.2278647
[Epoch 27; Iter    40/   40] train: loss: 0.1228296
[Epoch 27] ogbg-molclintox: 0.837920 val loss: 0.425066
[Epoch 27] ogbg-molclintox: 0.667518 test loss: 0.377902
[Epoch 28; Iter    30/   40] train: loss: 0.2860988
[Epoch 28] ogbg-molclintox: 0.803693 val loss: 0.221504
[Epoch 28] ogbg-molclintox: 0.603581 test loss: 0.294060
[Epoch 29; Iter    20/   40] train: loss: 0.2128897
[Epoch 29] ogbg-molclintox: 0.856488 val loss: 0.151918
[Epoch 29] ogbg-molclintox: 0.642708 test loss: 0.233882
[Epoch 30; Iter    10/   40] train: loss: 0.2069277
[Epoch 30; Iter    40/   40] train: loss: 0.0843717
[Epoch 30] ogbg-molclintox: 0.917124 val loss: 0.195478
[Epoch 30] ogbg-molclintox: 0.660048 test loss: 0.295465
[Epoch 31; Iter    30/   40] train: loss: 0.3091596
[Epoch 31] ogbg-molclintox: 0.799746 val loss: 0.250161
[ Using Seed :  6  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.05/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.05_6_26-05_10-30-57
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.05
logdir: runs/static_noise/GraphCL/clintox/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6933632
[Epoch 1] ogbg-molclintox: 0.435543 val loss: 0.691734
[Epoch 1] ogbg-molclintox: 0.605936 test loss: 0.694301
[Epoch 2; Iter    20/   40] train: loss: 0.6936039
[Epoch 2] ogbg-molclintox: 0.380614 val loss: 0.688280
[Epoch 2] ogbg-molclintox: 0.561520 test loss: 0.693687
[Epoch 3; Iter    10/   40] train: loss: 0.6916760
[Epoch 3; Iter    40/   40] train: loss: 0.6935499
[Epoch 3] ogbg-molclintox: 0.363346 val loss: 0.688345
[Epoch 3] ogbg-molclintox: 0.543292 test loss: 0.694600
[Epoch 4; Iter    30/   40] train: loss: 0.6937822
[Epoch 4] ogbg-molclintox: 0.384560 val loss: 0.686590
[Epoch 4] ogbg-molclintox: 0.534212 test loss: 0.692522
[Epoch 5; Iter    20/   40] train: loss: 0.6923540
[Epoch 5] ogbg-molclintox: 0.388644 val loss: 0.686427
[Epoch 5] ogbg-molclintox: 0.540745 test loss: 0.692418
[Epoch 6; Iter    10/   40] train: loss: 0.6906149
[Epoch 6; Iter    40/   40] train: loss: 0.6904550
[Epoch 6] ogbg-molclintox: 0.387582 val loss: 0.686308
[Epoch 6] ogbg-molclintox: 0.538059 test loss: 0.692133
[Epoch 7; Iter    30/   40] train: loss: 0.6888296
[Epoch 7] ogbg-molclintox: 0.385709 val loss: 0.685442
[Epoch 7] ogbg-molclintox: 0.549239 test loss: 0.691576
[Epoch 8; Iter    20/   40] train: loss: 0.6887616
[Epoch 8] ogbg-molclintox: 0.391416 val loss: 0.684886
[Epoch 8] ogbg-molclintox: 0.543479 test loss: 0.690542
[Epoch 9; Iter    10/   40] train: loss: 0.6897004
[Epoch 9; Iter    40/   40] train: loss: 0.6891479
[Epoch 9] ogbg-molclintox: 0.396023 val loss: 0.682368
[Epoch 9] ogbg-molclintox: 0.535885 test loss: 0.688687
[Epoch 10; Iter    30/   40] train: loss: 0.6884475
[Epoch 10] ogbg-molclintox: 0.401393 val loss: 0.680723
[Epoch 10] ogbg-molclintox: 0.546691 test loss: 0.687278
[Epoch 11; Iter    20/   40] train: loss: 0.6842591
[Epoch 11] ogbg-molclintox: 0.408410 val loss: 0.679336
[Epoch 11] ogbg-molclintox: 0.545866 test loss: 0.686137
[Epoch 12; Iter    10/   40] train: loss: 0.6855252
[Epoch 12; Iter    40/   40] train: loss: 0.6820813
[Epoch 12] ogbg-molclintox: 0.405364 val loss: 0.677734
[Epoch 12] ogbg-molclintox: 0.548615 test loss: 0.684799
[Epoch 13; Iter    30/   40] train: loss: 0.6841036
[Epoch 13] ogbg-molclintox: 0.405002 val loss: 0.676385
[Epoch 13] ogbg-molclintox: 0.545179 test loss: 0.683047
[Epoch 14; Iter    20/   40] train: loss: 0.6791450
[Epoch 14] ogbg-molclintox: 0.406474 val loss: 0.674662
[Epoch 14] ogbg-molclintox: 0.537222 test loss: 0.681431
[Epoch 15; Iter    10/   40] train: loss: 0.6797688
[Epoch 15; Iter    40/   40] train: loss: 0.6757894
[Epoch 15] ogbg-molclintox: 0.419399 val loss: 0.672023
[Epoch 15] ogbg-molclintox: 0.553299 test loss: 0.679781
[Epoch 16; Iter    30/   40] train: loss: 0.6755130
[Epoch 16] ogbg-molclintox: 0.412719 val loss: 0.669163
[Epoch 16] ogbg-molclintox: 0.553561 test loss: 0.677103
[Epoch 17; Iter    20/   40] train: loss: 0.6737599
[Epoch 17] ogbg-molclintox: 0.417076 val loss: 0.666349
[Epoch 17] ogbg-molclintox: 0.551375 test loss: 0.675057
[Epoch 18; Iter    10/   40] train: loss: 0.6707342
[Epoch 18; Iter    40/   40] train: loss: 0.6537891
[Epoch 18] ogbg-molclintox: 0.675790 val loss: 0.664157
[Epoch 18] ogbg-molclintox: 0.683816 test loss: 0.693794
[Epoch 19; Iter    30/   40] train: loss: 0.6377420
[Epoch 19] ogbg-molclintox: 0.726973 val loss: 0.606392
[Epoch 19] ogbg-molclintox: 0.689889 test loss: 0.646812
[Epoch 20; Iter    20/   40] train: loss: 0.6393142
[Epoch 20] ogbg-molclintox: 0.939115 val loss: 0.537746
[Epoch 20] ogbg-molclintox: 0.631790 test loss: 0.584727
[Epoch 21; Iter    10/   40] train: loss: 0.5547552
[Epoch 21; Iter    40/   40] train: loss: 0.4723431
[Epoch 21] ogbg-molclintox: 0.928014 val loss: 0.433594
[Epoch 21] ogbg-molclintox: 0.692414 test loss: 0.486164
[Epoch 22; Iter    30/   40] train: loss: 0.4513579
[Epoch 22] ogbg-molclintox: 0.762662 val loss: 0.386009
[Epoch 22] ogbg-molclintox: 0.595401 test loss: 0.452837
[Epoch 23; Iter    20/   40] train: loss: 0.4053440
[Epoch 23] ogbg-molclintox: 0.922606 val loss: 0.371991
[Epoch 23] ogbg-molclintox: 0.739183 test loss: 0.449019
[Epoch 24; Iter    10/   40] train: loss: 0.3991204
[Epoch 24; Iter    40/   40] train: loss: 0.3111914
[Epoch 24] ogbg-molclintox: 0.969919 val loss: 0.215897
[Epoch 24] ogbg-molclintox: 0.754711 test loss: 0.302246
[Epoch 25; Iter    30/   40] train: loss: 0.2461330
[Epoch 25] ogbg-molclintox: 0.973103 val loss: 0.217892
[Epoch 25] ogbg-molclintox: 0.745567 test loss: 0.304789
[Epoch 26; Iter    20/   40] train: loss: 0.3061061
[Epoch 26] ogbg-molclintox: 0.943522 val loss: 0.292741
[Epoch 26] ogbg-molclintox: 0.647744 test loss: 0.351190
[Epoch 27; Iter    10/   40] train: loss: 0.2531230
[Epoch 27; Iter    40/   40] train: loss: 0.1538529
[Epoch 27] ogbg-molclintox: 0.956721 val loss: 0.157597
[Epoch 27] ogbg-molclintox: 0.717224 test loss: 0.261556
[Epoch 28; Iter    30/   40] train: loss: 0.2393706
[Epoch 28] ogbg-molclintox: 0.836873 val loss: 0.184437
[Epoch 28] ogbg-molclintox: 0.661695 test loss: 0.255027
[Epoch 29; Iter    20/   40] train: loss: 0.2078145
[Epoch 29] ogbg-molclintox: 0.894999 val loss: 0.176581
[Epoch 29] ogbg-molclintox: 0.642559 test loss: 0.259733
[Epoch 30; Iter    10/   40] train: loss: 0.2142065
[Epoch 30; Iter    40/   40] train: loss: 0.0824494
[Epoch 30] ogbg-molclintox: 0.884734 val loss: 0.144074
[Epoch 30] ogbg-molclintox: 0.618474 test loss: 0.241081
[Epoch 31; Iter    30/   40] train: loss: 0.2882991
[Epoch 31] ogbg-molclintox: 0.954847 val loss: 0.143157
[ Using Seed :  5  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.05/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.05_5_26-05_10-30-57
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.05
logdir: runs/static_noise/GraphCL/clintox/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6930356
[Epoch 1] ogbg-molclintox: 0.553321 val loss: 0.692939
[Epoch 1] ogbg-molclintox: 0.583687 test loss: 0.693747
[Epoch 2; Iter    20/   40] train: loss: 0.6930276
[Epoch 2] ogbg-molclintox: 0.576198 val loss: 0.690007
[Epoch 2] ogbg-molclintox: 0.570957 test loss: 0.696178
[Epoch 3; Iter    10/   40] train: loss: 0.6923568
[Epoch 3; Iter    40/   40] train: loss: 0.6926587
[Epoch 3] ogbg-molclintox: 0.585513 val loss: 0.687509
[Epoch 3] ogbg-molclintox: 0.528655 test loss: 0.697385
[Epoch 4; Iter    30/   40] train: loss: 0.6928489
[Epoch 4] ogbg-molclintox: 0.592868 val loss: 0.686232
[Epoch 4] ogbg-molclintox: 0.529567 test loss: 0.696222
[Epoch 5; Iter    20/   40] train: loss: 0.6933357
[Epoch 5] ogbg-molclintox: 0.589959 val loss: 0.686741
[Epoch 5] ogbg-molclintox: 0.532391 test loss: 0.696654
[Epoch 6; Iter    10/   40] train: loss: 0.6909578
[Epoch 6; Iter    40/   40] train: loss: 0.6911122
[Epoch 6] ogbg-molclintox: 0.597138 val loss: 0.686759
[Epoch 6] ogbg-molclintox: 0.532678 test loss: 0.695894
[Epoch 7; Iter    30/   40] train: loss: 0.6917317
[Epoch 7] ogbg-molclintox: 0.602307 val loss: 0.685487
[Epoch 7] ogbg-molclintox: 0.529993 test loss: 0.695944
[Epoch 8; Iter    20/   40] train: loss: 0.6894428
[Epoch 8] ogbg-molclintox: 0.602307 val loss: 0.684476
[Epoch 8] ogbg-molclintox: 0.535678 test loss: 0.694394
[Epoch 9; Iter    10/   40] train: loss: 0.6896555
[Epoch 9; Iter    40/   40] train: loss: 0.6865624
[Epoch 9] ogbg-molclintox: 0.601133 val loss: 0.684510
[Epoch 9] ogbg-molclintox: 0.535651 test loss: 0.694264
[Epoch 10; Iter    30/   40] train: loss: 0.6894959
[Epoch 10] ogbg-molclintox: 0.586213 val loss: 0.681576
[Epoch 10] ogbg-molclintox: 0.537613 test loss: 0.692669
[Epoch 11; Iter    20/   40] train: loss: 0.6855139
[Epoch 11] ogbg-molclintox: 0.579258 val loss: 0.680532
[Epoch 11] ogbg-molclintox: 0.529679 test loss: 0.691163
[Epoch 12; Iter    10/   40] train: loss: 0.6879280
[Epoch 12; Iter    40/   40] train: loss: 0.6867307
[Epoch 12] ogbg-molclintox: 0.587161 val loss: 0.679445
[Epoch 12] ogbg-molclintox: 0.527819 test loss: 0.690437
[Epoch 13; Iter    30/   40] train: loss: 0.6878353
[Epoch 13] ogbg-molclintox: 0.593592 val loss: 0.677862
[Epoch 13] ogbg-molclintox: 0.535252 test loss: 0.688833
[Epoch 14; Iter    20/   40] train: loss: 0.6823726
[Epoch 14] ogbg-molclintox: 0.591245 val loss: 0.675493
[Epoch 14] ogbg-molclintox: 0.532327 test loss: 0.687333
[Epoch 15; Iter    10/   40] train: loss: 0.6826619
[Epoch 15; Iter    40/   40] train: loss: 0.6825911
[Epoch 15] ogbg-molclintox: 0.587724 val loss: 0.675065
[Epoch 15] ogbg-molclintox: 0.541609 test loss: 0.686652
[Epoch 16; Iter    30/   40] train: loss: 0.6784990
[Epoch 16] ogbg-molclintox: 0.580207 val loss: 0.671768
[Epoch 16] ogbg-molclintox: 0.538248 test loss: 0.683728
[Epoch 17; Iter    20/   40] train: loss: 0.6794163
[Epoch 17] ogbg-molclintox: 0.588672 val loss: 0.670643
[Epoch 17] ogbg-molclintox: 0.542196 test loss: 0.682891
[Epoch 18; Iter    10/   40] train: loss: 0.6764183
[Epoch 18; Iter    40/   40] train: loss: 0.6892025
[Epoch 18] ogbg-molclintox: 0.706883 val loss: 0.655584
[Epoch 18] ogbg-molclintox: 0.724421 test loss: 0.693293
[Epoch 19; Iter    30/   40] train: loss: 0.6409102
[Epoch 19] ogbg-molclintox: 0.759176 val loss: 0.621621
[Epoch 19] ogbg-molclintox: 0.676449 test loss: 0.676283
[Epoch 20; Iter    20/   40] train: loss: 0.6012805
[Epoch 20] ogbg-molclintox: 0.825424 val loss: 0.481717
[Epoch 20] ogbg-molclintox: 0.636336 test loss: 0.546798
[Epoch 21; Iter    10/   40] train: loss: 0.5515113
[Epoch 21; Iter    40/   40] train: loss: 0.4758457
[Epoch 21] ogbg-molclintox: 0.793108 val loss: 0.453858
[Epoch 21] ogbg-molclintox: 0.626352 test loss: 0.500513
[Epoch 22; Iter    30/   40] train: loss: 0.4651053
[Epoch 22] ogbg-molclintox: 0.852443 val loss: 0.325534
[Epoch 22] ogbg-molclintox: 0.661971 test loss: 0.396005
[Epoch 23; Iter    20/   40] train: loss: 0.3336881
[Epoch 23] ogbg-molclintox: 0.842330 val loss: 0.238570
[Epoch 23] ogbg-molclintox: 0.722008 test loss: 0.322979
[Epoch 24; Iter    10/   40] train: loss: 0.3019744
[Epoch 24; Iter    40/   40] train: loss: 0.5033966
[Epoch 24] ogbg-molclintox: 0.931061 val loss: 0.265091
[Epoch 24] ogbg-molclintox: 0.667978 test loss: 0.325257
[Epoch 25; Iter    30/   40] train: loss: 0.2689135
[Epoch 25] ogbg-molclintox: 0.878915 val loss: 0.181779
[Epoch 25] ogbg-molclintox: 0.705431 test loss: 0.272326
[Epoch 26; Iter    20/   40] train: loss: 0.2365101
[Epoch 26] ogbg-molclintox: 0.932136 val loss: 0.181149
[Epoch 26] ogbg-molclintox: 0.672464 test loss: 0.261047
[Epoch 27; Iter    10/   40] train: loss: 0.1879425
[Epoch 27; Iter    40/   40] train: loss: 0.1003797
[Epoch 27] ogbg-molclintox: 0.817317 val loss: 0.147925
[Epoch 27] ogbg-molclintox: 0.673390 test loss: 0.239566
[Epoch 28; Iter    30/   40] train: loss: 0.2918514
[Epoch 28] ogbg-molclintox: 0.922170 val loss: 0.159366
[Epoch 28] ogbg-molclintox: 0.668590 test loss: 0.247819
[Epoch 29; Iter    20/   40] train: loss: 0.2157267
[Epoch 29] ogbg-molclintox: 0.877791 val loss: 0.134181
[Epoch 29] ogbg-molclintox: 0.686930 test loss: 0.244155
[Epoch 30; Iter    10/   40] train: loss: 0.1858670
[Epoch 30; Iter    40/   40] train: loss: 0.3155502
[Epoch 30] ogbg-molclintox: 0.799785 val loss: 0.137041
[Epoch 30] ogbg-molclintox: 0.712689 test loss: 0.228055
[Epoch 31; Iter    30/   40] train: loss: 0.2449649
[Epoch 31] ogbg-molclintox: 0.821200 val loss: 0.128937
[ Using Seed :  4  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.1/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.1_4_26-05_10-31-00
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.1
logdir: runs/static_noise/GraphCL/clintox/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6934665
[Epoch 1] ogbg-molclintox: 0.485963 val loss: 0.692328
[Epoch 1] ogbg-molclintox: 0.568295 test loss: 0.692793
[Epoch 2; Iter    20/   40] train: loss: 0.6934224
[Epoch 2] ogbg-molclintox: 0.499721 val loss: 0.692174
[Epoch 2] ogbg-molclintox: 0.611396 test loss: 0.694809
[Epoch 3; Iter    10/   40] train: loss: 0.6928447
[Epoch 3; Iter    40/   40] train: loss: 0.6919084
[Epoch 3] ogbg-molclintox: 0.503066 val loss: 0.694287
[Epoch 3] ogbg-molclintox: 0.638017 test loss: 0.697124
[Epoch 4; Iter    30/   40] train: loss: 0.6924993
[Epoch 4] ogbg-molclintox: 0.511008 val loss: 0.692760
[Epoch 4] ogbg-molclintox: 0.642989 test loss: 0.696014
[Epoch 5; Iter    20/   40] train: loss: 0.6912776
[Epoch 5] ogbg-molclintox: 0.513131 val loss: 0.692937
[Epoch 5] ogbg-molclintox: 0.646101 test loss: 0.695795
[Epoch 6; Iter    10/   40] train: loss: 0.6911352
[Epoch 6; Iter    40/   40] train: loss: 0.6905533
[Epoch 6] ogbg-molclintox: 0.502954 val loss: 0.691347
[Epoch 6] ogbg-molclintox: 0.642866 test loss: 0.694857
[Epoch 7; Iter    30/   40] train: loss: 0.6896657
[Epoch 7] ogbg-molclintox: 0.501668 val loss: 0.690380
[Epoch 7] ogbg-molclintox: 0.640479 test loss: 0.694244
[Epoch 8; Iter    20/   40] train: loss: 0.6908360
[Epoch 8] ogbg-molclintox: 0.513581 val loss: 0.690313
[Epoch 8] ogbg-molclintox: 0.649361 test loss: 0.693936
[Epoch 9; Iter    10/   40] train: loss: 0.6887969
[Epoch 9; Iter    40/   40] train: loss: 0.6879967
[Epoch 9] ogbg-molclintox: 0.498646 val loss: 0.688313
[Epoch 9] ogbg-molclintox: 0.644827 test loss: 0.691852
[Epoch 10; Iter    30/   40] train: loss: 0.6879719
[Epoch 10] ogbg-molclintox: 0.513082 val loss: 0.687051
[Epoch 10] ogbg-molclintox: 0.656768 test loss: 0.691629
[Epoch 11; Iter    20/   40] train: loss: 0.6858423
[Epoch 11] ogbg-molclintox: 0.516965 val loss: 0.684580
[Epoch 11] ogbg-molclintox: 0.643639 test loss: 0.688621
[Epoch 12; Iter    10/   40] train: loss: 0.6838589
[Epoch 12; Iter    40/   40] train: loss: 0.6850427
[Epoch 12] ogbg-molclintox: 0.513331 val loss: 0.684340
[Epoch 12] ogbg-molclintox: 0.658730 test loss: 0.688007
[Epoch 13; Iter    30/   40] train: loss: 0.6841914
[Epoch 13] ogbg-molclintox: 0.513781 val loss: 0.681399
[Epoch 13] ogbg-molclintox: 0.649836 test loss: 0.686177
[Epoch 14; Iter    20/   40] train: loss: 0.6811202
[Epoch 14] ogbg-molclintox: 0.515766 val loss: 0.679628
[Epoch 14] ogbg-molclintox: 0.654658 test loss: 0.683965
[Epoch 15; Iter    10/   40] train: loss: 0.6793596
[Epoch 15; Iter    40/   40] train: loss: 0.6767326
[Epoch 15] ogbg-molclintox: 0.515204 val loss: 0.678695
[Epoch 15] ogbg-molclintox: 0.655446 test loss: 0.683669
[Epoch 16; Iter    30/   40] train: loss: 0.6769679
[Epoch 16] ogbg-molclintox: 0.520798 val loss: 0.677217
[Epoch 16] ogbg-molclintox: 0.661180 test loss: 0.682259
[Epoch 17; Iter    20/   40] train: loss: 0.6755535
[Epoch 17] ogbg-molclintox: 0.536433 val loss: 0.675528
[Epoch 17] ogbg-molclintox: 0.662842 test loss: 0.680954
[Epoch 18; Iter    10/   40] train: loss: 0.6714444
[Epoch 18; Iter    40/   40] train: loss: 0.6606354
[Epoch 18] ogbg-molclintox: 0.674592 val loss: 0.626590
[Epoch 18] ogbg-molclintox: 0.683531 test loss: 0.669887
[Epoch 19; Iter    30/   40] train: loss: 0.6383889
[Epoch 19] ogbg-molclintox: 0.699679 val loss: 0.609968
[Epoch 19] ogbg-molclintox: 0.695425 test loss: 0.655191
[Epoch 20; Iter    20/   40] train: loss: 0.5975610
[Epoch 20] ogbg-molclintox: 0.582815 val loss: 0.553665
[Epoch 20] ogbg-molclintox: 0.611530 test loss: 0.589434
[Epoch 21; Iter    10/   40] train: loss: 0.5387059
[Epoch 21; Iter    40/   40] train: loss: 0.4789537
[Epoch 21] ogbg-molclintox: 0.862596 val loss: 0.574973
[Epoch 21] ogbg-molclintox: 0.619796 test loss: 0.574018
[Epoch 22; Iter    30/   40] train: loss: 0.4399264
[Epoch 22] ogbg-molclintox: 0.741746 val loss: 0.356866
[Epoch 22] ogbg-molclintox: 0.717298 test loss: 0.441952
[Epoch 23; Iter    20/   40] train: loss: 0.3974182
[Epoch 23] ogbg-molclintox: 0.846037 val loss: 0.358489
[Epoch 23] ogbg-molclintox: 0.645693 test loss: 0.383323
[Epoch 24; Iter    10/   40] train: loss: 0.3259177
[Epoch 24; Iter    40/   40] train: loss: 0.3108656
[Epoch 24] ogbg-molclintox: 0.949791 val loss: 0.293169
[Epoch 24] ogbg-molclintox: 0.703306 test loss: 0.350500
[Epoch 25; Iter    30/   40] train: loss: 0.3058691
[Epoch 25] ogbg-molclintox: 0.757229 val loss: 0.261851
[Epoch 25] ogbg-molclintox: 0.709353 test loss: 0.316381
[Epoch 26; Iter    20/   40] train: loss: 0.1981636
[Epoch 26] ogbg-molclintox: 0.802884 val loss: 0.188722
[Epoch 26] ogbg-molclintox: 0.691966 test loss: 0.291868
[Epoch 27; Iter    10/   40] train: loss: 0.2351360
[Epoch 27; Iter    40/   40] train: loss: 0.1804771
[Epoch 27] ogbg-molclintox: 0.767831 val loss: 0.167310
[Epoch 27] ogbg-molclintox: 0.697774 test loss: 0.253640
[Epoch 28; Iter    30/   40] train: loss: 0.2054721
[Epoch 28] ogbg-molclintox: 0.750711 val loss: 0.158181
[Epoch 28] ogbg-molclintox: 0.671941 test loss: 0.251009
[Epoch 29; Iter    20/   40] train: loss: 0.3963309
[Epoch 29] ogbg-molclintox: 0.829778 val loss: 0.150995
[Epoch 29] ogbg-molclintox: 0.732064 test loss: 0.240309
[Epoch 30; Iter    10/   40] train: loss: 0.1918256
[Epoch 30; Iter    40/   40] train: loss: 0.0943715
[Epoch 30] ogbg-molclintox: 0.905425 val loss: 0.131455
[Epoch 30] ogbg-molclintox: 0.702208 test loss: 0.227050
[Epoch 31; Iter    30/   40] train: loss: 0.1866053
[Epoch 31] ogbg-molclintox: 0.860621 val loss: 0.145928
[ Using Seed :  4  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.2/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.2_4_26-05_10-31-03
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.2
logdir: runs/static_noise/GraphCL/clintox/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6934139
[Epoch 1] ogbg-molclintox: 0.458353 val loss: 0.691526
[Epoch 1] ogbg-molclintox: 0.574055 test loss: 0.692735
[Epoch 2; Iter    20/   40] train: loss: 0.6936970
[Epoch 2] ogbg-molclintox: 0.468091 val loss: 0.689514
[Epoch 2] ogbg-molclintox: 0.611374 test loss: 0.695415
[Epoch 3; Iter    10/   40] train: loss: 0.6932949
[Epoch 3; Iter    40/   40] train: loss: 0.6915429
[Epoch 3] ogbg-molclintox: 0.484449 val loss: 0.690824
[Epoch 3] ogbg-molclintox: 0.644139 test loss: 0.697127
[Epoch 4; Iter    30/   40] train: loss: 0.6927822
[Epoch 4] ogbg-molclintox: 0.476844 val loss: 0.689294
[Epoch 4] ogbg-molclintox: 0.638342 test loss: 0.696484
[Epoch 5; Iter    20/   40] train: loss: 0.6922470
[Epoch 5] ogbg-molclintox: 0.489007 val loss: 0.690744
[Epoch 5] ogbg-molclintox: 0.647400 test loss: 0.697323
[Epoch 6; Iter    10/   40] train: loss: 0.6908955
[Epoch 6; Iter    40/   40] train: loss: 0.6905557
[Epoch 6] ogbg-molclintox: 0.478355 val loss: 0.689006
[Epoch 6] ogbg-molclintox: 0.654819 test loss: 0.696360
[Epoch 7; Iter    30/   40] train: loss: 0.6885887
[Epoch 7] ogbg-molclintox: 0.488869 val loss: 0.686907
[Epoch 7] ogbg-molclintox: 0.648573 test loss: 0.694422
[Epoch 8; Iter    20/   40] train: loss: 0.6907612
[Epoch 8] ogbg-molclintox: 0.487134 val loss: 0.687502
[Epoch 8] ogbg-molclintox: 0.653833 test loss: 0.694744
[Epoch 9; Iter    10/   40] train: loss: 0.6890900
[Epoch 9; Iter    40/   40] train: loss: 0.6886329
[Epoch 9] ogbg-molclintox: 0.490268 val loss: 0.685891
[Epoch 9] ogbg-molclintox: 0.656694 test loss: 0.692969
[Epoch 10; Iter    30/   40] train: loss: 0.6883428
[Epoch 10] ogbg-molclintox: 0.484062 val loss: 0.684229
[Epoch 10] ogbg-molclintox: 0.661202 test loss: 0.692935
[Epoch 11; Iter    20/   40] train: loss: 0.6865258
[Epoch 11] ogbg-molclintox: 0.490493 val loss: 0.681696
[Epoch 11] ogbg-molclintox: 0.650859 test loss: 0.689934
[Epoch 12; Iter    10/   40] train: loss: 0.6838629
[Epoch 12; Iter    40/   40] train: loss: 0.6864154
[Epoch 12] ogbg-molclintox: 0.492004 val loss: 0.681822
[Epoch 12] ogbg-molclintox: 0.663189 test loss: 0.689610
[Epoch 13; Iter    30/   40] train: loss: 0.6849952
[Epoch 13] ogbg-molclintox: 0.489881 val loss: 0.677937
[Epoch 13] ogbg-molclintox: 0.655080 test loss: 0.686779
[Epoch 14; Iter    20/   40] train: loss: 0.6811862
[Epoch 14] ogbg-molclintox: 0.494464 val loss: 0.676509
[Epoch 14] ogbg-molclintox: 0.664325 test loss: 0.684884
[Epoch 15; Iter    10/   40] train: loss: 0.6800424
[Epoch 15; Iter    40/   40] train: loss: 0.6766957
[Epoch 15] ogbg-molclintox: 0.494576 val loss: 0.674216
[Epoch 15] ogbg-molclintox: 0.661878 test loss: 0.683760
[Epoch 16; Iter    30/   40] train: loss: 0.6764960
[Epoch 16] ogbg-molclintox: 0.493065 val loss: 0.673240
[Epoch 16] ogbg-molclintox: 0.661094 test loss: 0.682699
[Epoch 17; Iter    20/   40] train: loss: 0.6753393
[Epoch 17] ogbg-molclintox: 0.494801 val loss: 0.671152
[Epoch 17] ogbg-molclintox: 0.667026 test loss: 0.680267
[Epoch 18; Iter    10/   40] train: loss: 0.6714408
[Epoch 18; Iter    40/   40] train: loss: 0.6611914
[Epoch 18] ogbg-molclintox: 0.617889 val loss: 0.587189
[Epoch 18] ogbg-molclintox: 0.617876 test loss: 0.661135
[Epoch 19; Iter    30/   40] train: loss: 0.6332051
[Epoch 19] ogbg-molclintox: 0.693923 val loss: 0.513028
[Epoch 19] ogbg-molclintox: 0.634704 test loss: 0.589890
[Epoch 20; Iter    20/   40] train: loss: 0.5966402
[Epoch 20] ogbg-molclintox: 0.707846 val loss: 0.407283
[Epoch 20] ogbg-molclintox: 0.606921 test loss: 0.494744
[Epoch 21; Iter    10/   40] train: loss: 0.5435037
[Epoch 21; Iter    40/   40] train: loss: 0.4789371
[Epoch 21] ogbg-molclintox: 0.707097 val loss: 0.351196
[Epoch 21] ogbg-molclintox: 0.680297 test loss: 0.426364
[Epoch 22; Iter    30/   40] train: loss: 0.4411902
[Epoch 22] ogbg-molclintox: 0.585387 val loss: 0.265406
[Epoch 22] ogbg-molclintox: 0.541852 test loss: 0.352377
[Epoch 23; Iter    20/   40] train: loss: 0.4269819
[Epoch 23] ogbg-molclintox: 0.693846 val loss: 0.275372
[Epoch 23] ogbg-molclintox: 0.598173 test loss: 0.335492
[Epoch 24; Iter    10/   40] train: loss: 0.3232458
[Epoch 24; Iter    40/   40] train: loss: 0.3348535
[Epoch 24] ogbg-molclintox: 0.655489 val loss: 0.221219
[Epoch 24] ogbg-molclintox: 0.564607 test loss: 0.287415
[Epoch 25; Iter    30/   40] train: loss: 0.3010644
[Epoch 25] ogbg-molclintox: 0.713215 val loss: 0.187352
[Epoch 25] ogbg-molclintox: 0.615452 test loss: 0.258920
[Epoch 26; Iter    20/   40] train: loss: 0.2061761
[Epoch 26] ogbg-molclintox: 0.670660 val loss: 0.172163
[Epoch 26] ogbg-molclintox: 0.607421 test loss: 0.236587
[Epoch 27; Iter    10/   40] train: loss: 0.2609480
[Epoch 27; Iter    40/   40] train: loss: 0.2204543
[Epoch 27] ogbg-molclintox: 0.704462 val loss: 0.164649
[Epoch 27] ogbg-molclintox: 0.640736 test loss: 0.229434
[Epoch 28; Iter    30/   40] train: loss: 0.2272341
[Epoch 28] ogbg-molclintox: 0.634134 val loss: 0.183969
[Epoch 28] ogbg-molclintox: 0.627196 test loss: 0.233489
[Epoch 29; Iter    20/   40] train: loss: 0.4417515
[Epoch 29] ogbg-molclintox: 0.702413 val loss: 0.176265
[Epoch 29] ogbg-molclintox: 0.629269 test loss: 0.232373
[Epoch 30; Iter    10/   40] train: loss: 0.1965052
[Epoch 30; Iter    40/   40] train: loss: 0.0833376
[Epoch 30] ogbg-molclintox: 0.706985 val loss: 0.178177
[Epoch 30] ogbg-molclintox: 0.603910 test loss: 0.243277
[Epoch 31; Iter    30/   40] train: loss: 0.2141936
[Epoch 31] ogbg-molclintox: 0.705885 val loss: 0.179825
[ Using Seed :  5  ]
using device:  cuda:2
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.1/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.1_5_26-05_10-31-01
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.1
logdir: runs/static_noise/GraphCL/clintox/noise=0.1
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:2
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.1
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6934760
[Epoch 1] ogbg-molclintox: 0.554308 val loss: 0.693511
[Epoch 1] ogbg-molclintox: 0.602748 test loss: 0.694327
[Epoch 2; Iter    20/   40] train: loss: 0.6925526
[Epoch 2] ogbg-molclintox: 0.560739 val loss: 0.689752
[Epoch 2] ogbg-molclintox: 0.585809 test loss: 0.697179
[Epoch 3; Iter    10/   40] train: loss: 0.6928364
[Epoch 3; Iter    40/   40] train: loss: 0.6926654
[Epoch 3] ogbg-molclintox: 0.570417 val loss: 0.682989
[Epoch 3] ogbg-molclintox: 0.547918 test loss: 0.697315
[Epoch 4; Iter    30/   40] train: loss: 0.6929674
[Epoch 4] ogbg-molclintox: 0.580144 val loss: 0.683407
[Epoch 4] ogbg-molclintox: 0.541747 test loss: 0.696799
[Epoch 5; Iter    20/   40] train: loss: 0.6922806
[Epoch 5] ogbg-molclintox: 0.575136 val loss: 0.683363
[Epoch 5] ogbg-molclintox: 0.541060 test loss: 0.697576
[Epoch 6; Iter    10/   40] train: loss: 0.6905523
[Epoch 6; Iter    40/   40] train: loss: 0.6911020
[Epoch 6] ogbg-molclintox: 0.571302 val loss: 0.682242
[Epoch 6] ogbg-molclintox: 0.528069 test loss: 0.696026
[Epoch 7; Iter    30/   40] train: loss: 0.6915679
[Epoch 7] ogbg-molclintox: 0.574824 val loss: 0.682045
[Epoch 7] ogbg-molclintox: 0.535737 test loss: 0.696324
[Epoch 8; Iter    20/   40] train: loss: 0.6890790
[Epoch 8] ogbg-molclintox: 0.583739 val loss: 0.681114
[Epoch 8] ogbg-molclintox: 0.535177 test loss: 0.694978
[Epoch 9; Iter    10/   40] train: loss: 0.6894148
[Epoch 9; Iter    40/   40] train: loss: 0.6884811
[Epoch 9] ogbg-molclintox: 0.573762 val loss: 0.681258
[Epoch 9] ogbg-molclintox: 0.536089 test loss: 0.694178
[Epoch 10; Iter    30/   40] train: loss: 0.6887707
[Epoch 10] ogbg-molclintox: 0.561502 val loss: 0.675557
[Epoch 10] ogbg-molclintox: 0.550753 test loss: 0.691231
[Epoch 11; Iter    20/   40] train: loss: 0.6863903
[Epoch 11] ogbg-molclintox: 0.566059 val loss: 0.676403
[Epoch 11] ogbg-molclintox: 0.549778 test loss: 0.691181
[Epoch 12; Iter    10/   40] train: loss: 0.6879643
[Epoch 12; Iter    40/   40] train: loss: 0.6863238
[Epoch 12] ogbg-molclintox: 0.573825 val loss: 0.676096
[Epoch 12] ogbg-molclintox: 0.547018 test loss: 0.691015
[Epoch 13; Iter    30/   40] train: loss: 0.6867072
[Epoch 13] ogbg-molclintox: 0.574549 val loss: 0.673670
[Epoch 13] ogbg-molclintox: 0.551579 test loss: 0.688733
[Epoch 14; Iter    20/   40] train: loss: 0.6818907
[Epoch 14] ogbg-molclintox: 0.570192 val loss: 0.671634
[Epoch 14] ogbg-molclintox: 0.557100 test loss: 0.688103
[Epoch 15; Iter    10/   40] train: loss: 0.6828609
[Epoch 15; Iter    40/   40] train: loss: 0.6805906
[Epoch 15] ogbg-molclintox: 0.571840 val loss: 0.670940
[Epoch 15] ogbg-molclintox: 0.551691 test loss: 0.687762
[Epoch 16; Iter    30/   40] train: loss: 0.6784719
[Epoch 16] ogbg-molclintox: 0.571703 val loss: 0.667753
[Epoch 16] ogbg-molclintox: 0.559135 test loss: 0.684304
[Epoch 17; Iter    20/   40] train: loss: 0.6803740
[Epoch 17] ogbg-molclintox: 0.569243 val loss: 0.667223
[Epoch 17] ogbg-molclintox: 0.555363 test loss: 0.683384
[Epoch 18; Iter    10/   40] train: loss: 0.6761892
[Epoch 18; Iter    40/   40] train: loss: 0.6875576
[Epoch 18] ogbg-molclintox: 0.657696 val loss: 0.653324
[Epoch 18] ogbg-molclintox: 0.656353 test loss: 0.698194
[Epoch 19; Iter    30/   40] train: loss: 0.6433173
[Epoch 19] ogbg-molclintox: 0.734441 val loss: 0.636951
[Epoch 19] ogbg-molclintox: 0.700348 test loss: 0.679303
[Epoch 20; Iter    20/   40] train: loss: 0.6030702
[Epoch 20] ogbg-molclintox: 0.684871 val loss: 0.545000
[Epoch 20] ogbg-molclintox: 0.681185 test loss: 0.610988
[Epoch 21; Iter    10/   40] train: loss: 0.5627270
[Epoch 21; Iter    40/   40] train: loss: 0.4816095
[Epoch 21] ogbg-molclintox: 0.761401 val loss: 0.472731
[Epoch 21] ogbg-molclintox: 0.696086 test loss: 0.522207
[Epoch 22; Iter    30/   40] train: loss: 0.4889890
[Epoch 22] ogbg-molclintox: 0.676029 val loss: 0.360105
[Epoch 22] ogbg-molclintox: 0.677674 test loss: 0.428221
[Epoch 23; Iter    20/   40] train: loss: 0.3407098
[Epoch 23] ogbg-molclintox: 0.664405 val loss: 0.295628
[Epoch 23] ogbg-molclintox: 0.650818 test loss: 0.389482
[Epoch 24; Iter    10/   40] train: loss: 0.3101439
[Epoch 24; Iter    40/   40] train: loss: 0.4772180
[Epoch 24] ogbg-molclintox: 0.841729 val loss: 0.328543
[Epoch 24] ogbg-molclintox: 0.682833 test loss: 0.372653
[Epoch 25; Iter    30/   40] train: loss: 0.2842623
[Epoch 25] ogbg-molclintox: 0.667163 val loss: 0.229270
[Epoch 25] ogbg-molclintox: 0.663372 test loss: 0.316178
[Epoch 26; Iter    20/   40] train: loss: 0.2265861
[Epoch 26] ogbg-molclintox: 0.835326 val loss: 0.227739
[Epoch 26] ogbg-molclintox: 0.676662 test loss: 0.296071
[Epoch 27; Iter    10/   40] train: loss: 0.2186754
[Epoch 27; Iter    40/   40] train: loss: 0.1025802
[Epoch 27] ogbg-molclintox: 0.823986 val loss: 0.221500
[Epoch 27] ogbg-molclintox: 0.688630 test loss: 0.325061
[Epoch 28; Iter    30/   40] train: loss: 0.3534615
[Epoch 28] ogbg-molclintox: 0.845675 val loss: 1.539162
[Epoch 28] ogbg-molclintox: 0.647393 test loss: 1.045073
[Epoch 29; Iter    20/   40] train: loss: 0.2234280
[Epoch 29] ogbg-molclintox: 0.826358 val loss: 2.560184
[Epoch 29] ogbg-molclintox: 0.659749 test loss: 1.636677
[Epoch 30; Iter    10/   40] train: loss: 0.1743055
[Epoch 30; Iter    40/   40] train: loss: 0.3506803
[Epoch 30] ogbg-molclintox: 0.833963 val loss: 2.358118
[Epoch 30] ogbg-molclintox: 0.653765 test loss: 1.612173
[Epoch 31; Iter    30/   40] train: loss: 0.3239744
[Epoch 31] ogbg-molclintox: 0.834736 val loss: 2.476963
[ Using Seed :  4  ]
using device:  cuda:1
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.05/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.05_4_26-05_10-30-58
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.05
logdir: runs/static_noise/GraphCL/clintox/noise=0.05
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:1
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.05
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6932933
[Epoch 1] ogbg-molclintox: 0.470269 val loss: 0.692605
[Epoch 1] ogbg-molclintox: 0.556025 test loss: 0.692764
[Epoch 2; Iter    20/   40] train: loss: 0.6937116
[Epoch 2] ogbg-molclintox: 0.489396 val loss: 0.692913
[Epoch 2] ogbg-molclintox: 0.589594 test loss: 0.693964
[Epoch 3; Iter    10/   40] train: loss: 0.6930808
[Epoch 3; Iter    40/   40] train: loss: 0.6935492
[Epoch 3] ogbg-molclintox: 0.509149 val loss: 0.694347
[Epoch 3] ogbg-molclintox: 0.620473 test loss: 0.695495
[Epoch 4; Iter    30/   40] train: loss: 0.6922995
[Epoch 4] ogbg-molclintox: 0.521037 val loss: 0.692901
[Epoch 4] ogbg-molclintox: 0.624870 test loss: 0.694496
[Epoch 5; Iter    20/   40] train: loss: 0.6921322
[Epoch 5] ogbg-molclintox: 0.514744 val loss: 0.692535
[Epoch 5] ogbg-molclintox: 0.619285 test loss: 0.693621
[Epoch 6; Iter    10/   40] train: loss: 0.6911948
[Epoch 6; Iter    40/   40] train: loss: 0.6905622
[Epoch 6] ogbg-molclintox: 0.522197 val loss: 0.691620
[Epoch 6] ogbg-molclintox: 0.627794 test loss: 0.693464
[Epoch 7; Iter    30/   40] train: loss: 0.6898397
[Epoch 7] ogbg-molclintox: 0.514793 val loss: 0.690752
[Epoch 7] ogbg-molclintox: 0.624059 test loss: 0.692600
[Epoch 8; Iter    20/   40] train: loss: 0.6915039
[Epoch 8] ogbg-molclintox: 0.528217 val loss: 0.690413
[Epoch 8] ogbg-molclintox: 0.630267 test loss: 0.692125
[Epoch 9; Iter    10/   40] train: loss: 0.6885812
[Epoch 9; Iter    40/   40] train: loss: 0.6876926
[Epoch 9] ogbg-molclintox: 0.523958 val loss: 0.688495
[Epoch 9] ogbg-molclintox: 0.639075 test loss: 0.690580
[Epoch 10; Iter    30/   40] train: loss: 0.6873744
[Epoch 10] ogbg-molclintox: 0.536046 val loss: 0.687098
[Epoch 10] ogbg-molclintox: 0.634615 test loss: 0.690038
[Epoch 11; Iter    20/   40] train: loss: 0.6855124
[Epoch 11] ogbg-molclintox: 0.533249 val loss: 0.684895
[Epoch 11] ogbg-molclintox: 0.624533 test loss: 0.687350
[Epoch 12; Iter    10/   40] train: loss: 0.6838652
[Epoch 12; Iter    40/   40] train: loss: 0.6834467
[Epoch 12] ogbg-molclintox: 0.536545 val loss: 0.684475
[Epoch 12] ogbg-molclintox: 0.642747 test loss: 0.686762
[Epoch 13; Iter    30/   40] train: loss: 0.6837325
[Epoch 13] ogbg-molclintox: 0.529615 val loss: 0.681636
[Epoch 13] ogbg-molclintox: 0.626057 test loss: 0.684562
[Epoch 14; Iter    20/   40] train: loss: 0.6804594
[Epoch 14] ogbg-molclintox: 0.537083 val loss: 0.680028
[Epoch 14] ogbg-molclintox: 0.639000 test loss: 0.682883
[Epoch 15; Iter    10/   40] train: loss: 0.6790555
[Epoch 15; Iter    40/   40] train: loss: 0.6767129
[Epoch 15] ogbg-molclintox: 0.533836 val loss: 0.678629
[Epoch 15] ogbg-molclintox: 0.639239 test loss: 0.681989
[Epoch 16; Iter    30/   40] train: loss: 0.6770604
[Epoch 16] ogbg-molclintox: 0.536408 val loss: 0.677738
[Epoch 16] ogbg-molclintox: 0.634955 test loss: 0.681056
[Epoch 17; Iter    20/   40] train: loss: 0.6749371
[Epoch 17] ogbg-molclintox: 0.552541 val loss: 0.675541
[Epoch 17] ogbg-molclintox: 0.648835 test loss: 0.679676
[Epoch 18; Iter    10/   40] train: loss: 0.6714519
[Epoch 18; Iter    40/   40] train: loss: 0.6608623
[Epoch 18] ogbg-molclintox: 0.669634 val loss: 0.646901
[Epoch 18] ogbg-molclintox: 0.650743 test loss: 0.684205
[Epoch 19; Iter    30/   40] train: loss: 0.6387912
[Epoch 19] ogbg-molclintox: 0.727338 val loss: 0.626010
[Epoch 19] ogbg-molclintox: 0.656671 test loss: 0.663268
[Epoch 20; Iter    20/   40] train: loss: 0.6017816
[Epoch 20] ogbg-molclintox: 0.860487 val loss: 0.608876
[Epoch 20] ogbg-molclintox: 0.568612 test loss: 0.618856
[Epoch 21; Iter    10/   40] train: loss: 0.5390041
[Epoch 21; Iter    40/   40] train: loss: 0.4762846
[Epoch 21] ogbg-molclintox: 0.890740 val loss: 0.622036
[Epoch 21] ogbg-molclintox: 0.601845 test loss: 0.618052
[Epoch 22; Iter    30/   40] train: loss: 0.4499163
[Epoch 22] ogbg-molclintox: 0.775088 val loss: 0.371515
[Epoch 22] ogbg-molclintox: 0.696549 test loss: 0.455201
[Epoch 23; Iter    20/   40] train: loss: 0.4281768
[Epoch 23] ogbg-molclintox: 0.854341 val loss: 0.418613
[Epoch 23] ogbg-molclintox: 0.580370 test loss: 0.426641
[Epoch 24; Iter    10/   40] train: loss: 0.3270338
[Epoch 24; Iter    40/   40] train: loss: 0.3137520
[Epoch 24] ogbg-molclintox: 0.936479 val loss: 0.300253
[Epoch 24] ogbg-molclintox: 0.717134 test loss: 0.369866
[Epoch 25; Iter    30/   40] train: loss: 0.3124847
[Epoch 25] ogbg-molclintox: 0.762848 val loss: 0.238213
[Epoch 25] ogbg-molclintox: 0.703519 test loss: 0.325736
[Epoch 26; Iter    20/   40] train: loss: 0.2158452
[Epoch 26] ogbg-molclintox: 0.709670 val loss: 0.230020
[Epoch 26] ogbg-molclintox: 0.682284 test loss: 0.309001
[Epoch 27; Iter    10/   40] train: loss: 0.2221475
[Epoch 27; Iter    40/   40] train: loss: 0.1878915
[Epoch 27] ogbg-molclintox: 0.771265 val loss: 0.208430
[Epoch 27] ogbg-molclintox: 0.708203 test loss: 0.261368
[Epoch 28; Iter    30/   40] train: loss: 0.2083730
[Epoch 28] ogbg-molclintox: 0.770116 val loss: 0.182451
[Epoch 28] ogbg-molclintox: 0.694076 test loss: 0.241961
[Epoch 29; Iter    20/   40] train: loss: 0.3587611
[Epoch 29] ogbg-molclintox: 0.778082 val loss: 0.261355
[Epoch 29] ogbg-molclintox: 0.731388 test loss: 0.372738
[Epoch 30; Iter    10/   40] train: loss: 0.1992534
[Epoch 30; Iter    40/   40] train: loss: 0.0975898
[Epoch 30] ogbg-molclintox: 0.789970 val loss: 0.183461
[Epoch 30] ogbg-molclintox: 0.686830 test loss: 0.221602
[Epoch 31; Iter    30/   40] train: loss: 0.1996141
[Epoch 31] ogbg-molclintox: 0.879463 val loss: 0.142380
[ Using Seed :  6  ]
using device:  cuda:3
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.2/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.2_6_26-05_10-31-05
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.2
logdir: runs/static_noise/GraphCL/clintox/noise=0.2
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:3
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.2
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6936906
[Epoch 1] ogbg-molclintox: 0.464987 val loss: 0.692069
[Epoch 1] ogbg-molclintox: 0.606283 test loss: 0.695262
[Epoch 2; Iter    20/   40] train: loss: 0.6942108
[Epoch 2] ogbg-molclintox: 0.445794 val loss: 0.686265
[Epoch 2] ogbg-molclintox: 0.562382 test loss: 0.696203
[Epoch 3; Iter    10/   40] train: loss: 0.6920688
[Epoch 3; Iter    40/   40] train: loss: 0.6964900
[Epoch 3] ogbg-molclintox: 0.421831 val loss: 0.684461
[Epoch 3] ogbg-molclintox: 0.536635 test loss: 0.696543
[Epoch 4; Iter    30/   40] train: loss: 0.6931788
[Epoch 4] ogbg-molclintox: 0.449550 val loss: 0.681369
[Epoch 4] ogbg-molclintox: 0.537524 test loss: 0.694469
[Epoch 5; Iter    20/   40] train: loss: 0.6924128
[Epoch 5] ogbg-molclintox: 0.437114 val loss: 0.682578
[Epoch 5] ogbg-molclintox: 0.536123 test loss: 0.695257
[Epoch 6; Iter    10/   40] train: loss: 0.6905654
[Epoch 6; Iter    40/   40] train: loss: 0.6904816
[Epoch 6] ogbg-molclintox: 0.451086 val loss: 0.682083
[Epoch 6] ogbg-molclintox: 0.542533 test loss: 0.694321
[Epoch 7; Iter    30/   40] train: loss: 0.6889206
[Epoch 7] ogbg-molclintox: 0.446142 val loss: 0.679593
[Epoch 7] ogbg-molclintox: 0.538510 test loss: 0.692322
[Epoch 8; Iter    20/   40] train: loss: 0.6882181
[Epoch 8] ogbg-molclintox: 0.450112 val loss: 0.679543
[Epoch 8] ogbg-molclintox: 0.538447 test loss: 0.692289
[Epoch 9; Iter    10/   40] train: loss: 0.6883581
[Epoch 9; Iter    40/   40] train: loss: 0.6858749
[Epoch 9] ogbg-molclintox: 0.468442 val loss: 0.677653
[Epoch 9] ogbg-molclintox: 0.541920 test loss: 0.690677
[Epoch 10; Iter    30/   40] train: loss: 0.6878405
[Epoch 10] ogbg-molclintox: 0.455644 val loss: 0.674551
[Epoch 10] ogbg-molclintox: 0.554426 test loss: 0.688254
[Epoch 11; Iter    20/   40] train: loss: 0.6840077
[Epoch 11] ogbg-molclintox: 0.461913 val loss: 0.672720
[Epoch 11] ogbg-molclintox: 0.558460 test loss: 0.687239
[Epoch 12; Iter    10/   40] train: loss: 0.6847454
[Epoch 12; Iter    40/   40] train: loss: 0.6820478
[Epoch 12] ogbg-molclintox: 0.447603 val loss: 0.671224
[Epoch 12] ogbg-molclintox: 0.548080 test loss: 0.685687
[Epoch 13; Iter    30/   40] train: loss: 0.6845387
[Epoch 13] ogbg-molclintox: 0.462612 val loss: 0.670497
[Epoch 13] ogbg-molclintox: 0.558710 test loss: 0.684688
[Epoch 14; Iter    20/   40] train: loss: 0.6791310
[Epoch 14] ogbg-molclintox: 0.458216 val loss: 0.668714
[Epoch 14] ogbg-molclintox: 0.548442 test loss: 0.682694
[Epoch 15; Iter    10/   40] train: loss: 0.6787287
[Epoch 15; Iter    40/   40] train: loss: 0.6758083
[Epoch 15] ogbg-molclintox: 0.485362 val loss: 0.665713
[Epoch 15] ogbg-molclintox: 0.567443 test loss: 0.680432
[Epoch 16; Iter    30/   40] train: loss: 0.6755669
[Epoch 16] ogbg-molclintox: 0.474774 val loss: 0.661989
[Epoch 16] ogbg-molclintox: 0.564070 test loss: 0.677381
[Epoch 17; Iter    20/   40] train: loss: 0.6744100
[Epoch 17] ogbg-molclintox: 0.465135 val loss: 0.660555
[Epoch 17] ogbg-molclintox: 0.569479 test loss: 0.675798
[Epoch 18; Iter    10/   40] train: loss: 0.6704061
[Epoch 18; Iter    40/   40] train: loss: 0.6557623
[Epoch 18] ogbg-molclintox: 0.627366 val loss: 0.637171
[Epoch 18] ogbg-molclintox: 0.653578 test loss: 0.698612
[Epoch 19; Iter    30/   40] train: loss: 0.6423873
[Epoch 19] ogbg-molclintox: 0.715510 val loss: 0.616313
[Epoch 19] ogbg-molclintox: 0.693953 test loss: 0.684635
[Epoch 20; Iter    20/   40] train: loss: 0.6449466
[Epoch 20] ogbg-molclintox: 0.752060 val loss: 0.604793
[Epoch 20] ogbg-molclintox: 0.734425 test loss: 0.695968
[Epoch 21; Iter    10/   40] train: loss: 0.5331529
[Epoch 21; Iter    40/   40] train: loss: 0.4740236
[Epoch 21] ogbg-molclintox: 0.782155 val loss: 0.686629
[Epoch 21] ogbg-molclintox: 0.633878 test loss: 0.690872
[Epoch 22; Iter    30/   40] train: loss: 0.4466059
[Epoch 22] ogbg-molclintox: 0.840865 val loss: 0.538223
[Epoch 22] ogbg-molclintox: 0.625858 test loss: 0.823508
[Epoch 23; Iter    20/   40] train: loss: 0.4124900
[Epoch 23] ogbg-molclintox: 0.734465 val loss: 0.481933
[Epoch 23] ogbg-molclintox: 0.668493 test loss: 0.882928
[Epoch 24; Iter    10/   40] train: loss: 0.3834815
[Epoch 24; Iter    40/   40] train: loss: 0.2610829
[Epoch 24] ogbg-molclintox: 0.605625 val loss: 7.097733
[Epoch 24] ogbg-molclintox: 0.624193 test loss: 5.194262
[Epoch 25; Iter    30/   40] train: loss: 0.2417627
[Epoch 25] ogbg-molclintox: 0.726811 val loss: 0.230318
[Epoch 25] ogbg-molclintox: 0.621122 test loss: 0.304008
[Epoch 26; Iter    20/   40] train: loss: 0.3365780
[Epoch 26] ogbg-molclintox: 0.784175 val loss: 0.213158
[Epoch 26] ogbg-molclintox: 0.722106 test loss: 0.301045
[Epoch 27; Iter    10/   40] train: loss: 0.2545460
[Epoch 27; Iter    40/   40] train: loss: 0.1260112
[Epoch 27] ogbg-molclintox: 0.790694 val loss: 0.156797
[Epoch 27] ogbg-molclintox: 0.705917 test loss: 0.250798
[Epoch 28; Iter    30/   40] train: loss: 0.2940058
[Epoch 28] ogbg-molclintox: 0.826446 val loss: 0.312079
[Epoch 28] ogbg-molclintox: 0.632612 test loss: 0.382729
[Epoch 29; Iter    20/   40] train: loss: 0.2114850
[Epoch 29] ogbg-molclintox: 0.791073 val loss: 0.175631
[Epoch 29] ogbg-molclintox: 0.594176 test loss: 0.260185
[Epoch 30; Iter    10/   40] train: loss: 0.2171947
[Epoch 30; Iter    40/   40] train: loss: 0.0765528
[Epoch 30] ogbg-molclintox: 0.861773 val loss: 0.161894
[Epoch 30] ogbg-molclintox: 0.655887 test loss: 0.263054
[Epoch 31; Iter    30/   40] train: loss: 0.3313435
[Epoch 31] ogbg-molclintox: 0.753533 val loss: 0.182491
[ Using Seed :  5  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.0/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.0_5_26-05_10-30-32
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.0
logdir: runs/static_noise/GraphCL/clintox/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 5
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6942186
[Epoch 1] ogbg-molclintox: 0.462338 val loss: 0.692389
[Epoch 1] ogbg-molclintox: 0.545800 test loss: 0.693097
[Epoch 2; Iter    20/   40] train: loss: 0.6928378
[Epoch 2] ogbg-molclintox: 0.517045 val loss: 0.690763
[Epoch 2] ogbg-molclintox: 0.531106 test loss: 0.694808
[Epoch 3; Iter    10/   40] train: loss: 0.6924217
[Epoch 3; Iter    40/   40] train: loss: 0.6926721
[Epoch 3] ogbg-molclintox: 0.535002 val loss: 0.690142
[Epoch 3] ogbg-molclintox: 0.510651 test loss: 0.695703
[Epoch 4; Iter    30/   40] train: loss: 0.6927174
[Epoch 4] ogbg-molclintox: 0.541795 val loss: 0.689743
[Epoch 4] ogbg-molclintox: 0.510913 test loss: 0.695277
[Epoch 5; Iter    20/   40] train: loss: 0.6921130
[Epoch 5] ogbg-molclintox: 0.537124 val loss: 0.689136
[Epoch 5] ogbg-molclintox: 0.511989 test loss: 0.694738
[Epoch 6; Iter    10/   40] train: loss: 0.6912262
[Epoch 6; Iter    40/   40] train: loss: 0.6911170
[Epoch 6] ogbg-molclintox: 0.526185 val loss: 0.689044
[Epoch 6] ogbg-molclintox: 0.510002 test loss: 0.694375
[Epoch 7; Iter    30/   40] train: loss: 0.6916732
[Epoch 7] ogbg-molclintox: 0.526298 val loss: 0.688108
[Epoch 7] ogbg-molclintox: 0.512650 test loss: 0.693563
[Epoch 8; Iter    20/   40] train: loss: 0.6900567
[Epoch 8] ogbg-molclintox: 0.525011 val loss: 0.686754
[Epoch 8] ogbg-molclintox: 0.507267 test loss: 0.692336
[Epoch 9; Iter    10/   40] train: loss: 0.6901042
[Epoch 9; Iter    40/   40] train: loss: 0.6876632
[Epoch 9] ogbg-molclintox: 0.522028 val loss: 0.686678
[Epoch 9] ogbg-molclintox: 0.506180 test loss: 0.691987
[Epoch 10; Iter    30/   40] train: loss: 0.6898121
[Epoch 10] ogbg-molclintox: 0.544142 val loss: 0.685496
[Epoch 10] ogbg-molclintox: 0.515223 test loss: 0.691364
[Epoch 11; Iter    20/   40] train: loss: 0.6866023
[Epoch 11] ogbg-molclintox: 0.543193 val loss: 0.683703
[Epoch 11] ogbg-molclintox: 0.515810 test loss: 0.689572
[Epoch 12; Iter    10/   40] train: loss: 0.6885810
[Epoch 12; Iter    40/   40] train: loss: 0.6865059
[Epoch 12] ogbg-molclintox: 0.546964 val loss: 0.682835
[Epoch 12] ogbg-molclintox: 0.511989 test loss: 0.688988
[Epoch 13; Iter    30/   40] train: loss: 0.6865133
[Epoch 13] ogbg-molclintox: 0.557615 val loss: 0.681220
[Epoch 13] ogbg-molclintox: 0.515548 test loss: 0.687637
[Epoch 14; Iter    20/   40] train: loss: 0.6825012
[Epoch 14] ogbg-molclintox: 0.546802 val loss: 0.679525
[Epoch 14] ogbg-molclintox: 0.513162 test loss: 0.686067
[Epoch 15; Iter    10/   40] train: loss: 0.6828437
[Epoch 15; Iter    40/   40] train: loss: 0.6827788
[Epoch 15] ogbg-molclintox: 0.553845 val loss: 0.678825
[Epoch 15] ogbg-molclintox: 0.514349 test loss: 0.685593
[Epoch 16; Iter    30/   40] train: loss: 0.6787480
[Epoch 16] ogbg-molclintox: 0.554407 val loss: 0.676420
[Epoch 16] ogbg-molclintox: 0.512512 test loss: 0.683062
[Epoch 17; Iter    20/   40] train: loss: 0.6804053
[Epoch 17] ogbg-molclintox: 0.560999 val loss: 0.675173
[Epoch 17] ogbg-molclintox: 0.513711 test loss: 0.682280
[Epoch 18; Iter    10/   40] train: loss: 0.6765088
[Epoch 18; Iter    40/   40] train: loss: 0.6804767
[Epoch 18] ogbg-molclintox: 0.730456 val loss: 0.673999
[Epoch 18] ogbg-molclintox: 0.716999 test loss: 0.705552
[Epoch 19; Iter    30/   40] train: loss: 0.6407422
[Epoch 19] ogbg-molclintox: 0.858772 val loss: 0.683822
[Epoch 19] ogbg-molclintox: 0.722722 test loss: 0.713937
[Epoch 20; Iter    20/   40] train: loss: 0.5964442
[Epoch 20] ogbg-molclintox: 0.908711 val loss: 0.559918
[Epoch 20] ogbg-molclintox: 0.685156 test loss: 0.590074
[Epoch 21; Iter    10/   40] train: loss: 0.5590362
[Epoch 21; Iter    40/   40] train: loss: 0.4730608
[Epoch 21] ogbg-molclintox: 0.934733 val loss: 0.477496
[Epoch 21] ogbg-molclintox: 0.699646 test loss: 0.526698
[Epoch 22; Iter    30/   40] train: loss: 0.4587640
[Epoch 22] ogbg-molclintox: 0.950515 val loss: 0.353726
[Epoch 22] ogbg-molclintox: 0.819406 test loss: 0.428635
[Epoch 23; Iter    20/   40] train: loss: 0.3348340
[Epoch 23] ogbg-molclintox: 0.846923 val loss: 0.227159
[Epoch 23] ogbg-molclintox: 0.801843 test loss: 0.320861
[Epoch 24; Iter    10/   40] train: loss: 0.2853800
[Epoch 24; Iter    40/   40] train: loss: 0.4837876
[Epoch 24] ogbg-molclintox: 0.919710 val loss: 0.361750
[Epoch 24] ogbg-molclintox: 0.780529 test loss: 0.396580
[Epoch 25; Iter    30/   40] train: loss: 0.2644050
[Epoch 25] ogbg-molclintox: 0.871472 val loss: 0.209893
[Epoch 25] ogbg-molclintox: 0.835147 test loss: 0.326208
[Epoch 26; Iter    20/   40] train: loss: 0.2144746
[Epoch 26] ogbg-molclintox: 0.975651 val loss: 0.294552
[Epoch 26] ogbg-molclintox: 0.803991 test loss: 0.413479
[Epoch 27; Iter    10/   40] train: loss: 0.1950933
[Epoch 27; Iter    40/   40] train: loss: 0.0968612
[Epoch 27] ogbg-molclintox: 0.962702 val loss: 0.151148
[Epoch 27] ogbg-molclintox: 0.858593 test loss: 0.234134
[Epoch 28; Iter    30/   40] train: loss: 0.2578132
[Epoch 28] ogbg-molclintox: 0.918835 val loss: 0.147080
[Epoch 28] ogbg-molclintox: 0.813434 test loss: 0.217948
[Epoch 29; Iter    20/   40] train: loss: 0.1934852
[Epoch 29] ogbg-molclintox: 0.971842 val loss: 0.119780
[Epoch 29] ogbg-molclintox: 0.850174 test loss: 0.209238
[Epoch 30; Iter    10/   40] train: loss: 0.1587804
[Epoch 30; Iter    40/   40] train: loss: 0.2774376
[Epoch 30] ogbg-molclintox: 0.984317 val loss: 0.111408
[Epoch 30] ogbg-molclintox: 0.847824 test loss: 0.195894
[Epoch 31; Iter    30/   40] train: loss: 0.2124465
[Epoch 31] ogbg-molclintox: 0.931409 val loss: 0.107499
[ Using Seed :  4  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.0/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.0_4_26-05_10-30-31
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.0
logdir: runs/static_noise/GraphCL/clintox/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 4
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6932858
[Epoch 1] ogbg-molclintox: 0.404352 val loss: 0.692232
[Epoch 1] ogbg-molclintox: 0.533061 test loss: 0.692145
[Epoch 2; Iter    20/   40] train: loss: 0.6927287
[Epoch 2] ogbg-molclintox: 0.392249 val loss: 0.690143
[Epoch 2] ogbg-molclintox: 0.544580 test loss: 0.690392
[Epoch 3; Iter    10/   40] train: loss: 0.6929488
[Epoch 3; Iter    40/   40] train: loss: 0.6907994
[Epoch 3] ogbg-molclintox: 0.395595 val loss: 0.690195
[Epoch 3] ogbg-molclintox: 0.581320 test loss: 0.690197
[Epoch 4; Iter    30/   40] train: loss: 0.6926960
[Epoch 4] ogbg-molclintox: 0.401439 val loss: 0.689186
[Epoch 4] ogbg-molclintox: 0.577947 test loss: 0.689454
[Epoch 5; Iter    20/   40] train: loss: 0.6929329
[Epoch 5] ogbg-molclintox: 0.409855 val loss: 0.688996
[Epoch 5] ogbg-molclintox: 0.584417 test loss: 0.689058
[Epoch 6; Iter    10/   40] train: loss: 0.6910385
[Epoch 6; Iter    40/   40] train: loss: 0.6905471
[Epoch 6] ogbg-molclintox: 0.399278 val loss: 0.688083
[Epoch 6] ogbg-molclintox: 0.608278 test loss: 0.688689
[Epoch 7; Iter    30/   40] train: loss: 0.6899234
[Epoch 7] ogbg-molclintox: 0.418046 val loss: 0.687226
[Epoch 7] ogbg-molclintox: 0.587827 test loss: 0.687893
[Epoch 8; Iter    20/   40] train: loss: 0.6906822
[Epoch 8] ogbg-molclintox: 0.407894 val loss: 0.687078
[Epoch 8] ogbg-molclintox: 0.591088 test loss: 0.687514
[Epoch 9; Iter    10/   40] train: loss: 0.6886008
[Epoch 9; Iter    40/   40] train: loss: 0.6887452
[Epoch 9] ogbg-molclintox: 0.435940 val loss: 0.684786
[Epoch 9] ogbg-molclintox: 0.613761 test loss: 0.685874
[Epoch 10; Iter    30/   40] train: loss: 0.6881732
[Epoch 10] ogbg-molclintox: 0.442522 val loss: 0.683941
[Epoch 10] ogbg-molclintox: 0.614385 test loss: 0.685556
[Epoch 11; Iter    20/   40] train: loss: 0.6855643
[Epoch 11] ogbg-molclintox: 0.442209 val loss: 0.682257
[Epoch 11] ogbg-molclintox: 0.603205 test loss: 0.683534
[Epoch 12; Iter    10/   40] train: loss: 0.6838807
[Epoch 12; Iter    40/   40] train: loss: 0.6830626
[Epoch 12] ogbg-molclintox: 0.454009 val loss: 0.681486
[Epoch 12] ogbg-molclintox: 0.629102 test loss: 0.682910
[Epoch 13; Iter    30/   40] train: loss: 0.6830951
[Epoch 13] ogbg-molclintox: 0.452112 val loss: 0.679229
[Epoch 13] ogbg-molclintox: 0.609275 test loss: 0.680838
[Epoch 14; Iter    20/   40] train: loss: 0.6810161
[Epoch 14] ogbg-molclintox: 0.459129 val loss: 0.677468
[Epoch 14] ogbg-molclintox: 0.614497 test loss: 0.679055
[Epoch 15; Iter    10/   40] train: loss: 0.6794674
[Epoch 15; Iter    40/   40] train: loss: 0.6767147
[Epoch 15] ogbg-molclintox: 0.458093 val loss: 0.675848
[Epoch 15] ogbg-molclintox: 0.614785 test loss: 0.677744
[Epoch 16; Iter    30/   40] train: loss: 0.6761931
[Epoch 16] ogbg-molclintox: 0.476549 val loss: 0.674289
[Epoch 16] ogbg-molclintox: 0.634111 test loss: 0.676325
[Epoch 17; Iter    20/   40] train: loss: 0.6742786
[Epoch 17] ogbg-molclintox: 0.482192 val loss: 0.672268
[Epoch 17] ogbg-molclintox: 0.648626 test loss: 0.674849
[Epoch 18; Iter    10/   40] train: loss: 0.6714510
[Epoch 18; Iter    40/   40] train: loss: 0.6613396
[Epoch 18] ogbg-molclintox: 0.700814 val loss: 0.655544
[Epoch 18] ogbg-molclintox: 0.696698 test loss: 0.685536
[Epoch 19; Iter    30/   40] train: loss: 0.6361142
[Epoch 19] ogbg-molclintox: 0.880229 val loss: 0.590612
[Epoch 19] ogbg-molclintox: 0.677010 test loss: 0.597151
[Epoch 20; Iter    20/   40] train: loss: 0.6001445
[Epoch 20] ogbg-molclintox: 0.887946 val loss: 0.501754
[Epoch 20] ogbg-molclintox: 0.691562 test loss: 0.540164
[Epoch 21; Iter    10/   40] train: loss: 0.5393751
[Epoch 21; Iter    40/   40] train: loss: 0.4751354
[Epoch 21] ogbg-molclintox: 0.911768 val loss: 0.513294
[Epoch 21] ogbg-molclintox: 0.735497 test loss: 0.543605
[Epoch 22; Iter    30/   40] train: loss: 0.4427426
[Epoch 22] ogbg-molclintox: 0.897420 val loss: 0.340815
[Epoch 22] ogbg-molclintox: 0.825752 test loss: 0.451608
[Epoch 23; Iter    20/   40] train: loss: 0.3915509
[Epoch 23] ogbg-molclintox: 0.870773 val loss: 0.357392
[Epoch 23] ogbg-molclintox: 0.728762 test loss: 0.410176
[Epoch 24; Iter    10/   40] train: loss: 0.3157658
[Epoch 24; Iter    40/   40] train: loss: 0.3100341
[Epoch 24] ogbg-molclintox: 0.985016 val loss: 0.241064
[Epoch 24] ogbg-molclintox: 0.877430 test loss: 0.329937
[Epoch 25; Iter    30/   40] train: loss: 0.2836562
[Epoch 25] ogbg-molclintox: 0.989349 val loss: 0.157313
[Epoch 25] ogbg-molclintox: 0.828225 test loss: 0.242922
[Epoch 26; Iter    20/   40] train: loss: 0.2077828
[Epoch 26] ogbg-molclintox: 0.970819 val loss: 0.164187
[Epoch 26] ogbg-molclintox: 0.795422 test loss: 0.275852
[Epoch 27; Iter    10/   40] train: loss: 0.2200270
[Epoch 27; Iter    40/   40] train: loss: 0.1741571
[Epoch 27] ogbg-molclintox: 0.976350 val loss: 0.141696
[Epoch 27] ogbg-molclintox: 0.842991 test loss: 0.247607
[Epoch 28; Iter    30/   40] train: loss: 0.2126665
[Epoch 28] ogbg-molclintox: 0.902727 val loss: 0.131094
[Epoch 28] ogbg-molclintox: 0.811110 test loss: 0.208256
[Epoch 29; Iter    20/   40] train: loss: 0.3255654
[Epoch 29] ogbg-molclintox: 0.979422 val loss: 0.150702
[Epoch 29] ogbg-molclintox: 0.839330 test loss: 0.268605
[Epoch 30; Iter    10/   40] train: loss: 0.1971967
[Epoch 30; Iter    40/   40] train: loss: 0.0899969
[Epoch 30] ogbg-molclintox: 0.989711 val loss: 0.103431
[Epoch 30] ogbg-molclintox: 0.873658 test loss: 0.199864
[Epoch 31; Iter    30/   40] train: loss: 0.2055140
[Epoch 31] ogbg-molclintox: 0.849808 val loss: 0.128629
[ Using Seed :  6  ]
using device:  cuda:0
Log directory:  runs/static_noise/GraphCL/clintox/noise=0.0/PNA_ogbg-molclintox_GraphCL_clintox_static_noise=0.0_6_26-05_10-30-32
config: <_io.TextIOWrapper name='configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
experiment_name: GraphCL_clintox_static_noise=0.0
logdir: runs/static_noise/GraphCL/clintox/noise=0.0
num_epochs: 1000
batch_size: 30
patience: 60
minimum_epochs: 120
dataset: ogbg-molclintox
num_train: -1
seed: 6
num_val: None
multiple_seeds: [4, 5, 6]
seed_data: 123
loss_func: OGBNanLabelBCEWithLogitsLoss
critic_loss: MSELoss
optimizer: Adam
optimizer_params/lr: 0.001
lr_scheduler: WarmUpWrapper
lr_scheduler_params/warmup_steps: [700, 700, 350]
lr_scheduler_params/interpolation: linear
lr_scheduler_params/wrapped_scheduler: ReduceLROnPlateau
lr_scheduler_params/factor: 0.5
lr_scheduler_params/patience: 25
lr_scheduler_params/min_lr: 1e-06
lr_scheduler_params/mode: min
lr_scheduler_params/verbose: True
scheduler_step_per_batch: False
log_iterations: 30
expensive_log_iterations: 100
eval_per_epochs: 0
linear_probing_samples: 500
num_conformers: 3
metrics: ['prcauc', 'rocauc']
main_metric: ogbg-molclintox
main_metric_goal: max
val_per_batch: False
tensorboard_functions: []
checkpoint: None
pretrain_checkpoint: runs/PNA_graphcl_drugs_smaller/best_checkpoint.pt
transfer_layers: ['gnn.']
frozen_layers: []
exclude_from_transfer: []
transferred_lr: None
num_epochs_local_only: 1
required_data: ['dgl_graph', 'targets']
collate_function: graph_collate
use_e_features: True
targets: []
device: cuda:0
dist_embedding: False
num_radial: 6
models_to_save: []
model_type: PNA
model_parameters/target_dim: 2
model_parameters/hidden_dim: 50
model_parameters/mid_batch_norm: True
model_parameters/last_batch_norm: True
model_parameters/readout_batchnorm: True
model_parameters/batch_norm_momentum: 0.1
model_parameters/readout_hidden_dim: 50
model_parameters/readout_layers: 2
model_parameters/dropout: 0.0
model_parameters/propagation_depth: 3
model_parameters/aggregators: ['mean', 'max', 'min', 'std']
model_parameters/scalers: ['identity', 'amplification', 'attenuation']
model_parameters/readout_aggregators: ['min', 'max', 'mean', 'sum']
model_parameters/pretrans_layers: 2
model_parameters/posttrans_layers: 1
model_parameters/residual: True
model3d_type: None
model3d_parameters: None
critic_type: None
critic_parameters: None
trainer: contrastive
train_sampler: None
eval_on_test: True
force_random_split: False
reuse_pre_train_data: False
transfer_3d: False
noise_level: 0.0
dynamic_noise: False
train_prop: 0.8
[Epoch 1; Iter    30/   40] train: loss: 0.6939604
[Epoch 1] ogbg-molclintox: 0.403765 val loss: 0.689784
[Epoch 1] ogbg-molclintox: 0.646083 test loss: 0.692462
[Epoch 2; Iter    20/   40] train: loss: 0.6930832
[Epoch 2] ogbg-molclintox: 0.357252 val loss: 0.683971
[Epoch 2] ogbg-molclintox: 0.624729 test loss: 0.688730
[Epoch 3; Iter    10/   40] train: loss: 0.6920408
[Epoch 3; Iter    40/   40] train: loss: 0.6964488
[Epoch 3] ogbg-molclintox: 0.355379 val loss: 0.683406
[Epoch 3] ogbg-molclintox: 0.621524 test loss: 0.688951
[Epoch 4; Iter    30/   40] train: loss: 0.6945525
[Epoch 4] ogbg-molclintox: 0.364270 val loss: 0.683313
[Epoch 4] ogbg-molclintox: 0.621871 test loss: 0.688000
[Epoch 5; Iter    20/   40] train: loss: 0.6928284
[Epoch 5] ogbg-molclintox: 0.374946 val loss: 0.682587
[Epoch 5] ogbg-molclintox: 0.623709 test loss: 0.687441
[Epoch 6; Iter    10/   40] train: loss: 0.6905742
[Epoch 6; Iter    40/   40] train: loss: 0.6904730
[Epoch 6] ogbg-molclintox: 0.351159 val loss: 0.682434
[Epoch 6] ogbg-molclintox: 0.628654 test loss: 0.686939
[Epoch 7; Iter    30/   40] train: loss: 0.6905738
[Epoch 7] ogbg-molclintox: 0.365669 val loss: 0.681943
[Epoch 7] ogbg-molclintox: 0.625793 test loss: 0.686481
[Epoch 8; Iter    20/   40] train: loss: 0.6894931
[Epoch 8] ogbg-molclintox: 0.371288 val loss: 0.680915
[Epoch 8] ogbg-molclintox: 0.626693 test loss: 0.685304
[Epoch 9; Iter    10/   40] train: loss: 0.6896556
[Epoch 9; Iter    40/   40] train: loss: 0.6895282
[Epoch 9] ogbg-molclintox: 0.374422 val loss: 0.678845
[Epoch 9] ogbg-molclintox: 0.617673 test loss: 0.683732
[Epoch 10; Iter    30/   40] train: loss: 0.6884939
[Epoch 10] ogbg-molclintox: 0.388482 val loss: 0.677591
[Epoch 10] ogbg-molclintox: 0.629629 test loss: 0.682679
[Epoch 11; Iter    20/   40] train: loss: 0.6846946
[Epoch 11] ogbg-molclintox: 0.392404 val loss: 0.676830
[Epoch 11] ogbg-molclintox: 0.633888 test loss: 0.681978
[Epoch 12; Iter    10/   40] train: loss: 0.6857284
[Epoch 12; Iter    40/   40] train: loss: 0.6820987
[Epoch 12] ogbg-molclintox: 0.393128 val loss: 0.675110
[Epoch 12] ogbg-molclintox: 0.634089 test loss: 0.680817
[Epoch 13; Iter    30/   40] train: loss: 0.6851407
[Epoch 13] ogbg-molclintox: 0.396037 val loss: 0.673292
[Epoch 13] ogbg-molclintox: 0.633652 test loss: 0.678749
[Epoch 14; Iter    20/   40] train: loss: 0.6791525
[Epoch 14] ogbg-molclintox: 0.396624 val loss: 0.671768
[Epoch 14] ogbg-molclintox: 0.628767 test loss: 0.677019
[Epoch 15; Iter    10/   40] train: loss: 0.6799986
[Epoch 15; Iter    40/   40] train: loss: 0.6758305
[Epoch 15] ogbg-molclintox: 0.406302 val loss: 0.670434
[Epoch 15] ogbg-molclintox: 0.642609 test loss: 0.676418
[Epoch 16; Iter    30/   40] train: loss: 0.6751449
[Epoch 16] ogbg-molclintox: 0.405378 val loss: 0.666858
[Epoch 16] ogbg-molclintox: 0.638262 test loss: 0.673174
[Epoch 17; Iter    20/   40] train: loss: 0.6741731
[Epoch 17] ogbg-molclintox: 0.406826 val loss: 0.664478
[Epoch 17] ogbg-molclintox: 0.641310 test loss: 0.671427
[Epoch 18; Iter    10/   40] train: loss: 0.6707578
[Epoch 18; Iter    40/   40] train: loss: 0.6544307
[Epoch 18] ogbg-molclintox: 0.732304 val loss: 0.665512
[Epoch 18] ogbg-molclintox: 0.714814 test loss: 0.688749
[Epoch 19; Iter    30/   40] train: loss: 0.6279544
[Epoch 19] ogbg-molclintox: 0.707305 val loss: 0.573451
[Epoch 19] ogbg-molclintox: 0.708479 test loss: 0.637622
[Epoch 20; Iter    20/   40] train: loss: 0.6136516
[Epoch 20] ogbg-molclintox: 0.894960 val loss: 0.487512
[Epoch 20] ogbg-molclintox: 0.762193 test loss: 0.554374
[Epoch 21; Iter    10/   40] train: loss: 0.5444769
[Epoch 21; Iter    40/   40] train: loss: 0.4687817
[Epoch 21] ogbg-molclintox: 0.922683 val loss: 0.439673
[Epoch 21] ogbg-molclintox: 0.798593 test loss: 0.474945
[Epoch 22; Iter    30/   40] train: loss: 0.4402296
[Epoch 22] ogbg-molclintox: 0.838370 val loss: 0.400316
[Epoch 22] ogbg-molclintox: 0.783566 test loss: 0.489814
[Epoch 23; Iter    20/   40] train: loss: 0.3978947
[Epoch 23] ogbg-molclintox: 0.792929 val loss: 0.291729
[Epoch 23] ogbg-molclintox: 0.846338 test loss: 0.366278
[Epoch 24; Iter    10/   40] train: loss: 0.3722059
[Epoch 24; Iter    40/   40] train: loss: 0.2685770
[Epoch 24] ogbg-molclintox: 0.902864 val loss: 0.221455
[Epoch 24] ogbg-molclintox: 0.887885 test loss: 0.310620
[Epoch 25; Iter    30/   40] train: loss: 0.2199354
[Epoch 25] ogbg-molclintox: 0.954960 val loss: 0.208610
[Epoch 25] ogbg-molclintox: 0.864850 test loss: 0.283032
[Epoch 26; Iter    20/   40] train: loss: 0.2602497
[Epoch 26] ogbg-molclintox: 0.920620 val loss: 0.284766
[Epoch 26] ogbg-molclintox: 0.823291 test loss: 0.372848
[Epoch 27; Iter    10/   40] train: loss: 0.1999894
[Epoch 27; Iter    40/   40] train: loss: 0.1335052
[Epoch 27] ogbg-molclintox: 0.969470 val loss: 0.144060
[Epoch 27] ogbg-molclintox: 0.831961 test loss: 0.237999
[Epoch 28; Iter    30/   40] train: loss: 0.2335747
[Epoch 28] ogbg-molclintox: 0.859349 val loss: 0.163493
[Epoch 28] ogbg-molclintox: 0.768274 test loss: 0.228672
[Epoch 29; Iter    20/   40] train: loss: 0.2074621
[Epoch 29] ogbg-molclintox: 0.942661 val loss: 0.127252
[Epoch 29] ogbg-molclintox: 0.784616 test loss: 0.208116
[Epoch 30; Iter    10/   40] train: loss: 0.2047492
[Epoch 30; Iter    40/   40] train: loss: 0.1025933
[Epoch 30] ogbg-molclintox: 0.907723 val loss: 0.118636
[Epoch 30] ogbg-molclintox: 0.821591 test loss: 0.197195
[Epoch 31; Iter    30/   40] train: loss: 0.2687601
[Epoch 31] ogbg-molclintox: 0.965523 val loss: 0.114612
[Epoch 31] ogbg-molclintox: 0.706593 test loss: 0.565135
[Epoch 32; Iter    20/   40] train: loss: 0.2745364
[Epoch 32] ogbg-molclintox: 0.789345 val loss: 0.253552
[Epoch 32] ogbg-molclintox: 0.698211 test loss: 0.559817
[Epoch 33; Iter    10/   40] train: loss: 0.2202525
[Epoch 33; Iter    40/   40] train: loss: 0.3292359
[Epoch 33] ogbg-molclintox: 0.898780 val loss: 0.354830
[Epoch 33] ogbg-molclintox: 0.643209 test loss: 1.182114
[Epoch 34; Iter    30/   40] train: loss: 0.1220336
[Epoch 34] ogbg-molclintox: 0.891425 val loss: 0.205453
[Epoch 34] ogbg-molclintox: 0.653940 test loss: 0.551718
[Epoch 35; Iter    20/   40] train: loss: 0.1923775
[Epoch 35] ogbg-molclintox: 0.937340 val loss: 0.208149
[Epoch 35] ogbg-molclintox: 0.691936 test loss: 0.440491
[Epoch 36; Iter    10/   40] train: loss: 0.1775794
[Epoch 36; Iter    40/   40] train: loss: 0.2299288
[Epoch 36] ogbg-molclintox: 0.915500 val loss: 0.127244
[Epoch 36] ogbg-molclintox: 0.775222 test loss: 0.343197
[Epoch 37; Iter    30/   40] train: loss: 0.0818436
[Epoch 37] ogbg-molclintox: 0.901641 val loss: 0.205385
[Epoch 37] ogbg-molclintox: 0.735011 test loss: 0.470260
[Epoch 38; Iter    20/   40] train: loss: 0.2273672
[Epoch 38] ogbg-molclintox: 0.784014 val loss: 0.140541
[Epoch 38] ogbg-molclintox: 0.737346 test loss: 0.232547
[Epoch 39; Iter    10/   40] train: loss: 0.0757822
[Epoch 39; Iter    40/   40] train: loss: 0.1229926
[Epoch 39] ogbg-molclintox: 0.790019 val loss: 0.405242
[Epoch 39] ogbg-molclintox: 0.728325 test loss: 0.520525
[Epoch 40; Iter    30/   40] train: loss: 0.1797457
[Epoch 40] ogbg-molclintox: 0.903964 val loss: 0.209915
[Epoch 40] ogbg-molclintox: 0.768001 test loss: 0.225642
[Epoch 41; Iter    20/   40] train: loss: 0.0871204
[Epoch 41] ogbg-molclintox: 0.684193 val loss: 2.125471
[Epoch 41] ogbg-molclintox: 0.669222 test loss: 1.186149
[Epoch 42; Iter    10/   40] train: loss: 0.0731426
[Epoch 42; Iter    40/   40] train: loss: 0.0614120
[Epoch 42] ogbg-molclintox: 0.856537 val loss: 0.133473
[Epoch 42] ogbg-molclintox: 0.779420 test loss: 0.322790
[Epoch 43; Iter    30/   40] train: loss: 0.1540981
[Epoch 43] ogbg-molclintox: 0.913515 val loss: 0.123588
[Epoch 43] ogbg-molclintox: 0.830623 test loss: 0.364736
[Epoch 44; Iter    20/   40] train: loss: 0.0507266
[Epoch 44] ogbg-molclintox: 0.861584 val loss: 1.226347
[Epoch 44] ogbg-molclintox: 0.734395 test loss: 0.867063
[Epoch 45; Iter    10/   40] train: loss: 0.1603123
[Epoch 45; Iter    40/   40] train: loss: 0.0824149
[Epoch 45] ogbg-molclintox: 0.881273 val loss: 0.094848
[Epoch 45] ogbg-molclintox: 0.853670 test loss: 0.431129
[Epoch 46; Iter    30/   40] train: loss: 0.1106529
[Epoch 46] ogbg-molclintox: 0.947654 val loss: 0.181918
[Epoch 46] ogbg-molclintox: 0.824341 test loss: 0.409011
[Epoch 47; Iter    20/   40] train: loss: 0.1911140
[Epoch 47] ogbg-molclintox: 0.952563 val loss: 0.083866
[Epoch 47] ogbg-molclintox: 0.803767 test loss: 1.773795
[Epoch 48; Iter    10/   40] train: loss: 0.0477176
[Epoch 48; Iter    40/   40] train: loss: 0.0791211
[Epoch 48] ogbg-molclintox: 0.921569 val loss: 0.139882
[Epoch 48] ogbg-molclintox: 0.800767 test loss: 0.600209
[Epoch 49; Iter    30/   40] train: loss: 0.0446046
[Epoch 49] ogbg-molclintox: 0.924068 val loss: 0.102917
[Epoch 49] ogbg-molclintox: 0.819578 test loss: 0.279030
[Epoch 50; Iter    20/   40] train: loss: 0.0801083
[Epoch 50] ogbg-molclintox: 0.897944 val loss: 0.134025
[Epoch 50] ogbg-molclintox: 0.816183 test loss: 0.585042
[Epoch 51; Iter    10/   40] train: loss: 0.0254906
[Epoch 51; Iter    40/   40] train: loss: 0.2820503
[Epoch 51] ogbg-molclintox: 0.955796 val loss: 0.096356
[Epoch 51] ogbg-molclintox: 0.835793 test loss: 0.246440
[Epoch 52; Iter    30/   40] train: loss: 0.0244852
[Epoch 52] ogbg-molclintox: 0.883234 val loss: 0.137173
[Epoch 52] ogbg-molclintox: 0.813759 test loss: 0.444507
[Epoch 53; Iter    20/   40] train: loss: 0.0865200
[Epoch 53] ogbg-molclintox: 0.934543 val loss: 0.140860
[Epoch 53] ogbg-molclintox: 0.838243 test loss: 0.602763
[Epoch 54; Iter    10/   40] train: loss: 0.0588128
[Epoch 54; Iter    40/   40] train: loss: 0.0186643
[Epoch 54] ogbg-molclintox: 0.891924 val loss: 0.144367
[Epoch 54] ogbg-molclintox: 0.782842 test loss: 0.601530
[Epoch 55; Iter    30/   40] train: loss: 0.0090073
[Epoch 55] ogbg-molclintox: 0.920670 val loss: 0.096362
[Epoch 55] ogbg-molclintox: 0.838094 test loss: 0.369217
[Epoch 56; Iter    20/   40] train: loss: 0.0070667
[Epoch 56] ogbg-molclintox: 0.803130 val loss: 0.138186
[Epoch 56] ogbg-molclintox: 0.816131 test loss: 0.274411
[Epoch 57; Iter    10/   40] train: loss: 0.0505363
[Epoch 57; Iter    40/   40] train: loss: 0.0250236
[Epoch 57] ogbg-molclintox: 0.861194 val loss: 0.144024
[Epoch 57] ogbg-molclintox: 0.820217 test loss: 0.415047
[Epoch 58; Iter    30/   40] train: loss: 0.0175295
[Epoch 58] ogbg-molclintox: 0.850005 val loss: 0.208387
[Epoch 58] ogbg-molclintox: 0.740894 test loss: 0.504337
[Epoch 59; Iter    20/   40] train: loss: 0.0104892
[Epoch 59] ogbg-molclintox: 0.924029 val loss: 0.139665
[Epoch 59] ogbg-molclintox: 0.774030 test loss: 0.533181
[Epoch 60; Iter    10/   40] train: loss: 0.0068987
[Epoch 60; Iter    40/   40] train: loss: 0.0124354
[Epoch 60] ogbg-molclintox: 0.899093 val loss: 0.163355
[Epoch 60] ogbg-molclintox: 0.786536 test loss: 0.631595
[Epoch 61; Iter    30/   40] train: loss: 0.0207120
[Epoch 61] ogbg-molclintox: 0.883508 val loss: 0.128808
[Epoch 61] ogbg-molclintox: 0.784160 test loss: 0.312418
[Epoch 62; Iter    20/   40] train: loss: 0.0550770
[Epoch 62] ogbg-molclintox: 0.873219 val loss: 0.182517
[Epoch 62] ogbg-molclintox: 0.853259 test loss: 0.386259
[Epoch 63; Iter    10/   40] train: loss: 0.0678486
[Epoch 63; Iter    40/   40] train: loss: 0.0497008
[Epoch 63] ogbg-molclintox: 0.947155 val loss: 0.106060
[Epoch 63] ogbg-molclintox: 0.816396 test loss: 0.389799
[Epoch 64; Iter    30/   40] train: loss: 0.0079741
[Epoch 64] ogbg-molclintox: 0.947067 val loss: 0.104059
[Epoch 64] ogbg-molclintox: 0.832271 test loss: 0.485318
[Epoch 65; Iter    20/   40] train: loss: 0.0251466
[Epoch 65] ogbg-molclintox: 0.933232 val loss: 0.099274
[Epoch 65] ogbg-molclintox: 0.822503 test loss: 0.463718
[Epoch 66; Iter    10/   40] train: loss: 0.0077971
[Epoch 66; Iter    40/   40] train: loss: 0.0014843
[Epoch 66] ogbg-molclintox: 0.927663 val loss: 0.085057
[Epoch 66] ogbg-molclintox: 0.839951 test loss: 1.171925
[Epoch 67; Iter    30/   40] train: loss: 0.0077217
[Epoch 67] ogbg-molclintox: 0.909495 val loss: 0.109006
[Epoch 67] ogbg-molclintox: 0.854780 test loss: 0.473086
[Epoch 68; Iter    20/   40] train: loss: 0.0164806
[Epoch 68] ogbg-molclintox: 0.905960 val loss: 0.164673
[Epoch 68] ogbg-molclintox: 0.818331 test loss: 0.782220
[Epoch 69; Iter    10/   40] train: loss: 0.0027334
[Epoch 69; Iter    40/   40] train: loss: 0.0104175
[Epoch 69] ogbg-molclintox: 0.951625 val loss: 0.090500
[Epoch 69] ogbg-molclintox: 0.842151 test loss: 0.489465
[Epoch 70; Iter    30/   40] train: loss: 0.0924945
[Epoch 70] ogbg-molclintox: 0.890438 val loss: 0.177681
[Epoch 70] ogbg-molclintox: 0.794570 test loss: 0.331345
[Epoch 71; Iter    20/   40] train: loss: 0.0149201
[Epoch 71] ogbg-molclintox: 0.893797 val loss: 0.112106
[Epoch 71] ogbg-molclintox: 0.767912 test loss: 0.415089
[Epoch 72; Iter    10/   40] train: loss: 0.0027953
[Epoch 72; Iter    40/   40] train: loss: 0.0133278
[Epoch 72] ogbg-molclintox: 0.917173 val loss: 0.152434
[Epoch 72] ogbg-molclintox: 0.797630 test loss: 0.677723
[Epoch 73; Iter    30/   40] train: loss: 0.0379783
[Epoch 73] ogbg-molclintox: 0.892374 val loss: 0.119465
[Epoch 73] ogbg-molclintox: 0.842386 test loss: 0.345522
[Epoch 74; Iter    20/   40] train: loss: 0.0053026
[Epoch 74] ogbg-molclintox: 0.863517 val loss: 0.361732
[Epoch 74] ogbg-molclintox: 0.801115 test loss: 0.315922
[Epoch 75; Iter    10/   40] train: loss: 0.0026290
[Epoch 75; Iter    40/   40] train: loss: 0.0019895
[Epoch 75] ogbg-molclintox: 0.873718 val loss: 0.126434
[Epoch 75] ogbg-molclintox: 0.781777 test loss: 0.390476
[Epoch 76; Iter    30/   40] train: loss: 0.0080705
[Epoch 31] ogbg-molclintox: 0.648606 test loss: 0.366354
[Epoch 32; Iter    20/   40] train: loss: 0.1958747
[Epoch 32] ogbg-molclintox: 0.660746 val loss: 0.228112
[Epoch 32] ogbg-molclintox: 0.586369 test loss: 0.315864
[Epoch 33; Iter    10/   40] train: loss: 0.2527763
[Epoch 33; Iter    40/   40] train: loss: 0.6423604
[Epoch 33] ogbg-molclintox: 0.835801 val loss: 0.168846
[Epoch 33] ogbg-molclintox: 0.664971 test loss: 0.327748
[Epoch 34; Iter    30/   40] train: loss: 0.1842134
[Epoch 34] ogbg-molclintox: 0.799514 val loss: 0.589407
[Epoch 34] ogbg-molclintox: 0.625209 test loss: 0.823496
[Epoch 35; Iter    20/   40] train: loss: 0.2269484
[Epoch 35] ogbg-molclintox: 0.827022 val loss: 0.262019
[Epoch 35] ogbg-molclintox: 0.641685 test loss: 0.593996
[Epoch 36; Iter    10/   40] train: loss: 0.2350157
[Epoch 36; Iter    40/   40] train: loss: 0.3348607
[Epoch 36] ogbg-molclintox: 0.726850 val loss: 0.170936
[Epoch 36] ogbg-molclintox: 0.606704 test loss: 0.306455
[Epoch 37; Iter    30/   40] train: loss: 0.3435487
[Epoch 37] ogbg-molclintox: 0.805070 val loss: 0.136551
[Epoch 37] ogbg-molclintox: 0.521614 test loss: 0.270665
[Epoch 38; Iter    20/   40] train: loss: 0.1654812
[Epoch 38] ogbg-molclintox: 0.864704 val loss: 0.496727
[Epoch 38] ogbg-molclintox: 0.673439 test loss: 0.592724
[Epoch 39; Iter    10/   40] train: loss: 0.1027557
[Epoch 39; Iter    40/   40] train: loss: 0.1136742
[Epoch 39] ogbg-molclintox: 0.843989 val loss: 0.516483
[Epoch 39] ogbg-molclintox: 0.728004 test loss: 3.135595
[Epoch 40; Iter    30/   40] train: loss: 0.2684976
[Epoch 40] ogbg-molclintox: 0.729584 val loss: 2.644218
[Epoch 40] ogbg-molclintox: 0.655464 test loss: 12.608449
[Epoch 41; Iter    20/   40] train: loss: 0.2921315
[Epoch 41] ogbg-molclintox: 0.875496 val loss: 2.357650
[Epoch 41] ogbg-molclintox: 0.681959 test loss: 0.729478
[Epoch 42; Iter    10/   40] train: loss: 0.0870369
[Epoch 42; Iter    40/   40] train: loss: 0.0863183
[Epoch 42] ogbg-molclintox: 0.897459 val loss: 0.419798
[Epoch 42] ogbg-molclintox: 0.769925 test loss: 0.226316
[Epoch 43; Iter    30/   40] train: loss: 0.3099524
[Epoch 43] ogbg-molclintox: 0.956158 val loss: 0.190874
[Epoch 43] ogbg-molclintox: 0.814408 test loss: 0.369122
[Epoch 44; Iter    20/   40] train: loss: 0.0660312
[Epoch 44] ogbg-molclintox: 0.950177 val loss: 0.249243
[Epoch 44] ogbg-molclintox: 0.763130 test loss: 0.559203
[Epoch 45; Iter    10/   40] train: loss: 0.1256240
[Epoch 45; Iter    40/   40] train: loss: 0.4999748
[Epoch 45] ogbg-molclintox: 0.722141 val loss: 5.173263
[Epoch 45] ogbg-molclintox: 0.694950 test loss: 10.318624
[Epoch 46; Iter    30/   40] train: loss: 0.0862322
[Epoch 46] ogbg-molclintox: 0.826495 val loss: 0.267488
[Epoch 46] ogbg-molclintox: 0.696870 test loss: 3.172308
[Epoch 47; Iter    20/   40] train: loss: 0.0857843
[Epoch 47] ogbg-molclintox: 0.918723 val loss: 0.337724
[Epoch 47] ogbg-molclintox: 0.803165 test loss: 0.417844
[Epoch 48; Iter    10/   40] train: loss: 0.0454374
[Epoch 48; Iter    40/   40] train: loss: 0.1139079
[Epoch 48] ogbg-molclintox: 0.917974 val loss: 0.262398
[Epoch 48] ogbg-molclintox: 0.785878 test loss: 2.115841
[Epoch 49; Iter    30/   40] train: loss: 0.0427280
[Epoch 49] ogbg-molclintox: 0.933633 val loss: 0.123830
[Epoch 49] ogbg-molclintox: 0.774598 test loss: 0.909096
[Epoch 50; Iter    20/   40] train: loss: 0.0841657
[Epoch 50] ogbg-molclintox: 0.942436 val loss: 0.433678
[Epoch 50] ogbg-molclintox: 0.768326 test loss: 0.684727
[Epoch 51; Iter    10/   40] train: loss: 0.0241981
[Epoch 51; Iter    40/   40] train: loss: 0.0502354
[Epoch 51] ogbg-molclintox: 0.930136 val loss: 0.500281
[Epoch 51] ogbg-molclintox: 0.740569 test loss: 0.875839
[Epoch 52; Iter    30/   40] train: loss: 0.0298895
[Epoch 52] ogbg-molclintox: 0.906574 val loss: 0.106384
[Epoch 52] ogbg-molclintox: 0.815783 test loss: 0.226758
[Epoch 53; Iter    20/   40] train: loss: 0.0279315
[Epoch 53] ogbg-molclintox: 0.932870 val loss: 0.581127
[Epoch 53] ogbg-molclintox: 0.754536 test loss: 0.786538
[Epoch 54; Iter    10/   40] train: loss: 0.0401216
[Epoch 54; Iter    40/   40] train: loss: 0.0084187
[Epoch 54] ogbg-molclintox: 0.947243 val loss: 0.317044
[Epoch 54] ogbg-molclintox: 0.739434 test loss: 0.974647
[Epoch 55; Iter    30/   40] train: loss: 0.0163384
[Epoch 55] ogbg-molclintox: 0.883747 val loss: 0.775514
[Epoch 55] ogbg-molclintox: 0.756508 test loss: 0.986194
[Epoch 56; Iter    20/   40] train: loss: 0.0391094
[Epoch 56] ogbg-molclintox: 0.914453 val loss: 0.152644
[Epoch 56] ogbg-molclintox: 0.764979 test loss: 7.168993
[Epoch 57; Iter    10/   40] train: loss: 0.0081573
[Epoch 57; Iter    40/   40] train: loss: 0.0027230
[Epoch 57] ogbg-molclintox: 0.927315 val loss: 0.697166
[Epoch 57] ogbg-molclintox: 0.798720 test loss: 1.588359
[Epoch 58; Iter    30/   40] train: loss: 0.0269937
[Epoch 58] ogbg-molclintox: 0.936205 val loss: 0.138998
[Epoch 58] ogbg-molclintox: 0.757808 test loss: 0.403622
[Epoch 59; Iter    20/   40] train: loss: 0.0653678
[Epoch 59] ogbg-molclintox: 0.902565 val loss: 0.635398
[Epoch 59] ogbg-molclintox: 0.752675 test loss: 0.731587
[Epoch 60; Iter    10/   40] train: loss: 0.0062521
[Epoch 60; Iter    40/   40] train: loss: 0.0318311
[Epoch 60] ogbg-molclintox: 0.956721 val loss: 0.306534
[Epoch 60] ogbg-molclintox: 0.736486 test loss: 0.552817
[Epoch 61; Iter    30/   40] train: loss: 0.0074401
[Epoch 61] ogbg-molclintox: 0.895210 val loss: 0.397021
[Epoch 61] ogbg-molclintox: 0.790323 test loss: 0.622844
[Epoch 62; Iter    20/   40] train: loss: 0.0058078
[Epoch 62] ogbg-molclintox: 0.912791 val loss: 0.362639
[Epoch 62] ogbg-molclintox: 0.802840 test loss: 0.850847
[Epoch 63; Iter    10/   40] train: loss: 0.0840365
[Epoch 63; Iter    40/   40] train: loss: 0.0052858
[Epoch 63] ogbg-molclintox: 0.973054 val loss: 0.076837
[Epoch 63] ogbg-molclintox: 0.847212 test loss: 0.236604
[Epoch 64; Iter    30/   40] train: loss: 0.0208308
[Epoch 64] ogbg-molclintox: 0.954036 val loss: 0.658317
[Epoch 64] ogbg-molclintox: 0.825140 test loss: 1.102040
[Epoch 65; Iter    20/   40] train: loss: 0.0072808
[Epoch 65] ogbg-molclintox: 0.933457 val loss: 0.136584
[Epoch 65] ogbg-molclintox: 0.823717 test loss: 1.099336
[Epoch 66; Iter    10/   40] train: loss: 0.0925079
[Epoch 66; Iter    40/   40] train: loss: 0.0443459
[Epoch 66] ogbg-molclintox: 0.919598 val loss: 0.172621
[Epoch 66] ogbg-molclintox: 0.834134 test loss: 0.265551
[Epoch 67; Iter    30/   40] train: loss: 0.0027339
[Epoch 67] ogbg-molclintox: 0.935668 val loss: 0.481855
[Epoch 67] ogbg-molclintox: 0.794361 test loss: 0.780674
[Epoch 68; Iter    20/   40] train: loss: 0.0072228
[Epoch 68] ogbg-molclintox: 0.902628 val loss: 0.157214
[Epoch 68] ogbg-molclintox: 0.826115 test loss: 0.893891
[Epoch 69; Iter    10/   40] train: loss: 0.0153206
[Epoch 69; Iter    40/   40] train: loss: 0.0051478
[Epoch 69] ogbg-molclintox: 0.902842 val loss: 0.098150
[Epoch 69] ogbg-molclintox: 0.802444 test loss: 0.388220
[Epoch 70; Iter    30/   40] train: loss: 0.0199861
[Epoch 70] ogbg-molclintox: 0.924468 val loss: 0.108437
[Epoch 70] ogbg-molclintox: 0.766754 test loss: 0.276945
[Epoch 71; Iter    20/   40] train: loss: 0.0143863
[Epoch 71] ogbg-molclintox: 0.922058 val loss: 0.147651
[Epoch 71] ogbg-molclintox: 0.756011 test loss: 0.412496
[Epoch 72; Iter    10/   40] train: loss: 0.0029330
[Epoch 72; Iter    40/   40] train: loss: 0.0088945
[Epoch 72] ogbg-molclintox: 0.885458 val loss: 0.105806
[Epoch 72] ogbg-molclintox: 0.764468 test loss: 0.314016
[Epoch 73; Iter    30/   40] train: loss: 0.0020474
[Epoch 73] ogbg-molclintox: 0.918399 val loss: 0.126720
[Epoch 73] ogbg-molclintox: 0.782169 test loss: 0.418161
[Epoch 74; Iter    20/   40] train: loss: 0.0048385
[Epoch 74] ogbg-molclintox: 0.909533 val loss: 0.139365
[Epoch 74] ogbg-molclintox: 0.773937 test loss: 0.399501
[Epoch 75; Iter    10/   40] train: loss: 0.0130721
[Epoch 75; Iter    40/   40] train: loss: 0.0051622
[Epoch 75] ogbg-molclintox: 0.913367 val loss: 0.133296
[Epoch 75] ogbg-molclintox: 0.772812 test loss: 0.397559
[Epoch 76; Iter    30/   40] train: loss: 0.0010927
[Epoch 31] ogbg-molclintox: 0.702507 test loss: 0.223730
[Epoch 32; Iter    20/   40] train: loss: 0.1770710
[Epoch 32] ogbg-molclintox: 0.864792 val loss: 0.135608
[Epoch 32] ogbg-molclintox: 0.693624 test loss: 0.231467
[Epoch 33; Iter    10/   40] train: loss: 0.2129232
[Epoch 33; Iter    40/   40] train: loss: 0.7950255
[Epoch 33] ogbg-molclintox: 0.907597 val loss: 0.123135
[Epoch 33] ogbg-molclintox: 0.724780 test loss: 0.229322
[Epoch 34; Iter    30/   40] train: loss: 0.1716650
[Epoch 34] ogbg-molclintox: 0.895860 val loss: 0.133681
[Epoch 34] ogbg-molclintox: 0.714874 test loss: 0.232798
[Epoch 35; Iter    20/   40] train: loss: 0.2016225
[Epoch 35] ogbg-molclintox: 0.957082 val loss: 0.106463
[Epoch 35] ogbg-molclintox: 0.780219 test loss: 0.208706
[Epoch 36; Iter    10/   40] train: loss: 0.1846903
[Epoch 36; Iter    40/   40] train: loss: 0.2893732
[Epoch 36] ogbg-molclintox: 0.974839 val loss: 0.090012
[Epoch 36] ogbg-molclintox: 0.829798 test loss: 0.215933
[Epoch 37; Iter    30/   40] train: loss: 0.2571810
[Epoch 37] ogbg-molclintox: 0.921158 val loss: 0.729669
[Epoch 37] ogbg-molclintox: 0.789389 test loss: 2.653347
[Epoch 38; Iter    20/   40] train: loss: 0.1443333
[Epoch 38] ogbg-molclintox: 0.959679 val loss: 0.168198
[Epoch 38] ogbg-molclintox: 0.856543 test loss: 0.254332
[Epoch 39; Iter    10/   40] train: loss: 0.1192919
[Epoch 39; Iter    40/   40] train: loss: 0.0604773
[Epoch 39] ogbg-molclintox: 0.965724 val loss: 0.282759
[Epoch 39] ogbg-molclintox: 0.852934 test loss: 0.623442
[Epoch 40; Iter    30/   40] train: loss: 0.2922643
[Epoch 40] ogbg-molclintox: 0.899929 val loss: 0.200879
[Epoch 40] ogbg-molclintox: 0.830795 test loss: 0.460303
[Epoch 41; Iter    20/   40] train: loss: 0.1842069
[Epoch 41] ogbg-molclintox: 0.684157 val loss: 14.649013
[Epoch 41] ogbg-molclintox: 0.665404 test loss: 78.436001
[Epoch 42; Iter    10/   40] train: loss: 0.1282355
[Epoch 42; Iter    40/   40] train: loss: 0.1229826
[Epoch 42] ogbg-molclintox: 0.975226 val loss: 0.083750
[Epoch 42] ogbg-molclintox: 0.837332 test loss: 0.288475
[Epoch 43; Iter    30/   40] train: loss: 0.4351546
[Epoch 43] ogbg-molclintox: 0.987588 val loss: 0.082050
[Epoch 43] ogbg-molclintox: 0.810457 test loss: 0.244164
[Epoch 44; Iter    20/   40] train: loss: 0.1722130
[Epoch 44] ogbg-molclintox: 0.922669 val loss: 0.150831
[Epoch 44] ogbg-molclintox: 0.824602 test loss: 0.214120
[Epoch 45; Iter    10/   40] train: loss: 0.1142563
[Epoch 45; Iter    40/   40] train: loss: 0.2774175
[Epoch 45] ogbg-molclintox: 0.983255 val loss: 0.081206
[Epoch 45] ogbg-molclintox: 0.892084 test loss: 0.225082
[Epoch 46; Iter    30/   40] train: loss: 0.1314218
[Epoch 46] ogbg-molclintox: 0.979646 val loss: 0.062553
[Epoch 46] ogbg-molclintox: 0.866262 test loss: 0.250942
[Epoch 47; Iter    20/   40] train: loss: 0.1228327
[Epoch 47] ogbg-molclintox: 0.969195 val loss: 0.068544
[Epoch 47] ogbg-molclintox: 0.800255 test loss: 0.370521
[Epoch 48; Iter    10/   40] train: loss: 0.0721220
[Epoch 48; Iter    40/   40] train: loss: 0.2021886
[Epoch 48] ogbg-molclintox: 0.968296 val loss: 0.407153
[Epoch 48] ogbg-molclintox: 0.855482 test loss: 0.409096
[Epoch 49; Iter    30/   40] train: loss: 0.1308489
[Epoch 49] ogbg-molclintox: 0.930200 val loss: 0.121856
[Epoch 49] ogbg-molclintox: 0.838766 test loss: 0.329561
[Epoch 50; Iter    20/   40] train: loss: 0.0888344
[Epoch 50] ogbg-molclintox: 0.968022 val loss: 0.109178
[Epoch 50] ogbg-molclintox: 0.839054 test loss: 0.192364
[Epoch 51; Iter    10/   40] train: loss: 0.1094197
[Epoch 51; Iter    40/   40] train: loss: 0.0509654
[Epoch 51] ogbg-molclintox: 0.958232 val loss: 0.461082
[Epoch 51] ogbg-molclintox: 0.840503 test loss: 0.315400
[Epoch 52; Iter    30/   40] train: loss: 0.0614251
[Epoch 52] ogbg-molclintox: 0.943947 val loss: 0.083874
[Epoch 52] ogbg-molclintox: 0.797219 test loss: 0.257907
[Epoch 53; Iter    20/   40] train: loss: 0.0453290
[Epoch 53] ogbg-molclintox: 0.995667 val loss: 0.057441
[Epoch 53] ogbg-molclintox: 0.813008 test loss: 0.253146
[Epoch 54; Iter    10/   40] train: loss: 0.0454325
[Epoch 54; Iter    40/   40] train: loss: 0.0043712
[Epoch 54] ogbg-molclintox: 0.929774 val loss: 0.226788
[Epoch 54] ogbg-molclintox: 0.810509 test loss: 0.379520
[Epoch 55; Iter    30/   40] train: loss: 0.0124074
[Epoch 55] ogbg-molclintox: 0.956334 val loss: 0.073228
[Epoch 55] ogbg-molclintox: 0.767026 test loss: 0.259685
[Epoch 56; Iter    20/   40] train: loss: 0.0986606
[Epoch 56] ogbg-molclintox: 0.998127 val loss: 0.042217
[Epoch 56] ogbg-molclintox: 0.850958 test loss: 0.266760
[Epoch 57; Iter    10/   40] train: loss: 0.0189650
[Epoch 57; Iter    40/   40] train: loss: 0.0188623
[Epoch 57] ogbg-molclintox: 0.906648 val loss: 7.874330
[Epoch 57] ogbg-molclintox: 0.830444 test loss: 0.308290
[Epoch 58; Iter    30/   40] train: loss: 0.0192800
[Epoch 58] ogbg-molclintox: 0.969245 val loss: 0.090568
[Epoch 58] ogbg-molclintox: 0.849969 test loss: 0.206040
[Epoch 59; Iter    20/   40] train: loss: 0.0262984
[Epoch 59] ogbg-molclintox: 0.975700 val loss: 0.087824
[Epoch 59] ogbg-molclintox: 0.840690 test loss: 0.310014
[Epoch 60; Iter    10/   40] train: loss: 0.0060730
[Epoch 60; Iter    40/   40] train: loss: 0.0250857
[Epoch 60] ogbg-molclintox: 0.934856 val loss: 0.388936
[Epoch 60] ogbg-molclintox: 0.817390 test loss: 0.351420
[Epoch 61; Iter    30/   40] train: loss: 0.0164364
[Epoch 61] ogbg-molclintox: 0.973279 val loss: 0.061946
[Epoch 61] ogbg-molclintox: 0.803188 test loss: 0.379831
[Epoch 62; Iter    20/   40] train: loss: 0.0042190
[Epoch 62] ogbg-molclintox: 0.977274 val loss: 0.066084
[Epoch 62] ogbg-molclintox: 0.827336 test loss: 0.319545
[Epoch 63; Iter    10/   40] train: loss: 0.0424291
[Epoch 63; Iter    40/   40] train: loss: 0.0027519
[Epoch 63] ogbg-molclintox: 0.937453 val loss: 0.089049
[Epoch 63] ogbg-molclintox: 0.739976 test loss: 0.570456
[Epoch 64; Iter    30/   40] train: loss: 0.0083305
[Epoch 64] ogbg-molclintox: 0.955972 val loss: 0.092887
[Epoch 64] ogbg-molclintox: 0.817778 test loss: 1.612083
[Epoch 65; Iter    20/   40] train: loss: 0.0094045
[Epoch 65] ogbg-molclintox: 0.951938 val loss: 0.613230
[Epoch 65] ogbg-molclintox: 0.846622 test loss: 2.268457
[Epoch 66; Iter    10/   40] train: loss: 0.0243412
[Epoch 66; Iter    40/   40] train: loss: 0.3560988
[Epoch 66] ogbg-molclintox: 0.966859 val loss: 0.070129
[Epoch 66] ogbg-molclintox: 0.744391 test loss: 6.618311
[Epoch 67; Iter    30/   40] train: loss: 0.0127220
[Epoch 67] ogbg-molclintox: 0.976463 val loss: 0.087382
[Epoch 67] ogbg-molclintox: 0.781766 test loss: 1.539991
[Epoch 68; Iter    20/   40] train: loss: 0.0049124
[Epoch 68] ogbg-molclintox: 0.856537 val loss: 0.242507
[Epoch 68] ogbg-molclintox: 0.799079 test loss: 1.250318
[Epoch 69; Iter    10/   40] train: loss: 0.0077866
[Epoch 69; Iter    40/   40] train: loss: 0.2418418
[Epoch 69] ogbg-molclintox: 0.812622 val loss: 0.188475
[Epoch 69] ogbg-molclintox: 0.818465 test loss: 0.932757
[Epoch 70; Iter    30/   40] train: loss: 0.0890547
[Epoch 70] ogbg-molclintox: 0.948916 val loss: 0.714903
[Epoch 70] ogbg-molclintox: 0.780141 test loss: 1.912917
[Epoch 71; Iter    20/   40] train: loss: 0.0325504
[Epoch 71] ogbg-molclintox: 0.977524 val loss: 0.055391
[Epoch 71] ogbg-molclintox: 0.837280 test loss: 0.275043
[Epoch 72; Iter    10/   40] train: loss: 0.0031921
[Epoch 72; Iter    40/   40] train: loss: 0.0221445
[Epoch 72] ogbg-molclintox: 0.956896 val loss: 0.085448
[Epoch 72] ogbg-molclintox: 0.804637 test loss: 0.341884
[Epoch 73; Iter    30/   40] train: loss: 0.0041230
[Epoch 73] ogbg-molclintox: 0.951277 val loss: 0.071164
[Epoch 73] ogbg-molclintox: 0.812870 test loss: 0.358104
[Epoch 74; Iter    20/   40] train: loss: 0.0070381
[Epoch 74] ogbg-molclintox: 0.960055 val loss: 0.103004
[Epoch 74] ogbg-molclintox: 0.804675 test loss: 0.358035
[Epoch 75; Iter    10/   40] train: loss: 0.0522514
[Epoch 75; Iter    40/   40] train: loss: 0.0279331
[Epoch 75] ogbg-molclintox: 0.963763 val loss: 0.077207
[Epoch 75] ogbg-molclintox: 0.829634 test loss: 0.244494
[Epoch 76; Iter    30/   40] train: loss: 0.0027896
[Epoch 31] ogbg-molclintox: 0.645633 test loss: 1.690915
[Epoch 32; Iter    20/   40] train: loss: 0.1587379
[Epoch 32] ogbg-molclintox: 0.769483 val loss: 2.911129
[Epoch 32] ogbg-molclintox: 0.620786 test loss: 1.706563
[Epoch 33; Iter    10/   40] train: loss: 0.2575201
[Epoch 33; Iter    40/   40] train: loss: 0.8291774
[Epoch 33] ogbg-molclintox: 0.798165 val loss: 3.214622
[Epoch 33] ogbg-molclintox: 0.640299 test loss: 1.720225
[Epoch 34; Iter    30/   40] train: loss: 0.2010513
[Epoch 34] ogbg-molclintox: 0.820054 val loss: 4.558193
[Epoch 34] ogbg-molclintox: 0.647893 test loss: 1.696915
[Epoch 35; Iter    20/   40] train: loss: 0.2180594
[Epoch 35] ogbg-molclintox: 0.901904 val loss: 1.033730
[Epoch 35] ogbg-molclintox: 0.691316 test loss: 0.953410
[Epoch 36; Iter    10/   40] train: loss: 0.2105889
[Epoch 36; Iter    40/   40] train: loss: 0.3145440
[Epoch 36] ogbg-molclintox: 0.935393 val loss: 0.151669
[Epoch 36] ogbg-molclintox: 0.723480 test loss: 1.275046
[Epoch 37; Iter    30/   40] train: loss: 0.2605546
[Epoch 37] ogbg-molclintox: 0.881350 val loss: 2.711328
[Epoch 37] ogbg-molclintox: 0.707179 test loss: 1.396736
[Epoch 38; Iter    20/   40] train: loss: 0.2112984
[Epoch 38] ogbg-molclintox: 0.948891 val loss: 6.297915
[Epoch 38] ogbg-molclintox: 0.757957 test loss: 3.985077
[Epoch 39; Iter    10/   40] train: loss: 0.1000640
[Epoch 39; Iter    40/   40] train: loss: 0.0836708
[Epoch 39] ogbg-molclintox: 0.963988 val loss: 0.121847
[Epoch 39] ogbg-molclintox: 0.826813 test loss: 0.765266
[Epoch 40; Iter    30/   40] train: loss: 0.3193537
[Epoch 40] ogbg-molclintox: 0.980096 val loss: 0.085992
[Epoch 40] ogbg-molclintox: 0.788575 test loss: 1.578559
[Epoch 41; Iter    20/   40] train: loss: 0.2760104
[Epoch 41] ogbg-molclintox: 0.829866 val loss: 0.125366
[Epoch 41] ogbg-molclintox: 0.779233 test loss: 0.230252
[Epoch 42; Iter    10/   40] train: loss: 0.0512429
[Epoch 42; Iter    40/   40] train: loss: 0.0741182
[Epoch 42] ogbg-molclintox: 0.847671 val loss: 0.275699
[Epoch 42] ogbg-molclintox: 0.760844 test loss: 0.989421
[Epoch 43; Iter    30/   40] train: loss: 0.3388286
[Epoch 43] ogbg-molclintox: 0.973279 val loss: 0.093222
[Epoch 43] ogbg-molclintox: 0.818693 test loss: 0.882401
[Epoch 44; Iter    20/   40] train: loss: 0.0736187
[Epoch 44] ogbg-molclintox: 0.852844 val loss: 0.335853
[Epoch 44] ogbg-molclintox: 0.784713 test loss: 0.618622
[Epoch 45; Iter    10/   40] train: loss: 0.0949439
[Epoch 45; Iter    40/   40] train: loss: 0.1703367
[Epoch 45] ogbg-molclintox: 0.886383 val loss: 0.115812
[Epoch 45] ogbg-molclintox: 0.841590 test loss: 0.677382
[Epoch 46; Iter    30/   40] train: loss: 0.0488508
[Epoch 46] ogbg-molclintox: 0.926503 val loss: 0.124266
[Epoch 46] ogbg-molclintox: 0.814667 test loss: 0.916697
[Epoch 47; Iter    20/   40] train: loss: 0.1154442
[Epoch 47] ogbg-molclintox: 0.765610 val loss: 0.153745
[Epoch 47] ogbg-molclintox: 0.817715 test loss: 0.635428
[Epoch 48; Iter    10/   40] train: loss: 0.0399634
[Epoch 48; Iter    40/   40] train: loss: 0.1725322
[Epoch 48] ogbg-molclintox: 0.910570 val loss: 0.097031
[Epoch 48] ogbg-molclintox: 0.880116 test loss: 1.596749
[Epoch 49; Iter    30/   40] train: loss: 0.0703444
[Epoch 49] ogbg-molclintox: 0.786438 val loss: 0.230864
[Epoch 49] ogbg-molclintox: 0.855504 test loss: 0.263206
[Epoch 50; Iter    20/   40] train: loss: 0.0755337
[Epoch 50] ogbg-molclintox: 0.765410 val loss: 0.140309
[Epoch 50] ogbg-molclintox: 0.808996 test loss: 1.024246
[Epoch 51; Iter    10/   40] train: loss: 0.0337391
[Epoch 51; Iter    40/   40] train: loss: 0.0430480
[Epoch 51] ogbg-molclintox: 0.924855 val loss: 0.674063
[Epoch 51] ogbg-molclintox: 0.883089 test loss: 0.658927
[Epoch 52; Iter    30/   40] train: loss: 0.0858959
[Epoch 52] ogbg-molclintox: 0.891063 val loss: 0.153360
[Epoch 52] ogbg-molclintox: 0.856393 test loss: 0.344880
[Epoch 53; Iter    20/   40] train: loss: 0.0858949
[Epoch 53] ogbg-molclintox: 0.737740 val loss: 0.323443
[Epoch 53] ogbg-molclintox: 0.751820 test loss: 0.349020
[Epoch 54; Iter    10/   40] train: loss: 0.0594611
[Epoch 54; Iter    40/   40] train: loss: 0.0112888
[Epoch 54] ogbg-molclintox: 0.794454 val loss: 0.188459
[Epoch 54] ogbg-molclintox: 0.811693 test loss: 0.325294
[Epoch 55; Iter    30/   40] train: loss: 0.0189557
[Epoch 55] ogbg-molclintox: 0.849709 val loss: 0.362872
[Epoch 55] ogbg-molclintox: 0.792546 test loss: 0.315867
[Epoch 56; Iter    20/   40] train: loss: 0.0236263
[Epoch 56] ogbg-molclintox: 0.941213 val loss: 0.112762
[Epoch 56] ogbg-molclintox: 0.871446 test loss: 0.235327
[Epoch 57; Iter    10/   40] train: loss: 0.0266858
[Epoch 57; Iter    40/   40] train: loss: 0.0063836
[Epoch 57] ogbg-molclintox: 0.863695 val loss: 0.158648
[Epoch 57] ogbg-molclintox: 0.795818 test loss: 0.328281
[Epoch 58; Iter    30/   40] train: loss: 0.0239260
[Epoch 58] ogbg-molclintox: 0.865305 val loss: 0.319344
[Epoch 58] ogbg-molclintox: 0.838415 test loss: 0.407860
[Epoch 59; Iter    20/   40] train: loss: 0.0811731
[Epoch 59] ogbg-molclintox: 0.934807 val loss: 0.242098
[Epoch 59] ogbg-molclintox: 0.878603 test loss: 0.378523
[Epoch 60; Iter    10/   40] train: loss: 0.0074154
[Epoch 60; Iter    40/   40] train: loss: 0.0118616
[Epoch 60] ogbg-molclintox: 0.842755 val loss: 0.353274
[Epoch 60] ogbg-molclintox: 0.851769 test loss: 0.780351
[Epoch 61; Iter    30/   40] train: loss: 0.0163445
[Epoch 61] ogbg-molclintox: 0.915577 val loss: 0.126502
[Epoch 61] ogbg-molclintox: 0.858078 test loss: 0.246402
[Epoch 62; Iter    20/   40] train: loss: 0.0040017
[Epoch 62] ogbg-molclintox: 0.919099 val loss: 0.171578
[Epoch 62] ogbg-molclintox: 0.823698 test loss: 0.414755
[Epoch 63; Iter    10/   40] train: loss: 0.0586510
[Epoch 63; Iter    40/   40] train: loss: 0.0032759
[Epoch 63] ogbg-molclintox: 0.803007 val loss: 0.214235
[Epoch 63] ogbg-molclintox: 0.817674 test loss: 0.270562
[Epoch 64; Iter    30/   40] train: loss: 0.0159592
[Epoch 64] ogbg-molclintox: 0.869301 val loss: 0.595494
[Epoch 64] ogbg-molclintox: 0.831330 test loss: 1.274008
[Epoch 65; Iter    20/   40] train: loss: 0.0113669
[Epoch 65] ogbg-molclintox: 0.908946 val loss: 0.298629
[Epoch 65] ogbg-molclintox: 0.798351 test loss: 3.019500
[Epoch 66; Iter    10/   40] train: loss: 0.0457341
[Epoch 66; Iter    40/   40] train: loss: 0.1076315
[Epoch 66] ogbg-molclintox: 0.957483 val loss: 0.096543
[Epoch 66] ogbg-molclintox: 0.881013 test loss: 0.295986
[Epoch 67; Iter    30/   40] train: loss: 0.0149609
[Epoch 67] ogbg-molclintox: 0.811624 val loss: 0.257890
[Epoch 67] ogbg-molclintox: 0.780500 test loss: 0.366483
[Epoch 68; Iter    20/   40] train: loss: 0.0056197
[Epoch 68] ogbg-molclintox: 0.916800 val loss: 0.976978
[Epoch 68] ogbg-molclintox: 0.816303 test loss: 2.092684
[Epoch 69; Iter    10/   40] train: loss: 0.0062689
[Epoch 69; Iter    40/   40] train: loss: 0.0263624
[Epoch 69] ogbg-molclintox: 0.965587 val loss: 0.365712
[Epoch 69] ogbg-molclintox: 0.835293 test loss: 0.478284
[Epoch 70; Iter    30/   40] train: loss: 0.0382471
[Epoch 70] ogbg-molclintox: 0.927178 val loss: 0.743815
[Epoch 70] ogbg-molclintox: 0.825473 test loss: 0.619356
[Epoch 71; Iter    20/   40] train: loss: 0.0120027
[Epoch 71] ogbg-molclintox: 0.919011 val loss: 0.179058
[Epoch 71] ogbg-molclintox: 0.799363 test loss: 0.545899
[Epoch 72; Iter    10/   40] train: loss: 0.0053349
[Epoch 72; Iter    40/   40] train: loss: 0.0157754
[Epoch 72] ogbg-molclintox: 0.869575 val loss: 0.771648
[Epoch 72] ogbg-molclintox: 0.805945 test loss: 0.396279
[Epoch 73; Iter    30/   40] train: loss: 0.0044212
[Epoch 73] ogbg-molclintox: 0.846775 val loss: 0.538574
[Epoch 73] ogbg-molclintox: 0.799128 test loss: 0.394401
[Epoch 74; Iter    20/   40] train: loss: 0.0056835
[Epoch 74] ogbg-molclintox: 0.911157 val loss: 0.385009
[Epoch 74] ogbg-molclintox: 0.799389 test loss: 0.513731
[Epoch 75; Iter    10/   40] train: loss: 0.0303684
[Epoch 75; Iter    40/   40] train: loss: 0.0077912
[Epoch 75] ogbg-molclintox: 0.915964 val loss: 0.167859
[Epoch 75] ogbg-molclintox: 0.811432 test loss: 0.413972
[Epoch 76; Iter    30/   40] train: loss: 0.0026882
[Epoch 31] ogbg-molclintox: 0.615288 test loss: 0.232245
[Epoch 32; Iter    20/   40] train: loss: 0.1926809
[Epoch 32] ogbg-molclintox: 0.918023 val loss: 0.148958
[Epoch 32] ogbg-molclintox: 0.632265 test loss: 0.237916
[Epoch 33; Iter    10/   40] train: loss: 0.0723649
[Epoch 33; Iter    40/   40] train: loss: 0.2282541
[Epoch 33] ogbg-molclintox: 0.870235 val loss: 0.135826
[Epoch 33] ogbg-molclintox: 0.717597 test loss: 0.236348
[Epoch 34; Iter    30/   40] train: loss: 0.0821828
[Epoch 34] ogbg-molclintox: 0.917848 val loss: 0.166075
[Epoch 34] ogbg-molclintox: 0.690251 test loss: 0.254939
[Epoch 35; Iter    20/   40] train: loss: 0.2674727
[Epoch 35] ogbg-molclintox: 0.896932 val loss: 0.146178
[Epoch 35] ogbg-molclintox: 0.707467 test loss: 0.285078
[Epoch 36; Iter    10/   40] train: loss: 0.1706083
[Epoch 36; Iter    40/   40] train: loss: 0.3144392
[Epoch 36] ogbg-molclintox: 0.892813 val loss: 0.149349
[Epoch 36] ogbg-molclintox: 0.675460 test loss: 0.248550
[Epoch 37; Iter    30/   40] train: loss: 0.2207048
[Epoch 37] ogbg-molclintox: 0.981994 val loss: 0.118183
[Epoch 37] ogbg-molclintox: 0.751861 test loss: 0.448824
[Epoch 38; Iter    20/   40] train: loss: 0.0688318
[Epoch 38] ogbg-molclintox: 0.997428 val loss: 0.077796
[Epoch 38] ogbg-molclintox: 0.812858 test loss: 0.277011
[Epoch 39; Iter    10/   40] train: loss: 0.1826438
[Epoch 39; Iter    40/   40] train: loss: 0.1993225
[Epoch 39] ogbg-molclintox: 0.939913 val loss: 0.280862
[Epoch 39] ogbg-molclintox: 0.765491 test loss: 0.770461
[Epoch 40; Iter    30/   40] train: loss: 0.1595367
[Epoch 40] ogbg-molclintox: 0.988200 val loss: 0.116444
[Epoch 40] ogbg-molclintox: 0.790125 test loss: 0.357712
[Epoch 41; Iter    20/   40] train: loss: 0.0677220
[Epoch 41] ogbg-molclintox: 0.971954 val loss: 0.078065
[Epoch 41] ogbg-molclintox: 0.882451 test loss: 0.190072
[Epoch 42; Iter    10/   40] train: loss: 0.1102527
[Epoch 42; Iter    40/   40] train: loss: 0.1486523
[Epoch 42] ogbg-molclintox: 0.963626 val loss: 0.126624
[Epoch 42] ogbg-molclintox: 0.855695 test loss: 0.383339
[Epoch 43; Iter    30/   40] train: loss: 0.0410271
[Epoch 43] ogbg-molclintox: 0.967533 val loss: 0.079016
[Epoch 43] ogbg-molclintox: 0.881203 test loss: 0.174526
[Epoch 44; Iter    20/   40] train: loss: 0.0741681
[Epoch 44] ogbg-molclintox: 0.859510 val loss: 0.123134
[Epoch 44] ogbg-molclintox: 0.690375 test loss: 0.295832
[Epoch 45; Iter    10/   40] train: loss: 0.1927062
[Epoch 45; Iter    40/   40] train: loss: 0.0591232
[Epoch 45] ogbg-molclintox: 0.996029 val loss: 0.056395
[Epoch 45] ogbg-molclintox: 0.822578 test loss: 0.189003
[Epoch 46; Iter    30/   40] train: loss: 0.0518710
[Epoch 46] ogbg-molclintox: 0.972179 val loss: 0.095886
[Epoch 46] ogbg-molclintox: 0.899241 test loss: 0.226852
[Epoch 47; Iter    20/   40] train: loss: 0.1169305
[Epoch 47] ogbg-molclintox: 0.743957 val loss: 0.846922
[Epoch 47] ogbg-molclintox: 0.642802 test loss: 1.301776
[Epoch 48; Iter    10/   40] train: loss: 0.0380290
[Epoch 48; Iter    40/   40] train: loss: 0.0765475
[Epoch 48] ogbg-molclintox: 0.974277 val loss: 0.182092
[Epoch 48] ogbg-molclintox: 0.811533 test loss: 0.231041
[Epoch 49; Iter    30/   40] train: loss: 0.0253443
[Epoch 49] ogbg-molclintox: 0.927989 val loss: 0.274720
[Epoch 49] ogbg-molclintox: 0.805164 test loss: 0.190252
[Epoch 50; Iter    20/   40] train: loss: 0.0589722
[Epoch 50] ogbg-molclintox: 0.982219 val loss: 0.152485
[Epoch 50] ogbg-molclintox: 0.875567 test loss: 0.169390
[Epoch 51; Iter    10/   40] train: loss: 0.0205602
[Epoch 51; Iter    40/   40] train: loss: 0.0127218
[Epoch 51] ogbg-molclintox: 0.973802 val loss: 0.264769
[Epoch 51] ogbg-molclintox: 0.846211 test loss: 0.190292
[Epoch 52; Iter    30/   40] train: loss: 0.0094643
[Epoch 52] ogbg-molclintox: 0.955796 val loss: 0.162318
[Epoch 52] ogbg-molclintox: 0.815316 test loss: 0.261678
[Epoch 53; Iter    20/   40] train: loss: 0.1289850
[Epoch 53] ogbg-molclintox: 0.966672 val loss: 0.270192
[Epoch 53] ogbg-molclintox: 0.874969 test loss: 0.368988
[Epoch 54; Iter    10/   40] train: loss: 0.0179497
[Epoch 54; Iter    40/   40] train: loss: 0.0097896
[Epoch 54] ogbg-molclintox: 0.961053 val loss: 0.202090
[Epoch 54] ogbg-molclintox: 0.789561 test loss: 0.258929
[Epoch 55; Iter    30/   40] train: loss: 0.0794908
[Epoch 55] ogbg-molclintox: 0.958007 val loss: 0.254546
[Epoch 55] ogbg-molclintox: 0.823474 test loss: 0.297738
[Epoch 56; Iter    20/   40] train: loss: 0.0648442
[Epoch 56] ogbg-molclintox: 0.982307 val loss: 0.100660
[Epoch 56] ogbg-molclintox: 0.853121 test loss: 0.185634
[Epoch 57; Iter    10/   40] train: loss: 0.0182816
[Epoch 57; Iter    40/   40] train: loss: 0.0162732
[Epoch 57] ogbg-molclintox: 0.969245 val loss: 0.201164
[Epoch 57] ogbg-molclintox: 0.878603 test loss: 0.181734
[Epoch 58; Iter    30/   40] train: loss: 0.0170407
[Epoch 58] ogbg-molclintox: 0.978698 val loss: 0.120023
[Epoch 58] ogbg-molclintox: 0.875593 test loss: 0.266615
[Epoch 59; Iter    20/   40] train: loss: 0.0100548
[Epoch 59] ogbg-molclintox: 0.959293 val loss: 0.253905
[Epoch 59] ogbg-molclintox: 0.837354 test loss: 0.252211
[Epoch 60; Iter    10/   40] train: loss: 0.0223105
[Epoch 60; Iter    40/   40] train: loss: 0.0350349
[Epoch 60] ogbg-molclintox: 0.956221 val loss: 0.256654
[Epoch 60] ogbg-molclintox: 0.899827 test loss: 0.259906
[Epoch 61; Iter    30/   40] train: loss: 0.0306287
[Epoch 61] ogbg-molclintox: 0.955684 val loss: 0.416716
[Epoch 61] ogbg-molclintox: 0.782341 test loss: 0.418090
[Epoch 62; Iter    20/   40] train: loss: 0.0889704
[Epoch 62] ogbg-molclintox: 0.974976 val loss: 0.113497
[Epoch 62] ogbg-molclintox: 0.839390 test loss: 0.244812
[Epoch 63; Iter    10/   40] train: loss: 0.0379278
[Epoch 63; Iter    40/   40] train: loss: 0.0096773
[Epoch 63] ogbg-molclintox: 0.979060 val loss: 0.141236
[Epoch 63] ogbg-molclintox: 0.828210 test loss: 0.223590
[Epoch 64; Iter    30/   40] train: loss: 0.0038422
[Epoch 64] ogbg-molclintox: 0.958843 val loss: 0.152270
[Epoch 64] ogbg-molclintox: 0.850596 test loss: 0.191743
[Epoch 65; Iter    20/   40] train: loss: 0.0060454
[Epoch 65] ogbg-molclintox: 0.966086 val loss: 0.209976
[Epoch 65] ogbg-molclintox: 0.816743 test loss: 0.273041
[Epoch 66; Iter    10/   40] train: loss: 0.0144418
[Epoch 66; Iter    40/   40] train: loss: 0.0103766
[Epoch 66] ogbg-molclintox: 0.950427 val loss: 0.177104
[Epoch 66] ogbg-molclintox: 0.856954 test loss: 0.196832
[Epoch 67; Iter    30/   40] train: loss: 0.0074270
[Epoch 67] ogbg-molclintox: 0.976125 val loss: 0.276762
[Epoch 67] ogbg-molclintox: 0.824539 test loss: 0.298353
[Epoch 68; Iter    20/   40] train: loss: 0.0035860
[Epoch 68] ogbg-molclintox: 0.971592 val loss: 0.090556
[Epoch 68] ogbg-molclintox: 0.839965 test loss: 0.249956
[Epoch 69; Iter    10/   40] train: loss: 0.0025646
[Epoch 69; Iter    40/   40] train: loss: 0.0125868
[Epoch 69] ogbg-molclintox: 0.974639 val loss: 0.209366
[Epoch 69] ogbg-molclintox: 0.826787 test loss: 0.413472
[Epoch 70; Iter    30/   40] train: loss: 0.0026251
[Epoch 70] ogbg-molclintox: 0.970756 val loss: 0.196825
[Epoch 70] ogbg-molclintox: 0.863188 test loss: 0.299586
[Epoch 71; Iter    20/   40] train: loss: 0.0186912
[Epoch 71] ogbg-molclintox: 0.953786 val loss: 0.264647
[Epoch 71] ogbg-molclintox: 0.722390 test loss: 0.355012
[Epoch 72; Iter    10/   40] train: loss: 0.0092631
[Epoch 72; Iter    40/   40] train: loss: 0.0021187
[Epoch 72] ogbg-molclintox: 0.968521 val loss: 0.149808
[Epoch 72] ogbg-molclintox: 0.864925 test loss: 0.209472
[Epoch 73; Iter    30/   40] train: loss: 0.0590279
[Epoch 73] ogbg-molclintox: 0.982219 val loss: 0.091144
[Epoch 73] ogbg-molclintox: 0.893018 test loss: 0.155877
[Epoch 74; Iter    20/   40] train: loss: 0.0092181
[Epoch 74] ogbg-molclintox: 0.975201 val loss: 0.155508
[Epoch 74] ogbg-molclintox: 0.832282 test loss: 0.178607
[Epoch 75; Iter    10/   40] train: loss: 0.0188371
[Epoch 75; Iter    40/   40] train: loss: 0.0143506
[Epoch 75] ogbg-molclintox: 0.969220 val loss: 0.117975
[Epoch 75] ogbg-molclintox: 0.855217 test loss: 0.157150
[Epoch 76; Iter    30/   40] train: loss: 0.0025930
[Epoch 31] ogbg-molclintox: 0.654303 test loss: 0.227692
[Epoch 32; Iter    20/   40] train: loss: 0.2138419
[Epoch 32] ogbg-molclintox: 0.753048 val loss: 0.241255
[Epoch 32] ogbg-molclintox: 0.649204 test loss: 0.245622
[Epoch 33; Iter    10/   40] train: loss: 0.0751866
[Epoch 33; Iter    40/   40] train: loss: 0.2694042
[Epoch 33] ogbg-molclintox: 0.718184 val loss: 0.601574
[Epoch 33] ogbg-molclintox: 0.616677 test loss: 0.241932
[Epoch 34; Iter    30/   40] train: loss: 0.0766773
[Epoch 34] ogbg-molclintox: 0.758192 val loss: 1.731825
[Epoch 34] ogbg-molclintox: 0.661747 test loss: 0.236553
[Epoch 35; Iter    20/   40] train: loss: 0.2983027
[Epoch 35] ogbg-molclintox: 0.746705 val loss: 2.878639
[Epoch 35] ogbg-molclintox: 0.630180 test loss: 0.257189
[Epoch 36; Iter    10/   40] train: loss: 0.1999507
[Epoch 36; Iter    40/   40] train: loss: 0.2919669
[Epoch 36] ogbg-molclintox: 0.741198 val loss: 1.328595
[Epoch 36] ogbg-molclintox: 0.582223 test loss: 0.329657
[Epoch 37; Iter    30/   40] train: loss: 0.2237361
[Epoch 37] ogbg-molclintox: 0.758090 val loss: 0.273210
[Epoch 37] ogbg-molclintox: 0.668654 test loss: 0.496116
[Epoch 38; Iter    20/   40] train: loss: 0.0584494
[Epoch 38] ogbg-molclintox: 0.808113 val loss: 0.340972
[Epoch 38] ogbg-molclintox: 0.628944 test loss: 0.294462
[Epoch 39; Iter    10/   40] train: loss: 0.2061829
[Epoch 39; Iter    40/   40] train: loss: 0.2158885
[Epoch 39] ogbg-molclintox: 0.887820 val loss: 0.356544
[Epoch 39] ogbg-molclintox: 0.737473 test loss: 0.317646
[Epoch 40; Iter    30/   40] train: loss: 0.2130139
[Epoch 40] ogbg-molclintox: 0.340592 val loss: 2.391077
[Epoch 40] ogbg-molclintox: 0.418987 test loss: 5.558699
[Epoch 41; Iter    20/   40] train: loss: 0.1110969
[Epoch 41] ogbg-molclintox: 0.800284 val loss: 0.206699
[Epoch 41] ogbg-molclintox: 0.760945 test loss: 0.334620
[Epoch 42; Iter    10/   40] train: loss: 0.1252429
[Epoch 42; Iter    40/   40] train: loss: 0.1628573
[Epoch 42] ogbg-molclintox: 0.779344 val loss: 0.157414
[Epoch 42] ogbg-molclintox: 0.834934 test loss: 0.301331
[Epoch 43; Iter    30/   40] train: loss: 0.1276718
[Epoch 43] ogbg-molclintox: 0.555802 val loss: 0.594432
[Epoch 43] ogbg-molclintox: 0.671866 test loss: 0.559650
[Epoch 44; Iter    20/   40] train: loss: 0.0625205
[Epoch 44] ogbg-molclintox: 0.653300 val loss: 0.247130
[Epoch 44] ogbg-molclintox: 0.703127 test loss: 0.313282
[Epoch 45; Iter    10/   40] train: loss: 0.1112569
[Epoch 45; Iter    40/   40] train: loss: 0.0947831
[Epoch 45] ogbg-molclintox: 0.929736 val loss: 0.099794
[Epoch 45] ogbg-molclintox: 0.843376 test loss: 0.307067
[Epoch 46; Iter    30/   40] train: loss: 0.0731908
[Epoch 46] ogbg-molclintox: 0.816168 val loss: 0.211365
[Epoch 46] ogbg-molclintox: 0.775371 test loss: 0.299003
[Epoch 47; Iter    20/   40] train: loss: 0.1442094
[Epoch 47] ogbg-molclintox: 0.772351 val loss: 0.253818
[Epoch 47] ogbg-molclintox: 0.735011 test loss: 0.610638
[Epoch 48; Iter    10/   40] train: loss: 0.0300722
[Epoch 48; Iter    40/   40] train: loss: 0.0719716
[Epoch 48] ogbg-molclintox: 0.790694 val loss: 0.603176
[Epoch 48] ogbg-molclintox: 0.823866 test loss: 0.308110
[Epoch 49; Iter    30/   40] train: loss: 0.1050536
[Epoch 49] ogbg-molclintox: 0.586670 val loss: 2.977082
[Epoch 49] ogbg-molclintox: 0.706119 test loss: 1.481593
[Epoch 50; Iter    20/   40] train: loss: 0.0331740
[Epoch 50] ogbg-molclintox: 0.790582 val loss: 0.341888
[Epoch 50] ogbg-molclintox: 0.821976 test loss: 0.414559
[Epoch 51; Iter    10/   40] train: loss: 0.0128814
[Epoch 51; Iter    40/   40] train: loss: 0.0219986
[Epoch 51] ogbg-molclintox: 0.768580 val loss: 0.668103
[Epoch 51] ogbg-molclintox: 0.834194 test loss: 0.363428
[Epoch 52; Iter    30/   40] train: loss: 0.0135008
[Epoch 52] ogbg-molclintox: 0.799697 val loss: 0.388955
[Epoch 52] ogbg-molclintox: 0.778770 test loss: 0.391371
[Epoch 53; Iter    20/   40] train: loss: 0.0458639
[Epoch 53] ogbg-molclintox: 0.794103 val loss: 1.037333
[Epoch 53] ogbg-molclintox: 0.740569 test loss: 0.378570
[Epoch 54; Iter    10/   40] train: loss: 0.0082908
[Epoch 54; Iter    40/   40] train: loss: 0.0079961
[Epoch 54] ogbg-molclintox: 0.815381 val loss: 0.184449
[Epoch 54] ogbg-molclintox: 0.776622 test loss: 0.431864
[Epoch 55; Iter    30/   40] train: loss: 0.0912623
[Epoch 55] ogbg-molclintox: 0.809424 val loss: 0.185810
[Epoch 55] ogbg-molclintox: 0.797521 test loss: 0.462633
[Epoch 56; Iter    20/   40] train: loss: 0.0115591
[Epoch 56] ogbg-molclintox: 0.610183 val loss: 0.466899
[Epoch 56] ogbg-molclintox: 0.695451 test loss: 0.426729
[Epoch 57; Iter    10/   40] train: loss: 0.0106544
[Epoch 57; Iter    40/   40] train: loss: 0.0032755
[Epoch 57] ogbg-molclintox: 0.805903 val loss: 0.169419
[Epoch 57] ogbg-molclintox: 0.834758 test loss: 0.252323
[Epoch 58; Iter    30/   40] train: loss: 0.0032419
[Epoch 58] ogbg-molclintox: 0.625254 val loss: 1.226495
[Epoch 58] ogbg-molclintox: 0.737361 test loss: 0.862900
[Epoch 59; Iter    20/   40] train: loss: 0.0048823
[Epoch 59] ogbg-molclintox: 0.803218 val loss: 0.184819
[Epoch 59] ogbg-molclintox: 0.822529 test loss: 0.282437
[Epoch 60; Iter    10/   40] train: loss: 0.0252777
[Epoch 60; Iter    40/   40] train: loss: 0.0166190
[Epoch 60] ogbg-molclintox: 0.829328 val loss: 0.187235
[Epoch 60] ogbg-molclintox: 0.736311 test loss: 0.461623
[Epoch 61; Iter    30/   40] train: loss: 0.0306266
[Epoch 61] ogbg-molclintox: 0.815567 val loss: 0.206382
[Epoch 61] ogbg-molclintox: 0.752190 test loss: 0.487180
[Epoch 62; Iter    20/   40] train: loss: 0.0286907
[Epoch 62] ogbg-molclintox: 0.583036 val loss: 1.344511
[Epoch 62] ogbg-molclintox: 0.725606 test loss: 1.069766
[Epoch 63; Iter    10/   40] train: loss: 0.0129941
[Epoch 63; Iter    40/   40] train: loss: 0.0028803
[Epoch 63] ogbg-molclintox: 0.777141 val loss: 0.268841
[Epoch 63] ogbg-molclintox: 0.803169 test loss: 0.433681
[Epoch 64; Iter    30/   40] train: loss: 0.0030199
[Epoch 64] ogbg-molclintox: 0.679934 val loss: 0.505262
[Epoch 64] ogbg-molclintox: 0.686206 test loss: 0.791491
[Epoch 65; Iter    20/   40] train: loss: 0.0080149
[Epoch 65] ogbg-molclintox: 0.731208 val loss: 0.603265
[Epoch 65] ogbg-molclintox: 0.776757 test loss: 0.471238
[Epoch 66; Iter    10/   40] train: loss: 0.0596304
[Epoch 66; Iter    40/   40] train: loss: 0.0178818
[Epoch 66] ogbg-molclintox: 0.801096 val loss: 0.235760
[Epoch 66] ogbg-molclintox: 0.824890 test loss: 0.646795
[Epoch 67; Iter    30/   40] train: loss: 0.0117278
[Epoch 67] ogbg-molclintox: 0.778033 val loss: 0.365648
[Epoch 67] ogbg-molclintox: 0.772947 test loss: 0.494412
[Epoch 68; Iter    20/   40] train: loss: 0.0087527
[Epoch 68] ogbg-molclintox: 0.726175 val loss: 0.826106
[Epoch 68] ogbg-molclintox: 0.784029 test loss: 0.456009
[Epoch 69; Iter    10/   40] train: loss: 0.0062429
[Epoch 69; Iter    40/   40] train: loss: 0.1986331
[Epoch 69] ogbg-molclintox: 0.771352 val loss: 1.634916
[Epoch 69] ogbg-molclintox: 0.820355 test loss: 0.355846
[Epoch 70; Iter    30/   40] train: loss: 0.0033878
[Epoch 70] ogbg-molclintox: 0.744930 val loss: 0.464513
[Epoch 70] ogbg-molclintox: 0.811984 test loss: 0.499582
[Epoch 71; Iter    20/   40] train: loss: 0.0181417
[Epoch 71] ogbg-molclintox: 0.769367 val loss: 1.318526
[Epoch 71] ogbg-molclintox: 0.787350 test loss: 0.446167
[Epoch 72; Iter    10/   40] train: loss: 0.0035400
[Epoch 72; Iter    40/   40] train: loss: 0.0018836
[Epoch 72] ogbg-molclintox: 0.737027 val loss: 0.624881
[Epoch 72] ogbg-molclintox: 0.746389 test loss: 0.616520
[Epoch 73; Iter    30/   40] train: loss: 0.0132511
[Epoch 73] ogbg-molclintox: 0.774399 val loss: 2.040494
[Epoch 73] ogbg-molclintox: 0.768513 test loss: 0.462069
[Epoch 74; Iter    20/   40] train: loss: 0.0070093
[Epoch 74] ogbg-molclintox: 0.771714 val loss: 0.432502
[Epoch 74] ogbg-molclintox: 0.771823 test loss: 0.523940
[Epoch 75; Iter    10/   40] train: loss: 0.0286629
[Epoch 75; Iter    40/   40] train: loss: 0.0046336
[Epoch 75] ogbg-molclintox: 0.770903 val loss: 0.446438
[Epoch 75] ogbg-molclintox: 0.778307 test loss: 0.455865
[Epoch 76; Iter    30/   40] train: loss: 0.0038799
[Epoch 31] ogbg-molclintox: 0.680722 test loss: 0.268581
[Epoch 32; Iter    20/   40] train: loss: 0.2632326
[Epoch 32] ogbg-molclintox: 0.940362 val loss: 0.136616
[Epoch 32] ogbg-molclintox: 0.672703 test loss: 0.252387
[Epoch 33; Iter    10/   40] train: loss: 0.2727732
[Epoch 33; Iter    40/   40] train: loss: 0.3404197
[Epoch 33] ogbg-molclintox: 0.956158 val loss: 0.131476
[Epoch 33] ogbg-molclintox: 0.688417 test loss: 0.241420
[Epoch 34; Iter    30/   40] train: loss: 0.1116609
[Epoch 34] ogbg-molclintox: 0.977661 val loss: 0.127711
[Epoch 34] ogbg-molclintox: 0.701259 test loss: 0.232132
[Epoch 35; Iter    20/   40] train: loss: 0.1766110
[Epoch 35] ogbg-molclintox: 0.972966 val loss: 0.119642
[Epoch 35] ogbg-molclintox: 0.686568 test loss: 0.237057
[Epoch 36; Iter    10/   40] train: loss: 0.2267622
[Epoch 36; Iter    40/   40] train: loss: 0.2998850
[Epoch 36] ogbg-molclintox: 0.902389 val loss: 0.096405
[Epoch 36] ogbg-molclintox: 0.755697 test loss: 0.212729
[Epoch 37; Iter    30/   40] train: loss: 0.0913302
[Epoch 37] ogbg-molclintox: 0.964300 val loss: 0.332491
[Epoch 37] ogbg-molclintox: 0.710590 test loss: 1.765761
[Epoch 38; Iter    20/   40] train: loss: 0.2085225
[Epoch 38] ogbg-molclintox: 0.855026 val loss: 0.126059
[Epoch 38] ogbg-molclintox: 0.738881 test loss: 0.249050
[Epoch 39; Iter    10/   40] train: loss: 0.0810009
[Epoch 39; Iter    40/   40] train: loss: 0.1995553
[Epoch 39] ogbg-molclintox: 0.850743 val loss: 0.120626
[Epoch 39] ogbg-molclintox: 0.797432 test loss: 0.600059
[Epoch 40; Iter    30/   40] train: loss: 0.1583521
[Epoch 40] ogbg-molclintox: 0.841778 val loss: 0.126886
[Epoch 40] ogbg-molclintox: 0.798731 test loss: 0.327403
[Epoch 41; Iter    20/   40] train: loss: 0.1771204
[Epoch 41] ogbg-molclintox: 0.926577 val loss: 0.260595
[Epoch 41] ogbg-molclintox: 0.811712 test loss: 2.487225
[Epoch 42; Iter    10/   40] train: loss: 0.0601121
[Epoch 42; Iter    40/   40] train: loss: 0.2222891
[Epoch 42] ogbg-molclintox: 0.809663 val loss: 9.759973
[Epoch 42] ogbg-molclintox: 0.684518 test loss: 0.916869
[Epoch 43; Iter    30/   40] train: loss: 0.2249010
[Epoch 43] ogbg-molclintox: 0.894212 val loss: 1.501815
[Epoch 43] ogbg-molclintox: 0.823328 test loss: 0.261186
[Epoch 44; Iter    20/   40] train: loss: 0.1188904
[Epoch 44] ogbg-molclintox: 0.935218 val loss: 0.190375
[Epoch 44] ogbg-molclintox: 0.848612 test loss: 0.208633
[Epoch 45; Iter    10/   40] train: loss: 0.3074189
[Epoch 45; Iter    40/   40] train: loss: 0.2066576
[Epoch 45] ogbg-molclintox: 0.918547 val loss: 0.210965
[Epoch 45] ogbg-molclintox: 0.823952 test loss: 0.224148
[Epoch 46; Iter    30/   40] train: loss: 0.1358746
[Epoch 46] ogbg-molclintox: 0.950827 val loss: 0.162850
[Epoch 46] ogbg-molclintox: 0.850622 test loss: 0.243245
[Epoch 47; Iter    20/   40] train: loss: 0.0856674
[Epoch 47] ogbg-molclintox: 0.982331 val loss: 0.084318
[Epoch 47] ogbg-molclintox: 0.838654 test loss: 0.224875
[Epoch 48; Iter    10/   40] train: loss: 0.1164264
[Epoch 48; Iter    40/   40] train: loss: 0.0571259
[Epoch 48] ogbg-molclintox: 0.805903 val loss: 0.448841
[Epoch 48] ogbg-molclintox: 0.862202 test loss: 0.196941
[Epoch 49; Iter    30/   40] train: loss: 0.1102885
[Epoch 49] ogbg-molclintox: 0.936817 val loss: 0.097225
[Epoch 49] ogbg-molclintox: 0.808511 test loss: 0.195033
[Epoch 50; Iter    20/   40] train: loss: 0.0698957
[Epoch 50] ogbg-molclintox: 0.973353 val loss: 0.071161
[Epoch 50] ogbg-molclintox: 0.872395 test loss: 0.209072
[Epoch 51; Iter    10/   40] train: loss: 0.1791572
[Epoch 51; Iter    40/   40] train: loss: 0.5268394
[Epoch 51] ogbg-molclintox: 0.855350 val loss: 0.152041
[Epoch 51] ogbg-molclintox: 0.845524 test loss: 0.205479
[Epoch 52; Iter    30/   40] train: loss: 0.1922097
[Epoch 52] ogbg-molclintox: 0.946807 val loss: 0.118942
[Epoch 52] ogbg-molclintox: 0.798817 test loss: 0.287475
[Epoch 53; Iter    20/   40] train: loss: 0.0603572
[Epoch 53] ogbg-molclintox: 0.982918 val loss: 0.068573
[Epoch 53] ogbg-molclintox: 0.794156 test loss: 0.298910
[Epoch 54; Iter    10/   40] train: loss: 0.1527269
[Epoch 54; Iter    40/   40] train: loss: 0.0779223
[Epoch 54] ogbg-molclintox: 0.973778 val loss: 0.076880
[Epoch 54] ogbg-molclintox: 0.777153 test loss: 0.249971
[Epoch 55; Iter    30/   40] train: loss: 0.1682418
[Epoch 55] ogbg-molclintox: 0.945981 val loss: 0.105403
[Epoch 55] ogbg-molclintox: 0.826982 test loss: 0.198087
[Epoch 56; Iter    20/   40] train: loss: 0.0321511
[Epoch 56] ogbg-molclintox: 0.944857 val loss: 0.086320
[Epoch 56] ogbg-molclintox: 0.786633 test loss: 0.271643
[Epoch 57; Iter    10/   40] train: loss: 0.0615201
[Epoch 57; Iter    40/   40] train: loss: 0.0428015
[Epoch 57] ogbg-molclintox: 0.985740 val loss: 0.060348
[Epoch 57] ogbg-molclintox: 0.835950 test loss: 0.333792
[Epoch 58; Iter    30/   40] train: loss: 0.0923463
[Epoch 58] ogbg-molclintox: 0.979534 val loss: 0.101494
[Epoch 58] ogbg-molclintox: 0.843301 test loss: 0.368909
[Epoch 59; Iter    20/   40] train: loss: 0.0247249
[Epoch 59] ogbg-molclintox: 0.959230 val loss: 0.118220
[Epoch 59] ogbg-molclintox: 0.831517 test loss: 0.329268
[Epoch 60; Iter    10/   40] train: loss: 0.0186182
[Epoch 60; Iter    40/   40] train: loss: 0.1346687
[Epoch 60] ogbg-molclintox: 0.980233 val loss: 0.059704
[Epoch 60] ogbg-molclintox: 0.792034 test loss: 0.408540
[Epoch 61; Iter    30/   40] train: loss: 0.0529250
[Epoch 61] ogbg-molclintox: 0.957371 val loss: 0.090186
[Epoch 61] ogbg-molclintox: 0.805885 test loss: 0.250608
[Epoch 62; Iter    20/   40] train: loss: 0.0709205
[Epoch 62] ogbg-molclintox: 0.979010 val loss: 0.077655
[Epoch 62] ogbg-molclintox: 0.780578 test loss: 0.467731
[Epoch 63; Iter    10/   40] train: loss: 0.1045013
[Epoch 63; Iter    40/   40] train: loss: 0.0297439
[Epoch 63] ogbg-molclintox: 0.949829 val loss: 0.066183
[Epoch 63] ogbg-molclintox: 0.786110 test loss: 0.330077
[Epoch 64; Iter    30/   40] train: loss: 0.0469352
[Epoch 64] ogbg-molclintox: 0.984542 val loss: 0.183694
[Epoch 64] ogbg-molclintox: 0.800936 test loss: 0.501457
[Epoch 65; Iter    20/   40] train: loss: 0.0107583
[Epoch 65] ogbg-molclintox: 0.945159 val loss: 0.086846
[Epoch 65] ogbg-molclintox: 0.764573 test loss: 0.374320
[Epoch 66; Iter    10/   40] train: loss: 0.0628242
[Epoch 66; Iter    40/   40] train: loss: 0.0316400
[Epoch 66] ogbg-molclintox: 0.977187 val loss: 0.119258
[Epoch 66] ogbg-molclintox: 0.805033 test loss: 0.392651
[Epoch 67; Iter    30/   40] train: loss: 0.0109586
[Epoch 67] ogbg-molclintox: 0.956671 val loss: 0.123533
[Epoch 67] ogbg-molclintox: 0.827221 test loss: 0.408822
[Epoch 68; Iter    20/   40] train: loss: 0.0291972
[Epoch 68] ogbg-molclintox: 0.963626 val loss: 0.074546
[Epoch 68] ogbg-molclintox: 0.817550 test loss: 0.367359
[Epoch 69; Iter    10/   40] train: loss: 0.0179559
[Epoch 69; Iter    40/   40] train: loss: 0.0126922
[Epoch 69] ogbg-molclintox: 0.966535 val loss: 0.108495
[Epoch 69] ogbg-molclintox: 0.822313 test loss: 0.384118
[Epoch 70; Iter    30/   40] train: loss: 0.1075990
[Epoch 70] ogbg-molclintox: 0.926102 val loss: 1.061693
[Epoch 70] ogbg-molclintox: 0.718681 test loss: 1.491927
[Epoch 71; Iter    20/   40] train: loss: 0.2610959
[Epoch 71] ogbg-molclintox: 0.818638 val loss: 0.131439
[Epoch 71] ogbg-molclintox: 0.816317 test loss: 0.252724
[Epoch 72; Iter    10/   40] train: loss: 0.0440530
[Epoch 72; Iter    40/   40] train: loss: 0.1000231
[Epoch 72] ogbg-molclintox: 0.947854 val loss: 0.083270
[Epoch 72] ogbg-molclintox: 0.799479 test loss: 0.184174
[Epoch 73; Iter    30/   40] train: loss: 0.0962396
[Epoch 73] ogbg-molclintox: 0.907833 val loss: 0.093826
[Epoch 73] ogbg-molclintox: 0.840679 test loss: 0.210545
[Epoch 74; Iter    20/   40] train: loss: 0.0509835
[Epoch 74] ogbg-molclintox: 0.911891 val loss: 0.117886
[Epoch 74] ogbg-molclintox: 0.817565 test loss: 0.316411
[Epoch 75; Iter    10/   40] train: loss: 0.0738948
[Epoch 75; Iter    40/   40] train: loss: 0.0213643
[Epoch 75] ogbg-molclintox: 0.943834 val loss: 0.085074
[Epoch 75] ogbg-molclintox: 0.822552 test loss: 0.224877
[Epoch 76; Iter    30/   40] train: loss: 0.0699647
[Epoch 31] ogbg-molclintox: 0.714187 test loss: 0.275961
[Epoch 32; Iter    20/   40] train: loss: 0.2897904
[Epoch 32] ogbg-molclintox: 0.752359 val loss: 0.181381
[Epoch 32] ogbg-molclintox: 0.715737 test loss: 0.280630
[Epoch 33; Iter    10/   40] train: loss: 0.2485880
[Epoch 33; Iter    40/   40] train: loss: 0.3400130
[Epoch 33] ogbg-molclintox: 0.897146 val loss: 0.205451
[Epoch 33] ogbg-molclintox: 0.660470 test loss: 0.307725
[Epoch 34; Iter    30/   40] train: loss: 0.1437561
[Epoch 34] ogbg-molclintox: 0.881888 val loss: 0.149407
[Epoch 34] ogbg-molclintox: 0.677335 test loss: 0.246587
[Epoch 35; Iter    20/   40] train: loss: 0.1955422
[Epoch 35] ogbg-molclintox: 0.902315 val loss: 0.201186
[Epoch 35] ogbg-molclintox: 0.738459 test loss: 0.454186
[Epoch 36; Iter    10/   40] train: loss: 0.2809300
[Epoch 36; Iter    40/   40] train: loss: 0.2473981
[Epoch 36] ogbg-molclintox: 0.854239 val loss: 0.144112
[Epoch 36] ogbg-molclintox: 0.763381 test loss: 0.389662
[Epoch 37; Iter    30/   40] train: loss: 0.0874713
[Epoch 37] ogbg-molclintox: 0.923168 val loss: 0.242363
[Epoch 37] ogbg-molclintox: 0.742844 test loss: 0.883705
[Epoch 38; Iter    20/   40] train: loss: 0.1549223
[Epoch 38] ogbg-molclintox: 0.688627 val loss: 0.316960
[Epoch 38] ogbg-molclintox: 0.676857 test loss: 0.278249
[Epoch 39; Iter    10/   40] train: loss: 0.0987601
[Epoch 39; Iter    40/   40] train: loss: 0.0713869
[Epoch 39] ogbg-molclintox: 0.818476 val loss: 0.146401
[Epoch 39] ogbg-molclintox: 0.611354 test loss: 0.290972
[Epoch 40; Iter    30/   40] train: loss: 0.2735807
[Epoch 40] ogbg-molclintox: 0.750588 val loss: 1.197991
[Epoch 40] ogbg-molclintox: 0.696445 test loss: 0.507234
[Epoch 41; Iter    20/   40] train: loss: 0.1577207
[Epoch 41] ogbg-molclintox: 0.744818 val loss: 0.392577
[Epoch 41] ogbg-molclintox: 0.785202 test loss: 1.044868
[Epoch 42; Iter    10/   40] train: loss: 0.0779675
[Epoch 42; Iter    40/   40] train: loss: 0.0698881
[Epoch 42] ogbg-molclintox: 0.769416 val loss: 0.754887
[Epoch 42] ogbg-molclintox: 0.743868 test loss: 5.447759
[Epoch 43; Iter    30/   40] train: loss: 0.0875404
[Epoch 43] ogbg-molclintox: 0.668073 val loss: 0.994410
[Epoch 43] ogbg-molclintox: 0.706179 test loss: 0.813424
[Epoch 44; Iter    20/   40] train: loss: 0.0283034
[Epoch 44] ogbg-molclintox: 0.766394 val loss: 1.399308
[Epoch 44] ogbg-molclintox: 0.807476 test loss: 0.958942
[Epoch 45; Iter    10/   40] train: loss: 0.0911690
[Epoch 45; Iter    40/   40] train: loss: 0.0345143
[Epoch 45] ogbg-molclintox: 0.804729 val loss: 0.358793
[Epoch 45] ogbg-molclintox: 0.816582 test loss: 1.522755
[Epoch 46; Iter    30/   40] train: loss: 0.1019623
[Epoch 46] ogbg-molclintox: 0.917500 val loss: 0.180192
[Epoch 46] ogbg-molclintox: 0.738810 test loss: 0.738552
[Epoch 47; Iter    20/   40] train: loss: 0.1335746
[Epoch 47] ogbg-molclintox: 0.966697 val loss: 0.089456
[Epoch 47] ogbg-molclintox: 0.764415 test loss: 0.340889
[Epoch 48; Iter    10/   40] train: loss: 0.0564959
[Epoch 48; Iter    40/   40] train: loss: 0.0901248
[Epoch 48] ogbg-molclintox: 0.788596 val loss: 2.326553
[Epoch 48] ogbg-molclintox: 0.710403 test loss: 7.356296
[Epoch 49; Iter    30/   40] train: loss: 0.1333718
[Epoch 49] ogbg-molclintox: 0.792293 val loss: 0.724550
[Epoch 49] ogbg-molclintox: 0.696362 test loss: 2.636006
[Epoch 50; Iter    20/   40] train: loss: 0.0974213
[Epoch 50] ogbg-molclintox: 0.951601 val loss: 0.330410
[Epoch 50] ogbg-molclintox: 0.790951 test loss: 0.915278
[Epoch 51; Iter    10/   40] train: loss: 0.0432447
[Epoch 51; Iter    40/   40] train: loss: 0.1909411
[Epoch 51] ogbg-molclintox: 0.937140 val loss: 0.174645
[Epoch 51] ogbg-molclintox: 0.787765 test loss: 0.793507
[Epoch 52; Iter    30/   40] train: loss: 0.0107691
[Epoch 52] ogbg-molclintox: 0.930347 val loss: 0.739384
[Epoch 52] ogbg-molclintox: 0.746691 test loss: 2.983298
[Epoch 53; Iter    20/   40] train: loss: 0.0468609
[Epoch 53] ogbg-molclintox: 0.908335 val loss: 0.326046
[Epoch 53] ogbg-molclintox: 0.590272 test loss: 1.381771
[Epoch 54; Iter    10/   40] train: loss: 0.0413369
[Epoch 54; Iter    40/   40] train: loss: 0.0457650
[Epoch 54] ogbg-molclintox: 0.937316 val loss: 0.352902
[Epoch 54] ogbg-molclintox: 0.793162 test loss: 0.477556
[Epoch 55; Iter    30/   40] train: loss: 0.0058739
[Epoch 55] ogbg-molclintox: 0.796201 val loss: 0.395256
[Epoch 55] ogbg-molclintox: 0.769114 test loss: 0.698087
[Epoch 56; Iter    20/   40] train: loss: 0.0126820
[Epoch 56] ogbg-molclintox: 0.806065 val loss: 0.197572
[Epoch 56] ogbg-molclintox: 0.783443 test loss: 0.442471
[Epoch 57; Iter    10/   40] train: loss: 0.0198749
[Epoch 57; Iter    40/   40] train: loss: 0.0181913
[Epoch 57] ogbg-molclintox: 0.787872 val loss: 0.784527
[Epoch 57] ogbg-molclintox: 0.741933 test loss: 1.964149
[Epoch 58; Iter    30/   40] train: loss: 0.0440987
[Epoch 58] ogbg-molclintox: 0.809062 val loss: 0.328806
[Epoch 58] ogbg-molclintox: 0.764079 test loss: 0.506331
[Epoch 59; Iter    20/   40] train: loss: 0.0038587
[Epoch 59] ogbg-molclintox: 0.933893 val loss: 0.420027
[Epoch 59] ogbg-molclintox: 0.776260 test loss: 1.142813
[Epoch 60; Iter    10/   40] train: loss: 0.0246807
[Epoch 60; Iter    40/   40] train: loss: 0.0420763
[Epoch 60] ogbg-molclintox: 0.791256 val loss: 0.941284
[Epoch 60] ogbg-molclintox: 0.733962 test loss: 1.686005
[Epoch 61; Iter    30/   40] train: loss: 0.0197881
[Epoch 61] ogbg-molclintox: 0.761239 val loss: 3.446551
[Epoch 61] ogbg-molclintox: 0.775584 test loss: 1.898475
[Epoch 62; Iter    20/   40] train: loss: 0.0728529
[Epoch 62] ogbg-molclintox: 0.913540 val loss: 0.458578
[Epoch 62] ogbg-molclintox: 0.740046 test loss: 1.136600
[Epoch 63; Iter    10/   40] train: loss: 0.1413462
[Epoch 63; Iter    40/   40] train: loss: 0.0270332
[Epoch 63] ogbg-molclintox: 0.763386 val loss: 1.598106
[Epoch 63] ogbg-molclintox: 0.757882 test loss: 1.230272
[Epoch 64; Iter    30/   40] train: loss: 0.1120653
[Epoch 64] ogbg-molclintox: 0.972130 val loss: 0.236095
[Epoch 64] ogbg-molclintox: 0.790951 test loss: 0.571328
[Epoch 65; Iter    20/   40] train: loss: 0.0787568
[Epoch 65] ogbg-molclintox: 0.915328 val loss: 0.719651
[Epoch 65] ogbg-molclintox: 0.760841 test loss: 0.610251
[Epoch 66; Iter    10/   40] train: loss: 0.0232758
[Epoch 66; Iter    40/   40] train: loss: 0.0027965
[Epoch 66] ogbg-molclintox: 0.940587 val loss: 0.471814
[Epoch 66] ogbg-molclintox: 0.806788 test loss: 0.821414
[Epoch 67; Iter    30/   40] train: loss: 0.0097337
[Epoch 67] ogbg-molclintox: 0.932146 val loss: 0.530285
[Epoch 67] ogbg-molclintox: 0.807177 test loss: 0.624460
[Epoch 68; Iter    20/   40] train: loss: 0.0161706
[Epoch 68] ogbg-molclintox: 0.955933 val loss: 0.217122
[Epoch 68] ogbg-molclintox: 0.829962 test loss: 0.483273
[Epoch 69; Iter    10/   40] train: loss: 0.0026813
[Epoch 69; Iter    40/   40] train: loss: 0.0399373
[Epoch 69] ogbg-molclintox: 0.953762 val loss: 0.415912
[Epoch 69] ogbg-molclintox: 0.804364 test loss: 0.551321
[Epoch 70; Iter    30/   40] train: loss: 0.0779215
[Epoch 70] ogbg-molclintox: 0.945282 val loss: 0.265783
[Epoch 70] ogbg-molclintox: 0.796946 test loss: 0.407888
[Epoch 71; Iter    20/   40] train: loss: 0.0267164
[Epoch 71] ogbg-molclintox: 0.957508 val loss: 0.490629
[Epoch 71] ogbg-molclintox: 0.802142 test loss: 0.650442
[Epoch 72; Iter    10/   40] train: loss: 0.0087745
[Epoch 72; Iter    40/   40] train: loss: 0.0074790
[Epoch 72] ogbg-molclintox: 0.945433 val loss: 0.664422
[Epoch 72] ogbg-molclintox: 0.802478 test loss: 0.771865
[Epoch 73; Iter    30/   40] train: loss: 0.0248521
[Epoch 73] ogbg-molclintox: 0.944421 val loss: 0.420780
[Epoch 73] ogbg-molclintox: 0.806139 test loss: 0.498196
[Epoch 74; Iter    20/   40] train: loss: 0.0032640
[Epoch 74] ogbg-molclintox: 0.938665 val loss: 0.506706
[Epoch 74] ogbg-molclintox: 0.808787 test loss: 0.557112
[Epoch 75; Iter    10/   40] train: loss: 0.0038325
[Epoch 75; Iter    40/   40] train: loss: 0.0063409
[Epoch 75] ogbg-molclintox: 0.946245 val loss: 0.460048
[Epoch 75] ogbg-molclintox: 0.807775 test loss: 0.476399
[Epoch 76; Iter    30/   40] train: loss: 0.0056731
[Epoch 31] ogbg-molclintox: 0.717459 test loss: 0.226155
[Epoch 32; Iter    20/   40] train: loss: 0.2012181
[Epoch 32] ogbg-molclintox: 0.908985 val loss: 0.139126
[Epoch 32] ogbg-molclintox: 0.674500 test loss: 0.222761
[Epoch 33; Iter    10/   40] train: loss: 0.0827929
[Epoch 33; Iter    40/   40] train: loss: 0.3427804
[Epoch 33] ogbg-molclintox: 0.808813 val loss: 0.135855
[Epoch 33] ogbg-molclintox: 0.718710 test loss: 0.230706
[Epoch 34; Iter    30/   40] train: loss: 0.0858183
[Epoch 34] ogbg-molclintox: 0.925354 val loss: 0.218526
[Epoch 34] ogbg-molclintox: 0.728366 test loss: 0.299806
[Epoch 35; Iter    20/   40] train: loss: 0.2608247
[Epoch 35] ogbg-molclintox: 0.954348 val loss: 0.123257
[Epoch 35] ogbg-molclintox: 0.726330 test loss: 0.234562
[Epoch 36; Iter    10/   40] train: loss: 0.1842496
[Epoch 36; Iter    40/   40] train: loss: 0.2298567
[Epoch 36] ogbg-molclintox: 0.780967 val loss: 0.186326
[Epoch 36] ogbg-molclintox: 0.712248 test loss: 0.228042
[Epoch 37; Iter    30/   40] train: loss: 0.2352931
[Epoch 37] ogbg-molclintox: 0.978497 val loss: 0.154349
[Epoch 37] ogbg-molclintox: 0.832495 test loss: 0.424266
[Epoch 38; Iter    20/   40] train: loss: 0.0613179
[Epoch 38] ogbg-molclintox: 0.984928 val loss: 0.099123
[Epoch 38] ogbg-molclintox: 0.784403 test loss: 0.358231
[Epoch 39; Iter    10/   40] train: loss: 0.0745269
[Epoch 39; Iter    40/   40] train: loss: 0.1108242
[Epoch 39] ogbg-molclintox: 0.965861 val loss: 1.142807
[Epoch 39] ogbg-molclintox: 0.750912 test loss: 2.630740
[Epoch 40; Iter    30/   40] train: loss: 0.1112994
[Epoch 40] ogbg-molclintox: 0.942822 val loss: 0.129565
[Epoch 40] ogbg-molclintox: 0.804316 test loss: 0.571838
[Epoch 41; Iter    20/   40] train: loss: 0.0836585
[Epoch 41] ogbg-molclintox: 0.874930 val loss: 0.164109
[Epoch 41] ogbg-molclintox: 0.823463 test loss: 0.335577
[Epoch 42; Iter    10/   40] train: loss: 0.0655212
[Epoch 42; Iter    40/   40] train: loss: 0.0652432
[Epoch 42] ogbg-molclintox: 0.757531 val loss: 0.182914
[Epoch 42] ogbg-molclintox: 0.777496 test loss: 4.309586
[Epoch 43; Iter    30/   40] train: loss: 0.0382343
[Epoch 43] ogbg-molclintox: 0.804504 val loss: 0.313820
[Epoch 43] ogbg-molclintox: 0.838344 test loss: 0.896644
[Epoch 44; Iter    20/   40] train: loss: 0.1780822
[Epoch 44] ogbg-molclintox: 0.945869 val loss: 0.109067
[Epoch 44] ogbg-molclintox: 0.758607 test loss: 0.336140
[Epoch 45; Iter    10/   40] train: loss: 0.1748309
[Epoch 45; Iter    40/   40] train: loss: 0.0886528
[Epoch 45] ogbg-molclintox: 0.983168 val loss: 0.073908
[Epoch 45] ogbg-molclintox: 0.819231 test loss: 0.306980
[Epoch 46; Iter    30/   40] train: loss: 0.1103244
[Epoch 46] ogbg-molclintox: 0.916565 val loss: 0.095293
[Epoch 46] ogbg-molclintox: 0.785015 test loss: 1.042898
[Epoch 47; Iter    20/   40] train: loss: 0.0799948
[Epoch 47] ogbg-molclintox: 0.918536 val loss: 0.117971
[Epoch 47] ogbg-molclintox: 0.872855 test loss: 1.099738
[Epoch 48; Iter    10/   40] train: loss: 0.0517800
[Epoch 48; Iter    40/   40] train: loss: 0.0784736
[Epoch 48] ogbg-molclintox: 0.967572 val loss: 0.071111
[Epoch 48] ogbg-molclintox: 0.832921 test loss: 4.645679
[Epoch 49; Iter    30/   40] train: loss: 0.0693008
[Epoch 49] ogbg-molclintox: 0.825048 val loss: 0.642913
[Epoch 49] ogbg-molclintox: 0.700494 test loss: 0.946593
[Epoch 50; Iter    20/   40] train: loss: 0.1306299
[Epoch 50] ogbg-molclintox: 0.908795 val loss: 0.113635
[Epoch 50] ogbg-molclintox: 0.816870 test loss: 1.026566
[Epoch 51; Iter    10/   40] train: loss: 0.0303114
[Epoch 51; Iter    40/   40] train: loss: 0.0105729
[Epoch 51] ogbg-molclintox: 0.981045 val loss: 0.061859
[Epoch 51] ogbg-molclintox: 0.844100 test loss: 0.333486
[Epoch 52; Iter    30/   40] train: loss: 0.0605168
[Epoch 52] ogbg-molclintox: 0.944597 val loss: 0.119821
[Epoch 52] ogbg-molclintox: 0.770672 test loss: 1.168159
[Epoch 53; Iter    20/   40] train: loss: 0.0775414
[Epoch 53] ogbg-molclintox: 0.967684 val loss: 0.073712
[Epoch 53] ogbg-molclintox: 0.834344 test loss: 0.453810
[Epoch 54; Iter    10/   40] train: loss: 0.0182081
[Epoch 54; Iter    40/   40] train: loss: 0.0107486
[Epoch 54] ogbg-molclintox: 0.903977 val loss: 0.095397
[Epoch 54] ogbg-molclintox: 0.770848 test loss: 0.253838
[Epoch 55; Iter    30/   40] train: loss: 0.0513015
[Epoch 55] ogbg-molclintox: 0.962090 val loss: 0.081639
[Epoch 55] ogbg-molclintox: 0.803502 test loss: 0.350825
[Epoch 56; Iter    20/   40] train: loss: 0.0166628
[Epoch 56] ogbg-molclintox: 0.957806 val loss: 0.110762
[Epoch 56] ogbg-molclintox: 0.798918 test loss: 0.255663
[Epoch 57; Iter    10/   40] train: loss: 0.0072345
[Epoch 57; Iter    40/   40] train: loss: 0.0054363
[Epoch 57] ogbg-molclintox: 0.976962 val loss: 0.087254
[Epoch 57] ogbg-molclintox: 0.827747 test loss: 0.241916
[Epoch 58; Iter    30/   40] train: loss: 0.0173490
[Epoch 58] ogbg-molclintox: 0.915964 val loss: 0.145614
[Epoch 58] ogbg-molclintox: 0.749560 test loss: 0.686367
[Epoch 59; Iter    20/   40] train: loss: 0.0378810
[Epoch 59] ogbg-molclintox: 0.963938 val loss: 0.214463
[Epoch 59] ogbg-molclintox: 0.796621 test loss: 0.995983
[Epoch 60; Iter    10/   40] train: loss: 0.0608698
[Epoch 60; Iter    40/   40] train: loss: 0.1673216
[Epoch 60] ogbg-molclintox: 0.977837 val loss: 0.086439
[Epoch 60] ogbg-molclintox: 0.805563 test loss: 0.391234
[Epoch 61; Iter    30/   40] train: loss: 0.0355082
[Epoch 61] ogbg-molclintox: 0.910795 val loss: 0.112831
[Epoch 61] ogbg-molclintox: 0.842950 test loss: 0.287001
[Epoch 62; Iter    20/   40] train: loss: 0.0702421
[Epoch 62] ogbg-molclintox: 0.959268 val loss: 0.201119
[Epoch 62] ogbg-molclintox: 0.833059 test loss: 0.541528
[Epoch 63; Iter    10/   40] train: loss: 0.0949092
[Epoch 63; Iter    40/   40] train: loss: 0.0027137
[Epoch 63] ogbg-molclintox: 0.937491 val loss: 0.112292
[Epoch 63] ogbg-molclintox: 0.816093 test loss: 0.286679
[Epoch 64; Iter    30/   40] train: loss: 0.0091954
[Epoch 64] ogbg-molclintox: 0.881238 val loss: 0.237071
[Epoch 64] ogbg-molclintox: 0.801402 test loss: 0.326799
[Epoch 65; Iter    20/   40] train: loss: 0.0438062
[Epoch 65] ogbg-molclintox: 0.954710 val loss: 0.228232
[Epoch 65] ogbg-molclintox: 0.839980 test loss: 0.417911
[Epoch 66; Iter    10/   40] train: loss: 0.0305888
[Epoch 66; Iter    40/   40] train: loss: 0.0314605
[Epoch 66] ogbg-molclintox: 0.982444 val loss: 0.164827
[Epoch 66] ogbg-molclintox: 0.849035 test loss: 0.574550
[Epoch 67; Iter    30/   40] train: loss: 0.0527997
[Epoch 67] ogbg-molclintox: 0.984679 val loss: 0.104307
[Epoch 67] ogbg-molclintox: 0.847585 test loss: 0.494829
[Epoch 68; Iter    20/   40] train: loss: 0.0152090
[Epoch 68] ogbg-molclintox: 0.982106 val loss: 0.095553
[Epoch 68] ogbg-molclintox: 0.840966 test loss: 0.422073
[Epoch 69; Iter    10/   40] train: loss: 0.0153775
[Epoch 69; Iter    40/   40] train: loss: 0.0281436
[Epoch 69] ogbg-molclintox: 0.975338 val loss: 0.112509
[Epoch 69] ogbg-molclintox: 0.860954 test loss: 0.321000
[Epoch 70; Iter    30/   40] train: loss: 0.0037394
[Epoch 70] ogbg-molclintox: 0.982219 val loss: 0.088398
[Epoch 70] ogbg-molclintox: 0.862702 test loss: 0.398033
[Epoch 71; Iter    20/   40] train: loss: 0.0222849
[Epoch 71] ogbg-molclintox: 0.972154 val loss: 0.136384
[Epoch 71] ogbg-molclintox: 0.834870 test loss: 0.919454
[Epoch 72; Iter    10/   40] train: loss: 0.0034383
[Epoch 72; Iter    40/   40] train: loss: 0.0035294
[Epoch 72] ogbg-molclintox: 0.973216 val loss: 0.069202
[Epoch 72] ogbg-molclintox: 0.815234 test loss: 0.321047
[Epoch 73; Iter    30/   40] train: loss: 0.0218853
[Epoch 73] ogbg-molclintox: 0.970756 val loss: 0.079709
[Epoch 73] ogbg-molclintox: 0.831710 test loss: 0.320223
[Epoch 74; Iter    20/   40] train: loss: 0.0079060
[Epoch 74] ogbg-molclintox: 0.974027 val loss: 0.091170
[Epoch 74] ogbg-molclintox: 0.829062 test loss: 0.383286
[Epoch 75; Iter    10/   40] train: loss: 0.0247648
[Epoch 75; Iter    40/   40] train: loss: 0.0255812
[Epoch 75] ogbg-molclintox: 0.973915 val loss: 0.076075
[Epoch 75] ogbg-molclintox: 0.832872 test loss: 0.325148
[Epoch 76; Iter    30/   40] train: loss: 0.0041790
[Epoch 31] ogbg-molclintox: 0.865787 test loss: 0.200150
[Epoch 32; Iter    20/   40] train: loss: 0.2297032
[Epoch 32] ogbg-molclintox: 0.975563 val loss: 0.130555
[Epoch 32] ogbg-molclintox: 0.869411 test loss: 0.227947
[Epoch 33; Iter    10/   40] train: loss: 0.3050676
[Epoch 33; Iter    40/   40] train: loss: 0.3276593
[Epoch 33] ogbg-molclintox: 0.881087 val loss: 0.141401
[Epoch 33] ogbg-molclintox: 0.808974 test loss: 0.208196
[Epoch 34; Iter    30/   40] train: loss: 0.1163705
[Epoch 34] ogbg-molclintox: 0.957806 val loss: 0.101790
[Epoch 34] ogbg-molclintox: 0.856819 test loss: 0.411761
[Epoch 35; Iter    20/   40] train: loss: 0.1654755
[Epoch 35] ogbg-molclintox: 0.921471 val loss: 0.152293
[Epoch 35] ogbg-molclintox: 0.848698 test loss: 0.213458
[Epoch 36; Iter    10/   40] train: loss: 0.2245532
[Epoch 36; Iter    40/   40] train: loss: 0.2917740
[Epoch 36] ogbg-molclintox: 0.810960 val loss: 0.173590
[Epoch 36] ogbg-molclintox: 0.748795 test loss: 0.514338
[Epoch 37; Iter    30/   40] train: loss: 0.1252732
[Epoch 37] ogbg-molclintox: 0.890364 val loss: 0.229176
[Epoch 37] ogbg-molclintox: 0.827751 test loss: 0.576738
[Epoch 38; Iter    20/   40] train: loss: 0.2409561
[Epoch 38] ogbg-molclintox: 0.869287 val loss: 0.115614
[Epoch 38] ogbg-molclintox: 0.817692 test loss: 0.203968
[Epoch 39; Iter    10/   40] train: loss: 0.0894077
[Epoch 39; Iter    40/   40] train: loss: 0.1694465
[Epoch 39] ogbg-molclintox: 0.842664 val loss: 0.340921
[Epoch 39] ogbg-molclintox: 0.854794 test loss: 1.893434
[Epoch 40; Iter    30/   40] train: loss: 0.2009077
[Epoch 40] ogbg-molclintox: 0.942123 val loss: 0.293059
[Epoch 40] ogbg-molclintox: 0.715360 test loss: 0.441583
[Epoch 41; Iter    20/   40] train: loss: 0.0587601
[Epoch 41] ogbg-molclintox: 0.954872 val loss: 0.142799
[Epoch 41] ogbg-molclintox: 0.922452 test loss: 0.171771
[Epoch 42; Iter    10/   40] train: loss: 0.0754601
[Epoch 42; Iter    40/   40] train: loss: 0.0803262
[Epoch 42] ogbg-molclintox: 0.940861 val loss: 0.086686
[Epoch 42] ogbg-molclintox: 0.811932 test loss: 0.179531
[Epoch 43; Iter    30/   40] train: loss: 0.2321632
[Epoch 43] ogbg-molclintox: 0.902839 val loss: 0.107430
[Epoch 43] ogbg-molclintox: 0.824651 test loss: 0.198411
[Epoch 44; Iter    20/   40] train: loss: 0.0895336
[Epoch 44] ogbg-molclintox: 0.795276 val loss: 0.206845
[Epoch 44] ogbg-molclintox: 0.778554 test loss: 0.679918
[Epoch 45; Iter    10/   40] train: loss: 0.2626065
[Epoch 45; Iter    40/   40] train: loss: 0.1635024
[Epoch 45] ogbg-molclintox: 0.989261 val loss: 0.074935
[Epoch 45] ogbg-molclintox: 0.889906 test loss: 0.843870
[Epoch 46; Iter    30/   40] train: loss: 0.1610825
[Epoch 46] ogbg-molclintox: 0.997902 val loss: 0.071707
[Epoch 46] ogbg-molclintox: 0.812907 test loss: 0.203972
[Epoch 47; Iter    20/   40] train: loss: 0.0610085
[Epoch 47] ogbg-molclintox: 0.999301 val loss: 0.055721
[Epoch 47] ogbg-molclintox: 0.850745 test loss: 0.201276
[Epoch 48; Iter    10/   40] train: loss: 0.0670668
[Epoch 48; Iter    40/   40] train: loss: 0.1273030
[Epoch 48] ogbg-molclintox: 0.993432 val loss: 0.079849
[Epoch 48] ogbg-molclintox: 0.880041 test loss: 0.208108
[Epoch 49; Iter    30/   40] train: loss: 0.1741082
[Epoch 49] ogbg-molclintox: 0.806964 val loss: 0.116741
[Epoch 49] ogbg-molclintox: 0.712420 test loss: 0.228388
[Epoch 50; Iter    20/   40] train: loss: 0.0570102
[Epoch 50] ogbg-molclintox: 0.957943 val loss: 0.075041
[Epoch 50] ogbg-molclintox: 0.920502 test loss: 0.166380
[Epoch 51; Iter    10/   40] train: loss: 0.1879561
[Epoch 51; Iter    40/   40] train: loss: 0.6038517
[Epoch 51] ogbg-molclintox: 0.985290 val loss: 0.070391
[Epoch 51] ogbg-molclintox: 0.858791 test loss: 0.173477
[Epoch 52; Iter    30/   40] train: loss: 0.1979793
[Epoch 52] ogbg-molclintox: 0.992533 val loss: 0.087729
[Epoch 52] ogbg-molclintox: 0.786913 test loss: 0.203955
[Epoch 53; Iter    20/   40] train: loss: 0.0694837
[Epoch 53] ogbg-molclintox: 0.975813 val loss: 0.086420
[Epoch 53] ogbg-molclintox: 0.850970 test loss: 0.242570
[Epoch 54; Iter    10/   40] train: loss: 0.1213567
[Epoch 54; Iter    40/   40] train: loss: 0.0906530
[Epoch 54] ogbg-molclintox: 0.994855 val loss: 0.054326
[Epoch 54] ogbg-molclintox: 0.872870 test loss: 0.375097
[Epoch 55; Iter    30/   40] train: loss: 0.1067288
[Epoch 55] ogbg-molclintox: 0.942622 val loss: 0.092871
[Epoch 55] ogbg-molclintox: 0.890893 test loss: 0.195430
[Epoch 56; Iter    20/   40] train: loss: 0.0528772
[Epoch 56] ogbg-molclintox: 0.971367 val loss: 0.079831
[Epoch 56] ogbg-molclintox: 0.772081 test loss: 0.196100
[Epoch 57; Iter    10/   40] train: loss: 0.0840015
[Epoch 57; Iter    40/   40] train: loss: 0.1770222
[Epoch 57] ogbg-molclintox: 0.872383 val loss: 0.096909
[Epoch 57] ogbg-molclintox: 0.830097 test loss: 0.183003
[Epoch 58; Iter    30/   40] train: loss: 0.0608071
[Epoch 58] ogbg-molclintox: 0.991584 val loss: 0.068297
[Epoch 58] ogbg-molclintox: 0.876142 test loss: 0.170299
[Epoch 59; Iter    20/   40] train: loss: 0.0200942
[Epoch 59] ogbg-molclintox: 0.984791 val loss: 0.067508
[Epoch 59] ogbg-molclintox: 0.845273 test loss: 0.175755
[Epoch 60; Iter    10/   40] train: loss: 0.0498676
[Epoch 60; Iter    40/   40] train: loss: 0.0282621
[Epoch 60] ogbg-molclintox: 0.957831 val loss: 0.070193
[Epoch 60] ogbg-molclintox: 0.848635 test loss: 0.359914
[Epoch 61; Iter    30/   40] train: loss: 0.1002248
[Epoch 61] ogbg-molclintox: 0.921432 val loss: 0.093303
[Epoch 61] ogbg-molclintox: 0.802751 test loss: 0.220708
[Epoch 62; Iter    20/   40] train: loss: 0.0612399
[Epoch 62] ogbg-molclintox: 0.970780 val loss: 0.064821
[Epoch 62] ogbg-molclintox: 0.864387 test loss: 0.204711
[Epoch 63; Iter    10/   40] train: loss: 0.0980502
[Epoch 63; Iter    40/   40] train: loss: 0.1700541
[Epoch 63] ogbg-molclintox: 0.992870 val loss: 0.055482
[Epoch 63] ogbg-molclintox: 0.783417 test loss: 0.231787
[Epoch 64; Iter    30/   40] train: loss: 0.0947831
[Epoch 64] ogbg-molclintox: 0.985965 val loss: 0.205333
[Epoch 64] ogbg-molclintox: 0.864574 test loss: 0.534216
[Epoch 65; Iter    20/   40] train: loss: 0.1939677
[Epoch 65] ogbg-molclintox: 0.973103 val loss: 0.094186
[Epoch 65] ogbg-molclintox: 0.834355 test loss: 0.227688
[Epoch 66; Iter    10/   40] train: loss: 0.0712563
[Epoch 66; Iter    40/   40] train: loss: 0.0172286
[Epoch 66] ogbg-molclintox: 0.973827 val loss: 0.079286
[Epoch 66] ogbg-molclintox: 0.841759 test loss: 0.217498
[Epoch 67; Iter    30/   40] train: loss: 0.0213537
[Epoch 67] ogbg-molclintox: 0.964037 val loss: 0.117493
[Epoch 67] ogbg-molclintox: 0.850245 test loss: 0.208836
[Epoch 68; Iter    20/   40] train: loss: 0.0820612
[Epoch 68] ogbg-molclintox: 0.964712 val loss: 0.119284
[Epoch 68] ogbg-molclintox: 0.825450 test loss: 0.208034
[Epoch 69; Iter    10/   40] train: loss: 0.0521735
[Epoch 69; Iter    40/   40] train: loss: 0.0848315
[Epoch 69] ogbg-molclintox: 0.977910 val loss: 0.063014
[Epoch 69] ogbg-molclintox: 0.831270 test loss: 0.190528
[Epoch 70; Iter    30/   40] train: loss: 0.0736723
[Epoch 70] ogbg-molclintox: 0.983754 val loss: 0.064446
[Epoch 70] ogbg-molclintox: 0.844960 test loss: 0.193163
[Epoch 71; Iter    20/   40] train: loss: 0.0864967
[Epoch 71] ogbg-molclintox: 0.956770 val loss: 0.067724
[Epoch 71] ogbg-molclintox: 0.822126 test loss: 0.221558
[Epoch 72; Iter    10/   40] train: loss: 0.0082988
[Epoch 72; Iter    40/   40] train: loss: 0.0273249
[Epoch 72] ogbg-molclintox: 0.976287 val loss: 0.063153
[Epoch 72] ogbg-molclintox: 0.773593 test loss: 0.222386
[Epoch 73; Iter    30/   40] train: loss: 0.1299094
[Epoch 73] ogbg-molclintox: 0.984679 val loss: 0.073087
[Epoch 73] ogbg-molclintox: 0.817364 test loss: 0.224079
[Epoch 74; Iter    20/   40] train: loss: 0.0517179
[Epoch 74] ogbg-molclintox: 0.976175 val loss: 0.062170
[Epoch 74] ogbg-molclintox: 0.774542 test loss: 0.207993
[Epoch 75; Iter    10/   40] train: loss: 0.0447759
[Epoch 75; Iter    40/   40] train: loss: 0.0336912
[Epoch 75] ogbg-molclintox: 0.980209 val loss: 0.066287
[Epoch 75] ogbg-molclintox: 0.779775 test loss: 0.218153
[Epoch 76; Iter    30/   40] train: loss: 0.0313020
[Epoch 31] ogbg-molclintox: 0.764277 test loss: 0.208777
[Epoch 32; Iter    20/   40] train: loss: 0.2018348
[Epoch 32] ogbg-molclintox: 0.940412 val loss: 0.116561
[Epoch 32] ogbg-molclintox: 0.882327 test loss: 0.188560
[Epoch 33; Iter    10/   40] train: loss: 0.0825539
[Epoch 33; Iter    40/   40] train: loss: 0.1460862
[Epoch 33] ogbg-molclintox: 0.971880 val loss: 0.102980
[Epoch 33] ogbg-molclintox: 0.865537 test loss: 0.186093
[Epoch 34; Iter    30/   40] train: loss: 0.0957851
[Epoch 34] ogbg-molclintox: 0.951214 val loss: 0.140083
[Epoch 34] ogbg-molclintox: 0.824277 test loss: 0.236728
[Epoch 35; Iter    20/   40] train: loss: 0.2241222
[Epoch 35] ogbg-molclintox: 0.907984 val loss: 0.124945
[Epoch 35] ogbg-molclintox: 0.836021 test loss: 0.275135
[Epoch 36; Iter    10/   40] train: loss: 0.1396085
[Epoch 36; Iter    40/   40] train: loss: 0.2914232
[Epoch 36] ogbg-molclintox: 0.916126 val loss: 0.107221
[Epoch 36] ogbg-molclintox: 0.767762 test loss: 0.340533
[Epoch 37; Iter    30/   40] train: loss: 0.2241822
[Epoch 37] ogbg-molclintox: 0.909020 val loss: 0.113019
[Epoch 37] ogbg-molclintox: 0.841788 test loss: 0.188857
[Epoch 38; Iter    20/   40] train: loss: 0.0587421
[Epoch 38] ogbg-molclintox: 0.968408 val loss: 0.096694
[Epoch 38] ogbg-molclintox: 0.869097 test loss: 0.170143
[Epoch 39; Iter    10/   40] train: loss: 0.0993847
[Epoch 39; Iter    40/   40] train: loss: 0.1008605
[Epoch 39] ogbg-molclintox: 0.973465 val loss: 0.090320
[Epoch 39] ogbg-molclintox: 0.847036 test loss: 0.209215
[Epoch 40; Iter    30/   40] train: loss: 0.1393874
[Epoch 40] ogbg-molclintox: 0.922694 val loss: 0.093910
[Epoch 40] ogbg-molclintox: 0.828734 test loss: 0.170765
[Epoch 41; Iter    20/   40] train: loss: 0.0620950
[Epoch 41] ogbg-molclintox: 0.974302 val loss: 0.085602
[Epoch 41] ogbg-molclintox: 0.881013 test loss: 0.224347
[Epoch 42; Iter    10/   40] train: loss: 0.0985084
[Epoch 42; Iter    40/   40] train: loss: 0.0951557
[Epoch 42] ogbg-molclintox: 0.961078 val loss: 0.085423
[Epoch 42] ogbg-molclintox: 0.879178 test loss: 0.189650
[Epoch 43; Iter    30/   40] train: loss: 0.0787806
[Epoch 43] ogbg-molclintox: 0.908982 val loss: 0.094453
[Epoch 43] ogbg-molclintox: 0.878928 test loss: 0.193879
[Epoch 44; Iter    20/   40] train: loss: 0.0736350
[Epoch 44] ogbg-molclintox: 0.994630 val loss: 0.054883
[Epoch 44] ogbg-molclintox: 0.877737 test loss: 0.164612
[Epoch 45; Iter    10/   40] train: loss: 0.1252571
[Epoch 45; Iter    40/   40] train: loss: 0.0426074
[Epoch 45] ogbg-molclintox: 0.985740 val loss: 0.105519
[Epoch 45] ogbg-molclintox: 0.831908 test loss: 0.185972
[Epoch 46; Iter    30/   40] train: loss: 0.0448932
[Epoch 46] ogbg-molclintox: 0.980370 val loss: 0.062060
[Epoch 46] ogbg-molclintox: 0.895218 test loss: 0.176407
[Epoch 47; Iter    20/   40] train: loss: 0.1261697
[Epoch 47] ogbg-molclintox: 0.998601 val loss: 0.072223
[Epoch 47] ogbg-molclintox: 0.889234 test loss: 0.210827
[Epoch 48; Iter    10/   40] train: loss: 0.0374776
[Epoch 48; Iter    40/   40] train: loss: 0.0505906
[Epoch 48] ogbg-molclintox: 1.000000 val loss: 0.073077
[Epoch 48] ogbg-molclintox: 0.869246 test loss: 0.177460
[Epoch 49; Iter    30/   40] train: loss: 0.0906236
[Epoch 49] ogbg-molclintox: 0.942636 val loss: 0.145869
[Epoch 49] ogbg-molclintox: 0.812235 test loss: 0.217010
[Epoch 50; Iter    20/   40] train: loss: 0.1397348
[Epoch 50] ogbg-molclintox: 0.981857 val loss: 0.067185
[Epoch 50] ogbg-molclintox: 0.812758 test loss: 0.174756
[Epoch 51; Iter    10/   40] train: loss: 0.1173059
[Epoch 51; Iter    40/   40] train: loss: 0.0219361
[Epoch 51] ogbg-molclintox: 0.984478 val loss: 0.058487
[Epoch 51] ogbg-molclintox: 0.844638 test loss: 0.172014
[Epoch 52; Iter    30/   40] train: loss: 0.0703861
[Epoch 52] ogbg-molclintox: 0.996254 val loss: 0.051276
[Epoch 52] ogbg-molclintox: 0.786876 test loss: 0.204818
[Epoch 53; Iter    20/   40] train: loss: 0.0521784
[Epoch 53] ogbg-molclintox: 0.996142 val loss: 0.049344
[Epoch 53] ogbg-molclintox: 0.863165 test loss: 0.163316
[Epoch 54; Iter    10/   40] train: loss: 0.0274387
[Epoch 54; Iter    40/   40] train: loss: 0.0583106
[Epoch 54] ogbg-molclintox: 0.992283 val loss: 0.059729
[Epoch 54] ogbg-molclintox: 0.835158 test loss: 0.186783
[Epoch 55; Iter    30/   40] train: loss: 0.1894451
[Epoch 55] ogbg-molclintox: 0.979509 val loss: 0.070263
[Epoch 55] ogbg-molclintox: 0.842890 test loss: 0.203485
[Epoch 56; Iter    20/   40] train: loss: 0.0844222
[Epoch 56] ogbg-molclintox: 0.954710 val loss: 0.087923
[Epoch 56] ogbg-molclintox: 0.857405 test loss: 0.154945
[Epoch 57; Iter    10/   40] train: loss: 0.1039934
[Epoch 57; Iter    40/   40] train: loss: 0.0313361
[Epoch 57] ogbg-molclintox: 0.947717 val loss: 0.112429
[Epoch 57] ogbg-molclintox: 0.851444 test loss: 0.166543
[Epoch 58; Iter    30/   40] train: loss: 0.0897403
[Epoch 58] ogbg-molclintox: 0.997902 val loss: 0.056382
[Epoch 58] ogbg-molclintox: 0.844888 test loss: 0.183987
[Epoch 59; Iter    20/   40] train: loss: 0.0302594
[Epoch 59] ogbg-molclintox: 0.990161 val loss: 0.057188
[Epoch 59] ogbg-molclintox: 0.830298 test loss: 0.202033
[Epoch 60; Iter    10/   40] train: loss: 0.1214896
[Epoch 60; Iter    40/   40] train: loss: 0.2629426
[Epoch 60] ogbg-molclintox: 0.978086 val loss: 0.074091
[Epoch 60] ogbg-molclintox: 0.839816 test loss: 0.187953
[Epoch 61; Iter    30/   40] train: loss: 0.1324163
[Epoch 61] ogbg-molclintox: 0.926152 val loss: 0.074593
[Epoch 61] ogbg-molclintox: 0.887848 test loss: 0.160923
[Epoch 62; Iter    20/   40] train: loss: 0.0581530
[Epoch 62] ogbg-molclintox: 0.946993 val loss: 0.083934
[Epoch 62] ogbg-molclintox: 0.869997 test loss: 0.169179
[Epoch 63; Iter    10/   40] train: loss: 0.0431822
[Epoch 63; Iter    40/   40] train: loss: 0.0190334
[Epoch 63] ogbg-molclintox: 0.973802 val loss: 0.072475
[Epoch 63] ogbg-molclintox: 0.783391 test loss: 0.203841
[Epoch 64; Iter    30/   40] train: loss: 0.1627322
[Epoch 64] ogbg-molclintox: 0.946769 val loss: 0.098559
[Epoch 64] ogbg-molclintox: 0.841403 test loss: 0.189255
[Epoch 65; Iter    20/   40] train: loss: 0.0168990
[Epoch 65] ogbg-molclintox: 0.944534 val loss: 0.109159
[Epoch 65] ogbg-molclintox: 0.819702 test loss: 0.184779
[Epoch 66; Iter    10/   40] train: loss: 0.0463108
[Epoch 66; Iter    40/   40] train: loss: 0.0218432
[Epoch 66] ogbg-molclintox: 0.969357 val loss: 0.097890
[Epoch 66] ogbg-molclintox: 0.705753 test loss: 0.237031
[Epoch 67; Iter    30/   40] train: loss: 0.1085787
[Epoch 67] ogbg-molclintox: 0.980683 val loss: 0.067883
[Epoch 67] ogbg-molclintox: 0.837056 test loss: 0.184310
[Epoch 68; Iter    20/   40] train: loss: 0.1661635
[Epoch 68] ogbg-molclintox: 0.957694 val loss: 0.074411
[Epoch 68] ogbg-molclintox: 0.839117 test loss: 0.190522
[Epoch 69; Iter    10/   40] train: loss: 0.0546087
[Epoch 69; Iter    40/   40] train: loss: 0.1368582
[Epoch 69] ogbg-molclintox: 0.957058 val loss: 0.146919
[Epoch 69] ogbg-molclintox: 0.771311 test loss: 0.227807
[Epoch 70; Iter    30/   40] train: loss: 0.0670712
[Epoch 70] ogbg-molclintox: 0.970056 val loss: 0.084889
[Epoch 70] ogbg-molclintox: 0.842315 test loss: 0.192400
[Epoch 71; Iter    20/   40] train: loss: 0.0716703
[Epoch 71] ogbg-molclintox: 0.983730 val loss: 0.075409
[Epoch 71] ogbg-molclintox: 0.814408 test loss: 0.220062
[Epoch 72; Iter    10/   40] train: loss: 0.0192509
[Epoch 72; Iter    40/   40] train: loss: 0.0502482
[Epoch 72] ogbg-molclintox: 0.983730 val loss: 0.064461
[Epoch 72] ogbg-molclintox: 0.835095 test loss: 0.212283
[Epoch 73; Iter    30/   40] train: loss: 0.1652988
[Epoch 73] ogbg-molclintox: 0.977299 val loss: 0.065996
[Epoch 73] ogbg-molclintox: 0.795157 test loss: 0.199081
[Epoch 74; Iter    20/   40] train: loss: 0.0162839
[Epoch 74] ogbg-molclintox: 0.982556 val loss: 0.076329
[Epoch 74] ogbg-molclintox: 0.826675 test loss: 0.216603
[Epoch 75; Iter    10/   40] train: loss: 0.0487777
[Epoch 75; Iter    40/   40] train: loss: 0.0560513
[Epoch 75] ogbg-molclintox: 0.990161 val loss: 0.053590
[Epoch 75] ogbg-molclintox: 0.841478 test loss: 0.203596
[Epoch 76; Iter    30/   40] train: loss: 0.0148007
[Epoch 31] ogbg-molclintox: 0.860453 test loss: 0.201789
[Epoch 32; Iter    20/   40] train: loss: 0.1586776
[Epoch 32] ogbg-molclintox: 0.861671 val loss: 0.138309
[Epoch 32] ogbg-molclintox: 0.808537 test loss: 0.208537
[Epoch 33; Iter    10/   40] train: loss: 0.1973344
[Epoch 33; Iter    40/   40] train: loss: 0.6622324
[Epoch 33] ogbg-molclintox: 0.989486 val loss: 0.112288
[Epoch 33] ogbg-molclintox: 0.827762 test loss: 0.203577
[Epoch 34; Iter    30/   40] train: loss: 0.1815879
[Epoch 34] ogbg-molclintox: 0.974140 val loss: 0.116746
[Epoch 34] ogbg-molclintox: 0.854357 test loss: 0.195935
[Epoch 35; Iter    20/   40] train: loss: 0.1742542
[Epoch 35] ogbg-molclintox: 0.949165 val loss: 0.099087
[Epoch 35] ogbg-molclintox: 0.862003 test loss: 0.250633
[Epoch 36; Iter    10/   40] train: loss: 0.1955257
[Epoch 36; Iter    40/   40] train: loss: 0.2380027
[Epoch 36] ogbg-molclintox: 0.955933 val loss: 0.158903
[Epoch 36] ogbg-molclintox: 0.837881 test loss: 0.932325
[Epoch 37; Iter    30/   40] train: loss: 0.2788168
[Epoch 37] ogbg-molclintox: 0.924503 val loss: 0.102129
[Epoch 37] ogbg-molclintox: 0.871121 test loss: 0.433930
[Epoch 38; Iter    20/   40] train: loss: 0.2438206
[Epoch 38] ogbg-molclintox: 0.960516 val loss: 0.078048
[Epoch 38] ogbg-molclintox: 0.869048 test loss: 0.191935
[Epoch 39; Iter    10/   40] train: loss: 0.0988167
[Epoch 39; Iter    40/   40] train: loss: 0.0668840
[Epoch 39] ogbg-molclintox: 0.855652 val loss: 0.153667
[Epoch 39] ogbg-molclintox: 0.738560 test loss: 2.360707
[Epoch 40; Iter    30/   40] train: loss: 0.3096189
[Epoch 40] ogbg-molclintox: 0.973578 val loss: 0.137811
[Epoch 40] ogbg-molclintox: 0.812821 test loss: 31.592473
[Epoch 41; Iter    20/   40] train: loss: 0.2375076
[Epoch 41] ogbg-molclintox: 0.986776 val loss: 0.082267
[Epoch 41] ogbg-molclintox: 0.855370 test loss: 0.182198
[Epoch 42; Iter    10/   40] train: loss: 0.0574655
[Epoch 42; Iter    40/   40] train: loss: 0.1131026
[Epoch 42] ogbg-molclintox: 0.991584 val loss: 0.066349
[Epoch 42] ogbg-molclintox: 0.864488 test loss: 0.224851
[Epoch 43; Iter    30/   40] train: loss: 0.3236190
[Epoch 43] ogbg-molclintox: 0.994044 val loss: 0.076133
[Epoch 43] ogbg-molclintox: 0.848273 test loss: 0.173897
[Epoch 44; Iter    20/   40] train: loss: 0.0720191
[Epoch 44] ogbg-molclintox: 0.839220 val loss: 0.153483
[Epoch 44] ogbg-molclintox: 0.726361 test loss: 0.203786
[Epoch 45; Iter    10/   40] train: loss: 0.1914315
[Epoch 45; Iter    40/   40] train: loss: 0.2747357
[Epoch 45] ogbg-molclintox: 0.994269 val loss: 0.075445
[Epoch 45] ogbg-molclintox: 0.892506 test loss: 0.190014
[Epoch 46; Iter    30/   40] train: loss: 0.1053455
[Epoch 46] ogbg-molclintox: 0.984092 val loss: 0.096946
[Epoch 46] ogbg-molclintox: 0.869493 test loss: 0.187725
[Epoch 47; Iter    20/   40] train: loss: 0.1571699
[Epoch 47] ogbg-molclintox: 0.971655 val loss: 0.079113
[Epoch 47] ogbg-molclintox: 0.820740 test loss: 0.196560
[Epoch 48; Iter    10/   40] train: loss: 0.1008863
[Epoch 48; Iter    40/   40] train: loss: 0.1905752
[Epoch 48] ogbg-molclintox: 0.962652 val loss: 0.108964
[Epoch 48] ogbg-molclintox: 0.756605 test loss: 0.209449
[Epoch 49; Iter    30/   40] train: loss: 0.2189221
[Epoch 49] ogbg-molclintox: 0.886955 val loss: 0.127062
[Epoch 49] ogbg-molclintox: 0.798429 test loss: 0.430398
[Epoch 50; Iter    20/   40] train: loss: 0.1432959
[Epoch 50] ogbg-molclintox: 0.985353 val loss: 0.118810
[Epoch 50] ogbg-molclintox: 0.798967 test loss: 0.204189
[Epoch 51; Iter    10/   40] train: loss: 0.1692861
[Epoch 51; Iter    40/   40] train: loss: 0.0492690
[Epoch 51] ogbg-molclintox: 0.902140 val loss: 0.102749
[Epoch 51] ogbg-molclintox: 0.810434 test loss: 0.368501
[Epoch 52; Iter    30/   40] train: loss: 0.1433144
[Epoch 52] ogbg-molclintox: 0.982194 val loss: 0.084528
[Epoch 52] ogbg-molclintox: 0.892080 test loss: 0.872993
[Epoch 53; Iter    20/   40] train: loss: 0.0912163
[Epoch 53] ogbg-molclintox: 0.920659 val loss: 0.342263
[Epoch 53] ogbg-molclintox: 0.713537 test loss: 1.411958
[Epoch 54; Iter    10/   40] train: loss: 0.1293307
[Epoch 54; Iter    40/   40] train: loss: 0.0388866
[Epoch 54] ogbg-molclintox: 0.944084 val loss: 0.154431
[Epoch 54] ogbg-molclintox: 0.866684 test loss: 0.219438
[Epoch 55; Iter    30/   40] train: loss: 0.1644021
[Epoch 55] ogbg-molclintox: 0.936704 val loss: 0.280757
[Epoch 55] ogbg-molclintox: 0.859165 test loss: 0.172350
[Epoch 56; Iter    20/   40] train: loss: 0.2707117
[Epoch 56] ogbg-molclintox: 0.976737 val loss: 0.087926
[Epoch 56] ogbg-molclintox: 0.837579 test loss: 0.214396
[Epoch 57; Iter    10/   40] train: loss: 0.0869027
[Epoch 57; Iter    40/   40] train: loss: 0.1139342
[Epoch 57] ogbg-molclintox: 0.989461 val loss: 0.095525
[Epoch 57] ogbg-molclintox: 0.873818 test loss: 0.323217
[Epoch 58; Iter    30/   40] train: loss: 0.3266472
[Epoch 58] ogbg-molclintox: 0.968545 val loss: 0.116282
[Epoch 58] ogbg-molclintox: 0.862340 test loss: 0.267445
[Epoch 59; Iter    20/   40] train: loss: 0.0853465
[Epoch 59] ogbg-molclintox: 0.946431 val loss: 0.214116
[Epoch 59] ogbg-molclintox: 0.779641 test loss: 0.268738
[Epoch 60; Iter    10/   40] train: loss: 0.0481592
[Epoch 60; Iter    40/   40] train: loss: 0.0827351
[Epoch 60] ogbg-molclintox: 0.986776 val loss: 0.063212
[Epoch 60] ogbg-molclintox: 0.866647 test loss: 0.295922
[Epoch 61; Iter    30/   40] train: loss: 0.0805497
[Epoch 61] ogbg-molclintox: 0.848933 val loss: 0.118718
[Epoch 61] ogbg-molclintox: 0.787970 test loss: 0.228878
[Epoch 62; Iter    20/   40] train: loss: 0.0969136
[Epoch 62] ogbg-molclintox: 0.955860 val loss: 0.092530
[Epoch 62] ogbg-molclintox: 0.862049 test loss: 0.188696
[Epoch 63; Iter    10/   40] train: loss: 0.3364135
[Epoch 63; Iter    40/   40] train: loss: 0.0312899
[Epoch 63] ogbg-molclintox: 0.953287 val loss: 0.117635
[Epoch 63] ogbg-molclintox: 0.819463 test loss: 0.196372
[Epoch 64; Iter    30/   40] train: loss: 0.0454118
[Epoch 64] ogbg-molclintox: 0.998826 val loss: 0.051973
[Epoch 64] ogbg-molclintox: 0.834180 test loss: 0.226392
[Epoch 65; Iter    20/   40] train: loss: 0.0218674
[Epoch 65] ogbg-molclintox: 0.993344 val loss: 0.049859
[Epoch 65] ogbg-molclintox: 0.836268 test loss: 0.262891
[Epoch 66; Iter    10/   40] train: loss: 0.0561758
[Epoch 66; Iter    40/   40] train: loss: 0.1807697
[Epoch 66] ogbg-molclintox: 0.896869 val loss: 0.168148
[Epoch 66] ogbg-molclintox: 0.708498 test loss: 0.280607
[Epoch 67; Iter    30/   40] train: loss: 0.1239404
[Epoch 67] ogbg-molclintox: 0.966061 val loss: 0.081404
[Epoch 67] ogbg-molclintox: 0.759276 test loss: 0.223469
[Epoch 68; Iter    20/   40] train: loss: 0.0568833
[Epoch 68] ogbg-molclintox: 0.963376 val loss: 0.090602
[Epoch 68] ogbg-molclintox: 0.814566 test loss: 0.187397
[Epoch 69; Iter    10/   40] train: loss: 0.0465411
[Epoch 69; Iter    40/   40] train: loss: 0.1292638
[Epoch 69] ogbg-molclintox: 0.996254 val loss: 0.052953
[Epoch 69] ogbg-molclintox: 0.821024 test loss: 0.200302
[Epoch 70; Iter    30/   40] train: loss: 0.1456795
[Epoch 70] ogbg-molclintox: 0.979260 val loss: 0.082642
[Epoch 70] ogbg-molclintox: 0.843238 test loss: 0.200101
[Epoch 71; Iter    20/   40] train: loss: 0.0237824
[Epoch 71] ogbg-molclintox: 0.987838 val loss: 0.067236
[Epoch 71] ogbg-molclintox: 0.831083 test loss: 0.211732
[Epoch 72; Iter    10/   40] train: loss: 0.0193401
[Epoch 72; Iter    40/   40] train: loss: 0.0643628
[Epoch 72] ogbg-molclintox: 0.985016 val loss: 0.068067
[Epoch 72] ogbg-molclintox: 0.816542 test loss: 0.203213
[Epoch 73; Iter    30/   40] train: loss: 0.0564372
[Epoch 73] ogbg-molclintox: 0.986864 val loss: 0.064673
[Epoch 73] ogbg-molclintox: 0.816403 test loss: 0.208428
[Epoch 74; Iter    20/   40] train: loss: 0.0961481
[Epoch 74] ogbg-molclintox: 0.987676 val loss: 0.065069
[Epoch 74] ogbg-molclintox: 0.810644 test loss: 0.296999
[Epoch 75; Iter    10/   40] train: loss: 0.0540665
[Epoch 75; Iter    40/   40] train: loss: 0.0156227
[Epoch 75] ogbg-molclintox: 0.982307 val loss: 0.064468
[Epoch 75] ogbg-molclintox: 0.787746 test loss: 0.219839
[Epoch 76; Iter    30/   40] train: loss: 0.0092626
[Epoch 76] ogbg-molclintox: 0.867652 val loss: 0.931316
[Epoch 76] ogbg-molclintox: 0.782173 test loss: 0.446623
[Epoch 77; Iter    20/   40] train: loss: 0.0037854
[Epoch 77] ogbg-molclintox: 0.891952 val loss: 0.290784
[Epoch 77] ogbg-molclintox: 0.805620 test loss: 0.401802
[Epoch 78; Iter    10/   40] train: loss: 0.0277297
[Epoch 78; Iter    40/   40] train: loss: 0.0294987
[Epoch 78] ogbg-molclintox: 0.821077 val loss: 0.233217
[Epoch 78] ogbg-molclintox: 0.782136 test loss: 0.478610
[Epoch 79; Iter    30/   40] train: loss: 0.0020033
[Epoch 79] ogbg-molclintox: 0.878029 val loss: 0.264478
[Epoch 79] ogbg-molclintox: 0.783447 test loss: 0.379684
[Epoch 80; Iter    20/   40] train: loss: 0.0205971
[Epoch 80] ogbg-molclintox: 0.892901 val loss: 0.183275
[Epoch 80] ogbg-molclintox: 0.794915 test loss: 0.357263
[Epoch 81; Iter    10/   40] train: loss: 0.0017403
[Epoch 81; Iter    40/   40] train: loss: 0.0068783
[Epoch 81] ogbg-molclintox: 0.889155 val loss: 0.361848
[Epoch 81] ogbg-molclintox: 0.771117 test loss: 0.487613
[Epoch 82; Iter    30/   40] train: loss: 0.0136444
[Epoch 82] ogbg-molclintox: 0.779045 val loss: 0.318699
[Epoch 82] ogbg-molclintox: 0.738325 test loss: 0.452961
[Epoch 83; Iter    20/   40] train: loss: 0.0015994
[Epoch 83] ogbg-molclintox: 0.852605 val loss: 0.298888
[Epoch 83] ogbg-molclintox: 0.765372 test loss: 0.474231
[Epoch 84; Iter    10/   40] train: loss: 0.0204205
[Epoch 84; Iter    40/   40] train: loss: 0.0336535
[Epoch 84] ogbg-molclintox: 0.820177 val loss: 0.184813
[Epoch 84] ogbg-molclintox: 0.772379 test loss: 0.371899
[Epoch 85; Iter    30/   40] train: loss: 0.0036901
[Epoch 85] ogbg-molclintox: 0.905025 val loss: 0.188300
[Epoch 85] ogbg-molclintox: 0.809232 test loss: 0.397464
[Epoch 86; Iter    20/   40] train: loss: 0.0013254
[Epoch 86] ogbg-molclintox: 0.900442 val loss: 0.258608
[Epoch 86] ogbg-molclintox: 0.783836 test loss: 0.460680
[Epoch 87; Iter    10/   40] train: loss: 0.0015001
[Epoch 87; Iter    40/   40] train: loss: 0.0040463
[Epoch 87] ogbg-molclintox: 0.874782 val loss: 0.525492
[Epoch 87] ogbg-molclintox: 0.784146 test loss: 0.541617
[Epoch 88; Iter    30/   40] train: loss: 0.0018160
[Epoch 88] ogbg-molclintox: 0.834349 val loss: 0.310885
[Epoch 88] ogbg-molclintox: 0.765521 test loss: 0.451183
[Epoch 89; Iter    20/   40] train: loss: 0.0016689
[Epoch 89] ogbg-molclintox: 0.797438 val loss: 0.321194
[Epoch 89] ogbg-molclintox: 0.765671 test loss: 0.478584
[Epoch 90; Iter    10/   40] train: loss: 0.0118439
[Epoch 90; Iter    40/   40] train: loss: 0.0007933
[Epoch 90] ogbg-molclintox: 0.891826 val loss: 0.522574
[Epoch 90] ogbg-molclintox: 0.799337 test loss: 0.667664
[Epoch 91; Iter    30/   40] train: loss: 0.0015300
[Epoch 91] ogbg-molclintox: 0.748627 val loss: 0.496186
[Epoch 91] ogbg-molclintox: 0.729144 test loss: 0.454684
[Epoch 92; Iter    20/   40] train: loss: 0.0029524
[Epoch 92] ogbg-molclintox: 0.771328 val loss: 0.575768
[Epoch 92] ogbg-molclintox: 0.743248 test loss: 0.505144
[Epoch 93; Iter    10/   40] train: loss: 0.0010389
[Epoch 93; Iter    40/   40] train: loss: 0.0018715
[Epoch 93] ogbg-molclintox: 0.780693 val loss: 0.620768
[Epoch 93] ogbg-molclintox: 0.775577 test loss: 0.469774
[Epoch 94; Iter    30/   40] train: loss: 0.0008363
[Epoch 94] ogbg-molclintox: 0.779069 val loss: 0.405319
[Epoch 94] ogbg-molclintox: 0.752754 test loss: 0.491390
[Epoch 95; Iter    20/   40] train: loss: 0.0024182
[Epoch 95] ogbg-molclintox: 0.848634 val loss: 0.229223
[Epoch 95] ogbg-molclintox: 0.764322 test loss: 0.428170
[Epoch 96; Iter    10/   40] train: loss: 0.0020624
[Epoch 96; Iter    40/   40] train: loss: 0.0113722
[Epoch 96] ogbg-molclintox: 0.829992 val loss: 0.249697
[Epoch 96] ogbg-molclintox: 0.745646 test loss: 0.437935
[Epoch 97; Iter    30/   40] train: loss: 0.0022893
[Epoch 97] ogbg-molclintox: 0.839157 val loss: 0.266396
[Epoch 97] ogbg-molclintox: 0.742038 test loss: 0.460769
[Epoch 98; Iter    20/   40] train: loss: 0.0028379
[Epoch 98] ogbg-molclintox: 0.848522 val loss: 0.242476
[Epoch 98] ogbg-molclintox: 0.757603 test loss: 0.437993
[Epoch 99; Iter    10/   40] train: loss: 0.0057179
[Epoch 99; Iter    40/   40] train: loss: 0.0071697
[Epoch 99] ogbg-molclintox: 0.917275 val loss: 0.268979
[Epoch 99] ogbg-molclintox: 0.792755 test loss: 0.525947
[Epoch 100; Iter    30/   40] train: loss: 0.0033156
[Epoch 100] ogbg-molclintox: 0.859985 val loss: 0.329778
[Epoch 100] ogbg-molclintox: 0.778251 test loss: 0.481802
[Epoch 101; Iter    20/   40] train: loss: 0.0021427
[Epoch 101] ogbg-molclintox: 0.839568 val loss: 0.301044
[Epoch 101] ogbg-molclintox: 0.782599 test loss: 0.516650
[Epoch 102; Iter    10/   40] train: loss: 0.0068636
[Epoch 102; Iter    40/   40] train: loss: 0.0055646
[Epoch 102] ogbg-molclintox: 0.873546 val loss: 0.289290
[Epoch 102] ogbg-molclintox: 0.781299 test loss: 0.475557
[Epoch 103; Iter    30/   40] train: loss: 0.0042833
[Epoch 103] ogbg-molclintox: 0.827005 val loss: 0.318535
[Epoch 103] ogbg-molclintox: 0.760176 test loss: 0.536354
[Epoch 104; Iter    20/   40] train: loss: 0.0092235
[Epoch 104] ogbg-molclintox: 0.916351 val loss: 0.266163
[Epoch 104] ogbg-molclintox: 0.776903 test loss: 0.434997
[Epoch 105; Iter    10/   40] train: loss: 0.0022630
[Epoch 105; Iter    40/   40] train: loss: 0.0044842
[Epoch 105] ogbg-molclintox: 0.802744 val loss: 0.472057
[Epoch 105] ogbg-molclintox: 0.702474 test loss: 0.542239
[Epoch 106; Iter    30/   40] train: loss: 0.0012419
[Epoch 106] ogbg-molclintox: 0.808401 val loss: 0.292864
[Epoch 106] ogbg-molclintox: 0.733028 test loss: 0.453396
[Epoch 107; Iter    20/   40] train: loss: 0.0031010
[Epoch 107] ogbg-molclintox: 0.796151 val loss: 0.323771
[Epoch 107] ogbg-molclintox: 0.748694 test loss: 0.485373
[Epoch 108; Iter    10/   40] train: loss: 0.0006841
[Epoch 108; Iter    40/   40] train: loss: 0.0077459
[Epoch 108] ogbg-molclintox: 0.837171 val loss: 0.259719
[Epoch 108] ogbg-molclintox: 0.772167 test loss: 0.440898
[Epoch 109; Iter    30/   40] train: loss: 0.0021399
[Epoch 109] ogbg-molclintox: 0.819390 val loss: 0.262357
[Epoch 109] ogbg-molclintox: 0.767920 test loss: 0.424751
[Epoch 110; Iter    20/   40] train: loss: 0.0045513
[Epoch 110] ogbg-molclintox: 0.822574 val loss: 0.337746
[Epoch 110] ogbg-molclintox: 0.783485 test loss: 0.459102
[Epoch 111; Iter    10/   40] train: loss: 0.0024001
[Epoch 111; Iter    40/   40] train: loss: 0.0367301
[Epoch 111] ogbg-molclintox: 0.822549 val loss: 0.317830
[Epoch 111] ogbg-molclintox: 0.786958 test loss: 0.471068
[Epoch 112; Iter    30/   40] train: loss: 0.0533204
[Epoch 112] ogbg-molclintox: 0.867203 val loss: 0.411929
[Epoch 112] ogbg-molclintox: 0.822910 test loss: 0.525681
[Epoch 113; Iter    20/   40] train: loss: 0.0005115
[Epoch 113] ogbg-molclintox: 0.894036 val loss: 0.238689
[Epoch 113] ogbg-molclintox: 0.830470 test loss: 0.462586
[Epoch 114; Iter    10/   40] train: loss: 0.0065907
[Epoch 114; Iter    40/   40] train: loss: 0.0003657
[Epoch 114] ogbg-molclintox: 0.887268 val loss: 0.212125
[Epoch 114] ogbg-molclintox: 0.826612 test loss: 0.470309
[Epoch 115; Iter    30/   40] train: loss: 0.0005852
[Epoch 115] ogbg-molclintox: 0.880338 val loss: 0.318909
[Epoch 115] ogbg-molclintox: 0.818966 test loss: 0.560311
[Epoch 116; Iter    20/   40] train: loss: 0.0028015
[Epoch 116] ogbg-molclintox: 0.856850 val loss: 0.255151
[Epoch 116] ogbg-molclintox: 0.810121 test loss: 0.492678
[Epoch 117; Iter    10/   40] train: loss: 0.0038429
[Epoch 117; Iter    40/   40] train: loss: 0.0004959
[Epoch 117] ogbg-molclintox: 0.863281 val loss: 0.265247
[Epoch 117] ogbg-molclintox: 0.798952 test loss: 0.461260
[Epoch 118; Iter    30/   40] train: loss: 0.0010652
[Epoch 118] ogbg-molclintox: 0.852155 val loss: 0.330035
[Epoch 118] ogbg-molclintox: 0.796416 test loss: 0.502167
[Epoch 119; Iter    20/   40] train: loss: 0.0015147
[Epoch 119] ogbg-molclintox: 0.830291 val loss: 0.387749
[Epoch 119] ogbg-molclintox: 0.773493 test loss: 0.473769
[Epoch 120; Iter    10/   40] train: loss: 0.0006032
[Epoch 120; Iter    40/   40] train: loss: 0.0007316
[Epoch 120] ogbg-molclintox: 0.852742 val loss: 0.310857
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.794380 test loss: 0.463987
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 40.
Statistics on  val_best_checkpoint
mean_pred: 0.17987607419490814
std_pred: 4.18662166595459
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.8739575511212015
rocauc: 0.9800961948849273
ogbg-molclintox: 0.9800961948849273
OGBNanLabelBCEWithLogitsLoss: 0.0859923105686903
Statistics on  test
mean_pred: 0.04251578822731972
std_pred: 12.60492992401123
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6485452514643735
rocauc: 0.7885752267751016
ogbg-molclintox: 0.7885752267751016
OGBNanLabelBCEWithLogitsLoss: 1.578558659553528
Statistics on  train
mean_pred: 0.14015929400920868
std_pred: 4.345335960388184
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.8314549473901005
rocauc: 0.9149346190308769
ogbg-molclintox: 0.9149346190308769
OGBNanLabelBCEWithLogitsLoss: 0.16649731956422328
[Epoch 76] ogbg-molclintox: 0.859595 val loss: 0.126162
[Epoch 76] ogbg-molclintox: 0.820979 test loss: 0.335272
[Epoch 77; Iter    20/   40] train: loss: 0.0044763
[Epoch 77] ogbg-molclintox: 0.939575 val loss: 0.100005
[Epoch 77] ogbg-molclintox: 0.812832 test loss: 0.388833
[Epoch 78; Iter    10/   40] train: loss: 0.0047118
[Epoch 78; Iter    40/   40] train: loss: 0.0664847
[Epoch 78] ogbg-molclintox: 0.909221 val loss: 0.109860
[Epoch 78] ogbg-molclintox: 0.803375 test loss: 0.328478
[Epoch 79; Iter    30/   40] train: loss: 0.0269176
[Epoch 79] ogbg-molclintox: 0.909980 val loss: 0.107644
[Epoch 79] ogbg-molclintox: 0.800480 test loss: 0.345670
[Epoch 80; Iter    20/   40] train: loss: 0.0041079
[Epoch 80] ogbg-molclintox: 0.926289 val loss: 0.086320
[Epoch 80] ogbg-molclintox: 0.789349 test loss: 0.352680
[Epoch 81; Iter    10/   40] train: loss: 0.0039081
[Epoch 81; Iter    40/   40] train: loss: 0.0006223
[Epoch 81] ogbg-molclintox: 0.939013 val loss: 0.124287
[Epoch 81] ogbg-molclintox: 0.810613 test loss: 0.502354
[Epoch 82; Iter    30/   40] train: loss: 0.0482547
[Epoch 82] ogbg-molclintox: 0.917823 val loss: 0.111467
[Epoch 82] ogbg-molclintox: 0.808398 test loss: 0.311046
[Epoch 83; Iter    20/   40] train: loss: 0.0243799
[Epoch 83] ogbg-molclintox: 0.928362 val loss: 0.095475
[Epoch 83] ogbg-molclintox: 0.809922 test loss: 0.409246
[Epoch 84; Iter    10/   40] train: loss: 0.0126078
[Epoch 84; Iter    40/   40] train: loss: 0.1119513
[Epoch 84] ogbg-molclintox: 0.913926 val loss: 0.103155
[Epoch 84] ogbg-molclintox: 0.808925 test loss: 0.419789
[Epoch 85; Iter    30/   40] train: loss: 0.0016876
[Epoch 85] ogbg-molclintox: 0.934206 val loss: 0.102439
[Epoch 85] ogbg-molclintox: 0.807913 test loss: 0.414062
[Epoch 86; Iter    20/   40] train: loss: 0.0024047
[Epoch 86] ogbg-molclintox: 0.916449 val loss: 0.111158
[Epoch 86] ogbg-molclintox: 0.810236 test loss: 0.427606
[Epoch 87; Iter    10/   40] train: loss: 0.0035753
[Epoch 87; Iter    40/   40] train: loss: 0.0051635
[Epoch 87] ogbg-molclintox: 0.911417 val loss: 0.121381
[Epoch 87] ogbg-molclintox: 0.791511 test loss: 0.465417
[Epoch 88; Iter    30/   40] train: loss: 0.0097485
[Epoch 88] ogbg-molclintox: 0.923467 val loss: 0.105167
[Epoch 88] ogbg-molclintox: 0.799393 test loss: 0.412338
[Epoch 89; Iter    20/   40] train: loss: 0.0199491
[Epoch 89] ogbg-molclintox: 0.911891 val loss: 0.143372
[Epoch 89] ogbg-molclintox: 0.813885 test loss: 0.561465
[Epoch 90; Iter    10/   40] train: loss: 0.0072887
[Epoch 90; Iter    40/   40] train: loss: 0.0002124
[Epoch 90] ogbg-molclintox: 0.923980 val loss: 0.111217
[Epoch 90] ogbg-molclintox: 0.813034 test loss: 0.468405
[Epoch 91; Iter    30/   40] train: loss: 0.0019162
[Epoch 91] ogbg-molclintox: 0.921819 val loss: 0.110447
[Epoch 91] ogbg-molclintox: 0.822327 test loss: 0.423193
[Epoch 92; Iter    20/   40] train: loss: 0.0300601
[Epoch 92] ogbg-molclintox: 0.936891 val loss: 0.103395
[Epoch 92] ogbg-molclintox: 0.816306 test loss: 0.471080
[Epoch 93; Iter    10/   40] train: loss: 0.0033507
[Epoch 93; Iter    40/   40] train: loss: 0.0047492
[Epoch 93] ogbg-molclintox: 0.932895 val loss: 0.105589
[Epoch 93] ogbg-molclintox: 0.799792 test loss: 0.487366
[Epoch 94; Iter    30/   40] train: loss: 0.0003995
[Epoch 94] ogbg-molclintox: 0.942011 val loss: 0.106488
[Epoch 94] ogbg-molclintox: 0.796569 test loss: 0.445577
[Epoch 95; Iter    20/   40] train: loss: 0.0014497
[Epoch 95] ogbg-molclintox: 0.933257 val loss: 0.107395
[Epoch 95] ogbg-molclintox: 0.809235 test loss: 0.445850
[Epoch 96; Iter    10/   40] train: loss: 0.0072118
[Epoch 96; Iter    40/   40] train: loss: 0.0229037
[Epoch 96] ogbg-molclintox: 0.933731 val loss: 0.117392
[Epoch 96] ogbg-molclintox: 0.800454 test loss: 0.469777
[Epoch 97; Iter    30/   40] train: loss: 0.0010977
[Epoch 97] ogbg-molclintox: 0.925389 val loss: 0.114867
[Epoch 97] ogbg-molclintox: 0.806038 test loss: 0.437289
[Epoch 98; Iter    20/   40] train: loss: 0.0005824
[Epoch 98] ogbg-molclintox: 0.935355 val loss: 0.113913
[Epoch 98] ogbg-molclintox: 0.820665 test loss: 0.418506
[Epoch 99; Iter    10/   40] train: loss: 0.0025562
[Epoch 99; Iter    40/   40] train: loss: 0.0002372
[Epoch 99] ogbg-molclintox: 0.926851 val loss: 0.117628
[Epoch 99] ogbg-molclintox: 0.815619 test loss: 0.442320
[Epoch 100; Iter    30/   40] train: loss: 0.0003389
[Epoch 100] ogbg-molclintox: 0.933282 val loss: 0.111534
[Epoch 100] ogbg-molclintox: 0.817942 test loss: 0.456909
[Epoch 101; Iter    20/   40] train: loss: 0.0007945
[Epoch 101] ogbg-molclintox: 0.942959 val loss: 0.112036
[Epoch 101] ogbg-molclintox: 0.818917 test loss: 0.460838
[Epoch 102; Iter    10/   40] train: loss: 0.0016191
[Epoch 102; Iter    40/   40] train: loss: 0.0017044
[Epoch 102] ogbg-molclintox: 0.940250 val loss: 0.114934
[Epoch 102] ogbg-molclintox: 0.813546 test loss: 0.424413
[Epoch 103; Iter    30/   40] train: loss: 0.0003187
[Epoch 103] ogbg-molclintox: 0.946906 val loss: 0.107593
[Epoch 103] ogbg-molclintox: 0.819530 test loss: 0.482113
[Epoch 104; Iter    20/   40] train: loss: 0.0006780
[Epoch 104] ogbg-molclintox: 0.949004 val loss: 0.096583
[Epoch 104] ogbg-molclintox: 0.813620 test loss: 0.494220
[Epoch 105; Iter    10/   40] train: loss: 0.0008317
[Epoch 105; Iter    40/   40] train: loss: 0.0084887
[Epoch 105] ogbg-molclintox: 0.943722 val loss: 0.106723
[Epoch 105] ogbg-molclintox: 0.798705 test loss: 0.496932
[Epoch 106; Iter    30/   40] train: loss: 0.0012647
[Epoch 106] ogbg-molclintox: 0.947018 val loss: 0.098631
[Epoch 106] ogbg-molclintox: 0.798231 test loss: 0.539362
[Epoch 107; Iter    20/   40] train: loss: 0.0006877
[Epoch 107] ogbg-molclintox: 0.946431 val loss: 0.102950
[Epoch 107] ogbg-molclintox: 0.810748 test loss: 0.498407
[Epoch 108; Iter    10/   40] train: loss: 0.0004393
[Epoch 108; Iter    40/   40] train: loss: 0.0059422
[Epoch 108] ogbg-molclintox: 0.916800 val loss: 0.120571
[Epoch 108] ogbg-molclintox: 0.812597 test loss: 0.491752
[Epoch 109; Iter    30/   40] train: loss: 0.0006390
[Epoch 109] ogbg-molclintox: 0.915739 val loss: 0.121753
[Epoch 109] ogbg-molclintox: 0.804039 test loss: 0.482974
[Epoch 110; Iter    20/   40] train: loss: 0.0008463
[Epoch 110] ogbg-molclintox: 0.923955 val loss: 0.113539
[Epoch 110] ogbg-molclintox: 0.808675 test loss: 0.506135
[Epoch 111; Iter    10/   40] train: loss: 0.0007044
[Epoch 111; Iter    40/   40] train: loss: 0.0002122
[Epoch 111] ogbg-molclintox: 0.929462 val loss: 0.114702
[Epoch 111] ogbg-molclintox: 0.815757 test loss: 0.466303
[Epoch 112; Iter    30/   40] train: loss: 0.0003042
[Epoch 112] ogbg-molclintox: 0.935780 val loss: 0.107506
[Epoch 112] ogbg-molclintox: 0.817569 test loss: 0.524177
[Epoch 113; Iter    20/   40] train: loss: 0.0011354
[Epoch 113] ogbg-molclintox: 0.925965 val loss: 0.115858
[Epoch 113] ogbg-molclintox: 0.809986 test loss: 0.486482
[Epoch 114; Iter    10/   40] train: loss: 0.0006715
[Epoch 114; Iter    40/   40] train: loss: 0.0031735
[Epoch 114] ogbg-molclintox: 0.931472 val loss: 0.112262
[Epoch 114] ogbg-molclintox: 0.814633 test loss: 0.490319
[Epoch 115; Iter    30/   40] train: loss: 0.0022067
[Epoch 115] ogbg-molclintox: 0.934044 val loss: 0.116911
[Epoch 115] ogbg-molclintox: 0.817281 test loss: 0.469370
[Epoch 116; Iter    20/   40] train: loss: 0.0002814
[Epoch 116] ogbg-molclintox: 0.930885 val loss: 0.116654
[Epoch 116] ogbg-molclintox: 0.813146 test loss: 0.492560
[Epoch 117; Iter    10/   40] train: loss: 0.0004506
[Epoch 117; Iter    40/   40] train: loss: 0.0054395
[Epoch 117] ogbg-molclintox: 0.936754 val loss: 0.113852
[Epoch 117] ogbg-molclintox: 0.792583 test loss: 0.496392
[Epoch 118; Iter    30/   40] train: loss: 0.0001487
[Epoch 118] ogbg-molclintox: 0.938851 val loss: 0.110252
[Epoch 118] ogbg-molclintox: 0.797693 test loss: 0.487423
[Epoch 119; Iter    20/   40] train: loss: 0.0008172
[Epoch 119] ogbg-molclintox: 0.937565 val loss: 0.111472
[Epoch 119] ogbg-molclintox: 0.807636 test loss: 0.500413
[Epoch 120; Iter    10/   40] train: loss: 0.0030659
[Epoch 120; Iter    40/   40] train: loss: 0.0023672
[Epoch 120] ogbg-molclintox: 0.931721 val loss: 0.120307
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 76] ogbg-molclintox: 0.902603 val loss: 0.124614
[Epoch 76] ogbg-molclintox: 0.776372 test loss: 0.374457
[Epoch 77; Iter    20/   40] train: loss: 0.0011589
[Epoch 77] ogbg-molclintox: 0.892539 val loss: 0.223868
[Epoch 77] ogbg-molclintox: 0.765330 test loss: 0.488899
[Epoch 78; Iter    10/   40] train: loss: 0.0127283
[Epoch 78; Iter    40/   40] train: loss: 0.0231564
[Epoch 78] ogbg-molclintox: 0.887419 val loss: 0.236293
[Epoch 78] ogbg-molclintox: 0.767467 test loss: 0.493774
[Epoch 79; Iter    30/   40] train: loss: 0.0013885
[Epoch 79] ogbg-molclintox: 0.892276 val loss: 0.419616
[Epoch 79] ogbg-molclintox: 0.766992 test loss: 0.855438
[Epoch 80; Iter    20/   40] train: loss: 0.0214951
[Epoch 80] ogbg-molclintox: 0.880813 val loss: 0.373183
[Epoch 80] ogbg-molclintox: 0.773451 test loss: 0.737498
[Epoch 81; Iter    10/   40] train: loss: 0.0016536
[Epoch 81; Iter    40/   40] train: loss: 0.0081596
[Epoch 81] ogbg-molclintox: 0.847411 val loss: 0.344518
[Epoch 81] ogbg-molclintox: 0.741343 test loss: 1.526022
[Epoch 82; Iter    30/   40] train: loss: 0.0033104
[Epoch 82] ogbg-molclintox: 0.870537 val loss: 0.259749
[Epoch 82] ogbg-molclintox: 0.796247 test loss: 0.779147
[Epoch 83; Iter    20/   40] train: loss: 0.0037603
[Epoch 83] ogbg-molclintox: 0.885883 val loss: 0.223544
[Epoch 83] ogbg-molclintox: 0.788751 test loss: 0.407079
[Epoch 84; Iter    10/   40] train: loss: 0.0147849
[Epoch 84; Iter    40/   40] train: loss: 0.0099185
[Epoch 84] ogbg-molclintox: 0.898358 val loss: 0.158548
[Epoch 84] ogbg-molclintox: 0.817356 test loss: 0.310846
[Epoch 85; Iter    30/   40] train: loss: 0.0015822
[Epoch 85] ogbg-molclintox: 0.899058 val loss: 0.157639
[Epoch 85] ogbg-molclintox: 0.820691 test loss: 0.356974
[Epoch 86; Iter    20/   40] train: loss: 0.0009996
[Epoch 86] ogbg-molclintox: 0.887145 val loss: 0.214442
[Epoch 86] ogbg-molclintox: 0.816145 test loss: 0.508191
[Epoch 87; Iter    10/   40] train: loss: 0.0018233
[Epoch 87; Iter    40/   40] train: loss: 0.0305301
[Epoch 87] ogbg-molclintox: 0.876181 val loss: 0.319820
[Epoch 87] ogbg-molclintox: 0.802205 test loss: 0.752136
[Epoch 88; Iter    30/   40] train: loss: 0.0013747
[Epoch 88] ogbg-molclintox: 0.845440 val loss: 0.130378
[Epoch 88] ogbg-molclintox: 0.818831 test loss: 0.592871
[Epoch 89; Iter    20/   40] train: loss: 0.0025916
[Epoch 89] ogbg-molclintox: 0.858414 val loss: 0.163544
[Epoch 89] ogbg-molclintox: 0.827863 test loss: 1.503099
[Epoch 90; Iter    10/   40] train: loss: 0.0099766
[Epoch 90; Iter    40/   40] train: loss: 0.0010831
[Epoch 90] ogbg-molclintox: 0.882788 val loss: 0.152166
[Epoch 90] ogbg-molclintox: 0.838019 test loss: 0.534602
[Epoch 91; Iter    30/   40] train: loss: 0.0006900
[Epoch 91] ogbg-molclintox: 0.878817 val loss: 0.163582
[Epoch 91] ogbg-molclintox: 0.835483 test loss: 0.670261
[Epoch 92; Iter    20/   40] train: loss: 0.0016331
[Epoch 92] ogbg-molclintox: 0.886396 val loss: 0.191314
[Epoch 92] ogbg-molclintox: 0.839618 test loss: 0.807984
[Epoch 93; Iter    10/   40] train: loss: 0.0012465
[Epoch 93; Iter    40/   40] train: loss: 0.0043520
[Epoch 93] ogbg-molclintox: 0.863309 val loss: 0.149297
[Epoch 93] ogbg-molclintox: 0.829723 test loss: 0.415741
[Epoch 94; Iter    30/   40] train: loss: 0.0009657
[Epoch 94] ogbg-molclintox: 0.894275 val loss: 0.201466
[Epoch 94] ogbg-molclintox: 0.831934 test loss: 0.518095
[Epoch 95; Iter    20/   40] train: loss: 0.0087045
[Epoch 95] ogbg-molclintox: 0.914966 val loss: 0.272345
[Epoch 95] ogbg-molclintox: 0.775610 test loss: 6.367169
[Epoch 96; Iter    10/   40] train: loss: 0.0103371
[Epoch 96; Iter    40/   40] train: loss: 0.0629577
[Epoch 96] ogbg-molclintox: 0.955571 val loss: 0.243954
[Epoch 96] ogbg-molclintox: 0.803554 test loss: 0.648810
[Epoch 97; Iter    30/   40] train: loss: 0.0040550
[Epoch 97] ogbg-molclintox: 0.942861 val loss: 0.535088
[Epoch 97] ogbg-molclintox: 0.778598 test loss: 4.508138
[Epoch 98; Iter    20/   40] train: loss: 0.0057743
[Epoch 98] ogbg-molclintox: 0.922644 val loss: 0.726532
[Epoch 98] ogbg-molclintox: 0.768292 test loss: 3.134312
[Epoch 99; Iter    10/   40] train: loss: 0.0036966
[Epoch 99; Iter    40/   40] train: loss: 0.0023883
[Epoch 99] ogbg-molclintox: 0.923818 val loss: 0.967173
[Epoch 99] ogbg-molclintox: 0.750852 test loss: 3.013541
[Epoch 100; Iter    30/   40] train: loss: 0.0015579
[Epoch 100] ogbg-molclintox: 0.927989 val loss: 0.364470
[Epoch 100] ogbg-molclintox: 0.767691 test loss: 2.108887
[Epoch 101; Iter    20/   40] train: loss: 0.0014938
[Epoch 101] ogbg-molclintox: 0.927315 val loss: 0.577150
[Epoch 101] ogbg-molclintox: 0.765031 test loss: 2.719631
[Epoch 102; Iter    10/   40] train: loss: 0.0174359
[Epoch 102; Iter    40/   40] train: loss: 0.0093778
[Epoch 102] ogbg-molclintox: 0.931173 val loss: 0.436293
[Epoch 102] ogbg-molclintox: 0.779147 test loss: 1.809879
[Epoch 103; Iter    30/   40] train: loss: 0.0065463
[Epoch 103] ogbg-molclintox: 0.933159 val loss: 0.333646
[Epoch 103] ogbg-molclintox: 0.781508 test loss: 1.201466
[Epoch 104; Iter    20/   40] train: loss: 0.0064704
[Epoch 104] ogbg-molclintox: 0.928126 val loss: 0.323099
[Epoch 104] ogbg-molclintox: 0.788904 test loss: 1.564343
[Epoch 105; Iter    10/   40] train: loss: 0.0028166
[Epoch 105; Iter    40/   40] train: loss: 0.0021510
[Epoch 105] ogbg-molclintox: 0.925080 val loss: 0.283404
[Epoch 105] ogbg-molclintox: 0.794749 test loss: 1.599301
[Epoch 106; Iter    30/   40] train: loss: 0.0004985
[Epoch 106] ogbg-molclintox: 0.929188 val loss: 0.318399
[Epoch 106] ogbg-molclintox: 0.792613 test loss: 1.731073
[Epoch 107; Iter    20/   40] train: loss: 0.0018619
[Epoch 107] ogbg-molclintox: 0.934694 val loss: 0.451105
[Epoch 107] ogbg-molclintox: 0.778998 test loss: 1.937740
[Epoch 108; Iter    10/   40] train: loss: 0.0006479
[Epoch 108; Iter    40/   40] train: loss: 0.0042209
[Epoch 108] ogbg-molclintox: 0.926703 val loss: 0.306123
[Epoch 108] ogbg-molclintox: 0.784518 test loss: 2.872083
[Epoch 109; Iter    30/   40] train: loss: 0.0017817
[Epoch 109] ogbg-molclintox: 0.921084 val loss: 0.225528
[Epoch 109] ogbg-molclintox: 0.788153 test loss: 1.749146
[Epoch 110; Iter    20/   40] train: loss: 0.0071185
[Epoch 110] ogbg-molclintox: 0.911582 val loss: 0.298900
[Epoch 110] ogbg-molclintox: 0.786629 test loss: 2.650540
[Epoch 111; Iter    10/   40] train: loss: 0.0017824
[Epoch 111; Iter    40/   40] train: loss: 0.0494557
[Epoch 111] ogbg-molclintox: 0.923319 val loss: 0.185795
[Epoch 111] ogbg-molclintox: 0.786267 test loss: 1.412045
[Epoch 112; Iter    30/   40] train: loss: 0.0545593
[Epoch 112] ogbg-molclintox: 0.915627 val loss: 0.584942
[Epoch 112] ogbg-molclintox: 0.777123 test loss: 1.800602
[Epoch 113; Iter    20/   40] train: loss: 0.0003546
[Epoch 113] ogbg-molclintox: 0.910795 val loss: 0.373815
[Epoch 113] ogbg-molclintox: 0.763194 test loss: 1.722321
[Epoch 114; Iter    10/   40] train: loss: 0.0110171
[Epoch 114; Iter    40/   40] train: loss: 0.0004423
[Epoch 114] ogbg-molclintox: 0.918649 val loss: 0.382864
[Epoch 114] ogbg-molclintox: 0.776110 test loss: 1.757036
[Epoch 115; Iter    30/   40] train: loss: 0.0004631
[Epoch 115] ogbg-molclintox: 0.917837 val loss: 0.535783
[Epoch 115] ogbg-molclintox: 0.772663 test loss: 2.145629
[Epoch 116; Iter    20/   40] train: loss: 0.0029043
[Epoch 116] ogbg-molclintox: 0.929100 val loss: 0.341589
[Epoch 116] ogbg-molclintox: 0.778759 test loss: 1.971418
[Epoch 117; Iter    10/   40] train: loss: 0.0035833
[Epoch 117; Iter    40/   40] train: loss: 0.0004631
[Epoch 117] ogbg-molclintox: 0.931672 val loss: 0.308148
[Epoch 117] ogbg-molclintox: 0.773712 test loss: 1.444580
[Epoch 118; Iter    30/   40] train: loss: 0.0011196
[Epoch 118] ogbg-molclintox: 0.927202 val loss: 0.295702
[Epoch 118] ogbg-molclintox: 0.773025 test loss: 1.642342
[Epoch 119; Iter    20/   40] train: loss: 0.0012706
[Epoch 119] ogbg-molclintox: 0.928850 val loss: 0.288537
[Epoch 119] ogbg-molclintox: 0.762645 test loss: 1.504614
[Epoch 120; Iter    10/   40] train: loss: 0.0007721
[Epoch 120; Iter    40/   40] train: loss: 0.0006632
[Epoch 120] ogbg-molclintox: 0.907137 val loss: 0.204411
[Epoch 120] ogbg-molclintox: 0.801978 test loss: 0.485461
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 51.
Statistics on  val_best_checkpoint
mean_pred: 0.37912577390670776
std_pred: 6.457724094390869
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.7817285291567424
rocauc: 0.9557963163596966
ogbg-molclintox: 0.9557963163596966
OGBNanLabelBCEWithLogitsLoss: 0.09635590948164463
Statistics on  test
mean_pred: 0.2662549614906311
std_pred: 5.550947666168213
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6345243375581654
rocauc: 0.8357931046467174
ogbg-molclintox: 0.8357931046467174
OGBNanLabelBCEWithLogitsLoss: 0.2464395247399807
Statistics on  train
mean_pred: 0.18122895061969757
std_pred: 6.6374711990356445
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9965748219130655
rocauc: 0.9991395547407678
ogbg-molclintox: 0.9991395547407678
OGBNanLabelBCEWithLogitsLoss: 0.026345213287277146
[Epoch 76] ogbg-molclintox: 0.948431 val loss: 0.082984
[Epoch 76] ogbg-molclintox: 0.818678 test loss: 0.292769
[Epoch 77; Iter    20/   40] train: loss: 0.0034129
[Epoch 77] ogbg-molclintox: 0.986664 val loss: 0.052412
[Epoch 77] ogbg-molclintox: 0.822626 test loss: 0.301159
[Epoch 78; Iter    10/   40] train: loss: 0.0210368
[Epoch 78; Iter    40/   40] train: loss: 0.0091159
[Epoch 78] ogbg-molclintox: 0.984816 val loss: 0.064690
[Epoch 78] ogbg-molclintox: 0.787537 test loss: 0.360171
[Epoch 79; Iter    30/   40] train: loss: 0.0018725
[Epoch 79] ogbg-molclintox: 0.965787 val loss: 0.072909
[Epoch 79] ogbg-molclintox: 0.797241 test loss: 0.350931
[Epoch 80; Iter    20/   40] train: loss: 0.0643573
[Epoch 80] ogbg-molclintox: 0.980957 val loss: 0.081543
[Epoch 80] ogbg-molclintox: 0.812108 test loss: 0.301463
[Epoch 81; Iter    10/   40] train: loss: 0.0028259
[Epoch 81; Iter    40/   40] train: loss: 0.0121337
[Epoch 81] ogbg-molclintox: 0.983417 val loss: 0.067713
[Epoch 81] ogbg-molclintox: 0.786939 test loss: 0.347746
[Epoch 82; Iter    30/   40] train: loss: 0.0047454
[Epoch 82] ogbg-molclintox: 0.977886 val loss: 0.069581
[Epoch 82] ogbg-molclintox: 0.795841 test loss: 0.341640
[Epoch 83; Iter    20/   40] train: loss: 0.0035185
[Epoch 83] ogbg-molclintox: 0.990997 val loss: 0.053640
[Epoch 83] ogbg-molclintox: 0.808723 test loss: 0.377142
[Epoch 84; Iter    10/   40] train: loss: 0.0071226
[Epoch 84; Iter    40/   40] train: loss: 0.0125096
[Epoch 84] ogbg-molclintox: 0.965811 val loss: 0.095880
[Epoch 84] ogbg-molclintox: 0.802038 test loss: 0.351238
[Epoch 85; Iter    30/   40] train: loss: 0.0022068
[Epoch 85] ogbg-molclintox: 0.980708 val loss: 0.089694
[Epoch 85] ogbg-molclintox: 0.831669 test loss: 0.292998
[Epoch 86; Iter    20/   40] train: loss: 0.0010734
[Epoch 86] ogbg-molclintox: 0.961577 val loss: 0.086274
[Epoch 86] ogbg-molclintox: 0.797167 test loss: 0.356680
[Epoch 87; Iter    10/   40] train: loss: 0.0019628
[Epoch 87; Iter    40/   40] train: loss: 0.0037944
[Epoch 87] ogbg-molclintox: 0.943659 val loss: 0.101826
[Epoch 87] ogbg-molclintox: 0.804772 test loss: 0.319064
[Epoch 88; Iter    30/   40] train: loss: 0.0030333
[Epoch 88] ogbg-molclintox: 0.960154 val loss: 0.092449
[Epoch 88] ogbg-molclintox: 0.825573 test loss: 0.361730
[Epoch 89; Iter    20/   40] train: loss: 0.0016518
[Epoch 89] ogbg-molclintox: 0.954197 val loss: 0.097795
[Epoch 89] ogbg-molclintox: 0.791919 test loss: 0.315147
[Epoch 90; Iter    10/   40] train: loss: 0.0061232
[Epoch 90; Iter    40/   40] train: loss: 0.0018972
[Epoch 90] ogbg-molclintox: 0.943272 val loss: 0.097676
[Epoch 90] ogbg-molclintox: 0.824722 test loss: 0.209294
[Epoch 91; Iter    30/   40] train: loss: 0.0013741
[Epoch 91] ogbg-molclintox: 0.855188 val loss: 0.157496
[Epoch 91] ogbg-molclintox: 0.774295 test loss: 0.357594
[Epoch 92; Iter    20/   40] train: loss: 0.0030257
[Epoch 92] ogbg-molclintox: 0.929311 val loss: 0.099979
[Epoch 92] ogbg-molclintox: 0.838927 test loss: 0.267538
[Epoch 93; Iter    10/   40] train: loss: 0.0007199
[Epoch 93; Iter    40/   40] train: loss: 0.0045310
[Epoch 93] ogbg-molclintox: 0.920494 val loss: 0.144759
[Epoch 93] ogbg-molclintox: 0.827135 test loss: 0.335180
[Epoch 94; Iter    30/   40] train: loss: 0.0003664
[Epoch 94] ogbg-molclintox: 0.925937 val loss: 0.096009
[Epoch 94] ogbg-molclintox: 0.823026 test loss: 0.362536
[Epoch 95; Iter    20/   40] train: loss: 0.0149784
[Epoch 95] ogbg-molclintox: 0.976150 val loss: 0.086726
[Epoch 95] ogbg-molclintox: 0.841314 test loss: 0.318000
[Epoch 96; Iter    10/   40] train: loss: 0.0043023
[Epoch 96; Iter    40/   40] train: loss: 0.0036512
[Epoch 96] ogbg-molclintox: 0.970281 val loss: 0.098664
[Epoch 96] ogbg-molclintox: 0.815044 test loss: 0.349816
[Epoch 97; Iter    30/   40] train: loss: 0.0012873
[Epoch 97] ogbg-molclintox: 0.958207 val loss: 0.097740
[Epoch 97] ogbg-molclintox: 0.821001 test loss: 0.334893
[Epoch 98; Iter    20/   40] train: loss: 0.0013227
[Epoch 98] ogbg-molclintox: 0.951351 val loss: 0.104730
[Epoch 98] ogbg-molclintox: 0.835042 test loss: 0.314555
[Epoch 99; Iter    10/   40] train: loss: 0.0012901
[Epoch 99; Iter    40/   40] train: loss: 0.0021632
[Epoch 99] ogbg-molclintox: 0.957870 val loss: 0.100734
[Epoch 99] ogbg-molclintox: 0.816754 test loss: 0.369829
[Epoch 100; Iter    30/   40] train: loss: 0.0010467
[Epoch 100] ogbg-molclintox: 0.951038 val loss: 0.107986
[Epoch 100] ogbg-molclintox: 0.821177 test loss: 0.347919
[Epoch 101; Iter    20/   40] train: loss: 0.0007687
[Epoch 101] ogbg-molclintox: 0.963039 val loss: 0.097899
[Epoch 101] ogbg-molclintox: 0.814207 test loss: 0.377309
[Epoch 102; Iter    10/   40] train: loss: 0.0070507
[Epoch 102; Iter    40/   40] train: loss: 0.0054201
[Epoch 102] ogbg-molclintox: 0.929472 val loss: 0.122713
[Epoch 102] ogbg-molclintox: 0.817191 test loss: 0.330096
[Epoch 103; Iter    30/   40] train: loss: 0.0016721
[Epoch 103] ogbg-molclintox: 0.952300 val loss: 0.121530
[Epoch 103] ogbg-molclintox: 0.810408 test loss: 0.368710
[Epoch 104; Iter    20/   40] train: loss: 0.0043087
[Epoch 104] ogbg-molclintox: 0.934118 val loss: 0.114383
[Epoch 104] ogbg-molclintox: 0.828984 test loss: 0.346573
[Epoch 105; Iter    10/   40] train: loss: 0.0010761
[Epoch 105; Iter    40/   40] train: loss: 0.0031044
[Epoch 105] ogbg-molclintox: 0.950901 val loss: 0.102885
[Epoch 105] ogbg-molclintox: 0.823900 test loss: 0.361296
[Epoch 106; Iter    30/   40] train: loss: 0.0003418
[Epoch 106] ogbg-molclintox: 0.951376 val loss: 0.114552
[Epoch 106] ogbg-molclintox: 0.829447 test loss: 0.330311
[Epoch 107; Iter    20/   40] train: loss: 0.0008619
[Epoch 107] ogbg-molclintox: 0.931746 val loss: 0.123392
[Epoch 107] ogbg-molclintox: 0.823325 test loss: 0.352149
[Epoch 108; Iter    10/   40] train: loss: 0.0004814
[Epoch 108; Iter    40/   40] train: loss: 0.0032864
[Epoch 108] ogbg-molclintox: 0.947131 val loss: 0.123377
[Epoch 108] ogbg-molclintox: 0.822114 test loss: 0.357095
[Epoch 109; Iter    30/   40] train: loss: 0.0026693
[Epoch 109] ogbg-molclintox: 0.949415 val loss: 0.101030
[Epoch 109] ogbg-molclintox: 0.813269 test loss: 0.346373
[Epoch 110; Iter    20/   40] train: loss: 0.0020237
[Epoch 110] ogbg-molclintox: 0.952637 val loss: 0.104535
[Epoch 110] ogbg-molclintox: 0.808985 test loss: 0.376928
[Epoch 111; Iter    10/   40] train: loss: 0.0015717
[Epoch 111; Iter    40/   40] train: loss: 0.0338349
[Epoch 111] ogbg-molclintox: 0.953674 val loss: 0.113680
[Epoch 111] ogbg-molclintox: 0.806587 test loss: 0.365854
[Epoch 112; Iter    30/   40] train: loss: 0.0371584
[Epoch 112] ogbg-molclintox: 0.956583 val loss: 0.110570
[Epoch 112] ogbg-molclintox: 0.795531 test loss: 0.413791
[Epoch 113; Iter    20/   40] train: loss: 0.0003529
[Epoch 113] ogbg-molclintox: 0.936103 val loss: 0.118571
[Epoch 113] ogbg-molclintox: 0.800241 test loss: 0.330118
[Epoch 114; Iter    10/   40] train: loss: 0.0074360
[Epoch 114; Iter    40/   40] train: loss: 0.0015723
[Epoch 114] ogbg-molclintox: 0.942060 val loss: 0.102859
[Epoch 114] ogbg-molclintox: 0.803251 test loss: 0.339482
[Epoch 115; Iter    30/   40] train: loss: 0.0003410
[Epoch 115] ogbg-molclintox: 0.956408 val loss: 0.090767
[Epoch 115] ogbg-molclintox: 0.810647 test loss: 0.385951
[Epoch 116; Iter    20/   40] train: loss: 0.0017765
[Epoch 116] ogbg-molclintox: 0.955708 val loss: 0.093785
[Epoch 116] ogbg-molclintox: 0.810797 test loss: 0.353287
[Epoch 117; Iter    10/   40] train: loss: 0.0004914
[Epoch 117; Iter    40/   40] train: loss: 0.0000888
[Epoch 117] ogbg-molclintox: 0.951738 val loss: 0.096999
[Epoch 117] ogbg-molclintox: 0.808361 test loss: 0.331326
[Epoch 118; Iter    30/   40] train: loss: 0.0006858
[Epoch 118] ogbg-molclintox: 0.955009 val loss: 0.102936
[Epoch 118] ogbg-molclintox: 0.809598 test loss: 0.334525
[Epoch 119; Iter    20/   40] train: loss: 0.0037963
[Epoch 119] ogbg-molclintox: 0.965362 val loss: 0.106070
[Epoch 119] ogbg-molclintox: 0.785449 test loss: 0.363950
[Epoch 120; Iter    10/   40] train: loss: 0.0005678
[Epoch 120; Iter    40/   40] train: loss: 0.0005167
[Epoch 120] ogbg-molclintox: 0.947043 val loss: 0.100989
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 76] ogbg-molclintox: 0.765646 val loss: 0.680529
[Epoch 76] ogbg-molclintox: 0.775334 test loss: 0.455544
[Epoch 77; Iter    20/   40] train: loss: 0.0021007
[Epoch 77] ogbg-molclintox: 0.784126 val loss: 0.523277
[Epoch 77] ogbg-molclintox: 0.793147 test loss: 0.467054
[Epoch 78; Iter    10/   40] train: loss: 0.0017087
[Epoch 78; Iter    40/   40] train: loss: 0.0007420
[Epoch 78] ogbg-molclintox: 0.771964 val loss: 0.767176
[Epoch 78] ogbg-molclintox: 0.778281 test loss: 0.462763
[Epoch 79; Iter    30/   40] train: loss: 0.0148783
[Epoch 79] ogbg-molclintox: 0.778982 val loss: 0.377504
[Epoch 79] ogbg-molclintox: 0.782080 test loss: 0.504113
[Epoch 80; Iter    20/   40] train: loss: 0.0013463
[Epoch 80] ogbg-molclintox: 0.739673 val loss: 0.524471
[Epoch 80] ogbg-molclintox: 0.754222 test loss: 0.555853
[Epoch 81; Iter    10/   40] train: loss: 0.0078169
[Epoch 81; Iter    40/   40] train: loss: 0.0030524
[Epoch 81] ogbg-molclintox: 0.770478 val loss: 0.627818
[Epoch 81] ogbg-molclintox: 0.806239 test loss: 0.449103
[Epoch 82; Iter    30/   40] train: loss: 0.0081477
[Epoch 82] ogbg-molclintox: 0.766370 val loss: 0.603648
[Epoch 82] ogbg-molclintox: 0.774097 test loss: 0.656296
[Epoch 83; Iter    20/   40] train: loss: 0.0082693
[Epoch 83] ogbg-molclintox: 0.753283 val loss: 0.850789
[Epoch 83] ogbg-molclintox: 0.762342 test loss: 0.628131
[Epoch 84; Iter    10/   40] train: loss: 0.0015755
[Epoch 84; Iter    40/   40] train: loss: 0.0054362
[Epoch 84] ogbg-molclintox: 0.765807 val loss: 0.946686
[Epoch 84] ogbg-molclintox: 0.768913 test loss: 0.597024
[Epoch 85; Iter    30/   40] train: loss: 0.0025120
[Epoch 85] ogbg-molclintox: 0.762761 val loss: 1.037102
[Epoch 85] ogbg-molclintox: 0.761480 test loss: 0.609066
[Epoch 86; Iter    20/   40] train: loss: 0.0067912
[Epoch 86] ogbg-molclintox: 0.776571 val loss: 0.869854
[Epoch 86] ogbg-molclintox: 0.755410 test loss: 0.588365
[Epoch 87; Iter    10/   40] train: loss: 0.0029821
[Epoch 87; Iter    40/   40] train: loss: 0.0053054
[Epoch 87] ogbg-molclintox: 0.786048 val loss: 0.827791
[Epoch 87] ogbg-molclintox: 0.774960 test loss: 0.581393
[Epoch 88; Iter    30/   40] train: loss: 0.0108114
[Epoch 88] ogbg-molclintox: 0.780429 val loss: 0.307320
[Epoch 88] ogbg-molclintox: 0.789539 test loss: 0.547463
[Epoch 89; Iter    20/   40] train: loss: 0.0047441
[Epoch 89] ogbg-molclintox: 0.769441 val loss: 0.356379
[Epoch 89] ogbg-molclintox: 0.792411 test loss: 0.551673
[Epoch 90; Iter    10/   40] train: loss: 0.0029759
[Epoch 90; Iter    40/   40] train: loss: 0.0016905
[Epoch 90] ogbg-molclintox: 0.782801 val loss: 0.277802
[Epoch 90] ogbg-molclintox: 0.787802 test loss: 0.580523
[Epoch 91; Iter    30/   40] train: loss: 0.0037101
[Epoch 91] ogbg-molclintox: 0.786909 val loss: 0.325894
[Epoch 91] ogbg-molclintox: 0.770564 test loss: 0.622434
[Epoch 92; Iter    20/   40] train: loss: 0.0026963
[Epoch 92] ogbg-molclintox: 0.800382 val loss: 0.293134
[Epoch 92] ogbg-molclintox: 0.790151 test loss: 0.552584
[Epoch 93; Iter    10/   40] train: loss: 0.0014585
[Epoch 93; Iter    40/   40] train: loss: 0.0064648
[Epoch 93] ogbg-molclintox: 0.804403 val loss: 0.292030
[Epoch 93] ogbg-molclintox: 0.784556 test loss: 0.594170
[Epoch 94; Iter    30/   40] train: loss: 0.0019829
[Epoch 94] ogbg-molclintox: 0.779579 val loss: 0.295623
[Epoch 94] ogbg-molclintox: 0.782169 test loss: 0.594107
[Epoch 95; Iter    20/   40] train: loss: 0.0010718
[Epoch 95] ogbg-molclintox: 0.792641 val loss: 0.282684
[Epoch 95] ogbg-molclintox: 0.795474 test loss: 0.559410
[Epoch 96; Iter    10/   40] train: loss: 0.0060438
[Epoch 96; Iter    40/   40] train: loss: 0.0045218
[Epoch 96] ogbg-molclintox: 0.802368 val loss: 0.260222
[Epoch 96] ogbg-molclintox: 0.797323 test loss: 0.596311
[Epoch 97; Iter    30/   40] train: loss: 0.0010061
[Epoch 97] ogbg-molclintox: 0.791017 val loss: 0.273110
[Epoch 97] ogbg-molclintox: 0.786767 test loss: 0.564969
[Epoch 98; Iter    20/   40] train: loss: 0.0017401
[Epoch 98] ogbg-molclintox: 0.801581 val loss: 0.278401
[Epoch 98] ogbg-molclintox: 0.762308 test loss: 0.607788
[Epoch 99; Iter    10/   40] train: loss: 0.0014463
[Epoch 99; Iter    40/   40] train: loss: 0.0058795
[Epoch 99] ogbg-molclintox: 0.791404 val loss: 0.292195
[Epoch 99] ogbg-molclintox: 0.763571 test loss: 0.586833
[Epoch 100; Iter    30/   40] train: loss: 0.0062762
[Epoch 100] ogbg-molclintox: 0.789419 val loss: 0.273275
[Epoch 100] ogbg-molclintox: 0.762084 test loss: 0.590693
[Epoch 101; Iter    20/   40] train: loss: 0.0016667
[Epoch 101] ogbg-molclintox: 0.790842 val loss: 0.286941
[Epoch 101] ogbg-molclintox: 0.758424 test loss: 0.616404
[Epoch 102; Iter    10/   40] train: loss: 0.0005852
[Epoch 102; Iter    40/   40] train: loss: 0.0038317
[Epoch 102] ogbg-molclintox: 0.776669 val loss: 0.297784
[Epoch 102] ogbg-molclintox: 0.762570 test loss: 0.633971
[Epoch 103; Iter    30/   40] train: loss: 0.0011688
[Epoch 103] ogbg-molclintox: 0.792328 val loss: 0.266228
[Epoch 103] ogbg-molclintox: 0.764706 test loss: 0.634152
[Epoch 104; Iter    20/   40] train: loss: 0.0078843
[Epoch 104] ogbg-molclintox: 0.780528 val loss: 0.285683
[Epoch 104] ogbg-molclintox: 0.768304 test loss: 0.604912
[Epoch 105; Iter    10/   40] train: loss: 0.0041154
[Epoch 105; Iter    40/   40] train: loss: 0.0038837
[Epoch 105] ogbg-molclintox: 0.772224 val loss: 0.315708
[Epoch 105] ogbg-molclintox: 0.767766 test loss: 0.625739
[Epoch 106; Iter    30/   40] train: loss: 0.0017223
[Epoch 106] ogbg-molclintox: 0.792553 val loss: 0.298630
[Epoch 106] ogbg-molclintox: 0.757397 test loss: 0.663090
[Epoch 107; Iter    20/   40] train: loss: 0.0005737
[Epoch 107] ogbg-molclintox: 0.786460 val loss: 0.314768
[Epoch 107] ogbg-molclintox: 0.740995 test loss: 0.651534
[Epoch 108; Iter    10/   40] train: loss: 0.0012337
[Epoch 108; Iter    40/   40] train: loss: 0.0027207
[Epoch 108] ogbg-molclintox: 0.779916 val loss: 0.318974
[Epoch 108] ogbg-molclintox: 0.741929 test loss: 0.695836
[Epoch 109; Iter    30/   40] train: loss: 0.0007831
[Epoch 109] ogbg-molclintox: 0.784137 val loss: 0.289983
[Epoch 109] ogbg-molclintox: 0.748951 test loss: 0.694483
[Epoch 110; Iter    20/   40] train: loss: 0.0008435
[Epoch 110] ogbg-molclintox: 0.787633 val loss: 0.275614
[Epoch 110] ogbg-molclintox: 0.750800 test loss: 0.683473
[Epoch 111; Iter    10/   40] train: loss: 0.0015207
[Epoch 111; Iter    40/   40] train: loss: 0.0017883
[Epoch 111] ogbg-molclintox: 0.780616 val loss: 0.288970
[Epoch 111] ogbg-molclintox: 0.754973 test loss: 0.656036
[Epoch 112; Iter    30/   40] train: loss: 0.0052607
[Epoch 112] ogbg-molclintox: 0.778518 val loss: 0.296020
[Epoch 112] ogbg-molclintox: 0.746815 test loss: 0.656180
[Epoch 113; Iter    20/   40] train: loss: 0.0262493
[Epoch 113] ogbg-molclintox: 0.777232 val loss: 0.306061
[Epoch 113] ogbg-molclintox: 0.742967 test loss: 0.701139
[Epoch 114; Iter    10/   40] train: loss: 0.0006133
[Epoch 114; Iter    40/   40] train: loss: 0.0001940
[Epoch 114] ogbg-molclintox: 0.771637 val loss: 0.330810
[Epoch 114] ogbg-molclintox: 0.735710 test loss: 0.677393
[Epoch 115; Iter    30/   40] train: loss: 0.0004032
[Epoch 115] ogbg-molclintox: 0.774684 val loss: 0.295274
[Epoch 115] ogbg-molclintox: 0.736621 test loss: 0.704123
[Epoch 116; Iter    20/   40] train: loss: 0.0029623
[Epoch 116] ogbg-molclintox: 0.768953 val loss: 0.314579
[Epoch 116] ogbg-molclintox: 0.737559 test loss: 0.671849
[Epoch 117; Iter    10/   40] train: loss: 0.0014163
[Epoch 117; Iter    40/   40] train: loss: 0.0002293
[Epoch 117] ogbg-molclintox: 0.759862 val loss: 0.321169
[Epoch 117] ogbg-molclintox: 0.732598 test loss: 0.712731
[Epoch 118; Iter    30/   40] train: loss: 0.0014168
[Epoch 118] ogbg-molclintox: 0.765593 val loss: 0.314817
[Epoch 118] ogbg-molclintox: 0.741369 test loss: 0.681938
[Epoch 119; Iter    20/   40] train: loss: 0.0011190
[Epoch 119] ogbg-molclintox: 0.762996 val loss: 0.308678
[Epoch 119] ogbg-molclintox: 0.748088 test loss: 0.676715
[Epoch 120; Iter    10/   40] train: loss: 0.0003136
[Epoch 120; Iter    40/   40] train: loss: 0.0003437
[Epoch 120] ogbg-molclintox: 0.755216 val loss: 0.308129
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.784437 test loss: 0.355703
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 56.
Statistics on  val_best_checkpoint
mean_pred: 0.19254715740680695
std_pred: 6.505613803863525
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.9791423853923853
rocauc: 0.9981269903805114
ogbg-molclintox: 0.9981269903805114
OGBNanLabelBCEWithLogitsLoss: 0.04221692550927401
Statistics on  test
mean_pred: 0.18186770379543304
std_pred: 5.591775894165039
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.7005031764458903
rocauc: 0.8509583637437876
ogbg-molclintox: 0.8509583637437876
OGBNanLabelBCEWithLogitsLoss: 0.26676038056612017
Statistics on  train
mean_pred: 0.13234013319015503
std_pred: 6.625617027282715
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9935750363416994
rocauc: 0.9988621702306659
ogbg-molclintox: 0.9988621702306659
OGBNanLabelBCEWithLogitsLoss: 0.025363112124614416
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 76] ogbg-molclintox: 0.924827 val loss: 0.124017
[Epoch 76] ogbg-molclintox: 0.852707 test loss: 0.193457
[Epoch 77; Iter    20/   40] train: loss: 0.0020347
[Epoch 77] ogbg-molclintox: 0.974614 val loss: 0.136956
[Epoch 77] ogbg-molclintox: 0.860950 test loss: 0.245791
[Epoch 78; Iter    10/   40] train: loss: 0.0014342
[Epoch 78; Iter    40/   40] train: loss: 0.0013926
[Epoch 78] ogbg-molclintox: 0.970643 val loss: 0.129690
[Epoch 78] ogbg-molclintox: 0.891195 test loss: 0.206248
[Epoch 79; Iter    30/   40] train: loss: 0.0128494
[Epoch 79] ogbg-molclintox: 0.973328 val loss: 0.134001
[Epoch 79] ogbg-molclintox: 0.874517 test loss: 0.194374
[Epoch 80; Iter    20/   40] train: loss: 0.0012659
[Epoch 80] ogbg-molclintox: 0.969220 val loss: 0.157430
[Epoch 80] ogbg-molclintox: 0.843350 test loss: 0.202357
[Epoch 81; Iter    10/   40] train: loss: 0.0024504
[Epoch 81; Iter    40/   40] train: loss: 0.0021961
[Epoch 81] ogbg-molclintox: 0.964687 val loss: 0.164591
[Epoch 81] ogbg-molclintox: 0.813057 test loss: 0.198512
[Epoch 82; Iter    30/   40] train: loss: 0.0033256
[Epoch 82] ogbg-molclintox: 0.973103 val loss: 0.184448
[Epoch 82] ogbg-molclintox: 0.845124 test loss: 0.230308
[Epoch 83; Iter    20/   40] train: loss: 0.0106610
[Epoch 83] ogbg-molclintox: 0.973441 val loss: 0.172196
[Epoch 83] ogbg-molclintox: 0.843462 test loss: 0.203563
[Epoch 84; Iter    10/   40] train: loss: 0.0016575
[Epoch 84; Iter    40/   40] train: loss: 0.0102592
[Epoch 84] ogbg-molclintox: 0.970056 val loss: 0.184414
[Epoch 84] ogbg-molclintox: 0.847697 test loss: 0.220200
[Epoch 85; Iter    30/   40] train: loss: 0.0030175
[Epoch 85] ogbg-molclintox: 0.979783 val loss: 0.080514
[Epoch 85] ogbg-molclintox: 0.861488 test loss: 0.169180
[Epoch 86; Iter    20/   40] train: loss: 0.0086517
[Epoch 86] ogbg-molclintox: 0.987138 val loss: 0.076985
[Epoch 86] ogbg-molclintox: 0.880863 test loss: 0.163792
[Epoch 87; Iter    10/   40] train: loss: 0.0017167
[Epoch 87; Iter    40/   40] train: loss: 0.0042958
[Epoch 87] ogbg-molclintox: 0.980820 val loss: 0.113489
[Epoch 87] ogbg-molclintox: 0.856741 test loss: 0.181127
[Epoch 88; Iter    30/   40] train: loss: 0.0048930
[Epoch 88] ogbg-molclintox: 0.973216 val loss: 0.120865
[Epoch 88] ogbg-molclintox: 0.854780 test loss: 0.206150
[Epoch 89; Iter    20/   40] train: loss: 0.0032174
[Epoch 89] ogbg-molclintox: 0.980346 val loss: 0.099292
[Epoch 89] ogbg-molclintox: 0.837216 test loss: 0.237542
[Epoch 90; Iter    10/   40] train: loss: 0.0037261
[Epoch 90; Iter    40/   40] train: loss: 0.0013406
[Epoch 90] ogbg-molclintox: 0.972291 val loss: 0.154897
[Epoch 90] ogbg-molclintox: 0.799752 test loss: 0.252769
[Epoch 91; Iter    30/   40] train: loss: 0.0022379
[Epoch 91] ogbg-molclintox: 0.969832 val loss: 0.151142
[Epoch 91] ogbg-molclintox: 0.836891 test loss: 0.204175
[Epoch 92; Iter    20/   40] train: loss: 0.0024536
[Epoch 92] ogbg-molclintox: 0.973465 val loss: 0.117734
[Epoch 92] ogbg-molclintox: 0.841000 test loss: 0.189321
[Epoch 93; Iter    10/   40] train: loss: 0.0013349
[Epoch 93; Iter    40/   40] train: loss: 0.0058628
[Epoch 93] ogbg-molclintox: 0.974976 val loss: 0.150270
[Epoch 93] ogbg-molclintox: 0.837190 test loss: 0.251734
[Epoch 94; Iter    30/   40] train: loss: 0.0013372
[Epoch 94] ogbg-molclintox: 0.965049 val loss: 0.253326
[Epoch 94] ogbg-molclintox: 0.836791 test loss: 0.265113
[Epoch 95; Iter    20/   40] train: loss: 0.0009287
[Epoch 95] ogbg-molclintox: 0.961053 val loss: 0.198963
[Epoch 95] ogbg-molclintox: 0.843286 test loss: 0.272020
[Epoch 96; Iter    10/   40] train: loss: 0.0017590
[Epoch 96; Iter    40/   40] train: loss: 0.0044948
[Epoch 96] ogbg-molclintox: 0.968071 val loss: 0.223014
[Epoch 96] ogbg-molclintox: 0.851481 test loss: 0.331684
[Epoch 97; Iter    30/   40] train: loss: 0.0010064
[Epoch 97] ogbg-molclintox: 0.966672 val loss: 0.158353
[Epoch 97] ogbg-molclintox: 0.863561 test loss: 0.241334
[Epoch 98; Iter    20/   40] train: loss: 0.0009445
[Epoch 98] ogbg-molclintox: 0.961190 val loss: 0.210735
[Epoch 98] ogbg-molclintox: 0.843824 test loss: 0.330025
[Epoch 99; Iter    10/   40] train: loss: 0.0019806
[Epoch 99; Iter    40/   40] train: loss: 0.0109941
[Epoch 99] ogbg-molclintox: 0.966560 val loss: 0.162035
[Epoch 99] ogbg-molclintox: 0.861574 test loss: 0.271873
[Epoch 100; Iter    30/   40] train: loss: 0.0103097
[Epoch 100] ogbg-molclintox: 0.971705 val loss: 0.136341
[Epoch 100] ogbg-molclintox: 0.858676 test loss: 0.255211
[Epoch 101; Iter    20/   40] train: loss: 0.0013421
[Epoch 101] ogbg-molclintox: 0.971817 val loss: 0.162694
[Epoch 101] ogbg-molclintox: 0.863173 test loss: 0.276222
[Epoch 102; Iter    10/   40] train: loss: 0.0005168
[Epoch 102; Iter    40/   40] train: loss: 0.0036365
[Epoch 102] ogbg-molclintox: 0.965973 val loss: 0.168493
[Epoch 102] ogbg-molclintox: 0.859438 test loss: 0.285285
[Epoch 103; Iter    30/   40] train: loss: 0.0010378
[Epoch 103] ogbg-molclintox: 0.961890 val loss: 0.166147
[Epoch 103] ogbg-molclintox: 0.855590 test loss: 0.270175
[Epoch 104; Iter    20/   40] train: loss: 0.0032863
[Epoch 104] ogbg-molclintox: 0.967621 val loss: 0.137817
[Epoch 104] ogbg-molclintox: 0.858564 test loss: 0.265796
[Epoch 105; Iter    10/   40] train: loss: 0.0044258
[Epoch 105; Iter    40/   40] train: loss: 0.0032546
[Epoch 105] ogbg-molclintox: 0.967034 val loss: 0.161964
[Epoch 105] ogbg-molclintox: 0.857615 test loss: 0.286489
[Epoch 106; Iter    30/   40] train: loss: 0.0007971
[Epoch 106] ogbg-molclintox: 0.967846 val loss: 0.151559
[Epoch 106] ogbg-molclintox: 0.855254 test loss: 0.242736
[Epoch 107; Iter    20/   40] train: loss: 0.0009157
[Epoch 107] ogbg-molclintox: 0.968321 val loss: 0.132263
[Epoch 107] ogbg-molclintox: 0.858627 test loss: 0.234589
[Epoch 108; Iter    10/   40] train: loss: 0.0014378
[Epoch 108; Iter    40/   40] train: loss: 0.0227206
[Epoch 108] ogbg-molclintox: 0.969020 val loss: 0.182475
[Epoch 108] ogbg-molclintox: 0.848934 test loss: 0.238949
[Epoch 109; Iter    30/   40] train: loss: 0.0006847
[Epoch 109] ogbg-molclintox: 0.961978 val loss: 0.148976
[Epoch 109] ogbg-molclintox: 0.819000 test loss: 0.236665
[Epoch 110; Iter    20/   40] train: loss: 0.0031513
[Epoch 110] ogbg-molclintox: 0.960354 val loss: 0.134920
[Epoch 110] ogbg-molclintox: 0.827172 test loss: 0.292888
[Epoch 111; Iter    10/   40] train: loss: 0.0008222
[Epoch 111; Iter    40/   40] train: loss: 0.0043918
[Epoch 111] ogbg-molclintox: 0.957669 val loss: 0.156598
[Epoch 111] ogbg-molclintox: 0.831307 test loss: 0.254193
[Epoch 112; Iter    30/   40] train: loss: 0.0057490
[Epoch 112] ogbg-molclintox: 0.964575 val loss: 0.109966
[Epoch 112] ogbg-molclintox: 0.834243 test loss: 0.279858
[Epoch 113; Iter    20/   40] train: loss: 0.0226647
[Epoch 113] ogbg-molclintox: 0.972404 val loss: 0.140728
[Epoch 113] ogbg-molclintox: 0.853005 test loss: 0.280737
[Epoch 114; Iter    10/   40] train: loss: 0.0007624
[Epoch 114; Iter    40/   40] train: loss: 0.0004711
[Epoch 114] ogbg-molclintox: 0.971230 val loss: 0.110794
[Epoch 114] ogbg-molclintox: 0.845897 test loss: 0.239100
[Epoch 115; Iter    30/   40] train: loss: 0.0003179
[Epoch 115] ogbg-molclintox: 0.967846 val loss: 0.141711
[Epoch 115] ogbg-molclintox: 0.852068 test loss: 0.266301
[Epoch 116; Iter    20/   40] train: loss: 0.0038684
[Epoch 116] ogbg-molclintox: 0.972991 val loss: 0.098122
[Epoch 116] ogbg-molclintox: 0.852793 test loss: 0.224382
[Epoch 117; Iter    10/   40] train: loss: 0.0007358
[Epoch 117; Iter    40/   40] train: loss: 0.0002838
[Epoch 117] ogbg-molclintox: 0.968545 val loss: 0.115893
[Epoch 117] ogbg-molclintox: 0.851844 test loss: 0.268122
[Epoch 118; Iter    30/   40] train: loss: 0.0009635
[Epoch 118] ogbg-molclintox: 0.962002 val loss: 0.137289
[Epoch 118] ogbg-molclintox: 0.839065 test loss: 0.236812
[Epoch 119; Iter    20/   40] train: loss: 0.0008191
[Epoch 119] ogbg-molclintox: 0.965973 val loss: 0.163714
[Epoch 119] ogbg-molclintox: 0.856341 test loss: 0.261891
[Epoch 120; Iter    10/   40] train: loss: 0.0004517
[Epoch 120; Iter    40/   40] train: loss: 0.0002479
[Epoch 120] ogbg-molclintox: 0.962589 val loss: 0.165719
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.741906 test loss: 0.703547
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 45.
Statistics on  val_best_checkpoint
mean_pred: 0.2579227089881897
std_pred: 5.774201393127441
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.774884730300441
rocauc: 0.9297358744541844
ogbg-molclintox: 0.9297358744541844
OGBNanLabelBCEWithLogitsLoss: 0.09979386439081281
Statistics on  test
mean_pred: 0.21846970915794373
std_pred: 5.751258850097656
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6233574218213088
rocauc: 0.8433757341952525
ogbg-molclintox: 0.8433757341952525
OGBNanLabelBCEWithLogitsLoss: 0.30706664025783537
Statistics on  train
mean_pred: 0.16473349928855896
std_pred: 4.955439567565918
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9483407569653394
rocauc: 0.9866302288033083
ogbg-molclintox: 0.9866302288033083
OGBNanLabelBCEWithLogitsLoss: 0.10760649731382728
[Epoch 120] ogbg-molclintox: 0.774261 test loss: 1.203354
[Epoch 121; Iter    30/   40] train: loss: 0.0011556
[Epoch 121] ogbg-molclintox: 0.920747 val loss: 0.295154
[Epoch 121] ogbg-molclintox: 0.776323 test loss: 1.671597
[Epoch 122; Iter    20/   40] train: loss: 0.0020165
[Epoch 122] ogbg-molclintox: 0.914404 val loss: 0.251465
[Epoch 122] ogbg-molclintox: 0.772439 test loss: 1.979638
[Epoch 123; Iter    10/   40] train: loss: 0.0020283
[Epoch 123; Iter    40/   40] train: loss: 0.0004592
[Epoch 123] ogbg-molclintox: 0.918737 val loss: 0.228476
[Epoch 123] ogbg-molclintox: 0.770814 test loss: 1.816345
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 123 epochs. Best model checkpoint was in epoch 63.
Statistics on  val_best_checkpoint
mean_pred: -0.10929584503173828
std_pred: 8.467524528503418
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.848067961699398
rocauc: 0.9730539413638005
ogbg-molclintox: 0.9730539413638005
OGBNanLabelBCEWithLogitsLoss: 0.07683654655702413
Statistics on  test
mean_pred: -0.14236140251159668
std_pred: 7.2686991691589355
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6319591704379006
rocauc: 0.8472117957807667
ogbg-molclintox: 0.8472117957807667
OGBNanLabelBCEWithLogitsLoss: 0.2366040162742138
Statistics on  train
mean_pred: 0.06932429224252701
std_pred: 10.06534481048584
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.999967522548725
rocauc: 0.9995345140781108
ogbg-molclintox: 0.9995345140781108
OGBNanLabelBCEWithLogitsLoss: 0.018497841348289513
[Epoch 120] ogbg-molclintox: 0.841463 test loss: 0.242710
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 38.
Statistics on  val_best_checkpoint
mean_pred: 0.15770474076271057
std_pred: 3.8660027980804443
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.9791184390322321
rocauc: 0.9974276896812109
ogbg-molclintox: 0.9974276896812109
OGBNanLabelBCEWithLogitsLoss: 0.07779569327831268
Statistics on  test
mean_pred: 0.11435550451278687
std_pred: 3.613936185836792
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6547761990380448
rocauc: 0.8128584089250339
ogbg-molclintox: 0.8128584089250339
OGBNanLabelBCEWithLogitsLoss: 0.27701104432344437
Statistics on  train
mean_pred: 0.12070129811763763
std_pred: 3.521267890930176
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9058861965716412
rocauc: 0.9834471280821373
ogbg-molclintox: 0.9834471280821373
OGBNanLabelBCEWithLogitsLoss: 0.12697313632816076
[Epoch 76] ogbg-molclintox: 0.967597 val loss: 0.085884
[Epoch 76] ogbg-molclintox: 0.818069 test loss: 0.351620
[Epoch 77; Iter    20/   40] train: loss: 0.0063370
[Epoch 77] ogbg-molclintox: 0.975788 val loss: 0.090916
[Epoch 77] ogbg-molclintox: 0.830224 test loss: 0.392808
[Epoch 78; Iter    10/   40] train: loss: 0.0025764
[Epoch 78; Iter    40/   40] train: loss: 0.0006024
[Epoch 78] ogbg-molclintox: 0.973915 val loss: 0.087433
[Epoch 78] ogbg-molclintox: 0.818469 test loss: 0.423786
[Epoch 79; Iter    30/   40] train: loss: 0.0229955
[Epoch 79] ogbg-molclintox: 0.979896 val loss: 0.099627
[Epoch 79] ogbg-molclintox: 0.843477 test loss: 0.507102
[Epoch 80; Iter    20/   40] train: loss: 0.0011490
[Epoch 80] ogbg-molclintox: 0.970868 val loss: 0.081911
[Epoch 80] ogbg-molclintox: 0.807875 test loss: 0.375305
[Epoch 81; Iter    10/   40] train: loss: 0.0072342
[Epoch 81; Iter    40/   40] train: loss: 0.0013297
[Epoch 81] ogbg-molclintox: 0.978248 val loss: 0.130963
[Epoch 81] ogbg-molclintox: 0.810412 test loss: 0.534179
[Epoch 82; Iter    30/   40] train: loss: 0.0030487
[Epoch 82] ogbg-molclintox: 0.979284 val loss: 0.109083
[Epoch 82] ogbg-molclintox: 0.795971 test loss: 0.476950
[Epoch 83; Iter    20/   40] train: loss: 0.0090069
[Epoch 83] ogbg-molclintox: 0.980346 val loss: 0.084694
[Epoch 83] ogbg-molclintox: 0.833933 test loss: 0.421567
[Epoch 84; Iter    10/   40] train: loss: 0.0018281
[Epoch 84; Iter    40/   40] train: loss: 0.0063744
[Epoch 84] ogbg-molclintox: 0.975900 val loss: 0.089181
[Epoch 84] ogbg-molclintox: 0.850372 test loss: 0.380362
[Epoch 85; Iter    30/   40] train: loss: 0.0014611
[Epoch 85] ogbg-molclintox: 0.979896 val loss: 0.081213
[Epoch 85] ogbg-molclintox: 0.849935 test loss: 0.364669
[Epoch 86; Iter    20/   40] train: loss: 0.0032318
[Epoch 86] ogbg-molclintox: 0.972404 val loss: 0.089494
[Epoch 86] ogbg-molclintox: 0.853707 test loss: 0.393232
[Epoch 87; Iter    10/   40] train: loss: 0.0026374
[Epoch 87; Iter    40/   40] train: loss: 0.0054725
[Epoch 87] ogbg-molclintox: 0.968521 val loss: 0.104489
[Epoch 87] ogbg-molclintox: 0.825289 test loss: 0.468220
[Epoch 88; Iter    30/   40] train: loss: 0.0111937
[Epoch 88] ogbg-molclintox: 0.968208 val loss: 0.100401
[Epoch 88] ogbg-molclintox: 0.847350 test loss: 0.350944
[Epoch 89; Iter    20/   40] train: loss: 0.0061560
[Epoch 89] ogbg-molclintox: 0.971093 val loss: 0.090610
[Epoch 89] ogbg-molclintox: 0.794809 test loss: 0.381509
[Epoch 90; Iter    10/   40] train: loss: 0.0035449
[Epoch 90; Iter    40/   40] train: loss: 0.0014163
[Epoch 90] ogbg-molclintox: 0.963201 val loss: 0.098080
[Epoch 90] ogbg-molclintox: 0.832648 test loss: 0.385082
[Epoch 91; Iter    30/   40] train: loss: 0.0027086
[Epoch 91] ogbg-molclintox: 0.971817 val loss: 0.093440
[Epoch 91] ogbg-molclintox: 0.810725 test loss: 0.418431
[Epoch 92; Iter    20/   40] train: loss: 0.0050811
[Epoch 92] ogbg-molclintox: 0.966198 val loss: 0.097918
[Epoch 92] ogbg-molclintox: 0.818420 test loss: 0.379338
[Epoch 93; Iter    10/   40] train: loss: 0.0016634
[Epoch 93; Iter    40/   40] train: loss: 0.0166034
[Epoch 93] ogbg-molclintox: 0.966086 val loss: 0.097629
[Epoch 93] ogbg-molclintox: 0.786790 test loss: 0.405858
[Epoch 94; Iter    30/   40] train: loss: 0.0033408
[Epoch 94] ogbg-molclintox: 0.974140 val loss: 0.103124
[Epoch 94] ogbg-molclintox: 0.803580 test loss: 0.431285
[Epoch 95; Iter    20/   40] train: loss: 0.0010886
[Epoch 95] ogbg-molclintox: 0.975788 val loss: 0.107344
[Epoch 95] ogbg-molclintox: 0.823642 test loss: 0.441058
[Epoch 96; Iter    10/   40] train: loss: 0.0037970
[Epoch 96; Iter    40/   40] train: loss: 0.0029867
[Epoch 96] ogbg-molclintox: 0.974252 val loss: 0.130369
[Epoch 96] ogbg-molclintox: 0.810038 test loss: 0.512835
[Epoch 97; Iter    30/   40] train: loss: 0.0013597
[Epoch 97] ogbg-molclintox: 0.972154 val loss: 0.123102
[Epoch 97] ogbg-molclintox: 0.815809 test loss: 0.509314
[Epoch 98; Iter    20/   40] train: loss: 0.0010461
[Epoch 98] ogbg-molclintox: 0.969108 val loss: 0.144371
[Epoch 98] ogbg-molclintox: 0.803767 test loss: 0.514939
[Epoch 99; Iter    10/   40] train: loss: 0.0023987
[Epoch 99; Iter    40/   40] train: loss: 0.0090310
[Epoch 99] ogbg-molclintox: 0.969470 val loss: 0.123612
[Epoch 99] ogbg-molclintox: 0.810987 test loss: 0.464043
[Epoch 100; Iter    30/   40] train: loss: 0.0124049
[Epoch 100] ogbg-molclintox: 0.966198 val loss: 0.116355
[Epoch 100] ogbg-molclintox: 0.806127 test loss: 0.447389
[Epoch 101; Iter    20/   40] train: loss: 0.0023879
[Epoch 101] ogbg-molclintox: 0.966785 val loss: 0.124647
[Epoch 101] ogbg-molclintox: 0.808626 test loss: 0.463362
[Epoch 102; Iter    10/   40] train: loss: 0.0019885
[Epoch 102; Iter    40/   40] train: loss: 0.0027356
[Epoch 102] ogbg-molclintox: 0.966785 val loss: 0.138422
[Epoch 102] ogbg-molclintox: 0.805903 test loss: 0.498849
[Epoch 103; Iter    30/   40] train: loss: 0.0010201
[Epoch 103] ogbg-molclintox: 0.968770 val loss: 0.174343
[Epoch 103] ogbg-molclintox: 0.807464 test loss: 0.581409
[Epoch 104; Iter    20/   40] train: loss: 0.0150474
[Epoch 104] ogbg-molclintox: 0.968658 val loss: 0.154297
[Epoch 104] ogbg-molclintox: 0.811413 test loss: 0.538624
[Epoch 105; Iter    10/   40] train: loss: 0.0027679
[Epoch 105; Iter    40/   40] train: loss: 0.0107735
[Epoch 105] ogbg-molclintox: 0.967846 val loss: 0.186707
[Epoch 105] ogbg-molclintox: 0.801619 test loss: 0.653532
[Epoch 106; Iter    30/   40] train: loss: 0.0053946
[Epoch 106] ogbg-molclintox: 0.965611 val loss: 0.154683
[Epoch 106] ogbg-molclintox: 0.798608 test loss: 0.562308
[Epoch 107; Iter    20/   40] train: loss: 0.0013559
[Epoch 107] ogbg-molclintox: 0.971005 val loss: 0.118650
[Epoch 107] ogbg-molclintox: 0.802956 test loss: 0.507635
[Epoch 108; Iter    10/   40] train: loss: 0.0023777
[Epoch 108; Iter    40/   40] train: loss: 0.0043649
[Epoch 108] ogbg-molclintox: 0.962614 val loss: 0.133046
[Epoch 108] ogbg-molclintox: 0.796811 test loss: 0.526272
[Epoch 109; Iter    30/   40] train: loss: 0.0014750
[Epoch 109] ogbg-molclintox: 0.960129 val loss: 0.129970
[Epoch 109] ogbg-molclintox: 0.802582 test loss: 0.457979
[Epoch 110; Iter    20/   40] train: loss: 0.0022063
[Epoch 110] ogbg-molclintox: 0.966785 val loss: 0.142546
[Epoch 110] ogbg-molclintox: 0.808140 test loss: 0.558076
[Epoch 111; Iter    10/   40] train: loss: 0.0032196
[Epoch 111; Iter    40/   40] train: loss: 0.0023281
[Epoch 111] ogbg-molclintox: 0.967259 val loss: 0.145682
[Epoch 111] ogbg-molclintox: 0.805567 test loss: 0.541748
[Epoch 112; Iter    30/   40] train: loss: 0.0078902
[Epoch 112] ogbg-molclintox: 0.962926 val loss: 0.143481
[Epoch 112] ogbg-molclintox: 0.794611 test loss: 0.493151
[Epoch 113; Iter    20/   40] train: loss: 0.0405338
[Epoch 113] ogbg-molclintox: 0.964799 val loss: 0.179376
[Epoch 113] ogbg-molclintox: 0.803606 test loss: 0.621412
[Epoch 114; Iter    10/   40] train: loss: 0.0006050
[Epoch 114; Iter    40/   40] train: loss: 0.0010993
[Epoch 114] ogbg-molclintox: 0.963513 val loss: 0.147685
[Epoch 114] ogbg-molclintox: 0.797947 test loss: 0.508538
[Epoch 115; Iter    30/   40] train: loss: 0.0007342
[Epoch 115] ogbg-molclintox: 0.964350 val loss: 0.126493
[Epoch 115] ogbg-molclintox: 0.811988 test loss: 0.526828
[Epoch 116; Iter    20/   40] train: loss: 0.0052723
[Epoch 116] ogbg-molclintox: 0.960516 val loss: 0.131063
[Epoch 116] ogbg-molclintox: 0.816197 test loss: 0.539434
[Epoch 117; Iter    10/   40] train: loss: 0.0010736
[Epoch 117; Iter    40/   40] train: loss: 0.0003149
[Epoch 117] ogbg-molclintox: 0.962814 val loss: 0.147023
[Epoch 117] ogbg-molclintox: 0.789203 test loss: 0.543346
[Epoch 118; Iter    30/   40] train: loss: 0.0020351
[Epoch 118] ogbg-molclintox: 0.961415 val loss: 0.147621
[Epoch 118] ogbg-molclintox: 0.795948 test loss: 0.564082
[Epoch 119; Iter    20/   40] train: loss: 0.0016029
[Epoch 119] ogbg-molclintox: 0.962364 val loss: 0.153321
[Epoch 119] ogbg-molclintox: 0.795836 test loss: 0.621484
[Epoch 120; Iter    10/   40] train: loss: 0.0011443
[Epoch 120; Iter    40/   40] train: loss: 0.0007590
[Epoch 120] ogbg-molclintox: 0.955459 val loss: 0.167204
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.796124 test loss: 0.579684
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 38.
Statistics on  val_best_checkpoint
mean_pred: 0.17743881046772003
std_pred: 3.4662959575653076
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.9267404905259813
rocauc: 0.9849281000689452
ogbg-molclintox: 0.9849281000689452
OGBNanLabelBCEWithLogitsLoss: 0.09912309125065803
Statistics on  test
mean_pred: 0.11689049750566483
std_pred: 3.2605574131011963
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6714797615118184
rocauc: 0.7844029124526466
ogbg-molclintox: 0.7844029124526466
OGBNanLabelBCEWithLogitsLoss: 0.35823089331388475
Statistics on  train
mean_pred: 0.16302864253520966
std_pred: 4.206413269042969
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9588419723295935
rocauc: 0.9861789476052353
ogbg-molclintox: 0.9861789476052353
OGBNanLabelBCEWithLogitsLoss: 0.09796324050985276
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --seed 4 --device cuda:2
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --seed 5 --device cuda:2
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --seed 6 --device cuda:2
All runs completed.
[Epoch 76] ogbg-molclintox: 0.944470 val loss: 0.086432
[Epoch 76] ogbg-molclintox: 0.807610 test loss: 0.237712
[Epoch 77; Iter    20/   40] train: loss: 0.0187346
[Epoch 77] ogbg-molclintox: 0.934768 val loss: 0.087760
[Epoch 77] ogbg-molclintox: 0.799878 test loss: 0.284268
[Epoch 78; Iter    10/   40] train: loss: 0.0207055
[Epoch 78; Iter    40/   40] train: loss: 0.1743321
[Epoch 78] ogbg-molclintox: 0.939364 val loss: 0.097148
[Epoch 78] ogbg-molclintox: 0.808047 test loss: 0.275677
[Epoch 79; Iter    30/   40] train: loss: 0.0771102
[Epoch 79] ogbg-molclintox: 0.962227 val loss: 0.080390
[Epoch 79] ogbg-molclintox: 0.801051 test loss: 0.334013
[Epoch 80; Iter    20/   40] train: loss: 0.0191530
[Epoch 80] ogbg-molclintox: 0.911856 val loss: 0.103113
[Epoch 80] ogbg-molclintox: 0.785886 test loss: 0.251943
[Epoch 81; Iter    10/   40] train: loss: 0.0125752
[Epoch 81; Iter    40/   40] train: loss: 0.0156166
[Epoch 81] ogbg-molclintox: 0.958256 val loss: 0.086732
[Epoch 81] ogbg-molclintox: 0.852893 test loss: 0.292115
[Epoch 82; Iter    30/   40] train: loss: 0.0337760
[Epoch 82] ogbg-molclintox: 0.949004 val loss: 0.087770
[Epoch 82] ogbg-molclintox: 0.818353 test loss: 0.298413
[Epoch 83; Iter    20/   40] train: loss: 0.1250998
[Epoch 83] ogbg-molclintox: 0.932870 val loss: 0.096052
[Epoch 83] ogbg-molclintox: 0.843861 test loss: 0.292468
[Epoch 84; Iter    10/   40] train: loss: 0.0114382
[Epoch 84; Iter    40/   40] train: loss: 0.0577720
[Epoch 84] ogbg-molclintox: 0.905113 val loss: 0.111255
[Epoch 84] ogbg-molclintox: 0.824300 test loss: 0.289888
[Epoch 85; Iter    30/   40] train: loss: 0.0130932
[Epoch 85] ogbg-molclintox: 0.916351 val loss: 0.110091
[Epoch 85] ogbg-molclintox: 0.794444 test loss: 0.336510
[Epoch 86; Iter    20/   40] train: loss: 0.0062343
[Epoch 86] ogbg-molclintox: 0.951938 val loss: 0.092531
[Epoch 86] ogbg-molclintox: 0.801529 test loss: 0.342064
[Epoch 87; Iter    10/   40] train: loss: 0.0095333
[Epoch 87; Iter    40/   40] train: loss: 0.1161944
[Epoch 87] ogbg-molclintox: 0.947106 val loss: 0.091170
[Epoch 87] ogbg-molclintox: 0.800140 test loss: 0.349329
[Epoch 88; Iter    30/   40] train: loss: 0.0517316
[Epoch 88] ogbg-molclintox: 0.943023 val loss: 0.091184
[Epoch 88] ogbg-molclintox: 0.830231 test loss: 0.329966
[Epoch 89; Iter    20/   40] train: loss: 0.0729021
[Epoch 89] ogbg-molclintox: 0.908022 val loss: 0.115793
[Epoch 89] ogbg-molclintox: 0.785524 test loss: 0.390715
[Epoch 90; Iter    10/   40] train: loss: 0.0348269
[Epoch 90; Iter    40/   40] train: loss: 0.0022773
[Epoch 90] ogbg-molclintox: 0.921158 val loss: 0.106986
[Epoch 90] ogbg-molclintox: 0.801675 test loss: 0.356791
[Epoch 91; Iter    30/   40] train: loss: 0.0028792
[Epoch 91] ogbg-molclintox: 0.912942 val loss: 0.122702
[Epoch 91] ogbg-molclintox: 0.803449 test loss: 0.469103
[Epoch 92; Iter    20/   40] train: loss: 0.0567166
[Epoch 92] ogbg-molclintox: 0.868851 val loss: 0.133384
[Epoch 92] ogbg-molclintox: 0.773780 test loss: 0.338628
[Epoch 93; Iter    10/   40] train: loss: 0.0234195
[Epoch 93; Iter    40/   40] train: loss: 0.0117204
[Epoch 93] ogbg-molclintox: 0.924841 val loss: 0.106174
[Epoch 93] ogbg-molclintox: 0.797578 test loss: 0.397738
[Epoch 94; Iter    30/   40] train: loss: 0.0051940
[Epoch 94] ogbg-molclintox: 0.942734 val loss: 0.098689
[Epoch 94] ogbg-molclintox: 0.788071 test loss: 0.393060
[Epoch 95; Iter    20/   40] train: loss: 0.0087109
[Epoch 95] ogbg-molclintox: 0.927364 val loss: 0.145499
[Epoch 95] ogbg-molclintox: 0.784101 test loss: 0.638764
[Epoch 96; Iter    10/   40] train: loss: 0.0281704
[Epoch 96; Iter    40/   40] train: loss: 0.0087579
[Epoch 96] ogbg-molclintox: 0.956520 val loss: 0.101129
[Epoch 96] ogbg-molclintox: 0.797592 test loss: 0.476646
[Epoch 97; Iter    30/   40] train: loss: 0.0050214
[Epoch 97] ogbg-molclintox: 0.901054 val loss: 0.118286
[Epoch 97] ogbg-molclintox: 0.766373 test loss: 0.453877
[Epoch 98; Iter    20/   40] train: loss: 0.0026961
[Epoch 98] ogbg-molclintox: 0.858758 val loss: 0.134501
[Epoch 98] ogbg-molclintox: 0.772159 test loss: 0.441164
[Epoch 99; Iter    10/   40] train: loss: 0.0040129
[Epoch 99; Iter    40/   40] train: loss: 0.0012460
[Epoch 99] ogbg-molclintox: 0.903823 val loss: 0.094903
[Epoch 99] ogbg-molclintox: 0.776219 test loss: 0.415373
[Epoch 100; Iter    30/   40] train: loss: 0.0024820
[Epoch 100] ogbg-molclintox: 0.927687 val loss: 0.097298
[Epoch 100] ogbg-molclintox: 0.759067 test loss: 0.469708
[Epoch 101; Iter    20/   40] train: loss: 0.0017658
[Epoch 101] ogbg-molclintox: 0.956858 val loss: 0.092611
[Epoch 101] ogbg-molclintox: 0.779166 test loss: 0.482075
[Epoch 102; Iter    10/   40] train: loss: 0.0018686
[Epoch 102; Iter    40/   40] train: loss: 0.0284936
[Epoch 102] ogbg-molclintox: 0.942822 val loss: 0.106330
[Epoch 102] ogbg-molclintox: 0.778729 test loss: 0.442385
[Epoch 103; Iter    30/   40] train: loss: 0.0012172
[Epoch 103] ogbg-molclintox: 0.889113 val loss: 0.115364
[Epoch 103] ogbg-molclintox: 0.755895 test loss: 0.454642
[Epoch 104; Iter    20/   40] train: loss: 0.0016661
[Epoch 104] ogbg-molclintox: 0.927912 val loss: 0.110508
[Epoch 104] ogbg-molclintox: 0.783626 test loss: 0.474246
[Epoch 105; Iter    10/   40] train: loss: 0.0033520
[Epoch 105; Iter    40/   40] train: loss: 0.0090814
[Epoch 105] ogbg-molclintox: 0.890962 val loss: 0.118877
[Epoch 105] ogbg-molclintox: 0.775745 test loss: 0.463760
[Epoch 106; Iter    30/   40] train: loss: 0.0038986
[Epoch 106] ogbg-molclintox: 0.937527 val loss: 0.095947
[Epoch 106] ogbg-molclintox: 0.784239 test loss: 0.519352
[Epoch 107; Iter    20/   40] train: loss: 0.0172219
[Epoch 107] ogbg-molclintox: 0.957831 val loss: 0.091936
[Epoch 107] ogbg-molclintox: 0.786312 test loss: 0.530412
[Epoch 108; Iter    10/   40] train: loss: 0.0015063
[Epoch 108; Iter    40/   40] train: loss: 0.0039766
[Epoch 108] ogbg-molclintox: 0.922729 val loss: 0.098482
[Epoch 108] ogbg-molclintox: 0.798642 test loss: 0.577991
[Epoch 109; Iter    30/   40] train: loss: 0.0019926
[Epoch 109] ogbg-molclintox: 0.930597 val loss: 0.102160
[Epoch 109] ogbg-molclintox: 0.768984 test loss: 0.472182
[Epoch 110; Iter    20/   40] train: loss: 0.0032236
[Epoch 110] ogbg-molclintox: 0.938563 val loss: 0.097778
[Epoch 110] ogbg-molclintox: 0.768973 test loss: 0.542109
[Epoch 111; Iter    10/   40] train: loss: 0.0020461
[Epoch 111; Iter    40/   40] train: loss: 0.0005766
[Epoch 111] ogbg-molclintox: 0.946231 val loss: 0.102263
[Epoch 111] ogbg-molclintox: 0.790809 test loss: 0.546620
[Epoch 112; Iter    30/   40] train: loss: 0.0064059
[Epoch 112] ogbg-molclintox: 0.930846 val loss: 0.093943
[Epoch 112] ogbg-molclintox: 0.801290 test loss: 0.510622
[Epoch 113; Iter    20/   40] train: loss: 0.0070757
[Epoch 113] ogbg-molclintox: 0.933071 val loss: 0.131776
[Epoch 113] ogbg-molclintox: 0.773007 test loss: 0.411109
[Epoch 114; Iter    10/   40] train: loss: 0.0131156
[Epoch 114; Iter    40/   40] train: loss: 0.0066484
[Epoch 114] ogbg-molclintox: 0.909143 val loss: 0.109219
[Epoch 114] ogbg-molclintox: 0.768113 test loss: 0.501750
[Epoch 115; Iter    30/   40] train: loss: 0.0052722
[Epoch 115] ogbg-molclintox: 0.896869 val loss: 0.117665
[Epoch 115] ogbg-molclintox: 0.781026 test loss: 0.471524
[Epoch 116; Iter    20/   40] train: loss: 0.0025184
[Epoch 116] ogbg-molclintox: 0.880061 val loss: 0.119802
[Epoch 116] ogbg-molclintox: 0.750121 test loss: 0.480324
[Epoch 117; Iter    10/   40] train: loss: 0.0009231
[Epoch 117; Iter    40/   40] train: loss: 0.0081433
[Epoch 117] ogbg-molclintox: 0.867498 val loss: 0.131289
[Epoch 117] ogbg-molclintox: 0.777930 test loss: 0.526165
[Epoch 118; Iter    30/   40] train: loss: 0.0008314
[Epoch 118] ogbg-molclintox: 0.916474 val loss: 0.126483
[Epoch 118] ogbg-molclintox: 0.795067 test loss: 0.531478
[Epoch 119; Iter    20/   40] train: loss: 0.0029947
[Epoch 119] ogbg-molclintox: 0.918821 val loss: 0.109738
[Epoch 119] ogbg-molclintox: 0.783664 test loss: 0.482618
[Epoch 120; Iter    10/   40] train: loss: 0.0136050
[Epoch 120; Iter    40/   40] train: loss: 0.0026902
[Epoch 120] ogbg-molclintox: 0.929834 val loss: 0.105867
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.805974 test loss: 0.516857
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 57.
Statistics on  val_best_checkpoint
mean_pred: 0.21574188768863678
std_pred: 5.418295383453369
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.9034361816787728
rocauc: 0.9857398470074527
ogbg-molclintox: 0.9857398470074527
OGBNanLabelBCEWithLogitsLoss: 0.06034768824465573
Statistics on  test
mean_pred: 0.09538175910711288
std_pred: 4.474916934967041
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.654916685595052
rocauc: 0.8359503701386717
ogbg-molclintox: 0.8359503701386717
OGBNanLabelBCEWithLogitsLoss: 0.33379224836826327
Statistics on  train
mean_pred: 0.176614448428154
std_pred: 5.0819220542907715
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9725882104543204
rocauc: 0.9953467103638503
ogbg-molclintox: 0.9953467103638503
OGBNanLabelBCEWithLogitsLoss: 0.06306596689391882
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --seed 4 --device cuda:1
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --seed 5 --device cuda:1
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --seed 6 --device cuda:1
All runs completed.
[Epoch 76] ogbg-molclintox: 0.946769 val loss: 0.438364
[Epoch 76] ogbg-molclintox: 0.809437 test loss: 0.555207
[Epoch 77; Iter    20/   40] train: loss: 0.0038683
[Epoch 77] ogbg-molclintox: 0.947756 val loss: 0.530580
[Epoch 77] ogbg-molclintox: 0.805276 test loss: 0.485053
[Epoch 78; Iter    10/   40] train: loss: 0.0030140
[Epoch 78; Iter    40/   40] train: loss: 0.0700043
[Epoch 78] ogbg-molclintox: 0.938191 val loss: 0.634100
[Epoch 78] ogbg-molclintox: 0.805489 test loss: 0.598974
[Epoch 79; Iter    30/   40] train: loss: 0.0349137
[Epoch 79] ogbg-molclintox: 0.933770 val loss: 0.586147
[Epoch 79] ogbg-molclintox: 0.806538 test loss: 0.575380
[Epoch 80; Iter    20/   40] train: loss: 0.0028778
[Epoch 80] ogbg-molclintox: 0.929075 val loss: 0.951464
[Epoch 80] ogbg-molclintox: 0.801279 test loss: 0.859246
[Epoch 81; Iter    10/   40] train: loss: 0.0027937
[Epoch 81; Iter    40/   40] train: loss: 0.0018134
[Epoch 81] ogbg-molclintox: 0.935369 val loss: 0.839426
[Epoch 81] ogbg-molclintox: 0.802004 test loss: 0.772566
[Epoch 82; Iter    30/   40] train: loss: 0.0217337
[Epoch 82] ogbg-molclintox: 0.923706 val loss: 0.894089
[Epoch 82] ogbg-molclintox: 0.801615 test loss: 0.816017
[Epoch 83; Iter    20/   40] train: loss: 0.0079236
[Epoch 83] ogbg-molclintox: 0.887904 val loss: 1.075541
[Epoch 83] ogbg-molclintox: 0.791735 test loss: 0.965825
[Epoch 84; Iter    10/   40] train: loss: 0.0015885
[Epoch 84; Iter    40/   40] train: loss: 0.0462873
[Epoch 84] ogbg-molclintox: 0.939090 val loss: 0.939416
[Epoch 84] ogbg-molclintox: 0.797992 test loss: 0.812935
[Epoch 85; Iter    30/   40] train: loss: 0.0010333
[Epoch 85] ogbg-molclintox: 0.904463 val loss: 1.115591
[Epoch 85] ogbg-molclintox: 0.781915 test loss: 0.964595
[Epoch 86; Iter    20/   40] train: loss: 0.0023328
[Epoch 86] ogbg-molclintox: 0.881723 val loss: 1.146895
[Epoch 86] ogbg-molclintox: 0.756833 test loss: 1.100215
[Epoch 87; Iter    10/   40] train: loss: 0.0040670
[Epoch 87; Iter    40/   40] train: loss: 0.0100113
[Epoch 87] ogbg-molclintox: 0.861556 val loss: 1.011052
[Epoch 87] ogbg-molclintox: 0.757072 test loss: 0.883638
[Epoch 88; Iter    30/   40] train: loss: 0.0083234
[Epoch 88] ogbg-molclintox: 0.927153 val loss: 1.179984
[Epoch 88] ogbg-molclintox: 0.774105 test loss: 0.898263
[Epoch 89; Iter    20/   40] train: loss: 0.0250471
[Epoch 89] ogbg-molclintox: 0.939652 val loss: 1.238698
[Epoch 89] ogbg-molclintox: 0.798291 test loss: 1.017128
[Epoch 90; Iter    10/   40] train: loss: 0.0058040
[Epoch 90; Iter    40/   40] train: loss: 0.0004440
[Epoch 90] ogbg-molclintox: 0.937555 val loss: 1.460589
[Epoch 90] ogbg-molclintox: 0.789296 test loss: 1.241285
[Epoch 91; Iter    30/   40] train: loss: 0.0030086
[Epoch 91] ogbg-molclintox: 0.932410 val loss: 1.452379
[Epoch 91] ogbg-molclintox: 0.791784 test loss: 1.106746
[Epoch 92; Iter    20/   40] train: loss: 0.0235867
[Epoch 92] ogbg-molclintox: 0.919534 val loss: 1.303503
[Epoch 92] ogbg-molclintox: 0.788512 test loss: 1.207852
[Epoch 93; Iter    10/   40] train: loss: 0.0069838
[Epoch 93; Iter    40/   40] train: loss: 0.0110449
[Epoch 93] ogbg-molclintox: 0.885919 val loss: 1.956032
[Epoch 93] ogbg-molclintox: 0.769723 test loss: 1.631538
[Epoch 94; Iter    30/   40] train: loss: 0.0012643
[Epoch 94] ogbg-molclintox: 0.864852 val loss: 1.306978
[Epoch 94] ogbg-molclintox: 0.766802 test loss: 1.071557
[Epoch 95; Iter    20/   40] train: loss: 0.0026044
[Epoch 95] ogbg-molclintox: 0.863267 val loss: 1.727196
[Epoch 95] ogbg-molclintox: 0.761804 test loss: 1.373841
[Epoch 96; Iter    10/   40] train: loss: 0.0122280
[Epoch 96; Iter    40/   40] train: loss: 0.0157449
[Epoch 96] ogbg-molclintox: 0.865252 val loss: 1.795459
[Epoch 96] ogbg-molclintox: 0.760280 test loss: 1.482029
[Epoch 97; Iter    30/   40] train: loss: 0.0009075
[Epoch 97] ogbg-molclintox: 0.857335 val loss: 1.663828
[Epoch 97] ogbg-molclintox: 0.760643 test loss: 1.323672
[Epoch 98; Iter    20/   40] train: loss: 0.0007213
[Epoch 98] ogbg-molclintox: 0.856636 val loss: 1.591488
[Epoch 98] ogbg-molclintox: 0.759268 test loss: 1.257128
[Epoch 99; Iter    10/   40] train: loss: 0.0037881
[Epoch 99; Iter    40/   40] train: loss: 0.0004853
[Epoch 99] ogbg-molclintox: 0.836581 val loss: 1.793287
[Epoch 99] ogbg-molclintox: 0.751562 test loss: 1.453859
[Epoch 100; Iter    30/   40] train: loss: 0.0004971
[Epoch 100] ogbg-molclintox: 0.823519 val loss: 1.689361
[Epoch 100] ogbg-molclintox: 0.738560 test loss: 1.355036
[Epoch 101; Iter    20/   40] train: loss: 0.0015424
[Epoch 101] ogbg-molclintox: 0.838630 val loss: 1.789858
[Epoch 101] ogbg-molclintox: 0.755309 test loss: 1.365220
[Epoch 102; Iter    10/   40] train: loss: 0.0011768
[Epoch 102; Iter    40/   40] train: loss: 0.0035181
[Epoch 102] ogbg-molclintox: 0.836307 val loss: 1.656209
[Epoch 102] ogbg-molclintox: 0.756321 test loss: 1.222090
[Epoch 103; Iter    30/   40] train: loss: 0.0006727
[Epoch 103] ogbg-molclintox: 0.835970 val loss: 1.736764
[Epoch 103] ogbg-molclintox: 0.752761 test loss: 1.400658
[Epoch 104; Iter    20/   40] train: loss: 0.0006167
[Epoch 104] ogbg-molclintox: 0.839354 val loss: 1.462190
[Epoch 104] ogbg-molclintox: 0.762092 test loss: 1.187783
[Epoch 105; Iter    10/   40] train: loss: 0.0013281
[Epoch 105; Iter    40/   40] train: loss: 0.0046852
[Epoch 105] ogbg-molclintox: 0.787943 val loss: 1.830343
[Epoch 105] ogbg-molclintox: 0.731440 test loss: 1.428112
[Epoch 106; Iter    30/   40] train: loss: 0.0017047
[Epoch 106] ogbg-molclintox: 0.821084 val loss: 1.474142
[Epoch 106] ogbg-molclintox: 0.751002 test loss: 1.235940
[Epoch 107; Iter    20/   40] train: loss: 0.0011141
[Epoch 107] ogbg-molclintox: 0.845784 val loss: 1.428256
[Epoch 107] ogbg-molclintox: 0.765278 test loss: 1.103752
[Epoch 108; Iter    10/   40] train: loss: 0.0006240
[Epoch 108; Iter    40/   40] train: loss: 0.0042186
[Epoch 108] ogbg-molclintox: 0.827329 val loss: 2.030588
[Epoch 108] ogbg-molclintox: 0.755073 test loss: 1.561373
[Epoch 109; Iter    30/   40] train: loss: 0.0012402
[Epoch 109] ogbg-molclintox: 0.838380 val loss: 2.284578
[Epoch 109] ogbg-molclintox: 0.757808 test loss: 1.687830
[Epoch 110; Iter    20/   40] train: loss: 0.0014194
[Epoch 110] ogbg-molclintox: 0.850606 val loss: 2.326641
[Epoch 110] ogbg-molclintox: 0.762977 test loss: 1.804638
[Epoch 111; Iter    10/   40] train: loss: 0.0010315
[Epoch 111; Iter    40/   40] train: loss: 0.0003686
[Epoch 111] ogbg-molclintox: 0.838517 val loss: 1.902988
[Epoch 111] ogbg-molclintox: 0.753374 test loss: 1.431470
[Epoch 112; Iter    30/   40] train: loss: 0.0006992
[Epoch 112] ogbg-molclintox: 0.841627 val loss: 2.420863
[Epoch 112] ogbg-molclintox: 0.753508 test loss: 1.826210
[Epoch 113; Iter    20/   40] train: loss: 0.0007853
[Epoch 113] ogbg-molclintox: 0.831387 val loss: 2.210317
[Epoch 113] ogbg-molclintox: 0.746703 test loss: 1.679785
[Epoch 114; Iter    10/   40] train: loss: 0.0009186
[Epoch 114; Iter    40/   40] train: loss: 0.0022932
[Epoch 114] ogbg-molclintox: 0.833485 val loss: 2.307882
[Epoch 114] ogbg-molclintox: 0.751062 test loss: 1.787900
[Epoch 115; Iter    30/   40] train: loss: 0.0059694
[Epoch 115] ogbg-molclintox: 0.837881 val loss: 2.394940
[Epoch 115] ogbg-molclintox: 0.759717 test loss: 1.731139
[Epoch 116; Iter    20/   40] train: loss: 0.0005185
[Epoch 116] ogbg-molclintox: 0.832248 val loss: 1.927234
[Epoch 116] ogbg-molclintox: 0.746829 test loss: 1.383979
[Epoch 117; Iter    10/   40] train: loss: 0.0005319
[Epoch 117; Iter    40/   40] train: loss: 0.0074819
[Epoch 117] ogbg-molclintox: 0.821660 val loss: 2.903858
[Epoch 117] ogbg-molclintox: 0.738321 test loss: 2.150935
[Epoch 118; Iter    30/   40] train: loss: 0.0003666
[Epoch 118] ogbg-molclintox: 0.827079 val loss: 2.177191
[Epoch 118] ogbg-molclintox: 0.742381 test loss: 1.552059
[Epoch 119; Iter    20/   40] train: loss: 0.0009100
[Epoch 119] ogbg-molclintox: 0.821710 val loss: 2.267941
[Epoch 119] ogbg-molclintox: 0.741469 test loss: 1.641333
[Epoch 120; Iter    10/   40] train: loss: 0.0060183
[Epoch 120; Iter    40/   40] train: loss: 0.0055961
[Epoch 120] ogbg-molclintox: 0.818937 val loss: 2.127667
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.737398 test loss: 1.553917
[Epoch 121; Iter    30/   40] train: loss: 0.0008630
[Epoch 121] ogbg-molclintox: 0.821934 val loss: 2.510373
[Epoch 121] ogbg-molclintox: 0.745791 test loss: 1.855270
[Epoch 122; Iter    20/   40] train: loss: 0.0002577
[Epoch 122] ogbg-molclintox: 0.830101 val loss: 2.568410
[Epoch 122] ogbg-molclintox: 0.753460 test loss: 1.979884
[Epoch 123; Iter    10/   40] train: loss: 0.0002558
[Epoch 123; Iter    40/   40] train: loss: 0.0028842
[Epoch 123] ogbg-molclintox: 0.838380 val loss: 2.377404
[Epoch 123] ogbg-molclintox: 0.755533 test loss: 1.789371
[Epoch 124; Iter    30/   40] train: loss: 0.0013422
[Epoch 124] ogbg-molclintox: 0.836894 val loss: 2.085958
[Epoch 124] ogbg-molclintox: 0.758932 test loss: 1.581394
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 124 epochs. Best model checkpoint was in epoch 64.
Statistics on  val_best_checkpoint
mean_pred: 1.2179646492004395
std_pred: 5.051664352416992
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6944697356131715
rocauc: 0.9721297481860862
ogbg-molclintox: 0.9721297481860862
OGBNanLabelBCEWithLogitsLoss: 0.23609507214277983
Statistics on  test
mean_pred: 0.9849038124084473
std_pred: 3.900413990020752
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.6359376279295462
rocauc: 0.7909507176867203
ogbg-molclintox: 0.7909507176867203
OGBNanLabelBCEWithLogitsLoss: 0.571328192949295
Statistics on  train
mean_pred: 0.2952720522880554
std_pred: 9.394039154052734
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9819581970814175
rocauc: 0.9924729090018709
ogbg-molclintox: 0.9924729090018709
OGBNanLabelBCEWithLogitsLoss: 0.039924886275548487
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --seed 4 --device cuda:3
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --seed 5 --device cuda:3
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --seed 6 --device cuda:3
All runs completed.
[Epoch 76] ogbg-molclintox: 0.987276 val loss: 0.052430
[Epoch 76] ogbg-molclintox: 0.836951 test loss: 0.205426
[Epoch 77; Iter    20/   40] train: loss: 0.0140063
[Epoch 77] ogbg-molclintox: 0.981207 val loss: 0.056472
[Epoch 77] ogbg-molclintox: 0.841762 test loss: 0.193828
[Epoch 78; Iter    10/   40] train: loss: 0.0656557
[Epoch 78; Iter    40/   40] train: loss: 0.2616974
[Epoch 78] ogbg-molclintox: 0.989373 val loss: 0.048586
[Epoch 78] ogbg-molclintox: 0.831244 test loss: 0.193857
[Epoch 79; Iter    30/   40] train: loss: 0.1405250
[Epoch 79] ogbg-molclintox: 0.984317 val loss: 0.058857
[Epoch 79] ogbg-molclintox: 0.797077 test loss: 0.212887
[Epoch 80; Iter    20/   40] train: loss: 0.0264617
[Epoch 80] ogbg-molclintox: 0.932009 val loss: 0.093928
[Epoch 80] ogbg-molclintox: 0.730507 test loss: 0.225095
[Epoch 81; Iter    10/   40] train: loss: 0.0976350
[Epoch 81; Iter    40/   40] train: loss: 0.0777568
[Epoch 81] ogbg-molclintox: 0.982106 val loss: 0.065228
[Epoch 81] ogbg-molclintox: 0.831143 test loss: 0.198373
[Epoch 82; Iter    30/   40] train: loss: 0.0745839
[Epoch 82] ogbg-molclintox: 0.954085 val loss: 0.090860
[Epoch 82] ogbg-molclintox: 0.775278 test loss: 0.233103
[Epoch 83; Iter    20/   40] train: loss: 0.0627878
[Epoch 83] ogbg-molclintox: 0.968271 val loss: 0.075763
[Epoch 83] ogbg-molclintox: 0.722711 test loss: 0.233167
[Epoch 84; Iter    10/   40] train: loss: 0.0405877
[Epoch 84; Iter    40/   40] train: loss: 0.1033799
[Epoch 84] ogbg-molclintox: 0.986439 val loss: 0.057420
[Epoch 84] ogbg-molclintox: 0.843111 test loss: 0.200586
[Epoch 85; Iter    30/   40] train: loss: 0.0332754
[Epoch 85] ogbg-molclintox: 0.954173 val loss: 0.086339
[Epoch 85] ogbg-molclintox: 0.746658 test loss: 0.214044
[Epoch 86; Iter    20/   40] train: loss: 0.0428752
[Epoch 86] ogbg-molclintox: 0.972091 val loss: 0.081365
[Epoch 86] ogbg-molclintox: 0.799214 test loss: 0.230142
[Epoch 87; Iter    10/   40] train: loss: 0.0256927
[Epoch 87; Iter    40/   40] train: loss: 0.0573903
[Epoch 87] ogbg-molclintox: 0.979060 val loss: 0.068019
[Epoch 87] ogbg-molclintox: 0.782398 test loss: 0.243935
[Epoch 88; Iter    30/   40] train: loss: 0.0687884
[Epoch 88] ogbg-molclintox: 0.968071 val loss: 0.077866
[Epoch 88] ogbg-molclintox: 0.783735 test loss: 0.227051
[Epoch 89; Iter    20/   40] train: loss: 0.1917157
[Epoch 89] ogbg-molclintox: 0.976849 val loss: 0.065900
[Epoch 89] ogbg-molclintox: 0.821749 test loss: 0.221164
[Epoch 90; Iter    10/   40] train: loss: 0.1062596
[Epoch 90; Iter    40/   40] train: loss: 0.0146580
[Epoch 90] ogbg-molclintox: 0.969695 val loss: 0.084922
[Epoch 90] ogbg-molclintox: 0.791217 test loss: 0.227457
[Epoch 91; Iter    30/   40] train: loss: 0.0050559
[Epoch 91] ogbg-molclintox: 0.980008 val loss: 0.073957
[Epoch 91] ogbg-molclintox: 0.838651 test loss: 0.219481
[Epoch 92; Iter    20/   40] train: loss: 0.0382268
[Epoch 92] ogbg-molclintox: 0.964437 val loss: 0.065971
[Epoch 92] ogbg-molclintox: 0.824558 test loss: 0.212416
[Epoch 93; Iter    10/   40] train: loss: 0.0241363
[Epoch 93; Iter    40/   40] train: loss: 0.0113882
[Epoch 93] ogbg-molclintox: 0.984591 val loss: 0.060068
[Epoch 93] ogbg-molclintox: 0.830392 test loss: 0.223042
[Epoch 94; Iter    30/   40] train: loss: 0.0034688
[Epoch 94] ogbg-molclintox: 0.972316 val loss: 0.075400
[Epoch 94] ogbg-molclintox: 0.855714 test loss: 0.211520
[Epoch 95; Iter    20/   40] train: loss: 0.0983119
[Epoch 95] ogbg-molclintox: 0.985965 val loss: 0.069318
[Epoch 95] ogbg-molclintox: 0.812728 test loss: 0.226148
[Epoch 96; Iter    10/   40] train: loss: 0.0647696
[Epoch 96; Iter    40/   40] train: loss: 0.2170579
[Epoch 96] ogbg-molclintox: 0.974189 val loss: 0.070929
[Epoch 96] ogbg-molclintox: 0.804533 test loss: 0.232849
[Epoch 97; Iter    30/   40] train: loss: 0.0170494
[Epoch 97] ogbg-molclintox: 0.969656 val loss: 0.069706
[Epoch 97] ogbg-molclintox: 0.854675 test loss: 0.209049
[Epoch 98; Iter    20/   40] train: loss: 0.0185838
[Epoch 98] ogbg-molclintox: 0.939962 val loss: 0.079926
[Epoch 98] ogbg-molclintox: 0.794926 test loss: 0.227443
[Epoch 99; Iter    10/   40] train: loss: 0.0590154
[Epoch 99; Iter    40/   40] train: loss: 0.0147822
[Epoch 99] ogbg-molclintox: 0.965049 val loss: 0.081078
[Epoch 99] ogbg-molclintox: 0.803296 test loss: 0.223332
[Epoch 100; Iter    30/   40] train: loss: 0.0501106
[Epoch 100] ogbg-molclintox: 0.970106 val loss: 0.071110
[Epoch 100] ogbg-molclintox: 0.811503 test loss: 0.226330
[Epoch 101; Iter    20/   40] train: loss: 0.0815192
[Epoch 101] ogbg-molclintox: 0.957894 val loss: 0.081059
[Epoch 101] ogbg-molclintox: 0.798698 test loss: 0.229484
[Epoch 102; Iter    10/   40] train: loss: 0.0051017
[Epoch 102; Iter    40/   40] train: loss: 0.1027002
[Epoch 102] ogbg-molclintox: 0.974077 val loss: 0.066124
[Epoch 102] ogbg-molclintox: 0.822634 test loss: 0.213695
[Epoch 103; Iter    30/   40] train: loss: 0.0092200
[Epoch 103] ogbg-molclintox: 0.957757 val loss: 0.080697
[Epoch 103] ogbg-molclintox: 0.795139 test loss: 0.216537
[Epoch 104; Iter    20/   40] train: loss: 0.0344428
[Epoch 104] ogbg-molclintox: 0.972130 val loss: 0.077131
[Epoch 104] ogbg-molclintox: 0.796151 test loss: 0.236187
[Epoch 105; Iter    10/   40] train: loss: 0.0064170
[Epoch 105; Iter    40/   40] train: loss: 0.0136243
[Epoch 105] ogbg-molclintox: 0.976600 val loss: 0.069745
[Epoch 105] ogbg-molclintox: 0.857899 test loss: 0.206915
[Epoch 106; Iter    30/   40] train: loss: 0.0207234
[Epoch 106] ogbg-molclintox: 0.962427 val loss: 0.079340
[Epoch 106] ogbg-molclintox: 0.831741 test loss: 0.222713
[Epoch 107; Iter    20/   40] train: loss: 0.0472075
[Epoch 107] ogbg-molclintox: 0.972291 val loss: 0.064031
[Epoch 107] ogbg-molclintox: 0.832977 test loss: 0.219566
[Epoch 108; Iter    10/   40] train: loss: 0.0224746
[Epoch 108; Iter    40/   40] train: loss: 0.0071914
[Epoch 108] ogbg-molclintox: 0.965137 val loss: 0.082508
[Epoch 108] ogbg-molclintox: 0.797824 test loss: 0.237544
[Epoch 109; Iter    30/   40] train: loss: 0.0407027
[Epoch 109] ogbg-molclintox: 0.953386 val loss: 0.080210
[Epoch 109] ogbg-molclintox: 0.817061 test loss: 0.227727
[Epoch 110; Iter    20/   40] train: loss: 0.0401822
[Epoch 110] ogbg-molclintox: 0.941898 val loss: 0.087652
[Epoch 110] ogbg-molclintox: 0.802833 test loss: 0.225602
[Epoch 111; Iter    10/   40] train: loss: 0.0116380
[Epoch 111; Iter    40/   40] train: loss: 0.0013054
[Epoch 111] ogbg-molclintox: 0.949777 val loss: 0.080565
[Epoch 111] ogbg-molclintox: 0.828767 test loss: 0.222384
[Epoch 112; Iter    30/   40] train: loss: 0.0307370
[Epoch 112] ogbg-molclintox: 0.975001 val loss: 0.067953
[Epoch 112] ogbg-molclintox: 0.819661 test loss: 0.227208
[Epoch 113; Iter    20/   40] train: loss: 0.1088191
[Epoch 113] ogbg-molclintox: 0.961103 val loss: 0.079590
[Epoch 113] ogbg-molclintox: 0.826194 test loss: 0.221770
[Epoch 114; Iter    10/   40] train: loss: 0.0182720
[Epoch 114; Iter    40/   40] train: loss: 0.2321128
[Epoch 114] ogbg-molclintox: 0.966697 val loss: 0.073182
[Epoch 114] ogbg-molclintox: 0.822858 test loss: 0.228453
[Epoch 115; Iter    30/   40] train: loss: 0.1026571
[Epoch 115] ogbg-molclintox: 0.965411 val loss: 0.072223
[Epoch 115] ogbg-molclintox: 0.808817 test loss: 0.232180
[Epoch 116; Iter    20/   40] train: loss: 0.0372707
[Epoch 116] ogbg-molclintox: 0.969856 val loss: 0.079409
[Epoch 116] ogbg-molclintox: 0.833739 test loss: 0.220285
[Epoch 117; Iter    10/   40] train: loss: 0.0509687
[Epoch 117; Iter    40/   40] train: loss: 0.0093757
[Epoch 117] ogbg-molclintox: 0.978947 val loss: 0.073072
[Epoch 117] ogbg-molclintox: 0.801362 test loss: 0.233982
[Epoch 118; Iter    30/   40] train: loss: 0.0377989
[Epoch 118] ogbg-molclintox: 0.966423 val loss: 0.079394
[Epoch 118] ogbg-molclintox: 0.834090 test loss: 0.219370
[Epoch 119; Iter    20/   40] train: loss: 0.0497565
[Epoch 119] ogbg-molclintox: 0.957033 val loss: 0.084826
[Epoch 119] ogbg-molclintox: 0.811914 test loss: 0.224965
[Epoch 120; Iter    10/   40] train: loss: 0.1437366
[Epoch 120; Iter    40/   40] train: loss: 0.0055138
[Epoch 120] ogbg-molclintox: 0.973602 val loss: 0.067955
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.869530 test loss: 0.212148
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 47.
Statistics on  val_best_checkpoint
mean_pred: 0.222097709774971
std_pred: 5.602169036865234
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.9999757187257187
rocauc: 0.9993006993006993
ogbg-molclintox: 0.9993006993006993
OGBNanLabelBCEWithLogitsLoss: 0.05572129786014557
Statistics on  test
mean_pred: 0.16637589037418365
std_pred: 4.990085124969482
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.693798424321
rocauc: 0.8507454905640705
ogbg-molclintox: 0.8507454905640705
OGBNanLabelBCEWithLogitsLoss: 0.20127553939819337
Statistics on  train
mean_pred: 0.2020862102508545
std_pred: 5.751931667327881
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.90256110181349
rocauc: 0.9767087720317098
ogbg-molclintox: 0.9767087720317098
OGBNanLabelBCEWithLogitsLoss: 0.10188012951985001
[Epoch 76] ogbg-molclintox: 0.969220 val loss: 0.080703
[Epoch 76] ogbg-molclintox: 0.812821 test loss: 0.224729
[Epoch 77; Iter    20/   40] train: loss: 0.0183359
[Epoch 77] ogbg-molclintox: 0.982444 val loss: 0.064930
[Epoch 77] ogbg-molclintox: 0.816493 test loss: 0.220159
[Epoch 78; Iter    10/   40] train: loss: 0.0077401
[Epoch 78; Iter    40/   40] train: loss: 0.0050406
[Epoch 78] ogbg-molclintox: 0.978810 val loss: 0.091045
[Epoch 78] ogbg-molclintox: 0.814633 test loss: 0.251806
[Epoch 79; Iter    30/   40] train: loss: 0.0865549
[Epoch 79] ogbg-molclintox: 0.956945 val loss: 0.094143
[Epoch 79] ogbg-molclintox: 0.799318 test loss: 0.243682
[Epoch 80; Iter    20/   40] train: loss: 0.0536749
[Epoch 80] ogbg-molclintox: 0.976463 val loss: 0.065908
[Epoch 80] ogbg-molclintox: 0.788325 test loss: 0.256483
[Epoch 81; Iter    10/   40] train: loss: 0.0292201
[Epoch 81; Iter    40/   40] train: loss: 0.0093906
[Epoch 81] ogbg-molclintox: 0.967621 val loss: 0.073041
[Epoch 81] ogbg-molclintox: 0.784291 test loss: 0.230824
[Epoch 82; Iter    30/   40] train: loss: 0.0181183
[Epoch 82] ogbg-molclintox: 0.975201 val loss: 0.066528
[Epoch 82] ogbg-molclintox: 0.776832 test loss: 0.263269
[Epoch 83; Iter    20/   40] train: loss: 0.0114132
[Epoch 83] ogbg-molclintox: 0.958007 val loss: 0.092500
[Epoch 83] ogbg-molclintox: 0.783253 test loss: 0.241918
[Epoch 84; Iter    10/   40] train: loss: 0.0163525
[Epoch 84; Iter    40/   40] train: loss: 0.0147301
[Epoch 84] ogbg-molclintox: 0.955884 val loss: 0.086553
[Epoch 84] ogbg-molclintox: 0.815682 test loss: 0.230067
[Epoch 85; Iter    30/   40] train: loss: 0.0231001
[Epoch 85] ogbg-molclintox: 0.950040 val loss: 0.087200
[Epoch 85] ogbg-molclintox: 0.754782 test loss: 0.235533
[Epoch 86; Iter    20/   40] train: loss: 0.0373843
[Epoch 86] ogbg-molclintox: 0.949541 val loss: 0.090277
[Epoch 86] ogbg-molclintox: 0.812672 test loss: 0.217940
[Epoch 87; Iter    10/   40] train: loss: 0.0976380
[Epoch 87; Iter    40/   40] train: loss: 0.0120754
[Epoch 87] ogbg-molclintox: 0.976712 val loss: 0.075200
[Epoch 87] ogbg-molclintox: 0.841153 test loss: 0.244442
[Epoch 88; Iter    30/   40] train: loss: 0.0380908
[Epoch 88] ogbg-molclintox: 0.960242 val loss: 0.080930
[Epoch 88] ogbg-molclintox: 0.822951 test loss: 0.234477
[Epoch 89; Iter    20/   40] train: loss: 0.0494428
[Epoch 89] ogbg-molclintox: 0.956021 val loss: 0.104168
[Epoch 89] ogbg-molclintox: 0.792621 test loss: 0.233482
[Epoch 90; Iter    10/   40] train: loss: 0.0232002
[Epoch 90; Iter    40/   40] train: loss: 0.0352879
[Epoch 90] ogbg-molclintox: 0.958955 val loss: 0.082025
[Epoch 90] ogbg-molclintox: 0.804977 test loss: 0.250367
[Epoch 91; Iter    30/   40] train: loss: 0.0212057
[Epoch 91] ogbg-molclintox: 0.962926 val loss: 0.086324
[Epoch 91] ogbg-molclintox: 0.808787 test loss: 0.256607
[Epoch 92; Iter    20/   40] train: loss: 0.0315185
[Epoch 92] ogbg-molclintox: 0.966086 val loss: 0.085476
[Epoch 92] ogbg-molclintox: 0.810823 test loss: 0.246806
[Epoch 93; Iter    10/   40] train: loss: 0.0146942
[Epoch 93; Iter    40/   40] train: loss: 0.2332588
[Epoch 93] ogbg-molclintox: 0.970893 val loss: 0.078549
[Epoch 93] ogbg-molclintox: 0.789860 test loss: 0.265820
[Epoch 94; Iter    30/   40] train: loss: 0.0604744
[Epoch 94] ogbg-molclintox: 0.993907 val loss: 0.048824
[Epoch 94] ogbg-molclintox: 0.824800 test loss: 0.236859
[Epoch 95; Iter    20/   40] train: loss: 0.0049837
[Epoch 95] ogbg-molclintox: 0.952412 val loss: 0.105389
[Epoch 95] ogbg-molclintox: 0.791410 test loss: 0.275207
[Epoch 96; Iter    10/   40] train: loss: 0.0363946
[Epoch 96; Iter    40/   40] train: loss: 0.0284595
[Epoch 96] ogbg-molclintox: 0.959992 val loss: 0.111039
[Epoch 96] ogbg-molclintox: 0.800655 test loss: 0.285302
[Epoch 97; Iter    30/   40] train: loss: 0.0293601
[Epoch 97] ogbg-molclintox: 0.961753 val loss: 0.097494
[Epoch 97] ogbg-molclintox: 0.804327 test loss: 0.276407
[Epoch 98; Iter    20/   40] train: loss: 0.0355522
[Epoch 98] ogbg-molclintox: 0.898331 val loss: 0.124518
[Epoch 98] ogbg-molclintox: 0.776469 test loss: 0.263875
[Epoch 99; Iter    10/   40] train: loss: 0.0096982
[Epoch 99; Iter    40/   40] train: loss: 0.0343329
[Epoch 99] ogbg-molclintox: 0.959518 val loss: 0.109385
[Epoch 99] ogbg-molclintox: 0.800666 test loss: 0.263614
[Epoch 100; Iter    30/   40] train: loss: 0.0287008
[Epoch 100] ogbg-molclintox: 0.941761 val loss: 0.101279
[Epoch 100] ogbg-molclintox: 0.797043 test loss: 0.266722
[Epoch 101; Iter    20/   40] train: loss: 0.0276246
[Epoch 101] ogbg-molclintox: 0.946431 val loss: 0.093477
[Epoch 101] ogbg-molclintox: 0.794821 test loss: 0.263705
[Epoch 102; Iter    10/   40] train: loss: 0.0579837
[Epoch 102; Iter    40/   40] train: loss: 0.0064685
[Epoch 102] ogbg-molclintox: 0.927775 val loss: 0.102855
[Epoch 102] ogbg-molclintox: 0.751645 test loss: 0.258916
[Epoch 103; Iter    30/   40] train: loss: 0.0893345
[Epoch 103] ogbg-molclintox: 0.944021 val loss: 0.100404
[Epoch 103] ogbg-molclintox: 0.784601 test loss: 0.284275
[Epoch 104; Iter    20/   40] train: loss: 0.0313924
[Epoch 104] ogbg-molclintox: 0.914263 val loss: 0.124887
[Epoch 104] ogbg-molclintox: 0.799378 test loss: 0.275137
[Epoch 105; Iter    10/   40] train: loss: 0.0098173
[Epoch 105; Iter    40/   40] train: loss: 0.0435181
[Epoch 105] ogbg-molclintox: 0.908982 val loss: 0.106313
[Epoch 105] ogbg-molclintox: 0.765514 test loss: 0.258451
[Epoch 106; Iter    30/   40] train: loss: 0.1041549
[Epoch 106] ogbg-molclintox: 0.922293 val loss: 0.102133
[Epoch 106] ogbg-molclintox: 0.761528 test loss: 0.262506
[Epoch 107; Iter    20/   40] train: loss: 0.0276143
[Epoch 107] ogbg-molclintox: 0.930372 val loss: 0.097628
[Epoch 107] ogbg-molclintox: 0.762428 test loss: 0.266576
[Epoch 108; Iter    10/   40] train: loss: 0.0062586
[Epoch 108; Iter    40/   40] train: loss: 0.0352811
[Epoch 108] ogbg-molclintox: 0.941898 val loss: 0.122747
[Epoch 108] ogbg-molclintox: 0.747812 test loss: 0.269464
[Epoch 109; Iter    30/   40] train: loss: 0.0788406
[Epoch 109] ogbg-molclintox: 0.898018 val loss: 0.114894
[Epoch 109] ogbg-molclintox: 0.744290 test loss: 0.261506
[Epoch 110; Iter    20/   40] train: loss: 0.0610634
[Epoch 110] ogbg-molclintox: 0.882222 val loss: 0.122153
[Epoch 110] ogbg-molclintox: 0.811734 test loss: 0.256122
[Epoch 111; Iter    10/   40] train: loss: 0.0212433
[Epoch 111; Iter    40/   40] train: loss: 0.0230184
[Epoch 111] ogbg-molclintox: 0.956882 val loss: 0.093144
[Epoch 111] ogbg-molclintox: 0.791485 test loss: 0.270908
[Epoch 112; Iter    30/   40] train: loss: 0.0530958
[Epoch 112] ogbg-molclintox: 0.941448 val loss: 0.099952
[Epoch 112] ogbg-molclintox: 0.790510 test loss: 0.257412
[Epoch 113; Iter    20/   40] train: loss: 0.2574820
[Epoch 113] ogbg-molclintox: 0.955209 val loss: 0.104411
[Epoch 113] ogbg-molclintox: 0.769410 test loss: 0.273811
[Epoch 114; Iter    10/   40] train: loss: 0.0034365
[Epoch 114; Iter    40/   40] train: loss: 0.0026629
[Epoch 114] ogbg-molclintox: 0.925227 val loss: 0.100496
[Epoch 114] ogbg-molclintox: 0.807924 test loss: 0.270813
[Epoch 115; Iter    30/   40] train: loss: 0.0115262
[Epoch 115] ogbg-molclintox: 0.944133 val loss: 0.092078
[Epoch 115] ogbg-molclintox: 0.796169 test loss: 0.282126
[Epoch 116; Iter    20/   40] train: loss: 0.0558471
[Epoch 116] ogbg-molclintox: 0.955933 val loss: 0.085728
[Epoch 116] ogbg-molclintox: 0.784239 test loss: 0.266487
[Epoch 117; Iter    10/   40] train: loss: 0.0994785
[Epoch 117; Iter    40/   40] train: loss: 0.0040721
[Epoch 117] ogbg-molclintox: 0.946256 val loss: 0.092756
[Epoch 117] ogbg-molclintox: 0.813882 test loss: 0.254845
[Epoch 118; Iter    30/   40] train: loss: 0.0423102
[Epoch 118] ogbg-molclintox: 0.932582 val loss: 0.091675
[Epoch 118] ogbg-molclintox: 0.807248 test loss: 0.243379
[Epoch 119; Iter    20/   40] train: loss: 0.0249867
[Epoch 119] ogbg-molclintox: 0.895108 val loss: 0.120206
[Epoch 119] ogbg-molclintox: 0.815331 test loss: 0.253251
[Epoch 120; Iter    10/   40] train: loss: 0.0776350
[Epoch 120; Iter    40/   40] train: loss: 0.0046950
[Epoch 120] ogbg-molclintox: 0.932108 val loss: 0.107819
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.822002 test loss: 0.259192
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 120 epochs. Best model checkpoint was in epoch 48.
Statistics on  val_best_checkpoint
mean_pred: 0.18293479084968567
std_pred: 4.282137870788574
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 1.0
rocauc: 1.0
ogbg-molclintox: 1.0
OGBNanLabelBCEWithLogitsLoss: 0.0730767128057778
Statistics on  test
mean_pred: 0.15773430466651917
std_pred: 4.3739519119262695
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.7452966028884309
rocauc: 0.8692463420567893
ogbg-molclintox: 0.8692463420567893
OGBNanLabelBCEWithLogitsLoss: 0.17745999097824097
Statistics on  train
mean_pred: 0.19540813565254211
std_pred: 4.842502593994141
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.92890319568282
rocauc: 0.9835921117480129
ogbg-molclintox: 0.9835921117480129
OGBNanLabelBCEWithLogitsLoss: 0.09488755678758025
[Epoch 76] ogbg-molclintox: 0.984092 val loss: 0.059361
[Epoch 76] ogbg-molclintox: 0.814442 test loss: 0.249873
[Epoch 77; Iter    20/   40] train: loss: 0.0138852
[Epoch 77] ogbg-molclintox: 0.965161 val loss: 0.087848
[Epoch 77] ogbg-molclintox: 0.797589 test loss: 0.215704
[Epoch 78; Iter    10/   40] train: loss: 0.0905093
[Epoch 78; Iter    40/   40] train: loss: 0.0253500
[Epoch 78] ogbg-molclintox: 0.986327 val loss: 0.055537
[Epoch 78] ogbg-molclintox: 0.768823 test loss: 0.263303
[Epoch 79; Iter    30/   40] train: loss: 0.0246280
[Epoch 79] ogbg-molclintox: 0.987251 val loss: 0.055801
[Epoch 79] ogbg-molclintox: 0.837340 test loss: 0.239500
[Epoch 80; Iter    20/   40] train: loss: 0.0411764
[Epoch 80] ogbg-molclintox: 0.961415 val loss: 0.089251
[Epoch 80] ogbg-molclintox: 0.811742 test loss: 0.211325
[Epoch 81; Iter    10/   40] train: loss: 0.0558172
[Epoch 81; Iter    40/   40] train: loss: 0.0160279
[Epoch 81] ogbg-molclintox: 0.983505 val loss: 0.061122
[Epoch 81] ogbg-molclintox: 0.803498 test loss: 0.207512
[Epoch 82; Iter    30/   40] train: loss: 0.0485769
[Epoch 82] ogbg-molclintox: 0.984292 val loss: 0.082334
[Epoch 82] ogbg-molclintox: 0.823049 test loss: 0.234239
[Epoch 83; Iter    20/   40] train: loss: 0.0281424
[Epoch 83] ogbg-molclintox: 0.992396 val loss: 0.047356
[Epoch 83] ogbg-molclintox: 0.826436 test loss: 1.299254
[Epoch 84; Iter    10/   40] train: loss: 0.0367499
[Epoch 84; Iter    40/   40] train: loss: 0.3329104
[Epoch 84] ogbg-molclintox: 0.991197 val loss: 0.059853
[Epoch 84] ogbg-molclintox: 0.815014 test loss: 0.221243
[Epoch 85; Iter    30/   40] train: loss: 0.0098026
[Epoch 85] ogbg-molclintox: 0.984067 val loss: 0.078080
[Epoch 85] ogbg-molclintox: 0.805485 test loss: 0.576500
[Epoch 86; Iter    20/   40] train: loss: 0.0424321
[Epoch 86] ogbg-molclintox: 0.982219 val loss: 0.063494
[Epoch 86] ogbg-molclintox: 0.822074 test loss: 0.233688
[Epoch 87; Iter    10/   40] train: loss: 0.0571198
[Epoch 87; Iter    40/   40] train: loss: 0.0469389
[Epoch 87] ogbg-molclintox: 0.952637 val loss: 0.097030
[Epoch 87] ogbg-molclintox: 0.805620 test loss: 0.240672
[Epoch 88; Iter    30/   40] train: loss: 0.0214151
[Epoch 88] ogbg-molclintox: 0.942710 val loss: 0.074920
[Epoch 88] ogbg-molclintox: 0.820837 test loss: 0.265465
[Epoch 89; Iter    20/   40] train: loss: 0.0580124
[Epoch 89] ogbg-molclintox: 0.972291 val loss: 0.068593
[Epoch 89] ogbg-molclintox: 0.815216 test loss: 0.265904
[Epoch 90; Iter    10/   40] train: loss: 0.0687742
[Epoch 90; Iter    40/   40] train: loss: 0.2527331
[Epoch 90] ogbg-molclintox: 0.961503 val loss: 0.081781
[Epoch 90] ogbg-molclintox: 0.794328 test loss: 0.212907
[Epoch 91; Iter    30/   40] train: loss: 0.0658782
[Epoch 91] ogbg-molclintox: 0.963176 val loss: 0.073732
[Epoch 91] ogbg-molclintox: 0.777489 test loss: 0.223820
[Epoch 92; Iter    20/   40] train: loss: 0.0463087
[Epoch 92] ogbg-molclintox: 0.968770 val loss: 0.079284
[Epoch 92] ogbg-molclintox: 0.804671 test loss: 0.214753
[Epoch 93; Iter    10/   40] train: loss: 0.0197342
[Epoch 93; Iter    40/   40] train: loss: 0.0189065
[Epoch 93] ogbg-molclintox: 0.957694 val loss: 0.070832
[Epoch 93] ogbg-molclintox: 0.810644 test loss: 0.248149
[Epoch 94; Iter    30/   40] train: loss: 0.0285151
[Epoch 94] ogbg-molclintox: 0.958281 val loss: 0.078357
[Epoch 94] ogbg-molclintox: 0.821312 test loss: 0.240966
[Epoch 95; Iter    20/   40] train: loss: 0.0227829
[Epoch 95] ogbg-molclintox: 0.967034 val loss: 0.074732
[Epoch 95] ogbg-molclintox: 0.827008 test loss: 0.212702
[Epoch 96; Iter    10/   40] train: loss: 0.0259368
[Epoch 96; Iter    40/   40] train: loss: 0.0263231
[Epoch 96] ogbg-molclintox: 0.961415 val loss: 0.074868
[Epoch 96] ogbg-molclintox: 0.848546 test loss: 0.229214
[Epoch 97; Iter    30/   40] train: loss: 0.0212325
[Epoch 97] ogbg-molclintox: 0.968208 val loss: 0.075307
[Epoch 97] ogbg-molclintox: 0.807446 test loss: 0.221065
[Epoch 98; Iter    20/   40] train: loss: 0.0207501
[Epoch 98] ogbg-molclintox: 0.963763 val loss: 0.074787
[Epoch 98] ogbg-molclintox: 0.805773 test loss: 0.234499
[Epoch 99; Iter    10/   40] train: loss: 0.0806550
[Epoch 99; Iter    40/   40] train: loss: 0.0083069
[Epoch 99] ogbg-molclintox: 0.971954 val loss: 0.072332
[Epoch 99] ogbg-molclintox: 0.816303 test loss: 0.231961
[Epoch 100; Iter    30/   40] train: loss: 0.0084483
[Epoch 100] ogbg-molclintox: 0.973465 val loss: 0.074206
[Epoch 100] ogbg-molclintox: 0.820512 test loss: 0.211605
[Epoch 101; Iter    20/   40] train: loss: 0.0440671
[Epoch 101] ogbg-molclintox: 0.963151 val loss: 0.093682
[Epoch 101] ogbg-molclintox: 0.795015 test loss: 0.229003
[Epoch 102; Iter    10/   40] train: loss: 0.0958575
[Epoch 102; Iter    40/   40] train: loss: 0.1103909
[Epoch 102] ogbg-molclintox: 0.961166 val loss: 0.085624
[Epoch 102] ogbg-molclintox: 0.801836 test loss: 0.235927
[Epoch 103; Iter    30/   40] train: loss: 0.0962715
[Epoch 103] ogbg-molclintox: 0.965836 val loss: 0.080657
[Epoch 103] ogbg-molclintox: 0.796252 test loss: 0.237295
[Epoch 104; Iter    20/   40] train: loss: 0.0324187
[Epoch 104] ogbg-molclintox: 0.974751 val loss: 0.074705
[Epoch 104] ogbg-molclintox: 0.814554 test loss: 0.233972
[Epoch 105; Iter    10/   40] train: loss: 0.0253811
[Epoch 105; Iter    40/   40] train: loss: 0.0117484
[Epoch 105] ogbg-molclintox: 0.971118 val loss: 0.078656
[Epoch 105] ogbg-molclintox: 0.812967 test loss: 0.246608
[Epoch 106; Iter    30/   40] train: loss: 0.0967817
[Epoch 106] ogbg-molclintox: 0.975813 val loss: 0.066309
[Epoch 106] ogbg-molclintox: 0.829032 test loss: 0.245242
[Epoch 107; Iter    20/   40] train: loss: 0.0960646
[Epoch 107] ogbg-molclintox: 0.956383 val loss: 0.106287
[Epoch 107] ogbg-molclintox: 0.768308 test loss: 0.239524
[Epoch 108; Iter    10/   40] train: loss: 0.0075437
[Epoch 108; Iter    40/   40] train: loss: 0.0331048
[Epoch 108] ogbg-molclintox: 0.974976 val loss: 0.079199
[Epoch 108] ogbg-molclintox: 0.817214 test loss: 0.226836
[Epoch 109; Iter    30/   40] train: loss: 0.0679747
[Epoch 109] ogbg-molclintox: 0.961890 val loss: 0.084554
[Epoch 109] ogbg-molclintox: 0.832517 test loss: 0.238675
[Epoch 110; Iter    20/   40] train: loss: 0.0862597
[Epoch 110] ogbg-molclintox: 0.958843 val loss: 0.100806
[Epoch 110] ogbg-molclintox: 0.774180 test loss: 0.260002
[Epoch 111; Iter    10/   40] train: loss: 0.0679746
[Epoch 111; Iter    40/   40] train: loss: 0.1385207
[Epoch 111] ogbg-molclintox: 0.960716 val loss: 0.084187
[Epoch 111] ogbg-molclintox: 0.804522 test loss: 0.224621
[Epoch 112; Iter    30/   40] train: loss: 0.2692289
[Epoch 112] ogbg-molclintox: 0.969357 val loss: 0.086776
[Epoch 112] ogbg-molclintox: 0.818450 test loss: 0.235261
[Epoch 113; Iter    20/   40] train: loss: 0.0182391
[Epoch 113] ogbg-molclintox: 0.979896 val loss: 0.071045
[Epoch 113] ogbg-molclintox: 0.803935 test loss: 0.247380
[Epoch 114; Iter    10/   40] train: loss: 0.0856504
[Epoch 114; Iter    40/   40] train: loss: 0.0078403
[Epoch 114] ogbg-molclintox: 0.962115 val loss: 0.084698
[Epoch 114] ogbg-molclintox: 0.820912 test loss: 0.250469
[Epoch 115; Iter    30/   40] train: loss: 0.0210759
[Epoch 115] ogbg-molclintox: 0.965386 val loss: 0.102644
[Epoch 115] ogbg-molclintox: 0.799001 test loss: 0.242431
[Epoch 116; Iter    20/   40] train: loss: 0.0156544
[Epoch 116] ogbg-molclintox: 0.969494 val loss: 0.080824
[Epoch 116] ogbg-molclintox: 0.782674 test loss: 0.787126
[Epoch 117; Iter    10/   40] train: loss: 0.0518676
[Epoch 117; Iter    40/   40] train: loss: 0.0034500
[Epoch 117] ogbg-molclintox: 0.968883 val loss: 0.081177
[Epoch 117] ogbg-molclintox: 0.797327 test loss: 1.083747
[Epoch 118; Iter    30/   40] train: loss: 0.1496315
[Epoch 118] ogbg-molclintox: 0.978135 val loss: 0.067725
[Epoch 118] ogbg-molclintox: 0.793405 test loss: 2.873065
[Epoch 119; Iter    20/   40] train: loss: 0.0269173
[Epoch 119] ogbg-molclintox: 0.969944 val loss: 0.079039
[Epoch 119] ogbg-molclintox: 0.779839 test loss: 1.597396
[Epoch 120; Iter    10/   40] train: loss: 0.0388280
[Epoch 120; Iter    40/   40] train: loss: 0.0091178
[Epoch 120] ogbg-molclintox: 0.952050 val loss: 0.086120
/workspace/trainer/trainer.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(self.writer.log_dir, 'best_checkpoint.pt'), map_location=self.device)
[Epoch 120] ogbg-molclintox: 0.791493 test loss: 0.905753
[Epoch 121; Iter    30/   40] train: loss: 0.0211537
[Epoch 121] ogbg-molclintox: 0.955596 val loss: 0.082402
[Epoch 121] ogbg-molclintox: 0.790981 test loss: 0.229632
[Epoch 122; Iter    20/   40] train: loss: 0.0221092
[Epoch 122] ogbg-molclintox: 0.952637 val loss: 0.090249
[Epoch 122] ogbg-molclintox: 0.831180 test loss: 0.223796
[Epoch 123; Iter    10/   40] train: loss: 0.0335577
[Epoch 123; Iter    40/   40] train: loss: 0.0079199
[Epoch 123] ogbg-molclintox: 0.947268 val loss: 0.090672
[Epoch 123] ogbg-molclintox: 0.802998 test loss: 0.238709
[Epoch 124; Iter    30/   40] train: loss: 0.0410254
[Epoch 124] ogbg-molclintox: 0.936054 val loss: 0.086622
[Epoch 124] ogbg-molclintox: 0.802960 test loss: 0.233181
Early stopping criterion based on -ogbg-molclintox- that should be max reached after 124 epochs. Best model checkpoint was in epoch 64.
Statistics on  val_best_checkpoint
mean_pred: 0.2575280964374542
std_pred: 5.276273250579834
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.9742063492063491
rocauc: 0.9988262910798122
ogbg-molclintox: 0.9988262910798122
OGBNanLabelBCEWithLogitsLoss: 0.05197263262234628
Statistics on  test
mean_pred: 0.200507253408432
std_pred: 6.063806533813477
mean_targets: 0.5033783912658691
std_targets: 0.5008352994918823
prcauc: 0.7193785387731777
rocauc: 0.8341796128314739
ogbg-molclintox: 0.8341796128314739
OGBNanLabelBCEWithLogitsLoss: 0.22639201804995537
Statistics on  train
mean_pred: 0.2541411221027374
std_pred: 6.205709934234619
mean_targets: 0.5067738890647888
std_targets: 0.500059962272644
prcauc: 0.9377951835469049
rocauc: 0.9889947986643897
ogbg-molclintox: 0.9889947986643897
OGBNanLabelBCEWithLogitsLoss: 0.0743236402864568
Starting process for seed 4: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --seed 4 --device cuda:0
Starting process for seed 5: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --seed 5 --device cuda:0
Starting process for seed 6: python train.py --config configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --seed 6 --device cuda:0
All runs completed.
